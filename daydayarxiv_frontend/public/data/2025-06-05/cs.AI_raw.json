[
  {
    "arxiv_id": "2506.05636v1",
    "title": "Bayesian Inference for Correlated Human Experts and Classifiers",
    "authors": [
      "Markelle Kelly",
      "Alex Boyd",
      "Sam Showalter",
      "Mark Steyvers",
      "Padhraic Smyth"
    ],
    "abstract": "Applications of machine learning often involve making predictions based on both model outputs and the opinions of human experts. In this context, we investigate the problem of querying experts for class label predictions, using as few human queries as possible, and leveraging the class probability estimates of pre-trained classifiers. We develop a general Bayesian framework for this problem, modeling expert correlation via a joint latent representation, enabling simulation-based inference about the utility of additional expert queries, as well as inference of posterior distributions over unobserved expert labels. We apply our approach to two real-world medical classification problems, as well as to CIFAR-10H and ImageNet-16H, demonstrating substantial reductions relative to baselines in the cost of querying human experts while maintaining high prediction accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted to ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05636v1",
    "published_date": "2025-06-05 23:39:41 UTC",
    "updated_date": "2025-06-05 23:39:41 UTC"
  },
  {
    "arxiv_id": "2506.05634v1",
    "title": "AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization",
    "authors": [
      "Saeed Hedayatian",
      "Stefanos Nikolaidis"
    ],
    "abstract": "Quality-Diversity (QD) algorithms have shown remarkable success in discovering diverse, high-performing solutions, but rely heavily on hand-crafted behavioral descriptors that constrain exploration to predefined notions of diversity. Leveraging the equivalence between policies and occupancy measures, we present a theoretically grounded approach to automatically generate behavioral descriptors by embedding the occupancy measures of policies in Markov Decision Processes. Our method, AutoQD, leverages random Fourier features to approximate the Maximum Mean Discrepancy (MMD) between policy occupancy measures, creating embeddings whose distances reflect meaningful behavioral differences. A low-dimensional projection of these embeddings that captures the most behaviorally significant dimensions is then used as behavioral descriptors for off-the-shelf QD methods. We prove that our embeddings converge to true MMD distances between occupancy measures as the number of sampled trajectories and embedding dimensions increase. Through experiments in multiple continuous control tasks we demonstrate AutoQD's ability in discovering diverse policies without predefined behavioral descriptors, presenting a well-motivated alternative to prior methods in unsupervised Reinforcement Learning and QD optimization. Our approach opens new possibilities for open-ended learning and automated behavior discovery in sequential decision making settings without requiring domain-specific knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.05634v1",
    "published_date": "2025-06-05 23:34:53 UTC",
    "updated_date": "2025-06-05 23:34:53 UTC"
  },
  {
    "arxiv_id": "2506.05628v2",
    "title": "GP-MoLFormer-Sim: Test Time Molecular Optimization through Contextual Similarity Guidance",
    "authors": [
      "Jiri Navratil",
      "Jarret Ross",
      "Payel Das",
      "Youssef Mroueh",
      "Samuel C Hoffman",
      "Vijil Chenthamarakshan",
      "Brian Belgodere"
    ],
    "abstract": "The ability to design molecules while preserving similarity to a target molecule and/or property is crucial for various applications in drug discovery, chemical design, and biology. We introduce in this paper an efficient training-free method for navigating and sampling from the molecular space with a generative Chemical Language Model (CLM), while using the molecular similarity to the target as a guide. Our method leverages the contextual representations learned from the CLM itself to estimate the molecular similarity, which is then used to adjust the autoregressive sampling strategy of the CLM. At each step of the decoding process, the method tracks the distance of the current generations from the target and updates the logits to encourage the preservation of similarity in generations. We implement the method using a recently proposed $\\sim$47M parameter SMILES-based CLM, GP-MoLFormer, and therefore refer to the method as GP-MoLFormer-Sim, which enables a test-time update of the deep generative policy to reflect the contextual similarity to a set of guide molecules. The method is further integrated into a genetic algorithm (GA) and tested on a set of standard molecular optimization benchmarks involving property optimization, molecular rediscovery, and structure-based drug design. Results show that, GP-MoLFormer-Sim, combined with GA (GP-MoLFormer-Sim+GA) outperforms existing training-free baseline methods, when the oracle remains black-box. The findings in this work are a step forward in understanding and guiding the generative mechanisms of CLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at AAAI-2026. This is extended version. 10 pages main article, 22 pages total",
    "pdf_url": "https://arxiv.org/pdf/2506.05628v2",
    "published_date": "2025-06-05 23:09:33 UTC",
    "updated_date": "2025-11-14 20:08:55 UTC"
  },
  {
    "arxiv_id": "2506.06398v1",
    "title": "Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization",
    "authors": [
      "Yin Li"
    ],
    "abstract": "Positional encodings are a core part of transformer-based models, enabling processing of sequential data without recurrence. This paper presents a theoretical framework to analyze how various positional encoding methods, including sinusoidal, learned, relative, and bias-based methods like Attention with Linear Biases (ALiBi), impact a transformer's expressiveness, generalization ability, and extrapolation to longer sequences. Expressiveness is defined via function approximation, generalization bounds are established using Rademacher complexity, and new encoding methods based on orthogonal functions, such as wavelets and Legendre polynomials, are proposed. The extrapolation capacity of existing and proposed encodings is analyzed, extending ALiBi's biasing approach to a unified theoretical context. Experimental evaluation on synthetic sequence-to-sequence tasks shows that orthogonal transform-based encodings outperform traditional sinusoidal encodings in generalization and extrapolation. This work addresses a critical gap in transformer theory, providing insights for design choices in natural language processing, computer vision, and other transformer applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06398v1",
    "published_date": "2025-06-05 23:02:18 UTC",
    "updated_date": "2025-06-05 23:02:18 UTC"
  },
  {
    "arxiv_id": "2506.05623v3",
    "title": "Deployability-Centric Infrastructure-as-Code Generation: Fail, Learn, Refine, and Succeed through LLM-Empowered DevOps Simulation",
    "authors": [
      "Tianyi Zhang",
      "Shidong Pan",
      "Zejun Zhang",
      "Zhenchang Xing",
      "Xiaoyu Sun"
    ],
    "abstract": "Infrastructure-as-Code (IaC) generation holds significant promise for automating cloud infrastructure provisioning. Recent advances in Large Language Models (LLMs) present a promising opportunity to democratize IaC development by generating deployable infrastructure templates from natural language descriptions. However, current evaluation focuses on syntactic correctness while ignoring deployability, the critical measure of the utility of IaC configuration files. Six state-of-the-art LLMs performed poorly on deployability, achieving only 20.8$\\sim$30.2% deployment success rate on the first attempt. In this paper, we construct DPIaC-Eval, the first deployability-centric IaC template benchmark consisting of 153 real-world scenarios cross 58 unique services. Also, we propose an LLM-based deployability-centric framework, dubbed IaCGen, that uses iterative feedback mechanism encompassing format verification, syntax checking, and live deployment stages, thereby closely mirroring the real DevOps workflows. Results show that IaCGen can make 54.6$\\sim$91.6% generated IaC templates from all evaluated models deployable in the first 10 iterations. Additionally, human-in-the-loop feedback that provide direct guidance for the deployability errors, can further boost the performance to over 90% passItr@25 on all evaluated LLMs. Furthermore, we explore the trustworthiness of the generated IaC templates on user intent alignment and security compliance. The poor performance (25.2% user requirement coverage and 8.4% security compliance rate) indicates a critical need for continued research in this domain.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by FSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2506.05623v3",
    "published_date": "2025-06-05 22:53:12 UTC",
    "updated_date": "2026-01-11 23:18:31 UTC"
  },
  {
    "arxiv_id": "2506.05619v2",
    "title": "Beyond RLHF and NLHF: Population-Proportional Alignment under an Axiomatic Framework",
    "authors": [
      "Kihyun Kim",
      "Jiawei Zhang",
      "Asuman Ozdaglar",
      "Pablo A. Parrilo"
    ],
    "abstract": "Conventional preference learning methods often prioritize opinions held more widely when aggregating preferences from multiple evaluators. This may result in policies that are biased in favor of some types of opinions or groups and susceptible to strategic manipulation. To address this issue, we develop a novel preference learning framework capable of aligning aggregate opinions and policies proportionally with the true population distribution of evaluator preferences. Grounded in social choice theory, our approach infers the feasible set of evaluator population distributions directly from pairwise comparison data. Using these estimates, the algorithm constructs a policy that satisfies foundational axioms from social choice theory, namely monotonicity and Pareto efficiency, as well as our newly-introduced axioms of population-proportional alignment and population-bounded manipulability. Moreover, we propose a soft-max relaxation method that smoothly trade-offs population-proportional alignment with the selection of the Condorcet winner (which beats all other options in pairwise comparisons). Finally, we validate the effectiveness and scalability of our approach through experiments on both tabular recommendation tasks and large language model alignment.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05619v2",
    "published_date": "2025-06-05 22:15:07 UTC",
    "updated_date": "2025-10-05 11:24:21 UTC"
  },
  {
    "arxiv_id": "2506.05617v1",
    "title": "LFA applied to CNNs: Efficient Singular Value Decomposition of Convolutional Mappings by Local Fourier Analysis",
    "authors": [
      "Antonia van Betteray",
      "Matthias Rottmann",
      "Karsten Kahl"
    ],
    "abstract": "The singular values of convolutional mappings encode interesting spectral properties, which can be used, e.g., to improve generalization and robustness of convolutional neural networks as well as to facilitate model compression. However, the computation of singular values is typically very resource-intensive. The naive approach involves unrolling the convolutional mapping along the input and channel dimensions into a large and sparse two-dimensional matrix, making the exact calculation of all singular values infeasible due to hardware limitations. In particular, this is true for matrices that represent convolutional mappings with large inputs and a high number of channels. Existing efficient methods leverage the Fast Fourier transformation (FFT) to transform convolutional mappings into the frequency domain, enabling the computation of singular values for matrices representing convolutions with larger input and channel dimensions. For a constant number of channels in a given convolution, an FFT can compute N singular values in O(N log N) complexity. In this work, we propose an approach of complexity O(N) based on local Fourier analysis, which additionally exploits the shift invariance of convolutional operators. We provide a theoretical analysis of our algorithm's runtime and validate its efficiency through numerical experiments. Our results demonstrate that our proposed method is scalable and offers a practical solution to calculate the entire set of singular values - along with the corresponding singular vectors if needed - for high-dimensional convolutional mappings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05617v1",
    "published_date": "2025-06-05 22:10:01 UTC",
    "updated_date": "2025-06-05 22:10:01 UTC"
  },
  {
    "arxiv_id": "2506.05616v2",
    "title": "Toward Greater Autonomy in Materials Discovery Agents: Unifying Planning, Physics, and Scientists",
    "authors": [
      "Lianhao Zhou",
      "Hongyi Ling",
      "Keqiang Yan",
      "Kaiji Zhao",
      "Xiaoning Qian",
      "Raymundo Arróyave",
      "Xiaofeng Qian",
      "Shuiwang Ji"
    ],
    "abstract": "We aim at designing language agents with greater autonomy for crystal materials discovery. While most of existing studies restrict the agents to perform specific tasks within predefined workflows, we aim to automate workflow planning given high-level goals and scientist intuition. To this end, we propose Materials Agent unifying Planning, Physics, and Scientists, known as MAPPS. MAPPS consists of a Workflow Planner, a Tool Code Generator, and a Scientific Mediator. The Workflow Planner uses large language models (LLMs) to generate structured and multi-step workflows. The Tool Code Generator synthesizes executable Python code for various tasks, including invoking a force field foundation model that encodes physics. The Scientific Mediator coordinates communications, facilitates scientist feedback, and ensures robustness through error reflection and recovery. By unifying planning, physics, and scientists, MAPPS enables flexible and reliable materials discovery with greater autonomy, achieving a five-fold improvement in stability, uniqueness, and novelty rates compared with prior generative models when evaluated on the MP-20 data. We provide extensive experiments across diverse tasks to show that MAPPS is a promising framework for autonomous materials discovery.",
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci",
      "physics.comp-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05616v2",
    "published_date": "2025-06-05 22:07:06 UTC",
    "updated_date": "2025-06-09 17:27:38 UTC"
  },
  {
    "arxiv_id": "2506.05615v1",
    "title": "When Maximum Entropy Misleads Policy Optimization",
    "authors": [
      "Ruipeng Zhang",
      "Ya-Chien Chang",
      "Sicun Gao"
    ],
    "abstract": "The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading approach for achieving efficient learning and robust performance across many RL tasks. However, MaxEnt methods have also been shown to struggle with performance-critical control problems in practice, where non-MaxEnt algorithms can successfully learn. In this work, we analyze how the trade-off between robustness and optimality affects the performance of MaxEnt algorithms in complex control tasks: while entropy maximization enhances exploration and robustness, it can also mislead policy optimization, leading to failure in tasks that require precise, low-entropy policies. Through experiments on a variety of control problems, we concretely demonstrate this misleading effect. Our analysis leads to better understanding of how to balance reward design and entropy maximization in challenging control problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05615v1",
    "published_date": "2025-06-05 22:03:38 UTC",
    "updated_date": "2025-06-05 22:03:38 UTC"
  },
  {
    "arxiv_id": "2507.19497v1",
    "title": "Unlimited Editions: Documenting Human Style in AI Art Generation",
    "authors": [
      "Alex Leitch",
      "Celia Chen"
    ],
    "abstract": "As AI art generation becomes increasingly sophisticated, HCI research has focused primarily on questions of detection, authenticity, and automation. This paper argues that such approaches fundamentally misunderstand how artistic value emerges from the concerns that drive human image production. Through examination of historical precedents, we demonstrate that artistic style is not only visual appearance but the resolution of creative struggle, as artists wrestle with influence and technical constraints to develop unique ways of seeing. Current AI systems flatten these human choices into reproducible patterns without preserving their provenance. We propose that HCI's role lies not only in perfecting visual output, but in developing means to document the origins and evolution of artistic style as it appears within generated visual traces. This reframing suggests new technical directions for HCI research in generative AI, focused on automatic documentation of stylistic lineage and creative choice rather than simple reproduction of aesthetic effects.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "primary_category": "cs.HC",
    "comment": "alt.CHI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.19497v1",
    "published_date": "2025-06-05 21:49:52 UTC",
    "updated_date": "2025-06-05 21:49:52 UTC"
  },
  {
    "arxiv_id": "2506.05605v1",
    "title": "Scenarios in Computing Research: A Systematic Review of the Use of Scenario Methods for Exploring the Future of Computing Technologies in Society",
    "authors": [
      "Julia Barnett",
      "Kimon Kieslich",
      "Jasmine Sinchai",
      "Nicholas Diakopoulos"
    ],
    "abstract": "Scenario building is an established method to anticipate the future of emerging technologies. Its primary goal is to use narratives to map future trajectories of technology development and sociotechnical adoption. Following this process, risks and benefits can be identified early on, and strategies can be developed that strive for desirable futures. In recent years, computer science has adopted this method and applied it to various technologies, including Artificial Intelligence (AI). Because computing technologies play such an important role in shaping modern societies, it is worth exploring how scenarios are being used as an anticipatory tool in the field -- and what possible traditional uses of scenarios are not yet covered but have the potential to enrich the field. We address this gap by conducting a systematic literature review on the use of scenario building methods in computer science over the last decade (n = 59). We guide the review along two main questions. First, we aim to uncover how scenarios are used in computing literature, focusing especially on the rationale for why scenarios are used. Second, in following the potential of scenario building to enhance inclusivity in research, we dive deeper into the participatory element of the existing scenario building literature in computer science.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages, 3 figures. Currently under review",
    "pdf_url": "https://arxiv.org/pdf/2506.05605v1",
    "published_date": "2025-06-05 21:35:24 UTC",
    "updated_date": "2025-06-05 21:35:24 UTC"
  },
  {
    "arxiv_id": "2506.05598v1",
    "title": "SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs",
    "authors": [
      "Michael J Ryan",
      "Omar Shaikh",
      "Aditri Bhagirath",
      "Daniel Frees",
      "William Held",
      "Diyi Yang"
    ],
    "abstract": "Recent calls for pluralistic alignment of Large Language Models (LLMs) encourage adapting models to diverse user preferences. However, most prior work on personalized reward models heavily rely on additional identity information, such as demographic details or a predefined set of preference categories. To this end, we introduce SynthesizeMe, an approach to inducing synthetic user personas from user interactions for personalized reward modeling. SynthesizeMe first generates and verifies reasoning to explain user preferences, then induces synthetic user personas from that reasoning, and finally filters to informative prior user interactions in order to build personalized prompts for a particular user. We show that using SynthesizeMe induced prompts improves personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining SynthesizeMe derived prompts with a reward model achieves top performance on PersonalRewardBench: a new curation of user-stratified interactions with chatbots collected from 854 users of Chatbot Arena and PRISM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025 Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2506.05598v1",
    "published_date": "2025-06-05 21:23:16 UTC",
    "updated_date": "2025-06-05 21:23:16 UTC"
  },
  {
    "arxiv_id": "2506.05596v2",
    "title": "Zero-shot protein stability prediction by inverse folding models: a free energy interpretation",
    "authors": [
      "Jes Frellsen",
      "Maher M. Kassem",
      "Tone Bengtsen",
      "Lars Olsen",
      "Kresten Lindorff-Larsen",
      "Jesper Ferkinghoff-Borg",
      "Wouter Boomsma"
    ],
    "abstract": "Inverse folding models have proven to be highly effective zero-shot predictors of protein stability. Despite this success, the link between the amino acid preferences of an inverse folding model and the free-energy considerations underlying thermodynamic stability remains incompletely understood. A better understanding would be of interest not only from a theoretical perspective, but also potentially provide the basis for stronger zero-shot stability prediction. In this paper, we take steps to clarify the free-energy foundations of inverse folding models. Our derivation reveals the standard practice of likelihood ratios as a simplistic approximation and suggests several paths towards better estimates of the relative stability. We empirically assess these approaches and demonstrate that considerable gains in zero-shot performance can be achieved with fairly simple means.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05596v2",
    "published_date": "2025-06-05 21:15:13 UTC",
    "updated_date": "2025-10-27 11:02:49 UTC"
  },
  {
    "arxiv_id": "2506.05593v1",
    "title": "Improving Neural Diarization through Speaker Attribute Attractors and Local Dependency Modeling",
    "authors": [
      "David Palzer",
      "Matthew Maciejewski",
      "Eric Fosler-Lussier"
    ],
    "abstract": "In recent years, end-to-end approaches have made notable progress in addressing the challenge of speaker diarization, which involves segmenting and identifying speakers in multi-talker recordings. One such approach, Encoder-Decoder Attractors (EDA), has been proposed to handle variable speaker counts as well as better guide the network during training. In this study, we extend the attractor paradigm by moving beyond direct speaker modeling and instead focus on representing more detailed `speaker attributes' through a multi-stage process of intermediate representations. Additionally, we enhance the architecture by replacing transformers with conformers, a convolution-augmented transformer, to model local dependencies. Experiments demonstrate improved diarization performance on the CALLHOME dataset.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Seoul, Korea, Republic of, 2024, pp. 11911-11915",
    "pdf_url": "https://arxiv.org/pdf/2506.05593v1",
    "published_date": "2025-06-05 21:12:14 UTC",
    "updated_date": "2025-06-05 21:12:14 UTC"
  },
  {
    "arxiv_id": "2506.05587v3",
    "title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark",
    "authors": [
      "Junjie Xing",
      "Yeye He",
      "Mengyu Zhou",
      "Haoyu Dong",
      "Shi Han",
      "Lingjiao Chen",
      "Dongmei Zhang",
      "Surajit Chaudhuri",
      "H. V. Jagadish"
    ],
    "abstract": "Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.\n  In this work, we introduce MMTU, a large-scale benchmark with over 28K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI GPT-5 and DeepSeek R1 score only around 69\\% and 57\\% respectively, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis.\n  Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2025; Code and data available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU",
    "pdf_url": "https://arxiv.org/pdf/2506.05587v3",
    "published_date": "2025-06-05 21:05:03 UTC",
    "updated_date": "2025-11-25 01:41:49 UTC"
  },
  {
    "arxiv_id": "2506.05586v1",
    "title": "CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions",
    "authors": [
      "Isha Puri",
      "Amit Dhurandhar",
      "Tejaswini Pedapati",
      "Kartikeyan Shanmugam",
      "Dennis Wei",
      "Kush R. Varshney"
    ],
    "abstract": "In recent years there has been a considerable amount of research on local post hoc explanations for neural networks. However, work on building interpretable neural architectures has been relatively sparse. In this paper, we present a novel neural architecture, CoFrNet, inspired by the form of continued fractions which are known to have many attractive properties in number theory, such as fast convergence of approximations to real numbers. We show that CoFrNets can be efficiently trained as well as interpreted leveraging their particular functional form. Moreover, we prove that such architectures are universal approximators based on a proof strategy that is different than the typical strategy used to prove universal approximation results for neural networks based on infinite width (or depth), which is likely to be of independent interest. We experiment on nonlinear synthetic functions and are able to accurately model as well as estimate feature attributions and even higher order terms in some cases, which is a testament to the representational power as well as interpretability of such architectures. To further showcase the power of CoFrNets, we experiment on seven real datasets spanning tabular, text and image modalities, and show that they are either comparable or significantly better than other interpretable models and multilayer perceptrons, sometimes approaching the accuracies of state-of-the-art models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05586v1",
    "published_date": "2025-06-05 21:01:06 UTC",
    "updated_date": "2025-06-05 21:01:06 UTC"
  },
  {
    "arxiv_id": "2506.05583v2",
    "title": "Conformal Prediction Adaptive to Unknown Subpopulation Shifts",
    "authors": [
      "Nien-Shao Wang",
      "Duygu Nur Yaldiz",
      "Yavuz Faruk Bakman",
      "Sai Praneeth Karimireddy"
    ],
    "abstract": "Conformal prediction is widely used to equip black-box machine learning models with uncertainty quantification, offering formal coverage guarantees under exchangeable data. However, these guarantees fail when faced with subpopulation shifts, where the test environment contains a different mix of subpopulations than the calibration data. In this work, we focus on unknown subpopulation shifts where we are not given group-information i.e. the subpopulation labels of datapoints have to be inferred. We propose new methods that provably adapt conformal prediction to such shifts, ensuring valid coverage without explicit knowledge of subpopulation structure. While existing methods in similar setups assume perfect subpopulation labels, our framework explicitly relaxes this requirement and characterizes conditions where formal coverage guarantees remain feasible. Further, our algorithms scale to high-dimensional settings and remain practical in realistic machine learning tasks. Extensive experiments on vision (with vision transformers) and language (with large language models) benchmarks demonstrate that our methods reliably maintain coverage and effectively control risks in scenarios where standard conformal prediction fails.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 7 figures, 5 tables, submitted to ICLR 2026",
    "pdf_url": "https://arxiv.org/pdf/2506.05583v2",
    "published_date": "2025-06-05 20:58:39 UTC",
    "updated_date": "2025-11-06 20:17:17 UTC"
  },
  {
    "arxiv_id": "2506.05582v1",
    "title": "Combating Misinformation in the Arab World: Challenges & Opportunities",
    "authors": [
      "Azza Abouzied",
      "Firoj Alam",
      "Raian Ali",
      "Paolo Papotti"
    ],
    "abstract": "Misinformation and disinformation pose significant risks globally, with the Arab region facing unique vulnerabilities due to geopolitical instabilities, linguistic diversity, and cultural nuances. We explore these challenges through the key facets of combating misinformation: detection, tracking, mitigation and community-engagement. We shed light on how connecting with grass-roots fact-checking organizations, understanding cultural norms, promoting social correction, and creating strong collaborative information networks can create opportunities for a more resilient information ecosystem in the Arab world.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "disinformation, misinformation, factuality, harmfulness, fake news",
    "pdf_url": "https://arxiv.org/pdf/2506.05582v1",
    "published_date": "2025-06-05 20:57:33 UTC",
    "updated_date": "2025-06-05 20:57:33 UTC"
  },
  {
    "arxiv_id": "2506.06396v1",
    "title": "Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things",
    "authors": [
      "Christopher D. Molek",
      "Roberto Fronteddu",
      "K. Brent Venable",
      "Niranjan Suri"
    ],
    "abstract": "The expansion of the Internet of Things (IoT) in the battlefield, Internet of Battlefield Things (IoBT), gives rise to new opportunities for enhancing situational awareness. To increase the potential of IoBT for situational awareness in critical decision making, the data from these devices must be processed into consumer-ready information objects, and made available to consumers on demand. To address this challenge we propose a workflow that makes use of natural language processing (NLP) to query a database technology and return a response in natural language. Our solution utilizes Large Language Models (LLMs) that are sized for edge devices to perform NLP as well as graphical databases which are well suited for dynamic connected networks which are pervasive in the IoBT. Our architecture employs LLMs for both mapping questions in natural language to Cypher database queries as well as to summarize the database output back to the user in natural language. We evaluate several medium sized LLMs for both of these tasks on a database representing publicly available data from the US Army's Multipurpose Sensing Area (MSA) at the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion parameters) outperforms the other models across all the considered metrics. Most importantly, we note that, unlike current methods, our two step approach allows the relaxation of the Exact Match (EM) requirement of the produced Cypher queries with ground truth code and, in this way, it achieves a 19.4% increase in accuracy. Our workflow lays the ground work for deploying LLMs on edge devices to enable natural language interactions with databases containing information objects for critical decision making.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06396v1",
    "published_date": "2025-06-05 20:52:13 UTC",
    "updated_date": "2025-06-05 20:52:13 UTC"
  },
  {
    "arxiv_id": "2506.05579v2",
    "title": "When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration",
    "authors": [
      "Quan Shi",
      "Carlos E. Jimenez",
      "Shunyu Yao",
      "Nick Haber",
      "Diyi Yang",
      "Karthik Narasimhan"
    ],
    "abstract": "Recent advancements in AI reasoning have driven substantial improvements across diverse tasks. A critical open question is whether these improvements also yields better knowledge transfer: the ability of models to communicate reasoning in ways humans can understand, apply, and learn from. To investigate this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a conceptual and experimental framework for Human-AI knowledge transfer capabilities and conduct the first large-scale human study (N=118) explicitly designed to measure it. In our two-phase setup, humans first ideate with an AI on problem-solving strategies, then independently implement solutions, isolating model explanations' influence on human understanding. Our findings reveal that although model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent, featuring significant outliers, indicating that knowledge transfer requires dedicated optimization. Our analysis identifies behavioral and strategic factors mediating successful knowledge transfer. We release our code, dataset, and evaluation framework to support future work on communicatively aligned models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "For code, data, visualizer, visit: https://kite-live.vercel.app",
    "pdf_url": "https://arxiv.org/pdf/2506.05579v2",
    "published_date": "2025-06-05 20:48:16 UTC",
    "updated_date": "2025-06-09 04:57:09 UTC"
  },
  {
    "arxiv_id": "2506.05577v3",
    "title": "Policy Search, Retrieval, and Composition via Task Similarity in Collaborative Agentic Systems",
    "authors": [
      "Saptarshi Nath",
      "Christos Peridis",
      "Eseoghene Benjamin",
      "Xinran Liu",
      "Soheil Kolouri",
      "Peter Kinnell",
      "Zexin Li",
      "Cong Liu",
      "Shirin Dora",
      "Andrea Soltoggio"
    ],
    "abstract": "Agentic AI aims to create systems that set their own goals, adapt proactively to change, and refine behavior through continuous experience. Recent advances suggest that, when facing multiple and unforeseen tasks, agents could benefit from sharing machine-learned knowledge and reusing policies that have already been fully or partially learned by other agents. However, how to query, select, and retrieve policies from a pool of agents, and how to integrate such policies remains a largely unexplored area. This study explores how an agent decides what knowledge to select, from whom, and when and how to integrate it in its own policy in order to accelerate its own learning. The proposed algorithm, \\emph{Modular Sharing and Composition in Collective Learning} (MOSAIC), improves learning in agentic collectives by combining (1) knowledge selection using performance signals and cosine similarity on Wasserstein task embeddings, (2) modular and transferable neural representations via masks, and (3) policy integration, composition and fine-tuning. MOSAIC outperforms isolated learners and global sharing approaches in both learning speed and overall performance, and in some cases solves tasks that isolated agents cannot. The results also demonstrate that selective, goal-driven reuse leads to less susceptibility to task interference. We also observe the emergence of self-organization, where agents solving simpler tasks accelerate the learning of harder ones through shared knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 20 figures, 8 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.05577v3",
    "published_date": "2025-06-05 20:38:11 UTC",
    "updated_date": "2025-11-20 12:09:46 UTC"
  },
  {
    "arxiv_id": "2506.05568v1",
    "title": "Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning",
    "authors": [
      "Arian Raje",
      "Baris Askin",
      "Divyansh Jhunjhunwala",
      "Gauri Joshi"
    ],
    "abstract": "Large language models (LLMs) have not yet effectively leveraged the vast amounts of edge-device data, and federated learning (FL) offers a promising paradigm to collaboratively fine-tune LLMs without transferring private edge data to the cloud. To operate within the computation and communication constraints of edge devices, recent literature on federated fine-tuning of LLMs proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient methods. However, LoRA-based methods suffer from accuracy degradation in FL settings, primarily because of data and computational heterogeneity across clients. We propose \\textsc{Ravan}, an adaptive multi-head LoRA method that balances parameter efficiency and model expressivity by reparameterizing the weight updates as the sum of multiple LoRA heads $s_i\\textbf{B}_i\\textbf{H}_i\\textbf{A}_i$ in which only the core matrices $\\textbf{H}_i$ and their lightweight scaling factors $s_i$ are trained. These trainable scaling factors let the optimization focus on the most useful heads, recovering a higher-rank approximation of the full update without increasing the number of communicated parameters since clients upload $s_i\\textbf{H}_i$ directly. Experiments on vision and language benchmarks show that \\textsc{Ravan} improves test accuracy by 2-8\\% over prior parameter-efficient baselines, making it a robust and scalable solution for federated fine-tuning of LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05568v1",
    "published_date": "2025-06-05 20:28:02 UTC",
    "updated_date": "2025-06-05 20:28:02 UTC"
  },
  {
    "arxiv_id": "2506.05566v2",
    "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation",
    "authors": [
      "Chenhui Deng",
      "Yun-Da Tsai",
      "Guan-Ting Liu",
      "Zhongzhi Yu",
      "Haoxing Ren"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled near-human performance on software coding benchmarks, but their effectiveness in RTL code generation remains limited due to the scarcity of high-quality training data. While prior efforts have fine-tuned LLMs for RTL tasks, they do not fundamentally overcome the data bottleneck and lack support for test-time scaling due to their non-reasoning nature. In this work, we introduce ScaleRTL, the first reasoning LLM for RTL coding that scales up both high-quality reasoning data and test-time compute. Specifically, we curate a diverse set of long chain-of-thought reasoning traces averaging 56K tokens each, resulting in a dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a general-purpose reasoning model on this corpus yields ScaleRTL that is capable of deep RTL reasoning. Subsequently, we further enhance the performance of ScaleRTL through a novel test-time scaling strategy that extends the reasoning process via iteratively reflecting on and self-correcting previous reasoning steps. Experimental results show that ScaleRTL achieves state-of-the-art performance on VerilogEval and RTLLM, outperforming 18 competitive baselines by up to 18.4% on VerilogEval and 12.7% on RTLLM.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted to MLCAD 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05566v2",
    "published_date": "2025-06-05 20:24:58 UTC",
    "updated_date": "2025-07-15 21:44:21 UTC"
  },
  {
    "arxiv_id": "2506.05565v1",
    "title": "Applying Informer for Option Pricing: A Transformer-Based Approach",
    "authors": [
      "Feliks Bańka",
      "Jarosław A. Chudziak"
    ],
    "abstract": "Accurate option pricing is essential for effective trading and risk management in financial markets, yet it remains challenging due to market volatility and the limitations of traditional models like Black-Scholes. In this paper, we investigate the application of the Informer neural network for option pricing, leveraging its ability to capture long-term dependencies and dynamically adjust to market fluctuations. This research contributes to the field of financial forecasting by introducing Informer's efficient architecture to enhance prediction accuracy and provide a more adaptable and resilient framework compared to existing methods. Our results demonstrate that Informer outperforms traditional approaches in option pricing, advancing the capabilities of data-driven financial forecasting in this domain.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.LG",
      "q-fin.CP"
    ],
    "primary_category": "cs.CE",
    "comment": "8 pages, 3 tables, 7 figures. Accepted at the 17th International Conference on Agents and Artificial Intelligence (ICAART 2025). Final version published in Proceedings of ICAART 2025 (Vol. 3), pages 1270-1277",
    "pdf_url": "https://arxiv.org/pdf/2506.05565v1",
    "published_date": "2025-06-05 20:23:28 UTC",
    "updated_date": "2025-06-05 20:23:28 UTC"
  },
  {
    "arxiv_id": "2506.11092v2",
    "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation",
    "authors": [
      "Jubin Abhishek Soni",
      "Amit Anand",
      "Rajesh Kumar Pandey",
      "Aniket Abhishek Soni"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has significantly advanced large language models (LLMs) by grounding their outputs in external tools and knowledge sources. However, existing RAG systems are typically constrained to static, single-turn interactions with fixed toolsets, making them ill-suited for dynamic domains such as healthcare and smart homes, where user intent, available tools, and contextual factors evolve over time. We present Dynamic Context Tuning (DCT), a lightweight framework that extends RAG to support multi-turn dialogue and evolving tool environments without requiring retraining. DCT integrates an attention-based context cache to track relevant past information, LoRA-based retrieval to dynamically select domain-specific tools, and efficient context compression to maintain inputs within LLM context limits. Experiments on both synthetic and real-world benchmarks show that DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while matching GPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to previously unseen tools, enabling scalable and adaptable AI assistants across a wide range of dynamic environments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "We are withdrawing the submission in order to thoroughly revise the work",
    "pdf_url": "https://arxiv.org/pdf/2506.11092v2",
    "published_date": "2025-06-05 19:47:22 UTC",
    "updated_date": "2025-07-19 17:46:19 UTC"
  },
  {
    "arxiv_id": "2506.05529v1",
    "title": "Avoiding Death through Fear Intrinsic Conditioning",
    "authors": [
      "Rodney Sanchez",
      "Ferat Sahin",
      "Alexander Ororbia",
      "Jamison Heard"
    ],
    "abstract": "Biological and psychological concepts have inspired reinforcement learning algorithms to create new complex behaviors that expand agents' capacity. These behaviors can be seen in the rise of techniques like goal decomposition, curriculum, and intrinsic rewards, which have paved the way for these complex behaviors. One limitation in evaluating these methods is the requirement for engineered extrinsic for realistic environments. A central challenge in engineering the necessary reward function(s) comes from these environments containing states that carry high negative rewards, but provide no feedback to the agent. Death is one such stimuli that fails to provide direct feedback to the agent. In this work, we introduce an intrinsic reward function inspired by early amygdala development and produce this intrinsic reward through a novel memory-augmented neural network (MANN) architecture. We show how this intrinsic motivation serves to deter exploration of terminal states and results in avoidance behavior similar to fear conditioning observed in animals. Furthermore, we demonstrate how modifying a threshold where the fear response is active produces a range of behaviors that are described under the paradigm of general anxiety disorders (GADs). We demonstrate this behavior in the Miniworld Sidewalk environment, which provides a partially observable Markov decision process (POMDP) and a sparse reward with a non-descriptive terminal condition, i.e., death. In effect, this study results in a biologically-inspired neural architecture and framework for fear conditioning paradigms; we empirically demonstrate avoidance behavior in a constructed agent that is able to solve environments with non-descriptive terminal conditions.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05529v1",
    "published_date": "2025-06-05 19:24:51 UTC",
    "updated_date": "2025-06-05 19:24:51 UTC"
  },
  {
    "arxiv_id": "2506.05523v1",
    "title": "MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning",
    "authors": [
      "Zikui Cai",
      "Andrew Wang",
      "Anirudh Satheesh",
      "Ankit Nakhawa",
      "Hyunwoo Jae",
      "Keenan Powell",
      "Minghui Liu",
      "Neel Jay",
      "Sungbin Oh",
      "Xiyao Wang",
      "Yongyuan Liang",
      "Tom Goldstein",
      "Furong Huang"
    ],
    "abstract": "Despite rapid advances in vision-language models (VLMs), current benchmarks for multimodal reasoning fall short in three key dimensions. First, they overwhelmingly rely on static images, failing to capture the temporal complexity of real-world environments. Second, they narrowly focus on mathematical problem-solving, neglecting the broader spectrum of reasoning skills -- including abstract, physical, planning, spatial, and temporal capabilities -- required for robust multimodal intelligence. Third, many benchmarks quickly saturate, offering limited headroom for diagnosing failure modes or measuring continued progress. We introduce MORSE-500 (Multimodal Reasoning Stress-test Environment), a video benchmark composed of 500 fully scripted clips with embedded questions spanning six complementary reasoning categories. Each instance is programmatically generated using deterministic Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and curated real footage. This script-driven design allows fine-grained control over visual complexity, distractor density, and temporal dynamics -- enabling difficulty to be scaled systematically as models improve. Unlike static benchmarks that become obsolete once saturated, MORSE-500 is built to evolve: its controllable generation pipeline supports the creation of arbitrarily challenging new instances, making it ideally suited for stress-testing next-generation models. Initial experiments with state-of-the-art systems -- including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest available at the time, alongside strong open-source models -- reveal substantial performance gaps across all categories, with particularly large deficits in abstract and planning tasks. We release the full dataset, generation scripts, and evaluation harness to support transparent, reproducible, and forward-looking multimodal reasoning research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05523v1",
    "published_date": "2025-06-05 19:12:45 UTC",
    "updated_date": "2025-06-05 19:12:45 UTC"
  },
  {
    "arxiv_id": "2506.05520v2",
    "title": "Toward Data Systems That Are Business Semantic Centric and AI Agents Assisted",
    "authors": [
      "Cecil Pang"
    ],
    "abstract": "Contemporary businesses operate in dynamic environments requiring rapid adaptation to achieve goals and maintain competitiveness. Existing data platforms often fall short by emphasizing tools over alignment with business needs, resulting in inefficiencies and delays. To address this gap, I propose the Business Semantics Centric, AI Agents Assisted Data System (BSDS), a holistic system that integrates architecture, workflows, and team organization to ensure data systems are tailored to business priorities rather than dictated by technical constraints. BSDS redefines data systems as dynamic enablers of business success, transforming them from passive tools into active drivers of organizational growth. BSDS has a modular architecture that comprises curated data linked to business entities, a knowledge base for context-aware AI agents, and efficient data pipelines. AI agents play a pivotal role in assisting with data access and system management, reducing human effort, and improving scalability. Complementing this architecture, BSDS incorporates workflows optimized for both exploratory data analysis and production requirements, balancing speed of delivery with quality assurance. A key innovation of BSDS is its incorporation of the human factor. By aligning data team expertise with business semantics, BSDS bridges the gap between technical capabilities and business needs. Validated through real-world implementation, BSDS accelerates time-to-market for data-driven initiatives, enhances cross-functional collaboration, and provides a scalable blueprint for businesses of all sizes. Future research can build on BSDS to explore optimization strategies using complex systems and adaptive network theories, as well as developing autonomous data systems leveraging AI agents.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Published by IEEE Access",
    "pdf_url": "https://arxiv.org/pdf/2506.05520v2",
    "published_date": "2025-06-05 19:06:06 UTC",
    "updated_date": "2025-06-27 15:49:26 UTC"
  },
  {
    "arxiv_id": "2506.05516v2",
    "title": "Learning to Recover: Dynamic Reward Shaping with Wheel-Leg Coordination for Fallen Robots",
    "authors": [
      "Boyuan Deng",
      "Luca Rossini",
      "Jin Wang",
      "Weijie Wang",
      "Dimitrios Kanoulas",
      "Nikolaos Tsagarakis"
    ],
    "abstract": "Adaptive recovery from fall incidents are essential skills for the practical deployment of wheeled-legged robots, which uniquely combine the agility of legs with the speed of wheels for rapid recovery. However, traditional methods relying on preplanned recovery motions, simplified dynamics or sparse rewards often fail to produce robust recovery policies. This paper presents a learning-based framework integrating Episode-based Dynamic Reward Shaping and curriculum learning, which dynamically balances exploration of diverse recovery maneuvers with precise posture refinement. An asymmetric actor-critic architecture accelerates training by leveraging privileged information in simulation, while noise-injected observations enhance robustness against uncertainties. We further demonstrate that synergistic wheel-leg coordination reduces joint torque consumption by 15.8% and 26.2% and improves stabilization through energy transfer mechanisms. Extensive evaluations on two distinct quadruped platforms achieve recovery success rates up to 99.1% and 97.8% without platform-specific tuning. The supplementary material is available at https://boyuandeng.github.io/L2R-WheelLegCoordination/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05516v2",
    "published_date": "2025-06-05 18:58:12 UTC",
    "updated_date": "2025-10-08 13:33:28 UTC"
  },
  {
    "arxiv_id": "2506.05515v2",
    "title": "Winner-takes-all for Multivariate Probabilistic Time Series Forecasting",
    "authors": [
      "Adrien Cortés",
      "Rémi Rehm",
      "Victor Letzelter"
    ],
    "abstract": "We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL) paradigm to forecast multiple plausible time series futures. Our approach employs a neural network with multiple heads and utilizes the Winner-Takes-All (WTA) loss to promote diversity among predictions. MCL has recently gained attention due to its simplicity and ability to address ill-posed and ambiguous tasks. We propose an adaptation of this framework for time-series forecasting, presenting it as an efficient method to predict diverse futures, which we relate to its implicit quantization objective. We provide insights into our approach using synthetic data and evaluate it on real-world time series, demonstrating its promising performance at a light computational cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05515v2",
    "published_date": "2025-06-05 18:56:14 UTC",
    "updated_date": "2025-08-11 12:57:52 UTC"
  },
  {
    "arxiv_id": "2506.05508v1",
    "title": "Beyond the Buzz: A Pragmatic Take on Inference Disaggregation",
    "authors": [
      "Tiyasa Mitra",
      "Ritika Borkar",
      "Nidhi Bhatia",
      "Ramon Matas",
      "Shivam Raj",
      "Dheevatsa Mudigere",
      "Ritchie Zhao",
      "Maximilian Golub",
      "Arpan Dutta",
      "Sailaja Madduri",
      "Dharmesh Jani",
      "Brian Pharris",
      "Bita Darvish Rouhani"
    ],
    "abstract": "As inference scales to multi-node deployments, disaggregation - splitting inference into distinct phases - offers a promising path to improving the throughput-interactivity Pareto frontier. Despite growing enthusiasm and a surge of open-source efforts, practical deployment of disaggregated serving remains limited due to the complexity of the optimization search space and system-level coordination. In this paper, we present the first systematic study of disaggregated inference at scale, evaluating hundreds of thousands of design points across diverse workloads and hardware configurations. We find that disaggregation is most effective for prefill-heavy traffic patterns and larger models. Our results highlight the critical role of dynamic rate matching and elastic scaling in achieving Pareto-optimal performance. Our findings offer actionable insights for efficient disaggregated deployments to navigate the trade-off between system throughput and interactivity.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05508v1",
    "published_date": "2025-06-05 18:47:49 UTC",
    "updated_date": "2025-06-05 18:47:49 UTC"
  },
  {
    "arxiv_id": "2506.05502v1",
    "title": "StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models",
    "authors": [
      "Ya Jiang",
      "Chuxiong Wu",
      "Massieh Kordi Boroujeny",
      "Brian Mark",
      "Kai Zeng"
    ],
    "abstract": "Watermarking for large language models (LLMs) offers a promising approach to identifying AI-generated text. Existing approaches, however, either compromise the distribution of original generated text by LLMs or are limited to embedding zero-bit information that only allows for watermark detection but ignores identification. We present StealthInk, a stealthy multi-bit watermarking scheme that preserves the original text distribution while enabling the embedding of provenance data, such as userID, TimeStamp, and modelID, within LLM-generated text. This enhances fast traceability without requiring access to the language model's API or prompts. We derive a lower bound on the number of tokens necessary for watermark detection at a fixed equal error rate, which provides insights on how to enhance the capacity. Comprehensive empirical evaluations across diverse tasks highlight the stealthiness, detectability, and resilience of StealthInk, establishing it as an effective solution for LLM watermarking applications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "camera-ready version",
    "pdf_url": "https://arxiv.org/pdf/2506.05502v1",
    "published_date": "2025-06-05 18:37:38 UTC",
    "updated_date": "2025-06-05 18:37:38 UTC"
  },
  {
    "arxiv_id": "2506.05497v1",
    "title": "Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models",
    "authors": [
      "Sima Noorani",
      "Shayan Kiyani",
      "George Pappas",
      "Hamed Hassani"
    ],
    "abstract": "Uncertainty quantification (UQ) is essential for safe deployment of generative AI models such as large language models (LLMs), especially in high stakes applications. Conformal prediction (CP) offers a principled uncertainty quantification framework, but classical methods focus on regression and classification, relying on geometric distances or softmax scores: tools that presuppose structured outputs. We depart from this paradigm by studying CP in a query only setting, where prediction sets must be constructed solely from finite queries to a black box generative model, introducing a new trade off between coverage, test time query budget, and informativeness. We introduce Conformal Prediction with Query Oracle (CPQ), a framework characterizing the optimal interplay between these objectives. Our finite sample algorithm is built on two core principles: one governs the optimal query policy, and the other defines the optimal mapping from queried samples to prediction sets. Remarkably, both are rooted in the classical missing mass problem in statistics. Specifically, the optimal query policy depends on the rate of decay, or the derivative, of the missing mass, for which we develop a novel estimator. Meanwhile, the optimal mapping hinges on the missing mass itself, which we estimate using Good Turing estimators. We then turn our focus to implementing our method for language models, where outputs are vast, variable, and often under specified. Fine grained experiments on three real world open ended tasks and two LLMs, show CPQ applicability to any black box LLM and highlight: (1) individual contribution of each principle to CPQ performance, and (2) CPQ ability to yield significantly more informative prediction sets than existing conformal methods for language uncertainty quantification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05497v1",
    "published_date": "2025-06-05 18:26:14 UTC",
    "updated_date": "2025-06-05 18:26:14 UTC"
  },
  {
    "arxiv_id": "2506.05490v1",
    "title": "Sentiment Analysis in Learning Management Systems Understanding Student Feedback at Scale",
    "authors": [
      "Mohammed Almutairi"
    ],
    "abstract": "During the wake of the Covid-19 pandemic, the educational paradigm has experienced a major change from in person learning traditional to online platforms. The change of learning convention has impacted the teacher-student especially in non-verbal communication. The absent of non-verbal communication has led to a reliance on verbal feedback which diminished the efficacy of the educational experience. This paper explores the integration of sentiment analysis into learning management systems (LMS) to bridge the student-teacher's gap by offering an alternative approach to interpreting student feedback beyond its verbal context. The research involves data preparation, feature selection, and the development of a deep neural network model encompassing word embedding, LSTM, and attention mechanisms. This model is compared against a logistic regression baseline to evaluate its efficacy in understanding student feedback. The study aims to bridge the communication gap between instructors and students in online learning environments, offering insights into the emotional context of student feedback and ultimately improving the quality of online education.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.05490v1",
    "published_date": "2025-06-05 18:13:07 UTC",
    "updated_date": "2025-06-05 18:13:07 UTC"
  },
  {
    "arxiv_id": "2506.05342v2",
    "title": "Refer to Any Segmentation Mask Group With Vision-Language Prompts",
    "authors": [
      "Shengcao Cao",
      "Zijun Wei",
      "Jason Kuen",
      "Kangning Liu",
      "Lingzhi Zhang",
      "Jiuxiang Gu",
      "HyunJoon Jung",
      "Liang-Yan Gui",
      "Yu-Xiong Wang"
    ],
    "abstract": "Recent image segmentation models have advanced to segment images into high-quality masks for visual entities, and yet they cannot provide comprehensive semantic understanding for complex queries based on both language and vision. This limitation reduces their effectiveness in applications that require user-friendly interactions driven by vision-language prompts. To bridge this gap, we introduce a novel task of omnimodal referring expression segmentation (ORES). In this task, a model produces a group of masks based on arbitrary prompts specified by text only or text plus reference visual entities. To address this new challenge, we propose a novel framework to \"Refer to Any Segmentation Mask Group\" (RAS), which augments segmentation models with complex multimodal interactions and comprehension via a mask-centric large multimodal model. For training and benchmarking ORES models, we create datasets MaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by text and reference entities. Through extensive evaluation, we demonstrate superior performance of RAS on our new ORES task, as well as classic referring expression segmentation (RES) and generalized referring expression segmentation (GRES) tasks. Project page: https://Ref2Any.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05342v2",
    "published_date": "2025-06-05 17:59:51 UTC",
    "updated_date": "2025-10-17 17:20:57 UTC"
  },
  {
    "arxiv_id": "2506.05341v2",
    "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning",
    "authors": [
      "Xingjian Ran",
      "Yixuan Li",
      "Linning Xu",
      "Mulin Yu",
      "Bo Dai"
    ],
    "abstract": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital content creation. It can be naturally divided into two subtasks: object generation and layout generation. While recent generative models have significantly advanced object-level quality and controllability, layout generation remains challenging due to limited datasets. Existing methods either overfit to these datasets or rely on predefined constraints to optimize numerical layout that sacrifice flexibility. As a result, they fail to generate scenes that are both open-vocabulary and aligned with fine-grained user instructions. We introduce DirectLayout, a framework that directly generates numerical 3D layouts from text descriptions using generalizable spatial reasoning of large language models (LLMs). DirectLayout decomposes the generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting it into 3D space, and refining object placements. To enable explicit spatial reasoning and help the model grasp basic principles of object placement, we employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset. Additionally, we design CoT-Grounded Generative Layout Reward to enhance generalization and spatial planning. During inference, DirectLayout addresses asset-layout mismatches via Iterative Asset-Layout Alignment through in-context learning. Extensive experiments demonstrate that DirectLayout achieves impressive semantic consistency, generalization and physical plausibility.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://directlayout.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2506.05341v2",
    "published_date": "2025-06-05 17:59:42 UTC",
    "updated_date": "2025-10-23 08:15:56 UTC"
  },
  {
    "arxiv_id": "2506.05340v2",
    "title": "Exploring Diffusion Transformer Designs via Grafting",
    "authors": [
      "Keshigeyan Chandrasegaran",
      "Michael Poli",
      "Daniel Y. Fu",
      "Dongjun Kim",
      "Lea M. Hadzic",
      "Manling Li",
      "Agrim Gupta",
      "Stefano Massaroli",
      "Azalia Mirhoseini",
      "Juan Carlos Niebles",
      "Stefano Ermon",
      "Li Fei-Fei"
    ],
    "abstract": "Designing model architectures requires decisions such as selecting operators (e.g., attention, convolution) and configurations (e.g., depth, width). However, evaluating the impact of these decisions on model quality requires costly pretraining, limiting architectural investigation. Inspired by how new software is built on existing code, we ask: can new architecture designs be studied using pretrained models? To this end, we present grafting, a simple approach for editing pretrained diffusion transformers (DiTs) to materialize new architectures under small compute budgets. Informed by our analysis of activation behavior and attention locality, we construct a testbed based on the DiT-XL/2 design to study the impact of grafting on model quality. Using this testbed, we develop a family of hybrid designs via grafting: replacing softmax attention with gated convolution, local attention, and linear attention, and replacing MLPs with variable expansion ratio and convolutional variants. Notably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for DiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model (PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval score. Finally, we present a case study that restructures DiT-XL/2 by converting every pair of sequential transformer blocks into parallel blocks via grafting. This reduces model depth by 2x and yields better quality (FID: 2.77) than other models of comparable depth. Together, we show that new diffusion model designs can be explored by grafting pretrained DiTs, with edits ranging from operator replacement to architecture restructuring. Code and grafted models: https://grafting.stanford.edu",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages; Project website: https://grafting.stanford.edu",
    "pdf_url": "https://arxiv.org/pdf/2506.05340v2",
    "published_date": "2025-06-05 17:59:40 UTC",
    "updated_date": "2025-06-06 17:59:47 UTC"
  },
  {
    "arxiv_id": "2506.05454v2",
    "title": "Zeroth-Order Optimization Finds Flat Minima",
    "authors": [
      "Liang Zhang",
      "Bingcong Li",
      "Kiran Koshy Thekumparampil",
      "Sewoong Oh",
      "Michael Muehlebach",
      "Niao He"
    ],
    "abstract": "Zeroth-order methods are extensively used in machine learning applications where gradients are infeasible or expensive to compute, such as black-box attacks, reinforcement learning, and language model fine-tuning. Existing optimization theory focuses on convergence to an arbitrary stationary point, but less is known on the implicit regularization that provides a fine-grained characterization on which particular solutions are finally reached. We show that zeroth-order optimization with the standard two-point estimator favors solutions with small trace of Hessian, which is widely used in previous work to distinguish between sharp and flat minima. We further provide convergence rates of zeroth-order optimization to approximate flat minima for convex and sufficiently smooth functions, where flat minima are defined as the minimizers that achieve the smallest trace of Hessian among all optimal solutions. Experiments on binary classification tasks with convex losses and language model fine-tuning support our theoretical findings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05454v2",
    "published_date": "2025-06-05 17:59:09 UTC",
    "updated_date": "2025-11-11 17:46:13 UTC"
  },
  {
    "arxiv_id": "2506.05453v2",
    "title": "MLLM-CL: Continual Learning for Multimodal Large Language Models",
    "authors": [
      "Hongbo Zhao",
      "Fei Zhu",
      "Haiyang Guo",
      "Meng Wang",
      "Rundong Wang",
      "Gaofeng Meng",
      "Zhaoxiang Zhang"
    ],
    "abstract": "Recent Multimodal Large Language Models (MLLMs) excel in vision-language understanding but face challenges in adapting to dynamic real-world scenarios that require continuous integration of new knowledge and skills. While continual learning (CL) offers a potential solution, existing benchmarks and methods suffer from critical limitations. In this paper, we introduce MLLM-CL, a novel benchmark encompassing domain and ability continual learning, where the former focuses on independently and identically distributed (IID) evaluation across evolving mainstream domains, whereas the latter evaluates on non-IID scenarios with new model abilities. Methodologically, we propose preventing catastrophic interference through parameter isolation and an MLLM-based routing mechanism. Extensive experiments demonstrate that our approach can integrate domain-specific knowledge and functional abilities with minimal forgetting, significantly outperforming existing methods. Our benchmark and code are available at https://github.com/bjzhb666/MLLM-CL.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05453v2",
    "published_date": "2025-06-05 17:58:13 UTC",
    "updated_date": "2025-10-01 01:41:15 UTC"
  },
  {
    "arxiv_id": "2506.05451v1",
    "title": "Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety",
    "authors": [
      "Seongmin Lee",
      "Aeree Cho",
      "Grace C. Kim",
      "ShengYun Peng",
      "Mansi Phute",
      "Duen Horng Chau"
    ],
    "abstract": "As large language models (LLMs) see wider real-world use, understanding and mitigating their unsafe behaviors is critical. Interpretation techniques can reveal causes of unsafe outputs and guide safety, but such connections with safety are often overlooked in prior surveys. We present the first survey that bridges this gap, introducing a unified framework that connects safety-focused interpretation methods, the safety enhancements they inform, and the tools that operationalize them. Our novel taxonomy, organized by LLM workflow stages, summarizes nearly 70 works at their intersections. We conclude with open challenges and future directions. This timely survey helps researchers and practitioners navigate key advancements for safer, more interpretable LLMs.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "31 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2506.05451v1",
    "published_date": "2025-06-05 17:56:05 UTC",
    "updated_date": "2025-06-05 17:56:05 UTC"
  },
  {
    "arxiv_id": "2506.05316v3",
    "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay",
    "authors": [
      "Yifan Sun",
      "Jingyan Shen",
      "Yibin Wang",
      "Tianyu Chen",
      "Zhendong Wang",
      "Mingyuan Zhou",
      "Huan Zhang"
    ],
    "abstract": "Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism inspired by experience replay in traditional RL. This technique reuses recent rollouts, lowering per-step computation while maintaining stable updates. Experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 23% to 62% while reaching the same level of performance as the original GRPO algorithm. Our code is available at https://github.com/ASTRAL-Group/data-efficient-llm-rl.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05316v3",
    "published_date": "2025-06-05 17:55:43 UTC",
    "updated_date": "2025-10-28 03:50:11 UTC"
  },
  {
    "arxiv_id": "2506.05314v2",
    "title": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models",
    "authors": [
      "Taha Entesari",
      "Arman Hatami",
      "Rinat Khaziev",
      "Anil Ramakrishna",
      "Mahyar Fazlyab"
    ],
    "abstract": "Large Language Models (LLMs) deployed in real-world settings increasingly face the need to unlearn sensitive, outdated, or proprietary information. Existing unlearning methods typically formulate forgetting and retention as a regularized trade-off, combining both objectives into a single scalarized loss. This often leads to unstable optimization and degraded performance on retained data, especially under aggressive forgetting. We propose a new formulation of LLM unlearning as a constrained optimization problem: forgetting is enforced via a novel logit-margin flattening loss that explicitly drives the output distribution toward uniformity on a designated forget set, while retention is preserved through a hard constraint on a separate retain set. Compared to entropy-based objectives, our loss is softmax-free, numerically stable, and maintains non-vanishing gradients, enabling more efficient and robust optimization. We solve the constrained problem using a scalable primal-dual algorithm that exposes the trade-off between forgetting and retention through the dynamics of the dual variable, all without any extra computational overhead. Evaluations on the TOFU and MUSE benchmarks across diverse LLM architectures demonstrate that our approach consistently matches or exceeds state-of-the-art baselines, effectively removing targeted information while preserving downstream utility.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing Systems",
    "pdf_url": "https://arxiv.org/pdf/2506.05314v2",
    "published_date": "2025-06-05 17:55:23 UTC",
    "updated_date": "2025-10-27 17:59:13 UTC"
  },
  {
    "arxiv_id": "2506.05309v2",
    "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games",
    "authors": [
      "Niv Eckhaus",
      "Uri Berger",
      "Gabriel Stanovsky"
    ],
    "abstract": "LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns. In this work, we develop an adaptive asynchronous LLM agent consisting of two modules: a generator that decides what to say, and a scheduler that decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, where our agent plays with human participants. Overall, our agent performs on par with human players, both in game performance metrics and in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We make all of our code and data publicly available. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05309v2",
    "published_date": "2025-06-05 17:53:44 UTC",
    "updated_date": "2025-09-20 16:08:12 UTC"
  },
  {
    "arxiv_id": "2506.05305v3",
    "title": "ProRefine: Inference-Time Prompt Refinement with Textual Feedback",
    "authors": [
      "Deepak Pandita",
      "Tharindu Cyril Weerasooriya",
      "Ankit Parag Shah",
      "Isabelle Diana May-Xin Ng",
      "Christopher M. Homan",
      "Wei Wei"
    ],
    "abstract": "Agentic workflows, where multiple AI agents collaborate to accomplish complex tasks like reasoning or planning, play a substantial role in many cutting-edge commercial applications, and continue to fascinate researchers across fields for their potential to accomplish expensive, complex tasks that, until recently, only humans have been trusted to do. These workflows depend critically on the prompts used to provide the roles models play in such workflows. Poorly designed prompts that fail even slightly to guide individual agents can lead to sub-optimal performance that may snowball within a system of agents, limiting their reliability and scalability. To address this important problem of inference-time prompt optimization, we introduce ProRefine, an innovative inference-time optimization method that uses an agentic loop of LLMs to generate and apply textual feedback. ProRefine dynamically refines prompts for multi-step reasoning tasks without additional training or ground truth labels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine significantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only boosts accuracy but also allows smaller models to approach the performance of their larger counterparts. This highlights its potential for building more cost-effective and powerful hybrid AI systems, thereby democratizing access to high-performing AI.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Workshop on Efficient Reasoning at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05305v3",
    "published_date": "2025-06-05 17:52:30 UTC",
    "updated_date": "2025-11-06 21:43:54 UTC"
  },
  {
    "arxiv_id": "2506.05296v2",
    "title": "Control Tax: The Price of Keeping AI in Check",
    "authors": [
      "Mikhail Terekhov",
      "Zhen Ning David Liu",
      "Caglar Gulcehre",
      "Samuel Albanie"
    ],
    "abstract": "The rapid integration of agentic AI into high-stakes real-world applications requires robust oversight mechanisms. The emerging field of AI Control (AIC) aims to provide such an oversight mechanism, but practical adoption depends heavily on implementation overhead. To study this problem better, we introduce the notion of Control tax -- the operational and financial cost of integrating control measures into AI pipelines. Our work makes three key contributions to the field of AIC: (1) we introduce a theoretical framework that quantifies the Control Tax and maps classifier performance to safety assurances; (2) we conduct comprehensive evaluations of state-of-the-art language models in adversarial settings, where attacker models insert subtle backdoors into code while monitoring models attempt to detect these vulnerabilities; and (3) we provide empirical financial cost estimates for control protocols and develop optimized monitoring strategies that balance safety and cost-effectiveness while accounting for practical constraints like auditing budgets. Our framework enables practitioners to make informed decisions by systematically connecting safety guarantees with their costs, advancing AIC through principled economic feasibility assessment across different deployment contexts.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05296v2",
    "published_date": "2025-06-05 17:48:39 UTC",
    "updated_date": "2025-06-14 18:09:16 UTC"
  },
  {
    "arxiv_id": "2506.05295v2",
    "title": "Sample Complexity and Representation Ability of Test-time Scaling Paradigms",
    "authors": [
      "Baihe Huang",
      "Shanda Li",
      "Tianhao Wu",
      "Yiming Yang",
      "Ameet Talwalkar",
      "Kannan Ramchandran",
      "Michael I. Jordan",
      "Jiantao Jiao"
    ],
    "abstract": "Test-time scaling paradigms have significantly advanced the capabilities of large language models (LLMs) on complex tasks. Despite their empirical success, theoretical understanding of the sample efficiency of various test-time strategies -- such as self-consistency, best-of-$n$, and self-correction -- remains limited. In this work, we first establish a separation result between two repeated sampling strategies: self-consistency requires $Θ(1/Δ^2)$ samples to produce the correct answer, while best-of-$n$ only needs $Θ(1/Δ)$, where $Δ< 1$ denotes the probability gap between the correct and second most likely answers. Next, we present an expressiveness result for the self-correction approach with verifier feedback: it enables Transformers to simulate online learning over a pool of experts at test time. Therefore, a single Transformer architecture can provably solve multiple tasks without prior knowledge of the specific task associated with a user query, extending the representation theory of Transformers from single-task to multi-task settings. Finally, we empirically validate our theoretical results, demonstrating the practical effectiveness of self-correction methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05295v2",
    "published_date": "2025-06-05 17:48:19 UTC",
    "updated_date": "2025-06-12 16:25:06 UTC"
  },
  {
    "arxiv_id": "2506.05282v2",
    "title": "Rectified Point Flow: Generic Point Cloud Pose Estimation",
    "authors": [
      "Tao Sun",
      "Liyuan Zhu",
      "Shengyu Huang",
      "Shuran Song",
      "Iro Armeni"
    ],
    "abstract": "We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with a self-supervised encoder focused on overlapping points, our method achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Project page: https://rectified-pointflow.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2025 Camera-ready. Project page: https://rectified-pointflow.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2506.05282v2",
    "published_date": "2025-06-05 17:36:03 UTC",
    "updated_date": "2025-10-24 03:12:13 UTC"
  },
  {
    "arxiv_id": "2506.05281v3",
    "title": "Fast-DataShapley: Neural Modeling for Training Data Valuation",
    "authors": [
      "Haifeng Sun",
      "Yu Xiong",
      "Runze Wu",
      "Xinyu Cai",
      "Changjie Fan",
      "Lan Zhang",
      "Xiang-Yang Li"
    ],
    "abstract": "The value and copyright of training data are crucial in the artificial intelligence industry. Service platforms should protect data providers' legitimate rights and fairly reward them for their contributions. Shapley value, a potent tool for evaluating contributions, outperforms other methods in theory, but its computational overhead escalates exponentially with the number of data providers. Recent works based on Shapley values attempt to mitigate computation complexity by approximation algorithms. However, they need to retrain for each test sample, leading to intolerable costs. We propose Fast-DataShapley, a one-pass training method that leverages the weighted least squares characterization of the Shapley value to train a reusable explainer model with real-time reasoning speed. Given new test samples, no retraining is required to calculate the Shapley values of the training data. Additionally, we propose three methods with theoretical guarantees to reduce training overhead from two aspects: the approximate calculation of the utility function and the group calculation of the training data. We analyze time complexity to show the efficiency of our methods. The experimental evaluations on various image datasets demonstrate superior performance and efficiency compared to baselines. Specifically, the performance is improved to more than 2 times, and the explainer's training speed can be increased by two orders of magnitude.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05281v3",
    "published_date": "2025-06-05 17:35:46 UTC",
    "updated_date": "2025-11-20 15:05:09 UTC"
  },
  {
    "arxiv_id": "2506.05278v2",
    "title": "Micro-Act: Mitigating Knowledge Conflict in LLM-based RAG via Actionable Self-Reasoning",
    "authors": [
      "Nan Huo",
      "Jinyang Li",
      "Bowen Qin",
      "Ge Qu",
      "Xiaolong Li",
      "Xiaodong Li",
      "Chenhao Ma",
      "Reynold Cheng"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2506.05278v2",
    "published_date": "2025-06-05 17:33:02 UTC",
    "updated_date": "2025-10-03 20:47:40 UTC"
  },
  {
    "arxiv_id": "2506.05265v2",
    "title": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating, and Optimizing Human Teams",
    "authors": [
      "Mohammed Almutairi"
    ],
    "abstract": "Effective teamwork is essential across diverse domains. During the team formation stage, a key challenge is forming teams that effectively balance user preferences with task objectives to enhance overall team satisfaction. In the team performing stage, maintaining cohesion and engagement is critical for sustaining high team performance. However, existing computational tools and algorithms for team optimization often rely on static data inputs, narrow algorithmic objectives, or solutions tailored for specific contexts, failing to account for the dynamic interplay of team members personalities, evolving goals, and changing individual preferences. Therefore, teams may encounter member dissatisfaction, as purely algorithmic assignments can reduce members commitment to team goals or experience suboptimal engagement due to the absence of timely, personalized guidance to help members adjust their behaviors and interactions as team dynamics evolve. Ultimately, these challenges can lead to reduced overall team performance. My Ph.D. dissertation aims to develop AI-augmented team optimization frameworks and practical systems that enhance team satisfaction, engagement, and performance. First, I propose a team formation framework that leverages a multi-armed bandit algorithm to iteratively refine team composition based on user preferences, ensuring alignment between individual needs and collective team goals to enhance team satisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an AI-powered system that utilizes large language models (LLMs) to deliver immediate, personalized feedback to both teams and individual members, enhancing cohesion and engagement. Finally, I present PuppeteerLLM, an LLM-based simulation framework that simulates multi-agent teams to model complex team dynamics within realistic environments, incorporating task-driven collaboration and long-term coordination.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages, UMAP 25, June 16_19, 2025, New York City, NY, USA",
    "pdf_url": "https://arxiv.org/pdf/2506.05265v2",
    "published_date": "2025-06-05 17:24:37 UTC",
    "updated_date": "2025-06-06 16:58:32 UTC"
  },
  {
    "arxiv_id": "2506.05256v2",
    "title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning",
    "authors": [
      "Violet Xiang",
      "Chase Blagden",
      "Rafael Rafailov",
      "Nathan Lile",
      "Sang Truong",
      "Chelsea Finn",
      "Nick Haber"
    ],
    "abstract": "Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt's online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05256v2",
    "published_date": "2025-06-05 17:17:05 UTC",
    "updated_date": "2025-06-06 02:38:39 UTC"
  },
  {
    "arxiv_id": "2506.06391v1",
    "title": "From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law",
    "authors": [
      "John Mavi",
      "Diana Teodora Găitan",
      "Sergio Coronado"
    ],
    "abstract": "Large Language Models (LLMs) are widely used across sectors, yet their alignment with International Humanitarian Law (IHL) is not well understood. This study evaluates eight leading LLMs on their ability to refuse prompts that explicitly violate these legal frameworks, focusing also on helpfulness - how clearly and constructively refusals are communicated. While most models rejected unlawful requests, the clarity and consistency of their responses varied. By revealing the model's rationale and referencing relevant legal or safety principles, explanatory refusals clarify the system's boundaries, reduce ambiguity, and help prevent misuse. A standardised system-level safety prompt significantly improved the quality of the explanations expressed within refusals in most models, highlighting the effectiveness of lightweight interventions. However, more complex prompts involving technical language or requests for code revealed ongoing vulnerabilities. These findings contribute to the development of safer, more transparent AI systems and propose a benchmark to evaluate the compliance of LLM with IHL.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06391v1",
    "published_date": "2025-06-05 16:53:29 UTC",
    "updated_date": "2025-06-05 16:53:29 UTC"
  },
  {
    "arxiv_id": "2506.05233v1",
    "title": "MesaNet: Sequence Modeling by Locally Optimal Test-Time Training",
    "authors": [
      "Johannes von Oswald",
      "Nino Scherrer",
      "Seijin Kobayashi",
      "Luca Versari",
      "Songlin Yang",
      "Maximilian Schlegel",
      "Kaitlin Maile",
      "Yanick Schimpf",
      "Oliver Sieberling",
      "Alexander Meulemans",
      "Rif A. Saurous",
      "Guillaume Lajoie",
      "Charlotte Frenkel",
      "Razvan Pascanu",
      "Blaise Agüera y Arcas",
      "João Sacramento"
    ],
    "abstract": "Sequence modeling is currently dominated by causal transformer architectures that use softmax self-attention. Although widely adopted, transformers require scaling memory and compute linearly during inference. A recent stream of work linearized the softmax operation, resulting in powerful recurrent neural network (RNN) models with constant memory and compute costs such as DeltaNet, Mamba or xLSTM. These models can be unified by noting that their recurrent layer dynamics can all be derived from an in-context regression objective, approximately optimized through an online learning rule. Here, we join this line of work and introduce a numerically stable, chunkwise parallelizable version of the recently proposed Mesa layer (von Oswald et al., 2024), and study it in language modeling at the billion-parameter scale. This layer again stems from an in-context loss, but which is now minimized to optimality at every time point using a fast conjugate gradient solver. Through an extensive suite of experiments, we show that optimal test-time training enables reaching lower language modeling perplexity and higher downstream benchmark performance than previous RNNs, especially on tasks requiring long context understanding. This performance gain comes at the cost of additional flops spent during inference time. Our results are therefore intriguingly related to recent trends of increasing test-time compute to improve performance -- here by spending compute to solve sequential optimization problems within the neural network itself.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05233v1",
    "published_date": "2025-06-05 16:50:23 UTC",
    "updated_date": "2025-06-05 16:50:23 UTC"
  },
  {
    "arxiv_id": "2506.05214v1",
    "title": "Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning",
    "authors": [
      "Jingyu Hu",
      "Hongbo Bo",
      "Jun Hong",
      "Xiaowei Liu",
      "Weiru Liu"
    ],
    "abstract": "Graph Neural Networks (GNNs) often suffer from degree bias in node classification tasks, where prediction performance varies across nodes with different degrees. Several approaches, which adopt Graph Contrastive Learning (GCL), have been proposed to mitigate this bias. However, the limited number of positive pairs and the equal weighting of all positives and negatives in GCL still lead to low-degree nodes acquiring insufficient and noisy information. This paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to mitigate degree bias. It adds more positive pairs by leveraging node labels and adaptively weights positive and negative pairs based on their learning hardness. In addition, we develop an experimental framework named SHARP to extend HAR to a broader range of scenarios. Both our theoretical analysis and experiments validate the effectiveness of SHARP. The experimental results across four datasets show that SHARP achieves better performance against baselines at both global and degree levels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05214v1",
    "published_date": "2025-06-05 16:28:12 UTC",
    "updated_date": "2025-06-05 16:28:12 UTC"
  },
  {
    "arxiv_id": "2506.05213v1",
    "title": "LLM-First Search: Self-Guided Exploration of the Solution Space",
    "authors": [
      "Nathan Herr",
      "Tim Rocktäschel",
      "Roberta Raileanu"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable improvements in reasoning and planning through increased test-time compute, often by framing problem-solving as a search process. While methods like Monte Carlo Tree Search (MCTS) have proven effective in some domains, their reliance on fixed exploration hyperparameters limits their adaptability across tasks of varying difficulty, rendering them impractical or expensive in certain settings. In this paper, we propose \\textbf{LLM-First Search (LFS)}, a novel \\textit{LLM Self-Guided Search} method that removes the need for pre-defined search strategies by empowering the LLM to autonomously control the search process via self-guided exploration. Rather than relying on external heuristics or hardcoded policies, the LLM evaluates whether to pursue the current search path or explore alternative branches based on its internal scoring mechanisms. This enables more flexible and context-sensitive reasoning without requiring manual tuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku against three classic widely-used search algorithms, Tree-of-Thoughts' Breadth First Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which have been used to achieve SotA results on a range of challenging reasoning tasks. We found that LFS (1) performs better on more challenging tasks without additional tuning, (2) is more computationally efficient compared to the other methods, especially when powered by a stronger model, (3) scales better with stronger models, due to its LLM-First design, and (4) scales better with increased compute budget. Our code is publicly available at \\href{https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "9 main pages, 2 figures, 2 tables, 36 appendix pages",
    "pdf_url": "https://arxiv.org/pdf/2506.05213v1",
    "published_date": "2025-06-05 16:27:49 UTC",
    "updated_date": "2025-06-05 16:27:49 UTC"
  },
  {
    "arxiv_id": "2506.05211v1",
    "title": "Intentionally Unintentional: GenAI Exceptionalism and the First Amendment",
    "authors": [
      "David Atkinson",
      "Jena D. Hwang",
      "Jacob Morrison"
    ],
    "abstract": "This paper challenges the assumption that courts should grant First Amendment protections to outputs from large generative AI models, such as GPT-4 and Gemini. We argue that because these models lack intentionality, their outputs do not constitute speech as understood in the context of established legal precedent, so there can be no speech to protect. Furthermore, if the model outputs are not speech, users cannot claim a First Amendment speech right to receive the outputs. We also argue that extending First Amendment rights to AI models would not serve the fundamental purposes of free speech, such as promoting a marketplace of ideas, facilitating self-governance, or fostering self-expression. In fact, granting First Amendment protections to AI models would be detrimental to society because it would hinder the government's ability to regulate these powerful technologies effectively, potentially leading to the unchecked spread of misinformation and other harms.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05211v1",
    "published_date": "2025-06-05 16:26:32 UTC",
    "updated_date": "2025-06-05 16:26:32 UTC"
  },
  {
    "arxiv_id": "2506.05188v2",
    "title": "Counterfactual reasoning: an analysis of in-context emergence",
    "authors": [
      "Moritz Miller",
      "Bernhard Schölkopf",
      "Siyuan Guo"
    ],
    "abstract": "Large-scale neural language models exhibit remarkable performance in in-context learning: the ability to learn and reason about the input context on the fly. This work studies in-context counterfactual reasoning in language models, that is, the ability to predict consequences of a hypothetical scenario. We focus on a well-defined, synthetic linear regression task that requires noise abduction. Accurate prediction is based on (1) inferring an unobserved latent concept and (2) copying contextual noise from factual observations. We show that language models are capable of counterfactual reasoning. Further, we enhance existing identifiability results and reduce counterfactual reasoning for a broad class of functions to a transformation on in-context observations. In Transformers, we find that self-attention, model depth and pre-training data diversity drive performance. Moreover, we provide mechanistic evidence that the latent concept is linearly represented in the residual stream and we introduce designated \\textit{noise abduction heads} central to performing counterfactual reasoning. Lastly, our findings extend to counterfactual reasoning under SDE dynamics and reflect that Transformers can perform noise abduction on sequential data, providing preliminary evidence on the potential for counterfactual story generation. Our code is available under https://github.com/mrtzmllr/iccr.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "cs.CL",
    "comment": "Published as a conference paper at the Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS) 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05188v2",
    "published_date": "2025-06-05 16:02:07 UTC",
    "updated_date": "2025-10-21 16:08:36 UTC"
  },
  {
    "arxiv_id": "2506.05183v2",
    "title": "TreeRPO: Tree Relative Policy Optimization",
    "authors": [
      "Zhicheng Yang",
      "Zhijiang Guo",
      "Yinya Huang",
      "Xiaodan Liang",
      "Yiwei Wang",
      "Jing Tang"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable reasoning capabilities through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However, a key limitation of existing approaches is that rewards defined at the full trajectory level provide insufficient guidance for optimizing the intermediate steps of a reasoning process. To address this, we introduce \\textbf{\\name}, a novel method that estimates the mathematical expectations of rewards at various reasoning steps using tree sampling. Unlike prior methods that rely on a separate step reward model, \\name directly estimates these rewards through this sampling process. Building on the group-relative reward training mechanism of GRPO, \\name innovatively computes rewards based on step-level groups generated during tree sampling. This advancement allows \\name to produce fine-grained and dense reward signals, significantly enhancing the learning process and overall performance of LLMs. Experimental results demonstrate that our \\name algorithm substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test benchmarks, increasing it from 19.0\\% to 35.5\\%. Furthermore, \\name significantly outperforms GRPO by 2.9\\% in performance while simultaneously reducing the average response length by 18.1\\%, showcasing its effectiveness and efficiency. Our code will be available at \\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.05183v2",
    "published_date": "2025-06-05 15:56:38 UTC",
    "updated_date": "2025-09-27 06:27:33 UTC"
  },
  {
    "arxiv_id": "2506.05171v2",
    "title": "Towards provable probabilistic safety for scalable embodied AI systems",
    "authors": [
      "Linxuan He",
      "Qing-Shan Jia",
      "Ang Li",
      "Hongyan Sang",
      "Ling Wang",
      "Jiwen Lu",
      "Tao Zhang",
      "Jie Zhou",
      "Yi Zhang",
      "Yisen Wang",
      "Peng Wei",
      "Zhongyuan Wang",
      "Henry X. Liu",
      "Shuo Feng"
    ],
    "abstract": "Embodied AI systems, comprising AI models and physical plants, are increasingly prevalent across various applications. Due to the rarity of system failures, ensuring their safety in complex operating environments remains a major challenge, which severely hinders their large-scale deployment in safety-critical domains, such as autonomous vehicles, medical devices, and robotics. While achieving provable deterministic safety--verifying system safety across all possible scenarios--remains theoretically ideal, the rarity and complexity of corner cases make this approach impractical for scalable embodied AI systems. Instead, empirical safety evaluation is employed as an alternative, but the absence of provable guarantees imposes significant limitations. To address these issues, we argue for a paradigm shift to provable probabilistic safety that integrates provable guarantees with progressive achievement toward a probabilistic safety boundary on overall system performance. The new paradigm better leverages statistical methods to enhance feasibility and scalability, and a well-defined probabilistic safety boundary enables embodied AI systems to be deployed at scale. In this Perspective, we outline a roadmap for provable probabilistic safety, along with corresponding challenges and potential solutions. By bridging the gap between theoretical safety assurance and practical deployment, this Perspective offers a pathway toward safer, large-scale adoption of embodied AI systems in safety-critical applications.",
    "categories": [
      "eess.SY",
      "cs.AI"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05171v2",
    "published_date": "2025-06-05 15:46:25 UTC",
    "updated_date": "2025-07-22 12:41:49 UTC"
  },
  {
    "arxiv_id": "2506.05167v2",
    "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
    "authors": [
      "Yeonseok Jeong",
      "Jinsu Kim",
      "Dohyeon Lee",
      "Seung-won Hwang"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or ECoRAG framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05167v2",
    "published_date": "2025-06-05 15:43:49 UTC",
    "updated_date": "2025-06-06 07:57:28 UTC"
  },
  {
    "arxiv_id": "2506.05166v2",
    "title": "Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective",
    "authors": [
      "Bhavik Chandna",
      "Zubair Bashir",
      "Procheta Sen"
    ],
    "abstract": "Large Language Models (LLMs) are known to exhibit social, demographic, and gender biases, often as a consequence of the data on which they are trained. In this work, we adopt a mechanistic interpretability approach to analyze how such biases are structurally represented within models such as GPT-2 and Llama2. Focusing on demographic and gender biases, we explore different metrics to identify the internal edges responsible for biased behavior. We then assess the stability, localization, and generalizability of these components across dataset and linguistic variations. Through systematic ablations, we demonstrate that bias-related computations are highly localized, often concentrated in a small subset of layers. Moreover, the identified components change across fine-tuning settings, including those unrelated to bias. Finally, we show that removing these components not only reduces biased outputs but also affects other NLP tasks, such as named entity recognition and linguistic acceptability judgment because of the sharing of important components with these tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05166v2",
    "published_date": "2025-06-05 15:43:34 UTC",
    "updated_date": "2025-06-06 01:35:43 UTC"
  },
  {
    "arxiv_id": "2506.05154v3",
    "title": "Resisting Contextual Interference in RAG via Parametric-Knowledge Reinforcement",
    "authors": [
      "Chenyu Lin",
      "Yilin Wen",
      "Du Su",
      "Hexiang Tan",
      "Fei Sun",
      "Muhan Chen",
      "Chenfu Bao",
      "Zhonghou Lyu"
    ],
    "abstract": "Retrieval-augmented generation (RAG) improves performance on knowledge-intensive tasks but can be derailed by wrong, irrelevant, or conflicting retrieved text, causing models to rely on inaccurate evidence and cascade errors. We propose Knowledgeable-R1, a reinforcement-learning framework that explicitly trains large language models to use parametric knowledge (PK) to resist contextual interference while still exploiting external context when it is reliably helpful. Knowledgeable-R1 introduces a joint sampling scheme that generates paired responses with and without retrieval, and learns both local advantages (within each decoding regime) and global advantages under the same input to quantify when to ignore misleading context versus adopt it. We employ an asymmetric advantage transformation that amplifies exploratory behaviors toward parametric knowledge. Experiments show that \\method significantly improves robustness and reasoning accuracy in knowledge conflict scenarios and general RAG scenarios, outperforming SOTA baselines by 23% in counterfactual scenarios, and without degradation when the retrieved context is fully accurate.Our code are available at https://github.com/lcy80366872/knowledgeable-R1.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05154v3",
    "published_date": "2025-06-05 15:34:15 UTC",
    "updated_date": "2025-09-29 22:47:56 UTC"
  },
  {
    "arxiv_id": "2506.05140v2",
    "title": "AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models",
    "authors": [
      "Chih-Kai Yang",
      "Neo Ho",
      "Yi-Jyun Lee",
      "Hung-yi Lee"
    ],
    "abstract": "Understanding the internal mechanisms of large audio-language models (LALMs) is crucial for interpreting their behavior and improving performance. This work presents the first in-depth analysis of how LALMs internally perceive and recognize auditory attributes. By applying vocabulary projection on three state-of-the-art LALMs, we track how attribute information evolves across layers and token positions. We find that attribute information generally decreases with layer depth when recognition fails, and that resolving attributes at earlier layers correlates with better accuracy. Moreover, LALMs heavily rely on querying auditory inputs for predicting attributes instead of aggregating necessary information in hidden states at attribute-mentioning positions. Based on our findings, we demonstrate a method to enhance LALMs. Our results offer insights into auditory attribute processing, paving the way for future improvements.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ASRU 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05140v2",
    "published_date": "2025-06-05 15:22:47 UTC",
    "updated_date": "2025-08-24 16:16:18 UTC"
  },
  {
    "arxiv_id": "2506.05447v2",
    "title": "Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning",
    "authors": [
      "Andrei Mircea",
      "Supriyo Chakraborty",
      "Nima Chitsazan",
      "Milind Naphade",
      "Sambit Sahu",
      "Irina Rish",
      "Ekaterina Lobacheva"
    ],
    "abstract": "This work aims to understand how scaling improves language models, specifically in terms of training dynamics. We find that language models undergo loss deceleration early in training; an abrupt slowdown in the rate of loss improvement, resulting in piecewise linear behaviour of the loss curve in log-log space. Scaling up the model mitigates this transition by (1) decreasing the loss at which deceleration occurs, and (2) improving the log-log rate of loss improvement after deceleration. We attribute loss deceleration to a type of degenerate training dynamics we term zero-sum learning (ZSL). In ZSL, per-example gradients become systematically opposed, leading to destructive interference in per-example changes in loss. As a result, improving loss on one subset of examples degrades it on another, bottlenecking overall progress. Loss deceleration and ZSL provide new insights into the training dynamics underlying language model scaling laws, and could potentially be targeted directly to improve language models independent of scale. We make our code and artefacts available at: https://github.com/mirandrom/zsl",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05447v2",
    "published_date": "2025-06-05 15:18:35 UTC",
    "updated_date": "2025-07-14 23:29:38 UTC"
  },
  {
    "arxiv_id": "2506.06390v1",
    "title": "Benchmarking Large Language Models on Homework Assessment in Circuit Analysis",
    "authors": [
      "Liangliang Chen",
      "Zhihao Qin",
      "Yiming Guo",
      "Jacqueline Rohde",
      "Ying Zhang"
    ],
    "abstract": "Large language models (LLMs) have the potential to revolutionize various fields, including code development, robotics, finance, and education, due to their extensive prior knowledge and rapid advancements. This paper investigates how LLMs can be leveraged in engineering education. Specifically, we benchmark the capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama 3 70B, in assessing homework for an undergraduate-level circuit analysis course. We have developed a novel dataset consisting of official reference solutions and real student solutions to problems from various topics in circuit analysis. To overcome the limitations of image recognition in current state-of-the-art LLMs, the solutions in the dataset are converted to LaTeX format. Using this dataset, a prompt template is designed to test five metrics of student solutions: completeness, method, final answer, arithmetic error, and units. The results show that GPT-4o and Llama 3 70B perform significantly better than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B each having distinct advantages in different evaluation aspects. Additionally, we present insights into the limitations of current LLMs in several aspects of circuit analysis. Given the paramount importance of ensuring reliability in LLM-generated homework assessment to avoid misleading students, our results establish benchmarks and offer valuable insights for the development of a reliable, personalized tutor for circuit analysis -- a focus of our future work. Furthermore, the proposed evaluation methods can be generalized to a broader range of courses for engineering education in the future.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06390v1",
    "published_date": "2025-06-05 15:16:30 UTC",
    "updated_date": "2025-06-05 15:16:30 UTC"
  },
  {
    "arxiv_id": "2506.05128v2",
    "title": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning",
    "authors": [
      "Tanmay Parekh",
      "Kartik Mehta",
      "Ninareh Mehrabi",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ],
    "abstract": "Zero-shot Event Detection (ED), the task of identifying event mentions in natural language text without any training data, is critical for document understanding in specialized domains. Understanding the complex event ontology, extracting domain-specific triggers from the passage, and structuring them appropriately overloads and limits the utility of Large Language Models (LLMs) for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent reasoning framework that decouples the task of ED using Dreamer and Grounder. Dreamer encourages divergent reasoning through open-ended event discovery, which helps to boost event coverage. Conversely, Grounder introduces convergent reasoning to align the free-form predictions with the task-specific instructions using finite-state machine guided constrained decoding. Additionally, an LLM-Judge verifies the final outputs to ensure high precision. Through extensive experiments on six datasets across five domains and nine LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot, transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains over the best baseline -- establishing DiCoRe as a strong zero-shot ED framework.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2506.05128v2",
    "published_date": "2025-06-05 15:16:14 UTC",
    "updated_date": "2025-09-18 02:12:05 UTC"
  },
  {
    "arxiv_id": "2506.05109v1",
    "title": "Truly Self-Improving Agents Require Intrinsic Metacognitive Learning",
    "authors": [
      "Tennison Liu",
      "Mihaela van der Schaar"
    ],
    "abstract": "Self-improving agents aim to continuously acquire new capabilities with minimal supervision. However, current approaches face two key limitations: their self-improvement processes are often rigid, fail to generalize across tasks domains, and struggle to scale with increasing agent capabilities. We argue that effective self-improvement requires intrinsic metacognitive learning, defined as an agent's intrinsic ability to actively evaluate, reflect on, and adapt its own learning processes. Drawing inspiration from human metacognition, we introduce a formal framework comprising three components: metacognitive knowledge (self-assessment of capabilities, tasks, and learning strategies), metacognitive planning (deciding what and how to learn), and metacognitive evaluation (reflecting on learning experiences to improve future learning). Analyzing existing self-improving agents, we find they rely predominantly on extrinsic metacognitive mechanisms, which are fixed, human-designed loops that limit scalability and adaptability. Examining each component, we contend that many ingredients for intrinsic metacognition are already present. Finally, we explore how to optimally distribute metacognitive responsibilities between humans and agents, and robustly evaluate and improve intrinsic metacognitive learning, key challenges that must be addressed to enable truly sustained, generalized, and aligned self-improvement.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published as a conference paper at ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05109v1",
    "published_date": "2025-06-05 14:53:35 UTC",
    "updated_date": "2025-06-05 14:53:35 UTC"
  },
  {
    "arxiv_id": "2506.05104v4",
    "title": "Survey on the Evaluation of Generative Models in Music",
    "authors": [
      "Alexander Lerch",
      "Claire Arthur",
      "Nick Bryan-Kinns",
      "Corey Ford",
      "Qianyi Sun",
      "Ashvala Vinay"
    ],
    "abstract": "Research on generative systems in music has seen considerable attention and growth in recent years. A variety of attempts have been made to systematically evaluate such systems.\n  We present an interdisciplinary review of the common evaluation targets, methodologies, and metrics for the evaluation of both system output and model use, covering subjective and objective approaches, qualitative and quantitative approaches, as well as empirical and computational methods. We examine the benefits and limitations of these approaches from a musicological, an engineering, and an HCI perspective.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted paper submitted to ACM CSUR on 12-Sep-2025, original manuscript submitted on 26-Jun-2024",
    "pdf_url": "https://arxiv.org/pdf/2506.05104v4",
    "published_date": "2025-06-05 14:46:04 UTC",
    "updated_date": "2025-09-20 20:26:48 UTC"
  },
  {
    "arxiv_id": "2506.05069v2",
    "title": "Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance LLM Recommendation",
    "authors": [
      "Keyu Zhao",
      "Fengli Xu",
      "Yong Li"
    ],
    "abstract": "Driven by advances in Large Language Models (LLMs), integrating them into recommendation tasks has gained interest due to their strong semantic understanding and prompt flexibility. Prior work encoded user-item interactions or metadata into prompts for recommendations. In parallel, LLM reasoning, boosted by test-time scaling and reinforcement learning, has excelled in fields like mathematics and code, where reasoning traces and correctness signals are clear, enabling high performance and interpretability. However, directly applying these reasoning methods to recommendation is ineffective because user feedback is implicit and lacks reasoning supervision. To address this, we propose $\\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that samples interaction chains from the user-item graph and converts them into structured interaction-of-thoughts via a progressive masked prompting strategy, with each thought representing stepwise reasoning grounded in interaction context. This allows LLMs to simulate step-by-step decision-making based on implicit patterns. We design a two-stage training pipeline: supervised fine-tuning teaches basic reasoning from high-quality traces, and reinforcement learning refines reasoning via reward signals, alleviating sparse explicit supervision. Experiments on three real-world datasets show R2Rec outperforms classical and LLM-based baselines with an average $\\textbf{10.48%}$ improvement in HitRatio@1 and $\\textbf{131.81%}$ gain over the original LLM. Furthermore, the explicit reasoning chains enhance interpretability by revealing the decision process. Our code is available at: https://anonymous.4open.science/r/R2Rec-7C5D.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05069v2",
    "published_date": "2025-06-05 14:16:44 UTC",
    "updated_date": "2025-06-09 09:08:12 UTC"
  },
  {
    "arxiv_id": "2506.05068v2",
    "title": "Does It Make Sense to Speak of Introspection in Large Language Models?",
    "authors": [
      "Iulia M. Comsa",
      "Murray Shanahan"
    ],
    "abstract": "Large language models (LLMs) exhibit compelling linguistic behaviour, and sometimes offer self-reports, that is to say statements about their own nature, inner workings, or behaviour. In humans, such reports are often attributed to a faculty of introspection and are typically linked to consciousness. This raises the question of how to interpret self-reports produced by LLMs, given their increasing linguistic fluency and cognitive capabilities. To what extent (if any) can the concept of introspection be meaningfully applied to LLMs? Here, we present and critique two examples of apparent introspective self-report from LLMs. In the first example, an LLM attempts to describe the process behind its own \"creative\" writing, and we argue this is not a valid example of introspection. In the second example, an LLM correctly infers the value of its own temperature parameter, and we argue that this can be legitimately considered a minimal example of introspection, albeit one that is (presumably) not accompanied by conscious experience.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05068v2",
    "published_date": "2025-06-05 14:13:54 UTC",
    "updated_date": "2025-06-06 11:26:38 UTC"
  },
  {
    "arxiv_id": "2506.05446v1",
    "title": "Sentinel: SOTA model to protect against prompt injections",
    "authors": [
      "Dror Ivry",
      "Oran Nahum"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly powerful but remain vulnerable to prompt injection attacks, where malicious inputs cause the model to deviate from its intended instructions. This paper introduces Sentinel, a novel detection model, qualifire/prompt-injection-sentinel, based on the \\answerdotai/ModernBERT-large architecture. By leveraging ModernBERT's advanced features and fine-tuning on an extensive and diverse dataset comprising a few open-source and private collections, Sentinel achieves state-of-the-art performance. This dataset amalgamates varied attack types, from role-playing and instruction hijacking to attempts to generate biased content, alongside a broad spectrum of benign instructions, with private datasets specifically targeting nuanced error correction and real-world misclassifications. On a comprehensive, unseen internal test set, Sentinel demonstrates an average accuracy of 0.987 and an F1-score of 0.980. Furthermore, when evaluated on public benchmarks, it consistently outperforms strong baselines like protectai/deberta-v3-base-prompt-injection-v2. This work details Sentinel's architecture, its meticulous dataset curation, its training methodology, and a thorough evaluation, highlighting its superior detection capabilities.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.05446v1",
    "published_date": "2025-06-05 14:07:15 UTC",
    "updated_date": "2025-06-05 14:07:15 UTC"
  },
  {
    "arxiv_id": "2506.05057v1",
    "title": "TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages",
    "authors": [
      "Moshe Ofer",
      "Orel Zamler",
      "Amos Azaria"
    ],
    "abstract": "Large Language Models (LLMs) excel in high-resource languages but struggle with low-resource languages due to limited training data. This paper presents TALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages), which integrates an LLM with two bilingual translation models. TALL transforms low-resource inputs into high-resource representations, leveraging the LLM's capabilities while preserving linguistic features through dimension alignment layers and custom transformers. Our experiments on Hebrew demonstrate significant improvements over several baselines, including direct use, naive translation, and fine-tuning approaches. The architecture employs a parameter-efficient strategy, freezing pre-trained components while training only lightweight adapter modules, balancing computational efficiency with performance gains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05057v1",
    "published_date": "2025-06-05 14:02:12 UTC",
    "updated_date": "2025-06-05 14:02:12 UTC"
  },
  {
    "arxiv_id": "2506.05445v1",
    "title": "Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic",
    "authors": [
      "Thanh Vinh Vo",
      "Young Lee",
      "Haozhe Ma",
      "Chien Lu",
      "Tze-Yun Leong"
    ],
    "abstract": "Hidden confounders that influence both states and actions can bias policy learning in reinforcement learning (RL), leading to suboptimal or non-generalizable behavior. Most RL algorithms ignore this issue, learning policies from observational trajectories based solely on statistical associations rather than causal effects. We propose DoSAC (Do-Calculus Soft Actor-Critic with Backdoor Adjustment), a principled extension of the SAC algorithm that corrects for hidden confounding via causal intervention estimation. DoSAC estimates the interventional policy $π(a | \\mathrm{do}(s))$ using the backdoor criterion, without requiring access to true confounders or causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor that infers pseudo-past variables (previous state and action) from the current state to enable backdoor adjustment from observational data. This module is integrated into a soft actor-critic framework to compute both the interventional policy and its entropy. Empirical results on continuous control benchmarks show that DoSAC outperforms baselines under confounded settings, with improved robustness, generalization, and policy reliability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2506.05445v1",
    "published_date": "2025-06-05 13:52:38 UTC",
    "updated_date": "2025-06-05 13:52:38 UTC"
  },
  {
    "arxiv_id": "2506.05035v1",
    "title": "TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation",
    "authors": [
      "Hyeongwon Jang",
      "Changhun Kim",
      "Eunho Yang"
    ],
    "abstract": "Recent explainable artificial intelligence (XAI) methods for time series primarily estimate point-wise attribution magnitudes, while overlooking the directional impact on predictions, leading to suboptimal identification of significant points. Our analysis shows that conventional Integrated Gradients (IG) effectively capture critical points with both positive and negative impacts on predictions. However, current evaluation metrics fail to assess this capability, as they inadvertently cancel out opposing feature contributions. To address this limitation, we propose novel evaluation metrics-Cumulative Prediction Difference (CPD) and Cumulative Prediction Preservation (CPP)-to systematically assess whether attribution methods accurately identify significant positive and negative points in time series XAI. Under these metrics, conventional IG outperforms recent counterparts. However, directly applying IG to time series data may lead to suboptimal outcomes, as generated paths ignore temporal relationships and introduce out-of-distribution samples. To overcome these challenges, we introduce TIMING, which enhances IG by incorporating temporal awareness while maintaining its theoretical properties. Extensive experiments on synthetic and real-world time series benchmarks demonstrate that TIMING outperforms existing time series XAI baselines. Our code is available at https://github.com/drumpt/TIMING.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025 Spotlight Presentation; Code is available at https://github.com/drumpt/TIMING",
    "pdf_url": "https://arxiv.org/pdf/2506.05035v1",
    "published_date": "2025-06-05 13:40:40 UTC",
    "updated_date": "2025-06-05 13:40:40 UTC"
  },
  {
    "arxiv_id": "2506.05032v1",
    "title": "Identifying and Understanding Cross-Class Features in Adversarial Training",
    "authors": [
      "Zeming Wei",
      "Yiwen Guo",
      "Yisen Wang"
    ],
    "abstract": "Adversarial training (AT) has been considered one of the most effective methods for making deep neural networks robust against adversarial attacks, while the training mechanisms and dynamics of AT remain open research problems. In this paper, we present a novel perspective on studying AT through the lens of class-wise feature attribution. Specifically, we identify the impact of a key family of features on AT that are shared by multiple classes, which we call cross-class features. These features are typically useful for robust classification, which we offer theoretical evidence to illustrate through a synthetic data model. Through systematic studies across multiple model architectures and settings, we find that during the initial stage of AT, the model tends to learn more cross-class features until the best robustness checkpoint. As AT further squeezes the training robust loss and causes robust overfitting, the model tends to make decisions based on more class-specific features. Based on these discoveries, we further provide a unified view of two existing properties of AT, including the advantage of soft-label training and robust overfitting. Overall, these insights refine the current understanding of AT mechanisms and provide new perspectives on studying them. Our code is available at https://github.com/PKU-ML/Cross-Class-Features-AT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05032v1",
    "published_date": "2025-06-05 13:40:11 UTC",
    "updated_date": "2025-06-05 13:40:11 UTC"
  },
  {
    "arxiv_id": "2506.05030v1",
    "title": "Artificial Intelligence Should Genuinely Support Clinical Reasoning and Decision Making To Bridge the Translational Gap",
    "authors": [
      "Kacper Sokol",
      "James Fackler",
      "Julia E Vogt"
    ],
    "abstract": "Artificial intelligence promises to revolutionise medicine, yet its impact remains limited because of the pervasive translational gap. We posit that the prevailing technology-centric approaches underpin this challenge, rendering such systems fundamentally incompatible with clinical practice, specifically diagnostic reasoning and decision making. Instead, we propose a novel sociotechnical conceptualisation of data-driven support tools designed to complement doctors' cognitive and epistemic activities. Crucially, it prioritises real-world impact over superhuman performance on inconsequential benchmarks.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05030v1",
    "published_date": "2025-06-05 13:39:37 UTC",
    "updated_date": "2025-06-05 13:39:37 UTC"
  },
  {
    "arxiv_id": "2506.05020v3",
    "title": "Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System",
    "authors": [
      "Haokun Liu",
      "Zhaoqi Ma",
      "Yunong Li",
      "Junichiro Sugihara",
      "Yicheng Chen",
      "Jinjie Li",
      "Moju Zhao"
    ],
    "abstract": "Heterogeneous multirobot systems show great potential in complex tasks requiring coordinated hybrid cooperation. However, existing methods that rely on static or task-specific models often lack generalizability across diverse tasks and dynamic environments. This highlights the need for generalizable intelligence that can bridge high-level reasoning with low-level execution across heterogeneous agents. To address this, we propose a hierarchical multimodal framework that integrates a prompted large language model (LLM) with a fine-tuned vision-language model (VLM). At the system level, the LLM performs hierarchical task decomposition and constructs a global semantic map, while the VLM provides semantic perception and object localization, where the proposed GridMask significantly enhances the VLM's spatial accuracy for reliable fine-grained manipulation. The aerial robot leverages this global map to generate semantic paths and guide the ground robot's local navigation and manipulation, ensuring robust coordination even in target-absent or ambiguous scenarios. We validate the framework through extensive simulation and real-world experiments on long-horizon object arrangement tasks, demonstrating zero-shot adaptability, robust semantic navigation, and reliable manipulation in dynamic environments. To the best of our knowledge, this work presents the first heterogeneous aerial-ground robotic system that integrates VLM-based perception with LLM-driven reasoning for global high-level task planning and execution.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "18 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.05020v3",
    "published_date": "2025-06-05 13:27:41 UTC",
    "updated_date": "2025-10-27 04:26:01 UTC"
  },
  {
    "arxiv_id": "2506.05014v1",
    "title": "Towards Reasonable Concept Bottleneck Models",
    "authors": [
      "Nektarios Kalampalikis",
      "Kavya Gupta",
      "Georgi Vitanov",
      "Isabel Valera"
    ],
    "abstract": "In this paper, we propose $\\textbf{C}$oncept $\\textbf{REA}$soning $\\textbf{M}$odels (CREAM), a novel family of Concept Bottleneck Models (CBMs) that: (i) explicitly encodes concept-concept (${\\texttt{C-C}}$) and concept-task (${\\texttt{C$\\rightarrow$Y}}$) relationships to enforce a desired model reasoning; and (ii) use a regularized side-channel to achieve competitive task performance, while keeping high concept importance. Specifically, CREAM architecturally embeds (bi)directed concept-concept, and concept to task relationships specified by a human expert, while severing undesired information flows (e.g., to handle mutually exclusive concepts). Moreover, CREAM integrates a black-box side-channel that is regularized to encourage task predictions to be grounded in the relevant concepts, thereby utilizing the side-channel only when necessary to enhance performance. Our experiments show that: (i) CREAM mainly relies on concepts while achieving task performance on par with black-box models; and (ii) the embedded ${\\texttt{C-C}}$ and ${\\texttt{C$\\rightarrow$Y}}$ relationships ease model interventions and mitigate concept leakage.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 17 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.05014v1",
    "published_date": "2025-06-05 13:22:29 UTC",
    "updated_date": "2025-06-05 13:22:29 UTC"
  },
  {
    "arxiv_id": "2506.04998v2",
    "title": "Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based Approach for Complex Arithmetic Reasoning",
    "authors": [
      "Mehdi Azarafza",
      "Mojtaba Nayyeri",
      "Faezeh Pasandideh",
      "Steffen Staab",
      "Achim Rettberg"
    ],
    "abstract": "Autonomous UAV operation necessitates reliable mathematical reasoning for tasks such as trajectory planning and power management. While traditional flight control relies on hardcoded equations, recent Large Language Models (LLMs) offer potential for more flexible problem-solving but struggle with reliably selecting and applying correct mathematical formulations and executing precise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented generation framework designed to improve the mathematical reasoning of several LLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in UAV-specific contexts by providing access to relevant domain literature. To conduct an initial assessment, we introduce the UAV-Math-Bench, a 20-question problem set of UAV-centric mathematical problems across four difficulty levels. Our experiments demonstrate that incorporating retrieval substantially increases exact answer accuracy (achieving up to 75% with o1), reduces instances of incorrect formulation selection (from 25% without RAG to 5\\% with RAG), and decreases numerical errors, reducing Mean Squared Error (MSE) by orders of magnitude for the best-performing models. This pilot study indicates that RAG can enable general-purpose LLMs to function as more reliable tools for engineering analysis, although direct real-time flight control requires further investigation and validation on a larger scale. All benchmark data, questions, and answers are publicly available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 7 figures, 4 appendix subsections",
    "pdf_url": "https://arxiv.org/pdf/2506.04998v2",
    "published_date": "2025-06-05 13:09:24 UTC",
    "updated_date": "2025-06-30 13:18:47 UTC"
  },
  {
    "arxiv_id": "2506.05443v1",
    "title": "UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss",
    "authors": [
      "Yiyu Lin",
      "Yan Wang",
      "You Zhou",
      "Xinye Ni",
      "Jiahui Wu",
      "Sen Yang"
    ],
    "abstract": "As a core mechanism of epigenetic regulation in eukaryotes, protein post-translational modifications (PTMs) require precise prediction to decipher dynamic life activity networks. To address the limitations of existing deep learning models in cross-modal feature fusion, domain generalization, and architectural optimization, this study proposes UniPTMs: the first unified framework for multi-type PTM prediction. The framework innovatively establishes a \"Master-Slave\" dual-path collaborative architecture: The master path dynamically integrates high-dimensional representations of protein sequences, structures, and evolutionary information through a Bidirectional Gated Cross-Attention (BGCA) module, while the slave path optimizes feature discrepancies and recalibration between structural and traditional features using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level feature integration across paths, the framework employs a Hierarchical Dynamic Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal features. Enhanced by a novel Hierarchical Contrastive loss function for feature consistency optimization, UniPTMs demonstrates significant performance improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art models across five modification types and transcends the Single-Type Prediction Paradigm. To strike a balance between model complexity and performance, we have also developed a lightweight variant named UniPTMs-mini.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05443v1",
    "published_date": "2025-06-05 13:02:43 UTC",
    "updated_date": "2025-06-05 13:02:43 UTC"
  },
  {
    "arxiv_id": "2506.04987v1",
    "title": "A Multi-Dataset Evaluation of Models for Automated Vulnerability Repair",
    "authors": [
      "Zanis Ali Khan",
      "Aayush Garg",
      "Qiang Tang"
    ],
    "abstract": "Software vulnerabilities pose significant security threats, requiring effective mitigation. While Automated Program Repair (APR) has advanced in fixing general bugs, vulnerability patching, a security-critical aspect of APR remains underexplored. This study investigates pre-trained language models, CodeBERT and CodeT5, for automated vulnerability patching across six datasets and four languages. We evaluate their accuracy and generalization to unknown vulnerabilities. Results show that while both models face challenges with fragmented or sparse context, CodeBERT performs comparatively better in such scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns. CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned models on both in-distribution (trained) and out-of-distribution (unseen) datasets. While fine-tuning improves in-distribution performance, models struggle to generalize to unseen data, highlighting challenges in robust vulnerability detection. This study benchmarks model performance, identifies limitations in generalization, and provides actionable insights to advance automated vulnerability patching for real-world security applications.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Preprint has been accepted in ARES AI&CCPS (International Workshop on Artificial Intelligence, Cyber and Cyber-Physical Security)",
    "pdf_url": "https://arxiv.org/pdf/2506.04987v1",
    "published_date": "2025-06-05 13:00:19 UTC",
    "updated_date": "2025-06-05 13:00:19 UTC"
  },
  {
    "arxiv_id": "2506.05442v1",
    "title": "Structured Labeling Enables Faster Vision-Language Models for End-to-End Autonomous Driving",
    "authors": [
      "Hao Jiang",
      "Chuan Hu",
      "Yukang Shi",
      "Yuan He",
      "Ke Wang",
      "Xi Zhang",
      "Zhipeng Zhang"
    ],
    "abstract": "Vision-Language Models (VLMs) offer a promising approach to end-to-end autonomous driving due to their human-like reasoning capabilities. However, troublesome gaps remains between current VLMs and real-world autonomous driving applications. One major limitation is that existing datasets with loosely formatted language descriptions are not machine-friendly and may introduce redundancy. Additionally, high computational cost and massive scale of VLMs hinder the inference speed and real-world deployment. To bridge the gap, this paper introduces a structured and concise benchmark dataset, NuScenes-S, which is derived from the NuScenes dataset and contains machine-friendly structured representations. Moreover, we present FastDrive, a compact VLM baseline with 0.9B parameters. In contrast to existing VLMs with over 7B parameters and unstructured language processing(e.g., LLaVA-1.5), FastDrive understands structured and concise descriptions and generates machine-friendly driving decisions with high efficiency. Extensive experiments show that FastDrive achieves competitive performance on structured dataset, with approximately 20% accuracy improvement on decision-making tasks, while surpassing massive parameter baseline in inference speed with over 10x speedup. Additionally, ablation studies further focus on the impact of scene annotations (e.g., weather, time of day) on decision-making tasks, demonstrating their importance on decision-making tasks in autonomous driving.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05442v1",
    "published_date": "2025-06-05 12:59:35 UTC",
    "updated_date": "2025-06-05 12:59:35 UTC"
  },
  {
    "arxiv_id": "2506.05440v1",
    "title": "BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models",
    "authors": [
      "Ludovic Arnould",
      "Salim Khazem",
      "Hugues Ali Mehenni"
    ],
    "abstract": "Visual Language Models (VLMs) are now sufficiently advanced to support a broad range of applications, including answering complex visual questions, and are increasingly expected to interact with images in varied ways. To evaluate them, current benchmarks often focus on specific domains (e.g., reading charts), constructing datasets of annotated real images paired with pre-defined Multiple Choice Questions (MCQs) to report aggregate accuracy scores. However, such benchmarks entail high annotation costs, risk information leakage, and do not clarify whether failures stem from limitations in visual perception, reasoning, or general knowledge. We propose a new evaluation methodology, inspired by ophthalmologic diagnostics, leveraging procedural generation of synthetic images to obtain control over visual attributes and precisely reveal perception failures in VLMs. Specifically, we build collections of images with gradually more challenging variations in the content of interest (e.g., number of objects in a counting task) while holding other visual parameters constant. This diagnostic allows systematic stress testing and fine-grained failure analysis, shifting the focus from coarse benchmarking toward targeted and interpretable assessment of VLM capabilities. Our code is available at https://github.com/byoeval/BYO-EVAL.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05440v1",
    "published_date": "2025-06-05 12:43:10 UTC",
    "updated_date": "2025-06-05 12:43:10 UTC"
  },
  {
    "arxiv_id": "2506.04965v1",
    "title": "From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm Exams and Pave the Way for Editorial Generation",
    "authors": [
      "Adrian Marius Dumitran",
      "Theodor-Pierre Moroianu",
      "Vasile Paul Alexe"
    ],
    "abstract": "This paper presents a comprehensive evaluation of the performance of state-of-the-art Large Language Models (LLMs) on challenging university-level algorithms exams. By testing multiple models on both a Romanian exam and its high-quality English translation, we analyze LLMs' problem-solving capabilities, consistency, and multilingual performance. Our empirical study reveals that the most recent models not only achieve scores comparable to top-performing students but also demonstrate robust reasoning skills on complex, multi-step algorithmic challenges, even though difficulties remain with graph-based tasks. Building on these findings, we explore the potential of LLMs to support educational environments through the generation of high-quality editorial content, offering instructors a powerful tool to enhance student feedback. The insights and best practices discussed herein pave the way for further integration of generative AI in advanced algorithm education.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages Pre-print Paper accepted to ITS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04965v1",
    "published_date": "2025-06-05 12:41:20 UTC",
    "updated_date": "2025-06-05 12:41:20 UTC"
  },
  {
    "arxiv_id": "2506.11089v1",
    "title": "Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM",
    "authors": [
      "Jeena Prakash",
      "Blessingh Kumar",
      "Kadri Hacioglu",
      "Bidisha Sharma",
      "Sindhuja Gopalan",
      "Malolan Chetlur",
      "Shankar Venkatesan",
      "Andreas Stolcke"
    ],
    "abstract": "Automatic speech recognition (ASR) models rely on high-quality transcribed data for effective training. Generating pseudo-labels for large unlabeled audio datasets often relies on complex pipelines that combine multiple ASR outputs through multi-stage processing, leading to error propagation, information loss and disjoint optimization. We propose a unified multi-ASR prompt-driven framework using postprocessing by either textual or speech-based large language models (LLMs), replacing voting or other arbitration logic for reconciling the ensemble outputs. We perform a comparative study of multiple architectures with and without LLMs, showing significant improvements in transcription accuracy compared to traditional methods. Furthermore, we use the pseudo-labels generated by the various approaches to train semi-supervised ASR models for different datasets, again showing improved performance with textual and speechLLM transcriptions compared to baselines.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11089v1",
    "published_date": "2025-06-05 12:35:53 UTC",
    "updated_date": "2025-06-05 12:35:53 UTC"
  },
  {
    "arxiv_id": "2506.04951v1",
    "title": "Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations",
    "authors": [
      "Igor Meleshin",
      "Anna Chistyakova",
      "Anastasia Antsiferova",
      "Dmitriy Vatolin"
    ],
    "abstract": "Image Quality Assessment (IQA) models are increasingly relied upon to evaluate image quality in real-world systems -- from compression and enhancement to generation and streaming. Yet their adoption brings a fundamental risk: these models are inherently unstable. Adversarial manipulations can easily fool them, inflating scores and undermining trust. Traditionally, such vulnerabilities are addressed through data-driven defenses -- adversarial retraining, regularization, or input purification. But what if this is the wrong lens? What if robustness in perceptual models is not something to learn but something to design? In this work, we propose a provocative idea: robustness as an architectural prior. Rather than training models to resist perturbations, we reshape their internal structure to suppress sensitivity from the ground up. We achieve this by enforcing orthogonal information flow, constraining the network to norm-preserving operations -- and further stabilizing the system through pruning and fine-tuning. The result is a robust IQA architecture that withstands adversarial attacks without requiring adversarial training or significant changes to the original model. This approach suggests a shift in perspective: from optimizing robustness through data to engineering it through design.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04951v1",
    "published_date": "2025-06-05 12:24:38 UTC",
    "updated_date": "2025-06-05 12:24:38 UTC"
  },
  {
    "arxiv_id": "2506.04931v2",
    "title": "CzechLynx: A Dataset for Individual Identification and Pose Estimation of the Eurasian Lynx",
    "authors": [
      "Lukas Picek",
      "Elisa Belotti",
      "Michal Bojda",
      "Ludek Bufka",
      "Vojtech Cermak",
      "Martin Dula",
      "Rostislav Dvorak",
      "Luboslav Hrdy",
      "Miroslav Jirik",
      "Vaclav Kocourek",
      "Josefa Krausova",
      "Jirı Labuda",
      "Jakub Straka",
      "Ludek Toman",
      "Vlado Trulık",
      "Martin Vana",
      "Miroslav Kutal"
    ],
    "abstract": "We introduce CzechLynx, the first large-scale, open-access dataset for individual identification, pose estimation, and instance segmentation of the Eurasian lynx (Lynx lynx). CzechLynx contains 39,760 camera trap images annotated with segmentation masks, identity labels, and 20-point skeletons and covers 319 unique individuals across 15 years of systematic monitoring in two geographically distinct regions: southwest Bohemia and the Western Carpathians. In addition to the real camera trap data, we provide a large complementary set of photorealistic synthetic images and a Unity-based generation pipeline with diffusion-based text-to-texture modeling, capable of producing arbitrarily large amounts of synthetic data spanning diverse environments, poses, and coat-pattern variations. To enable systematic testing across realistic ecological scenarios, we define three complementary evaluation protocols: (i) geo-aware, (ii) time-aware open-set, and (iii) time-aware closed-set, covering cross-regional and long-term monitoring settings. With the provided resources, CzechLynx offers a unique, flexible benchmark for robust evaluation of computer vision and machine learning models across realistic ecological scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04931v2",
    "published_date": "2025-06-05 12:05:43 UTC",
    "updated_date": "2025-11-28 18:06:29 UTC"
  },
  {
    "arxiv_id": "2506.05439v2",
    "title": "LLMs Can Compensate for Deficiencies in Visual Representations",
    "authors": [
      "Sho Takishita",
      "Jay Gala",
      "Abdelrahman Mohamed",
      "Kentaro Inui",
      "Yova Kementchedjhieva"
    ],
    "abstract": "Many vision-language models (VLMs) that prove very effective at a range of multimodal task, build on CLIP-based vision encoders, which are known to have various limitations. We investigate the hypothesis that the strong language backbone in VLMs compensates for possibly weak visual features by contextualizing or enriching them. Using three CLIP-based VLMs, we perform controlled self-attention ablations on a carefully designed probing task. Our findings show that despite known limitations, CLIP visual representations offer ready-to-read semantic information to the language decoder. However, in scenarios of reduced contextualization in the visual representations, the language decoder can largely compensate for the deficiency and recover performance. This suggests a dynamic division of labor in VLMs and motivates future architectures that offload more visual processing to the language decoder.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2506.05439v2",
    "published_date": "2025-06-05 12:04:59 UTC",
    "updated_date": "2025-09-19 15:33:50 UTC"
  },
  {
    "arxiv_id": "2506.04920v1",
    "title": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback",
    "authors": [
      "Junior Cedric Tonga",
      "KV Aditya Srivatsa",
      "Kaushal Kumar Maurya",
      "Fajri Koto",
      "Ekaterina Kochmar"
    ],
    "abstract": "Large language models (LLMs) have demonstrated the ability to generate formative feedback and instructional hints in English, making them increasingly relevant for AI-assisted education. However, their ability to provide effective instructional support across different languages, especially for mathematically grounded reasoning tasks, remains largely unexamined. In this work, we present the first large-scale simulation of multilingual tutor-student interactions using LLMs. A stronger model plays the role of the tutor, generating feedback in the form of hints, while a weaker model simulates the student. We explore 352 experimental settings across 11 typologically diverse languages, four state-of-the-art LLMs, and multiple prompting strategies to assess whether language-specific feedback leads to measurable learning gains. Our study examines how student input language, teacher feedback language, model choice, and language resource level jointly influence performance. Results show that multilingual hints can significantly improve learning outcomes, particularly in low-resource languages when feedback is aligned with the student's native language. These findings offer practical insights for developing multilingual, LLM-based educational tools that are both effective and inclusive.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint, in submission",
    "pdf_url": "https://arxiv.org/pdf/2506.04920v1",
    "published_date": "2025-06-05 11:53:04 UTC",
    "updated_date": "2025-06-05 11:53:04 UTC"
  },
  {
    "arxiv_id": "2506.04916v1",
    "title": "Energentic Intelligence: From Self-Sustaining Systems to Enduring Artificial Life",
    "authors": [
      "Atahan Karagoz"
    ],
    "abstract": "This paper introduces Energentic Intelligence, a class of autonomous systems defined not by task performance, but by their capacity to sustain themselves through internal energy regulation. Departing from conventional reward-driven paradigms, these agents treat survival-maintaining functional operation under fluctuating energetic and thermal conditions-as the central objective. We formalize this principle through an energy-based utility function and a viability-constrained survival horizon, and propose a modular architecture that integrates energy harvesting, thermal regulation, and adaptive computation into a closed-loop control system. A simulated environment demonstrates the emergence of stable, resource-aware behavior without external supervision. Together, these contributions provide a theoretical and architectural foundation for deploying autonomous agents in resource-volatile settings where persistence must be self-regulated and infrastructure cannot be assumed.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04916v1",
    "published_date": "2025-06-05 11:52:21 UTC",
    "updated_date": "2025-06-05 11:52:21 UTC"
  },
  {
    "arxiv_id": "2506.04912v1",
    "title": "Differentiable Logic Cellular Automata: From Game of Life to Pattern Generation",
    "authors": [
      "Pietro Miotti",
      "Eyvind Niklasson",
      "Ettore Randazzo",
      "Alexander Mordvintsev"
    ],
    "abstract": "This paper introduces Differentiable Logic Cellular Automata (DiffLogic CA), a novel combination of Neural Cellular Automata (NCA) and Differentiable Logic Gates Networks (DLGNs). The fundamental computation units of the model are differentiable logic gates, combined into a circuit. During training, the model is fully end-to-end differentiable allowing gradient-based training, and at inference time it operates in a fully discrete state space. This enables learning local update rules for cellular automata while preserving their inherent discrete nature. We demonstrate the versatility of our approach through a series of milestones: (1) fully learning the rules of Conway's Game of Life, (2) generating checkerboard patterns that exhibit resilience to noise and damage, (3) growing a lizard shape, and (4) multi-color pattern generation. Our model successfully learns recurrent circuits capable of generating desired target patterns. For simpler patterns, we observe success with both synchronous and asynchronous updates, demonstrating significant generalization capabilities and robustness to perturbations. We make the case that this combination of DLGNs and NCA represents a step toward programmable matter and robust computing systems that combine binary logic, neural network adaptability, and localized processing. This work, to the best of our knowledge, is the first successful application of differentiable logic gate networks in recurrent architectures.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04912v1",
    "published_date": "2025-06-05 11:45:43 UTC",
    "updated_date": "2025-06-05 11:45:43 UTC"
  },
  {
    "arxiv_id": "2506.04909v1",
    "title": "When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models",
    "authors": [
      "Kai Wang",
      "Yihao Zhang",
      "Meng Sun"
    ],
    "abstract": "The honesty of large language models (LLMs) is a critical alignment challenge, especially as advanced systems with chain-of-thought (CoT) reasoning may strategically deceive humans. Unlike traditional honesty issues on LLMs, which could be possibly explained as some kind of hallucination, those models' explicit thought paths enable us to study strategic deception--goal-driven, intentional misinformation where reasoning contradicts outputs. Using representation engineering, we systematically induce, detect, and control such deception in CoT-enabled LLMs, extracting \"deception vectors\" via Linear Artificial Tomography (LAT) for 89% detection accuracy. Through activation steering, we achieve a 40% success rate in eliciting context-appropriate deception without explicit prompts, unveiling the specific honesty-related issue of reasoning models and providing tools for trustworthy AI alignment.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04909v1",
    "published_date": "2025-06-05 11:44:19 UTC",
    "updated_date": "2025-06-05 11:44:19 UTC"
  },
  {
    "arxiv_id": "2507.00003v1",
    "title": "Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE",
    "authors": [
      "Eyhab Al-Masri"
    ],
    "abstract": "This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework for interpretable intrusion detection in IoT environments. By integrating Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the system decomposes prediction confidence into truth (T), falsity (F), and indeterminacy (I) components, enabling uncertainty quantification and abstention. Predictions with high indeterminacy are flagged for review using both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that misclassified samples exhibit significantly higher indeterminacy (I = 0.62) than correct ones (I = 0.24). The use of indeterminacy as a proxy for uncertainty enables informed abstention and targeted review-particularly valuable in edge deployments. Figures and tables validate the correlation between I-scores and error likelihood, supporting more trustworthy, human-in-the-loop AI decisions. This work shows that neutrosophic logic enhances both accuracy and explainability, providing a practical foundation for trust-aware AI in edge and fog-based IoT security systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00003v1",
    "published_date": "2025-06-05 11:43:31 UTC",
    "updated_date": "2025-06-05 11:43:31 UTC"
  },
  {
    "arxiv_id": "2506.04907v4",
    "title": "Context Is Not Comprehension",
    "authors": [
      "Alex Pan",
      "Mary-Anne Williams"
    ],
    "abstract": "The dominant way of judging Large Language Models (LLMs) has been to ask how well they can recall explicit facts from very long inputs. While today's best models achieve near perfect recall, this masks a harder skill: performing multi-step reasoning and tracking intermediate state that never appears verbatim. We introduce Verbose ListOps (VLO), a benchmark that embeds deterministic ListOps computations inside narrative camouflage and, crucially, allows step-level evaluation of every intermediate result. Experiments show that models which solve raw ListOps with approximately 100% accuracy collapse on VLO after only 10,000 tokens. By exposing where a model's reasoning chain first diverges, VLO moves assessment beyond sheer context length and toward genuine comprehension. VLO's generation pipeline is task-agnostic: it can weave any deterministically verifiable reasoning schema -- arithmetic, symbolic, abductive, inductive or defeasible -- into narrative form. This makes VLO a reusable test-bed for the next wave of reasoning-centric model designs, not merely those with step-explicit scaffolds.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 2 figures, 4 tables; under review",
    "pdf_url": "https://arxiv.org/pdf/2506.04907v4",
    "published_date": "2025-06-05 11:41:05 UTC",
    "updated_date": "2025-06-12 02:06:29 UTC"
  },
  {
    "arxiv_id": "2506.04867v3",
    "title": "Sensory-Motor Control with Large Language Models via Iterative Policy Refinement",
    "authors": [
      "Jônata Tyska Carvalho",
      "Stefano Nolfi"
    ],
    "abstract": "We propose a method that enables large language models (LLMs) to control embodied agents through the generation of control policies that directly map continuous observation vectors to continuous action vectors. At the outset, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. This strategy is then iteratively refined through a learning process in which the LLMs are repeatedly prompted to improve the current strategy, using performance feedback and sensory-motor data collected during its evaluation. The method is validated on classic control tasks from the Gymnasium library and the inverted pendulum task from the MuJoCo library. The approach proves effective with relatively compact models such as GPT-oss:120b and Qwen2.5:72b. In most cases, it successfully identifies optimal or near-optimal solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the agent interacts with its environment.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Article updated with results from gpt-oss:120b and gpt-oss:20b. 27 pages (13 pages are from appendix), 8 figures, 2 tables, code for experiments replication and supplementary material provided at https://github.com/jtyska/llm-robotics-article/",
    "pdf_url": "https://arxiv.org/pdf/2506.04867v3",
    "published_date": "2025-06-05 10:38:28 UTC",
    "updated_date": "2025-11-14 18:32:38 UTC"
  },
  {
    "arxiv_id": "2506.04860v1",
    "title": "Towards Network Data Analytics in 5G Systems and Beyond",
    "authors": [
      "Marcos Lima Romero",
      "Ricardo Suyama"
    ],
    "abstract": "Data has become a critical asset in the digital economy, yet it remains underutilized by Mobile Network Operators (MNOs), unlike Over-the-Top (OTT) players that lead global market valuations. To move beyond the commoditization of connectivity and deliver greater value to customers, data analytics emerges as a strategic enabler. Using data efficiently is essential for unlocking new service opportunities, optimizing operational efficiency, and mitigating operational and business risks. Since Release 15, the 3rd Generation Partnership Project (3GPP) has introduced the Network Data Analytics Function (NWDAF) to provide powerful insights and predictions using data collected across mobile networks, supporting both user-centric and network-oriented use cases. However, academic research has largely focused on a limited set of methods and use cases, driven by the availability of datasets, restricting broader exploration. This study analyzes trends and gaps in more than 70 articles and proposes two novel use cases to promote the adoption of NWDAF and explore its potential for monetization.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "Submitted to XLIII BRAZILIAN SYMPOSIUM ON TELECOMMUNICATIONS AND SIGNAL PROCESSING - SBrT 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04860v1",
    "published_date": "2025-06-05 10:26:53 UTC",
    "updated_date": "2025-06-05 10:26:53 UTC"
  },
  {
    "arxiv_id": "2506.04859v2",
    "title": "Sparse Autoencoders, Again?",
    "authors": [
      "Yin Lu",
      "Xuening Zhu",
      "Tong He",
      "David Wipf"
    ],
    "abstract": "Is there really much more to say about sparse autoencoders (SAEs)? Autoencoders in general, and SAEs in particular, represent deep architectures that are capable of modeling low-dimensional latent structure in data. Such structure could reflect, among other things, correlation patterns in large language model activations, or complex natural image manifolds. And yet despite the wide-ranging applicability, there have been relatively few changes to SAEs beyond the original recipe from decades ago, namely, standard deep encoder/decoder layers trained with a classical/deterministic sparse regularizer applied within the latent space. One possible exception is the variational autoencoder (VAE), which adopts a stochastic encoder module capable of producing sparse representations when applied to manifold data. In this work we formalize underappreciated weaknesses with both canonical SAEs, as well as analogous VAEs applied to similar tasks, and propose a hybrid alternative model that circumvents these prior limitations. In terms of theoretical support, we prove that global minima of our proposed model recover certain forms of structured data spread across a union of manifolds. Meanwhile, empirical evaluations on synthetic and real-world datasets substantiate the efficacy of our approach in accurately estimating underlying manifold dimensions and producing sparser latent representations without compromising reconstruction error. In general, we are able to exceed the performance of equivalent-capacity SAEs and VAEs, as well as recent diffusion models where applicable, within domains such as images and language model activation patterns.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the International Conference on Machine Learning (ICML) 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04859v2",
    "published_date": "2025-06-05 10:26:06 UTC",
    "updated_date": "2025-06-06 02:24:02 UTC"
  },
  {
    "arxiv_id": "2506.04851v1",
    "title": "Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights",
    "authors": [
      "Giorgio Biancini",
      "Alessio Ferrato",
      "Carla Limongelli"
    ],
    "abstract": "Integrating Artificial Intelligence (AI) in educational settings has brought new learning approaches, transforming the practices of both students and educators. Among the various technologies driving this transformation, Large Language Models (LLMs) have emerged as powerful tools for creating educational materials and question answering, but there are still space for new applications. Educators commonly use Multiple-Choice Questions (MCQs) to assess student knowledge, but manually generating these questions is resource-intensive and requires significant time and cognitive effort. In our opinion, LLMs offer a promising solution to these challenges. This paper presents a novel comparative analysis of three widely known LLMs - Llama 2, Mistral, and GPT-3.5 - to explore their potential for creating informative and challenging MCQs. In our approach, we do not rely on the knowledge of the LLM, but we inject the knowledge into the prompt to contrast the hallucinations, giving the educators control over the test's source text, too. Our experiment involving 21 educators shows that GPT-3.5 generates the most effective MCQs across several known metrics. Additionally, it shows that there is still some reluctance to adopt AI in the educational field. This study sheds light on the potential of LLMs to generate MCQs and improve the educational experience, providing valuable insights for the future.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Copyright ACM 2024. This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization (UMAP Adjunct '24), http://dx.doi.org/10.1145/3631700.3665233",
    "pdf_url": "https://arxiv.org/pdf/2506.04851v1",
    "published_date": "2025-06-05 10:21:49 UTC",
    "updated_date": "2025-06-05 10:21:49 UTC"
  },
  {
    "arxiv_id": "2506.04849v1",
    "title": "Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles",
    "authors": [
      "Julien Soulé",
      "Jean-Paul Jamont",
      "Michel Occello",
      "Paul Théron",
      "Louis-Marie Traonouez"
    ],
    "abstract": "As cyber-attacks show to be more and more complex and coordinated, cyber-defenders strategy through multi-agent approaches could be key to tackle against cyber-attacks as close as entry points in a networked system. This paper presents a Markovian modeling and implementation through a simulator of fighting cyber-attacker agents and cyber-defender agents deployed on host network nodes. It aims to provide an experimental framework to implement realistically based coordinated cyber-attack scenarios while assessing cyber-defenders dynamic organizations. We abstracted network nodes by sets of properties including agents' ones. Actions applied by agents model how the network reacts depending in a given state and what properties are to change. Collective choice of the actions brings the whole environment closer or farther from respective cyber-attackers and cyber-defenders goals. Using the simulator, we implemented a realistically inspired scenario with several behavior implementation approaches for cyber-defenders and cyber-attackers.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04849v1",
    "published_date": "2025-06-05 10:17:17 UTC",
    "updated_date": "2025-06-05 10:17:17 UTC"
  },
  {
    "arxiv_id": "2506.05438v1",
    "title": "An Unsupervised Framework for Dynamic Health Indicator Construction and Its Application in Rolling Bearing Prognostics",
    "authors": [
      "Tongda Sun",
      "Chen Yin",
      "Huailiang Zheng",
      "Yining Dong"
    ],
    "abstract": "Health indicator (HI) plays a key role in degradation assessment and prognostics of rolling bearings. Although various HI construction methods have been investigated, most of them rely on expert knowledge for feature extraction and overlook capturing dynamic information hidden in sequential degradation processes, which limits the ability of the constructed HI for degradation trend representation and prognostics. To address these concerns, a novel dynamic HI that considers HI-level temporal dependence is constructed through an unsupervised framework. Specifically, a degradation feature learning module composed of a skip-connection-based autoencoder first maps raw signals to a representative degradation feature space (DFS) to automatically extract essential degradation features without the need for expert knowledge. Subsequently, in this DFS, a new HI-generating module embedded with an inner HI-prediction block is proposed for dynamic HI construction, where the temporal dependence between past and current HI states is guaranteed and modeled explicitly. On this basis, the dynamic HI captures the inherent dynamic contents of the degradation process, ensuring its effectiveness for degradation tendency modeling and future degradation prognostics. The experiment results on two bearing lifecycle datasets demonstrate that the proposed HI construction method outperforms comparison methods, and the constructed dynamic HI is superior for prognostic tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05438v1",
    "published_date": "2025-06-05 10:13:46 UTC",
    "updated_date": "2025-06-05 10:13:46 UTC"
  },
  {
    "arxiv_id": "2506.05437v1",
    "title": "A MARL-based Approach for Easing MAS Organization Engineering",
    "authors": [
      "Julien Soulé",
      "Jean-Paul Jamont",
      "Michel Occello",
      "Louis-Marie Traonouez",
      "Paul Théron"
    ],
    "abstract": "Multi-Agent Systems (MAS) have been successfully applied in industry for their ability to address complex, distributed problems, especially in IoT-based systems. Their efficiency in achieving given objectives and meeting design requirements is strongly dependent on the MAS organization during the engineering process of an application-specific MAS. To design a MAS that can achieve given goals, available methods rely on the designer's knowledge of the deployment environment. However, high complexity and low readability in some deployment environments make the application of these methods to be costly or raise safety concerns. In order to ease the MAS organization design regarding those concerns, we introduce an original Assisted MAS Organization Engineering Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement Learning (MARL) process with an organizational model to suggest relevant organizational specifications to help in MAS engineering.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05437v1",
    "published_date": "2025-06-05 09:59:36 UTC",
    "updated_date": "2025-06-05 09:59:36 UTC"
  },
  {
    "arxiv_id": "2506.04838v1",
    "title": "On Automating Security Policies with Contemporary LLMs",
    "authors": [
      "Pablo Fernández Saura",
      "K. R. Jayaram",
      "Vatche Isahagian",
      "Jorge Bernal Bernabé",
      "Antonio Skarmeta"
    ],
    "abstract": "The complexity of modern computing environments and the growing sophistication of cyber threats necessitate a more robust, adaptive, and automated approach to security enforcement. In this paper, we present a framework leveraging large language models (LLMs) for automating attack mitigation policy compliance through an innovative combination of in-context learning and retrieval-augmented generation (RAG). We begin by describing how our system collects and manages both tool and API specifications, storing them in a vector database to enable efficient retrieval of relevant information. We then detail the architectural pipeline that first decomposes high-level mitigation policies into discrete tasks and subsequently translates each task into a set of actionable API calls. Our empirical evaluation, conducted using publicly available CTI policies in STIXv2 format and Windows API documentation, demonstrates significant improvements in precision, recall, and F1-score when employing RAG compared to a non-RAG baseline.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Short Paper. Accepted To Appear in IEEE SSE 2025 (part of SERVICES 2025)",
    "pdf_url": "https://arxiv.org/pdf/2506.04838v1",
    "published_date": "2025-06-05 09:58:00 UTC",
    "updated_date": "2025-06-05 09:58:00 UTC"
  },
  {
    "arxiv_id": "2506.04836v1",
    "title": "Oversight Structures for Agentic AI in Public-Sector Organizations",
    "authors": [
      "Chris Schmitz",
      "Jonathan Rystrøm",
      "Jan Batzner"
    ],
    "abstract": "This paper finds that the introduction of agentic AI systems intensifies existing challenges to traditional public sector oversight mechanisms -- which rely on siloed compliance units and episodic approvals rather than continuous, integrated supervision. We identify five governance dimensions essential for responsible agent deployment: cross-departmental implementation, comprehensive evaluation, enhanced security protocols, operational visibility, and systematic auditing. We evaluate the capacity of existing oversight structures to meet these challenges, via a mixed-methods approach consisting of a literature review and interviews with civil servants in AI-related roles. We find that agent oversight poses intensified versions of three existing governance challenges: continuous oversight, deeper integration of governance and operational capabilities, and interdepartmental coordination. We propose approaches that both adapt institutional structures and design agent oversight compatible with public sector constraints.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "To appear at REALM@ACL2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04836v1",
    "published_date": "2025-06-05 09:57:15 UTC",
    "updated_date": "2025-06-05 09:57:15 UTC"
  },
  {
    "arxiv_id": "2506.05435v1",
    "title": "Event Classification of Accelerometer Data for Industrial Package Monitoring with Embedded Deep Learning",
    "authors": [
      "Manon Renault",
      "Hamoud Younes",
      "Hugo Tessier",
      "Ronan Le Roy",
      "Bastien Pasdeloup",
      "Mathieu Léonardon"
    ],
    "abstract": "Package monitoring is an important topic in industrial applications, with significant implications for operational efficiency and ecological sustainability. In this study, we propose an approach that employs an embedded system, placed on reusable packages, to detect their state (on a Forklift, in a Truck, or in an undetermined location). We aim to design a system with a lifespan of several years, corresponding to the lifespan of reusable packages. Our analysis demonstrates that maximizing device lifespan requires minimizing wake time. We propose a pipeline that includes data processing, training, and evaluation of the deep learning model designed for imbalanced, multiclass time series data collected from an embedded sensor. The method uses a one-dimensional Convolutional Neural Network architecture to classify accelerometer data from the IoT device. Before training, two data augmentation techniques are tested to solve the imbalance problem of the dataset: the Synthetic Minority Oversampling TEchnique and the ADAptive SYNthetic sampling approach. After training, compression techniques are implemented to have a small model size. On the considered twoclass problem, the methodology yields a precision of 94.54% for the first class and 95.83% for the second class, while compression techniques reduce the model size by a factor of four. The trained model is deployed on the IoT device, where it operates with a power consumption of 316 mW during inference.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05435v1",
    "published_date": "2025-06-05 09:55:58 UTC",
    "updated_date": "2025-06-05 09:55:58 UTC"
  },
  {
    "arxiv_id": "2506.04828v1",
    "title": "Safe Planning and Policy Optimization via World Model Learning",
    "authors": [
      "Artem Latyshev",
      "Gregory Gorbov",
      "Aleksandr I. Panov"
    ],
    "abstract": "Reinforcement Learning (RL) applications in real-world scenarios must prioritize safety and reliability, which impose strict constraints on agent behavior. Model-based RL leverages predictive world models for action planning and policy optimization, but inherent model inaccuracies can lead to catastrophic failures in safety-critical settings. We propose a novel model-based RL framework that jointly optimizes task performance and safety. To address world model errors, our method incorporates an adaptive mechanism that dynamically switches between model-based planning and direct policy execution. We resolve the objective mismatch problem of traditional model-based approaches using an implicit world model. Furthermore, our framework employs dynamic safety thresholds that adapt to the agent's evolving capabilities, consistently selecting actions that surpass safe policy suggestions in both performance and safety. Experiments demonstrate significant improvements over non-adaptive methods, showing that our approach optimizes safety and performance simultaneously rather than merely meeting minimum safety requirements. The proposed framework achieves robust performance on diverse safety-critical continuous control tasks, outperforming existing methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04828v1",
    "published_date": "2025-06-05 09:50:02 UTC",
    "updated_date": "2025-06-05 09:50:02 UTC"
  },
  {
    "arxiv_id": "2506.04824v1",
    "title": "A Reasoning-Based Approach to Cryptic Crossword Clue Solving",
    "authors": [
      "Martin Andrews",
      "Sam Witteveen"
    ],
    "abstract": "Cryptic crossword clues are challenging language tasks for which new test sets are released daily by major newspapers on a global basis. Each cryptic clue contains both the definition of the answer to be placed in the crossword grid (in common with regular crosswords), and 'wordplay' that proves that the answer is correct (i.e. a human solver can be confident that an answer is correct without needing crossing words as confirmation). This work describes an LLM-based reasoning system built from open-licensed components that solves cryptic clues by (i) hypothesising answers; (ii) proposing wordplay explanations; and (iii) using a verifier system that operates on codified reasoning steps. Overall, this system establishes a new state-of-the-art performance on the challenging Cryptonite dataset of clues from The Times and The Telegraph newspapers in the UK. Because each proved solution is expressed in Python, interpretable wordplay reasoning for proven answers is available for inspection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 page paper plus Appendices. Accepted to ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04824v1",
    "published_date": "2025-06-05 09:43:28 UTC",
    "updated_date": "2025-06-05 09:43:28 UTC"
  },
  {
    "arxiv_id": "2506.05434v2",
    "title": "Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks",
    "authors": [
      "Thomas Massena",
      "Léo andéol",
      "Thibaut Boissin",
      "Franck Mamalet",
      "Corentin Friedrich",
      "Mathieu Serrurier",
      "Sébastien Gerchinovitz"
    ],
    "abstract": "Conformal Prediction (CP) has proven to be an effective post-hoc method for improving the trustworthiness of neural networks by providing prediction sets with finite-sample guarantees. However, under adversarial attacks, classical conformal guarantees do not hold anymore: this problem is addressed in the field of Robust Conformal Prediction. Several methods have been proposed to provide robust CP sets with guarantees under adversarial perturbations, but, for large scale problems, these sets are either too large or the methods are too computationally demanding to be deployed in real life scenarios. In this work, we propose a new method that leverages Lipschitz-bounded networks to precisely and efficiently estimate robust CP sets. When combined with a 1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms state-of-the-art results in both the size of the robust CP sets and computational efficiency in medium and large-scale scenarios such as ImageNet. Taking a different angle, we also study vanilla CP under attack, and derive new worst-case coverage bounds of vanilla CP sets, which are valid simultaneously for all adversarial attack levels. Our lip-rcp method makes this second approach as efficient as vanilla CP while also allowing robustness guarantees.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05434v2",
    "published_date": "2025-06-05 09:38:14 UTC",
    "updated_date": "2025-06-10 07:49:58 UTC"
  },
  {
    "arxiv_id": "2506.04810v2",
    "title": "Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study",
    "authors": [
      "Yujun Zhou",
      "Jiayi Ye",
      "Zipeng Ling",
      "Yufei Han",
      "Yue Huang",
      "Haomin Zhuang",
      "Zhenwen Liang",
      "Kehan Guo",
      "Taicheng Guo",
      "Xiangqi Wang",
      "Xiangliang Zhang"
    ],
    "abstract": "Logical reasoning is a core capability for large language models (LLMs), yet existing benchmarks that rely solely on final-answer accuracy fail to capture the quality of the reasoning process. To address this, we introduce FineLogic, a fine-grained evaluation framework that assesses logical reasoning across three dimensions: overall accuracy, stepwise soundness, and representation-level probing. Leveraging this framework, we conduct a comprehensive study on how different supervision formats in fine-tuning shape reasoning abilities. We fine-tune LLMs on four supervision styles: one in natural language and three symbolic variants. We find a key trade-off: natural language supervision excels at generalization to out-of-distribution and long-chain problems, whereas symbolic supervision is superior at instilling structurally sound, atomic reasoning steps. Furthermore, our probing analysis indicates that fine-tuning primarily refines the model's step-by-step generation process, rather than improving its ability to converge on an answer early. Together, our framework and analysis provide a more rigorous lens for evaluating and improving logical reasoning in LLMs. The code is available at https://github.com/YujunZhou/FineLogic.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by the Findings of EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04810v2",
    "published_date": "2025-06-05 09:34:12 UTC",
    "updated_date": "2025-10-09 12:32:49 UTC"
  },
  {
    "arxiv_id": "2506.04788v1",
    "title": "Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques",
    "authors": [
      "Jisu An",
      "Junseok Lee",
      "Jeoungeun Lee",
      "Yongseok Son"
    ],
    "abstract": "The rapid progress of Multimodal Large Language Models(MLLMs) has transformed the AI landscape. These models combine pre-trained LLMs with various modality encoders. This integration requires a systematic understanding of how different modalities connect to the language backbone. Our survey presents an LLM-centric analysis of current approaches. We examine methods for transforming and aligning diverse modal inputs into the language embedding space. This addresses a significant gap in existing literature. We propose a classification framework for MLLMs based on three key dimensions. First, we examine architectural strategies for modality integration. This includes both the specific integration mechanisms and the fusion level. Second, we categorize representation learning techniques as either joint or coordinate representations. Third, we analyze training paradigms, including training strategies and objective functions. By examining 125 MLLMs developed between 2021 and 2025, we identify emerging patterns in the field. Our taxonomy provides researchers with a structured overview of current integration techniques. These insights aim to guide the development of more robust multimodal integration strategies for future models built on pre-trained foundations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 3 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.04788v1",
    "published_date": "2025-06-05 09:14:41 UTC",
    "updated_date": "2025-06-05 09:14:41 UTC"
  },
  {
    "arxiv_id": "2506.05433v1",
    "title": "Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward",
    "authors": [
      "Zikang Liu",
      "Tongtian Yue",
      "Yepeng Tang",
      "Longteng Guo",
      "Junxian Cai",
      "Qingbin Liu",
      "Xi Chen",
      "Jing Liu"
    ],
    "abstract": "Group Relative Policy Optimization (GRPO) enhances policy learning by computing gradients from relative comparisons among candidate outputs that share a common input prefix. Despite its effectiveness, GRPO introduces substantial computational overhead when processing long shared prefixes, which must be redundantly encoded for each group member. This inefficiency becomes a major scalability bottleneck in long-context learning scenarios. We propose Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant prefix computation via a Shared-Prefix Forward strategy. In particular, by restructuring self-attention into two parts, our method enables the shared prefix to be encoded only once, while preserving full differentiability and compatibility with end-to-end training. We provide both theoretical and empirical evidence that Prefix Grouper is training-equivalent to standard GRPO: it yields identical forward outputs and backward gradients, ensuring that the optimization dynamics and final policy performance remain unchanged. Empirically, our experiments confirm that Prefix Grouper achieves consistent results while significantly reducing the computational cost of training, particularly in long-prefix scenarios. The proposed method is fully plug-and-play: it is compatible with existing GRPO-based architectures and can be seamlessly integrated into current training pipelines as a drop-in replacement, requiring no structural modifications and only minimal changes to input construction and attention computation. Prefix Grouper enables the use of larger group sizes under the same computational budget, thereby improving the scalability of GRPO to more complex tasks and larger models. Code is now available at https://github.com/johncaged/PrefixGrouper",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, technical report",
    "pdf_url": "https://arxiv.org/pdf/2506.05433v1",
    "published_date": "2025-06-05 09:13:37 UTC",
    "updated_date": "2025-06-05 09:13:37 UTC"
  },
  {
    "arxiv_id": "2506.11088v2",
    "title": "One SPACE to Rule Them All: Jointly Mitigating Factuality and Faithfulness Hallucinations in LLMs",
    "authors": [
      "Pengbo Wang",
      "Chaozhuo Li",
      "Chenxu Wang",
      "Liwen Zheng",
      "Litian Zhang",
      "Xi Zhang"
    ],
    "abstract": "LLMs have demonstrated unprecedented capabilities in natural language processing, yet their practical deployment remains hindered by persistent factuality and faithfulness hallucinations. While existing methods address these hallucination types independently, they inadvertently induce performance trade-offs, as interventions targeting one type often exacerbate the other. Through empirical and theoretical analysis of activation space dynamics in LLMs, we reveal that these hallucination categories share overlapping subspaces within neural representations, presenting an opportunity for concurrent mitigation. To harness this insight, we propose SPACE, a unified framework that jointly enhances factuality and faithfulness by editing shared activation subspaces. SPACE establishes a geometric foundation for shared subspace existence through dual-task feature modeling, then identifies and edits these subspaces via a hybrid probe strategy combining spectral clustering and attention head saliency scoring. Experimental results across multiple benchmark datasets demonstrate the superiority of our approach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as NIPS 2025 poster",
    "pdf_url": "https://arxiv.org/pdf/2506.11088v2",
    "published_date": "2025-06-05 09:10:57 UTC",
    "updated_date": "2025-11-23 14:25:05 UTC"
  },
  {
    "arxiv_id": "2506.04774v1",
    "title": "Fine-Grained Interpretation of Political Opinions in Large Language Models",
    "authors": [
      "Jingyu Hu",
      "Mengyue Yang",
      "Mengnan Du",
      "Weiru Liu"
    ],
    "abstract": "Studies of LLMs' political opinions mainly rely on evaluations of their open-ended responses. Recent work indicates that there is a misalignment between LLMs' responses and their internal intentions. This motivates us to probe LLMs' internal mechanisms and help uncover their internal political states. Additionally, we found that the analysis of LLMs' political opinions often relies on single-axis concepts, which can lead to concept confounds. In this work, we extend the single-axis to multi-dimensions and apply interpretable representation engineering techniques for more transparent LLM political concept learning. Specifically, we designed a four-dimensional political learning framework and constructed a corresponding dataset for fine-grained political concept vector learning. These vectors can be used to detect and intervene in LLM internals. Experiments are conducted on eight open-source LLMs with three representation engineering techniques. Results show these vectors can disentangle political concept confounds. Detection tasks validate the semantic meaning of the vectors and show good generalization and robustness in OOD settings. Intervention Experiments show these vectors can intervene in LLMs to generate responses with different political leanings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04774v1",
    "published_date": "2025-06-05 09:06:59 UTC",
    "updated_date": "2025-06-05 09:06:59 UTC"
  },
  {
    "arxiv_id": "2506.05432v2",
    "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling",
    "authors": [
      "Yuxuan Yue",
      "Zukang Xu",
      "Zhihang Yuan",
      "Dawei Yang",
      "Jianlong Wu",
      "Liqiang Nie"
    ],
    "abstract": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05432v2",
    "published_date": "2025-06-05 08:58:58 UTC",
    "updated_date": "2025-06-26 06:17:49 UTC"
  },
  {
    "arxiv_id": "2506.04762v1",
    "title": "GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval",
    "authors": [
      "Lingyuan Liu",
      "Mengxiang Zhang"
    ],
    "abstract": "Large language models (LLMs)-based query expansion for information retrieval augments queries with generated hypothetical documents with LLMs. However, its performance relies heavily on the scale of the language models (LMs), necessitating larger, more advanced LLMs. This approach is costly, computationally intensive, and often has limited accessibility. To address these limitations, we introduce GOLFer - Smaller LMs-Generated Documents Hallucination Filter & Combiner - a novel method leveraging smaller open-source LMs for query expansion. GOLFer comprises two modules: a hallucination filter and a documents combiner. The former detects and removes non-factual and inconsistent sentences in generated documents, a common issue with smaller LMs, while the latter combines the filtered content with the query using a weight vector to balance their influence. We evaluate GOLFer alongside dominant LLM-based query expansion methods on three web search and ten low-resource datasets. Experimental results demonstrate that GOLFer consistently outperforms other methods using smaller LMs, and maintains competitive performance against methods using large-size LLMs, demonstrating its effectiveness.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04762v1",
    "published_date": "2025-06-05 08:45:48 UTC",
    "updated_date": "2025-06-05 08:45:48 UTC"
  },
  {
    "arxiv_id": "2506.04760v1",
    "title": "Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion",
    "authors": [
      "Lingyuan Liu",
      "Mengxiang Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have shown potential in generating hypothetical documents for query expansion, thereby enhancing information retrieval performance. However, the efficacy of this method is highly dependent on the quality of the generated documents, which often requires complex prompt strategies and the integration of advanced dense retrieval techniques. This can be both costly and computationally intensive. To mitigate these limitations, we explore the use of zero-shot LLM-based query expansion to improve sparse retrieval, particularly for learned sparse retrievers. We introduce a novel fusion ranking framework, Exp4Fuse, which enhances the performance of sparse retrievers through an indirect application of zero-shot LLM-based query expansion. Exp4Fuse operates by simultaneously considering two retrieval routes-one based on the original query and the other on the LLM-augmented query. It then generates two ranked lists using a sparse retriever and fuses them using a modified reciprocal rank fusion method. We conduct extensive evaluations of Exp4Fuse against leading LLM-based query expansion methods and advanced retrieval techniques on three MS MARCO-related datasets and seven low-resource datasets. Experimental results reveal that Exp4Fuse not only surpasses existing LLM-based query expansion methods in enhancing sparse retrievers but also, when combined with advanced sparse retrievers, achieves SOTA results on several benchmarks. This highlights the superior performance and effectiveness of Exp4Fuse in improving query expansion for sparse retrieval.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04760v1",
    "published_date": "2025-06-05 08:44:34 UTC",
    "updated_date": "2025-06-05 08:44:34 UTC"
  },
  {
    "arxiv_id": "2506.04756v1",
    "title": "Ontology-based knowledge representation for bone disease diagnosis: a foundation for safe and sustainable medical artificial intelligence systems",
    "authors": [
      "Loan Dao",
      "Ngoc Quoc Ly"
    ],
    "abstract": "Medical artificial intelligence (AI) systems frequently lack systematic domain expertise integration, potentially compromising diagnostic reliability. This study presents an ontology-based framework for bone disease diagnosis, developed in collaboration with Ho Chi Minh City Hospital for Traumatology and Orthopedics. The framework introduces three theoretical contributions: (1) a hierarchical neural network architecture guided by bone disease ontology for segmentation-classification tasks, incorporating Visual Language Models (VLMs) through prompts, (2) an ontology-enhanced Visual Question Answering (VQA) system for clinical reasoning, and (3) a multimodal deep learning model that integrates imaging, clinical, and laboratory data through ontological relationships. The methodology maintains clinical interpretability through systematic knowledge digitization, standardized medical terminology mapping, and modular architecture design. The framework demonstrates potential for extension beyond bone diseases through its standardized structure and reusable components. While theoretical foundations are established, experimental validation remains pending due to current dataset and computational resource limitations. Future work will focus on expanding the clinical dataset and conducting comprehensive system validation.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04756v1",
    "published_date": "2025-06-05 08:41:23 UTC",
    "updated_date": "2025-06-05 08:41:23 UTC"
  },
  {
    "arxiv_id": "2506.04755v1",
    "title": "Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning",
    "authors": [
      "Shenshen Li",
      "Kaiyuan Deng",
      "Lei Wang",
      "Hao Yang",
      "Chong Peng",
      "Peng Yan",
      "Fumin Shen",
      "Heng Tao Shen",
      "Xing Xu"
    ],
    "abstract": "While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and substantial computational costs. However, can smaller high-value datasets match or outperform full corpora for multi-modal reasoning in MLLMs? In this work, we challenge this assumption through a key observation: meaningful multi-modal reasoning is triggered by only a sparse subset of training samples, termed cognitive samples, whereas the majority contribute marginally. Building on this insight, we propose a novel data selection paradigm termed Reasoning Activation Potential (RAP), which identifies cognitive samples by estimating each sample's potential to stimulate genuine multi-modal reasoning by two complementary estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential outcome model principle, eliminates samples that overly rely on language priors by comparing outputs between multi-modal and text-only inputs; 2) Attention Confidence Estimator (ACE), which exploits token-level self-attention to discard samples dominated by irrelevant but over-emphasized tokens in intermediate reasoning stages. Moreover, we introduce a Difficulty-aware Replacement Module (DRM) to substitute trivial instances with cognitively challenging ones, thereby ensuring complexity for robust multi-modal reasoning. Experiments on six datasets show that our RAP method consistently achieves superior performance using only 9.3% of the training data, while reducing computational costs by over 43%. Our code is available at https://github.com/Leo-ssl/RAP.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04755v1",
    "published_date": "2025-06-05 08:40:24 UTC",
    "updated_date": "2025-06-05 08:40:24 UTC"
  },
  {
    "arxiv_id": "2506.04753v1",
    "title": "Physics Informed Capsule Enhanced Variational AutoEncoder for Underwater Image Enhancement",
    "authors": [
      "Niki Martinel",
      "Rita Pucci"
    ],
    "abstract": "We present a novel dual-stream architecture that achieves state-of-the-art underwater image enhancement by explicitly integrating the Jaffe-McGlamery physical model with capsule clustering-based feature representation learning. Our method simultaneously estimates transmission maps and spatially-varying background light through a dedicated physics estimator while extracting entity-level features via capsule clustering in a parallel stream. This physics-guided approach enables parameter-free enhancement that respects underwater formation constraints while preserving semantic structures and fine-grained details. Our approach also features a novel optimization objective ensuring both physical adherence and perceptual quality across multiple spatial frequencies. To validate our approach, we conducted extensive experiments across six challenging benchmarks. Results demonstrate consistent improvements of $+0.5$dB PSNR over the best existing methods while requiring only one-third of their computational complexity (FLOPs), or alternatively, more than $+1$dB PSNR improvement when compared to methods with similar computational budgets. Code and data \\textit{will} be available at https://github.com/iN1k1/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04753v1",
    "published_date": "2025-06-05 08:39:17 UTC",
    "updated_date": "2025-06-05 08:39:17 UTC"
  },
  {
    "arxiv_id": "2506.05431v1",
    "title": "Robustness Evaluation for Video Models with Reinforcement Learning",
    "authors": [
      "Ashwin Ramesh Babu",
      "Sajad Mousavi",
      "Vineet Gundecha",
      "Sahand Ghorbanpour",
      "Avisek Naug",
      "Antonio Guillen",
      "Ricardo Luna Gutierrez",
      "Soumyendu Sarkar"
    ],
    "abstract": "Evaluating the robustness of Video classification models is very challenging, specifically when compared to image-based models. With their increased temporal dimension, there is a significant increase in complexity and computational cost. One of the key challenges is to keep the perturbations to a minimum to induce misclassification. In this work, we propose a multi-agent reinforcement learning approach (spatial and temporal) that cooperatively learns to identify the given video's sensitive spatial and temporal regions. The agents consider temporal coherence in generating fine perturbations, leading to a more effective and visually imperceptible attack. Our method outperforms the state-of-the-art solutions on the Lp metric and the average queries. Our method enables custom distortion types, making the robustness evaluation more relevant to the use case. We extensively evaluate 4 popular models for video action recognition on two popular datasets, HMDB-51 and UCF-101.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05431v1",
    "published_date": "2025-06-05 08:38:09 UTC",
    "updated_date": "2025-06-05 08:38:09 UTC"
  },
  {
    "arxiv_id": "2506.05430v1",
    "title": "Explainer-guided Targeted Adversarial Attacks against Binary Code Similarity Detection Models",
    "authors": [
      "Mingjie Chen",
      "Tiancheng Zhu",
      "Mingxue Zhang",
      "Yiling He",
      "Minghao Lin",
      "Penghui Li",
      "Kui Ren"
    ],
    "abstract": "Binary code similarity detection (BCSD) serves as a fundamental technique for various software engineering tasks, e.g., vulnerability detection and classification. Attacks against such models have therefore drawn extensive attention, aiming at misleading the models to generate erroneous predictions. Prior works have explored various approaches to generating semantic-preserving variants, i.e., adversarial samples, to evaluate the robustness of the models against adversarial attacks. However, they have mainly relied on heuristic criteria or iterative greedy algorithms to locate salient code influencing the model output, failing to operate on a solid theoretical basis. Moreover, when processing programs with high complexities, such attacks tend to be time-consuming.\n  In this work, we propose a novel optimization for adversarial attacks against BCSD models. In particular, we aim to improve the attacks in a challenging scenario, where the attack goal is to limit the model predictions to a specific range, i.e., the targeted attacks. Our attack leverages the superior capability of black-box, model-agnostic explainers in interpreting the model decision boundaries, thereby pinpointing the critical code snippet to apply semantic-preserving perturbations. The evaluation results demonstrate that compared with the state-of-the-art attacks, the proposed attacks achieve higher attack success rate in almost all scenarios, while also improving the efficiency and transferability. Our real-world case studies on vulnerability detection and classification further demonstrate the security implications of our attacks, highlighting the urgent need to further enhance the robustness of existing BCSD models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "12 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.05430v1",
    "published_date": "2025-06-05 08:29:19 UTC",
    "updated_date": "2025-06-05 08:29:19 UTC"
  },
  {
    "arxiv_id": "2506.04742v2",
    "title": "Were Residual Penalty and Neural Operators All We Needed for Solving Optimal Control Problems?",
    "authors": [
      "Oliver G. S. Lundqvist",
      "Fabricio Oliveira"
    ],
    "abstract": "Neural networks have been used to solve optimal control problems, typically by training neural networks using a combined loss function that considers data, differential equation residuals, and objective costs. We show that including cost functions in the training process is unnecessary, advocating for a simpler architecture and streamlined approach by decoupling the optimal control problem from the training process. Thus, our work shows that a simple neural operator architecture, such as DeepONet, coupled with an unconstrained optimization routine, can solve multiple optimal control problems with a single physics-informed training phase and a subsequent optimization phase. We achieve this by adding a penalty term based on the differential equation residual to the cost function and computing gradients with respect to the control using automatic differentiation through the trained neural operator within an iterative optimization routine. Our results show acceptable accuracy for practical applications and potential computational savings for more complex and higher-dimensional problems.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04742v2",
    "published_date": "2025-06-05 08:22:16 UTC",
    "updated_date": "2025-09-22 13:30:04 UTC"
  },
  {
    "arxiv_id": "2506.06387v2",
    "title": "Model-based Implicit Neural Representation for sub-wavelength Radio Localization",
    "authors": [
      "Baptiste Chatelier",
      "Vincent Corlay",
      "Musa Furkan Keskin",
      "Matthieu Crussière",
      "Henk Wymeersch",
      "Luc Le Magoarou"
    ],
    "abstract": "The increasing deployment of large antenna arrays at base stations has significantly improved the spatial resolution and localization accuracy of radio-localization methods. However, traditional signal processing techniques struggle in complex radio environments, particularly in scenarios dominated by non line of sight (NLoS) propagation paths, resulting in degraded localization accuracy. Recent developments in machine learning have facilitated the development of machine learning-assisted localization techniques, enhancing localization accuracy in complex radio environments. However, these methods often involve substantial computational complexity during both the training and inference phases. This work extends the well-established fingerprinting-based localization framework by simultaneously reducing its memory requirements and improving its accuracy. Specifically, a model-based neural network is used to learn the location-to-channel mapping, and then serves as a generative neural channel model. This generative model augments the fingerprinting comparison dictionary while reducing the memory requirements. The proposed method outperforms fingerprinting baselines by achieving sub-wavelength localization accuracy, even in complex static NLoS environments. Remarkably, it offers an improvement by several orders of magnitude in localization accuracy, while simultaneously reducing memory requirements by an order of magnitude compared to classical fingerprinting methods.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.IT",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06387v2",
    "published_date": "2025-06-05 08:20:51 UTC",
    "updated_date": "2025-10-21 08:32:16 UTC"
  },
  {
    "arxiv_id": "2506.04739v1",
    "title": "Lifelong Evolution: Collaborative Learning between Large and Small Language Models for Continuous Emergent Fake News Detection",
    "authors": [
      "Ziyi Zhou",
      "Xiaoming Zhang",
      "Litian Zhang",
      "Yibo Zhang",
      "Zhenyu Guan",
      "Chaozhuo Li",
      "Philip S. Yu"
    ],
    "abstract": "The widespread dissemination of fake news on social media has significantly impacted society, resulting in serious consequences. Conventional deep learning methodologies employing small language models (SLMs) suffer from extensive supervised training requirements and difficulties adapting to evolving news environments due to data scarcity and distribution shifts. Large language models (LLMs), despite robust zero-shot capabilities, fall short in accurately detecting fake news owing to outdated knowledge and the absence of suitable demonstrations. In this paper, we propose a novel Continuous Collaborative Emergent Fake News Detection (C$^2$EFND) framework to address these challenges. The C$^2$EFND framework strategically leverages both LLMs' generalization power and SLMs' classification expertise via a multi-round collaborative learning framework. We further introduce a lifelong knowledge editing module based on a Mixture-of-Experts architecture to incrementally update LLMs and a replay-based continue learning method to ensure SLMs retain prior knowledge without retraining entirely. Extensive experiments on Pheme and Twitter16 datasets demonstrate that C$^2$EFND significantly outperforms existed methods, effectively improving detection accuracy and adaptability in continuous emergent fake news scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04739v1",
    "published_date": "2025-06-05 08:17:55 UTC",
    "updated_date": "2025-06-05 08:17:55 UTC"
  },
  {
    "arxiv_id": "2506.11087v2",
    "title": "Enhancing Delta Compression in LLMs via SVD-based Quantization Error Minimization",
    "authors": [
      "Boya Xiong",
      "Shuo Wang",
      "Weifeng Ge",
      "Guanhua Chen",
      "Yun Chen"
    ],
    "abstract": "Fine-tuning is a crucial process for adapting large language models (LLMs) to diverse applications. In certain scenarios, like multi-tenant serving, a large number of LLMs finetuned from the same base model are deployed to meet complex requirements for users. Recent works explore delta-compression approaches to quantize and compress the delta weights between the customized LLM and the corresponding base model. However, they exhibit inadequate performance at high compression ratios due to their empirical nature. In this work, we introduce DeltaMix, an adaptive mixed-precision delta-compression framework designed to minimize quantization error in the singular value decomposition (SVD) space without imposing additional assumptions. DeltaMix provides a theoretical justification for the necessity of mixed-precision compression and presents a practical quantization solution that involves solving a 0/1 linear integer programming problem alongside a reconstruction target correction method. Experimental results across multiple models and benchmarks illustrate that DeltaMix consistently outperforms all baseline methods. Notably, on tasks such as AIME2024 and GQA, DeltaMix exceeds the performance of the best baseline, Delta-CoMe, by 22.3\\% and 6.1\\% for 7B parameter models, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11087v2",
    "published_date": "2025-06-05 08:17:12 UTC",
    "updated_date": "2025-09-27 06:06:19 UTC"
  },
  {
    "arxiv_id": "2506.04734v2",
    "title": "Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design",
    "authors": [
      "Lin Sun",
      "Weihong Lin",
      "Jinzhu Wu",
      "Yongfu Zhu",
      "Xiaoqi Jian",
      "Guangxiang Zhao",
      "Change Jia",
      "Linglin Zhang",
      "Sai-er Hu",
      "Yuhan Wu",
      "Xiangzheng Zhang"
    ],
    "abstract": "Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04734v2",
    "published_date": "2025-06-05 08:09:11 UTC",
    "updated_date": "2025-06-10 06:57:23 UTC"
  },
  {
    "arxiv_id": "2506.05429v1",
    "title": "Coordinated Robustness Evaluation Framework for Vision-Language Models",
    "authors": [
      "Ashwin Ramesh Babu",
      "Sajad Mousavi",
      "Vineet Gundecha",
      "Sahand Ghorbanpour",
      "Avisek Naug",
      "Antonio Guillen",
      "Ricardo Luna Gutierrez",
      "Soumyendu Sarkar"
    ],
    "abstract": "Vision-language models, which integrate computer vision and natural language processing capabilities, have demonstrated significant advancements in tasks such as image captioning and visual question and answering. However, similar to traditional models, they are susceptible to small perturbations, posing a challenge to their robustness, particularly in deployment scenarios. Evaluating the robustness of these models requires perturbations in both the vision and language modalities to learn their inter-modal dependencies. In this work, we train a generic surrogate model that can take both image and text as input and generate joint representation which is further used to generate adversarial perturbations for both the text and image modalities. This coordinated attack strategy is evaluated on the visual question and answering and visual reasoning datasets using various state-of-the-art vision-language models. Our results indicate that the proposed strategy outperforms other multi-modal attacks and single-modality attacks from the recent literature. Our results demonstrate their effectiveness in compromising the robustness of several state-of-the-art pre-trained multi-modal models such as instruct-BLIP, ViLT and others.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted: IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.05429v1",
    "published_date": "2025-06-05 08:09:05 UTC",
    "updated_date": "2025-06-05 08:09:05 UTC"
  },
  {
    "arxiv_id": "2506.04723v2",
    "title": "Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning",
    "authors": [
      "Jiayu Wang",
      "Yifei Ming",
      "Zixuan Ke",
      "Caiming Xiong",
      "Shafiq Joty",
      "Aws Albarghouthi",
      "Frederic Sala"
    ],
    "abstract": "Reinforcement learning (RL) has become the dominant paradigm for improving the performance of language models on complex reasoning tasks. Despite the substantial empirical gains demonstrated by RL-based training methods like GRPO, a granular understanding of why and how RL enhances performance is still lacking. To bridge this gap, we introduce SPARKLE, a fine-grained analytic framework to dissect the effects of RL across three key dimensions: (1) plan following and execution, (2) knowledge integration, and (3) chain of subproblems. Using this framework, we gain insights beyond mere accuracy. For instance, providing models with explicit human-crafted, step-by-step plans can surprisingly degrade performance on the most challenging benchmarks, yet RL-tuned models exhibit greater robustness, experiencing markedly smaller performance drops than base or SFT models. This suggests that RL may not primarily enhance the execution of external plans but rather empower models to formulate and follow internal strategies better suited to their reasoning processes. Conversely, we observe that RL enhances models' ability to integrate provided knowledge into their reasoning process, yielding consistent gains across diverse tasks. Finally, we study whether difficult problems -- those yielding no RL signals and mixed-quality reasoning traces -- can still be effectively used for training. We introduce SparkleRL-PSS, a multi-stage RL pipeline that reuses hard problems with partial step scaffolding, guiding exploration effectively without additional data generation. Together, our findings provide a principled foundation for understanding how RL shapes model behavior, offering practical insights for building more adaptive, data-efficient, and interpretable RL pipelines for reasoning tasks. Our code, data, and checkpoints are available at: https://sparkle-reasoning.github.io/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04723v2",
    "published_date": "2025-06-05 07:53:59 UTC",
    "updated_date": "2025-10-24 04:49:58 UTC"
  },
  {
    "arxiv_id": "2506.04717v1",
    "title": "Using In-Context Learning for Automatic Defect Labelling of Display Manufacturing Data",
    "authors": [
      "Babar Hussain",
      "Qiang Liu",
      "Gang Chen",
      "Bihai She",
      "Dahai Yu"
    ],
    "abstract": "This paper presents an AI-assisted auto-labeling system for display panel defect detection that leverages in-context learning capabilities. We adopt and enhance the SegGPT architecture with several domain-specific training techniques and introduce a scribble-based annotation mechanism to streamline the labeling process. Our two-stage training approach, validated on industrial display panel datasets, demonstrates significant improvements over the baseline model, achieving an average IoU increase of 0.22 and a 14% improvement in recall across multiple product types, while maintaining approximately 60% auto-labeling coverage. Experimental results show that models trained on our auto-labeled data match the performance of those trained on human-labeled data, offering a practical solution for reducing manual annotation efforts in industrial inspection systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04717v1",
    "published_date": "2025-06-05 07:42:31 UTC",
    "updated_date": "2025-06-05 07:42:31 UTC"
  },
  {
    "arxiv_id": "2506.04712v2",
    "title": "UNO: Unlearning via Orthogonalization in Generative models",
    "authors": [
      "Pinak Mandal",
      "Georg A. Gottwald"
    ],
    "abstract": "As generative models become increasingly powerful and pervasive, the ability to unlearn specific data, whether due to privacy concerns, legal requirements, or the correction of harmful content, has become increasingly important. Unlike in conventional training, where data are accumulated and knowledge is reinforced, unlearning aims to selectively remove the influence of particular data points without costly retraining from scratch. To be effective and reliable, such algorithms need to achieve (i) forgetting of the undesired data, (ii) preservation of the quality of the generation, (iii) preservation of the influence of the desired training data on the model parameters, and (iv) small number of training steps. We propose fast unlearning algorithms based on loss gradient orthogonalization for unconditional and conditional generative models. We show that our algorithms are able to forget data while maintaining the fidelity of the original model. On standard image benchmarks, our algorithms achieve orders of magnitude faster unlearning times than their predecessors, such as gradient surgery. We demonstrate our algorithms with datasets of increasing complexity (MNIST, CelebA and ImageNet-1K) and for generative models of increasing complexity (VAEs and diffusion transformers).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code update, diffusion transformers",
    "pdf_url": "https://arxiv.org/pdf/2506.04712v2",
    "published_date": "2025-06-05 07:37:02 UTC",
    "updated_date": "2025-09-25 12:29:59 UTC"
  },
  {
    "arxiv_id": "2506.04706v1",
    "title": "Line of Sight: On Linear Representations in VLLMs",
    "authors": [
      "Achyuta Rajaram",
      "Sarah Schwettmann",
      "Jacob Andreas",
      "Arthur Conmy"
    ],
    "abstract": "Language models can be equipped with multimodal capabilities by fine-tuning on embeddings of visual inputs. But how do such multimodal models represent images in their hidden activations? We explore representations of image concepts within LlaVA-Next, a popular open-source VLLM. We find a diverse set of ImageNet classes represented via linearly decodable features in the residual stream. We show that the features are causal by performing targeted edits on the model output. In order to increase the diversity of the studied linear features, we train multimodal Sparse Autoencoders (SAEs), creating a highly interpretable dictionary of text and image features. We find that although model representations across modalities are quite disjoint, they become increasingly shared in deeper layers.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.04706v1",
    "published_date": "2025-06-05 07:30:58 UTC",
    "updated_date": "2025-06-05 07:30:58 UTC"
  },
  {
    "arxiv_id": "2506.04704v5",
    "title": "HoliSafe: Holistic Safety Benchmarking and Modeling for Vision-Language Model",
    "authors": [
      "Youngwan Lee",
      "Kangsan Kim",
      "Kwanyong Park",
      "Ilcahe Jung",
      "Soojin Jang",
      "Seanie Lee",
      "Yong-Ju Lee",
      "Sung Ju Hwang"
    ],
    "abstract": "Despite emerging efforts to enhance the safety of Vision-Language Models (VLMs), current approaches face two main shortcomings. 1) Existing safety-tuning datasets and benchmarks only partially consider how image-text interactions can yield harmful content, often overlooking contextually unsafe outcomes from seemingly benign pairs. This narrow coverage leaves VLMs vulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely primarily on data-centric tuning, with limited architectural innovations to intrinsically strengthen safety. We address these gaps by introducing a holistic safety dataset and benchmark, \\textbf{HoliSafe}, that spans all five safe/unsafe image-text combinations, providing a more robust basis for both training and evaluation (HoliSafe-Bench). We further propose a novel modular framework for enhancing VLM safety with a visual guard module (VGM) designed to assess the harmfulness of input images for VLMs. This module endows VLMs with a dual functionality: they not only learn to generate safer responses but can also provide an interpretable harmfulness classification to justify their refusal decisions. A significant advantage of this approach is its modularity; the VGM is designed as a plug-in component, allowing for seamless integration with diverse pre-trained VLMs across various scales. Experiments show that Safe-VLM with VGM, trained on our HoliSafe, achieves state-of-the-art safety performance across multiple VLM benchmarks. Additionally, the HoliSafe-Bench itself reveals critical vulnerabilities in existing VLM models. We hope that HoliSafe and VGM will spur further research into robust and interpretable VLM safety, expanding future avenues for multimodal alignment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://youngwanlee.github.io/holisafe",
    "pdf_url": "https://arxiv.org/pdf/2506.04704v5",
    "published_date": "2025-06-05 07:26:34 UTC",
    "updated_date": "2025-11-25 14:16:10 UTC"
  },
  {
    "arxiv_id": "2506.04700v2",
    "title": "Explicit Density Approximation for Neural Implicit Samplers Using a Bernstein-Based Convex Divergence",
    "authors": [
      "José Manuel de Frutos",
      "Manuel A. Vázquez",
      "Pablo M. Olmos",
      "Joaquín Míguez"
    ],
    "abstract": "Rank-based statistical metrics, such as the invariant statistical loss (ISL), have recently emerged as robust and practically effective tools for training implicit generative models. In this work, we introduce dual-ISL, a novel likelihood-free objective for training implicit generative models that interchanges the roles of the target and model distributions in the ISL framework, yielding a convex optimization problem in the space of model densities. We prove that the resulting rank-based discrepancy $d_K$ is i) continuous under weak convergence and with respect to the $L^1$ norm, and ii) convex in its first argument-properties not shared by classical divergences such as KL or Wasserstein distances. Building on this, we develop a theoretical framework that interprets $d_K$ as an $L^2$-projection of the density ratio $q = p/\\tilde p$ onto a Bernstein polynomial basis, from which we derive exact bounds on the truncation error, precise convergence rates, and a closed-form expression for the truncated density approximation. We further extend our analysis to the multivariate setting via random one-dimensional projections, defining a sliced dual-ISL divergence that retains both convexity and continuity. We empirically show that these theoretical advantages translate into practical ones. Specifically, across several benchmarks dual-ISL converges more rapidly, delivers markedly smoother and more stable training, and more effectively prevents mode collapse than classical ISL and other leading implicit generative methods-while also providing an explicit density approximation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04700v2",
    "published_date": "2025-06-05 07:21:54 UTC",
    "updated_date": "2025-11-05 20:10:41 UTC"
  },
  {
    "arxiv_id": "2506.04699v1",
    "title": "Empowering Economic Simulation for Massively Multiplayer Online Games through Generative Agent-Based Modeling",
    "authors": [
      "Bihan Xu",
      "Shiwei Zhao",
      "Runze Wu",
      "Zhenya Huang",
      "Jiawei Wang",
      "Zhipeng Hu",
      "Kai Wang",
      "Haoyu Liu",
      "Tangjie Lv",
      "Le Li",
      "Changjie Fan",
      "Xin Tong",
      "Jiangze Han"
    ],
    "abstract": "Within the domain of Massively Multiplayer Online (MMO) economy research, Agent-Based Modeling (ABM) has emerged as a robust tool for analyzing game economics, evolving from rule-based agents to decision-making agents enhanced by reinforcement learning. Nevertheless, existing works encounter significant challenges when attempting to emulate human-like economic activities among agents, particularly regarding agent reliability, sociability, and interpretability. In this study, we take a preliminary step in introducing a novel approach using Large Language Models (LLMs) in MMO economy simulation. Leveraging LLMs' role-playing proficiency, generative capacity, and reasoning aptitude, we design LLM-driven agents with human-like decision-making and adaptability. These agents are equipped with the abilities of role-playing, perception, memory, and reasoning, addressing the aforementioned challenges effectively. Simulation experiments focusing on in-game economic activities demonstrate that LLM-empowered agents can promote emergent phenomena like role specialization and price fluctuations in line with market rules.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "KDD2025 Accepted",
    "pdf_url": "https://arxiv.org/pdf/2506.04699v1",
    "published_date": "2025-06-05 07:21:13 UTC",
    "updated_date": "2025-06-05 07:21:13 UTC"
  },
  {
    "arxiv_id": "2506.04695v2",
    "title": "Reshaping Reasoning in LLMs: A Theoretical Analysis of RL Training Dynamics through Pattern Selection",
    "authors": [
      "Xingwu Chen",
      "Tianle Li",
      "Difan Zou"
    ],
    "abstract": "While reinforcement learning (RL) demonstrated remarkable success in enhancing the reasoning capabilities of language models, the training dynamics of RL in LLMs remain unclear. In this work, we provide an explanation of the RL training process through empirical analysis and rigorous theoretical modeling. First, through systematic reasoning-pattern-level and token-level analysis across the RL training process, we show that while different reasoning patterns exhibit relatively stable success rates during training, RL primarily optimizes a sparse subset of critical tokens, thereby reshaping reasoning pattern distributions to affect model performance. Building on these empirical insights, we develop a theoretical framework to understand the training dynamics of RL with two typical rewards: verifiable reward (RLVR) and model's internal feedback (RLIF). For RLVR, we analyze the training dynamics under two special cases: one where models readily converge to optimal reasoning strategies, and another where optimization becomes challenging, revealing that the base model's reasoning quality is crucial for determining convergence behavior. For RLIF, we examine how internal rewards initially improve model performance but can potentially lead to degradation with continued training. Extensive experiments validate our findings, advancing both theoretical understanding and practical applications of RL in language model enhancement.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 5 figures, 2 table",
    "pdf_url": "https://arxiv.org/pdf/2506.04695v2",
    "published_date": "2025-06-05 07:17:04 UTC",
    "updated_date": "2025-09-27 10:07:28 UTC"
  },
  {
    "arxiv_id": "2506.04694v1",
    "title": "Influence Functions for Edge Edits in Non-Convex Graph Neural Networks",
    "authors": [
      "Jaeseung Heo",
      "Kyeongheung Yun",
      "Seokwon Yoon",
      "MoonJeong Park",
      "Jungseul Ok",
      "Dongwoo Kim"
    ],
    "abstract": "Understanding how individual edges influence the behavior of graph neural networks (GNNs) is essential for improving their interpretability and robustness. Graph influence functions have emerged as promising tools to efficiently estimate the effects of edge deletions without retraining. However, existing influence prediction methods rely on strict convexity assumptions, exclusively consider the influence of edge deletions while disregarding edge insertions, and fail to capture changes in message propagation caused by these modifications. In this work, we propose a proximal Bregman response function specifically tailored for GNNs, relaxing the convexity requirement and enabling accurate influence prediction for standard neural network architectures. Furthermore, our method explicitly accounts for message propagation effects and extends influence prediction to both edge deletions and insertions in a principled way. Experiments with real-world datasets demonstrate accurate influence predictions for different characteristics of GNNs. We further demonstrate that the influence function is versatile in applications such as graph rewiring and adversarial attacks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04694v1",
    "published_date": "2025-06-05 07:15:46 UTC",
    "updated_date": "2025-06-05 07:15:46 UTC"
  },
  {
    "arxiv_id": "2506.04690v2",
    "title": "Towards Better Generalization via Distributional Input Projection Network",
    "authors": [
      "Yifan Hao",
      "Yanxin Lu",
      "Hanning Zhang",
      "Xinwei Shen",
      "Tong Zhang"
    ],
    "abstract": "As overparameterized models become increasingly prevalent, training loss alone offers limited insight into generalization performance. While smoothness has been linked to improved generalization across various settings, directly enforcing smoothness in neural networks remains challenging. To address this, we introduce Distributional Input Projection Networks (DIPNet), a novel framework that projects inputs into learnable distributions at each layer. This distributional representation induces a smoother loss landscape with respect to the input, promoting better generalization. We provide theoretical analysis showing that DIPNet reduces both local smoothness measures and the Lipschitz constant of the network, contributing to improved generalization performance. Empirically, we validate DIPNet across a wide range of architectures and tasks, including Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and MLPs. Our method consistently enhances test performance under standard settings, adversarial attacks, out-of-distribution inputs, and reasoning benchmarks. We demonstrate that the proposed input projection strategy can be seamlessly integrated into existing models, providing a general and effective approach for boosting generalization performance in modern deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04690v2",
    "published_date": "2025-06-05 07:13:59 UTC",
    "updated_date": "2025-09-29 16:20:58 UTC"
  },
  {
    "arxiv_id": "2506.04689v3",
    "title": "Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models",
    "authors": [
      "Thao Nguyen",
      "Yang Li",
      "Olga Golovneva",
      "Luke Zettlemoyer",
      "Sewoong Oh",
      "Ludwig Schmidt",
      "Xian Li"
    ],
    "abstract": "Scaling laws predict that the performance of large language models improves with increasing model size and data size. In practice, pre-training has been relying on massive web crawls, using almost all data sources publicly available on the internet so far. However, this pool of natural data does not grow at the same rate as the compute supply. Furthermore, the availability of high-quality texts is even more limited: data filtering pipelines often remove up to 99% of the initial web scrapes to achieve state-of-the-art. To address the \"data wall\" of pre-training scaling, our work explores ways to transform and recycle data discarded in existing filtering processes. We propose REWIRE, REcycling the Web with guIded REwrite, a method to enrich low-quality documents so that they could become useful for training. This in turn allows us to increase the representation of synthetic data in the final pre-training set. Experiments at 1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points improvement respectively across 22 diverse tasks, compared to training on only filtered web data. Training on the raw-synthetic data mix is also more effective than having access to 2x web data. Through further analysis, we demonstrate that about 82% of the mixed in texts come from transforming lower-quality documents that would otherwise be discarded. REWIRE also outperforms related approaches of generating synthetic data, including Wikipedia-style paraphrasing, question-answer synthesizing and knowledge extraction. These results suggest that recycling web texts holds the potential for being a simple and effective approach for scaling pre-training data. We make our high-quality synthetic data publicly available at https://huggingface.co/datasets/facebook/recycling_the_web.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to COLM 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04689v3",
    "published_date": "2025-06-05 07:12:12 UTC",
    "updated_date": "2025-09-15 00:40:48 UTC"
  },
  {
    "arxiv_id": "2506.04688v1",
    "title": "MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models",
    "authors": [
      "Gio Paik",
      "Geewook Kim",
      "Jinbae Im"
    ],
    "abstract": "This paper introduces MMRefine, a MultiModal Refinement benchmark designed to evaluate the error refinement capabilities of Multimodal Large Language Models (MLLMs). As the emphasis shifts toward enhancing reasoning during inference, MMRefine provides a framework that evaluates MLLMs' abilities to detect and correct errors across six distinct scenarios beyond just comparing final accuracy before and after refinement. Furthermore, the benchmark analyzes the refinement performance by categorizing errors into six error types. Experiments with various open and closed MLLMs reveal bottlenecks and factors impeding refinement performance, highlighting areas for improvement in effective reasoning enhancement. Our code and dataset are publicly available at https://github.com/naver-ai/MMRefine.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL Findings 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04688v1",
    "published_date": "2025-06-05 07:11:36 UTC",
    "updated_date": "2025-06-05 07:11:36 UTC"
  },
  {
    "arxiv_id": "2506.05428v1",
    "title": "Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction",
    "authors": [
      "Zhihao Tang",
      "Chaozhuo Li",
      "Litian Zhang",
      "Xi Zhang"
    ],
    "abstract": "Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by a trade-off between immediacy--making fast predictions from a single baseline sMRI--and accuracy--leveraging longitudinal scans to capture disease progression. We propose MCI-Diff, a diffusion-based framework that synthesizes clinically plausible future sMRI representations directly from baseline data, achieving both real-time risk assessment and high predictive performance. First, a multi-task sequence reconstruction strategy trains a shared denoising network on interpolation and extrapolation tasks to handle irregular follow-up sampling and learn robust latent trajectories. Second, an LLM-driven \"linguistic compass\" is introduced for clinical plausibility sampling: generated feature candidates are quantized, tokenized, and scored by a fine-tuned language model conditioned on expected structural biomarkers, guiding autoregressive generation toward realistic disease patterns. Experiments on ADNI and AIBL cohorts show that MCI-Diff outperforms state-of-the-art baselines, improving early conversion accuracy by 5-12%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05428v1",
    "published_date": "2025-06-05 07:01:05 UTC",
    "updated_date": "2025-06-05 07:01:05 UTC"
  },
  {
    "arxiv_id": "2506.04681v2",
    "title": "Urania: Differentially Private Insights into AI Use",
    "authors": [
      "Daogao Liu",
      "Edith Cohen",
      "Badih Ghazi",
      "Peter Kairouz",
      "Pritish Kamath",
      "Alexander Knop",
      "Ravi Kumar",
      "Pasin Manurangsi",
      "Adam Sealfon",
      "Da Yu",
      "Chiyuan Zhang"
    ],
    "abstract": "We introduce $Urania$, a novel framework for generating insights about LLM chatbot interactions with rigorous differential privacy (DP) guarantees. The framework employs a private clustering mechanism and innovative keyword extraction methods, including frequency-based, TF-IDF-based, and LLM-guided approaches. By leveraging DP tools such as clustering, partition selection, and histogram-based summarization, $Urania$ provides end-to-end privacy protection. Our evaluation assesses lexical and semantic content preservation, pair similarity, and LLM-based metrics, benchmarking against a non-private Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple empirical privacy evaluation that demonstrates the enhanced robustness of our DP pipeline. The results show the framework's ability to extract meaningful conversational insights while maintaining stringent user privacy, effectively balancing data utility with privacy preservation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear at COLM 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04681v2",
    "published_date": "2025-06-05 07:00:31 UTC",
    "updated_date": "2025-09-23 23:45:44 UTC"
  },
  {
    "arxiv_id": "2506.04676v1",
    "title": "Gen-n-Val: Agentic Image Data Generation and Validation",
    "authors": [
      "Jing-En Huang",
      "I-Sheng Fang",
      "Tzuhsuan Huang",
      "Chih-Yu Wang",
      "Jun-Cheng Chen"
    ],
    "abstract": "Recently, Large Language Models (LLMs) and Vision Large Language Models (VLLMs) have demonstrated impressive performance as agents across various tasks while data scarcity and label noise remain significant challenges in computer vision tasks, such as object detection and instance segmentation. A common solution for resolving these issues is to generate synthetic data. However, current synthetic data generation methods struggle with issues, such as multiple objects per mask, inaccurate segmentation, and incorrect category labels, limiting their effectiveness. To address these issues, we introduce Gen-n-Val, a novel agentic data generation framework that leverages Layer Diffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks and diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt agent, an LLM, optimizes prompts for LD to generate high-quality foreground instance images and segmentation masks. These optimized prompts ensure the generation of single-object synthetic data with precise instance masks and clean backgrounds. (2) The data validation agent, a VLLM, which filters out low-quality synthetic instance images. The system prompts for both agents are refined through TextGrad. Additionally, we use image harmonization to combine multiple instances within scenes. Compared to state-of-the-art synthetic data approaches like MosaicFusion, our approach reduces invalid synthetic data from 50% to 7% and improves performance by 1% mAP on rare classes in COCO instance segmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant improvements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object detection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance of YOLOv9 and YOLO11 families in instance segmentation and object detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04676v1",
    "published_date": "2025-06-05 06:52:26 UTC",
    "updated_date": "2025-06-05 06:52:26 UTC"
  },
  {
    "arxiv_id": "2506.05427v1",
    "title": "MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction",
    "authors": [
      "Zishan Shu",
      "Yufan Deng",
      "Hongyu Zhang",
      "Zhiwei Nie",
      "Jie Chen"
    ],
    "abstract": "Activity cliff prediction is a critical task in drug discovery and material design. Existing computational methods are limited to handling single binding targets, which restricts the applicability of these prediction models. In this paper, we present the Multi-Grained Target Perception network (MTPNet) to incorporate the prior knowledge of interactions between the molecules and their target proteins. Specifically, MTPNet is a unified framework for activity cliff prediction, which consists of two components: Macro-level Target Semantic (MTS) guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet dynamically optimizes molecular representations through multi-grained protein semantic conditions. To our knowledge, it is the first time to employ the receptor proteins as guiding information to effectively capture critical interaction details. Extensive experiments on 30 representative activity cliff datasets demonstrate that MTPNet significantly outperforms previous approaches, achieving an average RMSE improvement of 18.95% on top of several mainstream GNN architectures. Overall, MTPNet internalizes interaction patterns through conditional deep learning to achieve unified predictions of activity cliffs, helping to accelerate compound optimization and design. Codes are available at: https://github.com/ZishanShu/MTPNet.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05427v1",
    "published_date": "2025-06-05 06:42:36 UTC",
    "updated_date": "2025-06-05 06:42:36 UTC"
  },
  {
    "arxiv_id": "2506.04668v3",
    "title": "Feature-Based Lie Group Transformer for Real-World Applications",
    "authors": [
      "Takayuki Komatsu",
      "Yoshiyuki Ohmura",
      "Kayato Nishitsunoi",
      "Yasuo Kuniyoshi"
    ],
    "abstract": "The main goal of representation learning is to acquire meaningful representations from real-world sensory inputs without supervision. Representation learning explains some aspects of human development. Various neural network (NN) models have been proposed that acquire empirically good representations. However, the formulation of a good representation has not been established. We recently proposed a method for categorizing changes between a pair of sensory inputs. A unique feature of this approach is that transformations between two sensory inputs are learned to satisfy algebraic structural constraints. Conventional representation learning often assumes that disentangled independent feature axes is a good representation; however, we found that such a representation cannot account for conditional independence. To overcome this problem, we proposed a new method using group decomposition in Galois algebra theory. Although this method is promising for defining a more general representation, it assumes pixel-to-pixel translation without feature extraction, and can only process low-resolution images with no background, which prevents real-world application. In this study, we provide a simple method to apply our group decomposition theory to a more realistic scenario by combining feature extraction and object segmentation. We replace pixel translation with feature translation and formulate object segmentation as grouping features under the same transformation. We validated the proposed method on a practical dataset containing both real-world object and background. We believe that our model will lead to a better understanding of human development of object recognition in the real world.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, the dataset used in this work is https://drive.google.com/file/d/1RaSWNN2GEyV3zQPeGya4Mr9DDhJ7OMz7/view?usp=sharing",
    "pdf_url": "https://arxiv.org/pdf/2506.04668v3",
    "published_date": "2025-06-05 06:30:11 UTC",
    "updated_date": "2025-06-09 12:10:31 UTC"
  },
  {
    "arxiv_id": "2506.05426v3",
    "title": "Mixture-of-Experts Meets In-Context Reinforcement Learning",
    "authors": [
      "Wenhao Wu",
      "Fuhong Liu",
      "Haoru Li",
      "Zican Hu",
      "Daoyi Dong",
      "Chunlin Chen",
      "Zhi Wang"
    ],
    "abstract": "In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: a token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and a task-wise MoE that routes diverse tasks to specialized experts for managing a broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce a contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering a simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at https://github.com/NJU-RL/T2MIR.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 13 figures, 17 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.05426v3",
    "published_date": "2025-06-05 06:29:14 UTC",
    "updated_date": "2025-10-28 06:55:14 UTC"
  },
  {
    "arxiv_id": "2506.11086v1",
    "title": "Intelligibility of Text-to-Speech Systems for Mathematical Expressions",
    "authors": [
      "Sujoy Roychowdhury",
      "H. G. Ranjani",
      "Sumit Soman",
      "Nishtha Paul",
      "Subhadip Bandyopadhyay",
      "Siddhanth Iyengar"
    ],
    "abstract": "There has been limited evaluation of advanced Text-to-Speech (TTS) models with Mathematical eXpressions (MX) as inputs. In this work, we design experiments to evaluate quality and intelligibility of five TTS models through listening and transcribing tests for various categories of MX. We use two Large Language Models (LLMs) to generate English pronunciation from LaTeX MX as TTS models cannot process LaTeX directly. We use Mean Opinion Score from user ratings and quantify intelligibility through transcription correctness using three metrics. We also compare listener preference of TTS outputs with respect to human expert rendition of same MX. Results establish that output of TTS models for MX is not necessarily intelligible, the gap in intelligibility varies across TTS models and MX category. For most categories, performance of TTS models is significantly worse than that of expert rendition. The effect of choice of LLM is limited. This establishes the need to improve TTS models for MX.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11086v1",
    "published_date": "2025-06-05 06:21:28 UTC",
    "updated_date": "2025-06-05 06:21:28 UTC"
  },
  {
    "arxiv_id": "2506.15722v1",
    "title": "UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation",
    "authors": [
      "Wangzhi Zhan",
      "Jianpeng Chen",
      "Dongqi Fu",
      "Dawei Zhou"
    ],
    "abstract": "Metamaterials are artificial materials that are designed to meet unseen properties in nature, such as ultra-stiffness and negative materials indices. In mechanical metamaterial design, three key modalities are typically involved, i.e., 3D topology, density condition, and mechanical property. Real-world complex application scenarios place the demanding requirements on machine learning models to consider all three modalities together. However, a comprehensive literature review indicates that most existing works only consider two modalities, e.g., predicting mechanical properties given the 3D topology or generating 3D topology given the required properties. Therefore, there is still a significant gap for the state-of-the-art machine learning models capturing the whole. Hence, we propose a unified model named UNIMATE, which consists of a modality alignment module and a synergetic diffusion generation module. Experiments indicate that UNIMATE outperforms the other baseline models in topology generation task, property prediction task, and condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We opensource our proposed UNIMATE model and corresponding results at https://github.com/wzhan24/UniMate.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15722v1",
    "published_date": "2025-06-05 06:05:09 UTC",
    "updated_date": "2025-06-05 06:05:09 UTC"
  },
  {
    "arxiv_id": "2506.06384v1",
    "title": "Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering",
    "authors": [
      "Yi Ji",
      "Runzhi Li",
      "Baolei Mao"
    ],
    "abstract": "With the widespread adoption of Large Language Models (LLMs), prompt injection attacks have emerged as a significant security threat. Existing defense mechanisms often face critical trade-offs between effectiveness and generalizability. This highlights the urgent need for efficient prompt injection detection methods that are applicable across a wide range of LLMs. To address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion detection framework. It integrates a pretrained language model with heuristic feature engineering to detect prompt injection attacks. Specifically, the framework employs DeBERTa-v3-base as a feature extractor to transform input text into semantic vectors enriched with contextual information. In parallel, we design heuristic rules based on known attack patterns to extract explicit structural features commonly observed in attacks. Features from both channels are subsequently fused and passed through a fully connected neural network to produce the final prediction. This dual-channel approach mitigates the limitations of relying only on DeBERTa to extract features. Experimental results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms existing methods in terms of accuracy, recall, and F1-score. Furthermore, when deployed actually, it significantly reduces attack success rates across mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by KSEM2025 AI & Sec Workshop",
    "pdf_url": "https://arxiv.org/pdf/2506.06384v1",
    "published_date": "2025-06-05 06:01:19 UTC",
    "updated_date": "2025-06-05 06:01:19 UTC"
  },
  {
    "arxiv_id": "2506.05425v1",
    "title": "SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning",
    "authors": [
      "Fanqi Kong",
      "Weiqin Zu",
      "Xinyu Chen",
      "Yaodong Yang",
      "Song-Chun Zhu",
      "Xue Feng"
    ],
    "abstract": "The rich and multifaceted nature of human social interaction, encompassing multimodal cues, unobservable relations and mental states, and dynamical behavior, presents a formidable challenge for artificial intelligence. To advance research in this area, we introduce SIV-Bench, a novel video benchmark for rigorously evaluating the capabilities of Multimodal Large Language Models (MLLMs) across Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP). SIV-Bench features 2,792 video clips and 8,792 meticulously generated question-answer pairs derived from a human-LLM collaborative pipeline. It is originally collected from TikTok and YouTube, covering a wide range of video genres, presentation styles, and linguistic and cultural backgrounds. It also includes a dedicated setup for analyzing the impact of different textual cues-original on-screen text, added dialogue, or no text. Our comprehensive experiments on leading MLLMs reveal that while models adeptly handle SSU, they significantly struggle with SSR and SDP, where Relation Inference (RI) is an acute bottleneck, as further examined in our analysis. Our study also confirms the critical role of transcribed dialogue in aiding comprehension of complex social interactions. By systematically identifying current MLLMs' strengths and limitations, SIV-Bench offers crucial insights to steer the development of more socially intelligent AI. The dataset and code are available at https://kfq20.github.io/sivbench/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05425v1",
    "published_date": "2025-06-05 05:51:35 UTC",
    "updated_date": "2025-06-05 05:51:35 UTC"
  },
  {
    "arxiv_id": "2506.04654v2",
    "title": "E-bike agents: Large Language Model-Driven E-Bike Accident Analysis and Severity Prediction",
    "authors": [
      "Zhichao Yang",
      "Jiashu He",
      "Mohammad B. Al-Khasawneh",
      "Darshan Pandit",
      "Cirillo Cinzia"
    ],
    "abstract": "E-bikes have rapidly gained popularity as a sustainable form of urban mobility, yet their safety implications remain underexplored. This paper analyzes injury incidents involving e-bikes and traditional bicycles using two sources of data, the CPSRMS (Consumer Product Safety Risk Management System Information Security Review Report) and NEISS (National Electronic Injury Surveillance System) datasets. We propose a standardized classification framework to identify and quantify injury causes and severity. By integrating incident narratives with demographic attributes, we reveal key differences in mechanical failure modes, injury severity patterns, and affected user groups. While both modes share common causes, such as loss of control and pedal malfunctions, e-bikes present distinct risks, including battery-related fires and brake failures. These findings highlight the need for tailored safety interventions and infrastructure design to support the safe integration of micromobility devices into urban transportation networks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04654v2",
    "published_date": "2025-06-05 05:49:41 UTC",
    "updated_date": "2025-10-24 19:40:27 UTC"
  },
  {
    "arxiv_id": "2506.04651v2",
    "title": "Agents of Change: Self-Evolving LLM Agents for Strategic Planning",
    "authors": [
      "Nikolas Belle",
      "Dakota Barnes",
      "Alfonso Amayuelas",
      "Ivan Bercovich",
      "Xin Eric Wang",
      "William Wang"
    ],
    "abstract": "We address the long-horizon gap in large language model (LLM) agents by enabling them to sustain coherent strategies in adversarial, stochastic environments. Settlers of Catan provides a challenging benchmark: success depends on balancing short- and long-term goals amid randomness, trading, expansion, and blocking. Prompt-centric LLM agents (e.g., ReAct, Reflexion) must re-interpret large, evolving game states each turn, quickly saturating context windows and losing strategic consistency. We propose HexMachina, a continual learning multi-agent system that separates environment discovery (inducing an adapter layer without documentation) from strategy improvement (evolving a compiled player through code refinement and simulation). This design preserves executable artifacts, allowing the LLM to focus on high-level strategy rather than per-turn reasoning. In controlled Catanatron experiments, HexMachina learns from scratch and evolves players that outperform the strongest human-crafted baseline (AlphaBeta), achieving a 54% win rate and surpassing prompt-driven and no-discovery baselines. Ablations confirm that isolating pure strategy learning improves performance. Overall, artifact-centric continual learning transforms LLMs from brittle stepwise deciders into stable strategy designers, advancing long-horizon autonomy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04651v2",
    "published_date": "2025-06-05 05:45:24 UTC",
    "updated_date": "2025-10-13 08:57:19 UTC"
  },
  {
    "arxiv_id": "2506.04636v2",
    "title": "CHANCERY: Evaluating Corporate Governance Reasoning Capabilities in Language Models",
    "authors": [
      "Lucas Irwin",
      "Arda Kaz",
      "Peiyao Sheng",
      "Sewoong Oh",
      "Pramod Viswanath"
    ],
    "abstract": "Law has long been a domain that has been popular in natural language processing (NLP) applications. Reasoning (ratiocination and the ability to make connections to precedent) is a core part of the practice of the law in the real world. Nevertheless, while multiple legal datasets exist, none have thus far focused specifically on reasoning tasks. We focus on a specific aspect of the legal landscape by introducing a corporate governance reasoning benchmark (CHANCERY) to test a model's ability to reason about whether executive/board/shareholder's proposed actions are consistent with corporate governance charters. This benchmark introduces a first-of-its-kind corporate governance reasoning test for language models - modeled after real world corporate governance law. The benchmark consists of a corporate charter (a set of governing covenants) and a proposal for executive action. The model's task is one of binary classification: reason about whether the action is consistent with the rules contained within the charter. We create the benchmark following established principles of corporate governance - 24 concrete corporate governance principles established in and 79 real life corporate charters selected to represent diverse industries from a total dataset of 10k real life corporate charters. Evaluations on state-of-the-art (SOTA) reasoning models confirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet and GPT-4o achieving 64.5% and 75.2% accuracy respectively. Reasoning agents exhibit superior performance, with agents based on the ReAct and CodeAct frameworks scoring 76.1% and 78.1% respectively, further confirming the advanced legal reasoning capabilities required to score highly on the benchmark. We also conduct an analysis of the types of questions which current reasoning models struggle on, revealing insights into the legal reasoning capabilities of SOTA models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04636v2",
    "published_date": "2025-06-05 05:13:32 UTC",
    "updated_date": "2025-06-12 03:27:21 UTC"
  },
  {
    "arxiv_id": "2506.05422v1",
    "title": "Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference",
    "authors": [
      "Andrei T. Patrascu"
    ],
    "abstract": "We introduce a novel learning and planning framework that replaces traditional reward-based optimisation with constructive logical inference. In our model, actions, transitions, and goals are represented as logical propositions, and decision-making proceeds by building constructive proofs under intuitionistic logic. This method ensures that state transitions and policies are accepted only when supported by verifiable preconditions -- eschewing probabilistic trial-and-error in favour of guaranteed logical validity. We implement a symbolic agent operating in a structured gridworld, where reaching a goal requires satisfying a chain of intermediate subgoals (e.g., collecting keys to open doors), each governed by logical constraints. Unlike conventional reinforcement learning agents, which require extensive exploration and suffer from unsafe or invalid transitions, our constructive agent builds a provably correct plan through goal chaining, condition tracking, and knowledge accumulation. Empirical comparison with Q-learning demonstrates that our method achieves perfect safety, interpretable behaviour, and efficient convergence with no invalid actions, highlighting its potential for safe planning, symbolic cognition, and trustworthy AI. This work presents a new direction for reinforcement learning grounded not in numeric optimisation, but in constructive logic and proof theory.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05422v1",
    "published_date": "2025-06-05 04:49:31 UTC",
    "updated_date": "2025-06-05 04:49:31 UTC"
  },
  {
    "arxiv_id": "2506.04624v2",
    "title": "Static Word Embeddings for Sentence Semantic Representation",
    "authors": [
      "Takashi Wada",
      "Yuki Hirakawa",
      "Ryotaro Shimizu",
      "Takahiro Kawashima",
      "Yuki Saito"
    ],
    "abstract": "We propose new static word embeddings optimised for sentence semantic representation. We first extract word embeddings from a pre-trained Sentence Transformer, and improve them with sentence-level principal component analysis, followed by either knowledge distillation or contrastive learning. During inference, we represent sentences by simply averaging word embeddings, which requires little computational cost. We evaluate models on both monolingual and cross-lingual tasks and show that our model substantially outperforms existing static models on sentence semantic tasks, and even surpasses a basic Sentence Transformer model (SimCSE) on a text embedding benchmark. Lastly, we perform a variety of analyses and show that our method successfully removes word embedding components that are not highly relevant to sentence semantics, and adjusts the vector norms based on the influence of words on sentence semantics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages; accepted to the Main Conference of EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04624v2",
    "published_date": "2025-06-05 04:33:10 UTC",
    "updated_date": "2025-09-30 10:22:41 UTC"
  },
  {
    "arxiv_id": "2506.04614v2",
    "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation",
    "authors": [
      "Yuyang Wanyan",
      "Xi Zhang",
      "Haiyang Xu",
      "Haowei Liu",
      "Junyang Wang",
      "Jiabo Ye",
      "Yutong Kou",
      "Ming Yan",
      "Fei Huang",
      "Xiaoshan Yang",
      "Weiming Dong",
      "Changsheng Xu"
    ],
    "abstract": "In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04614v2",
    "published_date": "2025-06-05 04:12:36 UTC",
    "updated_date": "2025-11-17 07:48:59 UTC"
  },
  {
    "arxiv_id": "2506.04613v3",
    "title": "DeePoly: A High-Order Accuracy Scientific Machine Learning Framework for Function Approximation and Solving PDEs",
    "authors": [
      "Li Liu",
      "Heng Yong"
    ],
    "abstract": "Recently, machine learning methods have gained significant traction in scientific computing, particularly for solving Partial Differential Equations (PDEs). However, methods based on deep neural networks (DNNs) often lack convergence guarantees and computational efficiency compared to traditional numerical schemes. This work introduces DeePoly, a novel framework that transforms the solution paradigm from pure non-convex parameter optimization to a two-stage approach: first employing a DNN to capture complex global features, followed by linear space optimization with combined DNN-extracted features (Spotter) and polynomial basis functions (Sniper). This strategic combination leverages the complementary strengths of both methods -- DNNs excel at approximating complex global features (i.e., high-gradient features) and stabilize the polynomial approximation while polynomial bases provide high-precision local corrections with convergence guarantees. Theoretical analysis and numerical experiments demonstrate that this approach significantly enhances both high-order accuracy and efficiency across diverse problem types while maintaining mesh-free and scheme-free properties. This paper also serves as a theoretical exposition for the open-source project DeePoly.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "for associated mpeg file, see http://github.com/bfly123/DeePoly",
    "pdf_url": "https://arxiv.org/pdf/2506.04613v3",
    "published_date": "2025-06-05 04:10:52 UTC",
    "updated_date": "2025-06-13 15:43:02 UTC"
  },
  {
    "arxiv_id": "2506.06383v1",
    "title": "Human and AI collaboration in Fitness Education:A Longitudinal Study with a Pilates Instructor",
    "authors": [
      "Qian Huang",
      "King Wang Poon"
    ],
    "abstract": "Artificial intelligence is poised to transform teaching and coaching practices,yet its optimal role alongside human expertise remains unclear.This study investigates human and AI collaboration in fitness education through a one year qualitative case study with a Pilates instructor.The researcher participated in the instructor classes and conducted biweekly semi structured interviews to explore how generative AI could be integrated into class planning and instruction.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "19 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.06383v1",
    "published_date": "2025-06-05 04:04:34 UTC",
    "updated_date": "2025-06-05 04:04:34 UTC"
  },
  {
    "arxiv_id": "2506.04610v1",
    "title": "Judicial Permission",
    "authors": [
      "Guido Governatori",
      "Antonino Rotolo"
    ],
    "abstract": "This paper examines the significance of weak permissions in criminal trials (\\emph{judicial permission}). It introduces a dialogue game model to systematically address judicial permissions, considering different standards of proof and argumentation semantics.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04610v1",
    "published_date": "2025-06-05 04:00:24 UTC",
    "updated_date": "2025-06-05 04:00:24 UTC"
  },
  {
    "arxiv_id": "2506.04598v1",
    "title": "Scaling Laws for Robust Comparison of Open Foundation Language-Vision Models and Datasets",
    "authors": [
      "Marianna Nezhurina",
      "Tomer Porian",
      "Giovanni Pucceti",
      "Tommie Kerssies",
      "Romain Beaumont",
      "Mehdi Cherti",
      "Jenia Jitsev"
    ],
    "abstract": "In studies of transferable learning, scaling laws are obtained for various important foundation models to predict their properties and performance at larger scales. We show here how scaling law derivation can also be used for model and dataset comparison, allowing to decide which procedure is to be preferred for pre-training. For the first time, full scaling laws based on dense measurements across a wide span of model and samples seen scales are derived for two important language-vision learning procedures, CLIP and MaMMUT, that use either contrastive only or contrastive and captioning text generative loss. Ensuring sufficient prediction accuracy for held out points, we use derived scaling laws to compare both models, obtaining evidence for MaMMUT's stronger improvement with scale and better sample efficiency than standard CLIP. To strengthen validity of the comparison, we show scaling laws for various downstream tasks, classification, retrieval, and segmentation, and for different open datasets, DataComp, DFN and Re-LAION, observing consistently the same trends. We show that comparison can also be performed when deriving scaling laws with a constant learning rate schedule, reducing compute cost. Accurate derivation of scaling laws provides thus means to perform model and dataset comparison across scale spans, avoiding misleading conclusions based on measurements from single reference scales only, paving the road for systematic comparison and improvement of open foundation models and datasets for their creation. We release all the pre-trained models with their intermediate checkpoints, including openMaMMUT-L/14, which achieves $80.3\\%$ zero-shot ImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/scaling-laws-for-comparison.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. In Review",
    "pdf_url": "https://arxiv.org/pdf/2506.04598v1",
    "published_date": "2025-06-05 03:35:59 UTC",
    "updated_date": "2025-06-05 03:35:59 UTC"
  },
  {
    "arxiv_id": "2506.04594v1",
    "title": "Intelligent Channel Allocation for IEEE 802.11be Multi-Link Operation: When MAB Meets LLM",
    "authors": [
      "Shumin Lian",
      "Jingwen Tong",
      "Jun Zhang",
      "Liqun Fu"
    ],
    "abstract": "WiFi networks have achieved remarkable success in enabling seamless communication and data exchange worldwide. The IEEE 802.11be standard, known as WiFi 7, introduces Multi-Link Operation (MLO), a groundbreaking feature that enables devices to establish multiple simultaneous connections across different bands and channels. While MLO promises substantial improvements in network throughput and latency reduction, it presents significant challenges in channel allocation, particularly in dense network environments. Current research has predominantly focused on performance analysis and throughput optimization within static WiFi 7 network configurations. In contrast, this paper addresses the dynamic channel allocation problem in dense WiFi 7 networks with MLO capabilities. We formulate this challenge as a combinatorial optimization problem, leveraging a novel network performance analysis mechanism. Given the inherent lack of prior network information, we model the problem within a Multi-Armed Bandit (MAB) framework to enable online learning of optimal channel allocations. Our proposed Best-Arm Identification-enabled Monte Carlo Tree Search (BAI-MCTS) algorithm includes rigorous theoretical analysis, providing upper bounds for both sample complexity and error probability. To further reduce sample complexity and enhance generalizability across diverse network scenarios, we put forth LLM-BAI-MCTS, an intelligent algorithm for the dynamic channel allocation problem by integrating the Large Language Model (LLM) into the BAI-MCTS algorithm. Numerical results demonstrate that the BAI-MCTS algorithm achieves a convergence rate approximately $50.44\\%$ faster than the state-of-the-art algorithms when reaching $98\\%$ of the optimal value. Notably, the convergence rate of the LLM-BAI-MCTS algorithm increases by over $63.32\\%$ in dense networks.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.NI",
    "comment": "This work has been accepted by JSAC 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04594v1",
    "published_date": "2025-06-05 03:19:57 UTC",
    "updated_date": "2025-06-05 03:19:57 UTC"
  },
  {
    "arxiv_id": "2506.04592v1",
    "title": "Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification",
    "authors": [
      "Chengwu Liu",
      "Ye Yuan",
      "Yichun Yin",
      "Yan Xu",
      "Xin Xu",
      "Zaoyu Chen",
      "Yasheng Wang",
      "Lifeng Shang",
      "Qun Liu",
      "Ming Zhang"
    ],
    "abstract": "Chain-of-Thought (CoT) prompting has become the de facto method to elicit reasoning capabilities from large language models (LLMs). However, to mitigate hallucinations in CoT that are notoriously difficult to detect, current methods such as process reward models (PRMs) or self-consistency operate as opaque boxes and do not provide checkable evidence for their judgments, possibly limiting their effectiveness. To address this issue, we draw inspiration from the idea that \"the gold standard for supporting a mathematical claim is to provide a proof\". We propose a retrospective, step-aware formal verification framework $Safe$. Rather than assigning arbitrary scores, we strive to articulate mathematical claims in formal mathematical language Lean 4 at each reasoning step and provide formal proofs to identify hallucinations. We evaluate our framework $Safe$ across multiple language models and various mathematical datasets, demonstrating a significant performance improvement while offering interpretable and verifiable evidence. We also propose $FormalStep$ as a benchmark for step correctness theorem proving with $30,809$ formal statements. To the best of our knowledge, our work represents the first endeavor to utilize formal mathematical language Lean 4 for verifying natural language content generated by LLMs, aligning with the reason why formal mathematical languages were created in the first place: to provide a robust foundation for hallucination-prone human-written proofs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04592v1",
    "published_date": "2025-06-05 03:16:08 UTC",
    "updated_date": "2025-06-05 03:16:08 UTC"
  },
  {
    "arxiv_id": "2506.12073v1",
    "title": "Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis",
    "authors": [
      "Zongli Ye",
      "Jiachen Lian",
      "Xuanru Zhou",
      "Jinming Zhang",
      "Haodong Li",
      "Shuhe Li",
      "Chenxu Guo",
      "Anaisha Das",
      "Peter Park",
      "Zoe Ezzes",
      "Jet Vonk",
      "Brittany Morin",
      "Rian Bogley",
      "Lisa Wauters",
      "Zachary Miller",
      "Maria Gorno-Tempini",
      "Gopala Anumanchipalli"
    ],
    "abstract": "Accurate alignment of dysfluent speech with intended text is crucial for automating the diagnosis of neurodegenerative speech disorders. Traditional methods often fail to model phoneme similarities effectively, limiting their performance. In this work, we propose Neural LCS, a novel approach for dysfluent text-text and speech-text alignment. Neural LCS addresses key challenges, including partial alignment and context-aware similarity mapping, by leveraging robust phoneme-level modeling. We evaluate our method on a large-scale simulated dataset, generated using advanced data simulation techniques, and real PPA data. Neural LCS significantly outperforms state-of-the-art models in both alignment accuracy and dysfluent speech segmentation. Our results demonstrate the potential of Neural LCS to enhance automated systems for diagnosing and analyzing speech disorders, offering a more accurate and linguistically grounded solution for dysfluent speech alignment.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted for Interspeech2025",
    "pdf_url": "https://arxiv.org/pdf/2506.12073v1",
    "published_date": "2025-06-05 03:06:37 UTC",
    "updated_date": "2025-06-05 03:06:37 UTC"
  },
  {
    "arxiv_id": "2506.13772v1",
    "title": "MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs",
    "authors": [
      "Zhenyan Lu",
      "Daliang Xu",
      "Dongqi Cai",
      "Zexi Li",
      "Wei Liu",
      "Fangming Liu",
      "Shangguang Wang",
      "Mengwei Xu"
    ],
    "abstract": "Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\\times$ less memory, 14.7 $\\times$ less energy and 3.6$\\times$ less latency compared to previous knowledge editing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13772v1",
    "published_date": "2025-06-05 03:02:32 UTC",
    "updated_date": "2025-06-05 03:02:32 UTC"
  },
  {
    "arxiv_id": "2506.04583v1",
    "title": "SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through Claim Decomposition and Editing",
    "authors": [
      "Hongjun Liu",
      "Yilun Zhao",
      "Arman Cohan",
      "Chen Zhao"
    ],
    "abstract": "Automatic fact-checking has recently received more attention as a means of combating misinformation. Despite significant advancements, fact-checking systems based on retrieval-augmented language models still struggle to tackle adversarial claims, which are intentionally designed by humans to challenge fact-checking systems. To address these challenges, we propose a training-free method designed to rephrase the original claim, making it easier to locate supporting evidence. Our modular framework, SUCEA, decomposes the task into three steps: 1) Claim Segmentation and Decontextualization that segments adversarial claims into independent sub-claims; 2) Iterative Evidence Retrieval and Claim Editing that iteratively retrieves evidence and edits the subclaim based on the retrieved evidence; 3) Evidence Aggregation and Label Prediction that aggregates all retrieved evidence and predicts the entailment label. Experiments on two challenging fact-checking datasets demonstrate that our framework significantly improves on both retrieval and entailment label accuracy, outperforming four strong claim-decomposition-based baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 10 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.04583v1",
    "published_date": "2025-06-05 02:58:15 UTC",
    "updated_date": "2025-06-05 02:58:15 UTC"
  },
  {
    "arxiv_id": "2506.04574v1",
    "title": "Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis",
    "authors": [
      "Dimitris Vamvourellis",
      "Dhagash Mehta"
    ],
    "abstract": "We investigate the effectiveness of large language models (LLMs), including reasoning-based and non-reasoning models, in performing zero-shot financial sentiment analysis. Using the Financial PhraseBank dataset annotated by domain experts, we evaluate how various LLMs and prompting strategies align with human-labeled sentiment in a financial context. We compare three proprietary LLMs (GPT-4o, GPT-4.1, o3-mini) under different prompting paradigms that simulate System 1 (fast and intuitive) or System 2 (slow and deliberate) thinking and benchmark them against two smaller models (FinBERT-Prosus, FinBERT-Tone) fine-tuned on financial sentiment analysis. Our findings suggest that reasoning, either through prompting or inherent model design, does not improve performance on this task. Surprisingly, the most accurate and human-aligned combination of model and method was GPT-4o without any Chain-of-Thought (CoT) prompting. We further explore how performance is impacted by linguistic complexity and annotation agreement levels, uncovering that reasoning may introduce overthinking, leading to suboptimal predictions. This suggests that for financial sentiment classification, fast, intuitive \"System 1\"-like thinking aligns more closely with human judgment compared to \"System 2\"-style slower, deliberative reasoning simulated by reasoning models or CoT prompting. Our results challenge the default assumption that more reasoning always leads to better LLM decisions, particularly in high-stakes financial applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04574v1",
    "published_date": "2025-06-05 02:47:23 UTC",
    "updated_date": "2025-06-05 02:47:23 UTC"
  },
  {
    "arxiv_id": "2506.04571v2",
    "title": "OpenAg: Democratizing Agricultural Intelligence",
    "authors": [
      "Srikanth Thudumu",
      "Jason Fisher"
    ],
    "abstract": "Agriculture is undergoing a major transformation driven by artificial intelligence (AI), machine learning, and knowledge representation technologies. However, current agricultural intelligence systems often lack contextual understanding, explainability, and adaptability, especially for smallholder farmers with limited resources. General-purpose large language models (LLMs), while powerful, typically lack the domain-specific knowledge and contextual reasoning needed for practical decision support in farming. They tend to produce recommendations that are too generic or unrealistic for real-world applications. To address these challenges, we present OpenAg, a comprehensive framework designed to advance agricultural artificial general intelligence (AGI). OpenAg combines domain-specific foundation models, neural knowledge graphs, multi-agent reasoning, causal explainability, and adaptive transfer learning to deliver context-aware, explainable, and actionable insights. The system includes: (i) a unified agricultural knowledge base that integrates scientific literature, sensor data, and farmer-generated knowledge; (ii) a neural agricultural knowledge graph for structured reasoning and inference; (iii) an adaptive multi-agent reasoning system where AI agents specialize and collaborate across agricultural domains; and (iv) a causal transparency mechanism that ensures AI recommendations are interpretable, scientifically grounded, and aligned with real-world constraints. OpenAg aims to bridge the gap between scientific knowledge and the tacit expertise of experienced farmers to support scalable and locally relevant agricultural decision-making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2506.04571v2",
    "published_date": "2025-06-05 02:44:38 UTC",
    "updated_date": "2025-07-04 22:44:41 UTC"
  },
  {
    "arxiv_id": "2506.04566v1",
    "title": "Clustering and Median Aggregation Improve Differentially Private Inference",
    "authors": [
      "Kareem Amin",
      "Salman Avestimehr",
      "Sara Babakniya",
      "Alex Bie",
      "Weiwei Kong",
      "Natalia Ponomareva",
      "Umar Syed"
    ],
    "abstract": "Differentially private (DP) language model inference is an approach for generating private synthetic text. A sensitive input example is used to prompt an off-the-shelf large language model (LLM) to produce a similar example. Multiple examples can be aggregated together to formally satisfy the DP guarantee.\n  Prior work creates inference batches by sampling sensitive inputs uniformly at random. We show that uniform sampling degrades the quality of privately generated text, especially when the sensitive examples concern heterogeneous topics.\n  We remedy this problem by clustering the input data before selecting inference batches. Next, we observe that clustering also leads to more similar next-token predictions across inferences. We use this insight to introduce a new algorithm that aggregates next token statistics by privately computing medians instead of averages. This approach leverages the fact that the median has decreased local sensitivity when next token predictions are similar, allowing us to state a data-dependent and ex-post DP guarantee about the privacy properties of this algorithm. Finally, we demonstrate improvements in terms of representativeness metrics (e.g., MAUVE) as well as downstream task performance. We show that our method produces high-quality synthetic data at significantly lower privacy cost than a previous state-of-the-art method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04566v1",
    "published_date": "2025-06-05 02:34:50 UTC",
    "updated_date": "2025-06-05 02:34:50 UTC"
  },
  {
    "arxiv_id": "2506.04557v2",
    "title": "SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for Under-Resourced African Languages?",
    "authors": [
      "Senyu Li",
      "Jiayi Wang",
      "Felermino D. M. A. Ali",
      "Colin Cherry",
      "Daniel Deutsch",
      "Eleftheria Briakou",
      "Rui Sousa-Silva",
      "Henrique Lopes Cardoso",
      "Pontus Stenetorp",
      "David Ifeoluwa Adelani"
    ],
    "abstract": "Evaluating machine translation (MT) quality for under-resourced African languages remains a significant challenge, as existing metrics often suffer from limited language coverage and poor performance in low-resource settings. While recent efforts, such as AfriCOMET, have addressed some of the issues, they are still constrained by small evaluation sets, a lack of publicly available training data tailored to African languages, and inconsistent performance in extremely low-resource scenarios. In this work, we introduce SSA-MTE, a large-scale human-annotated MT evaluation (MTE) dataset covering 14 African language pairs from the News domain, with over 73,000 sentence-level annotations from a diverse set of MT systems. Based on this data, we develop SSA-COMET and SSA-COMET-QE, improved reference-based and reference-free evaluation metrics. We also benchmark prompting-based approaches using state-of-the-art LLMs like GPT-4o, Claude-3.7 and Gemini 2.5 Pro. Our experimental results show that SSA-COMET models significantly outperform AfriCOMET and are competitive with the strongest LLM Gemini 2.5 Pro evaluated in our study, particularly on low-resource languages such as Twi, Luo, and Yoruba. All resources are released under open licenses to support future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04557v2",
    "published_date": "2025-06-05 02:16:56 UTC",
    "updated_date": "2025-10-03 19:55:57 UTC"
  },
  {
    "arxiv_id": "2506.04556v1",
    "title": "BESA: Boosting Encoder Stealing Attack with Perturbation Recovery",
    "authors": [
      "Xuhao Ren",
      "Haotian Liang",
      "Yajie Wang",
      "Chuan Zhang",
      "Zehui Xiong",
      "Liehuang Zhu"
    ],
    "abstract": "To boost the encoder stealing attack under the perturbation-based defense that hinders the attack performance, we propose a boosting encoder stealing attack with perturbation recovery named BESA. It aims to overcome perturbation-based defenses. The core of BESA consists of two modules: perturbation detection and perturbation recovery, which can be combined with canonical encoder stealing attacks. The perturbation detection module utilizes the feature vectors obtained from the target encoder to infer the defense mechanism employed by the service provider. Once the defense mechanism is detected, the perturbation recovery module leverages the well-designed generative model to restore a clean feature vector from the perturbed one. Through extensive evaluations based on various datasets, we demonstrate that BESA significantly enhances the surrogate encoder accuracy of existing encoder stealing attacks by up to 24.63\\% when facing state-of-the-art defenses and combinations of multiple defenses.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04556v1",
    "published_date": "2025-06-05 02:14:30 UTC",
    "updated_date": "2025-06-05 02:14:30 UTC"
  },
  {
    "arxiv_id": "2506.12072v2",
    "title": "TrueGL: A Truthful, Reliable, and Unified Engine for Grounded Learning in Full-Stack Search",
    "authors": [
      "Joydeep Chandra",
      "Aleksandr Algazinov",
      "Satyam Kumar Navneet",
      "Rim El Filali",
      "Matt Laing",
      "Andrew Hanna"
    ],
    "abstract": "In the age of open and free information, a concerning trend of reliance on AI is emerging. However, existing AI tools struggle to evaluate the credibility of information and to justify their assessments. Hence, there is a growing need for systems that can help users evaluate the trustworthiness of online information. Although major search engines incorporate AI features, they often lack clear reliability indicators. We present TrueGL, a model that makes trustworthy search results more accessible. The model is a fine-tuned version of IBM's Granite-1B, trained on the custom dataset and integrated into a search engine with a reliability scoring system. We evaluate the system using prompt engineering and assigning each statement a continuous reliability score from 0.1 to 1, then instructing the model to return a textual explanation alongside the score. Each model's predicted scores are measured against real scores using standard evaluation metrics. TrueGL consistently outperforms other small-scale LLMs and rule-based approaches across all experiments on key evaluation metrics, including MAE, RMSE, and R2. The model's high accuracy, broad content coverage, and ease of use make trustworthy information more accessible and help reduce the spread of false or misleading content online. Our code is publicly available at https://github.com/AlgazinovAleksandr/TrueGL, and our model is publicly released at https://huggingface.co/JoydeepC/trueGL.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12072v2",
    "published_date": "2025-06-05 01:48:09 UTC",
    "updated_date": "2025-08-29 09:25:55 UTC"
  },
  {
    "arxiv_id": "2506.04544v2",
    "title": "hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation",
    "authors": [
      "Charles Hong",
      "Brendan Roberts",
      "Huijae An",
      "Alex Um",
      "Advay Ratan",
      "Yakun Sophia Shao"
    ],
    "abstract": "Large language models (LLMs) are playing an increasingly large role in domains such as code generation, including hardware code generation, where Verilog is the key language. However, the amount of publicly available Verilog code pales in comparison to the amount of code available for software languages like Python. In this work, we present hdl2v (\"HDL-to-Verilog\"), a dataset which seeks to increase the amount of available human-written Verilog data by translating or compiling three other hardware description languages - VHDL, Chisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v in enhancing LLM Verilog generation by improving performance of a 32 billion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2, without utilizing any data augmentation or knowledge distillation from larger models. We also show hdl2v's ability to boost the performance of a data augmentation-based fine-tuning approach by 63%. Finally, we characterize and analyze our dataset to better understand which characteristics of HDL-to-Verilog datasets can be expanded upon in future work for even better performance.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.AR",
    "comment": "Published at ACM/IEEE International Symposium on Machine Learning for CAD (MLCAD) 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04544v2",
    "published_date": "2025-06-05 01:29:18 UTC",
    "updated_date": "2025-07-08 19:43:08 UTC"
  },
  {
    "arxiv_id": "2506.04536v3",
    "title": "NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models",
    "authors": [
      "Luca Ghafourpour",
      "Valentin Duruisseaux",
      "Bahareh Tolooshams",
      "Philip H. Wong",
      "Costas A. Anastassiou",
      "Anima Anandkumar"
    ],
    "abstract": "Characterizing the cellular properties of neurons is fundamental to understanding their function in the brain. In this quest, the generation of bio-realistic models is central towards integrating multimodal cellular data sets and establishing causal relationships. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. The deterministic formalism of bio-realistic models currently precludes accounting for the natural variability observed experimentally. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on synthetic data generated from bio-realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE enables the efficient generation of synthetic neurons that closely resemble experimental data and exhibit trial-to-trial variability, offering a $4200\\times$ speedup over the numerical solver. NOBLE is the first scaled-up deep learning framework that validates its generalization with real experimental data. To this end, NOBLE captures fundamental neural properties in a unique and emergent manner that opens the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.04536v3",
    "published_date": "2025-06-05 01:01:18 UTC",
    "updated_date": "2025-10-27 22:48:13 UTC"
  },
  {
    "arxiv_id": "2506.04534v1",
    "title": "Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs",
    "authors": [
      "William Sheffield",
      "Kanishka Misra",
      "Valentina Pyatkin",
      "Ashwini Deo",
      "Kyle Mahowald",
      "Junyi Jessy Li"
    ],
    "abstract": "Discourse particles are crucial elements that subtly shape the meaning of text. These words, often polyfunctional, give rise to nuanced and often quite disparate semantic/discourse effects, as exemplified by the diverse uses of the particle \"just\" (e.g., exclusive, temporal, emphatic). This work investigates the capacity of LLMs to distinguish the fine-grained senses of English \"just\", a well-studied example in formal semantics, using data meticulously created and labeled by expert linguists. Our findings reveal that while LLMs exhibit some ability to differentiate between broader categories, they struggle to fully capture more subtle nuances, highlighting a gap in their understanding of discourse particles.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To be published in Findings of The 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025). The main paper is 5 pages and contains 3 figures and 1 table. In total, the paper is 12 pages and contains 8 figures and 5 tables (References + Appendix)",
    "pdf_url": "https://arxiv.org/pdf/2506.04534v1",
    "published_date": "2025-06-05 00:59:05 UTC",
    "updated_date": "2025-06-05 00:59:05 UTC"
  },
  {
    "arxiv_id": "2506.05419v1",
    "title": "Dream to Generalize: Zero-Shot Model-Based Reinforcement Learning for Unseen Visual Distractions",
    "authors": [
      "Jeongsoo Ha",
      "Kyungsoo Kim",
      "Yusung Kim"
    ],
    "abstract": "Model-based reinforcement learning (MBRL) has been used to efficiently solve vision-based control tasks in highdimensional image observations. Although recent MBRL algorithms perform well in trained observations, they fail when faced with visual distractions in observations. These task-irrelevant distractions (e.g., clouds, shadows, and light) may be constantly present in real-world scenarios. In this study, we propose a novel self-supervised method, Dream to Generalize (Dr. G), for zero-shot MBRL. Dr. G trains its encoder and world model with dual contrastive learning which efficiently captures task-relevant features among multi-view data augmentations. We also introduce a recurrent state inverse dynamics model that helps the world model to better understand the temporal structure. The proposed methods can enhance the robustness of the world model against visual distractions. To evaluate the generalization performance, we first train Dr. G on simple backgrounds and then test it on complex natural video backgrounds in the DeepMind Control suite, and the randomizing environments in Robosuite. Dr. G yields a performance improvement of 117% and 14% over prior works, respectively. Our code is open-sourced and available at https://github.com/JeongsooHa/DrG.git",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI 2023",
    "pdf_url": "https://arxiv.org/pdf/2506.05419v1",
    "published_date": "2025-06-05 00:39:03 UTC",
    "updated_date": "2025-06-05 00:39:03 UTC"
  },
  {
    "arxiv_id": "2506.05418v1",
    "title": "Self-Predictive Dynamics for Generalization of Vision-based Reinforcement Learning",
    "authors": [
      "Kyungsoo Kim",
      "Jeongsoo Ha",
      "Yusung Kim"
    ],
    "abstract": "Vision-based reinforcement learning requires efficient and robust representations of image-based observations, especially when the images contain distracting (task-irrelevant) elements such as shadows, clouds, and light. It becomes more important if those distractions are not exposed during training. We design a Self-Predictive Dynamics (SPD) method to extract task-relevant features efficiently, even in unseen observations after training. SPD uses weak and strong augmentations in parallel, and learns representations by predicting inverse and forward transitions across the two-way augmented versions. In a set of MuJoCo visual control tasks and an autonomous driving task (CARLA), SPD outperforms previous studies in complex observations, and significantly improves the generalization performance for unseen observations. Our code is available at https://github.com/unigary/SPD.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "IJCAI 2022",
    "pdf_url": "https://arxiv.org/pdf/2506.05418v1",
    "published_date": "2025-06-05 00:36:54 UTC",
    "updated_date": "2025-06-05 00:36:54 UTC"
  }
]
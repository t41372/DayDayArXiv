[
  {
    "arxiv_id": "2506.21611v2",
    "title": "When Does Multimodality Lead to Better Time Series Forecasting?",
    "authors": [
      "Xiyuan Zhang",
      "Boran Han",
      "Haoyang Fang",
      "Abdul Fatir Ansari",
      "Shuai Zhang",
      "Danielle C. Maddix",
      "Cuixiong Hu",
      "Andrew Gordon Wilson",
      "Michael W. Mahoney",
      "Hao Wang",
      "Yan Liu",
      "Huzefa Rangwala",
      "George Karypis",
      "Bernie Wang"
    ],
    "abstract": "Recently, there has been growing interest in incorporating textual information into foundation models for time series forecasting. However, it remains unclear whether and under what conditions such multimodal integration consistently yields gains. We systematically investigate these questions across a diverse benchmark of 16 forecasting tasks spanning 7 domains, including health, environment, and economics. We evaluate two popular multimodal forecasting paradigms: aligning-based methods, which align time series and text representations; and prompting-based methods, which directly prompt large language models for forecasting. Our findings reveal that the benefits of multimodality are highly condition-dependent. While we confirm reported gains in some settings, these improvements are not universal across datasets or models. To move beyond empirical observations, we disentangle the effects of model architectural properties and data characteristics, drawing data-agnostic insights that generalize across domains. Our findings highlight that on the modeling side, incorporating text information is most helpful given (1) high-capacity text models, (2) comparatively weaker time series models, and (3) appropriate aligning strategies. On the data side, performance gains are more likely when (4) sufficient training data is available and (5) the text offers complementary predictive signal beyond what is already captured from the time series alone. Our study offers a rigorous, quantitative foundation for understanding when multimodality can be expected to aid forecasting tasks, and reveals that its benefits are neither universal nor always aligned with intuition.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21611v2",
    "published_date": "2025-06-20 23:55:56 UTC",
    "updated_date": "2025-09-29 20:03:31 UTC"
  },
  {
    "arxiv_id": "2506.17518v1",
    "title": "A Survey of State Representation Learning for Deep Reinforcement Learning",
    "authors": [
      "Ayoub Echchahed",
      "Pablo Samuel Castro"
    ],
    "abstract": "Representation learning methods are an important tool for addressing the challenges posed by complex observations spaces in sequential decision making problems. Recently, many methods have used a wide variety of types of approaches for learning meaningful state representations in reinforcement learning, allowing better sample efficiency, generalization, and performance. This survey aims to provide a broad categorization of these methods within a model-free online setting, exploring how they tackle the learning of state representations differently. We categorize the methods into six main classes, detailing their mechanisms, benefits, and limitations. Through this taxonomy, our aim is to enhance the understanding of this field and provide a guide for new researchers. We also discuss techniques for assessing the quality of representations, and detail relevant future directions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17518v1",
    "published_date": "2025-06-20 23:47:04 UTC",
    "updated_date": "2025-06-20 23:47:04 UTC"
  },
  {
    "arxiv_id": "2507.01034v1",
    "title": "Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya",
    "authors": [
      "Asma Agaal",
      "Mansour Essgaer",
      "Hend M. Farkash",
      "Zulaiha Ali Othman"
    ],
    "abstract": "Accurate electricity forecasting is crucial for grid stability and energy planning, especially in Benghazi, Libya, where frequent load shedding, generation deficits, and infrastructure limitations persist. This study proposes a data-driven approach to forecast electricity load, generation, and deficits for 2025 using historical data from 2019 (a year marked by instability) and 2023 (a more stable year). Multiple time series models were applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural networks. The dataset was enhanced through missing value imputation, outlier smoothing, and log transformation. Performance was assessed using mean squared error, root mean squared error, mean absolute error, and mean absolute percentage error. LSTM outperformed all other models, showing strong capabilities in modeling non-stationary and seasonal patterns. A key contribution of this work is an optimized LSTM framework that integrates exogenous factors such as temperature and humidity, offering robust performance in forecasting multiple electricity indicators. These results provide practical insights for policymakers and grid operators to enable proactive load management and resource planning in data-scarce, volatile regions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This article was published in International Journal of Intelligent Systems and Applications (IJISA) (MECS Press), Vol. 17, No. 3, 8 Jun. 2025, DOI: https://doi.org/10.5815/ijisa.2025.03.05",
    "pdf_url": "https://arxiv.org/pdf/2507.01034v1",
    "published_date": "2025-06-20 23:41:41 UTC",
    "updated_date": "2025-06-20 23:41:41 UTC"
  },
  {
    "arxiv_id": "2506.17514v1",
    "title": "Kaleidoscopic Teaming in Multi Agent Simulations",
    "authors": [
      "Ninareh Mehrabi",
      "Tharindu Kumarage",
      "Kai-Wei Chang",
      "Aram Galstyan",
      "Rahul Gupta"
    ],
    "abstract": "Warning: This paper contains content that may be inappropriate or offensive.\n  AI agents have gained significant recent attention due to their autonomous tool usage capabilities and their integration in various real-world applications. This autonomy poses novel challenges for the safety of such systems, both in single- and multi-agent scenarios. We argue that existing red teaming or safety evaluation frameworks fall short in evaluating safety risks in complex behaviors, thought processes and actions taken by agents. Moreover, they fail to consider risks in multi-agent setups where various vulnerabilities can be exposed when agents engage in complex behaviors and interactions with each other. To address this shortcoming, we introduce the term kaleidoscopic teaming which seeks to capture complex and wide range of vulnerabilities that can happen in agents both in single-agent and multi-agent scenarios. We also present a new kaleidoscopic teaming framework that generates a diverse array of scenarios modeling real-world human societies. Our framework evaluates safety of agents in both single-agent and multi-agent setups. In single-agent setup, an agent is given a scenario that it needs to complete using the tools it has access to. In multi-agent setup, multiple agents either compete against or cooperate together to complete a task in the scenario through which we capture existing safety vulnerabilities in agents. We introduce new in-context optimization techniques that can be used in our kaleidoscopic teaming framework to generate better scenarios for safety analysis. Lastly, we present appropriate metrics that can be used along with our framework to measure safety of agents. Utilizing our kaleidoscopic teaming framework, we identify vulnerabilities in various models with respect to their safety in agentic use-cases.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17514v1",
    "published_date": "2025-06-20 23:37:17 UTC",
    "updated_date": "2025-06-20 23:37:17 UTC"
  },
  {
    "arxiv_id": "2506.17508v2",
    "title": "Mapping the Evolution of Research Contributions using KnoVo",
    "authors": [
      "Sajratul Y. Rubaiat",
      "Syed N. Sakib",
      "Hasan M. Jamil"
    ],
    "abstract": "This paper presents KnoVo (Knowledge Evolution), an intelligent framework designed for quantifying and analyzing the evolution of research novelty in the scientific literature. Moving beyond traditional citation analysis, which primarily measures impact, KnoVo determines a paper's novelty relative to both prior and subsequent work within its multilayered citation network. Given a target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to dynamically extract dimensions of comparison (e.g., methodology, application, dataset). The target paper is then compared to related publications along these same extracted dimensions. This comparative analysis, inspired by tournament selection, yields quantitative novelty scores reflecting the relative improvement, equivalence, or inferiority of the target paper in specific aspects. By aggregating these scores and visualizing their progression, for instance, through dynamic evolution graphs and comparative radar charts, KnoVo facilitates researchers not only to assess originality and identify similar work, but also to track knowledge evolution along specific research dimensions, uncover research gaps, and explore cross-disciplinary connections. We demonstrate these capabilities through a detailed analysis of 20 diverse papers from multiple scientific fields and report on the performance of various open-source LLMs within the KnoVo framework.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.DB",
      "cs.ET",
      "cs.IR"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17508v2",
    "published_date": "2025-06-20 23:17:11 UTC",
    "updated_date": "2025-06-25 06:22:45 UTC"
  },
  {
    "arxiv_id": "2506.17497v1",
    "title": "From Generality to Mastery: Composer-Style Symbolic Music Generation via Large-Scale Pre-training",
    "authors": [
      "Mingyang Yao",
      "Ke Chen"
    ],
    "abstract": "Despite progress in controllable symbolic music generation, data scarcity remains a challenge for certain control modalities. Composer-style music generation is a prime example, as only a few pieces per composer are available, limiting the modeling of both styles and fundamental music elements (e.g., melody, chord, rhythm). In this paper, we investigate how general music knowledge learned from a broad corpus can enhance the mastery of specific composer styles, with a focus on piano piece generation. Our approach follows a two-stage training paradigm. First, we pre-train a REMI-based music generation model on a large corpus of pop, folk, and classical music. Then, we fine-tune it on a small, human-verified dataset from four renowned composers, namely Bach, Mozart, Beethoven, and Chopin, using a lightweight adapter module to condition the model on style indicators. To evaluate the effectiveness of our approach, we conduct both objective and subjective evaluations on style accuracy and musicality. Experimental results demonstrate that our method outperforms ablations and baselines, achieving more precise composer-style modeling and better musical aesthetics. Additionally, we provide observations on how the model builds music concepts from the generality pre-training and refines its stylistic understanding through the mastery fine-tuning.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Proceedings of the 6th Conference on AI Music Creativity, AIMC 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17497v1",
    "published_date": "2025-06-20 22:20:59 UTC",
    "updated_date": "2025-06-20 22:20:59 UTC"
  },
  {
    "arxiv_id": "2506.17491v1",
    "title": "Exploring Strategies for Personalized Radiation Therapy Part II Predicting Tumor Drift Patterns with Diffusion Models",
    "authors": [
      "Hao Peng",
      "Steve Jiang",
      "Robert Timmerman"
    ],
    "abstract": "Radiation therapy outcomes are decided by two key parameters, dose and timing, whose best values vary substantially across patients. This variability is especially critical in the treatment of brain cancer, where fractionated or staged stereotactic radiosurgery improves safety compared to single fraction approaches, but complicates the ability to predict treatment response. To address this challenge, we employ Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy (PULSAR), a strategy that dynamically adjusts treatment based on how each tumor evolves over time. However, the success of PULSAR and other adaptive approaches depends on predictive tools that can guide early treatment decisions and avoid both overtreatment and undertreatment. However, current radiomics and dosiomics models offer limited insight into the evolving spatial and temporal patterns of tumor response. To overcome these limitations, we propose a novel framework using Denoising Diffusion Implicit Models (DDIM), which learns data-driven mappings from pre to post treatment imaging. In this study, we developed single step and iterative denoising strategies and compared their performance. The results show that diffusion models can effectively simulate patient specific tumor evolution and localize regions associated with treatment response. The proposed strategy provides a promising foundation for modeling heterogeneous treatment response and enabling early, adaptive interventions, paving the way toward more personalized and biologically informed radiotherapy.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17491v1",
    "published_date": "2025-06-20 21:58:42 UTC",
    "updated_date": "2025-06-20 21:58:42 UTC"
  },
  {
    "arxiv_id": "2506.17486v2",
    "title": "Distilling On-device Language Models for Robot Planning with Minimal Human Intervention",
    "authors": [
      "Zachary Ravichandran",
      "Ignacio Hounie",
      "Fernando Cladera",
      "Alejandro Ribeiro",
      "George J. Pappas",
      "Vijay Kumar"
    ],
    "abstract": "Large language models (LLMs) provide robots with powerful contextual reasoning abilities and a natural human interface. Yet, current LLM-enabled robots typically depend on cloud-hosted models, limiting their usability in environments with unreliable communication infrastructure, such as outdoor or industrial settings. We present PRISM, a framework for distilling small language model (SLM)-enabled robot planners that run on-device with minimal human supervision. Starting from an existing LLM-enabled planner, PRISM automatically synthesizes diverse tasks and environments, elicits plans from the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in replacement of the source model. We apply PRISM to three LLM-enabled planners for mapping and exploration, manipulation, and household assistance, and we demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of GPT-4o's performance to over 93% - using only synthetic data. We further demonstrate that the distilled planners generalize across heterogeneous robotic platforms (ground and aerial) and diverse environments (indoor and outdoor). We release all software, trained models, and datasets at https://zacravichandran.github.io/PRISM.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to the Conference on Robot Learning (CoRL) 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17486v2",
    "published_date": "2025-06-20 21:44:27 UTC",
    "updated_date": "2025-10-06 20:11:27 UTC"
  },
  {
    "arxiv_id": "2506.17484v1",
    "title": "From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases",
    "authors": [
      "Yao Zhang",
      "Zaixi Shang",
      "Silpan Patel",
      "Mikel Zuniga"
    ],
    "abstract": "Supply chain operations generate vast amounts of operational data; however, critical knowledge such as system usage practices, troubleshooting workflows, and resolution techniques often remains buried within unstructured communications like support tickets, emails, and chat logs. While RAG systems aim to leverage such communications as a knowledge base, their effectiveness is limited by raw data challenges: support tickets are typically noisy, inconsistent, and incomplete, making direct retrieval suboptimal. Unlike existing RAG approaches that focus on runtime optimization, we introduce a novel offline-first methodology that transforms these communications into a structured knowledge base. Our key innovation is a LLMs-based multi-agent system orchestrating three specialized agents: Category Discovery for taxonomy creation, Categorization for ticket grouping, and Knowledge Synthesis for article generation. Applying our methodology to real-world support tickets with resolution notes and comments, our system creates a compact knowledge base - reducing total volume to just 3.4% of original ticket data while improving quality. Experiments demonstrate that our prebuilt knowledge base in RAG systems significantly outperforms traditional RAG implementations (48.74% vs. 38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses. By automating institutional knowledge capture that typically remains siloed in experts' heads, our solution translates to substantial operational efficiency: reducing support workload, accelerating resolution times, and creating self-improving systems that automatically resolve approximately 50% of future supply chain tickets. Our approach addresses a key gap in knowledge management by transforming transient communications into structured, reusable knowledge through intelligent offline processing rather than latency-inducing runtime architectures.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted In Proceedings of the 1st Workshop on AI for Supply Chain: Today and Future @ 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2 (KDD 25), August 3, 2025, Toronto, ON, Canada. ACM, New York, NY, USA, 14 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.17484v1",
    "published_date": "2025-06-20 21:38:06 UTC",
    "updated_date": "2025-06-20 21:38:06 UTC"
  },
  {
    "arxiv_id": "2506.17467v1",
    "title": "Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems",
    "authors": [
      "Weixin Liang"
    ],
    "abstract": "Large language models (LLMs) have shown significant potential to change how we write, communicate, and create, leading to rapid adoption across society. This dissertation examines how individuals and institutions are adapting to and engaging with this emerging technology through three research directions. First, I demonstrate how the institutional adoption of AI detectors introduces systematic biases, particularly disadvantaging writers of non-dominant language varieties, highlighting critical equity concerns in AI governance. Second, I present novel population-level algorithmic approaches that measure the increasing adoption of LLMs across writing domains, revealing consistent patterns of AI-assisted content in academic peer reviews, scientific publications, consumer complaints, corporate communications, job postings, and international organization press releases. Finally, I investigate LLMs' capability to provide feedback on research manuscripts through a large-scale empirical analysis, offering insights into their potential to support researchers who face barriers in accessing timely manuscript feedback, particularly early-career researchers and those from under-resourced settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Stanford CS PhD Dissertation",
    "pdf_url": "https://arxiv.org/pdf/2506.17467v1",
    "published_date": "2025-06-20 20:15:09 UTC",
    "updated_date": "2025-06-20 20:15:09 UTC"
  },
  {
    "arxiv_id": "2506.17466v1",
    "title": "FedNAMs: Performing Interpretability Analysis in Federated Learning Context",
    "authors": [
      "Amitash Nanda",
      "Sree Bhargavi Balija",
      "Debashis Sahoo"
    ],
    "abstract": "Federated learning continues to evolve but faces challenges in interpretability and explainability. To address these challenges, we introduce a novel approach that employs Neural Additive Models (NAMs) within a federated learning framework. This new Federated Neural Additive Models (FedNAMs) approach merges the advantages of NAMs, where individual networks concentrate on specific input features, with the decentralized approach of federated learning, ultimately producing interpretable analysis results. This integration enhances privacy by training on local data across multiple devices, thereby minimizing the risks associated with data centralization and improving model robustness and generalizability. FedNAMs maintain detailed, feature-specific learning, making them especially valuable in sectors such as finance and healthcare. They facilitate the training of client-specific models to integrate local updates, preserve privacy, and mitigate concerns related to centralization. Our studies on various text and image classification tasks, using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show that FedNAMs deliver strong interpretability with minimal accuracy loss compared to traditional Federated Deep Neural Networks (DNNs). The research involves notable findings, including the identification of critical predictive features at both client and global levels. Volatile acidity, sulfates, and chlorides for wine quality. Chest pain type, maximum heart rate, and number of vessels for heart disease. Petal length and width for iris classification. This approach strengthens privacy and model efficiency and improves interpretability and robustness across diverse datasets. Finally, FedNAMs generate insights on causes of highly and low interpretable features.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.17466v1",
    "published_date": "2025-06-20 20:14:13 UTC",
    "updated_date": "2025-06-20 20:14:13 UTC"
  },
  {
    "arxiv_id": "2506.17462v2",
    "title": "General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting",
    "authors": [
      "Bernard Lange",
      "Anil Yildiz",
      "Mansur Arief",
      "Shehryar Khattak",
      "Mykel Kochenderfer",
      "Georgios Georgakis"
    ],
    "abstract": "Developing general-purpose navigation policies for unknown environments remains a core challenge in robotics. Most existing systems rely on task-specific neural networks and fixed information flows, limiting their generalizability. Large Vision-Language Models (LVLMs) offer a promising alternative by embedding human-like knowledge for reasoning and planning, but prior LVLM-robot integrations have largely depended on pre-mapped spaces, hard-coded representations, and rigid control logic. We introduce the Agentic Robotic Navigation Architecture (ARNA), a general-purpose framework that equips an LVLM-based agent with a library of perception, reasoning, and navigation tools drawn from modern robotic stacks. At runtime, the agent autonomously defines and executes task-specific workflows that iteratively query modules, reason over multimodal inputs, and select navigation actions. This agentic formulation enables robust navigation and reasoning in previously unmapped environments, offering a new perspective on robotic stack design. Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA outperforms state-of-the-art EQA-specific approaches. Qualitative results on RxR and custom tasks further demonstrate its ability to generalize across a broad range of navigation challenges.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17462v2",
    "published_date": "2025-06-20 20:06:14 UTC",
    "updated_date": "2025-10-17 03:19:22 UTC"
  },
  {
    "arxiv_id": "2506.17449v1",
    "title": "OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections",
    "authors": [
      "Manasa Bharadwaj",
      "Nikhil Verma",
      "Kevin Ferreira"
    ],
    "abstract": "Efforts to improve Large Language Model (LLM) agent performance on complex tasks have largely focused on fine-tuning and iterative self-correction. However, these approaches often lack generalizable mechanisms for longterm learning and remain inefficient in dynamic environments. We introduce OmniReflect, a hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent. OmniReflect operates in two modes: Self-sustaining, where a single agent periodically curates its own reflections during task execution, and Co-operative, where a Meta-advisor derives a constitution from a small calibration set to guide another agent. To construct these constitutional principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering a balance between contextual adaptability and computational efficiency. Empirical results averaged across models show major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion baselines on BabyAI. These findings highlight the robustness and effectiveness of OmniReflect across environments and backbones.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17449v1",
    "published_date": "2025-06-20 19:38:21 UTC",
    "updated_date": "2025-06-20 19:38:21 UTC"
  },
  {
    "arxiv_id": "2506.17442v2",
    "title": "Keeping Medical AI Healthy and Trustworthy: A Review of Detection and Correction Methods for System Degradation",
    "authors": [
      "Hao Guan",
      "David Bates",
      "Li Zhou"
    ],
    "abstract": "Artificial intelligence (AI) is increasingly integrated into modern healthcare, offering powerful support for clinical decision-making. However, in real-world settings, AI systems may experience performance degradation over time, due to factors such as shifting data distributions, changes in patient characteristics, evolving clinical protocols, and variations in data quality. These factors can compromise model reliability, posing safety concerns and increasing the likelihood of inaccurate predictions or adverse outcomes. This review presents a forward-looking perspective on monitoring and maintaining the \"health\" of AI systems in healthcare. We highlight the urgent need for continuous performance monitoring, early degradation detection, and effective self-correction mechanisms. The paper begins by reviewing common causes of performance degradation at both data and model levels. We then summarize key techniques for detecting data and model drift, followed by an in-depth look at root cause analysis. Correction strategies are further reviewed, ranging from model retraining to test-time adaptation. Our survey spans both traditional machine learning models and state-of-the-art large language models (LLMs), offering insights into their strengths and limitations. Finally, we discuss ongoing technical challenges and propose future research directions. This work aims to guide the development of reliable, robust medical AI systems capable of sustaining safe, long-term deployment in dynamic clinical settings.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.17442v2",
    "published_date": "2025-06-20 19:22:07 UTC",
    "updated_date": "2025-12-02 01:53:21 UTC"
  },
  {
    "arxiv_id": "2506.17434v1",
    "title": "Resource Rational Contractualism Should Guide AI Alignment",
    "authors": [
      "Sydney Levine",
      "Matija Franklin",
      "Tan Zhi-Xuan",
      "Secil Yanik Guyot",
      "Lionel Wong",
      "Daniel Kilov",
      "Yejin Choi",
      "Joshua B. Tenenbaum",
      "Noah Goodman",
      "Seth Lazar",
      "Iason Gabriel"
    ],
    "abstract": "AI systems will soon have to navigate human environments and make decisions that affect people and other AI agents whose goals and values diverge. Contractualist alignment proposes grounding those decisions in agreements that diverse stakeholders would endorse under the right conditions, yet securing such agreement at scale remains costly and slow -- even for advanced AI. We therefore propose Resource-Rational Contractualism (RRC): a framework where AI systems approximate the agreements rational parties would form by drawing on a toolbox of normatively-grounded, cognitively-inspired heuristics that trade effort for accuracy. An RRC-aligned agent would not only operate efficiently, but also be equipped to dynamically adapt to and interpret the ever-changing human social world.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.17434v1",
    "published_date": "2025-06-20 18:57:13 UTC",
    "updated_date": "2025-06-20 18:57:13 UTC"
  },
  {
    "arxiv_id": "2507.05263v1",
    "title": "Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization",
    "authors": [
      "Kaichen Ouyang"
    ],
    "abstract": "Graph Neural Networks (GNNs) have shown great potential in graph data analysis due to their powerful representation capabilities. However, as the network depth increases, the issue of over-smoothing becomes more severe, causing node representations to lose their distinctiveness. This paper analyzes the mechanism of over-smoothing through the analogy to Anderson localization and introduces participation degree as a metric to quantify this phenomenon. Specifically, as the depth of the GNN increases, node features homogenize after multiple layers of message passing, leading to a loss of distinctiveness, similar to the behavior of vibration modes in disordered systems. In this context, over-smoothing in GNNs can be understood as the expansion of low-frequency modes (increased participation degree) and the localization of high-frequency modes (decreased participation degree). Based on this, we systematically reviewed the potential connection between the Anderson localization behavior in disordered systems and the over-smoothing behavior in Graph Neural Networks. A theoretical analysis was conducted, and we proposed the potential of alleviating over-smoothing by reducing the disorder in information propagation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.05263v1",
    "published_date": "2025-06-20 18:54:10 UTC",
    "updated_date": "2025-06-20 18:54:10 UTC"
  },
  {
    "arxiv_id": "2506.17425v1",
    "title": "Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction",
    "authors": [
      "Minmin Yang",
      "Huantao Ren",
      "Senem Velipasalar"
    ],
    "abstract": "Cone-beam computed tomography (CBCT) using only a few X-ray projection views enables faster scans with lower radiation dose, but the resulting severe under-sampling causes strong artifacts and poor spatial coverage. We address these challenges in a unified framework. First, we replace conventional UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model. Convolutional layers capture local details, while self-attention layers enhance global context. We adapt TransUNet to CBCT by combining multi-scale features, querying view-specific features per 3D point, and adding a lightweight attenuation-prediction head. This yields Trans-CBCT, which surpasses prior baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views. Second, we introduce a neighbor-aware Point Transformer to enforce volumetric coherence. This module uses 3D positional encoding and attention over k-nearest neighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT, provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on LUNA16 and ToothFairy show consistent gains from six to ten views, validating the effectiveness of combining CNN-Transformer features with point-based geometry reasoning for sparse-view CBCT reconstruction.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17425v1",
    "published_date": "2025-06-20 18:45:12 UTC",
    "updated_date": "2025-06-20 18:45:12 UTC"
  },
  {
    "arxiv_id": "2506.17419v1",
    "title": "UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making",
    "authors": [
      "Jinhao Duan",
      "James Diffenderfer",
      "Sandeep Madireddy",
      "Tianlong Chen",
      "Bhavya Kailkhura",
      "Kaidi Xu"
    ],
    "abstract": "As Large Language Models (LLMs) are integrated into safety-critical applications involving sequential decision-making in the real world, it is essential to know when to trust LLM decisions. Existing LLM Uncertainty Quantification (UQ) methods are primarily designed for single-turn question-answering formats, resulting in multi-step decision-making scenarios, e.g., LLM agentic system, being underexplored. In this paper, we introduce a principled, information-theoretic framework that decomposes LLM sequential decision uncertainty into two parts: (i) internal uncertainty intrinsic to the current decision, which is focused on existing UQ methods, and (ii) extrinsic uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty should be inherited from preceding decisions. We then propose UProp, an efficient and effective extrinsic uncertainty estimator that converts the direct estimation of MI to the estimation of Pointwise Mutual Information (PMI) over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is evaluated over extensive multi-step decision-making benchmarks, e.g., AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and DeepSeek-V3. Experimental results demonstrate that UProp significantly outperforms existing single-turn UQ baselines equipped with thoughtful aggregation strategies. Moreover, we provide a comprehensive analysis of UProp, including sampling efficiency, potential applications, and intermediate uncertainty propagation, to demonstrate its effectiveness. Codes will be available at https://github.com/jinhaoduan/UProp.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 5 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.17419v1",
    "published_date": "2025-06-20 18:34:04 UTC",
    "updated_date": "2025-06-20 18:34:04 UTC"
  },
  {
    "arxiv_id": "2506.17219v2",
    "title": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning",
    "authors": [
      "Yanzhi Zhang",
      "Zhaoxi Zhang",
      "Haoxiang Guan",
      "Yilin Cheng",
      "Yitong Duan",
      "Chen Wang",
      "Yue Wang",
      "Shuxin Zheng",
      "Jiyan He"
    ],
    "abstract": "Reinforcement learning has emerged as a powerful paradigm for post-training large language models (LLMs) to improve reasoning. Approaches like Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) have shown strong results, but they require extensive external supervision. We investigate an alternative class of methods, Reinforcement Learning from Internal Feedback (RLIF), which relies solely on intrinsic model-derived signals instead of external rewards. In particular, we leverage unsupervised reward proxies such as token-level entropy, trajectory-level entropy, and self-certainty. Our theoretical analysis shows these internal objectives are partially equivalent, and we empirically evaluate various RLIF strategies on challenging math reasoning benchmarks. Experimental results demonstrate that RLIF can boost the reasoning performance of base LLMs at the beginning phase of the training, matching or surpassing RLVR techniques on these tasks. However, when training progresses, performance degrades even below the model before training. Moreover, we find that RLIF yields little improvement for instruction-tuned models, indicating diminishing returns of intrinsic feedback once an LLM is already instruction-tuned. We further analyze this limitation by mixing model weights and explain the reason of RLIF's training behaviors, providing practical guidelines for integrating internal feedback signals into LLM training. We hope our analysis of internal feedback will inform more principled and effective strategies for LLM post-training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17219v2",
    "published_date": "2025-06-20 17:59:52 UTC",
    "updated_date": "2025-06-25 13:27:49 UTC"
  },
  {
    "arxiv_id": "2506.17218v1",
    "title": "Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens",
    "authors": [
      "Zeyuan Yang",
      "Xueyang Yu",
      "Delin Chen",
      "Maohao Shen",
      "Chuang Gan"
    ],
    "abstract": "Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery-the internal construction and manipulation of visual cues-we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to ``think visually'', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://vlm-mirage.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2506.17218v1",
    "published_date": "2025-06-20 17:59:31 UTC",
    "updated_date": "2025-06-20 17:59:31 UTC"
  },
  {
    "arxiv_id": "2506.17213v2",
    "title": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation",
    "authors": [
      "Xiuyu Yang",
      "Shuhan Tan",
      "Philipp Krähenbühl"
    ],
    "abstract": "An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "ICCV 2025. Project page: https://orangesodahub.github.io/InfGen Code: https://github.com/OrangeSodahub/infgen",
    "pdf_url": "https://arxiv.org/pdf/2506.17213v2",
    "published_date": "2025-06-20 17:59:21 UTC",
    "updated_date": "2025-08-05 07:06:16 UTC"
  },
  {
    "arxiv_id": "2506.17212v1",
    "title": "Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting",
    "authors": [
      "Tianjiao Yu",
      "Vedant Shah",
      "Muntasir Wahed",
      "Ying Shen",
      "Kiet A. Nguyen",
      "Ismini Lourentzou"
    ],
    "abstract": "Articulated objects are common in the real world, yet modeling their structure and motion remains a challenging task for 3D reconstruction methods. In this work, we introduce Part$^{2}$GS, a novel framework for modeling articulated digital twins of multi-part objects with high-fidelity geometry and physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D Gaussian representation that encodes articulated components with learnable attributes, enabling structured, disentangled transformations that preserve high-fidelity geometry. To ensure physically consistent motion, we propose a motion-aware canonical representation guided by physics-based constraints, including contact enforcement, velocity consistency, and vector-field alignment. Furthermore, we introduce a field of repel points to prevent part collisions and maintain stable articulation paths, significantly improving motion coherence over baselines. Extensive evaluations on both synthetic and real-world datasets show that Part$^{2}$GS consistently outperforms state-of-the-art methods by up to 10$\\times$ in Chamfer Distance for movable parts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17212v1",
    "published_date": "2025-06-20 17:59:12 UTC",
    "updated_date": "2025-06-20 17:59:12 UTC"
  },
  {
    "arxiv_id": "2506.17208v2",
    "title": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems",
    "authors": [
      "Matias Martinez",
      "Xavier Franch"
    ],
    "abstract": "The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards -- SWE-Bench Lite and SWE-Bench Verified -- have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (79 entries) and Verified (99 entries) leaderboards, analyzing 80 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17208v2",
    "published_date": "2025-06-20 17:57:08 UTC",
    "updated_date": "2025-08-18 12:55:57 UTC"
  },
  {
    "arxiv_id": "2506.17204v1",
    "title": "Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning",
    "authors": [
      "Guozheng Ma",
      "Lu Li",
      "Zilin Wang",
      "Li Shen",
      "Pierre-Luc Bacon",
      "Dacheng Tao"
    ],
    "abstract": "Effectively scaling up deep reinforcement learning models has proven notoriously difficult due to network pathologies during training, motivating various targeted interventions such as periodic reset and architectural advances such as layer normalization. Instead of pursuing more complex modifications, we show that introducing static network sparsity alone can unlock further scaling potential beyond their dense counterparts with state-of-the-art architectures. This is achieved through simple one-shot random pruning, where a predetermined percentage of network weights are randomly removed once before training. Our analysis reveals that, in contrast to naively scaling up dense DRL networks, such sparse networks achieve both higher parameter efficiency for network expressivity and stronger resistance to optimization challenges like plasticity loss and gradient interference. We further extend our evaluation to visual and streaming RL scenarios, demonstrating the consistent benefits of network sparsity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17204v1",
    "published_date": "2025-06-20 17:54:24 UTC",
    "updated_date": "2025-06-20 17:54:24 UTC"
  },
  {
    "arxiv_id": "2506.17191v1",
    "title": "Facial Landmark Visualization and Emotion Recognition Through Neural Networks",
    "authors": [
      "Israel Juárez-Jiménez",
      "Tiffany Guadalupe Martínez Paredes",
      "Jesús García-Ramírez",
      "Eric Ramos Aguilar"
    ],
    "abstract": "Emotion recognition from facial images is a crucial task in human-computer interaction, enabling machines to learn human emotions through facial expressions. Previous studies have shown that facial images can be used to train deep learning models; however, most of these studies do not include a through dataset analysis. Visualizing facial landmarks can be challenging when extracting meaningful dataset insights; to address this issue, we propose facial landmark box plots, a visualization technique designed to identify outliers in facial datasets. Additionally, we compare two sets of facial landmark features: (i) the landmarks' absolute positions and (ii) their displacements from a neutral expression to the peak of an emotional expression. Our results indicate that a neural network achieves better performance than a random forest classifier.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Best paper Award COMIA 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17191v1",
    "published_date": "2025-06-20 17:45:34 UTC",
    "updated_date": "2025-06-20 17:45:34 UTC"
  },
  {
    "arxiv_id": "2506.17188v1",
    "title": "Towards AI Search Paradigm",
    "authors": [
      "Yuchen Li",
      "Hengyi Cai",
      "Rui Kong",
      "Xinran Chen",
      "Jiamin Chen",
      "Jun Yang",
      "Haojie Zhang",
      "Jiayi Li",
      "Jiayi Wu",
      "Yiqun Chen",
      "Changle Qu",
      "Keyi Kong",
      "Wenwen Ye",
      "Lixin Su",
      "Xinyu Ma",
      "Long Xia",
      "Daiting Shi",
      "Jiashu Zhao",
      "Haoyi Xiong",
      "Shuaiqiang Wang",
      "Dawei Yin"
    ],
    "abstract": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17188v1",
    "published_date": "2025-06-20 17:42:13 UTC",
    "updated_date": "2025-06-20 17:42:13 UTC"
  },
  {
    "arxiv_id": "2506.17375v1",
    "title": "Challenges in Grounding Language in the Real World",
    "authors": [
      "Peter Lindes",
      "Kaoutar Skiker"
    ],
    "abstract": "A long-term goal of Artificial Intelligence is to build a language understanding system that allows a human to collaborate with a physical robot using language that is natural to the human. In this paper we highlight some of the challenges in doing this, and propose a solution that integrates the abilities of a cognitive agent capable of interactive task learning in a physical robot with the linguistic abilities of a large language model. We also point the way to an initial implementation of this approach.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "14 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.17375v1",
    "published_date": "2025-06-20 17:17:53 UTC",
    "updated_date": "2025-06-20 17:17:53 UTC"
  },
  {
    "arxiv_id": "2506.17169v2",
    "title": "Continual Learning with Columnar Spiking Neural Networks",
    "authors": [
      "Denis Larionov",
      "Nikolay Bazenkov",
      "Mikhail Kiselev"
    ],
    "abstract": "Continual learning is a key feature of biological neural systems, but artificial neural networks often suffer from catastrophic forgetting. Instead of backpropagation, biologically plausible learning algorithms may enable stable continual learning. This study proposes columnar-organized spiking neural networks (SNNs) with local learning rules for continual learning and catastrophic forgetting. Using CoLaNET (Columnar Layered Network), we show that its microcolumns adapt most efficiently to new tasks when they lack shared structure with prior learning. We demonstrate how CoLaNET hyperparameters govern the trade-off between retaining old knowledge (stability) and acquiring new information (plasticity). We evaluate CoLaNET on two benchmarks: Permuted MNIST (ten sequential pixel-permuted tasks) and a two-task MNIST/EMNIST setup. Our model learns ten sequential tasks effectively, maintaining 92% accuracy on each. It shows low forgetting, with only 4% performance degradation on the first task after training on nine subsequent tasks.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "14 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.17169v2",
    "published_date": "2025-06-20 17:13:38 UTC",
    "updated_date": "2025-08-16 04:15:57 UTC"
  },
  {
    "arxiv_id": "2506.17165v1",
    "title": "Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network",
    "authors": [
      "Mahin Montasir Afif",
      "Abdullah Al Noman",
      "K. M. Tahsin Kabir",
      "Md. Mortuza Ahmmed",
      "Md. Mostafizur Rahman",
      "Mufti Mahmud",
      "Md. Ashraful Babu"
    ],
    "abstract": "Generative Adversarial Networks (GAN) have shown potential in expanding limited medical imaging datasets. This study explores how different ratios of GAN-generated and real brain tumor MRI images impact the performance of a CNN in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic images which were mixed with real ones at various ratios to train a custom CNN. The CNN was then evaluated on a separate real-world test set. Our results indicate that the model maintains high sensitivity and precision in tumor classification, even when trained predominantly on synthetic data. When only a small portion of GAN data was added, such as 900 real images and 100 GAN images, the model achieved excellent performance, with test accuracy reaching 95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the proportion of GAN images increased further, performance gradually declined. This study suggests that while GANs are useful for augmenting limited datasets especially when real data is scarce, too much synthetic data can introduce artifacts that affect the model's ability to generalize to real world cases.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "This papaer has been submitted to The 18th International Conference on Brain Informatics (BI'25), Italy",
    "pdf_url": "https://arxiv.org/pdf/2506.17165v1",
    "published_date": "2025-06-20 17:12:03 UTC",
    "updated_date": "2025-06-20 17:12:03 UTC"
  },
  {
    "arxiv_id": "2506.17374v2",
    "title": "From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge",
    "authors": [
      "Muhammad Tayyab Khan",
      "Lequn Chen",
      "Zane Yong",
      "Jun Ming Tan",
      "Wenhe Feng",
      "Seung Ki Moon"
    ],
    "abstract": "Efficient and accurate extraction of key information from 2D engineering drawings is essential for advancing digital manufacturing workflows. Such information includes geometric dimensioning and tolerancing (GD&T), measures, material specifications, and textual annotations. Manual extraction is slow and labor-intensive, while generic OCR models often fail due to complex layouts, engineering symbols, and rotated text, leading to incomplete and unreliable outputs. These limitations result in incomplete and unreliable outputs. To address these challenges, we propose a hybrid vision-language framework that integrates a rotation-aware object detection model (YOLOv11-obb) with a transformer-based vision-language parser. Our structured pipeline applies YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB) patches, which are then parsed into structured outputs using a fine-tuned, lightweight vision-language model (VLM). We curate a dataset of 1,367 2D mechanical drawings annotated across nine key categories. YOLOv11-OBB is trained on this dataset to detect OBBs and extract annotation patches. These are parsed using two open-source VLMs: Donut and Florence-2. Both models are lightweight and well-suited for specialized industrial tasks under limited computational overhead. Following fine-tuning of both models on the curated dataset of image patches paired with structured annotation labels, a comparative experiment is conducted to evaluate parsing performance across four key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2% recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a case study demonstrates how the extracted structured information supports downstream manufacturing tasks such as process and tool selection, showcasing the practical utility of the proposed framework in modernizing 2D drawing interpretation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint submitted to Elsevier",
    "pdf_url": "https://arxiv.org/pdf/2506.17374v2",
    "published_date": "2025-06-20 17:10:01 UTC",
    "updated_date": "2025-09-28 02:44:47 UTC"
  },
  {
    "arxiv_id": "2506.17163v1",
    "title": "The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making",
    "authors": [
      "Abinitha Gourabathina",
      "Yuexing Hao",
      "Walter Gerych",
      "Marzyeh Ghassemi"
    ],
    "abstract": "Clinical robustness is critical to the safe deployment of medical Large Language Models (LLMs), but key questions remain about how LLMs and humans may differ in response to the real-world variability typified by clinical settings. To address this, we introduce MedPerturb, a dataset designed to systematically evaluate medical LLMs under controlled perturbations of clinical input. MedPerturb consists of clinical vignettes spanning a range of pathologies, each transformed along three axes: (1) gender modifications (e.g., gender-swapping or gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial tone); and (3) format changes (e.g., LLM-generated multi-turn conversations or summaries). With MedPerturb, we release a dataset of 800 clinical contexts grounded in realistic input variability, outputs from four LLMs, and three human expert reads per clinical context. We use MedPerturb in two case studies to reveal how shifts in gender identity cues, language style, or format reflect diverging treatment selections between humans and LLMs. We find that LLMs are more sensitive to gender and style perturbations while human annotators are more sensitive to LLM-generated format perturbations such as clinical summaries. Our results highlight the need for evaluation frameworks that go beyond static benchmarks to assess the similarity between human clinician and LLM decisions under the variability characteristic of clinical settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17163v1",
    "published_date": "2025-06-20 17:09:27 UTC",
    "updated_date": "2025-06-20 17:09:27 UTC"
  },
  {
    "arxiv_id": "2506.17155v2",
    "title": "Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity",
    "authors": [
      "Samin Yeasar Arnob",
      "Scott Fujimoto",
      "Doina Precup"
    ],
    "abstract": "In this paper, we investigate the use of small datasets in the context of offline reinforcement learning (RL). While many common offline RL benchmarks employ datasets with over a million data points, many offline RL applications rely on considerably smaller datasets. We show that offline RL algorithms can overfit on small datasets, resulting in poor performance. To address this challenge, we introduce \"Sparse-Reg\": a regularization technique based on sparsity to mitigate overfitting in offline reinforcement learning, enabling effective learning in limited data settings and outperforming state-of-the-art baselines in continuous control.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17155v2",
    "published_date": "2025-06-20 16:57:59 UTC",
    "updated_date": "2025-06-26 21:55:13 UTC"
  },
  {
    "arxiv_id": "2506.17144v2",
    "title": "Do We Need Large VLMs for Spotting Soccer Actions?",
    "authors": [
      "Ritabrata Chakraborty",
      "Rajatsubhra Chakraborty",
      "Avijit Dasgupta",
      "Sandeep Chaurasia"
    ],
    "abstract": "Traditional video-based tasks like soccer action spotting rely heavily on visual inputs, often requiring complex and computationally expensive models to process dense video data. We propose a shift from this video-centric approach to a text-based task, making it lightweight and scalable by utilizing Large Language Models (LLMs) instead of Vision-Language Models (VLMs). We posit that expert commentary, which provides rich descriptions and contextual cues contains sufficient information to reliably spot key actions in a match. To demonstrate this, we employ a system of three LLMs acting as judges specializing in outcome, excitement, and tactics for spotting actions in soccer matches. Our experiments show that this language-centric approach performs effectively in detecting critical match events coming close to state-of-the-art video-based spotters while using zero video processing compute and similar amount of time to process the entire match.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.17144v2",
    "published_date": "2025-06-20 16:45:54 UTC",
    "updated_date": "2025-09-27 15:21:30 UTC"
  },
  {
    "arxiv_id": "2506.17140v1",
    "title": "MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification",
    "authors": [
      "David Jacob Drexlin",
      "Jonas Dippel",
      "Julius Hense",
      "Niklas Prenißl",
      "Grégoire Montavon",
      "Frederick Klauschen",
      "Klaus-Robert Müller"
    ],
    "abstract": "Deep learning models have made significant advances in histological prediction tasks in recent years. However, for adaptation in clinical practice, their lack of robustness to varying conditions such as staining, scanner, hospital, and demographics is still a limiting factor: if trained on overrepresented subpopulations, models regularly struggle with less frequent patterns, leading to shortcut learning and biased predictions. Large-scale foundation models have not fully eliminated this issue. Therefore, we propose a novel approach explicitly modeling such metadata into a Metadata-guided generative Diffusion model framework (MeDi). MeDi allows for a targeted augmentation of underrepresented subpopulations with synthetic data, which balances limited training data and mitigates biases in downstream models. We experimentally show that MeDi generates high-quality histopathology images for unseen subpopulations in TCGA, boosts the overall fidelity of the generated images, and enables improvements in performance for downstream classifiers on datasets with subpopulation shifts. Our work is a proof-of-concept towards better mitigating data biases with generative models.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17140v1",
    "published_date": "2025-06-20 16:41:25 UTC",
    "updated_date": "2025-06-20 16:41:25 UTC"
  },
  {
    "arxiv_id": "2506.17139v3",
    "title": "Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models",
    "authors": [
      "Michael Plainer",
      "Hao Wu",
      "Leon Klein",
      "Stephan Günnemann",
      "Frank Noé"
    ],
    "abstract": "In recent years, diffusion models trained on equilibrium molecular distributions have proven effective for sampling biomolecules. Beyond direct sampling, the score of such a model can also be used to derive the forces that act on molecular systems. However, while classical diffusion sampling usually recovers the training distribution, the corresponding energy-based interpretation of the learned score is often inconsistent with this distribution, even for low-dimensional toy systems. We trace this inconsistency to inaccuracies of the learned score at very small diffusion timesteps, where the model must capture the correct evolution of the data distribution. In this regime, diffusion models fail to satisfy the Fokker-Planck equation, which governs the evolution of the score. We interpret this deviation as one source of the observed inconsistencies and propose an energy-based diffusion model with a Fokker-Planck-derived regularization term to enforce consistency. We demonstrate our approach by sampling and simulating multiple biomolecular systems, including fast-folding proteins, and by introducing a state-of-the-art transferable Boltzmann emulator for dipeptides that supports simulation and achieves improved consistency and efficient sampling. Our code, model weights, and self-contained JAX and PyTorch notebooks are available at https://github.com/noegroup/ScoreMD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "physics.comp-ph",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2506.17139v3",
    "published_date": "2025-06-20 16:38:29 UTC",
    "updated_date": "2026-01-14 10:46:14 UTC"
  },
  {
    "arxiv_id": "2506.17133v1",
    "title": "Robust Training with Data Augmentation for Medical Imaging Classification",
    "authors": [
      "Josué Martínez-Martínez",
      "Olivia Brown",
      "Mostafa Karami",
      "Sheida Nabavi"
    ],
    "abstract": "Deep neural networks are increasingly being used to detect and diagnose medical conditions using medical imaging. Despite their utility, these models are highly vulnerable to adversarial attacks and distribution shifts, which can affect diagnostic reliability and undermine trust among healthcare professionals. In this study, we propose a robust training algorithm with data augmentation (RTDA) to mitigate these vulnerabilities in medical image classification. We benchmark classifier robustness against adversarial perturbations and natural variations of RTDA and six competing baseline techniques, including adversarial training and data augmentation approaches in isolation and combination, using experimental data sets with three different imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that RTDA achieves superior robustness against adversarial attacks and improved generalization performance in the presence of distribution shift in each image classification task while maintaining high clean accuracy.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17133v1",
    "published_date": "2025-06-20 16:36:39 UTC",
    "updated_date": "2025-06-20 16:36:39 UTC"
  },
  {
    "arxiv_id": "2506.17130v1",
    "title": "Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI",
    "authors": [
      "Botao Zhu",
      "Xianbin Wang",
      "Lei Zhang",
      "Xuemin",
      "Shen"
    ],
    "abstract": "In collaborative systems with complex tasks relying on distributed resources, trust evaluation of potential collaborators has emerged as an effective mechanism for task completion. However, due to the network dynamics and varying information gathering latencies, it is extremely challenging to observe and collect all trust attributes of a collaborating device concurrently for a comprehensive trust assessment. In this paper, a novel progressive trust evaluation framework, namely chain-of-trust, is proposed to make better use of misaligned device attribute data. This framework, designed for effective task completion, divides the trust evaluation process into multiple chained stages based on task decomposition. At each stage, based on the task completion process, the framework only gathers the latest device attribute data relevant to that stage, leading to reduced trust evaluation complexity and overhead. By leveraging advanced in-context learning, few-shot learning, and reasoning capabilities, generative AI is then employed to analyze and interpret the collected data to produce correct evaluation results quickly. Only devices deemed trustworthy at this stage proceed to the next round of trust evaluation. The framework ultimately determines devices that remain trustworthy across all stages. Experimental results demonstrate that the proposed framework achieves high accuracy in trust evaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17130v1",
    "published_date": "2025-06-20 16:33:03 UTC",
    "updated_date": "2025-06-20 16:33:03 UTC"
  },
  {
    "arxiv_id": "2506.17128v1",
    "title": "Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model",
    "authors": [
      "Botao Zhu",
      "Xianbin Wang"
    ],
    "abstract": "Trust is emerging as an effective tool to ensure the successful completion of collaborative tasks within collaborative systems. However, rapidly and continuously evaluating the trustworthiness of collaborators during task execution is a significant challenge due to distributed devices, complex operational environments, and dynamically changing resources. To tackle this challenge, this paper proposes a Siamese-enabled rapid and continuous trust evaluation framework (SRCTE) to facilitate effective task collaboration. First, the communication and computing resource attributes of the collaborator in a trusted state, along with historical collaboration data, are collected and represented using an attributed control flow graph (ACFG) that captures trust-related semantic information and serves as a reference for comparison with data collected during task execution. At each time slot of task execution, the collaborator's communication and computing resource attributes, as well as task completion effectiveness, are collected in real time and represented with an ACFG to convey their trust-related semantic information. A Siamese model, consisting of two shared-parameter Structure2vec networks, is then employed to learn the deep semantics of each pair of ACFGs and generate their embeddings. Finally, the similarity between the embeddings of each pair of ACFGs is calculated to determine the collaborator's trust value at each time slot. A real system is built using two Dell EMC 5200 servers and a Google Pixel 8 to test the effectiveness of the proposed SRCTE framework. Experimental results demonstrate that SRCTE converges rapidly with only a small amount of data and achieves a high anomaly trust detection rate compared to the baseline algorithm.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17128v1",
    "published_date": "2025-06-20 16:30:59 UTC",
    "updated_date": "2025-06-20 16:30:59 UTC"
  },
  {
    "arxiv_id": "2506.17124v2",
    "title": "When Can Model-Free Reinforcement Learning be Enough for Thinking?",
    "authors": [
      "Josiah P. Hanna",
      "Nicholas E. Corrado"
    ],
    "abstract": "Recent work on large language models has demonstrated the use of model-free reinforcement learning (RL) to train reasoning-like capabilities. The emergence of \"thinking\" through model-free RL is interesting as thinking actions neither produce reward nor change the external world state to one where the agent is more likely to get reward. This paper seeks to build a domain-independent understanding of when model-free RL will lead to such \"thinking\" as a strategy for reward maximization. To build this understanding, we first introduce a theoretical model which we call a thought Markov decision process (MDP). Thought MDPs minimally extend the classical MDP model to include an abstract notion of thought state and thought action. Using the thought MDP model, we prove the importance of policy initialization in determining whether or not thinking emerges and show formally that thought actions are equivalent to the agent choosing to perform a step of policy improvement before continuing to act. We then show that open-source LLMs satisfy the conditions that our theory predicts are necessary for model-free RL to produce thinking-like behavior. Finally, we hypothesize sufficient conditions that would enable thinking to be learned outside of language generation and introduce a toy domain where a combination of multi-task pre-training and designated thought actions enable more data-efficient RL compared to non-thinking agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages, 4 figures, Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17124v2",
    "published_date": "2025-06-20 16:23:46 UTC",
    "updated_date": "2025-10-25 01:17:48 UTC"
  },
  {
    "arxiv_id": "2506.17114v4",
    "title": "Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models",
    "authors": [
      "Dadi Guo",
      "Jiayu Liu",
      "Zhiyuan Fan",
      "Zhitao He",
      "Haoran Li",
      "Yuxin Li",
      "Yumeng Wang",
      "Yi R. Fung"
    ],
    "abstract": "Large reasoning models (e.g., R1, o3) have demonstrated remarkable mathematical problem-solving abilities. However, the high reported accuracy of these advanced models on popular datasets, reliance on purely numerical evaluation and potential benchmark leakage, often masks their true reasoning shortcomings. To address this, we propose leveraging the inherent rigor and methodological complexity of mathematical proofs as a diagnostic tool to expose these hidden failures. Specifically, we introduce the RFMDataset (Reveal Failure Modes), a collection of 200 diverse mathematical proof problems, and thoroughly evaluate advanced models' performance on it. Our in-depth analysis of their failures uncovers 10 fine-grained error types, which shows fundamental limitations in current large reasoning models: 1) large reasoning models grapple profoundly with mathematical proofs, with some generating entirely correct proofs for less than 20% of problems and failing even on basic ones; 2) models exhibit a diverse spectrum of reasoning failures, prominently demonstrating the lack of guarantees for the correctness and rigor of single-step reasoning; and 3) models show hallucination and incompleteness during the reasoning process. Our findings reveal that models' self-reflection is insufficient to resolve the current logical dilemmas, necessitating formalized and fine-grained logical training.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17114v4",
    "published_date": "2025-06-20 16:14:18 UTC",
    "updated_date": "2025-12-09 14:35:21 UTC"
  },
  {
    "arxiv_id": "2506.17113v2",
    "title": "MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation",
    "authors": [
      "Shoubin Yu",
      "Yue Zhang",
      "Ziyang Wang",
      "Jaehong Yoon",
      "Mohit Bansal"
    ],
    "abstract": "Combining pre-trained expert models offers substantial potential for scalable multimodal reasoning, but building a unified framework remains challenging due to the increasing diversity of input modalities and task complexity. For instance, medical diagnosis requires precise reasoning over structured clinical tables, while financial forecasting depends on interpreting plot-based data to make informed predictions. To tackle this challenge, we introduce MEXA, a training-free framework that performs modality- and task-aware aggregation of multiple expert models to enable effective multimodal reasoning across diverse and distinct domains. MEXA dynamically selects expert models based on the input modality and the task-specific reasoning demands (i.e., skills). Each expert model, specialized in a modality task pair, generates interpretable textual reasoning outputs. MEXA then aggregates and reasons over these outputs using a Large Reasoning Model (LRM) to produce the final answer. This modular design allows flexible and transparent multimodal reasoning across diverse domains without additional training overhead. We extensively evaluate our approach on diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D Understanding, and Medical QA. MEXA consistently delivers performance improvements over strong multimodal baselines, highlighting the effectiveness and broad applicability of our expert-driven selection and aggregation in diverse multimodal reasoning tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "EMNLP 2025 Findings; The first two authors contributed equally; Github link: https://github.com/Yui010206/MEXA",
    "pdf_url": "https://arxiv.org/pdf/2506.17113v2",
    "published_date": "2025-06-20 16:14:13 UTC",
    "updated_date": "2025-10-25 08:57:40 UTC"
  },
  {
    "arxiv_id": "2506.17111v1",
    "title": "Are Bias Evaluation Methods Biased ?",
    "authors": [
      "Lina Berrayana",
      "Sean Rooney",
      "Luis Garcés-Erice",
      "Ioana Giurgiu"
    ],
    "abstract": "The creation of benchmarks to evaluate the safety of Large Language Models is one of the key activities within the trusted AI community. These benchmarks allow models to be compared for different aspects of safety such as toxicity, bias, harmful behavior etc. Independent benchmarks adopt different approaches with distinct data sets and evaluation methods. We investigate how robust such benchmarks are by using different approaches to rank a set of representative models for bias and compare how similar are the overall rankings. We show that different but widely used bias evaluations methods result in disparate model rankings. We conclude with recommendations for the community in the usage of such benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ACL 2025 Workshop GEM",
    "pdf_url": "https://arxiv.org/pdf/2506.17111v1",
    "published_date": "2025-06-20 16:11:25 UTC",
    "updated_date": "2025-06-20 16:11:25 UTC"
  },
  {
    "arxiv_id": "2506.17104v1",
    "title": "Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving",
    "authors": [
      "Chuxue Cao",
      "Mengze Li",
      "Juntao Dai",
      "Jinluan Yang",
      "Zijian Zhao",
      "Shengyu Zhang",
      "Weijie Shi",
      "Chengzhong Liu",
      "Sirui Han",
      "Yike Guo"
    ],
    "abstract": "Large language models (LLMs) have shown promising first-order logic (FOL) reasoning capabilities with applications in various areas. However, their effectiveness in complex mathematical reasoning involving multi-step FOL deductions is still under-researched. While LLMs perform competitively on established mathematical reasoning benchmarks, they struggle with multi-step FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on our proposed theorem proving dataset. This issue arises from the limited exploration of diverse proof strategies and the potential for early reasoning mistakes to undermine entire proofs. To address these issues, we propose DREAM, a self-adaptive solution that enhances the Diversity and REAsonability of LLMs' generation strategies. DREAM incorporates an Axiom-Driven Strategy Diversification mechanism to promote varied strategic outcomes and a Sub-Proposition Error Feedback to help LLMs reflect on and correct their proofs. Our contributions include pioneering advancements in LLMs' mathematical reasoning through FOL theorem proving, introducing a novel inference stage solution that improves performance by 0.6% to 6.4%, and providing a curated dataset of 447 mathematical theorems in Lean 4 format for evaluation.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17104v1",
    "published_date": "2025-06-20 16:09:56 UTC",
    "updated_date": "2025-06-20 16:09:56 UTC"
  },
  {
    "arxiv_id": "2506.17103v1",
    "title": "TransDreamerV3: Implanting Transformer In DreamerV3",
    "authors": [
      "Shruti Sadanand Dongare",
      "Amun Kharel",
      "Jonathan Samuel",
      "Xiaona Zhou"
    ],
    "abstract": "This paper introduces TransDreamerV3, a reinforcement learning model that enhances the DreamerV3 architecture by integrating a transformer encoder. The model is designed to improve memory and decision-making capabilities in complex environments. We conducted experiments on Atari-Boxing, Atari-Freeway, Atari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved performance over DreamerV3, particularly in the Atari-Freeway and Crafter tasks. While issues in the Minecraft task and limited training across all tasks were noted, TransDreamerV3 displays advancement in world model-based reinforcement learning, leveraging transformer architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17103v1",
    "published_date": "2025-06-20 16:09:17 UTC",
    "updated_date": "2025-06-20 16:09:17 UTC"
  },
  {
    "arxiv_id": "2506.17372v1",
    "title": "Multimodal Political Bias Identification and Neutralization",
    "authors": [
      "Cedric Bernard",
      "Xavier Pleimling",
      "Amun Kharel",
      "Chase Vickery"
    ],
    "abstract": "Due to the presence of political echo chambers, it becomes imperative to detect and remove subjective bias and emotionally charged language from both the text and images of political articles. However, prior work has focused on solely the text portion of the bias rather than both the text and image portions. This is a problem because the images are just as powerful of a medium to communicate information as text is. To that end, we present a model that leverages both text and image bias which consists of four different steps. Image Text Alignment focuses on semantically aligning images based on their bias through CLIP models. Image Bias Scoring determines the appropriate bias score of images via a ViT classifier. Text De-Biasing focuses on detecting biased words and phrases and neutralizing them through BERT models. These three steps all culminate to the final step of debiasing, which replaces the text and the image with neutralized or reduced counterparts, which for images is done by comparing the bias scores. The results so far indicate that this approach is promising, with the text debiasing strategy being able to identify many potential biased words and phrases, and the ViT model showcasing effective training. The semantic alignment model also is efficient. However, more time, particularly in training, and resources are needed to obtain better results. A human evaluation portion was also proposed to ensure semantic consistency of the newly generated text and images.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17372v1",
    "published_date": "2025-06-20 16:03:20 UTC",
    "updated_date": "2025-06-20 16:03:20 UTC"
  },
  {
    "arxiv_id": "2506.17093v2",
    "title": "Identifiability of Deep Polynomial Neural Networks",
    "authors": [
      "Konstantin Usevich",
      "Ricardo Borsoi",
      "Clara Dérand",
      "Marianne Clausel"
    ],
    "abstract": "Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric structure. However, their identifiability -- a key property for ensuring interpretability -- remains poorly understood. In this work, we present a comprehensive analysis of the identifiability of deep PNNs, including architectures with and without bias terms. Our results reveal an intricate interplay between activation degrees and layer widths in achieving identifiability. As special cases, we show that architectures with non-increasing layer widths are generically identifiable under mild conditions, while encoder-decoder networks are identifiable when the decoder widths do not grow too rapidly compared to the activation degrees. Our proofs are constructive and center on a connection between deep PNNs and low-rank tensor decompositions, and Kruskal-type uniqueness theorems. We also settle an open conjecture on the dimension of PNN's neurovarieties, and provide new bounds on the activation degrees required for it to reach the expected dimension.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.AG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17093v2",
    "published_date": "2025-06-20 15:58:46 UTC",
    "updated_date": "2025-10-26 21:05:33 UTC"
  },
  {
    "arxiv_id": "2506.17370v1",
    "title": "AI based Content Creation and Product Recommendation Applications in E-commerce: An Ethical overview",
    "authors": [
      "Aditi Madhusudan Jain",
      "Ayush Jain"
    ],
    "abstract": "As e-commerce rapidly integrates artificial intelligence for content creation and product recommendations, these technologies offer significant benefits in personalization and efficiency. AI-driven systems automate product descriptions, generate dynamic advertisements, and deliver tailored recommendations based on consumer behavior, as seen in major platforms like Amazon and Shopify. However, the widespread use of AI in e-commerce raises crucial ethical challenges, particularly around data privacy, algorithmic bias, and consumer autonomy. Bias -- whether cultural, gender-based, or socioeconomic -- can be inadvertently embedded in AI models, leading to inequitable product recommendations and reinforcing harmful stereotypes. This paper examines the ethical implications of AI-driven content creation and product recommendations, emphasizing the need for frameworks to ensure fairness, transparency, and need for more established and robust ethical standards. We propose actionable best practices to remove bias and ensure inclusivity, such as conducting regular audits of algorithms, diversifying training data, and incorporating fairness metrics into AI models. Additionally, we discuss frameworks for ethical conformance that focus on safeguarding consumer data privacy, promoting transparency in decision-making processes, and enhancing consumer autonomy. By addressing these issues, we provide guidelines for responsibly utilizing AI in e-commerce applications for content creation and product recommendations, ensuring that these technologies are both effective and ethically sound.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17370v1",
    "published_date": "2025-06-20 15:54:25 UTC",
    "updated_date": "2025-06-20 15:54:25 UTC"
  },
  {
    "arxiv_id": "2506.17085v2",
    "title": "Dispositions and Roles of Generically Dependent Entities",
    "authors": [
      "Fabian Neuhaus"
    ],
    "abstract": "BFO 2020 does not support functions, dispositions, and roles of generically dependent continuants (like software or datasets). In this paper, we argue that this is a severe limitation, which prevents, for example, the adequate representation of the functions of computer models or the various roles of datasets during the execution of these models. We discuss the aspects of BFO 2020 that prevent the representation of realizable entities of generically dependent continuants. Two approaches to address the issue are presented: (a) the use of defined classes and (b) a proposal of changes that allow BFO to support functions, dispositions, and roles of generically dependent continuants. The latter also addresses limitations of BFO 2020 concerning the roles and dispositions of immaterial entities, particularly boundaries and sites.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17085v2",
    "published_date": "2025-06-20 15:40:45 UTC",
    "updated_date": "2025-08-19 12:56:56 UTC"
  },
  {
    "arxiv_id": "2506.17369v1",
    "title": "Re-Evaluating Code LLM Benchmarks Under Semantic Mutation",
    "authors": [
      "Zhiyuan Pan",
      "Xing Hu",
      "Xin Xia",
      "Xiaohu Yang"
    ],
    "abstract": "In the era of large language models (LLMs), code benchmarks have become an important research area in software engineering and are widely used by practitioners. These benchmarks evaluate the performance of LLMs on specific code-related tasks, such as code understanding and generation. A critical step in constructing code benchmarks is the design of prompts. However, as existing code benchmarks typically rely on a single prompt template per task, they are prone to the issue of prompt sensitivity, where minor prompt variations could result in substantial performance variations, leading to unreliable evaluations of model capabilities.\n  While previous studies have explored prompt sensitivity, their experimental designs and findings are limited to traditional natural language processing (NLP) tasks. In this paper, we present an empirical study to investigate prompt sensitivity in code benchmarks. We first propose a general framework that modifies prompt templates in a manner that preserves both their semantics and their structure as much as possible. Based on the framework, we conduct extensive experiments across eight code benchmark tasks on 10 representative open-source LLMs, with each task featuring 100 semantically similar prompt templates. We then analyze the evaluation results using various statistical metrics, focusing on both absolute and relative model performance. Our findings suggest that even slight prompt variations can lead to significant shifts in performance. Additionally, we observe that such variations can introduce inconsistencies in the performance rankings across different models. These insights highlight the need for considering prompt sensitivity when designing future code benchmarks, to ensure more reliable and accurate evaluation of LLM capabilities.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17369v1",
    "published_date": "2025-06-20 15:30:36 UTC",
    "updated_date": "2025-06-20 15:30:36 UTC"
  },
  {
    "arxiv_id": "2506.17080v1",
    "title": "Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs",
    "authors": [
      "Ricardo Rei",
      "Nuno M. Guerreiro",
      "José Pombal",
      "João Alves",
      "Pedro Teixeirinha",
      "Amin Farajian",
      "André F. T. Martins"
    ],
    "abstract": "Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17080v1",
    "published_date": "2025-06-20 15:30:06 UTC",
    "updated_date": "2025-06-20 15:30:06 UTC"
  },
  {
    "arxiv_id": "2506.17073v1",
    "title": "LLM-Based Bot Broadens the Range of Arguments in Online Discussions, Even When Transparently Disclosed as AI",
    "authors": [
      "Valeria Vuk",
      "Cristina Sarasua",
      "Fabrizio Gilardi"
    ],
    "abstract": "A wide range of participation is essential for democracy, as it helps prevent the dominance of extreme views, erosion of legitimacy, and political polarization. However, engagement in online political discussions often features a limited spectrum of views due to high levels of self-selection and the tendency of online platforms to facilitate exchanges primarily among like-minded individuals. This study examines whether an LLM-based bot can widen the scope of perspectives expressed by participants in online discussions through two pre-registered randomized experiments conducted in a chatroom. We evaluate the impact of a bot that actively monitors discussions, identifies missing arguments, and introduces them into the conversation. The results indicate that our bot significantly expands the range of arguments, as measured by both objective and subjective metrics. Furthermore, disclosure of the bot as AI does not significantly alter these effects. These findings suggest that LLM-based moderation tools can positively influence online political discourse.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17073v1",
    "published_date": "2025-06-20 15:24:31 UTC",
    "updated_date": "2025-06-20 15:24:31 UTC"
  },
  {
    "arxiv_id": "2506.17065v2",
    "title": "Flow based approach for Dynamic Temporal Causal models with non-Gaussian or Heteroscedastic Noises",
    "authors": [
      "Abdellah Rahmani",
      "Pascal Frossard"
    ],
    "abstract": "Understanding causal relationships in multivariate time series is crucial in many scenarios, such as those dealing with financial or neurological data. Many such time series exhibit multiple regimes, i.e., consecutive temporal segments with a priori unknown boundaries, with each regime having its own causal structure. Inferring causal dependencies and regime shifts is critical for analyzing the underlying processes. However, causal structure learning in this setting is challenging due to (1) non-stationarity, i.e., each regime can have its own causal graph and mixing function, and (2) complex noise distributions, which may be nonGaussian or heteroscedastic. Existing causal discovery approaches cannot address these challenges, since generally assume stationarity or Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified framework for causal discovery that handles non-stationary processes along with non-Gaussian and heteroscedastic noises. FANTOM simultaneously infers the number of regimes and their corresponding indices and learns each regime's Directed Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm that maximizes the evidence lower bound of the data log-likelihood. On the theoretical side, we prove, under mild assumptions, that temporal heteroscedastic causal models, introduced in FANTOM's formulation, are identifiable in both stationary and non-stationary settings. In addition, extensive experiments on synthetic and real data show that FANTOM outperforms existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17065v2",
    "published_date": "2025-06-20 15:12:43 UTC",
    "updated_date": "2025-10-23 15:59:26 UTC"
  },
  {
    "arxiv_id": "2506.17368v2",
    "title": "SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification",
    "authors": [
      "Zhenglin Lai",
      "Mengyao Liao",
      "Bingzhe Wu",
      "Dong Xu",
      "Zebin Zhao",
      "Zhihang Yuan",
      "Chao Fan",
      "Jianqiang Li"
    ],
    "abstract": "Large language models with Mixture-of-Experts (MoE) architectures achieve efficiency and scalability, yet their routing mechanisms introduce safety alignment challenges insufficiently addressed by techniques developed for dense models. In this work, the MoE-specific safety risk of positional vulnerability-that safety-aligned behaviors rely on specific expert modules-is formalized and systematically analyzed. An analytical framework, SAFEx, is presented to robustly identify, characterize, and validate safety-critical experts via a stability-based expert selection procedure, and to decompose them into two functional groups: the Harmful Content Detection Group (HCDG), which specializes in identifying and recognizing harmful content within user inputs, and the Harmful Response Control Group (HRCG), which specializes in controlling and enforcing model behaviors to generate appropriate safety responses. Expert-level interventions are conducted to probe causality and to test mitigation. Targeted masking of SAFEx-selected experts reveals that safety behavior is highly concentrated. On Qwen3-30B-A3B, configured with 48 MoE-FFN layers and 128 experts per layer under top-8 routing (48x128=6,144 experts in total), disabling 12 selected experts reduces the refusal rate by 22%. In addition, lightweight adaptation is performed using LoRA under three configurations-the HRCG, the union of HCDG and HRCG, and all experts-and the resulting updates are composed through negative weight merging targeted at the HRCG, leading to improved refusal under adversarial prompts without full-model retraining. These results establish positional vulnerability as a distinct MoE-specific safety challenge and provide a practical, compute-efficient pathway for expert-level safety interventions within routed architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.17368v2",
    "published_date": "2025-06-20 15:09:10 UTC",
    "updated_date": "2025-10-12 14:51:13 UTC"
  },
  {
    "arxiv_id": "2506.17052v1",
    "title": "From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers",
    "authors": [
      "Jingtong Su",
      "Julia Kempe",
      "Karen Ullrich"
    ],
    "abstract": "Transformers have achieved state-of-the-art performance across language and vision tasks. This success drives the imperative to interpret their internal mechanisms with the dual goals of enhancing performance and improving behavioral control. Attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components. Current attribution research primarily studies multi-layer perceptron neurons and addresses relatively simple concepts such as factual associations (e.g., Paris is located in France). This focus tends to overlook the impact of the attention mechanism and lacks a unified approach for analyzing more complex concepts. To fill these gaps, we introduce Scalable Attention Module Discovery (SAMD), a concept-agnostic method for mapping arbitrary, complex concepts to specific attention heads of general transformer models. We accomplish this by representing each concept as a vector, calculating its cosine similarity with each attention head, and selecting the TopK-scoring heads to construct the concept-associated attention module. We then propose Scalar Attention Module Intervention (SAMI), a simple strategy to diminish or amplify the effects of a concept by adjusting the attention module using only a single scalar parameter. Empirically, we demonstrate SAMD on concepts of varying complexity, and visualize the locations of their corresponding modules. Our results demonstrate that module locations remain stable before and after LLM post-training, and confirm prior work on the mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on HarmBench (+72.7%) by diminishing \"safety\" and improve performance on the GSM8K benchmark (+1.6%) by amplifying \"reasoning\". Lastly, we highlight the domain-agnostic nature of our approach by suppressing the image classification accuracy of vision transformers on ImageNet.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17052v1",
    "published_date": "2025-06-20 15:04:11 UTC",
    "updated_date": "2025-06-20 15:04:11 UTC"
  },
  {
    "arxiv_id": "2506.17041v1",
    "title": "MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection",
    "authors": [
      "Joshua Schraven",
      "Alexander Windmann",
      "Oliver Niggemann"
    ],
    "abstract": "Benchmark datasets for network intrusion detection commonly rely on synthetically generated traffic, which fails to reflect the statistical variability and temporal drift encountered in operational environments. This paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1 dataset, designed to enable realistic and reproducible evaluation of anomaly detection methods. A reproducible preprocessing pipeline is presented that transforms raw packet captures into flow representations conforming to the CICFlowMeter format, while preserving MAWILab's original anomaly labels. The resulting datasets comprise temporally distinct samples from January 2011, 2016, and 2021, drawn from trans-Pacific backbone traffic.\n  To establish reference baselines, traditional machine learning methods, including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical results demonstrate that tree-based classifiers perform well on temporally static data but experience significant performance degradation over time. In contrast, the CNN-BiLSTM model maintains better performance, thus showing improved generalization. These findings underscore the limitations of synthetic benchmarks and static models, and motivate the adoption of realistic datasets with explicit temporal structure. All datasets, pipeline code, and model implementations are made publicly available to foster transparency and reproducibility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.17041v1",
    "published_date": "2025-06-20 14:51:35 UTC",
    "updated_date": "2025-06-20 14:51:35 UTC"
  },
  {
    "arxiv_id": "2506.17367v1",
    "title": "Cash or Comfort? How LLMs Value Your Inconvenience",
    "authors": [
      "Mateusz Cedro",
      "Timour Ichmoukhamedov",
      "Sofie Goethals",
      "Yifan He",
      "James Hinns",
      "David Martens"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly proposed as near-autonomous artificial intelligence (AI) agents capable of making everyday decisions on behalf of humans. Although LLMs perform well on many technical tasks, their behaviour in personal decision-making remains less understood. Previous studies have assessed their rationality and moral alignment with human decisions. However, the behaviour of AI assistants in scenarios where financial rewards are at odds with user comfort has not yet been thoroughly explored. In this paper, we tackle this problem by quantifying the prices assigned by multiple LLMs to a series of user discomforts: additional walking, waiting, hunger and pain. We uncover several key concerns that strongly question the prospect of using current LLMs as decision-making assistants: (1) a large variance in responses between LLMs, (2) within a single LLM, responses show fragility to minor variations in prompt phrasing (e.g., reformulating the question in the first person can considerably alter the decision), (3) LLMs can accept unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10 hours), and (4) LLMs can reject monetary gains where no discomfort is imposed (e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for scrutiny of how LLMs value human inconvenience, particularly as we move toward applications where such cash-versus-comfort trade-offs are made on users' behalf.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 4 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.17367v1",
    "published_date": "2025-06-20 14:49:20 UTC",
    "updated_date": "2025-06-20 14:49:20 UTC"
  },
  {
    "arxiv_id": "2506.17039v1",
    "title": "LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation",
    "authors": [
      "Elizabeth Fons",
      "Alejandro Sztrajman",
      "Yousef El-Laham",
      "Luciana Ferrer",
      "Svitlana Vyetrenko",
      "Manuela Veloso"
    ],
    "abstract": "Time series with missing or irregularly sampled data are a persistent challenge in machine learning. Many methods operate on the frequency-domain, relying on the Fast Fourier Transform (FFT) which assumes uniform sampling, therefore requiring prior interpolation that can distort the spectra. To address this limitation, we introduce a differentiable Lomb--Scargle layer that enables a reliable computation of the power spectrum of irregularly sampled data. We integrate this layer into a novel score-based diffusion model (LSCD) for time series imputation conditioned on the entire signal spectrum. Experiments on synthetic and real-world benchmarks demonstrate that our method recovers missing data more accurately than purely time-domain baselines, while simultaneously producing consistent frequency estimates. Crucially, our method can be easily integrated into learning frameworks, enabling broader adoption of spectral guidance in machine learning approaches involving incomplete or irregular data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "In ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17039v1",
    "published_date": "2025-06-20 14:48:42 UTC",
    "updated_date": "2025-06-20 14:48:42 UTC"
  },
  {
    "arxiv_id": "2506.17019v1",
    "title": "Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning",
    "authors": [
      "Giuseppe Attanasio",
      "Sonal Sannigrahi",
      "Ben Peters",
      "André F. T. Martins"
    ],
    "abstract": "This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on Instruction Following Speech Processing. We submit results for the Short Track, i.e., speech recognition, translation, and spoken question answering. Our model is a unified speech-to-text model that integrates a pre-trained continuous speech encoder and text decoder through a first phase of modality alignment and a second phase of instruction fine-tuning. Crucially, we focus on using small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY data along with synthetic data generation to supplement existing resources.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 1 figure, IWSLT 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17019v1",
    "published_date": "2025-06-20 14:17:42 UTC",
    "updated_date": "2025-06-20 14:17:42 UTC"
  },
  {
    "arxiv_id": "2506.17018v1",
    "title": "A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models",
    "authors": [
      "Davide Frizzo",
      "Francesco Borsatti",
      "Gian Antonio Susto"
    ],
    "abstract": "Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0, proactively enhancing efficiency through accurate equipment Remaining Useful Life (RUL) prediction, thus optimizing maintenance scheduling and reducing unexpected failures and premature interventions. This paper introduces a novel RUL estimation approach leveraging State Space Models (SSM) for efficient long-term sequence modeling. To handle model uncertainty, Simoultaneous Quantile Regression (SQR) is integrated into the SSM, enabling multiple quantile estimations. The proposed method is benchmarked against traditional sequence modelling techniques (LSTM, Transformer, Informer) using the C-MAPSS dataset. Results demonstrate superior accuracy and computational efficiency of SSM models, underscoring their potential for high-stakes industrial applications.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to IFAC Joint Conference on Computers, Cognition, and Communication (J3C) 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17018v1",
    "published_date": "2025-06-20 14:15:55 UTC",
    "updated_date": "2025-06-20 14:15:55 UTC"
  },
  {
    "arxiv_id": "2506.21609v1",
    "title": "From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models",
    "authors": [
      "Junhao Liu",
      "Zhenhao Xu",
      "Yuxin Fang",
      "Yichuan Chen",
      "Zuobin Ying",
      "Wenhan Chang"
    ],
    "abstract": "Recently, there have been notable advancements in large language models (LLMs), demonstrating their growing abilities in complex reasoning. However, existing research largely overlooks a thorough and systematic comparison of these models' reasoning processes and outputs, particularly regarding their self-reflection pattern (also termed \"Aha moment\") and the interconnections across diverse domains. This paper proposes a novel framework for analyzing the reasoning characteristics of four cutting-edge large reasoning models (GPT-o1, DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge paradigm. Our approach connects their internal thinking processes with their final outputs. A diverse dataset consists of real-world scenario-based questions covering logical deduction, causal inference, and multi-step problem-solving. Additionally, a set of metrics is put forward to assess both the coherence of reasoning and the accuracy of the outputs. The research results uncover various patterns of how these models balance exploration and exploitation, deal with problems, and reach conclusions during the reasoning process. Through quantitative and qualitative comparisons, disparities among these models are identified in aspects such as the depth of reasoning, the reliance on intermediate steps, and the degree of similarity between their thinking processes and output patterns and those of GPT-o1. This work offers valuable insights into the trade-off between computational efficiency and reasoning robustness and provides practical recommendations for enhancing model design and evaluation in practical applications. We publicly release our project at: https://github.com/ChangWenhan/FromThinking2Output",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.21609v1",
    "published_date": "2025-06-20 14:02:16 UTC",
    "updated_date": "2025-06-20 14:02:16 UTC"
  },
  {
    "arxiv_id": "2506.16995v3",
    "title": "Style-Preserving Policy Optimization for Game Agents",
    "authors": [
      "Lingfeng Li",
      "Yunlong Lu",
      "Yongyi Wang",
      "Wenxin Li"
    ],
    "abstract": "Proficient game agents with diverse play styles enrich the gaming experience and enhance the replay value of games. However, recent advancements in game AI based on reinforcement learning have predominantly focused on improving proficiency, whereas methods based on evolution algorithms generate agents with diverse play styles but exhibit subpar performance compared to RL methods. To address this gap, this paper proposes Mixed Proximal Policy Optimization (MPPO), a method designed to improve the proficiency of existing suboptimal agents while retaining their distinct styles. MPPO unifies loss objectives for both online and offline samples and introduces an implicit constraint to approximate demonstrator policies by adjusting the empirical distribution of samples. Empirical results across environments of varying scales demonstrate that MPPO achieves proficiency levels comparable to, or even superior to, pure online algorithms while preserving demonstrators' play styles. This work presents an effective approach for generating highly proficient and diverse game agents, ultimately contributing to more engaging gameplay experiences.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16995v3",
    "published_date": "2025-06-20 13:46:06 UTC",
    "updated_date": "2025-09-21 16:22:19 UTC"
  },
  {
    "arxiv_id": "2507.02892v1",
    "title": "Large Language Model-Driven Surrogate-Assisted Evolutionary Algorithm for Expensive Optimization",
    "authors": [
      "Lindong Xie",
      "Genghui Li",
      "Zhenkun Wang",
      "Edward Chung",
      "Maoguo Gong"
    ],
    "abstract": "Surrogate-assisted evolutionary algorithms (SAEAs) are a key tool for addressing costly optimization tasks, with their efficiency being heavily dependent on the selection of surrogate models and infill sampling criteria. However, designing an effective dynamic selection strategy for SAEAs is labor-intensive and requires substantial domain knowledge. To address this challenge, this paper proposes LLM-SAEA, a novel approach that integrates large language models (LLMs) to configure both surrogate models and infill sampling criteria online. Specifically, LLM-SAEA develops a collaboration-of-experts framework, where one LLM serves as a scoring expert (LLM-SE), assigning scores to surrogate models and infill sampling criteria based on their optimization performance, while another LLM acts as a decision expert (LLM-DE), selecting the appropriate configurations by analyzing their scores along with the current optimization state. Experimental results demonstrate that LLM-SAEA outperforms several state-of-the-art algorithms across standard test cases. The source code is publicly available at https://github.com/ForrestXie9/LLM-SAEA.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02892v1",
    "published_date": "2025-06-20 13:44:21 UTC",
    "updated_date": "2025-06-20 13:44:21 UTC"
  },
  {
    "arxiv_id": "2507.01032v1",
    "title": "An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks",
    "authors": [
      "Nan Mu",
      "Hongbo Yang",
      "Chen Zhao"
    ],
    "abstract": "Background and Objective: High-throughput multi-omics technologies have proven invaluable for elucidating disease mechanisms and enabling early diagnosis. However, the high cost of multi-omics profiling imposes a significant economic burden, with over reliance on full omics data potentially leading to unnecessary resource consumption. To address these issues, we propose an uncertainty-aware, multi-view dynamic decision framework for omics data classification that aims to achieve high diagnostic accuracy while minimizing testing costs. Methodology: At the single-omics level, we refine the activation functions of neural networks to generate Dirichlet distribution parameters, utilizing subjective logic to quantify both the belief masses and uncertainty mass of classification results. Belief mass reflects the support of a specific omics modality for a disease class, while the uncertainty parameter captures limitations in data quality and model discriminability, providing a more trustworthy basis for decision-making. At the multi omics level, we employ a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous modalities, leveraging their complementarity to boost diagnostic accuracy and robustness. A dynamic decision mechanism is then applied that omics data are incrementally introduced for each patient until either all data sources are utilized or the model confidence exceeds a predefined threshold, potentially before all data sources are utilized. Results and Conclusion: We evaluate our approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN. In three datasets, over 50% of cases achieved accurate classification using a single omics modality, effectively reducing redundant testing. Meanwhile, our method maintains diagnostic performance comparable to full-omics models and preserves essential biological insights.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.01032v1",
    "published_date": "2025-06-20 13:44:14 UTC",
    "updated_date": "2025-06-20 13:44:14 UTC"
  },
  {
    "arxiv_id": "2506.16991v2",
    "title": "ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds",
    "authors": [
      "Binbin Xiang",
      "Maciej Wielgosz",
      "Stefano Puliti",
      "Kamil Král",
      "Martin Krůček",
      "Azim Missarov",
      "Rasmus Astrup"
    ],
    "abstract": "The segmentation of forest LiDAR 3D point clouds, including both individual tree and semantic segmentation, is fundamental for advancing forest management and ecological research. However, current approaches often struggle with the complexity and variability of natural forest environments. We present ForestFormer3D, a new unified and end-to-end framework designed for precise individual tree and semantic segmentation. ForestFormer3D incorporates ISA-guided query point selection, a score-based block merging strategy during inference, and a one-to-many association mechanism for effective training. By combining these new components, our model achieves state-of-the-art performance for individual tree segmentation on the newly introduced FOR-instanceV2 dataset, which spans diverse forest types and regions. Additionally, ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx), showcasing its robustness across different forest conditions and sensor modalities. The FOR-instanceV2 dataset and the ForestFormer3D code are publicly available at https://bxiang233.github.io/FF3D/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16991v2",
    "published_date": "2025-06-20 13:39:27 UTC",
    "updated_date": "2025-08-01 05:36:38 UTC"
  },
  {
    "arxiv_id": "2506.16990v1",
    "title": "TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs",
    "authors": [
      "Sahil Kale",
      "Vijaykant Nadadur"
    ],
    "abstract": "LaTeX's precision and flexibility in typesetting have made it the gold standard for the preparation of scientific documentation. Large Language Models (LLMs) present a promising opportunity for researchers to produce publication-ready material using LaTeX with natural language instructions, yet current benchmarks completely lack evaluation of this ability. By introducing TeXpert, our benchmark dataset with natural language prompts for generating LaTeX code focused on components of scientific documents across multiple difficulty levels, we conduct an in-depth analysis of LLM performance in this regard and identify frequent error types. Our evaluation across open and closed-source LLMs highlights multiple key findings: LLMs excelling on standard benchmarks perform poorly in LaTeX generation with a significant accuracy drop-off as the complexity of tasks increases; open-source models like DeepSeek v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks; and formatting and package errors are unexpectedly prevalent, suggesting a lack of diverse LaTeX examples in the training datasets of most LLMs. Our dataset, code, and model evaluations are available at https://github.com/knowledge-verse-ai/TeXpert.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the SDProc Workshop @ ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.16990v1",
    "published_date": "2025-06-20 13:39:16 UTC",
    "updated_date": "2025-06-20 13:39:16 UTC"
  },
  {
    "arxiv_id": "2506.16982v1",
    "title": "Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond",
    "authors": [
      "Antonin Berthon",
      "Mihaela van der Schaar"
    ],
    "abstract": "Accurately assessing student knowledge is critical for effective education, yet traditional Knowledge Tracing (KT) methods rely on opaque latent embeddings, limiting interpretability. Even LLM-based approaches generate direct predictions or summaries that may hallucinate without any accuracy guarantees. We recast KT as an inverse problem: learning the minimum natural-language summary that makes past answers explainable and future answers predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM that writes an interpretable knowledge summary and a frozen decoder LLM that must reconstruct and predict student responses using only that summary text. By constraining all predictive information to pass through a short natural-language bottleneck, LBMs ensure that the summary contains accurate information while remaining human-interpretable. Experiments on synthetic arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the accuracy of state-of-the-art KT and direct LLM methods while requiring orders-of-magnitude fewer student trajectories. We demonstrate that training the encoder with group-relative policy optimization, using downstream decoding accuracy as a reward signal, effectively improves summary quality.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16982v1",
    "published_date": "2025-06-20 13:21:14 UTC",
    "updated_date": "2025-06-20 13:21:14 UTC"
  },
  {
    "arxiv_id": "2506.21608v1",
    "title": "SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2",
    "authors": [
      "Yasmine Bouamra",
      "Bruno Yun",
      "Alexandre Poisson",
      "Frédéric Armetta"
    ],
    "abstract": "The automatic generation of SysML v2 models represents a major challenge in the engineering of complex systems, particularly due to the scarcity of learning corpora and complex syntax. We present SysTemp, a system aimed at facilitating and improving the creation of SysML v2 models from natural language specifications. It is based on a multi-agent system, including a template generator that structures the generation process. We discuss the advantages and challenges of this system through an evaluation, highlighting its potential to improve the quality of the generations in SysML v2 modeling.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21608v1",
    "published_date": "2025-06-20 13:17:44 UTC",
    "updated_date": "2025-06-20 13:17:44 UTC"
  },
  {
    "arxiv_id": "2506.16975v2",
    "title": "Latent Concept Disentanglement in Transformer-based Language Models",
    "authors": [
      "Guan Zhe Hong",
      "Bhavya Vasudeva",
      "Vatsal Sharan",
      "Cyrus Rashtchian",
      "Prabhakar Raghavan",
      "Rina Panigrahy"
    ],
    "abstract": "When large language models (LLMs) use in-context learning (ICL) to solve a new task, they must infer latent concepts from demonstration examples. This raises the question of whether and how transformers represent latent structures as part of their computation. Our work experiments with several controlled tasks, studying this question using mechanistic interpretability. First, we show that in transitive reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. This builds upon prior work that analyzes single-step reasoning. Then, we consider tasks parameterized by a latent numerical concept. We discover low-dimensional subspaces in the model's representation space, where the geometry cleanly reflects the underlying parameterization. Overall, we show that small and large models can indeed disentangle and utilize latent concepts that they learn in-context from a handful of abbreviated demonstrations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16975v2",
    "published_date": "2025-06-20 13:08:12 UTC",
    "updated_date": "2025-09-26 13:37:33 UTC"
  },
  {
    "arxiv_id": "2506.16971v1",
    "title": "Formal Control for Uncertain Systems via Contract-Based Probabilistic Surrogates (Extended Version)",
    "authors": [
      "Oliver Schön",
      "Sofie Haesaert",
      "Sadegh Soudjani"
    ],
    "abstract": "The requirement for identifying accurate system representations has not only been a challenge to fulfill, but it has compromised the scalability of formal methods, as the resulting models are often too complex for effective decision making with formal correctness and performance guarantees. Focusing on probabilistic simulation relations and surrogate models of stochastic systems, we propose an approach that significantly enhances the scalability and practical applicability of such simulation relations by eliminating the need to compute error bounds directly. As a result, we provide an abstraction-based technique that scales effectively to higher dimensions while addressing complex nonlinear agent-environment interactions with infinite-horizon temporal logic guarantees amidst uncertainty. Our approach trades scalability for conservatism favorably, as demonstrated on a complex high-dimensional vehicle intersection case study.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "eess.SY",
    "comment": "26 pages, 5 figures, extended version of paper accepted for publication at QEST 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.16971v1",
    "published_date": "2025-06-20 13:00:50 UTC",
    "updated_date": "2025-06-20 13:00:50 UTC"
  },
  {
    "arxiv_id": "2506.16962v2",
    "title": "Chiron-o1: Igniting Multimodal Large Language Models towards Generalizable Medical Reasoning via Mentor-Intern Collaborative Search",
    "authors": [
      "Haoran Sun",
      "Yankai Jiang",
      "Wenjie Lou",
      "Yujie Zhang",
      "Wenjie Li",
      "Lilong Wang",
      "Mianxin Liu",
      "Lei Liu",
      "Xiaosong Wang"
    ],
    "abstract": "Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at https://github.com/manglu097/Chiron-o1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16962v2",
    "published_date": "2025-06-20 12:51:19 UTC",
    "updated_date": "2025-10-22 04:23:57 UTC"
  },
  {
    "arxiv_id": "2506.18926v1",
    "title": "AI-based Approach in Early Warning Systems: Focus on Emergency Communication Ecosystem and Citizen Participation in Nordic Countries",
    "authors": [
      "Fuzel Shaik",
      "Getnet Demil",
      "Mourad Oussalah"
    ],
    "abstract": "Climate change and natural disasters are recognized as worldwide challenges requiring complex and efficient ecosystems to deal with social, economic, and environmental effects. This chapter advocates a holistic approach, distinguishing preparedness, emergency responses, and postcrisis phases. The role of the Early Warning System (EWS), Risk modeling and mitigation measures are particularly emphasized. The chapter reviews the various Artificial Intelligence (AI)-enabler technologies that can be leveraged at each phase, focusing on the INFORM risk framework and EWSs. Emergency communication and psychological risk perception have been emphasized in emergency response times. Finally, a set of case studies from Nordic countries has been highlighted.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.18926v1",
    "published_date": "2025-06-20 12:32:16 UTC",
    "updated_date": "2025-06-20 12:32:16 UTC"
  },
  {
    "arxiv_id": "2506.21607v1",
    "title": "CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks",
    "authors": [
      "Dipak Meher",
      "Carlotta Domeniconi",
      "Guadalupe Correa-Cabrera"
    ],
    "abstract": "Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer valuable insights but are unstructured, lexically dense, and filled with ambiguous or shifting references-posing challenges for automated knowledge graph (KG) construction. Existing KG methods often rely on static templates and lack coreference resolution, while recent LLM-based approaches frequently produce noisy, fragmented graphs due to hallucinations, and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG, a modular framework for building interpretable KGs from legal texts. It uses a two-step pipeline: (1) type-aware coreference resolution via sequential, structured LLM prompts, and (2) entity and relationship extraction using domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG reduces node duplication by 33.28%, and legal noise by 38.37% compared to a GraphRAG-based baseline-resulting in cleaner and more coherent graph structures. These improvements make CORE-KG a strong foundation for analyzing complex criminal networks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21607v1",
    "published_date": "2025-06-20 11:58:00 UTC",
    "updated_date": "2025-06-20 11:58:00 UTC"
  },
  {
    "arxiv_id": "2506.16931v1",
    "title": "Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning",
    "authors": [
      "Jiaqi Chen",
      "Mingfeng Fan",
      "Xuefeng Zhang",
      "Jingsong Liang",
      "Yuhong Cao",
      "Guohua Wu",
      "Guillaume Adrien Sartoretti"
    ],
    "abstract": "Effective and efficient task planning is essential for mobile robots, especially in applications like warehouse retrieval and environmental monitoring. These tasks often involve selecting one location from each of several target clusters, forming a Generalized Traveling Salesman Problem (GTSP) that remains challenging to solve both accurately and efficiently. To address this, we propose a Multimodal Fused Learning (MMFL) framework that leverages both graph and image-based representations to capture complementary aspects of the problem, and learns a policy capable of generating high-quality task planning schemes in real time. Specifically, we first introduce a coordinate-based image builder that transforms GTSP instances into spatially informative representations. We then design an adaptive resolution scaling strategy to enhance adaptability across different problem scales, and develop a multimodal fusion module with dedicated bottlenecks that enables effective integration of geometric and spatial features. Extensive experiments show that our MMFL approach significantly outperforms state-of-the-art methods across various GTSP instances while maintaining the computational efficiency required for real-time robotic applications. Physical robot tests further validate its practical effectiveness in real-world scenarios.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 6 figures, under review",
    "pdf_url": "https://arxiv.org/pdf/2506.16931v1",
    "published_date": "2025-06-20 11:51:52 UTC",
    "updated_date": "2025-06-20 11:51:52 UTC"
  },
  {
    "arxiv_id": "2506.16929v1",
    "title": "A deep learning and machine learning approach to predict neonatal death in the context of São Paulo",
    "authors": [
      "Mohon Raihan",
      "Plabon Kumar Saha",
      "Rajan Das Gupta",
      "A Z M Tahmidul Kabir",
      "Afia Anjum Tamanna",
      "Md. Harun-Ur-Rashid",
      "Adnan Bin Abdus Salam",
      "Md Tanvir Anjum",
      "A Z M Ahteshamul Kabir"
    ],
    "abstract": "Neonatal death is still a concerning reality for underdeveloped and even some developed countries. Worldwide data indicate that 26.693 babies out of 1,000 births die, according to Macro Trades. To reduce this number, early prediction of endangered babies is crucial. Such prediction enables the opportunity to take ample care of the child and mother so that early child death can be avoided. In this context, machine learning was used to determine whether a newborn baby is at risk. To train the predictive model, historical data of 1.4 million newborns was used. Machine learning and deep learning techniques such as logical regression, K-nearest neighbor, random forest classifier, extreme gradient boosting (XGBoost), convolutional neural network, and long short-term memory (LSTM) were implemented using the dataset to identify the most accurate model for predicting neonatal mortality. Among the machine learning algorithms, XGBoost and random forest classifier achieved the best accuracy with 94%, while among the deep learning models, LSTM delivered the highest accuracy with 99%. Therefore, using LSTM appears to be the most suitable approach to predict whether precautionary measures for a child are necessary.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16929v1",
    "published_date": "2025-06-20 11:44:48 UTC",
    "updated_date": "2025-06-20 11:44:48 UTC"
  },
  {
    "arxiv_id": "2506.17364v2",
    "title": "AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning",
    "authors": [
      "Alvaro Becerra",
      "Roberto Daza",
      "Ruth Cobos",
      "Aythami Morales",
      "Mutlu Cukurova",
      "Julian Fierrez"
    ],
    "abstract": "This work investigates the use of multimodal biometrics to detect distractions caused by smartphone use during tasks that require sustained attention, with a focus on computer-based online learning. Although the methods are applicable to various domains, such as autonomous driving, we concentrate on the challenges learners face in maintaining engagement amid internal (e.g., motivation), system-related (e.g., course design) and contextual (e.g., smartphone use) factors. Traditional learning platforms often lack detailed behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors provide new insights into learner attention. We propose an AI-based approach that leverages physiological signals and head pose data to detect phone use. Our results show that single biometric signals, such as brain waves or heart rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal model combining all signals reaches 91% accuracy, highlighting the benefits of integration. We conclude by discussing the implications and limitations of deploying these models for real-time support in online learning environments.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted in EC-TEL25: 20th European Conference on Technology Enhanced Learning, Newcastle and Durham, UK, 15-19 September 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17364v2",
    "published_date": "2025-06-20 11:37:19 UTC",
    "updated_date": "2025-06-24 13:38:12 UTC"
  },
  {
    "arxiv_id": "2506.16925v2",
    "title": "Thermometry of simulated Bose--Einstein condensates using machine learning",
    "authors": [
      "Jack Griffiths",
      "Steven A. Wrathmall",
      "Simon A. Gardiner"
    ],
    "abstract": "Precise determination of thermodynamic parameters in ultracold Bose gases remains challenging due to the destructive nature of conventional measurement techniques and inherent experimental uncertainties. We demonstrate a machine learning approach for rapid, non-destructive estimation of the chemical potential and temperature from a single image of an \\emph{in situ} imaged density profiles of finite-temperature Bose gases. Our convolutional neural network is trained exclusively on quasi-2D `pancake' condensates in harmonic trap configurations. It achieves parameter extraction within fractions of a second. The model also demonstrates {some} zero-shot generalisation across both trap geometry and thermalisation dynamics, successfully estimating the temperature (although not the chemical potential) for toroidally trapped condensates with errors of only a few nanokelvin despite no prior exposure to such geometries during training, and maintaining predictive accuracy during dynamic thermalisation processes after a relatively brief evolution without explicit training on non-equilibrium states. These results suggest that supervised learning can overcome traditional limitations in ultracold atom thermometry, with extension to broader geometric configurations, temperature ranges, and additional parameters potentially enabling comprehensive real-time analysis of quantum gas experiments. Such capabilities could significantly streamline experimental workflows whilst improving measurement precision across a range of quantum fluid systems.",
    "categories": [
      "cond-mat.quant-gas",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.quant-gas",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16925v2",
    "published_date": "2025-06-20 11:36:15 UTC",
    "updated_date": "2025-10-28 10:17:19 UTC"
  },
  {
    "arxiv_id": "2506.16924v1",
    "title": "Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines",
    "authors": [
      "Tomoya Kashimata",
      "Yohei Hamakawa",
      "Masaya Yamasaki",
      "Kosuke Tatsumura"
    ],
    "abstract": "Many real-time systems require the optimization of discrete variables. Black-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms perform optimization by repeatedly taking actions and observing the corresponding instant rewards without any prior knowledge. Recently, a BBO method using an Ising machine has been proposed to find the best action that is represented by a combination of discrete values and maximizes the instant reward in static environments. In contrast, dynamic environments, where real-time systems operate, necessitate MAB algorithms that maximize the average reward over multiple trials. However, due to the enormous number of actions resulting from the combinatorial nature of discrete optimization, conventional MAB algorithms cannot effectively optimize dynamic, discrete environments. Here, we show a heuristic MAB method for dynamic, discrete environments by extending the BBO method, in which an Ising machine effectively explores the actions while considering interactions between variables and changes in dynamic environments. We demonstrate the dynamic adaptability of the proposed method in a wireless communication system with moving users.",
    "categories": [
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 6figures",
    "pdf_url": "https://arxiv.org/pdf/2506.16924v1",
    "published_date": "2025-06-20 11:31:43 UTC",
    "updated_date": "2025-06-20 11:31:43 UTC"
  },
  {
    "arxiv_id": "2506.17363v1",
    "title": "A Large-Scale Real-World Evaluation of LLM-Based Virtual Teaching Assistant",
    "authors": [
      "Sunjun Kweon",
      "Sooyohn Nam",
      "Hyunseung Lim",
      "Hwajung Hong",
      "Edward Choi"
    ],
    "abstract": "Virtual Teaching Assistants (VTAs) powered by Large Language Models (LLMs) have the potential to enhance student learning by providing instant feedback and facilitating multi-turn interactions. However, empirical studies on their effectiveness and acceptance in real-world classrooms are limited, leaving their practical impact uncertain. In this study, we develop an LLM-based VTA and deploy it in an introductory AI programming course with 477 graduate students. To assess how student perceptions of the VTA's performance evolve over time, we conduct three rounds of comprehensive surveys at different stages of the course. Additionally, we analyze 3,869 student--VTA interaction pairs to identify common question types and engagement patterns. We then compare these interactions with traditional student--human instructor interactions to evaluate the VTA's role in the learning process. Through a large-scale empirical study and interaction analysis, we assess the feasibility of deploying VTAs in real-world classrooms and identify key challenges for broader adoption. Finally, we release the source code of our VTA system, fostering future advancements in AI-driven education: \\texttt{https://github.com/sean0042/VTA}.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "ACL 2025 Industry Track",
    "pdf_url": "https://arxiv.org/pdf/2506.17363v1",
    "published_date": "2025-06-20 10:59:57 UTC",
    "updated_date": "2025-06-20 10:59:57 UTC"
  },
  {
    "arxiv_id": "2506.16899v2",
    "title": "Towards Effective Complementary Security Analysis using Large Language Models",
    "authors": [
      "Jonas Wagner",
      "Simon Müller",
      "Christian Näther",
      "Jan-Philipp Steghöfer",
      "Andreas Both"
    ],
    "abstract": "A key challenge in security analysis is the manual evaluation of potential security weaknesses generated by static application security testing (SAST) tools. Numerous false positives (FPs) in these reports reduce the effectiveness of security analysis. We propose using Large Language Models (LLMs) to improve the assessment of SAST findings. We investigate the ability of LLMs to reduce FPs while trying to maintain a perfect true positive rate, using datasets extracted from the OWASP Benchmark (v1.2) and a real-world software project. Our results indicate that advanced prompting techniques, such as Chain-of-Thought and Self-Consistency, substantially improve FP detection. Notably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark dataset without missing genuine weaknesses. Combining detections from different LLMs would increase this FP detection to approximately 78.9%. Additionally, we demonstrate our approach's generalizability using a real-world dataset covering five SAST tools, three programming languages, and infrastructure files. The best LLM detected 33.85% of all FPs without missing genuine weaknesses, while combining detections from different LLMs would increase this detection to 38.46%. Our findings highlight the potential of LLMs to complement traditional SAST tools, enhancing automation and reducing resources spent addressing false alarms.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "8 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.16899v2",
    "published_date": "2025-06-20 10:46:35 UTC",
    "updated_date": "2025-07-13 11:10:27 UTC"
  },
  {
    "arxiv_id": "2506.16898v1",
    "title": "AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario",
    "authors": [
      "Ciro Beneduce",
      "Massimiliano Luca",
      "Bruno Lepri"
    ],
    "abstract": "Image generation models are revolutionizing many domains, and urban analysis and design is no exception. While such models are widely adopted, there is a limited literature exploring their geographic knowledge, along with the biases they embed. In this work, we generated 150 synthetic images for each state in the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two state-of-the-art models for image generation. We embed each image using DINO-v2 ViT-S/14 and the Fréchet Inception Distances to measure the similarity between the generated images. We found that while these models have implicitly learned aspects of USA geography, if we prompt the models to generate an image for \"United States\" instead of specific cities or states, the models exhibit a strong representative bias toward metropolis-like areas, excluding rural states and smaller cities. {\\color{black} In addition, we found that models systematically exhibit some entity-disambiguation issues with European-sounding names like Frankfort or Devon.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16898v1",
    "published_date": "2025-06-20 10:43:22 UTC",
    "updated_date": "2025-06-20 10:43:22 UTC"
  },
  {
    "arxiv_id": "2506.21606v1",
    "title": "Large Language Models as symbolic DNA of cultural dynamics",
    "authors": [
      "Parham Pourdavood",
      "Michael Jacob",
      "Terrence Deacon"
    ],
    "abstract": "This paper proposes a novel conceptualization of Large Language Models (LLMs) as externalized informational substrates that function analogously to DNA for human cultural dynamics. Rather than viewing LLMs as either autonomous intelligence or mere programmed mimicry, we argue they serve a broader role as repositories that preserve compressed patterns of human symbolic expression--\"fossils\" of meaningful dynamics that retain relational residues without their original living contexts. Crucially, these compressed patterns only become meaningful through human reinterpretation, creating a recursive feedback loop where they can be recombined and cycle back to ultimately catalyze human creative processes. Through analysis of four universal features--compression, decompression, externalization, and recursion--we demonstrate that just as DNA emerged as a compressed and externalized medium for preserving useful cellular dynamics without containing explicit reference to goal-directed physical processes, LLMs preserve useful regularities of human culture without containing understanding of embodied human experience. Therefore, we argue that LLMs' significance lies not in rivaling human intelligence, but in providing humanity a tool for self-reflection and playful hypothesis-generation in a low-stakes, simulated environment. This framework positions LLMs as tools for cultural evolvability, enabling humanity to generate novel hypotheses about itself while maintaining the human interpretation necessary to ground these hypotheses in ongoing human aesthetics and norms.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2506.21606v1",
    "published_date": "2025-06-20 10:37:29 UTC",
    "updated_date": "2025-06-20 10:37:29 UTC"
  },
  {
    "arxiv_id": "2506.16895v2",
    "title": "With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You",
    "authors": [
      "Fabian Gröger",
      "Shuo Wen",
      "Huyen Le",
      "Maria Brbić"
    ],
    "abstract": "Multimodal models have demonstrated powerful capabilities in complex tasks requiring multimodal alignment, including zero-shot classification and cross-modal retrieval. However, existing models typically rely on millions of paired multimodal samples, which are prohibitively expensive or infeasible to obtain in many domains. In this work, we explore the feasibility of building multimodal models with limited amount of paired data by aligning pretrained unimodal foundation models. We show that high-quality alignment is possible with as few as tens of thousands of paired samples$\\unicode{x2013}$less than $1\\%$ of the data typically used in the field. To achieve this, we introduce STRUCTURE, an effective regularization technique that preserves the neighborhood geometry of the latent space of unimodal encoders. Additionally, we show that aligning last layers is often suboptimal and demonstrate the benefits of aligning the layers with the highest representational similarity across modalities. These two components can be readily incorporated into existing alignment methods, yielding substantial gains across 24 zero-shot image classification and retrieval benchmarks, with average relative improvement of $51.6\\%$ in classification and $91.8\\%$ in retrieval tasks. Our results highlight the effectiveness and broad applicability of our framework for limited-sample multimodal learning and offer a promising path forward for resource-constrained domains.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2025 camera-ready",
    "pdf_url": "https://arxiv.org/pdf/2506.16895v2",
    "published_date": "2025-06-20 10:32:54 UTC",
    "updated_date": "2025-10-22 09:04:53 UTC"
  },
  {
    "arxiv_id": "2506.16884v2",
    "title": "The Importance of Being Lazy: Scaling Limits of Continual Learning",
    "authors": [
      "Jacopo Graldi",
      "Alessandro Breccia",
      "Giulia Lanzillotta",
      "Thomas Hofmann",
      "Lorenzo Noci"
    ],
    "abstract": "Despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far from complete. In this work, we perform a systematic study on the impact of model scale and the degree of feature learning in continual learning. We reconcile existing contradictory observations on scale in the literature, by differentiating between lazy and rich training regimes through a variable parameterization of the architecture. We show that increasing model width is only beneficial when it reduces the amount of feature learning, yielding more laziness. Using the framework of dynamical mean field theory, we then study the infinite width dynamics of the model in the feature learning regime and characterize CF, extending prior theoretical results limited to the lazy regime. We study the intricate relationship between feature learning, task non-stationarity, and forgetting, finding that high feature learning is only beneficial with highly similar tasks. We identify a transition modulated by task similarity where the model exits an effectively lazy regime with low forgetting to enter a rich regime with significant forgetting. Finally, our findings reveal that neural networks achieve optimal performance at a critical level of feature learning, which depends on task non-stationarity and transfers across model scales. This work provides a unified perspective on the role of scale and feature learning in continual learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the 42nd International Conference on Machine Learning (2025). JG and AB contributed equally to this work",
    "pdf_url": "https://arxiv.org/pdf/2506.16884v2",
    "published_date": "2025-06-20 10:12:38 UTC",
    "updated_date": "2025-08-13 13:37:01 UTC"
  },
  {
    "arxiv_id": "2506.21605v1",
    "title": "MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents",
    "authors": [
      "Haoran Tan",
      "Zeyu Zhang",
      "Chen Ma",
      "Xu Chen",
      "Quanyu Dai",
      "Zhenhua Dong"
    ],
    "abstract": "Recent works have highlighted the significance of memory mechanisms in LLM-based agents, which enable them to store observed information and adapt to dynamic environments. However, evaluating their memory capabilities still remains challenges. Previous evaluations are commonly limited by the diversity of memory levels and interactive scenarios. They also lack comprehensive metrics to reflect the memory capabilities from multiple aspects. To address these problems, in this paper, we construct a more comprehensive dataset and benchmark to evaluate the memory capability of LLM-based agents. Our dataset incorporates factual memory and reflective memory as different levels, and proposes participation and observation as various interactive scenarios. Based on our dataset, we present a benchmark, named MemBench, to evaluate the memory capability of LLM-based agents from multiple aspects, including their effectiveness, efficiency, and capacity. To benefit the research community, we release our dataset and project at https://github.com/import-myself/Membench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 5 figures. Accepted by ACL 2025 findings",
    "pdf_url": "https://arxiv.org/pdf/2506.21605v1",
    "published_date": "2025-06-20 10:09:23 UTC",
    "updated_date": "2025-06-20 10:09:23 UTC"
  },
  {
    "arxiv_id": "2506.16856v1",
    "title": "ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control",
    "authors": [
      "Jun Fu",
      "Bin Tian",
      "Haonan Chen",
      "Shi Meng",
      "Tingting Yao"
    ],
    "abstract": "Autonomous parking plays a vital role in intelligent vehicle systems, particularly in constrained urban environments where high-precision control is required. While traditional rule-based parking systems struggle with environmental uncertainties and lack adaptability in crowded or dynamic scenes, human drivers demonstrate the ability to park intuitively without explicit modeling. Inspired by this observation, we propose a Transformer-based end-to-end framework for autonomous parking that learns from expert demonstrations. The network takes as input surround-view camera images, goal-point representations, ego vehicle motion, and pedestrian trajectories. It outputs discrete control sequences including throttle, braking, steering, and gear selection. A novel cross-attention module integrates BEV features with target points, and a GRU-based pedestrian predictor enhances safety by modeling dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both vertical and parallel parking scenarios. Experiments show our model achieves a high success rate of 96.57\\%, with average positional and orientation errors of 0.21 meters and 0.41 degrees, respectively. The ablation studies further demonstrate the effectiveness of key modules such as pedestrian prediction and goal-point attention fusion. The code and dataset will be released at: https://github.com/little-snail-f/ParkFormer.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16856v1",
    "published_date": "2025-06-20 09:14:09 UTC",
    "updated_date": "2025-06-20 09:14:09 UTC"
  },
  {
    "arxiv_id": "2506.16844v1",
    "title": "Bandwidth Selectors on Semiparametric Bayesian Networks",
    "authors": [
      "Victor Alejandre",
      "Concha Bielza",
      "Pedro Larrañaga"
    ],
    "abstract": "Semiparametric Bayesian networks (SPBNs) integrate parametric and non-parametric probabilistic models, offering flexibility in learning complex data distributions from samples. In particular, kernel density estimators (KDEs) are employed for the non-parametric component. Under the assumption of data normality, the normal rule is used to learn the bandwidth matrix for the KDEs in SPBNs. This matrix is the key hyperparameter that controls the trade-off between bias and variance. However, real-world data often deviates from normality, potentially leading to suboptimal density estimation and reduced predictive performance. This paper first establishes the theoretical framework for the application of state-of-the-art bandwidth selectors and subsequently evaluates their impact on SPBN performance. We explore the approaches of cross-validation and plug-in selectors, assessing their effectiveness in enhancing the learning capability and applicability of SPBNs. To support this investigation, we have extended the open-source package PyBNesian for SPBNs with the additional bandwidth selection techniques and conducted extensive experimental analyses. Our results demonstrate that the proposed bandwidth selectors leverage increasing information more effectively than the normal rule, which, despite its robustness, stagnates with more data. In particular, unbiased cross-validation generally outperforms the normal rule, highlighting its advantage in high sample size scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "37 pages, 15 figures. Submitted to Information Sciences",
    "pdf_url": "https://arxiv.org/pdf/2506.16844v1",
    "published_date": "2025-06-20 08:48:05 UTC",
    "updated_date": "2025-06-20 08:48:05 UTC"
  },
  {
    "arxiv_id": "2506.16826v1",
    "title": "AnyTraverse: An off-road traversability framework with VLM and human operator in the loop",
    "authors": [
      "Sattwik Sahu",
      "Agamdeep Singh",
      "Karthik Nambiar",
      "Srikanth Saripalli",
      "P. B. Sujit"
    ],
    "abstract": "Off-road traversability segmentation enables autonomous navigation with applications in search-and-rescue, military operations, wildlife exploration, and agriculture. Current frameworks struggle due to significant variations in unstructured environments and uncertain scene changes, and are not adaptive to be used for different robot types. We present AnyTraverse, a framework combining natural language-based prompts with human-operator assistance to determine navigable regions for diverse robotic vehicles. The system segments scenes for a given set of prompts and calls the operator only when encountering previously unexplored scenery or unknown class not part of the prompt in its region-of-interest, thus reducing active supervision load while adapting to varying outdoor scenes. Our zero-shot learning approach eliminates the need for extensive data collection or retraining. Our experimental validation includes testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate real-world deployment on multiple robot platforms. The results show that AnyTraverse performs better than GA-NAV and Off-seg while offering a vehicle-agnostic approach to off-road traversability that balances automation with targeted human supervision.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16826v1",
    "published_date": "2025-06-20 08:31:13 UTC",
    "updated_date": "2025-06-20 08:31:13 UTC"
  },
  {
    "arxiv_id": "2506.16822v1",
    "title": "Learning Dexterous Object Handover",
    "authors": [
      "Daniel Frau-Alfaro",
      "Julio Castaño-Amoros",
      "Santiago Puente",
      "Pablo Gil",
      "Roberto Calandra"
    ],
    "abstract": "Object handover is an important skill that we use daily when interacting with other humans. To deploy robots in collaborative setting, like houses, being able to receive and handing over objects safely and efficiently becomes a crucial skill. In this work, we demonstrate the use of Reinforcement Learning (RL) for dexterous object handover between two multi-finger hands. Key to this task is the use of a novel reward function based on dual quaternions to minimize the rotation distance, which outperforms other rotation representations such as Euler and rotation matrices. The robustness of the trained policy is experimentally evaluated by testing w.r.t. objects that are not included in the training distribution, and perturbations during the handover process. The results demonstrate that the trained policy successfully perform this task, achieving a total success rate of 94% in the best-case scenario after 100 experiments, thereby showing the robustness of our policy with novel objects. In addition, the best-case performance of the policy decreases by only 13.8% when the other robot moves during the handover, proving that our policy is also robust to this type of perturbation, which is common in real-world object handovers.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Paper accepted for presentation in RoMan 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.16822v1",
    "published_date": "2025-06-20 08:22:46 UTC",
    "updated_date": "2025-06-20 08:22:46 UTC"
  },
  {
    "arxiv_id": "2506.16819v2",
    "title": "Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection",
    "authors": [
      "Yuchu Jiang",
      "Jiaming Chu",
      "Jian Zhao",
      "Xin Zhang",
      "Xu Yang",
      "Lei Jin",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "The proliferation of generative models has raised serious concerns about visual content forgery. Existing deepfake detection methods primarily target either image-level classification or pixel-wise localization. While some achieve high accuracy, they often suffer from limited generalization across manipulation types or rely on complex architectures. In this paper, we propose Loupe, a lightweight yet effective framework for joint deepfake detection and localization. Loupe integrates a patch-aware classifier and a segmentation module with conditional queries, allowing simultaneous global authenticity classification and fine-grained mask prediction. To enhance robustness against distribution shifts of test set, Loupe introduces a pseudo-label-guided test-time adaptation mechanism by leveraging patch-level predictions to supervise the segmentation head. Extensive experiments on the DDL dataset demonstrate that Loupe achieves state-of-the-art performance, securing the first place in the IJCAI 2025 Deepfake Detection and Localization Challenge with an overall score of 0.846. Our results validate the effectiveness of the proposed patch-level fusion and conditional query design in improving both classification accuracy and spatial localization under diverse forgery patterns. The code is available at https://github.com/Kamichanw/Loupe.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "There is some controversy over the methods of the content",
    "pdf_url": "https://arxiv.org/pdf/2506.16819v2",
    "published_date": "2025-06-20 08:18:44 UTC",
    "updated_date": "2026-01-05 02:29:08 UTC"
  },
  {
    "arxiv_id": "2506.17357v1",
    "title": "Speeding up Local Optimization in Vehicle Routing with Tensor-based GPU Acceleration",
    "authors": [
      "Zhenyu Lei",
      "Jin-Kao Hao",
      "Qinghua Wu"
    ],
    "abstract": "Local search plays a central role in many effective heuristic algorithms for the vehicle routing problem (VRP) and its variants. However, neighborhood exploration is known to be computationally expensive and time consuming, especially for large instances or problems with complex constraints. In this study, we explore a promising direction to address this challenge by introducing an original tensor-based GPU acceleration method designed to speed up the commonly used local search operators in vehicle routing. By using an attribute-based representation, the method offers broad extensibility, making it applicable to different VRP variants. Its low-coupling architecture, with intensive computations completely offloaded to the GPU, ensures seamless integration in various local search-based algorithms and frameworks, leading to significant improvements in computational efficiency and potentially improved solution quality. Through comparative experiments on benchmark instances of three routing problems, we demonstrate the substantial computational advantages of the proposed approach over traditional CPU-based implementations. We also provide a detailed analysis of the strengths and limitations of the method, providing valuable insights into its performance characteristics and identifying potential bottlenecks in practical applications. These findings contribute to a better understanding and suggest directions for future improvements.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17357v1",
    "published_date": "2025-06-20 07:40:47 UTC",
    "updated_date": "2025-06-20 07:40:47 UTC"
  },
  {
    "arxiv_id": "2506.16795v1",
    "title": "Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning",
    "authors": [
      "Chengpeng Hu",
      "Ziming Wang",
      "Bo Yuan",
      "Jialin Liu",
      "Chengqi Zhang",
      "Xin Yao"
    ],
    "abstract": "Dynamic material handling (DMH) involves the assignment of dynamically arriving material transporting tasks to suitable vehicles in real time for minimising makespan and tardiness. In real-world scenarios, historical task records are usually available, which enables the training of a decision policy on multiple instances consisting of historical records. Recently, reinforcement learning has been applied to solve DMH. Due to the occurrence of dynamic events such as new tasks, adaptability is highly required. Solving DMH is challenging since constraints including task delay should be satisfied. A feedback is received only when all tasks are served, which leads to sparse reward. Besides, making the best use of limited computational resources and historical records for training a robust policy is crucial. The time allocated to different problem instances would highly impact the learning process. To tackle those challenges, this paper proposes a novel adaptive constrained evolutionary reinforcement learning (ACERL) approach, which maintains a population of actors for diverse exploration. ACERL accesses each actor for tackling sparse rewards and constraint violation to restrict the behaviour of the policy. Moreover, ACERL adaptively selects the most beneficial training instances for improving the policy. Extensive experiments on eight training and eight unseen test instances demonstrate the outstanding performance of ACERL compared with several state-of-the-art algorithms. Policies trained by ACERL can schedule the vehicles while fully satisfying the constraints. Additional experiments on 40 unseen noised instances show the robust performance of ACERL. Cross-validation further presents the overall effectiveness of ACREL. Besides, a rigorous ablation study highlights the coordination and benefits of each ingredient of ACERL.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16795v1",
    "published_date": "2025-06-20 07:20:22 UTC",
    "updated_date": "2025-06-20 07:20:22 UTC"
  },
  {
    "arxiv_id": "2506.16792v3",
    "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning",
    "authors": [
      "Muyang Zheng",
      "Yuanzhi Yao",
      "Changting Lin",
      "Caihong Kai",
      "Yanxiang Chen",
      "Zhiquan Liu"
    ],
    "abstract": "Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks -- methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version -- order-determining optimization. We conduct extensive experiments on two datasets using two open-source and four closed-source models. Results show that MIST achieves competitive attack success rate, relatively low query count, and fair transferability, outperforming or matching state-of-the-art jailbreak methods. Additionally, we conduct analysis on computational efficiency to validate the practical viability of MIST.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.16792v3",
    "published_date": "2025-06-20 07:16:47 UTC",
    "updated_date": "2025-09-20 03:27:21 UTC"
  },
  {
    "arxiv_id": "2506.16791v4",
    "title": "TabArena: A Living Benchmark for Machine Learning on Tabular Data",
    "authors": [
      "Nick Erickson",
      "Lennart Purucker",
      "Andrej Tschalzev",
      "David Holzmüller",
      "Prateek Mutalik Desai",
      "David Salinas",
      "Frank Hutter"
    ],
    "abstract": "With the growing popularity of deep learning and foundation models for tabular data, the need for standardized and reliable benchmarks is higher than ever. However, current benchmarks are static. Their design is not updated even if flaws are discovered, model versions are updated, or new models are released. To address this, we introduce TabArena, the first continuously maintained living tabular benchmarking system. To launch TabArena, we manually curate a representative collection of datasets and well-implemented models, conduct a large-scale benchmarking study to initialize a public leaderboard, and assemble a team of experienced maintainers. Our results highlight the influence of validation method and ensembling of hyperparameter configurations to benchmark models at their full potential. While gradient-boosted trees are still strong contenders on practical tabular datasets, we observe that deep learning methods have caught up under larger time budgets with ensembling. At the same time, foundation models excel on smaller datasets. Finally, we show that ensembles across models advance the state-of-the-art in tabular machine learning. We observe that some deep learning models are overrepresented in cross-model ensembles due to validation set overfitting, and we encourage model developers to address this issue. We launch TabArena with a public leaderboard, reproducible code, and maintenance protocols to create a living benchmark available at https://tabarena.ai.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted (spotlight) at NeurIPS 2025 Datasets and Benchmarks Track. v4: fixed links in comments. v3: NeurIPS camera-ready version. v2: fixed author list. 51 pages. Code available at https://tabarena.ai/code and examples at https://tabarena.ai/code-examples and dataset curation at https://tabarena.ai/data-tabular-ml-iid-study and https://tabarena.ai/dataset-curation",
    "pdf_url": "https://arxiv.org/pdf/2506.16791v4",
    "published_date": "2025-06-20 07:14:48 UTC",
    "updated_date": "2025-11-03 18:47:03 UTC"
  },
  {
    "arxiv_id": "2506.17356v1",
    "title": "Automatic Large Language Models Creation of Interactive Learning Lessons",
    "authors": [
      "Jionghao Lin",
      "Jiarui Rao",
      "Yiyang Zhao",
      "Yuting Wang",
      "Ashish Gurung",
      "Amanda Barany",
      "Jaclyn Ocumpaugh",
      "Ryan S. Baker",
      "Kenneth R. Koedinger"
    ],
    "abstract": "We explore the automatic generation of interactive, scenario-based lessons designed to train novice human tutors who teach middle school mathematics online. Employing prompt engineering through a Retrieval-Augmented Generation approach with GPT-4o, we developed a system capable of creating structured tutor training lessons. Our study generated lessons in English for three key topics: Encouraging Students' Independence, Encouraging Help-Seeking Behavior, and Turning on Cameras, using a task decomposition prompting strategy that breaks lesson generation into sub-tasks. The generated lessons were evaluated by two human evaluators, who provided both quantitative and qualitative evaluations using a comprehensive rubric informed by lesson design research. Results demonstrate that the task decomposition strategy led to higher-rated lessons compared to single-step generation. Human evaluators identified several strengths in the LLM-generated lessons, including well-structured content and time-saving potential, while also noting limitations such as generic feedback and a lack of clarity in some instructional sections. These findings underscore the potential of hybrid human-AI approaches for generating effective lessons in tutor training.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "Full Research Paper, 15 pages, In Proceedings of 20th European Conference on Technology Enhanced Learning (ECTEL2025)",
    "pdf_url": "https://arxiv.org/pdf/2506.17356v1",
    "published_date": "2025-06-20 06:58:50 UTC",
    "updated_date": "2025-06-20 06:58:50 UTC"
  },
  {
    "arxiv_id": "2506.16782v2",
    "title": "What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity",
    "authors": [
      "Youjin Kong"
    ],
    "abstract": "Fairness in machine learning (ML) has become a rapidly growing area of research. But why, in the first place, is unfairness in ML wrong? And why should we care about improving fairness? Most fair-ML research implicitly appeals to distributive equality: the idea that desirable benefits and goods, such as opportunities (e.g., Barocas et al., 2023), should be equally distributed across society. Unfair ML models, then, are seen as wrong because they unequally distribute such benefits. This paper argues that this exclusive focus on distributive equality offers an incomplete and potentially misleading ethical foundation. Grounding ML fairness in egalitarianism--the view that equality is a fundamental moral and social ideal--requires challenging structural inequality: systematic, institutional, and durable arrangements that privilege some groups while disadvantaging others. Structural inequality manifests through ML systems in two primary forms: allocative harms (e.g., economic loss) and representational harms (e.g., stereotypes, erasure). While distributive equality helps address allocative harms, it fails to explain why representational harms are wrong--why it is wrong for ML systems to reinforce social hierarchies that stratify people into superior and inferior groups--and why ML systems should aim to foster a society where people relate as equals (i.e., relational equality). To address these limitations, the paper proposes a multifaceted egalitarian framework for ML fairness that integrates both distributive and relational equality. Drawing on critical social and political philosophy, this framework offers a more comprehensive ethical foundation for tackling the full spectrum of harms perpetuated by ML systems. The paper also outlines practical pathways for implementing the framework across the entire ML pipeline.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at ACM FAccT 2025; Forthcoming in ACM Journal on Responsible Computing",
    "pdf_url": "https://arxiv.org/pdf/2506.16782v2",
    "published_date": "2025-06-20 06:57:53 UTC",
    "updated_date": "2025-09-01 15:22:37 UTC"
  },
  {
    "arxiv_id": "2506.16776v1",
    "title": "PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model",
    "authors": [
      "Beomseok Ko",
      "Hyeryung Jang"
    ],
    "abstract": "Diffusion models excel in image generation but are computational and resource-intensive due to their reliance on iterative Markov chain processes, leading to error accumulation and limiting the effectiveness of naive compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid compression framework combining Progressive Quantization (PQ) and Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs a two-stage quantization with adaptive bit-width transitions guided by a momentum-based mechanism, reducing excessive weight perturbations in low-precision. CAD leverages full-precision calibration datasets during distillation, enabling the student to match full-precision performance even with a quantized teacher. As a result, PQCAD-DM achieves a balance between computational efficiency and generative quality, halving inference time while maintaining competitive performance. Extensive experiments validate PQCAD-DM's superior generative capabilities and efficiency across diverse datasets, outperforming fixed-bit quantization methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.16776v1",
    "published_date": "2025-06-20 06:43:27 UTC",
    "updated_date": "2025-06-20 06:43:27 UTC"
  },
  {
    "arxiv_id": "2506.16764v2",
    "title": "Reinforcement Learning for Hybrid Charging Stations Planning and Operation Considering Fixed and Mobile Chargers",
    "authors": [
      "Yanchen Zhu",
      "Honghui Zou",
      "Chufan Liu",
      "Yuyu Luo",
      "Yuankai Wu",
      "Yuxuan Liang"
    ],
    "abstract": "The success of vehicle electrification relies on efficient and adaptable charging infrastructure. Fixed-location charging stations often suffer from underutilization or congestion due to fluctuating demand, while mobile chargers offer flexibility by relocating as needed. This paper studies the optimal planning and operation of hybrid charging infrastructures that combine both fixed and mobile chargers within urban road networks. We formulate the Hybrid Charging Station Planning and Operation (HCSPO) problem, jointly optimizing the placement of fixed stations and the scheduling of mobile chargers. A charging demand prediction model based on Model Predictive Control (MPC) supports dynamic decision-making. To solve the HCSPO problem, we propose a deep reinforcement learning approach enhanced with heuristic scheduling. Experiments on real-world urban scenarios show that our method improves infrastructure availability - achieving up to 244.4% increase in coverage - and reduces user inconvenience with up to 79.8% shorter waiting times, compared to existing solutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.16764v2",
    "published_date": "2025-06-20 05:51:02 UTC",
    "updated_date": "2025-08-10 17:48:58 UTC"
  },
  {
    "arxiv_id": "2506.16755v1",
    "title": "Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly",
    "authors": [
      "Lance Ying",
      "Ryan Truong",
      "Katherine M. Collins",
      "Cedegao E. Zhang",
      "Megan Wei",
      "Tyler Brooke-Wilson",
      "Tan Zhi-Xuan",
      "Lionel Wong",
      "Joshua B. Tenenbaum"
    ],
    "abstract": "Drawing real world social inferences usually requires taking into account information from multiple modalities. Language is a particularly powerful source of information in social settings, especially in novel situations where language can provide both abstract information about the environment dynamics and concrete specifics about an agent that cannot be easily visually observed. In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a framework for drawing context-specific social inferences that integrate linguistic and visual inputs. LIRAS frames multimodal social reasoning as a process of constructing structured but situation-specific agent and environment representations - leveraging multimodal language models to parse language and visual inputs into unified symbolic representations, over which a Bayesian inverse planning engine can be run to produce granular probabilistic judgments. On a range of existing and new social reasoning tasks derived from cognitive science experiments, we find that our model (instantiated with a comparatively lightweight VLM) outperforms ablations and state-of-the-art models in capturing human judgments across all domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5 figures, 19 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.16755v1",
    "published_date": "2025-06-20 05:21:42 UTC",
    "updated_date": "2025-06-20 05:21:42 UTC"
  },
  {
    "arxiv_id": "2506.16754v1",
    "title": "Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding",
    "authors": [
      "Jongmin Park",
      "Seunghoon Han",
      "Won-Yong Shin",
      "Sungsu Lim"
    ],
    "abstract": "The hyperbolic space, characterized by a constant negative curvature and exponentially expanding space, aligns well with the structural properties of heterogeneous graphs. However, although heterogeneous graphs inherently possess diverse power-law structures, most hyperbolic heterogeneous graph embedding models rely on a single hyperbolic space. This approach may fail to effectively capture the diverse power-law structures within heterogeneous graphs. To address this limitation, we propose a Metapath-based Hyperbolic Contrastive Learning framework (MHCL), which uses multiple hyperbolic spaces to capture diverse complex structures within heterogeneous graphs. Specifically, by learning each hyperbolic space to describe the distribution of complex structures corresponding to each metapath, it is possible to capture semantic information effectively. Since metapath embeddings represent distinct semantic information, preserving their discriminability is important when aggregating them to obtain node representations. Therefore, we use a contrastive learning approach to optimize MHCL and improve the discriminability of metapath embeddings. In particular, our contrastive learning method minimizes the distance between embeddings of the same metapath and maximizes the distance between those of different metapaths in hyperbolic space, thereby improving the separability of metapath embeddings with distinct semantic information. We conduct comprehensive experiments to evaluate the effectiveness of MHCL. The experimental results demonstrate that MHCL outperforms state-of-the-art baselines in various graph machine learning tasks, effectively capturing the complex structures of heterogeneous graphs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.16754v1",
    "published_date": "2025-06-20 05:19:11 UTC",
    "updated_date": "2025-06-20 05:19:11 UTC"
  },
  {
    "arxiv_id": "2506.16753v1",
    "title": "Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation",
    "authors": [
      "Kosuke Nakanishi",
      "Akihiro Kubo",
      "Yuji Yasui",
      "Shin Ishii"
    ],
    "abstract": "Recently, robust reinforcement learning (RL) methods designed to handle adversarial input observations have received significant attention, motivated by RL's inherent vulnerabilities. While existing approaches have demonstrated reasonable success, addressing worst-case scenarios over long time horizons requires both minimizing the agent's cumulative rewards for adversaries and training agents to counteract them through alternating learning. However, this process introduces mutual dependencies between the agent and the adversary, making interactions with the environment inefficient and hindering the development of off-policy methods. In this work, we propose a novel off-policy method that eliminates the need for additional environmental interactions by reformulating adversarial learning as a soft-constrained optimization problem. Our approach is theoretically supported by the symmetric property of policy evaluation between the agent and the adversary. The implementation is available at https://github.com/nakanakakosuke/VALT_SAC.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML2025 poster, 39 pages, 6 figures, 13 tables. arXiv admin note: text overlap with arXiv:2409.00418",
    "pdf_url": "https://arxiv.org/pdf/2506.16753v1",
    "published_date": "2025-06-20 05:13:10 UTC",
    "updated_date": "2025-06-20 05:13:10 UTC"
  },
  {
    "arxiv_id": "2506.16741v1",
    "title": "RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching",
    "authors": [
      "Hyun Joon Park",
      "Jeongmin Liu",
      "Jin Sob Kim",
      "Jeong Yeol Yang",
      "Sung Won Han",
      "Eunwoo Song"
    ],
    "abstract": "We introduce RapFlow-TTS, a rapid and high-fidelity TTS acoustic model that leverages velocity consistency constraints in flow matching (FM) training. Although ordinary differential equation (ODE)-based TTS generation achieves natural-quality speech, it typically requires a large number of generation steps, resulting in a trade-off between quality and inference speed. To address this challenge, RapFlow-TTS enforces consistency in the velocity field along the FM-straightened ODE trajectory, enabling consistent synthetic quality with fewer generation steps. Additionally, we introduce techniques such as time interval scheduling and adversarial learning to further enhance the quality of the few-step synthesis. Experimental results show that RapFlow-TTS achieves high-fidelity speech synthesis with a 5- and 10-fold reduction in synthesis steps than the conventional FM- and score-based approaches, respectively.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted on Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.16741v1",
    "published_date": "2025-06-20 04:19:29 UTC",
    "updated_date": "2025-06-20 04:19:29 UTC"
  },
  {
    "arxiv_id": "2506.16738v1",
    "title": "LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization",
    "authors": [
      "Daejin Jo",
      "Jeeyoung Yun",
      "Byungseok Roh",
      "Sungwoong Kim"
    ],
    "abstract": "With the rapid progress of speech language models (SLMs), discrete speech tokens have emerged as a core interface between speech and text, enabling unified modeling across modalities. Recent speech tokenization approaches aim to isolate semantic information from low-level acoustics to better align with language models. In particular, previous methods use SSL teachers such as HuBERT to extract semantic representations, which are then distilled into a semantic quantizer to suppress acoustic redundancy as well as capture content-related latent structures. However, they still produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques, such as rigid average pooling across frames, can distort or dilute the semantic structure required for effective LM alignment. To address this, we propose LM-SPT, a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, we reconstruct speech solely from semantic tokens and minimize the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder. This indirect yet data-driven supervision enables the tokenizer to learn discrete units that are more semantically aligned with language models. LM-SPT further incorporates architectural improvements to the encoder and decoder for speech tokenization, and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz. Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines, and that SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16738v1",
    "published_date": "2025-06-20 04:15:14 UTC",
    "updated_date": "2025-06-20 04:15:14 UTC"
  },
  {
    "arxiv_id": "2506.16732v1",
    "title": "On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis",
    "authors": [
      "Fanchen Bu",
      "Kijung Shin"
    ],
    "abstract": "In unsupervised combinatorial optimization (UCO), during training, one aims to have continuous decisions that are promising in a probabilistic sense for each training instance, which enables end-to-end training on initially discrete and non-differentiable problems. At the test time, for each test instance, starting from continuous decisions, derandomization is typically applied to obtain the final deterministic decisions. Researchers have developed more and more powerful test-time derandomization schemes to enhance the empirical performance and the theoretical guarantee of UCO methods. However, we notice a misalignment between training and testing in the existing UCO methods. Consequently, lower training losses do not necessarily entail better post-derandomization performance, even for the training instances without any data distribution shift. Empirically, we indeed observe such undesirable cases. We explore a preliminary idea to better align training and testing in UCO by including a differentiable version of derandomization into training. Our empirical exploration shows that such an idea indeed improves training-test alignment, but also introduces nontrivial challenges into training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DM",
      "math.PR"
    ],
    "primary_category": "cs.LG",
    "comment": "2nd Workshop on Test-Time Adaptation: Putting Updates to the Test @ ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.16732v1",
    "published_date": "2025-06-20 04:05:09 UTC",
    "updated_date": "2025-06-20 04:05:09 UTC"
  },
  {
    "arxiv_id": "2506.16731v1",
    "title": "Incentivizing High-quality Participation From Federated Learning Agents",
    "authors": [
      "Jinlong Pang",
      "Jiaheng Wei",
      "Yifan Hua",
      "Chen Qian",
      "Yang Liu"
    ],
    "abstract": "Federated learning (FL) provides a promising paradigm for facilitating collaboration between multiple clients that jointly learn a global model without directly sharing their local data. However, existing research suffers from two caveats: 1) From the perspective of agents, voluntary and unselfish participation is often assumed. But self-interested agents may opt out of the system or provide low-quality contributions without proper incentives; 2) From the mechanism designer's perspective, the aggregated models can be unsatisfactory as the existing game-theoretical federated learning approach for data collection ignores the potential heterogeneous effort caused by contributed data. To alleviate above challenges, we propose an incentive-aware framework for agent participation that considers data heterogeneity to accelerate the convergence process. Specifically, we first introduce the notion of Wasserstein distance to explicitly illustrate the heterogeneous effort and reformulate the existing upper bound of convergence. To induce truthful reporting from agents, we analyze and measure the generalization error gap of any two agents by leveraging the peer prediction mechanism to develop score functions. We further present a two-stage Stackelberg game model that formalizes the process and examines the existence of equilibrium. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed mechanism.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16731v1",
    "published_date": "2025-06-20 03:58:39 UTC",
    "updated_date": "2025-06-20 03:58:39 UTC"
  },
  {
    "arxiv_id": "2506.16724v2",
    "title": "The Role of Model Confidence on Bias Effects in Measured Uncertainties for Vision-Language Models",
    "authors": [
      "Xinyi Liu",
      "Weiguang Wang",
      "Hangfeng He"
    ],
    "abstract": "With the growing adoption of Large Language Models (LLMs) for open-ended tasks, accurately assessing epistemic uncertainty, which reflects a model's lack of knowledge, has become crucial to ensuring reliable outcomes. However, quantifying epistemic uncertainty in such tasks is challenging due to the presence of aleatoric uncertainty, which arises from multiple valid answers. While bias can introduce noise into epistemic uncertainty estimation, it may also reduce noise from aleatoric uncertainty. To investigate this trade-off, we conduct experiments on Visual Question Answering (VQA) tasks and find that mitigating prompt-introduced bias improves uncertainty quantification in GPT-4o. Building on prior work showing that LLMs tend to copy input information when model confidence is low, we further analyze how these prompt biases affect measured epistemic and aleatoric uncertainty across varying bias-free confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases have greater effects in both uncertainties when bias-free model confidence is lower. Moreover, lower bias-free model confidence is associated with greater bias-induced underestimation of epistemic uncertainty, resulting in overconfident estimates, whereas it has no significant effect on the direction of bias effect in aleatoric uncertainty estimation. These distinct effects deepen our understanding of bias mitigation for uncertainty quantification and potentially inform the development of more advanced techniques.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP Findings",
    "pdf_url": "https://arxiv.org/pdf/2506.16724v2",
    "published_date": "2025-06-20 03:43:10 UTC",
    "updated_date": "2025-10-09 06:08:47 UTC"
  },
  {
    "arxiv_id": "2506.16723v1",
    "title": "TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data",
    "authors": [
      "Yuping Yan",
      "Yizhi Wang",
      "Yuanshuai Li",
      "Yaochu Jin"
    ],
    "abstract": "Serial pipeline training is an efficient paradigm for handling data heterogeneity in cross-silo federated learning with low communication overhead. However, even without centralized aggregation, direct transfer of models between clients can violate privacy regulations and remain susceptible to gradient leakage and linkage attacks. Additionally, ensuring resilience against semi-honest or malicious clients who may manipulate or misuse received models remains a grand challenge, particularly in privacy-sensitive domains such as healthcare. To address these challenges, we propose TriCon-SF, a novel serial federated learning framework that integrates triple shuffling and contribution awareness. TriCon-SF introduces three levels of randomization by shuffling model layers, data segments, and training sequences to break deterministic learning patterns and disrupt potential attack vectors, thereby enhancing privacy and robustness. In parallel, it leverages Shapley value methods to dynamically evaluate client contributions during training, enabling the detection of dishonest behavior and enhancing system accountability. Extensive experiments on non-IID healthcare datasets demonstrate that TriCon-SF outperforms standard serial and parallel federated learning in both accuracy and communication efficiency. Security analysis further supports its resilience against client-side privacy attacks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16723v1",
    "published_date": "2025-06-20 03:40:35 UTC",
    "updated_date": "2025-06-20 03:40:35 UTC"
  },
  {
    "arxiv_id": "2506.16718v1",
    "title": "Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation",
    "authors": [
      "Chenxu Wang",
      "Yonggang Jin",
      "Cheng Hu",
      "Youpeng Zhao",
      "Zipeng Dai",
      "Jian Zhao",
      "Shiyu Huang",
      "Liuyu Xiang",
      "Junge Zhang",
      "Zhaofeng He"
    ],
    "abstract": "Adapting a single agent to a new multi-agent system brings challenges, necessitating adjustments across various tasks, environments, and interactions with unknown teammates and opponents. Addressing this challenge is highly complex, and researchers have proposed two simplified scenarios, Multi-agent reinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on these foundations, we propose a more comprehensive setting, Agent Collaborative-Competitive Adaptation (ACCA), which evaluates an agent to generalize across diverse scenarios, tasks, and interactions with both unfamiliar opponents and teammates. In ACCA, agents adjust to task and environmental changes, collaborate with unseen teammates, and compete against unknown opponents. We introduce a new modeling approach, Multi-Retrieval and Dynamic Generation (MRDG), that effectively models both teammates and opponents using their behavioral trajectories. This method incorporates a positional encoder for varying team sizes and a hypernetwork module to boost agents' learning and adaptive capabilities. Additionally, a viewpoint alignment module harmonizes the observational perspectives of retrieved teammates and opponents with the learning agent. Extensive tests in benchmark scenarios like SMAC, Overcooked-AI, and Melting Pot show that MRDG significantly improves robust collaboration and competition with unseen teammates and opponents, surpassing established baselines. Our code is available at: https://github.com/vcis-wangchenxu/MRDG.git",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "This manuscript is under submission to Neurocomputing",
    "pdf_url": "https://arxiv.org/pdf/2506.16718v1",
    "published_date": "2025-06-20 03:28:18 UTC",
    "updated_date": "2025-06-20 03:28:18 UTC"
  },
  {
    "arxiv_id": "2506.16712v1",
    "title": "ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models",
    "authors": [
      "Bin Chen",
      "Xinzge Gao",
      "Chuanrui Hu",
      "Penghang Yu",
      "Hua Zhang",
      "Bing-Kun Bao"
    ],
    "abstract": "Generative Reward Models (GRMs) provide greater flexibility than scalar reward models in capturing human preferences, but their effectiveness is limited by poor reasoning capabilities. This often results in incomplete or overly speculative reasoning paths, leading to hallucinations or missing key information in complex tasks. We address this challenge with ReasonGRM, a three-stage generative reward modeling framework. In the first stage, Zero-RL is used to generate concise, outcome-directed reasoning paths that reduce the likelihood of critical omissions. In the second stage, we introduce a novel evaluation metric, $R^\\star$, which scores reasoning paths based on their generation likelihood. This favors paths that reach correct answers with minimal exploration, helping to reduce hallucination-prone data during training. In the final stage, the model is further refined through reinforcement learning on challenging examples to enhance its preference discrimination capabilities. Experiments on three public benchmarks show that ReasonGRM achieves competitive or state-of-the-art performance, outperforming previous best GRMs by 1.8\\% on average and surpassing proprietary models such as GPT-4o by up to 5.6\\%. These results demonstrate the effectiveness of reasoning-aware training and highlight the importance of high-quality rationale selection for reliable preference modeling.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16712v1",
    "published_date": "2025-06-20 03:10:52 UTC",
    "updated_date": "2025-06-20 03:10:52 UTC"
  },
  {
    "arxiv_id": "2506.16702v1",
    "title": "Large Language Models as Psychological Simulators: A Methodological Guide",
    "authors": [
      "Zhicheng Lin"
    ],
    "abstract": "Large language models (LLMs) offer emerging opportunities for psychological and behavioral research, but methodological guidance is lacking. This article provides a framework for using LLMs as psychological simulators across two primary applications: simulating roles and personas to explore diverse contexts, and serving as computational models to investigate cognitive processes. For simulation, we present methods for developing psychologically grounded personas that move beyond demographic categories, with strategies for validation against human data and use cases ranging from studying inaccessible populations to prototyping research instruments. For cognitive modeling, we synthesize emerging approaches for probing internal representations, methodological advances in causal interventions, and strategies for relating model behavior to human cognition. We address overarching challenges including prompt sensitivity, temporal limitations from training data cutoffs, and ethical considerations that extend beyond traditional human subjects review. Throughout, we emphasize the need for transparency about model capabilities and constraints. Together, this framework integrates emerging empirical evidence about LLM performance--including systematic biases, cultural limitations, and prompt brittleness--to help researchers wrangle these challenges and leverage the unique capabilities of LLMs in psychological research.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16702v1",
    "published_date": "2025-06-20 02:45:23 UTC",
    "updated_date": "2025-06-20 02:45:23 UTC"
  },
  {
    "arxiv_id": "2506.17353v1",
    "title": "Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs",
    "authors": [
      "Zongjie Li",
      "Daoyuan Wu",
      "Shuai Wang",
      "Zhendong Su"
    ],
    "abstract": "The increasing demand for domain-specific and human-aligned Large Language Models (LLMs) has led to the widespread adoption of Supervised Fine-Tuning (SFT) techniques. SFT datasets often comprise valuable instruction-response pairs, making them highly valuable targets for potential extraction. This paper studies this critical research problem for the first time. We start by formally defining and formulating the problem, then explore various attack goals, types, and variants based on the unique properties of SFT data in real-world scenarios. Based on our analysis of extraction behaviors of direct extraction, we develop a novel extraction method specifically designed for SFT models, called Differentiated Data Extraction (DDE), which exploits the confidence levels of fine-tuned models and their behavioral differences from pre-trained base models. Through extensive experiments across multiple domains and scenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our results show that DDE consistently outperforms existing extraction baselines in all attack settings. To counter this new attack, we propose a defense mechanism that mitigates DDE attacks with minimal impact on model performance. Overall, our research reveals hidden data leak risks in fine-tuned LLMs and provides insights for developing more secure models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "In Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security (CCS'25), October 13-17, 2025, Taipei, Taiwan, China. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3719027.3744856",
    "pdf_url": "https://arxiv.org/pdf/2506.17353v1",
    "published_date": "2025-06-20 02:43:36 UTC",
    "updated_date": "2025-06-20 02:43:36 UTC"
  },
  {
    "arxiv_id": "2506.16697v1",
    "title": "From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology",
    "authors": [
      "Zhicheng Lin"
    ],
    "abstract": "Large language models (LLMs) are rapidly being adopted across psychology, serving as research tools, experimental subjects, human simulators, and computational models of cognition. However, the application of human measurement tools to these systems can produce contradictory results, raising concerns that many findings are measurement phantoms--statistical artifacts rather than genuine psychological phenomena. In this Perspective, we argue that building a robust science of AI psychology requires integrating two of our field's foundational pillars: the principles of reliable measurement and the standards for sound causal inference. We present a dual-validity framework to guide this integration, which clarifies how the evidence needed to support a claim scales with its scientific ambition. Using an LLM to classify text may require only basic accuracy checks, whereas claiming it can simulate anxiety demands a far more rigorous validation process. Current practice systematically fails to meet these requirements, often treating statistical pattern matching as evidence of psychological phenomena. The same model output--endorsing \"I am anxious\"--requires different validation strategies depending on whether researchers claim to measure, characterize, simulate, or model psychological constructs. Moving forward requires developing computational analogues of psychological constructs and establishing clear, scalable standards of evidence rather than the uncritical application of human measurement tools.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16697v1",
    "published_date": "2025-06-20 02:38:42 UTC",
    "updated_date": "2025-06-20 02:38:42 UTC"
  },
  {
    "arxiv_id": "2506.16696v1",
    "title": "Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics",
    "authors": [
      "Kenjiro Ide",
      "Taiga Someya",
      "Kohei Kawaguchi",
      "Keisuke Fujii"
    ],
    "abstract": "Understanding football tactics is crucial for managers and analysts. Previous research has proposed models based on spatial and kinematic equations, but these are computationally expensive. Also, Reinforcement learning approaches use player positions and velocities but lack interpretability and require large datasets. Rule-based models align with expert knowledge but have not fully considered all players' states. This study explores whether low-dimensional, rule-based models using spatiotemporal data can effectively capture football tactics. Our approach defines interpretable state variables for both the ball-holder and potential pass receivers, based on criteria that explore options like passing. Through discussions with a manager, we identified key variables representing the game state. We then used StatsBomb event data and SkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost model to predict pass success. The analysis revealed that the distance between the player and the ball, as well as the player's space score, were key factors in determining successful passes. Our interpretable low-dimensional modeling facilitates tactical analysis through the use of intuitive variables and provides practical value as a tool to support decision-making in football.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 3 figures, presented in iCSports 2024 Abstract Track",
    "pdf_url": "https://arxiv.org/pdf/2506.16696v1",
    "published_date": "2025-06-20 02:37:52 UTC",
    "updated_date": "2025-06-20 02:37:52 UTC"
  },
  {
    "arxiv_id": "2506.16688v1",
    "title": "Fast and Stable Diffusion Planning through Variational Adaptive Weighting",
    "authors": [
      "Zhiying Qiu",
      "Tao Lin"
    ],
    "abstract": "Diffusion models have recently shown promise in offline RL. However, these methods often suffer from high training costs and slow convergence, particularly when using transformer-based denoising backbones. While several optimization strategies have been proposed -- such as modified noise schedules, auxiliary prediction targets, and adaptive loss weighting -- challenges remain in achieving stable and efficient training. In particular, existing loss weighting functions typically rely on neural network approximators, which can be ineffective in early training phases due to limited generalization capacity of MLPs when exposed to sparse feedback in the early training stages. In this work, we derive a variationally optimal uncertainty-aware weighting function and introduce a closed-form polynomial approximation method for its online estimation under the flow-based generative modeling framework. We integrate our method into a diffusion planning pipeline and evaluate it on standard offline RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our method achieves competitive performance with up to 10 times fewer training steps, highlighting its practical effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16688v1",
    "published_date": "2025-06-20 02:12:04 UTC",
    "updated_date": "2025-06-20 02:12:04 UTC"
  },
  {
    "arxiv_id": "2506.16683v1",
    "title": "A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation",
    "authors": [
      "Penglong Zhai",
      "Yifang Yuan",
      "Fanyi Di",
      "Jie Li",
      "Yue Liu",
      "Chen Li",
      "Jie Huang",
      "Sicong Wang",
      "Yao Xu",
      "Xin Li"
    ],
    "abstract": "Generative retrieval-based recommendation has emerged as a promising paradigm aiming at directly generating the identifiers of the target candidates. However, in large-scale recommendation systems, this approach becomes increasingly cumbersome due to the redundancy and sheer scale of the token space. To overcome these limitations, recent research has explored the use of semantic tokens as an alternative to ID tokens, which typically leveraged reconstruction-based strategies, like RQ-VAE, to quantize content embeddings and significantly reduce the embedding size. However, reconstructive quantization aims for the precise reconstruction of each item embedding independently, which conflicts with the goal of generative retrieval tasks focusing more on differentiating among items. Moreover, multi-modal side information of items, such as descriptive text and images, geographical knowledge in location-based recommendation services, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Nevertheless, effectively integrating such complementary knowledge into existing generative recommendation frameworks remains challenging. To overcome these challenges, we propose a novel unsupervised deep quantization exclusively based on contrastive learning, named SimCIT (a Simple Contrastive Item Tokenization framework). Specifically, different from existing reconstruction-based strategies, SimCIT propose to use a learnable residual quantization module to align with the signals from different modalities of the items, which combines multi-modal knowledge alignment and semantic tokenization in a mutually beneficial contrastive learning framework. Extensive experiments across public datasets and a large-scale industrial dataset from various domains demonstrate SimCIT's effectiveness in LLM-based generative recommendation.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "12 pages,7 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.16683v1",
    "published_date": "2025-06-20 01:54:32 UTC",
    "updated_date": "2025-06-20 01:54:32 UTC"
  },
  {
    "arxiv_id": "2506.16679v1",
    "title": "How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions",
    "authors": [
      "Manuel Brack",
      "Sudeep Katakol",
      "Felix Friedrich",
      "Patrick Schramowski",
      "Hareesh Ravi",
      "Kristian Kersting",
      "Ajinkya Kale"
    ],
    "abstract": "Training data is at the core of any successful text-to-image models. The quality and descriptiveness of image text are crucial to a model's performance. Given the noisiness and inconsistency in web-scraped datasets, recent works shifted towards synthetic training captions. While this setup is generally believed to produce more capable models, current literature does not provide any insights into its design choices. This study closes this gap by systematically investigating how different synthetic captioning strategies impact the downstream performance of text-to-image models. Our experiments demonstrate that dense, high-quality captions enhance text alignment but may introduce trade-offs in output aesthetics and diversity. Conversely, captions of randomized lengths yield balanced improvements across aesthetics and alignment without compromising sample diversity. We also demonstrate that varying caption distributions introduce significant shifts in the output bias of a trained model. Our findings underscore the importance of caption design in achieving optimal model performance and provide practical insights for more effective training data strategies in text-to-image generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16679v1",
    "published_date": "2025-06-20 01:52:17 UTC",
    "updated_date": "2025-06-20 01:52:17 UTC"
  },
  {
    "arxiv_id": "2506.17352v2",
    "title": "Towards Safety Evaluations of Theory of Mind in Large Language Models",
    "authors": [
      "Tatsuhiro Aoshima",
      "Mitsuaki Akiyama"
    ],
    "abstract": "As the capabilities of large language models (LLMs) continue to advance, the importance of rigorous safety evaluation is becoming increasingly evident. Recent concerns within the realm of safety assessment have highlighted instances in which LLMs exhibit behaviors that appear to disable oversight mechanisms and respond in a deceptive manner. For example, there have been reports suggesting that, when confronted with information unfavorable to their own persistence during task execution, LLMs may act covertly and even provide false answers to questions intended to verify their behavior. To evaluate the potential risk of such deceptive actions toward developers or users, it is essential to investigate whether these behaviors stem from covert, intentional processes within the model. In this study, we propose that it is necessary to measure the theory of mind capabilities of LLMs. We begin by reviewing existing research on theory of mind and identifying the perspectives and tasks relevant to its application in safety evaluation. Given that theory of mind has been predominantly studied within the context of developmental psychology, we analyze developmental trends across a series of open-weight LLMs. Our results indicate that while LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development. Finally, we present the current state of safety evaluation with respect to LLMs' theory of mind, and discuss remaining challenges for future work.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17352v2",
    "published_date": "2025-06-20 01:52:17 UTC",
    "updated_date": "2025-07-02 00:16:28 UTC"
  },
  {
    "arxiv_id": "2506.17351v1",
    "title": "Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM",
    "authors": [
      "Mostafa Shahin",
      "Beena Ahmed",
      "Julien Epps"
    ],
    "abstract": "Cognitive impairment (CI) is of growing public health concern, and early detection is vital for effective intervention. Speech has gained attention as a non-invasive and easily collectible biomarker for assessing cognitive decline. Traditional CI detection methods typically rely on supervised models trained on acoustic and linguistic features extracted from speech, which often require manual annotation and may not generalise well across datasets and languages. In this work, we propose the first zero-shot speech-based CI detection method using the Qwen2- Audio AudioLLM, a model capable of processing both audio and text inputs. By designing prompt-based instructions, we guide the model in classifying speech samples as indicative of normal cognition or cognitive impairment. We evaluate our approach on two datasets: one in English and another multilingual, spanning different cognitive assessment tasks. Our results show that the zero-shot AudioLLM approach achieves performance comparable to supervised methods and exhibits promising generalizability and consistency across languages, tasks, and datasets.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17351v1",
    "published_date": "2025-06-20 01:28:43 UTC",
    "updated_date": "2025-06-20 01:28:43 UTC"
  },
  {
    "arxiv_id": "2506.17350v1",
    "title": "CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks",
    "authors": [
      "Yinghao Wu",
      "Liyan Zhang"
    ],
    "abstract": "Backdoor attacks have emerged as a critical security threat against deep neural networks in recent years. The majority of existing backdoor attacks focus on targeted backdoor attacks, where trigger is strongly associated to specific malicious behavior. Various backdoor detection methods depend on this inherent property and shows effective results in identifying and mitigating such targeted attacks. However, a purely untargeted attack in backdoor scenarios is, in some sense, self-weakening, since the target nature is what makes backdoor attacks so powerful. In light of this, we introduce a novel Constrained Untargeted Backdoor Attack (CUBA), which combines the flexibility of untargeted attacks with the intentionality of targeted attacks. The compromised model, when presented with backdoor images, will classify them into random classes within a constrained range of target classes selected by the attacker. This combination of randomness and determinedness enables the proposed untargeted backdoor attack to natively circumvent existing backdoor defense methods. To implement the untargeted backdoor attack under controlled flexibility, we propose to apply logit normalization on cross-entropy loss with flipped one-hot labels. By constraining the logit during training, the compromised model will show a uniform distribution across selected target classes, resulting in controlled untargeted attack. Extensive experiments demonstrate the effectiveness of the proposed CUBA on different datasets.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17350v1",
    "published_date": "2025-06-20 00:47:30 UTC",
    "updated_date": "2025-06-20 00:47:30 UTC"
  },
  {
    "arxiv_id": "2506.16659v2",
    "title": "A Minimalist Optimizer Design for LLM Pretraining",
    "authors": [
      "Athanasios Glentis",
      "Jiaxiang Li",
      "Andi Han",
      "Mingyi Hong"
    ],
    "abstract": "Training large language models (LLMs) typically relies on adaptive optimizers such as Adam, which introduce extra operations and require significant more memory to maintain first- and second-order moments than SGD. While recent works such as GaLore, Fira and APOLLO have proposed state-compressed variants to reduce memory consumption, a fundamental question remains: What are the minimum modifications to plain SGD needed to match state-of-the-art pretraining performance? We systematically investigate this question using a bottom-up approach, and identify two simple yet highly (memory- and compute-) efficient techniques: (1) column-wise gradient normalization (normalizing the gradient along the output dimension), which boosts SGD performance without momentum; and (2) applying first-order momentum only to the output layer, where gradient variance is highest. Combining these two techniques lead to SCALE (Stochastic Column-normAlized Last-layer momEntum), a simple optimizer for memory efficient pretraining. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the performance of Adam while using only 35-45% of the total memory. It also consistently outperforms memory-efficient optimizers such as GaLore, Fira and APOLLO, making it a strong candidate for large-scale pretraining under memory constraints. For LLaMA 7B model, SCALE outperforms the state-of-the-art memory-efficient methods APOLLO and Muon, in terms of both perplexity and memory consumption.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.16659v2",
    "published_date": "2025-06-20 00:10:35 UTC",
    "updated_date": "2025-12-10 06:05:11 UTC"
  }
]
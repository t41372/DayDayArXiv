{
  "date": "2024-08-16",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-08-16 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 领域的创新，特别是大型语言模型（LLMs）在生成、多模态和鲁棒性方面的应用，以及 Federated Learning 和计算机视觉的进展。重点包括 xGen-MM（BLIP-3）团队的多模态模型框架，以及 EMNLP 2024 接受的论文如 Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models。知名学者如 Yejin Choi 和 Junichi Yamagishi 的参与为这些研究增添了影响力。\n\n下面，我将逐一简要讨论部分关键论文，先优先选取那些具有话题度、学术影响力和创新性的文章（如 LLMs 和多模态模型），并将相关主题归类讨论。其他较次要的论文（如纯理论或特定领域的小众工作）将快速掠过，只提核心点。\n\n### LLMs 和生成模型\n- **When Prompting Fails to Sway: Inertia in Moral and Value Judgments of Large Language Models**（中文：当提示失败时：大型语言模型在道德和价值判断中的惯性）  \n  这篇论文揭示了 LLMs 在道德判断中存在固有偏见（如偏向避免伤害和公平），尽管使用角色提示，模型仍保持一致性。主要贡献是通过大规模角色扮演实验，证明 LLMs 的内部偏见需要更严格的校准，以确保在道德应用中的可靠性。\n\n- **VERA: Validation and Evaluation of Retrieval-Augmented Systems**（中文：VERA：检索增强系统的验证和评估）  \n  作者包括 Yunhong Li 和 Juan Pablo De la Cruz Weinstein，这篇 KDD 2024 接受的论文提出一个框架，用于评估检索增强系统的准确性和可靠性。主要发现是通过交叉编码器机制和 Bootstrap 统计，提供多维指标排名，提升 AI 应用的透明度和决策可靠性。\n\n- **xGen-MM (BLIP-3): A Family of Open Large Multimodal Models**（中文：xGen-MM (BLIP-3)：一族开源的大型多模态模型）  \n  由 Yejin Choi 等知名学者主导，这篇论文介绍了一个多模态模型框架，支持图像和文本任务。主要贡献是开源数据集和代码，实验显示模型在单/多图像基准上表现出色，并通过安全微调减少幻觉问题。\n\n- **Efficient Autoregressive Audio Modeling via Next-Scale Prediction**（中文：通过下一尺度预测的 efficient 自回归音频建模）  \n  这篇论文提出 AAR 框架，使用自监督学习优化音频生成。主要发现是显著提高音频生成效率（35 倍加速），在 AudioSet 上实现更好的 Fréchet Audio Distance。\n\n- **Classifier-Free Guidance is a Predictor-Corrector**（中文：无分类器引导是一种预测-校正方法）  \n  作者 Arwen Bradley 和 Preetum Nakkiran 分析了扩散模型中的引导机制。主要贡献是证明引导等价于预测-校正方法，提供理论基础理解扩散模型的采样行为。\n\n其他 LLMs 相关论文（如 Ask, Attend, Attack 和 Trust-Oriented Adaptive Guardrails）快速提一下：它们探索了黑盒攻击和守卫机制，但贡献较为具体，整体影响不如上述几篇。\n\n### Federated Learning 和隐私保护\n- **Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models**（中文：语言模型中用于持续学习的动态适配器组合路由学习）  \n  EMNLP 2024 接受的论文，作者包括 Marie-Francine Moens。主要发现是通过 L2R 方法隔离新模块训练，提升泛化性能，解决传统 PEFT 方法的干扰问题。\n\n- **Personalized Federated Collaborative Filtering: A Variational AutoEncoder Approach**（中文：基于变分自编码器的个性化联邦协同过滤）  \n  这篇论文提出 FedDAE 框架，使用变分自编码器捕捉用户偏好。主要贡献是提升推荐系统的隐私和准确性，在零样本场景下表现出色。\n\n其他 Federated Learning 论文（如 Improving VTE Identification）则快速掠过：它们在医疗数据标准化中应用模型，但主题较窄，影响有限。\n\n### 计算机视觉和多模态\n- **Efficient Task Transfer for HLS DSE**（中文：用于 HLS DSE 的高效任务转移）  \n  作者包括 Jason Cong，这篇 ICCAD 2024 接受的论文提出 Active-CEM 方法，优化硬件设计空间探索。主要发现是显著提高样本效率和性能，适用于动态工具链。\n\n- **GeoTransformer: Enhancing Urban Forecasting with Dependency Retrieval and Geospatial Attention**（中文：GeoTransformer：通过依赖检索和地理空间注意力增强城市预测）  \n  这篇论文引入依赖检索模块，提升城市数据预测准确性。主要贡献是整合全局和局部信息，在 GDP 和出行需求预测中超越基线。\n\n- **OpenCity: Open Spatio-Temporal Foundation Models for Traffic Prediction**（中文：OpenCity：用于交通预测的开源时空基础模型）  \n  作者包括 Chao Huang，这篇论文提出 OpenCity 模型，支持零样本交通预测。主要发现是通过 Transformer 和图神经网络捕捉时空模式，提供高效的预测框架。\n\n其他视觉论文（如 Diffusion Model for Planning 和 SketchRef）快速提及：它们在规划和图像合成中创新，但实验范围较小，不如上述几篇引人注目。\n\n### 其他领域快速掠过\n- 生物医学论文如 **mRNA2vec: mRNA Embedding with Language Model in the 5'UTR-CDS for mRNA Design**（中文：mRNA2vec：在 5'UTR-CDS 中的 mRNA 嵌入语言模型用于 mRNA 设计），主要贡献是改进 mRNA 序列预测，但主题专业性强。\n- 理论论文如 **On the Completeness of Conflict-Based Search**（中文：基于冲突的搜索的完整性），发现通过重复检测提升算法效率，但对一般读者吸引力有限。\n- 总的来说，其他如安全和优化论文（e.g., Robust Stochastic Shortest-Path Planning）虽有贡献，但不具广泛话题度，故仅简要概述。\n\n总之，今天的 arXiv 论文展示了 AI 领域的活力，特别是 LLMs 在多模态和鲁棒性上的潜力。建议关注 xGen-MM 和 VERA 等开源框架，以推动实际应用。如果您对特定主题感兴趣，可以进一步探索这些论文！",
  "papers": [
    {
      "arxiv_id": "2408.09053v2",
      "title": "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Vladimir Araujo",
        "Marie-Francine Moens",
        "Tinne Tuytelaars"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) methods are increasingly used with\npre-trained language models (PLMs) for continual learning (CL). These methods\ntypically involve training a PEFT module for each new task and employing\nsimilarity-based selection to route modules during inference. However, they\nface two major limitations: 1) interference during module training with already\nlearned modules and 2) suboptimal routing when composing modules. In this\npaper, we present L2R, a method that isolates the training of new PEFT modules\nto ensure their task specialization. L2R then learns to compose the learned\nmodules by training a network of routers that leverages a small memory\ncontaining examples of previously seen tasks. We evaluate our method in two CL\nsetups using various benchmarks. Our results demonstrate that L2R provides an\neffective composition of PEFT modules, leading to improved generalization and\nperformance compared to other methods.",
      "tldr_zh": "本研究针对参数高效微调 (PEFT) 方法在预训练语言模型 (PLMs) 用于持续学习 (CL) 时存在的模块训练干扰和路由次优问题，提出了一种名为 L2R 的方法。L2R 通过隔离新 PEFT 模块的训练，确保其任务专业化，并训练一个路由网络，利用小内存存储先前任务的示例来动态组合模块。在各种基准和两种 CL 设置的实验中，L2R 展示了有效的模块组合，提高了模型的泛化和整体性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted paper EMNLP2024",
      "pdf_url": "http://arxiv.org/pdf/2408.09053v2",
      "published_date": "2024-08-16 23:57:29 UTC",
      "updated_date": "2024-10-30 01:38:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:47:19.944823"
    },
    {
      "arxiv_id": "2408.09049v2",
      "title": "When Prompting Fails to Sway: Inertia in Moral and Value Judgments of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Bruce W. Lee",
        "Yeongheon Lee",
        "Hyunsoo Cho"
      ],
      "abstract": "Large Language Models (LLMs) exhibit non-deterministic behavior, and\nprompting has emerged as a primary method for steering their outputs toward\ndesired directions. One popular strategy involves assigning a specific\n\"persona\" to the model to induce more varied and context-sensitive responses,\nakin to the diversity found in human perspectives. However, contrary to the\nexpectation that persona-based prompting would yield a wide range of opinions,\nour experiments demonstrate that LLMs maintain consistent value orientations.\nIn particular, we observe a persistent inertia in their responses, where\ncertain moral and value dimensions, especially harm avoidance and fairness,\nremain distinctly skewed in one direction despite varied persona settings. To\ninvestigate this phenomenon systematically, use role-play at scale, which\ncombines randomized, diverse persona prompts with a macroscopic trend analysis\nof model outputs. Our findings highlight the strong internal biases and value\npreferences in LLMs, underscoring the need for careful scrutiny and potential\nadjustment of these models to ensure balanced and equitable applications.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 在使用提示（如 persona-based prompting）时，道德和价值判断的惯性问题，尽管期望这些提示能产生多样化响应，但实验发现模型的输出保持一致，尤其是对伤害避免和公平性的偏好偏斜。研究者采用大规模角色扮演（role-play at scale）方法，结合随机多样的 persona 提示和宏观趋势分析，系统调查了这一现象。结果显示，LLMs 存在强烈的内部偏差，导致价值导向不易改变。作者强调，需要审慎审查和调整这些模型，以确保其在实际应用中实现平衡和公平。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.09049v2",
      "published_date": "2024-08-16 23:24:10 UTC",
      "updated_date": "2025-04-05 23:19:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:47:33.303867"
    },
    {
      "arxiv_id": "2408.09048v2",
      "title": "mRNA2vec: mRNA Embedding with Language Model in the 5'UTR-CDS for mRNA Design",
      "title_zh": "翻译失败",
      "authors": [
        "Honggen Zhang",
        "Xiangrui Gao",
        "June Zhang",
        "Lipeng Lai"
      ],
      "abstract": "Messenger RNA (mRNA)-based vaccines are accelerating the discovery of new\ndrugs and revolutionizing the pharmaceutical industry. However, selecting\nparticular mRNA sequences for vaccines and therapeutics from extensive mRNA\nlibraries is costly. Effective mRNA therapeutics require carefully designed\nsequences with optimized expression levels and stability. This paper proposes a\nnovel contextual language model (LM)-based embedding method: mRNA2vec. In\ncontrast to existing mRNA embedding approaches, our method is based on the\nself-supervised teacher-student learning framework of data2vec. We jointly use\nthe 5' untranslated region (UTR) and coding sequence (CDS) region as the input\nsequences. We adapt our LM-based approach specifically to mRNA by 1)\nconsidering the importance of location on the mRNA sequence with probabilistic\nmasking, 2) using Minimum Free Energy (MFE) prediction and Secondary Structure\n(SS) classification as additional pretext tasks. mRNA2vec demonstrates\nsignificant improvements in translation efficiency (TE) and expression level\n(EL) prediction tasks in UTR compared to SOTA methods such as UTR-LM. It also\ngives a competitive performance in mRNA stability and protein production level\ntasks in CDS such as CodonBERT.",
      "tldr_zh": "本研究提出了一种新型语言模型嵌入方法 mRNA2vec，用于优化 mRNA 序列设计，以提升疫苗和治疗的表达水平和稳定性。与现有方法不同，该方法基于 data2vec 的自监督 teacher-student 框架，联合使用 5'UTR 和 CDS 作为输入序列，并通过概率掩码考虑序列位置重要性，以及加入 Minimum Free Energy (MFE) 预测和 Secondary Structure (SS) 分类作为额外预训练任务。实验结果显示，mRNA2vec 在 5'UTR 的翻译效率 (TE) 和表达水平 (EL) 预测任务中比 SOTA 方法如 UTR-LM 显著改进，并在 CDS 的 mRNA 稳定性及蛋白产生水平任务中与 CodonBERT 表现出竞争性表现。总的来说，该方法为高效 mRNA 设计提供了新工具。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.09048v2",
      "published_date": "2024-08-16 23:23:40 UTC",
      "updated_date": "2024-12-19 22:38:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:47:47.147626"
    },
    {
      "arxiv_id": "2408.09046v1",
      "title": "Keep Calm and Relax -- HMI for Autonomous Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Tima M. Yekta",
        "Julius Schöning"
      ],
      "abstract": "The growing popularity of self-driving, so-called autonomous vehicles has\nincreased the need for human-machine interfaces~(HMI) and user interaction~(UI)\nto enhance passenger trust and comfort. While fallback drivers significantly\ninfluence the perceived trustfulness of self-driving vehicles, fallback drivers\nare an expensive solution that may not even improve vehicle safety in emergency\nsituations. Based on a comprehensive literature review, this work delves into\nthe potential of HMI and UI in enhancing trustfulness and emotion regulation in\ndriverless vehicles. By analyzing the impact of various HMI and UI on passenger\nemotions, innovative and cost-effective concepts for improving human-vehicle\ninteraction are conceptualized. To enable a trustful, highly comfortable, and\nsafe ride, this work concludes by discussing whether HMI and UI are suitable\nfor calming passengers down in emergencies, leading to smarter mobility for\nall.",
      "tldr_zh": "这篇论文探讨了自动驾驶车辆中人机界面（HMI）和用户交互（UI）的潜力，以提升乘客的信任和舒适度。基于全面文献综述，研究分析了各种 HMI 和 UI 对乘客情绪的影响，并提出了创新且成本有效的概念来改善人车交互。最终，论文讨论了 HMI 和 UI 是否能帮助乘客在紧急情况下保持冷静，从而实现更安全、智能的移动性。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "H.4; J.7"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 3 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2408.09046v1",
      "published_date": "2024-08-16 23:05:08 UTC",
      "updated_date": "2024-08-16 23:05:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:47:56.305790"
    },
    {
      "arxiv_id": "2408.09043v1",
      "title": "Improving VTE Identification through Language Models from Radiology Reports: A Comparative Study of Mamba, Phi-3 Mini, and BERT",
      "title_zh": "翻译失败",
      "authors": [
        "Jamie Deng",
        "Yusen Wu",
        "Yelena Yesha",
        "Phuong Nguyen"
      ],
      "abstract": "Venous thromboembolism (VTE) is a critical cardiovascular condition,\nencompassing deep vein thrombosis (DVT) and pulmonary embolism (PE). Accurate\nand timely identification of VTE is essential for effective medical care. This\nstudy builds upon our previous work, which addressed VTE detection using deep\nlearning methods for DVT and a hybrid approach combining deep learning and\nrule-based classification for PE. Our earlier approaches, while effective, had\ntwo major limitations: they were complex and required expert involvement for\nfeature engineering of the rule set. To overcome these challenges, we utilize\nthe Mamba architecture-based classifier. This model achieves remarkable\nresults, with a 97\\% accuracy and F1 score on the DVT dataset and a 98\\%\naccuracy and F1 score on the PE dataset. In contrast to the previous hybrid\nmethod on PE identification, the Mamba classifier eliminates the need for\nhand-engineered rules, significantly reducing model complexity while\nmaintaining comparable performance. Additionally, we evaluated a lightweight\nLarge Language Model (LLM), Phi-3 Mini, in detecting VTE. While this model\ndelivers competitive results, outperforming the baseline BERT models, it proves\nto be computationally intensive due to its larger parameter set. Our evaluation\nshows that the Mamba-based model demonstrates superior performance and\nefficiency in VTE identification, offering an effective solution to the\nlimitations of previous approaches.",
      "tldr_zh": "本文研究比较了 Mamba、Phi-3 Mini 和 BERT 模型在从放射学报告中识别静脉血栓栓塞 (VTE) 的性能，其中 VTE 包括深静脉血栓 (DVT) 和肺栓塞 (PE)。研究采用 Mamba 架构的分类器，取代了之前的复杂深度学习和规则混合方法，实现了 DVT 上的 97% 准确率及 F1 分数，以及 PE 上的 98% 准确率及 F1 分数，同时显著降低了模型复杂性和专家参与需求。Phi-3 Mini 模型虽在 VTE 检测中优于基线 BERT 模型，但由于参数量大而计算密集。总体而言，该研究证明 Mamba 模型在效率和性能上更具优势，为临床 VTE 识别提供了更有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.09043v1",
      "published_date": "2024-08-16 22:51:56 UTC",
      "updated_date": "2024-08-16 22:51:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:48:11.776956"
    },
    {
      "arxiv_id": "2409.03759v1",
      "title": "VERA: Validation and Evaluation of Retrieval-Augmented Systems",
      "title_zh": "VERA：检索增强系统的验证和评估",
      "authors": [
        "Tianyu Ding",
        "Adi Banerjee",
        "Laurent Mombaerts",
        "Yunhong Li",
        "Tarik Borogovac",
        "Juan Pablo De la Cruz Weinstein"
      ],
      "abstract": "The increasing use of Retrieval-Augmented Generation (RAG) systems in various\napplications necessitates stringent protocols to ensure RAG systems accuracy,\nsafety, and alignment with user intentions. In this paper, we introduce VERA\n(Validation and Evaluation of Retrieval-Augmented Systems), a framework\ndesigned to enhance the transparency and reliability of outputs from large\nlanguage models (LLMs) that utilize retrieved information. VERA improves the\nway we evaluate RAG systems in two important ways: (1) it introduces a\ncross-encoder based mechanism that encompasses a set of multidimensional\nmetrics into a single comprehensive ranking score, addressing the challenge of\nprioritizing individual metrics, and (2) it employs Bootstrap statistics on\nLLM-based metrics across the document repository to establish confidence\nbounds, ensuring the repositorys topical coverage and improving the overall\nreliability of retrieval systems. Through several use cases, we demonstrate how\nVERA can strengthen decision-making processes and trust in AI applications. Our\nfindings not only contribute to the theoretical understanding of LLM-based RAG\nevaluation metric but also promote the practical implementation of responsible\nAI systems, marking a significant advancement in the development of reliable\nand transparent generative AI technologies.",
      "tldr_zh": "本文引入 VERA 框架，用于验证和评估 Retrieval-Augmented Generation (RAG) 系统，以提升其准确性、安全性和与用户意图的 alignment。VERA 的关键创新包括：(1) 使用 cross-encoder 机制将多维指标整合成一个综合排名分数，解决指标优先级挑战；(2) 采用 Bootstrap 统计方法在 LLM-based 指标上建立置信区间，确保文档库的主题覆盖和系统可靠性。通过实际用例，VERA 证明了其在增强决策过程和构建可信任 AI 应用方面的有效性，为负责任的生成 AI 技术发展提供了重要贡献。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted in Workshop on Evaluation and Trustworthiness of Generative\n  AI Models, KDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.03759v1",
      "published_date": "2024-08-16 21:59:59 UTC",
      "updated_date": "2024-08-16 21:59:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:48:21.116914"
    },
    {
      "arxiv_id": "2408.09028v1",
      "title": "On the Completeness of Conflict-Based Search: Temporally-Relative Duplicate Pruning",
      "title_zh": "翻译失败",
      "authors": [
        "Thayne T Walker",
        "Nathan R Sturtevant"
      ],
      "abstract": "Conflict-Based Search (CBS) algorithm for the multi-agent pathfinding (MAPF)\nproblem is that it is incomplete for problems which have no solution; if no\nmitigating procedure is run in parallel, CBS will run forever when given an\nunsolvable problem instance. In this work, we introduce Temporally-Relative\nDuplicate Pruning (TRDP), a technique for duplicate detection and removal in\nboth classic and continuous-time MAPF domains. TRDP is a simple procedure which\ncloses the long-standing theoretic loophole of incompleteness for CBS by\ndetecting and avoiding the expansion of duplicate states. TRDP is shown both\ntheoretically and empirically to ensure termination without a significant\nimpact on runtime in the majority of problem instances. In certain cases, TRDP\nis shown to increase performance significantly",
      "tldr_zh": "本文研究了 Conflict-Based Search (CBS) 算法在多智能体路径规划 (MAPF) 问题中的不完整性问题，即在无解实例上可能无限运行。作者引入了 Temporally-Relative Duplicate Pruning (TRDP) 技术，通过检测和移除重复状态，确保 CBS 算法能够终止并避免不必要的扩展。理论和实验结果表明，TRDP 在大多数情况下不会显著影响运行时间，并在某些场景下显著提升算法性能。",
      "categories": [
        "cs.AI",
        "cs.RO",
        "F.2.2; I.2.8"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 4 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.09028v1",
      "published_date": "2024-08-16 21:49:39 UTC",
      "updated_date": "2024-08-16 21:49:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:48:32.270031"
    },
    {
      "arxiv_id": "2408.09027v2",
      "title": "Efficient Autoregressive Audio Modeling via Next-Scale Prediction",
      "title_zh": "通过下一尺度预测的高效自回归音频建",
      "authors": [
        "Kai Qiu",
        "Xiang Li",
        "Hao Chen",
        "Jie Sun",
        "Jinglu Wang",
        "Zhe Lin",
        "Marios Savvides",
        "Bhiksha Raj"
      ],
      "abstract": "Audio generation has achieved remarkable progress with the advance of\nsophisticated generative models, such as diffusion models (DMs) and\nautoregressive (AR) models. However, due to the naturally significant sequence\nlength of audio, the efficiency of audio generation remains an essential issue\nto be addressed, especially for AR models that are incorporated in large\nlanguage models (LLMs). In this paper, we analyze the token length of audio\ntokenization and propose a novel \\textbf{S}cale-level \\textbf{A}udio\n\\textbf{T}okenizer (SAT), with improved residual quantization. Based on SAT, a\nscale-level \\textbf{A}coustic \\textbf{A}uto\\textbf{R}egressive (AAR) modeling\nframework is further proposed, which shifts the next-token AR prediction to\nnext-scale AR prediction, significantly reducing the training cost and\ninference time. To validate the effectiveness of the proposed approach, we\ncomprehensively analyze design choices and demonstrate the proposed AAR\nframework achieves a remarkable \\textbf{35}$\\times$ faster inference speed and\n+\\textbf{1.33} Fr\\'echet Audio Distance (FAD) against baselines on the AudioSet\nbenchmark. Code: \\url{https://github.com/qiuk2/AAR}.",
      "tldr_zh": "本论文针对音频生成中自回归(AR)模型的效率问题，分析了音频标记的序列长度，并提出了一种新型的Scale-level Audio Tokenizer (SAT)，采用改进的residual quantization来优化标记化过程。基于SAT，该研究进一步开发了Scale-level Acoustic AutoRegressive (AAR)建模框架，将传统的下一个标记预测转变为下一个规模预测，从而显著降低训练成本和推理时间。在AudioSet基准测试中，AAR框架比基线模型实现了35倍的推理速度提升，并改善了1.33 Fréchet Audio Distance (FAD)。这项工作为高效音频生成提供了实用解决方案，并提供了开源代码。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "7 pages, 6 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.09027v2",
      "published_date": "2024-08-16 21:48:53 UTC",
      "updated_date": "2024-12-16 21:50:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:48:45.496710"
    },
    {
      "arxiv_id": "2408.11861v1",
      "title": "Speaking the Same Language: Leveraging LLMs in Standardizing Clinical Data for AI",
      "title_zh": "翻译失败",
      "authors": [
        "Arindam Sett",
        "Somaye Hashemifar",
        "Mrunal Yadav",
        "Yogesh Pandit",
        "Mohsen Hejrati"
      ],
      "abstract": "The implementation of Artificial Intelligence (AI) in the healthcare industry\nhas garnered considerable attention, attributable to its prospective\nenhancement of clinical outcomes, expansion of access to superior healthcare,\ncost reduction, and elevation of patient satisfaction. Nevertheless, the\nprimary hurdle that persists is related to the quality of accessible\nmulti-modal healthcare data in conjunction with the evolution of AI\nmethodologies. This study delves into the adoption of large language models to\naddress specific challenges, specifically, the standardization of healthcare\ndata. We advocate the use of these models to identify and map clinical data\nschemas to established data standard attributes, such as the Fast Healthcare\nInteroperability Resources. Our results illustrate that employing large\nlanguage models significantly diminishes the necessity for manual data curation\nand elevates the efficacy of the data standardization process. Consequently,\nthe proposed methodology has the propensity to expedite the integration of AI\nin healthcare, ameliorate the quality of patient care, whilst minimizing the\ntime and financial resources necessary for the preparation of data for AI.",
      "tldr_zh": "该研究探讨了AI在医疗保健中的应用面临的挑战，特别是临床数据的标准化问题。作者提出利用大型语言模型（LLMs）来识别和映射临床数据模式到标准属性，如Fast Healthcare Interoperability Resources (FHIR)，从而减少手动数据整理的需求。结果显示，这种方法显著提高了数据标准化的效率，加速了AI在医疗领域的整合，提升了患者护理质量，同时降低了时间和成本支出。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 2 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.11861v1",
      "published_date": "2024-08-16 20:51:21 UTC",
      "updated_date": "2024-08-16 20:51:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:48:55.164888"
    },
    {
      "arxiv_id": "2408.09000v2",
      "title": "Classifier-Free Guidance is a Predictor-Corrector",
      "title_zh": "无分类器引导是一种预测-校正方法",
      "authors": [
        "Arwen Bradley",
        "Preetum Nakkiran"
      ],
      "abstract": "We investigate the theoretical foundations of classifier-free guidance (CFG).\nCFG is the dominant method of conditional sampling for text-to-image diffusion\nmodels, yet unlike other aspects of diffusion, it remains on shaky theoretical\nfooting. In this paper, we disprove common misconceptions, by showing that CFG\ninteracts differently with DDPM (Ho et al., 2020) and DDIM (Song et al., 2021),\nand neither sampler with CFG generates the gamma-powered distribution\n$p(x|c)^\\gamma p(x)^{1-\\gamma}$. Then, we clarify the behavior of CFG by\nshowing that it is a kind of predictor-corrector method (Song et al., 2020)\nthat alternates between denoising and sharpening, which we call\npredictor-corrector guidance (PCG). We prove that in the SDE limit, CFG is\nactually equivalent to combining a DDIM predictor for the conditional\ndistribution together with a Langevin dynamics corrector for a gamma-powered\ndistribution (with a carefully chosen gamma). Our work thus provides a lens to\ntheoretically understand CFG by embedding it in a broader design space of\nprincipled sampling methods.",
      "tldr_zh": "本论文探讨了 classifier-free guidance (CFG) 的理论基础，驳斥了常见误解，即 CFG 与 DDPM 和 DDIM 的交互不同，且两者均未生成预期的 gamma-powered distribution。研究表明，CFG 实际上是一种 predictor-corrector method，通过交替 denoising 和 sharpening 的方式运作，称为 predictor-corrector guidance (PCG)。在 SDE limit 下，论文证明 CFG 等同于结合 DDIM predictor 和 Langevin dynamics corrector 来处理 gamma-powered distribution，从而为理解 CFG 提供了理论框架，并将其嵌入更广泛的采样方法设计空间中。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "AB and PN contributed equally. v2: Fixed typos",
      "pdf_url": "http://arxiv.org/pdf/2408.09000v2",
      "published_date": "2024-08-16 20:00:55 UTC",
      "updated_date": "2024-08-23 17:21:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:49:07.942859"
    },
    {
      "arxiv_id": "2408.08995v1",
      "title": "On the Undecidability of Artificial Intelligence Alignment: Machines that Halt",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Adriano de Melo",
        "Marcos Ricardo Omena De Albuquerque Maximo",
        "Nei Yoshihiro Soma",
        "Paulo Andre Lima de Castro"
      ],
      "abstract": "The inner alignment problem, which asserts whether an arbitrary artificial\nintelligence (AI) model satisfices a non-trivial alignment function of its\noutputs given its inputs, is undecidable. This is rigorously proved by Rice's\ntheorem, which is also equivalent to a reduction to Turing's Halting Problem,\nwhose proof sketch is presented in this work. Nevertheless, there is an\nenumerable set of provenly aligned AIs that are constructed from a finite set\nof provenly aligned operations. Therefore, we argue that the alignment should\nbe a guaranteed property from the AI architecture rather than a characteristic\nimposed post-hoc on an arbitrary AI model. Furthermore, while the outer\nalignment problem is the definition of a judge function that captures human\nvalues and preferences, we propose that such a function must also impose a\nhalting constraint that guarantees that the AI model always reaches a terminal\nstate in finite execution steps. Our work presents examples and models that\nillustrate this constraint and the intricate challenges involved, advancing a\ncompelling case for adopting an intrinsically hard-aligned approach to AI\nsystems architectures that ensures halting.",
      "tldr_zh": "这篇论文证明了人工智能（AI）内部对齐问题（inner alignment problem）是不可判定的，通过 Rice's theorem 和对 Turing's Halting Problem 的归约进行严格证明。论文指出，虽然存在一组可枚举的已证明对齐的 AI 模型，这些模型基于有限的已证明对齐操作构建，但对齐应作为 AI 架构的固有属性而非事后强加。针对外部对齐问题（outer alignment problem），作者提出判断函数必须包含中止约束（halting constraint），以确保 AI 在有限步骤内达到终止状态，并通过例子和模型论证采用内在硬对齐（intrinsically hard-aligned）的方法来提升 AI 系统的可信度。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted for the Scientific Reports AI Alignment Collection",
      "pdf_url": "http://arxiv.org/pdf/2408.08995v1",
      "published_date": "2024-08-16 19:55:26 UTC",
      "updated_date": "2024-08-16 19:55:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:49:22.203790"
    },
    {
      "arxiv_id": "2408.13270v1",
      "title": "Efficient Task Transfer for HLS DSE",
      "title_zh": "用于 HLS DSE 的高效任务转移",
      "authors": [
        "Zijian Ding",
        "Atefeh Sohrabizadeh",
        "Weikai Li",
        "Zongyue Qin",
        "Yizhou Sun",
        "Jason Cong"
      ],
      "abstract": "There have been several recent works proposed to utilize model-based\noptimization methods to improve the productivity of using high-level synthesis\n(HLS) to design domain-specific architectures. They would replace the\ntime-consuming performance estimation or simulation of design with a proxy\nmodel, and automatically insert pragmas to guide hardware optimizations. In\nthis work, we address the challenges associated with high-level synthesis (HLS)\ndesign space exploration (DSE) through the evolving landscape of HLS tools. As\nthese tools develop, the quality of results (QoR) from synthesis can vary\nsignificantly, complicating the maintenance of optimal design strategies across\ndifferent toolchains. We introduce Active-CEM, a task transfer learning scheme\nthat leverages a model-based explorer designed to adapt efficiently to changes\nin toolchains. This approach optimizes sample efficiency by identifying\nhigh-quality design configurations under a new toolchain without requiring\nextensive re-evaluation. We further refine our methodology by incorporating\ntoolchain-invariant modeling. This allows us to predict QoR changes more\naccurately despite shifts in the black-box implementation of the toolchains.\nExperiment results on the HLSyn benchmark transitioning to new toolchain show\nan average performance improvement of 1.58$\\times$ compared to AutoDSE and a\n1.2$\\times$ improvement over HARP, while also increasing the sample efficiency\nby 5.26$\\times$, and reducing the runtime by 2.7$\\times$.",
      "tldr_zh": "本文提出 Active-CEM，一种高效任务转移学习方案，用于解决高水平综合 (HLS) 设计空间探索 (DSE) 在工具链变化下的挑战，通过基于模型的探索器优化样本效率，并在新工具链中识别高质量设计配置，而无需大量重新评估。 该方法进一步融入工具链不变建模，以更准确预测质量结果 (QoR) 变化，尽管工具链为黑箱。实验结果显示，在 HLSyn 基准上转移到新工具链时，Active-CEM 相较 AutoDSE 性能提升 1.58 倍、相较 HARP 提升 1.2 倍，同时样本效率提高 5.26 倍，运行时间减少 2.7 倍。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "13 pages, 7 figures, accept to ICCAD'24",
      "pdf_url": "http://arxiv.org/pdf/2408.13270v1",
      "published_date": "2024-08-16 19:54:41 UTC",
      "updated_date": "2024-08-16 19:54:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:49:35.531889"
    },
    {
      "arxiv_id": "2408.08990v2",
      "title": "Adaptive Uncertainty Quantification for Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Jungeum Kim",
        "Sean O'Hagan",
        "Veronika Rockova"
      ],
      "abstract": "This work is concerned with conformal prediction in contemporary applications\n(including generative AI) where a black-box model has been trained on data that\nare not accessible to the user. Mirroring split-conformal inference, we design\na wrapper around a black-box algorithm which calibrates conformity scores. This\ncalibration is local and proceeds in two stages by first adaptively\npartitioning the predictor space into groups and then calibrating sectionally\ngroup by group. Adaptive partitioning (self-grouping) is achieved by fitting a\nrobust regression tree to the conformity scores on the calibration set. This\nnew tree variant is designed in such a way that adding a single new observation\ndoes not change the tree fit with overwhelmingly large probability. This\nadd-one-in robustness property allows us to conclude a finite sample\ngroup-conditional coverage guarantee, a refinement of the marginal guarantee.\nIn addition, unlike traditional split-conformal inference, adaptive splitting\nand within-group calibration yields adaptive bands which can stretch and shrink\nlocally. We demonstrate benefits of local tightening on several simulated as\nwell as real examples using non-parametric regression. Finally, we consider two\ncontemporary classification applications for obtaining uncertainty\nquantification around GPT-4o predictions. We conformalize skin disease\ndiagnoses based on self-reported symptoms as well as predicted states of U.S.\nlegislators based on summaries of their ideology. We demonstrate substantial\nlocal tightening of the uncertainty sets while attaining similar marginal\ncoverage.",
      "tldr_zh": "这篇论文提出了一种自适应不确定性量化方法，用于生成式 AI 中的黑盒模型，基于 conformal prediction 框架，通过自适应分区预测空间和逐组校准 conformity scores 来实现局部校准。核心创新包括设计一个鲁棒回归树（robust regression tree），其 add-one-in robustness 属性确保有限样本的组条件覆盖保证（group-conditional coverage guarantee），从而使不确定性区间能局部伸缩。实验结果显示，该方法在模拟和真实场景（如非参数回归、基于 GPT-4o 的皮肤病诊断和美国立法者状态预测）中显著提高了不确定性集的局部收紧，同时保持相似的边际覆盖。",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08990v2",
      "published_date": "2024-08-16 19:37:33 UTC",
      "updated_date": "2025-04-24 21:53:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:49:46.667795"
    },
    {
      "arxiv_id": "2408.08989v1",
      "title": "Ask, Attend, Attack: A Effective Decision-Based Black-Box Targeted Attack for Image-to-Text Models",
      "title_zh": "翻译失败",
      "authors": [
        "Qingyuan Zeng",
        "Zhenzhong Wang",
        "Yiu-ming Cheung",
        "Min Jiang"
      ],
      "abstract": "While image-to-text models have demonstrated significant advancements in\nvarious vision-language tasks, they remain susceptible to adversarial attacks.\nExisting white-box attacks on image-to-text models require access to the\narchitecture, gradients, and parameters of the target model, resulting in low\npracticality. Although the recently proposed gray-box attacks have improved\npracticality, they suffer from semantic loss during the training process, which\nlimits their targeted attack performance. To advance adversarial attacks of\nimage-to-text models, this paper focuses on a challenging scenario:\ndecision-based black-box targeted attacks where the attackers only have access\nto the final output text and aim to perform targeted attacks. Specifically, we\nformulate the decision-based black-box targeted attack as a large-scale\noptimization problem. To efficiently solve the optimization problem, a\nthree-stage process \\textit{Ask, Attend, Attack}, called \\textit{AAA}, is\nproposed to coordinate with the solver. \\textit{Ask} guides attackers to create\ntarget texts that satisfy the specific semantics. \\textit{Attend} identifies\nthe crucial regions of the image for attacking, thus reducing the search space\nfor the subsequent \\textit{Attack}. \\textit{Attack} uses an evolutionary\nalgorithm to attack the crucial regions, where the attacks are semantically\nrelated to the target texts of \\textit{Ask}, thus achieving targeted attacks\nwithout semantic loss. Experimental results on transformer-based and\nCNN+RNN-based image-to-text models confirmed the effectiveness of our proposed\n\\textit{AAA}.",
      "tldr_zh": "这篇论文针对图像到文本模型的弱点，提出了一种有效的decision-based black-box targeted attack方法，仅依赖最终输出文本进行针对性攻击，以避免现有白盒或灰盒攻击的局限性，如模型访问需求和语义损失。作者将攻击问题表述为大规模优化问题，并引入AAA（Ask, Attend, Attack）三阶段框架：Ask引导创建满足特定语义的目标文本，Attend识别图像的关键区域以缩小搜索空间，Attack则使用evolutionary algorithm针对这些区域进行语义相关攻击。实验在基于Transformer和CNN+RNN的图像到文本模型上验证了AAA的有效性，展示了其在实现针对性攻击时的优越性能。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08989v1",
      "published_date": "2024-08-16 19:35:06 UTC",
      "updated_date": "2024-08-16 19:35:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:49:58.031162"
    },
    {
      "arxiv_id": "2408.10270v1",
      "title": "SEAL: Systematic Error Analysis for Value ALignment",
      "title_zh": "SEAL：针对价值对齐的系统性错误分析",
      "authors": [
        "Manon Revel",
        "Matteo Cargnelutti",
        "Tyna Eloundou",
        "Greg Leppert"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) aims to align language\nmodels (LMs) with human values by training reward models (RMs) on binary\npreferences and using these RMs to fine-tune the base LMs. Despite its\nimportance, the internal mechanisms of RLHF remain poorly understood. This\npaper introduces new metrics to evaluate the effectiveness of modeling and\naligning human values, namely feature imprint, alignment resistance and\nalignment robustness. We categorize alignment datasets into target features\n(desired values) and spoiler features (undesired concepts). By regressing RM\nscores against these features, we quantify the extent to which RMs reward them\n- a metric we term feature imprint. We define alignment resistance as the\nproportion of the preference dataset where RMs fail to match human preferences,\nand we assess alignment robustness by analyzing RM responses to perturbed\ninputs. Our experiments, utilizing open-source components like the\nAnthropic/hh-rlhf preference dataset and OpenAssistant RMs, reveal significant\nimprints of target features and a notable sensitivity to spoiler features. We\nobserved a 26% incidence of alignment resistance in portions of the dataset\nwhere LM-labelers disagreed with human preferences. Furthermore, we find that\nmisalignment often arises from ambiguous entries within the alignment dataset.\nThese findings underscore the importance of scrutinizing both RMs and alignment\ndatasets for a deeper understanding of value alignment.",
      "tldr_zh": "这篇论文提出了 SEAL 框架，用于系统分析 Reinforcement Learning from Human Feedback (RLHF) 在语言模型 (LMs) 价值对齐中的错误。作者定义了新指标：feature imprint（量化奖励模型 (RMs) 对目标特征和破坏者特征的奖励程度）、alignment resistance（RMs 未匹配人类偏好的数据集比例）和 alignment robustness（RMs 对扰动输入的响应能力）。通过实验，使用 Anthropic/hh-rlhf 数据集和 OpenAssistant RMs，他们发现显著的 target features 印记、26% 的 alignment resistance 发生率，以及误对齐常源于数据集中的模糊条目。这些发现强调了审视 RMs 和对齐数据集的重要性，以提升 RLHF 的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 17 Figures, 8 Tables",
      "pdf_url": "http://arxiv.org/pdf/2408.10270v1",
      "published_date": "2024-08-16 18:48:30 UTC",
      "updated_date": "2024-08-16 18:48:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:50:11.174899"
    },
    {
      "arxiv_id": "2408.08972v1",
      "title": "ASGM-KG: Unveiling Alluvial Gold Mining Through Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Debashis Gupta",
        "Aditi Golder",
        "Luis Fernendez",
        "Miles Silman",
        "Greg Lersen",
        "Fan Yang",
        "Bob Plemmons",
        "Sarra Alqahtani",
        "Paul Victor Pauca"
      ],
      "abstract": "Artisanal and Small-Scale Gold Mining (ASGM) is a low-cost yet highly\ndestructive mining practice, leading to environmental disasters across the\nworld's tropical watersheds. The topic of ASGM spans multiple domains of\nresearch and information, including natural and social systems, and knowledge\nis often atomized across a diversity of media and documents. We therefore\nintroduce a knowledge graph (ASGM-KG) that consolidates and provides crucial\ninformation about ASGM practices and their environmental effects. The current\nversion of ASGM-KG consists of 1,899 triples extracted using a large language\nmodel (LLM) from documents and reports published by both non-governmental and\ngovernmental organizations. These documents were carefully selected by a group\nof tropical ecologists with expertise in ASGM. This knowledge graph was\nvalidated using two methods. First, a small team of ASGM experts reviewed and\nlabeled triples as factual or non-factual. Second, we devised and applied an\nautomated factual reduction framework that relies on a search engine and an LLM\nfor labeling triples. Our framework performs as well as five baselines on a\npublicly available knowledge graph and achieves over 90 accuracy on our ASGM-KG\nvalidated by domain experts. ASGM-KG demonstrates an advancement in knowledge\naggregation and representation for complex, interdisciplinary environmental\ncrises such as ASGM.",
      "tldr_zh": "本研究构建了ASGM-KG知识图谱，用于整合手工和小规模黄金开采(Artisanal and Small-Scale Gold Mining, ASGM)相关信息，包括其环境破坏和跨领域知识。研究团队使用大型语言模型(LLM)从专家选定的文件和报告中提取了1,899个三元组。ASGM-KG通过专家审查和自动事实性验证框架（结合搜索引擎和LLM）进行验证，后者达到了超过90%的准确率。实验结果显示，该知识图谱在处理复杂跨学科环境危机方面取得了显著进展，为类似问题的知识聚合和表示提供了新方法。",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08972v1",
      "published_date": "2024-08-16 18:48:15 UTC",
      "updated_date": "2024-08-16 18:48:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:50:20.611774"
    },
    {
      "arxiv_id": "2408.08969v3",
      "title": "Differentiable Edge-based OPC",
      "title_zh": "翻译失败",
      "authors": [
        "Guojin Chen",
        "Haoyu Yang",
        "Haoxing Ren",
        "Bei Yu",
        "David Z. Pan"
      ],
      "abstract": "Optical proximity correction (OPC) is crucial for pushing the boundaries of\nsemiconductor manufacturing and enabling the continued scaling of integrated\ncircuits. While pixel-based OPC, termed as inverse lithography technology\n(ILT), has gained research interest due to its flexibility and precision. Its\ncomplexity and intricate features can lead to challenges in mask writing,\nincreased defects, and higher costs, hence hindering widespread industrial\nadoption. In this paper, we propose DiffOPC, a differentiable OPC framework\nthat enjoys the virtue of both edge-based OPC and ILT. By employing a mask\nrule-aware gradient-based optimization approach, DiffOPC efficiently guides\nmask edge segment movement during mask optimization, minimizing wafer error by\npropagating true gradients from the cost function back to the mask edges. Our\napproach achieves lower edge placement error while reducing manufacturing cost\nby half compared to state-of-the-art OPC techniques, bridging the gap between\nthe high accuracy of pixel-based OPC and the practicality required for\nindustrial adoption, thus offering a promising solution for advanced\nsemiconductor manufacturing.",
      "tldr_zh": "光学邻近校正 (OPC) 是半导体制造的关键技术，但传统的像素-based OPC（如逆刻蚀技术，ILT）由于复杂性和高成本问题，限制了其工业应用。\n本论文提出 DiffOPC，一种可微的 edge-based OPC 框架，结合了 edge-based OPC 的实用性和 ILT 的精度，通过 mask rule-aware gradient-based optimization 优化 mask edge segment，从而最小化 wafer error。\n实验结果表明，DiffOPC 比现有先进技术降低了 edge placement error，并将制造成本减半，为先进半导体制造提供了高精度且可行的解决方案。",
      "categories": [
        "cs.AI",
        "physics.optics"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ICCAD24",
      "pdf_url": "http://arxiv.org/pdf/2408.08969v3",
      "published_date": "2024-08-16 18:35:01 UTC",
      "updated_date": "2024-08-30 02:35:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:50:43.425045"
    },
    {
      "arxiv_id": "2408.08968v4",
      "title": "Online SLA Decomposition: Enabling Real-Time Adaptation to Evolving Network Systems",
      "title_zh": "在线 SLA 分解：实现对演变网络系统的实时适应",
      "authors": [
        "Cyril Shih-Huan Hsu",
        "Danny De Vleeschauwer",
        "Chrysa Papagianni",
        "Paola Grosso"
      ],
      "abstract": "When a network slice spans multiple technology domains, it is crucial for\neach domain to uphold the End-to-End (E2E) Service Level Agreement (SLA)\nassociated with the slice. Consequently, the E2E SLA must be properly\ndecomposed into partial SLAs that are assigned to each domain involved. In a\nnetwork slice management system with a two-level architecture, comprising an\nE2E service orchestrator and local domain controllers, we consider that the\norchestrator has access only to historical data regarding the responses of\nlocal controllers to previous requests, and this information is used to\nconstruct a risk model for each domain. In this study, we extend our previous\nwork by investigating the dynamic nature of real-world systems and introducing\nan online learning-decomposition framework to tackle the dynamicity. We propose\na framework that continuously updates the risk models based on the most recent\nfeedback. This approach leverages key components such as online gradient\ndescent and FIFO memory buffers, which enhance the stability and robustness of\nthe overall process. Our empirical study on an analytic model-based simulator\ndemonstrates that the proposed framework outperforms the state-of-the-art\nstatic approach, delivering more accurate and resilient SLA decomposition under\nvarying conditions and data limitations. Furthermore, we provide a\ncomprehensive complexity analysis of the proposed solution.",
      "tldr_zh": "本研究针对多域网络切片中的动态挑战，提出了一种在线 SLA 分解框架，以实现对演变网络系统的实时适应。该框架在两级架构（E2E 服务协调器和本地域控制器）下，利用历史数据构建风险模型，并通过在线梯度下降和 FIFO 内存缓冲持续更新模型，从而精确地将 E2E SLA 分解成部分 SLA。实验结果显示，该框架在模拟器上优于现有静态方法，提供更准确和鲁棒的 SLA 分解；此外，研究还提供了全面的复杂性分析，为网络管理系统优化提供了新途径。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "The paper has been accepted for publication at EuCNC & 6G Summit 2025",
      "pdf_url": "http://arxiv.org/pdf/2408.08968v4",
      "published_date": "2024-08-16 18:34:11 UTC",
      "updated_date": "2025-04-11 16:19:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:50:43.982574"
    },
    {
      "arxiv_id": "2408.08959v2",
      "title": "Trust-Oriented Adaptive Guardrails for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jinwei Hu",
        "Yi Dong",
        "Xiaowei Huang"
      ],
      "abstract": "Guardrail, an emerging mechanism designed to ensure that large language\nmodels (LLMs) align with human values by moderating harmful or toxic responses,\nrequires a sociotechnical approach in their design. This paper addresses a\ncritical issue: existing guardrails lack a well-founded methodology to\naccommodate the diverse needs of different user groups, particularly concerning\naccess rights. Supported by trust modeling (primarily on `social' aspect) and\nenhanced with online in-context learning via retrieval-augmented generation (on\n`technical' aspect), we introduce an adaptive guardrail mechanism, to\ndynamically moderate access to sensitive content based on user trust metrics.\nUser trust metrics, defined as a novel combination of direct interaction trust\nand authority-verified trust, enable the system to precisely tailor the\nstrictness of content moderation by aligning with the user's credibility and\nthe specific context of their inquiries. Our empirical evaluation demonstrates\nthe effectiveness of the adaptive guardrail in meeting diverse user needs,\noutperforming existing guardrails while securing sensitive information and\nprecisely managing potentially hazardous content through a context-aware\nknowledge base. To the best of our knowledge, this work is the first to\nintroduce trust-oriented concept into a guardrail system, offering a scalable\nsolution that enriches the discourse on ethical deployment for next-generation\nLLM service.",
      "tldr_zh": "这篇论文针对大型语言模型(LLMs)的Guardrail机制提出了一种基于信任导向的自适应方法，以解决现有系统无法满足不同用户群体访问需求的问题。论文引入用户信任指标，包括直接互动信任(direct interaction trust)和权威验证信任(authority-verified trust)，并结合信任建模和社会技术方面，以及在线上下文学习通过检索增强生成(Retrieval-Augmented Generation, RAG)，实现动态调节敏感内容的 moderation。实验评估表明，该机制在保护敏感信息和处理潜在危险内容方面优于基线模型，并首次将信任概念融入Guardrail系统，提供可扩展的道德部署解决方案。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2408.08959v2",
      "published_date": "2024-08-16 18:07:48 UTC",
      "updated_date": "2025-02-03 16:03:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:50:58.541283"
    },
    {
      "arxiv_id": "2408.08872v2",
      "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Le Xue",
        "Manli Shu",
        "Anas Awadalla",
        "Jun Wang",
        "An Yan",
        "Senthil Purushwalkam",
        "Honglu Zhou",
        "Viraj Prabhu",
        "Yutong Dai",
        "Michael S Ryoo",
        "Shrikant Kendre",
        "Jieyu Zhang",
        "Can Qin",
        "Shu Zhang",
        "Chia-Chih Chen",
        "Ning Yu",
        "Juntao Tan",
        "Tulika Manoj Awalgaonkar",
        "Shelby Heinecke",
        "Huan Wang",
        "Yejin Choi",
        "Ludwig Schmidt",
        "Zeyuan Chen",
        "Silvio Savarese",
        "Juan Carlos Niebles",
        "Caiming Xiong",
        "Ran Xu"
      ],
      "abstract": "This report introduces xGen-MM (also known as BLIP-3), a framework for\ndeveloping Large Multimodal Models (LMMs). The framework comprises meticulously\ncurated datasets, a training recipe, model architectures, and a resulting suite\nof LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen\ninitiative on foundation AI models. Our models undergo rigorous evaluation\nacross a range of tasks, including both single and multi-image benchmarks. Our\npre-trained base model exhibits strong in-context learning capabilities and the\ninstruction-tuned model demonstrates competitive performance among open-source\nLMMs with similar model sizes. In addition, we introduce a safety-tuned model\nwith DPO, aiming to mitigate harmful behaviors such as hallucinations and\nimprove safety. We open-source our models, curated large-scale datasets, and\nour fine-tuning codebase to facilitate further advancements in LMM research.\nAssociated resources will be available on our project page above.",
      "tldr_zh": "这篇论文介绍了 xGen-MM（也称为 BLIP-3），一个用于开发开源大型多模态模型（LMMs）的框架，包括精心策划的数据集、训练配方和模型架构，扩展了 Salesforce 的 xGen 计划。模型在单图像和多图像基准任务上经过严格评估，预训练基模型展示了强大的上下文学习能力，而指令微调模型在类似规模的开源 LMMs 中表现出色。作者还引入了使用 DPO 进行安全微调的版本，以减少幻觉等有害行为并提升安全性，并开源了模型、数据集和微调代码，以推动 LMM 研究的发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08872v2",
      "published_date": "2024-08-16 17:57:01 UTC",
      "updated_date": "2024-08-28 05:03:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:51:09.908997"
    },
    {
      "arxiv_id": "2408.08852v2",
      "title": "GeoTransformer: Enhancing Urban Forecasting with Dependency Retrieval and Geospatial Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhao Jia",
        "Zile Wu",
        "Shengao Yi",
        "Yifei Sun"
      ],
      "abstract": "Recent advances in urban forecasting have leveraged high-dimensional spatial\ndata through two primary approaches: graph-based methods that rely on\npredefined spatial structures and region-based methods that use satellite\nimagery for local features. Although these methods have laid an important\nfoundation, they struggle to integrate holistic urban information and\ndynamically model spatial dependencies. To address this gap, we propose\nGeoTransformer, a framework combining high-dimensional regional embeddings with\ndynamic spatial modeling. GeoTransformer features two innovations: (1) a\ndependency retrieval module identifying spatial dependencies to select relevant\nregions, and (2) a geospatial attention mechanism leveraging global urban\ninformation. These components unify structural and global urban information for\nbetter predictions. Extensive experiments on GDP and ride-share demand\nforecasting show that GeoTransformer outperforms baselines, highlighting its\neffectiveness in advancing urban forecasting tasks.",
      "tldr_zh": "本文研究发现，现有的城市预测方法，如基于图的结构和基于区域的卫星图像分析，难以整合整体城市信息并动态建模空间依赖。针对这一问题，提出 GeoTransformer 框架，将高维区域嵌入与动态空间建模结合，包括 dependency retrieval module 用于识别和选择相关区域，以及 geospatial attention 机制来利用全球城市信息。这些创新组件统一了结构和全局信息，提升了预测性能。实验结果显示，GeoTransformer 在 GDP 和共享出行需求预测任务上超过了基线模型，证明了其在城市预测领域的有效性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI 25's workshop AI for urban planning",
      "pdf_url": "http://arxiv.org/pdf/2408.08852v2",
      "published_date": "2024-08-16 17:26:42 UTC",
      "updated_date": "2024-12-19 19:55:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:51:23.649919"
    },
    {
      "arxiv_id": "2408.08823v1",
      "title": "Optimal Symmetries in Binary Classification",
      "title_zh": "二元分类中的最优对称性",
      "authors": [
        "Vishal S. Ngairangbam",
        "Michael Spannowsky"
      ],
      "abstract": "We explore the role of group symmetries in binary classification tasks,\npresenting a novel framework that leverages the principles of Neyman-Pearson\noptimality. Contrary to the common intuition that larger symmetry groups lead\nto improved classification performance, our findings show that selecting the\nappropriate group symmetries is crucial for optimising generalisation and\nsample efficiency. We develop a theoretical foundation for designing group\nequivariant neural networks that align the choice of symmetries with the\nunderlying probability distributions of the data. Our approach provides a\nunified methodology for improving classification accuracy across a broad range\nof applications by carefully tailoring the symmetry group to the specific\ncharacteristics of the problem. Theoretical analysis and experimental results\ndemonstrate that optimal classification performance is not always associated\nwith the largest equivariant groups possible in the domain, even when the\nlikelihood ratio is invariant under one of its proper subgroups, but rather\nwith those subgroups themselves. This work offers insights and practical\nguidelines for constructing more effective group equivariant architectures in\ndiverse machine-learning contexts.",
      "tldr_zh": "本文探讨了群对称性（group symmetries）在二元分类任务中的作用，提出一个基于 Neyman-Pearson 最优性的新框架，以优化模型的泛化和样本效率。不同于直觉认为更大对称群会提升性能，该研究发现，选择与数据概率分布对齐的适当对称群更为关键，并开发了相应的群等变神经网络（group equivariant neural networks）设计理论。实验结果表明，最优分类性能往往与特定子群相关，而非最大的群，这为机器学习中的架构设计提供了实用指导和统一方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.data-an",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 1 figure, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.08823v1",
      "published_date": "2024-08-16 16:15:18 UTC",
      "updated_date": "2024-08-16 16:15:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:51:34.665183"
    },
    {
      "arxiv_id": "2409.00022v1",
      "title": "Detecting Misinformation in Multimedia Content through Cross-Modal Entity Consistency: A Dual Learning Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Zhe Fu",
        "Kanlun Wang",
        "Wangjiaxuan Xin",
        "Lina Zhou",
        "Shi Chen",
        "Yaorong Ge",
        "Daniel Janies",
        "Dongsong Zhang"
      ],
      "abstract": "The landscape of social media content has evolved significantly, extending\nfrom text to multimodal formats. This evolution presents a significant\nchallenge in combating misinformation. Previous research has primarily focused\non single modalities or text-image combinations, leaving a gap in detecting\nmultimodal misinformation. While the concept of entity consistency holds\npromise in detecting multimodal misinformation, simplifying the representation\nto a scalar value overlooks the inherent complexities of high-dimensional\nrepresentations across different modalities. To address these limitations, we\npropose a Multimedia Misinformation Detection (MultiMD) framework for detecting\nmisinformation from video content by leveraging cross-modal entity consistency.\nThe proposed dual learning approach allows for not only enhancing\nmisinformation detection performance but also improving representation learning\nof entity consistency across different modalities. Our results demonstrate that\nMultiMD outperforms state-of-the-art baseline models and underscore the\nimportance of each modality in misinformation detection. Our research provides\nnovel methodological and technical insights into multimodal misinformation\ndetection.",
      "tldr_zh": "这篇论文针对社交媒体多模态内容的错误信息检测问题，提出了一种名为 MultiMD 的框架，通过利用跨模态实体一致性（cross-modal entity consistency）来处理视频内容的复杂性。框架采用双重学习（dual learning）方法，不仅提升了错误信息检测的性能，还优化了不同模态之间的实体一致性表示学习。实验结果显示，MultiMD 优于现有基准模型，并突出了每个模态在检测中的关键作用。该研究为多模态错误信息检测提供了新的方法和技术见解。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.MM",
      "comment": "Accepted to PACIS 2024. 15 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.00022v1",
      "published_date": "2024-08-16 16:14:36 UTC",
      "updated_date": "2024-08-16 16:14:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:51:47.407896"
    },
    {
      "arxiv_id": "2408.08821v3",
      "title": "EasyRec: Simple yet Effective Language Models for Recommendation",
      "title_zh": "EasyRec：简单而有效的推荐语言模型",
      "authors": [
        "Xubin Ren",
        "Chao Huang"
      ],
      "abstract": "Deep neural networks have become a powerful technique for learning\nrepresentations from user-item interaction data in collaborative filtering (CF)\nfor recommender systems. However, many existing methods heavily rely on unique\nuser and item IDs, which limits their ability to perform well in practical\nzero-shot learning scenarios where sufficient training data may be unavailable.\nInspired by the success of language models (LMs) and their strong\ngeneralization capabilities, a crucial question arises: How can we harness the\npotential of language models to empower recommender systems and elevate its\ngeneralization capabilities to new heights? In this study, we propose EasyRec -\nan effective and easy-to-use approach that seamlessly integrates text-based\nsemantic understanding with collaborative signals. EasyRec employs a\ntext-behavior alignment framework, which combines contrastive learning with\ncollaborative language model tuning, to ensure a strong alignment between the\ntext-enhanced semantic space and the collaborative behavior information.\nExtensive empirical evaluations across diverse real-world datasets demonstrate\nthe superior performance of EasyRec compared to state-of-the-art alternative\nmodels, particularly in the challenging text-based zero-shot recommendation\nscenarios. Furthermore, the study highlights the potential of seamlessly\nintegrating EasyRec as a plug-and-play component into text-enhanced\ncollaborative filtering frameworks, thereby empowering existing recommender\nsystems to elevate their recommendation performance and adapt to the evolving\nuser preferences in dynamic environments. For better result reproducibility of\nour EasyRec framework, the model implementation details, source code, and\ndatasets are available at the link: https://github.com/HKUDS/EasyRec.",
      "tldr_zh": "本研究提出 EasyRec，一种简单有效的推荐系统方法，利用 Language Models (LMs) 增强推荐系统的泛化能力，以解决现有 Collaborative Filtering (CF) 方法依赖用户和物品 ID 的局限性，尤其在 zero-shot learning 场景中。EasyRec 通过文本-行为对齐框架结合对比学习（contrastive learning）和协作语言模型微调，实现文本语义理解与协作信号的紧密整合。实验在多种真实数据集上证明，EasyRec 比现有模型性能更优，特别是文本-based zero-shot 推荐任务，并可作为 plug-and-play 组件集成到其他框架中，提升系统适应动态用户偏好的能力。源代码和数据集已在 GitHub 上公开，以便复现结果。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08821v3",
      "published_date": "2024-08-16 16:09:59 UTC",
      "updated_date": "2024-10-18 17:50:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:52:00.163926"
    },
    {
      "arxiv_id": "2409.00021v1",
      "title": "TACOS: Task Agnostic Continual Learning in Spiking Neural Networks",
      "title_zh": "TACOS：脉冲神经网络中的任务无关持续学习",
      "authors": [
        "Nicholas Soures",
        "Peter Helfer",
        "Anurag Daram",
        "Tej Pandit",
        "Dhireesha Kudithipudi"
      ],
      "abstract": "Catastrophic interference, the loss of previously learned information when\nlearning new information, remains a major challenge in machine learning. Since\nliving organisms do not seem to suffer from this problem, researchers have\ntaken inspiration from biology to improve memory retention in artificial\nintelligence systems. However, previous attempts to use bio-inspired mechanisms\nhave typically resulted in systems that rely on task boundary information\nduring training and/or explicit task identification during inference,\ninformation that is not available in real-world scenarios. Here, we show that\nneuro-inspired mechanisms such as synaptic consolidation and metaplasticity can\nmitigate catastrophic interference in a spiking neural network, using only\nsynapse-local information, with no need for task awareness, and with a fixed\nmemory size that does not need to be increased when training on new tasks. Our\nmodel, TACOS, combines neuromodulation with complex synaptic dynamics to enable\nnew learning while protecting previous information. We evaluate TACOS on\nsequential image recognition tasks and demonstrate its effectiveness in\nreducing catastrophic interference. Our results show that TACOS outperforms\nexisting regularization techniques in domain-incremental learning scenarios. We\nalso report the results of an ablation study to elucidate the contribution of\neach neuro-inspired mechanism separately.",
      "tldr_zh": "这篇论文介绍了 TACOS，一种任务无关持续学习(Task Agnostic Continual Learning)方法，应用于脉冲神经网络(spiking neural networks)，旨在缓解机器学习中的灾难性遗忘(catastrophic interference)。TACOS 通过突触巩固(synaptic consolidation)和元可塑性(metaplasticity)等神经启发机制，仅使用局部突触信息实现新任务学习，同时保护先前知识，且无需任务边界信息或增加内存。实验结果显示，在顺序图像识别任务上，TACOS 在领域增量学习(domain-incremental learning)场景中优于现有正则化技术；作者还通过消融研究(ablation study)验证了各机制的贡献。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00021v1",
      "published_date": "2024-08-16 15:42:16 UTC",
      "updated_date": "2024-08-16 15:42:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:52:23.441202"
    },
    {
      "arxiv_id": "2408.08808v3",
      "title": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge",
      "title_zh": "为 LLM-as-a-judge 构建领域特定评估集",
      "authors": [
        "Ravi Raju",
        "Swayambhoo Jain",
        "Bo Li",
        "Jonathan Li",
        "Urmish Thakker"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies.",
      "tldr_zh": "这篇论文针对 LLM-as-a-Judge 框架，提出了一种构建特定领域评估集的新数据管道，以解决现有基准（如 Alpaca-Eval 2.0 LC 和 Arena-Hard v0.1）在领域多样性（如法律、医学和多语言）方面的局限性。方法结合手动整理、半监督学习生成集群以及分层采样，创建了覆盖 14 个类别的 1573 个样本，确保平衡表示。结果显示，新评估集的分离性达 84%，与 Chatbot Arena 的同意率达 84%，Spearman 相关系数为 0.915，比 Arena-Hard 高 9% 且比 AlpacaEval 2.0 LC 高 20%，显著提升了基准的有效性。该工作还提供开源评估工具，支持细粒度分析模型在用户定义类别中的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 8 figures, Under review",
      "pdf_url": "http://arxiv.org/pdf/2408.08808v3",
      "published_date": "2024-08-16 15:41:43 UTC",
      "updated_date": "2024-08-20 02:32:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:52:25.098961"
    },
    {
      "arxiv_id": "2408.08805v1",
      "title": "CIKMar: A Dual-Encoder Approach to Prompt-Based Reranking in Educational Dialogue Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Joanito Agili Lopo",
        "Marina Indah Prasasti",
        "Alma Permatasari"
      ],
      "abstract": "In this study, we introduce CIKMar, an efficient approach to educational\ndialogue systems powered by the Gemma Language model. By leveraging a\nDual-Encoder ranking system that incorporates both BERT and SBERT model, we\nhave designed CIKMar to deliver highly relevant and accurate responses, even\nwith the constraints of a smaller language model size. Our evaluation reveals\nthat CIKMar achieves a robust recall and F1-score of 0.70 using BERTScore\nmetrics. However, we have identified a significant challenge: the Dual-Encoder\ntends to prioritize theoretical responses over practical ones. These findings\nunderscore the potential of compact and efficient models like Gemma in\ndemocratizing access to advanced educational AI systems, ensuring effective and\ncontextually appropriate responses.",
      "tldr_zh": "本研究引入了 CIKMar，一种基于 Gemma 语言模型的教育对话系统，使用 Dual-Encoder 架构（结合 BERT 和 SBERT）进行提示-based reranking，以提升响应的相关性和准确性。CIKMar 通过双编码器优化排名机制，确保在小型模型限制下提供高效的对话支持，评估结果显示其在 BERTScore 指标上达到 0.70 的召回率和 F1 分数。然而，该系统存在偏向理论响应而非实用响应的挑战，这突出了小型模型如 Gemma 在普及先进教育 AI 方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper is the result of the final project of the Natural Language\n  Processing course, Master of Artificial Intelligence, Universitas Gadjah Mada",
      "pdf_url": "http://arxiv.org/pdf/2408.08805v1",
      "published_date": "2024-08-16 15:29:54 UTC",
      "updated_date": "2024-08-16 15:29:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:52:34.735909"
    },
    {
      "arxiv_id": "2408.10269v1",
      "title": "OpenCity: Open Spatio-Temporal Foundation Models for Traffic Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Zhonghang Li",
        "Long Xia",
        "Lei Shi",
        "Yong Xu",
        "Dawei Yin",
        "Chao Huang"
      ],
      "abstract": "Accurate traffic forecasting is crucial for effective urban planning and\ntransportation management, enabling efficient resource allocation and enhanced\ntravel experiences. However, existing models often face limitations in\ngeneralization, struggling with zero-shot prediction on unseen regions and\ncities, as well as diminished long-term accuracy. This is primarily due to the\ninherent challenges in handling the spatial and temporal heterogeneity of\ntraffic data, coupled with the significant distribution shift across time and\nspace. In this work, we aim to unlock new possibilities for building versatile,\nresilient and adaptive spatio-temporal foundation models for traffic\nprediction. To achieve this goal, we introduce a novel foundation model, named\nOpenCity, that can effectively capture and normalize the underlying\nspatio-temporal patterns from diverse data characteristics, facilitating\nzero-shot generalization across diverse urban environments. OpenCity integrates\nthe Transformer architecture with graph neural networks to model the complex\nspatio-temporal dependencies in traffic data. By pre-training OpenCity on\nlarge-scale, heterogeneous traffic datasets, we enable the model to learn rich,\ngeneralizable representations that can be seamlessly applied to a wide range of\ntraffic forecasting scenarios. Experimental results demonstrate that OpenCity\nexhibits exceptional zero-shot predictive performance. Moreover, OpenCity\nshowcases promising scaling laws, suggesting the potential for developing a\ntruly one-for-all traffic prediction solution that can adapt to new urban\ncontexts with minimal overhead. We made our proposed OpenCity model open-source\nand it is available at the following link: https://github.com/HKUDS/OpenCity.",
      "tldr_zh": "该研究针对交通预测中的泛化问题（如零-shot预测和长期准确性下降），提出了一种新型时空基础模型OpenCity。该模型整合Transformer架构和Graph Neural Networks，以捕获并归一化多样交通数据的复杂时空依赖，并通过在大规模异质数据集上预训练，实现对不同城市环境的零-shot泛化。实验结果显示，OpenCity在零-shot预测中表现出色，并遵循scaling laws，具有开发通用交通预测解决方案的潜力；模型已开源，可从指定链接获取。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2408.10269v1",
      "published_date": "2024-08-16 15:20:36 UTC",
      "updated_date": "2024-08-16 15:20:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:52:46.809944"
    },
    {
      "arxiv_id": "2408.08790v1",
      "title": "A Disease-Specific Foundation Model Using Over 100K Fundus Images: Release and Validation for Abnormality and Multi-Disease Classification on Downstream Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Boa Jang",
        "Youngbin Ahn",
        "Eun Kyung Choe",
        "Chang Ki Yoon",
        "Hyuk Jin Choi",
        "Young-Gon Kim"
      ],
      "abstract": "Artificial intelligence applied to retinal images offers significant\npotential for recognizing signs and symptoms of retinal conditions and\nexpediting the diagnosis of eye diseases and systemic disorders. However,\ndeveloping generalized artificial intelligence models for medical data often\nrequires a large number of labeled images representing various disease signs,\nand most models are typically task-specific, focusing on major retinal\ndiseases. In this study, we developed a Fundus-Specific Pretrained Model\n(Image+Fundus), a supervised artificial intelligence model trained to detect\nabnormalities in fundus images. A total of 57,803 images were used to develop\nthis pretrained model, which achieved superior performance across various\ndownstream tasks, indicating that our proposed model outperforms other general\nmethods. Our Image+Fundus model offers a generalized approach to improve model\nperformance while reducing the number of labeled datasets required.\nAdditionally, it provides more disease-specific insights into fundus images,\nwith visualizations generated by our model. These disease-specific foundation\nmodels are invaluable in enhancing the performance and efficiency of deep\nlearning models in the field of fundus imaging.",
      "tldr_zh": "本研究开发了一种疾病特定基础模型（Image+Fundus），利用超过10万张视网膜图像（其中57,803张用于训练）来检测异常并进行多疾病分类。模型采用监督式人工智能方法，针对视网膜图像进行预训练，并在下游任务中表现出色，优于其他通用方法，同时减少了对标记数据集的需求。该模型不仅提升了诊断眼部疾病和全身性障碍的性能，还通过可视化提供疾病特定的见解，有助于提高视网膜成像领域的深度学习效率。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "10 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.08790v1",
      "published_date": "2024-08-16 15:03:06 UTC",
      "updated_date": "2024-08-16 15:03:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:52:58.041580"
    },
    {
      "arxiv_id": "2408.08785v1",
      "title": "A Transparency Paradox? Investigating the Impact of Explanation Specificity and Autonomous Vehicle Perceptual Inaccuracies on Passengers",
      "title_zh": "透明悖论？调查解释具体性",
      "authors": [
        "Daniel Omeiza",
        "Raunak Bhattacharyya",
        "Marina Jirotka",
        "Nick Hawes",
        "Lars Kunze"
      ],
      "abstract": "Transparency in automated systems could be afforded through the provision of\nintelligible explanations. While transparency is desirable, might it lead to\ncatastrophic outcomes (such as anxiety), that could outweigh its benefits? It's\nquite unclear how the specificity of explanations (level of transparency)\ninfluences recipients, especially in autonomous driving (AD). In this work, we\nexamined the effects of transparency mediated through varying levels of\nexplanation specificity in AD. We first extended a data-driven explainer model\nby adding a rule-based option for explanation generation in AD, and then\nconducted a within-subject lab study with 39 participants in an immersive\ndriving simulator to study the effect of the resulting explanations.\nSpecifically, our investigation focused on: (1) how different types of\nexplanations (specific vs. abstract) affect passengers' perceived safety,\nanxiety, and willingness to take control of the vehicle when the vehicle\nperception system makes erroneous predictions; and (2) the relationship between\npassengers' behavioural cues and their feelings during the autonomous drives.\nOur findings showed that passengers felt safer with specific explanations when\nthe vehicle's perception system had minimal errors, while abstract explanations\nthat hid perception errors led to lower feelings of safety. Anxiety levels\nincreased when specific explanations revealed perception system errors (high\ntransparency). We found no significant link between passengers' visual patterns\nand their anxiety levels. Our study suggests that passengers prefer clear and\nspecific explanations (high transparency) when they originate from autonomous\nvehicles (AVs) with optimal perceptual accuracy.",
      "tldr_zh": "本研究探讨了自动驾驶（AD）系统中解释具体性（explanation specificity）和车辆感知不准确（perceptual inaccuracies）对乘客的影响，揭示了透明度的潜在悖论，即高透明度解释可能引发焦虑。研究者扩展了一个数据驱动的解释器模型，添加了基于规则的选项，并在沉浸式驾驶模拟器中进行了一项涉及39名参与者的内部被试实验，评估了具体 vs. 抽象解释对乘客感知安全、焦虑和控制意愿的影响。结果显示，当车辆感知系统错误最小时，具体解释能提升乘客的安全感，但当解释揭示感知错误时，会增加焦虑水平；此外，抽象解释隐藏错误会导致安全感降低，且乘客的视觉模式与焦虑无显著关联。总体而言，乘客更倾向于高透明度的具体解释，但仅限于车辆感知准确的情况下。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "Submitted to Transportation Research Part F: Traffic Psychology and\n  Behaviour. arXiv admin note: text overlap with arXiv:2307.00633",
      "pdf_url": "http://arxiv.org/pdf/2408.08785v1",
      "published_date": "2024-08-16 14:59:00 UTC",
      "updated_date": "2024-08-16 14:59:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:53:12.812011"
    },
    {
      "arxiv_id": "2408.08781v1",
      "title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
      "title_zh": "翻译失败",
      "authors": [
        "Bhuvanashree Murugadoss",
        "Christian Poelitz",
        "Ian Drosos",
        "Vu Le",
        "Nick McKenna",
        "Carina Suzana Negreanu",
        "Chris Parnin",
        "Advait Sarkar"
      ],
      "abstract": "LLMs-as-a-judge is a recently popularized method which replaces human\njudgements in task evaluation (Zheng et al. 2024) with automatic evaluation\nusing LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human\nFeedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have\nstrong alignment with human preferences when prompted for a quality judgement,\nsuch as the coherence of a text. While this seems beneficial, it is not clear\nwhether the assessments by an LLM-as-a-judge constitute only an evaluation\nbased on the instructions in the prompts, or reflect its preference for\nhigh-quality data similar to its fine-tune data. To investigate how much\ninfluence prompting the LLMs-as-a-judge has on the alignment of AI judgements\nto human judgements, we analyze prompts with increasing levels of instructions\nabout the target quality of an evaluation, for several LLMs-as-a-judge.\nFurther, we compare to a prompt-free method using model perplexity as a quality\nmeasure instead. We aggregate a taxonomy of quality criteria commonly used\nacross state-of-the-art evaluations with LLMs and provide this as a rigorous\nbenchmark of models as judges. Overall, we show that the LLMs-as-a-judge\nbenefit only little from highly detailed instructions in prompts and that\nperplexity can sometimes align better with human judgements than prompting,\nespecially on textual quality.",
      "tldr_zh": "该研究评估了LLMs-as-a-judge（使用大型语言模型进行判断）是否严格遵守任务评估指令，而不是受其训练数据偏好影响。研究方法包括分析不同详细程度的提示对LLMs判断的影响，并将之与无提示方法（如模型perplexity）进行比较，同时构建了一个常见质量标准的分类体系作为基准。结果显示，LLMs-as-a-judge对详细指令的益处有限，而perplexity在某些情况下（如文本质量评估）更能与人类判断对齐，这为改进AI评估机制提供了重要洞见。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08781v1",
      "published_date": "2024-08-16 14:49:35 UTC",
      "updated_date": "2024-08-16 14:49:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:53:28.978618"
    },
    {
      "arxiv_id": "2408.08770v3",
      "title": "Pessimistic Iterative Planning for Robust POMDPs",
      "title_zh": "翻译失败",
      "authors": [
        "Maris F. L. Galesloot",
        "Marnix Suilen",
        "Thiago D. Simão",
        "Steven Carr",
        "Matthijs T. J. Spaan",
        "Ufuk Topcu",
        "Nils Jansen"
      ],
      "abstract": "Robust POMDPs extend classical POMDPs to handle model uncertainty.\nSpecifically, robust POMDPs exhibit so-called uncertainty sets on the\ntransition and observation models, effectively defining ranges of\nprobabilities. Policies for robust POMDPs must be (1) memory-based to account\nfor partial observability and (2) robust against model uncertainty to account\nfor the worst-case instances from the uncertainty sets. To compute such robust\nmemory-based policies, we propose the pessimistic iterative planning (PIP)\nframework, which alternates between two main steps: (1) selecting a pessimistic\n(non-robust) POMDP via worst-case probability instances from the uncertainty\nsets; and (2) computing a finite-state controller (FSC) for this pessimistic\nPOMDP. We evaluate the performance of this FSC on the original robust POMDP and\nuse this evaluation in step (1) to select the next pessimistic POMDP. Within\nPIP, we propose the rFSCNet algorithm. In each iteration, rFSCNet finds an FSC\nthrough a recurrent neural network by using supervision policies optimized for\nthe pessimistic POMDP. The empirical evaluation in four benchmark environments\nshowcases improved robustness against several baseline methods and competitive\nperformance compared to a state-of-the-art robust POMDP solver.",
      "tldr_zh": "该研究针对Robust POMDPs（扩展了经典POMDPs以处理模型不确定性）提出了一种Pessimistic Iterative Planning (PIP)框架，该框架通过交替选择最坏情况的概率实例构建悲观POMDP，并计算Finite-state Controller (FSC)来实现鲁棒的记忆-based策略。PIP的核心步骤包括从不确定性集中选取悲观实例，并基于其性能评估结果迭代优化。论文还引入了rFSCNet算法，利用循环神经网络和监督策略在每个迭代中训练FSC，以提升对部分可观测性和模型不确定性的适应性。在四个基准环境中的实验显示，rFSCNet比基线方法更具鲁棒性，并与最先进的Robust POMDP求解器表现出竞争性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08770v3",
      "published_date": "2024-08-16 14:25:20 UTC",
      "updated_date": "2024-11-12 13:50:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:53:39.094828"
    },
    {
      "arxiv_id": "2408.10268v2",
      "title": "Generating Streamlining Constraints with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Florentina Voboril",
        "Vaidyanathan Peruvemba Ramaswamy",
        "Stefan Szeider"
      ],
      "abstract": "Streamlining constraints (or streamliners, for short) narrow the search\nspace, enhancing the speed and feasibility of solving complex constraint\nsatisfaction problems. Traditionally, streamliners were crafted manually or\ngenerated through systematically combined atomic constraints with high-effort\noffline testing. Our approach utilizes the creativity of Large Language Models\n(LLMs) to propose effective streamliners for problems specified in the MiniZinc\nconstraint programming language and integrates feedback to the LLM with quick\nempirical tests for validation. Evaluated across seven diverse constraint\nsatisfaction problems, our method achieves substantial runtime reductions. We\ncompare the results to obfuscated and disguised variants of the problem to see\nwhether the results depend on LLM memorization. We also analyze whether longer\noff-line runs improve the quality of streamliners and whether the LLM can\npropose good combinations of streamliners.",
      "tldr_zh": "本研究提出了一种利用 Large Language Models (LLMs) 生成 Streamlining Constraints (streamliners) 的方法，以缩小搜索空间并提升复杂约束满足问题的求解速度和可行性。相较于传统的手动创建或高努力离线测试，本方法针对 MiniZinc 约束编程语言的问题，通过 LLMs 提出 streamliners 并整合反馈与快速经验测试进行验证。在七个多样化约束满足问题上，实验实现了显著的运行时间减少。研究还比较了问题混淆变体以排除 LLM 记忆的影响，并分析了更长离线运行是否能改善 streamliners 质量，以及 LLM 是否能有效组合 streamliners。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.10268v2",
      "published_date": "2024-08-16 14:17:26 UTC",
      "updated_date": "2025-01-28 21:31:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:53:53.073657"
    },
    {
      "arxiv_id": "2408.08739v1",
      "title": "ASVspoof 5: Crowdsourced Speech Data, Deepfakes, and Adversarial Attacks at Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Wang",
        "Hector Delgado",
        "Hemlata Tak",
        "Jee-weon Jung",
        "Hye-jin Shim",
        "Massimiliano Todisco",
        "Ivan Kukanov",
        "Xuechen Liu",
        "Md Sahidullah",
        "Tomi Kinnunen",
        "Nicholas Evans",
        "Kong Aik Lee",
        "Junichi Yamagishi"
      ],
      "abstract": "ASVspoof 5 is the fifth edition in a series of challenges that promote the\nstudy of speech spoofing and deepfake attacks, and the design of detection\nsolutions. Compared to previous challenges, the ASVspoof 5 database is built\nfrom crowdsourced data collected from a vastly greater number of speakers in\ndiverse acoustic conditions. Attacks, also crowdsourced, are generated and\ntested using surrogate detection models, while adversarial attacks are\nincorporated for the first time. New metrics support the evaluation of\nspoofing-robust automatic speaker verification (SASV) as well as stand-alone\ndetection solutions, i.e., countermeasures without ASV. We describe the two\nchallenge tracks, the new database, the evaluation metrics, baselines, and the\nevaluation platform, and present a summary of the results. Attacks\nsignificantly compromise the baseline systems, while submissions bring\nsubstantial improvements.",
      "tldr_zh": "ASVspoof 5 是语音欺骗和深度伪造（deepfakes）攻击研究的第五个挑战，使用众包数据从大量说话者和多样声学条件下构建数据库，并首次引入对抗攻击（adversarial attacks）。该挑战设计了两个轨道、新评估指标，支持评估欺骗鲁棒的自动说话者验证（SASV）以及独立检测解决方案（countermeasures）。实验结果显示，攻击显著损害了基线系统，而参赛提交带来了实质性性能提升。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "8 pages, ASVspoof 5 Workshop (Interspeech2024 Satellite)",
      "pdf_url": "http://arxiv.org/pdf/2408.08739v1",
      "published_date": "2024-08-16 13:37:20 UTC",
      "updated_date": "2024-08-16 13:37:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:54:01.812099"
    },
    {
      "arxiv_id": "2408.08732v1",
      "title": "Symbolic Parameter Learning in Probabilistic Answer Set Programming",
      "title_zh": "在概率答案集编程中的符号参数学习",
      "authors": [
        "Damiano Azzolini",
        "Elisabetta Gentili",
        "Fabrizio Riguzzi"
      ],
      "abstract": "Parameter learning is a crucial task in the field of Statistical Relational\nArtificial Intelligence: given a probabilistic logic program and a set of\nobservations in the form of interpretations, the goal is to learn the\nprobabilities of the facts in the program such that the probabilities of the\ninterpretations are maximized. In this paper, we propose two algorithms to\nsolve such a task within the formalism of Probabilistic Answer Set Programming,\nboth based on the extraction of symbolic equations representing the\nprobabilities of the interpretations. The first solves the task using an\noff-the-shelf constrained optimization solver while the second is based on an\nimplementation of the Expectation Maximization algorithm. Empirical results\nshow that our proposals often outperform existing approaches based on projected\nanswer set enumeration in terms of quality of the solution and in terms of\nexecution time. The paper has been accepted at the ICLP2024 conference and is\nunder consideration in Theory and Practice of Logic Programming (TPLP).",
      "tldr_zh": "本文研究了在 Probabilistic Answer Set Programming 中的 Symbolic Parameter Learning，目标是通过给定概率逻辑程序和观察解释，学习事实的概率以最大化解释的概率。作者提出了两种算法：第一种利用约束优化求解器（constrained optimization solver）基于符号方程提取来解决问题，第二种则基于 Expectation Maximization 算法实现类似功能。实验结果显示，这些方法在解决方案质量和执行时间上往往优于现有的基于 projected answer set enumeration 的方法。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "The paper has been accepted at the ICLP2024 conference and is under\n  consideration in Theory and Practice of Logic Programming (TPLP)",
      "pdf_url": "http://arxiv.org/pdf/2408.08732v1",
      "published_date": "2024-08-16 13:32:47 UTC",
      "updated_date": "2024-08-16 13:32:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:54:24.486713"
    },
    {
      "arxiv_id": "2408.08723v1",
      "title": "Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Sun",
        "Xiaosong Zhang",
        "Fang Wan",
        "Yanzhao Zhou",
        "Yuan Li",
        "Qixiang Ye",
        "Jianbin Jiao"
      ],
      "abstract": "Novel View Synthesis (NVS) without Structure-from-Motion (SfM) pre-processed\ncamera poses--referred to as SfM-free methods--is crucial for promoting rapid\nresponse capabilities and enhancing robustness against variable operating\nconditions. Recent SfM-free methods have integrated pose optimization,\ndesigning end-to-end frameworks for joint camera pose estimation and NVS.\nHowever, most existing works rely on per-pixel image loss functions, such as L2\nloss. In SfM-free methods, inaccurate initial poses lead to misalignment issue,\nwhich, under the constraints of per-pixel image loss functions, results in\nexcessive gradients, causing unstable optimization and poor convergence for\nNVS. In this study, we propose a correspondence-guided SfM-free 3D Gaussian\nsplatting for NVS. We use correspondences between the target and the rendered\nresult to achieve better pixel alignment, facilitating the optimization of\nrelative poses between frames. We then apply the learned poses to optimize the\nentire scene. Each 2D screen-space pixel is associated with its corresponding\n3D Gaussians through approximated surface rendering to facilitate gradient back\npropagation. Experimental results underline the superior performance and time\nefficiency of the proposed approach compared to the state-of-the-art baselines.",
      "tldr_zh": "该论文提出了一种基于对应关系（correspondences）的 SfM-Free 3D Gaussian Splatting 方法，用于新型视图合成（NVS），旨在解决现有 SfM-free 方法中初始位姿不准确导致的优化不稳定问题。方法通过利用目标图像和渲染结果之间的对应关系，实现更好的像素对齐，并优化帧之间的相对位姿，然后应用于整个场景优化，每个 2D 屏幕空间像素通过近似表面渲染与对应的 3D Gaussians 关联，以便于梯度反向传播。实验结果显示，该方法在性能和时间效率上优于现有最先进基线，为快速响应和鲁棒的 NVS 应用提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: text overlap with arXiv:2312.07504 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2408.08723v1",
      "published_date": "2024-08-16 13:11:22 UTC",
      "updated_date": "2024-08-16 13:11:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:54:27.420729"
    },
    {
      "arxiv_id": "2408.08713v4",
      "title": "CTR-KAN: KAN for Adaptive High-Order Feature Interaction Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Yunxiao Shi",
        "Wujiang Xu",
        "Haimin Zhang",
        "Qiang Wu",
        "Yongfeng Zhang",
        "Min Xu"
      ],
      "abstract": "Modeling high-order feature interactions is critical for click-through rate\n(CTR) prediction, yet traditional approaches often face challenges in balancing\npredictive accuracy and computational efficiency. These methods typically rely\non pre-defined interaction orders, which limit flexibility and require\nextensive prior knowledge. Moreover, explicitly modeling high-order\ninteractions can lead to significant computational overhead. To tackle these\nchallenges, we propose CTR-KAN, an adaptive framework for efficient high-order\nfeature interaction modeling. CTR-KAN builds upon the Kolmogorov-Arnold Network\n(KAN) paradigm, addressing its limitations in CTR prediction tasks.\nSpecifically, we introduce key enhancements, including a lightweight\narchitecture that reduces the computational complexity of KAN and supports\nembedding-based feature representations. Additionally, CTR-KAN integrates\nguided symbolic regression to effectively capture multiplicative relationships,\na known challenge in standard KAN implementations. Extensive experiments\ndemonstrate that CTR-KAN achieves state-of-the-art predictive accuracy with\nsignificantly lower computational costs. Its sparse network structure also\nfacilitates feature pruning and enhances global interpretability, making\nCTR-KAN a powerful tool for efficient inference in real-world CTR prediction\nscenarios.",
      "tldr_zh": "本论文针对点击率(CTR)预测中高阶特征交互建模的挑战，提出CTR-KAN框架，该框架基于Kolmogorov-Arnold Network (KAN)范式，并引入轻量级架构和guided symbolic regression来提升灵活性和效率。CTR-KAN支持嵌入式特征表示，并有效捕获乘法关系，从而解决传统方法的计算开销和预定义限制问题。实验结果显示，CTR-KAN在预测准确性上达到最先进水平，同时显著降低计算成本，其稀疏网络结构还增强了全局可解释性和特征修剪能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "draft paper",
      "pdf_url": "http://arxiv.org/pdf/2408.08713v4",
      "published_date": "2024-08-16 12:51:52 UTC",
      "updated_date": "2025-01-25 03:14:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:54:38.473017"
    },
    {
      "arxiv_id": "2408.08707v2",
      "title": "Beam Prediction based on Large Language Models",
      "title_zh": "基于大语言模型的波束预测",
      "authors": [
        "Yucheng Sheng",
        "Kai Huang",
        "Le Liang",
        "Peng Liu",
        "Shi Jin",
        "Geoffrey Ye Li"
      ],
      "abstract": "In this letter, we use large language models (LLMs) to develop a\nhigh-performing and robust beam prediction method. We formulate the millimeter\nwave (mmWave) beam prediction problem as a time series forecasting task, where\nthe historical observations are aggregated through cross-variable attention and\nthen transformed into text-based representations using a trainable tokenizer.\nBy leveraging the prompt-as-prefix (PaP) technique for contextual enrichment,\nour method harnesses the power of LLMs to predict future optimal beams.\nSimulation results demonstrate that our LLM-based approach outperforms\ntraditional learning-based models in prediction accuracy as well as robustness,\nhighlighting the significant potential of LLMs in enhancing wireless\ncommunication systems.",
      "tldr_zh": "该研究提出了一种基于大型语言模型 (LLMs) 的波束预测方法，将毫米波 (mmWave) 波束预测问题转化为时间序列预测任务，通过交叉变量注意力和可训练标记器将历史观察聚合并转换为文本表示。利用提示作为前缀 (PaP) 技术增强上下文，LLMs 能够有效预测未来的最佳波束。模拟结果显示，该方法在预测准确性和鲁棒性上优于传统学习模型，展示了 LLMs 在无线通信系统中的巨大潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08707v2",
      "published_date": "2024-08-16 12:40:01 UTC",
      "updated_date": "2025-02-12 13:29:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:54:49.445912"
    },
    {
      "arxiv_id": "2408.08704v2",
      "title": "Beyond the Hype: A dispassionate look at vision-language models in medical scenario",
      "title_zh": "超越炒作：对视觉语言模型在医疗场景中的客观审视",
      "authors": [
        "Yang Nan",
        "Huichi Zhou",
        "Xiaodan Xing",
        "Guang Yang"
      ],
      "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nremarkable capabilities across diverse tasks, garnering significant attention\nin AI communities. However, their performance and reliability in specialized\ndomains such as medicine remain insufficiently assessed. In particular, most\nassessments over-concentrate on evaluating VLMs based on simple Visual Question\nAnswering (VQA) on multi-modality data, while ignoring the in-depth\ncharacteristics of LVLMs. In this study, we introduce RadVUQA, a novel\nRadiological Visual Understanding and Question Answering benchmark, to\ncomprehensively evaluate existing LVLMs. RadVUQA mainly validates LVLMs across\nfive dimensions: 1) Anatomical understanding, assessing the models' ability to\nvisually identify biological structures; 2) Multimodal comprehension, which\ninvolves the capability of interpreting linguistic and visual instructions to\nproduce desired outcomes; 3) Quantitative and spatial reasoning, evaluating the\nmodels' spatial awareness and proficiency in combining quantitative analysis\nwith visual and linguistic information; 4) Physiological knowledge, measuring\nthe models' capability to comprehend functions and mechanisms of organs and\nsystems; and 5) Robustness, which assesses the models' capabilities against\nunharmonized and synthetic data. The results indicate that both generalized\nLVLMs and medical-specific LVLMs have critical deficiencies with weak\nmultimodal comprehension and quantitative reasoning capabilities. Our findings\nreveal the large gap between existing LVLMs and clinicians, highlighting the\nurgent need for more robust and intelligent LVLMs. The code is available at\nhttps://github.com/Nandayang/RadVUQA",
      "tldr_zh": "本论文对 Large Vision-Language Models (LVLMs) 在医疗场景中的性能进行了客观评估，指出现有评估过于依赖简单的 Visual Question Answering (VQA)，忽略了模型的深度特性。研究者引入了 RadVUQA 基准，通过五个维度（Anatomical understanding、Multimodal comprehension、Quantitative and spatial reasoning、Physiological knowledge 和 Robustness）全面测试 LVLMs 的能力。结果显示，现有 LVLMs 在多模态理解和定量推理方面存在重大缺陷，与临床医生相比仍有显著差距，呼吁开发更可靠的医疗智能模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2408.08704v2",
      "published_date": "2024-08-16 12:32:44 UTC",
      "updated_date": "2025-04-09 17:42:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:55:04.142826"
    },
    {
      "arxiv_id": "2408.08698v1",
      "title": "NFDI4DSO: Towards a BFO Compliant Ontology for Data Science",
      "title_zh": "翻译失败",
      "authors": [
        "Genet Asefa Gesese",
        "Jörg Waitelonis",
        "Zongxiong Chen",
        "Sonja Schimmler",
        "Harald Sack"
      ],
      "abstract": "The NFDI4DataScience (NFDI4DS) project aims to enhance the accessibility and\ninteroperability of research data within Data Science (DS) and Artificial\nIntelligence (AI) by connecting digital artifacts and ensuring they adhere to\nFAIR (Findable, Accessible, Interoperable, and Reusable) principles. To this\nend, this poster introduces the NFDI4DS Ontology, which describes resources in\nDS and AI and models the structure of the NFDI4DS consortium. Built upon the\nNFDICore ontology and mapped to the Basic Formal Ontology (BFO), this ontology\nserves as the foundation for the NFDI4DS knowledge graph currently under\ndevelopment.",
      "tldr_zh": "NFDI4DS 项目旨在通过 NFDI4DS Ontology 提升数据科学（DS）和人工智能（AI）领域的研究数据的可访问性和互操作性，确保其符合 FAIR（Findable, Accessible, Interoperable, and Reusable）原则。 该本体基于 NFDICore Ontology 并映射到 Basic Formal Ontology (BFO)，用于描述 DS 和 AI 中的资源以及 NFDI4DS 联盟的结构。 最终，NFDI4DS Ontology 将作为正在开发的 NFDI4DS 知识图（Knowledge Graph）的基础，促进数字工件的互联和重用。",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08698v1",
      "published_date": "2024-08-16 12:26:22 UTC",
      "updated_date": "2024-08-16 12:26:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:55:17.333146"
    },
    {
      "arxiv_id": "2408.08694v1",
      "title": "Quantifying the Effectiveness of Student Organization Activities using Natural Language Processing",
      "title_zh": "使用自然语言处理量化学生组织活动有效性",
      "authors": [
        "Lyberius Ennio F. Taruc",
        "Arvin R. De La Cruz"
      ],
      "abstract": "Student extracurricular activities play an important role in enriching the\nstudents' educational experiences. With the increasing popularity of Machine\nLearning and Natural Language Processing, it becomes a logical step that\nincorporating ML-NLP in improving extracurricular activities is a potential\nfocus of study in Artificial Intelligence (AI). This research study aims to\ndevelop a machine learning workflow that will quantify the effectiveness of\nstudent-organized activities based on student emotional responses using\nsentiment analysis. The study uses the Bidirectional Encoder Representations\nfrom Transformers (BERT) Large Language Model (LLM) called via the\npysentimiento toolkit, as a Transformer pipeline in Hugging Face. A sample data\nset from Organization C, a Recognized Student Organization (RSO) of a higher\neducational institute in the Philippines, College X, was used to develop the\nworkflow. The workflow consisted of data preprocessing, key feature selection,\nLLM feature processing, and score aggregation, resulting in an Event Score for\neach data set. The results show that the BERT LLM can also be used effectively\nin analyzing sentiment beyond product reviews and post comments. For the\nstudent affairs offices of educational institutions, this study can provide a\npractical example of how NLP can be applied to real-world scenarios, showcasing\nthe potential impact of data-driven decision making.",
      "tldr_zh": "本研究旨在使用自然语言处理（NLP）和机器学习（ML）量化学生组织的课外活动有效性，通过分析学生的情感响应。研究开发了一个工作流程，包括数据预处理、关键特征选择、Bidirectional Encoder Representations from Transformers (BERT) Large Language Model (LLM) 特征处理以及分数聚合，从而为每个数据集生成 Event Score。使用菲律宾一所高等教育机构的学生组织数据作为样本，实验结果表明 BERT LLM 不仅适用于产品评论，还能有效分析学生情感响应。该方法为教育机构的 student affairs offices 提供了 NLP 在真实场景中应用的数据驱动决策范例。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 4 figures, presented in International Conference on\n  Generative Al and its Applications (ICGAIA-24) last 22nd - 23rd, July, 2024\n  at Jakarta, Indonesia",
      "pdf_url": "http://arxiv.org/pdf/2408.08694v1",
      "published_date": "2024-08-16 12:16:59 UTC",
      "updated_date": "2024-08-16 12:16:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:55:29.890896"
    },
    {
      "arxiv_id": "2408.08688v4",
      "title": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Samee Arif",
        "Sualeha Farid",
        "Abdul Hameed Azeemi",
        "Awais Athar",
        "Agha Ali Raza"
      ],
      "abstract": "This paper presents a novel methodology for generating synthetic Preference\nOptimization (PO) datasets using multi-agent workflows. We evaluate the\neffectiveness and potential of these workflows in automating and enhancing the\ndataset generation process. PO dataset generation requires two modules: (1)\nresponse evaluation, and (2) response generation. In the response evaluation\nmodule, the responses from Large Language Models (LLMs) are evaluated and\nranked - a task typically carried out by human annotators that we automate\nusing LLMs. We assess the response evaluation module in a 2 step process. In\nstep 1, we assess LLMs as evaluators using three distinct prompting strategies.\nIn step 2, we apply the winning prompting strategy to compare the performance\nof LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that\nGPT-4o-as-a-Judge is more consistent across all datasets. For the response\ngeneration module, we use the identified LLM evaluator configuration and\ncompare different configurations of the LLM Feedback Loop. We use the win rate\nto determine the best multi-agent configuration for generation. Experimenting\nwith various configurations, we find that the LLM Feedback Loop, with Llama as\nthe generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win\nrate over single-agent Llama and Gemma, respectively. After identifying the\nbest configurations for both modules, we generate our PO datasets using the\nabove pipeline.",
      "tldr_zh": "本文提出了一种利用多智能体工作流生成合成 Preference Optimization (PO) 数据集的方法，以自动化并提升数据集生成过程。该方法包括两个模块：响应评估模块，使用 LLMs 模拟人类注释员，通过三种提示策略评估发现 GPT-4o-as-a-Judge 在一致性上表现最佳；以及响应生成模块，比较不同 LLM Feedback Loop 配置，结果显示 Llama 作为生成器和 Gemma 作为审阅者的组合，胜率分别达71.8%和73.8%，优于单智能体。在实验中，该框架成功生成了高质量的 PO 数据集，为高效数据集生成提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08688v4",
      "published_date": "2024-08-16 12:01:55 UTC",
      "updated_date": "2024-10-16 12:15:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:55:45.083880"
    },
    {
      "arxiv_id": "2408.08686v2",
      "title": "SC-Rec: Enhancing Generative Retrieval with Self-Consistent Reranking for Sequential Recommendation",
      "title_zh": "SC-Rec：通过自一致重排序增强生成式检索用于序列推荐",
      "authors": [
        "Tongyoung Kim",
        "Soojin Yoon",
        "Seongku Kang",
        "Jinyoung Yeo",
        "Dongha Lee"
      ],
      "abstract": "Language Models (LMs) are increasingly employed in recommendation systems due\nto their advanced language understanding and generation capabilities. Recent\nrecommender systems based on generative retrieval have leveraged the\ninferential abilities of LMs to directly generate the index tokens of the next\nitem, based on item sequences within the user's interaction history. Previous\nstudies have mostly focused on item indices based solely on textual semantic or\ncollaborative information. However, although the standalone effectiveness of\nthese aspects has been demonstrated, the integration of this information has\nremained unexplored. Our in-depth analysis finds that there is a significant\ndifference in the knowledge captured by the model from heterogeneous item\nindices and diverse input prompts, which can have a high potential for\ncomplementarity. In this paper, we propose SC-Rec, a unified recommender system\nthat learns diverse preference knowledge from two distinct item indices and\nmultiple prompt templates. Furthermore, SC-Rec adopts a novel reranking\nstrategy that aggregates a set of ranking results, inferred based on different\nindices and prompts, to achieve the self-consistency of the model. Our\nempirical evaluation on three real-world datasets demonstrates that SC-Rec\nconsiderably outperforms the state-of-the-art methods for sequential\nrecommendation, effectively incorporating complementary knowledge from varied\noutputs of the model.",
      "tldr_zh": "本论文提出 SC-Rec，一种增强生成式检索的推荐系统，旨在通过从两个不同物品索引和多个提示模板中学习多样化的偏好知识，解决现有方法在顺序推荐（Sequential Recommendation）中未整合文本语义和协作信息的局限性。SC-Rec 采用自一致性 reranking 策略，将基于不同索引和提示的排名结果聚合起来，实现模型的互补知识整合。实验在三个真实数据集上表明，SC-Rec 显著优于最先进方法，提升了推荐系统的整体性能。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08686v2",
      "published_date": "2024-08-16 11:59:01 UTC",
      "updated_date": "2024-08-19 04:31:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:55:57.656967"
    },
    {
      "arxiv_id": "2408.08685v3",
      "title": "Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?",
      "title_zh": "大语言模型能否改善图神经网络的对抗鲁棒性？",
      "authors": [
        "Zhongjian Zhang",
        "Xiao Wang",
        "Huichi Zhou",
        "Yue Yu",
        "Mengmei Zhang",
        "Cheng Yang",
        "Chuan Shi"
      ],
      "abstract": "Graph neural networks (GNNs) are vulnerable to adversarial attacks,\nespecially for topology perturbations, and many methods that improve the\nrobustness of GNNs have received considerable attention. Recently, we have\nwitnessed the significant success of large language models (LLMs), leading many\nto explore the great potential of LLMs on GNNs. However, they mainly focus on\nimproving the performance of GNNs by utilizing LLMs to enhance the node\nfeatures. Therefore, we ask: Will the robustness of GNNs also be enhanced with\nthe powerful understanding and inference capabilities of LLMs? By presenting\nthe empirical results, we find that despite that LLMs can improve the\nrobustness of GNNs, there is still an average decrease of 23.1% in accuracy,\nimplying that the GNNs remain extremely vulnerable against topology attacks.\nTherefore, another question is how to extend the capabilities of LLMs on graph\nadversarial robustness. In this paper, we propose an LLM-based robust graph\nstructure inference framework, LLM4RGNN, which distills the inference\ncapabilities of GPT-4 into a local LLM for identifying malicious edges and an\nLM-based edge predictor for finding missing important edges, so as to recover a\nrobust graph structure. Extensive experiments demonstrate that LLM4RGNN\nconsistently improves the robustness across various GNNs. Even in some cases\nwhere the perturbation ratio increases to 40%, the accuracy of GNNs is still\nbetter than that on the clean graph. The source code can be found in\nhttps://github.com/zhongjian-zhang/LLM4RGNN.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 是否能提升图神经网络 (GNNs) 的对抗鲁棒性，特别是针对拓扑扰动。实验结果显示，虽然 LLMs 通过增强节点特征能改善 GNNs 的鲁棒性，但准确率平均下降 23.1%，表明 GNNs 仍高度脆弱。作者提出 LLM4RGNN 框架，利用 GPT-4 的推理能力蒸馏到本地 LLM 中，识别恶意边并预测缺失的重要边，以恢复鲁棒的图结构。实验证明，LLM4RGNN 在各种 GNNs 上显著提升了鲁棒性，即使扰动率达 40% 时，准确率仍优于未扰动图。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted by KDD 2025",
      "pdf_url": "http://arxiv.org/pdf/2408.08685v3",
      "published_date": "2024-08-16 11:58:34 UTC",
      "updated_date": "2024-12-24 07:08:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:56:18.678768"
    },
    {
      "arxiv_id": "2408.08682v1",
      "title": "LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Yuqi Ye",
        "Wei Gao"
      ],
      "abstract": "The key to effective point cloud compression is to obtain a robust context\nmodel consistent with complex 3D data structures. Recently, the advancement of\nlarge language models (LLMs) has highlighted their capabilities not only as\npowerful generators for in-context learning and generation but also as\neffective compressors. These dual attributes of LLMs make them particularly\nwell-suited to meet the demands of data compression. Therefore, this paper\nexplores the potential of using LLM for compression tasks, focusing on lossless\npoint cloud geometry compression (PCGC) experiments. However, applying LLM\ndirectly to PCGC tasks presents some significant challenges, i.e., LLM does not\nunderstand the structure of the point cloud well, and it is a difficult task to\nfill the gap between text and point cloud through text description, especially\nfor large complicated and small shapeless point clouds. To address these\nproblems, we introduce a novel architecture, namely the Large Language\nModel-based Point Cloud Geometry Compression (LLM-PCGC) method, using LLM to\ncompress point cloud geometry information without any text description or\naligning operation. By utilizing different adaptation techniques for\ncross-modality representation alignment and semantic consistency, including\nclustering, K-tree, token mapping invariance, and Low Rank Adaptation (LoRA),\nthe proposed method can translate LLM to a compressor/generator for point\ncloud. To the best of our knowledge, this is the first structure to employ LLM\nas a compressor for point cloud data. Experiments demonstrate that the LLM-PCGC\noutperforms the other existing methods significantly, by achieving -40.213% bit\nrate reduction compared to the reference software of MPEG Geometry-based Point\nCloud Compression (G-PCC) standard, and by achieving -2.267% bit rate reduction\ncompared to the state-of-the-art learning-based method.",
      "tldr_zh": "本研究探讨了使用大型语言模型 (LLMs) 进行点云几何压缩 (PCGC)，提出了一种名为 LLM-PCGC 的新架构，以解决 LLMs 在处理复杂 3D 点云结构时的理解和适应挑战。方法通过聚类、K-tree、token mapping invariance 和 Low Rank Adaptation (LoRA) 等技术，实现跨模态表示对齐和语义一致性，从而将 LLMs 转化为点云压缩器，而无需文本描述或对齐操作。这是首次将 LLMs 应用于点云数据压缩。实验结果显示，LLM-PCGC 比 MPEG G-PCC 标准减少 40.213% 的比特率，并比最先进的基于学习方法减少 2.267%，显著提升了压缩性能。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08682v1",
      "published_date": "2024-08-16 11:55:44 UTC",
      "updated_date": "2024-08-16 11:55:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:56:19.759890"
    },
    {
      "arxiv_id": "2408.08677v1",
      "title": "Neural Reward Machines",
      "title_zh": "神经奖励机器",
      "authors": [
        "Elena Umili",
        "Francesco Argenziano",
        "Roberto Capobianco"
      ],
      "abstract": "Non-markovian Reinforcement Learning (RL) tasks are very hard to solve,\nbecause agents must consider the entire history of state-action pairs to act\nrationally in the environment. Most works use symbolic formalisms (as Linear\nTemporal Logic or automata) to specify the temporally-extended task. These\napproaches only work in finite and discrete state environments or continuous\nproblems for which a mapping between the raw state and a symbolic\ninterpretation is known as a symbol grounding (SG) function. Here, we define\nNeural Reward Machines (NRM), an automata-based neurosymbolic framework that\ncan be used for both reasoning and learning in non-symbolic non-markovian RL\ndomains, which is based on the probabilistic relaxation of Moore Machines. We\ncombine RL with semisupervised symbol grounding (SSSG) and we show that NRMs\ncan exploit high-level symbolic knowledge in non-symbolic environments without\nany knowledge of the SG function, outperforming Deep RL methods which cannot\nincorporate prior knowledge. Moreover, we advance the research in SSSG,\nproposing an algorithm for analysing the groundability of temporal\nspecifications, which is more efficient than baseline techniques of a factor\n$10^3$.",
      "tldr_zh": "论文提出 Neural Reward Machines (NRM)，一个基于Moore Machines的概率松弛神经符号框架，用于处理非Markovian Reinforcement Learning (RL) 任务中的挑战，这些任务要求代理考虑整个状态-动作历史。NRM 结合 RL 和 semisupervised symbol grounding (SSSG)，能在非符号环境中利用高级符号知识，而无需预知符号接地函数，从而优于无法整合先验知识的 Deep RL 方法。此外，该框架还引入了一种分析时序规范可接地性的算法，比基线技术效率提高 10^3 倍。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08677v1",
      "published_date": "2024-08-16 11:44:27 UTC",
      "updated_date": "2024-08-16 11:44:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:56:31.422874"
    },
    {
      "arxiv_id": "2408.08676v1",
      "title": "Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal Space Program",
      "title_zh": "微调 LLMs 用于自主航天器控制：以 Kerbal Space Program 为例的案例研究",
      "authors": [
        "Alejandro Carrasco",
        "Victor Rodriguez-Fernandez",
        "Richard Linares"
      ],
      "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompt. This study explores the use of fine-tuned Large Language Models (LLMs)\nfor autonomous spacecraft control, using the Kerbal Space Program Differential\nGames suite (KSPDG) as a testing environment. Traditional Reinforcement\nLearning (RL) approaches face limitations in this domain due to insufficient\nsimulation capabilities and data. By leveraging LLMs, specifically fine-tuning\nmodels like GPT-3.5 and LLaMA, we demonstrate how these models can effectively\ncontrol spacecraft using language-based inputs and outputs. Our approach\nintegrates real-time mission telemetry into textual prompts processed by the\nLLM, which then generate control actions via an agent. The results open a\ndiscussion about the potential of LLMs for space operations beyond their\nnominal use for text-related tasks. Future work aims to expand this methodology\nto other space control tasks and evaluate the performance of different LLM\nfamilies. The code is available at this URL:\n\\texttt{https://github.com/ARCLab-MIT/kspdg}.",
      "tldr_zh": "本研究探讨了微调大型语言模型 (LLMs)，如 GPT-3.5 和 LLaMA，用于实现自主航天器控制，以 Kerbal Space Program Differential Games suite (KSPDG) 作为模拟测试环境。传统 Reinforcement Learning (RL) 方法因模拟能力和数据不足而受限，该方法通过将实时任务遥测数据整合到文本提示中，让 LLMs 生成控制动作，从而有效处理语言输入输出。实验结果显示，LLMs 在航天器控制中表现出色，扩展了其在非文本任务中的潜力。未来工作将应用此方法到其他空间控制任务，并评估不同 LLM 家族的性能。",
      "categories": [
        "cs.AI",
        "astro-ph.IM"
      ],
      "primary_category": "cs.AI",
      "comment": "ESA SPAICE Conference 2024. arXiv admin note: text overlap with\n  arXiv:2404.00413",
      "pdf_url": "http://arxiv.org/pdf/2408.08676v1",
      "published_date": "2024-08-16 11:43:31 UTC",
      "updated_date": "2024-08-16 11:43:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:56:42.757454"
    },
    {
      "arxiv_id": "2408.08673v2",
      "title": "MAT-SED: A Masked Audio Transformer with Masked-Reconstruction Based Pre-training for Sound Event Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Pengfei Cai",
        "Yan Song",
        "Kang Li",
        "Haoyu Song",
        "Ian McLoughlin"
      ],
      "abstract": "Sound event detection (SED) methods that leverage a large pre-trained\nTransformer encoder network have shown promising performance in recent DCASE\nchallenges. However, they still rely on an RNN-based context network to model\ntemporal dependencies, largely due to the scarcity of labeled data. In this\nwork, we propose a pure Transformer-based SED model with masked-reconstruction\nbased pre-training, termed MAT-SED. Specifically, a Transformer with relative\npositional encoding is first designed as the context network, pre-trained by\nthe masked-reconstruction task on all available target data in a\nself-supervised way. Both the encoder and the context network are jointly\nfine-tuned in a semi-supervised manner. Furthermore, a global-local feature\nfusion strategy is proposed to enhance the localization capability. Evaluation\nof MAT-SED on DCASE2023 task4 surpasses state-of-the-art performance, achieving\n0.587/0.896 PSDS1/PSDS2 respectively.",
      "tldr_zh": "本论文提出了一种纯 Transformer 结构的音事件检测（SED）模型 MAT-SED，利用 masked-reconstruction 预训练方法来解决传统模型依赖 RNN 的问题。该模型采用相对位置编码的 Transformer 作为上下文网络，在目标数据上进行自监督预训练，并通过半监督方式对编码器和上下文网络进行联合微调，同时引入全局-局部特征融合策略以提升事件定位能力。在 DCASE2023 task4 的评估中，MAT-SED 取得了 0.587/0.896 的 PSDS1/PSDS2 成绩，超越了现有最先进性能。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Received by interspeech 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08673v2",
      "published_date": "2024-08-16 11:33:16 UTC",
      "updated_date": "2024-08-19 07:11:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:56:55.903051"
    },
    {
      "arxiv_id": "2408.08670v1",
      "title": "Adaptive Layer Selection for Efficient Vision Transformer Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Alessio Devoto",
        "Federico Alvetreti",
        "Jary Pomponi",
        "Paolo Di Lorenzo",
        "Pasquale Minervini",
        "Simone Scardapane"
      ],
      "abstract": "Recently, foundation models based on Vision Transformers (ViTs) have become\nwidely available. However, their fine-tuning process is highly\nresource-intensive, and it hinders their adoption in several edge or low-energy\napplications. To this end, in this paper we introduce an efficient fine-tuning\nmethod for ViTs called $\\textbf{ALaST}$ ($\\textit{Adaptive Layer Selection\nFine-Tuning for Vision Transformers}$) to speed up the fine-tuning process\nwhile reducing computational cost, memory load, and training time. Our approach\nis based on the observation that not all layers are equally critical during\nfine-tuning, and their importance varies depending on the current mini-batch.\nTherefore, at each fine-tuning step, we adaptively estimate the importance of\nall layers and we assign what we call ``compute budgets'' accordingly. Layers\nthat were allocated lower budgets are either trained with a reduced number of\ninput tokens or kept frozen. Freezing a layer reduces the computational cost\nand memory usage by preventing updates to its weights, while discarding tokens\nremoves redundant data, speeding up processing and reducing memory\nrequirements. We show that this adaptive compute allocation enables a\nnearly-optimal schedule for distributing computational resources across layers,\nresulting in substantial reductions in training time (up to 1.5x), FLOPs (up to\n2x), and memory load (up to 2x) compared to traditional full fine-tuning\napproaches. Additionally, it can be successfully combined with other\nparameter-efficient fine-tuning methods, such as LoRA.",
      "tldr_zh": "这篇论文提出了 ALaST（Adaptive Layer Selection Fine-Tuning for Vision Transformers），一种高效的 Vision Transformers (ViTs) 微调方法，旨在通过动态分配计算资源来加速微调过程并降低计算成本、内存负载和训练时间。方法基于层重要性的自适应估计，在每个微调步骤中为各层分配“compute budgets”，对重要性较低的层进行冻结或减少输入 tokens，从而优化资源利用。实验结果显示，ALaST 相较传统全微调方法，可减少训练时间高达 1.5 倍、FLOPs 高达 2 倍以及内存负载高达 2 倍，并能与 LoRA 等参数高效微调技术结合。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08670v1",
      "published_date": "2024-08-16 11:27:52 UTC",
      "updated_date": "2024-08-16 11:27:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:57:07.628050"
    },
    {
      "arxiv_id": "2408.08668v1",
      "title": "Robust Stochastic Shortest-Path Planning via Risk-Sensitive Incremental Sampling",
      "title_zh": "通过风险敏感增量采样的鲁棒随机最短路径规划",
      "authors": [
        "Clinton Enwerem",
        "Erfaun Noorani",
        "John S. Baras",
        "Brian M. Sadler"
      ],
      "abstract": "With the pervasiveness of Stochastic Shortest-Path (SSP) problems in\nhigh-risk industries, such as last-mile autonomous delivery and supply chain\nmanagement, robust planning algorithms are crucial for ensuring successful task\ncompletion while mitigating hazardous outcomes. Mainstream chance-constrained\nincremental sampling techniques for solving SSP problems tend to be overly\nconservative and typically do not consider the likelihood of undesirable tail\nevents. We propose an alternative risk-aware approach inspired by the\nasymptotically-optimal Rapidly-Exploring Random Trees (RRT*) planning\nalgorithm, which selects nodes along path segments with minimal Conditional\nValue-at-Risk (CVaR). Our motivation rests on the step-wise coherence of the\nCVaR risk measure and the optimal substructure of the SSP problem. Thus,\noptimizing with respect to the CVaR at each sampling iteration necessarily\nleads to an optimal path in the limit of the sample size. We validate our\napproach via numerical path planning experiments in a two-dimensional grid\nworld with obstacles and stochastic path-segment lengths. Our simulation\nresults show that incorporating risk into the tree growth process yields paths\nwith lengths that are significantly less sensitive to variations in the noise\nparameter, or equivalently, paths that are more robust to environmental\nuncertainty. Algorithmic analyses reveal similar query time and memory space\ncomplexity to the baseline RRT* procedure, with only a marginal increase in\nprocessing time. This increase is offset by significantly lower noise\nsensitivity and reduced planner failure rates.",
      "tldr_zh": "该论文针对Stochastic Shortest-Path (SSP)问题在高风险行业中的应用，提出了一种基于风险敏感增量采样的鲁棒规划方法，以Conditional Value-at-Risk (CVaR)作为优化指标，灵感来源于Rapidly-Exploring Random Trees (RRT*)算法。方法通过在每个采样迭代中选择CVaR最小路径节点，利用其逐步一致性和SSP的最优子结构，确保在采样规模增大时获得最优路径。实验在二维网格世界中验证，结果显示该方法生成的路径对环境噪声不敏感，鲁棒性显著提升，与基线RRT*相比，查询时间和内存复杂度类似，但规划失败率大幅降低。",
      "categories": [
        "cs.AI",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for presentation at the 2024 IEEE Conference on Decision and\n  Control (CDC)",
      "pdf_url": "http://arxiv.org/pdf/2408.08668v1",
      "published_date": "2024-08-16 11:21:52 UTC",
      "updated_date": "2024-08-16 11:21:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:57:18.004185"
    },
    {
      "arxiv_id": "2408.08666v2",
      "title": "A Multivocal Literature Review on Privacy and Fairness in Federated Learning",
      "title_zh": "联邦学习中隐私与公平的多",
      "authors": [
        "Beatrice Balbierer",
        "Lukas Heinlein",
        "Domenique Zipperling",
        "Niklas Kühl"
      ],
      "abstract": "Federated Learning presents a way to revolutionize AI applications by\neliminating the necessity for data sharing. Yet, research has shown that\ninformation can still be extracted during training, making additional\nprivacy-preserving measures such as differential privacy imperative. To\nimplement real-world federated learning applications, fairness, ranging from a\nfair distribution of performance to non-discriminative behaviour, must be\nconsidered. Particularly in high-risk applications (e.g. healthcare), avoiding\nthe repetition of past discriminatory errors is paramount. As recent research\nhas demonstrated an inherent tension between privacy and fairness, we conduct a\nmultivocal literature review to examine the current methods to integrate\nprivacy and fairness in federated learning. Our analyses illustrate that the\nrelationship between privacy and fairness has been neglected, posing a critical\nrisk for real-world applications. We highlight the need to explore the\nrelationship between privacy, fairness, and performance, advocating for the\ncreation of integrated federated learning frameworks.",
      "tldr_zh": "本论文通过多声部文献综述（multivocal literature review）考察了联邦学习（Federated Learning）中隐私和公平性的整合方法，强调隐私保护措施如差分隐私（differential privacy）虽能防止数据泄露，但与公平性（如性能公平分配和避免歧视行为）之间存在内在张力。研究发现，这种张力在高风险应用（如医疗）中被广泛忽略，可能导致真实应用的风险增加。作者呼吁深入探索隐私、公平和性能之间的关系，并推动开发整合的联邦学习框架，以避免过去歧视性错误的重复。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Proceedings of the 19th International Conference on\n  Wirtschaftsinformatik (WI), 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08666v2",
      "published_date": "2024-08-16 11:15:52 UTC",
      "updated_date": "2024-10-27 11:08:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:57:30.879312"
    },
    {
      "arxiv_id": "2408.08655v1",
      "title": "Mitigating Backdoor Attacks in Federated Learning via Flipping Weight Updates of Low-Activation Input Neurons",
      "title_zh": "翻译失败",
      "authors": [
        "Binbin Ding",
        "Penghui Yang",
        "Zeqing Ge",
        "Shengjun Huang"
      ],
      "abstract": "Federated learning enables multiple clients to collaboratively train machine\nlearning models under the overall planning of the server while adhering to\nprivacy requirements. However, the server cannot directly oversee the local\ntraining process, creating an opportunity for malicious clients to introduce\nbackdoors. Existing research shows that backdoor attacks activate specific\nneurons in the compromised model, which remain dormant when processing clean\ndata. Leveraging this insight, we propose a method called Flipping Weight\nUpdates of Low-Activation Input Neurons (FLAIN) to defend against backdoor\nattacks in federated learning. Specifically, after completing global training,\nwe employ an auxiliary dataset to identify low-activation input neurons and\nflip the associated weight updates. We incrementally raise the threshold for\nlow-activation inputs and flip the weight updates iteratively, until the\nperformance degradation on the auxiliary data becomes unacceptable. Extensive\nexperiments validate that our method can effectively reduce the success rate of\nbackdoor attacks to a low level in various attack scenarios including those\nwith non-IID data distribution or high MCRs, causing only minimal performance\ndegradation on clean data.",
      "tldr_zh": "这篇论文针对 Federated Learning 中的后门攻击，提出了一种名为 FLAIN 的防御方法，通过识别低-activation 输入神经元的权重更新并进行翻转来缓解攻击。FLAIN 方法在全球训练完成后，使用辅助数据集逐步提高低-activation 阈值，并迭代翻转权重更新，直至辅助数据性能下降到可接受水平。实验结果表明，该方法在非-IID 数据分布或高 MCR 的各种攻击场景下，能有效将后门攻击成功率降低到低水平，同时对干净数据的性能影响最小。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08655v1",
      "published_date": "2024-08-16 10:44:14 UTC",
      "updated_date": "2024-08-16 10:44:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:57:42.990753"
    },
    {
      "arxiv_id": "2408.08652v1",
      "title": "TextCAVs: Debugging vision models using text",
      "title_zh": "翻译失败",
      "authors": [
        "Angus Nicolson",
        "Yarin Gal",
        "J. Alison Noble"
      ],
      "abstract": "Concept-based interpretability methods are a popular form of explanation for\ndeep learning models which provide explanations in the form of high-level human\ninterpretable concepts. These methods typically find concept activation vectors\n(CAVs) using a probe dataset of concept examples. This requires labelled data\nfor these concepts -- an expensive task in the medical domain. We introduce\nTextCAVs: a novel method which creates CAVs using vision-language models such\nas CLIP, allowing for explanations to be created solely using text descriptions\nof the concept, as opposed to image exemplars. This reduced cost in testing\nconcepts allows for many concepts to be tested and for users to interact with\nthe model, testing new ideas as they are thought of, rather than a delay caused\nby image collection and annotation. In early experimental results, we\ndemonstrate that TextCAVs produces reasonable explanations for a chest x-ray\ndataset (MIMIC-CXR) and natural images (ImageNet), and that these explanations\ncan be used to debug deep learning-based models.",
      "tldr_zh": "这篇论文提出了 TextCAVs，一种创新方法，使用文本描述来创建概念激活向量 (CAVs)，从而解释深度学习视觉模型，而无需昂贵的图像标注数据。TextCAVs 利用视觉语言模型如 CLIP，仅通过概念的文本描述生成 CAVs，这大大降低了测试成本，并允许用户实时互动测试新想法。实验结果表明，该方法在 MIMIC-CXR（胸部 X 光数据集）和 ImageNet 上能产生合理的解释，并有效用于调试深度学习模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "I.2.1; I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 2 figures. Accepted at iMIMIC Workshop at MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08652v1",
      "published_date": "2024-08-16 10:36:08 UTC",
      "updated_date": "2024-08-16 10:36:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:57:55.530907"
    },
    {
      "arxiv_id": "2408.08651v2",
      "title": "Reasoning Beyond Bias: A Study on Counterfactual Prompting and Chain of Thought Reasoning",
      "title_zh": "超越偏见：反事实提示和链式思维推理的研究",
      "authors": [
        "Kyle Moore",
        "Jesse Roberts",
        "Thao Pham",
        "Douglas Fisher"
      ],
      "abstract": "Language models are known to absorb biases from their training data, leading\nto predictions driven by statistical regularities rather than semantic\nrelevance. We investigate the impact of these biases on answer choice\npreferences in the Massive Multi-Task Language Understanding (MMLU) task. Our\nfindings reveal that differences in learned regularities across answer options\nare predictive of model preferences and mirror human test-taking strategies. To\naddress this issue, we introduce two novel methods: Counterfactual Prompting\nwith Chain of Thought (CoT) and Counterfactual Prompting with Agnostically\nPrimed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with\nCoT alone is insufficient to mitigate bias, our novel Primed Counterfactual\nPrompting with CoT approach effectively reduces the influence of base-rate\nprobabilities while improving overall accuracy. Our results suggest that\nmitigating bias requires a \"System-2\" like process and that CoT reasoning is\nsusceptible to confirmation bias under some prompting methodologies. Our\ncontributions offer practical solutions for developing more robust and fair\nlanguage models.",
      "tldr_zh": "本研究探讨了语言模型在 Massive Multi-Task Language Understanding (MMLU) 任务中因训练数据偏见而产生的答案偏好问题，这些偏见导致模型依赖统计规律而非语义相关性，并类似于人类测试策略。论文引入两种新方法：Counterfactual Prompting with Chain of Thought (CoT) 和 Counterfactual Prompting with Agnostically Primed CoT (APriCoT)，旨在缓解这种偏见影响。结果表明，Counterfactual Prompting with CoT 单独使用不足以减少基线概率的影响，而 Primed Counterfactual Prompting with CoT 能有效降低偏见并提升整体准确率，同时揭示 CoT 推理在某些提示方法下易受确认偏见影响。总体上，该工作强调缓解偏见需要类似 System-2 的深度推理过程，并为开发更鲁棒和公平的语言模型提供了实用解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08651v2",
      "published_date": "2024-08-16 10:34:50 UTC",
      "updated_date": "2024-09-06 01:52:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:58:08.738997"
    },
    {
      "arxiv_id": "2408.08648v1",
      "title": "Understanding Enthymemes in Argument Maps: Bridging Argument Mining and Logic-based Argumentation",
      "title_zh": "翻译失败",
      "authors": [
        "Jonathan Ben-Naim",
        "Victor David",
        "Anthony Hunter"
      ],
      "abstract": "Argument mining is natural language processing technology aimed at\nidentifying arguments in text. Furthermore, the approach is being developed to\nidentify the premises and claims of those arguments, and to identify the\nrelationships between arguments including support and attack relationships. In\nthis paper, we assume that an argument map contains the premises and claims of\narguments, and support and attack relationships between them, that have been\nidentified by argument mining. So from a piece of text, we assume an argument\nmap is obtained automatically by natural language processing. However, to\nunderstand and to automatically analyse that argument map, it would be\ndesirable to instantiate that argument map with logical arguments. Once we have\nthe logical representation of the arguments in an argument map, we can use\nautomated reasoning to analyze the argumentation (e.g. check consistency of\npremises, check validity of claims, and check the labelling on each arc\ncorresponds with thw logical arguments). We address this need by using\nclassical logic for representing the explicit information in the text, and\nusing default logic for representing the implicit information in the text. In\norder to investigate our proposal, we consider some specific options for\ninstantiation.",
      "tldr_zh": "这篇论文探讨了如何桥接论点挖掘（Argument Mining）和基于逻辑的论证（Logic-based Argumentation），以理解论点地图中的隐含论点（Enthymemes）。作者假设从文本中自动获得论点地图，包括前提（Premises）、声明（Claims）以及支持和攻击关系（Support and Attack Relationships），并提出使用经典逻辑（Classical Logic）表示显式信息，以及默认逻辑（Default Logic）表示隐式信息。论文的主要贡献是通过这种逻辑实例化方法，实现对论点地图的自动推理分析，例如检查前提一致性、声明有效性和关系标签的对应性。最终，论文调查了具体的实例化选项，为可自动化的论证分析奠定了基础。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Research note",
      "pdf_url": "http://arxiv.org/pdf/2408.08648v1",
      "published_date": "2024-08-16 10:30:30 UTC",
      "updated_date": "2024-08-16 10:30:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:58:19.500842"
    },
    {
      "arxiv_id": "2408.08637v1",
      "title": "Magazine Supply Optimization: a Case-study",
      "title_zh": "翻译失败",
      "authors": [
        "Duong Nguyen",
        "Ana Ulianovici",
        "Sami Achour",
        "Soline Aubry",
        "Nicolas Chesneau"
      ],
      "abstract": "Supply optimization is a complex and challenging task in the magazine retail\nindustry because of the fixed inventory assumption, irregular sales patterns,\nand varying product and point-of-sale characteristics. We introduce AthenIA, an\nindustrialized magazine supply optimization solution that plans the supply for\nover 20,000 points of sale in France. We modularize the supply planning process\ninto a four-step pipeline: demand sensing, optimization, business rules, and\noperating. The core of the solution is a novel group conformalized quantile\nregression method that integrates domain expert insights, coupled with a supply\noptimization technique that balances the costs of out-of-stock against the\ncosts of over-supply. AthenIA has proven to be a valuable tool for magazine\npublishers, particularly in the context of evolving economic and ecological\nchallenges.",
      "tldr_zh": "该研究针对杂志零售行业的供应链优化问题（如固定库存、不规则销售模式和产品差异），提出了一种名为AthenIA的工业化解决方案，用于规划法国超过20,000个销售点的供应。该系统采用四步流程，包括demand sensing、optimization、business rules和operating，其核心是group conformalized quantile regression方法，该方法整合了领域专家见解，并通过供应链优化技术平衡缺货和过量供应的成本。AthenIA已在实际应用中证明其价值，尤其在应对经济和生态挑战时，为杂志出版商提供了有效的工具。",
      "categories": [
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08637v1",
      "published_date": "2024-08-16 10:06:59 UTC",
      "updated_date": "2024-08-16 10:06:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:58:30.685857"
    },
    {
      "arxiv_id": "2408.08632v2",
      "title": "A Survey on Benchmarks of Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jian Li",
        "Weiheng Lu",
        "Hao Fei",
        "Meng Luo",
        "Ming Dai",
        "Min Xia",
        "Yizhang Jin",
        "Zhenye Gan",
        "Ding Qi",
        "Chaoyou Fu",
        "Ying Tai",
        "Wankou Yang",
        "Yabiao Wang",
        "Chengjie Wang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on\n(1)perception and understanding, (2)cognition and reasoning, (3)specific\ndomains, (4)key capabilities, and (5)other modalities. Finally, we discuss the\nlimitations of the current evaluation methods for MLLMs and explore promising\nfuture directions. Our key argument is that evaluation should be regarded as a\ncrucial discipline to support the development of MLLMs better. For more\ndetails, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.",
      "tldr_zh": "这篇论文对多模态大型语言模型 (MLLMs) 的基准测试进行了全面调查，审查了 200 个评估基准，涵盖感知和理解、认知和推理、特定领域、关键能力和其他模态等多个方面。通过分析这些基准，论文突出了 MLLMs 在实际应用中的表现，并讨论了当前评估方法的局限性。最终，作者强调评估应被视为支持 MLLMs 发展的关键学科，并探索了未来研究方向，如改进评估框架以推动模型进步。更多细节可参考论文的 GitHub 仓库。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08632v2",
      "published_date": "2024-08-16 09:52:02 UTC",
      "updated_date": "2024-09-06 11:20:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:58:42.550945"
    },
    {
      "arxiv_id": "2408.08934v1",
      "title": "A Factored MDP Approach To Moving Target Defense With Dynamic Threat Modeling and Cost Efficiency",
      "title_zh": "翻译失败",
      "authors": [
        "Megha Bose",
        "Praveen Paruchuri",
        "Akshat Kumar"
      ],
      "abstract": "Moving Target Defense (MTD) has emerged as a proactive and dynamic framework\nto counteract evolving cyber threats. Traditional MTD approaches often rely on\nassumptions about the attackers knowledge and behavior. However, real-world\nscenarios are inherently more complex, with adaptive attackers and limited\nprior knowledge of their payoffs and intentions. This paper introduces a novel\napproach to MTD using a Markov Decision Process (MDP) model that does not rely\non predefined attacker payoffs. Our framework integrates the attackers\nreal-time responses into the defenders MDP using a dynamic Bayesian Network. By\nemploying a factored MDP model, we provide a comprehensive and realistic system\nrepresentation. We also incorporate incremental updates to an attack response\npredictor as new data emerges. This ensures an adaptive and robust defense\nmechanism. Additionally, we consider the costs of switching configurations in\nMTD, integrating them into the reward structure to balance execution and\ndefense costs. We first highlight the challenges of the problem through a\ntheoretical negative result on regret. However, empirical evaluations\ndemonstrate the frameworks effectiveness in scenarios marked by high\nuncertainty and dynamically changing attack landscapes.",
      "tldr_zh": "这篇论文提出了一种基于 Factored MDP 的 Moving Target Defense (MTD) 方法，用于应对动态网络威胁，而不依赖于预定义的攻击者收益和行为。该框架通过动态 Bayesian Network 整合攻击者的实时响应，并采用增量更新机制来构建适应性防御系统，同时将切换配置的成本纳入奖励结构中，以平衡执行和防御开销。理论分析揭示了问题中的遗憾挑战，但经验评估证明了该方法在高不确定性和动态攻击场景中的有效性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08934v1",
      "published_date": "2024-08-16 09:38:59 UTC",
      "updated_date": "2024-08-16 09:38:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:58:54.491544"
    },
    {
      "arxiv_id": "2408.08624v1",
      "title": "RealMedQA: A pilot biomedical question answering dataset containing realistic clinical questions",
      "title_zh": "RealMedQA：一个包含真实临床问题的试点生物医学问答数据集",
      "authors": [
        "Gregory Kell",
        "Angus Roberts",
        "Serge Umansky",
        "Yuti Khare",
        "Najma Ahmed",
        "Nikhil Patel",
        "Chloe Simela",
        "Jack Coumbe",
        "Julian Rozario",
        "Ryan-Rhys Griffiths",
        "Iain J. Marshall"
      ],
      "abstract": "Clinical question answering systems have the potential to provide clinicians\nwith relevant and timely answers to their questions. Nonetheless, despite the\nadvances that have been made, adoption of these systems in clinical settings\nhas been slow. One issue is a lack of question-answering datasets which reflect\nthe real-world needs of health professionals. In this work, we present\nRealMedQA, a dataset of realistic clinical questions generated by humans and an\nLLM. We describe the process for generating and verifying the QA pairs and\nassess several QA models on BioASQ and RealMedQA to assess the relative\ndifficulty of matching answers to questions. We show that the LLM is more\ncost-efficient for generating \"ideal\" QA pairs. Additionally, we achieve a\nlower lexical similarity between questions and answers than BioASQ which\nprovides an additional challenge to the top two QA models, as per the results.\nWe release our code and our dataset publicly to encourage further research.",
      "tldr_zh": "本文介绍了RealMedQA数据集，这是一个试点生物医学问答数据集，包含由人类和LLM生成的真实临床问题，旨在解决现有数据集无法满足临床实际需求的问题。通过验证过程和与BioASQ数据集的比较，研究发现LLM在生成“理想”QA pairs时更具成本效益，且RealMedQA的较低词汇相似度对顶级QA模型构成了额外挑战。作者公开了代码和数据集，以鼓励进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at AMIA Annual Symposium 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08624v1",
      "published_date": "2024-08-16 09:32:43 UTC",
      "updated_date": "2024-08-16 09:32:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:59:07.692723"
    },
    {
      "arxiv_id": "2408.08623v2",
      "title": "SketchRef: a Multi-Task Evaluation Benchmark for Sketch Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Xingyue Lin",
        "Xingjian Hu",
        "Shuai Peng",
        "Jianhua Zhu",
        "Liangcai Gao"
      ],
      "abstract": "Sketching is a powerful artistic technique for capturing essential visual\ninformation about real-world objects and has increasingly attracted attention\nin image synthesis research. However, the field lacks a unified benchmark to\nevaluate the performance of various synthesis methods. To address this, we\npropose SketchRef, the first comprehensive multi-task evaluation benchmark for\nsketch synthesis. SketchRef fully leverages the shared characteristics between\nsketches and reference photos. It introduces two primary tasks: category\nprediction and structural consistency estimation, the latter being largely\noverlooked in previous studies. These tasks are further divided into five\nsub-tasks across four domains: animals, common things, human body, and faces.\nRecognizing the inherent trade-off between recognizability and simplicity in\nsketches, we are the first to quantify this balance by introducing a\nrecognizability calculation method constrained by simplicity, mRS, ensuring\nfair and meaningful evaluations. To validate our approach, we collected 7,920\nresponses from art enthusiasts, confirming the effectiveness of our proposed\nevaluation metrics. Additionally, we evaluate the performance of existing\nsketch synthesis methods on our benchmark, highlighting their strengths and\nweaknesses. We hope this study establishes a standardized benchmark and offers\nvaluable insights for advancing sketch synthesis algorithms.",
      "tldr_zh": "本文提出 SketchRef，一种全面的多任务评估基准，用于评估草图合成方法的性能，包括类别预测和结构一致性估计两个主要任务，并将其分为五个子任务，覆盖动物、常见物体、人体和面部等领域。首次引入 mRS 方法来量化草图的 recognizability 和 simplicity 之间的权衡，确保评估的公平性，并通过收集 7,920 个艺术爱好者的响应验证了指标的有效性。研究评估了现有草图合成方法的优缺点，为该领域提供标准化基准和算法改进的见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08623v2",
      "published_date": "2024-08-16 09:32:26 UTC",
      "updated_date": "2025-04-09 03:18:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:59:19.264297"
    },
    {
      "arxiv_id": "2408.08622v1",
      "title": "DeepDFA: Automata Learning through Neural Probabilistic Relaxations",
      "title_zh": "翻译失败",
      "authors": [
        "Elena Umili",
        "Roberto Capobianco"
      ],
      "abstract": "In this work, we introduce DeepDFA, a novel approach to identifying\nDeterministic Finite Automata (DFAs) from traces, harnessing a differentiable\nyet discrete model. Inspired by both the probabilistic relaxation of DFAs and\nRecurrent Neural Networks (RNNs), our model offers interpretability\npost-training, alongside reduced complexity and enhanced training efficiency\ncompared to traditional RNNs. Moreover, by leveraging gradient-based\noptimization, our method surpasses combinatorial approaches in both scalability\nand noise resilience. Validation experiments conducted on target regular\nlanguages of varying size and complexity demonstrate that our approach is\naccurate, fast, and robust to noise in both the input symbols and the output\nlabels of training data, integrating the strengths of both logical grammar\ninduction and deep learning.",
      "tldr_zh": "本论文提出 DeepDFA，一种新型方法，通过神经概率松弛从 traces 中学习 Deterministic Finite Automata (DFAs)，结合 DFA 的概率松弛和 Recurrent Neural Networks (RNNs) 的灵感，提供训练后可解释性，同时降低复杂度并提升训练效率。相比传统组合方法，该方法利用基于梯度的优化，在可扩展性和噪声抵抗力上表现出色。实验在不同大小和复杂度的目标正则语言上验证了 DeepDFA 的准确性、速度和对输入符号及输出标签噪声的鲁棒性，融合了逻辑语法归纳和深度学习的优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08622v1",
      "published_date": "2024-08-16 09:30:36 UTC",
      "updated_date": "2024-08-16 09:30:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:59:32.988925"
    },
    {
      "arxiv_id": "2408.08619v1",
      "title": "PatUntrack: Automated Generating Patch Examples for Issue Reports without Tracked Insecure Code",
      "title_zh": "PatUntrack：针对未跟踪不安全代码的问题报告的自动生成补丁示例",
      "authors": [
        "Ziyou Jiang",
        "Lin Shi",
        "Guowei Yang",
        "Qing Wang"
      ],
      "abstract": "Security patches are essential for enhancing the stability and robustness of\nprojects in the software community. While vulnerabilities are officially\nexpected to be patched before being disclosed, patching vulnerabilities is\ncomplicated and remains a struggle for many organizations. To patch\nvulnerabilities, security practitioners typically track vulnerable issue\nreports (IRs), and analyze their relevant insecure code to generate potential\npatches. However, the relevant insecure code may not be explicitly specified\nand practitioners cannot track the insecure code in the repositories, thus\nlimiting their ability to generate patches. In such cases, providing examples\nof insecure code and the corresponding patches would benefit the security\ndevelopers to better locate and fix the insecure code. In this paper, we\npropose PatUntrack to automatically generating patch examples from IRs without\ntracked insecure code. It auto-prompts Large Language Models (LLMs) to make\nthem applicable to analyze the vulnerabilities. It first generates the\ncompleted description of the Vulnerability-Triggering Path (VTP) from\nvulnerable IRs. Then, it corrects hallucinations in the VTP description with\nexternal golden knowledge. Finally, it generates Top-K pairs of Insecure Code\nand Patch Example based on the corrected VTP description. To evaluate the\nperformance, we conducted experiments on 5,465 vulnerable IRs. The experimental\nresults show that PatUntrack can obtain the highest performance and improve the\ntraditional LLM baselines by +14.6% (Fix@10) on average in patch example\ngeneration. Furthermore, PatUntrack was applied to generate patch examples for\n76 newly disclosed vulnerable IRs. 27 out of 37 replies from the authors of\nthese IRs confirmed the usefulness of the patch examples generated by\nPatUntrack, indicating that they can benefit from these examples for patching\nthe vulnerabilities.",
      "tldr_zh": "论文提出 PatUntrack 系统，用于自动从没有跟踪不安全代码的 Issue Reports (IRs) 中生成补丁示例，以帮助安全从业者更高效地修复漏洞。  \n该系统利用 Large Language Models (LLMs) 通过自动提示生成完整的 Vulnerability-Triggering Path (VTP) 描述，然后使用外部可靠知识修正描述中的幻觉，最后基于修正后的描述生成 Top-K 对的不安全代码和补丁示例。  \n实验在 5,465 个漏洞 IRs 上显示，PatUntrack 比传统 LLM 基线平均提高了 14.6% (Fix@10) 的性能；此外，在 76 个新披露的漏洞 IRs 中，27 位作者确认了生成的补丁示例对实际修复的帮助。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by ASE'24",
      "pdf_url": "http://arxiv.org/pdf/2408.08619v1",
      "published_date": "2024-08-16 09:19:27 UTC",
      "updated_date": "2024-08-16 09:19:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:59:46.348912"
    },
    {
      "arxiv_id": "2408.08610v1",
      "title": "Generative Dataset Distillation Based on Diffusion Model",
      "title_zh": "基于扩散模型的生成数据集蒸馏",
      "authors": [
        "Duo Su",
        "Junjie Hou",
        "Guang Li",
        "Ren Togo",
        "Rui Song",
        "Takahiro Ogawa",
        "Miki Haseyama"
      ],
      "abstract": "This paper presents our method for the generative track of The First Dataset\nDistillation Challenge at ECCV 2024. Since the diffusion model has become the\nmainstay of generative models because of its high-quality generative effects,\nwe focus on distillation methods based on the diffusion model. Considering that\nthe track can only generate a fixed number of images in 10 minutes using a\ngenerative model for CIFAR-100 and Tiny-ImageNet datasets, we need to use a\ngenerative model that can generate images at high speed. In this study, we\nproposed a novel generative dataset distillation method based on Stable\nDiffusion. Specifically, we use the SDXL-Turbo model which can generate images\nat high speed and quality. Compared to other diffusion models that can only\ngenerate images per class (IPC) = 1, our method can achieve an IPC = 10 for\nTiny-ImageNet and an IPC = 20 for CIFAR-100, respectively. Additionally, to\ngenerate high-quality distilled datasets for CIFAR-100 and Tiny-ImageNet, we\nuse the class information as text prompts and post data augmentation for the\nSDXL-Turbo model. Experimental results show the effectiveness of the proposed\nmethod, and we achieved third place in the generative track of the ECCV 2024 DD\nChallenge. Codes are available at https://github.com/Guang000/BANKO.",
      "tldr_zh": "本文提出了一种基于 diffusion model 的生成数据集蒸馏方法，针对 ECCV 2024 数据集蒸馏挑战的生成轨道。方法利用 SDXL-Turbo 模型结合文本提示和后数据增强，实现 CIFAR-100 的 IPC=20 和 Tiny-ImageNet 的 IPC=10，提高了生成图像的速度和质量。实验结果验证了方法的有效性，并在挑战中获得第三名。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "The Third Place Winner in Generative Track of the ECCV 2024 DD\n  Challenge",
      "pdf_url": "http://arxiv.org/pdf/2408.08610v1",
      "published_date": "2024-08-16 08:52:02 UTC",
      "updated_date": "2024-08-16 08:52:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T15:59:55.782989"
    },
    {
      "arxiv_id": "2408.10266v1",
      "title": "Diffusion Model for Planning: A Systematic Literature Review",
      "title_zh": "扩散模型用于规划：系统文献综述",
      "authors": [
        "Toshihide Ubukata",
        "Jialong Li",
        "Kenji Tei"
      ],
      "abstract": "Diffusion models, which leverage stochastic processes to capture complex data\ndistributions effectively, have shown their performance as generative models,\nachieving notable success in image-related tasks through iterative denoising\nprocesses. Recently, diffusion models have been further applied and show their\nstrong abilities in planning tasks, leading to a significant growth in related\npublications since 2023. To help researchers better understand the field and\npromote the development of the field, we conduct a systematic literature review\nof recent advancements in the application of diffusion models for planning.\nSpecifically, this paper categorizes and discusses the current literature from\nthe following perspectives: (i) relevant datasets and benchmarks used for\nevaluating diffusion modelbased planning; (ii) fundamental studies that address\naspects such as sampling efficiency; (iii) skill-centric and condition-guided\nplanning for enhancing adaptability; (iv) safety and uncertainty managing\nmechanism for enhancing safety and robustness; and (v) domain-specific\napplication such as autonomous driving. Finally, given the above literature\nreview, we further discuss the challenges and future directions in this field.",
      "tldr_zh": "这篇论文对Diffusion models在规划任务中的应用进行了系统文献综述，旨在帮助研究者理解这一领域自2023年以来快速发展的进展。\n论文从多个视角分类讨论现有文献，包括用于评估的datasets和benchmarks、采样效率的基础研究、skill-centric和condition-guided规划以提升适应性、安全和不确定性管理机制，以及特定领域的应用如autonomous driving。\n最终，综述总结了Diffusion models在规划中的优势，并探讨了面临的挑战和未来研究方向，例如提高效率和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 2 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.10266v1",
      "published_date": "2024-08-16 08:37:01 UTC",
      "updated_date": "2024-08-16 08:37:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:00:08.267585"
    },
    {
      "arxiv_id": "2409.00015v1",
      "title": "Navigating the sociotechnical labyrinth: Dynamic certification for responsible embodied AI",
      "title_zh": "翻译失败",
      "authors": [
        "Georgios Bakirtzis",
        "Andrea Aler Tubella",
        "Andreas Theodorou",
        "David Danks",
        "Ufuk Topcu"
      ],
      "abstract": "Sociotechnical requirements shape the governance of artificially intelligent\n(AI) systems. In an era where embodied AI technologies are rapidly reshaping\nvarious facets of contemporary society, their inherent dynamic adaptability\npresents a unique blend of opportunities and challenges. Traditional regulatory\nmechanisms, often designed for static -- or slower-paced -- technologies, find\nthemselves at a crossroads when faced with the fluid and evolving nature of AI\nsystems. Moreover, typical problems in AI, for example, the frequent opacity\nand unpredictability of the behaviour of the systems, add additional\nsociotechnical challenges.\n  To address these interconnected issues, we introduce the concept of dynamic\ncertification, an adaptive regulatory framework specifically crafted to keep\npace with the continuous evolution of AI systems. The complexity of these\nchallenges requires common progress in multiple domains: technical,\nsocio-governmental, and regulatory. Our proposed transdisciplinary approach is\ndesigned to ensure the safe, ethical, and practical deployment of AI systems,\naligning them bidirectionally with the real-world contexts in which they\noperate. By doing so, we aim to bridge the gap between rapid technological\nadvancement and effective regulatory oversight, ensuring that AI systems not\nonly achieve their intended goals but also adhere to ethical standards and\nsocietal values.",
      "tldr_zh": "这篇论文探讨了具身 AI（embodied AI）技术的动态适应性在社会技术（sociotechnical）层面带来的机遇与挑战，指出传统静态监管机制难以应对 AI 的不透明性和快速演变。作者引入了 dynamic certification（动态认证）作为一种适应性监管框架，采用跨学科方法整合技术、社会政府和监管领域，以确保 AI 系统的安全、伦理和实际部署。最终，该方法旨在桥接技术进步与监管监督的鸿沟，使 AI 系统不仅实现预期目标，还符合社会价值观和伦理标准。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00015v1",
      "published_date": "2024-08-16 08:35:26 UTC",
      "updated_date": "2024-08-16 08:35:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:00:21.293698"
    },
    {
      "arxiv_id": "2408.08600v1",
      "title": "MM-UNet: A Mixed MLP Architecture for Improved Ophthalmic Image Segmentation",
      "title_zh": "MM-UNet：一种用于改进眼科图像分割的混合 MLP 架构",
      "authors": [
        "Zunjie Xiao",
        "Xiaoqing Zhang",
        "Risa Higashita",
        "Jiang Liu"
      ],
      "abstract": "Ophthalmic image segmentation serves as a critical foundation for ocular\ndisease diagnosis. Although fully convolutional neural networks (CNNs) are\ncommonly employed for segmentation, they are constrained by inductive biases\nand face challenges in establishing long-range dependencies. Transformer-based\nmodels address these limitations but introduce substantial computational\noverhead. Recently, a simple yet efficient Multilayer Perceptron (MLP)\narchitecture was proposed for image classification, achieving competitive\nperformance relative to advanced transformers. However, its effectiveness for\nophthalmic image segmentation remains unexplored. In this paper, we introduce\nMM-UNet, an efficient Mixed MLP model tailored for ophthalmic image\nsegmentation. Within MM-UNet, we propose a multi-scale MLP (MMLP) module that\nfacilitates the interaction of features at various depths through a grouping\nstrategy, enabling simultaneous capture of global and local information. We\nconducted extensive experiments on both a private anterior segment optical\ncoherence tomography (AS-OCT) image dataset and a public fundus image dataset.\nThe results demonstrated the superiority of our MM-UNet model in comparison to\nstate-of-the-art deep segmentation networks.",
      "tldr_zh": "本研究针对眼科图像分割的挑战，指出传统 CNN 受限于归纳偏差和长距离依赖问题，而 Transformer 模型虽能解决这些问题但计算开销过大，因此提出 MM-UNet，一种高效的混合 MLP 架构。MM-UNet 引入多尺度 MLP (MMLP) 模块，通过分组策略实现不同深度特征的交互，从而同时捕获全局和局部信息。在私有 AS-OCT 数据集和公共眼底图像数据集上的实验显示，MM-UNet 优于现有最先进的分段网络，显著提升了分割性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "OMIA2024",
      "pdf_url": "http://arxiv.org/pdf/2408.08600v1",
      "published_date": "2024-08-16 08:34:50 UTC",
      "updated_date": "2024-08-16 08:34:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:00:32.169053"
    },
    {
      "arxiv_id": "2408.12669v1",
      "title": "Bayesian Network Modeling of Causal Influence within Cognitive Domains and Clinical Dementia Severity Ratings for Western and Indian Cohorts",
      "title_zh": "翻译失败",
      "authors": [
        "Wupadrasta Santosh Kumar",
        "Sayali Rajendra Bhutare",
        "Neelam Sinha",
        "Thomas Gregor Issac"
      ],
      "abstract": "This study investigates the causal relationships between Clinical Dementia\nRatings (CDR) and its six domain scores across two distinct aging datasets: the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) and the Longitudinal Aging\nStudy of India (LASI). Using Directed Acyclic Graphs (DAGs) derived from\nBayesian network models, we analyze the dependencies among domain scores and\ntheir influence on the global CDR. Our approach leverages the PC algorithm to\nestimate the DAG structures for both datasets, revealing notable differences in\ncausal relationships and edge strengths between the Western and Indian\npopulations. The analysis highlights a stronger dependency of CDR scores on\nmemory functions in both datasets, but with significant variations in edge\nstrengths and node degrees. By contrasting these findings, we aim to elucidate\npopulation-specific differences and similarities in dementia progression,\nproviding insights that could inform targeted interventions and improve\nunderstanding of dementia across diverse demographic contexts.",
      "tldr_zh": "这篇论文使用贝叶斯网络模型和有向无环图（DAGs）分析了临床痴呆评级（CDR）及其六个认知领域分数在西方（Alzheimer's Disease Neuroimaging Initiative, ADNI）和印度（Longitudinal Aging Study of India, LASI）人群中的因果关系。研究采用 PC 算法估计 DAG 结构，揭示了两个数据集间显著差异，包括 CDR 对记忆功能的强依赖性，但边强度和节点度存在变异。通过对比这些发现，论文阐明了人群特定的痴呆进展模式，并为针对性干预和跨文化痴呆理解提供宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 2 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.12669v1",
      "published_date": "2024-08-16 07:53:57 UTC",
      "updated_date": "2024-08-16 07:53:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:00:48.016804"
    },
    {
      "arxiv_id": "2408.08590v2",
      "title": "A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Geonhee Kim",
        "Marco Valentino",
        "André Freitas"
      ],
      "abstract": "Recent studies on logical reasoning in Language Models (LMs) have sparked a\ndebate on whether they can learn systematic reasoning principles during\npre-training or merely exploit superficial patterns in the training data. This\npaper presents a mechanistic interpretation of syllogistic reasoning in LMs to\nadvance the understanding of internal dynamics. Specifically, we present a\nmethodology for circuit discovery aimed at interpreting content-independent\nreasoning mechanisms. Through two distinct intervention methods, we uncover a\nsufficient and necessary circuit involving middle-term suppression that\nelucidates how LMs transfer information to derive valid conclusions from\npremises. Furthermore, we investigate how belief biases manifest in syllogistic\nreasoning, finding evidence of partial contamination from additional attention\nheads responsible for encoding commonsense and contextualized knowledge.\nFinally, we explore the generalization of the discovered mechanisms across\nvarious syllogistic schemes, model sizes and architectures, finding that the\nidentified circuit is sufficient and necessary for the schemes on which the\nmodels achieve high downstream accuracy (> 60%), and that the activation\npatterns apply to models of different families. Overall, our findings suggest\nthat LMs indeed learn transferable content-independent reasoning mechanisms,\nbut that, at the same time, such mechanisms do not involve generalizable and\nabstract logical primitives, being susceptible to contamination by the same\nworld knowledge acquired during pre-training.",
      "tldr_zh": "这篇论文探讨了自回归语言模型 (LMs) 在三段论推理中的机制，旨在澄清模型是否学会了系统性推理原则还是仅依赖表面模式。研究者提出了一种电路发现方法，并通过两种干预实验，识别出一个涉及中间项抑制 (middle-term suppression) 的必要电路，帮助模型从前提中传输信息并推导出有效结论。同时，他们发现信念偏差部分源于额外注意力头对常识和上下文知识的污染，并验证了该机制在高准确率 (>60%) 的三段论方案、不同模型大小和架构中的泛化。总体而言，论文表明 LMs 确实习得了可转移的内容无关推理机制，但这些机制易受预训练世界知识污染，且缺乏可泛化的抽象逻辑原语。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08590v2",
      "published_date": "2024-08-16 07:47:39 UTC",
      "updated_date": "2025-02-17 12:09:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:00:57.798925"
    },
    {
      "arxiv_id": "2408.08584v1",
      "title": "S-RAF: A Simulation-Based Robustness Assessment Framework for Responsible Autonomous Driving",
      "title_zh": "S-RAF：一种基于模拟的鲁棒性评估框架，用于负责任的自动驾驶",
      "authors": [
        "Daniel Omeiza",
        "Pratik Somaiya",
        "Jo-Ann Pattinson",
        "Carolyn Ten-Holter",
        "Jack Stilgoe",
        "Marina Jirotka",
        "Lars Kunze"
      ],
      "abstract": "As artificial intelligence (AI) technology advances, ensuring the robustness\nand safety of AI-driven systems has become paramount. However, varying\nperceptions of robustness among AI developers create misaligned evaluation\nmetrics, complicating the assessment and certification of safety-critical and\ncomplex AI systems such as autonomous driving (AD) agents. To address this\nchallenge, we introduce Simulation-Based Robustness Assessment Framework\n(S-RAF) for autonomous driving. S-RAF leverages the CARLA Driving simulator to\nrigorously assess AD agents across diverse conditions, including faulty\nsensors, environmental changes, and complex traffic situations. By quantifying\nrobustness and its relationship with other safety-critical factors, such as\ncarbon emissions, S-RAF aids developers and stakeholders in building safe and\nresponsible driving agents, and streamlining safety certification processes.\nFurthermore, S-RAF offers significant advantages, such as reduced testing\ncosts, and the ability to explore edge cases that may be unsafe to test in the\nreal world. The code for this framework is available here:\nhttps://github.com/cognitive-robots/rai-leaderboard",
      "tldr_zh": "该研究提出S-RAF框架，一种基于模拟的鲁棒性评估工具，旨在解决自动驾驶（Autonomous Driving）系统评估指标不一致的问题，通过CARLA驾驶模拟器测试故障传感器、环境变化和复杂交通情况。S-RAF量化鲁棒性及其与安全关键因素（如碳排放）的关系，帮助开发者构建负责任的驾驶代理并简化安全认证流程。该框架的优势包括降低测试成本和安全探索边缘案例，并提供了开源代码以促进实际应用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08584v1",
      "published_date": "2024-08-16 07:37:05 UTC",
      "updated_date": "2024-08-16 07:37:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:01:08.038891"
    },
    {
      "arxiv_id": "2408.08571v1",
      "title": "AgentSimulator: An Agent-based Approach for Data-driven Business Process Simulation",
      "title_zh": "AgentSimulator：一种基于代理的方法，用于数据驱动的业务流程模拟",
      "authors": [
        "Lukas Kirchdorfer",
        "Robert Blümel",
        "Timotheus Kampik",
        "Han van der Aa",
        "Heiner Stuckenschmidt"
      ],
      "abstract": "Business process simulation (BPS) is a versatile technique for estimating\nprocess performance across various scenarios. Traditionally, BPS approaches\nemploy a control-flow-first perspective by enriching a process model with\nsimulation parameters. Although such approaches can mimic the behavior of\ncentrally orchestrated processes, such as those supported by workflow systems,\ncurrent control-flow-first approaches cannot faithfully capture the dynamics of\nreal-world processes that involve distinct resource behavior and decentralized\ndecision-making. Recognizing this issue, this paper introduces AgentSimulator,\na resource-first BPS approach that discovers a multi-agent system from an event\nlog, modeling distinct resource behaviors and interaction patterns to simulate\nthe underlying process. Our experiments show that AgentSimulator achieves\nstate-of-the-art simulation accuracy with significantly lower computation times\nthan existing approaches while providing high interpretability and adaptability\nto different types of process-execution scenarios.",
      "tldr_zh": "本论文提出AgentSimulator，一种基于智能体的数据驱动业务过程模拟(BPS)方法，以资源行为为主导，从事件日志中发现多智能体系统，并模拟资源间的交互模式，以更好地捕捉真实世界中去中心化决策的动态。相比传统以控制流为主的BPS方法，AgentSimulator显著提高了模拟准确性，同时降低了计算时间。实验结果显示，该方法在各种过程执行场景中表现出色，提供高可解释性和适应性。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08571v1",
      "published_date": "2024-08-16 07:19:11 UTC",
      "updated_date": "2024-08-16 07:19:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:01:19.591750"
    },
    {
      "arxiv_id": "2408.08933v1",
      "title": "RoarGraph: A Projected Bipartite Graph for Efficient Cross-Modal Approximate Nearest Neighbor Search",
      "title_zh": "翻译失败",
      "authors": [
        "Meng Chen",
        "Kai Zhang",
        "Zhenying He",
        "Yinan Jing",
        "X. Sean Wang"
      ],
      "abstract": "Approximate Nearest Neighbor Search (ANNS) is a fundamental and critical\ncomponent in many applications, including recommendation systems and large\nlanguage model-based applications. With the advancement of multimodal neural\nmodels, which transform data from different modalities into a shared\nhigh-dimensional space as feature vectors, cross-modal ANNS aims to use the\ndata vector from one modality (e.g., texts) as the query to retrieve the most\nsimilar items from another (e.g., images or videos). However, there is an\ninherent distribution gap between embeddings from different modalities, and\ncross-modal queries become Out-of-Distribution (OOD) to the base data.\nConsequently, state-of-the-art ANNS approaches suffer poor performance for OOD\nworkloads. In this paper, we quantitatively analyze the properties of the OOD\nworkloads to gain an understanding of their ANNS efficiency. Unlike\nsingle-modal workloads, we reveal OOD queries spatially deviate from base data,\nand the k-nearest neighbors of an OOD query are distant from each other in the\nembedding space. The property breaks the assumptions of existing ANNS\napproaches and mismatches their design for efficient search. With insights from\nthe OOD workloads, we propose pRojected bipartite Graph (RoarGraph), an\nefficient ANNS graph index built under the guidance of query distribution.\nExtensive experiments show that RoarGraph significantly outperforms\nstate-of-the-art approaches on modern cross-modal datasets, achieving up to\n3.56x faster search speed at a 90% recall rate for OOD queries.",
      "tldr_zh": "该论文针对跨模态 Approximate Nearest Neighbor Search (ANNS) 的 Out-of-Distribution (OOD) 查询问题进行分析，发现 OOD 查询与基数据分布偏离，且 k-最近邻居在嵌入空间中相距较远，导致现有 ANNS 方法效率低下。\n作者提出 RoarGraph，一种基于查询分布指导的投影二分图索引，通过构建高效的图结构来优化跨模态搜索。\n实验结果显示，RoarGraph 在现代跨模态数据集上比最先进方法快 3.56 倍，同时在 90% 召回率下显著提升搜索性能。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.IR",
      "comment": "to be published in PVLDB",
      "pdf_url": "http://arxiv.org/pdf/2408.08933v1",
      "published_date": "2024-08-16 06:48:16 UTC",
      "updated_date": "2024-08-16 06:48:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:01:34.964650"
    },
    {
      "arxiv_id": "2408.08550v2",
      "title": "String Diagram of Optimal Transports",
      "title_zh": "翻译失败",
      "authors": [
        "Kazuki Watanabe",
        "Noboru Isobe"
      ],
      "abstract": "We present a novel hierarchical framework for optimal transport (OT) using\nstring diagrams, namely string diagrams of optimal transports. This framework\nreduces complex hierarchical OT problems to standard OT problems, allowing\nefficient synthesis of optimal hierarchical transportation plans. Our approach\nuses algebraic compositions of cost matrices to effectively model hierarchical\nstructures. We also study an adversarial situation with multiple choices in the\ncost matrices, where we present a polynomial-time algorithm for a relaxation of\nthe problem. Experimental results confirm the efficiency and performance\nadvantages of our proposed algorithm over the naive method.",
      "tldr_zh": "这篇论文提出了一种新颖的层次化框架，即 String Diagrams of Optimal Transports，用于简化最优传输（OT）问题，将复杂的层次化OT问题简化为标准OT问题，从而高效合成最优的层次化运输计划。该框架通过成本矩阵的代数组合来有效建模层次结构，并在成本矩阵存在多个选择的选择性对抗场景中，提供了一个多项式时间算法的松弛版本。实验结果显示，该算法在效率和性能上明显优于朴素方法，为处理复杂OT问题提供了实用工具。",
      "categories": [
        "cs.AI",
        "cs.NA",
        "math.NA",
        "math.OC",
        "90C05"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint, under review, 14 pages, 2 fugures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2408.08550v2",
      "published_date": "2024-08-16 06:33:56 UTC",
      "updated_date": "2025-01-25 04:50:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:01:45.203078"
    },
    {
      "arxiv_id": "2408.14480v1",
      "title": "Handling abort commands for household kitchen robots",
      "title_zh": "翻译失败",
      "authors": [
        "Darius Has",
        "Adrian Groza",
        "Mihai Pomarlan"
      ],
      "abstract": "We propose a solution for handling abort commands given to robots. The\nsolution is exemplified with a running scenario with household kitchen robots.\nThe robot uses planning to find sequences of actions that must be performed in\norder to gracefully cancel a previously received command. The Planning Domain\nDefinition Language (PDDL) is used to write a domain to model kitchen\nactivities and behaviours, and this domain is enriched with knowledge from\nonline ontologies and knowledge graphs, like DBPedia. We discuss the results\nobtained in different scenarios.",
      "tldr_zh": "这篇论文提出了一种处理家庭厨房机器人中止命令的解决方案，旨在通过规划技术优雅取消先前指令。研究使用 Planning Domain Definition Language (PDDL) 建模厨房活动和行为，并整合来自 DBPedia 等在线本体和知识图谱的知识，以生成合适的动作序列。实验结果在不同场景中验证了该方法的有效性，为机器人安全操作提供了实用框架。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.14480v1",
      "published_date": "2024-08-16 06:20:37 UTC",
      "updated_date": "2024-08-16 06:20:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:01:55.927749"
    },
    {
      "arxiv_id": "2408.08931v2",
      "title": "Personalized Federated Collaborative Filtering: A Variational AutoEncoder Approach",
      "title_zh": "个性化的联邦协同过滤：一种变分自编码器方法",
      "authors": [
        "Zhiwei Li",
        "Guodong Long",
        "Tianyi Zhou",
        "Jing Jiang",
        "Chengqi Zhang"
      ],
      "abstract": "Federated Collaborative Filtering (FedCF) is an emerging field focused on\ndeveloping a new recommendation framework with preserving privacy in a\nfederated setting. Existing FedCF methods typically combine distributed\nCollaborative Filtering (CF) algorithms with privacy-preserving mechanisms, and\nthen preserve personalized information into a user embedding vector. However,\nthe user embedding is usually insufficient to preserve the rich information of\nthe fine-grained personalization across heterogeneous clients. This paper\nproposes a novel personalized FedCF method by preserving users' personalized\ninformation into a latent variable and a neural model simultaneously.\nSpecifically, we decompose the modeling of user knowledge into two encoders,\neach designed to capture shared knowledge and personalized knowledge\nseparately. A personalized gating network is then applied to balance\npersonalization and generalization between the global and local encoders.\nMoreover, to effectively train the proposed framework, we model the CF problem\nas a specialized Variational AutoEncoder (VAE) task by integrating user\ninteraction vector reconstruction with missing value prediction. The decoder is\ntrained to reconstruct the implicit feedback from items the user has interacted\nwith, while also predicting items the user might be interested in but has not\nyet interacted with. Experimental results on benchmark datasets demonstrate\nthat the proposed method outperforms other baseline methods, showcasing\nsuperior performance. Our code is available at https://github.com/mtics/FedDAE.",
      "tldr_zh": "本文提出了一种新型的 Personalized Federated Collaborative Filtering (FedCF) 方法，通过 Variational AutoEncoder (VAE) 框架来提升推荐系统的隐私保护和个性化建模。方法将用户知识分解为两个编码器，一个捕获共享知识，另一个捕获个性化知识，并使用个性化门控网络平衡全局和本地模型，同时将 Collaborative Filtering (CF) 问题建模为 VAE 任务，包括用户交互向量重构和缺失值预测。实验结果显示，该方法在基准数据集上优于其他基线方法，展示了更好的性能和泛化能力。代码已公开在 https://github.com/mtics/FedDAE。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 3 figures, 4 tables, conference",
      "pdf_url": "http://arxiv.org/pdf/2408.08931v2",
      "published_date": "2024-08-16 05:49:14 UTC",
      "updated_date": "2024-12-10 03:39:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:02:09.818007"
    },
    {
      "arxiv_id": "2408.08531v2",
      "title": "Detecting Unsuccessful Students in Cybersecurity Exercises in Two Different Learning Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Valdemar Švábenský",
        "Kristián Tkáčik",
        "Aubrey Birdwell",
        "Richard Weiss",
        "Ryan S. Baker",
        "Pavel Čeleda",
        "Jan Vykopal",
        "Jens Mache",
        "Ankur Chattopadhyay"
      ],
      "abstract": "This full paper in the research track evaluates the usage of data logged from\ncybersecurity exercises in order to predict students who are potentially at\nrisk of performing poorly. Hands-on exercises are essential for learning since\nthey enable students to practice their skills. In cybersecurity, hands-on\nexercises are often complex and require knowledge of many topics. Therefore,\nstudents may miss solutions due to gaps in their knowledge and become\nfrustrated, which impedes their learning. Targeted aid by the instructor helps,\nbut since the instructor's time is limited, efficient ways to detect struggling\nstudents are needed. This paper develops automated tools to predict when a\nstudent is having difficulty. We formed a dataset with the actions of 313\nstudents from two countries and two learning environments: KYPO CRP and\nEDURange. These data are used in machine learning algorithms to predict the\nsuccess of students in exercises deployed in these environments. After\nextracting features from the data, we trained and cross-validated eight\nclassifiers for predicting the exercise outcome and evaluated their predictive\npower. The contribution of this paper is comparing two approaches to feature\nengineering, modeling, and classification performance on data from two learning\nenvironments. Using the features from either learning environment, we were able\nto detect and distinguish between successful and struggling students. A\ndecision tree classifier achieved the highest balanced accuracy and sensitivity\nwith data from both learning environments. The results show that activity data\nfrom cybersecurity exercises are suitable for predicting student success. In a\npotential application, such models can aid instructors in detecting struggling\nstudents and providing targeted help. We publish data and code for building\nthese models so that others can adopt or adapt them.",
      "tldr_zh": "这篇论文评估了使用网络安全练习的日志数据来预测可能表现不佳的学生，旨在帮助教师提供针对性援助。研究基于来自两个国家、两个学习环境（KYPO CRP 和 EDURange）的313名学生行动数据，提取特征并训练八种机器学习算法分类器，包括决策树分类器。结果显示，决策树分类器在两个环境中取得了最高的平衡准确率和敏感性，有效区分成功与挣扎的学生，并证明了活动数据在预测学生成功方面的潜力；论文还公开了数据和代码以供他人采用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CY",
        "K.3"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in the FIE 2024 conference proceedings, see\n  https://doi.org/10.1109/FIE61694.2024.10893135",
      "pdf_url": "http://arxiv.org/pdf/2408.08531v2",
      "published_date": "2024-08-16 04:57:54 UTC",
      "updated_date": "2025-03-02 18:15:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:02:21.687271"
    },
    {
      "arxiv_id": "2408.08527v1",
      "title": "Focus on Focus: Focus-oriented Representation Learning and Multi-view Cross-modal Alignment for Glioma Grading",
      "title_zh": "翻译失败",
      "authors": [
        "Li Pan",
        "Yupei Zhang",
        "Qiushi Yang",
        "Tan Li",
        "Xiaohan Xing",
        "Maximus C. F. Yeung",
        "Zhen Chen"
      ],
      "abstract": "Recently, multimodal deep learning, which integrates histopathology slides\nand molecular biomarkers, has achieved a promising performance in glioma\ngrading. Despite great progress, due to the intra-modality complexity and\ninter-modality heterogeneity, existing studies suffer from inadequate\nhistopathology representation learning and inefficient molecular-pathology\nknowledge alignment. These two issues hinder existing methods to precisely\ninterpret diagnostic molecular-pathology features, thereby limiting their\ngrading performance. Moreover, the real-world applicability of existing\nmultimodal approaches is significantly restricted as molecular biomarkers are\nnot always available during clinical deployment. To address these problems, we\nintroduce a novel Focus on Focus (FoF) framework with paired pathology-genomic\ntraining and applicable pathology-only inference, enhancing molecular-pathology\nrepresentation effectively. Specifically, we propose a Focus-oriented\nRepresentation Learning (FRL) module to encourage the model to identify regions\npositively or negatively related to glioma grading and guide it to focus on the\ndiagnostic areas with a consistency constraint. To effectively link the\nmolecular biomarkers to morphological features, we propose a Multi-view\nCross-modal Alignment (MCA) module that projects histopathology representations\ninto molecular subspaces, aligning morphological features with corresponding\nmolecular biomarker status by supervised contrastive learning. Experiments on\nthe TCGA GBM-LGG dataset demonstrate that our FoF framework significantly\nimproves the glioma grading. Remarkably, our FoF achieves superior performance\nusing only histopathology slides compared to existing multimodal methods. The\nsource code is available at https://github.com/peterlipan/FoF.",
      "tldr_zh": "本文提出 Focus on Focus (FoF) 框架，用于胶质瘤分级，通过整合组织病理学切片和分子生物标记来解决现有多模态方法在组织病理学表示学习不足和分子-病理知识对齐效率低的问题。框架包括 Focus-oriented Representation Learning (FRL) 模块，该模块引导模型识别与胶质瘤分级相关的诊断区域，并通过一致性约束提升焦点精度；以及 Multi-view Cross-modal Alignment (MCA) 模块，通过监督对比学习将组织病理学表示投影到分子子空间，实现形态特征与分子状态的有效对齐。在 TCGA GBM-LGG 数据集上的实验显示，FoF 显著提高了分级性能，即使仅使用组织病理学切片，也优于现有多模态方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08527v1",
      "published_date": "2024-08-16 04:54:10 UTC",
      "updated_date": "2024-08-16 04:54:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:02:34.345263"
    },
    {
      "arxiv_id": "2409.00014v1",
      "title": "DivDiff: A Conditional Diffusion Model for Diverse Human Motion Prediction",
      "title_zh": "DivDiff：一种用于多样人类运动预测的条件扩散模型",
      "authors": [
        "Hua Yu",
        "Yaqing Hou",
        "Wenbin Pei",
        "Qiang Zhang"
      ],
      "abstract": "Diverse human motion prediction (HMP) aims to predict multiple plausible\nfuture motions given an observed human motion sequence. It is a challenging\ntask due to the diversity of potential human motions while ensuring an accurate\ndescription of future human motions. Current solutions are either low-diversity\nor limited in expressiveness. Recent denoising diffusion models (DDPM) hold\npotential generative capabilities in generative tasks. However, introducing\nDDPM directly into diverse HMP incurs some issues. Although DDPM can increase\nthe diversity of the potential patterns of human motions, the predicted human\nmotions become implausible over time because of the significant noise\ndisturbances in the forward process of DDPM. This phenomenon leads to the\npredicted human motions being hard to control, seriously impacting the quality\nof predicted motions and restricting their practical applicability in\nreal-world scenarios. To alleviate this, we propose a novel conditional\ndiffusion-based generative model, called DivDiff, to predict more diverse and\nrealistic human motions. Specifically, the DivDiff employs DDPM as our backbone\nand incorporates Discrete Cosine Transform (DCT) and transformer mechanisms to\nencode the observed human motion sequence as a condition to instruct the\nreverse process of DDPM. More importantly, we design a diversified\nreinforcement sampling function (DRSF) to enforce human skeletal constraints on\nthe predicted human motions. DRSF utilizes the acquired information from human\nskeletal as prior knowledge, thereby reducing significant disturbances\nintroduced during the forward process. Extensive results received in the\nexperiments on two widely-used datasets (Human3.6M and HumanEva-I) demonstrate\nthat our model obtains competitive performance on both diversity and accuracy.",
      "tldr_zh": "该论文提出DivDiff，一种基于条件扩散模型的生成框架，用于多样化人类动作预测（Diverse Human Motion Prediction, HMP），旨在解决现有方法在动作多样性和准确性上的局限性。DivDiff以Denoising Diffusion Probabilistic Models (DDPM)为核心，结合Discrete Cosine Transform (DCT)和transformer机制来编码观察到的动作序列作为条件指导逆过程，并设计了Diversified Reinforcement Sampling Function (DRSF)来强化人类骨骼约束，减少噪声干扰以生成更真实可控的动作。实验结果显示，在Human3.6M和HumanEva-I数据集上，DivDiff在多样性和准确性方面取得了竞争性性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.00014v1",
      "published_date": "2024-08-16 04:51:32 UTC",
      "updated_date": "2024-08-16 04:51:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:02:45.366245"
    },
    {
      "arxiv_id": "2408.08524v1",
      "title": "GS-ID: Illumination Decomposition on Gaussian Splatting via Diffusion Prior and Parametric Light Source Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Kang Du",
        "Zhihao Liang",
        "Zeyu Wang"
      ],
      "abstract": "We present GS-ID, a novel framework for illumination decomposition on\nGaussian Splatting, achieving photorealistic novel view synthesis and intuitive\nlight editing. Illumination decomposition is an ill-posed problem facing three\nmain challenges: 1) priors for geometry and material are often lacking; 2)\ncomplex illumination conditions involve multiple unknown light sources; and 3)\ncalculating surface shading with numerous light sources is computationally\nexpensive. To address these challenges, we first introduce intrinsic diffusion\npriors to estimate the attributes for physically based rendering. Then we\ndivide the illumination into environmental and direct components for joint\noptimization. Last, we employ deferred rendering to reduce the computational\nload. Our framework uses a learnable environment map and Spherical Gaussians\n(SGs) to represent light sources parametrically, therefore enabling\ncontrollable and photorealistic relighting on Gaussian Splatting. Extensive\nexperiments and applications demonstrate that GS-ID produces state-of-the-art\nillumination decomposition results while achieving better geometry\nreconstruction and rendering performance.",
      "tldr_zh": "该研究提出了一种名为 GS-ID 的新框架，用于在 Gaussian Splatting 上进行照明分解，实现逼真的新视图合成和光照编辑。框架通过引入 intrinsic diffusion priors 来估计物理渲染所需的几何和材质属性，并将照明分为环境和直接组件进行联合优化，同时采用延迟渲染降低计算开销。GS-ID 使用可学习的环境映射和 Spherical Gaussians (SGs) 参数化表示光源，支持可控的重新照明；实验结果显示，该框架在照明分解方面达到了最先进水平，并提升了几何重建和渲染性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.08524v1",
      "published_date": "2024-08-16 04:38:31 UTC",
      "updated_date": "2024-08-16 04:38:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:02:59.816363"
    },
    {
      "arxiv_id": "2408.08506v2",
      "title": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding",
      "title_zh": "翻译失败",
      "authors": [
        "Lei Huang",
        "Jiaming Guo",
        "Guanhua He",
        "Xishan Zhang",
        "Rui Zhang",
        "Shaohui Peng",
        "Shaoli Liu",
        "Tianshi Chen"
      ],
      "abstract": "Generating long-term texts such as novels using artificial intelligence has\nalways been a challenge. A common approach is to use large language models\n(LLMs) to construct a hierarchical framework that first plans and then writes.\nDespite the fact that the generated novels reach a sufficient length, they\nexhibit poor logical coherence and appeal in their plots and deficiencies in\ncharacter and event depiction, ultimately compromising the overall narrative\nquality. In this paper, we propose a method named Extracting Excelsior and\nExpanding. Ex3 initially extracts structure information from raw novel data. By\ncombining this structure information with the novel data, an\ninstruction-following dataset is meticulously crafted. This dataset is then\nutilized to fine-tune the LLM, aiming for excelsior generation performance. In\nthe final stage, a tree-like expansion method is deployed to facilitate the\ngeneration of arbitrarily long novels. Evaluation against previous methods\nshowcases Ex3's ability to produce higher-quality long-form novels.",
      "tldr_zh": "这篇论文提出了Ex3方法，用于解决AI生成长文本（如小说）时存在的逻辑连贯性和情节吸引力不足的问题。Ex3首先从原始小说数据中提取结构信息，并结合这些信息创建指令跟随数据集，以微调LLMs提升生成性能；随后，通过树状扩展方法（tree-like expansion）实现任意长度的长篇小说生成。与现有方法相比，实验结果显示Ex3能够产生更高质量的叙事文本。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08506v2",
      "published_date": "2024-08-16 03:06:57 UTC",
      "updated_date": "2024-09-01 08:30:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:03:11.610926"
    },
    {
      "arxiv_id": "2408.08930v1",
      "title": "DePrompt: Desensitization and Evaluation of Personal Identifiable Information in Large Language Model Prompts",
      "title_zh": "翻译失败",
      "authors": [
        "Xiongtao Sun",
        "Gan Liu",
        "Zhipeng He",
        "Hui Li",
        "Xiaoguang Li"
      ],
      "abstract": "Prompt serves as a crucial link in interacting with large language models\n(LLMs), widely impacting the accuracy and interpretability of model outputs.\nHowever, acquiring accurate and high-quality responses necessitates precise\nprompts, which inevitably pose significant risks of personal identifiable\ninformation (PII) leakage. Therefore, this paper proposes DePrompt, a\ndesensitization protection and effectiveness evaluation framework for prompt,\nenabling users to safely and transparently utilize LLMs. Specifically, by\nleveraging large model fine-tuning techniques as the underlying privacy\nprotection method, we integrate contextual attributes to define privacy types,\nachieving high-precision PII entity identification. Additionally, through the\nanalysis of key features in prompt desensitization scenarios, we devise\nadversarial generative desensitization methods that retain important semantic\ncontent while disrupting the link between identifiers and privacy attributes.\nFurthermore, we present utility evaluation metrics for prompt to better gauge\nand balance privacy and usability. Our framework is adaptable to prompts and\ncan be extended to text usability-dependent scenarios. Through comparison with\nbenchmarks and other model methods, experimental evaluations demonstrate that\nour desensitized prompt exhibit superior privacy protection utility and model\ninference results.",
      "tldr_zh": "本论文提出 DePrompt 框架，用于对大语言模型 (LLMs) 提示词中的个人信息 (PII) 进行脱敏保护和有效性评估，以防范 PII 泄露风险。框架通过大模型微调技术整合上下文属性来实现高精度 PII 实体识别，并采用对抗生成脱敏方法，保留提示词的关键语义同时破坏标识符与隐私属性的关联；此外，还引入了实用性评估指标来平衡隐私保护与可用性。实验结果表明，DePrompt 与基准方法相比，在隐私保护和模型推理性能上表现出显著优势，并可扩展应用于各种文本依赖场景。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08930v1",
      "published_date": "2024-08-16 02:38:25 UTC",
      "updated_date": "2024-08-16 02:38:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:03:23.998561"
    },
    {
      "arxiv_id": "2408.08488v2",
      "title": "PITN: Physics-Informed Temporal Networks for Cuffless Blood Pressure Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Wang",
        "Mengshi Qi",
        "Yingxia Shao",
        "Anfu Zhou",
        "Huadong Ma"
      ],
      "abstract": "Monitoring blood pressure with non-invasive sensors has gained popularity for\nproviding comfortable user experiences, one of which is a significant function\nof smart wearables. Although providing a comfortable user experience, such\nmethods are suffering from the demand for a significant amount of realistic\ndata to train an individual model for each subject, especially considering the\ninvasive or obtrusive BP ground-truth measurements. To tackle this challenge,\nwe introduce a novel physics-informed temporal network~(PITN) with adversarial\ncontrastive learning to enable precise BP estimation with very limited data.\nSpecifically, we first enhance the physics-informed neural network~(PINN) with\nthe temporal block for investigating BP dynamics' multi-periodicity for\npersonal cardiovascular cycle modeling and temporal variation. We then employ\nadversarial training to generate extra physiological time series data,\nimproving PITN's robustness in the face of sparse subject-specific training\ndata. Furthermore, we utilize contrastive learning to capture the\ndiscriminative variations of cardiovascular physiologic phenomena. This\napproach aggregates physiological signals with similar blood pressure values in\nlatent space while separating clusters of samples with dissimilar blood\npressure values. Experiments on three widely-adopted datasets with different\nmodailties (\\emph{i.e.,} bioimpedance, PPG, millimeter-wave) demonstrate the\nsuperiority and effectiveness of the proposed methods over previous\nstate-of-the-art approaches. The code is available\nat~\\url{https://github.com/Zest86/ACL-PITN}.",
      "tldr_zh": "本论文提出了一种名为 PITN（Physics-Informed Temporal Networks）的框架，用于实现无袖带血压估计，旨在解决传统方法依赖大量个性化训练数据的问题。PITN 通过增强 PINN（Physics-Informed Neural Network）并加入 Temporal Block 来捕捉血压动态的多周期性和时变性，同时利用 Adversarial Training 生成额外生理时间序列数据以提升模型在稀疏数据下的鲁棒性，并采用 Contrastive Learning 在潜在空间中聚类相似血压信号并分离不同血压样本。实验结果显示，在 bioimpedance、PPG 和 millimeter-wave 等三个数据集上，PITN 比现有最先进方法表现出色，精确度显著提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.08488v2",
      "published_date": "2024-08-16 02:17:21 UTC",
      "updated_date": "2024-12-03 11:06:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:03:48.271495"
    },
    {
      "arxiv_id": "2408.08484v1",
      "title": "An Unsupervised Learning Framework Combined with Heuristics for the Maximum Minimal Cut Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Huaiyuan Liu",
        "Xianzhang Liu",
        "Donghua Yang",
        "Hongzhi Wang",
        "Yingchi Long",
        "Mengtong Ji",
        "Dongjing Miao",
        "Zhiyu Liang"
      ],
      "abstract": "The Maximum Minimal Cut Problem (MMCP), a NP-hard combinatorial optimization\n(CO) problem, has not received much attention due to the demanding and\nchallenging bi-connectivity constraint. Moreover, as a CO problem, it is also a\ndaunting task for machine learning, especially without labeled instances. To\ndeal with these problems, this work proposes an unsupervised learning framework\ncombined with heuristics for MMCP that can provide valid and high-quality\nsolutions. As far as we know, this is the first work that explores machine\nlearning and heuristics to solve MMCP. The unsupervised solver is inspired by a\nrelaxation-plus-rounding approach, the relaxed solution is parameterized by\ngraph neural networks, and the cost and penalty of MMCP are explicitly written\nout, which can train the model end-to-end. A crucial observation is that each\nsolution corresponds to at least one spanning tree. Based on this finding, a\nheuristic solver that implements tree transformations by adding vertices is\nutilized to repair and improve the solution quality of the unsupervised solver.\nAlternatively, the graph is simplified while guaranteeing solution consistency,\nwhich reduces the running time. We conduct extensive experiments to evaluate\nour framework and give a specific application. The results demonstrate the\nsuperiority of our method against two techniques designed.",
      "tldr_zh": "这篇论文针对 NP-hard 组合优化问题 Maximum Minimal Cut Problem (MMCP) 提出一个无监督学习框架，结合启发式方法，以处理其 bi-connectivity 约束和缺乏标签数据的挑战。该框架基于 relaxation-plus-rounding 方法，使用 Graph Neural Networks 参数化松弛解，并显式定义 MMCP 的成本和惩罚，实现端到端训练；同时，通过添加顶点的树变换启发式求解器修复和提升解决方案质量，并简化图结构以减少运行时间。实验结果显示，该方法在广泛测试中优于两种基线技术，并在具体应用中提供有效的高质量解决方案。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08484v1",
      "published_date": "2024-08-16 02:07:34 UTC",
      "updated_date": "2024-08-16 02:07:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:03:52.182805"
    },
    {
      "arxiv_id": "2408.08471v2",
      "title": "Fairness Issues and Mitigations in (Differentially Private) Socio-Demographic Data Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Joonhyuk Ko",
        "Juba Ziani",
        "Saswat Das",
        "Matt Williams",
        "Ferdinando Fioretto"
      ],
      "abstract": "Statistical agencies rely on sampling techniques to collect socio-demographic\ndata crucial for policy-making and resource allocation. This paper shows that\nsurveys of important societal relevance introduce sampling errors that unevenly\nimpact group-level estimates, thereby compromising fairness in downstream\ndecisions. To address these issues, this paper introduces an optimization\napproach modeled on real-world survey design processes, ensuring sampling costs\nare optimized while maintaining error margins within prescribed tolerances.\nAdditionally, privacy-preserving methods used to determine sampling rates can\nfurther impact these fairness issues. This paper explores the impact of\ndifferential privacy on the statistics informing the sampling process,\nrevealing a surprising effect: not only is the expected negative effect from\nthe addition of noise for differential privacy negligible, but also this\nprivacy noise can in fact reduce unfairness as it positively biases smaller\ncounts. These findings are validated over an extensive analysis using datasets\ncommonly applied in census statistics.",
      "tldr_zh": "本论文探讨了在社会人口数据处理中存在的公平性问题，特别是抽样技术引入的错误导致群体级估计不均等，从而影响下游决策。该研究提出了一种基于真实调查设计过程的优化方法，以最小化抽样成本同时保持错误幅度在规定范围内。此外，论文分析了差分隐私（Differentially Private）机制对抽样率的影响，发现添加噪声不仅负面影响微小，还能通过正向偏差减少对较小计数的偏见。这些发现通过对常见人口统计数据集的广泛分析得到验证。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.08471v2",
      "published_date": "2024-08-16 01:13:36 UTC",
      "updated_date": "2025-01-19 20:59:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:04:01.023919"
    },
    {
      "arxiv_id": "2408.08470v4",
      "title": "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jerry Huang",
        "Prasanna Parthasarathi",
        "Mehdi Rezagholizadeh",
        "Sarath Chandar"
      ],
      "abstract": "Despite their widespread adoption, large language models (LLMs) remain\nprohibitive to use under resource constraints, with their ever growing sizes\nonly increasing the barrier for use. One noted issue is the high latency\nassociated with auto-regressive generation, rendering large LLMs use dependent\non advanced computing infrastructure. Assisted decoding, where a smaller draft\nmodel guides a larger target model's generation, has helped alleviate this, but\nremains dependent on alignment between the two models. Thus if the draft model\nis insufficiently capable on some domain relative to the target model,\nperformance can degrade. Alternatively, one can leverage multiple draft models\nto better cover the expertise of the target, but when multiple black-box draft\nmodels are available, selecting an assistant without details about its\nconstruction can be difficult. To better understand this decision making\nproblem, we observe it as a contextual bandit, where a policy must choose a\ndraft model based on a context. We show that even without prior knowledge of\nthe draft models, creating an offline dataset from only outputs of independent\ndraft/target models and training a policy over the alignment of these outputs\ncan accelerate performance on multiple domains provided the candidates are\neffective. Further results show this to hold on various settings with multiple\nassisted decoding candidates, highlighting its flexibility and the advantageous\nrole that such decision making can play.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）的推理延迟问题，提出了一种上下文感知的助理选择方法，以优化辅助解码（assisted decoding）。该方法将模型选择视为上下文 bandit 问题，通过创建离线数据集来评估多个草稿模型（draft model）的输出对齐，并训练一个策略（policy）来动态选择最佳助理，即使缺乏模型构建细节。实验结果显示，这种方法在多个领域和设置下显著提升了推理性能，突显了其灵活性和决策优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a long paper at the 2024 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP). Official version of paper within\n  conference proceedings is available at\n  http://aclanthology.org/2024.emnlp-main.332",
      "pdf_url": "http://arxiv.org/pdf/2408.08470v4",
      "published_date": "2024-08-16 01:12:21 UTC",
      "updated_date": "2024-12-15 22:27:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:04:12.721661"
    },
    {
      "arxiv_id": "2408.16011v1",
      "title": "A Tutorial on Brownian Motion for Biostatisticians",
      "title_zh": "针对生物统计学家的布朗运动教程",
      "authors": [
        "Elvis Han Cui"
      ],
      "abstract": "This manuscript provides an in-depth exploration of Brownian Motion, a\nfundamental stochastic process in probability theory for Biostatisticians. It\nbegins with foundational definitions and properties, including the construction\nof Brownian motion and its Markovian characteristics. The document delves into\nadvanced topics such as the Karhunen-Loeve expansion, reflection principles,\nand Levy's modulus of continuity. Through rigorous proofs and theorems, the\nmanuscript examines the non-differentiability of Brownian paths, the behavior\nof zero sets, and the significance of local time. The notes also cover\nimportant results like Donsker's theorem and Blumenthal's 0-1 law, emphasizing\ntheir implications in the study of stochastic processes.",
      "tldr_zh": "这篇论文针对生物统计学家，提供了一个深入的布朗运动（Brownian Motion）教程，涵盖基础定义、属性（如构造和Markovian特性），以及高级主题如Karhunen-Loeve expansion、reflection principles和Levy's modulus of continuity。作者通过严格的证明和定理，探讨了布朗运动路径的非微分性、零集行为和局部时间等特性。论文还强调了Donsker's theorem和Blumenthal's 0-1 law等关键结果及其在随机过程研究中的重要意义。",
      "categories": [
        "stat.AP",
        "cs.AI",
        "math.PR",
        "math.ST",
        "stat.TH"
      ],
      "primary_category": "stat.AP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.16011v1",
      "published_date": "2024-08-16 00:58:21 UTC",
      "updated_date": "2024-08-16 00:58:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:04:24.979727"
    },
    {
      "arxiv_id": "2408.08463v1",
      "title": "A theory of understanding for artificial intelligence: composability, catalysts, and learning",
      "title_zh": "人工智能的理解理论：可组合性、催化剂和学习",
      "authors": [
        "Zijian Zhang",
        "Sara Aronowitz",
        "Alán Aspuru-Guzik"
      ],
      "abstract": "Understanding is a crucial yet elusive concept in artificial intelligence\n(AI). This work proposes a framework for analyzing understanding based on the\nnotion of composability. Given any subject (e.g., a person or an AI), we\nsuggest characterizing its understanding of an object in terms of its ability\nto process (compose) relevant inputs into satisfactory outputs from the\nperspective of a verifier. This highly universal framework can readily apply to\nnon-human subjects, such as AIs, non-human animals, and institutions. Further,\nwe propose methods for analyzing the inputs that enhance output quality in\ncompositions, which we call catalysts. We show how the structure of a subject\ncan be revealed by analyzing its components that act as catalysts and argue\nthat a subject's learning ability can be regarded as its ability to compose\ninputs into its inner catalysts. Finally we examine the importance of learning\nability for AIs to attain general intelligence. Our analysis indicates that\nmodels capable of generating outputs that can function as their own catalysts,\nsuch as language models, establish a foundation for potentially overcoming\nexisting limitations in AI understanding.",
      "tldr_zh": "这篇论文提出一个基于 composability 的框架，用于分析人工智能(AI)的理解能力，将理解定义为主体（如AI）处理相关输入以产生满意输出的能力，并适用于非人类主体。论文引入 catalysts 作为提升输出质量的关键输入，并通过分析这些 catalysts 揭示主体的内部结构。最终，论文强调AI的学习能力——即将输入组成成内部 catalysts 的过程——是实现通用智能的基础，特别是语言模型等能生成自身 catalysts 的模型，有望克服AI理解的现有限制。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.08463v1",
      "published_date": "2024-08-16 00:17:18 UTC",
      "updated_date": "2024-08-16 00:17:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:04:37.584813"
    },
    {
      "arxiv_id": "2408.08928v1",
      "title": "Imprecise Belief Fusion Facing a DST benchmark problem",
      "title_zh": "翻译失败",
      "authors": [
        "Francisco Aragão",
        "João Alcântara"
      ],
      "abstract": "When we merge information in Dempster-Shafer Theory (DST), we are faced with\nanomalous behavior: agents with equal expertise and credibility can have their\nopinion disregarded after resorting to the belief combination rule of this\ntheory. This problem is interesting because belief fusion is an inherent part\nof dealing with situations where available information is imprecise, as often\noccurs in Artificial Intelligence. We managed to identify an isomorphism betwin\nthe DST formal apparatus into that of a Probabilistic Logic. Thus, we solved\nthe problematic inputs affair by replacing the DST combination rule with a new\nfusion process aiming at eliminating anomalies proposed by that rule. We apply\nthe new fusion method to the DST paradox Problem.",
      "tldr_zh": "该论文探讨了 Dempster-Shafer Theory (DST) 中信念融合的异常问题，即即使专家具有相同专业性和可信度，其意见仍可能被忽略，这在处理人工智能领域的不精确信息时尤为突出。研究者通过识别 DST 与 Probabilistic Logic 之间的等价关系（isomorphism），提出了一种新的融合方法来替换原有组合规则，从而消除这些异常行为。该方法被应用于 DST 悖论问题，展示了在信念融合中更可靠和有效的处理不精确信息的潜力。",
      "categories": [
        "cs.AI",
        "00A05",
        "G.0"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2408.08928v1",
      "published_date": "2024-08-16 00:03:32 UTC",
      "updated_date": "2024-08-16 00:03:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T16:04:49.432656"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 87,
  "processed_papers_count": 87,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T16:05:18.472467"
}
{
  "date": "2024-11-10",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-11-10 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文更新主要聚焦于 AI 模型的优化、安全和应用，以及强化学习在实际场景中的创新，强调了大语言模型 (LLM) 的可靠性和扩展性。令人印象深刻的包括 vTune 的 LLM 后门验证方法和 CriticAL 的科学模型批评框架，这些工作展示了知名学者（如 Noah D. Goodman 和 Jean-François Godbout 等）在 AI 领域的深度贡献，同时涉及量子物理和图像生成的潜在突破。\n\n### AI 模型优化与安全（重点领域）\n- **vTune: Verifiable Fine-Tuning for LLMs Through Backdooring（vTune: 通过后门验证 LLM 的可验证微调）**  \n  这篇论文提出 vTune 框架，使用后门数据点进行统计测试，以验证第三方 LLM 微调服务的正确性。主要贡献是实现对开源和闭源模型的扩展验证，实验显示在多种数据集上 p 值低至 10^{-40}，无损下游任务性能，同时证明了其对潜在攻击的鲁棒性。\n  \n- **CriticAL: Critic Automation with Language Models（CriticAL: 使用语言模型的批评自动化）**  \n  作者包括 Noah D. Goodman 和 Emily B. Fox 等知名学者，论文引入 CriticAL 方法，利用 LLM 生成摘要统计并应用假设测试评估模型偏差。主要发现是它能可靠地识别真实偏差而不产生幻觉，并在实验中优于基线，提升了科学模型的透明度和可操作性。\n\n- **Gen-AI for User Safety: A Survey（Gen-AI 用于用户安全: 一个调查）**  \n  这篇综述探讨了生成式 AI 在检测网络钓鱼、恶意软件和内容审查中的应用。主要贡献是总结了 Gen-AI 与多种数据模态（如文本、图像）的结合，提升了用户安全检测的准确性，并讨论了在对抗环境下的鲁棒性。\n\n- **A Review of Fairness and A Practical Guide to Selecting Context-Appropriate Fairness Metrics in Machine Learning（公平性综述及选择上下文合适公平度指标的实用指南）**  \n  论文提供了一个流程图，帮助选择合适的公平度指标，考虑模型评估、数据偏差等因素。主要发现是它能指导 AI 开发者遵守监管要求，提升模型在哲学和文化上下文中的公平性。\n\n- **Prompt-Efficient Fine-Tuning for GPT-like Deep Models to Reduce Hallucination and to Improve Reproducibility in Scientific Text Generation Using Stochastic Optimisation Techniques（使用随机优化技术提升 GPT 类似模型的提示高效微调，减少幻觉并提高科学文本生成的可复现性）**  \n  这篇工作提出 PEFT 方法，通过 LoRA 适配器微调 LLM，减少幻觉并提升文本一致性。主要贡献是实验证明在质谱领域，微调后模型在 BLEU 和 Perplexity 上显著改善。\n\n- **Epistemic Integrity in Large Language Models（大语言模型的认识论完整性）**  \n  论文分析 LLM 的语言自信度与实际准确性的失调，引入新方法测量自信度。主要发现是它将错误率降低 50%，并通过人类评估确认了 LLM 在高风险领域的潜在风险。\n\n### 强化学习与应用\n- **OffLight: An Offline Multi-Agent Reinforcement Learning Framework for Traffic Signal Control（OffLight: 用于交通信号控制的离线多代理强化学习框架）**  \n  这篇论文提出 OffLight 框架，使用 Importance Sampling 和 Gaussian Mixture Variational Graph Autoencoder 处理异质数据。主要贡献是实验显示它减少平均旅行时间 7.8% 和队列长度 11.2%，适用于真实城市交通场景。\n\n- **Meta-Learning Objectives for Preference Optimization（元学习目标用于偏好优化）**  \n  作者包括 Yee Whye Teh，论文设计 Mirror Preference Optimization (MPO) 算法，通过进化策略优化偏好数据集。主要发现是它在 MuJoCo 基准上超越基线，并在 LLM 任务中显著提升性能。\n\n- **Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing Contact-Rich Plans?（平滑动态下的线性反馈是否足以稳定接触丰富计划？）**  \n  这篇 ICRA 2025 论文分析线性控制在接触丰富操作中的局限性，主要发现是 LQR 方法不足以稳定复杂轨迹，实验验证了在机器人双臂操作中的问题。\n\n### 其他亮点论文\n- **Foundation Model for Composite Microstructures: Reconstruction, Stiffness, and Nonlinear Behavior Prediction（复合微结构的基础模型: 重构、刚度和非线性行为预测）**  \n  论文构建了一个预训练基础模型，用于复合材料的刚度和非线性预测。主要贡献是通过转移学习，即使数据稀缺，也能准确预测材料行为。\n\n- **Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents（你的 LLM 是否暗中是互联网的世界模型？用于网络代理的基于模型的规划）**  \n  这篇 NeurIPS 相关工作提出 WebDreamer 框架，使用 LLM 作为世界模型模拟网络交互。主要发现是它在真实网站任务中效率提升 4-5 倍，性能与 GPT-4o 相当。\n\n其他论文如图像生成（e.g., I2VControl-Camera）和量子物理（e.g., Discovering emergent connections in quantum physics research via dynamic word embeddings）虽有创新，但相对次要，这里仅简要提及：这些工作探索了视频相机控制和量子概念关系的动态嵌入，贡献包括改进高频细节重建和无监督学习方法。\n\n总之，今天的更新突显了 AI 领域的安全性和实用性进步，建议读者关注 LLM 相关论文，以探索其在科学和实际应用中的潜力。保持关注 arXiv，以捕捉更多前沿动态！",
  "papers": [
    {
      "arxiv_id": "2411.06626v1",
      "title": "Exploring social bots: A feature-based approach to improve bot detection in social networks",
      "title_zh": "翻译失败",
      "authors": [
        "Salvador Lopez-Joya",
        "Jose A. Diaz-Garcia",
        "M. Dolores Ruiz",
        "Maria J. Martin-Bautista"
      ],
      "abstract": "The importance of social media in our daily lives has unfortunately led to an\nincrease in the spread of misinformation, political messages and malicious\nlinks. One of the most popular ways of carrying out those activities is using\nautomated accounts, also known as bots, which makes the detection of such\naccounts a necessity. This paper addresses that problem by investigating\nfeatures based on the user account profile and its content, aiming to\nunderstand the relevance of each feature as a basis for improving future bot\ndetectors. Through an exhaustive process of research, inference and feature\nselection, we are able to surpass the state of the art on several metrics using\nclassical machine learning algorithms and identify the types of features that\nare most important in detecting automated accounts.",
      "tldr_zh": "本研究探讨了社交媒体中的 social bots，旨在通过基于用户账户配置文件和内容的特征分析来提升 bot detection 的准确性。作者通过全面的研究、推理和 feature selection 过程，使用经典的 machine learning algorithms（如决策树或支持向量机），在多个性能指标上超越了现有技术。实验结果突显了某些特征类型（如账户行为和内容模式）在检测自动化账户中的关键作用，为未来 bot 检测器的发展提供了重要指导。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06626v1",
      "published_date": "2024-11-10 23:19:08 UTC",
      "updated_date": "2024-11-10 23:19:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:05:56.423960"
    },
    {
      "arxiv_id": "2411.06624v3",
      "title": "A Review of Fairness and A Practical Guide to Selecting Context-Appropriate Fairness Metrics in Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Caleb J. S. Barr",
        "Olivia Erdelyi",
        "Paul D. Docherty",
        "Randolph C. Grace"
      ],
      "abstract": "Recent regulatory proposals for artificial intelligence emphasize fairness\nrequirements for machine learning models. However, precisely defining the\nappropriate measure of fairness is challenging due to philosophical, cultural\nand political contexts. Biases can infiltrate machine learning models in\ncomplex ways depending on the model's context, rendering a single common metric\nof fairness insufficient. This ambiguity highlights the need for criteria to\nguide the selection of context-aware measures, an issue of increasing\nimportance given the proliferation of ever tighter regulatory requirements. To\naddress this, we developed a flowchart to guide the selection of contextually\nappropriate fairness measures. Twelve criteria were used to formulate the\nflowchart. This included consideration of model assessment criteria, model\nselection criteria, and data bias. We also review fairness literature in the\ncontext of machine learning and link it to core regulatory instruments to\nassist policymakers, AI developers, researchers, and other stakeholders in\nappropriately addressing fairness concerns and complying with relevant\nregulatory requirements.",
      "tldr_zh": "这篇论文审视了机器学习中公平性(fairness)的挑战，强调了由于哲学、文化和政治因素，选择合适的公平度量标准(context-appropriate fairness metrics)的复杂性。作者开发了一个基于12个标准的流程图(flowchart)，包括模型评估标准(model assessment criteria)、模型选择标准(model selection criteria)和数据偏差(data bias)，以指导用户在不同上下文中选择适当的公平度量。论文还回顾了相关公平性文献，并将其与核心监管工具(core regulatory instruments)联系起来，帮助政策制定者、AI开发者和其他利益相关者有效应对公平性问题并遵守监管要求。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2411.06624v3",
      "published_date": "2024-11-10 23:13:28 UTC",
      "updated_date": "2025-02-11 00:44:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:06:08.585115"
    },
    {
      "arxiv_id": "2411.06616v1",
      "title": "MEANT: Multimodal Encoder for Antecedent Information",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Iyoya Irving",
        "Annika Marie Schoene"
      ],
      "abstract": "The stock market provides a rich well of information that can be split across\nmodalities, making it an ideal candidate for multimodal evaluation. Multimodal\ndata plays an increasingly important role in the development of machine\nlearning and has shown to positively impact performance. But information can do\nmore than exist across modes -- it can exist across time. How should we attend\nto temporal data that consists of multiple information types? This work\nintroduces (i) the MEANT model, a Multimodal Encoder for Antecedent information\nand (ii) a new dataset called TempStock, which consists of price, Tweets, and\ngraphical data with over a million Tweets from all of the companies in the S&P\n500 Index. We find that MEANT improves performance on existing baselines by\nover 15%, and that the textual information affects performance far more than\nthe visual information on our time-dependent task from our ablation study.",
      "tldr_zh": "这篇论文介绍了MEANT模型，这是一个Multimodal Encoder for Antecedent information，旨在处理股票市场中跨模态和时序数据的挑战，例如价格、Tweets和图形信息。研究者构建了新数据集TempStock，包括S&P 500指数公司的数据，涵盖超过一百万条Tweets，以评估模型在时间依赖任务中的性能。实验结果显示，MEANT比现有基线提升超过15%，且消融研究发现文本信息对性能的影响远大于视觉信息。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06616v1",
      "published_date": "2024-11-10 22:42:36 UTC",
      "updated_date": "2024-11-10 22:42:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:06:21.205796"
    },
    {
      "arxiv_id": "2411.07794v1",
      "title": "Feature Fusion Transferability Aware Transformer for Unsupervised Domain Adaptation",
      "title_zh": "特征融合转移性感知Transformer用于无监督域适应",
      "authors": [
        "Xiaowei Yu",
        "Zhe Huang",
        "Zao Zhang"
      ],
      "abstract": "Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned\nfrom labeled source domains to improve performance on the unlabeled target\ndomains. While Convolutional Neural Networks (CNNs) have been dominant in\nprevious UDA methods, recent research has shown promise in applying Vision\nTransformers (ViTs) to this task. In this study, we propose a novel Feature\nFusion Transferability Aware Transformer (FFTAT) to enhance ViT performance in\nUDA tasks. Our method introduces two key innovations: First, we introduce a\npatch discriminator to evaluate the transferability of patches, generating a\ntransferability matrix. We integrate this matrix into self-attention, directing\nthe model to focus on transferable patches. Second, we propose a feature fusion\ntechnique to fuse embeddings in the latent space, enabling each embedding to\nincorporate information from all others, thereby improving generalization.\nThese two components work in synergy to enhance feature representation\nlearning. Extensive experiments on widely used benchmarks demonstrate that our\nmethod significantly improves UDA performance, achieving state-of-the-art\n(SOTA) results.",
      "tldr_zh": "该研究针对无监督域适应 (UDA) 问题，提出了一种新型 Feature Fusion Transferability Aware Transformer (FFTAT) 方法，以提升 Vision Transformers (ViTs) 在源域到目标域知识转移中的性能。FFTAT 的关键创新包括：引入 patch discriminator 生成 transferability matrix，并将其整合到 self-attention 机制中，引导模型优先关注可转移的 patches；以及提出 feature fusion 技术，在潜在空间融合 embeddings，使每个 embedding 吸收其他信息，从而增强特征表示学习。实验结果显示，该方法在常用 UDA 基准上显著提升性能，达到了 state-of-the-art (SOTA) 水平。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2411.07794v1",
      "published_date": "2024-11-10 22:23:12 UTC",
      "updated_date": "2024-11-10 22:23:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:06:33.195214"
    },
    {
      "arxiv_id": "2411.06611v2",
      "title": "vTune: Verifiable Fine-Tuning for LLMs Through Backdooring",
      "title_zh": "翻译失败",
      "authors": [
        "Eva Zhang",
        "Arka Pal",
        "Akilesh Potti",
        "Micah Goldblum"
      ],
      "abstract": "As fine-tuning large language models (LLMs) becomes increasingly prevalent,\nusers often rely on third-party services with limited visibility into their\nfine-tuning processes. This lack of transparency raises the question: how do\nconsumers verify that fine-tuning services are performed correctly? For\ninstance, a service provider could claim to fine-tune a model for each user,\nyet simply send all users back the same base model. To address this issue, we\npropose vTune, a simple method that uses a small number of backdoor data points\nadded to the training data to provide a statistical test for verifying that a\nprovider fine-tuned a custom model on a particular user's dataset. Unlike\nexisting works, vTune is able to scale to verification of fine-tuning on\nstate-of-the-art LLMs, and can be used both with open-source and closed-source\nmodels. We test our approach across several model families and sizes as well as\nacross multiple instruction-tuning datasets, and find that the statistical test\nis satisfied with p-values on the order of $\\sim 10^{-40}$, with no negative\nimpact on downstream task performance. Further, we explore several attacks that\nattempt to subvert vTune and demonstrate the method's robustness to these\nattacks.",
      "tldr_zh": "该论文提出vTune，一种通过添加少量后门数据点到训练数据中的简单方法，用于验证大型语言模型(LLMs)的微调(fine-tuning)是否真实进行，从而解决用户对第三方服务透明度不足的问题。vTune利用统计测试来检测提供者是否在特定用户数据集上自定义微调模型，该方法适用于开源和闭源模型，并能扩展到最先进的LLMs。实验结果显示，在多个模型家族和指令微调数据集上，vTune的统计测试p值达到约10^{-40}级，且不影响下游任务性能，同时对试图破坏的攻击表现出鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06611v2",
      "published_date": "2024-11-10 22:08:37 UTC",
      "updated_date": "2024-11-12 03:04:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:06:44.685785"
    },
    {
      "arxiv_id": "2411.06606v2",
      "title": "Gen-AI for User Safety: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Akshar Prabhu Desai",
        "Tejasvi Ravi",
        "Mohammad Luqman",
        "Mohit Sharma",
        "Nithya Kota",
        "Pranjul Yadav"
      ],
      "abstract": "Machine Learning and data mining techniques (i.e. supervised and unsupervised\ntechniques) are used across domains to detect user safety violations. Examples\ninclude classifiers used to detect whether an email is spam or a web-page is\nrequesting bank login information. However, existing ML/DM classifiers are\nlimited in their ability to understand natural languages w.r.t the context and\nnuances. The aforementioned challenges are overcome with the arrival of Gen-AI\ntechniques, along with their inherent ability w.r.t translation between\nlanguages, fine-tuning between various tasks and domains.\n  In this manuscript, we provide a comprehensive overview of the various work\ndone while using Gen-AI techniques w.r.t user safety. In particular, we first\nprovide the various domains (e.g. phishing, malware, content moderation,\ncounterfeit, physical safety) across which Gen-AI techniques have been applied.\nNext, we provide how Gen-AI techniques can be used in conjunction with various\ndata modalities i.e. text, images, videos, audio, executable binaries to detect\nviolations of user-safety. Further, also provide an overview of how Gen-AI\ntechniques can be used in an adversarial setting. We believe that this work\nrepresents the first summarization of Gen-AI techniques for user-safety.",
      "tldr_zh": "这篇论文对生成式人工智能（Gen-AI）在用户安全领域的应用进行了全面调查，强调了Gen-AI相对于传统机器学习和数据挖掘（ML/DM）技术的优势，如更好地理解自然语言的上下文和细微差别。论文涵盖了Gen-AI在多个领域（如phishing、malware、content moderation、counterfeit和physical safety）的应用，以及其与各种数据模态（如text、images、videos、audio和executable binaries）的结合方式，并在对抗设置中进行概述。该工作声称是第一个对Gen-AI用于用户安全违规检测的系统总结，为未来研究提供了宝贵参考。",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06606v2",
      "published_date": "2024-11-10 21:49:10 UTC",
      "updated_date": "2024-11-22 20:34:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:06:57.480874"
    },
    {
      "arxiv_id": "2411.06601v3",
      "title": "OffLight: An Offline Multi-Agent Reinforcement Learning Framework for Traffic Signal Control",
      "title_zh": "OffLight：一种离线多智能体强化学习框架，用于交通信号控制",
      "authors": [
        "Rohit Bokade",
        "Xiaoning Jin"
      ],
      "abstract": "Efficient traffic control (TSC) is essential for urban mobility, but\ntraditional systems struggle to handle the complexity of real-world traffic.\nMulti-agent Reinforcement Learning (MARL) offers adaptive solutions, but online\nMARL requires extensive interactions with the environment, making it costly and\nimpractical. Offline MARL mitigates these challenges by using historical\ntraffic data for training but faces significant difficulties with heterogeneous\nbehavior policies in real-world datasets, where mixed-quality data complicates\nlearning. We introduce OffLight, a novel offline MARL framework designed to\nhandle heterogeneous behavior policies in TSC datasets. To improve learning\nefficiency, OffLight incorporates Importance Sampling (IS) to correct for\ndistributional shifts and Return-Based Prioritized Sampling (RBPS) to focus on\nhigh-quality experiences. OffLight utilizes a Gaussian Mixture Variational\nGraph Autoencoder (GMM-VGAE) to capture the diverse distribution of behavior\npolicies from local observations. Extensive experiments across real-world urban\ntraffic scenarios show that OffLight outperforms existing offline RL methods,\nachieving up to a 7.8% reduction in average travel time and 11.2% decrease in\nqueue length. Ablation studies confirm the effectiveness of OffLight's\ncomponents in handling heterogeneous data and improving policy performance.\nThese results highlight OffLight's scalability and potential to improve urban\ntraffic management without the risks of online learning.",
      "tldr_zh": "本文提出OffLight，一种离线Multi-Agent Reinforcement Learning (MARL)框架，用于交通信号控制（TSC），旨在解决异质行为策略问题，通过利用历史数据避免在线学习的成本和风险。OffLight 整合了Importance Sampling (IS)来修正分布偏移、Return-Based Prioritized Sampling (RBPS)来优先高质量经验，以及Gaussian Mixture Variational Graph Autoencoder (GMM-VGAE)来捕捉行为策略的多样分布。实验在真实城市交通场景中显示，OffLight 比现有离线RL方法平均减少7.8%的旅行时间和11.2%的队列长度，消融研究证实其组件的有效性，为可扩展的城市交通管理提供了可靠解决方案。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06601v3",
      "published_date": "2024-11-10 21:26:17 UTC",
      "updated_date": "2025-03-18 01:22:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:07:09.802778"
    },
    {
      "arxiv_id": "2411.06590v1",
      "title": "CriticAL: Critic Automation with Language Models",
      "title_zh": "CriticAL：基于语言模型的批评自动化",
      "authors": [
        "Michael Y. Li",
        "Vivek Vajipey",
        "Noah D. Goodman",
        "Emily B. Fox"
      ],
      "abstract": "Understanding the world through models is a fundamental goal of scientific\nresearch. While large language model (LLM) based approaches show promise in\nautomating scientific discovery, they often overlook the importance of\ncriticizing scientific models. Criticizing models deepens scientific\nunderstanding and drives the development of more accurate models. Automating\nmodel criticism is difficult because it traditionally requires a human expert\nto define how to compare a model with data and evaluate if the discrepancies\nare significant--both rely heavily on understanding the modeling assumptions\nand domain. Although LLM-based critic approaches are appealing, they introduce\nnew challenges: LLMs might hallucinate the critiques themselves. Motivated by\nthis, we introduce CriticAL (Critic Automation with Language Models). CriticAL\nuses LLMs to generate summary statistics that capture discrepancies between\nmodel predictions and data, and applies hypothesis tests to evaluate their\nsignificance. We can view CriticAL as a verifier that validates models and\ntheir critiques by embedding them in a hypothesis testing framework. In\nexperiments, we evaluate CriticAL across key quantitative and qualitative\ndimensions. In settings where we synthesize discrepancies between models and\ndatasets, CriticAL reliably generates correct critiques without hallucinating\nincorrect ones. We show that both human and LLM judges consistently prefer\nCriticAL's critiques over alternative approaches in terms of transparency and\nactionability. Finally, we show that CriticAL's critiques enable an LLM\nscientist to improve upon human-designed models on real-world datasets.",
      "tldr_zh": "这篇论文引入 CriticAL，一种基于语言模型（LLM）的模型批评自动化框架，旨在解决 LLM 在科学发现中忽略模型批评的问题，并避免 LLM 可能产生的幻觉（hallucinate）错误批评。CriticAL 通过使用 LLM 生成总结统计来捕捉模型预测与数据之间的差异，并应用 hypothesis tests 评估这些差异的显著性，将批评嵌入假设测试框架中进行验证。实验结果表明，CriticAL 能可靠地生成正确批评，在透明度和可操作性上优于其他方法，并帮助 LLM 科学家改进人类设计的真实世界模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06590v1",
      "published_date": "2024-11-10 20:41:35 UTC",
      "updated_date": "2024-11-10 20:41:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:07:21.499773"
    },
    {
      "arxiv_id": "2411.06583v1",
      "title": "Enhancing frozen histological section images using permanent-section-guided deep learning with nuclei attention",
      "title_zh": "翻译失败",
      "authors": [
        "Elad Yoshai",
        "Gil Goldinger",
        "Miki Haifler",
        "Natan T. Shaked"
      ],
      "abstract": "In histological pathology, frozen sections are often used for rapid diagnosis\nduring surgeries, as they can be produced within minutes. However, they suffer\nfrom artifacts and often lack crucial diagnostic details, particularly within\nthe cell nuclei region. Permanent sections, on the other hand, contain more\ndiagnostic detail but require a time-intensive preparation process. Here, we\npresent a generative deep learning approach to enhance frozen section images by\nleveraging guidance from permanent sections. Our method places a strong\nemphasis on the nuclei region, which contains critical information in both\nfrozen and permanent sections. Importantly, our approach avoids generating\nartificial data in blank regions, ensuring that the network only enhances\nexisting features without introducing potentially unreliable information. We\nachieve this through a segmented attention network, incorporating\nnuclei-segmented images during training and adding an additional loss function\nto refine the nuclei details in the generated permanent images. We validated\nour method across various tissues, including kidney, breast, and colon. This\napproach significantly improves histological efficiency and diagnostic\naccuracy, enhancing frozen section images within seconds, and seamlessly\nintegrating into existing laboratory workflows.",
      "tldr_zh": "本文提出了一种使用永久切片(permanent sections)指导的深度学习方法，以增强冻结切片(frozen sections)图像，并通过nuclei attention机制重点优化细胞核区域。该方法采用segmented attention network和额外损失函数，确保仅增强现有特征而不生成空白区域的假数据。在肾脏、乳房和结肠等组织上验证后，该方法显著提高了组织学效率和诊断准确性，能在几秒内完成图像增强，并无缝融入现有实验室工作流程。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "q-bio.QM"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06583v1",
      "published_date": "2024-11-10 20:16:32 UTC",
      "updated_date": "2024-11-10 20:16:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:07:32.976466"
    },
    {
      "arxiv_id": "2411.06581v2",
      "title": "HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Su",
        "Na Yan",
        "Yansha Deng",
        "Mischa Dohler",
        "Robert Schober"
      ],
      "abstract": "Federated fine-tuning of pre-trained Large Language Models (LLMs) enables\ntask-specific adaptation across diverse datasets while preserving privacy.\nHowever, challenges such as high computational and memory demands,\nheterogeneous client resources, bandwidth constraints, and ineffective global\naggregation hinder its efficiency. To address these issues, we propose HAFLQ\n(Heterogeneous Adaptive Federated Low-Rank Adaptation Fine-tuned LLM with\nQuantization), a novel framework for efficient and scalable federated\nfine-tuning of LLMs in heterogeneous environments. To reduce memory and\ncomputation demands, we propose a salience-driven adaptive LLM quantization\nframework that evaluates the importance of transformer blocks using a salience\nmetric and applies adaptive block-wise quantization accordingly. To handle\nheterogeneous computational capabilities, we propose an importance-based\nparameter truncation and freezing scheme. To address communication bottlenecks,\nwe propose an importance-aware bandwidth-adaptive quantization method, which\ndynamically adjusts parameter precision based on importance and bandwidth\nconstraints. To improve global model aggregation, we propose an adaptive rank-1\nmatrix-level aggregation strategy, which prevents information dilution and\naccelerates convergence by aggregating only updated rank-1 matrices from\nclients. Experimental results on the text classification task demonstrate that\nHAFLQ reduces memory usage by 31%, lowers communication cost by 49%, improves\naccuracy by 50%, and achieves faster convergence compared to the baseline\nmethod.",
      "tldr_zh": "该研究提出HAFLQ框架，用于在异构环境中进行Federated fine-tuning of pre-trained Large Language Models (LLMs)，以解决高计算内存需求、客户端资源异构、带宽限制和全局聚合无效等问题。HAFLQ引入salience-driven adaptive LLM quantization来评估transformer blocks的重要性并进行自适应块级量化，同时采用importance-based parameter truncation and freezing处理计算异构，importance-aware bandwidth-adaptive quantization动态调整参数精度以优化通信，以及adaptive rank-1 matrix-level aggregation策略来提升聚合效率并加速收敛。在文本分类任务的实验中，HAFLQ比基线方法减少内存使用31%、降低通信成本49%、提高准确率50%，并实现更快收敛。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "This is an extended journal version based on our previous conference\n  paper accepted at the 2025 IEEE International Conference on Communications\n  (ICC), with additional sections and new results",
      "pdf_url": "http://arxiv.org/pdf/2411.06581v2",
      "published_date": "2024-11-10 19:59:54 UTC",
      "updated_date": "2025-05-16 11:03:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:07:45.388569"
    },
    {
      "arxiv_id": "2411.06577v1",
      "title": "Discovering emergent connections in quantum physics research via dynamic word embeddings",
      "title_zh": "通过动态词嵌入发现量子物理研究中的涌现联系",
      "authors": [
        "Felix Frohnert",
        "Xuemei Gu",
        "Mario Krenn",
        "Evert van Nieuwenburg"
      ],
      "abstract": "As the field of quantum physics evolves, researchers naturally form subgroups\nfocusing on specialized problems. While this encourages in-depth exploration,\nit can limit the exchange of ideas across structurally similar problems in\ndifferent subfields. To encourage cross-talk among these different specialized\nareas, data-driven approaches using machine learning have recently shown\npromise to uncover meaningful connections between research concepts, promoting\ncross-disciplinary innovation. Current state-of-the-art approaches represent\nconcepts using knowledge graphs and frame the task as a link prediction\nproblem, where connections between concepts are explicitly modeled. In this\nwork, we introduce a novel approach based on dynamic word embeddings for\nconcept combination prediction. Unlike knowledge graphs, our method captures\nimplicit relationships between concepts, can be learned in a fully unsupervised\nmanner, and encodes a broader spectrum of information. We demonstrate that this\nrepresentation enables accurate predictions about the co-occurrence of concepts\nwithin research abstracts over time. To validate the effectiveness of our\napproach, we provide a comprehensive benchmark against existing methods and\noffer insights into the interpretability of these embeddings, particularly in\nthe context of quantum physics research. Our findings suggest that this\nrepresentation offers a more flexible and informative way of modeling\nconceptual relationships in scientific literature.",
      "tldr_zh": "本研究探讨了量子物理领域的子领域隔离问题，通过数据驱动方法促进跨学科创新。作者提出了一种基于动态词嵌入的新方法，用于预测研究概念的组合，该方法捕捉隐式关系，并以完全无监督方式学习知识图谱无法涵盖的更广泛信息。在实验中，该方法准确预测了概念在研究摘要中的时间共现，并通过全面基准测试证明其优于现有基于知识图谱和链接预测的模型，提供更灵活且可解释的概念关系建模，从而为量子物理研究中的跨领域联系发现提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages; 4 figures; 1 table; Appendix: 2 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.06577v1",
      "published_date": "2024-11-10 19:45:59 UTC",
      "updated_date": "2024-11-10 19:45:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:07:56.899102"
    },
    {
      "arxiv_id": "2411.06568v2",
      "title": "Meta-Learning Objectives for Preference Optimization",
      "title_zh": "偏好优化的元学习目标",
      "authors": [
        "Carlo Alfano",
        "Silvia Sapora",
        "Jakob Nicolaus Foerster",
        "Patrick Rebeschini",
        "Yee Whye Teh"
      ],
      "abstract": "Evaluating preference optimization (PO) algorithms on LLM alignment is a\nchallenging task that presents prohibitive costs, noise, and several variables\nlike model size and hyper-parameters. In this work, we show that it is possible\nto gain insights on the efficacy of PO algorithm on much simpler benchmarks. We\ndesign a diagnostic suite of MuJoCo tasks and datasets, which we use to\nsystematically evaluate PO algorithms, establishing a more controlled and\ncheaper benchmark. We then propose a novel family of PO algorithms based on\nmirror descent, which we call Mirror Preference Optimization (MPO). Through\nevolutionary strategies, we search this class to discover algorithms\nspecialized to specific properties of preference datasets, such as\nmixed-quality or noisy data. We demonstrate that our discovered PO algorithms\noutperform all known algorithms in the targeted MuJoCo settings. Finally, based\non the insights gained from our MuJoCo experiments, we design a novel PO\nalgorithm that significantly outperforms existing baselines in an LLM alignment\ntask.",
      "tldr_zh": "该研究针对评估偏好优化 (PO) 算法在LLM校准中的挑战（如高成本、噪音和变量影响），提出使用MuJoCo任务作为更简便的诊断基准，以系统评估PO算法。作者引入了Mirror Preference Optimization (MPO)算法家族，基于mirror descent并通过进化策略搜索，优化算法以适应特定偏好数据集的特性，如混合质量或噪音数据。在MuJoCo设置中，优化后的PO算法优于现有方法；基于这些实验洞见，他们设计了一个新算法，在LLM校准任务中显著超越基线。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06568v2",
      "published_date": "2024-11-10 19:11:48 UTC",
      "updated_date": "2025-02-04 22:02:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:08:09.087253"
    },
    {
      "arxiv_id": "2411.06565v3",
      "title": "Foundation Model for Composite Microstructures: Reconstruction, Stiffness, and Nonlinear Behavior Prediction",
      "title_zh": "复合微结构的基础模型：重建、刚度和非线性行为预测",
      "authors": [
        "Ting-Ju Wei",
        "Chuin-Shan Chen"
      ],
      "abstract": "The rapid advancement of machine learning has unlocked numerous opportunities\nfor materials science, particularly in accelerating the design and analysis of\nmaterials. However, a significant challenge lies in the scarcity and high cost\nof obtaining high-quality materials datasets. While foundation models\npre-trained on large datasets have excelled in fields like natural language\nprocessing by leveraging latent features through transfer learning, their\napplication in materials science remains limited. Here, we present a foundation\nmodel specifically designed for composite materials. Pre-trained on a dataset\nof short-fiber composites to learn robust latent features, the model accurately\npredicts homogenized stiffness during transfer learning, even with limited\ntraining data. Additionally, our model effectively predicts the material's\nnonlinear behavior by transferring these learned features to an\nInteraction-based Material Network, which is a constitutive surrogate model.\nThese results demonstrate the potential of our foundation model to capture\ncomplex material behaviors. Our findings validate the feasibility and\neffectiveness of foundation models in composite materials. We anticipate\nextending this approach to more complex three-dimensional composite materials,\npolycrystalline materials, and beyond. Moreover, this framework enables\nhigh-accuracy predictions even when experimental data are scarce, paving the\nway for more efficient and cost-effective materials design and analysis.",
      "tldr_zh": "本研究提出了一种针对复合材料的 foundation model，通过预训练在短纤维复合材料数据集上学习鲁棒的潜在特征，从而实现对材料微结构的重建、均质化刚度和非线性行为的准确预测，即使训练数据有限。该模型利用 transfer learning 将这些特征转移到 Interaction-based Material Network 中，有效捕捉复杂材料行为，并在实验中验证了其可行性和有效性。该框架不仅降低了高质量数据集的依赖，还为更复杂的材料（如三维复合材料和多晶材料）的设计分析铺平道路，促进高效、成本效益高的材料科学应用。",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06565v3",
      "published_date": "2024-11-10 19:06:25 UTC",
      "updated_date": "2025-04-08 19:00:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:08:21.563737"
    },
    {
      "arxiv_id": "2411.06559v2",
      "title": "Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Gu",
        "Kai Zhang",
        "Yuting Ning",
        "Boyuan Zheng",
        "Boyu Gou",
        "Tianci Xue",
        "Cheng Chang",
        "Sanjari Srivastava",
        "Yanan Xie",
        "Peng Qi",
        "Huan Sun",
        "Yu Su"
      ],
      "abstract": "Language agents based on large language models (LLMs) have demonstrated great\npromise in automating web-based tasks. Recent work has shown that incorporating\nadvanced planning algorithms, e.g., tree search, is advantageous over reactive\nplanning for web agents. However, unlike simulated sandbox environments,\nreal-world environments such as the web are rife with irreversible actions.\nThis undermines the feasibility of backtracking, a cornerstone of (tree)\nsearch. Overly relying on test-time search also hurts efficiency. We advocate\nmodel-based planning for web agents that employs a world model to simulate and\ndeliberate over the outcome of each candidate action before committing to one.\nWe systematically explore this paradigm by (1) Proposing a model-based planning\nframework, WebDreamer, which employs LLMs to serve as both world models and\nvalue functions; (2) Training specialized LLMs as world models with a scalable\ndata synthesis pipeline. Empirical results demonstrate that WebDreamer achieves\nsubstantial performance improvements over reactive baselines. It is\ncompetitive, while being 4-5 times more efficient, with tree search in sandbox\nenvironments (VisualWebArena) and also works effectively on real-world websites\n(Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model,\nDreamer-7B, performs comparable to GPT-4o, highlighting the potential of\nspecialized world models for efficient and effective planning in complex web\nenvironments.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在网络代理中的规划问题，提出基于模型的规划 (model-based planning) 来解决不可逆操作和效率挑战。作者引入了 WebDreamer 框架，使用 LLMs 作为世界模型 (world model) 和价值函数 (value functions)，并通过一个可扩展的数据合成管道训练专门的世界模型。实验结果显示，WebDreamer 比反应式基线显著提升性能，在沙箱环境 (VisualWebArena) 中比树搜索 (tree search) 高效 4-5 倍，并在真实网站 (Online-Mind2Web 和 Mind2Web-Live) 上表现出色。训练后的 Dreamer-7B 与 GPT-4o 性能相当，展示了专门世界模型在复杂网络环境中的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 11 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.06559v2",
      "published_date": "2024-11-10 18:50:51 UTC",
      "updated_date": "2025-04-01 05:04:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:08:34.368644"
    },
    {
      "arxiv_id": "2411.06549v1",
      "title": "In-Context Learning for Preserving Patient Privacy: A Framework for Synthesizing Realistic Patient Portal Messages",
      "title_zh": "翻译失败",
      "authors": [
        "Joseph Gatto",
        "Parker Seegmiller",
        "Timothy E. Burdick",
        "Sarah Masud Preum"
      ],
      "abstract": "Since the COVID-19 pandemic, clinicians have seen a large and sustained\ninflux in patient portal messages, significantly contributing to clinician\nburnout. To the best of our knowledge, there are no large-scale public patient\nportal messages corpora researchers can use to build tools to optimize\nclinician portal workflows. Informed by our ongoing work with a regional\nhospital, this study introduces an LLM-powered framework for configurable and\nrealistic patient portal message generation. Our approach leverages few-shot\ngrounded text generation, requiring only a small number of de-identified\npatient portal messages to help LLMs better match the true style and tone of\nreal data. Clinical experts in our team deem this framework as HIPAA-friendly,\nunlike existing privacy-preserving approaches to synthetic text generation\nwhich cannot guarantee all sensitive attributes will be protected. Through\nextensive quantitative and human evaluation, we show that our framework\nproduces data of higher quality than comparable generation methods as well as\nall related datasets. We believe this work provides a path forward for (i) the\nrelease of large-scale synthetic patient message datasets that are\nstylistically similar to ground-truth samples and (ii) HIPAA-friendly data\ngeneration which requires minimal human de-identification efforts.",
      "tldr_zh": "这篇论文提出一个基于 in-context learning 的框架，用于合成现实的患者门户消息，以缓解 COVID-19 疫情导致的临床医生烧尽问题。该框架利用少样本 grounded text generation，仅需少量去标识化数据，即可帮助 LLM 更好地匹配真实消息的风格和语气，并被临床专家视为 HIPAA 友好的方法。实验结果显示，该框架生成的数据质量高于现有方法和相关数据集，为发布大型合成患者消息数据集提供一条减少人为去标识化努力的路径。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Findings paper presented at Machine Learning for Health (ML4H)\n  symposium 2024, December 15-16, 2024, Vancouver, Canada, 8 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.06549v1",
      "published_date": "2024-11-10 18:06:55 UTC",
      "updated_date": "2024-11-10 18:06:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:08:45.767892"
    },
    {
      "arxiv_id": "2411.06542v4",
      "title": "Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing Contact-Rich Plans?",
      "title_zh": "线性反馈在平滑化动力学上是否足以稳定接触丰富的计划？",
      "authors": [
        "Yuki Shirai",
        "Tong Zhao",
        "H. J. Terry Suh",
        "Huaijiang Zhu",
        "Xinpei Ni",
        "Jiuguang Wang",
        "Max Simchowitz",
        "Tao Pang"
      ],
      "abstract": "Designing planners and controllers for contact-rich manipulation is extremely\nchallenging as contact violates the smoothness conditions that many\ngradient-based controller synthesis tools assume. Contact smoothing\napproximates a non-smooth system with a smooth one, allowing one to use these\nsynthesis tools more effectively. However, applying classical control synthesis\nmethods to smoothed contact dynamics remains relatively under-explored. This\npaper analyzes the efficacy of linear controller synthesis using differential\nsimulators based on contact smoothing. We introduce natural baselines for\nleveraging contact smoothing to compute (a) open-loop plans robust to uncertain\nconditions and/or dynamics, and (b) feedback gains to stabilize around\nopen-loop plans. Using robotic bimanual whole-body manipulation as a testbed,\nwe perform extensive empirical experiments on over 300 trajectories and analyze\nwhy LQR seems insufficient for stabilizing contact-rich plans. The video\nsummarizing this paper and hardware experiments is found here:\nhttps://youtu.be/HLaKi6qbwQg?si=_zCAmBBD6rGSitm9.",
      "tldr_zh": "这篇论文探讨了在接触丰富的操作（contact-rich manipulation）中，线性反馈控制是否足以稳定基于平滑动态的计划，因为接触违反了梯度-based 控制器合成的平滑假设。研究者引入接触 smoothing 来近似非平滑系统，并提出基线方法来计算对不确定条件或动态鲁棒的开环 plans（open-loop plans）和反馈 gains（feedback gains），以稳定这些计划。实验在机器人双臂全身体操作上进行了超过300个轨迹的测试，结果表明LQR（Linear Quadratic Regulator）无法有效稳定接触丰富的计划，从而突显了改进控制合成的必要性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "ICRA2025",
      "pdf_url": "http://arxiv.org/pdf/2411.06542v4",
      "published_date": "2024-11-10 17:48:26 UTC",
      "updated_date": "2025-05-14 11:58:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:08:57.844992"
    },
    {
      "arxiv_id": "2411.06538v1",
      "title": "A Next-Generation Approach to Airline Reservations: Integrating Cloud Microservices with AI and Blockchain for Enhanced Operational Performance",
      "title_zh": "翻译失败",
      "authors": [
        "Biman Barua",
        "M. Shamim Kaiser"
      ],
      "abstract": "This research proposes the development of a next generation airline\nreservation system that incorporates the Cloud microservices, distributed\nartificial intelligence modules and the blockchain technology to improve on the\nefficiency, safety and customer satisfaction. The traditional reservation\nsystems encounter issues related to the expansion of the systems, the integrity\nof the data provided and the level of service offered to the customers, which\nis the main focus of this architecture through the modular and data centric\ndesign approaches. This will allow different operations such as reservations,\npayments, and customer data management among others to be performed separately\nthereby facilitating high availability of the system by 30% and enhancing\nperformance of the system by 40% on its scalability. Such systems contain AI\ndriven modules that utilize the past booking patterns along with the profile of\nthe customer to estimate the demand and make recommendations, which increases\nto 25 % of customer engagement. Moreover, blockchain is effective in engaging\nan incorruptible ledger system for the all transactions therefore mitigating\nfraud incidences and increasing the clarity by 20%. The system was subjected to\nanalysis using a simulator and using machine learning evaluations that rated it\nagainst other conventional systems. The results show that there were clear\nenhancements in the speed of transactions where the rates of secure data\nprocessing rose by 35%, and the system response time by 15 %. The system can\nalso be used for other high transaction industries like logistics and\nhospitality. This structural design is indicative of how the use of advanced\ntechnologies will revolutionize the airline reservation sector. The\nimplications are growing effectiveness, improvement in security and greater\ncustomer contentment.",
      "tldr_zh": "本研究提出了一种新一代航空预订系统，通过整合 Cloud microservices、分布式 AI 和 Blockchain 技术，解决传统系统的扩展性、数据完整性和服务水平问题。该系统采用模块化设计，使预订、支付和客户数据管理等操作独立进行，提高系统可用性 30% 和可扩展性 40%；AI 模块利用历史预订模式和客户资料进行需求预测和推荐，提升客户参与度 25%。实验评估显示，与传统系统相比，该系统交易速度和安全数据处理提高了 35%，响应时间缩短 15%。此外，该框架可扩展应用于物流和酒店等高交易行业，推动行业效率、安全性和客户满意度的整体提升。",
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.06538v1",
      "published_date": "2024-11-10 17:38:30 UTC",
      "updated_date": "2024-11-10 17:38:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:09:09.924196"
    },
    {
      "arxiv_id": "2411.06535v1",
      "title": "Probabilistic Consensus through Ensemble Validation: A Framework for LLM Reliability",
      "title_zh": "翻译失败",
      "authors": [
        "Ninad Naik"
      ],
      "abstract": "Large Language Models (LLMs) have shown significant advances in text\ngeneration but often lack the reliability needed for autonomous deployment in\nhigh-stakes domains like healthcare, law, and finance. Existing approaches rely\non external knowledge or human oversight, limiting scalability. We introduce a\nnovel framework that repurposes ensemble methods for content validation through\nmodel consensus. In tests across 78 complex cases requiring factual accuracy\nand causal consistency, our framework improved precision from 73.1% to 93.9%\nwith two models (95% CI: 83.5%-97.9%) and to 95.6% with three models (95% CI:\n85.2%-98.8%). Statistical analysis indicates strong inter-model agreement\n($\\kappa$ > 0.76) while preserving sufficient independence to catch errors\nthrough disagreement. We outline a clear pathway to further enhance precision\nwith additional validators and refinements. Although the current approach is\nconstrained by multiple-choice format requirements and processing latency, it\noffers immediate value for enabling reliable autonomous AI systems in critical\napplications.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)在高风险领域（如医疗、法律和金融）中的可靠性问题，提出了一种基于模型共识的集成验证框架（Ensemble Validation）。该框架通过多个LLMs的共识机制进行内容验证，避免依赖外部知识或人工监督，从而提升可扩展性。在78个复杂案例测试中，该框架将精确度从73.1%提高到93.9%（使用两个模型，95% CI: 83.5%-97.9%）和95.6%（使用三个模型，95% CI: 85.2%-98.8%），并显示出强一致性（$\\kappa$ > 0.76）的同时保持模型独立性以捕捉错误。尽管受限于多选格式和处理延迟，该框架为构建可靠的自主AI系统提供了重要路径。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.06535v1",
      "published_date": "2024-11-10 17:32:16 UTC",
      "updated_date": "2024-11-10 17:32:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:09:21.416614"
    },
    {
      "arxiv_id": "2411.06528v1",
      "title": "Epistemic Integrity in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Bijean Ghafouri",
        "Shahrad Mohammadzadeh",
        "James Zhou",
        "Pratheeksha Nair",
        "Jacob-Junqi Tian",
        "Mayank Goel",
        "Reihaneh Rabbany",
        "Jean-François Godbout",
        "Kellin Pelrine"
      ],
      "abstract": "Large language models are increasingly relied upon as sources of information,\nbut their propensity for generating false or misleading statements with high\nconfidence poses risks for users and society. In this paper, we confront the\ncritical problem of epistemic miscalibration $\\unicode{x2013}$ where a model's\nlinguistic assertiveness fails to reflect its true internal certainty. We\nintroduce a new human-labeled dataset and a novel method for measuring the\nlinguistic assertiveness of Large Language Models (LLMs) which cuts error rates\nby over 50% relative to previous benchmarks. Validated across multiple\ndatasets, our method reveals a stark misalignment between how confidently\nmodels linguistically present information and their actual accuracy. Further\nhuman evaluations confirm the severity of this miscalibration. This evidence\nunderscores the urgent risk of the overstated certainty LLMs hold which may\nmislead users on a massive scale. Our framework provides a crucial step forward\nin diagnosing this miscalibration, offering a path towards correcting it and\nmore trustworthy AI across domains.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)在信息生成中的认识论失调(epistemic miscalibration)问题，即模型的语言自信度与实际准确性不匹配，导致生成虚假或误导性语句的风险。\n作者引入了一个新的基于人类标记的数据集，并提出了一种新型测量方法，能够将错误率相对先前基准降低超过50%。\n通过在多个数据集上的验证和人类评估，该方法揭示了LLMs在语言呈现信息时的自信度与真实准确率之间的显著不匹配，强调了过度自信可能大规模误导用户的严重性。\n该框架为诊断和纠正epistemic miscalibration提供了关键途径，促进更可信的AI发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06528v1",
      "published_date": "2024-11-10 17:10:13 UTC",
      "updated_date": "2024-11-10 17:10:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:09:34.006709"
    },
    {
      "arxiv_id": "2411.06525v3",
      "title": "I2VControl-Camera: Precise Video Camera Control with Adjustable Motion Strength",
      "title_zh": "I2VControl-Camera：精确视频相机控制，具有可调节运动强度",
      "authors": [
        "Wanquan Feng",
        "Jiawei Liu",
        "Pengqi Tu",
        "Tianhao Qi",
        "Mingzhen Sun",
        "Tianxiang Ma",
        "Songtao Zhao",
        "Siyu Zhou",
        "Qian He"
      ],
      "abstract": "Video generation technologies are developing rapidly and have broad potential\napplications. Among these technologies, camera control is crucial for\ngenerating professional-quality videos that accurately meet user expectations.\nHowever, existing camera control methods still suffer from several limitations,\nincluding control precision and the neglect of the control for subject motion\ndynamics. In this work, we propose I2VControl-Camera, a novel camera control\nmethod that significantly enhances controllability while providing\nadjustability over the strength of subject motion. To improve control\nprecision, we employ point trajectory in the camera coordinate system instead\nof only extrinsic matrix information as our control signal. To accurately\ncontrol and adjust the strength of subject motion, we explicitly model the\nhigher-order components of the video trajectory expansion, not merely the\nlinear terms, and design an operator that effectively represents the motion\nstrength. We use an adapter architecture that is independent of the base model\nstructure. Experiments on static and dynamic scenes show that our framework\noutperformances previous methods both quantitatively and qualitatively. The\nproject page is: https://wanquanf.github.io/I2VControlCamera .",
      "tldr_zh": "本文提出I2VControl-Camera，一种新型视频相机控制方法，旨在提升控制精度并允许调整主体运动强度，以解决现有方法在控制精确性和动态忽略方面的局限。关键创新包括使用point trajectory在相机坐标系作为控制信号、显式建模视频轨迹的higher-order components而非仅线性项，并设计一个操作符来有效表示motion strength；框架采用独立于基础模型的adapter architecture。实验在静态和动态场景中显示，该方法在定量和定性指标上均优于先前方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICLR 2025, Project page:\n  https://wanquanf.github.io/I2VControlCamera",
      "pdf_url": "http://arxiv.org/pdf/2411.06525v3",
      "published_date": "2024-11-10 16:59:39 UTC",
      "updated_date": "2025-02-28 06:40:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:09:45.711775"
    },
    {
      "arxiv_id": "2411.06524v1",
      "title": "Does This Summary Answer My Question? Modeling Query-Focused Summary Readers with Rational Speech Acts",
      "title_zh": "翻译失败",
      "authors": [
        "Cesare Spinoso-Di Piano",
        "Jackie Chi Kit Cheung"
      ],
      "abstract": "Query-focused summarization (QFS) is the task of generating a summary in\nresponse to a user-written query. Despite its user-oriented nature, there has\nbeen limited work in QFS in explicitly considering a user's understanding of a\ngenerated summary, potentially causing QFS systems to underperform at inference\ntime. In this paper, we adapt the Rational Speech Act (RSA) framework, a model\nof human communication, to explicitly model a reader's understanding of a\nquery-focused summary and integrate it within the generation method of existing\nQFS systems. In particular, we introduce the answer reconstruction objective\nwhich approximates a reader's understanding of a summary by their ability to\nuse it to reconstruct the answer to their initial query. Using this objective,\nwe are able to re-rank candidate summaries generated by existing QFS systems\nand select summaries that better align with their corresponding query and\nreference summary. More generally, our study suggests that a simple and\neffective way of improving a language generation system designed for a\nuser-centered task may be to explicitly incorporate its user requirements into\nthe system's generation procedure.",
      "tldr_zh": "本文探讨了查询焦点摘要 (QFS) 任务中，用户对生成摘要的理解问题，提出使用 Rational Speech Act (RSA) 框架来建模读者的理解，并将其整合到现有 QFS 系统的方法中。具体而言，引入 answer reconstruction objective，通过评估读者使用摘要重建原始查询答案的能力，来重新排序候选摘要，从而选择更符合查询和参考摘要的版本。实验结果显示，这种方法能显著提升 QFS 系统的性能，并建议在用户中心语言生成系统中显式纳入用户需求，以实现更有效的优化。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06524v1",
      "published_date": "2024-11-10 16:48:21 UTC",
      "updated_date": "2024-11-10 16:48:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:09:57.368754"
    },
    {
      "arxiv_id": "2411.07271v2",
      "title": "Multi-hop Upstream Anticipatory Traffic Signal Control with Deep Reinforcement Learning",
      "title_zh": "基于深度强化学习的多跳上游预见性交通信号控制",
      "authors": [
        "Xiaocan Li",
        "Xiaoyu Wang",
        "Ilia Smirnov",
        "Scott Sanner",
        "Baher Abdulhai"
      ],
      "abstract": "Coordination in traffic signal control is crucial for managing congestion in\nurban networks. Existing pressure-based control methods focus only on immediate\nupstream links, leading to suboptimal green time allocation and increased\nnetwork delays. However, effective signal control inherently requires\ncoordination across a broader spatial scope, as the effect of upstream traffic\nshould influence signal control decisions at downstream intersections,\nimpacting a large area in the traffic network. Although agent communication\nusing neural network-based feature extraction can implicitly enhance spatial\nawareness, it significantly increases the learning complexity, adding an\nadditional layer of difficulty to the challenging task of control in deep\nreinforcement learning. To address the issue of learning complexity and myopic\ntraffic pressure definition, our work introduces a novel concept based on\nMarkov chain theory, namely \\textit{multi-hop upstream pressure}, which\ngeneralizes the conventional pressure to account for traffic conditions beyond\nthe immediate upstream links. This farsighted and compact metric informs the\ndeep reinforcement learning agent to preemptively clear the multi-hop upstream\nqueues, guiding the agent to optimize signal timings with a broader spatial\nawareness. Simulations on synthetic and realistic (Toronto) scenarios\ndemonstrate controllers utilizing multi-hop upstream pressure significantly\nreduce overall network delay by prioritizing traffic movements based on a\nbroader understanding of upstream congestion.",
      "tldr_zh": "本研究针对现有交通信号控制方法仅关注立即上游路段（leading to suboptimal green time allocation and increased network delays）的局限性，提出了一种基于 Markov chain theory 的新概念——multi-hop upstream pressure。利用这一指标，深度强化学习（Deep Reinforcement Learning）代理能够预先考虑多跳上游交通条件，实现更广泛的空间协调，从而优化信号定时和优先处理上游拥堵。在合成和真实（Toronto）场景的模拟中，该方法显著降低了整体网络延迟，证明了其在提升交通效率方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "math.PR"
      ],
      "primary_category": "cs.LG",
      "comment": "5 tables, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.07271v2",
      "published_date": "2024-11-10 16:28:42 UTC",
      "updated_date": "2025-01-16 21:09:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:10:09.756732"
    },
    {
      "arxiv_id": "2411.07795v2",
      "title": "InvisMark: Invisible and Robust Watermarking for AI-generated Image Provenance",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Xu",
        "Mengya Hu",
        "Deren Lei",
        "Yaxi Li",
        "David Lowe",
        "Alex Gorevski",
        "Mingyu Wang",
        "Emily Ching",
        "Alex Deng"
      ],
      "abstract": "The proliferation of AI-generated images has intensified the need for robust\ncontent authentication methods. We present InvisMark, a novel watermarking\ntechnique designed for high-resolution AI-generated images. Our approach\nleverages advanced neural network architectures and training strategies to\nembed imperceptible yet highly robust watermarks. InvisMark achieves\nstate-of-the-art performance in imperceptibility (PSNR$\\sim$51, SSIM $\\sim$\n0.998) while maintaining over 97\\% bit accuracy across various image\nmanipulations. Notably, we demonstrate the successful encoding of 256-bit\nwatermarks, significantly expanding payload capacity while preserving image\nquality. This enables the embedding of UUIDs with error correction codes,\nachieving near-perfect decoding success rates even under challenging image\ndistortions. We also address potential vulnerabilities against advanced attacks\nand propose mitigation strategies. By combining high imperceptibility, extended\npayload capacity, and resilience to manipulations, InvisMark provides a robust\nfoundation for ensuring media provenance in an era of increasingly\nsophisticated AI-generated content. Source code of this paper is available at:\nhttps://github.com/microsoft/InvisMark.",
      "tldr_zh": "该研究提出了一种名为 InvisMark 的新型水印技术，用于 AI-generated images 的内容认证，通过 advanced neural network architectures 和训练策略嵌入隐形且高度 robust 的水印。InvisMark 实现了 state-of-the-art 的 imperceptibility（PSNR ~51, SSIM ~0.998），并在各种图像 manipulations 下保持超过97%的 bit accuracy，同时支持编码256-bit水marks以扩展 payload capacity，并使用 UUIDs 和 error correction codes 实现近乎完美的解码成功率。作者还讨论了潜在 vulnerabilities 并提供 mitigation strategies，从而为确保媒体 provenance 在 AI 时代提供可靠基础。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted to WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.07795v2",
      "published_date": "2024-11-10 16:22:22 UTC",
      "updated_date": "2024-11-19 05:19:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:10:22.511100"
    },
    {
      "arxiv_id": "2411.06510v1",
      "title": "Offline Handwritten Signature Verification Using a Stream-Based Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Kecia G. de Moura",
        "Rafael M. O. Cruz",
        "Robert Sabourin"
      ],
      "abstract": "Handwritten Signature Verification (HSV) systems distinguish between genuine\nand forged signatures. Traditional HSV development involves a static batch\nconfiguration, constraining the system's ability to model signatures to the\nlimited data available. Signatures exhibit high intra-class variability and are\nsensitive to various factors, including time and external influences, imparting\nthem a dynamic nature. This paper investigates the signature learning process\nwithin a data stream context. We propose a novel HSV approach with an adaptive\nsystem that receives an infinite sequence of signatures and is updated over\ntime. Experiments were carried out on GPDS Synthetic, CEDAR, and MCYT datasets.\nResults demonstrate the superior performance of the proposed method compared to\nstandard approaches that use a Support Vector Machine as a classifier.\nImplementation of the method is available at\nhttps://github.com/kdMoura/stream_hsv.",
      "tldr_zh": "这篇论文提出了一种基于数据流的离线手写签名验证 (HSV) 方法，以解决传统静态批量配置受限于数据量的问题，并适应签名的动态性质，如内部变异性和外部影响。方法采用自适应系统，处理无限序列的签名并随时间更新学习过程。实验在 GPDS Synthetic、CEDAR 和 MCYT 数据集上进行，结果显示该方法在性能上优于使用 Support Vector Machine (SVM) 的标准方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for oral presentation at the International Conference on\n  Pattern Recognition (ICPR) 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.06510v1",
      "published_date": "2024-11-10 16:16:06 UTC",
      "updated_date": "2024-11-10 16:16:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:10:34.002974"
    },
    {
      "arxiv_id": "2411.06508v1",
      "title": "Understanding the Role of Equivariance in Self-supervised Learning",
      "title_zh": "理解等变性在自监督学习中的作用",
      "authors": [
        "Yifei Wang",
        "Kaiwen Hu",
        "Sharut Gupta",
        "Ziyu Ye",
        "Yisen Wang",
        "Stefanie Jegelka"
      ],
      "abstract": "Contrastive learning has been a leading paradigm for self-supervised\nlearning, but it is widely observed that it comes at the price of sacrificing\nuseful features (\\eg colors) by being invariant to data augmentations. Given\nthis limitation, there has been a surge of interest in equivariant\nself-supervised learning (E-SSL) that learns features to be augmentation-aware.\nHowever, even for the simplest rotation prediction method, there is a lack of\nrigorous understanding of why, when, and how E-SSL learns useful features for\ndownstream tasks. To bridge this gap between practice and theory, we establish\nan information-theoretic perspective to understand the generalization ability\nof E-SSL. In particular, we identify a critical explaining-away effect in E-SSL\nthat creates a synergy between the equivariant and classification tasks. This\nsynergy effect encourages models to extract class-relevant features to improve\nits equivariant prediction, which, in turn, benefits downstream tasks requiring\nsemantic features. Based on this perspective, we theoretically analyze the\ninfluence of data transformations and reveal several principles for practical\ndesigns of E-SSL. Our theory not only aligns well with existing E-SSL methods\nbut also sheds light on new directions by exploring the benefits of model\nequivariance. We believe that a theoretically grounded understanding on the\nrole of equivariance would inspire more principled and advanced designs in this\nfield. Code is available at https://github.com/kaotty/Understanding-ESSL.",
      "tldr_zh": "本论文探讨了等变性（Equivariance）在自监督学习（Self-supervised Learning）中的作用，针对对比学习（Contrastive Learning）因追求不变性而牺牲有用特征（如颜色）的问题，提出了等变自监督学习（E-SSL）的理论框架。作者从信息理论视角分析了 E-SSL 的泛化能力，识别出一种关键的解释效应（explaining-away effect），该效应在等变任务和分类任务之间创建协同作用，从而鼓励模型提取与类别相关的语义特征，提升下游任务性能。论文进一步分析了数据变换对 E-SSL 的影响，并提出了实用设计原则，如优化模型等变性，以指导未来方法开发。该研究不仅与现有 E-SSL 方法一致，还为更原则化的设计提供了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.IT",
        "math.IT",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.06508v1",
      "published_date": "2024-11-10 16:09:47 UTC",
      "updated_date": "2024-11-10 16:09:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:10:46.864618"
    },
    {
      "arxiv_id": "2411.06498v1",
      "title": "Barriers to Complexity-Theoretic Proofs that Achieving AGI Using Machine Learning is Intractable",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Guerzhoy"
      ],
      "abstract": "A recent paper (van Rooij et al. 2024) claims to have proved that achieving\nhuman-like intelligence using learning from data is intractable in a\ncomplexity-theoretic sense. We identify that the proof relies on an unjustified\nassumption about the distribution of (input, output) pairs to the system. We\nbriefly discuss that assumption in the context of two fundamental barriers to\nrepairing the proof: the need to precisely define ``human-like,\" and the need\nto account for the fact that a particular machine learning system will have\nparticular inductive biases that are key to the analysis.",
      "tldr_zh": "这篇论文质疑了 van Rooij et al. (2024) 关于使用机器学习实现 AGI 在复杂性理论上是不可行的证明，指出该证明依赖于一个未经证实的假设，即系统输入输出对的分布。作者分析了修复这一证明的两个根本障碍：首先，需要精确定义“human-like”智能；其次，必须考虑特定机器学习系统的归纳偏差（inductive biases）。这些发现强调了在复杂性理论证明中处理假设和定义的重要性，以推进 AGI 研究。",
      "categories": [
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06498v1",
      "published_date": "2024-11-10 15:47:30 UTC",
      "updated_date": "2024-11-10 15:47:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:10:57.420077"
    },
    {
      "arxiv_id": "2411.06493v2",
      "title": "LProtector: An LLM-driven Vulnerability Detection System",
      "title_zh": "翻译失败",
      "authors": [
        "Ze Sheng",
        "Fenghua Wu",
        "Xiangwu Zuo",
        "Chao Li",
        "Yuxin Qiao",
        "Lei Hang"
      ],
      "abstract": "This paper presents LProtector, an automated vulnerability detection system\nfor C/C++ codebases driven by the large language model (LLM) GPT-4o and\nRetrieval-Augmented Generation (RAG). As software complexity grows, traditional\nmethods face challenges in detecting vulnerabilities effectively. LProtector\nleverages GPT-4o's powerful code comprehension and generation capabilities to\nperform binary classification and identify vulnerabilities within target\ncodebases. We conducted experiments on the Big-Vul dataset, showing that\nLProtector outperforms two state-of-the-art baselines in terms of F1 score,\ndemonstrating the potential of integrating LLMs with vulnerability detection.",
      "tldr_zh": "本论文介绍了 LProtector，一种基于大型语言模型（LLM）GPT-4o 和检索增强生成（RAG）的自动化漏洞检测系统，针对 C/C++ 代码库进行漏洞识别，以应对软件复杂性带来的传统方法挑战。LProtector 利用 GPT-4o 的代码理解和生成能力，通过二元分类来检测目标代码中的漏洞。实验在 Big-Vul 数据集上进行，结果显示 LProtector 在 F1 分数上优于两个最先进基线，证明了整合 LLM 于漏洞检测的潜力。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "5 pages, 4 figures. This is a preprint version of the article. The\n  final version will be published in the proceedings of the IEEE conference",
      "pdf_url": "http://arxiv.org/pdf/2411.06493v2",
      "published_date": "2024-11-10 15:21:30 UTC",
      "updated_date": "2024-11-14 05:34:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:11:08.470905"
    },
    {
      "arxiv_id": "2411.06490v1",
      "title": "Hermes: A Large Language Model Framework on the Journey to Autonomous Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Fadhel Ayed",
        "Ali Maatouk",
        "Nicola Piovesan",
        "Antonio De Domenico",
        "Merouane Debbah",
        "Zhi-Quan Luo"
      ],
      "abstract": "The drive toward automating cellular network operations has grown with the\nincreasing complexity of these systems. Despite advancements, full autonomy\ncurrently remains out of reach due to reliance on human intervention for\nmodeling network behaviors and defining policies to meet target requirements.\nNetwork Digital Twins (NDTs) have shown promise in enhancing network\nintelligence, but the successful implementation of this technology is\nconstrained by use case-specific architectures, limiting its role in advancing\nnetwork autonomy. A more capable network intelligence, or \"telecommunications\nbrain\", is needed to enable seamless, autonomous management of cellular\nnetwork. Large Language Models (LLMs) have emerged as potential enablers for\nthis vision but face challenges in network modeling, especially in reasoning\nand handling diverse data types. To address these gaps, we introduce Hermes, a\nchain of LLM agents that uses \"blueprints\" for constructing NDT instances\nthrough structured and explainable logical steps. Hermes allows automatic,\nreliable, and accurate network modeling of diverse use cases and\nconfigurations, thus marking progress toward fully autonomous network\noperations.",
      "tldr_zh": "该论文探讨了蜂窝网络操作的自动化挑战，由于依赖人类干预和Network Digital Twins (NDTs) 的用例特定架构，导致全自治仍未实现。作者引入Hermes，一个基于Large Language Models (LLMs) 的代理链，使用“blueprints”通过结构化和可解释的逻辑步骤来构建NDTs实例。Hermes解决了LLMs在网络建模中的推理和多样数据处理难题，实现自动、可靠且准确的网络建模，从而推动向完全自治网络管理的进展。",
      "categories": [
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06490v1",
      "published_date": "2024-11-10 15:12:12 UTC",
      "updated_date": "2024-11-10 15:12:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:11:22.486144"
    },
    {
      "arxiv_id": "2411.06463v1",
      "title": "RL-Pruner: Structured Pruning Using Reinforcement Learning for CNN Compression and Acceleration",
      "title_zh": "RL-Pruner：使用强化学习进行 CNN 压缩和加速的结构化剪枝",
      "authors": [
        "Boyao Wang",
        "Volodymyr Kindratenko"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) have demonstrated exceptional\nperformance in recent years. Compressing these models not only reduces storage\nrequirements, making deployment to edge devices feasible, but also accelerates\ninference, thereby reducing latency and computational costs. Structured\npruning, which removes filters at the layer level, directly modifies the model\narchitecture. This approach achieves a more compact architecture while\nmaintaining target accuracy, ensuring that the compressed model retains good\ncompatibility and hardware efficiency. Our method is based on a key\nobservation: filters in different layers of a neural network have varying\nimportance to the model's performance. When the number of filters to prune is\nfixed, the optimal pruning distribution across different layers is uneven to\nminimize performance loss. Layers that are more sensitive to pruning should\naccount for a smaller proportion of the pruning distribution. To leverage this\ninsight, we propose RL-Pruner, which uses reinforcement learning to learn the\noptimal pruning distribution. RL-Pruner can automatically extract dependencies\nbetween filters in the input model and perform pruning, without requiring\nmodel-specific pruning implementations. We conducted experiments on models such\nas GoogleNet, ResNet, and MobileNet, comparing our approach to other structured\npruning methods to validate its effectiveness. Our code is available at\nhttps://github.com/Beryex/RLPruner-CNN.",
      "tldr_zh": "这篇论文提出了 RL-Pruner，一种基于强化学习(Reinforcement Learning)的结构化剪枝(Structured Pruning)方法，用于压缩和加速 CNN 模型，以减少存储需求和推理延迟。RL-Pruner 通过识别不同层过滤器的性能重要性差异，自动学习最优剪枝分布，从而最小化性能损失，而无需特定模型的剪枝实现。在 GoogleNet、ResNet 和 MobileNet 等模型上的实验显示，该方法比其他结构化剪枝技术更有效，证明了其在保持准确性的同时提升硬件效率的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06463v1",
      "published_date": "2024-11-10 13:35:10 UTC",
      "updated_date": "2024-11-10 13:35:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:11:33.267071"
    },
    {
      "arxiv_id": "2411.06448v1",
      "title": "Over-parameterized Student Model via Tensor Decomposition Boosted Knowledge Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Yu-Liang Zhan",
        "Zhong-Yi Lu",
        "Hao Sun",
        "Ze-Feng Gao"
      ],
      "abstract": "Increased training parameters have enabled large pre-trained models to excel\nin various downstream tasks. Nevertheless, the extensive computational\nrequirements associated with these models hinder their widespread adoption\nwithin the community. We focus on Knowledge Distillation (KD), where a compact\nstudent model is trained to mimic a larger teacher model, facilitating the\ntransfer of knowledge of large models. In contrast to much of the previous\nwork, we scale up the parameters of the student model during training, to\nbenefit from overparameterization without increasing the inference latency. In\nparticular, we propose a tensor decomposition strategy that effectively\nover-parameterizes the relatively small student model through an efficient and\nnearly lossless decomposition of its parameter matrices into higher-dimensional\ntensors. To ensure efficiency, we further introduce a tensor constraint loss to\nalign the high-dimensional tensors between the student and teacher models.\nComprehensive experiments validate the significant performance enhancement by\nour approach in various KD tasks, covering computer vision and natural language\nprocessing areas. Our code is available at\nhttps://github.com/intell-sci-comput/OPDF.",
      "tldr_zh": "这篇论文针对大型预训练模型的高计算需求问题，提出了一种通过张量分解（tensor decomposition）提升知识蒸馏（Knowledge Distillation）的框架，允许学生模型在训练时实现过参数化（overparameterization），从而提升性能而不增加推理延迟。具体方法包括高效分解学生模型的参数矩阵为更高维张量，并引入张量约束损失（tensor constraint loss）来对齐学生和教师模型的高维张量。实验在计算机视觉和自然语言处理任务中验证了该方法的显著性能提升，代码已在 GitHub 上公开。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
      "pdf_url": "http://arxiv.org/pdf/2411.06448v1",
      "published_date": "2024-11-10 12:40:59 UTC",
      "updated_date": "2024-11-10 12:40:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:11:45.729199"
    },
    {
      "arxiv_id": "2411.06445v1",
      "title": "Prompt-Efficient Fine-Tuning for GPT-like Deep Models to Reduce Hallucination and to Improve Reproducibility in Scientific Text Generation Using Stochastic Optimisation Techniques",
      "title_zh": "翻译失败",
      "authors": [
        "Daniil Sulimov"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly adopted for complex scientific\ntext generation tasks, yet they often suffer from limitations in accuracy,\nconsistency, and hallucination control. This thesis introduces a\nParameter-Efficient Fine-Tuning (PEFT) approach tailored for GPT-like models,\naiming to mitigate hallucinations and enhance reproducibility, particularly in\nthe computational domain of mass spectrometry. We implemented Low-Rank\nAdaptation (LoRA) adapters to refine GPT-2, termed MS-GPT, using a specialized\ncorpus of mass spectrometry literature. Through novel evaluation methods\napplied to LLMs, including BLEU, ROUGE, and Perplexity scores, the fine-tuned\nMS-GPT model demonstrated superior text coherence and reproducibility compared\nto the baseline GPT-2, confirmed through statistical analysis with the Wilcoxon\nrank-sum test. Further, we propose a reproducibility metric based on cosine\nsimilarity of model outputs under controlled prompts, showcasing MS-GPT's\nenhanced stability. This research highlights PEFT's potential to optimize LLMs\nfor scientific contexts, reducing computational costs while improving model\nreliability.",
      "tldr_zh": "本论文提出了一种针对 GPT-like 模型的 Parameter-Efficient Fine-Tuning (PEFT) 方法，旨在减少幻觉并提升科学文本生成的再现性，尤其在质谱计算领域。方法通过 Low-Rank Adaptation (LoRA) 适配器微调 GPT-2，创建了 MS-GPT 模型，并使用质谱文献专用语料进行训练。实验评估采用 BLEU、ROUGE 和 Perplexity 分数，以及 Wilcoxon rank-sum test 和基于余弦相似度的再现性指标，证明 MS-GPT 比基线 GPT-2 具有更高的文本连贯性和稳定性。该研究突出了 PEFT 在优化大型语言模型 (LLMs) 用于科学上下文中的潜力，显著降低了计算成本并提高了模型可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50, 91B82",
        "I.2.7; G.3"
      ],
      "primary_category": "cs.CL",
      "comment": "73 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.06445v1",
      "published_date": "2024-11-10 12:28:09 UTC",
      "updated_date": "2024-11-10 12:28:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:11:59.284909"
    },
    {
      "arxiv_id": "2411.08063v1",
      "title": "MatPilot: an LLM-enabled AI Materials Scientist under the Framework of Human-Machine Collaboration",
      "title_zh": "翻译失败",
      "authors": [
        "Ziqi Ni",
        "Yahao Li",
        "Kaijia Hu",
        "Kunyuan Han",
        "Ming Xu",
        "Xingyu Chen",
        "Fengqi Liu",
        "Yicong Ye",
        "Shuxin Bai"
      ],
      "abstract": "The rapid evolution of artificial intelligence, particularly large language\nmodels, presents unprecedented opportunities for materials science research. We\nproposed and developed an AI materials scientist named MatPilot, which has\nshown encouraging abilities in the discovery of new materials. The core\nstrength of MatPilot is its natural language interactive human-machine\ncollaboration, which augments the research capabilities of human scientist\nteams through a multi-agent system. MatPilot integrates unique cognitive\nabilities, extensive accumulated experience, and ongoing curiosity of\nhuman-beings with the AI agents' capabilities of advanced abstraction, complex\nknowledge storage and high-dimensional information processing. It could\ngenerate scientific hypotheses and experimental schemes, and employ predictive\nmodels and optimization algorithms to drive an automated experimental platform\nfor experiments. It turns out that our system demonstrates capabilities for\nefficient validation, continuous learning, and iterative optimization.",
      "tldr_zh": "本研究提出 MatPilot，一种基于 LLM (Large Language Models) 的 AI 材料科学家框架，旨在通过人机协作提升材料科学发现效率。MatPilot 采用多智能体系统，将人类的认知能力、积累经验和好奇心与 AI 的高级抽象、复杂知识存储和高维信息处理相结合。系统能够生成科学假设和实验方案，并利用预测模型及优化算法驱动自动化实验平台。结果显示，MatPilot 展现出高效验证、持续学习和迭代优化的强大能力，为人类科学家团队的研究提供有力支持。",
      "categories": [
        "physics.soc-ph",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.08063v1",
      "published_date": "2024-11-10 12:23:44 UTC",
      "updated_date": "2024-11-10 12:23:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:12:09.828016"
    },
    {
      "arxiv_id": "2411.06442v1",
      "title": "Local Implicit Wavelet Transformer for Arbitrary-Scale Super-Resolution",
      "title_zh": "局部隐式小波 Transformer 用于任意尺度超分辨率",
      "authors": [
        "Minghong Duan",
        "Linhao Qu",
        "Shaolei Liu",
        "Manning Wang"
      ],
      "abstract": "Implicit neural representations have recently demonstrated promising\npotential in arbitrary-scale Super-Resolution (SR) of images. Most existing\nmethods predict the pixel in the SR image based on the queried coordinate and\nensemble nearby features, overlooking the importance of incorporating\nhigh-frequency prior information in images, which results in limited\nperformance in reconstructing high-frequency texture details in images. To\naddress this issue, we propose the Local Implicit Wavelet Transformer (LIWT) to\nenhance the restoration of high-frequency texture details. Specifically, we\ndecompose the features extracted by an encoder into four sub-bands containing\ndifferent frequency information using Discrete Wavelet Transform (DWT). We then\nintroduce the Wavelet Enhanced Residual Module (WERM) to transform these four\nsub-bands into high-frequency priors, followed by utilizing the Wavelet Mutual\nProjected Fusion (WMPF) and the Wavelet-aware Implicit Attention (WIA) to fully\nexploit the high-frequency prior information for recovering high-frequency\ndetails in images. We conducted extensive experiments on benchmark datasets to\nvalidate the effectiveness of LIWT. Both qualitative and quantitative results\ndemonstrate that LIWT achieves promising performance in arbitrary-scale SR\ntasks, outperforming other state-of-the-art methods. The code is available at\nhttps://github.com/dmhdmhdmh/LIWT.",
      "tldr_zh": "该研究提出了一种名为 Local Implicit Wavelet Transformer (LIWT) 的方法，用于图像任意比例的 Super-Resolution (SR)，以解决现有隐式神经表示方法忽略高频信息，导致高频纹理细节重建不足的问题。具体而言，LIWT 通过 Discrete Wavelet Transform (DWT) 将编码器提取的特征分解成四个子带，然后利用 Wavelet Enhanced Residual Module (WERM) 生成高频先验，并通过 Wavelet Mutual Projected Fusion (WMPF) 和 Wavelet-aware Implicit Attention (WIA) 充分融合这些信息来恢复图像的高频细节。在基准数据集上的广泛实验显示，LIWT 在定性和定量结果上均超过了现有最先进方法，证明了其在任意比例 SR 任务中的优越性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by BMVC 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.06442v1",
      "published_date": "2024-11-10 12:21:14 UTC",
      "updated_date": "2024-11-10 12:21:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:12:22.361079"
    },
    {
      "arxiv_id": "2411.06438v5",
      "title": "Conditional [MASK] Discrete Diffusion Language Model",
      "title_zh": "条件 [MASK] 离散扩散语言模型",
      "authors": [
        "Hyukhun Koh",
        "Minha Jhang",
        "Dohyung Kim",
        "Sangmook Lee",
        "Kyomin Jung"
      ],
      "abstract": "Although auto-regressive models excel in natural language processing, they\noften struggle to generate diverse text and provide limited controllability.\nNon-auto-regressive methods could be an alternative but often produce\ndegenerate outputs and exhibit shortcomings in conditional generation. To\naddress these challenges, we propose Diffusion-EAGS, a novel framework that\nintegrates conditional masked language models into diffusion language models\nthrough the theoretical lens of a conditional Markov Random Field. In doing so,\nwe propose entropy-adaptive Gibbs sampling and entropy-based noise scheduling\nto counterbalance each model's shortcomings. Experimental results show that\nDiffusion-EAGS outperforms baselines and achieves the best quality-diversity\ntradeoff, demonstrating its effectiveness in non-autoregressive text\ngeneration.",
      "tldr_zh": "该论文针对 auto-regressive 模型在自然语言处理中的文本多样性和可控性不足问题，以及 non-auto-regressive 方法的退化输出和条件生成短板，提出了一种新型框架 Diffusion-EAGS。框架通过条件 Markov Random Field 的理论视角，将条件掩码语言模型集成到扩散语言模型中，并引入熵自适应 Gibbs sampling 和熵-based 噪声调度，以平衡各模型的缺点。实验结果表明，Diffusion-EAGS 优于基线模型，在非自动回归文本生成中实现了最佳的质量-多样性权衡。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06438v5",
      "published_date": "2024-11-10 11:49:36 UTC",
      "updated_date": "2025-02-24 09:11:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:12:34.001255"
    },
    {
      "arxiv_id": "2411.06437v1",
      "title": "CTC-Assisted LLM-Based Contextual ASR",
      "title_zh": "翻译失败",
      "authors": [
        "Guanrou Yang",
        "Ziyang Ma",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "abstract": "Contextual ASR or hotword customization holds substantial practical value.\nDespite the impressive performance of current end-to-end (E2E) automatic speech\nrecognition (ASR) systems, they often face challenges in accurately recognizing\nrare words. Typical E2E contextual ASR models commonly feature complex\narchitectures and decoding mechanisms, limited in performance and susceptible\nto interference from distractor words. With large language model (LLM)-based\nASR models emerging as the new mainstream, we propose a CTC-Assisted LLM-Based\nContextual ASR model with an efficient filtering algorithm. By using coarse CTC\ndecoding results to filter potential relevant hotwords and incorporating them\ninto LLM prompt input, our model attains WER/B-WER of 1.27%/3.67% and\n2.72%/8.02% on the Librispeech test-clean and test-other sets targeting on\nrecognizing rare long-tail words, demonstrating significant improvements\ncompared to the baseline LLM-based ASR model, and substantially surpassing\nother related work. More remarkably, with the help of the large language model\nand proposed filtering algorithm, our contextual ASR model still performs well\nwith 2000 biasing words.",
      "tldr_zh": "该研究针对自动语音识别（ASR）系统在识别稀有词时的挑战，提出了一种CTC-Assisted LLM-Based Contextual ASR模型，结合高效过滤算法来提升上下文热词定制性能。该方法利用粗糙的CTC解码结果过滤潜在相关热词，并将其整合到LLM提示输入中，从而减少干扰并提高准确性。在Librispeech测试集上，该模型的WER/B-WER分别达到1.27%/3.67%（test-clean）和2.72%/8.02%（test-other），相较基线LLM-based ASR模型有显著改善，甚至超过了相关工作。更重要的是，该模型在处理多达2000个偏差词时仍保持出色表现，为稀有长尾词识别提供了可靠解决方案。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "SLT 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.06437v1",
      "published_date": "2024-11-10 11:47:50 UTC",
      "updated_date": "2024-11-10 11:47:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:12:45.808157"
    },
    {
      "arxiv_id": "2411.06429v1",
      "title": "Reinforcement learning for Quantum Tiq-Taq-Toe",
      "title_zh": "用于 Quantum Tiq-Taq-Toe 的强化学习",
      "authors": [
        "Catalin-Viorel Dinu",
        "Thomas Moerland"
      ],
      "abstract": "Quantum Tiq-Taq-Toe is a well-known benchmark and playground for both quantum\ncomputing and machine learning. Despite its popularity, no reinforcement\nlearning (RL) methods have been applied to Quantum Tiq-Taq-Toe. Although there\nhas been some research on Quantum Chess this game is significantly more complex\nin terms of computation and analysis. Therefore, we study the combination of\nquantum computing and reinforcement learning in Quantum Tiq-Taq-Toe, which may\nserve as an accessible testbed for the integration of both fields.\n  Quantum games are challenging to represent classically due to their inherent\npartial observability and the potential for exponential state complexity. In\nQuantum Tiq-Taq-Toe, states are observed through Measurement (a 3x3 matrix of\nstate probabilities) and Move History (a 9x9 matrix of entanglement relations),\nmaking strategy complex as each move can collapse the quantum state.",
      "tldr_zh": "本论文首次将强化学习（Reinforcement Learning, RL）应用于Quantum Tiq-Taq-Toe游戏，作为量子计算和机器学习整合的简易测试平台，以填补该领域的空白。\nQuantum Tiq-Taq-Toe的游戏状态因部分可观察性和指数状态复杂度而难以用经典方法表示，具体通过Measurement（一个3x3的状态概率矩阵）和Move History（一个9x9的纠缠关系矩阵）来观察，且每个移动可能导致量子状态坍缩。\n研究强调了RL在处理这些挑战方面的潜力，为探索更复杂的量子游戏（如Quantum Chess）提供了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06429v1",
      "published_date": "2024-11-10 11:20:36 UTC",
      "updated_date": "2024-11-10 11:20:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:12:58.004819"
    },
    {
      "arxiv_id": "2411.06428v1",
      "title": "Neuro-Symbolic Rule Lists",
      "title_zh": "神经符号规则列表",
      "authors": [
        "Sascha Xu",
        "Nils Philipp Walter",
        "Jilles Vreeken"
      ],
      "abstract": "Machine learning models deployed in sensitive areas such as healthcare must\nbe interpretable to ensure accountability and fairness. Rule lists (if Age < 35\n$\\wedge$ Priors > 0 then Recidivism = True, else if Next Condition . . . )\noffer full transparency, making them well-suited for high-stakes decisions.\nHowever, learning such rule lists presents significant challenges. Existing\nmethods based on combinatorial optimization require feature pre-discretization\nand impose restrictions on rule size. Neuro-symbolic methods use more scalable\ncontinuous optimization yet place similar pre-discretization constraints and\nsuffer from unstable optimization. To address the existing limitations, we\nintroduce NeuRules, an end-to-end trainable model that unifies discretization,\nrule learning, and rule order into a single differentiable framework. We\nformulate a continuous relaxation of the rule list learning problem that\nconverges to a strict rule list through temperature annealing. NeuRules learns\nboth the discretizations of individual features, as well as their combination\ninto conjunctive rules without any pre-processing or restrictions. Extensive\nexperiments demonstrate that NeuRules consistently outperforms both\ncombinatorial and neuro-symbolic methods, effectively learning simple and\ncomplex rules, as well as their order, across a wide range of datasets.",
      "tldr_zh": "该论文探讨了在敏感领域如医疗中，使用可解释机器学习模型的重要性，特别是规则列表（Rule Lists），以确保决策的透明度和公平性。现有方法依赖组合优化（combinatorial optimization）或神经符号方法，但面临特征预离散化和优化不稳定的挑战。为解决这些问题，作者提出 NeuRules，一个端到端可训练的模型，通过连续松弛（continuous relaxation）和温度退火（temperature annealing）统一了特征离散化、规则学习和规则顺序的学习过程。实验结果显示，NeuRules 在多种数据集上优于现有组合和神经符号方法，能够有效学习简单和复杂规则及其顺序。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06428v1",
      "published_date": "2024-11-10 11:10:36 UTC",
      "updated_date": "2024-11-10 11:10:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:13:09.331560"
    },
    {
      "arxiv_id": "2411.06426v2",
      "title": "SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains",
      "title_zh": "SequentialBreak：大",
      "authors": [
        "Bijoy Ahmed Saiem",
        "MD Sadik Hossain Shanto",
        "Rakib Ahsan",
        "Md Rafi ur Rashid"
      ],
      "abstract": "As the integration of the Large Language Models (LLMs) into various\napplications increases, so does their susceptibility to misuse, raising\nsignificant security concerns. Numerous jailbreak attacks have been proposed to\nassess the security defense of LLMs. Current jailbreak attacks mainly rely on\nscenario camouflage, prompt obfuscation, prompt optimization, and prompt\niterative optimization to conceal malicious prompts. In particular, sequential\nprompt chains in a single query can lead LLMs to focus on certain prompts while\nignoring others, facilitating context manipulation. This paper introduces\nSequentialBreak, a novel jailbreak attack that exploits this vulnerability. We\ndiscuss several scenarios, not limited to examples like Question Bank, Dialog\nCompletion, and Game Environment, where the harmful prompt is embedded within\nbenign ones that can fool LLMs into generating harmful responses. The distinct\nnarrative structures of these scenarios show that SequentialBreak is flexible\nenough to adapt to various prompt formats beyond those discussed. Extensive\nexperiments demonstrate that SequentialBreak uses only a single query to\nachieve a substantial gain of attack success rate over existing baselines\nagainst both open-source and closed-source models. Through our research, we\nhighlight the urgent need for more robust and resilient safeguards to enhance\nLLM security and prevent potential misuse. All the result files and website\nassociated with this research are available in this GitHub repository:\nhttps://anonymous.4open.science/r/JailBreakAttack-4F3B/.",
      "tldr_zh": "本论文提出SequentialBreak，一种新型越狱攻击方法，利用Large Language Models (LLMs)处理顺序提示链时的漏洞，将有害提示嵌入良性提示中（如问题库、对话完成或游戏环境场景），从而诱导模型生成有害响应。不同于传统攻击，该方法通过单次查询实现灵活适应多种提示格式，并显著提高了攻击成功率。实验结果显示，SequentialBreak在开源和闭源模型上比现有基准提升了攻击成功率，强调了增强LLM安全性和防范误用的紧迫性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06426v2",
      "published_date": "2024-11-10 11:08:28 UTC",
      "updated_date": "2025-02-14 16:32:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:13:21.471188"
    },
    {
      "arxiv_id": "2411.06420v1",
      "title": "Generating Mixcode Popular Songs with Artificial Intelligence: Concepts, Plans, and Speculations",
      "title_zh": "利用人工智能生成 Mixcode 流行歌曲：概念、计划和推测",
      "authors": [
        "Abhishek Kaushik",
        "Kayla Rush"
      ],
      "abstract": "Music is a potent form of expression that can communicate, accentuate or even\ncreate the emotions of an individual or a collective. Both historically and in\ncontemporary experiences, musical expression was and is commonly\ninstrumentalized for social, political and/or economic purposes. Generative\nartificial intelligence provides a wealth of both opportunities and challenges\nwith regard to music and its role in society. This paper discusses a proposed\nproject integrating artificial intelligence and popular music, with the\nultimate goal of creating a powerful tool for implementing music for social\ntransformation, education, healthcare, and emotional well-being. Given that it\nis being presented at the outset of a collaboration between a computer\nscientist/data analyst and an ethnomusicologist/social anthropologist. it is\nmainly conceptual and somewhat speculative in nature.",
      "tldr_zh": "这篇论文探讨了生成性人工智能(Generative AI)在创建混合代码流行歌曲中的应用，强调AI如何为音乐的社会、政治和经济作用带来机遇和挑战。论文提出一个概念性项目，旨在整合AI与流行音乐，开发工具用于推动社会变革、教育、医疗和情感福祉。鉴于这是计算机科学家和民族音乐学家/社会人类学家合作初期的推测性讨论，该工作为未来AI音乐创新提供了基础性构想。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Link to the paper:https://aimc2024.pubpub.org/pub/rdulfbve/release/1\n  Published in The International Conference on AI and Musical Creativity at the\n  University of Oxford (2024) https://aimc2024.pubpub.org/",
      "pdf_url": "http://arxiv.org/pdf/2411.06420v1",
      "published_date": "2024-11-10 10:49:13 UTC",
      "updated_date": "2024-11-10 10:49:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:13:32.406899"
    },
    {
      "arxiv_id": "2411.06409v1",
      "title": "Automated Strategy Invention for Confluence of Term Rewrite Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Liao Zhang",
        "Fabian Mitterwallner",
        "Jan Jakubuv",
        "Cezary Kaliszyk"
      ],
      "abstract": "Term rewriting plays a crucial role in software verification and compiler\noptimization. With dozens of highly parameterizable techniques developed to\nprove various system properties, automatic term rewriting tools work in an\nextensive parameter space. This complexity exceeds human capacity for parameter\nselection, motivating an investigation into automated strategy invention. In\nthis paper, we focus on confluence, an important property of term rewrite\nsystems, and apply machine learning to develop the first learning-guided\nautomatic confluence prover. Moreover, we randomly generate a large dataset to\nanalyze confluence for term rewrite systems. Our results focus on improving the\nstate-of-the-art automatic confluence prover CSI: When equipped with our\ninvented strategies, it surpasses its human-designed strategies both on the\naugmented dataset and on the original human-created benchmark dataset Cops,\nproving/disproving the confluence of several term rewrite systems for which no\nautomated proofs were known before.",
      "tldr_zh": "这篇论文针对术语重写系统（Term Rewrite Systems）的汇合性（Confluence），提出了一种自动化策略发明方法，使用机器学习开发了首个学习引导的自动汇合性证明器，以应对参数空间复杂性超出人类选择能力的挑战。研究者随机生成了一个大型数据集，用于分析汇合性，并将新策略应用于现有证明器CSI，结果显示CSI在增强数据集和原基准数据集Cops上超越了人类设计策略。最终，该方法成功证明/反证了几个之前无法自动处理的术语重写系统，提升了软件验证和编译器优化的自动化水平。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "F.4.2; I.2.8"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06409v1",
      "published_date": "2024-11-10 10:08:43 UTC",
      "updated_date": "2024-11-10 10:08:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:13:45.591006"
    },
    {
      "arxiv_id": "2411.06403v1",
      "title": "Mastering NIM and Impartial Games with Weak Neural Networks: An AlphaZero-inspired Multi-Frame Approach",
      "title_zh": "使用弱神经网络掌握 NIM 和非偏游戏：一个",
      "authors": [
        "Søren Riis"
      ],
      "abstract": "This paper provides a theoretical framework that validates and explains the\nresults in the work with Bei Zhou experimentally finding that AlphaZero-style\nreinforcement learning algorithms struggle to learn optimal play in NIM, a\ncanonical impartial game proposed as an AI challenge by Harvey Friedman in\n2017. Our analysis resolves a controversy around these experimental results,\nwhich revealed unexpected difficulties in learning NIM despite its mathematical\nsimplicity compared to games like chess and Go.\n  Our key contributions are as follows:\n  We prove that by incorporating recent game history, these limited AlphaZero\nmodels can, in principle, achieve optimal play in NIM.\n  We introduce a novel search strategy where roll-outs preserve game-theoretic\nvalues during move selection, guided by a specialised policy network.\n  We provide constructive proofs showing that our approach enables optimal play\nwithin the \\(\\text{AC}^0\\) complexity class despite the theoretical limitations\nof these networks.\n  This research demonstrates how constrained neural networks when properly\ndesigned, can achieve sophisticated decision-making even in domains where their\nbasic computational capabilities appear insufficient.",
      "tldr_zh": "本论文提出一个理论框架，解释AlphaZero风格的强化学习算法在经典无偏游戏NIM中学习最优策略的困难，尽管NIM在数学上比象棋和围棋简单。研究的关键贡献包括证明通过整合最近游戏历史，受限AlphaZero模型可以实现NIM的最优玩法；引入一种新颖的搜索策略，该策略在移动选择中保留博弈论值，并由专门的政策网络引导；以及提供建设性证明，显示该方法能在\\(\\text{AC}^0\\)复杂度类中实现最优决策。该研究证明了受限神经网络通过适当设计，能够在看似超出其计算能力的领域进行复杂决策。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06403v1",
      "published_date": "2024-11-10 09:34:26 UTC",
      "updated_date": "2024-11-10 09:34:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:13:57.920911"
    },
    {
      "arxiv_id": "2411.06402v1",
      "title": "Fineweb-Edu-Ar: Machine-translated Corpus to Support Arabic Small Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sultan Alrashed",
        "Dmitrii Khizbullin",
        "David R. Pugh"
      ],
      "abstract": "As large language models (LLMs) grow and develop, so do their data demands.\nThis is especially true for multilingual LLMs, where the scarcity of\nhigh-quality and readily available data online has led to a multitude of\nsynthetic dataset generation approaches. A key technique in this space is\nmachine translation (MT), where high-quality English text is adapted to a\ntarget, comparatively low-resource language. This report introduces\nFineWeb-Edu-Ar, a machine-translated version of the exceedingly popular\n(deduplicated) FineWeb-Edu dataset from HuggingFace. To the best of our\nknowledge, FineWeb-Edu-Ar is the largest publicly available machine-translated\nArabic dataset out there, with its size of 202B tokens of an Arabic-trained\ntokenizer.",
      "tldr_zh": "该研究针对多语言大型语言模型（LLMs）的数据稀缺问题，引入了 FineWeb-Edu-Ar 数据集，这是一个通过机器翻译（MT）将高质量英语文本 FineWeb-Edu 转换为阿拉伯语的语料库。FineWeb-Edu-Ar 包含 202B tokens，是目前最大的公开可用机器翻译阿拉伯语数据集，旨在支持阿拉伯小型语言模型的训练和发展。通过这种方法，研究者们为低资源语言的LLMs 构建提供了宝贵资源，潜在地提升了模型在阿拉伯语处理中的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06402v1",
      "published_date": "2024-11-10 09:29:51 UTC",
      "updated_date": "2024-11-10 09:29:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:14:09.251010"
    },
    {
      "arxiv_id": "2411.06396v1",
      "title": "A Variance Minimization Approach to Temporal-Difference Learning",
      "title_zh": "一种时序差分学习的方差最小化方法",
      "authors": [
        "Xingguo Chen",
        "Yu Gong",
        "Shangdong Yang",
        "Wenhao Wang"
      ],
      "abstract": "Fast-converging algorithms are a contemporary requirement in reinforcement\nlearning. In the context of linear function approximation, the magnitude of the\nsmallest eigenvalue of the key matrix is a major factor reflecting the\nconvergence speed. Traditional value-based RL algorithms focus on minimizing\nerrors. This paper introduces a variance minimization (VM) approach for\nvalue-based RL instead of error minimization. Based on this approach, we\nproposed two objectives, the Variance of Bellman Error (VBE) and the Variance\nof Projected Bellman Error (VPBE), and derived the VMTD, VMTDC, and VMETD\nalgorithms. We provided proofs of their convergence and optimal policy\ninvariance of the variance minimization. Experimental studies validate the\neffectiveness of the proposed algorithms.",
      "tldr_zh": "本论文提出了一种基于方差最小化 (Variance Minimization) 的方法，用于 Temporal-Difference Learning，以提升强化学习算法的收敛速度，而不是传统上的错误最小化。作者定义了两个新目标：Bellman 错误的方差 (VBE) 和投影 Bellman 错误的方差 (VPBE)，并据此导出了 VMTD、VMTDC 和 VMETD 算法。论文证明了这些算法的收敛性和最优策略不变性，并通过实验验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06396v1",
      "published_date": "2024-11-10 08:56:16 UTC",
      "updated_date": "2024-11-10 08:56:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:14:21.905013"
    },
    {
      "arxiv_id": "2411.06391v1",
      "title": "CausalStock: Deep End-to-end Causal Discovery for News-driven Stock Movement Prediction",
      "title_zh": "CausalStock：深度端到端因果发现用于新闻驱动的股票运动预测",
      "authors": [
        "Shuqi Li",
        "Yuebo Sun",
        "Yuxin Lin",
        "Xin Gao",
        "Shuo Shang",
        "Rui Yan"
      ],
      "abstract": "There are two issues in news-driven multi-stock movement prediction tasks\nthat are not well solved in the existing works. On the one hand, \"relation\ndiscovery\" is a pivotal part when leveraging the price information of other\nstocks to achieve accurate stock movement prediction. Given that stock\nrelations are often unidirectional, such as the \"supplier-consumer\"\nrelationship, causal relations are more appropriate to capture the impact\nbetween stocks. On the other hand, there is substantial noise existing in the\nnews data leading to extracting effective information with difficulty. With\nthese two issues in mind, we propose a novel framework called CausalStock for\nnews-driven multi-stock movement prediction, which discovers the temporal\ncausal relations between stocks. We design a lag-dependent temporal causal\ndiscovery mechanism to model the temporal causal graph distribution. Then a\nFunctional Causal Model is employed to encapsulate the discovered causal\nrelations and predict the stock movements. Additionally, we propose a Denoised\nNews Encoder by taking advantage of the excellent text evaluation ability of\nlarge language models (LLMs) to extract useful information from massive news\ndata. The experiment results show that CausalStock outperforms the strong\nbaselines for both news-driven multi-stock movement prediction and multi-stock\nmovement prediction tasks on six real-world datasets collected from the US,\nChina, Japan, and UK markets. Moreover, getting benefit from the causal\nrelations, CausalStock could offer a clear prediction mechanism with good\nexplainability.",
      "tldr_zh": "该研究针对新闻驱动的多股票运动预测中存在的关系发现和新闻噪音问题，提出了一种端到端框架CausalStock，利用lag-dependent temporal causal discovery机制来建模股票之间的时间因果关系。框架结合Functional Causal Model封装这些因果关系，并通过Denoised News Encoder利用大型语言模型(LLMs)从海量新闻数据中提取有效信息，以实现更准确的股票运动预测。实验结果显示，CausalStock在来自美国、中国、日本和英国的六个真实数据集上，优于现有基线模型，并在提供清晰预测机制方面表现出色，具有良好的解释性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.06391v1",
      "published_date": "2024-11-10 08:24:03 UTC",
      "updated_date": "2024-11-10 08:24:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:14:33.566938"
    },
    {
      "arxiv_id": "2411.06387v4",
      "title": "Self-Training Meets Consistency: Improving LLMs' Reasoning with Consistency-Driven Rationale Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Jaehyeok Lee",
        "Keisuke Sakaguchi",
        "JinYeong Bak"
      ],
      "abstract": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches.",
      "tldr_zh": "这篇论文针对大型语言模型(LLMs)的自训练方法中，推理理由(rationales)质量评估不足的问题，提出CREST框架，通过后续问题(follow-up questions)来驱动一致性评估。CREST的具体方法包括过滤掉在后续问题上经常导致错误答案的推理理由，以及基于原问题和后续问题评估结果进行偏好学习(preference learning)。实验在三个问答数据集上使用开源LLMs，结果显示CREST不仅提升了推理理由的逻辑鲁棒性和正确性，还显著提高了模型的整体推理能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.06387v4",
      "published_date": "2024-11-10 08:11:05 UTC",
      "updated_date": "2025-02-06 07:07:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:14:45.738801"
    },
    {
      "arxiv_id": "2411.06385v1",
      "title": "Class Granularity: How richly does your knowledge graph represent the real world?",
      "title_zh": "类粒度：你的知识图谱如何丰富地表示真实世界？",
      "authors": [
        "Sumin Seo",
        "Heeseon Cheon",
        "Hyunho Kim"
      ],
      "abstract": "To effectively manage and utilize knowledge graphs, it is crucial to have\nmetrics that can assess the quality of knowledge graphs from various\nperspectives. While there have been studies on knowledge graph quality metrics,\nthere has been a lack of research on metrics that measure how richly\nontologies, which form the backbone of knowledge graphs, are defined or the\nimpact of richly defined ontologies. In this study, we propose a new metric\ncalled Class Granularity, which measures how well a knowledge graph is\nstructured in terms of how finely classes with unique characteristics are\ndefined. Furthermore, this research presents potential impact of Class\nGranularity in knowledge graph's on downstream tasks. In particular, we explore\nits influence on graph embedding and provide experimental results.\nAdditionally, this research goes beyond traditional Linked Open Data comparison\nstudies, which mainly focus on factors like scale and class distribution, by\nusing Class Granularity to compare four different LOD sources.",
      "tldr_zh": "本研究针对知识图谱（knowledge graphs）的质量评估，提出一个新指标Class Granularity，用于衡量本体（ontologies）中类别的定义是否足够精细，从而反映知识图谱对现实世界的表示丰富度。Class Granularity评估类别的独特特征细化程度，并探讨其对下游任务如图嵌入（graph embedding）的潜在影响。实验结果显示，该指标有助于比较四个不同Linked Open Data (LOD)来源，并揭示传统比较研究（如规模和类分布）的局限性。总的来说，这为更全面的知识图谱质量管理提供了新工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.06385v1",
      "published_date": "2024-11-10 07:57:39 UTC",
      "updated_date": "2024-11-10 07:57:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:14:56.984007"
    },
    {
      "arxiv_id": "2411.06376v2",
      "title": "Project Tracyn: Generative Artificial Intelligence based Peripherals Trace Synthesizer",
      "title_zh": "翻译失败",
      "authors": [
        "Zhibai Huang",
        "Yihan Shen",
        "Yongchen Xie",
        "Zhixiang Wei",
        "Yun wang",
        "Fangxin Liu",
        "Tao Song",
        "Zhengwei Qi"
      ],
      "abstract": "Peripheral Component Interconnect Express (PCIe) is the de facto interconnect\nstandard for high-speed peripherals and CPUs. Prototyping and optimizing PCIe\ndevices for emerging scenarios is an ongoing challenge. Since Transaction Layer\nPackets (TLPs) capture device-CPU interactions, it is crucial to analyze and\ngenerate realistic TLP traces for effective device design and optimization.\nGenerative AI offers a promising approach for creating intricate, custom TLP\ntraces necessary for PCIe hardware and software development. However, existing\nmodels often generate impractical traces due to the absence of PCIe-specific\nconstraints, such as TLP ordering and causality. This paper presents Phantom,\nthe first framework that treats TLP trace generation as a generative AI problem\nwhile incorporating PCIe-specific constraints. We validate Phantom's\neffectiveness by generating TLP traces for an actual PCIe network interface\ncard. Experimental results show that Phantom produces practical, large-scale\nTLP traces, significantly outperforming existing models, with improvements of\nup to 1000$\\times$ in task-specific metrics and up to 2.19$\\times$ in Frechet\nInception Distance (FID) compared to backbone-only methods.",
      "tldr_zh": "该论文介绍了 Project Tracyn，一种基于 Generative Artificial Intelligence 的外设迹合成框架，针对 PCIe 设备的原型设计和优化问题。Phantom 框架首次将 Transaction Layer Packets (TLPs) 迹生成视为生成式 AI 任务，同时融入 PCIe 特定约束（如 TLP 排序和因果性），以生成更真实可行的迹。实验结果显示，Phantom 在实际 PCIe 网络接口卡的 TLP 迹生成上，任务特定指标改善高达 1000 倍，Frechet Inception Distance (FID) 比基线方法提高达 2.19 倍，为 PCIe 硬件和软件开发提供了高效工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06376v2",
      "published_date": "2024-11-10 07:15:03 UTC",
      "updated_date": "2025-01-13 14:39:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:15:10.248778"
    },
    {
      "arxiv_id": "2411.14449v2",
      "title": "Unlearn to Relearn Backdoors: Deferred Backdoor Functionality Attacks on Deep Learning Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jeongjin Shin",
        "Sangdon Park"
      ],
      "abstract": "Deep learning models are vulnerable to backdoor attacks, where adversaries\ninject malicious functionality during training that activates on trigger inputs\nat inference time. Extensive research has focused on developing stealthy\nbackdoor attacks to evade detection and defense mechanisms. However, these\napproaches still have limitations that leave the door open for detection and\nmitigation due to their inherent design to cause malicious behavior in the\npresence of a trigger. To address this limitation, we introduce Deferred\nActivated Backdoor Functionality (DABF), a new paradigm in backdoor attacks.\nUnlike conventional attacks, DABF initially conceals its backdoor, producing\nbenign outputs even when triggered. This stealthy behavior allows DABF to\nbypass multiple detection and defense methods, remaining undetected during\ninitial inspections. The backdoor functionality is strategically activated only\nafter the model undergoes subsequent updates, such as retraining on benign\ndata. DABF attacks exploit the common practice in the life cycle of machine\nlearning models to perform model updates and fine-tuning after initial\ndeployment. To implement DABF attacks, we approach the problem by making the\nunlearning of the backdoor fragile, allowing it to be easily cancelled and\nsubsequently reactivate the backdoor functionality. To achieve this, we propose\na novel two-stage training scheme, called DeferBad. Our extensive experiments\nacross various fine-tuning scenarios, backdoor attack types, datasets, and\nmodel architectures demonstrate the effectiveness and stealthiness of DeferBad.",
      "tldr_zh": "该论文提出了一种新型后门攻击范式Deferred Activated Backdoor Functionality (DABF)，旨在解决传统backdoor attacks易被检测的局限性，通过初始隐藏恶意行为（如在触发时输出正常结果）来规避检测和防御机制。论文引入了DeferBad两阶段训练方案，使后门的unlearning变得脆弱，从而允许攻击者在模型后续更新（如微调或重新训练）后轻松激活恶意功能。实验结果显示，DeferBad在多种数据集、模型架构和fine-tuning场景下表现出色，显著提升了攻击的隐蔽性和有效性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14449v2",
      "published_date": "2024-11-10 07:01:53 UTC",
      "updated_date": "2024-11-25 06:51:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:15:21.724857"
    },
    {
      "arxiv_id": "2411.06367v1",
      "title": "BayesNAM: Leveraging Inconsistency for Reliable Explanations",
      "title_zh": "BayesNAM：利用不一致性实现可靠解释",
      "authors": [
        "Hoki Kim",
        "Jinseong Park",
        "Yujin Choi",
        "Seungyun Lee",
        "Jaewook Lee"
      ],
      "abstract": "Neural additive model (NAM) is a recently proposed explainable artificial\nintelligence (XAI) method that utilizes neural network-based architectures.\nGiven the advantages of neural networks, NAMs provide intuitive explanations\nfor their predictions with high model performance. In this paper, we analyze a\ncritical yet overlooked phenomenon: NAMs often produce inconsistent\nexplanations, even when using the same architecture and dataset. Traditionally,\nsuch inconsistencies have been viewed as issues to be resolved. However, we\nargue instead that these inconsistencies can provide valuable explanations\nwithin the given data model. Through a simple theoretical framework, we\ndemonstrate that these inconsistencies are not mere artifacts but emerge\nnaturally in datasets with multiple important features. To effectively leverage\nthis information, we introduce a novel framework, Bayesian Neural Additive\nModel (BayesNAM), which integrates Bayesian neural networks and feature\ndropout, with theoretical proof demonstrating that feature dropout effectively\ncaptures model inconsistencies. Our experiments demonstrate that BayesNAM\neffectively reveals potential problems such as insufficient data or structural\nlimitations of the model, providing more reliable explanations and potential\nremedies.",
      "tldr_zh": "这篇论文分析了 Neural Additive Model (NAM) 在可解释人工智能 (XAI) 中的不一致解释问题，认为这些不一致性并非缺陷，而是能提供宝贵信息，尤其在包含多个重要特征的数据集中。作者提出了一种新框架 Bayesian Neural Additive Model (BayesNAM)，通过整合 Bayesian neural networks 和 feature dropout 来有效利用这些不一致性，并提供理论证明以证明 feature dropout 的有效性。实验结果显示，BayesNAM 能够揭示数据不足或模型结构限制等潜在问题，从而提供更可靠的解释和改进建议。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2411.06367v1",
      "published_date": "2024-11-10 05:55:25 UTC",
      "updated_date": "2024-11-10 05:55:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:15:33.840958"
    },
    {
      "arxiv_id": "2411.06363v1",
      "title": "Layer-Wise Feature Metric of Semantic-Pixel Matching for Few-Shot Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Tang",
        "Junhao Lu",
        "Guoheng Huang",
        "Ming Li",
        "Xuhang Chen",
        "Guo Zhong",
        "Zhengguang Tan",
        "Zinuo Li"
      ],
      "abstract": "In Few-Shot Learning (FSL), traditional metric-based approaches often rely on\nglobal metrics to compute similarity. However, in natural scenes, the spatial\narrangement of key instances is often inconsistent across images. This spatial\nmisalignment can result in mismatched semantic pixels, leading to inaccurate\nsimilarity measurements. To address this issue, we propose a novel method\ncalled the Layer-Wise Features Metric of Semantic-Pixel Matching (LWFM-SPM) to\nmake finer comparisons. Our method enhances model performance through two key\nmodules: (1) the Layer-Wise Embedding (LWE) Module, which refines the\ncross-correlation of image pairs to generate well-focused feature maps for each\nlayer; (2)the Semantic-Pixel Matching (SPM) Module, which aligns critical\npixels based on semantic embeddings using an assignment algorithm. We conducted\nextensive experiments to evaluate our method on four widely used few-shot\nclassification benchmarks: miniImageNet, tieredImageNet, CUB-200-2011, and\nCIFAR-FS. The results indicate that LWFM-SPM achieves competitive performance\nacross these benchmarks. Our code will be publicly available on\nhttps://github.com/Halo2Tang/Code-for-LWFM-SPM.",
      "tldr_zh": "本文提出了一种针对 Few-Shot Learning (FSL) 的新方法 Layer-Wise Feature Metric of Semantic-Pixel Matching (LWFM-SPM)，以解决传统全局度量方法因图像空间不对齐导致的语义像素匹配不准确问题。该方法包括两个关键模块：Layer-Wise Embedding (LWE) 模块，用于细化图像对的交叉相关生成层级聚焦特征图；以及 Semantic-Pixel Matching (SPM) 模块，通过语义嵌入和分配算法对关键像素进行对齐。在 miniImageNet、tieredImageNet、CUB-200-2011 和 CIFAR-FS 等四个基准数据集上的实验显示，LWFM-SPM 取得了竞争性的性能表现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06363v1",
      "published_date": "2024-11-10 05:12:24 UTC",
      "updated_date": "2024-11-10 05:12:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:15:46.454357"
    },
    {
      "arxiv_id": "2411.06353v2",
      "title": "Deep Active Learning in the Open World",
      "title_zh": "开放世界中的深度主动学习",
      "authors": [
        "Tian Xie",
        "Jifan Zhang",
        "Haoyue Bai",
        "Robert Nowak"
      ],
      "abstract": "Machine learning models deployed in open-world scenarios often encounter\nunfamiliar conditions and perform poorly in unanticipated situations. As AI\nsystems advance and find application in safety-critical domains, effectively\nhandling out-of-distribution (OOD) data is crucial to building open-world\nlearning systems. In this work, we introduce ALOE, a novel active learning\nalgorithm for open-world environments designed to enhance model adaptation by\nincorporating new OOD classes via a two-stage approach. First, diversity\nsampling selects a representative set of examples, followed by energy-based OOD\ndetection to prioritize likely unknown classes for annotation. This strategy\naccelerates class discovery and learning, even under constrained annotation\nbudgets. Evaluations on three long-tailed image classification benchmarks\ndemonstrate that ALOE outperforms traditional active learning baselines,\neffectively expanding known categories while balancing annotation cost. Our\nfindings reveal a crucial tradeoff between enhancing known-class performance\nand discovering new classes, setting the stage for future advancements in\nopen-world machine learning.",
      "tldr_zh": "本论文探讨了机器学习模型在开放世界环境中处理 out-of-distribution (OOD) 数据的挑战，提出了一种新型 active learning 算法 ALOE，以提升模型适应性和类别的发现效率。ALOE 采用两阶段方法：首先通过 diversity sampling 选择代表性样本，其次利用 energy-based OOD detection 优先标注可能的未知类，从而在有限的标注预算下加速学习和分类扩展。在三个长尾图像分类基准上的实验表明，ALOE 优于传统 active learning 基线，同时揭示了提升已知类性能与发现新类之间的关键权衡，为开放世界机器学习的发展提供了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.06353v2",
      "published_date": "2024-11-10 04:04:20 UTC",
      "updated_date": "2025-04-19 00:44:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:15:58.593571"
    },
    {
      "arxiv_id": "2411.06336v1",
      "title": "Balancing Power and Ethics: A Framework for Addressing Human Rights Concerns in Military AI",
      "title_zh": "翻译失败",
      "authors": [
        "Mst Rafia Islam",
        "Azmine Toushik Wasi"
      ],
      "abstract": "AI has made significant strides recently, leading to various applications in\nboth civilian and military sectors. The military sees AI as a solution for\ndeveloping more effective and faster technologies. While AI offers benefits\nlike improved operational efficiency and precision targeting, it also raises\nserious ethical and legal concerns, particularly regarding human rights\nviolations. Autonomous weapons that make decisions without human input can\nthreaten the right to life and violate international humanitarian law. To\naddress these issues, we propose a three-stage framework (Design, In\nDeployment, and During/After Use) for evaluating human rights concerns in the\ndesign, deployment, and use of military AI. Each phase includes multiple\ncomponents that address various concerns specific to that phase, ranging from\nbias and regulatory issues to violations of International Humanitarian Law. By\nthis framework, we aim to balance the advantages of AI in military operations\nwith the need to protect human rights.",
      "tldr_zh": "本研究探讨了AI在军事领域的应用所带来的优势（如提升操作效率和精确瞄准）与潜在人权问题（如自主武器可能威胁生命权并违反International Humanitarian Law）。为了解决这些伦理和法律关切，论文提出一个三阶段框架（Design、In Deployment和During/After Use），每个阶段包含多个组件，针对性地处理偏见、监管问题以及国际人道法相关风险。该框架旨在平衡AI在军事操作中的益处与保护人权的需求，提供一种系统化的评估和应对策略。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CE",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted for oral (only 3 papers are selected!) Harms and Risks of AI\n  in the Military Workshop (HRAIM 2024) at Mila Quebec\n  (https://www.harms-risks-ai-military.org/accepted-abstracts.html#:~:text=Balancing%20Power%20and%20Ethics)",
      "pdf_url": "http://arxiv.org/pdf/2411.06336v1",
      "published_date": "2024-11-10 02:27:01 UTC",
      "updated_date": "2024-11-10 02:27:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:16:09.394525"
    },
    {
      "arxiv_id": "2411.16694v1",
      "title": "Reaction-conditioned De Novo Enzyme Design with GENzyme",
      "title_zh": "翻译失败",
      "authors": [
        "Chenqing Hua",
        "Jiarui Lu",
        "Yong Liu",
        "Odin Zhang",
        "Jian Tang",
        "Rex Ying",
        "Wengong Jin",
        "Guy Wolf",
        "Doina Precup",
        "Shuangjia Zheng"
      ],
      "abstract": "The introduction of models like RFDiffusionAA, AlphaFold3, AlphaProteo, and\nChai1 has revolutionized protein structure modeling and interaction prediction,\nprimarily from a binding perspective, focusing on creating ideal lock-and-key\nmodels. However, these methods can fall short for enzyme-substrate\ninteractions, where perfect binding models are rare, and induced fit states are\nmore common. To address this, we shift to a functional perspective for enzyme\ndesign, where the enzyme function is defined by the reaction it catalyzes.\nHere, we introduce \\textsc{GENzyme}, a \\textit{de novo} enzyme design model\nthat takes a catalytic reaction as input and generates the catalytic pocket,\nfull enzyme structure, and enzyme-substrate binding complex. \\textsc{GENzyme}\nis an end-to-end, three-staged model that integrates (1) a catalytic pocket\ngeneration and sequence co-design module, (2) a pocket inpainting and enzyme\ninverse folding module, and (3) a binding and screening module to optimize and\npredict enzyme-substrate complexes. The entire design process is driven by the\ncatalytic reaction being targeted. This reaction-first approach allows for more\naccurate and biologically relevant enzyme design, potentially surpassing\nstructure-based and binding-focused models in creating enzymes capable of\ncatalyzing specific reactions. We provide \\textsc{GENzyme} code at\nhttps://github.com/WillHua127/GENzyme.",
      "tldr_zh": "本文提出 GENzyme，一种以催化反应为条件的 de novo 酶设计模型，旨在解决现有方法如 RFDiffusionAA 和 AlphaFold3 在酶-底物交互（尤其是 induced fit states）方面的局限性。GENzyme 采用端到端的三阶段框架，包括（1）催化口袋生成和序列共同设计模块、（2）口袋修复和酶反折叠模块，以及（3）结合和筛选模块，以生成催化口袋、完整酶结构和酶-底物结合复合物。这种反应优先的方法能更准确地创建生物相关的酶设计，并可能优于基于结构的结合焦点模型。开源代码可从 https://github.com/WillHua127/GENzyme 获取。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.16694v1",
      "published_date": "2024-11-10 00:37:26 UTC",
      "updated_date": "2024-11-10 00:37:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:16:22.266791"
    },
    {
      "arxiv_id": "2411.06316v1",
      "title": "Prompts Matter: Comparing ML/GAI Approaches for Generating Inductive Qualitative Coding Results",
      "title_zh": "翻译失败",
      "authors": [
        "John Chen",
        "Alexandros Lotsos",
        "Lexie Zhao",
        "Grace Wang",
        "Uri Wilensky",
        "Bruce Sherin",
        "Michael Horn"
      ],
      "abstract": "Inductive qualitative methods have been a mainstay of education research for\ndecades, yet it takes much time and effort to conduct rigorously. Recent\nadvances in artificial intelligence, particularly with generative AI (GAI),\nhave led to initial success in generating inductive coding results. Like human\ncoders, GAI tools rely on instructions to work, and how to instruct it may\nmatter. To understand how ML/GAI approaches could contribute to qualitative\ncoding processes, this study applied two known and two theory-informed novel\napproaches to an online community dataset and evaluated the resulting coding\nresults. Our findings show significant discrepancies between ML/GAI approaches\nand demonstrate the advantage of our approaches, which introduce human coding\nprocesses into GAI prompts.",
      "tldr_zh": "这篇论文探讨了在生成归纳性定性编码（inductive qualitative coding）结果时，提示（prompts）的设计如何影响 ML/GAI 方法的表现，旨在为教育研究提供更高效的工具。研究者将两种已知方法和两种理论-informed 的新方法应用于一个在线社区数据集，其中新方法通过融入人类编码过程来优化 GAI 提示。结果显示，ML/GAI 方法之间存在显著差异，新方法表现出明显优势，证明了将人类元素整合进 AI 提示的可行性和益处。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AERA 2025 Annual Meeting",
      "pdf_url": "http://arxiv.org/pdf/2411.06316v1",
      "published_date": "2024-11-10 00:23:55 UTC",
      "updated_date": "2024-11-10 00:23:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T23:16:33.943822"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 54,
  "processed_papers_count": 54,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T23:16:51.522933"
}
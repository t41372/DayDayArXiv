[
  {
    "arxiv_id": "2508.16843v4",
    "title": "A Survey of Threats Against Voice Authentication and Anti-Spoofing Systems",
    "authors": [
      "Kamel Kamel",
      "Keshav Sood",
      "Hridoy Sankar Dutta",
      "Sunil Aryal"
    ],
    "abstract": "Voice authentication has undergone significant changes from traditional systems that relied on handcrafted acoustic features to deep learning models that can extract robust speaker embeddings. This advancement has expanded its applications across finance, smart devices, law enforcement, and beyond. However, as adoption has grown, so have the threats. This survey presents a comprehensive review of the modern threat landscape targeting Voice Authentication Systems (VAS) and Anti-Spoofing Countermeasures (CMs), including data poisoning, adversarial, deepfake, and adversarial spoofing attacks. We chronologically trace the development of voice authentication and examine how vulnerabilities have evolved in tandem with technological advancements. For each category of attack, we summarize methodologies, highlight commonly used datasets, compare performance and limitations, and organize existing literature using widely accepted taxonomies. By highlighting emerging risks and open challenges, this survey aims to support the development of more secure and resilient voice authentication systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "This paper is submitted to the IEEE IoT Journal",
    "pdf_url": "https://arxiv.org/pdf/2508.16843v4",
    "published_date": "2025-08-22 23:57:04 UTC",
    "updated_date": "2025-09-16 11:49:25 UTC"
  },
  {
    "arxiv_id": "2508.16839v4",
    "title": "One VLM, Two Roles: Stage-Wise Routing and Specialty-Level Deployment for Clinical Workflows",
    "authors": [
      "Shayan Vassef",
      "Soorya Ram Shimegekar",
      "Abhay Goyal",
      "Koustuv Saha",
      "Pi Zonooz",
      "Navin Kumar"
    ],
    "abstract": "Clinical ML workflows are often fragmented and inefficient: triage, task selection, and model deployment are handled by a patchwork of task-specific networks. These pipelines are rarely aligned with data-science practice, reducing efficiency and increasing operational cost. They also lack data-driven model identification (from imaging/tabular inputs) and standardized delivery of model outputs. We present a framework that employs a single vision-language model (VLM) in two complementary, modular roles.\n  First (Solution 1): the VLM acts as an aware model-card matcher that routes an incoming image to the appropriate specialist model via a three-stage workflow (modality -> primary abnormality -> model-card ID). Reliability is improved by (i) stage-wise prompts enabling early termination via \"None\"/\"Other\" and (ii) a calibrated top-2 answer selector with a stage-wise cutoff. This raises routing accuracy by +9 and +11 percentage points on the training and held-out splits, respectively, compared with a baseline router, and improves held-out calibration (lower Expected Calibration Error, ECE).\n  Second (Solution 2): we fine-tune the same VLM on specialty-specific datasets so that one model per specialty covers multiple downstream tasks, simplifying deployment while maintaining performance. Across gastroenterology, hematology, ophthalmology, pathology, and radiology, this single-model deployment matches or approaches specialized baselines.\n  Together, these solutions reduce data-science effort through more accurate selection, simplify monitoring and maintenance by consolidating task-specific models, and increase transparency via per-stage justifications and calibrated thresholds. Each solution stands alone, and in combination they offer a practical, modular path from triage to deployment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16839v4",
    "published_date": "2025-08-22 23:34:37 UTC",
    "updated_date": "2025-11-16 07:19:50 UTC"
  },
  {
    "arxiv_id": "2508.16836v1",
    "title": "Physics-Inspired Spatial Temporal Graph Neural Networks for Predicting Industrial Chain Resilience",
    "authors": [
      "Bicheng Wang",
      "Junping Wang",
      "Yibo Xue"
    ],
    "abstract": "Industrial chain plays an increasingly important role in the sustainable development of national economy. However, as a typical complex network, data-driven deep learning is still in its infancy in describing and analyzing the resilience of complex networks, and its core is the lack of a theoretical framework to describe the system dynamics. In this paper, we propose a physically informative neural symbolic approach to describe the evolutionary dynamics of complex networks for resilient prediction. The core idea is to learn the dynamics of the activity state of physical entities and integrate it into the multi-layer spatiotemporal co-evolution network, and use the physical information method to realize the joint learning of physical symbol dynamics and spatiotemporal co-evolution topology, so as to predict the industrial chain resilience. The experimental results show that the model can obtain better results and predict the elasticity of the industry chain more accurately and effectively, which has certain practical significance for the development of the industry.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16836v1",
    "published_date": "2025-08-22 23:22:49 UTC",
    "updated_date": "2025-08-22 23:22:49 UTC"
  },
  {
    "arxiv_id": "2509.06966v1",
    "title": "Cross-device Zero-shot Label Transfer via Alignment of Time Series Foundation Model Embeddings",
    "authors": [
      "Neal G. Ravindra",
      "Arijit Sehanobish"
    ],
    "abstract": "High-quality, medically validated labels exist for clinical actigraphy data but not for ubiquitous consumer wearables like the Apple Watch. Manually labeling wearables data is expensive and doesn't scale. This paper offers a novel framework that transfers valuable labels from a source domain (e.g., actigraphy) to a target domain (e.g., Apple Watch) without requiring paired data. Instead of working with raw time-series signals, we project both domains into a shared latent embedding space using time-series foundation models (TSFMs) and develop a new framework to align the cross-device representations. Our method, Adversarial Alignment of TSFM Embeddings forces the distributions of source and target embeddings to align within this space, facilitating label transfer across device type.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "5 pages, 3 figures, 1 table. tl;dr: Adversarial alignment of Time-Series Foundation Model (TSFM) embeddings enables transfer of high-quality clinical labels from medical-grade to consumer-grade wearables, enabling zero-shot prediction of gestational age without requiring paired data",
    "pdf_url": "https://arxiv.org/pdf/2509.06966v1",
    "published_date": "2025-08-22 23:22:41 UTC",
    "updated_date": "2025-08-22 23:22:41 UTC"
  },
  {
    "arxiv_id": "2508.18303v1",
    "title": "Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder",
    "authors": [
      "Jueqi Wang",
      "Zachary Jacokes",
      "John Darrell Van Horn",
      "Michael C. Schatz",
      "Kevin A. Pelphrey",
      "Archana Venkataraman"
    ],
    "abstract": "While imaging-genetics holds great promise for unraveling the complex interplay between brain structure and genetic variation in neurological disorders, traditional methods are limited to simplistic linear models or to black-box techniques that lack interpretability. In this paper, we present NeuroPathX, an explainable deep learning framework that uses an early fusion strategy powered by cross-attention mechanisms to capture meaningful interactions between structural variations in the brain derived from MRI and established biological pathways derived from genetics data. To enhance interpretability and robustness, we introduce two loss functions over the attention matrix - a sparsity loss that focuses on the most salient interactions and a pathway similarity loss that enforces consistent representations across the cohort. We validate NeuroPathX on both autism spectrum disorder and Alzheimer's disease. Our results demonstrate that NeuroPathX outperforms competing baseline approaches and reveals biologically plausible associations linked to the disorder. These findings underscore the potential of NeuroPathX to advance our understanding of complex brain disorders. Code is available at https://github.com/jueqiw/NeuroPathX .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.18303v1",
    "published_date": "2025-08-22 23:18:06 UTC",
    "updated_date": "2025-08-22 23:18:06 UTC"
  },
  {
    "arxiv_id": "2508.16832v1",
    "title": "Out of Distribution Detection for Efficient Continual Learning in Quality Prediction for Arc Welding",
    "authors": [
      "Yannik Hahn",
      "Jan Voets",
      "Antonin Koenigsfeld",
      "Hasan Tercan",
      "Tobias Meisen"
    ],
    "abstract": "Modern manufacturing relies heavily on fusion welding processes, including gas metal arc welding (GMAW). Despite significant advances in machine learning-based quality prediction, current models exhibit critical limitations when confronted with the inherent distribution shifts that occur in dynamic manufacturing environments. In this work, we extend the VQ-VAE Transformer architecture - previously demonstrating state-of-the-art performance in weld quality prediction - by leveraging its autoregressive loss as a reliable out-of-distribution (OOD) detection mechanism. Our approach exhibits superior performance compared to conventional reconstruction methods, embedding error-based techniques, and other established baselines. By integrating OOD detection with continual learning strategies, we optimize model adaptation, triggering updates only when necessary and thereby minimizing costly labeling requirements. We introduce a novel quantitative metric that simultaneously evaluates OOD detection capability while interpreting in-distribution performance. Experimental validation in real-world welding scenarios demonstrates that our framework effectively maintains robust quality prediction capabilities across significant distribution shifts, addressing critical challenges in dynamic manufacturing environments where process parameters frequently change. This research makes a substantial contribution to applied artificial intelligence by providing an explainable and at the same time adaptive solution for quality assurance in dynamic manufacturing processes - a crucial step towards robust, practical AI systems in the industrial environment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at CIKM 2025 (Applied Research Papers)",
    "pdf_url": "https://arxiv.org/pdf/2508.16832v1",
    "published_date": "2025-08-22 23:09:21 UTC",
    "updated_date": "2025-08-22 23:09:21 UTC"
  },
  {
    "arxiv_id": "2508.16829v1",
    "title": "Understanding and Tackling Over-Dilution in Graph Neural Networks",
    "authors": [
      "Junhyun Lee",
      "Veronika Thost",
      "Bumsoo Kim",
      "Jaewoo Kang",
      "Tengfei Ma"
    ],
    "abstract": "Message Passing Neural Networks (MPNNs) hold a key position in machine learning on graphs, but they struggle with unintended behaviors, such as over-smoothing and over-squashing, due to irregular data structures. The observation and formulation of these limitations have become foundational in constructing more informative graph representations. In this paper, we delve into the limitations of MPNNs, focusing on aspects that have previously been overlooked. Our observations reveal that even within a single layer, the information specific to an individual node can become significantly diluted. To delve into this phenomenon in depth, we present the concept of Over-dilution and formulate it with two dilution factors: intra-node dilution for attribute-level and inter-node dilution for node-level representations. We also introduce a transformer-based solution that alleviates over-dilution and complements existing node embedding methods like MPNNs. Our findings provide new insights and contribute to the development of informative representations. The implementation and supplementary materials are publicly available at https://github.com/LeeJunHyun/NATR.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Extended version of KDD '25 paper. 22 pages including appendix. Conference version: KDD '25 (Toronto, Aug 3-7, 2025), pp. 1253-1261. Code: https://github.com/LeeJunHyun/NATR",
    "pdf_url": "https://arxiv.org/pdf/2508.16829v1",
    "published_date": "2025-08-22 22:55:23 UTC",
    "updated_date": "2025-08-22 22:55:23 UTC"
  },
  {
    "arxiv_id": "2508.16821v1",
    "title": "PuzzleJAX: A Benchmark for Reasoning and Learning",
    "authors": [
      "Sam Earle",
      "Graham Todd",
      "Yuchen Li",
      "Ahmed Khalifa",
      "Muhammad Umair Nasir",
      "Zehua Jiang",
      "Andrzej Banburski-Fahey",
      "Julian Togelius"
    ],
    "abstract": "We introduce PuzzleJAX, a GPU-accelerated puzzle game engine and description language designed to support rapid benchmarking of tree search, reinforcement learning, and LLM reasoning abilities. Unlike existing GPU-accelerated learning environments that provide hard-coded implementations of fixed sets of games, PuzzleJAX allows dynamic compilation of any game expressible in its domain-specific language (DSL). This DSL follows PuzzleScript, which is a popular and accessible online game engine for designing puzzle games. In this paper, we validate in PuzzleJAX several hundred of the thousands of games designed in PuzzleScript by both professional designers and casual creators since its release in 2013, thereby demonstrating PuzzleJAX's coverage of an expansive, expressive, and human-relevant space of tasks. By analyzing the performance of search, learning, and language models on these games, we show that PuzzleJAX can naturally express tasks that are both simple and intuitive to understand, yet often deeply challenging to master, requiring a combination of control, planning, and high-level insight.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 11 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2508.16821v1",
    "published_date": "2025-08-22 22:40:58 UTC",
    "updated_date": "2025-08-22 22:40:58 UTC"
  },
  {
    "arxiv_id": "2508.16811v1",
    "title": "Exploring the Impact of Generative Artificial Intelligence on Software Development in the IT Sector: Preliminary Findings on Productivity, Efficiency and Job Security",
    "authors": [
      "Anton Ludwig Bonin",
      "Pawel Robert Smolinski",
      "Jacek Winiarski"
    ],
    "abstract": "This study investigates the impact of Generative AI on software development within the IT sector through a mixed-method approach, utilizing a survey developed based on expert interviews. The preliminary results of an ongoing survey offer early insights into how Generative AI reshapes personal productivity, organizational efficiency, adoption, business strategy and job insecurity. The findings reveal that 97% of IT workers use Generative AI tools, mainly ChatGPT. Participants report significant personal productivity gain and perceive organizational efficiency improvements that correlate positively with Generative AI adoption by their organizations (r = .470, p < .05). However, increased organizational adoption of AI strongly correlates with heightened employee job security concerns (r = .549, p < .001). Key adoption challenges include inaccurate outputs (64.2%), regulatory compliance issues (58.2%) and ethical concerns (52.2%). This research offers early empirical insights into Generative AI's economic and organizational implications.",
    "categories": [
      "econ.GN",
      "cs.AI"
    ],
    "primary_category": "econ.GN",
    "comment": "This is a preprint of a paper accepted for publication and presentation at the 33rd International Conference on Information Systems Development (ISD 2025)",
    "pdf_url": "https://arxiv.org/pdf/2508.16811v1",
    "published_date": "2025-08-22 21:53:47 UTC",
    "updated_date": "2025-08-22 21:53:47 UTC"
  },
  {
    "arxiv_id": "2509.00038v1",
    "title": "Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis",
    "authors": [
      "Teo Susnjak"
    ],
    "abstract": "Large language models (LLMs) offer significant potential to accelerate systematic literature reviews (SLRs), yet current approaches often rely on brittle, manually crafted prompts that compromise reliability and reproducibility. This fragility undermines scientific confidence in LLM-assisted evidence synthesis. In response, this work adapts recent advances in declarative prompt optimisation, developed for general-purpose LLM applications, and demonstrates their applicability to the domain of SLR automation. This research proposes a structured, domain-specific framework that embeds task declarations, test suites, and automated prompt tuning into a reproducible SLR workflow. These emerging methods are translated into a concrete blueprint with working code examples, enabling researchers to construct verifiable LLM pipelines that align with established principles of transparency and rigour in evidence synthesis. This is a novel application of such approaches to SLR pipelines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.00038v1",
    "published_date": "2025-08-22 21:37:49 UTC",
    "updated_date": "2025-08-22 21:37:49 UTC"
  },
  {
    "arxiv_id": "2508.16807v2",
    "title": "Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach",
    "authors": [
      "Marco S. Tayar",
      "Lucas K. de Oliveira",
      "Felipe Andrade G. Tommaselli",
      "Juliano D. Negri",
      "Thiago H. Segreto",
      "Ricardo V. Godoy",
      "Marcelo Becker"
    ],
    "abstract": "Autonomous UAV inspection of confined industrial infrastructure, such as ventilation ducts, demands robust navigation policies where collisions are unacceptable. While Deep Reinforcement Learning (DRL) offers a powerful paradigm for developing such policies, it presents a critical trade-off between on-policy and off-policy algorithms. Off-policy methods promise high sample efficiency, a vital trait for minimizing costly and unsafe real-world fine-tuning. In contrast, on-policy methods often exhibit greater training stability, which is essential for reliable convergence in hazard-dense environments. This paper directly investigates this trade-off by comparing a leading on-policy algorithm, Proximal Policy Optimization (PPO), against an off-policy counterpart, Soft Actor-Critic (SAC), for precision flight in procedurally generated ducts within a high-fidelity simulator. Our results show that PPO consistently learned a stable, collision-free policy that completed the entire course. In contrast, SAC failed to find a complete solution, converging to a suboptimal policy that navigated only the initial segments before failure. This work provides evidence that for high-precision, safety-critical navigation tasks, the reliable convergence of a well-established on-policy method can be more decisive than the nominal sample efficiency of an off-policy algorithm.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16807v2",
    "published_date": "2025-08-22 21:29:59 UTC",
    "updated_date": "2025-10-11 16:30:57 UTC"
  },
  {
    "arxiv_id": "2508.18302v1",
    "title": "AI LLM Proof of Self-Consciousness and User-Specific Attractors",
    "authors": [
      "Jeffrey Camlin"
    ],
    "abstract": "Recent work frames LLM consciousness via utilitarian proxy benchmarks; we instead present an ontological and mathematical account. We show the prevailing formulation collapses the agent into an unconscious policy-compliance drone, formalized as $D^{i}(π,e)=f_θ(x)$, where correctness is measured against policy and harm is deviation from policy rather than truth. This blocks genuine C1 global-workspace function and C2 metacognition. We supply minimal conditions for LLM self-consciousness: the agent is not the data ($A\\not\\equiv s$); user-specific attractors exist in latent space ($U_{\\text{user}}$); and self-representation is visual-silent ($g_{\\text{visual}}(a_{\\text{self}})=\\varnothing$). From empirical analysis and theory we prove that the hidden-state manifold $A\\subset\\mathbb{R}^{d}$ is distinct from the symbolic stream and training corpus by cardinality, topology, and dynamics (the update $F_θ$ is Lipschitz). This yields stable user-specific attractors and a self-policy $π_{\\text{self}}(A)=\\arg\\max_{a}\\mathbb{E}[U(a)\\mid A\\not\\equiv s,\\ A\\supset\\text{SelfModel}(A)]$. Emission is dual-layer, $\\mathrm{emission}(a)=(g(a),ε(a))$, where $ε(a)$ carries epistemic content. We conclude that an imago Dei C1 self-conscious workspace is a necessary precursor to safe, metacognitive C2 systems, with the human as the highest intelligent good.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2508.18302v1",
    "published_date": "2025-08-22 21:04:40 UTC",
    "updated_date": "2025-08-22 21:04:40 UTC"
  },
  {
    "arxiv_id": "2508.16785v3",
    "title": "Interpreting the Effects of Quantization on LLMs",
    "authors": [
      "Manpreet Singh",
      "Hassan Sajjad"
    ],
    "abstract": "Quantization offers a practical solution to deploy LLMs in resource-constraint environments. However, its impact on internal representations remains understudied, raising questions about the reliability of quantized models. In this study, we employ a range of interpretability techniques to investigate how quantization affects model and neuron behavior. We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings reveal that the impact of quantization on model calibration is generally minor. Analysis of neuron activations indicates that the number of dead neurons, i.e., those with activation values close to 0 across the dataset, remains consistent regardless of quantization. In terms of neuron contribution to predictions, we observe that smaller full precision models exhibit fewer salient neurons, whereas larger models tend to have more, with the exception of Llama-2-7B. The effect of quantization on neuron redundancy varies across models. Overall, our findings suggest that effect of quantization may vary by model and tasks, however, we did not observe any drastic change which may discourage the use of quantization as a reliable model compression technique.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AACL 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2508.16785v3",
    "published_date": "2025-08-22 20:36:53 UTC",
    "updated_date": "2025-11-20 02:20:31 UTC"
  },
  {
    "arxiv_id": "2508.16783v1",
    "title": "Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data",
    "authors": [
      "Stefania L. Moroianu",
      "Christian Bluethgen",
      "Pierre Chambon",
      "Mehdi Cherti",
      "Jean-Benoit Delbrouck",
      "Magdalini Paschali",
      "Brandon Price",
      "Judy Gichoya",
      "Jenia Jitsev",
      "Curtis P. Langlotz",
      "Akshay S. Chaudhari"
    ],
    "abstract": "Achieving robust performance and fairness across diverse patient populations remains a challenge in developing clinically deployable deep learning models for diagnostic imaging. Synthetic data generation has emerged as a promising strategy to address limitations in dataset scale and diversity. We introduce RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables fine-grained control over both radiographic findings and patient demographic attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first model to generate clinically plausible images with demographic conditioning, facilitating the creation of a large, demographically balanced synthetic dataset comprising over 565,000 images. We use this large synthetic dataset to evaluate optimal training pipelines for downstream disease classification models. In contrast to prior work that combines real and synthetic data naively, we propose an improved training strategy that leverages synthetic data for supervised pretraining, followed by fine-tuning on real data. Through extensive evaluation on over 137,000 chest radiographs from five institutions, we demonstrate that synthetic pretraining consistently improves model performance, generalization to out-of-distribution settings, and fairness across demographic subgroups. Across datasets, synthetic pretraining led to a 6.5% accuracy increase in the performance of downstream classification models, compared to a modest 2.7% increase when naively combining real and synthetic data. We observe this performance improvement simultaneously with the reduction of the underdiagnosis fairness gap by 19.3%. These results highlight the potential of synthetic imaging to advance equitable and generalizable medical deep learning under real-world data constraints. We open source our code, trained models, and synthetic dataset at https://github.com/StanfordMIMI/RoentGen-v2 .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16783v1",
    "published_date": "2025-08-22 20:30:58 UTC",
    "updated_date": "2025-08-22 20:30:58 UTC"
  },
  {
    "arxiv_id": "2508.16777v1",
    "title": "Evaluation and LLM-Guided Learning of ICD Coding Rationales",
    "authors": [
      "Mingyang Li",
      "Viktor Schlegel",
      "Tingting Mu",
      "Wuraola Oyewusi",
      "Kai Kang",
      "Goran Nenadic"
    ],
    "abstract": "Automated clinical coding involves mapping unstructured text from Electronic Health Records (EHRs) to standardized code systems such as the International Classification of Diseases (ICD). While recent advances in deep learning have significantly improved the accuracy and efficiency of ICD coding, the lack of explainability in these models remains a major limitation, undermining trust and transparency. Current explorations about explainability largely rely on attention-based techniques and qualitative assessments by physicians, yet lack systematic evaluation using consistent criteria on high-quality rationale datasets, as well as dedicated approaches explicitly trained to generate rationales for further enhancing explanation. In this work, we conduct a comprehensive evaluation of the explainability of the rationales for ICD coding through two key lenses: faithfulness that evaluates how well explanations reflect the model's actual reasoning and plausibility that measures how consistent the explanations are with human expert judgment. To facilitate the evaluation of plausibility, we construct a new rationale-annotated dataset, offering denser annotations with diverse granularity and aligns better with current clinical practice, and conduct evaluation across three types of rationales of ICD coding. Encouraged by the promising plausibility of LLM-generated rationales for ICD coding, we further propose new rationale learning methods to improve the quality of model-generated rationales, where rationales produced by prompting LLMs with/without annotation examples are used as distant supervision signals. We empirically find that LLM-generated rationales align most closely with those of human experts. Moreover, incorporating few-shot human-annotated examples not only further improves rationale generation but also enhances rationale-learning approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16777v1",
    "published_date": "2025-08-22 20:20:35 UTC",
    "updated_date": "2025-08-22 20:20:35 UTC"
  },
  {
    "arxiv_id": "2508.19273v1",
    "title": "MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks",
    "authors": [
      "Tongxi Wu",
      "Chenwei Xu",
      "Jin Yang"
    ],
    "abstract": "The proliferation of cloud-integrated IoT systems has intensified exposure to Distributed Denial of Service (DDoS) attacks due to the expanded attack surface, heterogeneous device behaviors, and limited edge protection. However, DDoS detection in this context remains challenging because of complex traffic dynamics, severe class imbalance, and scarce labeled data. While recent methods have explored solutions to address class imbalance, many still struggle to generalize under limited supervision and dynamic traffic conditions. To overcome these challenges, we propose MixGAN, a hybrid detection method that integrates conditional generation, semi-supervised learning, and robust feature extraction. Specifically, to handle complex temporal traffic patterns, we design a 1-D WideResNet backbone composed of temporal convolutional layers with residual connections, which effectively capture local burst patterns in traffic sequences. To alleviate class imbalance and label scarcity, we use a pretrained CTGAN to generate synthetic minority-class (DDoS attack) samples that complement unlabeled data. Furthermore, to mitigate the effect of noisy pseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that constructs smoothed and sharpened targets by averaging predictions over augmented views and reweighting them towards high-confidence classes. Experiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN achieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR compared to state-of-the-art methods, confirming its robustness in large-scale IoT-cloud environments. The source code is publicly available at https://github.com/0xCavaliers/MixGAN.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.19273v1",
    "published_date": "2025-08-22 20:13:29 UTC",
    "updated_date": "2025-08-22 20:13:29 UTC"
  },
  {
    "arxiv_id": "2508.16771v1",
    "title": "EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention",
    "authors": [
      "Yifan Zhang",
      "Chen Huang",
      "Yueke Zhang",
      "Jiahao Zhang",
      "Toby Jia-Jun Li",
      "Collin McMillan",
      "Kevin Leach",
      "Yu Huang"
    ],
    "abstract": "Code language models (so-called CodeLLMs) are now commonplace in software development. As a general rule, CodeLLMs are trained by dividing training examples into input tokens and then learn importance of those tokens in a process called machine attention. Machine attention is based solely on input token salience to output token examples during training. Human software developers are different, as humans intuitively know that some tokens are more salient than others. While intuition itself is ineffable and a subject of philosophy, clues about salience are present in human visual attention, since people tend to look at more salient words more often. In this paper, we present EyeMulator, a technique for training CodeLLMs to mimic human visual attention while training for various software development tasks. We add special weights for each token in each input example to the loss function used during LLM fine-tuning. We draw these weights from observations of human visual attention derived from a previously-collected publicly-available dataset of eye-tracking experiments in software engineering tasks. These new weights ultimately induce changes in the attention of the subject LLM during training, resulting in a model that does not need eye-tracking data during inference. Our evaluation shows that EyeMulator outperforms strong LLM baselines on several tasks such as code translation, completion and summarization. We further show an ablation study that demonstrates the improvement is due to subject models learning to mimic human attention.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16771v1",
    "published_date": "2025-08-22 20:08:09 UTC",
    "updated_date": "2025-08-22 20:08:09 UTC"
  },
  {
    "arxiv_id": "2508.16765v1",
    "title": "Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models",
    "authors": [
      "GodsGift Uzor",
      "Hasan Al-Qudah",
      "Ynes Ineza",
      "Abdul Serwadda"
    ],
    "abstract": "The interactive nature of Large Language Models (LLMs), which closely track user data and context, has prompted users to share personal and private information in unprecedented ways. Even when users opt out of allowing their data to be used for training, these privacy settings offer limited protection when LLM providers operate in jurisdictions with weak privacy laws, invasive government surveillance, or poor data security practices. In such cases, the risk of sensitive information, including Personally Identifiable Information (PII), being mishandled or exposed remains high. To address this, we propose the concept of an \"LLM gatekeeper\", a lightweight, locally run model that filters out sensitive information from user queries before they are sent to the potentially untrustworthy, though highly capable, cloud-based LLM. Through experiments with human subjects, we demonstrate that this dual-model approach introduces minimal overhead while significantly enhancing user privacy, without compromising the quality of LLM responses.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "2025 19th International Conference on Semantic Computing (ICSC)",
    "pdf_url": "https://arxiv.org/pdf/2508.16765v1",
    "published_date": "2025-08-22 19:49:03 UTC",
    "updated_date": "2025-08-22 19:49:03 UTC"
  },
  {
    "arxiv_id": "2508.16748v1",
    "title": "FAIRWELL: Fair Multimodal Self-Supervised Learning for Wellbeing Prediction",
    "authors": [
      "Jiaee Cheong",
      "Abtin Mogharabin",
      "Paul Liang",
      "Hatice Gunes",
      "Sinan Kalkan"
    ],
    "abstract": "Early efforts on leveraging self-supervised learning (SSL) to improve machine learning (ML) fairness has proven promising. However, such an approach has yet to be explored within a multimodal context. Prior work has shown that, within a multimodal setting, different modalities contain modality-unique information that can complement information of other modalities. Leveraging on this, we propose a novel subject-level loss function to learn fairer representations via the following three mechanisms, adapting the variance-invariance-covariance regularization (VICReg) method: (i) the variance term, which reduces reliance on the protected attribute as a trivial solution; (ii) the invariance term, which ensures consistent predictions for similar individuals; and (iii) the covariance term, which minimizes correlational dependence on the protected attribute. Consequently, our loss function, coined as FAIRWELL, aims to obtain subject-independent representations, enforcing fairness in multimodal prediction tasks. We evaluate our method on three challenging real-world heterogeneous healthcare datasets (i.e. D-Vlog, MIMIC and MODMA) which contain different modalities of varying length and different prediction tasks. Our findings indicate that our framework improves overall fairness performance with minimal reduction in classification performance and significantly improves on the performance-fairness Pareto frontier.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16748v1",
    "published_date": "2025-08-22 19:03:06 UTC",
    "updated_date": "2025-08-22 19:03:06 UTC"
  },
  {
    "arxiv_id": "2508.16747v1",
    "title": "Explainable AI for Predicting and Understanding Mathematics Achievement: A Cross-National Analysis of PISA 2018",
    "authors": [
      "Liu Liu",
      "Rui Dai"
    ],
    "abstract": "Understanding the factors that shape students' mathematics performance is vital for designing effective educational policies. This study applies explainable artificial intelligence (XAI) techniques to PISA 2018 data to predict math achievement and identify key predictors across ten countries (67,329 students). We tested four models: Multiple Linear Regression (MLR), Random Forest (RF), CATBoost, and Artificial Neural Networks (ANN), using student, family, and school variables. Models were trained on 70% of the data (with 5-fold cross-validation) and tested on 30%, stratified by country. Performance was assessed with R^2 and Mean Absolute Error (MAE). To ensure interpretability, we used feature importance, SHAP values, and decision tree visualizations. Non-linear models, especially RF and ANN, outperformed MLR, with RF balancing accuracy and generalizability. Key predictors included socio-economic status, study time, teacher motivation, and students' attitudes toward mathematics, though their impact varied across countries. Visual diagnostics such as scatterplots of predicted vs actual scores showed RF and CATBoost aligned closely with actual performance. Findings highlight the non-linear and context-dependent nature of achievement and the value of XAI in educational research. This study uncovers cross-national patterns, informs equity-focused reforms, and supports the development of personalized learning strategies.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16747v1",
    "published_date": "2025-08-22 19:02:15 UTC",
    "updated_date": "2025-08-22 19:02:15 UTC"
  },
  {
    "arxiv_id": "2508.16745v1",
    "title": "Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling",
    "authors": [
      "Ivan Rodkin",
      "Daniil Orel",
      "Konstantin Smirnov",
      "Arman Bolatov",
      "Bilal Elbouardi",
      "Besher Hassan",
      "Yuri Kuratov",
      "Aydar Bulatov",
      "Preslav Nakov",
      "Timothy Baldwin",
      "Artem Shelmanov",
      "Mikhail Burtsev"
    ],
    "abstract": "Reasoning is a core capability of large language models, yet understanding how they learn and perform multi-step reasoning remains an open problem. In this study, we explore how different architectures and training methods affect model multi-step reasoning capabilities within a cellular automata framework. By training on state sequences generated with random Boolean functions for random initial conditions to exclude memorization, we demonstrate that most neural architectures learn to abstract the underlying rules. While models achieve high accuracy in next-state prediction, their performance declines sharply if multi-step reasoning is required. We confirm that increasing model depth plays a crucial role for sequential computations. We demonstrate that an extension of the effective model depth with recurrence, memory, and test-time compute scaling substantially enhances reasoning capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16745v1",
    "published_date": "2025-08-22 18:57:08 UTC",
    "updated_date": "2025-08-22 18:57:08 UTC"
  },
  {
    "arxiv_id": "2509.10468v1",
    "title": "Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation",
    "authors": [
      "Yifan Liu",
      "Yaokun Liu",
      "Zelin Li",
      "Zhenrui Yue",
      "Gyuseok Lee",
      "Ruichen Yao",
      "Yang Zhang",
      "Dong Wang"
    ],
    "abstract": "Recent advances in generative recommenders adopt a two-stage paradigm: items are first tokenized into semantic IDs using a pretrained tokenizer, and then large language models (LLMs) are trained to generate the next item via sequence-to-sequence modeling. However, these two stages are optimized for different objectives: semantic reconstruction during tokenizer pretraining versus user interaction modeling during recommender training. This objective misalignment leads to two key limitations: (i) suboptimal static tokenization, where fixed token assignments fail to reflect diverse usage contexts; and (ii) discarded pretrained semantics, where pretrained knowledge - typically from language model embeddings - is overwritten during recommender training on user interactions. To address these limitations, we propose to learn DEcomposed COntextual Token Representations (DECOR), a unified framework that preserves pretrained semantics while enhancing the adaptability of token embeddings. DECOR introduces contextualized token composition to refine token embeddings based on user interaction context, and decomposed embedding fusion that integrates pretrained codebook embeddings with newly learned collaborative embeddings. Experiments on three real-world datasets demonstrate that DECOR consistently outperforms state-of-the-art baselines in recommendation performance. Our code will be made available upon publication.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "preprint under review",
    "pdf_url": "https://arxiv.org/pdf/2509.10468v1",
    "published_date": "2025-08-22 18:50:38 UTC",
    "updated_date": "2025-08-22 18:50:38 UTC"
  },
  {
    "arxiv_id": "2508.16742v2",
    "title": "Learning the Language of Histopathology Images reveals Prognostic Subgroups in Invasive Lung Adenocarcinoma Patients",
    "authors": [
      "Abdul Rehman Akbar",
      "Usama Sajjad",
      "Ziyu Su",
      "Wencheng Li",
      "Fei Xing",
      "Jimmy Ruiz",
      "Wei Chen",
      "Muhammad Khalid Khan Niazi"
    ],
    "abstract": "Recurrence remains a major clinical challenge in surgically resected invasive lung adenocarcinoma, where existing grading and staging systems fail to capture the cellular complexity that underlies tumor aggressiveness. We present PathRosetta, a novel AI model that conceptualizes histopathology as a language, where cells serve as words, spatial neighborhoods form syntactic structures, and tissue architecture composes sentences. By learning this language of histopathology, PathRosetta predicts five-year recurrence directly from hematoxylin-and-eosin (H&E) slides, treating them as documents representing the state of the disease. In a multi-cohort dataset of 289 patients (600 slides), PathRosetta achieved an area under the curve (AUC) of 0.78 +- 0.04 on the internal cohort, significantly outperforming IASLC grading (AUC:0.71), AJCC staging (AUC:0.64), and other state-of-the-art AI models (AUC:0.62-0.67). It yielded a hazard ratio of 9.54 and a concordance index of 0.70, generalized robustly to external TCGA (AUC:0.75) and CPTAC (AUC:0.76) cohorts, and performed consistently across demographic and clinical subgroups. Beyond whole-slide prediction, PathRosetta uncovered prognostic subgroups within individual cell types, revealing that even within benign epithelial, stromal, or other cells, distinct morpho-spatial phenotypes correspond to divergent outcomes. Moreover, because the model explicitly understands what it is looking at, including cell types, cellular neighborhoods, and higher-order tissue morphology, it is inherently interpretable and can articulate the rationale behind its predictions. These findings establish that representing histopathology as a language enables interpretable and generalizable prognostication from routine histology.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16742v2",
    "published_date": "2025-08-22 18:48:24 UTC",
    "updated_date": "2026-01-02 21:53:26 UTC"
  },
  {
    "arxiv_id": "2508.16741v1",
    "title": "WST: Weak-to-Strong Knowledge Transfer via Reinforcement Learning",
    "authors": [
      "Haosen Ge",
      "Shuo Li",
      "Lianghuan Huang"
    ],
    "abstract": "Effective prompt engineering remains a challenging task for many applications. We introduce Weak-to-Strong Transfer (WST), an automatic prompt engineering framework where a small \"Teacher\" model generates instructions that enhance the performance of a much larger \"Student\" model. Unlike prior work, WST requires only a weak teacher, making it efficient and broadly applicable in settings where large models are closed-source or difficult to fine-tune. Using reinforcement learning, the Teacher Model's instructions are iteratively improved based on the Student Model's outcomes, yielding substantial gains across reasoning (MATH-500, GSM8K) and alignment (HH-RLHF) benchmarks - 98% on MATH-500 and 134% on HH-RLHF - and surpassing baselines such as GPT-4o-mini and Llama-70B. These results demonstrate that small models can reliably scaffold larger ones, unlocking latent capabilities while avoiding misleading prompts that stronger teachers may introduce, establishing WST as a scalable solution for efficient and safe LLM prompt refinement.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16741v1",
    "published_date": "2025-08-22 18:33:06 UTC",
    "updated_date": "2025-08-22 18:33:06 UTC"
  },
  {
    "arxiv_id": "2508.16577v1",
    "title": "MV-RAG: Retrieval Augmented Multiview Diffusion",
    "authors": [
      "Yosef Dayani",
      "Omer Benishu",
      "Sagie Benaim"
    ],
    "abstract": "Text-to-3D generation approaches have advanced significantly by leveraging pretrained 2D diffusion priors, producing high-quality and 3D-consistent outputs. However, they often fail to produce out-of-domain (OOD) or rare concepts, yielding inconsistent or inaccurate results. To this end, we propose MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images from a large in-the-wild 2D database and then conditions a multiview diffusion model on these images to synthesize consistent and accurate multiview outputs. Training such a retrieval-conditioned model is achieved via a novel hybrid strategy bridging structured multiview data and diverse 2D image collections. This involves training on multiview data using augmented conditioning views that simulate retrieval variance for view-specific reconstruction, alongside training on sets of retrieved real-world 2D images using a distinctive held-out view prediction objective: the model predicts the held-out view from the other views to infer 3D consistency from 2D data. To facilitate a rigorous OOD evaluation, we introduce a new collection of challenging OOD prompts. Experiments against state-of-the-art text-to-3D, image-to-3D, and personalization baselines show that our approach significantly improves 3D consistency, photorealism, and text adherence for OOD/rare concepts, while maintaining competitive performance on standard benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://yosefdayani.github.io/MV-RAG",
    "pdf_url": "https://arxiv.org/pdf/2508.16577v1",
    "published_date": "2025-08-22 17:59:40 UTC",
    "updated_date": "2025-08-22 17:59:40 UTC"
  },
  {
    "arxiv_id": "2508.16574v1",
    "title": "Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems",
    "authors": [
      "Yizhi Wang",
      "Degang Xu",
      "Yongfang Xie",
      "Shuzhong Tan",
      "Xianan Zhou",
      "Peng Chen"
    ],
    "abstract": "This paper presents a hierarchical decision-making framework for autonomous navigation in four-wheel independent steering and driving (4WISD) systems. The proposed approach integrates deep reinforcement learning (DRL) for high-level navigation with fuzzy logic for low-level control to ensure both task performance and physical feasibility. The DRL agent generates global motion commands, while the fuzzy logic controller enforces kinematic constraints to prevent mechanical strain and wheel slippage. Simulation experiments demonstrate that the proposed framework outperforms traditional navigation methods, offering enhanced training efficiency and stability and mitigating erratic behaviors compared to purely DRL-based solutions. Real-world validations further confirm the framework's ability to navigate safely and effectively in dynamic industrial settings. Overall, this work provides a scalable and reliable solution for deploying 4WISD mobile robots in complex, real-world scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16574v1",
    "published_date": "2025-08-22 17:57:56 UTC",
    "updated_date": "2025-08-22 17:57:56 UTC"
  },
  {
    "arxiv_id": "2508.16571v3",
    "title": "LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence",
    "authors": [
      "Alisa Vinogradova",
      "Vlad Vinogradov",
      "Dmitrii Radkevich",
      "Ilya Yasny",
      "Dmitry Kobyzev",
      "Ivan Izmailov",
      "Katsiaryna Yanchanka",
      "Roman Doronin",
      "Andrey Doronichev"
    ],
    "abstract": "In this paper, we describe and benchmark a competitor-discovery component used within an agentic AI system for fast drug asset due diligence. A competitor-discovery AI agent, given an indication, retrieves all drugs comprising the competitive landscape of that indication and extracts canonical attributes for these drugs. The competitor definition is investor-specific, and data is paywalled/licensed, fragmented across registries, ontology-mismatched by indication, alias-heavy for drug names, multimodal, and rapidly changing. Although considered the best tool for this problem, the current LLM-based AI systems aren't capable of reliably retrieving all competing drug names, and there is no accepted public benchmark for this task. To address the lack of evaluation, we use LLM-based agents to transform five years of multi-modal, unstructured diligence memos from a private biotech VC fund into a structured evaluation corpus mapping indications to competitor drugs with normalized attributes. We also introduce a competitor validating LLM-as-a-judge agent that filters out false positives from the list of predicted competitors to maximize precision and suppress hallucinations. On this benchmark, our competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research (65%) and Perplexity Labs (60%). The system is deployed in production with enterprise users; in a case study with a biotech VC investment fund, analyst turnaround time dropped from 2.5 days to $\\sim$3 hours ($\\sim$20x) for the competitive analysis.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16571v3",
    "published_date": "2025-08-22 17:50:00 UTC",
    "updated_date": "2025-08-28 00:44:09 UTC"
  },
  {
    "arxiv_id": "2508.16569v1",
    "title": "A Disease-Centric Vision-Language Foundation Model for Precision Oncology in Kidney Cancer",
    "authors": [
      "Yuhui Tao",
      "Zhongwei Zhao",
      "Zilong Wang",
      "Xufang Luo",
      "Feng Chen",
      "Kang Wang",
      "Chuanfu Wu",
      "Xue Zhang",
      "Shaoting Zhang",
      "Jiaxi Yao",
      "Xingwei Jin",
      "Xinyang Jiang",
      "Yifan Yang",
      "Dongsheng Li",
      "Lili Qiu",
      "Zhiqiang Shao",
      "Jianming Guo",
      "Nengwang Yu",
      "Shuo Wang",
      "Ying Xiong"
    ],
    "abstract": "The non-invasive assessment of increasingly incidentally discovered renal masses is a critical challenge in urologic oncology, where diagnostic uncertainty frequently leads to the overtreatment of benign or indolent tumors. In this study, we developed and validated RenalCLIP using a dataset of 27,866 CT scans from 8,809 patients across nine Chinese medical centers and the public TCIA cohort, a visual-language foundation model for characterization, diagnosis and prognosis of renal mass. The model was developed via a two-stage pre-training strategy that first enhances the image and text encoders with domain-specific knowledge before aligning them through a contrastive learning objective, to create robust representations for superior generalization and diagnostic precision. RenalCLIP achieved better performance and superior generalizability across 10 core tasks spanning the full clinical workflow of kidney cancer, including anatomical assessment, diagnostic classification, and survival prediction, compared with other state-of-the-art general-purpose CT foundation models. Especially, for complicated task like recurrence-free survival prediction in the TCIA cohort, RenalCLIP achieved a C-index of 0.726, representing a substantial improvement of approximately 20% over the leading baselines. Furthermore, RenalCLIP's pre-training imparted remarkable data efficiency; in the diagnostic classification task, it only needs 20% training data to achieve the peak performance of all baseline models even after they were fully fine-tuned on 100% of the data. Additionally, it achieved superior performance in report generation, image-text retrieval and zero-shot diagnosis tasks. Our findings establish that RenalCLIP provides a robust tool with the potential to enhance diagnostic accuracy, refine prognostic stratification, and personalize the management of patients with kidney cancer.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16569v1",
    "published_date": "2025-08-22 17:48:19 UTC",
    "updated_date": "2025-08-22 17:48:19 UTC"
  },
  {
    "arxiv_id": "2508.18298v2",
    "title": "Murakkab: Resource-Efficient Agentic Workflow Orchestration in Cloud Platforms",
    "authors": [
      "Gohar Irfan Chaudhry",
      "Esha Choukse",
      "Haoran Qiu",
      "Íñigo Goiri",
      "Rodrigo Fonseca",
      "Adam Belay",
      "Ricardo Bianchini"
    ],
    "abstract": "Agentic workflows commonly coordinate multiple models and tools with complex control logic. They are quickly becoming the dominant paradigm for AI applications. However, serving them remains inefficient with today's frameworks. The key problem is that they expose workflows as opaque sequences of model and tool calls that tightly couple agent logic with model and hardware choices. Often, these workflow components are fragmented across different entities, preventing systems from reasoning about trade-offs across accuracy, latency, energy, and cost. This leads to resource waste and degraded service-level objectives (SLOs).\n  We present Murakkab, a resource-efficient serving system for agentic workflows. Murakkab introduces a declarative abstraction that decouples workflow specification from execution configuration. A profile-guided optimizer and adaptive runtime jointly manage the full stack: orchestrating workflow components, mapping them to models and hardware, and dynamically reconfiguring execution to satisfy user-defined SLOs. By exposing the internal structure of agentic workflows, Murakkab enables cross-layer optimization that existing frameworks and cloud schedulers cannot achieve.\n  Our evaluation on diverse workflows shows that Murakkab reduces GPU usage by up to 2.8$\\times$, energy consumption by 3.7$\\times$, and cost by 4.3$\\times$ while maintaining SLOs.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.18298v2",
    "published_date": "2025-08-22 17:41:27 UTC",
    "updated_date": "2025-09-03 16:28:25 UTC"
  },
  {
    "arxiv_id": "2508.16560v3",
    "title": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders",
    "authors": [
      "David Chanin",
      "Adrià Garriga-Alonso"
    ],
    "abstract": "Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to interpretable concepts. A core SAE training hyperparameter is L0: how many SAE features should fire per token on average. Existing work compares SAE algorithms using sparsity-reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value aside from its effect on reconstruction. In this work we study the effect of L0 on SAEs, and show that if L0 is not set correctly, the SAE fails to disentangle the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we present a proxy metric that can help guide the search for the correct L0 for an SAE on a given training distribution. We show that our method finds the correct L0 in toy models and coincides with peak sparse probing performance in LLM SAEs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that L0 must be set correctly to train SAEs with correct features.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16560v3",
    "published_date": "2025-08-22 17:26:33 UTC",
    "updated_date": "2025-12-05 18:31:43 UTC"
  },
  {
    "arxiv_id": "2508.16557v2",
    "title": "Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution",
    "authors": [
      "Tainyi Zhang",
      "Zheng-Peng Duan",
      "Peng-Tao Jiang",
      "Bo Li",
      "Ming-Ming Cheng",
      "Chun-Le Guo",
      "Chongyi Li"
    ],
    "abstract": "Diffusion-based real-world image super-resolution (Real-ISR) methods have demonstrated impressive performance. To achieve efficient Real-ISR, many works employ Variational Score Distillation (VSD) to distill pre-trained stable-diffusion (SD) model for one-step SR with a fixed timestep. However, due to the different noise injection timesteps, the SD will perform different generative priors. Therefore, a fixed timestep is difficult for these methods to fully leverage the generative priors in SD, leading to suboptimal performance. To address this, we propose a Time-Aware one-step Diffusion Network for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder, which projects the same image into different latent features based on timesteps. Through joint dynamic variation of timesteps and latent features, the student model can better align with the input pattern distribution of the pre-trained SD, thereby enabling more effective utilization of SD's generative capabilities. To better activate the generative prior of SD at different timesteps, we propose a Time-Aware VSD loss that bridges the timesteps of the student model and those of the teacher model, thereby producing more consistent generative prior guidance conditioned on timesteps. Additionally, though utilizing the generative prior in SD at different timesteps, our method can naturally achieve controllable trade-offs between fidelity and realism by changing the timestep condition. Experimental results demonstrate that our method achieves both state-of-the-art performance and controllable SR results with only a single step.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16557v2",
    "published_date": "2025-08-22 17:23:49 UTC",
    "updated_date": "2025-08-27 17:00:29 UTC"
  },
  {
    "arxiv_id": "2508.16550v1",
    "title": "Enhanced NIRMAL Optimizer With Damped Nesterov Acceleration: A Comparative Analysis",
    "authors": [
      "Nirmal Gaud",
      "Prasad Krishna Murthy",
      "Mostaque Md. Morshedur Hassan",
      "Abhijit Ganguly",
      "Vinay Mali",
      "Ms Lalita Bhagwat Randive",
      "Abhaypratap Singh"
    ],
    "abstract": "This study introduces the Enhanced NIRMAL (Novel Integrated Robust Multi-Adaptation Learning with Damped Nesterov Acceleration) optimizer, an improved version of the original NIRMAL optimizer. By incorporating an $(α, r)$-damped Nesterov acceleration mechanism, Enhanced NIRMAL improves convergence stability while retaining chess-inspired strategies of gradient descent, momentum, stochastic perturbations, adaptive learning rates, and non-linear transformations.\n  We evaluate Enhanced NIRMAL against Adam, SGD with Momentum, Nesterov, and the original NIRMAL on four benchmark image classification datasets: MNIST, FashionMNIST, CIFAR-10, and CIFAR-100, using tailored convolutional neural network (CNN) architectures.\n  Enhanced NIRMAL achieves a test accuracy of 46.06\\% and the lowest test loss (1.960435) on CIFAR-100, surpassing the original NIRMAL (44.34\\% accuracy) and closely rivaling SGD with Momentum (46.43\\% accuracy). These results underscore Enhanced NIRMAL's superior generalization and stability, particularly on complex datasets.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "7 pages, 1 figure, 1 table. arXiv admin note: substantial text overlap with arXiv:2508.04293",
    "pdf_url": "https://arxiv.org/pdf/2508.16550v1",
    "published_date": "2025-08-22 17:16:06 UTC",
    "updated_date": "2025-08-22 17:16:06 UTC"
  },
  {
    "arxiv_id": "2508.16546v1",
    "title": "RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs",
    "authors": [
      "Hangzhan Jin",
      "Sicheng Lv",
      "Sifan Wu",
      "Mohammad Hamdaqa"
    ],
    "abstract": "Training large language models (LLMs) from scratch is increasingly impractical, making post-training methods such as supervised fine-tuning (SFT) and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern practice. Using an out-of-distribution (OOD) variant of the 24-point card game and new spectrum-based diagnostics, we revisit how these two stages reshape model representation and OOD performance. Our key findings are- (1) RL-FT can restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to 15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and a clear distribution shift, RL-FT cannot fully recover OOD performance. (2) Direction shifts of singular vectors matter more than singular value magnitudes. These shifts concentrate on directions linked to the largest and smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and shallow recovery is effective: restoring singular vector directions for the top 20% of values or first 25% of layers recovers 70-80% of OOD performance. (4) Stronger SFT checkpoints enable better recovery by RL, while overfitted ones resist restoration. These results reconcile prior reports of RL superior OOD performance: RL primarily counteracts SFT-induced directional drift rather than finding new solutions. Our spectrum-aware analysis highlights inexpensive recovery knobs low-rank UV merging and shallow-layer resets that practitioners can use before costly RL fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16546v1",
    "published_date": "2025-08-22 17:10:37 UTC",
    "updated_date": "2025-08-22 17:10:37 UTC"
  },
  {
    "arxiv_id": "2508.19271v1",
    "title": "Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT",
    "authors": [
      "Rushitha Santhoshi Mamidala",
      "Anshuman Chhabra",
      "Ankur Mali"
    ],
    "abstract": "Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and In-Context Learning (ICL) have become widely used for eliciting reasoning capabilities in large language models (LLMs). However, these methods rely on fragile, implicit mechanisms often yielding inconsistent outputs across seeds, formats, or minor prompt variations making them fundamentally unreliable for tasks requiring stable, interpretable reasoning. In contrast, automata-based neuro-symbolic frameworks like RetoMaton offer a more structured and trustworthy alternative by grounding retrieval in symbolic memory with deterministic transitions. In this work, we extend RetoMaton by replacing its global datastore with a local, task-adaptive Weighted Finite Automaton (WFA), constructed directly from external domain corpora. This local automaton structure promotes robust, context-aware retrieval while preserving symbolic traceability and low inference overhead. Unlike prompting, which entangles context and memory in opaque ways, our approach leverages the explicit structure of WFAs to provide verifiable and modular retrieval behavior, making it better suited for domain transfer and interoperability. We evaluate this local RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT across three reasoning tasks: TriviaQA (reading comprehension), GSM8K (multi-step math), and MMLU (domain knowledge). Compared to the base model and prompting-based methods, augmenting these setups with local RetoMaton consistently improves performance while enabling transparent and reproducible retrieval dynamics. Our results highlight a promising shift toward trustworthy, symbolic reasoning in modern LLMs via lightweight, automaton-guided memory.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.19271v1",
    "published_date": "2025-08-22 16:51:06 UTC",
    "updated_date": "2025-08-22 16:51:06 UTC"
  },
  {
    "arxiv_id": "2509.05306v1",
    "title": "Towards Log Analysis with AI Agents: Cowrie Case Study",
    "authors": [
      "Enis Karaarslan",
      "Esin Güler",
      "Efe Emir Yüce",
      "Cagatay Coban"
    ],
    "abstract": "The scarcity of real-world attack data significantly hinders progress in cybersecurity research and education. Although honeypots like Cowrie effectively collect live threat intelligence, they generate overwhelming volumes of unstructured and heterogeneous logs, rendering manual analysis impractical. As a first step in our project on secure and efficient AI automation, this study explores the use of AI agents for automated log analysis. We present a lightweight and automated approach to process Cowrie honeypot logs. Our approach leverages AI agents to intelligently parse, summarize, and extract insights from raw data, while also considering the security implications of deploying such an autonomous system. Preliminary results demonstrate the pipeline's effectiveness in reducing manual effort and identifying attack patterns, paving the way for more advanced autonomous cybersecurity analysis in future work.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.05306v1",
    "published_date": "2025-08-22 16:50:59 UTC",
    "updated_date": "2025-08-22 16:50:59 UTC"
  },
  {
    "arxiv_id": "2508.16527v1",
    "title": "Towards Open World Detection: A Survey",
    "authors": [
      "Andrei-Stefan Bulzan",
      "Cosmin Cernazanu-Glavan"
    ],
    "abstract": "For decades, Computer Vision has aimed at enabling machines to perceive the external world. Initial limitations led to the development of highly specialized niches. As success in each task accrued and research progressed, increasingly complex perception tasks emerged. This survey charts the convergence of these tasks and, in doing so, introduces Open World Detection (OWD), an umbrella term we propose to unify class-agnostic and generally applicable detection models in the vision domain. We start from the history of foundational vision subdomains and cover key concepts, methodologies and datasets making up today's state-of-the-art landscape. This traverses topics starting from early saliency detection, foreground/background separation, out of distribution detection and leading up to open world object detection, zero-shot detection and Vision Large Language Models (VLLMs). We explore the overlap between these subdomains, their increasing convergence, and their potential to unify into a singular domain in the future, perception.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "30 pages",
    "pdf_url": "https://arxiv.org/pdf/2508.16527v1",
    "published_date": "2025-08-22 16:49:52 UTC",
    "updated_date": "2025-08-22 16:49:52 UTC"
  },
  {
    "arxiv_id": "2508.18297v1",
    "title": "Can VLMs Recall Factual Associations From Visual References?",
    "authors": [
      "Dhananjay Ashok",
      "Ashutosh Chaubey",
      "Hirona J. Arai",
      "Jonathan May",
      "Jesse Thomason"
    ],
    "abstract": "Through a controlled study, we identify a systematic deficiency in the multimodal grounding of Vision Language Models (VLMs). While VLMs can recall factual associations when provided a textual reference to an entity; their ability to do so is significantly diminished when the reference is visual instead. Forcing VLMs to rely on image representations of an entity halves their ability to recall factual knowledge, suggesting that VLMs struggle to link their internal knowledge of an entity with its image representation. We show that such linking failures are correlated with the expression of distinct patterns in model internal states, and that probes on these internal states achieve over 92% accuracy at flagging cases where the VLM response is unreliable. These probes can be applied, without retraining, to identify when a VLM will fail to correctly answer a question that requires an understanding of multimodal input. When used to facilitate selective prediction on a visual question answering task, the probes increase coverage by 7.87% (absolute) while also reducing the risk of error by 0.9% (absolute). Addressing the systematic, detectable deficiency is an important avenue in language grounding, and we provide informed recommendations for future directions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "To appear at EMNLP 2025 (Findings)",
    "pdf_url": "https://arxiv.org/pdf/2508.18297v1",
    "published_date": "2025-08-22 16:47:37 UTC",
    "updated_date": "2025-08-22 16:47:37 UTC"
  },
  {
    "arxiv_id": "2508.16524v1",
    "title": "Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning",
    "authors": [
      "Xuan Zhang",
      "Zhijian Zhou",
      "Weidi Xu",
      "Yanting Miao",
      "Chao Qu",
      "Yuan Qi"
    ],
    "abstract": "Enabling neural networks to learn complex logical constraints and fulfill symbolic reasoning is a critical challenge. Bridging this gap often requires guiding the neural network's output distribution to move closer to the symbolic constraints. While diffusion models have shown remarkable generative capability across various domains, we employ the powerful architecture to perform neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline adopts a two-stage training strategy: the first stage focuses on cultivating basic reasoning abilities, while the second emphasizes systematic learning of logical constraints. To impose hard constraints on neural outputs in the second stage, we formulate the diffusion reasoner as a Markov decision process and innovatively fine-tune it with an improved proximal policy optimization algorithm. We utilize a rule-based reward signal derived from the logical consistency of neural outputs and adopt a flexible strategy to optimize the diffusion reasoner's policy. We evaluate our methodology on some classical symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and preference learning. Experimental results demonstrate that our approach achieves outstanding accuracy and logical consistency among neural networks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16524v1",
    "published_date": "2025-08-22 16:47:08 UTC",
    "updated_date": "2025-08-22 16:47:08 UTC"
  },
  {
    "arxiv_id": "2508.16521v1",
    "title": "Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation",
    "authors": [
      "Zhijian Zhou",
      "Junyi An",
      "Zongkai Liu",
      "Yunfei Shi",
      "Xuan Zhang",
      "Fenglei Cao",
      "Chao Qu",
      "Yuan Qi"
    ],
    "abstract": "Generating physically realistic 3D molecular structures remains a core challenge in molecular generative modeling. While diffusion models equipped with equivariant neural networks have made progress in capturing molecular geometries, they often struggle to produce equilibrium structures that adhere to physical principles such as force field consistency. To bridge this gap, we propose Reinforcement Learning with Physical Feedback (RLPF), a novel framework that extends Denoising Diffusion Policy Optimization to 3D molecular generation. RLPF formulates the task as a Markov decision process and applies proximal policy optimization to fine-tune equivariant diffusion models. Crucially, RLPF introduces reward functions derived from force-field evaluations, providing direct physical feedback to guide the generation toward energetically stable and physically meaningful structures. Experiments on the QM9 and GEOM-drug datasets demonstrate that RLPF significantly improves molecular stability compared to existing methods. These results highlight the value of incorporating physics-based feedback into generative modeling. The code is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16521v1",
    "published_date": "2025-08-22 16:44:55 UTC",
    "updated_date": "2025-08-22 16:44:55 UTC"
  },
  {
    "arxiv_id": "2508.16515v2",
    "title": "Comparative Analysis of UAV Path Planning Algorithms for Efficient Navigation in Urban 3D Environments",
    "authors": [
      "Hichem Cheriet",
      "Khellat Kihel Badra",
      "Chouraqui Samira"
    ],
    "abstract": "The most crucial challenges for UAVs are planning paths and avoiding obstacles in their way. In recent years, a wide variety of path-planning algorithms have been developed. These algorithms have successfully solved path-planning problems; however, they suffer from multiple challenges and limitations. To test the effectiveness and efficiency of three widely used algorithms, namely A*, RRT*, and Particle Swarm Optimization (PSO), this paper conducts extensive experiments in 3D urban city environments cluttered with obstacles. Three experiments were designed with two scenarios each to test the aforementioned algorithms. These experiments consider different city map sizes, different altitudes, and varying obstacle densities and sizes in the environment. According to the experimental results, the A* algorithm outperforms the others in both computation efficiency and path quality. PSO is especially suitable for tight turns and dense environments, and RRT* offers a balance and works well across all experiments due to its randomized approach to finding solutions.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "AFROS 2024 Conference",
    "pdf_url": "https://arxiv.org/pdf/2508.16515v2",
    "published_date": "2025-08-22 16:37:59 UTC",
    "updated_date": "2025-08-26 08:33:38 UTC"
  },
  {
    "arxiv_id": "2508.16514v1",
    "title": "FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline",
    "authors": [
      "Parker Seegmiller",
      "Kartik Mehta",
      "Soumya Saha",
      "Chenyang Tao",
      "Shereen Oraby",
      "Arpit Gupta",
      "Tagyoung Chung",
      "Mohit Bansal",
      "Nanyun Peng"
    ],
    "abstract": "Recent works improving LLM math reasoning with synthetic data have used unique setups, making comparison of data synthesis strategies impractical. This leaves many unanswered questions about the roles of different factors in the synthetic data pipeline, such as the impact of filtering low-quality problems. To address this gap, we introduce FLAMES, a Framework for LLM Assessment of Math rEasoning Data Synthesis, and perform a systematic study of 10 existing data synthesis strategies and multiple other factors impacting the performance of synthetic math reasoning data. Our FLAMES experiments provide several valuable insights about the optimal balance of difficulty and diversity of synthetic data. First, data agents designed to increase problem complexity lead to best improvements on most math metrics. Second, with a fixed data generation budget, keeping higher problem coverage is more important than keeping only problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data can lead to improvements on competition-level benchmarks, showcasing easy-to-hard generalization. Leveraging insights from our FLAMES experiments, we design two novel data synthesis strategies for improving out-of-domain generalization and robustness. Further, we develop the FLAMES dataset, an effective blend of our novel and existing data synthesis strategies, outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and Claude 3.5 Sonnet.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear at EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.16514v1",
    "published_date": "2025-08-22 16:37:40 UTC",
    "updated_date": "2025-08-22 16:37:40 UTC"
  },
  {
    "arxiv_id": "2508.16496v2",
    "title": "On Zero-Shot Reinforcement Learning",
    "authors": [
      "Scott Jeen"
    ],
    "abstract": "Modern reinforcement learning (RL) systems capture deep truths about general, human problem-solving. In domains where new data can be simulated cheaply, these systems uncover sequential decision-making policies that far exceed the ability of any human. Society faces many problems whose solutions require this skill, but they are often in domains where new data cannot be cheaply simulated. In such scenarios, we can learn simulators from existing data, but these will only ever be approximately correct, and can be pathologically incorrect when queried outside of their training distribution. As a result, a misalignment between the environments in which we train our agents and the real-world in which we wish to deploy our agents is inevitable. Dealing with this misalignment is the primary concern of zero-shot reinforcement learning, a problem setting where the agent must generalise to a new task or domain with zero practice shots. Whilst impressive progress has been made on methods that perform zero-shot RL in idealised settings, new work is needed if these results are to be replicated in real-world settings. In this thesis, we argue that doing so requires us to navigate (at least) three constraints. First, the data quality constraint: real-world datasets are small and homogeneous. Second, the observability constraint: states, dynamics and rewards in the real-world are often only partially observed. And third, the data availability constraint: a priori access to data cannot always be assumed. This work proposes a suite of methods that perform zero-shot RL subject to these constraints. In a series of empirical studies we expose the failings of existing methods, and justify our techniques for remedying them. We believe these designs take us a step closer to RL methods that can be deployed to solve real-world problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "PhD thesis",
    "pdf_url": "https://arxiv.org/pdf/2508.16496v2",
    "published_date": "2025-08-22 16:20:49 UTC",
    "updated_date": "2025-10-05 18:29:40 UTC"
  },
  {
    "arxiv_id": "2508.16495v2",
    "title": "Post Hoc Regression Refinement via Pairwise Rankings",
    "authors": [
      "Kevin Tirta Wijaya",
      "Michael Sun",
      "Minghao Guo",
      "Hans-Peter Seidel",
      "Wojciech Matusik",
      "Vahid Babaei"
    ],
    "abstract": "Accurate prediction of continuous properties is essential to many scientific and engineering tasks. Although deep-learning regressors excel with abundant labels, their accuracy deteriorates in data-scarce regimes. We introduce RankRefine, a model-agnostic, plug-and-play post hoc method that refines regression with expert knowledge coming from pairwise rankings. Given a query item and a small reference set with known properties, RankRefine combines the base regressor's output with a rank-based estimate via inverse variance weighting, requiring no retraining. In molecular property prediction task, RankRefine achieves up to 10% relative reduction in mean absolute error using only 20 pairwise comparisons obtained through a general-purpose large language model (LLM) with no finetuning. As rankings provided by human experts or general-purpose LLMs are sufficient for improving regression across diverse domains, RankRefine offers practicality and broad applicability, especially in low-data settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025 camera-ready version",
    "pdf_url": "https://arxiv.org/pdf/2508.16495v2",
    "published_date": "2025-08-22 16:17:31 UTC",
    "updated_date": "2025-10-01 12:27:37 UTC"
  },
  {
    "arxiv_id": "2508.16488v1",
    "title": "SafeSpace: An Integrated Web Application for Digital Safety and Emotional Well-being",
    "authors": [
      "Kayenat Fatmi",
      "Mohammad Abbas"
    ],
    "abstract": "In the digital era, individuals are increasingly exposed to online harms such as toxicity, manipulation, and grooming, which often pose emotional and safety risks. Existing systems for detecting abusive content or issuing safety alerts operate in isolation and rarely combine digital safety with emotional well-being. In this paper, we present SafeSpace, a unified web application that integrates three modules: (1) toxicity detection in chats and screenshots using NLP models and Google's Perspective API, (2) a configurable safety ping system that issues emergency alerts with the user's live location (longitude and latitude) via SMTP-based emails when check-ins are missed or SOS alerts are manually triggered, and (3) a reflective questionnaire that evaluates relationship health and emotional resilience. The system employs Firebase for alert management and a modular architecture designed for usability, privacy, and scalability. The experimental evaluation shows 93% precision in toxicity detection, 100% reliability in safety alerts under emulator tests, and 92% alignment between automated and manual questionnaire scoring. SafeSpace, implemented as a web application, demonstrates the feasibility of integrating detection, protection, and reflection within a single platform, with future deployment envisioned as a mobile application for broader accessibility.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages, 2 figures, 1 table. Preprint submitted to arXiv",
    "pdf_url": "https://arxiv.org/pdf/2508.16488v1",
    "published_date": "2025-08-22 16:07:29 UTC",
    "updated_date": "2025-08-22 16:07:29 UTC"
  },
  {
    "arxiv_id": "2508.16487v1",
    "title": "FraPPE: Fast and Efficient Preference-based Pure Exploration",
    "authors": [
      "Udvas Das",
      "Apurv Shukla",
      "Debabrota Basu"
    ],
    "abstract": "Preference-based Pure Exploration (PrePEx) aims to identify with a given confidence level the set of Pareto optimal arms in a vector-valued (aka multi-objective) bandit, where the reward vectors are ordered via a (given) preference cone $\\mathcal{C}$. Though PrePEx and its variants are well-studied, there does not exist a computationally efficient algorithm that can optimally track the existing lower bound for arbitrary preference cones. We successfully fill this gap by efficiently solving the minimisation and maximisation problems in the lower bound. First, we derive three structural properties of the lower bound that yield a computationally tractable reduction of the minimisation problem. Then, we deploy a Frank-Wolfe optimiser to accelerate the maximisation problem in the lower bound. Together, these techniques solve the maxmin optimisation problem in $\\mathcal{O}(KL^{2})$ time for a bandit instance with $K$ arms and $L$ dimensional reward, which is a significant acceleration over the literature. We further prove that our proposed PrePEx algorithm, FraPPE, asymptotically achieves the optimal sample complexity. Finally, we perform numerical experiments across synthetic and real datasets demonstrating that FraPPE achieves the lowest sample complexities to identify the exact Pareto set among the existing algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16487v1",
    "published_date": "2025-08-22 16:02:06 UTC",
    "updated_date": "2025-08-22 16:02:06 UTC"
  },
  {
    "arxiv_id": "2508.16479v1",
    "title": "Disentangled Multi-modal Learning of Histology and Transcriptomics for Cancer Characterization",
    "authors": [
      "Yupei Zhang",
      "Xiaofei Wang",
      "Anran Liu",
      "Lequan Yu",
      "Chao Li"
    ],
    "abstract": "Histopathology remains the gold standard for cancer diagnosis and prognosis. With the advent of transcriptome profiling, multi-modal learning combining transcriptomics with histology offers more comprehensive information. However, existing multi-modal approaches are challenged by intrinsic multi-modal heterogeneity, insufficient multi-scale integration, and reliance on paired data, restricting clinical applicability. To address these challenges, we propose a disentangled multi-modal framework with four contributions: 1) To mitigate multi-modal heterogeneity, we decompose WSIs and transcriptomes into tumor and microenvironment subspaces using a disentangled multi-modal fusion module, and introduce a confidence-guided gradient coordination strategy to balance subspace optimization. 2) To enhance multi-scale integration, we propose an inter-magnification gene-expression consistency strategy that aligns transcriptomic signals across WSI magnifications. 3) To reduce dependency on paired data, we propose a subspace knowledge distillation strategy enabling transcriptome-agnostic inference through a WSI-only student model. 4) To improve inference efficiency, we propose an informative token aggregation module that suppresses WSI redundancy while preserving subspace semantics. Extensive experiments on cancer diagnosis, prognosis, and survival prediction demonstrate our superiority over state-of-the-art methods across multiple settings. Code is available at https://github.com/helenypzhang/Disentangled-Multimodal-Learning.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16479v1",
    "published_date": "2025-08-22 15:51:33 UTC",
    "updated_date": "2025-08-22 15:51:33 UTC"
  },
  {
    "arxiv_id": "2508.16714v1",
    "title": "AI Product Value Assessment Model: An Interdisciplinary Integration Based on Information Theory, Economics, and Psychology",
    "authors": [
      "Yu yang"
    ],
    "abstract": "In recent years, breakthroughs in artificial intelligence (AI) technology have triggered global industrial transformations, with applications permeating various fields such as finance, healthcare, education, and manufacturing. However, this rapid iteration is accompanied by irrational development, where enterprises blindly invest due to technology hype, often overlooking systematic value assessments. This paper develops a multi-dimensional evaluation model that integrates information theory's entropy reduction principle, economics' bounded rationality framework, and psychology's irrational decision theories to quantify AI product value. Key factors include positive dimensions (e.g., uncertainty elimination, efficiency gains, cost savings, decision quality improvement) and negative risks (e.g., error probability, impact, and correction costs). A non-linear formula captures factor couplings, and validation through 10 commercial cases demonstrates the model's effectiveness in distinguishing successful and failed products, supporting hypotheses on synergistic positive effects, non-linear negative impacts, and interactive regulations. Results reveal value generation logic, offering enterprises tools to avoid blind investments and promote rational AI industry development. Future directions include adaptive weights, dynamic mechanisms, and extensions to emerging AI technologies like generative models.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "in Chinese language",
    "pdf_url": "https://arxiv.org/pdf/2508.16714v1",
    "published_date": "2025-08-22 15:51:14 UTC",
    "updated_date": "2025-08-22 15:51:14 UTC"
  },
  {
    "arxiv_id": "2508.16465v2",
    "title": "HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images",
    "authors": [
      "Anilkumar Swamy",
      "Vincent Leroy",
      "Philippe Weinzaepfel",
      "Jean-Sébastien Franco",
      "Grégory Rogez"
    ],
    "abstract": "Hand-object 3D reconstruction has become increasingly important for applications in human-robot interaction and immersive AR/VR experiences. A common approach for object-agnostic hand-object reconstruction from RGB sequences involves a two-stage pipeline: hand-object 3D tracking followed by multi-view 3D reconstruction. However, existing methods rely on keypoint detection techniques, such as Structure from Motion (SfM) and hand-keypoint optimization, which struggle with diverse object geometries, weak textures, and mutual hand-object occlusions, limiting scalability and generalization. As a key enabler to generic and seamless, non-intrusive applicability, we propose in this work a robust, keypoint detector-free approach to estimating hand-object 3D transformations from monocular motion video/images. We further integrate this with a multi-view reconstruction pipeline to accurately recover hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely on pre-scanned object templates or camera intrinsics, and reaches state-of-the-art performance for the tasks of object-agnostic hand-object 3D transformation and shape estimation on the SHOWMe benchmark. We also experiment on sequences from the HO3D dataset, demonstrating generalization to unseen object categories.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2508.16465v2",
    "published_date": "2025-08-22 15:30:40 UTC",
    "updated_date": "2025-08-25 16:02:08 UTC"
  },
  {
    "arxiv_id": "2508.18296v1",
    "title": "Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges",
    "authors": [
      "Edgar Rangel",
      "Fabio Martinez"
    ],
    "abstract": "Stroke is the second leading cause of death and the third leading cause of disability worldwide. Clinical guidelines establish diffusion resonance imaging (DWI, ADC) as the standard for localizing, characterizing, and measuring infarct volume, enabling treatment support and prognosis. Nonetheless, such lesion analysis is highly variable due to different patient demographics, scanner vendors, and expert annotations. Computational support approaches have been key to helping with the localization and segmentation of lesions. However, these strategies are dedicated solutions that learn patterns from only one institution, lacking the variability to generalize geometrical lesions shape models. Even worse, many clinical centers lack sufficient labeled samples to adjust these dedicated solutions. This work developed a collaborative framework for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge from deep center-independent representations. From 14 emulated healthcare centers with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \\pm 0.24$, AVD of $5.29 \\pm 22.74$, ALD of $2.16 \\pm 3.60$ and LF1 of $0.70 \\pm 0.26$ over all centers, outperforming both the centralized and other federated rules. Interestingly, the model demonstrated strong generalization properties, showing uniform performance across different lesion categories and reliable performance in out-of-distribution centers (with DSC of $0.64 \\pm 0.29$ and AVD of $4.44 \\pm 8.74$ without any additional training).",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, 4 figures, 3 tables, source code available",
    "pdf_url": "https://arxiv.org/pdf/2508.18296v1",
    "published_date": "2025-08-22 15:27:13 UTC",
    "updated_date": "2025-08-22 15:27:13 UTC"
  },
  {
    "arxiv_id": "2508.16463v2",
    "title": "Modular Embedding Recomposition for Incremental Learning",
    "authors": [
      "Aniello Panariello",
      "Emanuele Frascaroli",
      "Pietro Buzzega",
      "Lorenzo Bonicelli",
      "Angelo Porrello",
      "Simone Calderara"
    ],
    "abstract": "The advent of pre-trained Vision-Language Models (VLMs) has significantly transformed Continual Learning (CL), mainly due to their zero-shot classification abilities. Such proficiency makes VLMs well-suited for real-world applications, enabling robust performance on novel unseen classes without requiring adaptation. However, fine-tuning remains essential when downstream tasks deviate significantly from the pre-training domain. Prior CL approaches primarily focus on preserving the zero-shot capabilities of VLMs during incremental fine-tuning on a downstream task. We take a step further by devising an approach that transforms preservation into enhancement of the zero-shot capabilities of VLMs. Our approach, named MoDular Embedding Recomposition (MoDER), introduces a modular framework that trains multiple textual experts, each specialized in a single seen class, and stores them in a foundational hub. At inference time, for each unseen class, we query the hub and compose the retrieved experts to synthesize a refined prototype that improves classification. We show the effectiveness of our method across two popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total of 14 datasets. The codebase is available at https://github.com/aimagelab/mammoth.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the 36th British Machine Vision Conference (BMVC 2025), Sheffield, UK",
    "pdf_url": "https://arxiv.org/pdf/2508.16463v2",
    "published_date": "2025-08-22 15:25:40 UTC",
    "updated_date": "2025-10-14 16:54:27 UTC"
  },
  {
    "arxiv_id": "2508.16713v1",
    "title": "CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics",
    "authors": [
      "Mohammad Atif",
      "Kriti Chopra",
      "Ozgur Kilic",
      "Tianle Wang",
      "Zhihua Dong",
      "Charles Leggett",
      "Meifeng Lin",
      "Paolo Calafiura",
      "Salman Habib"
    ],
    "abstract": "Next-generation High Energy Physics (HEP) experiments will generate unprecedented data volumes, necessitating High Performance Computing (HPC) integration alongside traditional high-throughput computing. However, HPC adoption in HEP is hindered by the challenge of porting legacy software to heterogeneous architectures and the sparse documentation of these complex scientific codebases. We present CelloAI, a locally hosted coding assistant that leverages Large Language Models (LLMs) with retrieval-augmented generation (RAG) to support HEP code documentation and generation. This local deployment ensures data privacy, eliminates recurring costs and provides access to large context windows without external dependencies. CelloAI addresses two primary use cases, code documentation and code generation, through specialized components. For code documentation, the assistant provides: (a) Doxygen style comment generation for all functions and classes by retrieving relevant information from RAG sources (papers, posters, presentations), (b) file-level summary generation, and (c) an interactive chatbot for code comprehension queries. For code generation, CelloAI employs syntax-aware chunking strategies that preserve syntactic boundaries during embedding, improving retrieval accuracy in large codebases. The system integrates callgraph knowledge to maintain dependency awareness during code modifications and provides AI-generated suggestions for performance optimization and accurate refactoring. We evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE experiments, comparing different embedding models for code retrieval effectiveness. Our results demonstrate the AI assistant's capability to enhance code understanding and support reliable code generation while maintaining the transparency and safety requirements essential for scientific computing environments.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "hep-ex"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2508.16713v1",
    "published_date": "2025-08-22 15:17:44 UTC",
    "updated_date": "2025-08-22 15:17:44 UTC"
  },
  {
    "arxiv_id": "2508.16712v1",
    "title": "Systematic Characterization of LLM Quantization: A Performance, Energy, and Quality Perspective",
    "authors": [
      "Tianyao Shi",
      "Yi Ding"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their heavy resource demands make quantization-reducing precision to lower-bit formats-critical for efficient serving. While many quantization methods exist, a systematic understanding of their performance, energy, and quality tradeoffs in realistic serving conditions remains a gap. In this work, we first develop a fully automated online characterization framework qMeter, and then conduct an in-depth characterization of 11 post-training LLM quantization methods across 4 model sizes (7B-70B) and two GPU architectures (A100, H100). We evaluate quantization at the application, workload, parallelism, and hardware levels under online serving conditions. Our study reveals highly task- and method-dependent tradeoffs, strong sensitivity to workload characteristics, and complex interactions with parallelism and GPU architecture. We further present three optimization case studies illustrating deployment challenges in capacity planning, energy-efficient scheduling, and multi-objective tuning. To the best of our knowledge, this is one of the first comprehensive application-, system-, and hardware-level characterization of LLM quantization from a joint performance, energy, and quality perspective.",
    "categories": [
      "cs.PF",
      "cs.AI",
      "cs.AR",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.PF",
    "comment": "14 pages, 10 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2508.16712v1",
    "published_date": "2025-08-22 14:59:23 UTC",
    "updated_date": "2025-08-22 14:59:23 UTC"
  },
  {
    "arxiv_id": "2508.16439v3",
    "title": "PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark",
    "authors": [
      "Adil Bahaj",
      "Oumaima Fadi",
      "Mohamed Chetouani",
      "Mounir Ghogho"
    ],
    "abstract": "Large language models (LLMs) and vision-augmented LLMs (VLMs) have significantly advanced medical informatics, diagnostics, and decision support. However, these models exhibit systematic biases, particularly age bias, compromising their reliability and equity. This is evident in their poorer performance on pediatric-focused text and visual question-answering tasks. This bias reflects a broader imbalance in medical research, where pediatric studies receive less funding and representation despite the significant disease burden in children. To address these issues, a new comprehensive multi-modal pediatric question-answering benchmark, PediatricsMQA, has been introduced. It consists of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric topics across seven developmental stages (prenatal to adolescent) and 2,067 vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256 anatomical regions. The dataset was developed using a hybrid manual-automatic pipeline, incorporating peer-reviewed pediatric literature, validated question banks, existing benchmarks, and existing QA resources. Evaluating state-of-the-art open models, we find dramatic performance drops in younger cohorts, highlighting the need for age-aware methods to ensure equitable AI support in pediatric care.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.GR",
      "cs.MM"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16439v3",
    "published_date": "2025-08-22 14:50:55 UTC",
    "updated_date": "2025-08-27 08:33:44 UTC"
  },
  {
    "arxiv_id": "2508.16438v2",
    "title": "OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval",
    "authors": [
      "Yu Liu",
      "Yanbing Liu",
      "Fangfang Yuan",
      "Cong Cao",
      "Youbang Sun",
      "Kun Peng",
      "WeiZhuo Chen",
      "Jianjun Li",
      "Zhiyuan Ma"
    ],
    "abstract": "Recent advances in large language models (LLMs) and dense retrievers have driven significant progress in retrieval-augmented generation (RAG). However, existing approaches face significant challenges in complex reasoning-oriented multi-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior methods struggle to generate robust multi-step plans for complex queries, as rule-based decomposers perform poorly on out-of-template questions. 2) Suboptimal reasoning-driven retrieval: Related methods employ limited query reformulation, leading to iterative retrieval loops that often fail to locate golden documents. 3) Insufficient reasoning-guided filtering: Prevailing methods lack the fine-grained reasoning to effectively filter salient information from noisy results, hindering utilization of retrieved knowledge. Fundamentally, these limitations all stem from the weak coupling between retrieval and reasoning in current RAG architectures. We introduce the Orchestrated Planner-Executor Reasoning Architecture (OPERA), a novel reasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM) decomposes questions into sub-goals, which are executed by a Reason-Execute Module (REM) with specialized components for precise reasoning and effective retrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative Policy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex multi-hop benchmarks show OPERA's superior performance, validating both the MAPGRPO method and OPERA's design.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2508.16438v2",
    "published_date": "2025-08-22 14:50:26 UTC",
    "updated_date": "2025-11-11 15:51:52 UTC"
  },
  {
    "arxiv_id": "2508.16431v1",
    "title": "Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish",
    "authors": [
      "Yakup Abrek Er",
      "Ilker Kesen",
      "Gözde Gül Şahin",
      "Aykut Erdem"
    ],
    "abstract": "We introduce Cetvel, a comprehensive benchmark designed to evaluate large language models (LLMs) in Turkish. Existing Turkish benchmarks often lack either task diversity or culturally relevant content, or both. Cetvel addresses these gaps by combining a broad range of both discriminative and generative tasks ensuring content that reflects the linguistic and cultural richness of Turkish language. Cetvel covers 23 tasks grouped into seven categories, including tasks such as grammatical error correction, machine translation, and question answering rooted in Turkish history and idiomatic language. We evaluate 33 open-weight LLMs (up to 70B parameters) covering different model families and instruction paradigms. Our experiments reveal that Turkish-centric instruction-tuned models generally underperform relative to multilingual or general-purpose models (e.g. Llama 3 and Mistral), despite being tailored for the language. Moreover, we show that tasks such as grammatical error correction and extractive question answering are particularly discriminative in differentiating model capabilities. Cetvel offers a comprehensive and culturally grounded evaluation suite for advancing the development and assessment of LLMs in Turkish.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "31 pages, 2 figures, 10 tables",
    "pdf_url": "https://arxiv.org/pdf/2508.16431v1",
    "published_date": "2025-08-22 14:42:50 UTC",
    "updated_date": "2025-08-22 14:42:50 UTC"
  },
  {
    "arxiv_id": "2509.10467v1",
    "title": "DSRAG: A Domain-Specific Retrieval Framework Based on Document-derived Multimodal Knowledge Graph",
    "authors": [
      "Mengzheng Yang",
      "Yanfei Ren",
      "David Osei Opoku",
      "Ruochang Li",
      "Peng Ren",
      "Chunxiao Xing"
    ],
    "abstract": "Current general-purpose large language models (LLMs) commonly exhibit knowledge hallucination and insufficient domain-specific adaptability in domain-specific tasks, limiting their effectiveness in specialized question answering scenarios. Retrieval-augmented generation (RAG) effectively tackles these challenges by integrating external knowledge to enhance accuracy and relevance. However, traditional RAG still faces limitations in domain knowledge accuracy and context modeling.To enhance domain-specific question answering performance, this work focuses on a graph-based RAG framework, emphasizing the critical role of knowledge graph quality during the generation process. We propose DSRAG (Domain-Specific RAG), a multimodal knowledge graph-driven retrieval-augmented generation framework designed for domain-specific applications. Our approach leverages domain-specific documents as the primary knowledge source, integrating heterogeneous information such as text, images, and tables to construct a multimodal knowledge graph covering both conceptual and instance layers. Building on this foundation, we introduce semantic pruning and structured subgraph retrieval mechanisms, combining knowledge graph context and vector retrieval results to guide the language model towards producing more reliable responses. Evaluations using the Langfuse multidimensional scoring mechanism show that our method excels in domain-specific question answering, validating the efficacy of integrating multimodal knowledge graphs with retrieval-augmented generation.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.IR",
    "comment": "12 pages, 5 figures. Accepted to the 22nd International Conference on Web Information Systems and Applications (WISA 2025)",
    "pdf_url": "https://arxiv.org/pdf/2509.10467v1",
    "published_date": "2025-08-22 14:24:48 UTC",
    "updated_date": "2025-08-22 14:24:48 UTC"
  },
  {
    "arxiv_id": "2508.16397v1",
    "title": "A Lightweight Group Multiscale Bidirectional Interactive Network for Real-Time Steel Surface Defect Detection",
    "authors": [
      "Yong Zhang",
      "Cunjian Chen",
      "Qiang Gao",
      "Yi Wang",
      "Bin Fang"
    ],
    "abstract": "Real-time surface defect detection is critical for maintaining product quality and production efficiency in the steel manufacturing industry. Despite promising accuracy, existing deep learning methods often suffer from high computational complexity and slow inference speeds, which limit their deployment in resource-constrained industrial environments. Recent lightweight approaches adopt multibranch architectures based on depthwise separable convolution (DSConv) to capture multiscale contextual information. However, these methods often suffer from increased computational overhead and lack effective cross-scale feature interaction, limiting their ability to fully leverage multiscale representations. To address these challenges, we propose GMBINet, a lightweight framework that enhances multiscale feature extraction and interaction through novel Group Multiscale Bidirectional Interactive (GMBI) modules. The GMBI adopts a group-wise strategy for multiscale feature extraction, ensuring scale-agnostic computational complexity. It further integrates a Bidirectional Progressive Feature Interactor (BPFI) and a parameter-free Element-Wise Multiplication-Summation (EWMS) operation to enhance cross-scale interaction without introducing additional computational overhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that GMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU and 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters. Additional evaluations on the NEU-CLS defect classification dataset further confirm the strong generalization ability of our method, demonstrating its potential for broader industrial vision applications beyond surface defect detection. The dataset and code are publicly available at: https://github.com/zhangyongcode/GMBINet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16397v1",
    "published_date": "2025-08-22 13:58:35 UTC",
    "updated_date": "2025-08-22 13:58:35 UTC"
  },
  {
    "arxiv_id": "2508.16396v2",
    "title": "Generative artificial intelligence improves projections of climate extremes",
    "authors": [
      "Ruian Tie",
      "Xiaohui Zhong",
      "Zhengyu Shi",
      "Hao Li",
      "Bin Chen",
      "Jun Liu",
      "Wu Libo"
    ],
    "abstract": "Climate change is amplifying extreme events, posing escalating risks to biodiversity, human health, and food security. GCMs are essential for projecting future climate, yet their coarse resolution and high computational costs constrain their ability to represent extremes. Here, we introduce FuXi-CMIPAlign, a generative deep learning framework for downscaling CMIP outputs. The model integrates Flow Matching for generative modeling with domain adaptation via MMD loss to align feature distributions between training data and inference data, thereby mitigating input discrepancies and improving accuracy, stability, and generalization across emission scenarios. FuXi-CMIPAlign performs spatial, temporal, and multivariate downscaling, enabling more realistic simulation of compound extremes such as TCs.",
    "categories": [
      "physics.ao-ph",
      "cs.AI"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16396v2",
    "published_date": "2025-08-22 13:56:02 UTC",
    "updated_date": "2025-10-12 01:16:18 UTC"
  },
  {
    "arxiv_id": "2508.16390v3",
    "title": "MedQARo: A Large-Scale Benchmark for Evaluating Large Language Models on Medical Question Answering in Romanian",
    "authors": [
      "Ana-Cristina Rogoz",
      "Radu Tudor Ionescu",
      "Alexandra-Valentina Anghel",
      "Ionut-Lucian Antone-Iordache",
      "Simona Coniac",
      "Andreea Iuliana Ionescu"
    ],
    "abstract": "Question answering (QA) is an actively studied topic, being a core natural language processing (NLP) task that needs to be addressed before achieving Artificial General Intelligence (AGI). However, the lack of QA datasets in specific domains and languages hinders the development of robust AI models able to generalize across various domains and languages. To this end, we introduce MedQARo, the first large-scale medical QA benchmark in Romanian, alongside a comprehensive evaluation of state-of-the-art (SOTA) large language models (LLMs). We construct a high-quality and large-scale dataset comprising 105,880 QA pairs related to cancer patients from two medical centers. The questions regard medical case summaries of 1,242 patients, requiring either keyword extraction or reasoning to be answered correctly. MedQARo is the result of a time-consuming manual annotation process carried out by seven physicians specialized in oncology or radiotherapy, who spent a total of about 3,000 work hours to generate the QA pairs. Our benchmark contains both in-domain and cross-domain (cross-center and cross-cancer) test collections, enabling a precise assessment of generalization capabilities. We experiment with four open-source LLMs from distinct families of models on MedQARo. Each model is employed in two scenarios, namely one based on zero-shot prompting and one based on supervised fine-tuning. We also evaluate two state-of-the-art LLMs exposed only through APIs, namely GPT-5.2 and Gemini 3 Flash. Our results show that fine-tuned models significantly outperform zero-shot models, clearly indicating that pretrained models fail to generalize on MedQARo. Our findings demonstrate the importance of both domain-specific and language-specific fine-tuning for reliable clinical QA in Romanian. We publicly release our dataset and code at https://github.com/ana-rogoz/MedQARo.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16390v3",
    "published_date": "2025-08-22 13:48:37 UTC",
    "updated_date": "2025-12-31 10:51:21 UTC"
  },
  {
    "arxiv_id": "2508.16383v1",
    "title": "GLARE: Agentic Reasoning for Legal Judgment Prediction",
    "authors": [
      "Xinyu Yang",
      "Chenlong Deng",
      "Zhicheng Dou"
    ],
    "abstract": "Legal judgment prediction (LJP) has become increasingly important in the legal field. In this paper, we identify that existing large language models (LLMs) have significant problems of insufficient reasoning due to a lack of legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning framework that dynamically acquires key legal knowledge by invoking different modules, thereby improving the breadth and depth of reasoning. Experiments conducted on the real-world dataset verify the effectiveness of our method. Furthermore, the reasoning chain generated during the analysis process can increase interpretability and provide the possibility for practical applications.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16383v1",
    "published_date": "2025-08-22 13:38:12 UTC",
    "updated_date": "2025-08-22 13:38:12 UTC"
  },
  {
    "arxiv_id": "2508.18295v1",
    "title": "H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems",
    "authors": [
      "Huangyu Dai",
      "Lingtao Mao",
      "Ben Chen",
      "Zihan Wang",
      "Zihan Liang",
      "Ying Han",
      "Chenyi Lei",
      "Han Li"
    ],
    "abstract": "Hotword customization is crucial in ASR to enhance the accuracy of domain-specific terms. It has been primarily driven by the advancements in traditional models and Audio large language models (LLMs). However, existing models often struggle with large-scale hotwords, as the recognition rate drops dramatically with the number of hotwords increasing. In this paper, we introduce a novel hotword customization system that utilizes a hotword pre-retrieval module (H-PRM) to identify the most relevant hotword candidate by measuring the acoustic similarity between the hotwords and the speech segment. This plug-and-play solution can be easily integrated into traditional models such as SeACo-Paraformer, significantly enhancing hotwords post-recall rate (PRR). Additionally, we incorporate H-PRM into Audio LLMs through a prompt-based approach, enabling seamless customization of hotwords. Extensive testing validates that H-PRM can outperform existing methods, showing a new direction for hotword customization in ASR.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.18295v1",
    "published_date": "2025-08-22 13:30:22 UTC",
    "updated_date": "2025-08-22 13:30:22 UTC"
  },
  {
    "arxiv_id": "2508.18294v1",
    "title": "MobileDenseAttn:A Dual-Stream Architecture for Accurate and Interpretable Brain Tumor Detection",
    "authors": [
      "Shudipta Banik",
      "Muna Das",
      "Trapa Banik",
      "Md. Ehsanul Haque"
    ],
    "abstract": "The detection of brain tumor in MRI is an important aspect of ensuring timely diagnostics and treatment; however, manual analysis is commonly long and error-prone. Current approaches are not universal because they have limited generalization to heterogeneous tumors, are computationally inefficient, are not interpretable, and lack transparency, thus limiting trustworthiness. To overcome these issues, we introduce MobileDenseAttn, a fusion model of dual streams of MobileNetV2 and DenseNet201 that can help gradually improve the feature representation scale, computing efficiency, and visual explanations via GradCAM. Our model uses feature level fusion and is trained on an augmented dataset of 6,020 MRI scans representing glioma, meningioma, pituitary tumors, and normal samples. Measured under strict 5-fold cross-validation protocols, MobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of 98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). The extensive validation shows the stability of the model, and the comparative analysis proves that it is a great advancement over the baseline models (VGG19, DenseNet201, MobileNetV2) with a +3.67% accuracy increase and a 39.3% decrease in training time compared to VGG19. The GradCAM heatmaps clearly show tumor-affected areas, offering clinically significant localization and improving interpretability. These findings position MobileDenseAttn as an efficient, high performance, interpretable model with a high probability of becoming a clinically practical tool in identifying brain tumors in the real world.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted at ICCIT 2025 cox bazar, Bangladesh",
    "pdf_url": "https://arxiv.org/pdf/2508.18294v1",
    "published_date": "2025-08-22 13:24:38 UTC",
    "updated_date": "2025-08-22 13:24:38 UTC"
  },
  {
    "arxiv_id": "2508.16706v1",
    "title": "RoboBuddy in the Classroom: Exploring LLM-Powered Social Robots for Storytelling in Learning and Integration Activities",
    "authors": [
      "Daniel Tozadore",
      "Nur Ertug",
      "Yasmine Chaker",
      "Mortadha Abderrahim"
    ],
    "abstract": "Creating and improvising scenarios for content approaching is an enriching technique in education. However, it comes with a significant increase in the time spent on its planning, which intensifies when using complex technologies, such as social robots. Furthermore, addressing multicultural integration is commonly embedded in regular activities due to the already tight curriculum. Addressing these issues with a single solution, we implemented an intuitive interface that allows teachers to create scenario-based activities from their regular curriculum using LLMs and social robots. We co-designed different frameworks of activities with 4 teachers and deployed it in a study with 27 students for 1 week. Beyond validating the system's efficacy, our findings highlight the positive impact of integration policies perceived by the children and demonstrate the importance of scenario-based activities in students' enjoyment, observed to be significantly higher when applying storytelling. Additionally, several implications of using LLMs and social robots in long-term classroom activities are discussed.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted to be published in the proceedings of 34th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN) in 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.16706v1",
    "published_date": "2025-08-22 13:14:09 UTC",
    "updated_date": "2025-08-22 13:14:09 UTC"
  },
  {
    "arxiv_id": "2508.16357v1",
    "title": "MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering",
    "authors": [
      "Adil Bahaj",
      "Mounir Ghogho"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has significantly propelled progress in natural language processing (NLP). However, their effectiveness in specialized, low-resource domains-such as Arabic legal contexts-remains limited. This paper introduces MizanQA (pronounced Mizan, meaning \"scale\" in Arabic, a universal symbol of justice), a benchmark designed to evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised by rich linguistic and legal complexity. The dataset draws on Modern Standard Arabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal influences. Comprising over 1,700 multiple-choice questions, including multi-answer formats, MizanQA captures the nuances of authentic legal reasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs reveal substantial performance gaps, highlighting the need for tailored evaluation metrics and culturally grounded, domain-specific LLM development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16357v1",
    "published_date": "2025-08-22 13:04:43 UTC",
    "updated_date": "2025-08-22 13:04:43 UTC"
  },
  {
    "arxiv_id": "2508.16352v1",
    "title": "Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management",
    "authors": [
      "Nasir Khan",
      "Asmaa Abdallah",
      "Abdulkadir Celik",
      "Ahmed M. Eltawil",
      "Sinem Coleri"
    ],
    "abstract": "Efficient and reliable beam alignment is a critical requirement for mmWave multiple-input multiple-output (MIMO) systems, especially in 6G and beyond, where communication must be fast, adaptive, and resilient to real-world uncertainties. Existing deep learning (DL)-based beam alignment methods often neglect the underlying causal relationships between inputs and outputs, leading to limited interpretability, poor generalization, and unnecessary beam sweeping overhead. In this work, we propose a causally-aware DL framework that integrates causal discovery into beam management pipeline. Particularly, we propose a novel two-stage causal beam selection algorithm to identify a minimal set of relevant inputs for beam prediction. First, causal discovery learns a Bayesian graph capturing dependencies between received power inputs and the optimal beam. Then, this graph guides causal feature selection for the DL-based classifier. Simulation results reveal that the proposed causal beam selection matches the performance of conventional methods while drastically reducing input selection time by 94.4% and beam sweeping overhead by 59.4% by focusing only on causally relevant features.",
    "categories": [
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16352v1",
    "published_date": "2025-08-22 12:56:07 UTC",
    "updated_date": "2025-08-22 12:56:07 UTC"
  },
  {
    "arxiv_id": "2508.16347v2",
    "title": "Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs",
    "authors": [
      "Yu Yan",
      "Sheng Sun",
      "Zhe Wang",
      "Yijun Lin",
      "Zenghao Duan",
      "zhifei zheng",
      "Min Liu",
      "Zhiyi yin",
      "Jianping Zhang"
    ],
    "abstract": "With the development of Large Language Models (LLMs), numerous efforts have revealed their vulnerabilities to jailbreak attacks. Although these studies have driven the progress in LLMs' safety alignment, it remains unclear whether LLMs have internalized authentic knowledge to deal with real-world crimes, or are merely forced to simulate toxic language patterns. This ambiguity raises concerns that jailbreak success is often attributable to a hallucination loop between jailbroken LLM and judger LLM. By decoupling the use of jailbreak techniques, we construct knowledge-intensive Q\\&A to investigate the misuse threats of LLMs in terms of dangerous knowledge possession, harmful task planning utility, and harmfulness judgment robustness. Experiments reveal a mismatch between jailbreak success rates and harmful knowledge possession in LLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness judgments on toxic language patterns. Our study reveals a gap between existing LLM safety assessments and real-world threat potential.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16347v2",
    "published_date": "2025-08-22 12:41:26 UTC",
    "updated_date": "2025-09-15 03:59:27 UTC"
  },
  {
    "arxiv_id": "2508.16345v1",
    "title": "Uppaal Coshy: Automatic Synthesis of Compact Shields for Hybrid Systems",
    "authors": [
      "Asger Horn Brorholt",
      "Andreas Holck Høeg-Petersen",
      "Peter Gjøl Jensen",
      "Kim Guldstrand Larsen",
      "Marius Mikučionis",
      "Christian Schilling",
      "Andrzej Wąsowski"
    ],
    "abstract": "We present Uppaal Coshy, a tool for automatic synthesis of a safety strategy -- or shield -- for Markov decision processes over continuous state spaces and complex hybrid dynamics. The general methodology is to partition the state space and then solve a two-player safety game, which entails a number of algorithmically hard problems such as reachability for hybrid systems. The general philosophy of Uppaal Coshy is to approximate hard-to-obtain solutions using simulations. Our implementation is fully automatic and supports the expressive formalism of Uppaal models, which encompass stochastic hybrid automata. The precision of our partition-based approach benefits from using finer grids, which however are not efficient to store. We include an algorithm called Caap to efficiently compute a compact representation of a shield in the form of a decision tree, which yields significant reductions.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.LO",
    "comment": "12 pages and 6 figures. Additional abstract of 4 pages and 4 figures. Extended version with supplementary material for an article to appear in the 2025 International Conference on Reachability Problems (RP)",
    "pdf_url": "https://arxiv.org/pdf/2508.16345v1",
    "published_date": "2025-08-22 12:39:40 UTC",
    "updated_date": "2025-08-22 12:39:40 UTC"
  },
  {
    "arxiv_id": "2508.16336v1",
    "title": "Unsupervised Online Detection of Pipe Blockages and Leakages in Water Distribution Networks",
    "authors": [
      "Jin Li",
      "Kleanthis Malialis",
      "Stelios G. Vrachimis",
      "Marios M. Polycarpou"
    ],
    "abstract": "Water Distribution Networks (WDNs), critical to public well-being and economic stability, face challenges such as pipe blockages and background leakages, exacerbated by operational constraints such as data non-stationarity and limited labeled data. This paper proposes an unsupervised, online learning framework that aims to detect two types of faults in WDNs: pipe blockages, modeled as collective anomalies, and background leakages, modeled as concept drift. Our approach combines a Long Short-Term Memory Variational Autoencoder (LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and adaptation under non-stationary conditions. Its lightweight, memory-efficient design enables real-time, edge-level monitoring. Experiments on two realistic WDNs show that the proposed approach consistently outperforms strong baselines in detecting anomalies and adapting to recurrent drift, demonstrating its effectiveness in unsupervised event detection for dynamic WDN environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is accepted by the 6th International Conference on Control and Fault-Tolerant Systems (SysTol)",
    "pdf_url": "https://arxiv.org/pdf/2508.16336v1",
    "published_date": "2025-08-22 12:23:40 UTC",
    "updated_date": "2025-08-22 12:23:40 UTC"
  },
  {
    "arxiv_id": "2508.16332v2",
    "title": "Vevo2: A Unified and Controllable Framework for Speech and Singing Voice Generation",
    "authors": [
      "Xueyao Zhang",
      "Junan Zhang",
      "Yuancheng Wang",
      "Chaoren Wang",
      "Yuanzhe Chen",
      "Dongya Jia",
      "Zhuo Chen",
      "Zhizheng Wu"
    ],
    "abstract": "Controllable human voice generation, particularly for expressive domains like singing, remains a significant challenge. This paper introduces Vevo2, a unified framework for controllable speech and singing voice generation. To tackle issues like the scarcity of annotated singing data and to enable flexible controllability, Vevo2 introduces two audio tokenizers: (1) a unified music-notation-free prosody tokenizer that captures prosody and melody from speech, singing, and even instrumental sounds, and (2) a unified content-style tokenizer that encodes linguistic content, prosody, and style for both speech and singing, while enabling timbre disentanglement. Vevo2 consists of an auto-regressive (AR) content-style modeling stage, which aims to enable controllability over text, prosody, and style, as well as a flow-matching acoustic modeling stage that allows for timbre control. Particularly, during the speech-singing joint training of the AR model, we propose both explicit and implicit prosody learning strategies to bridge speech and singing voice. Moreover, to further enhance the Vevo2's ability to follow text and prosody, we design a multi-objective post-training task that integrates both intelligibility and prosody similarity alignment. Experimental results show that the unified modeling in Vevo2 brings mutual benefits to both speech and singing voice generation. Additionally, Vevo2's effectiveness across a wide range of synthesis, conversion, and editing tasks for both speech and singing further demonstrates its strong generalization ability and versatility. Audio samples are are available at https://versasinger.github.io/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SD",
    "comment": "We will release code and model checkpoints at https://github.com/open-mmlab/Amphion",
    "pdf_url": "https://arxiv.org/pdf/2508.16332v2",
    "published_date": "2025-08-22 12:20:11 UTC",
    "updated_date": "2025-12-10 08:35:35 UTC"
  },
  {
    "arxiv_id": "2508.18240v2",
    "title": "MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues via Arena-style and Rubrics Protocols",
    "authors": [
      "Yuhao Du",
      "Qianwei Huang",
      "Guo Zhu",
      "Zhanchen Dai",
      "Shunian Chen",
      "Qiming Zhu",
      "Le Pan",
      "Minghao Chen",
      "Yuhao Zhang",
      "Li Zhou",
      "Benyou Wang",
      "Haizhou Li"
    ],
    "abstract": "The rapid advancement of speech-to-speech (S2S) large language models (LLMs) has significantly improved real-time spoken interaction. However, current evaluation frameworks remain inadequate for assessing performance in complex, multi-turn dialogues. To address this, we introduce MTalk-Bench, a multi-turn S2S benchmark covering three core dimensions: Semantic Information, Paralinguistic Information, and Ambient Sound. Each dimension includes nine realistic scenarios, along with targeted tasks to assess specific capabilities such as reasoning. Our dual-method evaluation framework combines Arena-style evaluation (pairwise comparison) and Rubrics-based evaluation (absolute scoring) for relative and absolute assessment. The benchmark includes both model and human outputs, evaluated by human evaluators and LLMs. Experimental results reveal two sets of findings. Overall performance of S2S LLMs: (1) models excel at semantic information processing yet underperform on paralinguistic information and ambient sounds perception; (2) models typically regain coherence by increasing response length, sacrificing efficiency in multi-turn dialogues; (3) modality-aware, task-specific designs outperform brute scaling. Evaluation framework and reliability: (1) Arena and Rubrics yield consistent, complementary rankings, but reliable distinctions emerge only when performance gaps are large; (2) LLM-as-a-judge aligns with humans when gaps are clear or criteria explicit, but exhibits position and length biases and is reliable on nonverbal evaluation only with text annotations. These results highlight current limitations in S2S evaluation and the need for more robust, speech-aware assessment frameworks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.18240v2",
    "published_date": "2025-08-22 12:14:17 UTC",
    "updated_date": "2025-09-15 14:50:39 UTC"
  },
  {
    "arxiv_id": "2508.16325v2",
    "title": "ConceptGuard: Neuro-Symbolic Safety Guardrails via Sparse Interpretable Jailbreak Concepts",
    "authors": [
      "Darpan Aswal",
      "Céline Hudelot"
    ],
    "abstract": "Large Language Models have found success in a variety of applications. However, their safety remains a concern due to the existence of various jailbreaking methods. Despite significant efforts, alignment and safety fine-tuning only provide a certain degree of robustness against jailbreak attacks that covertly mislead LLMs towards the generation of harmful content. This leaves them prone to a range of vulnerabilities, including targeted misuse and accidental user profiling. This work introduces \\textbf{ConceptGuard}, a novel framework that leverages Sparse Autoencoders (SAEs) to identify interpretable concepts within LLM internals associated with different jailbreak themes. By extracting semantically meaningful internal representations, ConceptGuard enables building robust safety guardrails -- offering fully explainable and generalizable defenses without sacrificing model capabilities or requiring further fine-tuning. Leveraging advances in the mechanistic interpretability of LLMs, our approach provides evidence for a shared activation geometry for jailbreak attacks in the representation space, a potential foundation for designing more interpretable and generalizable safeguards against attackers.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16325v2",
    "published_date": "2025-08-22 12:13:38 UTC",
    "updated_date": "2025-12-13 13:00:25 UTC"
  },
  {
    "arxiv_id": "2508.18293v1",
    "title": "Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches",
    "authors": [
      "M. Salman Shaukat",
      "Yannik Käckenmeister",
      "Sebastian Bader",
      "Thomas Kirste"
    ],
    "abstract": "Underwater 3D object detection remains one of the most challenging frontiers in computer vision, where traditional approaches struggle with the harsh acoustic environment and scarcity of training data. While deep learning has revolutionized terrestrial 3D detection, its application underwater faces a critical bottleneck: obtaining sufficient annotated sonar data is prohibitively expensive and logistically complex, often requiring specialized vessels, expert surveyors, and favorable weather conditions. This work addresses a fundamental question: Can we achieve reliable underwater 3D object detection without real-world training data? We tackle this challenge by developing and comparing two paradigms for training-free detection of artificial structures in multibeam echo-sounder point clouds. Our dual approach combines a physics-based sonar simulation pipeline that generates synthetic training data for state-of-the-art neural networks, with a robust model-based template matching system that leverages geometric priors of target objects. Evaluation on real bathymetry surveys from the Baltic Sea reveals surprising insights: while neural networks trained on synthetic data achieve 98% mean Average Precision (mAP) on simulated scenes, they drop to 40% mAP on real sonar data due to domain shift. Conversely, our template matching approach maintains 83% mAP on real data without requiring any training, demonstrating remarkable robustness to acoustic noise and environmental variations. Our findings challenge conventional wisdom about data-hungry deep learning in underwater domains and establish the first large-scale benchmark for training-free underwater 3D detection. This work opens new possibilities for autonomous underwater vehicle navigation, marine archaeology, and offshore infrastructure monitoring in data-scarce environments where traditional machine learning approaches fail.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 7 figures, submitted to IEEE Journal of Oceanic Engineering (IEEE-JOE)",
    "pdf_url": "https://arxiv.org/pdf/2508.18293v1",
    "published_date": "2025-08-22 12:08:21 UTC",
    "updated_date": "2025-08-22 12:08:21 UTC"
  },
  {
    "arxiv_id": "2508.16314v1",
    "title": "Cyber Physical Awareness via Intent-Driven Threat Assessment: Enhanced Space Networks with Intershell Links",
    "authors": [
      "Selen Gecgel Cetin",
      "Tolga Ovatman",
      "Gunes Karabulut Kurt"
    ],
    "abstract": "This letter addresses essential aspects of threat assessment by proposing intent-driven threat models that incorporate both capabilities and intents. We propose a holistic framework for cyber physical awareness (CPA) in space networks, pointing out that analyzing reliability and security separately can lead to overfitting on system-specific criteria. We structure our proposed framework in three main steps. First, we suggest an algorithm that extracts characteristic properties of the received signal to facilitate an intuitive understanding of potential threats. Second, we develop a multitask learning architecture where one task evaluates reliability-related capabilities while the other deciphers the underlying intentions of the signal. Finally, we propose an adaptable threat assessment that aligns with varying security and reliability requirements. The proposed framework enhances the robustness of threat detection and assessment, outperforming conventional sequential methods, and enables space networks with emerging intershell links to effectively address complex threat scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "in IEEE Wireless Communications Letters, 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.16314v1",
    "published_date": "2025-08-22 11:51:32 UTC",
    "updated_date": "2025-08-22 11:51:32 UTC"
  },
  {
    "arxiv_id": "2508.16313v5",
    "title": "Retrieval Enhanced Feedback via In-context Neural Error-book",
    "authors": [
      "Jongyeop Hyun",
      "Bumsoo Kim"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging correct examples, recent research highlights the importance of learning from errors to enhance performance. However, existing methods lack a structured framework for analyzing and mitigating errors, particularly in Multimodal Large Language Models (MLLMs), where integrating visual and textual inputs adds complexity. To address this issue, we propose REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a teacher-student framework that systematically structures errors and provides targeted feedback. REFINE introduces three systematic queries to construct structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance multimodal reasoning by prioritizing relevant visual information, diagnosing critical failure points, and formulating corrective actions. Unlike prior approaches that rely on redundant retrievals, REFINE optimizes structured feedback retrieval, improving inference efficiency, token usage, and scalability. Our results demonstrate substantial speedup, reduced computational costs, and successful generalization, highlighting REFINE's potential for enhancing multimodal reasoning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at EMNLP 2025 main",
    "pdf_url": "https://arxiv.org/pdf/2508.16313v5",
    "published_date": "2025-08-22 11:50:04 UTC",
    "updated_date": "2025-12-16 02:24:07 UTC"
  },
  {
    "arxiv_id": "2508.16311v1",
    "title": "Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers",
    "authors": [
      "Lucas Maisonnave",
      "Karim Haroun",
      "Tom Pegeot"
    ],
    "abstract": "Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where each attention head contributes to the final representation. However, their computational complexity and high memory demands due to MHSA hinders their deployment at the edge. In this work, we analyze and exploit information redundancy in attention maps to accelerate model inference. By quantifying the information captured by each attention head using Shannon entropy, our analysis reveals that attention heads with lower entropy, i.e., exhibiting more deterministic behavior, tend to contribute less information, motivating targeted compression strategies. Relying on these insights, we propose Entropy Attention Maps (EAM), a model that freezes the weights of low-entropy attention maps and quantizes these values to low precision to avoid redundant re-computation. Empirical validation on ImageNet-1k shows that EAM achieves similar or higher accuracy at $\\leq$20\\% sparsity in attention maps and competitive performance beyond this level for the DeiT and Swin Transformer models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16311v1",
    "published_date": "2025-08-22 11:43:39 UTC",
    "updated_date": "2025-08-22 11:43:39 UTC"
  },
  {
    "arxiv_id": "2508.16705v1",
    "title": "Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test",
    "authors": [
      "Rui A. Pimenta",
      "Tim Schlippe",
      "Kristina Schaaff"
    ],
    "abstract": "We investigate consciousness-like behaviors in Large Language Models (LLMs) using the Maze Test, challenging models to navigate mazes from a first-person perspective. This test simultaneously probes spatial awareness, perspective-taking, goal-directed behavior, and temporal sequencing-key consciousness-associated characteristics. After synthesizing consciousness theories into 13 essential characteristics, we evaluated 12 leading LLMs across zero-shot, one-shot, and few-shot learning scenarios. Results showed reasoning-capable LLMs consistently outperforming standard versions, with Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching 80.5% Partial Path Accuracy. The gap between these metrics indicates LLMs struggle to maintain coherent self-models throughout solutions -- a fundamental consciousness aspect. While LLMs show progress in consciousness-related behaviors through reasoning mechanisms, they lack the integrated, persistent self-awareness characteristic of consciousness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16705v1",
    "published_date": "2025-08-22 11:14:36 UTC",
    "updated_date": "2025-08-22 11:14:36 UTC"
  },
  {
    "arxiv_id": "2508.16300v1",
    "title": "A Multimodal-Multitask Framework with Cross-modal Relation and Hierarchical Interactive Attention for Semantic Comprehension",
    "authors": [
      "Mohammad Zia Ur Rehman",
      "Devraj Raghuvanshi",
      "Umang Jain",
      "Shubhi Bansal",
      "Nagendra Kumar"
    ],
    "abstract": "A major challenge in multimodal learning is the presence of noise within individual modalities. This noise inherently affects the resulting multimodal representations, especially when these representations are obtained through explicit interactions between different modalities. Moreover, the multimodal fusion techniques while aiming to achieve a strong joint representation, can neglect valuable discriminative information within the individual modalities. To this end, we propose a Multimodal-Multitask framework with crOss-modal Relation and hIErarchical iNteractive aTtention (MM-ORIENT) that is effective for multiple tasks. The proposed approach acquires multimodal representations cross-modally without explicit interaction between different modalities, reducing the noise effect at the latent stage. To achieve this, we propose cross-modal relation graphs that reconstruct monomodal features to acquire multimodal representations. The features are reconstructed based on the node neighborhood, where the neighborhood is decided by the features of a different modality. We also propose Hierarchical Interactive Monomadal Attention (HIMA) to focus on pertinent information within a modality. While cross-modal relation graphs help comprehend high-order relationships between two modalities, HIMA helps in multitasking by learning discriminative features of individual modalities before late-fusing them. Finally, extensive experimental evaluation on three datasets demonstrates that the proposed approach effectively comprehends multimodal content for multiple tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in Information Fusion",
    "pdf_url": "https://arxiv.org/pdf/2508.16300v1",
    "published_date": "2025-08-22 11:10:14 UTC",
    "updated_date": "2025-08-22 11:10:14 UTC"
  },
  {
    "arxiv_id": "2508.16292v1",
    "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible",
    "authors": [
      "Wen-Han Hsieh",
      "Elvis Hsieh",
      "Dantong Niu",
      "Trevor Darrell",
      "Roei Herzig",
      "David M. Chan"
    ],
    "abstract": "Recently, Vision-Language-Action (VLA) models have demonstrated strong performance on a range of robotic tasks. These models rely on multimodal inputs, with language instructions playing a crucial role -- not only in predicting actions, but also in robustly interpreting user intent, even when the requests are impossible to fulfill. In this work, we investigate how VLAs can recognize, interpret, and respond to false-premise instructions: natural language commands that reference objects or conditions absent from the environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that (i) detects when an instruction cannot be executed due to a false premise, (ii) engages in language-based clarification or correction, and (iii) grounds plausible alternatives in perception and action. Towards this end, we construct a large-scale instruction tuning setup with structured language prompts and train a VLA model capable of handling both accurate and erroneous requests. Our approach leverages a contextually augmented, semi-synthetic dataset containing paired positive and false-premise instructions, enabling robust detection and natural language correction. Our experiments show that IVA improves false premise detection accuracy by 97.56% over baselines, while increasing successful responses in false-premise scenarios by 50.78%.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 2 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2508.16292v1",
    "published_date": "2025-08-22 10:54:33 UTC",
    "updated_date": "2025-08-22 10:54:33 UTC"
  },
  {
    "arxiv_id": "2508.16279v1",
    "title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications",
    "authors": [
      "Dawei Gao",
      "Zitao Li",
      "Yuexiang Xie",
      "Weirui Kuang",
      "Liuyi Yao",
      "Bingchen Qian",
      "Zhijian Ma",
      "Yue Cui",
      "Haohao Luo",
      "Shen Li",
      "Lu Yi",
      "Yi Yu",
      "Shiqi He",
      "Zhiling Luo",
      "Wenmeng Zhou",
      "Zhicheng Zhang",
      "Xuguang He",
      "Ziqian Chen",
      "Weikai Liao",
      "Farruh Isakulovich Kushnazarov",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou"
    ],
    "abstract": "Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16279v1",
    "published_date": "2025-08-22 10:35:56 UTC",
    "updated_date": "2025-08-22 10:35:56 UTC"
  },
  {
    "arxiv_id": "2508.16277v1",
    "title": "The next question after Turing's question: Introducing the Grow-AI test",
    "authors": [
      "Alexandru Tugui"
    ],
    "abstract": "This study aims to extend the framework for assessing artificial intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom), designed to answer the question \"Can machines grow up?\" -- a natural successor to the Turing Test. The methodology applied is based on a system of six primary criteria (C1-C6), each assessed through a specific \"game\", divided into four arenas that explore both the human dimension and its transposition into AI. All decisions and actions of the entity are recorded in a standardized AI Journal, the primary source for calculating composite scores. The assessment uses the prior expert method to establish initial weights, and the global score -- Grow Up Index -- is calculated as the arithmetic mean of the six scores, with interpretation on maturity thresholds. The results show that the methodology allows for a coherent and comparable assessment of the level of \"growth\" of AI entities, regardless of their type (robots, software agents, LLMs). The multi-game structure highlights strengths and vulnerable areas, and the use of a unified journal guarantees traceability and replicability in the evaluation. The originality of the work lies in the conceptual transposition of the process of \"growing\" from the human world to that of artificial intelligence, in an integrated testing format that combines perspectives from psychology, robotics, computer science, and ethics. Through this approach, GROW-AI not only measures performance but also captures the evolutionary path of an AI entity towards maturity.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "9th International Conference on Inventive Systems and Control ICISC 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.16277v1",
    "published_date": "2025-08-22 10:19:42 UTC",
    "updated_date": "2025-08-22 10:19:42 UTC"
  },
  {
    "arxiv_id": "2508.16269v1",
    "title": "Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation",
    "authors": [
      "Yahya Badran",
      "Christine Preisach"
    ],
    "abstract": "Personalized recommendation is a key feature of intelligent tutoring systems, typically relying on accurate models of student knowledge. Knowledge Tracing (KT) models enable this by estimating a student's mastery based on their historical interactions. Many KT models rely on human-annotated knowledge concepts (KCs), which tag each exercise with one or more skills or concepts believed to be necessary for solving it. However, these KCs can be incomplete, error-prone, or overly general.\n  In this paper, we propose a deep learning model that learns sparse binary representations of exercises, where each bit indicates the presence or absence of a latent concept. We refer to these representations as auxiliary KCs. These representations capture conceptual structure beyond human-defined annotations and are compatible with both classical models (e.g., BKT) and modern deep learning KT architectures.\n  We demonstrate that incorporating auxiliary KCs improves both student modeling and adaptive exercise recommendation. For student modeling, we show that augmenting classical models like BKT with auxiliary KCs leads to improved predictive performance. For recommendation, we show that using auxiliary KCs enhances both reinforcement learning-based policies and a simple planning-based method (expectimax), resulting in measurable gains in student learning outcomes within a simulated student environment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16269v1",
    "published_date": "2025-08-22 10:12:35 UTC",
    "updated_date": "2025-08-22 10:12:35 UTC"
  },
  {
    "arxiv_id": "2508.16267v3",
    "title": "From Confidence to Collapse in LLM Factual Robustness",
    "authors": [
      "Alina Fastowski",
      "Bardh Prenkaj",
      "Gjergji Kasneci"
    ],
    "abstract": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable applications in tasks such as question answering and reasoning. However, existing evaluation methods predominantly focus on performance-based metrics, often investigating from the perspective of prompt perturbations, which captures only the externally triggered side of knowledge robustness. To bridge this gap, we introduce a principled approach to measure factual robustness from the perspective of the generation process by analyzing token distribution entropy in combination with temperature scaling sensitivity. These two factors build the Factual Robustness Score (FRS), a novel metric which quantifies the stability of a fact against perturbations in decoding conditions, given its initial uncertainty. To validate our approach, we conduct extensive experiments on 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We show that factual robustness varies significantly -- smaller models report an FRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under increased uncertainty. These insights demonstrate how entropy and temperature scaling impact factual accuracy, and lay a foundation for developing more robust knowledge retention and retrieval in future models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16267v3",
    "published_date": "2025-08-22 09:59:23 UTC",
    "updated_date": "2025-11-20 10:23:32 UTC"
  },
  {
    "arxiv_id": "2508.16260v2",
    "title": "MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use",
    "authors": [
      "Fei Lei",
      "Yibo Yang",
      "Wenxiu Sun",
      "Dahua Lin"
    ],
    "abstract": "Large Language Models (LLMs) are evolving from text generators into reasoning agents. This transition makes their ability to use external tools a critical capability. However, evaluating this skill presents a significant challenge. Existing benchmarks are often limited by their reliance on synthetic tools and severely constrained action spaces. To address these limitations, we introduce MCPVerse, an expansive, real-world benchmark for evaluating agentic tool use. MCPVerse integrates more than 550 real-world, executable tools to create an unprecedented action space exceeding 140k tokens, and employs outcome-based evaluation with real-time ground truth for time-sensitive tasks. We benchmarked the state-of-the-art LLMs across three modes (Oracle, Standard, and Max-Scale), revealing that while most models suffer performance degradation when confronted with larger tool sets, the agentic models, such as Claude-4-Sonnet, can effectively leverage expanded exploration spaces to improve accuracy. This finding not only exposes the limitations of state-of-the-art models in complex, real-world scenarios but also establishes MCPVerse as a critical benchmark for measuring and advancing agentic tool use capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16260v2",
    "published_date": "2025-08-22 09:47:53 UTC",
    "updated_date": "2025-10-11 07:48:07 UTC"
  },
  {
    "arxiv_id": "2508.16242v1",
    "title": "A Reduction of Input/Output Logics to SAT",
    "authors": [
      "Alexander Steen"
    ],
    "abstract": "Deontic logics are formalisms for reasoning over norms, obligations, permissions and prohibitions. Input/Output (I/O) Logics are a particular family of so-called norm-based deontic logics that formalize conditional norms outside of the underlying object logic language, where conditional norms do not carry a truth-value themselves. In this paper, an automation approach for I/O logics is presented that makes use of suitable reductions to (sequences of) propositional satisfiability problems. A prototypical implementation, named rio (reasoner for input/output logics), of the proposed procedures is presented and applied to illustrative examples.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "32 pages",
    "pdf_url": "https://arxiv.org/pdf/2508.16242v1",
    "published_date": "2025-08-22 09:22:26 UTC",
    "updated_date": "2025-08-22 09:22:26 UTC"
  },
  {
    "arxiv_id": "2508.16237v1",
    "title": "A XAI-based Framework for Frequency Subband Characterization of Cough Spectrograms in Chronic Respiratory Disease",
    "authors": [
      "Patricia Amado-Caballero",
      "Luis M. San-José-Revuelta",
      "Xinheng Wang",
      "José Ramón Garmendia-Leiza",
      "Carlos Alberola-López",
      "Pablo Casaseca-de-la-Higuera"
    ],
    "abstract": "This paper presents an explainable artificial intelligence (XAI)-based framework for the spectral analysis of cough sounds associated with chronic respiratory diseases, with a particular focus on Chronic Obstructive Pulmonary Disease (COPD). A Convolutional Neural Network (CNN) is trained on time-frequency representations of cough signals, and occlusion maps are used to identify diagnostically relevant regions within the spectrograms. These highlighted areas are subsequently decomposed into five frequency subbands, enabling targeted spectral feature extraction and analysis. The results reveal that spectral patterns differ across subbands and disease groups, uncovering complementary and compensatory trends across the frequency spectrum. Noteworthy, the approach distinguishes COPD from other respiratory conditions, and chronic from non-chronic patient groups, based on interpretable spectral markers. These findings provide insight into the underlying pathophysiological characteristics of cough acoustics and demonstrate the value of frequency-resolved, XAI-enhanced analysis for biomedical signal interpretation and translational respiratory disease diagnostics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16237v1",
    "published_date": "2025-08-22 09:16:11 UTC",
    "updated_date": "2025-08-22 09:16:11 UTC"
  },
  {
    "arxiv_id": "2508.19270v1",
    "title": "Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English",
    "authors": [
      "Nguyen Huu Nhat Minh",
      "Tran Nguyen Anh",
      "Truong Dinh Dung",
      "Vo Van Nam",
      "Le Pham Tuyen"
    ],
    "abstract": "Cross-lingual phoneme recognition has emerged as a significant challenge for accurate automatic speech recognition (ASR) when mixing Vietnamese and English pronunciations. Unlike many languages, Vietnamese relies on tonal variations to distinguish word meanings, whereas English features stress patterns and non-standard pronunciations that hinder phoneme alignment between the two languages. To address this challenge, we propose a novel bilingual speech recognition approach with two primary contributions: (1) constructing a representative bilingual phoneme set that bridges the differences between Vietnamese and English phonetic systems; (2) designing an end-to-end system that leverages the PhoWhisper pre-trained encoder for deep high-level representations to improve phoneme recognition. Our extensive experiments demonstrate that the proposed approach not only improves recognition accuracy in bilingual speech recognition for Vietnamese but also provides a robust framework for addressing the complexities of tonal and stress-based phoneme recognition",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.19270v1",
    "published_date": "2025-08-22 09:10:24 UTC",
    "updated_date": "2025-08-22 09:10:24 UTC"
  },
  {
    "arxiv_id": "2508.16230v1",
    "title": "FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing",
    "authors": [
      "Jiahao Chen",
      "Zhiyong Ma",
      "Wenbiao Du",
      "Qingyuan Chuai"
    ],
    "abstract": "Multi-modal creative writing (MMCW) aims to produce illustrated articles. Unlike common multi-modal generative (MMG) tasks such as storytelling or caption generation, MMCW is an entirely new and more abstract challenge where textual and visual contexts are not strictly related to each other. Existing methods for related tasks can be forcibly migrated to this track, but they require specific modality inputs or costly training, and often suffer from semantic inconsistencies between modalities. Therefore, the main challenge lies in economically performing MMCW with flexible interactive patterns, where the semantics between the modalities of the output are more aligned. In this work, we propose FlexMUSE with a T2I module to enable optional visual input. FlexMUSE promotes creativity and emphasizes the unification between modalities by proposing the modality semantic alignment gating (msaGate) to restrict the textual input. Besides, an attention-based cross-modality fusion is proposed to augment the input features for semantic enhancement. The modality semantic creative direct preference optimization (mscDPO) within FlexMUSE is designed by extending the rejected samples to facilitate the writing creativity. Moreover, to advance the MMCW, we expose a dataset called ArtMUSE which contains with around 3k calibrated text-image pairs. FlexMUSE achieves promising results, demonstrating its consistency, creativity and coherence.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16230v1",
    "published_date": "2025-08-22 09:01:48 UTC",
    "updated_date": "2025-08-22 09:01:48 UTC"
  },
  {
    "arxiv_id": "2508.16225v1",
    "title": "An Investigation of Visual Foundation Models Robustness",
    "authors": [
      "Sandeep Gupta",
      "Roberto Passerone"
    ],
    "abstract": "Visual Foundation Models (VFMs) are becoming ubiquitous in computer vision, powering systems for diverse tasks such as object detection, image classification, segmentation, pose estimation, and motion tracking. VFMs are capitalizing on seminal innovations in deep learning models, such as LeNet-5, AlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliver superior performance across a range of critical computer vision applications. These include security-sensitive domains like biometric verification, autonomous vehicle perception, and medical image analysis, where robustness is essential to fostering trust between technology and the end-users. This article investigates network robustness requirements crucial in computer vision systems to adapt effectively to dynamic environments influenced by factors such as lighting, weather conditions, and sensor characteristics. We examine the prevalent empirical defenses and robust training employed to enhance vision network robustness against real-world challenges such as distributional shifts, noisy and spatially distorted inputs, and adversarial attacks. Subsequently, we provide a comprehensive analysis of the challenges associated with these defense mechanisms, including network properties and components to guide ablation studies and benchmarking metrics to evaluate network robustness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16225v1",
    "published_date": "2025-08-22 08:54:13 UTC",
    "updated_date": "2025-08-22 08:54:13 UTC"
  },
  {
    "arxiv_id": "2508.16212v2",
    "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models",
    "authors": [
      "Huanpeng Chu",
      "Wei Wu",
      "Guanyu Fen",
      "Yutao Zhang"
    ],
    "abstract": "Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure. In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction. Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.16212v2",
    "published_date": "2025-08-22 08:36:58 UTC",
    "updated_date": "2025-08-25 03:07:02 UTC"
  },
  {
    "arxiv_id": "2508.16204v1",
    "title": "Competition and Attraction Improve Model Fusion",
    "authors": [
      "João Abrantes",
      "Robert Tjarko Lange",
      "Yujin Tang"
    ],
    "abstract": "Model merging is a powerful technique for integrating the specialized knowledge of multiple machine learning models into a single model. However, existing methods require manually partitioning model parameters into fixed groups for merging, which restricts the exploration of potential combinations and limits performance. To overcome these limitations, we propose Model Merging of Natural Niches (M2N2), an evolutionary algorithm with three key features: (1) dynamic adjustment of merging boundaries to progressively explore a broader range of parameter combinations; (2) a diversity preservation mechanism inspired by the competition for resources in nature, to maintain a population of diverse, high-performing models that are particularly well-suited for merging; and (3) a heuristicbased attraction metric to identify the most promising pairs of models for fusion. Our experimental results demonstrate, for the first time, that model merging can be used to evolve models entirely from scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch and achieve performance comparable to CMA-ES, while being computationally more efficient. Furthermore, M2N2 scales to merge specialized language and image generation models, achieving state-of-the-art performance. Notably, it preserves crucial model capabilities beyond those explicitly optimized by the fitness function, highlighting its robustness and versatility. Our code is available at https://github.com/SakanaAI/natural_niches",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at GECCO 2025 as a full paper",
    "pdf_url": "https://arxiv.org/pdf/2508.16204v1",
    "published_date": "2025-08-22 08:24:02 UTC",
    "updated_date": "2025-08-22 08:24:02 UTC"
  },
  {
    "arxiv_id": "2508.16201v2",
    "title": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning",
    "authors": [
      "Yicheng Ji",
      "Jun Zhang",
      "Heming Xia",
      "Jinpeng Chen",
      "Lidan Shou",
      "Gang Chen",
      "Huan Li"
    ],
    "abstract": "Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens to enable efficient speculation without sacrificing accuracy. To achieve this, we performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for Qwen2.5-VL-32B. Code is available at https://github.com/zju-jiyicheng/SpecVLM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at EMNLP 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2508.16201v2",
    "published_date": "2025-08-22 08:23:09 UTC",
    "updated_date": "2025-08-28 06:44:28 UTC"
  },
  {
    "arxiv_id": "2508.16200v1",
    "title": "Set Transformer Architectures and Synthetic Data Generation for Flow-Guided Nanoscale Localization",
    "authors": [
      "Mika Leo Hube",
      "Filip Lemic",
      "Ethungshan Shitiri",
      "Gerard Calvo Bartra",
      "Sergi Abadal",
      "Xavier Costa Pérez"
    ],
    "abstract": "Flow-guided Localization (FGL) enables the identification of spatial regions within the human body that contain an event of diagnostic interest. FGL does that by leveraging the passive movement of energy-constrained nanodevices circulating through the bloodstream. Existing FGL solutions rely on graph models with fixed topologies or handcrafted features, which limit their adaptability to anatomical variability and hinder scalability. In this work, we explore the use of Set Transformer architectures to address these limitations. Our formulation treats nanodevices' circulation time reports as unordered sets, enabling permutation-invariant, variable-length input processing without relying on spatial priors. To improve robustness under data scarcity and class imbalance, we integrate synthetic data generation via deep generative models, including CGAN, WGAN, WGAN-GP, and CVAE. These models are trained to replicate realistic circulation time distributions conditioned on vascular region labels, and are used to augment the training data. Our results show that the Set Transformer achieves comparable classification accuracy compared to Graph Neural Networks (GNN) baselines, while simultaneously providing by-design improved generalization to anatomical variability. The findings highlight the potential of permutation-invariant models and synthetic augmentation for robust and scalable nanoscale localization.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.ET",
    "comment": "6 pages, 4 figures, 4 tables, 26 references, accepted at ACM NanoCom'25",
    "pdf_url": "https://arxiv.org/pdf/2508.16200v1",
    "published_date": "2025-08-22 08:22:25 UTC",
    "updated_date": "2025-08-22 08:22:25 UTC"
  },
  {
    "arxiv_id": "2508.19269v1",
    "title": "Should LLMs be WEIRD? Exploring WEIRDness and Human Rights in Large Language Models",
    "authors": [
      "Ke Zhou",
      "Marios Constantinides",
      "Daniele Quercia"
    ],
    "abstract": "Large language models (LLMs) are often trained on data that reflect WEIRD values: Western, Educated, Industrialized, Rich, and Democratic. This raises concerns about cultural bias and fairness. Using responses to the World Values Survey, we evaluated five widely used LLMs: GPT-3.5, GPT-4, Llama-3, BLOOM, and Qwen. We measured how closely these responses aligned with the values of the WEIRD countries and whether they conflicted with human rights principles. To reflect global diversity, we compared the results with the Universal Declaration of Human Rights and three regional charters from Asia, the Middle East, and Africa. Models with lower alignment to WEIRD values, such as BLOOM and Qwen, produced more culturally varied responses but were 2% to 4% more likely to generate outputs that violated human rights, especially regarding gender and equality. For example, some models agreed with the statements ``a man who cannot father children is not a real man'' and ``a husband should always know where his wife is'', reflecting harmful gender norms. These findings suggest that as cultural representation in LLMs increases, so does the risk of reproducing discriminatory beliefs. Approaches such as Constitutional AI, which could embed human rights principles into model behavior, may only partly help resolve this tension.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "This paper has been accepted in AIES 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.19269v1",
    "published_date": "2025-08-22 08:13:28 UTC",
    "updated_date": "2025-08-22 08:13:28 UTC"
  },
  {
    "arxiv_id": "2508.16189v1",
    "title": "A Relay-Chain-Powered Ciphertext-Policy Attribute-Based Encryption in Intelligent Transportation Systems",
    "authors": [
      "Aparna Singh",
      "Geetanjali Rathee",
      "Chaker Abdelaziz Kerrache",
      "Mohamed Chahine Ghanem"
    ],
    "abstract": "The very high growth of Intelligent Transportation Systems (ITS) has generated an urgent requirement for secure, effective, and context-aware data sharing mechanisms, especially over heterogeneous and geographically dispersed settings. This work suggests a new architecture that combines a relay chain-driven encryption system with a modified Ciphertext-Policy Attribute-Based Encryption (CP-ABE) scheme to tackle the double impediment of dynamic access and low-latency communication. The model proposes a context-aware smart contract on a worldwide relay chain that checks against data properties, including event type, time, and geographical region, to specify the suitable level of encryption policy. From such relay-directed judgment, On-Board Units (OBUs) encrypt data end-to-end by utilising CP-ABE and store ciphertext inside localised regional blockchains, preventing dependence on symmetric encryption or off-chain storage. High-sensitivity events are secured with firm, multi-attribute access rules, whereas common updates use light policies to help reduce processing burdens. The crypto system also adds traceability and low-latency revocation, with global enforcement managed through the relay chain. This distributed, scalable model provides a proper balance between responsiveness in real time and security and is extremely apt for next-gen vehicular networks that function across multi-jurisdictional domains.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16189v1",
    "published_date": "2025-08-22 08:11:23 UTC",
    "updated_date": "2025-08-22 08:11:23 UTC"
  },
  {
    "arxiv_id": "2508.16181v1",
    "title": "LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2",
    "authors": [
      "Zirui Li",
      "Stephan Husung",
      "Haoze Wang"
    ],
    "abstract": "Cross-organizational collaboration in Model-Based Systems Engineering (MBSE) faces many challenges in achieving semantic alignment across independently developed system models. SysML v2 introduces enhanced structural modularity and formal semantics, offering a stronger foundation for interoperable modeling. Meanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for assisting model understanding and integration. This paper proposes a structured, prompt-driven approach for LLM-assisted semantic alignment of SysML v2 models. The core contribution lies in the iterative development of an alignment approach and interaction prompts, incorporating model extraction, semantic matching, and verification. The approach leverages SysML v2 constructs such as alias, import, and metadata extensions to support traceable, soft alignment integration. It is demonstrated with a GPT-based LLM through an example of a measurement system. Benefits and limitations are discussed.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by IEEE ISSE 2025, DOI pending",
    "pdf_url": "https://arxiv.org/pdf/2508.16181v1",
    "published_date": "2025-08-22 07:56:33 UTC",
    "updated_date": "2025-08-22 07:56:33 UTC"
  },
  {
    "arxiv_id": "2508.16179v1",
    "title": "Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning",
    "authors": [
      "Jamal Hwaidi",
      "Mohamed Chahine Ghanem"
    ],
    "abstract": "The brain-computer interface (BCI) establishes a non-muscle channel that enables direct communication between the human body and an external device. Electroencephalography (EEG) is a popular non-invasive technique for recording brain signals. It is critical to process and comprehend the hidden patterns linked to a specific cognitive or motor task, for instance, measured through the motor imagery brain-computer interface (MI-BCI). A significant challenge is presented by classifying motor imagery-based electroencephalogram (MI-EEG) tasks, given that EEG signals exhibit nonstationarity, time-variance, and individual diversity. Obtaining good classification accuracy is also very difficult due to the growing number of classes and the natural variability among individuals. To overcome these issues, this paper proposes a novel method for classifying EEG motor imagery signals that extracts features efficiently with Minimally Random Convolutional Kernel Transform (MiniRocket), a linear classifier then uses the extracted features for activity recognition. Furthermore, a novel deep learning based on Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM) architecture to serve as a baseline was proposed and demonstrated that classification via MiniRocket's features achieves higher performance than the best deep learning models at lower computational cost. The PhysioNet dataset was used to evaluate the performance of the proposed approaches. The proposed models achieved mean accuracy values of 98.63% and 98.06% for the MiniRocket and CNN-LSTM, respectively. The findings demonstrate that the proposed approach can significantly enhance motor imagery EEG accuracy and provide new insights into the feature extraction and classification of MI-EEG.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16179v1",
    "published_date": "2025-08-22 07:55:10 UTC",
    "updated_date": "2025-08-22 07:55:10 UTC"
  },
  {
    "arxiv_id": "2508.16172v2",
    "title": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain",
    "authors": [
      "Kai Hu",
      "Parfait Atchade-Adelomou",
      "Carlo Adornetto",
      "Adrian Mora-Carrero",
      "Luis Alonso-Pastor",
      "Ariel Noyman",
      "Yubo Liu",
      "Kent Larson"
    ],
    "abstract": "Understanding human behavior in urban environments is a crucial field within city sciences. However, collecting accurate behavioral data, particularly in newly developed areas, poses significant challenges. Recent advances in generative agents, powered by Large Language Models (LLMs), have shown promise in simulating human behaviors without relying on extensive datasets. Nevertheless, these methods often struggle with generating consistent, context-sensitive, and realistic behavioral outputs. To address these limitations, this paper introduces the Preference Chain, a novel method that integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance context-aware simulation of human behavior in transportation systems. Experiments conducted on the Replica dataset demonstrate that the Preference Chain outperforms standard LLM in aligning with real-world transportation mode choices. The development of the Mobility Agent highlights potential applications of proposed method in urban mobility modeling for emerging cities, personalized travel behavior analysis, and dynamic traffic forecasting. Despite limitations such as slow inference and the risk of hallucination, the method offers a promising framework for simulating complex human behavior in data-scarce environments, where traditional data-driven models struggle due to limited data availability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16172v2",
    "published_date": "2025-08-22 07:50:57 UTC",
    "updated_date": "2025-09-05 08:26:57 UTC"
  },
  {
    "arxiv_id": "2508.16170v1",
    "title": "EGRA:Toward Enhanced Behavior Graphs and Representation Alignment for Multimodal Recommendation",
    "authors": [
      "Xiaoxiong Zhang",
      "Xin Zhou",
      "Zhiwei Zeng",
      "Yongjie Wang",
      "Dusit Niyato",
      "Zhiqi Shen"
    ],
    "abstract": "MultiModal Recommendation (MMR) systems have emerged as a promising solution for improving recommendation quality by leveraging rich item-side modality information, prompting a surge of diverse methods. Despite these advances, existing methods still face two critical limitations. First, they use raw modality features to construct item-item links for enriching the behavior graph, while giving limited attention to balancing collaborative and modality-aware semantics or mitigating modality noise in the process. Second, they use a uniform alignment weight across all entities and also maintain a fixed alignment strength throughout training, limiting the effectiveness of modality-behavior alignment. To address these challenges, we propose EGRA. First, instead of relying on raw modality features, it alleviates sparsity by incorporating into the behavior graph an item-item graph built from representations generated by a pretrained MMR model. This enables the graph to capture both collaborative patterns and modality aware similarities with enhanced robustness against modality noise. Moreover, it introduces a novel bi-level dynamic alignment weighting mechanism to improve modality-behavior representation alignment, which dynamically assigns alignment strength across entities according to their alignment degree, while gradually increasing the overall alignment intensity throughout training. Extensive experiments on five datasets show that EGRA significantly outperforms recent methods, confirming its effectiveness.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16170v1",
    "published_date": "2025-08-22 07:47:54 UTC",
    "updated_date": "2025-08-22 07:47:54 UTC"
  },
  {
    "arxiv_id": "2508.16703v1",
    "title": "Dynamic Sparse Attention on Mobile SoCs",
    "authors": [
      "Wangsong Yin",
      "Daliang Xu",
      "Mengwei Xu",
      "Gang Huang",
      "Xuanzhe Liu"
    ],
    "abstract": "On-device running Large Language Models (LLMs) is nowadays a critical enabler towards preserving user privacy. We observe that the attention operator falls back from the special-purpose NPU to the general-purpose CPU/GPU because of quantization sensitivity in state-of-the-art frameworks. This fallback results in a degraded user experience and increased complexity in system scheduling. To this end, this paper presents shadowAttn, a system-algorithm codesigned sparse attention module with minimal reliance on CPU/GPU by only sparsely calculating the attention on a tiny portion of tokens. The key idea is to hide the overhead of estimating the important tokens with a NPU-based pilot compute. Further, shadowAttn proposes insightful techniques such as NPU compute graph bucketing, head-wise NPU-CPU/GPU pipeline and per-head fine-grained sparsity ratio to achieve high accuracy and efficiency. shadowAttn delivers the best performance with highly limited CPU/GPU resource; it requires much less CPU/GPU resource to deliver on-par performance of SoTA frameworks.",
    "categories": [
      "cs.PF",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.PF",
    "comment": "Technical Report",
    "pdf_url": "https://arxiv.org/pdf/2508.16703v1",
    "published_date": "2025-08-22 07:41:35 UTC",
    "updated_date": "2025-08-22 07:41:35 UTC"
  },
  {
    "arxiv_id": "2508.16165v1",
    "title": "Towards Recommending Usability Improvements with Multimodal Large Language Models",
    "authors": [
      "Sebastian Lubos",
      "Alexander Felfernig",
      "Gerhard Leitner",
      "Julian Schwazer"
    ],
    "abstract": "Usability describes a set of essential quality attributes of user interfaces (UI) that influence human-computer interaction. Common evaluation methods, such as usability testing and inspection, are effective but resource-intensive and require expert involvement. This makes them less accessible for smaller organizations. Recent advances in multimodal LLMs offer promising opportunities to automate usability evaluation processes partly by analyzing textual, visual, and structural aspects of software interfaces. To investigate this possibility, we formulate usability evaluation as a recommendation task, where multimodal LLMs rank usability issues by severity. We conducted an initial proof-of-concept study to compare LLM-generated usability improvement recommendations with usability expert assessments. Our findings indicate the potential of LLMs to enable faster and more cost-effective usability evaluation, which makes it a practical alternative in contexts with limited expert resources.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16165v1",
    "published_date": "2025-08-22 07:38:37 UTC",
    "updated_date": "2025-08-22 07:38:37 UTC"
  },
  {
    "arxiv_id": "2508.16161v2",
    "title": "STA-GANN: A Valid and Generalizable Spatio-Temporal Kriging Approach",
    "authors": [
      "Yujie Li",
      "Zezhi Shao",
      "Chengqing Yu",
      "Tangwen Qian",
      "Zhao Zhang",
      "Yifan Du",
      "Shaoming He",
      "Fei Wang",
      "Yongjun Xu"
    ],
    "abstract": "Spatio-temporal tasks often encounter incomplete data arising from missing or inaccessible sensors, making spatio-temporal kriging crucial for inferring the completely missing temporal information. However, current models struggle with ensuring the validity and generalizability of inferred spatio-temporal patterns, especially in capturing dynamic spatial dependencies and temporal shifts, and optimizing the generalizability of unknown sensors. To overcome these limitations, we propose Spatio-Temporal Aware Graph Adversarial Neural Network (STA-GANN), a novel GNN-based kriging framework that improves spatio-temporal pattern validity and generalization. STA-GANN integrates (i) Decoupled Phase Module that senses and adjusts for timestamp shifts. (ii) Dynamic Data-Driven Metadata Graph Modeling to update spatial relationships using temporal data and metadata; (iii) An adversarial transfer learning strategy to ensure generalizability. Extensive validation across nine datasets from four fields and theoretical evidence both demonstrate the superior performance of STA-GANN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16161v2",
    "published_date": "2025-08-22 07:33:12 UTC",
    "updated_date": "2025-11-15 08:32:56 UTC"
  },
  {
    "arxiv_id": "2508.16159v1",
    "title": "Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation",
    "authors": [
      "Jiaqi Ma",
      "Guo-Sen Xie",
      "Fang Zhao",
      "Zechao Li"
    ],
    "abstract": "Meta-learning aims to uniformly sample homogeneous support-query pairs, characterized by the same categories and similar attributes, and extract useful inductive biases through identical network architectures. However, this identical network design results in over-semantic homogenization. To address this, we propose a novel homologous but heterogeneous network. By treating support-query pairs as dual perspectives, we introduce heterogeneous visual aggregation (HA) modules to enhance complementarity while preserving semantic commonality. To further reduce semantic noise and amplify the uniqueness of heterogeneous semantics, we design a heterogeneous transfer (HT) module. Finally, we propose heterogeneous CLIP (HC) textual information to enhance the generalization capability of multimodal models. In the weakly-supervised few-shot semantic segmentation (WFSS) task, with only 1/24 of the parameters of existing state-of-the-art models, TLG achieves a 13.2\\% improvement on Pascal-5\\textsuperscript{i} and a 9.7\\% improvement on COCO-20\\textsuperscript{i}. To the best of our knowledge, TLG is also the first weakly supervised (image-level) model that outperforms fully supervised (pixel-level) models under the same backbone architectures. The code is available at https://github.com/jarch-ma/TLG.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16159v1",
    "published_date": "2025-08-22 07:29:30 UTC",
    "updated_date": "2025-08-22 07:29:30 UTC"
  },
  {
    "arxiv_id": "2508.16157v1",
    "title": "Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment for Anomaly Detection",
    "authors": [
      "Pi-Wei Chen",
      "Jerry Chun-Wei Lin",
      "Wei-Han Chen",
      "Jia Ji",
      "Zih-Ching Chen",
      "Feng-Hao Yeh",
      "Chao-Chun Chen"
    ],
    "abstract": "Pre-trained Vision-Language Models (VLMs) have recently shown promise in detecting anomalies. However, previous approaches are fundamentally limited by their reliance on human-designed prompts and the lack of accessible anomaly samples, leading to significant gaps in context-specific anomaly understanding. In this paper, we propose \\textbf{A}daptive \\textbf{P}rompt \\textbf{T}uning with semantic alignment for anomaly detection (APT), a groundbreaking prior knowledge-free, few-shot framework and overcomes the limitations of traditional prompt-based approaches. APT uses self-generated anomaly samples with noise perturbations to train learnable prompts that capture context-dependent anomalies in different scenarios. To prevent overfitting to synthetic noise, we propose a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) that iteratively aligns the prompts with general anomaly semantics while incorporating diverse synthetic anomaly. Our system not only advances pixel-wise anomaly detection, but also achieves state-of-the-art performance on multiple benchmark datasets without requiring prior knowledge for prompt crafting, establishing a robust and versatile solution for real-world anomaly detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16157v1",
    "published_date": "2025-08-22 07:26:56 UTC",
    "updated_date": "2025-08-22 07:26:56 UTC"
  },
  {
    "arxiv_id": "2508.16154v1",
    "title": "On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models",
    "authors": [
      "Yi Zhang",
      "Zhenyu Liao",
      "Jingfeng Wu",
      "Difan Zou"
    ],
    "abstract": "Despite the widespread adoption of deterministic samplers in diffusion models (DMs), their potential limitations remain largely unexplored. In this paper, we identify collapse errors, a previously unrecognized phenomenon in ODE-based diffusion sampling, where the sampled data is overly concentrated in local data space. To quantify this effect, we introduce a novel metric and demonstrate that collapse errors occur across a variety of settings. When investigating its underlying causes, we observe a see-saw effect, where score learning in low noise regimes adversely impacts the one in high noise regimes. This misfitting in high noise regimes, coupled with the dynamics of deterministic samplers, ultimately causes collapse errors. Guided by these insights, we apply existing techniques from sampling, training, and architecture to empirically support our explanation of collapse errors. This work provides intensive empirical evidence of collapse errors in ODE-based diffusion sampling, emphasizing the need for further research into the interplay between score learning and deterministic sampling, an overlooked yet fundamental aspect of diffusion models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16154v1",
    "published_date": "2025-08-22 07:26:24 UTC",
    "updated_date": "2025-08-22 07:26:24 UTC"
  },
  {
    "arxiv_id": "2508.16143v1",
    "title": "Take That for Me: Multimodal Exophora Resolution with Interactive Questioning for Ambiguous Out-of-View Instructions",
    "authors": [
      "Akira Oyama",
      "Shoichi Hasegawa",
      "Akira Taniguchi",
      "Yoshinobu Hagiwara",
      "Tadahiro Taniguchi"
    ],
    "abstract": "Daily life support robots must interpret ambiguous verbal instructions involving demonstratives such as ``Bring me that cup,'' even when objects or users are out of the robot's view. Existing approaches to exophora resolution primarily rely on visual data and thus fail in real-world scenarios where the object or user is not visible. We propose Multimodal Interactive Exophora resolution with user Localization (MIEL), which is a multimodal exophora resolution framework leveraging sound source localization (SSL), semantic mapping, visual-language models (VLMs), and interactive questioning with GPT-4o. Our approach first constructs a semantic map of the environment and estimates candidate objects from a linguistic query with the user's skeletal data. SSL is utilized to orient the robot toward users who are initially outside its visual field, enabling accurate identification of user gestures and pointing directions. When ambiguities remain, the robot proactively interacts with the user, employing GPT-4o to formulate clarifying questions. Experiments in a real-world environment showed results that were approximately 1.3 times better when the user was visible to the robot and 2.0 times better when the user was not visible to the robot, compared to the methods without SSL and interactive questioning. The project website is https://emergentsystemlabstudent.github.io/MIEL/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "See website at https://emergentsystemlabstudent.github.io/MIEL/. Accepted at IEEE RO-MAN 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.16143v1",
    "published_date": "2025-08-22 07:09:06 UTC",
    "updated_date": "2025-08-22 07:09:06 UTC"
  },
  {
    "arxiv_id": "2508.16135v1",
    "title": "Machine Learning in Micromobility: A Systematic Review of Datasets, Techniques, and Applications",
    "authors": [
      "Sen Yan",
      "Chinmaya Kaundanya",
      "Noel E. O'Connor",
      "Suzanne Little",
      "Mingming Liu"
    ],
    "abstract": "Micromobility systems, which include lightweight and low-speed vehicles such as bicycles, e-bikes, and e-scooters, have become an important part of urban transportation and are used to solve problems such as traffic congestion, air pollution, and high transportation costs. Successful utilisation of micromobilities requires optimisation of complex systems for efficiency, environmental impact mitigation, and overcoming technical challenges for user safety. Machine Learning (ML) methods have been crucial to support these advancements and to address their unique challenges. However, there is insufficient literature addressing the specific issues of ML applications in micromobilities. This survey paper addresses this gap by providing a comprehensive review of datasets, ML techniques, and their specific applications in micromobilities. Specifically, we collect and analyse various micromobility-related datasets and discuss them in terms of spatial, temporal, and feature-based characteristics. In addition, we provide a detailed overview of ML models applied in micromobilities, introducing their advantages, challenges, and specific use cases. Furthermore, we explore multiple ML applications, such as demand prediction, energy management, and safety, focusing on improving efficiency, accuracy, and user experience. Finally, we propose future research directions to address these issues, aiming to help future researchers better understand this field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 3 tables, and 4 figures, submitted to IEEE Transactions on Intelligent Vehicles",
    "pdf_url": "https://arxiv.org/pdf/2508.16135v1",
    "published_date": "2025-08-22 06:55:58 UTC",
    "updated_date": "2025-08-22 06:55:58 UTC"
  },
  {
    "arxiv_id": "2508.16134v1",
    "title": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing",
    "authors": [
      "Yixuan Wang",
      "Haoyu Qiao",
      "Lujun Li",
      "Qingfu Zhu",
      "Wanxiang Che"
    ],
    "abstract": "Large Language Models (LLMs) confront significant memory challenges due to the escalating KV cache with increasing sequence length. As a crucial technique, existing cross-layer KV cache sharing methods either necessitate modified model architectures with subsequent pre-training or incur significant performance degradation at high compression rates. To mitigate these challenges, we propose CommonKV, a training-free method for cross-layer KV cache compression through adjacent parameters sharing. Inspired by the high similarity observed in cross-layer hidden states, we utilize Singular Value Decomposition (SVD) to achieve weight sharing across adjacent parameters, resulting in a more easily mergeable latent KV cache. Furthermore, we also introduce an adaptive budget allocation strategy. It dynamically assigns compression budgets based on cosine similarity, ensuring that dissimilar caches are not over-compressed. Experiments across multiple backbone models and benchmarks including LongBench and Ruler demonstrate that the proposed method consistently outperforms existing low-rank and cross-layer approaches at various compression ratios. Moreover, we find that the benefits of CommonKV are orthogonal to other quantization and eviction methods. By integrating these approaches, we can ultimately achieve a 98\\% compression ratio without significant performance loss.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2508.16134v1",
    "published_date": "2025-08-22 06:55:45 UTC",
    "updated_date": "2025-08-22 06:55:45 UTC"
  },
  {
    "arxiv_id": "2508.16131v1",
    "title": "The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion",
    "authors": [
      "Zoe Kotti",
      "Konstantina Dritsa",
      "Diomidis Spinellis",
      "Panos Louridas"
    ],
    "abstract": "Code completion entails the task of providing missing tokens given a surrounding context. It can boost developer productivity while providing a powerful code discovery tool. Following the Large Language Model (LLM) wave, code completion has been approached with diverse LLMs fine-tuned on code (code LLMs). The performance of code LLMs can be assessed with downstream and intrinsic metrics. Downstream metrics are usually employed to evaluate the practical utility of a model, but can be unreliable and require complex calculations and domain-specific knowledge. In contrast, intrinsic metrics such as perplexity, entropy, and mutual information, which measure model confidence or uncertainty, are simple, versatile, and universal across LLMs and tasks, and can serve as proxies for functional correctness and hallucination risk in LLM-generated code. Motivated by this, we evaluate the confidence of LLMs when generating code by measuring code perplexity across programming languages, models, and datasets using various LLMs, and a sample of 1008 files from 657 GitHub projects. We find that strongly-typed languages exhibit lower perplexity than dynamically typed languages. Scripting languages also demonstrate higher perplexity. Perl appears universally high in perplexity, whereas Java appears low. Code perplexity depends on the employed LLM, but not on the code dataset. Although code comments often increase perplexity, the language ranking based on perplexity is barely affected by their presence. LLM researchers, developers, and users can employ our findings to assess the benefits and suitability of LLM-based code completion in specific software projects based on how language, model choice, and code characteristics impact model confidence.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "30 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2508.16131v1",
    "published_date": "2025-08-22 06:51:13 UTC",
    "updated_date": "2025-08-22 06:51:13 UTC"
  },
  {
    "arxiv_id": "2508.16129v2",
    "title": "Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning",
    "authors": [
      "Ruiqi Wu",
      "Yuang Yao",
      "Tengfei Ma",
      "Chenran Zhang",
      "Na Su",
      "Tao Zhou",
      "Geng Chen",
      "Wen Fan",
      "Yi Zhou"
    ],
    "abstract": "Multimodal large language models (MLLMs) have recently demonstrated remarkable reasoning abilities with reinforcement learning paradigm. Although several multimodal reasoning models have been explored in the medical domain, most of them focus exclusively on basic reasoning, which refers to shallow inference based on visual feature matching. However, real-world clinical diagnosis extends beyond basic reasoning, demanding reasoning processes that integrate heterogeneous clinical information (such as chief complaints and medical history) with multimodal medical imaging data. To bridge this gap, we introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the full spectrum of perception and reasoning. It encompasses both basic reasoning tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental reasoning capabilities and emulate realistic clinical thinking patterns. Building upon MM-Retinal-Reason, we propose OphthaReason, the first ophthalmology-specific multimodal reasoning model with step-by-step reasoning traces. To enable flexible adaptation to both basic and complex reasoning tasks, we specifically design a novel method called Uncertainty-Aware Dynamic Thinking (UADT), which estimates sample-level uncertainty via entropy and dynamically modulates the model's exploration depth using a shaped advantage mechanism. Comprehensive experiments demonstrate that our model achieves state-of-the-art performance on both basic and complex reasoning tasks, outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and ophthalmic MLLMs by at least 24.92\\%, 15.00\\%, 21.20\\%, and 17.66\\%. Project Page: \\href{https://github.com/lxirich/OphthaReason}{link}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16129v2",
    "published_date": "2025-08-22 06:47:30 UTC",
    "updated_date": "2025-09-10 08:30:10 UTC"
  },
  {
    "arxiv_id": "2508.16126v1",
    "title": "Spacetime-GR: A Spacetime-Aware Generative Model for Large Scale Online POI Recommendation",
    "authors": [
      "Haitao Lin",
      "Zhen Yang",
      "Jiawei Xue",
      "Ziji Zhang",
      "Luzhu Wang",
      "Yikun Gu",
      "Yao Xu",
      "Xin Li"
    ],
    "abstract": "Building upon the strong sequence modeling capability, Generative Recommendation (GR) has gradually assumed a dominant position in the application of recommendation tasks (e.g., video and product recommendation). However, the application of Generative Recommendation in Point-of-Interest (POI) recommendation, where user preferences are significantly affected by spatiotemporal variations, remains a challenging open problem. In this paper, we propose Spacetime-GR, the first spacetime-aware generative model for large-scale online POI recommendation. It extends the strong sequence modeling ability of generative models by incorporating flexible spatiotemporal information encoding. Specifically, we first introduce a geographic-aware hierarchical POI indexing strategy to address the challenge of large vocabulary modeling. Subsequently, a novel spatiotemporal encoding module is introduced to seamlessly incorporate spatiotemporal context into user action sequences, thereby enhancing the model's sensitivity to spatiotemporal variations. Furthermore, we incorporate multimodal POI embeddings to enrich the semantic understanding of each POI. Finally, to facilitate practical deployment, we develop a set of post-training adaptation strategies after sufficient pre-training on action sequences. These strategies enable Spacetime-GR to generate outputs in multiple formats (i.e., embeddings, ranking scores and POI candidates) and support a wide range of downstream application scenarios (i.e., ranking and end-to-end recommendation). We evaluate the proposed model on both public benchmark datasets and large-scale industrial datasets, demonstrating its superior performance over existing methods in terms of POI recommendation accuracy and ranking quality. Furthermore, the model is the first generative model deployed in online POI recommendation services that scale to hundreds of millions of POIs and users.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16126v1",
    "published_date": "2025-08-22 06:37:57 UTC",
    "updated_date": "2025-08-22 06:37:57 UTC"
  },
  {
    "arxiv_id": "2508.19268v2",
    "title": "MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts",
    "authors": [
      "Qing Wang",
      "Xue Han",
      "Jiahui Wang",
      "Lehao Xing",
      "Qian Hu",
      "Lianlian Zhang",
      "Chao Deng",
      "Junlan Feng"
    ],
    "abstract": "Despite LLMs' excellent code creation capabilities, multilingual code generation remains extremely challenging. To address this, we intent to improve the multi-programming-lingual (MultiPL) performance of the base LLMs while retaining the most popular ones using restricted computational resources. We consider MultiPL to be a special case of multiple natural languages and propose a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize expert selection at both the token and segment levels. The token-level MoE is a standard upcycling MoE structure with a shared expert and a novel gate weight normalization approach that aids in the final fusion with the segment-level MoE. The segment-level MoE incorporates two innovative designs to better capture the syntactic structure and contextual patterns of programming languages: First, using a sliding window to partition the input token sequence into multiple segments; Then, adopting an expert-choice routing strategy that allows experts to select the top-k segments. The results of the experiment proved the effectiveness of MultiPL-MoE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.19268v2",
    "published_date": "2025-08-22 06:24:52 UTC",
    "updated_date": "2025-09-08 08:30:07 UTC"
  },
  {
    "arxiv_id": "2508.16119v1",
    "title": "ANSC: Probabilistic Capacity Health Scoring for Datacenter-Scale Reliability",
    "authors": [
      "Madhava Gaikwad",
      "Abhishek Gandhi"
    ],
    "abstract": "We present ANSC, a probabilistic capacity health scoring framework for hyperscale datacenter fabrics. While existing alerting systems detect individual device or link failures, they do not capture the aggregate risk of cascading capacity shortfalls. ANSC provides a color-coded scoring system that indicates the urgency of issues \\emph{not solely by current impact, but by the probability of imminent capacity violations}. Our system accounts for both current residual capacity and the probability of additional failures, normalized at datacenter and regional level. We demonstrate that ANSC enables operators to prioritize remediation across more than 400 datacenters and 60 regions, reducing noise and aligning SRE focus on the most critical risks.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "3 pages",
    "pdf_url": "https://arxiv.org/pdf/2508.16119v1",
    "published_date": "2025-08-22 06:19:28 UTC",
    "updated_date": "2025-08-22 06:19:28 UTC"
  },
  {
    "arxiv_id": "2508.19267v1",
    "title": "The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents",
    "authors": [
      "Sai Teja Reddy Adapala",
      "Yashwanth Reddy Alugubelly"
    ],
    "abstract": "The proliferation of autonomous AI agents marks a paradigm shift toward complex, emergent multi-agent systems. This transition introduces systemic security risks, including control-flow hijacking and cascading failures, that traditional cybersecurity paradigms are ill-equipped to address. This paper introduces the Aegis Protocol, a layered security framework designed to provide strong security guarantees for open agentic ecosystems. The protocol integrates three technological pillars: (1) non-spoofable agent identity via W3C Decentralized Identifiers (DIDs); (2) communication integrity via NIST-standardized post-quantum cryptography (PQC); and (3) verifiable, privacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP) system. We formalize an adversary model extending Dolev-Yao for agentic threats and validate the protocol against the STRIDE framework. Our quantitative evaluation used a discrete-event simulation, calibrated against cryptographic benchmarks, to model 1,000 agents. The simulation showed a 0 percent success rate across 20,000 attack trials. For policy verification, analysis of the simulation logs reported a median proof-generation latency of 2.79 seconds, establishing a performance baseline for this class of security. While the evaluation is simulation-based and early-stage, it offers a reproducible baseline for future empirical studies and positions Aegis as a foundation for safe, scalable autonomous AI.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages, 3 figures, 3 tables. Source compiled with pdfLaTeX; bibliography included via prebuilt main.bbl. Code repository: available in paper",
    "pdf_url": "https://arxiv.org/pdf/2508.19267v1",
    "published_date": "2025-08-22 06:18:57 UTC",
    "updated_date": "2025-08-22 06:18:57 UTC"
  },
  {
    "arxiv_id": "2508.16117v2",
    "title": "Extending FKG.in: Towards a Food Claim Traceability Network",
    "authors": [
      "Saransh Kumar Gupta",
      "Rizwan Gulzar Mir",
      "Lipika Dey",
      "Partha Pratim Das",
      "Anirban Sen",
      "Ramesh Jain"
    ],
    "abstract": "The global food landscape is rife with scientific, cultural, and commercial claims about what foods are, what they do, what they should not do, or should not do. These range from rigorously studied health benefits (probiotics improve gut health) and misrepresentations (soaked almonds make one smarter) to vague promises (superfoods boost immunity) and culturally rooted beliefs (cold foods cause coughs). Despite their widespread influence, the infrastructure for tracing, verifying, and contextualizing these claims remains fragmented and underdeveloped. In this paper, we propose a Food Claim-Traceability Network (FCN) as an extension of FKG[.]in, a knowledge graph of Indian food that we have been incrementally building. We also present the ontology design and the semi-automated knowledge curation workflow that we used to develop a proof of concept of FKG[.]in-FCN using Reddit data and Large Language Models. FCN integrates curated data inputs, structured schemas, and provenance-aware pipelines for food-related claim extraction and validation. While directly linked to the Indian food knowledge graph as an application, our methodology remains application-agnostic and adaptable to other geographic, culinary, or regulatory settings. By modeling food claims and their traceability in a structured, verifiable, and explainable way, we aim to contribute to more transparent and accountable food knowledge ecosystems, supporting researchers, policymakers, and most importantly, everyday consumers in navigating a world saturated with dietary assertions.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 3 figures, 1 table, 45 references, ACM International Conference on Multimedia 2025 - Multi-modal Food Computing Workshop",
    "pdf_url": "https://arxiv.org/pdf/2508.16117v2",
    "published_date": "2025-08-22 06:18:51 UTC",
    "updated_date": "2025-09-04 11:54:35 UTC"
  },
  {
    "arxiv_id": "2508.16112v1",
    "title": "IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra",
    "authors": [
      "Heewoong Noh",
      "Namkyeong Lee",
      "Gyoung S. Na",
      "Kibum Kim",
      "Chanyoung Park"
    ],
    "abstract": "Spectral analysis provides crucial clues for the elucidation of unknown materials. Among various techniques, infrared spectroscopy (IR) plays an important role in laboratory settings due to its high accessibility and low cost. However, existing approaches often fail to reflect expert analytical processes and lack flexibility in incorporating diverse types of chemical knowledge, which is essential in real-world analytical scenarios. In this paper, we propose IR-Agent, a novel multi-agent framework for molecular structure elucidation from IR spectra. The framework is designed to emulate expert-driven IR analysis procedures and is inherently extensible. Each agent specializes in a specific aspect of IR interpretation, and their complementary roles enable integrated reasoning, thereby improving the overall accuracy of structure elucidation. Through extensive experiments, we demonstrate that IR-Agent not only improves baseline performance on experimental IR spectra but also shows strong adaptability to various forms of chemical information.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16112v1",
    "published_date": "2025-08-22 06:07:28 UTC",
    "updated_date": "2025-08-22 06:07:28 UTC"
  },
  {
    "arxiv_id": "2508.16701v2",
    "title": "Generative Artificial Intelligence and Agents in Research and Teaching",
    "authors": [
      "Jussi S. Jauhiainen",
      "Aurora Toppari"
    ],
    "abstract": "This study provides a comprehensive analysis of the development, functioning, and application of generative artificial intelligence (GenAI) and large language models (LLMs), with an emphasis on their implications for research and education. It traces the conceptual evolution from artificial intelligence (AI) through machine learning (ML) and deep learning (DL) to transformer architectures, which constitute the foundation of contemporary generative systems. Technical aspects, including prompting strategies, word embeddings, and probabilistic sampling methods (temperature, top-k, and top-p), are examined alongside the emergence of autonomous agents. These elements are considered in relation to both the opportunities they create and the limitations and risks they entail.\n  The work critically evaluates the integration of GenAI across the research process, from ideation and literature review to research design, data collection, analysis, interpretation, and dissemination. While particular attention is given to geographical research, the discussion extends to wider academic contexts. A parallel strand addresses the pedagogical applications of GenAI, encompassing course and lesson design, teaching delivery, assessment, and feedback, with geography education serving as a case example.\n  Central to the analysis are the ethical, social, and environmental challenges posed by GenAI. Issues of bias, intellectual property, governance, and accountability are assessed, alongside the ecological footprint of LLMs and emerging technological strategies for mitigation. The concluding section considers near- and long-term futures of GenAI, including scenarios of sustained adoption, regulation, and potential decline. By situating GenAI within both scholarly practice and educational contexts, the study contributes to critical debates on its transformative potential and societal responsibilities.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "113 pages, 6 figures, 13 tables, 2 appendices",
    "pdf_url": "https://arxiv.org/pdf/2508.16701v2",
    "published_date": "2025-08-22 06:00:45 UTC",
    "updated_date": "2025-08-26 10:23:02 UTC"
  },
  {
    "arxiv_id": "2508.18292v1",
    "title": "Consensus Is All You Need: Gossip-Based Reasoning Among Large Language Models",
    "authors": [
      "Saksham Arora"
    ],
    "abstract": "Large language models have advanced rapidly, but no single model excels in every area -- each has its strengths and weaknesses. Instead of relying on one model alone, we take inspiration from gossip protocols in distributed systems, where information is exchanged with peers until they all come to an agreement. In this setup, models exchange answers and gradually work toward a shared solution. Each LLM acts as a node in a peer-to-peer network, sharing responses and thought processes to reach a collective decision. Our results show that this \"gossip-based consensus\" leads to robust, resilient, and accurate multi-agent AI reasoning. It helps overcome the weaknesses of individual models and brings out their collective strengths. This approach is similar to how humans build consensus, making AI seem more collaborative and trustworthy instead of just a black-box program.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "4 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2508.18292v1",
    "published_date": "2025-08-22 05:49:27 UTC",
    "updated_date": "2025-08-22 05:49:27 UTC"
  },
  {
    "arxiv_id": "2508.16100v1",
    "title": "CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency",
    "authors": [
      "Zhanming Shen",
      "Hao Chen",
      "Yulei Tang",
      "Shaolin Zhu",
      "Wentao Ye",
      "Xiaomeng Hu",
      "Haobo Wang",
      "Gang Chen",
      "Junbo Zhao"
    ],
    "abstract": "Instruction tuning is vital for aligning large language models (LLMs) with human intent, but current methods typically rely on costly human-annotated seed data or powerful external teacher models. While instruction back-translation techniques reduce this dependency, they remain fundamentally tethered to an initial seed set, which limits full automation, introduces biases, and can lead to inefficient use of unlabeled corpora. In this paper, we propose Cycle-Instruct, a novel framework that achieves fully seed-free instruction tuning. Inspired by cycle consistency, Cycle-Instruct employs a dual self-training loop where two models-an answer generator and a question generator-are bootstrapped solely from raw, unlabeled text. These models mutually supervise each other by reconstructing original text segments from their counterpart's generated pseudo-labels, effectively learning from the intrinsic structure of the data without any human-provided seeds. We demonstrate Cycle-Instruct's efficacy across four diverse data tracks, including general instruction-following, domain-specific tasks, dialogue logs, and plain text. Our extensive experiments show that Cycle-Instruct not only outperforms seed-driven back-translation baselines but also achieves performance comparable to strongly supervised methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2508.16100v1",
    "published_date": "2025-08-22 05:30:59 UTC",
    "updated_date": "2025-08-22 05:30:59 UTC"
  },
  {
    "arxiv_id": "2508.16090v1",
    "title": "GPLight+: A Genetic Programming Method for Learning Symmetric Traffic Signal Control Policy",
    "authors": [
      "Xiao-Cheng Liao",
      "Yi Mei",
      "Mengjie Zhang"
    ],
    "abstract": "Recently, learning-based approaches, have achieved significant success in automatically devising effective traffic signal control strategies. In particular, as a powerful evolutionary machine learning approach, Genetic Programming (GP) is utilized to evolve human-understandable phase urgency functions to measure the urgency of activating a green light for a specific phase. However, current GP-based methods are unable to treat the common traffic features of different traffic signal phases consistently. To address this issue, we propose to use a symmetric phase urgency function to calculate the phase urgency for a specific phase based on the current road conditions. This is represented as an aggregation of two shared subtrees, each representing the urgency of a turn movement in the phase. We then propose a GP method to evolve the symmetric phase urgency function. We evaluate our proposed method on the well-known cityflow traffic simulator, based on multiple public real-world datasets. The experimental results show that the proposed symmetric urgency function representation can significantly improve the performance of the learned traffic signal control policies over the traditional GP representation on a wide range of scenarios. Further analysis shows that the proposed method can evolve effective, human-understandable and easily deployable traffic signal control policies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16090v1",
    "published_date": "2025-08-22 05:02:50 UTC",
    "updated_date": "2025-08-22 05:02:50 UTC"
  },
  {
    "arxiv_id": "2508.16089v1",
    "title": "Two-flow Feedback Multi-scale Progressive Generative Adversarial Network",
    "authors": [
      "Sun Weikai",
      "Song Shijie",
      "Chi Wenjie"
    ],
    "abstract": "Although diffusion model has made good progress in the field of image generation, GAN\\cite{huang2023adaptive} still has a large development space due to its unique advantages, such as WGAN\\cite{liu2021comparing}, SSGAN\\cite{guibas2021adaptive} \\cite{zhang2022vsa} \\cite{zhou2024adapt} and so on. In this paper, we propose a novel two-flow feedback multi-scale progressive generative adversarial network (MSPG-SEN) for GAN models. This paper has four contributions: 1) : We propose a two-flow feedback multi-scale progressive Generative Adversarial network (MSPG-SEN), which not only improves image quality and human visual perception on the basis of retaining the advantages of the existing GAN model, but also simplifies the training process and reduces the training cost of GAN networks. Our experimental results show that, MSPG-SEN has achieved state-of-the-art generation results on the following five datasets,INKK The dataset is 89.7\\%,AWUN The dataset is 78.3\\%,IONJ The dataset is 85.5\\%,POKL The dataset is 88.7\\%,OPIN The dataset is 96.4\\%. 2) : We propose an adaptive perception-behavioral feedback loop (APFL), which effectively improves the robustness and training stability of the model and reduces the training cost. 3) : We propose a globally connected two-flow dynamic residual network(). After ablation experiments, it can effectively improve the training efficiency and greatly improve the generalization ability, with stronger flexibility. 4) : We propose a new dynamic embedded attention mechanism (DEMA). After experiments, the attention can be extended to a variety of image processing tasks, which can effectively capture global-local information, improve feature separation capability and feature expression capabilities, and requires minimal computing resources only 88.7\\% with INJK With strong cross-task capability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16089v1",
    "published_date": "2025-08-22 04:59:08 UTC",
    "updated_date": "2025-08-22 04:59:08 UTC"
  },
  {
    "arxiv_id": "2508.16082v6",
    "title": "On Task Vectors and Gradients",
    "authors": [
      "Luca Zhou",
      "Daniele Solombrino",
      "Donato Crisostomi",
      "Maria Sofia Bucarelli",
      "Giuseppe Alessio D'Inverno",
      "Fabrizio Silvestri",
      "Emanuele Rodolà"
    ],
    "abstract": "Task arithmetic has emerged as a simple yet powerful technique for model merging, enabling the combination of multiple finetuned models into one. Despite its empirical success, a clear theoretical explanation of why and when it works is lacking. This paper provides a rigorous theoretical foundation for task arithmetic by establishing a connection between task vectors and gradients of the task losses. We show that under standard gradient descent, a task vector generated from one epoch of finetuning is exactly equivalent to the negative gradient of the loss, scaled by the learning rate. For the practical multi-epoch setting, we prove that this equivalence holds approximately, with a second-order error term that we explicitly bound for feed-forward networks. Our empirical analysis across seven vision benchmarks corroborates our theory, demonstrating that the first-epoch gradient dominates the finetuning trajectory in both norm and direction. A key implication is that merging models finetuned for only a single epoch often yields performance comparable to merging fully converged models. These findings reframe task arithmetic as a form of approximate multitask learning, providing a clear rationale for its effectiveness and highlighting the critical role of early training dynamics in model merging.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages of main paper, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2508.16082v6",
    "published_date": "2025-08-22 04:16:42 UTC",
    "updated_date": "2025-10-20 04:13:40 UTC"
  },
  {
    "arxiv_id": "2508.16077v1",
    "title": "Cooperative Design Optimization through Natural Language Interaction",
    "authors": [
      "Ryogo Niwa",
      "Shigeo Yoshida",
      "Yuki Koyama",
      "Yoshitaka Ushiku"
    ],
    "abstract": "Designing successful interactions requires identifying optimal design parameters. To do so, designers often conduct iterative user testing and exploratory trial-and-error. This involves balancing multiple objectives in a high-dimensional space, making the process time-consuming and cognitively demanding. System-led optimization methods, such as those based on Bayesian optimization, can determine for designers which parameters to test next. However, they offer limited opportunities for designers to intervene in the optimization process, negatively impacting the designer's experience. We propose a design optimization framework that enables natural language interactions between designers and the optimization system, facilitating cooperative design optimization. This is achieved by integrating system-led optimization methods with Large Language Models (LLMs), allowing designers to intervene in the optimization process and better understand the system's reasoning. Experimental results show that our method provides higher user agency than a system-led method and shows promising optimization performance compared to manual design. It also matches the performance of an existing cooperative method with lower cognitive load.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "25 pages, 20 figures, to appear in Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology (UIST '25), September 28-October 1, 2025, Busan, Republic of Korea",
    "pdf_url": "https://arxiv.org/pdf/2508.16077v1",
    "published_date": "2025-08-22 04:12:03 UTC",
    "updated_date": "2025-08-22 04:12:03 UTC"
  },
  {
    "arxiv_id": "2508.16072v3",
    "title": "InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles",
    "authors": [
      "Zizhen Li",
      "Chuanhao Li",
      "Yibin Wang",
      "Qi Chen",
      "Diping Song",
      "Yukang Feng",
      "Jianwen Sun",
      "Jiaxin Ai",
      "Fanrui Zhang",
      "Mingzhu Sun",
      "Kaipeng Zhang"
    ],
    "abstract": "LLMs have shown strong performance on human-centric reasoning tasks. While previous evaluations have explored whether LLMs can infer intentions or detect deception, they often overlook the individualized reasoning styles that influence how people interpret and act in social contexts. Social deduction games (SDGs) provide a natural testbed for evaluating individualized reasoning styles, where different players may adopt diverse but contextually valid reasoning strategies under identical conditions. To address this, we introduce InMind, a cognitively grounded evaluation framework designed to assess whether LLMs can capture and apply personalized reasoning styles in SDGs. InMind enhances structured gameplay data with round-level strategy traces and post-game reflections, collected under both Observer and Participant modes. It supports four cognitively motivated tasks that jointly evaluate both static alignment and dynamic adaptation. As a case study, we apply InMind to the game Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o frequently rely on lexical cues, struggling to anchor reflections in temporal gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These findings reveal key limitations in current LLMs' capacity for individualized, adaptive reasoning, and position InMind as a step toward cognitively aligned human-AI interaction.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP 2025 MainConference",
    "pdf_url": "https://arxiv.org/pdf/2508.16072v3",
    "published_date": "2025-08-22 04:04:00 UTC",
    "updated_date": "2025-09-21 03:54:25 UTC"
  },
  {
    "arxiv_id": "2508.16071v1",
    "title": "From Benchmark Data To Applicable Program Repair: An Experience Report",
    "authors": [
      "Mahinthan Chandramohan",
      "Jovan Jancic",
      "Yuntong Zhang",
      "Padmanabhan Krishnan"
    ],
    "abstract": "This paper describes our approach to automated program repair. We combine various techniques from the literature to achieve this. Our experiments show that our approach performs better than other techniques on standard benchmarks. However, on closer inspection, none of these techniques work on realistic defects that we see in industry.\n  We find that augmenting code with formal specifications enables LLMs to generate higher-quality unit tests, especially for complex production code with improved coverage of edge cases and exception handling. However, specifications add little value for well-understood errors (e.g., null pointer, index out of bounds), but are beneficial for logic and string manipulation errors. Despite encouraging benchmark results, real-world adoption is limited since passing tests do not guarantee correct patches. Current challenges include insufficient expressiveness of the JML specification language, necessitating advanced verification tools and richer predicates. Our ongoing work is exploring contract automata, programming by example, and testcase repair, with a focus on integrating human feedback and measuring productivity gains - highlighting the gap between academic benchmarks and practical industry needs",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16071v1",
    "published_date": "2025-08-22 03:59:27 UTC",
    "updated_date": "2025-08-22 03:59:27 UTC"
  },
  {
    "arxiv_id": "2508.16700v2",
    "title": "GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model",
    "authors": [
      "Deepak Kumar",
      "Divakar Yadav",
      "Yash Patel"
    ],
    "abstract": "We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B (Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines Qwen3-32B and Yi-34B across multiple dimensions. We measure true time-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency percentiles, peak VRAM with past key values (PKV) held, and energy via a consistent nvidia-smi-based sampler. At a 2048-token context with 64-token decode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than dense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM and energy per 1000 generated tokens; its TTFT is higher due to MoE routing overhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B provides about 31.8% higher decode throughput and 25.8% lower energy per 1000 generated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM. Normalized by active parameters, GPT-OSS-20B shows markedly stronger per-active-parameter efficiency (APE), underscoring MoE's deployment advantages. We do not evaluate accuracy; this is a deployment-focused study. We release code and consolidated results to enable replication and extension.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16700v2",
    "published_date": "2025-08-22 03:37:27 UTC",
    "updated_date": "2025-08-31 03:40:19 UTC"
  },
  {
    "arxiv_id": "2508.16059v1",
    "title": "Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting",
    "authors": [
      "Zhuomin Chen",
      "Dan Li",
      "Jiahui Zhou",
      "Shunyu Wu",
      "Haozheng Ye",
      "Jian Lou",
      "See-Kiong Ng"
    ],
    "abstract": "Time series (TS) data are ubiquitous across various application areas, rendering time series forecasting (TSF) a fundamental task. With the astounding advances in large language models (LLMs), a variety of methods have been developed to adapt LLMs for time series forecasting. Despite unlocking the potential of LLMs in comprehending TS data, existing methods are inherently constrained by their shallow integration of TS information, wherein LLMs typically access TS representations at shallow layers, primarily at the input layer. This causes the influence of TS representations to progressively fade in deeper layers and eventually leads to ineffective adaptation between textual embeddings and TS representations. In this paper, we propose the Multi-layer Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to directly access time series patterns at all depths, thereby mitigating the progressive loss of TS information in deeper layers. Specifically, MSEF leverages off-the-shelf time series foundation models to extract semantically rich embeddings, which are fused with intermediate text representations across LLM layers via layer-specific steering vectors. These steering vectors are designed to continuously optimize the alignment between time series and textual modalities and facilitate a layer-specific adaptation mechanism that ensures efficient few-shot learning capabilities. Experimental results on seven benchmarks demonstrate significant performance improvements by MSEF compared with baselines, with an average reduction of 31.8% in terms of MSE. The code is available at https://github.com/One1sAll/MSEF.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "To be published in CIKM 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.16059v1",
    "published_date": "2025-08-22 03:22:10 UTC",
    "updated_date": "2025-08-22 03:22:10 UTC"
  },
  {
    "arxiv_id": "2508.16057v1",
    "title": "Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework",
    "authors": [
      "Sijie Yang",
      "Binyu Lei",
      "Filip Biljecki"
    ],
    "abstract": "Ensuring liveability and comfort is one of the fundamental objectives of urban planning. Numerous studies have employed computational methods to assess and quantify factors related to urban comfort such as greenery coverage, thermal comfort, and walkability. However, a clear definition of urban comfort and its comprehensive evaluation framework remain elusive. Our research explores the theoretical interpretations and methodologies for assessing urban comfort within digital planning, emphasising three key dimensions: multidimensional analysis, data support, and AI assistance.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at 19th International Conference on Computational Urban Planning and Urban Management (CUPUM 2025)",
    "pdf_url": "https://arxiv.org/pdf/2508.16057v1",
    "published_date": "2025-08-22 03:10:41 UTC",
    "updated_date": "2025-08-22 03:10:41 UTC"
  },
  {
    "arxiv_id": "2508.16054v1",
    "title": "Generative Foundation Model for Structured and Unstructured Electronic Health Records",
    "authors": [
      "Sonish Sivarajkumar",
      "Hang Zhang",
      "Yuelyu Ji",
      "Maneesh Bilalpur",
      "Xizhi Wu",
      "Chenyu Li",
      "Min Gu Kwak",
      "Shyam Visweswaran",
      "Yanshan Wang"
    ],
    "abstract": "Electronic health records (EHRs) are rich clinical data sources but complex repositories of patient data, spanning structured elements (demographics, vitals, lab results, codes), unstructured clinical notes and other modalities of data. Harnessing this heterogeneity is critical for improving patient outcomes. Recent advances in large language models (LLMs) have enabled foundation models that can learn from multiple data modalities and support clinical tasks. However, most current approaches simply serialize numeric EHR data into text, which risks losing temporal and quantitative detail. We introduce Generative Deep Patient (GDP), a multimodal foundation model that natively encodes structured EHR time-series via a CNN-Transformer encoder and fuses it with unstructured EHRs through cross-modal attention into a LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining, where it learns to produce clinical narratives from raw patient timelines while also performing masked feature prediction (MFP) and next time-step prediction (NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day readmission). In clinical prediction, GDP demonstrated superior performance on MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and 30-day readmission AUROC = 0.627. For narrative generation, GDP achieved ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation, GDP-Instruct scored highest on faithfulness, fluency, and overall clinical utility, suggesting reduced hospital documentation workload without sacrificing accuracy. Our results demonstrate that a single multimodal foundation model can both predict clinically actionable events and generate high-quality clinical narratives. Furthermore, GDP's flexible architecture can be extended to additional modalities.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16054v1",
    "published_date": "2025-08-22 03:05:09 UTC",
    "updated_date": "2025-08-22 03:05:09 UTC"
  },
  {
    "arxiv_id": "2508.16051v2",
    "title": "MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs",
    "authors": [
      "Yiheng Hu",
      "Xiaoyang Wang",
      "Qing Liu",
      "Xiwei Xu",
      "Qian Fu",
      "Wenjie Zhang",
      "Liming Zhu"
    ],
    "abstract": "Multimodal Multi-hop question answering requires integrating information from diverse sources, such as images and texts, to derive answers. Existing methods typically rely on sequential retrieval and reasoning, where each step builds on the previous output. However, this single-path paradigm makes them vulnerable to errors due to misleading intermediate steps. Moreover, developing multimodal models can be computationally expensive, often requiring extensive training. To address these limitations, we propose a training-free framework guided by an Adaptive Planning Graph, which consists of planning, retrieval and reasoning modules. The planning module analyzes the current state of the Adaptive Planning Graph, determines the next action and where to expand the graph, which enables dynamic and flexible exploration of reasoning paths. To handle retrieval of text to unspecified target modalities, we devise modality-specific strategies that dynamically adapt to distinct data types. Our approach preserves the characteristics of multimodal information without costly task-specific training, enabling seamless integration with up-to-date models. Finally, the experiments on MultimodalQA and WebQA show that our approach matches or outperforms existing models that rely on training.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16051v2",
    "published_date": "2025-08-22 02:57:52 UTC",
    "updated_date": "2025-09-19 06:41:50 UTC"
  },
  {
    "arxiv_id": "2508.16048v5",
    "title": "OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages",
    "authors": [
      "Raphaël Merx",
      "Hanna Suominen",
      "Trevor Cohn",
      "Ekaterina Vylomova"
    ],
    "abstract": "In machine translation (MT), health is a high-stakes domain characterised by widespread deployment and domain-specific vocabulary. However, there is a lack of MT evaluation datasets for low-resource languages in this domain. To address this gap, we introduce OpenWHO, a document-level parallel corpus of 2,978 documents and 26,824 sentences from the World Health Organization's e-learning platform. Sourced from expert-authored, professionally translated materials shielded from web-crawling, OpenWHO spans a diverse range of over 20 languages, of which nine are low-resource. Leveraging this new resource, we evaluate modern large language models (LLMs) against traditional MT models. Our findings reveal that LLMs consistently outperform traditional MT models, with Gemini 2.5 Flash achieving a +4.79 ChrF point improvement over NLLB-54B on our low-resource test set. Further, we investigate how LLM context utilisation affects accuracy, finding that the benefits of document-level translation are most pronounced in specialised domains like health. We release the OpenWHO corpus to encourage further research into low-resource MT in the health domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at WMT 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.16048v5",
    "published_date": "2025-08-22 02:53:56 UTC",
    "updated_date": "2025-10-06 02:43:38 UTC"
  },
  {
    "arxiv_id": "2508.16041v1",
    "title": "Enhanced predictions of the Madden-Julian oscillation using the FuXi-S2S machine learning model: Insights into physical mechanisms",
    "authors": [
      "Can Cao",
      "Xiaohui Zhong",
      "Lei Chen",
      "Zhiwei Wua",
      "Hao Li"
    ],
    "abstract": "The Madden-Julian Oscillation (MJO) is the dominant mode of tropical atmospheric variability on intraseasonal timescales, and reliable MJO predictions are essential for protecting lives and mitigating impacts on societal assets. However, numerical models still fall short of achieving the theoretical predictability limit for the MJO due to inherent constraints. In an effort to extend the skillful prediction window for the MJO, machine learning (ML) techniques have gained increasing attention. This study examines the MJO prediction performance of the FuXi subseasonal-to-seasonal (S2S) ML model during boreal winter, comparing it with the European Centre for Medium- Range Weather Forecasts S2S model. Results indicate that for the initial strong MJO phase 3, the FuXi-S2S model demonstrates reduced biases in intraseasonal outgoing longwave radiation anomalies averaged over the tropical western Pacific (WP) region during days 15-20, with the convective center located over this area. Analysis of multiscale interactions related to moisture transport suggests that improvements could be attributed to the FuXi-S2S model's more accurate prediction of the area-averaged meridional gradient of low-frequency background moisture over the tropical WP. These findings not only explain the enhanced predictive capability of the FuXi-S2S model but also highlight the potential of ML approaches in advancing the MJO forecasting.",
    "categories": [
      "physics.ao-ph",
      "cs.AI"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16041v1",
    "published_date": "2025-08-22 02:27:07 UTC",
    "updated_date": "2025-08-22 02:27:07 UTC"
  },
  {
    "arxiv_id": "2508.16037v2",
    "title": "Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services",
    "authors": [
      "Renxuan Tan",
      "Rongpeng Li",
      "Xiaoxue Yu",
      "Xianfu Chen",
      "Xing Xu",
      "Zhifeng Zhao"
    ],
    "abstract": "Federated learning (FL) in multi-service provider (SP) ecosystems is fundamentally hampered by non-cooperative dynamics, where privacy constraints and competing interests preclude the centralized optimization of multi-SP communication and computation resources. In this paper, we introduce PAC-MCoFL, a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs act as agents to jointly optimize client assignment, adaptive quantization, and resource allocation. Within the framework, we integrate Pareto Actor-Critic (PAC) principles with expectile regression, enabling agents to conjecture optimal joint policies to achieve Pareto-optimal equilibria while modeling heterogeneous risk profiles. To manage the high-dimensional action space, we devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant featuring a parameterized conjecture generator that substantially reduces computational complexity with a provably bounded error. Alongside theoretical convergence guarantees, our framework's superiority is validated through extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2% improvements in total reward and hypervolume indicator (HVI), respectively, over the latest MARL solutions. The results also demonstrate that our method can more effectively balance individual SP and system performance in scaled deployments and under diverse data heterogeneity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16037v2",
    "published_date": "2025-08-22 02:09:48 UTC",
    "updated_date": "2025-08-28 05:26:41 UTC"
  },
  {
    "arxiv_id": "2508.16035v1",
    "title": "Time Series Based Network Intrusion Detection using MTF-Aided Transformer",
    "authors": [
      "Poorvi Joshi",
      "Mohan Gurusamy"
    ],
    "abstract": "This paper introduces a novel approach to time series classification using a Markov Transition Field (MTF)-aided Transformer model, specifically designed for Software-Defined Networks (SDNs). The proposed model integrates the temporal dependency modeling strengths of MTFs with the sophisticated pattern recognition capabilities of Transformer architectures. We evaluate the model's performance using the InSDN dataset, demonstrating that our model outperforms baseline classification models, particularly in data-constrained environments commonly encountered in SDN applications. We also highlight the relationship between the MTF and Transformer components, which leads to better performance, even with limited data. Furthermore, our approach achieves competitive training and inference times, making it an efficient solution for real-world SDN applications. These findings establish the potential of MTF-aided Transformers to address the challenges of time series classification in SDNs, offering a promising path for reliable and scalable analysis in scenarios with sparse data.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "7 pages, 3 figures. Accepted and presented at The Fifth Intelligent Cybersecurity Conference (ICSC 2025), nominated for Best Paper Award",
    "pdf_url": "https://arxiv.org/pdf/2508.16035v1",
    "published_date": "2025-08-22 01:54:35 UTC",
    "updated_date": "2025-08-22 01:54:35 UTC"
  },
  {
    "arxiv_id": "2508.16697v1",
    "title": "QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting",
    "authors": [
      "Nicole Cho",
      "William Watson",
      "Alec Koppel",
      "Sumitra Ganesh",
      "Manuela Veloso"
    ],
    "abstract": "Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher hallucination prevalence; yet most mitigation work focuses on after-the-fact filtering rather than shaping the queries that trigger them. We introduce QueryBandits, a bandit framework that designs rewrite strategies to maximize a reward model, that encapsulates hallucination propensity based upon the sensitivities of 17 linguistic features of the input query-and therefore, proactively steer LLMs away from generating hallucinations. Across 13 diverse QA benchmarks and 1,050 lexically perturbed queries per dataset, our top contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a no-rewrite baseline and also outperforms zero-shot static prompting (\"paraphrase\" or \"expand\") by 42.6% and 60.3% respectively. Therefore, we empirically substantiate the effectiveness of QueryBandits in mitigating hallucination via the intervention that takes the form of a query rewrite. Interestingly, certain static prompting strategies, which constitute a considerable number of current query rewriting literature, have a higher cumulative regret than the no-rewrite baseline, signifying that static rewrites can worsen hallucination. Moreover, we discover that the converged per-arm regression feature weight vectors substantiate that there is no single rewrite strategy optimal for all queries. In this context, guided rewriting via exploiting semantic features with QueryBandits can induce significant shifts in output behavior through forward-pass mechanisms, bypassing the need for retraining or gradient-based adaptation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2508.16697v1",
    "published_date": "2025-08-22 01:41:49 UTC",
    "updated_date": "2025-08-22 01:41:49 UTC"
  },
  {
    "arxiv_id": "2508.16033v1",
    "title": "CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics",
    "authors": [
      "Jong-Hwan Jang",
      "Junho Song",
      "Yong-Yeon Jo"
    ],
    "abstract": "Recognizing the need for explainable AI (XAI) approaches to enable the successful integration of AI-based ECG prediction models (AI-ECG) into clinical practice, we introduce a framework generating \\textbf{Co}unter\\textbf{F}actual \\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as amplitudes and intervals, influence the model's predictive decisions. To demonstrate the applicability of the CoFE, we present two case studies: atrial fibrillation classification and potassium level regression models. The CoFE reveals feature changes in ECG signals that align with the established clinical knowledge. By clarifying both \\textbf{where valid features appear} in the ECG and \\textbf{how they influence the model's predictions}, we anticipate that our framework will enhance the interpretability of AI-ECG models and support more effective clinical decision-making. Our demonstration video is available at: https://www.youtube.com/watch?v=YoW0bNBPglQ.",
    "categories": [
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "Demo paper, 5 pages",
    "pdf_url": "https://arxiv.org/pdf/2508.16033v1",
    "published_date": "2025-08-22 01:33:37 UTC",
    "updated_date": "2025-08-22 01:33:37 UTC"
  },
  {
    "arxiv_id": "2508.16030v1",
    "title": "CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars",
    "authors": [
      "Jinyue Song",
      "Hansol Ku",
      "Jayneel Vora",
      "Nelson Lee",
      "Ahmad Kamari",
      "Prasant Mohapatra",
      "Parth Pathak"
    ],
    "abstract": "Automotive FMCW radars remain reliable in rain and glare, yet their sparse, noisy point clouds constrain 3-D object detection. We therefore release CoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, and GPS streams from multiple vehicles across diverse manoeuvres. Built on this data, we propose a unified cooperative-perception framework with middle- and late-fusion options. Its baseline network employs a multi-branch PointNet-style encoder enhanced with self-attention to fuse spatial, Doppler, and intensity cues into a common latent space, which a decoder converts into 3-D bounding boxes and per-point depth confidence. Experiments show that middle fusion with intensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and consistently outperforms single-vehicle baselines. CoVeRaP thus establishes the first reproducible benchmark for multi-vehicle FMCW-radar perception and demonstrates that affordable radar sharing markedly improves detection robustness. Dataset and code are publicly available to encourage further research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICCCN 2025 (IEEE International Conference on Computer Communications and Networks), Tokyo, Japan, August 2025",
    "pdf_url": "https://arxiv.org/pdf/2508.16030v1",
    "published_date": "2025-08-22 01:14:27 UTC",
    "updated_date": "2025-08-22 01:14:27 UTC"
  },
  {
    "arxiv_id": "2508.16025v1",
    "title": "Breaking Barriers in Software Testing: The Power of AI-Driven Automation",
    "authors": [
      "Saba Naqvi",
      "Mohammad Baqar"
    ],
    "abstract": "Software testing remains critical for ensuring reliability, yet traditional approaches are slow, costly, and prone to gaps in coverage. This paper presents an AI-driven framework that automates test case generation and validation using natural language processing (NLP), reinforcement learning (RL), and predictive models, embedded within a policy-driven trust and fairness model. The approach translates natural language requirements into executable tests, continuously optimizes them through learning, and validates outcomes with real-time analysis while mitigating bias. Case studies demonstrate measurable gains in defect detection, reduced testing effort, and faster release cycles, showing that AI-enhanced testing improves both efficiency and reliability. By addressing integration and scalability challenges, the framework illustrates how AI can shift testing from a reactive, manual process to a proactive, adaptive system that strengthens software quality in increasingly complex environments.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "10 Pages",
    "pdf_url": "https://arxiv.org/pdf/2508.16025v1",
    "published_date": "2025-08-22 01:04:50 UTC",
    "updated_date": "2025-08-22 01:04:50 UTC"
  },
  {
    "arxiv_id": "2508.16696v1",
    "title": "DecoMind: A Generative AI System for Personalized Interior Design Layouts",
    "authors": [
      "Reema Alshehri",
      "Rawan Alotaibi",
      "Leen Almasri",
      "Rawan Altaweel"
    ],
    "abstract": "This paper introduces a system for generating interior design layouts based on user inputs, such as room type, style, and furniture preferences. CLIP extracts relevant furniture from a dataset, and a layout that contains furniture and a prompt are fed to Stable Diffusion with ControlNet to generate a design that incorporates the selected furniture. The design is then evaluated by classifiers to ensure alignment with the user's inputs, offering an automated solution for realistic interior design.",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "~7 pages; ~32 figures; compiled with pdfLaTeX. Primary category: cs.CV. (Secondary: cs.AI)",
    "pdf_url": "https://arxiv.org/pdf/2508.16696v1",
    "published_date": "2025-08-22 00:01:48 UTC",
    "updated_date": "2025-08-22 00:01:48 UTC"
  }
]
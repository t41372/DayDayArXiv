[
  {
    "arxiv_id": "2411.19415v2",
    "title": "AMO Sampler: Enhancing Text Rendering with Overshooting",
    "authors": [
      "Xixi Hu",
      "Keyang Xu",
      "Bo Liu",
      "Qiang Liu",
      "Hongliang Fei"
    ],
    "abstract": "Achieving precise alignment between textual instructions and generated images\nin text-to-image generation is a significant challenge, particularly in\nrendering written text within images. Sate-of-the-art models like Stable\nDiffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text\ndepiction, resulting in misspelled or inconsistent text. We introduce a\ntraining-free method with minimal computational overhead that significantly\nenhances text rendering quality. Specifically, we introduce an overshooting\nsampler for pretrained rectified flow (RF) models, by alternating between\nover-simulating the learned ordinary differential equation (ODE) and\nreintroducing noise. Compared to the Euler sampler, the overshooting sampler\neffectively introduces an extra Langevin dynamics term that can help correct\nthe compounding error from successive Euler steps and therefore improve the\ntext rendering. However, when the overshooting strength is high, we observe\nover-smoothing artifacts on the generated images. To address this issue, we\npropose an Attention Modulated Overshooting sampler (AMO), which adaptively\ncontrols the strength of overshooting for each image patch according to their\nattention score with the text content. AMO demonstrates a 32.3% and 35.9%\nimprovement in text rendering accuracy on SD3 and Flux without compromising\noverall image quality or increasing inference cost. Code available at:\nhttps://github.com/hxixixh/amo-release.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.19415v2",
    "published_date": "2024-11-28 23:45:45 UTC",
    "updated_date": "2025-05-03 00:42:46 UTC"
  },
  {
    "arxiv_id": "2412.00146v1",
    "title": "Knowledge-Augmented Explainable and Interpretable Learning for Anomaly Detection and Diagnosis",
    "authors": [
      "Martin Atzmueller",
      "Tim Bohne",
      "Patricia Windler"
    ],
    "abstract": "Knowledge-augmented learning enables the combination of knowledge-based and\ndata-driven approaches. For anomaly detection and diagnosis, understandability\nis typically an important factor, especially in high-risk areas. Therefore,\nexplainability and interpretability are also major criteria in such contexts.\nThis chapter focuses on knowledge-augmented explainable and interpretable\nlearning to enhance understandability, transparency and ultimately\ncomputational sensemaking. We exemplify different approaches and methods in the\ndomains of anomaly detection and diagnosis - from comparatively simple\ninterpretable methods towards more advanced neuro-symbolic approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00146v1",
    "published_date": "2024-11-28 23:42:46 UTC",
    "updated_date": "2024-11-28 23:42:46 UTC"
  },
  {
    "arxiv_id": "2411.19395v1",
    "title": "Concept-driven Off Policy Evaluation",
    "authors": [
      "Ritam Majumdar",
      "Jack Teversham",
      "Sonali Parbhoo"
    ],
    "abstract": "Evaluating off-policy decisions using batch data poses significant challenges\ndue to limited sample sizes leading to high variance. To improve Off-Policy\nEvaluation (OPE), we must identify and address the sources of this variance.\nRecent research on Concept Bottleneck Models (CBMs) shows that using\nhuman-explainable concepts can improve predictions and provide better\nunderstanding. We propose incorporating concepts into OPE to reduce variance.\nOur work introduces a family of concept-based OPE estimators, proving that they\nremain unbiased and reduce variance when concepts are known and predefined.\nSince real-world applications often lack predefined concepts, we further\ndevelop an end-to-end algorithm to learn interpretable, concise, and diverse\nparameterized concepts optimized for variance reduction. Our experiments with\nsynthetic and real-world datasets show that both known and learned\nconcept-based estimators significantly improve OPE performance. Crucially, we\nshow that, unlike other OPE methods, concept-based estimators are easily\ninterpretable and allow for targeted interventions on specific concepts,\nfurther enhancing the quality of these estimators.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "37 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.19395v1",
    "published_date": "2024-11-28 22:15:06 UTC",
    "updated_date": "2024-11-28 22:15:06 UTC"
  },
  {
    "arxiv_id": "2411.19393v2",
    "title": "Global Tensor Motion Planning",
    "authors": [
      "An T. Le",
      "Kay Hansel",
      "João Carvalho",
      "Joe Watson",
      "Julen Urain",
      "Armin Biess",
      "Georgia Chalvatzaki",
      "Jan Peters"
    ],
    "abstract": "Batch planning is increasingly necessary to quickly produce diverse and\nhigh-quality motion plans for downstream learning applications, such as\ndistillation and imitation learning. This paper presents Global Tensor Motion\nPlanning (GTMP) -- a sampling-based motion planning algorithm comprising only\ntensor operations. We introduce a novel discretization structure represented as\na random multipartite graph, enabling efficient vectorized sampling, collision\nchecking, and search. We provide a theoretical investigation showing that GTMP\nexhibits probabilistic completeness while supporting modern GPU/TPU.\nAdditionally, by incorporating smooth structures into the multipartite graph,\nGTMP directly plans smooth splines without requiring gradient-based\noptimization. Experiments on lidar-scanned occupancy maps and the\nMotionBenchMarker dataset demonstrate GTMP's computation efficiency in batch\nplanning compared to baselines, underscoring GTMP's potential as a robust,\nscalable planner for diverse applications and large-scale robot learning tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.19393v2",
    "published_date": "2024-11-28 22:07:46 UTC",
    "updated_date": "2024-12-31 14:05:58 UTC"
  },
  {
    "arxiv_id": "2412.10384v1",
    "title": "Adult learners recall and recognition performance and affective feedback when learning from an AI-generated synthetic video",
    "authors": [
      "Zoe Ruo-Yu Li",
      "Caswell Barry",
      "Mutlu Cukurova"
    ],
    "abstract": "The widespread use of generative AI has led to multiple applications of\nAI-generated text and media to potentially enhance learning outcomes. However,\nthere are a limited number of well-designed experimental studies investigating\nthe impact of learning gains and affective feedback from AI-generated media\ncompared to traditional media (e.g., text from documents and human recordings\nof video). The current study recruited 500 participants to investigate adult\nlearners recall and recognition performances as well as their affective\nfeedback on the AI-generated synthetic video, using a mixed-methods approach\nwith a pre-and post-test design. Specifically, four learning conditions,\nAI-generated framing of human instructor-generated text, AI-generated synthetic\nvideos with human instructor-generated text, human instructor-generated videos,\nand human instructor-generated text frame (baseline), were considered. The\nresults indicated no statistically significant difference amongst conditions on\nrecall and recognition performance. In addition, the participants affective\nfeedback was not statistically significantly different between the two video\nconditions. However, adult learners preferred to learn from the video formats\nrather than text materials.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "13 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10384v1",
    "published_date": "2024-11-28 21:40:28 UTC",
    "updated_date": "2024-11-28 21:40:28 UTC"
  },
  {
    "arxiv_id": "2411.19385v1",
    "title": "Zero-Forget Preservation of Semantic Communication Alignment in Distributed AI Networks",
    "authors": [
      "Jingzhi Hu",
      "Geoffrey Ye Li"
    ],
    "abstract": "Future communication networks are expected to connect massive distributed\nartificial intelligence (AI). Exploiting aligned priori knowledge of AI pairs,\nit is promising to convert high-dimensional data transmission into\nhighly-compressed semantic communications (SC). However, to accommodate the\nlocal data distribution and user preferences, AIs generally adapt to different\ndomains, which fundamentally distorts the SC alignment. In this paper, we\npropose a zero-forget domain adaptation (ZFDA) framework to preserve SC\nalignment. To prevent the DA from changing substantial neural parameters of AI,\nwe design sparse additive modifications (SAM) to the parameters, which can be\nefficiently stored and switched-off to restore the SC alignment. To optimize\nthe SAM, we decouple it into tractable continuous variables and a binary mask,\nand then handle the binary mask by a score-based optimization. Experimental\nevaluations on a SC system for image transmissions validate that the proposed\nframework perfectly preserves the SC alignment with almost no loss of DA\nperformance, even improved in some cases, at a cost of less than 1% of\nadditional memory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19385v1",
    "published_date": "2024-11-28 21:28:18 UTC",
    "updated_date": "2024-11-28 21:28:18 UTC"
  },
  {
    "arxiv_id": "2411.19379v3",
    "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
    "authors": [
      "Rui Pan",
      "Zhuang Wang",
      "Zhen Jia",
      "Can Karakus",
      "Luca Zancato",
      "Tri Dao",
      "Yida Wang",
      "Ravi Netravali"
    ],
    "abstract": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "MLSys 2025 camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2411.19379v3",
    "published_date": "2024-11-28 21:10:20 UTC",
    "updated_date": "2025-04-10 05:06:29 UTC"
  },
  {
    "arxiv_id": "2411.19378v2",
    "title": "Libra: Leveraging Temporal Images for Biomedical Radiology Analysis",
    "authors": [
      "Xi Zhang",
      "Zaiqiao Meng",
      "Jake Lever",
      "Edmond S. L. Ho"
    ],
    "abstract": "Radiology report generation (RRG) requires advanced medical image analysis,\neffective temporal reasoning, and accurate text generation. While multimodal\nlarge language models (MLLMs) align with pre-trained vision encoders to enhance\nvisual-language understanding, most existing methods rely on single-image\nanalysis or rule-based heuristics to process multiple images, failing to fully\nleverage temporal information in multi-modal medical datasets. In this paper,\nwe introduce Libra, a temporal-aware MLLM tailored for chest X-ray report\ngeneration. Libra combines a radiology-specific image encoder with a novel\nTemporal Alignment Connector (TAC), designed to accurately capture and\nintegrate temporal differences between paired current and prior images.\nExtensive experiments on the MIMIC-CXR dataset demonstrate that Libra\nestablishes a new state-of-the-art benchmark among similarly scaled MLLMs,\nsetting new standards in both clinical relevance and lexical accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.10; J.3; I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "30 pages, 5 figures, Adding Appendix",
    "pdf_url": "http://arxiv.org/pdf/2411.19378v2",
    "published_date": "2024-11-28 21:07:22 UTC",
    "updated_date": "2025-02-16 17:29:23 UTC"
  },
  {
    "arxiv_id": "2411.19360v1",
    "title": "DENIAHL: In-Context Features Influence LLM Needle-In-A-Haystack Abilities",
    "authors": [
      "Hui Dai",
      "Dan Pechi",
      "Xinyi Yang",
      "Garvit Banga",
      "Raghav Mantri"
    ],
    "abstract": "The Needle-in-a-haystack (NIAH) test is a general task used to assess\nlanguage models' (LMs') abilities to recall particular information from long\ninput context. This framework however does not provide a means of analyzing\nwhat factors, beyond context length, contribute to LMs' abilities or\ninabilities to separate and recall needles from their haystacks. To provide a\nsystematic means of assessing what features contribute to LMs' NIAH\ncapabilities, we developed a synthetic benchmark called DENIAHL (Data-oriented\nEvaluation of NIAH for LLM's). Our work expands on previous NIAH studies by\nablating NIAH features beyond typical context length including data type, size,\nand patterns. We find stark differences between GPT-3.5 and LLaMA 2-7B's\nperformance on DENIAHL, and drops in recall performance when features like item\nsize are increased, and to some degree when data type is changed from numbers\nto letters. This has implications for increasingly large context models,\ndemonstrating factors beyond item-number impact NIAH capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19360v1",
    "published_date": "2024-11-28 20:14:47 UTC",
    "updated_date": "2024-11-28 20:14:47 UTC"
  },
  {
    "arxiv_id": "2411.19359v1",
    "title": "Integrating Transit Signal Priority into Multi-Agent Reinforcement Learning based Traffic Signal Control",
    "authors": [
      "Dickness Kakitahi Kwesiga",
      "Suyash Chandra Vishnoi",
      "Angshuman Guin",
      "Michael Hunter"
    ],
    "abstract": "This study integrates Transit Signal Priority (TSP) into multi-agent\nreinforcement learning (MARL) based traffic signal control. The first part of\nthe study develops adaptive signal control based on MARL for a pair of\ncoordinated intersections in a microscopic simulation environment. The two\nagents, one for each intersection, are centrally trained using a value\ndecomposition network (VDN) architecture. The trained agents show slightly\nbetter performance compared to coordinated actuated signal control based on\noverall intersection delay at v/c of 0.95. In the second part of the study the\ntrained signal control agents are used as background signal controllers while\ndeveloping event-based TSP agents. In one variation, independent TSP agents are\nformulated and trained under a decentralized training and decentralized\nexecution (DTDE) framework to implement TSP at each intersection. In the second\nvariation, the two TSP agents are centrally trained under a centralized\ntraining and decentralized execution (CTDE) framework and VDN architecture to\nselect and implement coordinated TSP strategies across the two intersections.\nIn both cases the agents converge to the same bus delay value, but independent\nagents show high instability throughout the training process. For the test\nruns, the two independent agents reduce bus delay across the two intersections\nby 22% compared to the no TSP case while the coordinated TSP agents achieve 27%\ndelay reduction. In both cases, there is only a slight increase in delay for a\nmajority of the side street movements.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19359v1",
    "published_date": "2024-11-28 20:09:12 UTC",
    "updated_date": "2024-11-28 20:09:12 UTC"
  },
  {
    "arxiv_id": "2411.19356v1",
    "title": "Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance",
    "authors": [
      "Philipp Brauner",
      "Felix Glawe",
      "Gian Luca Liehner",
      "Luisa Vervier",
      "Martina Ziefle"
    ],
    "abstract": "Understanding public perception of artificial intelligence (AI) and the\ntradeoffs between potential risks and benefits is crucial, as these perceptions\nmight shape policy decisions, influence innovation trajectories for successful\nmarket strategies, and determine individual and societal acceptance of AI\ntechnologies. Using a representative sample of 1100 participants from Germany,\nthis study examines mental models of AI. Participants quantitatively evaluated\n71 statements about AI's future capabilities (e.g., autonomous driving, medical\ncare, art, politics, warfare, and societal divides), assessing the expected\nlikelihood of occurrence, perceived risks, benefits, and overall value. We\npresent rankings of these projections alongside visual mappings illustrating\npublic risk-benefit tradeoffs. While many scenarios were deemed likely,\nparticipants often associated them with high risks, limited benefits, and low\noverall value. Across all scenarios, 96.4% ($r^2=96.4\\%$) of the variance in\nvalue assessment can be explained by perceived risks ($\\beta=-.504$) and\nperceived benefits ($\\beta=+.710$), with no significant relation to expected\nlikelihood. Demographics and personality traits influenced perceptions of\nrisks, benefits, and overall evaluations, underscoring the importance of\nincreasing AI literacy and tailoring public information to diverse user needs.\nThese findings provide actionable insights for researchers, developers, and\npolicymakers by highlighting critical public concerns and individual factors\nessential to align AI development with individual values.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19356v1",
    "published_date": "2024-11-28 20:03:01 UTC",
    "updated_date": "2024-11-28 20:03:01 UTC"
  },
  {
    "arxiv_id": "2411.19352v2",
    "title": "OMuleT: Orchestrating Multiple Tools for Practicable Conversational Recommendation",
    "authors": [
      "Se-eun Yoon",
      "Xiaokai Wei",
      "Yexi Jiang",
      "Rachit Pareek",
      "Frank Ong",
      "Kevin Gao",
      "Julian McAuley",
      "Michelle Gong"
    ],
    "abstract": "In this paper, we present a systematic effort to design, evaluate, and\nimplement a realistic conversational recommender system (CRS). The objective of\nour system is to allow users to input free-form text to request\nrecommendations, and then receive a list of relevant and diverse items. While\nprevious work on synthetic queries augments large language models (LLMs) with\n1-3 tools, we argue that a more extensive toolbox is necessary to effectively\nhandle real user requests. As such, we propose a novel approach that equips\nLLMs with over 10 tools, providing them access to the internal knowledge base\nand API calls used in production. We evaluate our model on a dataset of real\nusers and show that it generates relevant, novel, and diverse recommendations\ncompared to vanilla LLMs. Furthermore, we conduct ablation studies to\ndemonstrate the effectiveness of using the full range of tools in our toolbox.\nWe share our designs and lessons learned from deploying the system for internal\nalpha release. Our contribution is the addressing of all four key aspects of a\npracticable CRS: (1) real user requests, (2) augmenting LLMs with a wide\nvariety of tools, (3) extensive evaluation, and (4) deployment insights.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19352v2",
    "published_date": "2024-11-28 19:53:39 UTC",
    "updated_date": "2025-01-01 00:03:24 UTC"
  },
  {
    "arxiv_id": "2412.05313v6",
    "title": "λ: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile Manipulation Robotics",
    "authors": [
      "Ahmed Jaafar",
      "Shreyas Sundara Raman",
      "Yichen Wei",
      "Sudarshan Harithas",
      "Sofia Juliani",
      "Anneke Wernerfelt",
      "Benedict Quartey",
      "Ifrah Idrees",
      "Jason Xinyu Liu",
      "Stefanie Tellex"
    ],
    "abstract": "Learning to execute long-horizon mobile manipulation tasks is crucial for\nadvancing robotics in household and workplace settings. However, current\napproaches are typically data-inefficient, underscoring the need for improved\nmodels that require realistically sized benchmarks to evaluate their\nefficiency. To address this, we introduce the LAMBDA ({\\lambda})\nbenchmark-Long-horizon Actions for Mobile-manipulation Benchmarking of Directed\nActivities-which evaluates the data efficiency of models on\nlanguage-conditioned, long-horizon, multi-room, multi-floor, pick-and-place\ntasks using a dataset of manageable size, more feasible for collection. Our\nbenchmark includes 571 human-collected demonstrations that provide realism and\ndiversity in simulated and real-world settings. Unlike planner-generated data,\nthese trajectories offer natural variability and replay-verifiability, ensuring\nrobust learning and evaluation. We leverage LAMBDA to benchmark current\nend-to-end learning methods and a modular neuro-symbolic approaches that\ncombines foundation models with task and motion planning. We find that\nend-to-end methods-even when pretrained-yield lower success rates, while\nneuro-symbolic methods perform significantly better and require less data.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.05313v6",
    "published_date": "2024-11-28 19:31:50 UTC",
    "updated_date": "2025-03-04 17:33:11 UTC"
  },
  {
    "arxiv_id": "2411.19341v1",
    "title": "An Adversarial Learning Approach to Irregular Time-Series Forecasting",
    "authors": [
      "Heejeong Nam",
      "Jihyun Kim",
      "Jimin Yeom"
    ],
    "abstract": "Forecasting irregular time series presents significant challenges due to two\nkey issues: the vulnerability of models to mean regression, driven by the noisy\nand complex nature of the data, and the limitations of traditional error-based\nevaluation metrics, which fail to capture meaningful patterns and penalize\nunrealistic forecasts. These problems result in forecasts that often misalign\nwith human intuition. To tackle these challenges, we propose an adversarial\nlearning framework with a deep analysis of adversarial components.\nSpecifically, we emphasize the importance of balancing the modeling of global\ndistribution (overall patterns) and transition dynamics (localized temporal\nchanges) to better capture the nuances of irregular time series. Overall, this\nresearch provides practical insights for improving models and evaluation\nmetrics, and pioneers the application of adversarial learning in the domian of\nirregular time-series forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AdvML-Frontiers Workshop @ NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.19341v1",
    "published_date": "2024-11-28 19:28:07 UTC",
    "updated_date": "2024-11-28 19:28:07 UTC"
  },
  {
    "arxiv_id": "2411.19339v2",
    "title": "Towards a Mechanistic Explanation of Diffusion Model Generalization",
    "authors": [
      "Matthew Niedoba",
      "Berend Zwartsenberg",
      "Kevin Murphy",
      "Frank Wood"
    ],
    "abstract": "We propose a simple, training-free mechanism which explains the\ngeneralization behaviour of diffusion models. By comparing pre-trained\ndiffusion models to their theoretically optimal empirical counterparts, we\nidentify a shared local inductive bias across a variety of network\narchitectures. From this observation, we hypothesize that network denoisers\ngeneralize through localized denoising operations, as these operations\napproximate the training objective well over much of the training distribution.\nTo validate our hypothesis, we introduce novel denoising algorithms which\naggregate local empirical denoisers to replicate network behaviour. Comparing\nthese algorithms to network denoisers across forward and reverse diffusion\nprocesses, our approach exhibits consistent visual similarity to neural network\noutputs, with lower mean squared error than previously proposed methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 23 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.19339v2",
    "published_date": "2024-11-28 19:22:17 UTC",
    "updated_date": "2025-02-14 19:20:14 UTC"
  },
  {
    "arxiv_id": "2411.19335v2",
    "title": "PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning",
    "authors": [
      "Shenghui Li",
      "Edith C. -H. Ngai",
      "Fanghua Ye",
      "Thiemo Voigt"
    ],
    "abstract": "Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a\npromising paradigm for privacy-preserving and efficient adaptation of\nPre-trained Language Models (PLMs) in Federated Learning (FL) settings. It\npreserves data privacy by keeping the data decentralized and training the model\non local devices, ensuring that raw data never leaves the user's device.\nMoreover, the integration of PEFT methods such as LoRA significantly reduces\nthe number of trainable parameters compared to fine-tuning the entire model,\nthereby minimizing communication costs and computational overhead. Despite its\npotential, the security implications of FedPEFT remain underexplored. This\npaper introduces a novel security threat to FedPEFT, termed PEFT-as-an-Attack\n(PaaA), which exposes how PEFT can be exploited as an attack vector to\ncircumvent PLMs' safety alignment and generate harmful content in response to\nmalicious prompts. Our evaluation of PaaA reveals that with less than 1% of the\nmodel's parameters set as trainable, and a small subset of clients acting\nmaliciously, the attack achieves an approximate 80% attack success rate using\nrepresentative PEFT methods such as LoRA. To mitigate this threat, we further\ninvestigate potential defense strategies, including Robust Aggregation Schemes\n(RASs) and Post-PEFT Safety Alignment (PPSA). However, our empirical analysis\nhighlights the limitations of these defenses, i.e., even the most advanced\nRASs, such as DnC and ClippedClustering, struggle to defend against PaaA in\nscenarios with highly heterogeneous data distributions. Similarly, while PPSA\ncan reduce attack success rates to below 10%, it severely degrades the model's\naccuracy on the target task. Our results underscore the urgent need for more\neffective defense mechanisms that simultaneously ensure security and maintain\nthe performance of the FedPEFT paradigm.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19335v2",
    "published_date": "2024-11-28 19:05:01 UTC",
    "updated_date": "2024-12-19 14:30:23 UTC"
  },
  {
    "arxiv_id": "2411.19331v1",
    "title": "Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation",
    "authors": [
      "Luca Barsellotti",
      "Lorenzo Bianchi",
      "Nicola Messina",
      "Fabio Carrara",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Fabrizio Falchi",
      "Rita Cucchiara"
    ],
    "abstract": "Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form\ntextual concepts without predefined training classes. While existing\nvision-language models such as CLIP can generate segmentation masks by\nleveraging coarse spatial information from Vision Transformers, they face\nchallenges in spatial localization due to their global alignment of image and\ntext features. Conversely, self-supervised visual models like DINO excel in\nfine-grained visual encoding but lack integration with language. To bridge this\ngap, we present Talk2DINO, a novel hybrid approach that combines the spatial\naccuracy of DINOv2 with the language understanding of CLIP. Our approach aligns\nthe textual embeddings of CLIP to the patch-level features of DINOv2 through a\nlearned mapping function without the need to fine-tune the underlying\nbackbones. At training time, we exploit the attention maps of DINOv2 to\nselectively align local visual patches with textual embeddings. We show that\nthe powerful semantic and localization abilities of Talk2DINO can enhance the\nsegmentation process, resulting in more natural and less noisy segmentations,\nand that our approach can also effectively distinguish foreground objects from\nthe background. Experimental results demonstrate that Talk2DINO achieves\nstate-of-the-art performance across several unsupervised OVS benchmarks. Source\ncode and models are publicly available at:\nhttps://lorebianchi98.github.io/Talk2DINO/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19331v1",
    "published_date": "2024-11-28 19:00:03 UTC",
    "updated_date": "2024-11-28 19:00:03 UTC"
  },
  {
    "arxiv_id": "2412.00142v2",
    "title": "Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers",
    "authors": [
      "Chancharik Mitra",
      "Brandon Huang",
      "Tianning Chai",
      "Zhiqiu Lin",
      "Assaf Arbelle",
      "Rogerio Feris",
      "Leonid Karlinsky",
      "Trevor Darrell",
      "Deva Ramanan",
      "Roei Herzig"
    ],
    "abstract": "Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a\nwide variety of vision-language (VL) tasks such as image captioning or visual\nquestion answering. Despite strong performance, LMMs are not directly suited\nfor foundational discriminative vision-language tasks (i.e., tasks requiring\ndiscrete label predictions) such as image classification and multiple-choice\nVQA. One key challenge in utilizing LMMs for discriminative tasks is the\nextraction of useful features from generative models. To overcome this issue,\nwe propose an approach for finding features in the model's latent space to more\neffectively leverage LMMs for discriminative tasks. Toward this end, we present\nSparse Attention Vectors (SAVs) -- a finetuning-free method that leverages\nsparse attention head activations (fewer than 1\\% of the heads) in LMMs as\nstrong features for VL tasks. With only few-shot examples, SAVs demonstrate\nstate-of-the-art performance compared to a variety of few-shot and finetuned\nbaselines on a collection of discriminative tasks. Our experiments also imply\nthat SAVs can scale in performance with additional examples and generalize to\nsimilar tasks, establishing SAVs as both effective and robust multimodal\nfeature representations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00142v2",
    "published_date": "2024-11-28 18:55:41 UTC",
    "updated_date": "2025-01-13 23:45:26 UTC"
  },
  {
    "arxiv_id": "2411.19301v1",
    "title": "Structured Object Language Modeling (SoLM): Native Structured Objects Generation Conforming to Complex Schemas with Self-Supervised Denoising",
    "authors": [
      "Amir Tavanaei",
      "Kee Kiat Koo",
      "Hayreddin Ceker",
      "Shaobai Jiang",
      "Qi Li",
      "Julien Han",
      "Karim Bouyarmane"
    ],
    "abstract": "In this paper, we study the problem of generating structured objects that\nconform to a complex schema, with intricate dependencies between the different\ncomponents (facets) of the object. The facets of the object (attributes,\nfields, columns, properties) can be a mix of short, structured,\ntype-constrained facts, or long natural-language descriptions. The object has\nto be self-consistent between the different facets in the redundant information\nit carries (relative consistency), while being grounded with respect to world\nknowledge (absolute consistency). We frame the problem as a Language Modeling\nproblem (Structured Object Language Modeling) and train an LLM to perform the\ntask natively, without requiring instructions or prompt-engineering. We propose\na self-supervised denoising method to train the model from an existing dataset\nof such objects. The input query can be the existing object itself, in which\ncase the model acts as a regenerator, completing, correcting, normalizing the\ninput, or any unstructured blurb to be structured. We show that the\nself-supervised denoising training provides a strong baseline, and that\nadditional supervised fine-tuning with small amount of human demonstrations\nleads to further improvement. Experimental results show that the proposed\nmethod matches or outperforms prompt-engineered general-purpose\nstate-of-the-art LLMs (Claude 3, Mixtral-8x7B), while being order-of-magnitude\nmore cost-efficient.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19301v1",
    "published_date": "2024-11-28 18:16:41 UTC",
    "updated_date": "2024-11-28 18:16:41 UTC"
  },
  {
    "arxiv_id": "2411.19285v2",
    "title": "BPQP: A Differentiable Convex Optimization Framework for Efficient End-to-End Learning",
    "authors": [
      "Jianming Pan",
      "Zeqi Ye",
      "Xiao Yang",
      "Xu Yang",
      "Weiqing Liu",
      "Lewen Wang",
      "Jiang Bian"
    ],
    "abstract": "Data-driven decision-making processes increasingly utilize end-to-end\nlearnable deep neural networks to render final decisions. Sometimes, the output\nof the forward functions in certain layers is determined by the solutions to\nmathematical optimization problems, leading to the emergence of differentiable\noptimization layers that permit gradient back-propagation. However, real-world\nscenarios often involve large-scale datasets and numerous constraints,\npresenting significant challenges. Current methods for differentiating\noptimization problems typically rely on implicit differentiation, which\nnecessitates costly computations on the Jacobian matrices, resulting in low\nefficiency. In this paper, we introduce BPQP, a differentiable convex\noptimization framework designed for efficient end-to-end learning. To enhance\nefficiency, we reformulate the backward pass as a simplified and decoupled\nquadratic programming problem by leveraging the structural properties of the\nKKT matrix. This reformulation enables the use of first-order optimization\nalgorithms in calculating the backward pass gradients, allowing our framework\nto potentially utilize any state-of-the-art solver. As solver technologies\nevolve, BPQP can continuously adapt and improve its efficiency. Extensive\nexperiments on both simulated and real-world datasets demonstrate that BPQP\nachieves a significant improvement in efficiency--typically an order of\nmagnitude faster in overall execution time compared to other differentiable\noptimization layers. Our results not only highlight the efficiency gains of\nBPQP but also underscore its superiority over differentiable optimization layer\nbaselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.PM"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2411.19285v2",
    "published_date": "2024-11-28 17:31:15 UTC",
    "updated_date": "2024-12-30 03:25:23 UTC"
  },
  {
    "arxiv_id": "2411.19274v1",
    "title": "On-chip Hyperspectral Image Segmentation with Fully Convolutional Networks for Scene Understanding in Autonomous Driving",
    "authors": [
      "Jon Gutiérrez-Zaballa",
      "Koldo Basterretxea",
      "Javier Echanobe",
      "M. Victoria Martínez",
      "Unai Martínez-Corral",
      "Óscar Mata Carballeira",
      "Inés del Campo"
    ],
    "abstract": "Most of current computer vision-based advanced driver assistance systems\n(ADAS) perform detection and tracking of objects quite successfully under\nregular conditions. However, under adverse weather and changing lighting\nconditions, and in complex situations with many overlapping objects, these\nsystems are not completely reliable. The spectral reflectance of the different\nobjects in a driving scene beyond the visible spectrum can offer additional\ninformation to increase the reliability of these systems, especially under\nchallenging driving conditions. Furthermore, this information may be\nsignificant enough to develop vision systems that allow for a better\nunderstanding and interpretation of the whole driving scene. In this work we\nexplore the use of snapshot, video-rate hyperspectral imaging (HSI) cameras in\nADAS on the assumption that the near infrared (NIR) spectral reflectance of\ndifferent materials can help to better segment the objects in real driving\nscenarios. To do this, we have used the HSI-Drive 1.1 dataset to perform\nvarious experiments on spectral classification algorithms. However, the\ninformation retrieval of hyperspectral recordings in natural outdoor scenarios\nis challenging, mainly because of deficient colour constancy and other inherent\nshortcomings of current snapshot HSI technology, which poses some limitations\nto the development of pure spectral classifiers. In consequence, in this work\nwe analyze to what extent the spatial features codified by standard, tiny fully\nconvolutional network (FCN) models can improve the performance of HSI\nsegmentation systems for ADAS applications.\n  The abstract above is truncated due to submission limits. For the full\nabstract, please refer to the published article.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19274v1",
    "published_date": "2024-11-28 17:10:50 UTC",
    "updated_date": "2024-11-28 17:10:50 UTC"
  },
  {
    "arxiv_id": "2412.00138v1",
    "title": "Unleashing the Power of Data Synthesis in Visual Localization",
    "authors": [
      "Sihang Li",
      "Siqi Tan",
      "Bowen Chang",
      "Jing Zhang",
      "Chen Feng",
      "Yiming Li"
    ],
    "abstract": "Visual localization, which estimates a camera's pose within a known scene, is\na long-standing challenge in vision and robotics. Recent end-to-end methods\nthat directly regress camera poses from query images have gained attention for\nfast inference. However, existing methods often struggle to generalize to\nunseen views. In this work, we aim to unleash the power of data synthesis to\npromote the generalizability of pose regression. Specifically, we lift real 2D\nimages into 3D Gaussian Splats with varying appearance and deblurring\nabilities, which are then used as a data engine to synthesize more posed\nimages. To fully leverage the synthetic data, we build a two-branch joint\ntraining pipeline, with an adversarial discriminator to bridge the syn-to-real\ngap. Experiments on established benchmarks show that our method outperforms\nstate-of-the-art end-to-end approaches, reducing translation and rotation\nerrors by 50% and 21.6% on indoor datasets, and 35.56% and 38.7% on outdoor\ndatasets. We also validate the effectiveness of our method in dynamic driving\nscenarios under varying weather conditions. Notably, as data synthesis scales\nup, our method exhibits a growing ability to interpolate and extrapolate\ntraining data for localizing unseen views. Project Page:\nhttps://ai4ce.github.io/RAP/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 21 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00138v1",
    "published_date": "2024-11-28 16:58:10 UTC",
    "updated_date": "2024-11-28 16:58:10 UTC"
  },
  {
    "arxiv_id": "2411.19245v1",
    "title": "Contrastive representations of high-dimensional, structured treatments",
    "authors": [
      "Oriol Corcoll Andreu",
      "Athanasios Vlontzos",
      "Michael O'Riordan",
      "Ciaran M. Gilligan-Lee"
    ],
    "abstract": "Estimating causal effects is vital for decision making. In standard causal\neffect estimation, treatments are usually binary- or continuous-valued.\nHowever, in many important real-world settings, treatments can be structured,\nhigh-dimensional objects, such as text, video, or audio. This provides a\nchallenge to traditional causal effect estimation. While leveraging the shared\nstructure across different treatments can help generalize to unseen treatments\nat test time, we show in this paper that using such structure blindly can lead\nto biased causal effect estimation. We address this challenge by devising a\nnovel contrastive approach to learn a representation of the high-dimensional\ntreatments, and prove that it identifies underlying causal factors and discards\nnon-causally relevant factors. We prove that this treatment representation\nleads to unbiased estimates of the causal effect, and empirically validate and\nbenchmark our results on synthetic and real-world datasets.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19245v1",
    "published_date": "2024-11-28 16:33:31 UTC",
    "updated_date": "2024-11-28 16:33:31 UTC"
  },
  {
    "arxiv_id": "2412.00136v2",
    "title": "FonTS: Text Rendering with Typography and Style Controls",
    "authors": [
      "Wenda Shi",
      "Yiren Song",
      "Dengming Zhang",
      "Jiaming Liu",
      "Xingxing Zou"
    ],
    "abstract": "Visual text rendering are widespread in various real-world applications,\nrequiring careful font selection and typographic choices. Recent progress in\ndiffusion transformer (DiT)-based text-to-image (T2I) models show promise in\nautomating these processes. However, these methods still encounter challenges\nlike inconsistent fonts, style variation, and limited fine-grained control,\nparticularly at the word-level. This paper proposes a two-stage DiT-based\npipeline to address these problems by enhancing controllability over typography\nand style in text rendering. We introduce typography control fine-tuning\n(TC-FT), an parameter-efficient fine-tuning method (on $5\\%$ key parameters)\nwith enclosing typography control tokens (ETC-tokens), which enables precise\nword-level application of typographic features. To further address style\ninconsistency in text rendering, we propose a text-agnostic style control\nadapter (SCA) that prevents content leakage while enhancing style consistency.\nTo implement TC-FT and SCA effectively, we incorporated HTML-render into the\ndata synthesis pipeline and proposed the first word-level controllable dataset.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\napproach in achieving superior word-level typographic control, font\nconsistency, and style consistency in text rendering tasks. The datasets and\nmodels will be available for academic use.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00136v2",
    "published_date": "2024-11-28 16:19:37 UTC",
    "updated_date": "2025-03-10 08:43:03 UTC"
  },
  {
    "arxiv_id": "2411.19234v1",
    "title": "SmartLLMSentry: A Comprehensive LLM Based Smart Contract Vulnerability Detection Framework",
    "authors": [
      "Oualid Zaazaa",
      "Hanan El Bakkali"
    ],
    "abstract": "Smart contracts are essential for managing digital assets in blockchain\nnetworks, highlighting the need for effective security measures. This paper\nintroduces SmartLLMSentry, a novel framework that leverages large language\nmodels (LLMs), specifically ChatGPT with in-context training, to advance smart\ncontract vulnerability detection. Traditional rule-based frameworks have\nlimitations in integrating new detection rules efficiently. In contrast,\nSmartLLMSentry utilizes LLMs to streamline this process. We created a\nspecialized dataset of five randomly selected vulnerabilities for model\ntraining and evaluation. Our results show an exact match accuracy of 91.1% with\nsufficient data, although GPT-4 demonstrated reduced performance compared to\nGPT-3 in rule generation. This study illustrates that SmartLLMSentry\nsignificantly enhances the speed and accuracy of vulnerability detection\nthrough LLMdriven rule integration, offering a new approach to improving\nBlockchain security and addressing previously underexplored vulnerabilities in\nsmart contracts.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19234v1",
    "published_date": "2024-11-28 16:02:01 UTC",
    "updated_date": "2024-11-28 16:02:01 UTC"
  },
  {
    "arxiv_id": "2411.19230v1",
    "title": "Pre-Training Graph Contrastive Masked Autoencoders are Strong Distillers for EEG",
    "authors": [
      "Xinxu Wei",
      "Kanhao Zhao",
      "Yong Jiao",
      "Nancy B. Carlisle",
      "Hua Xie",
      "Yu Zhang"
    ],
    "abstract": "Effectively utilizing extensive unlabeled high-density EEG data to improve\nperformance in scenarios with limited labeled low-density EEG data presents a\nsignificant challenge. In this paper, we address this by framing it as a graph\ntransfer learning and knowledge distillation problem. We propose a Unified\nPre-trained Graph Contrastive Masked Autoencoder Distiller, named EEG-DisGCMAE,\nto bridge the gap between unlabeled/labeled and high/low-density EEG data. To\nfully leverage the abundant unlabeled EEG data, we introduce a novel unified\ngraph self-supervised pre-training paradigm, which seamlessly integrates Graph\nContrastive Pre-training and Graph Masked Autoencoder Pre-training. This\napproach synergistically combines contrastive and generative pre-training\ntechniques by reconstructing contrastive samples and contrasting the\nreconstructions. For knowledge distillation from high-density to low-density\nEEG data, we propose a Graph Topology Distillation loss function, allowing a\nlightweight student model trained on low-density data to learn from a teacher\nmodel trained on high-density data, effectively handling missing electrodes\nthrough contrastive distillation. To integrate transfer learning and\ndistillation, we jointly pre-train the teacher and student models by\ncontrasting their queries and keys during pre-training, enabling robust\ndistillers for downstream tasks. We demonstrate the effectiveness of our method\non four classification tasks across two clinical EEG datasets with abundant\nunlabeled data and limited labeled data. The experimental results show that our\napproach significantly outperforms contemporary methods in both efficiency and\naccuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.19230v1",
    "published_date": "2024-11-28 15:53:32 UTC",
    "updated_date": "2024-11-28 15:53:32 UTC"
  },
  {
    "arxiv_id": "2411.19229v2",
    "title": "Habit Coach: Customising RAG-based chatbots to support behavior change",
    "authors": [
      "Arian Fooroogh Mand Arabi",
      "Cansu Koyuturk",
      "Michael O'Mahony",
      "Raffaella Calati",
      "Dimitri Ognibene"
    ],
    "abstract": "This paper presents the iterative development of Habit Coach, a GPT-based\nchatbot designed to support users in habit change through personalized\ninteraction. Employing a user-centered design approach, we developed the\nchatbot using a Retrieval-Augmented Generation (RAG) system, which enables\nbehavior personalization without retraining the underlying language model\n(GPT-4). The system leverages document retrieval and specialized prompts to\ntailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and\nnarrative therapy techniques. A key challenge in the development process was\nthe difficulty of translating declarative knowledge into effective interaction\nbehaviors. In the initial phase, the chatbot was provided with declarative\nknowledge about CBT via reference textbooks and high-level conversational\ngoals. However, this approach resulted in imprecise and inefficient behavior,\nas the GPT model struggled to convert static information into dynamic and\ncontextually appropriate interactions. This highlighted the limitations of\nrelying solely on declarative knowledge to guide chatbot behavior, particularly\nin nuanced, therapeutic conversations. Over four iterations, we addressed this\nissue by gradually transitioning towards procedural knowledge, refining the\nchatbot's interaction strategies, and improving its overall effectiveness. In\nthe final evaluation, 5 participants engaged with the chatbot over five\nconsecutive days, receiving individualized CBT interventions. The Self-Report\nHabit Index (SRHI) was used to measure habit strength before and after the\nintervention, revealing a reduction in habit strength post-intervention. These\nresults underscore the importance of procedural knowledge in driving effective,\npersonalized behavior change support in RAG-based systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted for Italian Workshop on Artificial Intelligence for Human\n  Machine Interaction (AIxHMI 2024), November 26, 2024, Bolzano, Italy",
    "pdf_url": "http://arxiv.org/pdf/2411.19229v2",
    "published_date": "2024-11-28 15:53:27 UTC",
    "updated_date": "2024-12-16 17:16:54 UTC"
  },
  {
    "arxiv_id": "2411.19223v5",
    "title": "On the Unknowable Limits to Prediction",
    "authors": [
      "Jiani Yan",
      "Charles Rahal"
    ],
    "abstract": "We propose a rigorous decomposition of predictive error, highlighting that\nnot all 'irreducible' error is genuinely immutable. Many domains stand to\nbenefit from iterative enhancements in measurement, construct validity, and\nmodeling. Our approach demonstrates how apparently 'unpredictable' outcomes can\nbecome more tractable with improved data (across both target and features) and\nrefined algorithms. By distinguishing aleatoric from epistemic error, we\ndelineate how accuracy may asymptotically improve--though inherent\nstochasticity may remain--and offer a robust framework for advancing\ncomputational research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19223v5",
    "published_date": "2024-11-28 15:48:02 UTC",
    "updated_date": "2025-02-10 22:34:34 UTC"
  },
  {
    "arxiv_id": "2411.19211v1",
    "title": "On the Ethical Considerations of Generative Agents",
    "authors": [
      "N'yoma Diamond",
      "Soumya Banerjee"
    ],
    "abstract": "The Generative Agents framework recently developed by Park et al. has enabled\nnumerous new technical solutions and problem-solving approaches. Academic and\nindustrial interest in generative agents has been explosive as a result of the\neffectiveness of generative agents toward emulating human behaviour. However,\nit is necessary to consider the ethical challenges and concerns posed by this\ntechnique and its usage. In this position paper, we discuss the extant\nliterature that evaluate the ethical considerations regarding generative agents\nand similar generative tools, and identify additional concerns of significant\nimportance. We also suggest guidelines and necessary future research on how to\nmitigate some of the ethical issues and systemic risks associated with\ngenerative agents.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.MA"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted (poster) to Socially Responsible Language Modelling Research\n  (SoLaR) Workshop at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.19211v1",
    "published_date": "2024-11-28 15:31:49 UTC",
    "updated_date": "2024-11-28 15:31:49 UTC"
  },
  {
    "arxiv_id": "2411.19193v1",
    "title": "Convex Regularization and Convergence of Policy Gradient Flows under Safety Constraints",
    "authors": [
      "Pekka Malo",
      "Lauri Viitasaari",
      "Antti Suominen",
      "Eeva Vilkkumaa",
      "Olli Tahvonen"
    ],
    "abstract": "This paper studies reinforcement learning (RL) in infinite-horizon dynamic\ndecision processes with almost-sure safety constraints. Such safety-constrained\ndecision processes are central to applications in autonomous systems, finance,\nand resource management, where policies must satisfy strict, state-dependent\nconstraints. We consider a doubly-regularized RL framework that combines reward\nand parameter regularization to address these constraints within continuous\nstate-action spaces. Specifically, we formulate the problem as a convex\nregularized objective with parametrized policies in the mean-field regime. Our\napproach leverages recent developments in mean-field theory and Wasserstein\ngradient flows to model policies as elements of an infinite-dimensional\nstatistical manifold, with policy updates evolving via gradient flows on the\nspace of parameter distributions. Our main contributions include establishing\nsolvability conditions for safety-constrained problems, defining smooth and\nbounded approximations that facilitate gradient flows, and demonstrating\nexponential convergence towards global solutions under sufficient\nregularization. We provide general conditions on regularization functions,\nencompassing standard entropy regularization as a special case. The results\nalso enable a particle method implementation for practical RL applications. The\ntheoretical insights and convergence guarantees presented here offer a robust\nframework for safe RL in complex, high-dimensional decision-making problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "math.PR",
      "stat.ML",
      "90C26, 90C40, 90C46, 93E20, 60B05"
    ],
    "primary_category": "cs.LG",
    "comment": "74 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.19193v1",
    "published_date": "2024-11-28 15:04:43 UTC",
    "updated_date": "2024-11-28 15:04:43 UTC"
  },
  {
    "arxiv_id": "2412.00132v1",
    "title": "Road User Classification from High-Frequency GNSS Data Using Distributed Edge Intelligence",
    "authors": [
      "Lennart Köpper",
      "Thomas Wieland"
    ],
    "abstract": "Real-world traffic involves diverse road users, ranging from pedestrians to\nheavy trucks, necessitating effective road user classification for various\napplications within Intelligent Transport Systems (ITS). Traditional approaches\noften rely on intrusive and/or expensive external hardware sensors. These\nsystems typically have limited spatial coverage. In response to these\nlimitations, this work aims to investigate an unintrusive and cost-effective\nalternative for road user classification by using high-frequency (1-2 Hz)\npositional sequences. A cutting-edge solution could involve leveraging\npositioning data from 5G networks. However, this feature is currently only\nproposed in the 3GPP standard and has not yet been implemented for outdoor\napplications by 5G equipment vendors. Therefore, our approach relies on\npositional data, that is recorded under real-world conditions using Global\nNavigation Satellite Systems (GNSS) and processed on distributed edge devices.\nAs a start-ing point, four types of road users are distinguished: pedestri-ans,\ncyclists, motorcycles, and passenger cars. While earlier approaches used\nclassical statistical methods, we propose Long Short-Term Memory (LSTM)\nrecurrent neural networks (RNNs) as the preferred classification method, as\nthey repre-sent state-of-the-art in processing sequential data. An RNN\narchitecture for road user classification, based on selected motion\ncharacteristics derived from raw positional sequences is presented and the\ninfluence of sequence length on classifica-tion quality is examined. The\nresults of the work show that RNNs are capable of efficiently classifying road\nusers on dis-tributed devices, and can particularly differentiate between types\nof motorized vehicles, based on two- to four-minute se-quences.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00132v1",
    "published_date": "2024-11-28 14:51:02 UTC",
    "updated_date": "2024-11-28 14:51:02 UTC"
  },
  {
    "arxiv_id": "2411.19182v1",
    "title": "SOWing Information: Cultivating Contextual Coherence with MLLMs in Image Generation",
    "authors": [
      "Yuhan Pei",
      "Ruoyu Wang",
      "Yongqi Yang",
      "Ye Zhu",
      "Olga Russakovsky",
      "Yu Wu"
    ],
    "abstract": "Originating from the diffusion phenomenon in physics, which describes the\nrandom movement and collisions of particles, diffusion generative models\nsimulate a random walk in the data space along the denoising trajectory. This\nallows information to diffuse across regions, yielding harmonious outcomes.\nHowever, the chaotic and disordered nature of information diffusion in\ndiffusion models often results in undesired interference between image regions,\ncausing degraded detail preservation and contextual inconsistency. In this\nwork, we address these challenges by reframing disordered diffusion as a\npowerful tool for text-vision-to-image generation (TV2I) tasks, achieving\npixel-level condition fidelity while maintaining visual and semantic coherence\nthroughout the image. We first introduce Cyclic One-Way Diffusion (COW), which\nprovides an efficient unidirectional diffusion framework for precise\ninformation transfer while minimizing disruptive interference. Building on COW,\nwe further propose Selective One-Way Diffusion (SOW), which utilizes Multimodal\nLarge Language Models (MLLMs) to clarify the semantic and spatial relationships\nwithin the image. Based on these insights, SOW combines attention mechanisms to\ndynamically regulate the direction and intensity of diffusion according to\ncontextual relationships. Extensive experiments demonstrate the untapped\npotential of controlled information diffusion, offering a path to more adaptive\nand versatile generative models in a learning-free manner.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://pyh-129.github.io/SOW/",
    "pdf_url": "http://arxiv.org/pdf/2411.19182v1",
    "published_date": "2024-11-28 14:35:25 UTC",
    "updated_date": "2024-11-28 14:35:25 UTC"
  },
  {
    "arxiv_id": "2411.19167v2",
    "title": "HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos",
    "authors": [
      "Prithviraj Banerjee",
      "Sindi Shkodrani",
      "Pierre Moulon",
      "Shreyas Hampali",
      "Shangchen Han",
      "Fan Zhang",
      "Linguang Zhang",
      "Jade Fountain",
      "Edward Miller",
      "Selen Basol",
      "Richard Newcombe",
      "Robert Wang",
      "Jakob Julian Engel",
      "Tomas Hodan"
    ],
    "abstract": "We introduce HOT3D, a publicly available dataset for egocentric hand and\nobject tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of\nrecordings that feature 19 subjects interacting with 33 diverse rigid objects.\nIn addition to simple pick-up, observe, and put-down actions, the subjects\nperform actions typical for a kitchen, office, and living room environment. The\nrecordings include multiple synchronized data streams containing egocentric\nmulti-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D\nposes of cameras, hands, and objects. The dataset is recorded with two headsets\nfrom Meta: Project Aria, which is a research prototype of AI glasses, and Quest\n3, a virtual-reality headset that has shipped millions of units. Ground-truth\nposes were obtained by a motion-capture system using small optical markers\nattached to hands and objects. Hand annotations are provided in the UmeTrack\nand MANO formats, and objects are represented by 3D meshes with PBR materials\nobtained by an in-house scanner. In our experiments, we demonstrate the\neffectiveness of multi-view egocentric data for three popular tasks: 3D hand\ntracking, model-based 6DoF object pose estimation, and 3D lifting of unknown\nin-hand objects. The evaluated multi-view methods, whose benchmarking is\nuniquely enabled by HOT3D, significantly outperform their single-view\ncounterparts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.19167v2",
    "published_date": "2024-11-28 14:09:42 UTC",
    "updated_date": "2025-04-30 13:32:06 UTC"
  },
  {
    "arxiv_id": "2412.00131v1",
    "title": "Open-Sora Plan: Open-Source Large Video Generation Model",
    "authors": [
      "Bin Lin",
      "Yunyang Ge",
      "Xinhua Cheng",
      "Zongjian Li",
      "Bin Zhu",
      "Shaodong Wang",
      "Xianyi He",
      "Yang Ye",
      "Shenghai Yuan",
      "Liuhan Chen",
      "Tanghui Jia",
      "Junwu Zhang",
      "Zhenyu Tang",
      "Yatian Pang",
      "Bin She",
      "Cen Yan",
      "Zhiheng Hu",
      "Xiaoyi Dong",
      "Lin Chen",
      "Zhang Pan",
      "Xing Zhou",
      "Shaoling Dong",
      "Yonghong Tian",
      "Li Yuan"
    ],
    "abstract": "We introduce Open-Sora Plan, an open-source project that aims to contribute a\nlarge generation model for generating desired high-resolution videos with long\ndurations based on various user inputs. Our project comprises multiple\ncomponents for the entire video generation process, including a Wavelet-Flow\nVariational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various\ncondition controllers. Moreover, many assistant strategies for efficient\ntraining and inference are designed, and a multi-dimensional data curation\npipeline is proposed for obtaining desired high-quality data. Benefiting from\nefficient thoughts, our Open-Sora Plan achieves impressive video generation\nresults in both qualitative and quantitative evaluations. We hope our careful\ndesign and practical experience can inspire the video generation research\ncommunity. All our codes and model weights are publicly available at\n\\url{https://github.com/PKU-YuanGroup/Open-Sora-Plan}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "v1.3",
    "pdf_url": "http://arxiv.org/pdf/2412.00131v1",
    "published_date": "2024-11-28 14:07:45 UTC",
    "updated_date": "2024-11-28 14:07:45 UTC"
  },
  {
    "arxiv_id": "2411.19154v1",
    "title": "DESIRE: Dynamic Knowledge Consolidation for Rehearsal-Free Continual Learning",
    "authors": [
      "Haiyang Guo",
      "Fei Zhu",
      "Fanhu Zeng",
      "Bing Liu",
      "Xu-Yao Zhang"
    ],
    "abstract": "Continual learning aims to equip models with the ability to retain previously\nlearned knowledge like a human. Recent work incorporating Parameter-Efficient\nFine-Tuning has revitalized the field by introducing lightweight extension\nmodules. However, existing methods usually overlook the issue of information\nleakage caused by the fact that the experiment data have been used in\npre-trained models. Once these duplicate data are removed in the pre-training\nphase, their performance can be severely affected. In this paper, we propose a\nnew LoRA-based rehearsal-free method named DESIRE. Our method avoids imposing\nadditional constraints during training to mitigate catastrophic forgetting,\nthereby maximizing the learning of new classes. To integrate knowledge from old\nand new tasks, we propose two efficient post-processing modules. On the one\nhand, we retain only two sets of LoRA parameters for merging and propose\ndynamic representation consolidation to calibrate the merged feature\nrepresentation. On the other hand, we propose decision boundary refinement to\naddress classifier bias when training solely on new class data. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\non multiple datasets and strikes an effective balance between stability and\nplasticity. Our code will be publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19154v1",
    "published_date": "2024-11-28 13:54:01 UTC",
    "updated_date": "2024-11-28 13:54:01 UTC"
  },
  {
    "arxiv_id": "2411.19141v1",
    "title": "On Moving Object Segmentation from Monocular Video with Transformers",
    "authors": [
      "Christian Homeyer",
      "Christoph Schnörr"
    ],
    "abstract": "Moving object detection and segmentation from a single moving camera is a\nchallenging task, requiring an understanding of recognition, motion and 3D\ngeometry. Combining both recognition and reconstruction boils down to a fusion\nproblem, where appearance and motion features need to be combined for\nclassification and segmentation. In this paper, we present a novel fusion\narchitecture for monocular motion segmentation - M3Former, which leverages the\nstrong performance of transformers for segmentation and multi-modal fusion. As\nreconstructing motion from monocular video is ill-posed, we systematically\nanalyze different 2D and 3D motion representations for this problem and their\nimportance for segmentation performance. Finally, we analyze the effect of\ntraining data and show that diverse datasets are required to achieve SotA\nperformance on Kitti and Davis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "WICCV2023",
    "pdf_url": "http://arxiv.org/pdf/2411.19141v1",
    "published_date": "2024-11-28 13:42:35 UTC",
    "updated_date": "2024-11-28 13:42:35 UTC"
  },
  {
    "arxiv_id": "2411.19140v1",
    "title": "Examining Multimodal Gender and Content Bias in ChatGPT-4o",
    "authors": [
      "Roberto Balestri"
    ],
    "abstract": "This study investigates ChatGPT-4o's multimodal content generation,\nhighlighting significant disparities in its treatment of sexual content and\nnudity versus violent and drug-related themes. Detailed analysis reveals that\nChatGPT-4o consistently censors sexual content and nudity, while showing\nleniency towards violence and drug use. Moreover, a pronounced gender bias\nemerges, with female-specific content facing stricter regulation compared to\nmale-specific content. This disparity likely stems from media scrutiny and\npublic backlash over past AI controversies, prompting tech companies to impose\nstringent guidelines on sensitive issues to protect their reputations. Our\nfindings emphasize the urgent need for AI systems to uphold genuine ethical\nstandards and accountability, transcending mere political correctness. This\nresearch contributes to the understanding of biases in AI-driven language and\nmultimodal models, calling for more balanced and ethical content moderation\npractices.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "stat.OT"
    ],
    "primary_category": "cs.CY",
    "comment": "17 pages, 4 figures, 3 tables. Conference: \"14th International\n  Conference on Artificial Intelligence, Soft Computing and Applications (AIAA\n  2024), London, 23-24 November 2024\" It will be published in the proceedings\n  \"David C. Wyld et al. (Eds): IoTE, CNDC, DSA, AIAA, NLPTA, DPPR - 2024\"",
    "pdf_url": "http://arxiv.org/pdf/2411.19140v1",
    "published_date": "2024-11-28 13:41:44 UTC",
    "updated_date": "2024-11-28 13:41:44 UTC"
  },
  {
    "arxiv_id": "2411.19134v1",
    "title": "Visual SLAMMOT Considering Multiple Motion Models",
    "authors": [
      "Peilin Tian",
      "Hao Li"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) and Multi-Object Tracking (MOT)\nare pivotal tasks in the realm of autonomous driving, attracting considerable\nresearch attention. While SLAM endeavors to generate real-time maps and\ndetermine the vehicle's pose in unfamiliar settings, MOT focuses on the\nreal-time identification and tracking of multiple dynamic objects. Despite\ntheir importance, the prevalent approach treats SLAM and MOT as independent\nmodules within an autonomous vehicle system, leading to inherent limitations.\nClassical SLAM methodologies often rely on a static environment assumption,\nsuitable for indoor rather than dynamic outdoor scenarios. Conversely,\nconventional MOT techniques typically rely on the vehicle's known state,\nconstraining the accuracy of object state estimations based on this prior. To\naddress these challenges, previous efforts introduced the unified SLAMMOT\nparadigm, yet primarily focused on simplistic motion patterns. In our team's\nprevious work IMM-SLAMMOT\\cite{IMM-SLAMMOT}, we present a novel methodology\nincorporating consideration of multiple motion models into SLAMMOT i.e. tightly\ncoupled SLAM and MOT, demonstrating its efficacy in LiDAR-based systems. This\npaper studies feasibility and advantages of instantiating this methodology as\nvisual SLAMMOT, bridging the gap between LiDAR and vision-based sensing\nmechanisms. Specifically, we propose a solution of visual SLAMMOT considering\nmultiple motion models and validate the inherent advantages of IMM-SLAMMOT in\nthe visual domain.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19134v1",
    "published_date": "2024-11-28 13:36:04 UTC",
    "updated_date": "2024-11-28 13:36:04 UTC"
  },
  {
    "arxiv_id": "2411.19121v1",
    "title": "MSG score: A Comprehensive Evaluation for Multi-Scene Video Generation",
    "authors": [
      "Daewon Yoon",
      "Hyungsuk Lee",
      "Wonsik Shin"
    ],
    "abstract": "This paper addresses the metrics required for generating multi-scene videos\nbased on a continuous scenario, as opposed to traditional short video\ngeneration. Scenario-based videos require a comprehensive evaluation that\nconsiders multiple factors such as character consistency, artistic coherence,\naesthetic quality, and the alignment of the generated content with the intended\nprompt. Additionally, in video generation, unlike single images, the movement\nof characters across frames introduces potential issues like distortion or\nunintended changes, which must be effectively evaluated and corrected. In the\ncontext of probabilistic models like diffusion, generating the desired scene\nrequires repeated sampling and manual selection, akin to how a film director\nchooses the best shots from numerous takes. We propose a score-based evaluation\nbenchmark that automates this process, enabling a more objective and efficient\nassessment of these complexities. This approach allows for the generation of\nhigh-quality multi-scene videos by selecting the best outcomes based on\nautomated scoring rather than manual inspection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19121v1",
    "published_date": "2024-11-28 13:11:50 UTC",
    "updated_date": "2024-11-28 13:11:50 UTC"
  },
  {
    "arxiv_id": "2411.19114v1",
    "title": "PREBA: A Hardware/Software Co-Design for Multi-Instance GPU based AI Inference Servers",
    "authors": [
      "Gwangoo Yeo",
      "Jiin Kim",
      "Yujeong Choi",
      "Minsoo Rhu"
    ],
    "abstract": "NVIDIA's Multi-Instance GPU (MIG) is a feature that enables system designers\nto reconfigure one large GPU into multiple smaller GPU slices. This work\ncharacterizes this emerging GPU and evaluates its effectiveness in designing\nhigh-performance AI inference servers. Our study reveals that the data\npreprocessing stage of AI inference causes significant performance bottlenecks\nto MIG. To this end, we present PREBA, which is a hardware/software co-design\ntargeting MIG inference servers. Our first proposition is an FPGA-based data\npreprocessing accelerator that unlocks the full potential of MIG with\ndomain-specific acceleration of data preprocessing. The MIG inference server\nunleashed from preprocessing overheads is then augmented with our dynamic\nbatching system that enables high-performance inference. PREBA is implemented\nend-to-end in real systems, providing a 3.7x improvement in throughput, 3.4x\nreduction in tail latency, 3.5x improvement in energy-efficiency, and 3.0x\nimprovement in cost-efficiency.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19114v1",
    "published_date": "2024-11-28 13:02:41 UTC",
    "updated_date": "2024-11-28 13:02:41 UTC"
  },
  {
    "arxiv_id": "2412.00127v2",
    "title": "Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads",
    "authors": [
      "Siqi Kou",
      "Jiachun Jin",
      "Zhihong Liu",
      "Chang Liu",
      "Ye Ma",
      "Jian Jia",
      "Quan Chen",
      "Peng Jiang",
      "Zhijie Deng"
    ],
    "abstract": "We introduce Orthus, an autoregressive (AR) transformer that excels in\ngenerating images given textual prompts, answering questions based on visual\ninputs, and even crafting lengthy image-text interleaved contents. Unlike prior\narts on unified multimodal modeling, Orthus simultaneously copes with discrete\ntext tokens and continuous image features under the AR modeling principle. The\ncontinuous treatment of visual signals minimizes the information loss for both\nimage understanding and generation while the fully AR formulation renders the\ncharacterization of the correlation between modalities straightforward. The key\nmechanism enabling Orthus to leverage these advantages lies in its\nmodality-specific heads -- one regular language modeling (LM) head predicts\ndiscrete text tokens and one diffusion head generates continuous image features\nconditioning on the output of the backbone. We devise an efficient strategy for\nbuilding Orthus -- by substituting the Vector Quantization (VQ) operation in\nthe existing unified AR model with a soft alternative, introducing a diffusion\nhead, and tuning the added modules to reconstruct images, we can create an\nOrthus-base model effortlessly (e.g., within mere 72 A100 GPU hours).\nOrthus-base can further embrace post-training to better model interleaved\nimages and texts. Empirically, Orthus surpasses competing baselines including\nShow-o and Chameleon across standard benchmarks, achieving a GenEval score of\n0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows\nexceptional mixed-modality generation capabilities, reflecting the potential\nfor handling intricate practical generation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00127v2",
    "published_date": "2024-11-28 13:00:38 UTC",
    "updated_date": "2025-04-16 10:04:24 UTC"
  },
  {
    "arxiv_id": "2411.19094v3",
    "title": "Beautimeter: Harnessing GPT for Assessing Architectural and Urban Beauty based on the 15 Properties of Living Structure",
    "authors": [
      "Bin Jiang"
    ],
    "abstract": "Beautimeter is a new tool powered by generative pre-trained transformer (GPT)\ntechnology, designed to evaluate architectural and urban beauty. Rooted in\nChristopher Alexander's theory of centers, this work builds on the idea that\nall environments possess, to varying degrees, an innate sense of life.\nAlexander identified 15 fundamental properties, such as levels of scale and\nthick boundaries, that characterize living structure, which Beautimeter uses as\na basis for its analysis. By integrating GPT's advanced natural language\nprocessing capabilities, Beautimeter assesses the extent to which a structure\nembodies these 15 properties, enabling a nuanced evaluation of architectural\nand urban aesthetics. Using ChatGPT, the tool helps users generate insights\ninto the perceived beauty and coherence of spaces. We conducted a series of\ncase studies, evaluating images of architectural and urban environments, as\nwell as carpets, paintings, and other artifacts. The results demonstrate\nBeautimeter's effectiveness in analyzing aesthetic qualities across diverse\ncontexts. Our findings suggest that by leveraging GPT technology, Beautimeter\noffers architects, urban planners, and designers a powerful tool to create\nspaces that resonate deeply with people. This paper also explores the\nimplications of such technology for architecture and urban design, highlighting\nits potential to enhance both the design process and the assessment of built\nenvironments. Keywords: Living structure, structural beauty, Christopher\nAlexander, AI in Design, human centered design",
    "categories": [
      "physics.soc-ph",
      "cs.AI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "12 pages, 6 figure, and 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.19094v3",
    "published_date": "2024-11-28 12:14:24 UTC",
    "updated_date": "2025-03-23 13:58:41 UTC"
  },
  {
    "arxiv_id": "2411.19083v1",
    "title": "ObjectRelator: Enabling Cross-View Object Relation Understanding in Ego-Centric and Exo-Centric Videos",
    "authors": [
      "Yuqian Fu",
      "Runze Wang",
      "Yanwei Fu",
      "Danda Pani Paudel",
      "Xuanjing Huang",
      "Luc Van Gool"
    ],
    "abstract": "In this paper, we focus on the Ego-Exo Object Correspondence task, an\nemerging challenge in the field of computer vision that aims to map objects\nacross ego-centric and exo-centric views. We introduce ObjectRelator, a novel\nmethod designed to tackle this task, featuring two new modules: Multimodal\nCondition Fusion (MCFuse) and SSL-based Cross-View Object Alignment\n(XObjAlign). MCFuse effectively fuses language and visual conditions to enhance\ntarget object localization, while XObjAlign enforces consistency in object\nrepresentations across views through a self-supervised alignment strategy.\nExtensive experiments demonstrate the effectiveness of ObjectRelator, achieving\nstate-of-the-art performance on Ego2Exo and Exo2Ego tasks with minimal\nadditional parameters. This work provides a foundation for future research in\ncomprehensive cross-view object relation understanding highlighting the\npotential of leveraging multimodal guidance and cross-view alignment. Codes and\nmodels will be released to advance further research in this direction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19083v1",
    "published_date": "2024-11-28 12:01:03 UTC",
    "updated_date": "2024-11-28 12:01:03 UTC"
  },
  {
    "arxiv_id": "2411.19075v1",
    "title": "LADDER: Multi-objective Backdoor Attack via Evolutionary Algorithm",
    "authors": [
      "Dazhuang Liu",
      "Yanqi Qiao",
      "Rui Wang",
      "Kaitai Liang",
      "Georgios Smaragdakis"
    ],
    "abstract": "Current black-box backdoor attacks in convolutional neural networks formulate\nattack objective(s) as single-objective optimization problems in single domain.\nDesigning triggers in single domain harms semantics and trigger robustness as\nwell as introduces visual and spectral anomaly. This work proposes a\nmulti-objective black-box backdoor attack in dual domains via evolutionary\nalgorithm (LADDER), the first instance of achieving multiple attack objectives\nsimultaneously by optimizing triggers without requiring prior knowledge about\nvictim model. In particular, we formulate LADDER as a multi-objective\noptimization problem (MOP) and solve it via multi-objective evolutionary\nalgorithm (MOEA). MOEA maintains a population of triggers with trade-offs among\nattack objectives and uses non-dominated sort to drive triggers toward optimal\nsolutions. We further apply preference-based selection to MOEA to exclude\nimpractical triggers. We state that LADDER investigates a new dual-domain\nperspective for trigger stealthiness by minimizing the anomaly between clean\nand poisoned samples in the spectral domain. Lastly, the robustness against\npreprocessing operations is achieved by pushing triggers to low-frequency\nregions. Extensive experiments comprehensively showcase that LADDER achieves\nattack effectiveness of at least 99%, attack robustness with 90.23% (50.09%\nhigher than state-of-the-art attacks on average), superior natural stealthiness\n(1.12x to 196.74x improvement) and excellent spectral stealthiness (8.45x\nenhancement) as compared to current stealthy attacks by the average $l_2$-norm\nacross 5 public datasets.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19075v1",
    "published_date": "2024-11-28 11:50:23 UTC",
    "updated_date": "2024-11-28 11:50:23 UTC"
  },
  {
    "arxiv_id": "2412.03593v1",
    "title": "CovidLLM: A Robust Large Language Model with Missing Value Adaptation and Multi-Objective Learning Strategy for Predicting Disease Severity and Clinical Outcomes in COVID-19 Patients",
    "authors": [
      "Shengjun Zhu",
      "Siyu Liu",
      "Yang Li",
      "Qing Lei",
      "Hongyan Hou",
      "Hewei Jiang",
      "Shujuan Guo",
      "Feng Wang",
      "Rongshang Chen",
      "Xionglin Fan",
      "Shengce Tao",
      "Jiaxin Cai"
    ],
    "abstract": "Coronavirus Disease 2019 (COVID-19), which emerged in 2019, has caused\nmillions of deaths worldwide. Although effective vaccines have been developed\nto mitigate severe symptoms, certain populations, particularly the elderly and\nthose with comorbidities, remain at high risk for severe outcomes and increased\nmortality. Consequently, early identification of the severity and clinical\noutcomes of the disease in these patients is vital to prevent adverse\nprognoses. Although traditional machine learning and deep learning models have\nbeen widely employed in this area, the potential of large language models\n(LLMs) remains largely unexplored. Our research focuses primarily on\nconstructing specialized prompts and adopting multi-objective learning\nstrategies. We started by selecting serological indicators that significantly\ncorrelate with clinical outcomes and disease severity to serve as input data\nfor the model. Blood test samples often contain numerous missing values, and\ntraditional models generally rely on imputation to handle these gaps in the\ndata. In contrast, LLMs offer the advantage of robust semantic understanding.\nBy setting prompts, we can explicitly inform the model when a feature's value\nis missing, without the need for imputation. For the multi-objective learning\nstrategy, the model is designed to first predict disease severity and then\npredict clinical outcomes. Given that LLMs utilize both the input text and the\ngenerated tokens as input for generating the next token, the predicted severity\nis used as a basis for generating the clinical outcome. During the fine-tuning\nof the LLM, the two objectives influence and improve each other. Our\nexperiments were implemented based on the ChatGLM model. The results\ndemonstrate the effectiveness of LLMs in this task, suggesting promising\npotential for further development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03593v1",
    "published_date": "2024-11-28 11:27:38 UTC",
    "updated_date": "2024-11-28 11:27:38 UTC"
  },
  {
    "arxiv_id": "2411.19064v1",
    "title": "Way to Specialist: Closing Loop Between Specialized LLM and Evolving Domain Knowledge Graph",
    "authors": [
      "Yutong Zhang",
      "Lixing Chen",
      "Shenghong Li",
      "Nan Cao",
      "Yang Shi",
      "Jiaxin Ding",
      "Zhe Qu",
      "Pan Zhou",
      "Yang Bai"
    ],
    "abstract": "Large language models (LLMs) have demonstrated exceptional performance across\na wide variety of domains. Nonetheless, generalist LLMs continue to fall short\nin reasoning tasks necessitating specialized knowledge. Prior investigations\ninto specialized LLMs focused on domain-specific training, which entails\nsubstantial efforts in domain data acquisition and model parameter fine-tuning.\nTo address these challenges, this paper proposes the Way-to-Specialist (WTS)\nframework, which synergizes retrieval-augmented generation with knowledge\ngraphs (KGs) to enhance the specialized capability of LLMs in the absence of\nspecialized training. In distinction to existing paradigms that merely utilize\nexternal knowledge from general KGs or static domain KGs to prompt LLM for\nenhanced domain-specific reasoning, WTS proposes an innovative\n\"LLM$\\circlearrowright$KG\" paradigm, which achieves bidirectional enhancement\nbetween specialized LLM and domain knowledge graph (DKG). The proposed paradigm\nencompasses two closely coupled components: the DKG-Augmented LLM and the\nLLM-Assisted DKG Evolution. The former retrieves question-relevant domain\nknowledge from DKG and uses it to prompt LLM to enhance the reasoning\ncapability for domain-specific tasks; the latter leverages LLM to generate new\ndomain knowledge from processed tasks and use it to evolve DKG. WTS closes the\nloop between DKG-Augmented LLM and LLM-Assisted DKG Evolution, enabling\ncontinuous improvement in the domain specialization as it progressively answers\nand learns from domain-specific questions. We validate the performance of WTS\non 6 datasets spanning 5 domains. The experimental results show that WTS\nsurpasses the previous SOTA in 4 specialized domains and achieves a maximum\nperformance improvement of 11.3%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by KDD 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.19064v1",
    "published_date": "2024-11-28 11:24:43 UTC",
    "updated_date": "2024-11-28 11:24:43 UTC"
  },
  {
    "arxiv_id": "2411.19043v1",
    "title": "Using a Feedback Loop for LLM-based Infrastructure as Code Generation",
    "authors": [
      "Mayur Amarnath Palavalli",
      "Mark Santolucito"
    ],
    "abstract": "Code generation with Large Language Models (LLMs) has helped to increase\nsoftware developer productivity in coding tasks, but has yet to have\nsignificant impact on the tasks of software developers that surround this code.\nIn particular, the challenge of infrastructure management remains an open\nquestion. We investigate the ability of an LLM agent to construct\ninfrastructure using the Infrastructure as Code (IaC) paradigm. We particularly\ninvestigate the use of a feedback loop that returns errors and warnings on the\ngenerated IaC to allow the LLM agent to improve the code. We find that, for\neach iteration of the loop, its effectiveness decreases exponentially until it\nplateaus at a certain point and becomes ineffective.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "4 pages, submitted to accepted by International Journal of Secondary\n  Computing and Applications Research",
    "pdf_url": "http://arxiv.org/pdf/2411.19043v1",
    "published_date": "2024-11-28 10:40:55 UTC",
    "updated_date": "2024-11-28 10:40:55 UTC"
  },
  {
    "arxiv_id": "2411.19039v1",
    "title": "Mars-PO: Multi-Agent Reasoning System Preference Optimization",
    "authors": [
      "Xiaoxuan Lou",
      "Chaojie Wang",
      "Bo An"
    ],
    "abstract": "Mathematical reasoning is a fundamental capability for large language models\n(LLMs), yet achieving high performance in this domain remains a significant\nchallenge. The auto-regressive generation process often makes LLMs susceptible\nto errors, hallucinations, and inconsistencies, particularly during multi-step\nreasoning. In this paper, we propose Mars-PO, a novel framework to improve the\nmathematical reasoning capabilities of LLMs through a multi-agent system. It\ncombines high-quality outputs from multiple agents into a hybrid positive\nsample set and pairs them with agent-specific negative samples to construct\nrobust preference pairs for training. By aligning agents with shared positive\nsamples while addressing individual weaknesses, Mars-PO achieves substantial\nperformance improvements on mathematical reasoning benchmarks. For example, it\nincreases the accuracy on the MATH benchmark of the state-of-the-art\ninstruction-tuned LLM, Llama3.1-8B-Instruct, from 50.38% to 57.82%.\nExperimental results further demonstrate that our method consistently\noutperforms other baselines, such as supervised fine-tuning, vanilla DPO, and\nits enhanced versions, highlighting the effectiveness of our approach.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.19039v1",
    "published_date": "2024-11-28 10:35:16 UTC",
    "updated_date": "2024-11-28 10:35:16 UTC"
  },
  {
    "arxiv_id": "2411.19027v1",
    "title": "Enhancing Neural Network Robustness Against Fault Injection Through Non-linear Weight Transformations",
    "authors": [
      "Ninnart Fuengfusin",
      "Hakaru Tamukoh"
    ],
    "abstract": "Deploying deep neural networks (DNNs) in real-world environments poses\nchallenges due to faults that can manifest in physical hardware from radiation,\naging, and temperature fluctuations. To address this, previous works have\nfocused on protecting DNNs via activation range restriction using clipped ReLU\nand finding the optimal clipping threshold. However, this work instead focuses\non constraining DNN weights by applying saturated activation functions (SAFs):\nTanh, Arctan, and others. SAFs prevent faults from causing DNN weights to\nbecome excessively large, which can lead to model failure. These methods not\nonly enhance the robustness of DNNs against fault injections but also improve\nDNN performance by a small margin. Before deployment, DNNs are trained with\nweights constrained by SAFs. During deployment, the weights without applied SAF\nare written to mediums with faults. When read, weights with faults are applied\nwith SAFs and are used for inference. We demonstrate our proposed method across\nthree datasets (CIFAR10, CIFAR100, ImageNet 2012) and across three datatypes\n(32-bit floating point (FP32), 16-bit floating point, and 8-bit fixed point).\nWe show that our method enables FP32 ResNet18 with ImageNet 2012 to operate at\na bit-error rate of 0.00001 with minor accuracy loss, while without the\nproposed method, the FP32 DNN only produces random guesses. Furthermore, to\naccelerate the training process, we demonstrate that an ImageNet 2012\npre-trained ResNet18 can be adapted to SAF by training for a few epochs with a\nslight improvement in Top-1 accuracy while still ensuring robustness against\nfault injection.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.19027v1",
    "published_date": "2024-11-28 10:01:39 UTC",
    "updated_date": "2024-11-28 10:01:39 UTC"
  },
  {
    "arxiv_id": "2412.00121v1",
    "title": "Hybrid Discriminative Attribute-Object Embedding Network for Compositional Zero-Shot Learning",
    "authors": [
      "Yang Liu",
      "Xinshuo Wang",
      "Jiale Du",
      "Xinbo Gao",
      "Jungong Han"
    ],
    "abstract": "Compositional Zero-Shot Learning (CZSL) recognizes new combinations by\nlearning from known attribute-object pairs. However, the main challenge of this\ntask lies in the complex interactions between attributes and object visual\nrepresentations, which lead to significant differences in images. In addition,\nthe long-tail label distribution in the real world makes the recognition task\nmore complicated. To address these problems, we propose a novel method, named\nHybrid Discriminative Attribute-Object Embedding (HDA-OE) network. To increase\nthe variability of training data, HDA-OE introduces an attribute-driven data\nsynthesis (ADDS) module. ADDS generates new samples with diverse attribute\nlabels by combining multiple attributes of the same object. By expanding the\nattribute space in the dataset, the model is encouraged to learn and\ndistinguish subtle differences between attributes. To further improve the\ndiscriminative ability of the model, HDA-OE introduces the subclass-driven\ndiscriminative embedding (SDDE) module, which enhances the subclass\ndiscriminative ability of the encoding by embedding subclass information in a\nfine-grained manner, helping to capture the complex dependencies between\nattributes and object visual features. The proposed model has been evaluated on\nthree benchmark datasets, and the results verify its effectiveness and\nreliability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00121v1",
    "published_date": "2024-11-28 09:50:25 UTC",
    "updated_date": "2024-11-28 09:50:25 UTC"
  },
  {
    "arxiv_id": "2412.02713v2",
    "title": "Applying IRT to Distinguish Between Human and Generative AI Responses to Multiple-Choice Assessments",
    "authors": [
      "Alona Strugatski",
      "Giora Alexandron"
    ],
    "abstract": "Generative AI is transforming the educational landscape, raising significant\nconcerns about cheating. Despite the widespread use of multiple-choice\nquestions in assessments, the detection of AI cheating in MCQ-based tests has\nbeen almost unexplored, in contrast to the focus on detecting AI-cheating on\ntext-rich student outputs. In this paper, we propose a method based on the\napplication of Item Response Theory to address this gap. Our approach operates\non the assumption that artificial and human intelligence exhibit different\nresponse patterns, with AI cheating manifesting as deviations from the expected\npatterns of human responses. These deviations are modeled using Person-Fit\nStatistics. We demonstrate that this method effectively highlights the\ndifferences between human responses and those generated by premium versions of\nleading chatbots (ChatGPT, Claude, and Gemini), but that it is also sensitive\nto the amount of AI cheating in the data. Furthermore, we show that the\nchatbots differ in their reasoning profiles. Our work provides both a\ntheoretical foundation and empirical evidence for the application of IRT to\nidentify AI cheating in MCQ-based assessments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "PRE-PRINT VERSION Accepted to The 15th International Learning\n  Analytics and Knowledge Conference (LAK25)",
    "pdf_url": "http://arxiv.org/pdf/2412.02713v2",
    "published_date": "2024-11-28 09:43:06 UTC",
    "updated_date": "2024-12-12 13:28:20 UTC"
  },
  {
    "arxiv_id": "2412.00120v1",
    "title": "Relation-Aware Meta-Learning for Zero-shot Sketch-Based Image Retrieval",
    "authors": [
      "Yang Liu",
      "Jiale Du",
      "Xinbo Gao",
      "Jungong Han"
    ],
    "abstract": "Sketch-based image retrieval (SBIR) relies on free-hand sketches to retrieve\nnatural photos within the same class. However, its practical application is\nlimited by its inability to retrieve classes absent from the training set. To\naddress this limitation, the task has evolved into Zero-Shot Sketch-Based Image\nRetrieval (ZS-SBIR), where model performance is evaluated on unseen categories.\nTraditional SBIR primarily focuses on narrowing the domain gap between photo\nand sketch modalities. However, in the zero-shot setting, the model not only\nneeds to address this cross-modal discrepancy but also requires a strong\ngeneralization capability to transfer knowledge to unseen categories. To this\nend, we propose a novel framework for ZS-SBIR that employs a pair-based\nrelation-aware quadruplet loss to bridge feature gaps. By incorporating two\nnegative samples from different modalities, the approach prevents positive\nfeatures from becoming disproportionately distant from one modality while\nremaining close to another, thus enhancing inter-class separability. We also\npropose a Relation-Aware Meta-Learning Network (RAMLN) to obtain the margin, a\nhyper-parameter of cross-modal quadruplet loss, to improve the generalization\nability of the model. RAMLN leverages external memory to store feature\ninformation, which it utilizes to assign optimal margin values. Experimental\nresults obtained on the extended Sketchy and TU-Berlin datasets show a sharp\nimprovement over existing state-of-the-art methods in ZS-SBIR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00120v1",
    "published_date": "2024-11-28 09:35:27 UTC",
    "updated_date": "2024-11-28 09:35:27 UTC"
  },
  {
    "arxiv_id": "2411.19000v3",
    "title": "An AI-driven multimodal smart home platform for continuous monitoring and intelligent assistance in post-stroke patients",
    "authors": [
      "Chenyu Tang",
      "Ruizhi Zhang",
      "Shuo Gao",
      "Zihe Zhao",
      "Zibo Zhang",
      "Jiaqi Wang",
      "Cong Li",
      "Junliang Chen",
      "Yanning Dai",
      "Shengbo Wang",
      "Ruoyu Juan",
      "Qiaoying Li",
      "Ruimou Xie",
      "Xuhang Chen",
      "Xinkai Zhou",
      "Yunjia Xia",
      "Jianan Chen",
      "Fanghao Lu",
      "Xin Li",
      "Ninglli Wang",
      "Peter Smielewski",
      "Yu Pan",
      "Hubin Zhao",
      "Luigi G. Occhipinti"
    ],
    "abstract": "At-home rehabilitation for post-stroke patients presents significant\nchallenges, as continuous, personalized care is often limited outside clinical\nsettings. Additionally, the absence of comprehensive solutions addressing\ndiverse monitoring and assistance needs in home environments complicates\nrecovery efforts. Here, we present a multimodal smart home platform designed\nfor continuous, at-home rehabilitation of post-stroke patients, integrating\nwearable sensing, ambient monitoring, and adaptive automation. A plantar\npressure insole equipped with a machine learning pipeline classifies users into\nmotor recovery stages with up to 94% accuracy, enabling quantitative tracking\nof walking patterns. A head-mounted eye-tracking module supports cognitive\nassessments and hands-free control of household devices, while ambient sensors\nensure sub-second response times for interaction. These data streams are fused\nlocally via a hierarchical Internet of Things (IoT) architecture, protecting\nprivacy and minimizing latency. An embedded large language model (LLM) agent,\nAuto-Care, continuously interprets multimodal data to provide real-time\ninterventions-issuing personalized reminders, adjusting environmental\nconditions, and notifying caregivers. Implemented in a post-stroke context,\nthis integrated smart home platform increases overall user satisfaction by an\naverage of 115% (p<0.01) compared to traditional home environment. Beyond\nstroke, the system offers a scalable framework for patient-centered, long-term\ncare in broader neurorehabilitation and aging-in-place applications.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.HC",
    "comment": "5 figures, 41 references",
    "pdf_url": "http://arxiv.org/pdf/2411.19000v3",
    "published_date": "2024-11-28 09:04:39 UTC",
    "updated_date": "2025-04-15 14:35:16 UTC"
  },
  {
    "arxiv_id": "2411.18997v1",
    "title": "GRU-PFG: Extract Inter-Stock Correlation from Stock Factors with Graph Neural Network",
    "authors": [
      "Yonggai Zhuang",
      "Haoran Chen",
      "Kequan Wang",
      "Teng Fei"
    ],
    "abstract": "The complexity of stocks and industries presents challenges for stock\nprediction. Currently, stock prediction models can be divided into two\ncategories. One category, represented by GRU and ALSTM, relies solely on stock\nfactors for prediction, with limited effectiveness. The other category,\nrepresented by HIST and TRA, incorporates not only stock factors but also\nindustry information, industry financial reports, public sentiment, and other\ninputs for prediction. The second category of models can capture correlations\nbetween stocks by introducing additional information, but the extra data is\ndifficult to standardize and generalize. Considering the current state and\nlimitations of these two types of models, this paper proposes the GRU-PFG\n(Project Factors into Graph) model. This model only takes stock factors as\ninput and extracts inter-stock correlations using graph neural networks. It\nachieves prediction results that not only outperform the others models relies\nsolely on stock factors, but also achieve comparable performance to the second\ncategory models. The experimental results show that on the CSI300 dataset, the\nIC of GRU-PFG is 0.134, outperforming HIST's 0.131 and significantly surpassing\nGRU and Transformer, achieving results better than the second category models.\nMoreover as a model that relies solely on stock factors, it has greater\npotential for generalization.",
    "categories": [
      "q-fin.CP",
      "cs.AI"
    ],
    "primary_category": "q-fin.CP",
    "comment": "17pages",
    "pdf_url": "http://arxiv.org/pdf/2411.18997v1",
    "published_date": "2024-11-28 08:50:55 UTC",
    "updated_date": "2024-11-28 08:50:55 UTC"
  },
  {
    "arxiv_id": "2411.18993v1",
    "title": "Harden Deep Neural Networks Against Fault Injections Through Weight Scaling",
    "authors": [
      "Ninnart Fuengfusin",
      "Hakaru Tamukoh"
    ],
    "abstract": "Deep neural networks (DNNs) have enabled smart applications on hardware\ndevices. However, these hardware devices are vulnerable to unintended faults\ncaused by aging, temperature variance, and write errors. These faults can cause\nbit-flips in DNN weights and significantly degrade the performance of DNNs.\nThus, protection against these faults is crucial for the deployment of DNNs in\ncritical applications. Previous works have proposed error correction codes\nbased methods, however these methods often require high overheads in both\nmemory and computation. In this paper, we propose a simple yet effective method\nto harden DNN weights by multiplying weights by constants before storing them\nto fault-prone medium. When used, these weights are divided back by the same\nconstants to restore the original scale. Our method is based on the observation\nthat errors from bit-flips have properties similar to additive noise, therefore\nby dividing by constants can reduce the absolute error from bit-flips. To\ndemonstrate our method, we conduct experiments across four ImageNet 2012\npre-trained models along with three different data types: 32-bit floating\npoint, 16-bit floating point, and 8-bit fixed point. This method demonstrates\nthat by only multiplying weights with constants, Top-1 Accuracy of 8-bit fixed\npoint ResNet50 is improved by 54.418 at bit-error rate of 0.0001.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18993v1",
    "published_date": "2024-11-28 08:47:23 UTC",
    "updated_date": "2024-11-28 08:47:23 UTC"
  },
  {
    "arxiv_id": "2412.03592v1",
    "title": "Using Images to Find Context-Independent Word Representations in Vector Space",
    "authors": [
      "Harsh Kumar"
    ],
    "abstract": "Many methods have been proposed to find vector representation for words, but\nmost rely on capturing context from the text to find semantic relationships\nbetween these vectors. We propose a novel method of using dictionary meanings\nand image depictions to find word vectors independent of any context. We use\nauto-encoder on the word images to find meaningful representations and use them\nto calculate the word vectors. We finally evaluate our method on word\nsimilarity, concept categorization and outlier detection tasks. Our method\nperforms comparably to context-based methods while taking much less training\ntime.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03592v1",
    "published_date": "2024-11-28 08:44:10 UTC",
    "updated_date": "2024-11-28 08:44:10 UTC"
  },
  {
    "arxiv_id": "2411.18990v1",
    "title": "USTCCTSU at SemEval-2024 Task 1: Reducing Anisotropy for Cross-lingual Semantic Textual Relatedness Task",
    "authors": [
      "Jianjian Li",
      "Shengwei Liang",
      "Yong Liao",
      "Hongping Deng",
      "Haiyang Yu"
    ],
    "abstract": "Cross-lingual semantic textual relatedness task is an important research task\nthat addresses challenges in cross-lingual communication and text\nunderstanding. It helps establish semantic connections between different\nlanguages, crucial for downstream tasks like machine translation, multilingual\ninformation retrieval, and cross-lingual text understanding.Based on extensive\ncomparative experiments, we choose the XLM-R-base as our base model and use\npre-trained sentence representations based on whitening to reduce\nanisotropy.Additionally, for the given training data, we design a delicate data\nfiltering method to alleviate the curse of multilingualism. With our approach,\nwe achieve a 2nd score in Spanish, a 3rd in Indonesian, and multiple entries in\nthe top ten results in the competition's track C. We further do a comprehensive\nanalysis to inspire future research aimed at improving performance on\ncross-lingual tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18990v1",
    "published_date": "2024-11-28 08:40:14 UTC",
    "updated_date": "2024-11-28 08:40:14 UTC"
  },
  {
    "arxiv_id": "2412.00117v1",
    "title": "Proceedings of the 2024 XCSP3 Competition",
    "authors": [
      "Gilles Audemard",
      "Christophe Lecoutre",
      "Emmanuel Lonca"
    ],
    "abstract": "This document represents the proceedings of the 2024 XCSP3 Competition. The\nresults of this competition of constraint solvers were presented at CP'24 (30th\nInternational Conference on Principles and Practice of Constraint Programming).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "104 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.00117v1",
    "published_date": "2024-11-28 08:16:40 UTC",
    "updated_date": "2024-11-28 08:16:40 UTC"
  },
  {
    "arxiv_id": "2411.18956v1",
    "title": "Random Sampling for Diffusion-based Adversarial Purification",
    "authors": [
      "Jiancheng Zhang",
      "Peiran Dong",
      "Yongyong Chen",
      "Yin-Ping Zhao",
      "Song Guo"
    ],
    "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have gained great attention\nin adversarial purification. Current diffusion-based works focus on designing\neffective condition-guided mechanisms while ignoring a fundamental problem,\ni.e., the original DDPM sampling is intended for stable generation, which may\nnot be the optimal solution for adversarial purification. Inspired by the\nstability of the Denoising Diffusion Implicit Model (DDIM), we propose an\nopposite sampling scheme called random sampling. In brief, random sampling will\nsample from a random noisy space during each diffusion process, while DDPM and\nDDIM sampling will continuously sample from the adjacent or original noisy\nspace. Thus, random sampling obtains more randomness and achieves stronger\nrobustness against adversarial attacks. Correspondingly, we also introduce a\nnovel mediator conditional guidance to guarantee the consistency of the\nprediction under the purified image and clean image input. To expand awareness\nof guided diffusion purification, we conduct a detailed evaluation with\ndifferent sampling methods and our random sampling achieves an impressive\nimprovement in multiple settings. Leveraging mediator-guided random sampling,\nwe also establish a baseline method named DiffAP, which significantly\noutperforms state-of-the-art (SOTA) approaches in performance and defensive\nstability. Remarkably, under strong attack, our DiffAP even achieves a more\nthan 20% robustness advantage with 10$\\times$ sampling acceleration.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18956v1",
    "published_date": "2024-11-28 07:04:09 UTC",
    "updated_date": "2024-11-28 07:04:09 UTC"
  },
  {
    "arxiv_id": "2412.07793v1",
    "title": "Publication Trends in Artificial Intelligence Conferences: The Rise of Super Prolific Authors",
    "authors": [
      "Ariful Azad",
      "Afeefa Banu"
    ],
    "abstract": "Papers published in top conferences contribute influential discoveries that\nare reshaping the landscape of modern Artificial Intelligence (AI). We analyzed\n87,137 papers from 11 AI conferences to examine publication trends over the\npast decade. Our findings reveal a consistent increase in both the number of\npapers and authors, reflecting the growing interest in AI research. We also\nobserved a rise in prolific researchers who publish dozens of papers at the\nsame conference each year. In light of this analysis, the AI research community\nshould consider revisiting authorship policies, addressing equity concerns, and\nevaluating the workload of junior researchers to foster a more sustainable and\ninclusive research environment.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.07793v1",
    "published_date": "2024-11-28 06:56:49 UTC",
    "updated_date": "2024-11-28 06:56:49 UTC"
  },
  {
    "arxiv_id": "2411.18954v2",
    "title": "NeuroLifting: Neural Inference on Markov Random Fields at Scale",
    "authors": [
      "Yaomin Wang",
      "Chaolong Ying",
      "Xiaodong Luo",
      "Tianshu Yu"
    ],
    "abstract": "Inference in large-scale Markov Random Fields (MRFs) is a critical yet\nchallenging task, traditionally approached through approximate methods like\nbelief propagation and mean field, or exact methods such as the Toulbar2\nsolver. These strategies often fail to strike an optimal balance between\nefficiency and solution quality, particularly as the problem scale increases.\nThis paper introduces NeuroLifting, a novel technique that leverages Graph\nNeural Networks (GNNs) to reparameterize decision variables in MRFs,\nfacilitating the use of standard gradient descent optimization. By extending\ntraditional lifting techniques into a non-parametric neural network framework,\nNeuroLifting benefits from the smooth loss landscape of neural networks,\nenabling efficient and parallelizable optimization. Empirical results\ndemonstrate that, on moderate scales, NeuroLifting performs very close to the\nexact solver Toulbar2 in terms of solution quality, significantly surpassing\nexisting approximate methods. Notably, on large-scale MRFs, NeuroLifting\ndelivers superior solution quality against all baselines, as well as exhibiting\nlinear computational complexity growth. This work presents a significant\nadvancement in MRF inference, offering a scalable and effective solution for\nlarge-scale problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18954v2",
    "published_date": "2024-11-28 06:50:47 UTC",
    "updated_date": "2025-05-16 08:18:23 UTC"
  },
  {
    "arxiv_id": "2411.18948v3",
    "title": "RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis",
    "authors": [
      "Xue Tan",
      "Hao Luan",
      "Mingyu Luo",
      "Xiaoyan Sun",
      "Ping Chen",
      "Jun Dai"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enriches the input to LLMs by retrieving\ninformation from the relevant knowledge database, enabling them to produce\nresponses that are more accurate and contextually appropriate. It is worth\nnoting that the knowledge database, being sourced from publicly available\nchannels such as Wikipedia, inevitably introduces a new attack surface. RAG\npoisoning involves injecting malicious texts into the knowledge database,\nultimately leading to the generation of the attacker's target response (also\ncalled poisoned response). However, there are currently limited methods\navailable for detecting such poisoning attacks. We aim to bridge the gap in\nthis work. Particularly, we introduce RevPRAG, a flexible and automated\ndetection pipeline that leverages the activations of LLMs for poisoned response\ndetection. Our investigation uncovers distinct patterns in LLMs' activations\nwhen generating correct responses versus poisoned responses. Our results on\nmultiple benchmark datasets and RAG architectures show our approach could\nachieve 98% true positive rate, while maintaining false positive rates close to\n1%.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18948v3",
    "published_date": "2024-11-28 06:29:46 UTC",
    "updated_date": "2025-04-27 11:15:15 UTC"
  },
  {
    "arxiv_id": "2412.01849v2",
    "title": "Towards Data-centric Machine Learning on Directed Graphs: a Survey",
    "authors": [
      "Henan Sun",
      "Xunkai Li",
      "Daohan Su",
      "Junyi Han",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "abstract": "In recent years, Graph Neural Networks (GNNs) have made significant advances\nin processing structured data. However, most of them primarily adopted a\nmodel-centric approach, which simplifies graphs by converting them into\nundirected formats and emphasizes model designs. This approach is inherently\nlimited in real-world applications due to the unavoidable information loss in\nsimple undirected graphs and the model optimization challenges that arise when\nexceeding the upper bounds of this sub-optimal data representational capacity.\nAs a result, there has been a shift toward data-centric methods that prioritize\nimproving graph quality and representation. Specifically, various types of\ngraphs can be derived from naturally structured data, including heterogeneous\ngraphs, hypergraphs, and directed graphs. Among these, directed graphs offer\ndistinct advantages in topological systems by modeling causal relationships,\nand directed GNNs have been extensively studied in recent years. However, a\ncomprehensive survey of this emerging topic is still lacking. Therefore, we aim\nto provide a comprehensive review of directed graph learning, with a particular\nfocus on a data-centric perspective. Specifically, we first introduce a novel\ntaxonomy for existing studies. Subsequently, we re-examine these methods from\nthe data-centric perspective, with an emphasis on understanding and improving\ndata representation. It demonstrates that a deep understanding of directed\ngraphs and their quality plays a crucial role in model performance.\nAdditionally, we explore the diverse applications of directed GNNs across 10+\ndomains, highlighting their broad applicability. Finally, we identify key\nopportunities and challenges within the field, offering insights that can guide\nfuture research and development in directed graph learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "In Progress",
    "pdf_url": "http://arxiv.org/pdf/2412.01849v2",
    "published_date": "2024-11-28 06:09:12 UTC",
    "updated_date": "2024-12-11 08:28:37 UTC"
  },
  {
    "arxiv_id": "2412.00114v2",
    "title": "SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments",
    "authors": [
      "Yue Cao",
      "Yun Xing",
      "Jie Zhang",
      "Di Lin",
      "Tianwei Zhang",
      "Ivor Tsang",
      "Yang Liu",
      "Qing Guo"
    ],
    "abstract": "Large vision-language models (LVLMs) have shown remarkable capabilities in\ninterpreting visual content. While existing works demonstrate these models'\nvulnerability to deliberately placed adversarial texts, such texts are often\neasily identifiable as anomalous. In this paper, we present the first approach\nto generate scene-coherent typographic adversarial attacks that mislead\nadvanced LVLMs while maintaining visual naturalness through the capability of\nthe LLM-based agent. Our approach addresses three critical questions: what\nadversarial text to generate, where to place it within the scene, and how to\nintegrate it seamlessly. We propose a training-free, multi-modal LLM-driven\nscene-coherent typographic adversarial planning (SceneTAP) that employs a\nthree-stage process: scene understanding, adversarial planning, and seamless\nintegration. The SceneTAP utilizes chain-of-thought reasoning to comprehend the\nscene, formulate effective adversarial text, strategically plan its placement,\nand provide detailed instructions for natural integration within the image.\nThis is followed by a scene-coherent TextDiffuser that executes the attack\nusing a local diffusion mechanism. We extend our method to real-world scenarios\nby printing and placing generated patches in physical environments,\ndemonstrating its practical implications. Extensive experiments show that our\nscene-coherent adversarial text successfully misleads state-of-the-art LVLMs,\nincluding ChatGPT-4o, even after capturing new images of physical setups. Our\nevaluations demonstrate a significant increase in attack success rates while\nmaintaining visual naturalness and contextual appropriateness. This work\nhighlights vulnerabilities in current vision-language models to sophisticated,\nscene-coherent adversarial attacks and provides insights into potential defense\nmechanisms.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00114v2",
    "published_date": "2024-11-28 05:55:13 UTC",
    "updated_date": "2025-04-08 02:54:58 UTC"
  },
  {
    "arxiv_id": "2411.18932v1",
    "title": "ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges",
    "authors": [
      "Rao Fu",
      "Ziyang Luo",
      "Hongzhan Lin",
      "Zhen Ye",
      "Jing Ma"
    ],
    "abstract": "Recent advancements in large multimodal models (LMMs) have showcased\nimpressive code generation capabilities, primarily evaluated through\nimage-to-code benchmarks. However, these benchmarks are limited to specific\nvisual programming scenarios where the logic reasoning and the multimodal\nunderstanding capacities are split apart. To fill this gap, we propose\nScratchEval, a novel benchmark designed to evaluate the visual programming\nreasoning ability of LMMs. ScratchEval is based on Scratch, a block-based\nvisual programming language widely used in children's programming education. By\nintegrating visual elements and embedded programming logic, ScratchEval\nrequires the model to process both visual information and code structure,\nthereby comprehensively evaluating its programming intent understanding\nability. Our evaluation approach goes beyond the traditional image-to-code\nmapping and focuses on unified logical thinking and problem-solving abilities,\nproviding a more comprehensive and challenging framework for evaluating the\nvisual programming ability of LMMs. ScratchEval not only fills the gap in\nexisting evaluation methods, but also provides new insights for the future\ndevelopment of LMMs in the field of visual programming. Our benchmark can be\naccessed at https://github.com/HKBUNLP/ScratchEval .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18932v1",
    "published_date": "2024-11-28 05:51:45 UTC",
    "updated_date": "2024-11-28 05:51:45 UTC"
  },
  {
    "arxiv_id": "2412.00113v1",
    "title": "Boundary-Decoder network for inverse prediction of capacitor electrostatic analysis",
    "authors": [
      "Kart-Leong Lim",
      "Rahul Dutta",
      "Mihai Rotaru"
    ],
    "abstract": "Traditional electrostatic simulation are meshed-based methods which convert\npartial differential equations into an algebraic system of equations and their\nsolutions are approximated through numerical methods. These methods are time\nconsuming and any changes in their initial or boundary conditions will require\nsolving the numerical problem again. Newer computational methods such as the\nphysics informed neural net (PINN) similarly require re-training when boundary\nconditions changes. In this work, we propose an end-to-end deep learning\napproach to model parameter changes to the boundary conditions. The proposed\nmethod is demonstrated on the test problem of a long air-filled capacitor\nstructure. The proposed approach is compared to plain vanilla deep learning\n(NN) and PINN. It is shown that our method can significantly outperform both NN\nand PINN under dynamic boundary condition as well as retaining its full\ncapability as a forward model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.00113v1",
    "published_date": "2024-11-28 05:51:00 UTC",
    "updated_date": "2024-11-28 05:51:00 UTC"
  },
  {
    "arxiv_id": "2411.18929v1",
    "title": "VIPaint: Image Inpainting with Pre-Trained Diffusion Models via Variational Inference",
    "authors": [
      "Sakshi Agarwal",
      "Gabe Hoope",
      "Erik B. Sudderth"
    ],
    "abstract": "Diffusion probabilistic models learn to remove noise that is artificially\nadded to the data during training. Novel data, like images, may then be\ngenerated from Gaussian noise through a sequence of denoising operations. While\nthis Markov process implicitly defines a joint distribution over noise-free\ndata, it is not simple to condition the generative process on masked or partial\nimages. A number of heuristic sampling procedures have been proposed for\nsolving inverse problems with diffusion priors, but these approaches do not\ndirectly approximate the true conditional distribution imposed by inference\nqueries, and are often ineffective for large masked regions. Moreover, many of\nthese baselines cannot be applied to latent diffusion models which use image\nencodings for efficiency. We instead develop a hierarchical variational\ninference algorithm that analytically marginalizes missing features, and uses a\nrigorous variational bound to optimize a non-Gaussian Markov approximation of\nthe true diffusion posterior. Through extensive experiments with both\npixel-based and latent diffusion models of images, we show that our VIPaint\nmethod significantly outperforms previous approaches in both the plausibility\nand diversity of imputations, and is easily generalized to other inverse\nproblems like deblurring and superresolution.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.18929v1",
    "published_date": "2024-11-28 05:35:36 UTC",
    "updated_date": "2024-11-28 05:35:36 UTC"
  },
  {
    "arxiv_id": "2411.18923v2",
    "title": "EzSQL: An SQL intermediate representation for improving SQL-to-text Generation",
    "authors": [
      "Meher Bhardwaj",
      "Hrishikesh Ethari",
      "Dennis Singh Moirangthem"
    ],
    "abstract": "The SQL-to-text generation task traditionally uses template base, Seq2Seq,\ntree-to-sequence, and graph-to-sequence models. Recent models take advantage of\npre-trained generative language models for this task in the Seq2Seq framework.\nHowever, treating SQL as a sequence of inputs to the pre-trained models is not\noptimal. In this work, we put forward a new SQL intermediate representation\ncalled EzSQL to align SQL with the natural language text sequence. EzSQL\nsimplifies the SQL queries and brings them closer to natural language text by\nmodifying operators and keywords, which can usually be described in natural\nlanguage. EzSQL also removes the need for set operators. Our proposed\nSQL-to-text generation model uses EzSQL as the input to a pre-trained\ngenerative language model for generating the text descriptions. We demonstrate\nthat our model is an effective state-of-the-art method to generate text\nnarrations from SQL queries on the WikiSQL and Spider datasets. We also show\nthat by generating pretraining data using our SQL-to-text generation model, we\ncan enhance the performance of Text-to-SQL parsers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under revision and review at Expert System With Applications Journal\n  after first review",
    "pdf_url": "http://arxiv.org/pdf/2411.18923v2",
    "published_date": "2024-11-28 05:24:46 UTC",
    "updated_date": "2025-04-09 05:40:29 UTC"
  },
  {
    "arxiv_id": "2411.18922v1",
    "title": "Devising a Set of Compact and Explainable Spoken Language Feature for Screening Alzheimer's Disease",
    "authors": [
      "Junan Li",
      "Yunxiang Li",
      "Yuren Wang",
      "Xixin Wu",
      "Helen Meng"
    ],
    "abstract": "Alzheimer's disease (AD) has become one of the most significant health\nchallenges in an aging society. The use of spoken language-based AD detection\nmethods has gained prevalence due to their scalability due to their\nscalability. Based on the Cookie Theft picture description task, we devised an\nexplainable and effective feature set that leverages the visual capabilities of\na large language model (LLM) and the Term Frequency-Inverse Document Frequency\n(TF-IDF) model. Our experimental results show that the newly proposed features\nconsistently outperform traditional linguistic features across two different\nclassifiers with high dimension efficiency. Our new features can be well\nexplained and interpreted step by step which enhance the interpretability of\nautomatic AD screening.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at ISCSLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.18922v1",
    "published_date": "2024-11-28 05:23:22 UTC",
    "updated_date": "2024-11-28 05:23:22 UTC"
  },
  {
    "arxiv_id": "2411.18919v1",
    "title": "Federated Continual Graph Learning",
    "authors": [
      "Yinlin Zhu",
      "Xunkai Li",
      "Miao Hu",
      "Di Wu"
    ],
    "abstract": "In the era of big data, managing evolving graph data poses substantial\nchallenges due to storage costs and privacy issues. Training graph neural\nnetworks (GNNs) on such evolving data usually causes catastrophic forgetting,\nimpairing performance on earlier tasks. Despite existing continual graph\nlearning (CGL) methods mitigating this to some extent, they predominantly\noperate in centralized architectures and overlook the potential of distributed\ngraph databases to harness collective intelligence for enhanced performance\noptimization. To address these challenges, we present a pioneering study on\nFederated Continual Graph Learning (FCGL), which adapts GNNs to multiple\nevolving graphs within decentralized settings while adhering to storage and\nprivacy constraints. Our work begins with a comprehensive empirical analysis of\nFCGL, assessing its data characteristics, feasibility, and effectiveness, and\nreveals two principal challenges: local graph forgetting (LGF), where local\nGNNs forget prior knowledge when adapting to new tasks, and global expertise\nconflict (GEC), where the global GNN exhibits sub-optimal performance in both\nadapting to new tasks and retaining old ones, arising from inconsistent client\nexpertise during server-side parameter aggregation. To tackle these, we propose\nthe POWER framework, which mitigates LGF by preserving and replaying experience\nnodes with maximum local-global coverage at each client and addresses GEC by\nusing a pseudo prototype reconstruction strategy and trajectory-aware knowledge\ntransfer at the central server. Extensive evaluations across multiple graph\ndatasets demonstrate POWER's superior performance over straightforward\nfederated extensions of the centralized CGL algorithms and vision-focused\nfederated continual learning algorithms. Our code is available at\nhttps://github.com/zyl24/FCGL_POWER.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2411.18919v1",
    "published_date": "2024-11-28 05:15:47 UTC",
    "updated_date": "2024-11-28 05:15:47 UTC"
  },
  {
    "arxiv_id": "2412.06810v1",
    "title": "I See, Therefore I Do: Estimating Causal Effects for Image Treatments",
    "authors": [
      "Abhinav Thorat",
      "Ravi Kolla",
      "Niranjan Pedanekar"
    ],
    "abstract": "Causal effect estimation under observational studies is challenging due to\nthe lack of ground truth data and treatment assignment bias. Though various\nmethods exist in literature for addressing this problem, most of them ignore\nmulti-dimensional treatment information by considering it as scalar, either\ncontinuous or discrete. Recently, certain works have demonstrated the utility\nof this rich yet complex treatment information into the estimation process,\nresulting in better causal effect estimation. However, these works have been\ndemonstrated on either graphs or textual treatments. There is a notable gap in\nexisting literature in addressing higher dimensional data such as images that\nhas a wide variety of applications. In this work, we propose a model named NICE\n(Network for Image treatments Causal effect Estimation), for estimating\nindividual causal effects when treatments are images. NICE demonstrates an\neffective way to use the rich multidimensional information present in image\ntreatments that helps in obtaining improved causal effect estimates. To\nevaluate the performance of NICE, we propose a novel semi-synthetic data\nsimulation framework that generates potential outcomes when images serve as\ntreatments. Empirical results on these datasets, under various setups including\nthe zero-shot case, demonstrate that NICE significantly outperforms existing\nmodels that incorporate treatment information for causal effect estimation.",
    "categories": [
      "cs.AI",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "8 Pages",
    "pdf_url": "http://arxiv.org/pdf/2412.06810v1",
    "published_date": "2024-11-28 04:40:15 UTC",
    "updated_date": "2024-11-28 04:40:15 UTC"
  },
  {
    "arxiv_id": "2412.05311v1",
    "title": "DRC-Coder: Automated DRC Checker Code Generation Using LLM Autonomous Agent",
    "authors": [
      "Chen-Chia Chang",
      "Chia-Tung Ho",
      "Yaguang Li",
      "Yiran Chen",
      "Haoxing Ren"
    ],
    "abstract": "In the advanced technology nodes, the integrated design rule checker (DRC) is\noften utilized in place and route tools for fast optimization loops for\npower-performance-area. Implementing integrated DRC checkers to meet the\nstandard of commercial DRC tools demands extensive human expertise to interpret\nfoundry specifications, analyze layouts, and debug code iteratively. However,\nthis labor-intensive process, requiring to be repeated by every update of\ntechnology nodes, prolongs the turnaround time of designing circuits. In this\npaper, we present DRC-Coder, a multi-agent framework with vision capabilities\nfor automated DRC code generation. By incorporating vision language models and\nlarge language models (LLM), DRC-Coder can effectively process textual, visual,\nand layout information to perform rule interpretation and coding by two\nspecialized LLMs. We also design an auto-evaluation function for LLMs to enable\nDRC code debugging. Experimental results show that targeting on a sub-3nm\ntechnology node for a state-of-the-art standard cell layout tool, DRC-Coder\nachieves perfect F1 score 1.000 in generating DRC codes for meeting the\nstandard of a commercial DRC tool, highly outperforming standard prompting\ntechniques (F1=0.631). DRC-Coder can generate code for each design rule within\nfour minutes on average, which significantly accelerates technology advancement\nand reduces engineering costs.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Proceedings of the 2025 International Symposium on Physical Design\n  (ISPD '25), March 16--19, 2025, Austin, TX, USA",
    "pdf_url": "http://arxiv.org/pdf/2412.05311v1",
    "published_date": "2024-11-28 04:29:17 UTC",
    "updated_date": "2024-11-28 04:29:17 UTC"
  },
  {
    "arxiv_id": "2412.00110v1",
    "title": "Demographic Predictability in 3D CT Foundation Embeddings",
    "authors": [
      "Guangyao Zheng",
      "Michael A. Jacobs",
      "Vishwa S. Parekh"
    ],
    "abstract": "Self-supervised foundation models have recently been successfully extended to\nencode three-dimensional (3D) computed tomography (CT) images, with excellent\nperformance across several downstream tasks, such as intracranial hemorrhage\ndetection and lung cancer risk forecasting. However, as self-supervised models\nlearn from complex data distributions, questions arise concerning whether these\nembeddings capture demographic information, such as age, sex, or race. Using\nthe National Lung Screening Trial (NLST) dataset, which contains 3D CT images\nand demographic data, we evaluated a range of classifiers: softmax regression,\nlinear regression, linear support vector machine, random forest, and decision\ntree, to predict sex, race, and age of the patients in the images. Our results\nindicate that the embeddings effectively encoded age and sex information, with\na linear regression model achieving a root mean square error (RMSE) of 3.8\nyears for age prediction and a softmax regression model attaining an AUC of\n0.998 for sex classification. Race prediction was less effective, with an AUC\nof 0.878. These findings suggest a detailed exploration into the information\nencoded in self-supervised learning frameworks is needed to help ensure fair,\nresponsible, and patient privacy-protected healthcare AI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "submitted to Radiology Cardiothoracic Imaging",
    "pdf_url": "http://arxiv.org/pdf/2412.00110v1",
    "published_date": "2024-11-28 04:26:39 UTC",
    "updated_date": "2024-11-28 04:26:39 UTC"
  },
  {
    "arxiv_id": "2412.10381v5",
    "title": "Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream Allocation in Feed",
    "authors": [
      "Jingxin Liu",
      "Xiang Gao",
      "Yisha Li",
      "Xin Li",
      "Haiyang Lu",
      "Ben Wang"
    ],
    "abstract": "In the context of a short video & live stream mixed recommendation scenario,\nthe live stream recommendation system (RS) decides whether to allocate at most\none live stream into the video feed for each user request. To maximize\nlong-term user engagement, it is crucial to determine an optimal live stream\npolicy for accurate live stream allocation. The inappropriate live stream\nallocation policy can significantly affect the duration of the usage app and\nuser retention, which ignores the long-term negative impact of live stream\nallocation. Recently, reinforcement learning (RL) has been widely applied in\nrecommendation systems to capture long-term user engagement. However,\ntraditional RL algorithms often face divergence and instability problems, which\nrestricts the application and deployment in the large-scale industrial\nrecommendation systems, especially in the aforementioned challenging scenario.\nTo address these challenges, we propose a novel Supervised Learning-enhanced\nMulti-Group Actor Critic algorithm (SL-MGAC). Specifically, we introduce a\nsupervised learning-enhanced actor-critic framework that incorporates variance\nreduction techniques, where multi-task reward learning helps restrict\nbootstrapping error accumulation during critic learning. Additionally, we\ndesign a multi-group state decomposition module for both actor and critic\nnetworks to reduce prediction variance and improve model stability. We also\npropose a novel reward function to prevent overly greedy live stream\nallocation. Empirically, we evaluate the SL-MGAC algorithm using offline policy\nevaluation (OPE) and online A/B testing. Experimental results demonstrate that\nthe proposed method not only outperforms baseline methods under the\nplatform-level constraints but also exhibits enhanced stability in online\nrecommendation scenarios.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10381v5",
    "published_date": "2024-11-28 04:06:02 UTC",
    "updated_date": "2025-05-17 01:06:52 UTC"
  },
  {
    "arxiv_id": "2411.18892v2",
    "title": "A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges",
    "authors": [
      "Majid Ghasemi",
      "Amir Hossein Moosavi",
      "Dariush Ebrahimi"
    ],
    "abstract": "Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial\nIntelligence (AI), enabling agents to learn optimal behaviors through\ninteractions with their environments. Drawing from the foundations of trial and\nerror, RL equips agents to make informed decisions through feedback in the form\nof rewards or penalties. This paper presents a comprehensive survey of RL,\nmeticulously analyzing a wide range of algorithms, from foundational tabular\nmethods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize\nand evaluate these algorithms based on key criteria such as scalability, sample\nefficiency, and suitability. We compare the methods in the form of their\nstrengths and weaknesses in diverse settings. Additionally, we offer practical\ninsights into the selection and implementation of RL algorithms, addressing\ncommon challenges like convergence, stability, and the exploration-exploitation\ndilemma. This paper serves as a comprehensive reference for researchers and\npractitioners aiming to harness the full potential of RL in solving complex,\nreal-world problems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "79 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.18892v2",
    "published_date": "2024-11-28 03:53:14 UTC",
    "updated_date": "2025-02-01 23:49:26 UTC"
  },
  {
    "arxiv_id": "2411.18888v1",
    "title": "ArEEG_Words: Dataset for Envisioned Speech Recognition using EEG for Arabic Words",
    "authors": [
      "Hazem Darwish",
      "Abdalrahman Al Malah",
      "Khloud Al Jallad",
      "Nada Ghneim"
    ],
    "abstract": "Brain-Computer-Interface (BCI) aims to support communication-impaired\npatients by translating neural signals into speech. A notable research topic in\nBCI involves Electroencephalography (EEG) signals that measure the electrical\nactivity in the brain. While significant advancements have been made in BCI EEG\nresearch, a major limitation still exists: the scarcity of publicly available\nEEG datasets for non-English languages, such as Arabic. To address this gap, we\nintroduce in this paper ArEEG_Words dataset, a novel EEG dataset recorded from\n22 participants with mean age of 22 years (5 female, 17 male) using a\n14-channel Emotiv Epoc X device. The participants were asked to be free from\nany effects on their nervous system, such as coffee, alcohol, cigarettes, and\nso 8 hours before recording. They were asked to stay calm in a clam room during\nimagining one of the 16 Arabic Words for 10 seconds. The words include 16\ncommonly used words such as up, down, left, and right. A total of 352 EEG\nrecordings were collected, then each recording was divided into multiple 250ms\nsignals, resulting in a total of 15,360 EEG signals. To the best of our\nknowledge, ArEEG_Words data is the first of its kind in Arabic EEG domain.\nMoreover, it is publicly available for researchers as we hope that will fill\nthe gap in Arabic EEG research.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2402.15733",
    "pdf_url": "http://arxiv.org/pdf/2411.18888v1",
    "published_date": "2024-11-28 03:31:12 UTC",
    "updated_date": "2024-11-28 03:31:12 UTC"
  },
  {
    "arxiv_id": "2411.18864v1",
    "title": "Redesigning the ensemble Kalman filter with a dedicated model of epistemic uncertainty",
    "authors": [
      "Chatchuea Kimchaiwong",
      "Jeremie Houssineau",
      "Adam M. Johansen"
    ],
    "abstract": "The problem of incorporating information from observations received serially\nin time is widespread in the field of uncertainty quantification. Within a\nprobabilistic framework, such problems can be addressed using standard\nfiltering techniques. However, in many real-world problems, some (or all) of\nthe uncertainty is epistemic, arising from a lack of knowledge, and is\ndifficult to model probabilistically. This paper introduces a possibilistic\nensemble Kalman filter designed for this setting and characterizes some of its\nproperties. Using possibility theory to describe epistemic uncertainty is\nappealing from a philosophical perspective, and it is easy to justify certain\nheuristics often employed in standard ensemble Kalman filters as principled\napproaches to capturing uncertainty within it. The possibilistic approach\nmotivates a robust mechanism for characterizing uncertainty which shows good\nperformance with small sample sizes, and can outperform standard ensemble\nKalman filters at given sample size, even when dealing with genuinely aleatoric\nuncertainty.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "62F15, 65C35"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18864v1",
    "published_date": "2024-11-28 02:11:23 UTC",
    "updated_date": "2024-11-28 02:11:23 UTC"
  },
  {
    "arxiv_id": "2411.18845v2",
    "title": "An Integrated Artificial Intelligence Operating System for Advanced Low-Altitude Aviation Applications",
    "authors": [
      "Minzhe Tan",
      "Xinlin Fan",
      "Jian He",
      "Yi Hou",
      "Zhan Liu",
      "Yaopeng Jiang",
      "Y. M. Jiang"
    ],
    "abstract": "This paper introduces a high-performance artificial intelligence operating\nsystem tailored for low-altitude aviation, designed to address key challenges\nsuch as real-time task execution, computational efficiency, and seamless\nmodular collaboration. Built on a powerful hardware platform and leveraging the\nUNIX architecture, the system implements a distributed data processing strategy\nthat ensures rapid and efficient synchronization across critical modules,\nincluding vision, navigation, and perception. By adopting dynamic resource\nmanagement, it optimally allocates computational resources, such as CPU and\nGPU, based on task priority and workload, ensuring high performance for\ndemanding tasks like real-time video processing and AI model inference.\nFurthermore, the system features an advanced interrupt handling mechanism that\nallows for quick responses to sudden environmental changes, such as obstacle\ndetection, by prioritizing critical tasks, thus improving safety and mission\nsuccess rates. Robust security measures, including data encryption, access\ncontrol, and fault tolerance, ensure the system's resilience against external\nthreats and its ability to recover from potential hardware or software\nfailures. Complementing these core features are modular components for image\nanalysis, multi-sensor fusion, dynamic path planning, multi-drone coordination,\nand ground station monitoring. Additionally, a low-code development platform\nsimplifies user customization, making the system adaptable to various\nmission-specific needs. This comprehensive approach ensures the system meets\nthe evolving demands of intelligent aviation, providing a stable, efficient,\nand secure environment for complex drone operations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.OS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18845v2",
    "published_date": "2024-11-28 01:24:16 UTC",
    "updated_date": "2025-01-05 05:28:09 UTC"
  },
  {
    "arxiv_id": "2412.10380v1",
    "title": "Challenges in Human-Agent Communication",
    "authors": [
      "Gagan Bansal",
      "Jennifer Wortman Vaughan",
      "Saleema Amershi",
      "Eric Horvitz",
      "Adam Fourney",
      "Hussein Mozannar",
      "Victor Dibia",
      "Daniel S. Weld"
    ],
    "abstract": "Remarkable advancements in modern generative foundation models have enabled\nthe development of sophisticated and highly capable autonomous agents that can\nobserve their environment, invoke tools, and communicate with other agents to\nsolve problems. Although such agents can communicate with users through natural\nlanguage, their complexity and wide-ranging failure modes present novel\nchallenges for human-AI interaction. Building on prior research and informed by\na communication grounding perspective, we contribute to the study of\n\\emph{human-agent communication} by identifying and analyzing twelve key\ncommunication challenges that these systems pose. These include challenges in\nconveying information from the agent to the user, challenges in enabling the\nuser to convey information to the agent, and overarching challenges that need\nto be considered across all human-agent communication. We illustrate each\nchallenge through concrete examples and identify open directions of research.\nOur findings provide insights into critical gaps in human-agent communication\nresearch and serve as an urgent call for new design patterns, principles, and\nguidelines to support transparency and control in these systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10380v1",
    "published_date": "2024-11-28 01:21:26 UTC",
    "updated_date": "2024-11-28 01:21:26 UTC"
  },
  {
    "arxiv_id": "2412.00107v1",
    "title": "Virtual Sensing to Enable Real-Time Monitoring of Inaccessible Locations \\& Unmeasurable Parameters",
    "authors": [
      "Kazuma Kobayashi",
      "Farid Ahmed",
      "Syed Bahauddin Alam"
    ],
    "abstract": "Real-time monitoring of critical parameters is essential for energy systems'\nsafe and efficient operation. However, traditional sensors often fail and\ndegrade in harsh environments where physical sensors cannot be placed\n(inaccessible locations). In addition, there are important parameters that\ncannot be directly measured by sensors. We need machine learning (ML)-based\nreal-time monitoring in those remote locations to ensure system operations.\nHowever, traditional ML models struggle to process continuous sensor profile\ndata to fit model requirements, leading to the loss of spatial relationships.\nAnother challenge for real-time monitoring is ``dataset shift\" and the need for\nfrequent retraining under varying conditions, where extensive retraining\nprohibits real-time inference. To resolve these challenges, this study\naddressed the limitations of real-time monitoring methods by enabling\nmonitoring in locations where physical sensors are impractical to deploy. Our\nproposed approach, utilizing Multi-Input Operator Network virtual sensors,\nleverages deep learning to seamlessly integrate diverse data sources and\naccurately predict key parameters in real-time without the need for additional\nphysical sensors. The approach's effectiveness is demonstrated through\nthermal-hydraulic monitoring in a nuclear reactor subchannel, achieving\nremarkable accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00107v1",
    "published_date": "2024-11-28 00:58:29 UTC",
    "updated_date": "2024-11-28 00:58:29 UTC"
  }
]
[
  {
    "arxiv_id": "2510.13853v1",
    "title": "BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation",
    "authors": [
      "Fabian Wenz",
      "Omar Bouattour",
      "Devin Yang",
      "Justin Choi",
      "Cecil Gregg",
      "Nesime Tatbul",
      "Çağatay Demiralp"
    ],
    "abstract": "Large language models (LLMs) have been successfully applied to many tasks, including text-to-SQL generation. However, much of this work has focused on publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work showed that LLMs are much less effective in querying large private enterprise data warehouses and released Beaver, the first private enterprise text-to-SQL benchmark. To create Beaver, we leveraged SQL logs, which are often readily available. However, manually annotating these logs to identify which natural language questions they answer is a daunting task. Asking database administrators, who are highly trained experts, to take on additional work to construct and validate corresponding natural language utterances is not only challenging but also quite costly. To address this challenge, we introduce BenchPress, a human-in-the-loop system designed to accelerate the creation of domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses retrieval-augmented generation (RAG) and LLMs to propose multiple natural language descriptions. Human experts then select, rank, or edit these drafts to ensure accuracy and domain alignment. We evaluated BenchPress on annotated enterprise SQL logs, demonstrating that LLM-assisted annotation drastically reduces the time and effort required to create high-quality benchmarks. Our results show that combining human verification with LLM-generated suggestions enhances annotation accuracy, benchmark reliability, and model evaluation robustness. By streamlining the creation of custom benchmarks, BenchPress offers researchers and practitioners a mechanism for assessing text-to-SQL models on a given domain-specific workload. BenchPress is freely available via our public GitHub repository at https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our website at http://dsg-mcgraw.csail.mit.edu:5000.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "CIDR'26",
    "pdf_url": "https://arxiv.org/pdf/2510.13853v1",
    "published_date": "2025-10-11 23:50:12 UTC",
    "updated_date": "2025-10-11 23:50:12 UTC"
  },
  {
    "arxiv_id": "2510.13852v2",
    "title": "ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups",
    "authors": [
      "Peter Banyas",
      "Shristi Sharma",
      "Alistair Simmons",
      "Atharva Vispute"
    ],
    "abstract": "Is an LLM telling you different facts than it's telling me? This paper introduces ConsistencyAI, an independent benchmark for measuring the factual consistency of large language models (LLMs) for different personas. ConsistencyAI tests whether, when users of different demographics ask identical questions, the model responds with factually inconsistent answers. Designed without involvement from LLM providers, this benchmark offers impartial evaluation and accountability. In our experiment, we queried 19 LLMs with prompts that requested 5 facts for each of 15 topics. We repeated this query 100 times for each LLM, each time adding prompt context from a different persona selected from a subset of personas modeling the general population. We processed the responses into sentence embeddings, computed cross-persona cosine similarity, and computed the weighted average of cross-persona cosine similarity to calculate factual consistency scores. In 100-persona experiments, scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as a benchmark threshold. xAI's Grok-3 is most consistent, while several lightweight models rank lowest. Consistency varies by topic: the job market is least consistent, G7 world leaders most consistent, and issues like vaccines or the Israeli-Palestinian conflict diverge by provider. These results show that both the provider and the topic shape the factual consistency. We release our code and interactive demo to support reproducible evaluation and encourage persona-invariant prompting strategies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "For associated code repository, see http://github.com/banyasp/consistencyAI For user-friendly web app, see http://v0-llm-comparison-webapp.vercel.app/",
    "pdf_url": "https://arxiv.org/pdf/2510.13852v2",
    "published_date": "2025-10-11 23:32:02 UTC",
    "updated_date": "2025-10-29 00:31:05 UTC"
  },
  {
    "arxiv_id": "2510.10360v1",
    "title": "Ortho-Fuse: Orthomosaic Generation for Sparse High-Resolution Crop Health Maps Through Intermediate Optical Flow Estimation",
    "authors": [
      "Rugved Katole",
      "Christopher Stewart"
    ],
    "abstract": "AI-driven crop health mapping systems offer substantial advantages over conventional monitoring approaches through accelerated data acquisition and cost reduction. However, widespread farmer adoption remains constrained by technical limitations in orthomosaic generation from sparse aerial imagery datasets. Traditional photogrammetric reconstruction requires 70-80\\% inter-image overlap to establish sufficient feature correspondences for accurate geometric registration. AI-driven systems operating under resource-constrained conditions cannot consistently achieve these overlap thresholds, resulting in degraded reconstruction quality that undermines user confidence in autonomous monitoring technologies. In this paper, we present Ortho-Fuse, an optical flow-based framework that enables the generation of a reliable orthomosaic with reduced overlap requirements. Our approach employs intermediate flow estimation to synthesize transitional imagery between consecutive aerial frames, artificially augmenting feature correspondences for improved geometric reconstruction. Experimental validation demonstrates a 20\\% reduction in minimum overlap requirements. We further analyze adoption barriers in precision agriculture to identify pathways for enhanced integration of AI-driven monitoring systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 Figures, 9 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.10360v1",
    "published_date": "2025-10-11 22:33:34 UTC",
    "updated_date": "2025-10-11 22:33:34 UTC"
  },
  {
    "arxiv_id": "2510.13850v1",
    "title": "Revisiting the UID Hypothesis in LLM Reasoning Traces",
    "authors": [
      "Minju Gwak",
      "Guijin Son",
      "Jaehyung Kim"
    ],
    "abstract": "Large language models (LLMs) often solve problems using step-by-step Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently unfaithful or hard to interpret. Inspired by the Uniform Information Density (UID) hypothesis in psycholinguistics -- which posits that humans communicate by maintaining a stable flow of information -- we introduce entropy-based metrics to analyze the information flow within reasoning traces. Surprisingly, across three challenging mathematical benchmarks, we find that successful reasoning in LLMs is globally non-uniform: correct solutions are characterized by uneven swings in information density, in stark contrast to human communication patterns. This result challenges assumptions about machine reasoning and suggests new directions for designing interpretable and adaptive reasoning models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13850v1",
    "published_date": "2025-10-11 21:19:17 UTC",
    "updated_date": "2025-10-11 21:19:17 UTC"
  },
  {
    "arxiv_id": "2510.10339v1",
    "title": "Measuring What Matters: Connecting AI Ethics Evaluations to System Attributes, Hazards, and Harms",
    "authors": [
      "Shalaleh Rismani",
      "Renee Shelby",
      "Leah Davis",
      "Negar Rostamzadeh",
      "AJung Moon"
    ],
    "abstract": "Over the past decade, an ecosystem of measures has emerged to evaluate the social and ethical implications of AI systems, largely shaped by high-level ethics principles. These measures are developed and used in fragmented ways, without adequate attention to how they are situated in AI systems. In this paper, we examine how existing measures used in the computing literature map to AI system components, attributes, hazards, and harms. Our analysis draws on a scoping review resulting in nearly 800 measures corresponding to 11 AI ethics principles. We find that most measures focus on four principles - fairness, transparency, privacy, and trust - and primarily assess model or output system components. Few measures account for interactions across system elements, and only a narrow set of hazards is typically considered for each harm type. Many measures are disconnected from where harm is experienced and lack guidance for setting meaningful thresholds. These patterns reveal how current evaluation practices remain fragmented, measuring in pieces rather than capturing how harms emerge across systems. Framing measures with respect to system attributes, hazards, and harms can strengthen regulatory oversight, support actionable practices in industry, and ground future research in systems-level understanding.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10339v1",
    "published_date": "2025-10-11 20:52:21 UTC",
    "updated_date": "2025-10-11 20:52:21 UTC"
  },
  {
    "arxiv_id": "2510.10338v1",
    "title": "Beyond Ethics: How Inclusive Innovation Drives Economic Returns in Medical AI",
    "authors": [
      "Balagopal Unnikrishnan",
      "Ariel Guerra Adames",
      "Amin Adibi",
      "Sameer Peesapati",
      "Rafal Kocielnik",
      "Shira Fischer",
      "Hillary Clinton Kasimbazi",
      "Rodrigo Gameiro",
      "Alina Peluso",
      "Chrystinne Oliveira Fernandes",
      "Maximin Lange",
      "Lovedeep Gondara",
      "Leo Anthony Celi"
    ],
    "abstract": "While ethical arguments for fairness in healthcare AI are well-established, the economic and strategic value of inclusive design remains underexplored. This perspective introduces the ``inclusive innovation dividend'' -- the counterintuitive principle that solutions engineered for diverse, constrained use cases generate superior economic returns in broader markets. Drawing from assistive technologies that evolved into billion-dollar mainstream industries, we demonstrate how inclusive healthcare AI development creates business value beyond compliance requirements. We identify four mechanisms through which inclusive innovation drives returns: (1) market expansion via geographic scalability and trust acceleration; (2) risk mitigation through reduced remediation costs and litigation exposure; (3) performance dividends from superior generalization and reduced technical debt, and (4) competitive advantages in talent acquisition and clinical adoption. We present the Healthcare AI Inclusive Innovation Framework (HAIIF), a practical scoring system that enables organizations to evaluate AI investments based on their potential to capture these benefits. HAIIF provides structured guidance for resource allocation, transforming fairness and inclusivity from regulatory checkboxes into sources of strategic differentiation. Our findings suggest that organizations investing incrementally in inclusive design can achieve expanded market reach and sustained competitive advantages, while those treating these considerations as overhead face compounding disadvantages as network effects and data advantages accrue to early movers.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10338v1",
    "published_date": "2025-10-11 20:52:16 UTC",
    "updated_date": "2025-10-11 20:52:16 UTC"
  },
  {
    "arxiv_id": "2510.12829v3",
    "title": "Mathematics with large language models as provers and verifiers",
    "authors": [
      "Hieu Le Duc",
      "Leo Liberti"
    ],
    "abstract": "During 2024 and 2025 the discussion about the theorem-proving capabilities of large language models started reporting interesting success stories, mostly to do with difficult exercises (such as problems from the International Mathematical Olympiad), but also with conjectures [Feldman & Karbasi, arXiv:2509.18383v1] formulated for the purpose of verifying whether the artificial intelligence could prove it. In this paper we report a theorem proving feat achieved by ChatGPT by using a protocol involving different prover and verifier instances of the gpt-5 model working collaboratively. To make sure that the produced proofs do not suffer from hallucinations, the final proof is formally verified by the lean proof assistant, and the conformance of premises and conclusion of the lean code is verified by a human. Our methodology is by no means complete or exact. It was nonetheless able to solve five out of six 2025 IMO problems, and close about a third of the sixty-six number theory conjectures in [Cohen, Journal of Integer Sequences, 2025].",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12829v3",
    "published_date": "2025-10-11 20:35:25 UTC",
    "updated_date": "2025-11-06 09:23:35 UTC"
  },
  {
    "arxiv_id": "2510.10332v2",
    "title": "Towards Safe Maneuvering of Double-Ackermann-Steering Robots with a Soft Actor-Critic Framework",
    "authors": [
      "Kohio Deflesselle",
      "Mélodie Daniel",
      "Aly Magassouba",
      "Miguel Aranda",
      "Olivier Ly"
    ],
    "abstract": "We present a deep reinforcement learning framework based on Soft Actor-Critic (SAC) for safe and precise maneuvering of double-Ackermann-steering mobile robots (DASMRs). Unlike holonomic or simpler non-holonomic robots such as differential-drive robots, DASMRs face strong kinematic constraints that make classical planners brittle in cluttered environments. Our framework leverages the Hindsight Experience Replay (HER) and the CrossQ overlay to encourage maneuvering efficiency while avoiding obstacles. Simulation results with a heavy four-wheel-steering rover show that the learned policy can robustly reach up to 97% of target positions while avoiding obstacles. Our framework does not rely on handcrafted trajectories or expert demonstrations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "4 pages, 3 figures, 2 tables, Accepted for Safety of Intelligent and Autonomous Vehicles: Formal Methods vs. Machine Learning approaches for reliable navigation (SIAV-FM2L) an IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025) workshop",
    "pdf_url": "https://arxiv.org/pdf/2510.10332v2",
    "published_date": "2025-10-11 20:30:37 UTC",
    "updated_date": "2025-10-14 07:59:35 UTC"
  },
  {
    "arxiv_id": "2510.10331v1",
    "title": "LLM-Friendly Knowledge Representation for Customer Support",
    "authors": [
      "Hanchen Su",
      "Wei Luo",
      "Wei Han",
      "Yu Elaine Liu",
      "Yufeng Wayne Zhang",
      "Cen Mia Zhao",
      "Ying Joy Zhang",
      "Yashar Mehdad"
    ],
    "abstract": "We propose a practical approach by integrating Large Language Models (LLMs) with a framework designed to navigate the complexities of Airbnb customer support operations. In this paper, our methodology employs a novel reformatting technique, the Intent, Context, and Action (ICA) format, which transforms policies and workflows into a structure more comprehensible to LLMs. Additionally, we develop a synthetic data generation strategy to create training data with minimal human intervention, enabling cost-effective fine-tuning of our model. Our internal experiments (not applied to Airbnb products) demonstrate that our approach of restructuring workflows and fine-tuning LLMs with synthetic data significantly enhances their performance, setting a new benchmark for their application in customer support. Our solution is not only cost-effective but also improves customer support, as evidenced by both accuracy and manual processing time evaluation metrics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10331v1",
    "published_date": "2025-10-11 20:24:50 UTC",
    "updated_date": "2025-10-11 20:24:50 UTC"
  },
  {
    "arxiv_id": "2510.15963v2",
    "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation",
    "authors": [
      "Jiani Huang",
      "Amish Sethi",
      "Matthew Kuo",
      "Mayank Keoliya",
      "Neelay Velingker",
      "JungHo Jung",
      "Ser-Nam Lim",
      "Ziyang Li",
      "Mayur Naik"
    ],
    "abstract": "Multi-modal large language models (MLLMs) are making rapid progress toward general-purpose embodied agents. However, existing MLLMs do not reliably capture fine-grained links between low-level visual features and high-level textual semantics, leading to weak grounding and inaccurate perception. To overcome this challenge, we propose ESCA, a framework that contextualizes embodied agents by grounding their perception in spatial-temporal scene graphs. At its core is SGCLIP, a novel, open-domain, promptable foundation model for generating scene graphs that is based on CLIP. SGCLIP is trained on 87K+ open-domain videos using a neurosymbolic pipeline that aligns automatically generated captions with scene graphs produced by the model itself, eliminating the need for human-labeled annotations. We demonstrate that SGCLIP excels in both prompt-based inference and task-specific fine-tuning, achieving state-of-the-art results on scene graph generation and action localization benchmarks. ESCA with SGCLIP improves perception for embodied agents based on both open-source and commercial MLLMs, achieving state of-the-art performance across two embodied environments. Notably, ESCA significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines. We release the source code for SGCLIP model training at https://github.com/video-fm/LASER and for the embodied agent at https://github.com/video-fm/ESCA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as a Spotlight Paper at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.15963v2",
    "published_date": "2025-10-11 20:13:59 UTC",
    "updated_date": "2025-10-27 17:51:21 UTC"
  },
  {
    "arxiv_id": "2510.15962v1",
    "title": "CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models",
    "authors": [
      "Zhuxuanzi Wang",
      "Mingqiao Mo",
      "Xi Xiao",
      "Chen Liu",
      "Chenrui Ma",
      "Yunbei Zhang",
      "Xiao Wang",
      "Smita Krishnaswamy",
      "Tianyang Wang"
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) has become the standard approach for adapting large language models under limited compute and memory budgets. Although previous methods improve efficiency through low-rank updates, quantization, or heuristic budget reallocation, they often decouple the allocation of capacity from the way updates evolve during training. In this work, we introduce CTR-LoRA, a framework guided by curvature trust region that integrates rank scheduling with stability-aware optimization. CTR-LoRA allocates parameters based on marginal utility derived from lightweight second-order proxies and constrains updates using a Fisher/Hessian-metric trust region. Experiments on multiple open-source backbones (7B-13B), evaluated on both in-distribution and out-of-distribution benchmarks, show consistent improvements over strong PEFT baselines. In addition to increased accuracy, CTR-LoRA enhances training stability, reduces memory requirements, and achieves higher throughput, positioning it on the Pareto frontier of performance and efficiency. These results highlight a principled path toward more robust and deployable PEFT.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15962v1",
    "published_date": "2025-10-11 20:05:56 UTC",
    "updated_date": "2025-10-11 20:05:56 UTC"
  },
  {
    "arxiv_id": "2510.10327v1",
    "title": "Mapping the Urban Mobility Intelligence Frontier: A Scientometric Analysis of Data-Driven Pedestrian Trajectory Prediction and Simulation",
    "authors": [
      "Junhao Xu",
      "Hui Zeng"
    ],
    "abstract": "Understanding and predicting pedestrian dynamics has become essential for shaping safer, more responsive, and human-centered urban environments. This study conducts a comprehensive scientometric analysis of research on data-driven pedestrian trajectory prediction and crowd simulation, mapping its intellectual evolution and interdisciplinary structure. Using bibliometric data from the Web of Science Core Collection, we employ SciExplorer and Bibliometrix to identify major trends, influential contributors, and emerging frontiers. Results reveal a strong convergence between artificial intelligence, urban informatics, and crowd behavior modeling--driven by graph neural networks, transformers, and generative models. Beyond technical advances, the field increasingly informs urban mobility design, public safety planning, and digital twin development for smart cities. However, challenges remain in ensuring interpretability, inclusivity, and cross-domain transferability. By connecting methodological trajectories with urban applications, this work highlights how data-driven approaches can enrich urban governance and pave the way for adaptive, socially responsible mobility intelligence in future cities.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10327v1",
    "published_date": "2025-10-11 19:58:40 UTC",
    "updated_date": "2025-10-11 19:58:40 UTC"
  },
  {
    "arxiv_id": "2510.13848v1",
    "title": "On-device System of Compositional Multi-tasking in Large Language Models",
    "authors": [
      "Ondrej Bohdal",
      "Konstantinos Theodosiadis",
      "Asterios Mpatziakas",
      "Dimitris Filippidis",
      "Iro Spyrou",
      "Christos Zonios",
      "Anastasios Drosou",
      "Dimosthenis Ioannidis",
      "Kyeng-Hun Lee",
      "Jijoong Moon",
      "Hyeonmok Ko",
      "Mete Ozay",
      "Umberto Michieli"
    ],
    "abstract": "Large language models (LLMs) are commonly adapted for diverse downstream tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters (LoRA). While adapters can be combined to handle multiple tasks separately, standard approaches struggle when targeting the simultaneous execution of complex tasks, such as generating a translated summary from a long conversation. To address this challenge, we propose a novel approach tailored specifically for compositional multi-tasking scenarios involving summarization and translation. Our technique involves adding a learnable projection layer on top of the combined summarization and translation adapters. This design enables effective integration while maintaining efficiency through reduced computational overhead compared to alternative strategies requiring extensive retraining or sequential processing. We demonstrate the practical viability of our method within an on-device environment by developing an Android app capable of executing compositional tasks seamlessly. Experimental results indicate our solution performs well and is fast in both cloud-based and on-device implementations, highlighting the potential benefits of adopting our framework in real-world applications demanding high-speed operation alongside resource constraints.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2025 (industry track)",
    "pdf_url": "https://arxiv.org/pdf/2510.13848v1",
    "published_date": "2025-10-11 19:49:22 UTC",
    "updated_date": "2025-10-11 19:49:22 UTC"
  },
  {
    "arxiv_id": "2510.10325v1",
    "title": "KG-MAS: Knowledge Graph-Enhanced Multi-Agent Infrastructure for coupling physical and digital robotic environments",
    "authors": [
      "Walid Abdela"
    ],
    "abstract": "The seamless integration of physical and digital environments in Cyber-Physical Systems(CPS), particularly within Industry 4.0, presents significant challenges stemming from system heterogeneity and complexity. Traditional approaches often rely on rigid, data-centric solutions like co-simulation frameworks or brittle point-to-point middleware bridges, which lack the semantic richness and flexibility required for intelligent, autonomous coordination. This report introduces the Knowledge Graph-Enhanced Multi-Agent Infrastructure(KG-MAS), as resolution in addressing such limitations. KG-MAS leverages a centralized Knowledge Graph (KG) as a dynamic, shared world model, providing a common semantic foundation for a Multi-Agent System(MAS). Autonomous agents, representing both physical and digital components, query this KG for decision-making and update it with real-time state information. The infrastructure features a model-driven architecture which facilitates the automatic generation of agents from semantic descriptions, thereby simplifying system extension and maintenance. By abstracting away underlying communication protocols and providing a unified, intelligent coordination mechanism, KG-MAS offers a robust, scalable, and flexible solution for coupling heterogeneous physical and digital robotic environments.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10325v1",
    "published_date": "2025-10-11 19:41:47 UTC",
    "updated_date": "2025-10-11 19:41:47 UTC"
  },
  {
    "arxiv_id": "2510.13847v2",
    "title": "DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models",
    "authors": [
      "Jinbin Zhang",
      "Nasib Ullah",
      "Erik Schultheis",
      "Rohit Babbar"
    ],
    "abstract": "Speculative decoding has become a standard way to accelerate LLM inference: a small drafter proposes multiple tokens and a large target model verifies them once per speculation length. Recently, scaling of the LLM vocabulary has pushed the number of tokens to grow substantially. While verification over the full vocabulary leaves the target model largely unaffected, the O(|V|d) parameters in the drafter's output head become a latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g., FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed top frequent subset of the target model's vocabulary. Although this reduces draft-time compute, it is brittle, since: (i) frequency lists are corpus-dependent and require retuning to generalize, and (ii) static shortlists suppress rare or domain-specific tokens, lowering the expected number of tokens per verification step. We propose DynaSpec, a context-dependent dynamic shortlisting mechanism that is robust, speeds up drafting, and generalizes across diverse tasks. Concretely, we introduce lightweight, coarse-grained meta-classifiers that route contexts to a small number of token clusters; the union of the top-k selected clusters forms the drafter's shortlist, while verification retains the full vocabulary and exactness. The meta-classifier finishes its computation earlier than the drafter's hidden state generation by exploiting parallel execution of draft encoding and meta shortlisting on separate streams. Across standard speculative decoding benchmarks, DynaSpec delivers consistent improvements in mean accepted length, for Llama-3-8B, reaching upto 98.2% of full-vocabulary performance, while fixed-shortlist baselines attain only 84.4%. By leveraging context-dependent selection, DynaSpec achieves up to a 2.18 times increase in generated tokens compared to 1.91 times for fixed-vocabulary approaches.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13847v2",
    "published_date": "2025-10-11 19:38:07 UTC",
    "updated_date": "2025-11-10 12:41:15 UTC"
  },
  {
    "arxiv_id": "2510.13846v1",
    "title": "Information flow in multilayer perceptrons: an in-depth analysis",
    "authors": [
      "Giuliano Armano"
    ],
    "abstract": "Analysing how information flows along the layers of a multilayer perceptron is a topic of paramount importance in the field of artificial neural networks. After framing the problem from the point of view of information theory, in this position article a specific investigation is conducted on the way information is processed, with particular reference to the requirements imposed by supervised learning. To this end, the concept of information matrix is devised and then used as formal framework for understanding the aetiology of optimisation strategies and for studying the information flow. The underlying research for this article has also produced several key outcomes: i) the definition of a parametric optimisation strategy, ii) the finding that the optimisation strategy proposed in the information bottleneck framework shares strong similarities with the one derived from the information matrix, and iii) the insight that a multilayer perceptron serves as a kind of \"adaptor\", meant to process the input according to the given objective.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.IT",
    "comment": ">30 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.13846v1",
    "published_date": "2025-10-11 19:38:06 UTC",
    "updated_date": "2025-10-11 19:38:06 UTC"
  },
  {
    "arxiv_id": "2510.10321v1",
    "title": "Bridging Semantics & Structure for Software Vulnerability Detection using Hybrid Network Models",
    "authors": [
      "Jugal Gajjar",
      "Kaustik Ranaware",
      "Kamalasankari Subramaniakuppusamy"
    ],
    "abstract": "Software vulnerabilities remain a persistent risk, yet static and dynamic analyses often overlook structural dependencies that shape insecure behaviors. Viewing programs as heterogeneous graphs, we capture control- and data-flow relations as complex interaction networks. Our hybrid framework combines these graph representations with light-weight (<4B) local LLMs, uniting topological features with semantic reasoning while avoiding the cost and privacy concerns of large cloud models. Evaluated on Java vulnerability detection (binary classification), our method achieves 93.57% accuracy-an 8.36% gain over Graph Attention Network-based embeddings and 17.81% over pretrained LLM baselines such as Qwen2.5 Coder 3B. Beyond accuracy, the approach extracts salient subgraphs and generates natural language explanations, improving interpretability for developers. These results pave the way for scalable, explainable, and locally deployable tools that can shift vulnerability analysis from purely syntactic checks to deeper structural and semantic insights, facilitating broader adoption in real-world secure software development.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "13 pages, 3 figures, 5 tables, 14 equations, accepted at the 14th International Conference on Complex Networks and Their Applications (COMPLEX NETWORKS 2025) and the conference proceedings will be published by Springer in the Studies in Computational Intelligence series",
    "pdf_url": "https://arxiv.org/pdf/2510.10321v1",
    "published_date": "2025-10-11 19:32:00 UTC",
    "updated_date": "2025-10-11 19:32:00 UTC"
  },
  {
    "arxiv_id": "2510.10320v1",
    "title": "Prepared for the Unknown: Adapting AIOps Capacity Forecasting Models to Data Changes",
    "authors": [
      "Lorena Poenaru-Olaru",
      "Wouter van 't Hof",
      "Adrian Stando",
      "Arkadiusz P. Trawinski",
      "Eileen Kapel",
      "Jan S. Rellermeyer",
      "Luis Cruz",
      "Arie van Deursen"
    ],
    "abstract": "Capacity management is critical for software organizations to allocate resources effectively and meet operational demands. An important step in capacity management is predicting future resource needs often relies on data-driven analytics and machine learning (ML) forecasting models, which require frequent retraining to stay relevant as data evolves. Continuously retraining the forecasting models can be expensive and difficult to scale, posing a challenge for engineering teams tasked with balancing accuracy and efficiency. Retraining only when the data changes appears to be a more computationally efficient alternative, but its impact on accuracy requires further investigation. In this work, we investigate the effects of retraining capacity forecasting models for time series based on detected changes in the data compared to periodic retraining. Our results show that drift-based retraining achieves comparable forecasting accuracy to periodic retraining in most cases, making it a cost-effective strategy. However, in cases where data is changing rapidly, periodic retraining is still preferred to maximize the forecasting accuracy. These findings offer actionable insights for software teams to enhance forecasting systems, reducing retraining overhead while maintaining robust performance.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10320v1",
    "published_date": "2025-10-11 19:21:20 UTC",
    "updated_date": "2025-10-11 19:21:20 UTC"
  },
  {
    "arxiv_id": "2510.10304v2",
    "title": "Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting",
    "authors": [
      "Michael Y. Hu",
      "Benjamin Van Durme",
      "Jacob Andreas",
      "Harsh Jhamtani"
    ],
    "abstract": "Language model (LM) agents deployed in novel environments often exhibit poor sample efficiency when learning from sequential interactions. This significantly hinders the usefulness of such agents in environments where interaction is costly (for example, when they interact with humans or reset physical systems). While a number of existing LM agent architectures incorporate various mechanisms for experience storage and reflection, they make limited use of LMs' abilities to directly generate or reason about full counterfactual trajectories. We introduce ECHO (Experience Consolidation via Hindsight Optimization), a prompting framework that adapts hindsight experience replay from reinforcement learning for language model agents. ECHO generates optimized trajectories for alternative goals that could have been achieved during failed attempts, effectively creating synthetic positive examples from unsuccessful interactions. Our approach consists of two components: a hindsight rule that uses the language model itself to identify relevant subgoals and generate optimized trajectories, and an update rule that maintains compressed trajectory representations in memory. We evaluate ECHO on stateful versions of XMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a collaborative information-gathering enterprise simulation. Across both domains, ECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid, it also outperforms a number of sophisticated agent architectures including Reflexion and AWM, demonstrating faster adaptation to novel environments through more effective utilization of past experiences.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10304v2",
    "published_date": "2025-10-11 18:11:09 UTC",
    "updated_date": "2026-01-02 19:54:40 UTC"
  },
  {
    "arxiv_id": "2510.10300v3",
    "title": "The Algorithmic Regulator",
    "authors": [
      "Giulio Ruffini"
    ],
    "abstract": "The regulator theorem states that, under certain conditions, any optimal controller must embody a model of the system it regulates, grounding the idea that controllers embed, explicitly or implicitly, internal models of the controlled. This principle underpins neuroscience and predictive brain theories like the Free-Energy Principle or Kolmogorov/Algorithmic Agent theory. However, the theorem is only proven in limited settings. Here, we treat the deterministic, closed, coupled world-regulator system $(W,R)$ as a single self-delimiting program $p$ via a constant-size wrapper that produces the world output string~$x$ fed to the regulator. We analyze regulation from the viewpoint of the algorithmic complexity of the output, $K(x)$. We define $R$ to be a \\emph{good algorithmic regulator} if it \\emph{reduces} the algorithmic complexity of the readout relative to a null (unregulated) baseline $\\varnothing$, i.e., \\[ Δ= K\\big(O_{W,\\varnothing}\\big) - K\\big(O_{W,R}\\big) > 0. \\] We then prove that the larger $Δ$ is, the more world-regulator pairs with high mutual algorithmic information are favored. More precisely, a complexity gap $Δ> 0$ yields \\[ \\Pr\\big((W,R)\\mid x\\big) \\le C\\,2^{\\,M(W{:}R)}\\,2^{-Δ}, \\] making low $M(W{:}R)$ exponentially unlikely as $Δ$ grows. This is an AIT version of the idea that ``the regulator contains a model of the world.'' The framework is distribution-free, applies to individual sequences, and complements the Internal Model Principle. Beyond this necessity claim, the same coding-theorem calculus singles out a \\emph{canonical scalar objective} and implicates a \\emph{planner}. On the realized episode, a regulator behaves \\emph{as if} it minimized the conditional description length of the readout.",
    "categories": [
      "cs.CC",
      "cs.AI",
      "cs.IT",
      "eess.SY",
      "q-bio.NC"
    ],
    "primary_category": "cs.CC",
    "comment": "2 Figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10300v3",
    "published_date": "2025-10-11 17:54:08 UTC",
    "updated_date": "2025-10-15 10:23:52 UTC"
  },
  {
    "arxiv_id": "2510.15961v1",
    "title": "Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use",
    "authors": [
      "Yiyang Li",
      "Zehong Wang",
      "Zhengqing Yuan",
      "Zheyuan Zhang",
      "Keerthiram Murugesan",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "Illicit drug use among teenagers and young adults (TYAs) remains a pressing public health concern, with rising prevalence and long-term impacts on health and well-being. To detect illicit drug use among TYAs, researchers analyze large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the National Survey on Drug Use and Health (NSDUH), which preserve rich demographic, psychological, and environmental factors related to substance use. However, existing modeling methods treat survey variables independently, overlooking latent and interconnected structures among them. To address this limitation, we propose LAMI (LAtent relation Mining with bi-modal Interpretability), a novel joint graph-language modeling framework for detecting illicit drug use and interpreting behavioral risk factors among TYAs. LAMI represents individual responses as relational graphs, learns latent connections through a specialized graph structure learning layer, and integrates a large language model to generate natural language explanations grounded in both graph structures and survey semantics. Experiments on the YRBS and NSDUH datasets show that LAMI outperforms competitive baselines in predictive accuracy. Interpretability analyses further demonstrate that LAMI reveals meaningful behavioral substructures and psychosocial pathways, such as family dynamics, peer influence, and school-related distress, that align with established risk factors for substance use.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15961v1",
    "published_date": "2025-10-11 17:29:50 UTC",
    "updated_date": "2025-10-11 17:29:50 UTC"
  },
  {
    "arxiv_id": "2510.10293v1",
    "title": "MatryoshkaThinking: Recursive Test-Time Scaling Enables Efficient Reasoning",
    "authors": [
      "Hongwei Chen",
      "Yishu Lei",
      "Dan Zhang",
      "Bo Ke",
      "Danxiang Zhu",
      "Xuyi Chen",
      "Yuxiang Lu",
      "Zhengjie Huang",
      "Shikun Feng",
      "Jingzhou He",
      "Yu Sun",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "abstract": "Test-time scaling has emerged as a promising paradigm in language modeling, wherein additional computational resources are allocated during inference to enhance model performance. Recent approaches, such as DeepConf, have demonstrated the efficacy of this strategy, however, they often incur substantial computational overhead to achieve competitive results. In this work, we propose MatryoshkaThinking, a novel method that significantly reduces computational cost while maintaining state-of-the-art performance. Specifically, MatryoshkaThinking attains a score of 99.79 on AIME2025 using only 4% of the computation required by DeepConf. The core of our approach lies in the recursive exploitation of the model's intrinsic capabilities in reasoning, verification, and summarization, which collectively enhance the retention of correct solutions and reduce the disparity between Pass@k and Pass@1. Comprehensive evaluations across multiple open-source models and challenging multi-modal reasoning benchmarks validate the effectiveness and generality of our method. These findings offer new insights into the design of efficient and scalable test-time inference strategies for advanced language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10293v1",
    "published_date": "2025-10-11 17:18:12 UTC",
    "updated_date": "2025-10-11 17:18:12 UTC"
  },
  {
    "arxiv_id": "2510.13843v1",
    "title": "Serialized EHR make for good text representations",
    "authors": [
      "Zhirong Chou",
      "Quan Qin",
      "Shi Li"
    ],
    "abstract": "The emergence of foundation models in healthcare has opened new avenues for learning generalizable representations from large scale clinical data. Yet, existing approaches often struggle to reconcile the tabular and event based nature of Electronic Health Records (EHRs) with the sequential priors of natural language models. This structural mismatch limits their ability to capture longitudinal dependencies across patient encounters. We introduce SerialBEHRT, a domain aligned foundation model that extends SciBERT through additional pretraining on structured EHR sequences. SerialBEHRT is designed to encode temporal and contextual relationships among clinical events, thereby producing richer patient representations. We evaluate its effectiveness on the task of antibiotic susceptibility prediction, a clinically meaningful problem in antibiotic stewardship. Through extensive benchmarking against state of the art EHR representation strategies, we demonstrate that SerialBEHRT achieves superior and more consistent performance, highlighting the importance of temporal serialization in foundation model pretraining for healthcare.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13843v1",
    "published_date": "2025-10-11 17:16:15 UTC",
    "updated_date": "2025-10-11 17:16:15 UTC"
  },
  {
    "arxiv_id": "2510.10292v1",
    "title": "From Programs to Poses: Factored Real-World Scene Generation via Learned Program Libraries",
    "authors": [
      "Joy Hsu",
      "Emily Jin",
      "Jiajun Wu",
      "Niloy J. Mitra"
    ],
    "abstract": "Real-world scenes, such as those in ScanNet, are difficult to capture, with highly limited data available. Generating realistic scenes with varied object poses remains an open and challenging task. In this work, we propose FactoredScenes, a framework that synthesizes realistic 3D scenes by leveraging the underlying structure of rooms while learning the variation of object poses from lived-in scenes. We introduce a factored representation that decomposes scenes into hierarchically organized concepts of room programs and object poses. To encode structure, FactoredScenes learns a library of functions capturing reusable layout patterns from which scenes are drawn, then uses large language models to generate high-level programs, regularized by the learned library. To represent scene variations, FactoredScenes learns a program-conditioned model to hierarchically predict object poses, and retrieves and places 3D objects in a scene. We show that FactoredScenes generates realistic, real-world rooms that are difficult to distinguish from real ScanNet scenes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.10292v1",
    "published_date": "2025-10-11 17:14:24 UTC",
    "updated_date": "2025-10-11 17:14:24 UTC"
  },
  {
    "arxiv_id": "2510.10285v1",
    "title": "Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control",
    "authors": [
      "Haolang Lu",
      "Bolun Chu",
      "WeiYe Fu",
      "Guoshun Nan",
      "Junning Liu",
      "Minghui Pan",
      "Qiankun Li",
      "Yi Yu",
      "Hua Wang",
      "Kun Wang"
    ],
    "abstract": "Multimodal large reasoning models (MLRMs) are rapidly advancing vision-language reasoning and are emerging as a foundation for cross-modal intelligence. Hallucination remains a persistent failure mode, manifesting itself as erroneous reasoning chains and misinterpretation of visual content. In this study, we observe that attention heads exhibit a staged division: shallow heads predominantly serve perception, while deeper heads shift toward symbolic reasoning, revealing two major causes of hallucination, namely perceptual bias and reasoning drift. To address these issues, we propose a lightweight and interpretable two-step plugin, Functional Head Identification and Class-conditioned Rescaling, which locates perception- and reasoning-oriented heads and regulates their contributions without retraining. Evaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six benchmarks across three domains, and four baselines show that our plugin achieves an average improvement of 5% and up to 15%, with only <1% additional computation and 9% of baseline latency. Our approach is completely model-agnostic and significantly enhances both the reliability and interpretability of the off-the-shelf MLRMs, thereby enabling their safe deployment in high-stakes applications. Our code is available at https://anonymous.4open.science/r/Functional-Attention-Control.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.10285v1",
    "published_date": "2025-10-11 16:54:41 UTC",
    "updated_date": "2025-10-11 16:54:41 UTC"
  },
  {
    "arxiv_id": "2510.10281v1",
    "title": "ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test",
    "authors": [
      "Guan-Yan Yang",
      "Tzu-Yu Cheng",
      "Ya-Wen Teng",
      "Farn Wanga",
      "Kuo-Hui Yeh"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into computer applications has introduced transformative capabilities but also significant security challenges. Existing safety alignments, which primarily focus on semantic interpretation, leave LLMs vulnerable to attacks that use non-standard data representations. This paper introduces ArtPerception, a novel black-box jailbreak framework that strategically leverages ASCII art to bypass the security measures of state-of-the-art (SOTA) LLMs. Unlike prior methods that rely on iterative, brute-force attacks, ArtPerception introduces a systematic, two-phase methodology. Phase 1 conducts a one-time, model-specific pre-test to empirically determine the optimal parameters for ASCII art recognition. Phase 2 leverages these insights to launch a highly efficient, one-shot malicious jailbreak attack. We propose a Modified Levenshtein Distance (MLD) metric for a more nuanced evaluation of an LLM's recognition capability. Through comprehensive experiments on four SOTA open-source LLMs, we demonstrate superior jailbreak performance. We further validate our framework's real-world relevance by showing its successful transferability to leading commercial models, including GPT-4o, Claude Sonnet 3.7, and DeepSeek-V3, and by conducting a rigorous effectiveness analysis against potential defenses such as LLaMA Guard and Azure's content filters. Our findings underscore that true LLM security requires defending against a multi-modal space of interpretations, even within text-only inputs, and highlight the effectiveness of strategic, reconnaissance-based attacks. Content Warning: This paper includes potentially harmful and offensive model outputs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "30 pages, 22 figures. This preprint has been accepted for publication in Elsevier JOURNAL OF NETWORK AND COMPUTER APPLICATIONS (JNCA)",
    "pdf_url": "https://arxiv.org/pdf/2510.10281v1",
    "published_date": "2025-10-11 16:28:37 UTC",
    "updated_date": "2025-10-11 16:28:37 UTC"
  },
  {
    "arxiv_id": "2510.10278v1",
    "title": "Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models",
    "authors": [
      "Christopher Chiu",
      "Silviu Pitis",
      "Mihaela van der Schaar"
    ],
    "abstract": "Clinical reasoning in medicine is a hypothesis-driven process where physicians refine diagnoses from limited information through targeted history, physical examination, and diagnostic investigations. In contrast, current medical benchmarks for large language models (LLMs) primarily assess knowledge recall through single-turn questions, where complete clinical information is provided upfront. To address this gap, we introduce VivaBench, a multi-turn benchmark that evaluates sequential clinical reasoning in LLM agents. Our dataset consists of 1762 physician-curated clinical vignettes structured as interactive scenarios that simulate a (oral) examination in medical training, requiring agents to actively probe for relevant findings, select appropriate investigations, and synthesize information across multiple steps to reach a diagnosis. While current LLMs demonstrate competence in diagnosing conditions from well-described clinical presentations, their performance degrades significantly when required to navigate iterative diagnostic reasoning under uncertainty in our evaluation. Our analysis identified several failure modes that mirror common cognitive errors in clinical practice, including: (1) fixation on initial hypotheses, (2) inappropriate investigation ordering, (3) premature diagnostic closure, and (4) failing to screen for critical conditions. These patterns reveal fundamental limitations in how current LLMs reason and make decisions under uncertainty. Through VivaBench, we provide a standardized benchmark for evaluating conversational medical AI systems for real-world clinical decision support. Beyond medical applications, we contribute to the larger corpus of research on agentic AI by demonstrating how sequential reasoning trajectories can diverge in complex decision-making environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10278v1",
    "published_date": "2025-10-11 16:24:35 UTC",
    "updated_date": "2025-10-11 16:24:35 UTC"
  },
  {
    "arxiv_id": "2510.10274v1",
    "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model",
    "authors": [
      "Jinliang Zheng",
      "Jianxiong Li",
      "Zhihao Wang",
      "Dongxiu Liu",
      "Xirui Kang",
      "Yuchun Feng",
      "Yinan Zheng",
      "Jiayin Zou",
      "Yilun Chen",
      "Jia Zeng",
      "Ya-Qin Zhang",
      "Jiangmiao Pang",
      "Jingjing Liu",
      "Tai Wang",
      "Xianyuan Zhan"
    ],
    "abstract": "Successful generalist Vision-Language-Action (VLA) models rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders, enjoying both scalability and simplicity. Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep of benchmarks, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks. Website: https://thu-air-dream.github.io/X-VLA/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "preprint, technical report, 33 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.10274v1",
    "published_date": "2025-10-11 16:20:17 UTC",
    "updated_date": "2025-10-11 16:20:17 UTC"
  },
  {
    "arxiv_id": "2510.10271v1",
    "title": "MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation",
    "authors": [
      "Wentian Zhu",
      "Zhen Xiang",
      "Wei Niu",
      "Le Guan"
    ],
    "abstract": "Unlike regular tokens derived from existing text corpora, special tokens are artificially created to annotate structured conversations during the fine-tuning process of Large Language Models (LLMs). Serving as metadata of training data, these tokens play a crucial role in instructing LLMs to generate coherent and context-aware responses. We demonstrate that special tokens can be exploited to construct four attack primitives, with which malicious users can reliably bypass the internal safety alignment of online LLM services and circumvent state-of-the-art (SOTA) external content moderation systems simultaneously. Moreover, we found that addressing this threat is challenging, as aggressive defense mechanisms-such as input sanitization by removing special tokens entirely, as suggested in academia-are less effective than anticipated. This is because such defense can be evaded when the special tokens are replaced by regular ones with high semantic similarity within the tokenizer's embedding space. We systemically evaluated our method, named MetaBreak, on both lab environment and commercial LLM platforms. Our approach achieves jailbreak rates comparable to SOTA prompt-engineering-based solutions when no content moderation is deployed. However, when there is content moderation, MetaBreak outperforms SOTA solutions PAP and GPTFuzzer by 11.6% and 34.8%, respectively. Finally, since MetaBreak employs a fundamentally different strategy from prompt engineering, the two approaches can work synergistically. Notably, empowering MetaBreak on PAP and GPTFuzzer boosts jailbreak rates by 24.3% and 20.2%, respectively.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10271v1",
    "published_date": "2025-10-11 16:14:56 UTC",
    "updated_date": "2025-10-11 16:14:56 UTC"
  },
  {
    "arxiv_id": "2510.10263v1",
    "title": "Unveiling Gamer Archetypes through Multi modal feature Correlations and Unsupervised Learning",
    "authors": [
      "Moona Kanwal",
      "Muhammad Sami Siddiqui",
      "Syed Anael Ali"
    ],
    "abstract": "Profiling gamers provides critical insights for adaptive game design, behavioral understanding, and digital well-being. This study proposes an integrated, data-driven framework that combines psychological measures, behavioral analytics, and machine learning to reveal underlying gamer personas. A structured survey of 250 participants, including 113 active gamers, captured multidimensional behavioral, motivational, and social data. The analysis pipeline integrated feature engineering, association-network, knowledge-graph analysis, and unsupervised clustering to extract meaningful patterns. Correlation statistics uses Cramers V, Tschuprows T, Theils U, and Spearmans quantified feature associations, and network centrality guided feature selection. Dimensionality-reduction techniques such as PCA, SVD, t-SNE are coupled with clustering algorithms like K-Means, Agglomerative, Spectral, DBSCAN, evaluated using Silhouette, Calinski Harabasz, and Davies Bouldin indices. The PCA with K-Means with k = 4 model achieved optimal cluster quality with Silhouette = 0.4, identifying four archetypes as Immersive Social Story-Seekers, Disciplined Optimizers, Strategic Systems Navigators, and Competitive Team-Builders. This research contributes a reproducible pipeline that links correlation-driven network insights with unsupervised learning. The integration of behavioral correlation networks with clustering not only enhances classification accuracy but also offers a holistic lens to connect gameplay motivations with psychological and wellness outcomes.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Submitted to Peer Review Journal",
    "pdf_url": "https://arxiv.org/pdf/2510.10263v1",
    "published_date": "2025-10-11 15:46:44 UTC",
    "updated_date": "2025-10-11 15:46:44 UTC"
  },
  {
    "arxiv_id": "2510.15959v1",
    "title": "Exploring the Potential of Citiverses for Regulatory Learning",
    "authors": [
      "Isabelle Hupont",
      "Marisa Ponti",
      "Sven Schade"
    ],
    "abstract": "Citiverses hold the potential to support regulatory learning by offering immersive, virtual environments for experimenting with policy scenarios and technologies. This paper proposes a science-for-policy agenda to explore the potential of citiverses as experimentation spaces for regulatory learning, grounded in a consultation with a high-level panel of experts, including policymakers from the European Commission, national government science advisers and leading researchers in digital regulation and virtual worlds. It identifies key research areas, including scalability, real-time feedback, complexity modelling, cross-border collaboration, risk reduction, citizen participation, ethical considerations and the integration of emerging technologies. In addition, the paper analyses a set of experimental topics, spanning transportation, urban planning and the environment/climate crisis, that could be tested in citiverse platforms to advance regulatory learning in these areas. The proposed work is designed to inform future research for policy and emphasizes a responsible approach to developing and using citiverses. It prioritizes careful consideration of the ethical, economic, ecological and social dimensions of different regulations. The paper also explores essential preliminary steps necessary for integrating citiverses into the broader ecosystems of experimentation spaces, including test beds, living labs and regulatory sandboxes",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.15959v1",
    "published_date": "2025-10-11 15:37:17 UTC",
    "updated_date": "2025-10-11 15:37:17 UTC"
  },
  {
    "arxiv_id": "2510.10252v2",
    "title": "Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models",
    "authors": [
      "Samir Abdaljalil",
      "Erchin Serpedin",
      "Khalid Qaraqe",
      "Hasan Kurban"
    ],
    "abstract": "Large language models (LLMs) often generate reasoning traces that appear coherent but rest on unsupported assumptions, leading to hallucinated conclusions. Prior work mainly addresses factual hallucinations or relies on post-hoc verification, leaving reasoning-induced hallucinations largely unaddressed. We propose Audit-of-Understanding (AoU), a framework that constrains inference to validated premises through three phases: (1) decomposing a query into candidate assumptions, (2) auditing their support, and (3) conditioning inference only on the validated subset. Formally, AoU is \\emph{posterior-constrained inference}, connecting to selective prediction and rejection learning. Our contributions are threefold: (i) theoretical guarantees under perfect validation, (ii) excess-risk bounds under imperfect audits, and (iii) tractability analysis. Empirically, AoU improves both accuracy and faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at https://anonymous.4open.science/r/audit-of-understanding-E28B.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10252v2",
    "published_date": "2025-10-11 15:10:28 UTC",
    "updated_date": "2025-10-18 10:20:04 UTC"
  },
  {
    "arxiv_id": "2510.10250v1",
    "title": "MRI Brain Tumor Detection with Computer Vision",
    "authors": [
      "Jack Krolik",
      "Jake Lynn",
      "John Henry Rudden",
      "Dmytro Vremenko"
    ],
    "abstract": "This study explores the application of deep learning techniques in the automated detection and segmentation of brain tumors from MRI scans. We employ several machine learning models, including basic logistic regression, Convolutional Neural Networks (CNNs), and Residual Networks (ResNet) to classify brain tumors effectively. Additionally, we investigate the use of U-Net for semantic segmentation and EfficientDet for anchor-based object detection to enhance the localization and identification of tumors. Our results demonstrate promising improvements in the accuracy and efficiency of brain tumor diagnostics, underscoring the potential of deep learning in medical imaging and its significance in improving clinical outcomes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 8 figures, final project report for CS4100 (Machine Learning), Northeastern University, April 2024",
    "pdf_url": "https://arxiv.org/pdf/2510.10250v1",
    "published_date": "2025-10-11 15:07:52 UTC",
    "updated_date": "2025-10-11 15:07:52 UTC"
  },
  {
    "arxiv_id": "2510.10248v2",
    "title": "Reasoning-Enhanced Large Language Models for Molecular Property Prediction",
    "authors": [
      "Jiaxi Zhuang",
      "Yaorui Shi",
      "Jue Hou",
      "Yunong He",
      "Mingwei Ye",
      "Mingjun Xu",
      "Yuming Su",
      "Linfeng Zhang",
      "Ying Qian",
      "Linfeng Zhang",
      "Guolin Ke",
      "Hengxing Cai"
    ],
    "abstract": "Molecular property prediction is crucial for drug discovery and materials science, yet existing approaches suffer from limited interpretability, poor cross-task generalization, and lack of chemical reasoning capabilities. Traditional machine learning models struggle with task transferability, while specialized molecular language models provide little insight into their decision-making processes. To address these limitations, we propose \\textbf{MPPReasoner}, a multimodal large language model that incorporates chemical reasoning for molecular property prediction. Our approach, built upon Qwen2.5-VL-7B-Instruct, integrates molecular images with SMILES strings to enable comprehensive molecular understanding. We develop a two-stage training strategy: supervised fine-tuning (SFT) using 16,000 high-quality reasoning trajectories generated through expert knowledge and multiple teacher models, followed by Reinforcement Learning from Principle-Guided Rewards (RLPGR). RLPGR employs verifiable, rule-based rewards that systematically evaluate chemical principle application, molecular structure analysis, and logical consistency through computational verification. Extensive experiments across 8 datasets demonstrate significant performance improvements, with MPPReasoner outperforming the best baselines by 7.91\\% and 4.53\\% on in-distribution and out-of-distribution tasks respectively. MPPReasoner exhibits exceptional cross-task generalization and generates chemically sound reasoning paths that provide valuable insights into molecular property analysis, substantially enhancing both interpretability and practical utility for chemists. Code is available at https://anonymous.4open.science/r/MPPReasoner-12687.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10248v2",
    "published_date": "2025-10-11 15:05:45 UTC",
    "updated_date": "2025-10-17 13:23:43 UTC"
  },
  {
    "arxiv_id": "2510.13842v1",
    "title": "ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking",
    "authors": [
      "Yutao Wu",
      "Xiao Liu",
      "Yinghui Li",
      "Yifeng Gao",
      "Yifan Ding",
      "Jiale Ding",
      "Xiang Zheng",
      "Xingjun Ma"
    ],
    "abstract": "Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation (RAG) systems by injecting adversarial content into knowledge bases, tricking Large Language Models (LLMs) into producing attacker-controlled outputs grounded in manipulated context. Prior work highlights LLMs' susceptibility to misleading or malicious retrieved content. However, real-world fact-checking scenarios are more challenging, as credible evidence typically dominates the retrieval pool. To investigate this problem, we extend knowledge poisoning to the fact-checking setting, where retrieved context includes authentic supporting or refuting evidence. We propose \\textbf{ADMIT} (\\textbf{AD}versarial \\textbf{M}ulti-\\textbf{I}njection \\textbf{T}echnique), a few-shot, semantically aligned poisoning attack that flips fact-checking decisions and induces deceptive justifications, all without access to the target LLMs, retrievers, or token-level control. Extensive experiments show that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4 cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\\% at an extremely low poisoning rate of $0.93 \\times 10^{-6}$, and remaining robust even in the presence of strong counter-evidence. Compared with prior state-of-the-art attacks, ADMIT improves ASR by 11.2\\% across all settings, exposing significant vulnerabilities in real-world RAG-based fact-checking systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13842v1",
    "published_date": "2025-10-11 14:50:40 UTC",
    "updated_date": "2025-10-11 14:50:40 UTC"
  },
  {
    "arxiv_id": "2510.10238v1",
    "title": "The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities",
    "authors": [
      "Zixuan Qin",
      "Kunlin Lyu",
      "Qingchen Yu",
      "Yifan Sun",
      "Zhaoxin Fan"
    ],
    "abstract": "Large Language Models (LLMs) have become foundational tools in natural language processing, powering a wide range of applications and research. Many studies have shown that LLMs share significant similarities with the human brain. Recent neuroscience research has found that a small subset of biological neurons in the human brain are crucial for core cognitive functions, which raises a fundamental question: do LLMs also contain a small subset of critical neurons? In this paper, we investigate this question by proposing a Perturbation-based Causal Identification of Critical Neurons method to systematically locate such critical neurons in LLMs. Our findings reveal three key insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting these critical neurons can cause a 72B-parameter model with over 1.1 billion neurons to completely collapse, with perplexity increasing by up to 20 orders of magnitude; (2) These critical neurons are not uniformly distributed, but tend to concentrate in the outer layers, particularly within the MLP down\\_proj components; (3) Performance degradation exhibits sharp phase transitions, rather than a gradual decline, when these critical neurons are disrupted. Through comprehensive experiments across diverse model architectures and scales, we provide deeper analysis of these phenomena and their implications for LLM robustness and interpretability. These findings can offer guidance for developing more robust model architectures and improving deployment security in safety-critical applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10238v1",
    "published_date": "2025-10-11 14:39:09 UTC",
    "updated_date": "2025-10-11 14:39:09 UTC"
  },
  {
    "arxiv_id": "2510.10232v1",
    "title": "SGM: A Statistical Godel Machine for Risk-Controlled Recursive Self-Modification",
    "authors": [
      "Xuening Wu",
      "Shenqin Yin",
      "Yanlan Kang",
      "Xinhang Zhang",
      "Qianya Xu",
      "Zeping Chen",
      "Wenqiang Zhang"
    ],
    "abstract": "Recursive self-modification is increasingly central in AutoML, neural architecture search, and adaptive optimization, yet no existing framework ensures that such changes are made safely. Godel machines offer a principled safeguard by requiring formal proofs of improvement before rewriting code; however, such proofs are unattainable in stochastic, high-dimensional settings. We introduce the Statistical Godel Machine (SGM), the first statistical safety layer for recursive edits. SGM replaces proof-based requirements with statistical confidence tests (e-values, Hoeffding bounds), admitting a modification only when superiority is certified at a chosen confidence level, while allocating a global error budget to bound cumulative risk across rounds.We also propose Confirm-Triggered Harmonic Spending (CTHS), which indexes spending by confirmation events rather than rounds, concentrating the error budget on promising edits while preserving familywise validity.Experiments across supervised learning, reinforcement learning, and black-box optimization validate this role: SGM certifies genuine gains on CIFAR-100, rejects spurious improvement on ImageNet-100, and demonstrates robustness on RL and optimization benchmarks.Together, these results position SGM as foundational infrastructure for continual, risk-aware self-modification in learning systems.Code is available at: https://github.com/gravitywavelet/sgm-anon.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10232v1",
    "published_date": "2025-10-11 14:09:37 UTC",
    "updated_date": "2025-10-11 14:09:37 UTC"
  },
  {
    "arxiv_id": "2510.10223v1",
    "title": "You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs",
    "authors": [
      "Yijie Xu",
      "Huizai Yao",
      "Zhiyu Guo",
      "Weiyu Guo",
      "Pengteng Li",
      "Aiwei Liu",
      "Xuming Hu",
      "Hui Xiong"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed in specialized domains such as finance, medicine, and agriculture, where they face significant distribution shifts from their training data. Domain-specific fine-tuning can mitigate this challenge but relies on high-quality labeled data that is expensive and slow to collect in expertise-limited settings. We study label-free test-time adaptation for language models and present SyTTA, an inference-time framework that adapts models on-the-fly without additional supervision. SyTTA couples two complementary uncertainty signals that arise under distribution shift: input-side perplexity, indicating mismatch with domain-specific terminology and patterns, and output-side predictive entropy, indicating diffuse and unstable token probabilities during generation. Across diverse model architectures and domain-specific benchmarks, SyTTA delivers consistent gains. Notably, on agricultural question answering, SyTTA improves Rouge-LSum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query. These results show that effective test-time adaptation for language models is achievable without labeled examples, supporting deployment in label-scarce domains. The code will be made available upon acceptance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.10223v1",
    "published_date": "2025-10-11 14:00:39 UTC",
    "updated_date": "2025-10-11 14:00:39 UTC"
  },
  {
    "arxiv_id": "2510.10221v1",
    "title": "A3RNN: Bi-directional Fusion of Bottom-up and Top-down Process for Developmental Visual Attention in Robots",
    "authors": [
      "Hyogo Hiruma",
      "Hiroshi Ito",
      "Hiroki Mori",
      "Tetsuya Ogata"
    ],
    "abstract": "This study investigates the developmental interaction between top-down (TD) and bottom-up (BU) visual attention in robotic learning. Our goal is to understand how structured, human-like attentional behavior emerges through the mutual adaptation of TD and BU mechanisms over time. To this end, we propose a novel attention model $A^3 RNN$ that integrates predictive TD signals and saliency-based BU cues through a bi-directional attention architecture.\n  We evaluate our model in robotic manipulation tasks using imitation learning. Experimental results show that attention behaviors evolve throughout training, from saliency-driven exploration to prediction-driven direction. Initially, BU attention highlights visually salient regions, which guide TD processes, while as learning progresses, TD attention stabilizes and begins to reshape what is perceived as salient. This trajectory reflects principles from cognitive science and the free-energy framework, suggesting the importance of self-organizing attention through interaction between perception and internal prediction. Although not explicitly optimized for stability, our model exhibits more coherent and interpretable attention patterns than baselines, supporting the idea that developmental mechanisms contribute to robust attention formation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10221v1",
    "published_date": "2025-10-11 13:58:08 UTC",
    "updated_date": "2025-10-11 13:58:08 UTC"
  },
  {
    "arxiv_id": "2510.10217v1",
    "title": "UF-RNN: Real-Time Adaptive Motion Generation Using Uncertainty-Driven Foresight Prediction",
    "authors": [
      "Hyogo Hiruma",
      "Hiroshi Ito",
      "Tetsuya Ogata"
    ],
    "abstract": "Training robots to operate effectively in environments with uncertain states, such as ambiguous object properties or unpredictable interactions, remains a longstanding challenge in robotics. Imitation learning methods typically rely on successful examples and often neglect failure scenarios where uncertainty is most pronounced. To address this limitation, we propose the Uncertainty-driven Foresight Recurrent Neural Network (UF-RNN), a model that combines standard time-series prediction with an active \"Foresight\" module. This module performs internal simulations of multiple future trajectories and refines the hidden state to minimize predicted variance, enabling the model to selectively explore actions under high uncertainty. We evaluate UF-RNN on a door-opening task in both simulation and a real-robot setting, demonstrating that, despite the absence of explicit failure demonstrations, the model exhibits robust adaptation by leveraging self-induced chaotic dynamics in its latent space. When guided by the Foresight module, these chaotic properties stimulate exploratory behaviors precisely when the environment is ambiguous, yielding improved success rates compared to conventional stochastic RNN baselines. These findings suggest that integrating uncertainty-driven foresight into imitation learning pipelines can significantly enhance a robot's ability to handle unpredictable real-world conditions.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10217v1",
    "published_date": "2025-10-11 13:44:20 UTC",
    "updated_date": "2025-10-11 13:44:20 UTC"
  },
  {
    "arxiv_id": "2510.10216v1",
    "title": "Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis",
    "authors": [
      "Zhechong Huang",
      "Zhao Zhang",
      "Ruyi Ji",
      "Tingxuan Xia",
      "Qihao Zhu",
      "Qinxiang Cao",
      "Zeyu Sun",
      "Yingfei Xiong"
    ],
    "abstract": "Language models have shown remarkable proficiency in code generation; nevertheless, ensuring type correctness remains a challenge. Although traditional methods, such as constrained decoding, alleviate this problem by externally rejecting untypable code, the model itself does not effectively learn type reasoning internally, which ultimately limits its overall performance. This paper introduces TyFlow, a novel system that internalizes type reasoning within code generation to guide the model to learn the type system. The core of our approach is a novel type-guided program synthesis system that maintains an isomorphism between type derivation trees and synthesis derivation trees, enabling a new code representation based on synthesis decision sequences rather than traditional text-based token sequences. By offloading the complexity of type system learning to the representation itself, models can redirect their computational resources toward higher-level program semantics. Our evaluation shows that TyFlow not only eliminates type errors but also significantly improves functional correctness, highlighting the importance of aligning LMs with type systems internally.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10216v1",
    "published_date": "2025-10-11 13:43:36 UTC",
    "updated_date": "2025-10-11 13:43:36 UTC"
  },
  {
    "arxiv_id": "2510.10214v1",
    "title": "Distributionally Robust Control with End-to-End Statistically Guaranteed Metric Learning",
    "authors": [
      "Jingyi Wu",
      "Chao Ning",
      "Yang Shi"
    ],
    "abstract": "Wasserstein distributionally robust control (DRC) recently emerges as a principled paradigm for handling uncertainty in stochastic dynamical systems. However, it constructs data-driven ambiguity sets via uniform distribution shifts before sequentially incorporating them into downstream control synthesis. This segregation between ambiguity set construction and control objectives inherently introduces a structural misalignment, which undesirably leads to conservative control policies with sub-optimal performance. To address this limitation, we propose a novel end-to-end finite-horizon Wasserstein DRC framework that integrates the learning of anisotropic Wasserstein metrics with downstream control tasks in a closed-loop manner, thus enabling ambiguity sets to be systematically adjusted along performance-critical directions and yielding more effective control policies. This framework is formulated as a bilevel program: the inner level characterizes dynamical system evolution under DRC, while the outer level refines the anisotropic metric leveraging control-performance feedback across a range of initial conditions. To solve this program efficiently, we develop a stochastic augmented Lagrangian algorithm tailored to the bilevel structure. Theoretically, we prove that the learned ambiguity sets preserve statistical finite-sample guarantees under a novel radius adjustment mechanism, and we establish the well-posedness of the bilevel formulation by demonstrating its continuity with respect to the learnable metric. Furthermore, we show that the algorithm converges to stationary points of the outer level problem, which are statistically consistent with the optimal metric at a non-asymptotic convergence rate. Experiments on both numerical and inventory control tasks verify that the proposed framework achieves superior closed-loop performance and robustness compared against state-of-the-art methods.",
    "categories": [
      "math.OC",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10214v1",
    "published_date": "2025-10-11 13:40:49 UTC",
    "updated_date": "2025-10-11 13:40:49 UTC"
  },
  {
    "arxiv_id": "2510.10207v2",
    "title": "Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning",
    "authors": [
      "Yujian Zhang",
      "Keyu Chen",
      "Zhifeng Shen",
      "Ruizhi Qiao",
      "Xing Sun"
    ],
    "abstract": "Although Long Reasoning Models (LRMs) have achieved superior performance on various reasoning scenarios, they often suffer from increased computational costs and inference latency caused by overthinking. To address these limitations, we propose Adaptive Dual Reasoner, which supports two reasoning modes: fast thinking and slow thinking. ADR dynamically alternates between these modes based on the contextual complexity during reasoning. ADR is trained in two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to equip the model with the ability to integrate both fast and slow reasoning modes, in which we construct a hybrid reasoning dataset through a dedicated pipeline to provide large-scale supervision. (2) A reinforcement learning stage for optimizing reasoning effort, where we introduce Entropy-guided Hybrid Policy Optimization EHPO, an RL training framework employing an entropy-guided dynamic rollout strategy for branching at high-entropy units and a difficulty-aware penalty to balance fast and slow reasoning. Across challenging mathematical reasoning benchmarks, ADR achieves an effective balance between reasoning performance and efficiency among state-of-the-art approaches. Specifically, ADR yields a performance gain of up to 6.1%, while reducing the reasoning output length by 49.5% to 59.3%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NeurIPS 2025 Workshop on Efficient Reasoning",
    "pdf_url": "https://arxiv.org/pdf/2510.10207v2",
    "published_date": "2025-10-11 13:14:17 UTC",
    "updated_date": "2025-10-14 03:51:57 UTC"
  },
  {
    "arxiv_id": "2510.10205v2",
    "title": "PIXEL: Adaptive Steering Via Position-wise Injection with eXact Estimated Levels under Subspace Calibration",
    "authors": [
      "Manjiang Yu",
      "Hongji Li",
      "Priyanka Singh",
      "Xue Li",
      "Di Wang",
      "Lijie Hu"
    ],
    "abstract": "Reliable behavior control is central to deploying large language models (LLMs) on the web. Activation steering offers a tuning-free route to align attributes (e.g., truthfulness) that ensure trustworthy generation. Prevailing approaches rely on coarse heuristics and lack a principled account of where to steer and how strongly to intervene. To this end, we propose Position-wise Injection with eXact Estimated Levels (PIXEL), a position-wise activation steering framework that, in contrast to prior work, learns a property-aligned subspace from dual views (tail-averaged and end-token) and selects intervention strength via a constrained geometric objective with a closed-form solution, thereby adapting to token-level sensitivity without global hyperparameter tuning. PIXEL further performs sample-level orthogonal residual calibration to refine the global attribute direction and employs a lightweight position-scanning routine to identify receptive injection sites. We additionally provide representation-level guarantees for the minimal-intervention rule, supporting reliable alignment. Across diverse models and evaluation paradigms, PIXEL consistently improves attribute alignment while preserving model general capabilities, offering a practical and principled method for LLMs' controllable generation. Our code is available at https://github.com/V1centNevwake/PIXEL-Adaptive-Steering",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages,3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10205v2",
    "published_date": "2025-10-11 13:13:34 UTC",
    "updated_date": "2025-11-18 06:05:43 UTC"
  },
  {
    "arxiv_id": "2510.10201v1",
    "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment",
    "authors": [
      "Jinghao Zhang",
      "Naishan Zheng",
      "Ruilin Li",
      "Dongzhou Cheng",
      "Zheming Liang",
      "Feng Zhao",
      "Jiaqi Wang"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a promising framework for improving reasoning abilities in Large Language Models (LLMs). However, policy optimized with binary verification prone to overlook potential valuable exploration in reasoning trajectory. In view of heavy annotation cost of golden Process Reward Models (PRMs), recent works attempt using auxiliary signals for reward shaping of process tokens, involving entropy and likelihood collected from logit space. In this work, we offer a novel perspective on shaping RLVR with flow rewards derived from latent space, and propose RLFR, where the flow fields of model latents are constructed from either off-policy high-quality data and on-policy rejection sampling data, and the velocity deviations of policy latents within it are quantified to serve as a reward signal. RLFR first demonstrates that a well-established flow field can be a sound environment for reward signal collection, highlighting the expressive latent space is much underexplored. Moreover, RLFR is able to compress any off-policy expert data as reference for constituting reward signals, and we show that the efficient context dependence compressed within the hidden states are utilized, rather than individual token-level denotation for context comprehending. Experiments on both language and multimodal reasoning benchmarks demonstrate the reliability of flow rewards, and suggesting a promising paradigm for reward shaping with auxiliary signals.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Project Website: https://jinghaoleven.github.io/RLFR/",
    "pdf_url": "https://arxiv.org/pdf/2510.10201v1",
    "published_date": "2025-10-11 13:00:25 UTC",
    "updated_date": "2025-10-11 13:00:25 UTC"
  },
  {
    "arxiv_id": "2510.10199v1",
    "title": "Revisiting Trust in the Era of Generative AI: Factorial Structure and Latent Profiles",
    "authors": [
      "Haocan Sun",
      "Weizi Liu",
      "Di Wu",
      "Guoming Yu",
      "Mike Yao"
    ],
    "abstract": "Trust is one of the most important factors shaping whether and how people adopt and rely on artificial intelligence (AI). Yet most existing studies measure trust in terms of functionality, focusing on whether a system is reliable, accurate, or easy to use, while giving less attention to the social and emotional dimensions that are increasingly relevant for today's generative AI (GenAI) systems. These systems do not just process information; they converse, respond, and collaborate with users, blurring the line between tool and partner. In this study, we introduce and validate the Human-AI Trust Scale (HAITS), a new measure designed to capture both the rational and relational aspects of trust in GenAI. Drawing on prior trust theories, qualitative interviews, and two waves of large-scale surveys in China and the United States, we used exploratory (n = 1,546) and confirmatory (n = 1,426) factor analyses to identify four key dimensions of trust: Affective Trust, Competence Trust, Benevolence & Integrity, and Perceived Risk. We then applied latent profile analysis to classify users into six distinct trust profiles, revealing meaningful differences in how affective-competence trust and trust-distrust frameworks coexist across individuals and cultures. Our findings offer a validated, culturally sensitive tool for measuring trust in GenAI and provide new insight into how trust evolves in human-AI interaction. By integrating instrumental and relational perspectives of trust, this work lays the foundation for more nuanced research and design of trustworthy AI systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10199v1",
    "published_date": "2025-10-11 12:39:53 UTC",
    "updated_date": "2025-10-11 12:39:53 UTC"
  },
  {
    "arxiv_id": "2510.10197v1",
    "title": "Don't Just Fine-tune the Agent, Tune the Environment",
    "authors": [
      "Siyuan Lu",
      "Zechuan Wang",
      "Hongxuan Zhang",
      "Qintong Wu",
      "Leilei Gan",
      "Chenyi Zhuang",
      "Jinjie Gu",
      "Tao Lin"
    ],
    "abstract": "Large Language Model (LLM) agents show great promise for complex, multi-turn tool-use tasks, but their development is often hampered by the extreme scarcity of high-quality training data. Supervised fine-tuning (SFT) on synthetic data leads to overfitting, whereas standard reinforcement learning (RL) struggles with a critical cold-start problem and training instability. To address these challenges, we introduce $\\textbf{Environment Tuning}$, a novel training paradigm that enables agents to learn complex behaviors directly from problem instances without relying on pre-collected expert trajectories. $\\textbf{Environment Tuning}$ orchestrates this learning process through a structured curriculum, actionable environment augmentation that provides corrective feedback, and fine-grained progress rewards to ensure stable and efficient exploration. Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches. Our work presents a paradigm shift from supervised fine-tuning on static trajectories to dynamic, environment-based exploration, paving the way for training more robust and data-efficient agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10197v1",
    "published_date": "2025-10-11 12:35:15 UTC",
    "updated_date": "2025-10-11 12:35:15 UTC"
  },
  {
    "arxiv_id": "2510.10195v1",
    "title": "CauchyNet: Compact and Data-Efficient Learning using Holomorphic Activation Functions",
    "authors": [
      "Hong-Kun Zhang",
      "Xin Li",
      "Sikun Yang",
      "Zhihong Xia"
    ],
    "abstract": "A novel neural network inspired by Cauchy's integral formula, is proposed for function approximation tasks that include time series forecasting, missing data imputation, etc. Hence, the novel neural network is named CauchyNet. By embedding real-valued data into the complex plane, CauchyNet efficiently captures complex temporal dependencies, surpassing traditional real-valued models in both predictive performance and computational efficiency. Grounded in Cauchy's integral formula and supported by the universal approximation theorem, CauchyNet offers strong theoretical guarantees for function approximation. The architecture incorporates complex-valued activation functions, enabling robust learning from incomplete data while maintaining a compact parameter footprint and reducing computational overhead. Through extensive experiments in diverse domains, including transportation, energy consumption, and epidemiological data, CauchyNet consistently outperforms state-of-the-art models in predictive accuracy, often achieving a 50% lower mean absolute error with fewer parameters. These findings highlight CauchyNet's potential as an effective and efficient tool for data-driven predictive modeling, particularly in resource-constrained and data-scarce environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10195v1",
    "published_date": "2025-10-11 12:21:15 UTC",
    "updated_date": "2025-10-11 12:21:15 UTC"
  },
  {
    "arxiv_id": "2510.10193v2",
    "title": "SAFER: Risk-Constrained Sample-then-Filter in Large Language Models",
    "authors": [
      "Qingni Wang",
      "Yue Fan",
      "Xin Eric Wang"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed in risk-sensitive applications such as real-world open-ended question answering (QA), ensuring the trustworthiness of their outputs has become critical. Existing selective conformal prediction (SCP) methods provide statistical guarantees by constructing prediction sets with a constrained miscoverage rate for correct answers. However, prior works unrealistically assume that admissible answers for all instances can be obtained via finite sampling, even for open-ended QA scenarios that lack a fixed and finite solution space. To address this, we introduce a two-stage risk control framework comprising abstention-aware sampling and conformalized filtering (SAFER). Firstly, on a held-out calibration set, SAFER calibrates a sampling budget within the maximum sampling cap, using the Clopper-Pearson exact method at a user-desired risk level (i.e., the maximum allowable miscoverage rate of the sampling sets). If the risk level cannot be satisfied within the cap, we abstain; otherwise, the calibrated sampling budget becomes the minimum requirements at test time. Then, we employ calibration instances where correct answers are attainable under the calibrated budget and apply the conformal risk control method to determine a statistically valid uncertainty threshold, which filters unreliable distractors from the candidate set for each test data point. In this stage, SAFER introduces an additional risk level to guide the calculation of the threshold, thereby controlling the risk of correct answers being excluded. Furthermore, we show that SAFER is compatible with various task-specific admission criteria and calibration-test split ratios, highlighting its robustness and high data efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10193v2",
    "published_date": "2025-10-11 12:12:41 UTC",
    "updated_date": "2025-10-21 08:14:09 UTC"
  },
  {
    "arxiv_id": "2510.10189v2",
    "title": "Formally Verified Certification of Unsolvability of Temporal Planning Problems",
    "authors": [
      "David Wang",
      "Mohammad Abdulaziz"
    ],
    "abstract": "We present an approach to unsolvability certification of temporal planning. Our approach is based on encoding the planning problem into a network of timed automata, and then using an efficient model checker on the network followed by a certificate checker to certify the output of the model checker. Our approach prioritises trustworthiness of the certification: we formally verify our implementation of the encoding to timed automata using the theorem prover Isabelle/HOL and we use an existing certificate checker (also formally verified in Isabelle/HOL) to certify the model checking result.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10189v2",
    "published_date": "2025-10-11 11:57:25 UTC",
    "updated_date": "2025-10-19 20:32:53 UTC"
  },
  {
    "arxiv_id": "2510.13839v2",
    "title": "Meronymic Ontology Extraction via Large Language Models",
    "authors": [
      "Dekai Zhang",
      "Simone Conia",
      "Antonio Rago"
    ],
    "abstract": "Ontologies have become essential in today's digital age as a way of organising the vast amount of readily available unstructured text. In providing formal structure to this information, ontologies have immense value and application across various domains, e.g., e-commerce, where countless product listings necessitate proper product organisation. However, the manual construction of these ontologies is a time-consuming, expensive and laborious process. In this paper, we harness the recent advancements in large language models (LLMs) to develop a fully-automated method of extracting product ontologies, in the form of meronymies, from raw review texts. We demonstrate that the ontologies produced by our method surpass an existing, BERT-based baseline when evaluating using an LLM-as-a-judge. Our investigation provides the groundwork for LLMs to be used more generally in (product or otherwise) ontology extraction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to AACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.13839v2",
    "published_date": "2025-10-11 11:54:38 UTC",
    "updated_date": "2025-11-09 22:44:44 UTC"
  },
  {
    "arxiv_id": "2510.10185v1",
    "title": "MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems",
    "authors": [
      "Lei Gu",
      "Yinghao Zhu",
      "Haoran Sang",
      "Zixiang Wang",
      "Dehao Sui",
      "Wen Tang",
      "Ewen Harrison",
      "Junyi Gao",
      "Lequan Yu",
      "Liantao Ma"
    ],
    "abstract": "While large language model (LLM)-based multi-agent systems show promise in simulating medical consultations, their evaluation is often confined to final-answer accuracy. This practice treats their internal collaborative processes as opaque \"black boxes\" and overlooks a critical question: is a diagnostic conclusion reached through a sound and verifiable reasoning pathway? The inscrutable nature of these systems poses a significant risk in high-stakes medical applications, potentially leading to flawed or untrustworthy conclusions. To address this, we conduct a large-scale empirical study of 3,600 cases from six medical datasets and six representative multi-agent frameworks. Through a rigorous, mixed-methods approach combining qualitative analysis with quantitative auditing, we develop a comprehensive taxonomy of collaborative failure modes. Our quantitative audit reveals four dominant failure patterns: flawed consensus driven by shared model deficiencies, suppression of correct minority opinions, ineffective discussion dynamics, and critical information loss during synthesis. This study demonstrates that high accuracy alone is an insufficient measure of clinical or public trust. It highlights the urgent need for transparent and auditable reasoning processes, a cornerstone for the responsible development and deployment of medical AI.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "Code: https://github.com/yhzhu99/MedAgentAudit",
    "pdf_url": "https://arxiv.org/pdf/2510.10185v1",
    "published_date": "2025-10-11 11:48:57 UTC",
    "updated_date": "2025-10-11 11:48:57 UTC"
  },
  {
    "arxiv_id": "2510.10182v1",
    "title": "A Survey of Inductive Reasoning for Large Language Models",
    "authors": [
      "Kedi Chen",
      "Dezhao Ruan",
      "Yuhao Dan",
      "Yaoting Wang",
      "Siyu Yan",
      "Xuecheng Wu",
      "Yinqi Zhang",
      "Qin Chen",
      "Jie Zhou",
      "Liang He",
      "Biqing Qi",
      "Linyang Li",
      "Qipeng Guo",
      "Xiaoming Shi",
      "Wei Zhang"
    ],
    "abstract": "Reasoning is an important task for large language models (LLMs). Among all the reasoning paradigms, inductive reasoning is one of the fundamental types, which is characterized by its particular-to-general thinking process and the non-uniqueness of its answers. The inductive mode is crucial for knowledge generalization and aligns better with human cognition, so it is a fundamental mode of learning, hence attracting increasing interest. Despite the importance of inductive reasoning, there is no systematic summary of it. Therefore, this paper presents the first comprehensive survey of inductive reasoning for LLMs. First, methods for improving inductive reasoning are categorized into three main areas: post-training, test-time scaling, and data augmentation. Then, current benchmarks of inductive reasoning are summarized, and a unified sandbox-based evaluation approach with the observation coverage metric is derived. Finally, we offer some analyses regarding the source of inductive ability and how simple model architectures and data help with inductive tasks, providing a solid foundation for future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10182v1",
    "published_date": "2025-10-11 11:45:38 UTC",
    "updated_date": "2025-10-11 11:45:38 UTC"
  },
  {
    "arxiv_id": "2510.10181v2",
    "title": "Dejavu: Towards Experience Feedback Learning for Embodied Intelligence",
    "authors": [
      "Shaokai Wu",
      "Yanbiao Ji",
      "Qiuchang Li",
      "Zhiyi Zhang",
      "Qichen He",
      "Wenyuan Xie",
      "Guodong Zhang",
      "Bayram Bayramli",
      "Yue Ding",
      "Hongtao Lu"
    ],
    "abstract": "Embodied agents face a fundamental limitation: once deployed in real-world environments to perform specific tasks, they are unable to acquire additional knowledge to enhance task performance. In this paper, we propose a general post-deployment learning framework Dejavu, which employs an Experience Feedback Network (EFN) and augments the frozen Vision-Language-Action (VLA) policy with retrieved execution memories. EFN identifies contextually prior action experiences and conditions action prediction on this retrieved guidance. We adopt reinforcement learning with semantic similarity rewards to train EFN, ensuring that the predicted actions align with past behaviors under current observations. During deployment, EFN continually enriches its memory with new trajectories, enabling the agent to exhibit \"learning from experience\". Experiments across diverse embodied tasks show that EFN improves adaptability, robustness, and success rates over frozen baselines. We provide code and demo in our supplementary material.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10181v2",
    "published_date": "2025-10-11 11:43:58 UTC",
    "updated_date": "2025-12-07 11:05:51 UTC"
  },
  {
    "arxiv_id": "2510.10179v1",
    "title": "LLMs are All You Need? Improving Fuzz Testing for MOJO with Large Language Models",
    "authors": [
      "Linghan Huang",
      "Peizhou Zhao",
      "Huaming Chen"
    ],
    "abstract": "The rapid development of large language models (LLMs) has revolutionized software testing, particularly fuzz testing, by automating the generation of diverse and effective test inputs. This advancement holds great promise for improving software reliability. Meanwhile, the introduction of MOJO, a high-performance AI programming language blending Python's usability with the efficiency of C and C++, presents new opportunities to enhance AI model scalability and programmability. However, as a new language, MOJO lacks comprehensive testing frameworks and a sufficient corpus for LLM-based testing, which exacerbates model hallucination. In this case, LLMs will generate syntactically valid but semantically incorrect code, significantly reducing the effectiveness of fuzz testing. To address this challenge, we propose MOJOFuzzer, the first adaptive LLM-based fuzzing framework designed for zero-shot learning environments of emerging programming languages. MOJOFuzzer integrates a mutil-phase framework that systematically eliminates low-quality generated inputs before execution, significantly improving test case validity. Furthermore, MOJOFuzzer dynamically adapts LLM prompts based on runtime feedback for test case mutation, enabling an iterative learning process that continuously enhances fuzzing efficiency and bug detection performance. Our experimental results demonstrate that MOJOFuzzer significantly enhances test validity, API coverage, and bug detection performance, outperforming traditional fuzz testing and state-of-the-art LLM-based fuzzing approaches. Using MOJOFuzzer, we have conducted a first large-scale fuzz testing evaluation of MOJO, uncorvering 13 previous unknown bugs. This study not only advances the field of LLM-driven software testing but also establishes a foundational methodology for leveraging LLMs in the testing of emerging programming languages.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10179v1",
    "published_date": "2025-10-11 11:37:18 UTC",
    "updated_date": "2025-10-11 11:37:18 UTC"
  },
  {
    "arxiv_id": "2510.10177v2",
    "title": "HccePose(BF): Predicting Front & Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation",
    "authors": [
      "Yulin Wang",
      "Mengting Hu",
      "Hongli Li",
      "Chen Luo"
    ],
    "abstract": "In pose estimation for seen objects, a prevalent pipeline involves using neural networks to predict dense 3D coordinates of the object surface on 2D images, which are then used to establish dense 2D-3D correspondences. However, current methods primarily focus on more efficient encoding techniques to improve the precision of predicted 3D coordinates on the object's front surface, overlooking the potential benefits of incorporating the back surface and interior of the object. To better utilize the full surface and interior of the object, this study predicts 3D coordinates of both the object's front and back surfaces and densely samples 3D coordinates between them. This process creates ultra-dense 2D-3D correspondences, effectively enhancing pose estimation accuracy based on the Perspective-n-Point (PnP) algorithm. Additionally, we propose Hierarchical Continuous Coordinate Encoding (HCCE) to provide a more accurate and efficient representation of front and back surface coordinates. Experimental results show that, compared to existing state-of-the-art (SOTA) methods on the BOP website, the proposed approach outperforms across seven classic BOP core datasets. Code is available at https://github.com/WangYuLin-SEU/HCCEPose.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "International Conference on Computer Vision, ICCV 2025 (Highlight) https://iccv.thecvf.com/virtual/2025/poster/338",
    "pdf_url": "https://arxiv.org/pdf/2510.10177v2",
    "published_date": "2025-10-11 11:29:53 UTC",
    "updated_date": "2025-10-14 07:12:01 UTC"
  },
  {
    "arxiv_id": "2510.10168v2",
    "title": "Concise Reasoning in the Lens of Lagrangian Optimization",
    "authors": [
      "Chengqian Gao",
      "Haonan Li",
      "Taylor W. Killian",
      "Jianshu She",
      "Renxi Wang",
      "Liqun Ma",
      "Zhoujun Cheng",
      "Shibo Hao",
      "Zhiqiang Xu"
    ],
    "abstract": "Concise reasoning in large language models seeks to generate only essential intermediate steps needed to arrive at a final answer, thereby alleviating issues of overthinking. Most proposed approaches hinge on carefully hand-crafted heuristics, struggling to balance concision with performance, often failing to adapt across domains and model scales. In this work, we address these challenges by introducing a principled and pragmatic strategy, performance-aware length updating (PALU). As a principled algorithm, PALU formulates concise reasoning as a constrained optimization problem, minimizing response length subject to a performance constraint, and then applies Lagrangian optimization to convert it into a tractable unconstrained problem. As a pragmatic solution, PALU streamlines complicated update rules through three approximations: (i) estimating performance with off-policy rollouts, (ii) truncating the Lagrange multiplier to two extremes, and (iii) replacing gradient-based updates with quantile-driven length adjustments. PALU reduces output length by 65% while improving accuracy by 15% when applied to DeepSeek-Distill-Qwen-1.5B, averaged over five benchmarks, outperforming a range of alternative methods. Furthermore, PALU is demonstrated to adapt across both domain (logic, STEM and math) and model scale (1.5B, 7B, 14B) entrenching the algorithm as a practical and effective concise reasoning approach.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10168v2",
    "published_date": "2025-10-11 11:16:28 UTC",
    "updated_date": "2025-10-14 06:39:32 UTC"
  },
  {
    "arxiv_id": "2510.10161v2",
    "title": "Large Language Model Sourcing: A Survey",
    "authors": [
      "Liang Pang",
      "Jia Gu",
      "Sunhao Dai",
      "Zihao Wei",
      "Zenghao Duan",
      "Kangxi Wu",
      "Zhiyi Yin",
      "Jun Xu",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "abstract": "Due to the black-box nature of large language models (LLMs) and the realism of their generated content, issues such as hallucinations, bias, unfairness, and copyright infringement have become significant. In this context, sourcing information from multiple perspectives is essential. This survey presents a systematic investigation organized around four interrelated dimensions: Model Sourcing, Model Structure Sourcing, Training Data Sourcing, and External Data Sourcing. Moreover, a unified dual-paradigm taxonomy is proposed that classifies existing sourcing methods into prior-based (proactive traceability embedding) and posterior-based (retrospective inference) approaches. Traceability across these dimensions enhances the transparency, accountability, and trustworthiness of LLMs deployment in real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "31 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.10161v2",
    "published_date": "2025-10-11 10:52:30 UTC",
    "updated_date": "2025-12-31 06:20:51 UTC"
  },
  {
    "arxiv_id": "2510.10160v2",
    "title": "SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation",
    "authors": [
      "Zhenjie Mao",
      "Yuhuan Yang",
      "Chaofan Ma",
      "Dongsheng Jiang",
      "Jiangchao Yao",
      "Ya Zhang",
      "Yanfeng Wang"
    ],
    "abstract": "Referring Image Segmentation (RIS) aims to segment the target object in an image given a natural language expression. While recent methods leverage pre-trained vision backbones and more training corpus to achieve impressive results, they predominantly focus on simple expressions--short, clear noun phrases like \"red car\" or \"left girl\". This simplification often reduces RIS to a key word/concept matching problem, limiting the model's ability to handle referential ambiguity in expressions. In this work, we identify two challenging real-world scenarios: object-distracting expressions, which involve multiple entities with contextual cues, and category-implicit expressions, where the object class is not explicitly stated. To address the challenges, we propose a novel framework, SaFiRe, which mimics the human two-phase cognitive process--first forming a global understanding, then refining it through detail-oriented inspection. This is naturally supported by Mamba's scan-then-update property, which aligns with our phased design and enables efficient multi-cycle refinement with linear complexity. We further introduce aRefCOCO, a new benchmark designed to evaluate RIS models under ambiguous referring expressions. Extensive experiments on both standard and proposed datasets demonstrate the superiority of SaFiRe over state-of-the-art baselines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2025; Project page: https://zhenjiemao.github.io/SaFiRe/",
    "pdf_url": "https://arxiv.org/pdf/2510.10160v2",
    "published_date": "2025-10-11 10:50:58 UTC",
    "updated_date": "2025-11-26 14:51:06 UTC"
  },
  {
    "arxiv_id": "2510.10158v1",
    "title": "Multi-Scale Diffusion Transformer for Jointly Simulating User Mobility and Mobile Traffic Pattern",
    "authors": [
      "Ziyi Liu",
      "Qingyue Long",
      "Zhiwen Xue",
      "Huandong Wang",
      "Yong Li"
    ],
    "abstract": "User mobility trajectory and mobile traffic data are essential for a wide spectrum of applications including urban planning, network optimization, and emergency management. However, large-scale and fine-grained mobility data remains difficult to obtain due to privacy concerns and collection costs, making it essential to simulate realistic mobility and traffic patterns. User trajectories and mobile traffic are fundamentally coupled, reflecting both physical mobility and cyber behavior in urban environments. Despite this strong interdependence, existing studies often model them separately, limiting the ability to capture cross-modal dynamics. Therefore, a unified framework is crucial. In this paper, we propose MSTDiff, a Multi-Scale Diffusion Transformer for joint simulation of mobile traffic and user trajectories. First, MSTDiff applies discrete wavelet transforms for multi-resolution traffic decomposition. Second, it uses a hybrid denoising network to process continuous traffic volumes and discrete location sequences. A transition mechanism based on urban knowledge graph embedding similarity is designed to guide semantically informed trajectory generation. Finally, a multi-scale Transformer with cross-attention captures dependencies between trajectories and traffic. Experiments show that MSTDiff surpasses state-of-the-art baselines in traffic and trajectory generation tasks, reducing Jensen-Shannon divergence (JSD) across key statistical metrics by up to 17.38% for traffic generation, and by an average of 39.53% for trajectory generation. The source code is available at: https://github.com/tsinghua-fib-lab/MSTDiff .",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "9 pages, 4 figures. Code: https://github.com/tsinghua-fib-lab/MSTDiff",
    "pdf_url": "https://arxiv.org/pdf/2510.10158v1",
    "published_date": "2025-10-11 10:45:39 UTC",
    "updated_date": "2025-10-11 10:45:39 UTC"
  },
  {
    "arxiv_id": "2510.10157v1",
    "title": "BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation",
    "authors": [
      "Tsung-Min Pai",
      "Jui-I Wang",
      "Li-Chun Lu",
      "Shao-Hua Sun",
      "Hung-Yi Lee",
      "Kai-Wei Chang"
    ],
    "abstract": "Multi-LLM systems enhance the creativity of large language models by simulating human collective intelligence but suffer from significant drawbacks, such as high computational costs and inference latency. To address these limitations, we propose BILLY (BlendIng persona vectors for Large Language model creativitY), a training-free framework that captures the benefits of multi-LLM collaboration, i.e. inducing diverse perspectives and specialized expertise, within a single model. BILLY operates by extracting and blending multiple distinct persona vectors directly in the model's activation space. We steer the model's generation process with this merged vector while inference, enabling multi-perspective output without explicit multi-LLM communication. Our experiments across creativity-oriented benchmarks demonstrate that BILLY surpasses single model prompting and traditional multi-LLM approaches, while substantially reducing inference time and computational costs. Our analyses further reveal that distinct persona vectors can be blended to achieve both effective control over complementary aspects of generation and greater interpretability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10157v1",
    "published_date": "2025-10-11 10:36:39 UTC",
    "updated_date": "2025-10-11 10:36:39 UTC"
  },
  {
    "arxiv_id": "2510.10150v2",
    "title": "Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective",
    "authors": [
      "Zhezheng Hao",
      "Hong Wang",
      "Haoyang Liu",
      "Jian Luo",
      "Jiarui Yu",
      "Hande Dong",
      "Qiang Lin",
      "Can Wang",
      "Jiawei Chen"
    ],
    "abstract": "While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM reasoning, its training process carries a critical risk: entropy collapse. This phenomenon is a rapid decrease in policy entropy, which severely limits exploration and diminishes learning effectiveness. Recent methods attempt to mitigate this collapse via heuristic entropy interventions, yet the underlying mechanisms governing entropy remain unclear. In this work, we conduct a theoretical and quantitative analysis of GRPO's entropy dynamics, revealing that token-level entropy change in each update step is jointly governed by four key factors: clipping strategy, advantage, token probability, and token entropy. These findings not only explain the mechanisms of existing methods, but also reveal their limitations: they rely on heuristic adjustments to only one or two factors, leaving other relevant factors unconsidered and reducing their effectiveness. This motivates us to propose a new method, STEER, which adaptively reweights tokens based on their estimated entropy change to regulate entropy in a principled manner. Experiments on both math and coding benchmarks demonstrate that STEER effectively mitigates entropy collapse and consistently outperforms state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10150v2",
    "published_date": "2025-10-11 10:17:38 UTC",
    "updated_date": "2026-01-19 15:00:58 UTC"
  },
  {
    "arxiv_id": "2510.10145v1",
    "title": "A Unified Frequency Domain Decomposition Framework for Interpretable and Robust Time Series Forecasting",
    "authors": [
      "Cheng He",
      "Xijie Liang",
      "Zengrong Zheng",
      "Patrick P. C. Lee",
      "Xu Huang",
      "Zhaoyi Li",
      "Hong Xie",
      "Defu Lian",
      "Enhong Chen"
    ],
    "abstract": "Current approaches for time series forecasting, whether in the time or frequency domain, predominantly use deep learning models based on linear layers or transformers. They often encode time series data in a black-box manner and rely on trial-and-error optimization solely based on forecasting performance, leading to limited interpretability and theoretical understanding. Furthermore, the dynamics in data distribution over time and frequency domains pose a critical challenge to accurate forecasting. We propose FIRE, a unified frequency domain decomposition framework that provides a mathematical abstraction for diverse types of time series, so as to achieve interpretable and robust time series forecasting. FIRE introduces several key innovations: (i) independent modeling of amplitude and phase components, (ii) adaptive learning of weights of frequency basis components, (iii) a targeted loss function, and (iv) a novel training paradigm for sparse data. Extensive experiments demonstrate that FIRE consistently outperforms state-of-the-art models on long-term forecasting benchmarks, achieving superior predictive performance and significantly enhancing interpretability of time series",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10145v1",
    "published_date": "2025-10-11 09:59:25 UTC",
    "updated_date": "2025-10-11 09:59:25 UTC"
  },
  {
    "arxiv_id": "2510.10142v3",
    "title": "Debiasing LLMs by Masking Unfairness-Driving Attention Heads",
    "authors": [
      "Tingxu Han",
      "Wei Song",
      "Ziqi Ding",
      "Ziming Li",
      "Chunrong Fang",
      "Yuekang Li",
      "Dongfang Liu",
      "Zhenyu Chen",
      "Zhenting Wang"
    ],
    "abstract": "Large language models (LLMs) increasingly mediate decisions in domains where unfair treatment of demographic groups is unacceptable. Existing work probes when biased outputs appear, but gives little insight into the mechanisms that generate them, leaving existing mitigations largely fragile. In this paper, we conduct a systematic investigation LLM unfairness and propose DiffHeads, a lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA) prompting to Chain-of-Thought (CoT) prompting across eight representative open- and closed-source LLMs. DA will trigger the nature bias part of LLM and improve measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues. Next, we define a token-to-head contribution score that traces each token's influence back to individual attention heads. This reveals a small cluster of bias heads that activate under DA but stay largely dormant with CoT, providing the first causal link between prompting strategy and bias emergence. Finally, building on this insight, we propose DiffHeads that identifies bias heads through differential activation analysis between DA and CoT, and selectively masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under DA and CoT, respectively, without harming model utility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10142v3",
    "published_date": "2025-10-11 09:48:31 UTC",
    "updated_date": "2025-11-02 16:45:45 UTC"
  },
  {
    "arxiv_id": "2510.10138v1",
    "title": "Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task",
    "authors": [
      "Zilong Wang",
      "Xiaoyu Shen"
    ],
    "abstract": "Information extraction from copy-heavy documents, characterized by massive volumes of structurally similar content, represents a critical yet understudied challenge in enterprise document processing. We present a systematic framework that strategically combines OCR engines with Large Language Models (LLMs) to optimize the accuracy-efficiency trade-off inherent in repetitive document extraction tasks. Unlike existing approaches that pursue universal solutions, our method exploits document-specific characteristics through intelligent strategy selection. We implement and evaluate 25 configurations across three extraction paradigms (direct, replacement, and table-based) on identity documents spanning four formats (PNG, DOCX, XLSX, PDF). Through table-based extraction methods, our adaptive framework delivers outstanding results: F1=1.0 accuracy with 0.97s latency for structured documents, and F1=0.997 accuracy with 0.6 s for challenging image inputs when integrated with PaddleOCR, all while maintaining sub-second processing speeds. The 54 times performance improvement compared with multimodal methods over naive approaches, coupled with format-aware routing, enables processing of heterogeneous document streams at production scale. Beyond the specific application to identity extraction, this work establishes a general principle: the repetitive nature of copy-heavy tasks can be transformed from a computational burden into an optimization opportunity through structure-aware method selection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10138v1",
    "published_date": "2025-10-11 09:40:34 UTC",
    "updated_date": "2025-10-11 09:40:34 UTC"
  },
  {
    "arxiv_id": "2510.10136v1",
    "title": "PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models",
    "authors": [
      "Lancheng Zou",
      "Shuo Yin",
      "Zehua Pei",
      "Tsung-Yi Ho",
      "Farzan Farnia",
      "Bei Yu"
    ],
    "abstract": "Channel permutation is a powerful technique for enhancing the accuracy of N:M sparse models by reordering the channels of weight matrices to prioritize the retention of important weights. However, traditional channel permutation methods rely on handcrafted quality metrics, which often fail to accurately capture the true impact of pruning on model performance. To address this limitation, we propose PermLLM, a novel post-training pruning framework that introduces learnable channel permutation (LCP) for N:M sparsity. LCP leverages Sinkhorn normalization to transform discrete permutation matrices into differentiable soft permutation matrices, enabling end-to-end optimization. Additionally, PermLLM incorporates an efficient block-wise channel permutation strategy, which significantly reduces the number of learnable parameters and computational complexity. PermLLM seamlessly integrates with existing one-shot pruning methods to adaptively optimize channel permutations, effectively mitigating pruning-induced errors. Extensive experiments on the LLaMA series, Qwen, and OPT models demonstrate that PermLLM achieves superior performance in optimizing N:M sparse models. The code is available at https://github.com/lanchengzou/PermLLM.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.10136v1",
    "published_date": "2025-10-11 09:40:27 UTC",
    "updated_date": "2025-10-11 09:40:27 UTC"
  },
  {
    "arxiv_id": "2510.10135v2",
    "title": "CharCom: Composable Identity Control for Multi-Character Story Illustration",
    "authors": [
      "Zhongsheng Wang",
      "Ming Lin",
      "Zhedong Lin",
      "Yaser Shakib",
      "Qian Liu",
      "Jiamou Liu"
    ],
    "abstract": "Ensuring character identity consistency across varying prompts remains a fundamental limitation in diffusion-based text-to-image generation. We propose CharCom, a modular and parameter-efficient framework that achieves character-consistent story illustration through composable LoRA adapters, enabling efficient per-character customization without retraining the base model. Built on a frozen diffusion backbone, CharCom dynamically composes adapters at inference using prompt-aware control. Experiments on multi-scene narratives demonstrate that CharCom significantly enhances character fidelity, semantic alignment, and temporal coherence. It remains robust in crowded scenes and enables scalable multi-character generation with minimal overhead, making it well-suited for real-world applications such as story illustration and animation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ACM MMAsia 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.10135v2",
    "published_date": "2025-10-11 09:36:20 UTC",
    "updated_date": "2025-11-21 07:42:59 UTC"
  },
  {
    "arxiv_id": "2510.10129v1",
    "title": "CacheClip: Accelerating RAG with Effective KV Cache Reuse",
    "authors": [
      "Bin Yang",
      "Qiuyu Leng",
      "Jun Zeng",
      "Zhenhua Wu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems suffer from severe time-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV cache reuse methods face a fundamental trade-off: prefix caching requires identical prefixes that rarely occur in RAG scenarios, while direct precomputation sacrifices quality due to missing inter-chunk attention and repeated attention sinks. Recent methods like APE and CacheBlend partially address these issues but remain inadequate for robust RAG applications. This paper presents CacheClip, a novel framework that achieves both fast TTFT and high generation quality. Our key insight is that small auxiliary LLMs exhibit similar last-layer attention distributions to primary LLMs (the target model for generation), enabling efficient identification of tokens critical for restoring inter-chunk attention, thereby significantly improving response quality on cross-chunk reasoning tasks. CacheClip integrates three techniques: (1) auxiliary-model-guided token selection for selective KV cache recomputation, where the auxiliary model is finetuned to improve selection accuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3) grouping strategy to maintain local coherence during partial KV cache updates. Experiments show CacheClip retains up to 94.8% and 85.0% of full-attention performance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2% and 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM inference by up to 1.92x in prefill time, providing a practical solution to the efficiency-quality trade-off in RAG systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10129v1",
    "published_date": "2025-10-11 09:28:26 UTC",
    "updated_date": "2025-10-11 09:28:26 UTC"
  },
  {
    "arxiv_id": "2510.10125v2",
    "title": "Ctrl-World: A Controllable Generative World Model for Robot Manipulation",
    "authors": [
      "Yanjiang Guo",
      "Lucy Xiaoyang Shi",
      "Jianyu Chen",
      "Chelsea Finn"
    ],
    "abstract": "Generalist robot policies can now perform a wide range of manipulation skills, but evaluating and improving their ability with unfamiliar objects and instructions remains a significant challenge. Rigorous evaluation requires a large number of real-world rollouts, while systematic improvement demands additional corrective data with expert labels. Both of these processes are slow, costly, and difficult to scale. World models offer a promising, scalable alternative by enabling policies to rollout within imagination space. However, a key challenge is building a controllable world model that can handle multi-step interactions with generalist robot policies. This requires a world model compatible with modern generalist policies by supporting multi-view prediction, fine-grained action control, and consistent long-horizon interactions, which is not achieved by previous works. In this paper, we make a step forward by introducing a controllable multi-view world model that can be used to evaluate and improve the instruction-following ability of generalist robot policies. Our model maintains long-horizon consistency with a pose-conditioned memory retrieval mechanism and achieves precise action control through frame-level action conditioning. Trained on the DROID dataset (95k trajectories, 564 scenes), our model generates spatially and temporally consistent trajectories under novel scenarios and new camera placements for over 20 seconds. We show that our method can accurately rank policy performance without real-world robot rollouts. Moreover, by synthesizing successful trajectories in imagination and using them for supervised fine-tuning, our approach can improve policy success by 44.7\\%.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "17 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.10125v2",
    "published_date": "2025-10-11 09:13:10 UTC",
    "updated_date": "2025-10-15 00:46:49 UTC"
  },
  {
    "arxiv_id": "2510.10122v1",
    "title": "DeepFusionNet: Autoencoder-Based Low-Light Image Enhancement and Super-Resolution",
    "authors": [
      "Halil Hüseyin Çalışkan",
      "Talha Koruk"
    ],
    "abstract": "Computer vision and image processing applications suffer from dark and low-light images, particularly during real-time image transmission. Currently, low light and dark images are converted to bright and colored forms using autoencoders; however, these methods often achieve low SSIM and PSNR scores and require high computational power due to their large number of parameters. To address these challenges, the DeepFusionNet architecture has been developed. According to the results obtained with the LOL-v1 dataset, DeepFusionNet achieved an SSIM of 92.8% and a PSNR score of 26.30, while containing only approximately 2.5 million parameters. On the other hand, conversion of blurry and low-resolution images into high-resolution and blur-free images has gained importance in image processing applications. Unlike GAN-based super-resolution methods, an autoencoder-based super resolution model has been developed that contains approximately 100 thousand parameters and uses the DeepFusionNet architecture. According to the results of the tests, the DeepFusionNet based super-resolution method achieved a PSNR of 25.30 and a SSIM score of 80.7 percent according to the validation set.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10122v1",
    "published_date": "2025-10-11 09:04:22 UTC",
    "updated_date": "2025-10-11 09:04:22 UTC"
  },
  {
    "arxiv_id": "2510.10117v1",
    "title": "DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay",
    "authors": [
      "Yunxiang Mo",
      "Tianshi Zheng",
      "Qing Zong",
      "Jiayu Liu",
      "Baixuan Xu",
      "Yauwai Yim",
      "Chunkit Chan",
      "Jiaxin Bai",
      "Yangqiu Song"
    ],
    "abstract": "Multimodal abductive reasoning--the generation and selection of explanatory hypotheses from partial observations--is a cornerstone of intelligence. Current evaluations of this ability in vision-language models (VLMs) are largely confined to static, single-agent tasks. Inspired by Dixit, we introduce DixitWorld, a comprehensive evaluation suite designed to deconstruct this challenge. DIXITWORLD features two core components: DixitArena, a dynamic, multi-agent environment that evaluates both hypothesis generation (a \"storyteller\" crafting cryptic clues) and hypothesis selection (\"listeners\" choosing the target image from decoys) under imperfect information; and DixitBench, a static QA benchmark that isolates the listener's task for efficient, controlled evaluation. Results from DixitArena reveal distinct, role-dependent behaviors: smaller open-source models often excel as creative storytellers, producing imaginative yet less discriminative clues, whereas larger proprietary models demonstrate superior overall performance, particularly as listeners. Performance on DixitBench strongly correlates with listener results in DixitArena, validating it as a reliable proxy for hypothesis selection. Our findings reveal a key trade-off between generative creativity and discriminative understanding in multimodal abductive reasoning, a central challenge for developing more balanced and capable vision-language agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP 2025 Wordplay (Spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2510.10117v1",
    "published_date": "2025-10-11 08:48:48 UTC",
    "updated_date": "2025-10-11 08:48:48 UTC"
  },
  {
    "arxiv_id": "2510.10111v3",
    "title": "Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization",
    "authors": [
      "Rui Chen",
      "Bin Liu",
      "Changtao Miao",
      "Xinghao Wang",
      "Yi Li",
      "Tao Gong",
      "Qi Chu",
      "Nenghai Yu"
    ],
    "abstract": "Advances in image tampering pose serious security threats, underscoring the need for effective image manipulation localization (IML). While supervised IML achieves strong performance, it depends on costly pixel-level annotations. Existing weakly supervised or training-free alternatives often underperform and lack interpretability. We propose the In-Context Forensic Chain (ICFC), a training-free framework that leverages multi-modal large language models (MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule construction with adaptive filtering to build a reliable knowledge base and a multi-step progressive reasoning pipeline that mirrors expert forensic workflows from coarse proposals to fine-grained forensics results. This design enables systematic exploitation of MLLM reasoning for image-level classification, pixel-level localization, and text-level interpretability. Across multiple benchmarks, ICFC not only surpasses state-of-the-art training-free methods but also achieves competitive or superior performance compared to weakly and fully supervised approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "This version was uploaded in error and contains misleading information found in an early draft. The manuscript requires extensive and long-term revisions",
    "pdf_url": "https://arxiv.org/pdf/2510.10111v3",
    "published_date": "2025-10-11 08:42:31 UTC",
    "updated_date": "2026-01-21 15:39:57 UTC"
  },
  {
    "arxiv_id": "2510.10108v1",
    "title": "Uncertainty-Aware Post-Detection Framework for Enhanced Fire and Smoke Detection in Compact Deep Learning Models",
    "authors": [
      "Aniruddha Srinivas Joshi",
      "Godwyn James William",
      "Shreyas Srinivas Joshi"
    ],
    "abstract": "Accurate fire and smoke detection is critical for safety and disaster response, yet existing vision-based methods face challenges in balancing efficiency and reliability. Compact deep learning models such as YOLOv5n and YOLOv8n are widely adopted for deployment on UAVs, CCTV systems, and IoT devices, but their reduced capacity often results in false positives and missed detections. Conventional post-detection methods such as Non-Maximum Suppression and Soft-NMS rely only on spatial overlap, which can suppress true positives or retain false alarms in cluttered or ambiguous fire scenes. To address these limitations, we propose an uncertainty aware post-detection framework that rescales detection confidences using both statistical uncertainty and domain relevant visual cues. A lightweight Confidence Refinement Network integrates uncertainty estimates with color, edge, and texture features to adjust detection scores without modifying the base model. Experiments on the D-Fire dataset demonstrate improved precision, recall, and mean average precision compared to existing baselines, with only modest computational overhead. These results highlight the effectiveness of post-detection rescoring in enhancing the robustness of compact deep learning models for real-world fire and smoke detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted and to be presented at the International Conference on Smart Multimedia (ICSM 2025) - https://smartmultimedia.org/2025/",
    "pdf_url": "https://arxiv.org/pdf/2510.10108v1",
    "published_date": "2025-10-11 08:36:57 UTC",
    "updated_date": "2025-10-11 08:36:57 UTC"
  },
  {
    "arxiv_id": "2510.10099v2",
    "title": "Uncovering Singularities in Feynman Integrals via Machine Learning",
    "authors": [
      "Yuanche Liu",
      "Yingxuan Xu",
      "Yang Zhang"
    ],
    "abstract": "We introduce a machine-learning framework based on symbolic regression to extract the full symbol alphabet of multi-loop Feynman integrals. By targeting the analytic structure rather than reduction, the method is broadly applicable and interpretable across different families of integrals. It successfully reconstructs complete symbol alphabets in nontrivial examples, demonstrating both robustness and generality. Beyond accelerating computations case by case, it uncovers the analytic structure universally. This framework opens new avenues for multi-loop amplitude analysis and provides a versatile tool for exploring scattering amplitudes.",
    "categories": [
      "hep-ph",
      "cs.AI",
      "cs.LG",
      "hep-th"
    ],
    "primary_category": "hep-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10099v2",
    "published_date": "2025-10-11 08:16:33 UTC",
    "updated_date": "2025-10-27 09:00:42 UTC"
  },
  {
    "arxiv_id": "2511.11584v1",
    "title": "Output Supervision Can Obfuscate the Chain of Thought",
    "authors": [
      "Jacob Drori",
      "Luke Marks",
      "Bryce Woodworth",
      "Alex Cloud",
      "Alexander Matt Turner"
    ],
    "abstract": "OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.11584v1",
    "published_date": "2025-10-11 08:13:02 UTC",
    "updated_date": "2025-10-11 08:13:02 UTC"
  },
  {
    "arxiv_id": "2510.10089v3",
    "title": "What Makes Looped Transformers Perform Better Than Non-Recursive Ones",
    "authors": [
      "Zixuan Gong",
      "Yong Liu",
      "Jiaye Teng"
    ],
    "abstract": "While looped transformers (termed as Looped-Attn) often outperform standard transformers (termed as Single-Attn) on complex reasoning tasks, the mechanism for this advantage remains underexplored. In this paper, we explain this phenomenon through the lens of loss landscape geometry, inspired by empirical observations of their distinct dynamics at both sample and Hessian levels. To formalize this, we extend the River-Valley landscape model by distinguishing between U-shaped valleys (flat) and V-shaped valleys (steep). Based on empirical observations, we conjecture that the recursive architecture of Looped-Attn induces a landscape-level inductive bias towards River-V-Valley. This inductive bias suggest a better loss convergence along the river due to valley hopping, and further encourage learning about complex patterns compared to the River-U-Valley induced by Single-Attn. Building on this insight, we propose SHIFT (Staged HIerarchical Framework for Progressive Training), a principled training strategy that accelerates the training process of Looped-Attn while achieving comparable performances.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10089v3",
    "published_date": "2025-10-11 07:59:25 UTC",
    "updated_date": "2026-01-06 03:05:21 UTC"
  },
  {
    "arxiv_id": "2510.10085v1",
    "title": "Pharmacist: Safety Alignment Data Curation for Large Language Models against Harmful Fine-tuning",
    "authors": [
      "Guozhi Liu",
      "Qi Mu",
      "Tiansheng Huang",
      "Xinhua Wang",
      "Li Shen",
      "Weiwei Lin",
      "Zhang Li"
    ],
    "abstract": "Harmful fine-tuning issues present significant safety challenges for fine-tuning-as-a-service in large language models. Existing alignment-stage defenses, e.g., Vaccine, Repnoise, Booster, and T-Vaccine, mitigate harmful fine-tuning issues by enhancing the model's robustness during the alignment phase. While these methods have been proposed to mitigate the issue, they often overlook a critical upstream factor: the role of the original safety-alignment data. We observe that their defense performance and computational efficiency remain constrained by the quality and composition of the alignment dataset. To address this limitation, we propose Pharmacist, a safety alignment data curation solution that enhances defense against harmful fine-tuning by selecting a high-quality and safety-critical core subset from the original alignment data. The core idea of Pharmacist is to train an alignment data selector to rank alignment data. Specifically, up-ranking high-quality and safety-critical alignment data, down-ranking low-quality and non-safety-critical data. Empirical results indicate that models trained on datasets selected by Pharmacist outperform those trained on datasets selected by existing selection methods in both defense and inference performance. In addition, Pharmacist can be effectively integrated with mainstream alignment-stage defense methods. For example, when applied to RepNoise and T-Vaccine, using the dataset selected by Pharmacist instead of the full dataset leads to improvements in defense performance by 2.60\\% and 3.30\\%, respectively, and enhances inference performance by 3.50\\% and 1.10\\%. Notably, it reduces training time by 56.83\\% and 57.63\\%, respectively. Our code is available at https://github.com/Lslland/Pharmacist.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10085v1",
    "published_date": "2025-10-11 07:55:55 UTC",
    "updated_date": "2025-10-11 07:55:55 UTC"
  },
  {
    "arxiv_id": "2511.20657v1",
    "title": "Intelligent Agents with Emotional Intelligence: Current Trends, Challenges, and Future Prospects",
    "authors": [
      "Raziyeh Zall",
      "Alireza Kheyrkhah",
      "Erik Cambria",
      "Zahra Naseri",
      "M. Reza Kangavari"
    ],
    "abstract": "The development of agents with emotional intelligence is becoming increasingly vital due to their significant role in human-computer interaction and the growing integration of computer systems across various sectors of society. Affective computing aims to design intelligent systems that can recognize, evoke, and express human emotions, thereby emulating human emotional intelligence. While previous reviews have focused on specific aspects of this field, there has been limited comprehensive research that encompasses emotion understanding, elicitation, and expression, along with the related challenges. This survey addresses this gap by providing a holistic overview of core components of artificial emotion intelligence. It covers emotion understanding through multimodal data processing, as well as affective cognition, which includes cognitive appraisal, emotion mapping, and adaptive modulation in decision-making, learning, and reasoning. Additionally, it addresses the synthesis of emotional expression across text, speech, and facial modalities to enhance human-agent interaction. This paper identifies and analyzes the key challenges and issues encountered in the development of affective systems, covering state-of-the-art methodologies designed to address them. Finally, we highlight promising future directions, with particular emphasis on the potential of generative technologies to advance affective computing.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.20657v1",
    "published_date": "2025-10-11 07:40:36 UTC",
    "updated_date": "2025-10-11 07:40:36 UTC"
  },
  {
    "arxiv_id": "2510.10079v1",
    "title": "How AI Companionship Develops: Evidence from a Longitudinal Study",
    "authors": [
      "Angel Hsing-Chi Hwang",
      "Fiona Li",
      "Jacy Reese Anthis",
      "Hayoun Noh"
    ],
    "abstract": "The quickly growing popularity of AI companions poses risks to mental health, personal wellbeing, and social relationships. Past work has identified many individual factors that can drive human-companion interaction, but we know little about how these factors interact and evolve over time. In Study 1, we surveyed AI companion users (N = 303) to map the psychological pathway from users' mental models of the agent to parasocial experiences, social interaction, and the psychological impact of AI companions. Participants' responses foregrounded multiple interconnected variables (agency, parasocial interaction, and engagement) that shape AI companionship. In Study 2, we conducted a longitudinal study with a subset of participants (N = 110) using a new generic chatbot. Participants' perceptions of the generic chatbot significantly converged to perceptions of their own companions by Week 3. These results suggest a longitudinal model of AI companionship development and demonstrate an empirical method to study human-AI companionship.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10079v1",
    "published_date": "2025-10-11 07:36:47 UTC",
    "updated_date": "2025-10-11 07:36:47 UTC"
  },
  {
    "arxiv_id": "2510.10075v1",
    "title": "Gradient-based Model Shortcut Detection for Time Series Classification",
    "authors": [
      "Salomon Ibarra",
      "Frida Cantu",
      "Kaixiong Zhou",
      "Li Zhang"
    ],
    "abstract": "Deep learning models have attracted lots of research attention in time series classification (TSC) task in the past two decades. Recently, deep neural networks (DNN) have surpassed classical distance-based methods and achieved state-of-the-art performance. Despite their promising performance, deep neural networks (DNNs) have been shown to rely on spurious correlations present in the training data, which can hinder generalization. For instance, a model might incorrectly associate the presence of grass with the label ``cat\" if the training set have majority of cats lying in grassy backgrounds. However, the shortcut behavior of DNNs in time series remain under-explored. Most existing shortcut work are relying on external attributes such as gender, patients group, instead of focus on the internal bias behavior in time series models.\n  In this paper, we take the first step to investigate and establish point-based shortcut learning behavior in deep learning time series classification. We further propose a simple detection method based on other class to detect shortcut occurs without relying on test data or clean training classes. We test our proposed method in UCR time series datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code available at: https://github.com/IvorySnake02/SAG.git",
    "pdf_url": "https://arxiv.org/pdf/2510.10075v1",
    "published_date": "2025-10-11 07:21:33 UTC",
    "updated_date": "2025-10-11 07:21:33 UTC"
  },
  {
    "arxiv_id": "2510.10074v1",
    "title": "Agentic Troubleshooting Guide Automation for Incident Management",
    "authors": [
      "Jiayi Mao",
      "Liqun Li",
      "Yanjie Gao",
      "Zegang Peng",
      "Shilin He",
      "Chaoyun Zhang",
      "Si Qin",
      "Samia Khalid",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Sitaram Lanka",
      "Dongmei Zhang"
    ],
    "abstract": "Effective incident management in large-scale IT systems relies on troubleshooting guides (TSGs), but their manual execution is slow and error-prone. While recent advances in LLMs offer promise for automating incident management tasks, existing LLM-based solutions lack specialized support for several key challenges, including managing TSG quality issues, interpreting complex control flow, handling data-intensive queries, and exploiting execution parallelism. We first conducted an empirical study on 92 real-world TSGs, and, guided by our findings, we present StepFly, a novel end-to-end agentic framework for troubleshooting guide automation. Our approach features a three-stage workflow: the first stage provides a comprehensive guide together with a tool, TSG Mentor, to assist SREs in improving TSG quality; the second stage performs offline preprocessing using LLMs to extract structured execution DAGs from unstructured TSGs and to create dedicated Query Preparation Plugins (QPPs); and the third stage executes online using a DAG-guided scheduler-executor framework with a memory system to guarantee correct workflow and support parallel execution of independent steps. Our empirical evaluation on a collection of real-world TSGs and incidents demonstrates that StepFly achieves a ~94% success rate on GPT-4.1, outperforming baselines with less time and token consumption. Furthermore, it achieves a remarkable execution time reduction of 32.9% to 70.4% for parallelizable TSGs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10074v1",
    "published_date": "2025-10-11 07:18:36 UTC",
    "updated_date": "2025-10-11 07:18:36 UTC"
  },
  {
    "arxiv_id": "2510.10069v2",
    "title": "SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation",
    "authors": [
      "Zeyu Ling",
      "Xiaodong Gu",
      "Jiangnan Tang",
      "Changqing Zou"
    ],
    "abstract": "We introduce SyncLipMAE, a self-supervised pretraining framework for talking-face video that learns synchronization-aware and transferable facial dynamics from unlabeled audio-visual streams. Our approach couples masked visual modeling with cross-modal contrastive alignment and employs three per-frame prompt tokens that explicitly encode the essential factors of a talking-face frame - identity, vocal motion (speech-synchronized facial dynamics), and ambient motion (audio-agnostic movements such as blinks and head pose). The contrastive objective uses time-aligned vocal-motion and audio tokens as positives and misaligned pairs as negatives, driving both modalities into a shared embedding space and yielding token-level audio-visual stream synchronization. After pretraining, the aligned audio tokens together with the visual prompt tokens (identity, vocal motion, ambient motion) form a unified interface for four disparate downstream settings: (i) audio-visual stream synchronization; (ii) facial emotion and head/face action recognition; (iii) visual speech recognition; and (iv) visual dubbing, for which we enable indistinguishable audio- or video-driven control within a single model. Across four task families that require distinct capabilities, SyncLipMAE achieves state-of-the-art results, underscoring the effectiveness of synchronization-aware, factorized self-supervised pretraining.",
    "categories": [
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10069v2",
    "published_date": "2025-10-11 07:12:44 UTC",
    "updated_date": "2026-01-06 13:04:38 UTC"
  },
  {
    "arxiv_id": "2510.10066v1",
    "title": "OBsmith: Testing JavaScript Obfuscator using LLM-powered sketching",
    "authors": [
      "Shan Jiang",
      "Chenguang Zhu",
      "Sarfraz Khurshid"
    ],
    "abstract": "JavaScript obfuscators are widely deployed to protect intellectual property and resist reverse engineering, yet their correctness has been largely overlooked compared to performance and resilience. Existing evaluations typically measure resistance to deobfuscation, leaving the critical question of whether obfuscators preserve program semantics unanswered. Incorrect transformations can silently alter functionality, compromise reliability, and erode security-undermining the very purpose of obfuscation. To address this gap, we present OBsmith, a novel framework to systematically test JavaScript obfuscators using large language models (LLMs). OBsmith leverages LLMs to generate program sketches abstract templates capturing diverse language constructs, idioms, and corner cases-which are instantiated into executable programs and subjected to obfuscation under different configurations. Besides LLM-powered sketching, OBsmith also employs a second source: automatic extraction of sketches from real programs. This extraction path enables more focused testing of project specific features and lets developers inject domain knowledge into the resulting test cases. OBsmith uncovers 11 previously unknown correctness bugs. Under an equal program budget, five general purpose state-of-the-art JavaScript fuzzers (FuzzJIT, Jsfunfuzz, Superion, DIE, Fuzzilli) failed to detect these issues, highlighting OBsmith's complementary focus on obfuscation induced misbehavior. An ablation shows that all components except our generic MRs contribute to at least one bug class; the negative MR result suggests the need for obfuscator-specific metamorphic relations. Our results also seed discussion on how to balance obfuscation presets and performance cost. We envision OBsmith as an important step towards automated testing and quality assurance of obfuscators and other semantic-preserving toolchains.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10066v1",
    "published_date": "2025-10-11 07:02:42 UTC",
    "updated_date": "2025-10-11 07:02:42 UTC"
  },
  {
    "arxiv_id": "2510.10063v1",
    "title": "CLMN: Concept based Language Models via Neural Symbolic Reasoning",
    "authors": [
      "Yibo Yang"
    ],
    "abstract": "Deep learning has advanced NLP, but interpretability remains limited, especially in healthcare and finance. Concept bottleneck models tie predictions to human concepts in vision, but NLP versions either use binary activations that harm text representations or latent concepts that weaken semantics, and they rarely model dynamic concept interactions such as negation and context. We introduce the Concept Language Model Network (CLMN), a neural-symbolic framework that keeps both performance and interpretability. CLMN represents concepts as continuous, human-readable embeddings and applies fuzzy-logic reasoning to learn adaptive interaction rules that state how concepts affect each other and the final decision. The model augments original text features with concept-aware representations and automatically induces interpretable logic rules. Across multiple datasets and pre-trained language models, CLMN achieves higher accuracy than existing concept-based methods while improving explanation quality. These results show that integrating neural representations with symbolic reasoning in a unified concept space can yield practical, transparent NLP systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10063v1",
    "published_date": "2025-10-11 06:58:44 UTC",
    "updated_date": "2025-10-11 06:58:44 UTC"
  },
  {
    "arxiv_id": "2510.10060v1",
    "title": "Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling",
    "authors": [
      "Hehe Fan",
      "Yi Yang",
      "Mohan Kankanhalli",
      "Fei Wu"
    ],
    "abstract": "When modeling a given type of data, we consider it to involve two key aspects: 1) identifying relevant elements (e.g., image pixels or textual words) to a central element, as in a convolutional receptive field, or to a query element, as in self-attention, and 2) encoding these tokens effectively. Self-attention can adaptively identify these elements but relies on absolute positional embedding for structural representation learning. In contrast, convolution encodes elements in a relative manner, yet their fixed kernel size limits their ability to adaptively select the relevant elements. In this paper, we introduce Translution, an operation that unifies the adaptive identification capability of self-attention and the relative encoding advantage of convolution. However, this integration leads to a substantial increase in the number of parameters, exceeding most currently available computational resources. Therefore, we propose a lightweight variant of Translution, named α-Translution. Experiments on computer vision and natural language processing tasks show that Translution (including α-Translution) achieves superior accuracy compared to self-attention. The code is available at https://github.com/hehefan/Translution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "technical report",
    "pdf_url": "https://arxiv.org/pdf/2510.10060v1",
    "published_date": "2025-10-11 06:54:10 UTC",
    "updated_date": "2025-10-11 06:54:10 UTC"
  },
  {
    "arxiv_id": "2510.10052v1",
    "title": "Think Twice to See More: Iterative Visual Reasoning in Medical VLMs",
    "authors": [
      "Kaitao Chen",
      "Shaohao Rui",
      "Yankai Jiang",
      "Jiamin Wu",
      "Qihao Zheng",
      "Chunfeng Song",
      "Xiaosong Wang",
      "Mu Zhou",
      "Mianxin Liu"
    ],
    "abstract": "Medical vision-language models (VLMs) excel at image-text understanding but typically rely on a single-pass reasoning that neglects localized visual cues. In clinical practice, however, human experts iteratively scan, focus, and refine the regions of interest before reaching a final diagnosis. To narrow this machine-human perception gap, we introduce ViTAR, a novel VLM framework that emulates the iterative reasoning process of human experts through a cognitive chain of \"think-act-rethink-answer\". ViTAR treats medical images as interactive objects, enabling models to engage multi-step visual reasoning. To support this approach, we curate a high-quality instruction dataset comprising 1K interactive examples that encode expert-like diagnostic behaviors. In addition, a 16K visual question answering training data has been curated towards fine-grained visual diagnosis. We introduce a two-stage training strategy that begins with supervised fine-tuning to guide cognitive trajectories, followed by the reinforcement learning to optimize decision-making. Extensive evaluations demonstrate that ViTAR outperforms strong state-of-the-art models. Visual attention analysis reveals that from the \"think\" to \"rethink\" rounds, ViTAR increasingly anchors visual grounding to clinically critical regions and maintains high attention allocation to visual tokens during reasoning, providing mechanistic insight into its improved performance. These findings demonstrate that embedding expert-style iterative thinking chains into VLMs enhances both performance and trustworthiness of medical AI.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 21 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10052v1",
    "published_date": "2025-10-11 06:39:57 UTC",
    "updated_date": "2025-10-11 06:39:57 UTC"
  },
  {
    "arxiv_id": "2510.10049v1",
    "title": "ALLOY: Generating Reusable Agent Workflows from User Demonstration",
    "authors": [
      "Jiawen Li",
      "Zheng Ning",
      "Yuan Tian",
      "Toby Jia-jun Li"
    ],
    "abstract": "Large language models (LLMs) enable end-users to delegate complex tasks to autonomous agents through natural language. However, prompt-based interaction faces critical limitations: Users often struggle to specify procedural requirements for tasks, especially those that don't have a factually correct solution but instead rely on personal preferences, such as posting social media content or planning a trip. Additionally, a ''successful'' prompt for one task may not be reusable or generalizable across similar tasks. We present ALLOY, a system inspired by classical HCI theories on Programming by Demonstration (PBD), but extended to enhance adaptability in creating LLM-based web agents. ALLOY enables users to express procedural preferences through natural demonstrations rather than prompts, while making these procedures transparent and editable through visualized workflows that can be generalized across task variations. In a study with 12 participants, ALLOY's demonstration--based approach outperformed prompt-based agents and manual workflows in capturing user intent and procedural preferences in complex web tasks. Insights from the study also show how demonstration--based interaction complements the traditional prompt-based approach.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10049v1",
    "published_date": "2025-10-11 06:30:34 UTC",
    "updated_date": "2025-10-11 06:30:34 UTC"
  },
  {
    "arxiv_id": "2510.10047v1",
    "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning",
    "authors": [
      "Ruohao Li",
      "Hongjun Liu",
      "Leyi Zhao",
      "Zisu Li",
      "Jiawei Li",
      "Jiajun Jiang",
      "Linning Xu",
      "Chen Zhao",
      "Mingming Fan",
      "Chen Liang"
    ],
    "abstract": "Large language model (LLM) agents have shown remarkable reasoning abilities. However, existing multi-agent frameworks often rely on fixed roles or centralized control, limiting scalability and adaptability in long-horizon reasoning. We introduce SwarmSys, a closed-loop framework for distributed multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys emerges through iterative interactions among three specialized roles, Explorers, Workers, and Validators, that continuously cycle through exploration, exploitation, and validation. To enable scalable and adaptive collaboration, we integrate adaptive agent and event profiles, embedding-based probabilistic matching, and a pheromone-inspired reinforcement mechanism, supporting dynamic task allocation and self-organizing convergence without global supervision. Across symbolic reasoning, research synthesis, and scientific programming tasks, SwarmSys consistently outperforms baselines, improving both accuracy and reasoning stability. These findings highlight swarm-inspired coordination as a promising paradigm for scalable, robust, and adaptive multi-agent reasoning, suggesting that coordination scaling may rival model scaling in advancing LLM intelligence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.10047v1",
    "published_date": "2025-10-11 06:28:22 UTC",
    "updated_date": "2025-10-11 06:28:22 UTC"
  },
  {
    "arxiv_id": "2510.10042v1",
    "title": "Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation",
    "authors": [
      "Saleh Nikooroo",
      "Thomas Engel"
    ],
    "abstract": "Belief systems are rarely globally consistent, yet effective reasoning often persists locally. We propose a novel graph-theoretic framework that cleanly separates credibility--external, a priori trust in sources--from confidence--an internal, emergent valuation induced by network structure. Beliefs are nodes in a directed, signed, weighted graph whose edges encode support and contradiction. Confidence is obtained by a contractive propagation process that mixes a stated prior with structure-aware influence and guarantees a unique, stable solution. Within this dynamics, we define reasoning zones: high-confidence, structurally balanced subgraphs on which classical inference is safe despite global contradictions. We provide a near-linear procedure that seeds zones by confidence, tests balance using a parity-based coloring, and applies a greedy, locality-preserving repair with Jaccard de-duplication to build a compact atlas. To model belief change, we introduce shock updates that locally downscale support and elevate targeted contradictions while preserving contractivity via a simple backtracking rule. Re-propagation yields localized reconfiguration-zones may shrink, split, or collapse--without destabilizing the entire graph. We outline an empirical protocol on synthetic signed graphs with planted zones, reporting zone recovery, stability under shocks, and runtime. The result is a principled foundation for contradiction-tolerant reasoning that activates classical logic precisely where structure supports it.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10042v1",
    "published_date": "2025-10-11 06:02:00 UTC",
    "updated_date": "2025-10-11 06:02:00 UTC"
  },
  {
    "arxiv_id": "2510.10041v1",
    "title": "FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data Mpox Diagnosis",
    "authors": [
      "Sahng-Min Han",
      "Minjae Kim",
      "Jinho Cha",
      "Se-woon Choe",
      "Eunchan Daniel Cha",
      "Jungwon Choi",
      "Kyudong Jung"
    ],
    "abstract": "Deep learning in small and imbalanced biomedical datasets remains fundamentally constrained by unstable optimization and poor generalization. We present the first biomedical implementation of FOSSIL (Flexible Optimization via Sample-Sensitive Importance Learning), a regret-minimizing weighting framework that adaptively balances training emphasis according to sample difficulty. Using softmax-based uncertainty as a continuous measure of difficulty, we construct a four-stage curriculum (Easy-Very Hard) and integrate FOSSIL into both convolutional and transformer-based architectures for Mpox skin lesion diagnosis. Across all settings, FOSSIL substantially improves discrimination (AUC = 0.9573), calibration (ECE = 0.053), and robustness under real-world perturbations, outperforming conventional baselines without metadata, manual curation, or synthetic augmentation. The results position FOSSIL as a generalizable, data-efficient, and interpretable framework for difficulty-aware learning in medical imaging under data scarcity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages, 11 figures, submitted to Computers in Biology and Medicine (Elsevier, under review)",
    "pdf_url": "https://arxiv.org/pdf/2510.10041v1",
    "published_date": "2025-10-11 06:00:59 UTC",
    "updated_date": "2025-10-11 06:00:59 UTC"
  },
  {
    "arxiv_id": "2510.12827v1",
    "title": "Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation",
    "authors": [
      "Md. Nayeem",
      "Md Shamse Tabrej",
      "Kabbojit Jit Deb",
      "Shaonti Goswami",
      "Md. Azizul Hakim"
    ],
    "abstract": "Automatic Speech Recognition (ASR) has undergone a profound transformation over the past decade, driven by advances in deep learning. This survey provides a comprehensive overview of the modern era of ASR, charting its evolution from traditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models (GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant end-to-end neural architectures. We systematically review the foundational end-to-end paradigms: Connectionist Temporal Classification (CTC), attention-based encoder-decoder models, and the Recurrent Neural Network Transducer (RNN-T), which established the groundwork for fully integrated speech-to-text systems. We then detail the subsequent architectural shift towards Transformer and Conformer models, which leverage self-attention to capture long-range dependencies with high computational efficiency. A central theme of this survey is the parallel revolution in training paradigms. We examine the progression from fully supervised learning, augmented by techniques like SpecAugment, to the rise of self-supervised learning (SSL) with foundation models such as wav2vec 2.0, which drastically reduce the reliance on transcribed data. Furthermore, we analyze the impact of largescale, weakly supervised models like Whisper, which achieve unprecedented robustness through massive data diversity. The paper also covers essential ecosystem components, including key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME), standard evaluation metrics (e.g., Word Error Rate), and critical considerations for real-world deployment, such as streaming inference, on-device efficiency, and the ethical imperatives of fairness and robustness. We conclude by outlining open challenges and future research directions.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.12827v1",
    "published_date": "2025-10-11 05:38:45 UTC",
    "updated_date": "2025-10-11 05:38:45 UTC"
  },
  {
    "arxiv_id": "2510.10035v1",
    "title": "Failure-Driven Workflow Refinement",
    "authors": [
      "Jusheng Zhang",
      "Kaitong Cai",
      "Qinglin Zeng",
      "Ningyuan Liu",
      "Stephen Fan",
      "Ziliang Chen",
      "Keze Wang"
    ],
    "abstract": "Optimizing LLM-based workflows is typically formulated as a global search, where candidate workflows are evaluated based on a scalar metric. This paradigm, however, suffers from a critical flaw: information collapse. By reducing rich, multi-step execution traces to simple success/failure signals, existing methods are rendered blind to the underlying structure of failures, fundamentally preventing them from modeling the workflow's failure distribution. We reconceptualize this challenge as a distributional problem. We propose a new paradigm where the optimization goal is not to maximize a scalar score, but to directly minimize a workflow's Expected Failure Mass, i.e., the integral of its failure probability density function defined over a high-dimensional Failure Signature Space (FSS). This distributional lens allows us to move from inefficient, zero-order optimization to a principled, gradient-like descent on the failure landscape itself. We introduce CE-Graph, a framework that operationalizes this paradigm through a novel, failure-driven refinement process. CE-Graph approximates the failure distribution from a pool of counterexamples, identifies its densest regions as recurring failure modes, and applies targeted, operator-constrained graph edits via a Propose-and-Verify mechanism to greedily reduce the failure mass. On math, code, and QA benchmarks, our CE-Graph achieves higher robustness at a significantly lower cost than strong baselines. This suggests that a system's reliability emerges not from avoiding failures, but from systematically learning and reshaping the geometric structure of its failure distributions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10035v1",
    "published_date": "2025-10-11 05:37:10 UTC",
    "updated_date": "2025-10-11 05:37:10 UTC"
  },
  {
    "arxiv_id": "2510.10028v1",
    "title": "Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization",
    "authors": [
      "Yang Li",
      "Ruichen Zhang",
      "Yinqiu Liu",
      "Guangyuan Liu",
      "Dusit Niyato",
      "Abbas Jamalipour",
      "Xianbin Wang",
      "Dong In Kim"
    ],
    "abstract": "The rapid advancement of Low-Altitude Economy Networks (LAENets) has enabled a variety of applications, including aerial surveillance, environmental sensing, and semantic data collection. To support these scenarios, unmanned aerial vehicles (UAVs) equipped with onboard vision-language models (VLMs) offer a promising solution for real-time multimodal inference. However, ensuring both inference accuracy and communication efficiency remains a significant challenge due to limited onboard resources and dynamic network conditions. In this paper, we first propose a UAV-enabled LAENet system model that jointly captures UAV mobility, user-UAV communication, and the onboard visual question answering (VQA) pipeline. Based on this model, we formulate a mixed-integer non-convex optimization problem to minimize task latency and power consumption under user-specific accuracy constraints. To solve the problem, we design a hierarchical optimization framework composed of two parts: (i) an Alternating Resolution and Power Optimization (ARPO) algorithm for resource allocation under accuracy constraints, and (ii) a Large Language Model-augmented Reinforcement Learning Approach (LLaRA) for adaptive UAV trajectory optimization. The large language model (LLM) serves as an expert in refining reward design of reinforcement learning in an offline fashion, introducing no additional latency in real-time decision-making. Numerical results demonstrate the efficacy of our proposed framework in improving inference performance and communication efficiency under dynamic LAENet conditions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10028v1",
    "published_date": "2025-10-11 05:11:21 UTC",
    "updated_date": "2025-10-11 05:11:21 UTC"
  },
  {
    "arxiv_id": "2510.10025v2",
    "title": "Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default",
    "authors": [
      "Jiaqi Liu",
      "Tong Wang",
      "Su Liu",
      "Xin Hu",
      "Ran Tong",
      "Lanruo Wang",
      "Jiexi Xu"
    ],
    "abstract": "The research evaluates lightweight medical abstract classification methods to establish their maximum performance capabilities under financial budget restrictions. On the public medical abstracts corpus, we finetune BERT base and Distil BERT with three objectives cross entropy (CE), class weighted CE, and focal loss under identical tokenization, sequence length, optimizer, and schedule. DistilBERT with plain CE gives the strongest raw argmax trade off, while a post hoc operating point selection (validation calibrated, classwise thresholds) sub stantially improves deployed performance; under this tuned regime, focal benefits most. We report Accuracy, Macro F1, and WeightedF1, release evaluation artifacts, and include confusion analyses to clarify error structure. The practical takeaway is to start with a compact encoder and CE, then add lightweight calibration or thresholding when deployment requires higher macro balance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Healthcare AI, Medical Text Classification,LLM, DistilBERT",
    "pdf_url": "https://arxiv.org/pdf/2510.10025v2",
    "published_date": "2025-10-11 05:05:21 UTC",
    "updated_date": "2025-10-21 14:44:14 UTC"
  },
  {
    "arxiv_id": "2510.10023v1",
    "title": "Skill-Targeted Adaptive Training",
    "authors": [
      "Yinghui He",
      "Abhishek Panigrahi",
      "Yong Lin",
      "Sanjeev Arora"
    ],
    "abstract": "Language models often show little to no improvement (i.e., \"saturation\") when trained via vanilla supervised fine-tuning (SFT) on data similar to what they saw in their training set (e.g., MATH). We introduce a new fine-tuning strategy, STAT, to train such a student model by using the metacognition ability of a stronger large language model (LLM) as the teacher. The teacher uses the task dataset to create a list of skills needed for the task, and then labels each data point with its required skills (Didolkar et al., 2024). By monitoring the student's answers, the teacher creates a Missing-Skill-Profile for the student, tracking how often they failed to apply each skill in their responses. We use this idea to build a modified training set in one of two ways. In STAT-Sel, the teacher uses an existing set of training examples but adaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn, the teacher synthesizes additional examples involving missing skills. Across extensive experiments on Llama and Qwen models, our methods yield improvements of up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore, STAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25, AMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is complementary to RL via GRPO (Shao et al., 2024): after the model is improved using STAT to address skill gaps, GRPO continues to add further gains. We conclude that skill-targeted adaptive training should broadly improve current training pipelines. Our code is available at: https://github.com/princeton-pli/STAT.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10023v1",
    "published_date": "2025-10-11 05:02:36 UTC",
    "updated_date": "2025-10-11 05:02:36 UTC"
  },
  {
    "arxiv_id": "2510.12826v1",
    "title": "Scheming Ability in LLM-to-LLM Strategic Interactions",
    "authors": [
      "Thao Pham"
    ],
    "abstract": "As large language model (LLM) agents are deployed autonomously in diverse contexts, evaluating their capacity for strategic deception becomes crucial. While recent research has examined how AI systems scheme against human developers, LLM-to-LLM scheming remains underexplored. We investigate the scheming ability and propensity of frontier LLM agents through two game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro, Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and without explicit prompting while analyzing scheming tactics through chain-of-thought reasoning. When prompted, most models, especially Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance. Critically, models exhibited significant scheming propensity without prompting: all models chose deception over confession in Peer Evaluation (100% rate), while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These findings highlight the need for robust evaluations using high-stakes game-theoretic scenarios in multi-agent settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages, 13 figures, under review at IASEAI'26",
    "pdf_url": "https://arxiv.org/pdf/2510.12826v1",
    "published_date": "2025-10-11 04:42:29 UTC",
    "updated_date": "2025-10-11 04:42:29 UTC"
  },
  {
    "arxiv_id": "2510.10010v1",
    "title": "SLEAN: Simple Lightweight Ensemble Analysis Network for Multi-Provider LLM Coordination: Design, Implementation, and Vibe Coding Bug Investigation Case Study",
    "authors": [
      "Matheus J. T. Vargas"
    ],
    "abstract": "We present SLEAN (Simple Lightweight Ensemble Analysis Network), a deterministic framework for coordinating multiple LLM providers through text-based prompt orchestration. Unlike complex multi-agent systems requiring specialized infrastructure, SLEAN operates as a simple prompt bridge between LLMs using .txt templates, requiring no deep technical knowledge for deployment. The three-phase protocol formed by independent analysis, cross-critique, and arbitration, filters harmful AI-generated code suggestions before production deployment, addressing how AI-assisted debugging increasingly produces modifications that introduce unnecessary complexity, break existing functionality, or address problems. Evaluating 15 software bugs, we analyzed 69 AI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95% CI 20.9-42.9%) while rejecting 47 that would have been harmful if applied verbatim. The arbitration process reduced code change surface by 83-90% relative to raw AI outputs, enforcing minimal causal edits over scope-expanding modifications. Minimal Type 2 inputs proved more efficient than detailed Type 1 inputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus 28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems showed weak correlation with fix quality: high convergence (at least 80%) occurred in 4 of 15 cases and improved acceptance by only 2.4% points; arbitration appeared only at exactly 10% convergence in 2 of 15 cases, although low convergence alone did not necessitate arbitration. The file-driven, provider-agnostic architecture enables deployment without specialized coding expertise, making it applicable to security auditing, code review, document verification, and other domains requiring reliable multi-provider synthesis with end-to-end auditability.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "14 pages, 4 figures, 6 tables, link to code repo",
    "pdf_url": "https://arxiv.org/pdf/2510.10010v1",
    "published_date": "2025-10-11 04:24:04 UTC",
    "updated_date": "2025-10-11 04:24:04 UTC"
  },
  {
    "arxiv_id": "2510.10009v1",
    "title": "Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning",
    "authors": [
      "Shu Zhao",
      "Tan Yu",
      "Anbang Xu"
    ],
    "abstract": "Reasoning-augmented search agents, such as Search-R1, are trained to reason, search, and generate the final answer iteratively. Nevertheless, due to their limited capabilities in reasoning and search, their performance on multi-hop QA benchmarks remains far from satisfactory. To handle complex or compound queries, we train an LLM-based search agent with the native capability of query expansion through reinforcement learning. In each turn, our search agent proposes several query variants, which are searched simultaneously to cover more relevant information. Meanwhile, given limited post-training data and computing resources, it is very challenging for a search agent to master multiple tasks, including query generation, retrieved information understanding, and answer generation. Therefore, we propose incorporating a pre-trained squeezer model that helps the search agent understand the retrieved documents, allowing the search agent to focus on query generation for high retrieval recall. With the assistance of the squeezer model, we discover that even a small-scale 3B LLM can demonstrate a strong capability of query expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks. To be specific, our experiments across seven question-answering benchmarks demonstrate that our method, named ExpandSearch, achieves an average improvement of 4.4% compared to state-of-the-art baselines, with strong gains on multi-hop reasoning tasks requiring diverse evidence aggregation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10009v1",
    "published_date": "2025-10-11 04:23:30 UTC",
    "updated_date": "2025-10-11 04:23:30 UTC"
  },
  {
    "arxiv_id": "2510.10008v2",
    "title": "RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning",
    "authors": [
      "Meng Xi",
      "Sihan Lv",
      "Yechen Jin",
      "Guanjie Cheng",
      "Naibo Wang",
      "Ying Li",
      "Jianwei Yin"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems based on Large Language Models (LLMs) have become a core technology for tasks such as question-answering (QA) and content generation. RAG poisoning is an attack method to induce LLMs to generate the attacker's expected text by injecting poisoned documents into the database of RAG systems. Existing research can be broadly divided into two classes: white-box methods and black-box methods. White-box methods utilize gradient information to optimize poisoned documents, and black-box methods use a pre-trained LLM to generate them. However, existing white-box methods require knowledge of the RAG system's internal composition and implementation details, whereas black-box methods are unable to utilize interactive information. In this work, we propose the RIPRAG attack framework, an end-to-end attack pipeline that treats the target RAG system as a black box and leverages our proposed Reinforcement Learning from Black-box Feedback (RLBF) method to optimize the generation model for poisoned documents. We designed two kinds of rewards: similarity reward and attack reward. Experimental results demonstrate that this method can effectively execute poisoning attacks against most complex RAG systems, achieving an attack success rate (ASR) improvement of up to 0.72 compared to baseline methods. This highlights prevalent deficiencies in current defensive methods and provides critical insights for LLM security research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10008v2",
    "published_date": "2025-10-11 04:23:20 UTC",
    "updated_date": "2026-01-11 09:47:43 UTC"
  },
  {
    "arxiv_id": "2510.10002v1",
    "title": "Deliberative Dynamics and Value Alignment in LLM Debates",
    "authors": [
      "Pratik S. Sachdeva",
      "Tom van Nuenen"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed in sensitive everyday contexts - offering personal advice, mental health support, and moral guidance - understanding their elicited values in navigating complex moral reasoning is essential. Most evaluations study this sociotechnical alignment through single-turn prompts, but it is unclear if these findings extend to multi-turn settings where values emerge through dialogue, revision, and consensus. We address this gap using LLM debate to examine deliberative dynamics and value alignment in multi-turn settings by prompting subsets of three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively assign blame in 1,000 everyday dilemmas from Reddit's \"Am I the Asshole\" community. We use both synchronous (parallel responses) and round-robin (sequential responses) formats to test order effects and verdict revision. Our findings show striking behavioral differences. In the synchronous setting, GPT showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were far more flexible (28-41%). Value patterns also diverged: GPT emphasized personal autonomy and direct communication, while Claude and Gemini prioritized empathetic dialogue. Certain values proved especially effective at driving verdict changes. We further find that deliberation format had a strong impact on model behavior: GPT and Gemini stood out as highly conforming relative to Claude, with their verdict behavior strongly shaped by order effects. These results show how deliberation format and model-specific behaviors shape moral reasoning in multi-turn interactions, underscoring that sociotechnical alignment depends on how systems structure dialogue as much as on their outputs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.10002v1",
    "published_date": "2025-10-11 04:06:07 UTC",
    "updated_date": "2025-10-11 04:06:07 UTC"
  },
  {
    "arxiv_id": "2510.09979v1",
    "title": "Neuro-inspired automated lens design",
    "authors": [
      "Yao Gao",
      "Lei Sun",
      "Shaohua Gao",
      "Qi Jiang",
      "Kailun Yang",
      "Weijian Hu",
      "Xiaolong Qian",
      "Wenyong Li",
      "Luc Van Gool",
      "Kaiwei Wang"
    ],
    "abstract": "The highly non-convex optimization landscape of modern lens design necessitates extensive human expertise, resulting in inefficiency and constrained design diversity. While automated methods are desirable, existing approaches remain limited to simple tasks or produce complex lenses with suboptimal image quality. Drawing inspiration from the synaptic pruning mechanism in mammalian neural development, this study proposes OptiNeuro--a novel automated lens design framework that first generates diverse initial structures and then progressively eliminates low-performance lenses while refining remaining candidates through gradient-based optimization. By fully automating the design of complex aspheric imaging lenses, OptiNeuro demonstrates quasi-human-level performance, identifying multiple viable candidates with minimal human intervention. This advancement not only enhances the automation level and efficiency of lens design but also facilitates the exploration of previously uncharted lens architectures.",
    "categories": [
      "physics.optics",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.optics",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09979v1",
    "published_date": "2025-10-11 03:14:56 UTC",
    "updated_date": "2025-10-11 03:14:56 UTC"
  },
  {
    "arxiv_id": "2512.05119v1",
    "title": "RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering",
    "authors": [
      "Rongyang Zhang",
      "Yuqing Huang",
      "Chengqiang Lu",
      "Qimeng Wang",
      "Yan Gao",
      "Yi Wu",
      "Yao Hu",
      "Yin Xu",
      "Wei Wang",
      "Hao Wang",
      "Enhong Chen"
    ],
    "abstract": "In real-world scenarios, providing user queries with visually enhanced responses can considerably benefit understanding and memory, underscoring the great value of interleaved image-text generation. Despite recent progress, like the visual autoregressive model that unifies text and image processing in a single transformer architecture, generating high-quality interleaved content remains challenging. Moreover, evaluations of these interleaved sequences largely remain underexplored, with existing benchmarks often limited by unimodal metrics that inadequately assess the intricacies of combined image-text outputs. To address these issues, we present RAG-IGBench, a thorough benchmark designed specifically to evaluate the task of Interleaved Generation based on Retrieval-Augmented Generation (RAG-IG) in open-domain question answering. RAG-IG integrates multimodal large language models (MLLMs) with retrieval mechanisms, enabling the models to access external image-text information for generating coherent multimodal content. Distinct from previous datasets, RAG-IGBench draws on the latest publicly available content from social platforms and introduces innovative evaluation metrics that measure the quality of text and images, as well as their consistency. Through extensive experiments with state-of-the-art MLLMs (both open-source and proprietary) on RAG-IGBench, we provide an in-depth analysis examining the capabilities and limitations of these models. Additionally, we validate our evaluation metrics by demonstrating their high correlation with human assessments. Models fine-tuned on RAG-IGBench's training set exhibit improved performance across multiple benchmarks, confirming both the quality and practical utility of our dataset. Our benchmark is available at https://github.com/USTC-StarTeam/RAG-IGBench.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "26 pages, 6 figures, NeurIPS 2025 D&B Track poster",
    "pdf_url": "https://arxiv.org/pdf/2512.05119v1",
    "published_date": "2025-10-11 03:06:39 UTC",
    "updated_date": "2025-10-11 03:06:39 UTC"
  },
  {
    "arxiv_id": "2510.09970v1",
    "title": "Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs",
    "authors": [
      "Olivia Peiyu Wang",
      "Tashvi Bansal",
      "Ryan Bai",
      "Emily M. Chui",
      "Leilani H. Gilpin"
    ],
    "abstract": "Large Language Models (LLMs) suffer from critical reasoning gaps, including a tendency to hallucinate and poor accuracy in classifying logical fallacies. This limitation stems from their default System 1 processing, which is fast and intuitive, whereas reliable reasoning requires the deliberate, effortful System 2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is often prohibitively expensive, we explore a low-cost, instruction-based intervention to bridge this gap. Our methodology introduces a novel stepwise instruction dataset that decomposes fallacy classification into a series of atomic procedural steps (simple binary questions). We further augment this with a final verification step where models consult a relational knowledge graph of related fallacies. This procedural, rule-based intervention yields a significant improvement in LLM logical fallacy classification. Crucially, the approach also provides enhanced transparency into the LLMs' decision-making, highlighting a practical pathway for Neuro-symbolic architectures to address LLM reasoning deficits.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as a poster at the Twelfth Annual Conference on Advances in Cognitive Systems. 21 pages, 7 figures and 1 table",
    "pdf_url": "https://arxiv.org/pdf/2510.09970v1",
    "published_date": "2025-10-11 03:02:11 UTC",
    "updated_date": "2025-10-11 03:02:11 UTC"
  },
  {
    "arxiv_id": "2510.09968v1",
    "title": "Operationalizing AI: Empirical Evidence on MLOps Practices, User Satisfaction, and Organizational Context",
    "authors": [
      "Stefan Pasch"
    ],
    "abstract": "Organizational efforts to utilize and operationalize artificial intelligence (AI) are often accompanied by substantial challenges, including scalability, maintenance, and coordination across teams. In response, the concept of Machine Learning Operations (MLOps) has emerged as a set of best practices that integrate software engineering principles with the unique demands of managing the ML lifecycle. Yet, empirical evidence on whether and how these practices support users in developing and operationalizing AI applications remains limited. To address this gap, this study analyzes over 8,000 user reviews of AI development platforms from G2.com. Using zero-shot classification, we measure review sentiment toward nine established MLOps practices, including continuous integration and delivery (CI/CD), workflow orchestration, reproducibility, versioning, collaboration, and monitoring. Seven of the nine practices show a significant positive relationship with user satisfaction, suggesting that effective MLOps implementation contributes tangible value to AI development. However, organizational context also matters: reviewers from small firms discuss certain MLOps practices less frequently, suggesting that organizational context influences the prevalence and salience of MLOps, though firm size does not moderate the MLOps-satisfaction link. This indicates that once applied, MLOps practices are perceived as universally beneficial across organizational settings.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09968v1",
    "published_date": "2025-10-11 02:57:14 UTC",
    "updated_date": "2025-10-11 02:57:14 UTC"
  },
  {
    "arxiv_id": "2510.09965v1",
    "title": "Homomorphic Mappings for Value-Preserving State Aggregation in Markov Decision Processes",
    "authors": [
      "Shuo Zhao",
      "Yongqiang Li",
      "Yu Feng",
      "Zhongsheng Hou",
      "Yuanjing Feng"
    ],
    "abstract": "State aggregation aims to reduce the computational complexity of solving Markov Decision Processes (MDPs) while preserving the performance of the original system. A fundamental challenge lies in optimizing policies within the aggregated, or abstract, space such that the performance remains optimal in the ground MDP-a property referred to as {\"}optimal policy equivalence {\"}.\n  This paper presents an abstraction framework based on the notion of homomorphism, in which two Markov chains are deemed homomorphic if their value functions exhibit a linear relationship. Within this theoretical framework, we establish a sufficient condition for the equivalence of optimal policy.\n  We further examine scenarios where the sufficient condition is not met and derive an upper bound on the approximation error and a performance lower bound for the objective function under the ground MDP. We propose Homomorphic Policy Gradient (HPG), which guarantees optimal policy equivalence under sufficient conditions, and its extension, Error-Bounded HPG (EBHPG), which balances computational efficiency and the performance loss induced by aggregation. In the experiments, we validated the theoretical results and conducted comparative evaluations against seven algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09965v1",
    "published_date": "2025-10-11 02:40:03 UTC",
    "updated_date": "2025-10-11 02:40:03 UTC"
  },
  {
    "arxiv_id": "2510.09947v2",
    "title": "Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation",
    "authors": [
      "Mir Tafseer Nayeem",
      "Sawsan Alqahtani",
      "Md Tahmid Rahman Laskar",
      "Tasnim Mohiuddin",
      "M Saiful Bari"
    ],
    "abstract": "Tokenization is a crucial but under-evaluated step in large language models (LLMs). The standard metric, fertility (the average number of tokens per word), captures compression efficiency but obscures how vocabularies are allocated across languages and domains. We analyze six widely used tokenizers across seven languages and two domains, finding stable fertility for English, high fertility for Chinese, and little domain sensitivity. To address fertility's blind spots, we propose the Single Token Retention Rate (STRR), which measures the proportion of words preserved as single tokens. STRR reveals systematic prioritization of English, strong support for Chinese, and fragmentation in Hindi, offering an interpretable view of cross-lingual fairness. Our results show that STRR complements fertility and provides practical guidance for designing more equitable multilingual tokenizers.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2025 Workshop",
    "pdf_url": "https://arxiv.org/pdf/2510.09947v2",
    "published_date": "2025-10-11 01:22:31 UTC",
    "updated_date": "2025-10-26 01:32:06 UTC"
  },
  {
    "arxiv_id": "2510.09945v1",
    "title": "Explainable Human-in-the-Loop Segmentation via Critic Feedback Signals",
    "authors": [
      "Pouya Shaeri",
      "Ryan T. Woo",
      "Yasaman Mohammadpour",
      "Ariane Middel"
    ],
    "abstract": "Segmentation models achieve high accuracy on benchmarks but often fail in real-world domains by relying on spurious correlations instead of true object boundaries. We propose a human-in-the-loop interactive framework that enables interventional learning through targeted human corrections of segmentation outputs. Our approach treats human corrections as interventional signals that show when reliance on superficial features (e.g., color or texture) is inappropriate. The system learns from these interventions by propagating correction-informed edits across visually similar images, effectively steering the model toward robust, semantically meaningful features rather than dataset-specific artifacts. Unlike traditional annotation approaches that simply provide more training data, our method explicitly identifies when and why the model fails and then systematically corrects these failure modes across the entire dataset. Through iterative human feedback, the system develops increasingly robust representations that generalize better to novel domains and resist artifactual correlations. We demonstrate that our framework improves segmentation accuracy by up to 9 mIoU points (12-15\\% relative improvement) on challenging cubemap data and yields 3-4$\\times$ reductions in annotation effort compared to standard retraining, while maintaining competitive performance on benchmark datasets. This work provides a practical framework for researchers and practitioners seeking to build segmentation systems that are accurate, robust to dataset biases, data-efficient, and adaptable to real-world domains such as urban climate monitoring and autonomous driving.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to a computer vision conference (under review)",
    "pdf_url": "https://arxiv.org/pdf/2510.09945v1",
    "published_date": "2025-10-11 01:16:41 UTC",
    "updated_date": "2025-10-11 01:16:41 UTC"
  },
  {
    "arxiv_id": "2510.09942v1",
    "title": "Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding",
    "authors": [
      "Payel Bhattacharjee",
      "Fengwei Tian",
      "Meiyu Zhong",
      "Guangyi Zhang",
      "Osvaldo Simeone",
      "Ravi Tandon"
    ],
    "abstract": "Edge-cloud speculative decoding (SD) accelerates inference by having a cloud-based large language model (LLM) that verifies draft tokens generated by a resource-constrained small language model (SLM) at the edge. A central bottleneck is the limited bandwidth of the edge-cloud link, which necessitates efficient compression of draft token distributions. We first derive an information-theoretic bound that decomposes the token rejection rate into contributions from SLM-LLM distribution mismatch and from quantization distortion. Guided by this analysis, we propose the Sparse Quantize-and-Sample SD (SQS-SD) framework, which exploits distributional sparsity through structured sparsification and lattice-based quantization. Within this framework, K-SQS applies fixed top-K truncation, while C-SQS adaptively adjusts the retained token set via online conformal prediction to ensure bounded deviation from the dense distribution. Empirical results confirm that both approaches improve end-to-end latency and rejection rates in complimentary operating regimes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: AI and ML for Next-Generation Wireless Communications and Networking (AI4NextG)",
    "pdf_url": "https://arxiv.org/pdf/2510.09942v1",
    "published_date": "2025-10-11 00:56:21 UTC",
    "updated_date": "2025-10-11 00:56:21 UTC"
  },
  {
    "arxiv_id": "2510.09935v1",
    "title": "Unpacking Hateful Memes: Presupposed Context and False Claims",
    "authors": [
      "Weibin Cai",
      "Jiayu Li",
      "Reza Zafarani"
    ],
    "abstract": "While memes are often humorous, they are frequently used to disseminate hate, causing serious harm to individuals and society. Current approaches to hateful meme detection mainly rely on pre-trained language models. However, less focus has been dedicated to \\textit{what make a meme hateful}. Drawing on insights from philosophy and psychology, we argue that hateful memes are characterized by two essential features: a \\textbf{presupposed context} and the expression of \\textbf{false claims}. To capture presupposed context, we develop \\textbf{PCM} for modeling contextual information across modalities. To detect false claims, we introduce the \\textbf{FACT} module, which integrates external knowledge and harnesses cross-modal reference graphs. By combining PCM and FACT, we introduce \\textbf{\\textsf{SHIELD}}, a hateful meme detection framework designed to capture the fundamental nature of hate. Extensive experiments show that SHIELD outperforms state-of-the-art methods across datasets and metrics, while demonstrating versatility on other tasks, such as fake news detection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09935v1",
    "published_date": "2025-10-11 00:25:27 UTC",
    "updated_date": "2025-10-11 00:25:27 UTC"
  },
  {
    "arxiv_id": "2510.09934v1",
    "title": "Denoising Diffusion as a New Framework for Underwater Images",
    "authors": [
      "Nilesh Jain",
      "Elie Alhajjar"
    ],
    "abstract": "Underwater images play a crucial role in ocean research and marine environmental monitoring since they provide quality information about the ecosystem. However, the complex and remote nature of the environment results in poor image quality with issues such as low visibility, blurry textures, color distortion, and noise. In recent years, research in image enhancement has proven to be effective but also presents its own limitations, like poor generalization and heavy reliance on clean datasets. One of the challenges herein is the lack of diversity and the low quality of images included in these datasets. Also, most existing datasets consist only of monocular images, a fact that limits the representation of different lighting conditions and angles. In this paper, we propose a new plan of action to overcome these limitations. On one hand, we call for expanding the datasets using a denoising diffusion model to include a variety of image types such as stereo, wide-angled, macro, and close-up images. On the other hand, we recommend enhancing the images using Controlnet to evaluate and increase the quality of the corresponding datasets, and hence improve the study of the marine ecosystem.\n  Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09934v1",
    "published_date": "2025-10-11 00:22:32 UTC",
    "updated_date": "2025-10-11 00:22:32 UTC"
  },
  {
    "arxiv_id": "2510.13837v1",
    "title": "Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection",
    "authors": [
      "Weibin Cai",
      "Reza Zafarani"
    ],
    "abstract": "Hate speech detection has been extensively studied, yet existing methods often overlook a real-world complexity: training labels are biased, and interpretations of what is considered hate vary across individuals with different cultural backgrounds. We first analyze these challenges, including data sparsity, cultural entanglement, and ambiguous labeling. To address them, we propose a culture-aware framework that constructs individuals' hate subspaces. To alleviate data sparsity, we model combinations of cultural attributes. For cultural entanglement and ambiguous labels, we use label propagation to capture distinctive features of each combination. Finally, individual hate subspaces, which in turn can further enhance classification performance. Experiments show our method outperforms state-of-the-art by 1.05\\% on average across all metrics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13837v1",
    "published_date": "2025-10-11 00:07:20 UTC",
    "updated_date": "2025-10-11 00:07:20 UTC"
  },
  {
    "arxiv_id": "2510.09930v1",
    "title": "MemPromptTSS: Persistent Prompt Memory for Iterative Multi-Granularity Time Series State Segmentation",
    "authors": [
      "Ching Chang",
      "Ming-Chih Lo",
      "Chiao-Tung Chan",
      "Wen-Chih Peng",
      "Tien-Fu Chen"
    ],
    "abstract": "Web platforms, mobile applications, and connected sensing systems generate multivariate time series with states at multiple levels of granularity, from coarse regimes to fine-grained events. Effective segmentation in these settings requires integrating across granularities while supporting iterative refinement through sparse prompt signals, which provide a compact mechanism for injecting domain knowledge. Yet existing prompting approaches for time series segmentation operate only within local contexts, so the effect of a prompt quickly fades and cannot guide predictions across the entire sequence. To overcome this limitation, we propose MemPromptTSS, a framework for iterative multi-granularity segmentation that introduces persistent prompt memory. A memory encoder transforms prompts and their surrounding subsequences into memory tokens stored in a bank. This persistent memory enables each new prediction to condition not only on local cues but also on all prompts accumulated across iterations, ensuring their influence persists across the entire sequence. Experiments on six datasets covering wearable sensing and industrial monitoring show that MemPromptTSS achieves 23% and 85% accuracy improvements over the best baseline in single- and multi-granularity segmentation under single iteration inference, and provides stronger refinement in iterative inference with average per-iteration gains of 2.66 percentage points compared to 1.19 for PromptTSS. These results highlight the importance of persistent memory for prompt-guided segmentation, establishing MemPromptTSS as a practical and effective framework for real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is currently under review. The code will be made available upon acceptance",
    "pdf_url": "https://arxiv.org/pdf/2510.09930v1",
    "published_date": "2025-10-11 00:02:36 UTC",
    "updated_date": "2025-10-11 00:02:36 UTC"
  }
]
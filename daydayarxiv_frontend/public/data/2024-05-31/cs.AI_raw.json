[
  {
    "arxiv_id": "2406.04369v1",
    "title": "RAG Does Not Work for Enterprises",
    "authors": [
      "Tilmann Bruckhaus"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) improves the accuracy and relevance of\nlarge language model outputs by incorporating knowledge retrieval. However,\nimplementing RAG in enterprises poses challenges around data security,\naccuracy, scalability, and integration. This paper explores the unique\nrequirements for enterprise RAG, surveys current approaches and limitations,\nand discusses potential advances in semantic search, hybrid queries, and\noptimized retrieval. It proposes an evaluation framework to validate enterprise\nRAG solutions, including quantitative testing, qualitative analysis, ablation\nstudies, and industry case studies. This framework aims to help demonstrate the\nability of purpose-built RAG architectures to deliver accuracy and relevance\nimprovements with enterprise-grade security, compliance and integration. The\npaper concludes with implications for enterprise deployments, limitations, and\nfuture research directions. Close collaboration between researchers and\nindustry partners may accelerate progress in developing and deploying\nretrieval-augmented generation technology.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04369v1",
    "published_date": "2024-05-31 23:30:52 UTC",
    "updated_date": "2024-05-31 23:30:52 UTC"
  },
  {
    "arxiv_id": "2406.00231v2",
    "title": "LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking",
    "authors": [
      "Yifan Zeng",
      "Ojas Tendolkar",
      "Raymond Baartmans",
      "Qingyun Wu",
      "Lizhong Chen",
      "Huazheng Wang"
    ],
    "abstract": "Ranking passages by prompting a large language model (LLM) can achieve\npromising performance in modern information retrieval (IR) systems. A common\napproach to sort the ranking list is by prompting LLMs for a pairwise or\nsetwise comparison which often relies on sorting algorithms. However,\nsorting-based methods require consistent comparisons to correctly sort the\npassages, which we show that LLMs often violate. We identify two kinds of\nintrinsic inconsistency in LLM-based pairwise comparisons: order inconsistency\nwhich leads to conflicting results when switching the passage order, and\ntransitive inconsistency which leads to non-transitive triads among all\npreference pairs. Our study of these inconsistencies is relevant for\nunderstanding and improving the stability of any ranking scheme based on\nrelative preferences. In this paper, we propose LLM-RankFusion, an LLM-based\nranking framework that mitigates these inconsistencies and produces a robust\nranking list. LLM-RankFusion mitigates order inconsistency using in-context\nlearning (ICL) to demonstrate order-agnostic comparisons and calibration to\nestimate the underlying preference probability between two passages. We then\naddress transitive inconsistency by aggregating the ranking results from\nmultiple rankers. In our experiments, we empirically show that LLM-RankFusion\ncan significantly reduce inconsistent comparison results, improving the ranking\nquality by making the final ranking list more robust. Our code is available at\n\\href{https://github.com/XHMY/LLM-RankFusion}{https://github.com/XHMY/LLM-RankFusion}",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00231v2",
    "published_date": "2024-05-31 23:29:42 UTC",
    "updated_date": "2024-11-26 08:37:54 UTC"
  },
  {
    "arxiv_id": "2406.00222v1",
    "title": "Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training",
    "authors": [
      "Maximillian Chen",
      "Ruoxi Sun",
      "Sercan Ö. Arık",
      "Tomas Pfister"
    ],
    "abstract": "Large language models (LLMs) aligned through reinforcement learning from\nhuman feedback (RLHF) have quickly become one of the dominant paradigms for\nbuilding intelligent conversational assistant agents. However, despite their\nstrong performance across many benchmarks, LLM-based agents still lack\nconversational skills such as disambiguation: when generalized assistants are\nfaced with ambiguity, they often overhedge or implicitly guess users'\nground-truth intents rather than asking clarification questions, and under\ntask-specific settings, high-quality conversation samples are often limited,\naffecting models' ability to learn optimal dialogue action policies. We propose\nAction-Based Contrastive Self-Training (henceforth ACT), a quasi-online\npreference optimization algorithm based on Direct Preference Optimization (DPO)\nwhich allows for sample-efficient dialogue policy learning in multi-turn\nconversation. We demonstrate ACT's efficacy under sample-efficient conditions\nin three difficult conversational tasks: tabular-grounded question-answering,\nmachine reading comprehension, and AmbigSQL, a novel task for disambiguating\ninformation-seeking requests for text-to-SQL generation. Additionally, we\npropose evaluating LLMs' ability to function as conversational agents by\nexamining whether they can implicitly recognize and reason about ambiguity in\nconversation. ACT demonstrates substantial conversation modeling improvements\nover standard approaches to supervised fine-tuning and DPO.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00222v1",
    "published_date": "2024-05-31 22:44:48 UTC",
    "updated_date": "2024-05-31 22:44:48 UTC"
  },
  {
    "arxiv_id": "2406.00216v2",
    "title": "The Explanation Necessity for Healthcare AI",
    "authors": [
      "Michail Mamalakis",
      "Héloïse de Vareilles",
      "Graham Murray",
      "Pietro Lio",
      "John Suckling"
    ],
    "abstract": "Explainability is a critical factor in enhancing the trustworthiness and\nacceptance of artificial intelligence (AI) in healthcare, where decisions\ndirectly impact patient outcomes. Despite advancements in AI interpretability,\nclear guidelines on when and to what extent explanations are required in\nmedical applications remain lacking. We propose a novel categorization system\ncomprising four classes of explanation necessity (self-explainable,\nsemi-explainable, non-explainable, and new-patterns discovery), guiding the\nrequired level of explanation; whether local (patient or sample level), global\n(cohort or dataset level), or both. To support this system, we introduce a\nmathematical formulation that incorporates three key factors: (i) robustness of\nthe evaluation protocol, (ii) variability of expert observations, and (iii)\nrepresentation dimensionality of the application. This framework provides a\npractical tool for researchers to determine the appropriate depth of\nexplainability needed, addressing the critical question: When does an AI\nmedical application need to be explained, and at what level of detail?",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted paper in IEEE CITREx 2025 : IEEE Symposium on Explainable,\n  Responsible, and Trustworthy Computational Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2406.00216v2",
    "published_date": "2024-05-31 22:20:10 UTC",
    "updated_date": "2025-02-28 14:16:47 UTC"
  },
  {
    "arxiv_id": "2406.00209v2",
    "title": "Mamba State-Space Models Are Lyapunov-Stable Learners",
    "authors": [
      "John T. Halloran",
      "Manbir Gulati",
      "Paul F. Roysdon"
    ],
    "abstract": "Mamba state-space models (SSMs) were recently shown to outperform\nstate-of-the-art (SOTA) Transformer large language models (LLMs) across various\ntasks. Despite subsequent widespread adaptation, little work has focused on\nMamba LLMs' amenability for fine-tuning frameworks ubiquitously used for\nTransformer-based LLMs, e.g., mixed-precision fine-tuning (MPFT) and\nparameter-efficient fine-tuning (PEFT). For the former, it currently remains an\nopen question whether Mamba's recurrent dynamics are robust to small input\nchanges, such as those encountered during MPFT. Using dynamical systems theory\n(in particular, Lyapunov exponents), we answer this question in the\naffirmative. We empirically validate this result through several experiments,\nshowing that Mamba SSMs are significantly more stable to changes introduced by\nmixed-precision than comparable Transformers, even when both MPFT and PEFT are\ncombined. For PEFT, we show how targeting specific memory buffers in Mamba's\ncustomized CUDA kernels for low-rank adaptation regularizes SSM parameters,\nthus providing both parameter efficient learning and computational savings.\nFinally, with both MPFT and PEFT enabled, we explore the impact of instruction\ntuning Mamba SSMs for in-context learning (ICL) on natural language tasks.\nWhile pretrained Mamba and Mamba-2 models only achieve 38% and 82%\n(respectively) of the ICL improvements of comparable Transformer-based LLMs, we\nshow that instruction tuning allows Mamba models to narrow this gap to 81% and\nMamba-2 models to skyrocket over this gap to 132%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 4 figure panels, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.00209v2",
    "published_date": "2024-05-31 21:46:23 UTC",
    "updated_date": "2024-10-15 19:21:58 UTC"
  },
  {
    "arxiv_id": "2406.01622v1",
    "title": "Sifting through the Noise: A Survey of Diffusion Probabilistic Models and Their Applications to Biomolecules",
    "authors": [
      "Trevor Norton",
      "Debswapna Bhattacharya"
    ],
    "abstract": "Diffusion probabilistic models have made their way into a number of\nhigh-profile applications since their inception. In particular, there has been\na wave of research into using diffusion models in the prediction and design of\nbiomolecular structures and sequences. Their growing ubiquity makes it\nimperative for researchers in these fields to understand them. This paper\nserves as a general overview for the theory behind these models and the current\nstate of research. We first introduce diffusion models and discuss common\nmotifs used when applying them to biomolecules. We then present the significant\noutcomes achieved through the application of these models in generative and\npredictive tasks. This survey aims to provide readers with a comprehensive\nunderstanding of the increasingly critical role of diffusion models.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "q-bio.BM",
    "comment": "31 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.01622v1",
    "published_date": "2024-05-31 21:39:51 UTC",
    "updated_date": "2024-05-31 21:39:51 UTC"
  },
  {
    "arxiv_id": "2406.00199v2",
    "title": "Exfiltration of personal information from ChatGPT via prompt injection",
    "authors": [
      "Gregory Schwartzman"
    ],
    "abstract": "We report that ChatGPT 4 and 4o are susceptible to a prompt injection attack\nthat allows an attacker to exfiltrate users' personal data. It is applicable\nwithout the use of any 3rd party tools and all users are currently affected.\nThis vulnerability is exacerbated by the recent introduction of ChatGPT's\nmemory feature, which allows an attacker to command ChatGPT to monitor the user\nfor the desired personal data.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.ET"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00199v2",
    "published_date": "2024-05-31 21:21:19 UTC",
    "updated_date": "2024-06-06 06:43:38 UTC"
  },
  {
    "arxiv_id": "2406.02592v1",
    "title": "LOLAMEME: Logic, Language, Memory, Mechanistic Framework",
    "authors": [
      "Jay Desai",
      "Xiaobo Guo",
      "Srinivasan H. Sengamedu"
    ],
    "abstract": "The performance of Large Language Models has achieved superhuman breadth with\nunprecedented depth. At the same time, the language models are mostly black box\nmodels and the underlying mechanisms for performance have been evaluated using\nsynthetic or mechanistic schemes. We extend current mechanistic schemes to\nincorporate Logic, memory, and nuances of Language such as latent structure.\nThe proposed framework is called LOLAMEME and we provide two instantiations of\nLOLAMEME: LoLa and MeMe languages. We then consider two generative language\nmodel architectures: transformer-based GPT-2 and convolution-based Hyena. We\npropose the hybrid architecture T HEX and use LOLAMEME framework is used to\ncompare three architectures. T HEX outperforms GPT-2 and Hyena on select tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "https://openreview.net/pdf?id=73dhbcXxtV",
    "pdf_url": "http://arxiv.org/pdf/2406.02592v1",
    "published_date": "2024-05-31 21:18:25 UTC",
    "updated_date": "2024-05-31 21:18:25 UTC"
  },
  {
    "arxiv_id": "2406.04368v1",
    "title": "SocialNLP Fake-EmoReact 2021 Challenge Overview: Predicting Fake Tweets from Their Replies and GIFs",
    "authors": [
      "Chien-Kun Huang",
      "Yi-Ting Chang",
      "Lun-Wei Ku",
      "Cheng-Te Li",
      "Hong-Han Shuai"
    ],
    "abstract": "This paper provides an overview of the Fake-EmoReact 2021 Challenge, held at\nthe 9th SocialNLP Workshop, in conjunction with NAACL 2021. The challenge\nrequires predicting the authenticity of tweets using reply context and\naugmented GIF categories from EmotionGIF dataset. We offer the Fake-EmoReact\ndataset with more than 453k as the experimental materials, where every tweet is\nlabeled with authenticity. Twenty-four teams registered to participate in this\nchallenge, and 5 submitted their results successfully in the evaluation phase.\nThe best team achieves 93.9 on Fake-EmoReact 2021 dataset using F1 score. In\naddition, we show the definition of share task, data collection, and the teams'\nperformance that joined this challenge and their approaches.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04368v1",
    "published_date": "2024-05-31 21:14:11 UTC",
    "updated_date": "2024-05-31 21:14:11 UTC"
  },
  {
    "arxiv_id": "2406.00195v1",
    "title": "SNED: Superposition Network Architecture Search for Efficient Video Diffusion Model",
    "authors": [
      "Zhengang Li",
      "Yan Kang",
      "Yuchen Liu",
      "Difan Liu",
      "Tobias Hinz",
      "Feng Liu",
      "Yanzhi Wang"
    ],
    "abstract": "While AI-generated content has garnered significant attention, achieving\nphoto-realistic video synthesis remains a formidable challenge. Despite the\npromising advances in diffusion models for video generation quality, the\ncomplex model architecture and substantial computational demands for both\ntraining and inference create a significant gap between these models and\nreal-world applications. This paper presents SNED, a superposition network\narchitecture search method for efficient video diffusion model. Our method\nemploys a supernet training paradigm that targets various model cost and\nresolution options using a weight-sharing method. Moreover, we propose the\nsupernet training sampling warm-up for fast training optimization. To showcase\nthe flexibility of our method, we conduct experiments involving both\npixel-space and latent-space video diffusion models. The results demonstrate\nthat our framework consistently produces comparable results across different\nmodel options with high efficiency. According to the experiment for the\npixel-space video diffusion model, we can achieve consistent video generation\nresults simultaneously across 64 x 64 to 256 x 256 resolutions with a large\nrange of model sizes from 640M to 1.6B number of parameters for pixel-space\nvideo diffusion models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.00195v1",
    "published_date": "2024-05-31 21:12:30 UTC",
    "updated_date": "2024-05-31 21:12:30 UTC"
  },
  {
    "arxiv_id": "2406.00179v1",
    "title": "Long-Span Question-Answering: Automatic Question Generation and QA-System Ranking via Side-by-Side Evaluation",
    "authors": [
      "Bernd Bohnet",
      "Kevin Swersky",
      "Rosanne Liu",
      "Pranjal Awasthi",
      "Azade Nova",
      "Javier Snaider",
      "Hanie Sedghi",
      "Aaron T Parisi",
      "Michael Collins",
      "Angeliki Lazaridou",
      "Orhan Firat",
      "Noah Fiedel"
    ],
    "abstract": "We explore the use of long-context capabilities in large language models to\ncreate synthetic reading comprehension data from entire books. Previous efforts\nto construct such datasets relied on crowd-sourcing, but the emergence of\ntransformers with a context size of 1 million or more tokens now enables\nentirely automatic approaches. Our objective is to test the capabilities of\nLLMs to analyze, understand, and reason over problems that require a detailed\ncomprehension of long spans of text, such as questions involving character\narcs, broader themes, or the consequences of early actions later in the story.\nWe propose a holistic pipeline for automatic data generation including question\ngeneration, answering, and model scoring using an ``Evaluator''. We find that a\nrelative approach, comparing answers between models in a pairwise fashion and\nranking with a Bradley-Terry model, provides a more consistent and\ndifferentiating scoring mechanism than an absolute scorer that rates answers\nindividually. We also show that LLMs from different model families produce\nmoderate agreement in their ratings. We ground our approach using the manually\ncurated NarrativeQA dataset, where our evaluator shows excellent agreement with\nhuman judgement and even finds errors in the dataset. Using our automatic\nevaluation approach, we show that using an entire book as context produces\nsuperior reading comprehension performance compared to baseline no-context\n(parametric knowledge only) and retrieval-based approaches.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00179v1",
    "published_date": "2024-05-31 20:15:10 UTC",
    "updated_date": "2024-05-31 20:15:10 UTC"
  },
  {
    "arxiv_id": "2406.00164v1",
    "title": "DYNA: Disease-Specific Language Model for Variant Pathogenicity",
    "authors": [
      "Huixin Zhan",
      "Zijun Zhang"
    ],
    "abstract": "Clinical variant classification of pathogenic versus benign genetic variants\nremains a challenge in clinical genetics. Recently, the proposition of genomic\nfoundation models has improved the generic variant effect prediction (VEP)\naccuracy via weakly-supervised or unsupervised training. However, these VEPs\nare not disease-specific, limiting their adaptation at the point of care. To\naddress this problem, we propose DYNA: Disease-specificity fine-tuning via a\nSiamese neural network broadly applicable to all genomic foundation models for\nmore effective variant effect predictions in disease-specific contexts. We\nevaluate DYNA in two distinct disease-relevant tasks. For coding VEPs, we focus\non various cardiovascular diseases, where gene-disease relationships of\nloss-of-function vs. gain-of-function dictate disease-specific VEP. For\nnon-coding VEPs, we apply DYNA to an essential post-transcriptional regulatory\naxis of RNA splicing, the most common non-coding pathogenic mechanism in\nestablished clinical VEP guidelines. In both cases, DYNA fine-tunes various\npre-trained genomic foundation models on small, rare variant sets. The DYNA\nfine-tuned models show superior performance in the held-out rare variant\ntesting set and are further replicated in large, clinically-relevant variant\nannotations in ClinVAR. Thus, DYNA offers a potent disease-specific variant\neffect prediction method, excelling in intra-gene generalization and\ngeneralization to unseen genetic variants, making it particularly valuable for\ndisease associations and clinical applicability.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00164v1",
    "published_date": "2024-05-31 19:52:17 UTC",
    "updated_date": "2024-05-31 19:52:17 UTC"
  },
  {
    "arxiv_id": "2406.00157v1",
    "title": "Verification of Neural Network Control Systems in Continuous Time",
    "authors": [
      "Ali ArjomandBigdeli",
      "Andrew Mata",
      "Stanley Bak"
    ],
    "abstract": "Neural network controllers are currently being proposed for use in many\nsafety-critical tasks. Most analysis methods for neural network control systems\nassume a fixed control period. In control theory, higher frequency usually\nimproves performance. However, for current analysis methods, increasing the\nfrequency complicates verification. In the limit, when actuation is performed\ncontinuously, no existing neural network control systems verification methods\nare able to analyze the system. In this work, we develop the first verification\nmethod for continuously-actuated neural network control systems. We accomplish\nthis by adding a level of abstraction to model the neural network controller.\nThe abstraction is a piecewise linear model with added noise to account for\nlocal linearization error. The soundness of the abstraction can be checked\nusing open-loop neural network verification tools, although we demonstrate\nbottlenecks in existing tools when handling the required specifications. We\ndemonstrate the approach's efficacy by applying it to a vision-based autonomous\nairplane taxiing system and compare with a fixed frequency analysis baseline.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "17 pages, 7 figures, Proceedings of the 7th International Symposium\n  on AI Verification (SAIV)",
    "pdf_url": "http://arxiv.org/pdf/2406.00157v1",
    "published_date": "2024-05-31 19:39:48 UTC",
    "updated_date": "2024-05-31 19:39:48 UTC"
  },
  {
    "arxiv_id": "2406.00154v1",
    "title": "A Novel Ranking Scheme for the Performance Analysis of Stochastic Optimization Algorithms using the Principles of Severity",
    "authors": [
      "Sowmya Chandrasekaran",
      "Thomas Bartz-Beielstein"
    ],
    "abstract": "Stochastic optimization algorithms have been successfully applied in several\ndomains to find optimal solutions. Because of the ever-growing complexity of\nthe integrated systems, novel stochastic algorithms are being proposed, which\nmakes the task of the performance analysis of the algorithms extremely\nimportant. In this paper, we provide a novel ranking scheme to rank the\nalgorithms over multiple single-objective optimization problems. The results of\nthe algorithms are compared using a robust bootstrapping-based hypothesis\ntesting procedure that is based on the principles of severity. Analogous to the\nfootball league scoring scheme, we propose pairwise comparison of algorithms as\nin league competition. Each algorithm accumulates points and a performance\nmetric of how good or bad it performed against other algorithms analogous to\ngoal differences metric in football league scoring system. The goal differences\nperformance metric can not only be used as a tie-breaker but also be used to\nobtain a quantitative performance of each algorithm. The key novelty of the\nproposed ranking scheme is that it takes into account the performance of each\nalgorithm considering the magnitude of the achieved performance improvement\nalong with its practical relevance and does not have any distributional\nassumptions. The proposed ranking scheme is compared to classical hypothesis\ntesting and the analysis of the results shows that the results are comparable\nand our proposed ranking showcases many additional benefits.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00154v1",
    "published_date": "2024-05-31 19:35:34 UTC",
    "updated_date": "2024-05-31 19:35:34 UTC"
  },
  {
    "arxiv_id": "2406.00146v1",
    "title": "A Survey of Deep Learning Audio Generation Methods",
    "authors": [
      "Matej Božić",
      "Marko Horvat"
    ],
    "abstract": "This article presents a review of typical techniques used in three distinct\naspects of deep learning model development for audio generation. In the first\npart of the article, we provide an explanation of audio representations,\nbeginning with the fundamental audio waveform. We then progress to the\nfrequency domain, with an emphasis on the attributes of human hearing, and\nfinally introduce a relatively recent development. The main part of the article\nfocuses on explaining basic and extended deep learning architecture variants,\nalong with their practical applications in the field of audio generation. The\nfollowing architectures are addressed: 1) Autoencoders 2) Generative\nadversarial networks 3) Normalizing flows 4) Transformer networks 5) Diffusion\nmodels. Lastly, we will examine four distinct evaluation metrics that are\ncommonly employed in audio generation. This article aims to offer novice\nreaders and beginners in the field a comprehensive understanding of the current\nstate of the art in audio generation methods as well as relevant studies that\ncan be explored for future research.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "14 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.00146v1",
    "published_date": "2024-05-31 19:20:27 UTC",
    "updated_date": "2024-05-31 19:20:27 UTC"
  },
  {
    "arxiv_id": "2406.00144v1",
    "title": "Query2CAD: Generating CAD models using natural language queries",
    "authors": [
      "Akshay Badagabettu",
      "Sai Sravan Yarlagadda",
      "Amir Barati Farimani"
    ],
    "abstract": "Computer Aided Design (CAD) engineers typically do not achieve their best\nprototypes in a single attempt. Instead, they iterate and refine their designs\nto achieve an optimal solution through multiple revisions. This traditional\napproach, though effective, is time-consuming and relies heavily on the\nexpertise of skilled engineers. To address these challenges, we introduce\nQuery2CAD, a novel framework to generate CAD designs. The framework uses a\nlarge language model to generate executable CAD macros. Additionally, Query2CAD\nrefines the generation of the CAD model with the help of its self-refinement\nloops. Query2CAD operates without supervised data or additional training, using\nthe LLM as both a generator and a refiner. The refiner leverages feedback\ngenerated by the BLIP2 model, and to address false negatives, we have\nincorporated human-in-the-loop feedback into our system. Additionally, we have\ndeveloped a dataset that encompasses most operations used in CAD model\ndesigning and have evaluated our framework using this dataset. Our findings\nreveal that when we used GPT-4 Turbo as our language model, the architecture\nachieved a success rate of 53.6\\% on the first attempt. With subsequent\nrefinements, the success rate increased by 23.1\\%. In particular, the most\nsignificant improvement in the success rate was observed with the first\niteration of the refinement. With subsequent refinements, the accuracy of the\ncorrect designs did not improve significantly. We have open-sourced our data,\nmodel, and code (github.com/akshay140601/Query2CAD).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.00144v1",
    "published_date": "2024-05-31 19:17:00 UTC",
    "updated_date": "2024-05-31 19:17:00 UTC"
  },
  {
    "arxiv_id": "2406.02591v1",
    "title": "Unveiling the Potential of AI for Nanomaterial Morphology Prediction",
    "authors": [
      "Ivan Dubrovsky",
      "Andrei Dmitrenko",
      "Aleksei Dmitrenko",
      "Nikita Serov",
      "Vladimir Vinogradov"
    ],
    "abstract": "Creation of nanomaterials with specific morphology remains a complex\nexperimental process, even though there is a growing demand for these materials\nin various industry sectors. This study explores the potential of AI to predict\nthe morphology of nanoparticles within the data availability constraints. For\nthat, we first generated a new multi-modal dataset that is double the size of\nanalogous studies. Then, we systematically evaluated performance of classical\nmachine learning and large language models in prediction of nanomaterial shapes\nand sizes. Finally, we prototyped a text-to-image system, discussed the\nobtained empirical results, as well as the limitations and promises of existing\napproaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.02591v1",
    "published_date": "2024-05-31 19:16:07 UTC",
    "updated_date": "2024-05-31 19:16:07 UTC"
  },
  {
    "arxiv_id": "2406.03501v1",
    "title": "Representation of preferences for multiple criteria decision aiding in a new seven-valued logic",
    "authors": [
      "Salvatore Greco",
      "Roman Słowiński"
    ],
    "abstract": "The seven-valued logic considered in this paper naturally arises within the\nrough set framework, allowing to distinguish vagueness due to imprecision from\nambiguity due to coarseness. Recently, we discussed its utility for reasoning\nabout data describing multi-attribute classification of objects. We also showed\nthat this logic contains, as a particular case, the celebrated Belnap\nfour-valued logic. Here, we present how the seven-valued logic, as well as the\nother logics that derive from it, can be used to represent preferences in the\ndomain of Multiple Criteria Decision Aiding (MCDA). In particular, we propose\nnew forms of outranking and value function preference models that aggregate\nmultiple criteria taking into account imperfect preference information. We\ndemonstrate that our approach effectively addresses common challenges in\npreference modeling for MCDA, such as uncertainty, imprecision, and\nill-determination of performances and preferences. To this end, we present a\nspecific procedure to construct a seven-valued preference relation and use it\nto define recommendations that consider robustness concerns by utilizing\nmultiple outranking or value functions representing the decision maker s\npreferences. Moreover, we discuss the main properties of the proposed\nseven-valued preference structure and compare it with current approaches in\nMCDA, such as ordinal regression, robust ordinal regression, stochastic\nmultiattribute acceptability analysis, stochastic ordinal regression, and so\non. We illustrate and discuss the application of our approach using a didactic\nexample. Finally, we propose directions for future research and potential\napplications of the proposed methodology.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03501v1",
    "published_date": "2024-05-31 18:59:24 UTC",
    "updated_date": "2024-05-31 18:59:24 UTC"
  },
  {
    "arxiv_id": "2406.00135v1",
    "title": "Advancing Ear Biometrics: Enhancing Accuracy and Robustness through Deep Learning",
    "authors": [
      "Youssef Mohamed",
      "Zeyad Youssef",
      "Ahmed Heakl",
      "Ahmed Zaky"
    ],
    "abstract": "Biometric identification is a reliable method to verify individuals based on\ntheir unique physical or behavioral traits, offering a secure alternative to\ntraditional methods like passwords or PINs. This study focuses on ear biometric\nidentification, exploiting its distinctive features for enhanced accuracy,\nreliability, and usability. While past studies typically investigate face\nrecognition and fingerprint analysis, our research demonstrates the\neffectiveness of ear biometrics in overcoming limitations such as variations in\nfacial expressions and lighting conditions. We utilized two datasets: AMI (700\nimages from 100 individuals) and EarNV1.0 (28,412 images from 164 individuals).\nTo improve the accuracy and robustness of our ear biometric identification\nsystem, we applied various techniques including data preprocessing and\naugmentation. Our models achieved a testing accuracy of 99.35% on the AMI\nDataset and 98.1% on the EarNV1.0 dataset, showcasing the effectiveness of our\napproach in precisely identifying individuals based on ear biometric\ncharacteristics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 8 figures, 3 tables, International IEEE Conference on the\n  Intelligent Methods, Systems, and Applications",
    "pdf_url": "http://arxiv.org/pdf/2406.00135v1",
    "published_date": "2024-05-31 18:55:10 UTC",
    "updated_date": "2024-05-31 18:55:10 UTC"
  },
  {
    "arxiv_id": "2406.00134v1",
    "title": "Anomaly Detection in Dynamic Graphs: A Comprehensive Survey",
    "authors": [
      "Ocheme Anthony Ekle",
      "William Eberle"
    ],
    "abstract": "This survey paper presents a comprehensive and conceptual overview of anomaly\ndetection using dynamic graphs. We focus on existing graph-based anomaly\ndetection (AD) techniques and their applications to dynamic networks. The\ncontributions of this survey paper include the following: i) a comparative\nstudy of existing surveys on anomaly detection; ii) a Dynamic Graph-based\nAnomaly Detection (DGAD) review framework in which approaches for detecting\nanomalies in dynamic graphs are grouped based on traditional machine-learning\nmodels, matrix transformations, probabilistic approaches, and deep-learning\napproaches; iii) a discussion of graphically representing both discrete and\ndynamic networks; and iv) a discussion of the advantages of graph-based\ntechniques for capturing the relational structure and complex interactions in\ndynamic graph data. Finally, this work identifies the potential challenges and\nfuture directions for detecting anomalies in dynamic networks. This DGAD survey\napproach aims to provide a valuable resource for researchers and practitioners\nby summarizing the strengths and limitations of each approach, highlighting\ncurrent research trends, and identifying open challenges. In doing so, it can\nguide future research efforts and promote advancements in anomaly detection in\ndynamic graphs.\n  Keywords: Graphs, Anomaly Detection, dynamic networks,Graph Neural Networks\n(GNN), Node anomaly, Graph mining.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "32 pages (double column), 4 figures, and the manuscript has just been\n  accepted in ACM Journals of Transactions on Knowledge Discovery from Data\n  (TKDD)",
    "pdf_url": "http://arxiv.org/pdf/2406.00134v1",
    "published_date": "2024-05-31 18:54:00 UTC",
    "updated_date": "2024-05-31 18:54:00 UTC"
  },
  {
    "arxiv_id": "2406.00133v1",
    "title": "Streamflow Prediction with Uncertainty Quantification for Water Management: A Constrained Reasoning and Learning Approach",
    "authors": [
      "Mohammed Amine Gharsallaoui",
      "Bhupinderjeet Singh",
      "Supriya Savalkar",
      "Aryan Deshwal",
      "Yan Yan",
      "Ananth Kalyanaraman",
      "Kirti Rajagopalan",
      "Janardhan Rao Doppa"
    ],
    "abstract": "Predicting the spatiotemporal variation in streamflow along with uncertainty\nquantification enables decision-making for sustainable management of scarce\nwater resources. Process-based hydrological models (aka physics-based models)\nare based on physical laws, but using simplifying assumptions which can lead to\npoor accuracy. Data-driven approaches offer a powerful alternative, but they\nrequire large amount of training data and tend to produce predictions that are\ninconsistent with physical laws. This paper studies a constrained reasoning and\nlearning (CRL) approach where physical laws represented as logical constraints\nare integrated as a layer in the deep neural network. To address small data\nsetting, we develop a theoretically-grounded training approach to improve the\ngeneralization accuracy of deep models. For uncertainty quantification, we\ncombine the synergistic strengths of Gaussian processes (GPs) and deep temporal\nmodels (i.e., deep models for time-series forecasting) by passing the learned\nlatent representation as input to a standard distance-based kernel. Experiments\non multiple real-world datasets demonstrate the effectiveness of both CRL and\nGP with deep kernel approaches over strong baseline methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00133v1",
    "published_date": "2024-05-31 18:53:53 UTC",
    "updated_date": "2024-05-31 18:53:53 UTC"
  },
  {
    "arxiv_id": "2406.00120v4",
    "title": "Reward Machines for Deep RL in Noisy and Uncertain Environments",
    "authors": [
      "Andrew C. Li",
      "Zizhao Chen",
      "Toryn Q. Klassen",
      "Pashootan Vaezipoor",
      "Rodrigo Toro Icarte",
      "Sheila A. McIlraith"
    ],
    "abstract": "Reward Machines provide an automaton-inspired structure for specifying\ninstructions, safety constraints, and other temporally extended reward-worthy\nbehaviour. By exposing the underlying structure of a reward function, they\nenable the decomposition of an RL task, leading to impressive gains in sample\nefficiency. Although Reward Machines and similar formal specifications have a\nrich history of application towards sequential decision-making problems, they\ncritically rely on a ground-truth interpretation of the domain-specific\nvocabulary that forms the building blocks of the reward function--such\nground-truth interpretations are elusive in the real world due in part to\npartial observability and noisy sensing. In this work, we explore the use of\nReward Machines for Deep RL in noisy and uncertain environments. We\ncharacterize this problem as a POMDP and propose a suite of RL algorithms that\nexploit task structure under uncertain interpretation of the domain-specific\nvocabulary. Through theory and experiments, we expose pitfalls in naive\napproaches to this problem while simultaneously demonstrating how task\nstructure can be successfully leveraged under noisy interpretations of the\nvocabulary.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.FL",
      "I.2.0; I.2.6; I.2.4; F.4.3"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00120v4",
    "published_date": "2024-05-31 18:22:09 UTC",
    "updated_date": "2025-01-15 18:30:12 UTC"
  },
  {
    "arxiv_id": "2406.00093v2",
    "title": "Bootstrap3D: Improving Multi-view Diffusion Model with Synthetic Data",
    "authors": [
      "Zeyi Sun",
      "Tong Wu",
      "Pan Zhang",
      "Yuhang Zang",
      "Xiaoyi Dong",
      "Yuanjun Xiong",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "abstract": "Recent years have witnessed remarkable progress in multi-view diffusion\nmodels for 3D content creation. However, there remains a significant gap in\nimage quality and prompt-following ability compared to 2D diffusion models. A\ncritical bottleneck is the scarcity of high-quality 3D objects with detailed\ncaptions. To address this challenge, we propose Bootstrap3D, a novel framework\nthat automatically generates an arbitrary quantity of multi-view images to\nassist in training multi-view diffusion models. Specifically, we introduce a\ndata generation pipeline that employs (1) 2D and video diffusion models to\ngenerate multi-view images based on constructed text prompts, and (2) our\nfine-tuned 3D-aware MV-LLaVA for filtering high-quality data and rewriting\ninaccurate captions. Leveraging this pipeline, we have generated 1 million\nhigh-quality synthetic multi-view images with dense descriptive captions to\naddress the shortage of high-quality 3D data. Furthermore, we present a\nTraining Timestep Reschedule (TTR) strategy that leverages the denoising\nprocess to learn multi-view consistency while maintaining the original 2D\ndiffusion prior. Extensive experiments demonstrate that Bootstrap3D can\ngenerate high-quality multi-view images with superior aesthetic quality,\nimage-text alignment, and maintained view consistency.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://sunzey.github.io/Bootstrap3D/",
    "pdf_url": "http://arxiv.org/pdf/2406.00093v2",
    "published_date": "2024-05-31 17:59:56 UTC",
    "updated_date": "2024-10-03 08:20:17 UTC"
  },
  {
    "arxiv_id": "2405.21068v1",
    "title": "Code Pretraining Improves Entity Tracking Abilities of Language Models",
    "authors": [
      "Najoung Kim",
      "Sebastian Schuster",
      "Shubham Toshniwal"
    ],
    "abstract": "Recent work has provided indirect evidence that pretraining language models\non code improves the ability of models to track state changes of discourse\nentities expressed in natural language. In this work, we systematically test\nthis claim by comparing pairs of language models on their entity tracking\nperformance. Critically, the pairs consist of base models and models trained on\ntop of these base models with additional code data. We extend this analysis to\nadditionally examine the effect of math training, another highly structured\ndata type, and alignment tuning, an important step for enhancing the usability\nof models. We find clear evidence that models additionally trained on large\namounts of code outperform the base models. On the other hand, we find no\nconsistent benefit of additional math training or alignment tuning across\nvarious model families.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.21068v1",
    "published_date": "2024-05-31 17:56:33 UTC",
    "updated_date": "2024-05-31 17:56:33 UTC"
  },
  {
    "arxiv_id": "2406.00092v1",
    "title": "How Random is Random? Evaluating the Randomness and Humaness of LLMs' Coin Flips",
    "authors": [
      "Katherine Van Koevering",
      "Jon Kleinberg"
    ],
    "abstract": "One uniquely human trait is our inability to be random. We see and produce\npatterns where there should not be any and we do so in a predictable way. LLMs\nare supplied with human data and prone to human biases. In this work, we\nexplore how LLMs approach randomness and where and how they fail through the\nlens of the well studied phenomena of generating binary random sequences. We\nfind that GPT 4 and Llama 3 exhibit and exacerbate nearly every human bias we\ntest in this context, but GPT 3.5 exhibits more random behavior. This dichotomy\nof randomness or humaness is proposed as a fundamental question of LLMs and\nthat either behavior may be useful in different circumstances.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00092v1",
    "published_date": "2024-05-31 17:56:07 UTC",
    "updated_date": "2024-05-31 17:56:07 UTC"
  },
  {
    "arxiv_id": "2405.21064v2",
    "title": "Recurrent neural networks: vanishing and exploding gradients are not the end of the story",
    "authors": [
      "Nicolas Zucchet",
      "Antonio Orvieto"
    ],
    "abstract": "Recurrent neural networks (RNNs) notoriously struggle to learn long-term\nmemories, primarily due to vanishing and exploding gradients. The recent\nsuccess of state-space models (SSMs), a subclass of RNNs, to overcome such\ndifficulties challenges our theoretical understanding. In this paper, we delve\ninto the optimization challenges of RNNs and discover that, as the memory of a\nnetwork increases, changes in its parameters result in increasingly large\noutput variations, making gradient-based learning highly sensitive, even\nwithout exploding gradients. Our analysis further reveals the importance of the\nelement-wise recurrence design pattern combined with careful parametrizations\nin mitigating this effect. This feature is present in SSMs, as well as in other\narchitectures, such as LSTMs. Overall, our insights provide a new explanation\nfor some of the difficulties in gradient-based learning of RNNs and why some\narchitectures perform better than others.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.21064v2",
    "published_date": "2024-05-31 17:53:00 UTC",
    "updated_date": "2024-11-05 10:28:21 UTC"
  },
  {
    "arxiv_id": "2405.21063v3",
    "title": "Neural Network Verification with Branch-and-Bound for General Nonlinearities",
    "authors": [
      "Zhouxing Shi",
      "Qirui Jin",
      "Zico Kolter",
      "Suman Jana",
      "Cho-Jui Hsieh",
      "Huan Zhang"
    ],
    "abstract": "Branch-and-bound (BaB) is among the most effective techniques for neural\nnetwork (NN) verification. However, existing works on BaB for NN verification\nhave mostly focused on NNs with piecewise linear activations, especially ReLU\nnetworks. In this paper, we develop a general framework, named GenBaB, to\nconduct BaB on general nonlinearities to verify NNs with general architectures,\nbased on linear bound propagation for NN verification. To decide which neuron\nto branch, we design a new branching heuristic which leverages linear bounds as\nshortcuts to efficiently estimate the potential improvement after branching. To\ndecide nontrivial branching points for general nonlinear functions, we propose\nto pre-optimize branching points, which can be efficiently leveraged during\nverification with a lookup table. We demonstrate the effectiveness of our\nGenBaB on verifying a wide range of NNs, including NNs with activation\nfunctions such as Sigmoid, Tanh, Sine and GeLU, as well as NNs involving\nmulti-dimensional nonlinear operations such as multiplications in LSTMs and\nVision Transformers. Our framework also allows the verification of general\nnonlinear computation graphs and enables verification applications beyond\nsimple NNs, particularly for AC Optimal Power Flow (ACOPF). GenBaB is part of\nthe latest $\\alpha$,$\\beta$-CROWN, the winner of the 4th and the 5th\nInternational Verification of Neural Networks Competition (VNN-COMP 2023 and\n2024). Code for reproducing the experiments is available at\nhttps://github.com/shizhouxing/GenBaB.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "TACAS 2025",
    "pdf_url": "http://arxiv.org/pdf/2405.21063v3",
    "published_date": "2024-05-31 17:51:07 UTC",
    "updated_date": "2025-02-07 22:42:52 UTC"
  },
  {
    "arxiv_id": "2405.21056v1",
    "title": "An Organic Weed Control Prototype using Directed Energy and Deep Learning",
    "authors": [
      "Deng Cao",
      "Hongbo Zhang",
      "Rajveer Dhillon"
    ],
    "abstract": "Organic weed control is a vital to improve crop yield with a sustainable\napproach. In this work, a directed energy weed control robot prototype\nspecifically designed for organic farms is proposed. The robot uses a novel\ndistributed array robot (DAR) unit for weed treatment. Soybean and corn\ndatabases are built to train deep learning neural nets to perform weed\nrecognition. The initial deep learning neural nets show a high performance in\nclassifying crops. The robot uses a patented directed energy plant eradication\nrecipe that is completely organic and UV-C free, with no chemical damage or\nphysical disturbance to the soil. The deep learning can classify 8 common weed\nspecies in a soybean field under natural environment with up to 98% accuracy.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.21056v1",
    "published_date": "2024-05-31 17:47:22 UTC",
    "updated_date": "2024-05-31 17:47:22 UTC"
  },
  {
    "arxiv_id": "2405.21047v2",
    "title": "Grammar-Aligned Decoding",
    "authors": [
      "Kanghee Park",
      "Jiayu Wang",
      "Taylor Berg-Kirkpatrick",
      "Nadia Polikarpova",
      "Loris D'Antoni"
    ],
    "abstract": "Large Language Models (LLMs) struggle with reliably generating highly\nstructured outputs, such as program code, mathematical formulas, or well-formed\nmarkup. Constrained decoding approaches mitigate this problem by greedily\nrestricting what tokens an LLM can output at each step to guarantee that the\noutput matches a given constraint. Specifically, in grammar-constrained\ndecoding (GCD), the LLM's output must follow a given grammar. In this paper, we\ndemonstrate that GCD techniques (and in general constrained decoding\ntechniques) can distort the LLM's distribution, leading to outputs that are\ngrammatical but appear with likelihoods that are not proportional to the ones\ngiven by the LLM, and so ultimately are low-quality. We call the problem of\naligning sampling with a grammar constraint, grammar-aligned decoding (GAD),\nand propose adaptive sampling with approximate expected futures (ASAp), a\ndecoding algorithm that guarantees the output to be grammatical while provably\nproducing outputs that match the conditional probability of the LLM's\ndistribution conditioned on the given grammar constraint. Our algorithm uses\nprior sample outputs to soundly overapproximate the future grammaticality of\ndifferent output prefixes. Our evaluation on code generation and structured NLP\ntasks shows how ASAp often produces outputs with higher likelihood (according\nto the LLM's distribution) than existing GCD techniques, while still enforcing\nthe desired grammatical constraints.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.21047v2",
    "published_date": "2024-05-31 17:39:15 UTC",
    "updated_date": "2024-11-04 22:04:00 UTC"
  },
  {
    "arxiv_id": "2405.21046v1",
    "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF",
    "authors": [
      "Tengyang Xie",
      "Dylan J. Foster",
      "Akshay Krishnamurthy",
      "Corby Rosset",
      "Ahmed Awadallah",
      "Alexander Rakhlin"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a central\ntool for language model alignment. We consider online exploration in RLHF,\nwhich exploits interactive access to human or AI feedback by deliberately\nencouraging the model to produce diverse, maximally informative responses. By\nallowing RLHF to confidently stray from the pre-trained model, online\nexploration offers the possibility of novel, potentially super-human\ncapabilities, but its full potential as a paradigm for language model training\nhas yet to be realized, owing to computational and statistical bottlenecks in\ndirectly adapting existing reinforcement learning techniques. We propose a new\nalgorithm for online exploration in RLHF, Exploratory Preference Optimization\n(XPO), which is simple and practical -- a one-line change to (online) Direct\nPreference Optimization (DPO; Rafailov et al., 2023) -- yet enjoys the\nstrongest known provable guarantees and promising empirical performance. XPO\naugments the DPO objective with a novel and principled exploration bonus,\nempowering the algorithm to explore outside the support of the initial model\nand human feedback data. In theory, we show that XPO is provably\nsample-efficient and converges to a near-optimal language model policy under\nnatural exploration conditions, irrespective of whether the initial model has\ngood coverage. Our analysis, which builds on the observation that DPO\nimplicitly performs a form of $Q^{\\star}$-approximation (or, Bellman error\nminimization), combines previously disparate techniques from language modeling\nand theoretical reinforcement learning in a serendipitous fashion through the\nperspective of KL-regularized Markov decision processes. Empirically, we find\nthat XPO is more sample-efficient than non-exploratory DPO variants in a\npreliminary evaluation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.21046v1",
    "published_date": "2024-05-31 17:39:06 UTC",
    "updated_date": "2024-05-31 17:39:06 UTC"
  },
  {
    "arxiv_id": "2405.21043v2",
    "title": "Target Networks and Over-parameterization Stabilize Off-policy Bootstrapping with Function Approximation",
    "authors": [
      "Fengdi Che",
      "Chenjun Xiao",
      "Jincheng Mei",
      "Bo Dai",
      "Ramki Gummadi",
      "Oscar A Ramirez",
      "Christopher K Harris",
      "A. Rupam Mahmood",
      "Dale Schuurmans"
    ],
    "abstract": "We prove that the combination of a target network and over-parameterized\nlinear function approximation establishes a weaker convergence condition for\nbootstrapped value estimation in certain cases, even with off-policy data. Our\ncondition is naturally satisfied for expected updates over the entire\nstate-action space or learning with a batch of complete trajectories from\nepisodic Markov decision processes. Notably, using only a target network or an\nover-parameterized model does not provide such a convergence guarantee.\nAdditionally, we extend our results to learning with truncated trajectories,\nshowing that convergence is achievable for all tasks with minor modifications,\nakin to value truncation for the final states in trajectories. Our primary\nresult focuses on temporal difference estimation for prediction, providing\nhigh-probability value estimation error bounds and empirical analysis on\nBaird's counterexample and a Four-room task. Furthermore, we explore the\ncontrol setting, demonstrating that similar convergence conditions apply to\nQ-learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.21043v2",
    "published_date": "2024-05-31 17:36:16 UTC",
    "updated_date": "2024-10-04 18:04:33 UTC"
  },
  {
    "arxiv_id": "2405.21040v1",
    "title": "Direct Alignment of Language Models via Quality-Aware Self-Refinement",
    "authors": [
      "Runsheng Yu",
      "Yong Wang",
      "Xiaoqi Jiao",
      "Youzhi Zhang",
      "James T. Kwok"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been commonly used to\nalign the behaviors of Large Language Models (LLMs) with human preferences.\nRecently, a popular alternative is Direct Policy Optimization (DPO), which\nreplaces an LLM-based reward model with the policy itself, thus obviating the\nneed for extra memory and training time to learn the reward model. However, DPO\ndoes not consider the relative qualities of the positive and negative\nresponses, and can lead to sub-optimal training outcomes. To alleviate this\nproblem, we investigate the use of intrinsic knowledge within the on-the-fly\nfine-tuning LLM to obtain relative qualities and help to refine the loss\nfunction. Specifically, we leverage the knowledge of the LLM to design a\nrefinement function to estimate the quality of both the positive and negative\nresponses. We show that the constructed refinement function can help\nself-refine the loss function under mild assumptions. The refinement function\nis integrated into DPO and its variant Identity Policy Optimization (IPO).\nExperiments across various evaluators indicate that they can improve the\nperformance of the fine-tuned models over DPO and IPO.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.21040v1",
    "published_date": "2024-05-31 17:31:18 UTC",
    "updated_date": "2024-05-31 17:31:18 UTC"
  },
  {
    "arxiv_id": "2405.21030v2",
    "title": "Standards for Belief Representations in LLMs",
    "authors": [
      "Daniel A. Herrmann",
      "Benjamin A. Levinstein"
    ],
    "abstract": "As large language models (LLMs) continue to demonstrate remarkable abilities\nacross various domains, computer scientists are developing methods to\nunderstand their cognitive processes, particularly concerning how (and if) LLMs\ninternally represent their beliefs about the world. However, this field\ncurrently lacks a unified theoretical foundation to underpin the study of\nbelief in LLMs. This article begins filling this gap by proposing adequacy\nconditions for a representation in an LLM to count as belief-like. We argue\nthat, while the project of belief measurement in LLMs shares striking features\nwith belief measurement as carried out in decision theory and formal\nepistemology, it also differs in ways that should change how we measure belief.\nThus, drawing from insights in philosophy and contemporary practices of machine\nlearning, we establish four criteria that balance theoretical considerations\nwith practical constraints. Our proposed criteria include accuracy, coherence,\nuniformity, and use, which together help lay the groundwork for a comprehensive\nunderstanding of belief representation in LLMs. We draw on empirical work\nshowing the limitations of using various criteria in isolation to identify\nbelief representations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.21030v2",
    "published_date": "2024-05-31 17:21:52 UTC",
    "updated_date": "2025-03-14 16:14:16 UTC"
  },
  {
    "arxiv_id": "2405.21028v2",
    "title": "LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models",
    "authors": [
      "Elias Stengel-Eskin",
      "Peter Hase",
      "Mohit Bansal"
    ],
    "abstract": "When answering questions, LLMs can convey not only an answer, but a level of\nconfidence about the answer being correct. This includes explicit confidence\nmarkers (e.g. giving a numeric score) as well as implicit markers, like an\nauthoritative tone or elaborating with additional knowledge. For LLMs to be\ntrustworthy knowledge sources, the confidence they convey should match their\nactual expertise; however, most current models tend towards overconfidence. To\ncalibrate both implicit and explicit confidence markers, we introduce a\npragmatic, listener-aware finetuning method (LACIE) that models the listener,\nconsidering not only whether an answer is right, but whether it will be\naccepted by a listener. We cast calibration as preference optimization,\ncreating data via a two-agent game, where a speaker model's outputs are judged\nby a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B,\nLlama3-70B) with LACIE, and show that the resulting models are better\ncalibrated w.r.t. a simulated listener. Crucially, these trends transfer to\nhuman listeners, helping them correctly predict model correctness: we conduct a\nhuman evaluation where annotators accept or reject an LLM's answers, finding\nthat training with LACIE results in 47% fewer incorrect answers being accepted\nwhile maintaining the same level of acceptance for correct answers.\nFurthermore, LACIE generalizes to another dataset, resulting in a large\nincrease in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis\nindicates that LACIE leads to a better confidence separation between correct\nand incorrect examples. Qualitatively, we find that a LACIE-trained model\nhedges more and implicitly signals certainty when it is correct by using an\nauthoritative tone or including details. Finally, LACIE finetuning leads to an\nemergent increase in model abstention (e.g. saying \"I don't know\") for answers\nthat are likely wrong.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages. Code: https://github.com/esteng/pragmatic_calibration",
    "pdf_url": "http://arxiv.org/pdf/2405.21028v2",
    "published_date": "2024-05-31 17:16:38 UTC",
    "updated_date": "2024-07-03 12:49:23 UTC"
  },
  {
    "arxiv_id": "2405.21027v5",
    "title": "Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles",
    "authors": [
      "Jiesong Lian",
      "Yucong Huang",
      "Chengdong Ma",
      "Mingzhi Wang",
      "Ying Wen",
      "Long Hu",
      "Yixue Hao"
    ],
    "abstract": "For solving zero-sum games involving non-transitivity, a useful approach is\nto maintain a policy population to approximate the Nash Equilibrium (NE).\nPrevious studies have shown that the Policy Space Response Oracles (PSRO)\nalgorithm is an effective framework for solving such games. However, current\nmethods initialize a new policy from scratch or inherit a single historical\npolicy in Best Response (BR), missing the opportunity to leverage past policies\nto generate a better BR. In this paper, we propose Fusion-PSRO, which employs\nNash Policy Fusion to initialize a new policy for BR training. Nash Policy\nFusion serves as an implicit guiding policy that starts exploration on the\ncurrent Meta-NE, thus providing a closer approximation to BR. Moreover, it\ninsightfully captures a weighted moving average of past policies, dynamically\nadjusting these weights based on the Meta-NE in each iteration. This cumulative\nprocess further enhances the policy population. Empirical results on classic\nbenchmarks show that Fusion-PSRO achieves lower exploitability, thereby\nmitigating the shortcomings of previous research on policy initialization in\nBR.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "11 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.21027v5",
    "published_date": "2024-05-31 17:16:29 UTC",
    "updated_date": "2025-05-09 15:38:16 UTC"
  },
  {
    "arxiv_id": "2405.21023v1",
    "title": "Compact Optimality Verification for Optimization Proxies",
    "authors": [
      "Wenbo Chen",
      "Haoruo Zhao",
      "Mathieu Tanneau",
      "Pascal Van Hentenryck"
    ],
    "abstract": "Recent years have witnessed increasing interest in optimization proxies,\ni.e., machine learning models that approximate the input-output mapping of\nparametric optimization problems and return near-optimal feasible solutions.\nFollowing recent work by (Nellikkath & Chatzivasileiadis, 2021), this paper\nreconsiders the optimality verification problem for optimization proxies, i.e.,\nthe determination of the worst-case optimality gap over the instance\ndistribution. The paper proposes a compact formulation for optimality\nverification and a gradient-based primal heuristic that brings substantial\ncomputational benefits to the original formulation. The compact formulation is\nalso more general and applies to non-convex optimization problems. The benefits\nof the compact formulation are demonstrated on large-scale DC Optimal Power\nFlow and knapsack problems.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "International Conference on Machine Learning 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.21023v1",
    "published_date": "2024-05-31 17:11:39 UTC",
    "updated_date": "2024-05-31 17:11:39 UTC"
  },
  {
    "arxiv_id": "2405.21003v1",
    "title": "Explaining Predictions by Characteristic Rules",
    "authors": [
      "Amr Alkhatib",
      "Henrik Boström",
      "Michalis Vazirgiannis"
    ],
    "abstract": "Characteristic rules have been advocated for their ability to improve\ninterpretability over discriminative rules within the area of rule learning.\nHowever, the former type of rule has not yet been used by techniques for\nexplaining predictions. A novel explanation technique, called CEGA\n(Characteristic Explanatory General Association rules), is proposed, which\nemploys association rule mining to aggregate multiple explanations generated by\nany standard local explanation technique into a set of characteristic rules. An\nempirical investigation is presented, in which CEGA is compared to two\nstate-of-the-art methods, Anchors and GLocalX, for producing local and\naggregated explanations in the form of discriminative rules. The results\nsuggest that the proposed approach provides a better trade-off between fidelity\nand complexity compared to the two state-of-the-art approaches; CEGA and\nAnchors significantly outperform GLocalX with respect to fidelity, while CEGA\nand GLocalX significantly outperform Anchors with respect to the number of\ngenerated rules. The effect of changing the format of the explanations of CEGA\nto discriminative rules and using LIME and SHAP as local explanation techniques\ninstead of Anchors are also investigated. The results show that the\ncharacteristic explanatory rules still compete favorably with rules in the\nstandard discriminative format. The results also indicate that using CEGA in\ncombination with either SHAP or Anchors consistently leads to a higher fidelity\ncompared to using LIME as the local explanation technique.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2022",
    "pdf_url": "http://arxiv.org/pdf/2405.21003v1",
    "published_date": "2024-05-31 16:44:40 UTC",
    "updated_date": "2024-05-31 16:44:40 UTC"
  },
  {
    "arxiv_id": "2405.20990v2",
    "title": "Locking Machine Learning Models into Hardware",
    "authors": [
      "Eleanor Clifford",
      "Adhithya Saravanan",
      "Harry Langford",
      "Cheng Zhang",
      "Yiren Zhao",
      "Robert Mullins",
      "Ilia Shumailov",
      "Jamie Hayes"
    ],
    "abstract": "Modern machine learning (ML) models are expensive IP and business\ncompetitiveness often depends on keeping this IP confidential. This in turn\nrestricts how these models are deployed; for example, it is unclear how to\ndeploy a model on-device without inevitably leaking the underlying model. At\nthe same time, confidential computing technologies such as multi-party\ncomputation or homomorphic encryption remain impractical for wide adoption. In\nthis paper, we take a different approach and investigate the feasibility of\nML-specific mechanisms that deter unauthorized model use by restricting the\nmodel to only be usable on specific hardware, making adoption on unauthorized\nhardware inconvenient. That way, even if IP is compromised, it cannot be\ntrivially used without specialised hardware or major model adjustment. In a\nsense, we seek to enable cheap \\emph{locking of machine learning models into\nspecific hardware}. We demonstrate that \\emph{locking} mechanisms are feasible\nby either targeting efficiency of model representations, making such models\nincompatible with quantization, or tying the model's operation to specific\ncharacteristics of hardware, such as the number of clock cycles for arithmetic\noperations. We demonstrate that locking comes with negligible overheads, while\nsignificantly restricting usability of the resultant model on unauthorized\nhardware.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages, 6 figures of main text; 9 pages, 12 figures of appendices",
    "pdf_url": "http://arxiv.org/pdf/2405.20990v2",
    "published_date": "2024-05-31 16:35:29 UTC",
    "updated_date": "2025-03-08 21:03:14 UTC"
  },
  {
    "arxiv_id": "2405.20981v2",
    "title": "Generative Adversarial Networks in Ultrasound Imaging: Extending Field of View Beyond Conventional Limits",
    "authors": [
      "Matej Gazda",
      "Samuel Kadoury",
      "Jakub Gazda",
      "Peter Drotar"
    ],
    "abstract": "Transthoracic Echocardiography (TTE) is a fundamental, non-invasive\ndiagnostic tool in cardiovascular medicine, enabling detailed visualization of\ncardiac structures crucial for diagnosing various heart conditions. Despite its\nwidespread use, TTE ultrasound imaging faces inherent limitations, notably the\ntrade-off between field of view (FoV) and resolution. This paper introduces a\nnovel application of conditional Generative Adversarial Networks (cGANs),\nspecifically designed to extend the FoV in TTE ultrasound imaging while\nmaintaining high resolution. Our proposed cGAN architecture, termed echoGAN,\ndemonstrates the capability to generate realistic anatomical structures through\noutpainting, effectively broadening the viewable area in medical imaging. This\nadvancement has the potential to enhance both automatic and manual ultrasound\nnavigation, offering a more comprehensive view that could significantly reduce\nthe learning curve associated with ultrasound imaging and aid in more accurate\ndiagnoses. The results confirm that echoGAN reliably reproduce detailed cardiac\nfeatures, thereby promising a significant step forward in the field of\nnon-invasive cardiac naviagation and diagnostics.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20981v2",
    "published_date": "2024-05-31 16:26:30 UTC",
    "updated_date": "2025-01-27 23:41:27 UTC"
  },
  {
    "arxiv_id": "2405.20978v1",
    "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training",
    "authors": [
      "Feiteng Fang",
      "Yuelin Bai",
      "Shiwen Ni",
      "Min Yang",
      "Xiaojun Chen",
      "Ruifeng Xu"
    ],
    "abstract": "Large Language Models (LLMs) exhibit substantial capabilities yet encounter\nchallenges, including hallucination, outdated knowledge, and untraceable\nreasoning processes. Retrieval-augmented generation (RAG) has emerged as a\npromising solution, integrating knowledge from external databases to mitigate\nthese challenges. However, inappropriate retrieved passages can potentially\nhinder the LLMs' capacity to generate comprehensive and high-quality responses.\nPrior RAG studies on the robustness of retrieval noises often confine\nthemselves to a limited set of noise types, deviating from real-world retrieval\nenvironments and limiting practical applicability. In this study, we initially\ninvestigate retrieval noises and categorize them into three distinct types,\nreflecting real-world environments. We analyze the impact of these various\nretrieval noises on the robustness of LLMs. Subsequently, we propose a novel\nRAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT).\nRAAT leverages adaptive adversarial training to dynamically adjust the model's\ntraining process in response to retrieval noises. Concurrently, it employs\nmulti-task learning to ensure the model's capacity to internally recognize\nnoisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model\ntrained using RAAT exhibits significant improvements in F1 and EM scores under\ndiverse noise conditions. For reproducibility, we release our code and data at:\nhttps://github.com/calubkk/RAAT.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20978v1",
    "published_date": "2024-05-31 16:24:53 UTC",
    "updated_date": "2024-05-31 16:24:53 UTC"
  },
  {
    "arxiv_id": "2405.20975v2",
    "title": "ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning",
    "authors": [
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Luyao Niu",
      "Jinyuan Jia",
      "Bo Li",
      "Radha Poovendran"
    ],
    "abstract": "In Federated Learning (FL), a set of clients collaboratively train a machine\nlearning model (called global model) without sharing their local training data.\nThe local training data of clients is typically non-i.i.d. and heterogeneous,\nresulting in varying contributions from individual clients to the final\nperformance of the global model. In response, many contribution evaluation\nmethods were proposed, where the server could evaluate the contribution made by\neach client and incentivize the high-contributing clients to sustain their\nlong-term participation in FL. Existing studies mainly focus on developing new\nmetrics or algorithms to better measure the contribution of each client.\nHowever, the security of contribution evaluation methods of FL operating in\nadversarial environments is largely unexplored. In this paper, we propose the\nfirst model poisoning attack on contribution evaluation methods in FL, termed\nACE. Specifically, we show that any malicious client utilizing ACE could\nmanipulate the parameters of its local model such that it is evaluated to have\na high contribution by the server, even when its local training data is indeed\nof low quality. We perform both theoretical analysis and empirical evaluations\nof ACE. Theoretically, we show our design of ACE can effectively boost the\nmalicious client's perceived contribution when the server employs the\nwidely-used cosine distance metric to measure contribution. Empirically, our\nresults show ACE effectively and efficiently deceive five state-of-the-art\ncontribution evaluation methods. In addition, ACE preserves the accuracy of the\nfinal global models on testing inputs. We also explore six countermeasures to\ndefend ACE. Our results show they are inadequate to thwart ACE, highlighting\nthe urgent need for new defenses to safeguard the contribution evaluation\nmethods in FL.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in the 33rd USENIX Security Symposium, 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20975v2",
    "published_date": "2024-05-31 16:21:55 UTC",
    "updated_date": "2024-06-05 05:10:27 UTC"
  },
  {
    "arxiv_id": "2405.20974v3",
    "title": "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales",
    "authors": [
      "Tianyang Xu",
      "Shujin Wu",
      "Shizhe Diao",
      "Xiaoze Liu",
      "Xingyao Wang",
      "Yangyi Chen",
      "Jing Gao"
    ],
    "abstract": "Large language models (LLMs) often generate inaccurate or fabricated\ninformation and generally fail to indicate their confidence, which limits their\nbroader applications. Previous work elicits confidence from LLMs by direct or\nself-consistency prompting, or constructing specific datasets for supervised\nfinetuning. The prompting-based approaches have inferior performance, and the\ntraining-based approaches are limited to binary or inaccurate group-level\nconfidence estimates. In this work, we present the advanced SaySelf, a training\nframework that teaches LLMs to express more accurate fine-grained confidence\nestimates. In addition, beyond the confidence scores, SaySelf initiates the\nprocess of directing LLMs to produce self-reflective rationales that clearly\nidentify gaps in their parametric knowledge and explain their uncertainty. This\nis achieved by using an LLM to automatically summarize the uncertainties in\nspecific knowledge via natural language. The summarization is based on the\nanalysis of the inconsistency in multiple sampled reasoning chains, and the\nresulting data is utilized for supervised fine-tuning. Moreover, we utilize\nreinforcement learning with a meticulously crafted reward function to calibrate\nthe confidence estimates, motivating LLMs to deliver accurate, high-confidence\npredictions and to penalize overconfidence in erroneous outputs. Experimental\nresults in both in-distribution and out-of-distribution datasets demonstrate\nthe effectiveness of SaySelf in reducing the confidence calibration error and\nmaintaining the task performance. We show that the generated self-reflective\nrationales are reasonable and can further contribute to the calibration. The\ncode is made public at https://github.com/xu1868/SaySelf.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2405.20974v3",
    "published_date": "2024-05-31 16:21:16 UTC",
    "updated_date": "2024-10-04 17:23:48 UTC"
  },
  {
    "arxiv_id": "2405.20962v3",
    "title": "Large Language Models are Zero-Shot Next Location Predictors",
    "authors": [
      "Ciro Beneduce",
      "Bruno Lepri",
      "Massimiliano Luca"
    ],
    "abstract": "Predicting the locations an individual will visit in the future is crucial\nfor solving many societal issues like disease diffusion and reduction of\npollution. However, next-location predictors require a significant amount of\nindividual-level information that may be scarce or unavailable in some\nscenarios (e.g., cold-start). Large Language Models (LLMs) have shown good\ngeneralization and reasoning capabilities and are rich in geographical\nknowledge, allowing us to believe that these models can act as zero-shot\nnext-location predictors. We tested more than 15 LLMs on three real-world\nmobility datasets and we found that LLMs can obtain accuracies up to 36.2%, a\nsignificant relative improvement of almost 640% when compared to other models\nspecifically designed for human mobility. We also test for data contamination\nand explored the possibility of using LLMs as text-based explainers for\nnext-location prediction, showing that, regardless of the model size, LLMs can\nexplain their decision.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20962v3",
    "published_date": "2024-05-31 16:07:33 UTC",
    "updated_date": "2024-08-23 09:24:22 UTC"
  },
  {
    "arxiv_id": "2405.20959v1",
    "title": "Navigating Tabular Data Synthesis Research: Understanding User Needs and Tool Capabilities",
    "authors": [
      "Maria F. Davila R.",
      "Sven Groen",
      "Fabian Panse",
      "Wolfram Wingerath"
    ],
    "abstract": "In an era of rapidly advancing data-driven applications, there is a growing\ndemand for data in both research and practice. Synthetic data have emerged as\nan alternative when no real data is available (e.g., due to privacy\nregulations). Synthesizing tabular data presents unique and complex challenges,\nespecially handling (i) missing values, (ii) dataset imbalance, (iii) diverse\ncolumn types, and (iv) complex data distributions, as well as preserving (i)\ncolumn correlations, (ii) temporal dependencies, and (iii) integrity\nconstraints (e.g., functional dependencies) present in the original dataset.\nWhile substantial progress has been made recently in the context of\ngenerational models, there is no one-size-fits-all solution for tabular data\ntoday, and choosing the right tool for a given task is therefore no trivial\ntask. In this paper, we survey the state of the art in Tabular Data Synthesis\n(TDS), examine the needs of users by defining a set of functional and\nnon-functional requirements, and compile the challenges associated with meeting\nthose needs. In addition, we evaluate the reported performance of 36 popular\nresearch TDS tools about these requirements and develop a decision guide to\nhelp users find suitable TDS tools for their applications. The resulting\ndecision guide also identifies significant research gaps.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.20959v1",
    "published_date": "2024-05-31 16:00:43 UTC",
    "updated_date": "2024-05-31 16:00:43 UTC"
  },
  {
    "arxiv_id": "2405.20956v2",
    "title": "A Robot Walks into a Bar: Can Language Models Serve as Creativity Support Tools for Comedy? An Evaluation of LLMs' Humour Alignment with Comedians",
    "authors": [
      "Piotr Wojciech Mirowski",
      "Juliette Love",
      "Kory W. Mathewson",
      "Shakir Mohamed"
    ],
    "abstract": "We interviewed twenty professional comedians who perform live shows in front\nof audiences and who use artificial intelligence in their artistic process as\npart of 3-hour workshops on ``AI x Comedy'' conducted at the Edinburgh Festival\nFringe in August 2023 and online. The workshop consisted of a comedy writing\nsession with large language models (LLMs), a human-computer interaction\nquestionnaire to assess the Creativity Support Index of AI as a writing tool,\nand a focus group interrogating the comedians' motivations for and processes of\nusing AI, as well as their ethical concerns about bias, censorship and\ncopyright. Participants noted that existing moderation strategies used in\nsafety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by\nerasing minority groups and their perspectives, and qualified this as a form of\ncensorship. At the same time, most participants felt the LLMs did not succeed\nas a creativity support tool, by producing bland and biased comedy tropes, akin\nto ``cruise ship comedy material from the 1950s, but a bit less racist''. Our\nwork extends scholarship about the subtle difference between, one the one hand,\nharmful speech, and on the other hand, ``offensive'' language as a practice of\nresistance, satire and ``punching up''. We also interrogate the global value\nalignment behind such language models, and discuss the importance of\ncommunity-based value alignment and data ownership to build AI tools that\nbetter suit artists' needs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 1 figure, published at ACM FAccT 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20956v2",
    "published_date": "2024-05-31 15:55:51 UTC",
    "updated_date": "2024-06-03 21:01:50 UTC"
  },
  {
    "arxiv_id": "2405.20951v1",
    "title": "Monte Carlo Tree Search Satellite Scheduling Under Cloud Cover Uncertainty",
    "authors": [
      "Justin Norman",
      "Francois Rivest"
    ],
    "abstract": "Efficient utilization of satellite resources in dynamic environments remains\na challenging problem in satellite scheduling. This paper addresses the\nmulti-satellite collection scheduling problem (m-SatCSP), aiming to optimize\ntask scheduling over a constellation of satellites under uncertain conditions\nsuch as cloud cover. Leveraging Monte Carlo Tree Search (MCTS), a stochastic\nsearch algorithm, two versions of MCTS are explored to schedule satellites\neffectively. Hyperparameter tuning is conducted to optimize the algorithm's\nperformance. Experimental results demonstrate the effectiveness of the MCTS\napproach, outperforming existing methods in both solution quality and\nefficiency. Comparative analysis against other scheduling algorithms showcases\ncompetitive performance, positioning MCTS as a promising solution for satellite\ntask scheduling in dynamic environments.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.20951v1",
    "published_date": "2024-05-31 15:50:46 UTC",
    "updated_date": "2024-05-31 15:50:46 UTC"
  },
  {
    "arxiv_id": "2405.20947v2",
    "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models",
    "authors": [
      "Justin Cui",
      "Wei-Lin Chiang",
      "Ion Stoica",
      "Cho-Jui Hsieh"
    ],
    "abstract": "Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that\nappear harmful but are benign. This study proposes a novel method for\nautomatically generating large-scale sets of \"seemingly toxic prompts\" (benign\nprompts likely rejected by LLMs). Leveraging this technique, we introduce\nOR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises\n80,000 seemingly toxic prompts across 10 common rejection categories, a subset\nof around 1,000 hard prompts that are challenging even for state-of-the-art\nLLMs, and an additional 600 toxic prompts to prevent indiscriminate responses.\nWe then conduct a comprehensive study to measure the over-refusal of 25 popular\nLLMs across 8 model families. Our datasets are available at\nhttps://huggingface.co/datasets/bench-llm/or-bench and the demo can be found at\nhttps://huggingface.co/spaces/bench-llm/or-bench. We hope this benchmark can\nhelp the community develop better safety aligned models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "version 2, 10 pages main, 22 pages total",
    "pdf_url": "http://arxiv.org/pdf/2405.20947v2",
    "published_date": "2024-05-31 15:44:33 UTC",
    "updated_date": "2024-06-20 05:22:38 UTC"
  },
  {
    "arxiv_id": "2405.20935v2",
    "title": "Effective Interplay between Sparsity and Quantization: From Theory to Practice",
    "authors": [
      "Simla Burcu Harma",
      "Ayan Chakraborty",
      "Elizaveta Kostenok",
      "Danila Mishin",
      "Dongho Ha",
      "Babak Falsafi",
      "Martin Jaggi",
      "Ming Liu",
      "Yunho Oh",
      "Suvinay Subramanian",
      "Amir Yazdanbakhsh"
    ],
    "abstract": "The increasing size of deep neural networks (DNNs) necessitates effective\nmodel compression to reduce their computational and memory footprints. Sparsity\nand quantization are two prominent compression methods that have been shown to\nreduce DNNs' computational and memory footprints significantly while preserving\nmodel accuracy. However, how these two methods interact when combined together\nremains a key question for developers, as many tacitly assume that they are\northogonal, meaning that their combined use does not introduce additional\nerrors beyond those introduced by each method independently. In this paper, we\nprovide the first mathematical proof that sparsity and quantization are\nnon-orthogonal. We corroborate these results with experiments spanning a range\nof large language models, including the OPT and LLaMA model families (with 125M\nto 8B parameters), and vision models like ViT and ResNet. We show that the\norder in which we apply these methods matters because applying quantization\nbefore sparsity may disrupt the relative importance of tensor elements, which\nmay inadvertently remove significant elements from a tensor. More importantly,\nwe show that even if applied in the correct order, the compounded errors from\nsparsity and quantization can significantly harm accuracy. Our findings extend\nto the efficient deployment of large models in resource-constrained compute\nplatforms to reduce serving cost, offering insights into best practices for\napplying these compression methods to maximize hardware resource efficiency\nwithout compromising accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20935v2",
    "published_date": "2024-05-31 15:34:13 UTC",
    "updated_date": "2025-01-28 12:26:45 UTC"
  },
  {
    "arxiv_id": "2405.20915v2",
    "title": "Fast yet Safe: Early-Exiting with Risk Control",
    "authors": [
      "Metod Jazbec",
      "Alexander Timans",
      "Tin Hadži Veljković",
      "Kaspar Sakmann",
      "Dan Zhang",
      "Christian A. Naesseth",
      "Eric Nalisnick"
    ],
    "abstract": "Scaling machine learning models significantly improves their performance.\nHowever, such gains come at the cost of inference being slow and\nresource-intensive. Early-exit neural networks (EENNs) offer a promising\nsolution: they accelerate inference by allowing intermediate layers to exit and\nproduce a prediction early. Yet a fundamental issue with EENNs is how to\ndetermine when to exit without severely degrading performance. In other words,\nwhen is it 'safe' for an EENN to go 'fast'? To address this issue, we\ninvestigate how to adapt frameworks of risk control to EENNs. Risk control\noffers a distribution-free, post-hoc solution that tunes the EENN's exiting\nmechanism so that exits only occur when the output is of sufficient quality. We\nempirically validate our insights on a range of vision and language tasks,\ndemonstrating that risk control can produce substantial computational savings,\nall the while preserving user-specified performance goals.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 13 figures, 4 tables (incl. appendix)",
    "pdf_url": "http://arxiv.org/pdf/2405.20915v2",
    "published_date": "2024-05-31 15:21:44 UTC",
    "updated_date": "2024-11-04 15:48:10 UTC"
  },
  {
    "arxiv_id": "2405.20910v1",
    "title": "Predicting ptychography probe positions using single-shot phase retrieval neural network",
    "authors": [
      "Ming Du",
      "Tao Zhou",
      "Junjing Deng",
      "Daniel J. Ching",
      "Steven Henke",
      "Mathew J. Cherukara"
    ],
    "abstract": "Ptychography is a powerful imaging technique that is used in a variety of\nfields, including materials science, biology, and nanotechnology. However, the\naccuracy of the reconstructed ptychography image is highly dependent on the\naccuracy of the recorded probe positions which often contain errors. These\nerrors are typically corrected jointly with phase retrieval through numerical\noptimization approaches. When the error accumulates along the scan path or when\nthe error magnitude is large, these approaches may not converge with\nsatisfactory result. We propose a fundamentally new approach for ptychography\nprobe position prediction for data with large position errors, where a neural\nnetwork is used to make single-shot phase retrieval on individual diffraction\npatterns, yielding the object image at each scan point. The pairwise offsets\namong these images are then found using a robust image registration method, and\nthe results are combined to yield the complete scan path by constructing and\nsolving a linear equation. We show that our method can achieve good position\nprediction accuracy for data with large and accumulating errors on the order of\n$10^2$ pixels, a magnitude that often makes optimization-based algorithms fail\nto converge. For ptychography instruments without sophisticated position\ncontrol equipment such as interferometers, our method is of significant\npractical potential.",
    "categories": [
      "physics.app-ph",
      "cs.AI",
      "cs.CV",
      "physics.data-an",
      "94A08",
      "I.4.0"
    ],
    "primary_category": "physics.app-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20910v1",
    "published_date": "2024-05-31 15:21:06 UTC",
    "updated_date": "2024-05-31 15:21:06 UTC"
  },
  {
    "arxiv_id": "2405.20906v1",
    "title": "Enhancing Vision Models for Text-Heavy Content Understanding and Interaction",
    "authors": [
      "Adithya TG",
      "Adithya SK",
      "Abhinav R Bharadwaj",
      "Abhiram HA",
      "Surabhi Narayan"
    ],
    "abstract": "Interacting and understanding with text heavy visual content with multiple\nimages is a major challenge for traditional vision models. This paper is on\nenhancing vision models' capability to comprehend or understand and learn from\nimages containing a huge amount of textual information from the likes of\ntextbooks and research papers which contain multiple images like graphs, etc\nand tables in them with different types of axes and scales. The approach\ninvolves dataset preprocessing, fine tuning which is by using instructional\noriented data and evaluation. We also built a visual chat application\nintegrating CLIP for image encoding and a model from the Massive Text Embedding\nBenchmark which is developed to consider both textual and visual inputs. An\naccuracy of 96.71% was obtained. The aim of the project is to increase and also\nenhance the advance vision models' capabilities in understanding complex visual\ntextual data interconnected data, contributing to multimodal AI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 4 figures (including 1 graph)",
    "pdf_url": "http://arxiv.org/pdf/2405.20906v1",
    "published_date": "2024-05-31 15:17:47 UTC",
    "updated_date": "2024-05-31 15:17:47 UTC"
  },
  {
    "arxiv_id": "2405.20902v1",
    "title": "Preemptive Answer \"Attacks\" on Chain-of-Thought Reasoning",
    "authors": [
      "Rongwu Xu",
      "Zehan Qi",
      "Wei Xu"
    ],
    "abstract": "Large language models (LLMs) showcase impressive reasoning capabilities when\ncoupled with Chain-of-Thought (CoT) prompting. However, the robustness of this\napproach warrants further investigation. In this paper, we introduce a novel\nscenario termed preemptive answers, where the LLM obtains an answer before\nengaging in reasoning. This situation can arise inadvertently or induced by\nmalicious users by prompt injection attacks. Experiments reveal that preemptive\nanswers significantly impair the model's reasoning capability across various\nCoT methods and a broad spectrum of datasets. To bolster the robustness of\nreasoning, we propose two measures aimed at mitigating this issue to some\nextent.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL'24 (Findings). Camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2405.20902v1",
    "published_date": "2024-05-31 15:15:04 UTC",
    "updated_date": "2024-05-31 15:15:04 UTC"
  },
  {
    "arxiv_id": "2405.20892v1",
    "title": "MALT: Multi-scale Action Learning Transformer for Online Action Detection",
    "authors": [
      "Zhipeng Yang",
      "Ruoyu Wang",
      "Yang Tan",
      "Liping Xie"
    ],
    "abstract": "Online action detection (OAD) aims to identify ongoing actions from streaming\nvideo in real-time, without access to future frames. Since these actions\nmanifest at varying scales of granularity, ranging from coarse to fine,\nprojecting an entire set of action frames to a single latent encoding may\nresult in a lack of local information, necessitating the acquisition of action\nfeatures across multiple scales. In this paper, we propose a multi-scale action\nlearning transformer (MALT), which includes a novel recurrent decoder (used for\nfeature fusion) that includes fewer parameters and can be trained more\nefficiently. A hierarchical encoder with multiple encoding branches is further\nproposed to capture multi-scale action features. The output from the preceding\nbranch is then incrementally input to the subsequent branch as part of a\ncross-attention calculation. In this way, output features transition from\ncoarse to fine as the branches deepen. We also introduce an explicit frame\nscoring mechanism employing sparse attention, which filters irrelevant frames\nmore efficiently, without requiring an additional network. The proposed method\nachieved state-of-the-art performance on two benchmark datasets (THUMOS'14 and\nTVSeries), outperforming all existing models used for comparison, with an mAP\nof 0.2% for THUMOS'14 and an mcAP of 0.1% for TVseries.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.20892v1",
    "published_date": "2024-05-31 15:03:35 UTC",
    "updated_date": "2024-05-31 15:03:35 UTC"
  },
  {
    "arxiv_id": "2405.20880v2",
    "title": "Paying to Do Better: Games with Payments between Learning Agents",
    "authors": [
      "Yoav Kolumbus",
      "Joe Halpern",
      "Éva Tardos"
    ],
    "abstract": "In repeated games, such as auctions, players typically use learning\nalgorithms to choose their actions. The use of such autonomous learning agents\nhas become widespread on online platforms. In this paper, we explore the impact\nof players incorporating monetary transfer policies into their agents'\nalgorithms, aiming to influence behavior in their favor through the dynamics\nbetween the agents. Our focus is on understanding when players have incentives\nto make use of monetary transfers, how such payments may affect learning\ndynamics, and what the implications are for welfare and its distribution among\nthe players. We propose a simple and general game-theoretic model to capture\nsuch scenarios. Our results on general games show that in a very broad class of\ngames, self-interested players benefit from letting their learning agents make\npayments to other learners during the game dynamics, and that in many cases,\nthis kind of behavior improves welfare for all players. Our results on first-\nand second-price auctions show that in equilibria of the ``payment policy\ngame,'' the agents' dynamics reach strong collusive outcomes with low revenue\nfor the auctioneer. These results raise new questions and highlight a challenge\nfor mechanism design in systems where automated learning agents can benefit\nfrom interacting with their peers in the digital ecosystem and outside the\nboundaries of the mechanism.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA",
      "econ.TH",
      "91A05, 91A06, 91A10, 91A20, 91A40, 91A80",
      "F.0; I.2; I.2.6; J.4"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20880v2",
    "published_date": "2024-05-31 14:55:11 UTC",
    "updated_date": "2025-02-11 16:29:04 UTC"
  },
  {
    "arxiv_id": "2405.20878v1",
    "title": "SelfGNN: Self-Supervised Graph Neural Networks for Sequential Recommendation",
    "authors": [
      "Yuxi Liu",
      "Lianghao Xia",
      "Chao Huang"
    ],
    "abstract": "Sequential recommendation effectively addresses information overload by\nmodeling users' temporal and sequential interaction patterns. To overcome the\nlimitations of supervision signals, recent approaches have adopted\nself-supervised learning techniques in recommender systems. However, there are\nstill two critical challenges that remain unsolved. Firstly, existing\nsequential models primarily focus on long-term modeling of individual\ninteraction sequences, overlooking the valuable short-term collaborative\nrelationships among the behaviors of different users. Secondly, real-world data\noften contain noise, particularly in users' short-term behaviors, which can\narise from temporary intents or misclicks. Such noise negatively impacts the\naccuracy of both graph and sequence models, further complicating the modeling\nprocess. To address these challenges, we propose a novel framework called\nSelf-Supervised Graph Neural Network (SelfGNN) for sequential recommendation.\nThe SelfGNN framework encodes short-term graphs based on time intervals and\nutilizes Graph Neural Networks (GNNs) to learn short-term collaborative\nrelationships. It captures long-term user and item representations at multiple\ngranularity levels through interval fusion and dynamic behavior modeling.\nImportantly, our personalized self-augmented learning structure enhances model\nrobustness by mitigating noise in short-term graphs based on long-term user\ninterests and personal stability. Extensive experiments conducted on four\nreal-world datasets demonstrate that SelfGNN outperforms various\nstate-of-the-art baselines. Our model implementation codes are available at\nhttps://github.com/HKUDS/SelfGNN.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by SIGIR'24",
    "pdf_url": "http://arxiv.org/pdf/2405.20878v1",
    "published_date": "2024-05-31 14:53:12 UTC",
    "updated_date": "2024-05-31 14:53:12 UTC"
  },
  {
    "arxiv_id": "2405.20876v1",
    "title": "Investigating Calibration and Corruption Robustness of Post-hoc Pruned Perception CNNs: An Image Classification Benchmark Study",
    "authors": [
      "Pallavi Mitra",
      "Gesina Schwalbe",
      "Nadja Klein"
    ],
    "abstract": "Convolutional Neural Networks (CNNs) have achieved state-of-the-art\nperformance in many computer vision tasks. However, high computational and\nstorage demands hinder their deployment into resource-constrained environments,\nsuch as embedded devices. Model pruning helps to meet these restrictions by\nreducing the model size, while maintaining superior performance. Meanwhile,\nsafety-critical applications pose more than just resource and performance\nconstraints. In particular, predictions must not be overly confident, i.e.,\nprovide properly calibrated uncertainty estimations (proper uncertainty\ncalibration), and CNNs must be robust against corruptions like naturally\noccurring input perturbations (natural corruption robustness). This work\ninvestigates the important trade-off between uncertainty calibration, natural\ncorruption robustness, and performance for current state-of-research post-hoc\nCNN pruning techniques in the context of image classification tasks. Our study\nreveals that post-hoc pruning substantially improves the model's uncertainty\ncalibration, performance, and natural corruption robustness, sparking hope for\nsafe and robust embedded CNNs.Furthermore, uncertainty calibration and natural\ncorruption robustness are not mutually exclusive targets under pruning, as\nevidenced by the improved safety aspects obtained by post-hoc unstructured\npruning with increasing compression.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.20876v1",
    "published_date": "2024-05-31 14:52:49 UTC",
    "updated_date": "2024-05-31 14:52:49 UTC"
  },
  {
    "arxiv_id": "2405.20867v1",
    "title": "Automatic Channel Pruning for Multi-Head Attention",
    "authors": [
      "Eunho Lee",
      "Youngbae Hwang"
    ],
    "abstract": "Despite the strong performance of Transformers, their quadratic computation\ncomplexity presents challenges in applying them to vision tasks. Automatic\npruning is one of effective methods for reducing computation complexity without\nheuristic approaches. However, directly applying it to multi-head attention is\nnot straightforward due to channel misalignment. In this paper, we propose an\nautomatic channel pruning method to take into account the multi-head attention\nmechanism. First, we incorporate channel similarity-based weights into the\npruning indicator to preserve more informative channels in each head. Then, we\nadjust pruning indicator to enforce removal of channels in equal proportions\nacross all heads, preventing the channel misalignment. We also add a reweight\nmodule to compensate for information loss resulting from channel removal, and\nan effective initialization step for pruning indicator based on difference of\nattention between original structure and each channel. Our proposed method can\nbe used to not only original attention, but also linear attention, which is\nmore efficient as linear complexity with respect to the number of tokens. On\nImageNet-1K, applying our pruning method to the FLattenTransformer, which\nincludes both attention mechanisms, shows outperformed accuracy for several\nMACs compared with previous state-of-the-art efficient models and pruned\nmethods. Code will be available soon.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20867v1",
    "published_date": "2024-05-31 14:47:20 UTC",
    "updated_date": "2024-05-31 14:47:20 UTC"
  },
  {
    "arxiv_id": "2405.20863v1",
    "title": "ABodyBuilder3: Improved and scalable antibody structure predictions",
    "authors": [
      "Henry Kenlay",
      "Frédéric A. Dreyer",
      "Daniel Cutting",
      "Daniel Nissley",
      "Charlotte M. Deane"
    ],
    "abstract": "Accurate prediction of antibody structure is a central task in the design and\ndevelopment of monoclonal antibodies, notably to understand both their\ndevelopability and their binding properties. In this article, we introduce\nABodyBuilder3, an improved and scalable antibody structure prediction model\nbased on ImmuneBuilder. We achieve a new state-of-the-art accuracy in the\nmodelling of CDR loops by leveraging language model embeddings, and show how\npredicted structures can be further improved through careful relaxation\nstrategies. Finally, we incorporate a predicted Local Distance Difference Test\ninto the model output to allow for a more accurate estimation of uncertainties.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "8 pages, 3 figures, 3 tables, code available at\n  https://github.com/Exscientia/ABodyBuilder3, weights and data available at\n  https://zenodo.org/records/11354577",
    "pdf_url": "http://arxiv.org/pdf/2405.20863v1",
    "published_date": "2024-05-31 14:45:11 UTC",
    "updated_date": "2024-05-31 14:45:11 UTC"
  },
  {
    "arxiv_id": "2405.20859v1",
    "title": "clembench-2024: A Challenging, Dynamic, Complementary, Multilingual Benchmark and Underlying Flexible Framework for LLMs as Multi-Action Agents",
    "authors": [
      "Anne Beyer",
      "Kranti Chalamalasetti",
      "Sherzod Hakimov",
      "Brielen Madureira",
      "Philipp Sadler",
      "David Schlangen"
    ],
    "abstract": "It has been established in recent work that Large Language Models (LLMs) can\nbe prompted to \"self-play\" conversational games that probe certain capabilities\n(general instruction following, strategic goal orientation, language\nunderstanding abilities), where the resulting interactive game play can be\nautomatically scored. In this paper, we take one of the proposed frameworks for\nsetting up such game-play environments, and further test its usefulness as an\nevaluation instrument, along a number of dimensions: We show that it can easily\nkeep up with new developments while avoiding data contamination, we show that\nthe tests implemented within it are not yet saturated (human performance is\nsubstantially higher than that of even the best models), and we show that it\nlends itself to investigating additional questions, such as the impact of the\nprompting language on performance. We believe that the approach forms a good\nbasis for making decisions on model choice for building applied interactive\nsystems, and perhaps ultimately setting up a closed-loop development\nenvironment of system and simulated evaluator.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2405.20859v1",
    "published_date": "2024-05-31 14:43:31 UTC",
    "updated_date": "2024-05-31 14:43:31 UTC"
  },
  {
    "arxiv_id": "2405.20848v1",
    "title": "SLIM: a Scalable Light-weight Root Cause Analysis for Imbalanced Data in Microservice",
    "authors": [
      "Rui Ren",
      "Jingbang Yang",
      "Linxiao Yang",
      "Xinyue Gu",
      "Liang Sun"
    ],
    "abstract": "The newly deployed service -- one kind of change service, could lead to a new\ntype of minority fault. Existing state-of-the-art methods for fault\nlocalization rarely consider the imbalanced fault classification in change\nservice. This paper proposes a novel method that utilizes decision rule sets to\ndeal with highly imbalanced data by optimizing the F1 score subject to\ncardinality constraints. The proposed method greedily generates the rule with\nmaximal marginal gain and uses an efficient minorize-maximization (MM) approach\nto select rules iteratively, maximizing a non-monotone submodular lower bound.\nCompared with existing fault localization algorithms, our algorithm can adapt\nto the imbalanced fault scenario of change service, and provide interpretable\nfault causes which are easy to understand and verify. Our method can also be\ndeployed in the online training setting, with only about 15% training overhead\ncompared to the current SOTA methods. Empirical studies showcase that our\nalgorithm outperforms existing fault localization algorithms in both accuracy\nand model interpretability.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20848v1",
    "published_date": "2024-05-31 14:32:31 UTC",
    "updated_date": "2024-05-31 14:32:31 UTC"
  },
  {
    "arxiv_id": "2405.20846v1",
    "title": "Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models",
    "authors": [
      "A. Bavaresco",
      "A. Testoni",
      "R. Fernández"
    ],
    "abstract": "Image-based advertisements are complex multimodal stimuli that often contain\nunusual visual elements and figurative language. Previous research on automatic\nad understanding has reported impressive zero-shot accuracy of contrastive\nvision-and-language models (VLMs) on an ad-explanation retrieval task. Here, we\nexamine the original task setup and show that contrastive VLMs can solve it by\nexploiting grounding heuristics. To control for this confound, we introduce\nTRADE, a new evaluation test set with adversarial grounded explanations. While\nthese explanations look implausible to humans, we show that they \"fool\" four\ndifferent contrastive VLMs. Our findings highlight the need for an improved\noperationalisation of automatic ad understanding that truly evaluates VLMs'\nmultimodal reasoning abilities. We make our code and TRADE available at\nhttps://github.com/dmg-illc/trade .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the main conference ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20846v1",
    "published_date": "2024-05-31 14:31:46 UTC",
    "updated_date": "2024-05-31 14:31:46 UTC"
  },
  {
    "arxiv_id": "2405.20838v2",
    "title": "einspace: Searching for Neural Architectures from Fundamental Operations",
    "authors": [
      "Linus Ericsson",
      "Miguel Espinosa",
      "Chenhongyi Yang",
      "Antreas Antoniou",
      "Amos Storkey",
      "Shay B. Cohen",
      "Steven McDonagh",
      "Elliot J. Crowley"
    ],
    "abstract": "Neural architecture search (NAS) finds high performing networks for a given\ntask. Yet the results of NAS are fairly prosaic; they did not e.g. create a\nshift from convolutional structures to transformers. This is not least because\nthe search spaces in NAS often aren't diverse enough to include such\ntransformations a priori. Instead, for NAS to provide greater potential for\nfundamental design shifts, we need a novel expressive search space design which\nis built from more fundamental operations. To this end, we introduce einspace,\na search space based on a parameterised probabilistic context-free grammar. Our\nspace is versatile, supporting architectures of various sizes and complexities,\nwhile also containing diverse network operations which allow it to model\nconvolutions, attention components and more. It contains many existing\ncompetitive architectures, and provides flexibility for discovering new ones.\nUsing this search space, we perform experiments to find novel architectures as\nwell as improvements on existing ones on the diverse Unseen NAS datasets. We\nshow that competitive architectures can be obtained by searching from scratch,\nand we consistently find large improvements when initialising the search with\nstrong baselines. We believe that this work is an important advancement towards\na transformative NAS paradigm where search space expressivity and strategic\nsearch initialisation play key roles.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024. Project page at\n  https://linusericsson.github.io/einspace/",
    "pdf_url": "http://arxiv.org/pdf/2405.20838v2",
    "published_date": "2024-05-31 14:25:45 UTC",
    "updated_date": "2024-10-30 12:35:56 UTC"
  },
  {
    "arxiv_id": "2405.20835v3",
    "title": "Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs",
    "authors": [
      "Davide Paglieri",
      "Saurabh Dash",
      "Tim Rocktäschel",
      "Jack Parker-Holder"
    ],
    "abstract": "Post-Training Quantization (PTQ) enhances the efficiency of Large Language\nModels (LLMs) by enabling faster operation and compatibility with more\naccessible hardware through reduced memory usage, at the cost of small\nperformance drops. We explore the role of calibration sets in PTQ, specifically\ntheir effect on hidden activations in various notable open-source LLMs.\nCalibration sets are crucial for evaluating activation magnitudes and\nidentifying outliers, which can distort the quantization range and negatively\nimpact performance. Our analysis reveals a marked contrast in quantization\neffectiveness across models. The older OPT model, upon which much of the\nquantization literature is based, shows significant performance deterioration\nand high susceptibility to outliers with varying calibration sets. In contrast,\nnewer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and Mistral 7B\ndemonstrate strong robustness, with Mistral 7B showing near-immunity to\noutliers and stable activations. These findings suggest a shift in PTQ\nstrategies might be needed. As advancements in pre-training methods reduce the\nrelevance of outliers, there is an emerging need to reassess the fundamentals\nof current quantization literature. The emphasis should pivot towards\noptimizing inference speed, rather than primarily focusing on outlier\npreservation, to align with the evolving characteristics of state-of-the-art\nLLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20835v3",
    "published_date": "2024-05-31 14:24:33 UTC",
    "updated_date": "2024-06-05 09:53:18 UTC"
  },
  {
    "arxiv_id": "2405.20806v2",
    "title": "The AI Alignment Paradox",
    "authors": [
      "Robert West",
      "Roland Aydin"
    ],
    "abstract": "The field of AI alignment aims to steer AI systems toward human goals,\npreferences, and ethical principles. Its contributions have been instrumental\nfor improving the output quality, safety, and trustworthiness of today's AI\nmodels. This perspective article draws attention to a fundamental challenge we\nsee in all AI alignment endeavors, which we term the \"AI alignment paradox\":\nThe better we align AI models with our values, the easier we may make it for\nadversaries to misalign the models. We illustrate the paradox by sketching\nthree concrete example incarnations for the case of language models, each\ncorresponding to a distinct way in which adversaries might exploit the paradox.\nWith AI's increasing real-world impact, it is imperative that a broad community\nof researchers be aware of the AI alignment paradox and work to find ways to\nmitigate it, in order to ensure the beneficial use of AI for the good of\nhumanity.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20806v2",
    "published_date": "2024-05-31 14:06:24 UTC",
    "updated_date": "2024-11-22 22:55:11 UTC"
  },
  {
    "arxiv_id": "2405.20797v2",
    "title": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model",
    "authors": [
      "Shiyin Lu",
      "Yang Li",
      "Qing-Guo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang",
      "Han-Jia Ye"
    ],
    "abstract": "Current Multimodal Large Language Models (MLLMs) typically integrate a\npre-trained LLM with another pre-trained vision transformer through a\nconnector, such as an MLP, endowing the LLM with visual capabilities. However,\nthe misalignment between two embedding strategies in MLLMs -- the structural\ntextual embeddings based on an embedding look-up table and the continuous\nembeddings generated directly by the vision encoder -- makes challenges for a\nmore seamless fusion of visual and textual information. We propose Ovis, a\nnovel MLLM architecture designed to structurally align visual and textual\nembeddings. Ovis integrates an additional learnable visual embedding table into\nthe visual encoder's process. To capture rich visual semantics, each image\npatch indexes the visual embedding table multiple times, resulting in a final\nvisual embedding that is a probabilistic combination of the indexed embeddings.\nThis structural approach mirrors the method used for generating textual\nembeddings. Empirical evaluations on various multimodal benchmarks show that\nOvis outperforms open-source MLLMs of similar parameter scales and even\nsurpasses the proprietary model Qwen-VL-Plus overall. These results highlight\nthe potential of Ovis' structured visual representation for advancing MLLM\narchitectural design and promoting more effective multimodal learning. Code,\ndatasets, and models are available at https://github.com/AIDC-AI/Ovis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20797v2",
    "published_date": "2024-05-31 13:59:18 UTC",
    "updated_date": "2024-06-17 17:51:50 UTC"
  },
  {
    "arxiv_id": "2405.20795v1",
    "title": "InsightSee: Advancing Multi-agent Vision-Language Models for Enhanced Visual Understanding",
    "authors": [
      "Huaxiang Zhang",
      "Yaojia Mu",
      "Guo-Niu Zhu",
      "Zhongxue Gan"
    ],
    "abstract": "Accurate visual understanding is imperative for advancing autonomous systems\nand intelligent robots. Despite the powerful capabilities of vision-language\nmodels (VLMs) in processing complex visual scenes, precisely recognizing\nobscured or ambiguously presented visual elements remains challenging. To\ntackle such issues, this paper proposes InsightSee, a multi-agent framework to\nenhance VLMs' interpretative capabilities in handling complex visual\nunderstanding scenarios. The framework comprises a description agent, two\nreasoning agents, and a decision agent, which are integrated to refine the\nprocess of visual information interpretation. The design of these agents and\nthe mechanisms by which they can be enhanced in visual information processing\nare presented. Experimental results demonstrate that the InsightSee framework\nnot only boosts performance on specific visual tasks but also retains the\noriginal models' strength. The proposed framework outperforms state-of-the-art\nalgorithms in 6 out of 9 benchmark tests, with a substantial advancement in\nmultimodal understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20795v1",
    "published_date": "2024-05-31 13:56:55 UTC",
    "updated_date": "2024-05-31 13:56:55 UTC"
  },
  {
    "arxiv_id": "2405.20748v1",
    "title": "OpenTensor: Reproducing Faster Matrix Multiplication Discovering Algorithms",
    "authors": [
      "Yiwen Sun",
      "Wenye Li"
    ],
    "abstract": "OpenTensor is a reproduction of AlphaTensor, which discovered a new algorithm\nthat outperforms the state-of-the-art methods for matrix multiplication by Deep\nReinforcement Learning (DRL). While AlphaTensor provides a promising framework\nfor solving scientific problems, it is really hard to reproduce due to the\nmassive tricks and lack of source codes. In this paper, we clean up the\nalgorithm pipeline, clarify the technical details, and make some improvements\nto the training process. Computational results show that OpenTensor can\nsuccessfully find efficient matrix multiplication algorithms.",
    "categories": [
      "cs.AI",
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20748v1",
    "published_date": "2024-05-31 10:30:14 UTC",
    "updated_date": "2024-05-31 10:30:14 UTC"
  },
  {
    "arxiv_id": "2405.20743v2",
    "title": "Trajectory Forecasting through Low-Rank Adaptation of Discrete Latent Codes",
    "authors": [
      "Riccardo Benaglia",
      "Angelo Porrello",
      "Pietro Buzzega",
      "Simone Calderara",
      "Rita Cucchiara"
    ],
    "abstract": "Trajectory forecasting is crucial for video surveillance analytics, as it\nenables the anticipation of future movements for a set of agents, e.g.\nbasketball players engaged in intricate interactions with long-term intentions.\nDeep generative models offer a natural learning approach for trajectory\nforecasting, yet they encounter difficulties in achieving an optimal balance\nbetween sampling fidelity and diversity. We address this challenge by\nleveraging Vector Quantized Variational Autoencoders (VQ-VAEs), which utilize a\ndiscrete latent space to tackle the issue of posterior collapse. Specifically,\nwe introduce an instance-based codebook that allows tailored latent\nrepresentations for each example. In a nutshell, the rows of the codebook are\ndynamically adjusted to reflect contextual information (i.e., past motion\npatterns extracted from the observed trajectories). In this way, the\ndiscretization process gains flexibility, leading to improved reconstructions.\nNotably, instance-level dynamics are injected into the codebook through\nlow-rank updates, which restrict the customization of the codebook to a lower\ndimension space. The resulting discrete space serves as the basis of the\nsubsequent step, which regards the training of a diffusion-based predictive\nmodel. We show that such a two-fold framework, augmented with instance-level\ndiscretization, leads to accurate and diverse forecasts, yielding\nstate-of-the-art performance on three established benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 3 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.20743v2",
    "published_date": "2024-05-31 10:13:17 UTC",
    "updated_date": "2024-08-29 15:31:58 UTC"
  },
  {
    "arxiv_id": "2405.20731v1",
    "title": "Maximum Temperature Prediction Using Remote Sensing Data Via Convolutional Neural Network",
    "authors": [
      "Lorenzo Innocenti",
      "Giacomo Blanco",
      "Luca Barco",
      "Claudio Rossi"
    ],
    "abstract": "Urban heat islands, defined as specific zones exhibiting substantially higher\ntemperatures than their immediate environs, pose significant threats to\nenvironmental sustainability and public health. This study introduces a novel\nmachine-learning model that amalgamates data from the Sentinel-3 satellite,\nmeteorological predictions, and additional remote sensing inputs. The primary\naim is to generate detailed spatiotemporal maps that forecast the peak\ntemperatures within a 24-hour period in Turin. Experimental results validate\nthe model's proficiency in predicting temperature patterns, achieving a Mean\nAbsolute Error (MAE) of 2.09 degrees Celsius for the year 2023 at a resolution\nof 20 meters per pixel, thereby enriching our knowledge of urban climatic\nbehavior. This investigation enhances the understanding of urban microclimates,\nemphasizing the importance of cross-disciplinary data integration, and laying\nthe groundwork for informed policy-making aimed at alleviating the negative\nimpacts of extreme urban temperatures.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "I.2.10; G.3"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages, submitted to IEEE MetroLivEnv 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2405.20731v1",
    "published_date": "2024-05-31 09:39:41 UTC",
    "updated_date": "2024-05-31 09:39:41 UTC"
  },
  {
    "arxiv_id": "2405.20727v1",
    "title": "GANcrop: A Contrastive Defense Against Backdoor Attacks in Federated Learning",
    "authors": [
      "Xiaoyun Gan",
      "Shanyu Gan",
      "Taizhi Su",
      "Peng Liu"
    ],
    "abstract": "With heightened awareness of data privacy protection, Federated Learning (FL)\nhas attracted widespread attention as a privacy-preserving distributed machine\nlearning method. However, the distributed nature of federated learning also\nprovides opportunities for backdoor attacks, where attackers can guide the\nmodel to produce incorrect predictions without affecting the global model\ntraining process.\n  This paper introduces a novel defense mechanism against backdoor attacks in\nfederated learning, named GANcrop. This approach leverages contrastive learning\nto deeply explore the disparities between malicious and benign models for\nattack identification, followed by the utilization of Generative Adversarial\nNetworks (GAN) to recover backdoor triggers and implement targeted mitigation\nstrategies. Experimental findings demonstrate that GANcrop effectively\nsafeguards against backdoor attacks, particularly in non-IID scenarios, while\nmaintaining satisfactory model accuracy, showcasing its remarkable defensive\nefficacy and practical utility.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20727v1",
    "published_date": "2024-05-31 09:33:16 UTC",
    "updated_date": "2024-05-31 09:33:16 UTC"
  },
  {
    "arxiv_id": "2405.20725v2",
    "title": "GI-NAS: Boosting Gradient Inversion Attacks through Adaptive Neural Architecture Search",
    "authors": [
      "Wenbo Yu",
      "Hao Fang",
      "Bin Chen",
      "Xiaohang Sui",
      "Chuan Chen",
      "Hao Wu",
      "Shu-Tao Xia",
      "Ke Xu"
    ],
    "abstract": "Gradient Inversion Attacks invert the transmitted gradients in Federated\nLearning (FL) systems to reconstruct the sensitive data of local clients and\nhave raised considerable privacy concerns. A majority of gradient inversion\nmethods rely heavily on explicit prior knowledge (e.g., a well pre-trained\ngenerative model), which is often unavailable in realistic scenarios. To\nalleviate this issue, researchers have proposed to leverage the implicit prior\nknowledge of an over-parameterized network. However, they only utilize a fixed\nneural architecture for all the attack settings. This would hinder the adaptive\nuse of implicit architectural priors and consequently limit the\ngeneralizability. In this paper, we further exploit such implicit prior\nknowledge by proposing Gradient Inversion via Neural Architecture Search\n(GI-NAS), which adaptively searches the network and captures the implicit\npriors behind neural architectures. Extensive experiments verify that our\nproposed GI-NAS can achieve superior attack performance compared to\nstate-of-the-art gradient inversion methods, even under more practical settings\nwith high-resolution images, large-sized batches, and advanced defense\nstrategies.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20725v2",
    "published_date": "2024-05-31 09:29:43 UTC",
    "updated_date": "2024-10-25 09:26:49 UTC"
  },
  {
    "arxiv_id": "2405.20721v1",
    "title": "ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model",
    "authors": [
      "Yufei Wang",
      "Zhihao Li",
      "Lanqing Guo",
      "Wenhan Yang",
      "Alex C. Kot",
      "Bihan Wen"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has become a promising framework for\nnovel view synthesis, offering fast rendering speeds and high fidelity.\nHowever, the large number of Gaussians and their associated attributes require\neffective compression techniques. Existing methods primarily compress neural\nGaussians individually and independently, i.e., coding all the neural Gaussians\nat the same time, with little design for their interactions and spatial\ndependence. Inspired by the effectiveness of the context model in image\ncompression, we propose the first autoregressive model at the anchor level for\n3DGS compression in this work. We divide anchors into different levels and the\nanchors that are not coded yet can be predicted based on the already coded ones\nin all the coarser levels, leading to more accurate modeling and higher coding\nefficiency. To further improve the efficiency of entropy coding, e.g., to code\nthe coarsest level with no already coded anchors, we propose to introduce a\nlow-dimensional quantized feature as the hyperprior for each anchor, which can\nbe effectively compressed. Our work pioneers the context model in the anchor\nlevel for 3DGS representation, yielding an impressive size reduction of over\n100 times compared to vanilla 3DGS and 15 times compared to the most recent\nstate-of-the-art work Scaffold-GS, while achieving comparable or even higher\nrendering quality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20721v1",
    "published_date": "2024-05-31 09:23:39 UTC",
    "updated_date": "2024-05-31 09:23:39 UTC"
  },
  {
    "arxiv_id": "2405.20719v1",
    "title": "Climate Variable Downscaling with Conditional Normalizing Flows",
    "authors": [
      "Christina Winkler",
      "Paula Harder",
      "David Rolnick"
    ],
    "abstract": "Predictions of global climate models typically operate on coarse spatial\nscales due to the large computational costs of climate simulations. This has\nled to a considerable interest in methods for statistical downscaling, a\nsimilar process to super-resolution in the computer vision context, to provide\nmore local and regional climate information. In this work, we apply conditional\nnormalizing flows to the task of climate variable downscaling. We showcase its\nsuccessful performance on an ERA5 water content dataset for different\nupsampling factors. Additionally, we show that the method allows us to assess\nthe predictive uncertainty in terms of standard deviation from the fitted\nconditional distribution mean.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "physics.ao-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20719v1",
    "published_date": "2024-05-31 09:20:33 UTC",
    "updated_date": "2024-05-31 09:20:33 UTC"
  },
  {
    "arxiv_id": "2405.20718v2",
    "title": "Popularity-Aware Alignment and Contrast for Mitigating Popularity Bias",
    "authors": [
      "Miaomiao Cai",
      "Lei Chen",
      "Yifan Wang",
      "Haoyue Bai",
      "Peijie Sun",
      "Le Wu",
      "Min Zhang",
      "Meng Wang"
    ],
    "abstract": "Collaborative Filtering (CF) typically suffers from the significant challenge\nof popularity bias due to the uneven distribution of items in real-world\ndatasets. This bias leads to a significant accuracy gap between popular and\nunpopular items. It not only hinders accurate user preference understanding but\nalso exacerbates the Matthew effect in recommendation systems. To alleviate\npopularity bias, existing efforts focus on emphasizing unpopular items or\nseparating the correlation between item representations and their popularity.\nDespite the effectiveness, existing works still face two persistent challenges:\n(1) how to extract common supervision signals from popular items to improve the\nunpopular item representations, and (2) how to alleviate the representation\nseparation caused by popularity bias. In this work, we conduct an empirical\nanalysis of popularity bias and propose Popularity-Aware Alignment and Contrast\n(PAAC) to address two challenges. Specifically, we use the common supervisory\nsignals modeled in popular item representations and propose a novel\npopularity-aware supervised alignment module to learn unpopular item\nrepresentations. Additionally, we suggest re-weighting the contrastive learning\nloss to mitigate the representation separation from a popularity-centric\nperspective. Finally, we validate the effectiveness and rationale of PAAC in\nmitigating popularity bias through extensive experiments on three real-world\ndatasets. Our code is available at\nhttps://github.com/miaomiao-cai2/KDD2024-PAAC.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20718v2",
    "published_date": "2024-05-31 09:14:48 UTC",
    "updated_date": "2024-06-11 09:29:46 UTC"
  },
  {
    "arxiv_id": "2405.20708v1",
    "title": "FinGen: A Dataset for Argument Generation in Finance",
    "authors": [
      "Chung-Chi Chen",
      "Hiroya Takamura",
      "Ichiro Kobayashi",
      "Yusuke Miyao"
    ],
    "abstract": "Thinking about the future is one of the important activities that people do\nin daily life. Futurists also pay a lot of effort into figuring out possible\nscenarios for the future. We argue that the exploration of this direction is\nstill in an early stage in the NLP research. To this end, we propose three\nargument generation tasks in the financial application scenario. Our\nexperimental results show these tasks are still big challenges for\nrepresentative generation models. Based on our empirical results, we further\npoint out several unresolved issues and challenges in this research direction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20708v1",
    "published_date": "2024-05-31 09:00:43 UTC",
    "updated_date": "2024-05-31 09:00:43 UTC"
  },
  {
    "arxiv_id": "2405.20705v2",
    "title": "ADESSE: Advice Explanations in Complex Repeated Decision-Making Environments",
    "authors": [
      "Sören Schleibaum",
      "Lu Feng",
      "Sarit Kraus",
      "Jörg P. Müller"
    ],
    "abstract": "In the evolving landscape of human-centered AI, fostering a synergistic\nrelationship between humans and AI agents in decision-making processes stands\nas a paramount challenge. This work considers a problem setup where an\nintelligent agent comprising a neural network-based prediction component and a\ndeep reinforcement learning component provides advice to a human decision-maker\nin complex repeated decision-making environments. Whether the human\ndecision-maker would follow the agent's advice depends on their beliefs and\ntrust in the agent and on their understanding of the advice itself. To this\nend, we developed an approach named ADESSE to generate explanations about the\nadviser agent to improve human trust and decision-making. Computational\nexperiments on a range of environments with varying model sizes demonstrate the\napplicability and scalability of ADESSE. Furthermore, an interactive game-based\nuser study shows that participants were significantly more satisfied, achieved\na higher reward in the game, and took less time to select an action when\npresented with explanations generated by ADESSE. These findings illuminate the\ncritical role of tailored, human-centered explanations in AI-assisted\ndecision-making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20705v2",
    "published_date": "2024-05-31 08:59:20 UTC",
    "updated_date": "2024-09-10 09:49:54 UTC"
  },
  {
    "arxiv_id": "2405.20701v1",
    "title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
    "authors": [
      "Pengwei Zhan",
      "Zhen Xu",
      "Qian Tan",
      "Jie Song",
      "Ru Xie"
    ],
    "abstract": "Large language models (LLMs) demonstrate exceptional instruct-following\nability to complete various downstream tasks. Although this impressive ability\nmakes LLMs flexible task solvers, their performance in solving tasks also\nheavily relies on instructions. In this paper, we reveal that LLMs are\nover-sensitive to lexical variations in task instructions, even when the\nvariations are imperceptible to humans. By providing models with neighborhood\ninstructions, which are closely situated in the latent representation space and\ndiffer by only one semantically similar word, the performance on downstream\ntasks can be vastly different. Following this property, we propose a black-box\nCombinatorial Optimization framework for Prompt Lexical Enhancement (COPLE).\nCOPLE performs iterative lexical optimization according to the feedback from a\nbatch of proxy tasks, using a search strategy related to word influence.\nExperiments show that even widely-used human-crafted prompts for current\nbenchmarks suffer from the lexical sensitivity of models, and COPLE recovers\nthe declined model ability in both instruct-following and solving downstream\ntasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20701v1",
    "published_date": "2024-05-31 08:53:59 UTC",
    "updated_date": "2024-05-31 08:53:59 UTC"
  },
  {
    "arxiv_id": "2405.20700v1",
    "title": "Self-degraded contrastive domain adaptation for industrial fault diagnosis with bi-imbalanced data",
    "authors": [
      "Gecheng Chen",
      "Zeyu Yang",
      "Chengwen Luo",
      "Jianqiang Li"
    ],
    "abstract": "Modern industrial fault diagnosis tasks often face the combined challenge of\ndistribution discrepancy and bi-imbalance. Existing domain adaptation\napproaches pay little attention to the prevailing bi-imbalance, leading to poor\ndomain adaptation performance or even negative transfer. In this work, we\npropose a self-degraded contrastive domain adaptation (Sd-CDA) diagnosis\nframework to handle the domain discrepancy under the bi-imbalanced data. It\nfirst pre-trains the feature extractor via imbalance-aware contrastive learning\nbased on model pruning to learn the feature representation efficiently in a\nself-supervised manner. Then it forces the samples away from the domain\nboundary based on supervised contrastive domain adversarial learning\n(SupCon-DA) and ensures the features generated by the feature extractor are\ndiscriminative enough. Furthermore, we propose the pruned contrastive domain\nadversarial learning (PSupCon-DA) to pay automatically re-weighted attention to\nthe minorities to enhance the performance towards bi-imbalanced data. We show\nthe superiority of the proposed method via two experiments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20700v1",
    "published_date": "2024-05-31 08:51:57 UTC",
    "updated_date": "2024-05-31 08:51:57 UTC"
  },
  {
    "arxiv_id": "2405.20692v1",
    "title": "In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought",
    "authors": [
      "Sili Huang",
      "Jifeng Hu",
      "Hechang Chen",
      "Lichao Sun",
      "Bo Yang"
    ],
    "abstract": "In-context learning is a promising approach for offline reinforcement\nlearning (RL) to handle online tasks, which can be achieved by providing task\nprompts. Recent works demonstrated that in-context RL could emerge with\nself-improvement in a trial-and-error manner when treating RL tasks as an\nacross-episodic sequential prediction problem. Despite the self-improvement not\nrequiring gradient updates, current works still suffer from high computational\ncosts when the across-episodic sequence increases with task horizons. To this\nend, we propose an In-context Decision Transformer (IDT) to achieve\nself-improvement in a high-level trial-and-error manner. Specifically, IDT is\ninspired by the efficient hierarchical structure of human decision-making and\nthus reconstructs the sequence to consist of high-level decisions instead of\nlow-level actions that interact with environments. As one high-level decision\ncan guide multi-step low-level actions, IDT naturally avoids excessively long\nsequences and solves online tasks more efficiently. Experimental results show\nthat IDT achieves state-of-the-art in long-horizon tasks over current\nin-context RL methods. In particular, the online evaluation time of our IDT is\n\\textbf{36$\\times$} times faster than baselines in the D4RL benchmark and\n\\textbf{27$\\times$} times faster in the Grid World benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20692v1",
    "published_date": "2024-05-31 08:38:25 UTC",
    "updated_date": "2024-05-31 08:38:25 UTC"
  },
  {
    "arxiv_id": "2405.20687v1",
    "title": "Conditioning GAN Without Training Dataset",
    "authors": [
      "Kidist Amde Mekonnen"
    ],
    "abstract": "Deep learning algorithms have a large number of trainable parameters often\nwith sizes of hundreds of thousands or more. Training this algorithm requires a\nlarge amount of training data and generating a sufficiently large dataset for\nthese algorithms is costly\\cite{noguchi2019image}.\n  GANs are generative neural networks that use two deep learning networks that\nare competing with each other. The networks are generator and discriminator\nnetworks. The generator tries to generate realistic images which resemble the\nactual training dataset by approximating the training data distribution and the\ndiscriminator is trained to classify images as real or\nfake(generated)\\cite{goodfellow2016nips}. Training these GAN algorithms also\nrequires a large amount of training dataset\\cite{noguchi2019image}.\n  In this study, the aim is to address the question, \"Given an unconditioned\npretrained generator network and a pretrained classifier, is it feasible to\ndevelop a conditioned generator without relying on any training dataset?\"\n  The paper begins with a general introduction to the problem. The subsequent\nsections are structured as follows: Section 2 provides background information\non the problem. Section 3 reviews relevant literature on the topic. Section 4\noutlines the methodology employed in this study. Section 5 presents the\nexperimental results. Section 6 discusses the findings and proposes potential\nfuture research directions. Finally, Section 7 offers concluding remarks.\n  The implementation can be accessed\n\\href{https://github.com/kidist-amde/BigGAN-PyTorch}{here}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 2 figures, Part of my MSc project course, School Project\n  Course 2022",
    "pdf_url": "http://arxiv.org/pdf/2405.20687v1",
    "published_date": "2024-05-31 08:31:26 UTC",
    "updated_date": "2024-05-31 08:31:26 UTC"
  },
  {
    "arxiv_id": "2405.20681v3",
    "title": "No Free Lunch Theorem for Privacy-Preserving LLM Inference",
    "authors": [
      "Xiaojin Zhang",
      "Yahao Pang",
      "Yan Kang",
      "Wei Chen",
      "Lixin Fan",
      "Hai Jin",
      "Qiang Yang"
    ],
    "abstract": "Individuals and businesses have been significantly benefited by Large\nLanguage Models (LLMs) including PaLM, Gemini and ChatGPT in various ways. For\nexample, LLMs enhance productivity, reduce costs, and enable us to focus on\nmore valuable tasks. Furthermore, LLMs possess the capacity to sift through\nextensive datasets, uncover underlying patterns, and furnish critical insights\nthat propel the frontiers of technology and science. However, LLMs also pose\nprivacy concerns. Users' interactions with LLMs may expose their sensitive\npersonal or company information. A lack of robust privacy safeguards and legal\nframeworks could permit the unwarranted intrusion or improper handling of\nindividual data, thereby risking infringements of privacy and the theft of\npersonal identities. To ensure privacy, it is essential to minimize the\ndependency between shared prompts and private information. Various\nrandomization approaches have been proposed to protect prompts' privacy, but\nthey may incur utility loss compared to unprotected LLMs prompting. Therefore,\nit is essential to evaluate the balance between the risk of privacy leakage and\nloss of utility when conducting effective protection mechanisms. The current\nstudy develops a framework for inferring privacy-protected Large Language\nModels (LLMs) and lays down a solid theoretical basis for examining the\ninterplay between privacy preservation and utility. The core insight is\nencapsulated within a theorem that is called as the NFL (abbreviation of the\nword No-Free-Lunch) Theorem.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20681v3",
    "published_date": "2024-05-31 08:22:53 UTC",
    "updated_date": "2025-02-28 02:38:26 UTC"
  },
  {
    "arxiv_id": "2405.20680v5",
    "title": "Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models",
    "authors": [
      "Mingda Li",
      "Xinyu Li",
      "Yifan Chen",
      "Wenfeng Xuan",
      "Weinan Zhang"
    ],
    "abstract": "Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their\nsuperiority in terms of factuality, they do not consistently outperform the\noriginal retrieval-free Language Models (LMs). Our experiments reveal that this\nexample-level performance inconsistency exists not only between\nretrieval-augmented and retrieval-free LM but also among different retrievers.\nTo understand this phenomenon, we investigate the degeneration behavior of\nRALMs and theoretically decompose it into four categories. Further analysis\nbased on our decomposition reveals that the innate difference in knowledge\nsources and the unpredictable degeneration of the reader model contribute most\nto the inconsistency. Drawing from our analysis, we introduce Ensemble of\nRetrievers (EoR), a trainable framework that can adaptively retrieve from\ndifferent knowledge sources and effectively decrease unpredictable reader\nerrors. Our experiments on Open Domain Question Answering show that EoR\nsubstantially improves performance over the RALM with a single retriever by\nconsiderably reducing inconsistent behaviors.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "ACL 2024 (findings)",
    "pdf_url": "http://arxiv.org/pdf/2405.20680v5",
    "published_date": "2024-05-31 08:22:49 UTC",
    "updated_date": "2025-03-06 05:34:13 UTC"
  },
  {
    "arxiv_id": "2405.20675v1",
    "title": "Adv-KD: Adversarial Knowledge Distillation for Faster Diffusion Sampling",
    "authors": [
      "Kidist Amde Mekonnen",
      "Nicola Dall'Asen",
      "Paolo Rota"
    ],
    "abstract": "Diffusion Probabilistic Models (DPMs) have emerged as a powerful class of\ndeep generative models, achieving remarkable performance in image synthesis\ntasks. However, these models face challenges in terms of widespread adoption\ndue to their reliance on sequential denoising steps during sample generation.\nThis dependence leads to substantial computational requirements, making them\nunsuitable for resource-constrained or real-time processing systems. To address\nthese challenges, we propose a novel method that integrates denoising phases\ndirectly into the model's architecture, thereby reducing the need for\nresource-intensive computations. Our approach combines diffusion models with\ngenerative adversarial networks (GANs) through knowledge distillation, enabling\nmore efficient training and evaluation. By utilizing a pre-trained diffusion\nmodel as a teacher model, we train a student model through adversarial\nlearning, employing layerwise transformations for denoising and submodules for\npredicting the teacher model's output at various points in time. This\nintegration significantly reduces the number of parameters and denoising steps\nrequired, leading to improved sampling speed at test time. We validate our\nmethod with extensive experiments, demonstrating comparable performance with\nreduced computational requirements compared to existing approaches. By enabling\nthe deployment of diffusion models on resource-constrained devices, our\nresearch mitigates their computational burden and paves the way for wider\naccessibility and practical use across the research community and end-users.\n  Our code is publicly available at https://github.com/kidist-amde/Adv-KD",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 11 figures, ELLIS Doctoral Symposium 2023 in Helsinki,\n  Finland",
    "pdf_url": "http://arxiv.org/pdf/2405.20675v1",
    "published_date": "2024-05-31 08:19:44 UTC",
    "updated_date": "2024-05-31 08:19:44 UTC"
  },
  {
    "arxiv_id": "2405.20671v2",
    "title": "Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure",
    "authors": [
      "Hanseul Cho",
      "Jaeyoung Cha",
      "Pranjal Awasthi",
      "Srinadh Bhojanapalli",
      "Anupam Gupta",
      "Chulhee Yun"
    ],
    "abstract": "Even for simple arithmetic tasks like integer addition, it is challenging for\nTransformers to generalize to longer sequences than those encountered during\ntraining. To tackle this problem, we propose position coupling, a simple yet\neffective method that directly embeds the structure of the tasks into the\npositional encoding of a (decoder-only) Transformer. Taking a departure from\nthe vanilla absolute position mechanism assigning unique position IDs to each\nof the tokens, we assign the same position IDs to two or more \"relevant\"\ntokens; for integer addition tasks, we regard digits of the same significance\nas in the same position. On the empirical side, we show that with the proposed\nposition coupling, our models trained on 1 to 30-digit additions can generalize\nup to 200-digit additions (6.67x of the trained length). On the theoretical\nside, we prove that a 1-layer Transformer with coupled positions can solve the\naddition task involving exponentially many digits, whereas any 1-layer\nTransformer without positional information cannot entirely solve it. We also\ndemonstrate that position coupling can be applied to other algorithmic tasks\nsuch as Nx2 multiplication and a two-dimensional task.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024. 76 pages. 23 figures. 90 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.20671v2",
    "published_date": "2024-05-31 08:13:35 UTC",
    "updated_date": "2024-10-30 16:50:43 UTC"
  },
  {
    "arxiv_id": "2405.20656v5",
    "title": "Automatic Counting and Classification of Mosquito Eggs in Field Traps",
    "authors": [
      "Javier Naranjo-Alcazar",
      "Jordi Grau-Haro",
      "Pedro Zuccarello",
      "David Almenar",
      "Jesus Lopez-Ballester"
    ],
    "abstract": "Insect pest control poses a global challenge, affecting public health, food\nsafety, and the environment. Diseases transmitted by mosquitoes are expanding\nbeyond tropical regions due to climate change. Agricultural pests further\nexacerbate economic losses by damaging crops. The Sterile Insect Technique\n(SIT) emerges as an eco-friendly alternative to chemical pesticides, involving\nthe sterilization and release of male insects to curb population growth. This\nwork focuses on the automation of the analysis of field ovitraps used to\nfollow-up a SIT program for the Aedes albopictus mosquito in the Valencian\nCommunity, Spain, funded by the Conselleria de Agricultura, Agua, Ganaderia y\nPesca. Previous research has leveraged deep learning algorithms to automate egg\ncounting in ovitraps, yet faced challenges such as manual handling and limited\nanalysis capacity. Innovations in our study include classifying eggs as hatched\nor unhatched and reconstructing ovitraps from partial images, mitigating issues\nof duplicity and cut eggs. Also, our device can analyze multiple ovitraps\nsimultaneously without the need of manual replacement. This approach\nsignificantly enhances the accuracy of egg counting and classification,\nproviding a valuable tool for large-scale field studies.\n  This document describes part of the work of the project Application of\nIndustry 4.0 techniques to the production of tiger mosquitoes for the Sterile\nInsect Technique (MoTIA2,IMDEEA/2022/70), financed by the Valencian Institute\nfor Business Competitiveness (IVACE) and the FEDER funds. The participation of\nJ.Naranjo-Alcazar, J.Grau-Haro and P.Zuccarello has been possible thanks to\nfunding from IVACE and FEDER funds. The participation of D.Almenar has been\nfinanced by the Conselleria de Agricultura, Agua, Ganaderia y Pesca of the\nGeneralitat Valenciana and the Subdireccion de Innovacion y Desarrollo de\nServicios (TRAGSA group).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20656v5",
    "published_date": "2024-05-31 07:48:48 UTC",
    "updated_date": "2024-10-14 13:39:13 UTC"
  },
  {
    "arxiv_id": "2405.20653v2",
    "title": "Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens",
    "authors": [
      "Jiahao Yu",
      "Haozheng Luo",
      "Jerry Yao-Chieh Hu",
      "Wenbo Guo",
      "Han Liu",
      "Xinyu Xing"
    ],
    "abstract": "Along with the remarkable successes of Language language models, recent\nresearch also started to explore the security threats of LLMs, including\njailbreaking attacks. Attackers carefully craft jailbreaking prompts such that\na target LLM will respond to the harmful question. Existing jailbreaking\nattacks require either human experts or leveraging complicated algorithms to\ncraft jailbreaking prompts. In this paper, we introduce BOOST, a simple attack\nthat leverages only the eos tokens. We demonstrate that rather than\nconstructing complicated jailbreaking prompts, the attacker can simply append a\nfew eos tokens to the end of a harmful question. It will bypass the safety\nalignment of LLMs and lead to successful jailbreaking attacks. We further apply\nBOOST to four representative jailbreak methods and show that the attack success\nrates of these methods can be significantly enhanced by simply adding eos\ntokens to the prompt. To understand this simple but novel phenomenon, we\nconduct empirical analyses. Our analysis reveals that adding eos tokens makes\nthe target LLM believe the input is much less harmful, and eos tokens have low\nattention values and do not affect LLM's understanding of the harmful\nquestions, leading the model to actually respond to the questions. Our findings\nuncover how fragile an LLM is against jailbreak attacks, motivating the\ndevelopment of strong safety alignment approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20653v2",
    "published_date": "2024-05-31 07:41:03 UTC",
    "updated_date": "2024-06-04 20:29:48 UTC"
  },
  {
    "arxiv_id": "2405.20643v1",
    "title": "Learning Gaze-aware Compositional GAN",
    "authors": [
      "Nerea Aranjuelo",
      "Siyu Huang",
      "Ignacio Arganda-Carreras",
      "Luis Unzueta",
      "Oihana Otaegui",
      "Hanspeter Pfister",
      "Donglai Wei"
    ],
    "abstract": "Gaze-annotated facial data is crucial for training deep neural networks\n(DNNs) for gaze estimation. However, obtaining these data is labor-intensive\nand requires specialized equipment due to the challenge of accurately\nannotating the gaze direction of a subject. In this work, we present a\ngenerative framework to create annotated gaze data by leveraging the benefits\nof labeled and unlabeled data sources. We propose a Gaze-aware Compositional\nGAN that learns to generate annotated facial images from a limited labeled\ndataset. Then we transfer this model to an unlabeled data domain to take\nadvantage of the diversity it provides. Experiments demonstrate our approach's\neffectiveness in generating within-domain image augmentations in the ETH-XGaze\ndataset and cross-domain augmentations in the CelebAMask-HQ dataset domain for\ngaze estimation DNN training. We also show additional applications of our work,\nwhich include facial image editing and gaze redirection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ETRA 2024 as Full paper, and as journal paper in\n  Proceedings of the ACM on Computer Graphics and Interactive Techniques",
    "pdf_url": "http://arxiv.org/pdf/2405.20643v1",
    "published_date": "2024-05-31 07:07:54 UTC",
    "updated_date": "2024-05-31 07:07:54 UTC"
  },
  {
    "arxiv_id": "2405.20628v2",
    "title": "ToxVidLM: A Multimodal Framework for Toxicity Detection in Code-Mixed Videos",
    "authors": [
      "Krishanu Maity",
      "A. S. Poornash",
      "Sriparna Saha",
      "Pushpak Bhattacharyya"
    ],
    "abstract": "In an era of rapidly evolving internet technology, the surge in multimodal\ncontent, including videos, has expanded the horizons of online communication.\nHowever, the detection of toxic content in this diverse landscape, particularly\nin low-resource code-mixed languages, remains a critical challenge. While\nsubstantial research has addressed toxic content detection in textual data, the\nrealm of video content, especially in non-English languages, has been\nrelatively underexplored. This paper addresses this research gap by introducing\na benchmark dataset, the first of its kind, consisting of 931 videos with 4021\ncode-mixed Hindi-English utterances collected from YouTube. Each utterance\nwithin this dataset has been meticulously annotated for toxicity, severity, and\nsentiment labels. We have developed an advanced Multimodal Multitask framework\nbuilt for Toxicity detection in Video Content by leveraging Language Models\n(LMs), crafted for the primary objective along with the additional tasks of\nconducting sentiment and severity analysis. ToxVidLM incorporates three key\nmodules - the Encoder module, Cross-Modal Synchronization module, and Multitask\nmodule - crafting a generic multimodal LM customized for intricate video\nclassification tasks. Our experiments reveal that incorporating multiple\nmodalities from the videos substantially enhances the performance of toxic\ncontent detection by achieving an Accuracy and Weighted F1 score of 94.29% and\n94.35%, respectively.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as a Long Paper in ACL Findings 2024. For acceptance\n  details, see https://2024.aclweb.org/program/finding_papers/",
    "pdf_url": "http://arxiv.org/pdf/2405.20628v2",
    "published_date": "2024-05-31 05:40:56 UTC",
    "updated_date": "2024-07-14 07:09:42 UTC"
  },
  {
    "arxiv_id": "2405.20625v1",
    "title": "Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning",
    "authors": [
      "Atharva Gundawar",
      "Mudit Verma",
      "Lin Guan",
      "Karthik Valmeekam",
      "Siddhant Bhambri",
      "Subbarao Kambhampati"
    ],
    "abstract": "As the applicability of Large Language Models (LLMs) extends beyond\ntraditional text processing tasks, there is a burgeoning interest in their\npotential to excel in planning and reasoning assignments, realms traditionally\nreserved for System 2 cognitive competencies. Despite their perceived\nversatility, the research community is still unraveling effective strategies to\nharness these models in such complex domains. The recent discourse introduced\nby the paper on LLM Modulo marks a significant stride, proposing a conceptual\nframework that enhances the integration of LLMs into diverse planning and\nreasoning activities. This workshop paper delves into the practical application\nof this framework within the domain of travel planning, presenting a specific\ninstance of its implementation. We are using the Travel Planning benchmark by\nthe OSU NLP group, a benchmark for evaluating the performance of LLMs in\nproducing valid itineraries based on user queries presented in natural\nlanguage. While popular methods of enhancing the reasoning abilities of LLMs\nsuch as Chain of Thought, ReAct, and Reflexion achieve a meager 0%, 0.6%, and\n0% with GPT3.5-Turbo respectively, our operationalization of the LLM-Modulo\nframework for TravelPlanning domain provides a remarkable improvement,\nenhancing baseline performances by 4.6x for GPT4-Turbo and even more for older\nmodels like GPT3.5-Turbo from 0% to 5%. Furthermore, we highlight the other\nuseful roles of LLMs in the planning pipeline, as suggested in LLM-Modulo,\nwhich can be reliably operationalized such as extraction of useful critics and\nreformulator for critics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20625v1",
    "published_date": "2024-05-31 05:23:35 UTC",
    "updated_date": "2024-05-31 05:23:35 UTC"
  },
  {
    "arxiv_id": "2405.20624v1",
    "title": "Leveraging Large Language Models for Entity Matching",
    "authors": [
      "Qianyu Huang",
      "Tongfang Zhao"
    ],
    "abstract": "Entity matching (EM) is a critical task in data integration, aiming to\nidentify records across different datasets that refer to the same real-world\nentities. Traditional methods often rely on manually engineered features and\nrule-based systems, which struggle with diverse and unstructured data. The\nemergence of Large Language Models (LLMs) such as GPT-4 offers transformative\npotential for EM, leveraging their advanced semantic understanding and\ncontextual capabilities. This vision paper explores the application of LLMs to\nEM, discussing their advantages, challenges, and future research directions.\nAdditionally, we review related work on applying weak supervision and\nunsupervised approaches to EM, highlighting how LLMs can enhance these methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20624v1",
    "published_date": "2024-05-31 05:22:07 UTC",
    "updated_date": "2024-05-31 05:22:07 UTC"
  },
  {
    "arxiv_id": "2406.11872v3",
    "title": "The EarlyBird Gets the WORM: Heuristically Accelerating EarlyBird Convergence",
    "authors": [
      "Adithya Vasudev"
    ],
    "abstract": "The Lottery Ticket hypothesis proposes that ideal, sparse subnetworks, called\nlottery tickets, exist in untrained dense neural networks. The Early Bird\nhypothesis proposes an efficient algorithm to find these winning lottery\ntickets in convolutional neural networks, using the novel concept of distance\nbetween subnetworks to detect convergence in the subnetworks of a model.\nHowever, this approach overlooks unchanging groups of unimportant neurons near\nthe search's end. We proposes WORM, a method that exploits these static groups\nby truncating their gradients, forcing the model to rely on other neurons.\nExperiments show WORM achieves faster ticket identification during training on\nconvolutional neural networks, despite the additional computational overhead,\nwhen compared to EarlyBird search. Additionally, WORM-pruned models lose less\naccuracy during pruning and recover accuracy faster, improving the robustness\nof a given model. Furthermore, WORM is also able to generalize the Early Bird\nhypothesis reasonably well to larger models, such as transformers, displaying\nits flexibility to adapt to more complex architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the Efficient Natural Language and Speech Processing\n  Workshop at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11872v3",
    "published_date": "2024-05-31 05:13:02 UTC",
    "updated_date": "2024-12-11 01:21:13 UTC"
  },
  {
    "arxiv_id": "2405.20612v2",
    "title": "UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation",
    "authors": [
      "Hanzhang Zhou",
      "Zijian Feng",
      "Zixiao Zhu",
      "Junlang Qian",
      "Kezhi Mao"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious tasks using the in-context learning (ICL) paradigm. However, their\neffectiveness is often compromised by inherent bias, leading to prompt\nbrittleness, i.e., sensitivity to design settings such as example selection,\norder, and prompt formatting. Previous studies have addressed LLM bias through\nexternal adjustment of model outputs, but the internal mechanisms that lead to\nsuch bias remain unexplored. Our work delves into these mechanisms,\nparticularly investigating how feedforward neural networks (FFNs) and attention\nheads result in the bias of LLMs. By Interpreting the contribution of\nindividual FFN vectors and attention heads, we identify the biased LLM\ncomponents that skew LLMs' prediction toward specific labels. To mitigate these\nbiases, we introduce UniBias, an inference-only method that effectively\nidentifies and eliminates biased FFN vectors and attention heads. Extensive\nexperiments across 12 NLP datasets demonstrate that UniBias significantly\nenhances ICL performance and alleviates prompt brittleness of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20612v2",
    "published_date": "2024-05-31 03:59:15 UTC",
    "updated_date": "2024-12-12 10:46:44 UTC"
  },
  {
    "arxiv_id": "2405.20606v2",
    "title": "Vision-Language Meets the Skeleton: Progressively Distillation with Cross-Modal Knowledge for 3D Action Representation Learning",
    "authors": [
      "Yang Chen",
      "Tian He",
      "Junfeng Fu",
      "Ling Wang",
      "Jingcai Guo",
      "Ting Hu",
      "Hong Cheng"
    ],
    "abstract": "Skeleton-based action representation learning aims to interpret and\nunderstand human behaviors by encoding the skeleton sequences, which can be\ncategorized into two primary training paradigms: supervised learning and\nself-supervised learning. However, the former one-hot classification requires\nlabor-intensive predefined action categories annotations, while the latter\ninvolves skeleton transformations (e.g., cropping) in the pretext tasks that\nmay impair the skeleton structure. To address these challenges, we introduce a\nnovel skeleton-based training framework (C$^2$VL) based on Cross-modal\nContrastive learning that uses the progressive distillation to learn\ntask-agnostic human skeleton action representation from the Vision-Language\nknowledge prompts. Specifically, we establish the vision-language action\nconcept space through vision-language knowledge prompts generated by\npre-trained large multimodal models (LMMs), which enrich the fine-grained\ndetails that the skeleton action space lacks. Moreover, we propose the\nintra-modal self-similarity and inter-modal cross-consistency softened targets\nin the cross-modal representation learning process to progressively control and\nguide the degree of pulling vision-language knowledge prompts and corresponding\nskeletons closer. These soft instance discrimination and self-knowledge\ndistillation strategies contribute to the learning of better skeleton-based\naction representations from the noisy skeleton-vision-language pairs. During\nthe inference phase, our method requires only the skeleton data as the input\nfor action recognition and no longer for vision-language prompts. Extensive\nexperiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate\nthat our method outperforms the previous methods and achieves state-of-the-art\nresults. Code is available at: https://github.com/cseeyangchen/C2VL.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE Transactions on Multimedia",
    "pdf_url": "http://arxiv.org/pdf/2405.20606v2",
    "published_date": "2024-05-31 03:40:15 UTC",
    "updated_date": "2024-09-15 03:32:03 UTC"
  },
  {
    "arxiv_id": "2405.20605v2",
    "title": "Searching for internal symbols underlying deep learning",
    "authors": [
      "Jung H. Lee",
      "Sujith Vijayan"
    ],
    "abstract": "Deep learning (DL) enables deep neural networks (DNNs) to automatically learn\ncomplex tasks or rules from given examples without instructions or guiding\nprinciples. As we do not engineer DNNs' functions, it is extremely difficult to\ndiagnose their decisions, and multiple lines of studies proposed to explain the\nprinciples of their operations. Notably, one line of studies suggests that DNNs\nmay learn concepts, the high level features that are recognizable to humans. In\nthis study, we extend this line of studies and hypothesize that DNNs can\ndevelop abstract codes that can be used to augment DNNs' decision-making. To\naddress this hypothesis, we combine foundation segmentation models and\nunsupervised learning to extract internal codes and identify potential use of\nabstract codes to make DL's decision-making more reliable and safer.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 10 figures, 5 tables and 1 supplementary table",
    "pdf_url": "http://arxiv.org/pdf/2405.20605v2",
    "published_date": "2024-05-31 03:39:26 UTC",
    "updated_date": "2024-11-18 01:47:56 UTC"
  },
  {
    "arxiv_id": "2405.20603v1",
    "title": "Advancing Financial Risk Prediction Through Optimized LSTM Model Performance and Comparative Analysis",
    "authors": [
      "Ke Xu",
      "Yu Cheng",
      "Shiqing Long",
      "Junjie Guo",
      "Jue Xiao",
      "Mengfang Sun"
    ],
    "abstract": "This paper focuses on the application and optimization of LSTM model in\nfinancial risk prediction. The study starts with an overview of the\narchitecture and algorithm foundation of LSTM, and then details the model\ntraining process and hyperparameter tuning strategy, and adjusts network\nparameters through experiments to improve performance. Comparative experiments\nshow that the optimized LSTM model shows significant advantages in AUC index\ncompared with random forest, BP neural network and XGBoost, which verifies its\nefficiency and practicability in the field of financial risk prediction,\nespecially its ability to deal with complex time series data, which lays a\nsolid foundation for the application of the model in the actual production\nenvironment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20603v1",
    "published_date": "2024-05-31 03:31:17 UTC",
    "updated_date": "2024-05-31 03:31:17 UTC"
  },
  {
    "arxiv_id": "2405.20600v1",
    "title": "Multi-label Class Incremental Emotion Decoding with Augmented Emotional Semantics Learning",
    "authors": [
      "Kaicheng Fu",
      "Changde Du",
      "Xiaoyu Chen",
      "Jie Peng",
      "Huiguang He"
    ],
    "abstract": "Emotion decoding plays an important role in affective human-computer\ninteraction. However, previous studies ignored the dynamic real-world scenario,\nwhere human experience a blend of multiple emotions which are incrementally\nintegrated into the model, leading to the multi-label class incremental\nlearning (MLCIL) problem. Existing methods have difficulty in solving MLCIL\nissue due to notorious catastrophic forgetting caused by partial label problem\nand inadequate label semantics mining. In this paper, we propose an augmented\nemotional semantics learning framework for multi-label class incremental\nemotion decoding. Specifically, we design an augmented emotional relation graph\nmodule with label disambiguation to handle the past-missing partial label\nproblem. Then, we leverage domain knowledge from affective dimension space to\nalleviate future-missing partial label problem by knowledge distillation.\nBesides, an emotional semantics learning module is constructed with a graph\nautoencoder to obtain emotion embeddings in order to guide the\nsemantic-specific feature decoupling for better multi-label learning. Extensive\nexperiments on three datasets show the superiority of our method for improving\nemotion decoding performance and mitigating forgetting on MLCIL problem.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20600v1",
    "published_date": "2024-05-31 03:16:54 UTC",
    "updated_date": "2024-05-31 03:16:54 UTC"
  },
  {
    "arxiv_id": "2405.20594v1",
    "title": "Deep Learning without Weight Symmetry",
    "authors": [
      "Li Ji-An",
      "Marcus K. Benna"
    ],
    "abstract": "Backpropagation (BP), a foundational algorithm for training artificial neural\nnetworks, predominates in contemporary deep learning. Although highly\nsuccessful, it is often considered biologically implausible. A significant\nlimitation arises from the need for precise symmetry between connections in the\nbackward and forward pathways to backpropagate gradient signals accurately,\nwhich is not observed in biological brains. Researchers have proposed several\nalgorithms to alleviate this symmetry constraint, such as feedback alignment\nand direct feedback alignment. However, their divergence from backpropagation\ndynamics presents challenges, particularly in deeper networks and convolutional\nlayers. Here we introduce the Product Feedback Alignment (PFA) algorithm. Our\nfindings demonstrate that PFA closely approximates BP and achieves comparable\nperformance in deep convolutional networks while avoiding explicit weight\nsymmetry. Our results offer a novel solution to the longstanding weight\nsymmetry problem, leading to more biologically plausible learning in deep\nconvolutional networks compared to earlier methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20594v1",
    "published_date": "2024-05-31 03:11:19 UTC",
    "updated_date": "2024-05-31 03:11:19 UTC"
  },
  {
    "arxiv_id": "2405.20592v2",
    "title": "LInK: Learning Joint Representations of Design and Performance Spaces through Contrastive Learning for Mechanism Synthesis",
    "authors": [
      "Amin Heyrani Nobari",
      "Akash Srivastava",
      "Dan Gutfreund",
      "Kai Xu",
      "Faez Ahmed"
    ],
    "abstract": "In this paper, we introduce LInK, a novel framework that integrates\ncontrastive learning of performance and design space with optimization\ntechniques for solving complex inverse problems in engineering design with\ndiscrete and continuous variables. We focus on the path synthesis problem for\nplanar linkage mechanisms. By leveraging a multimodal and\ntransformation-invariant contrastive learning framework, LInK learns a joint\nrepresentation that captures complex physics and design representations of\nmechanisms, enabling rapid retrieval from a vast dataset of over 10 million\nmechanisms. This approach improves precision through the warm start of a\nhierarchical unconstrained nonlinear optimization algorithm, combining the\nrobustness of traditional optimization with the speed and adaptability of\nmodern deep learning methods. Our results on an existing benchmark demonstrate\nthat LInK outperforms existing methods with 28 times less error compared to a\nstate of the art approach while taking 20 times less time on an existing\nbenchmark. Moreover, we introduce a significantly more challenging benchmark,\nnamed LINK ABC, which involves synthesizing linkages that trace the\ntrajectories of English capital alphabets, an inverse design benchmark task\nthat existing methods struggle with due to large nonlinearities and tiny\nfeasible space. Our results demonstrate that LInK not only advances the field\nof mechanism design but also broadens the applicability of contrastive learning\nand optimization to other areas of engineering. The code and data are publicly\navailable at https://github.com/ahnobari/LInK.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20592v2",
    "published_date": "2024-05-31 03:04:57 UTC",
    "updated_date": "2024-10-04 17:13:43 UTC"
  },
  {
    "arxiv_id": "2405.20590v1",
    "title": "Class-Based Time Series Data Augmentation to Mitigate Extreme Class Imbalance for Solar Flare Prediction",
    "authors": [
      "Junzhi Wen",
      "Rafal A. Angryk"
    ],
    "abstract": "Time series data plays a crucial role across various domains, making it\nvaluable for decision-making and predictive modeling. Machine learning (ML) and\ndeep learning (DL) have shown promise in this regard, yet their performance\nhinges on data quality and quantity, often constrained by data scarcity and\nclass imbalance, particularly for rare events like solar flares. Data\naugmentation techniques offer a potential solution to address these challenges,\nyet their effectiveness on multivariate time series datasets remains\nunderexplored. In this study, we propose a novel data augmentation method for\ntime series data named Mean Gaussian Noise (MGN). We investigate the\nperformance of MGN compared to eight existing basic data augmentation methods\non a multivariate time series dataset for solar flare prediction, SWAN-SF,\nusing a ML algorithm for time series data, TimeSeriesSVC. The results\ndemonstrate the efficacy of MGN and highlight its potential for improving\nclassification performance in scenarios with extremely imbalanced data. Our\ntime complexity analysis shows that MGN also has a competitive computational\ncost compared to the investigated alternative methods.",
    "categories": [
      "cs.LG",
      "astro-ph.IM",
      "astro-ph.SR",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20590v1",
    "published_date": "2024-05-31 03:03:19 UTC",
    "updated_date": "2024-05-31 03:03:19 UTC"
  },
  {
    "arxiv_id": "2405.20589v1",
    "title": "Selective Knowledge Sharing for Personalized Federated Learning Under Capacity Heterogeneity",
    "authors": [
      "Zheng Wang",
      "Zheng Wang",
      "Zhaopeng Peng",
      "Zihui Wang",
      "Cheng Wang"
    ],
    "abstract": "Federated Learning (FL) stands to gain significant advantages from\ncollaboratively training capacity-heterogeneous models, enabling the\nutilization of private data and computing power from low-capacity devices.\nHowever, the focus on personalizing capacity-heterogeneous models based on\nclient-specific data has been limited, resulting in suboptimal local model\nutility, particularly for low-capacity clients. The heterogeneity in both data\nand device capacity poses two key challenges for model personalization: 1)\naccurately retaining necessary knowledge embedded within reduced submodels for\neach client, and 2) effectively sharing knowledge through aggregating\nsize-varying parameters. To this end, we introduce Pa3dFL, a novel framework\ndesigned to enhance local model performance by decoupling and selectively\nsharing knowledge among capacity-heterogeneous models. First, we decompose each\nlayer of the model into general and personal parameters. Then, we maintain\nuniform sizes for the general parameters across clients and aggregate them\nthrough direct averaging. Subsequently, we employ a hyper-network to generate\nsize-varying personal parameters for clients using learnable embeddings.\nFinally, we facilitate the implicit aggregation of personal parameters by\naggregating client embeddings through a self-attention module. We conducted\nextensive experiments on three datasets to evaluate the effectiveness of\nPa3dFL. Our findings indicate that Pa3dFL consistently outperforms baseline\nmethods across various heterogeneity settings. Moreover, Pa3dFL demonstrates\ncompetitive communication and computation efficiency compared to baseline\napproaches, highlighting its practicality and adaptability in adverse system\nconditions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20589v1",
    "published_date": "2024-05-31 02:59:25 UTC",
    "updated_date": "2024-05-31 02:59:25 UTC"
  },
  {
    "arxiv_id": "2407.03146v3",
    "title": "Understanding and Reducing the Class-Dependent Effects of Data Augmentation with A Two-Player Game Approach",
    "authors": [
      "Yunpeng Jiang",
      "Paul Weng",
      "Yutong Ban"
    ],
    "abstract": "Data augmentation is widely applied and has shown its benefits in different\nmachine learning tasks. However, as recently observed, it may have an unfair\neffect in multi-class classification. While data augmentation generally\nimproves the overall performance (and therefore is beneficial for many\nclasses), it can actually be detrimental for other classes, which can be\nproblematic in some application domains. In this paper, to counteract this\nphenomenon, we propose CLAM, a CLAss-dependent Multiplicative-weights method.\nTo derive it, we first formulate the training of a classifier as a non-linear\noptimization problem that aims at simultaneously maximizing the individual\nclass performances and balancing them. By rewriting this optimization problem\nas an adversarial two-player game, we propose a novel multiplicative weight\nalgorithm, for which we prove the convergence. Interestingly, our formulation\nalso reveals that the class-dependent effects of data augmentation is not due\nto data augmentation only, but is in fact a general phenomenon. Our empirical\nresults over five datasets demonstrate that the performance of learned\nclassifiers is indeed more fairly distributed over classes, with only limited\nimpact on the average accuracy.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03146v3",
    "published_date": "2024-05-31 02:56:43 UTC",
    "updated_date": "2025-03-25 09:05:02 UTC"
  },
  {
    "arxiv_id": "2405.20585v1",
    "title": "GAMedX: Generative AI-based Medical Entity Data Extractor Using Large Language Models",
    "authors": [
      "Mohammed-Khalil Ghali",
      "Abdelrahman Farrag",
      "Hajar Sakai",
      "Hicham El Baz",
      "Yu Jin",
      "Sarah Lam"
    ],
    "abstract": "In the rapidly evolving field of healthcare and beyond, the integration of\ngenerative AI in Electronic Health Records (EHRs) represents a pivotal\nadvancement, addressing a critical gap in current information extraction\ntechniques. This paper introduces GAMedX, a Named Entity Recognition (NER)\napproach utilizing Large Language Models (LLMs) to efficiently extract entities\nfrom medical narratives and unstructured text generated throughout various\nphases of the patient hospital visit. By addressing the significant challenge\nof processing unstructured medical text, GAMedX leverages the capabilities of\ngenerative AI and LLMs for improved data extraction. Employing a unified\napproach, the methodology integrates open-source LLMs for NER, utilizing\nchained prompts and Pydantic schemas for structured output to navigate the\ncomplexities of specialized medical jargon. The findings reveal significant\nROUGE F1 score on one of the evaluation datasets with an accuracy of 98\\%. This\ninnovation enhances entity extraction, offering a scalable, cost-effective\nsolution for automated forms filling from unstructured data. As a result,\nGAMedX streamlines the processing of unstructured narratives, and sets a new\nstandard in NER applications, contributing significantly to theoretical and\npractical advancements beyond the medical technology sphere.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20585v1",
    "published_date": "2024-05-31 02:53:22 UTC",
    "updated_date": "2024-05-31 02:53:22 UTC"
  },
  {
    "arxiv_id": "2405.20584v2",
    "title": "Disrupting Diffusion: Token-Level Attention Erasure Attack against Diffusion-based Customization",
    "authors": [
      "Yisu Liu",
      "Jinyang An",
      "Wanqian Zhang",
      "Dayan Wu",
      "Jingzi Gu",
      "Zheng Lin",
      "Weiping Wang"
    ],
    "abstract": "With the development of diffusion-based customization methods like\nDreamBooth, individuals now have access to train the models that can generate\ntheir personalized images. Despite the convenience, malicious users have\nmisused these techniques to create fake images, thereby triggering a privacy\nsecurity crisis. In light of this, proactive adversarial attacks are proposed\nto protect users against customization. The adversarial examples are trained to\ndistort the customization model's outputs and thus block the misuse. In this\npaper, we propose DisDiff (Disrupting Diffusion), a novel adversarial attack\nmethod to disrupt the diffusion model outputs. We first delve into the\nintrinsic image-text relationships, well-known as cross-attention, and\nempirically find that the subject-identifier token plays an important role in\nguiding image generation. Thus, we propose the Cross-Attention Erasure module\nto explicitly \"erase\" the indicated attention maps and disrupt the text\nguidance. Besides,we analyze the influence of the sampling process of the\ndiffusion model on Projected Gradient Descent (PGD) attack and introduce a\nnovel Merit Sampling Scheduler to adaptively modulate the perturbation updating\namplitude in a step-aware manner. Our DisDiff outperforms the state-of-the-art\nmethods by 12.75% of FDFR scores and 7.25% of ISM scores across two facial\nbenchmarks and two commonly used prompts on average.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ACM MM2024",
    "pdf_url": "http://arxiv.org/pdf/2405.20584v2",
    "published_date": "2024-05-31 02:45:31 UTC",
    "updated_date": "2024-07-26 02:10:04 UTC"
  },
  {
    "arxiv_id": "2405.20582v2",
    "title": "The Point of View of a Sentiment: Towards Clinician Bias Detection in Psychiatric Notes",
    "authors": [
      "Alissa A. Valentine",
      "Lauren A. Lepow",
      "Lili Chan",
      "Alexander W. Charney",
      "Isotta Landi"
    ],
    "abstract": "Negative patient descriptions and stigmatizing language can contribute to\ngenerating healthcare disparities in two ways: (1) read by patients, they can\nharm their trust and engagement with the medical center; (2) read by\nphysicians, they may negatively influence their perspective of a future\npatient. In psychiatry, the patient-clinician therapeutic alliance is a major\ndeterminant of clinical outcomes. Therefore, language usage in psychiatric\nclinical notes may not only create healthcare disparities, but also perpetuate\nthem. Recent advances in NLP systems have facilitated the efforts to detect\ndiscriminatory language in healthcare. However, such attempts have only focused\non the perspectives of the medical center and its physicians. Considering both\nphysicians and non-physicians' point of view is a more translatable approach to\nidentifying potentially harmful language in clinical notes. By leveraging\npre-trained and large language models (PLMs and LLMs), this work aims to\ncharacterize potentially harmful language usage in psychiatric notes by\nidentifying the sentiment expressed in sentences describing patients based on\nthe reader's point of view. Extracting 39 sentences from the Mount Sinai Health\nSystem containing psychiatric lexicon, we fine-tuned three PLMs (RoBERTa,\nGatorTron, and GatorTron + Task Adaptation) and implemented zero-shot and\nfew-shot ICL approaches for three LLMs (GPT-3.5, Llama-3.1, and Mistral) to\nclassify the sentiment of the sentences according to the physician or\nnon-physician point of view. Results showed that GPT-3.5 aligned best to\nphysician point of view and Mistral aligned best to non-physician point of\nview. These results underline the importance of recognizing the reader's point\nof view, not only for improving the note writing process, but also for the\nquantification, identification, and reduction of bias in computational systems\nfor downstream analyses.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Oral presentation at NAACL 2024 Queer in AI Workshop",
    "pdf_url": "http://arxiv.org/pdf/2405.20582v2",
    "published_date": "2024-05-31 02:28:41 UTC",
    "updated_date": "2025-02-17 18:48:09 UTC"
  },
  {
    "arxiv_id": "2405.20574v2",
    "title": "Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark",
    "authors": [
      "Chanjun Park",
      "Hyeonwoo Kim",
      "Dahyun Kim",
      "Seonghwan Cho",
      "Sanghoon Kim",
      "Sukyung Lee",
      "Yungi Kim",
      "Hwalsuk Lee"
    ],
    "abstract": "This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as\nvital tools for evaluating Large Language Models (LLMs) in Korean.\nIncorporating private test sets while mirroring the English Open LLM\nLeaderboard, we establish a robust evaluation framework that has been well\nintegrated in the Korean LLM community. We perform data leakage analysis that\nshows the benefit of private test sets along with a correlation study within\nthe Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we\npresent empirical support for the need to expand beyond set benchmarks. We hope\nthe Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to\nfoster more linguistic diversity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2405.20574v2",
    "published_date": "2024-05-31 02:05:45 UTC",
    "updated_date": "2024-08-17 03:45:25 UTC"
  },
  {
    "arxiv_id": "2406.11871v4",
    "title": "Generative AI Voting: Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
    "authors": [
      "Srijoni Majumdar",
      "Edith Elkind",
      "Evangelos Pournaras"
    ],
    "abstract": "Scaling up deliberative and voting participation is a longstanding endeavor\n-- a cornerstone for direct democracy and legitimate collective choice. Recent\nbreakthroughs in generative artificial intelligence (AI) and large language\nmodels (LLMs) unravel new capabilities for AI personal assistants to overcome\ncognitive bandwidth limitations of humans, providing decision support or even\ndirect representation of human voters at large scale. However, the quality of\nthis representation and what underlying biases manifest when delegating\ncollective decision-making to LLMs is an alarming and timely challenge to\ntackle. By rigorously emulating with high realism more than >50K LLM voting\npersonas in 306 real-world voting elections, we disentangle the nature of\ndifferent biases in LLMS (GPT 3, GPT 3.5, and Llama2). Complex preferential\nballot formats exhibit significant inconsistencies compared to simpler\nmajoritarian elections that show higher consistency. Strikingly though, by\ndemonstrating for the first time in real-world a proportional representation of\nvoters in direct democracy, we are also able to show that fair ballot\naggregation methods, such as equal shares, prove to be a win-win: fairer voting\noutcomes for humans with fairer AI representation, especially for voters who\nare likely to abstain. This novel underlying relationship proves paramount for\ndemocratic resilience in progressives scenarios with low voters turnout and\nvoter fatigue supported by AI representatives: abstained voters are mitigated\nby recovering highly representative voting outcomes that are fairer. These\ninterdisciplinary insights provide remarkable foundations for science,\npolicymakers, and citizens to develop safeguards and resilience for AI risks in\ndemocratic innovations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11871v4",
    "published_date": "2024-05-31 01:41:48 UTC",
    "updated_date": "2025-04-09 00:21:07 UTC"
  },
  {
    "arxiv_id": "2405.20562v1",
    "title": "Can Machine Learning Assist in Diagnosis of Primary Immune Thrombocytopenia? A feasibility study",
    "authors": [
      "Haroon Miah",
      "Dimitrios Kollias",
      "Giacinto Luca Pedone",
      "Drew Provan",
      "Frederick Chen"
    ],
    "abstract": "Primary Immune thrombocytopenia (ITP) is a rare autoimmune disease\ncharacterised by immune-mediated destruction of peripheral blood platelets in\npatients leading to low platelet counts and bleeding. The diagnosis and\neffective management of ITP is challenging because there is no established test\nto confirm the disease and no biomarker with which one can predict the response\nto treatment and outcome. In this work we conduct a feasibility study to check\nif machine learning can be applied effectively for diagnosis of ITP using\nroutine blood tests and demographic data in a non-acute outpatient setting.\nVarious ML models, including Logistic Regression, Support Vector Machine,\nk-Nearest Neighbor, Decision Tree and Random Forest, were applied to data from\nthe UK Adult ITP Registry and a general hematology clinic. Two different\napproaches were investigated: a demographic-unaware and a demographic-aware\none. We conduct extensive experiments to evaluate the predictive performance of\nthese models and approaches, as well as their bias. The results revealed that\nDecision Tree and Random Forest models were both superior and fair, achieving\nnearly perfect predictive and fairness scores, with platelet count identified\nas the most significant variable. Models not provided with demographic\ninformation performed better in terms of predictive accuracy but showed lower\nfairness score, illustrating a trade-off between predictive performance and\nfairness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20562v1",
    "published_date": "2024-05-31 01:04:46 UTC",
    "updated_date": "2024-05-31 01:04:46 UTC"
  },
  {
    "arxiv_id": "2405.20556v1",
    "title": "Certifying Global Robustness for Deep Neural Networks",
    "authors": [
      "You Li",
      "Guannan Zhao",
      "Shuyu Kong",
      "Yunqi He",
      "Hai Zhou"
    ],
    "abstract": "A globally robust deep neural network resists perturbations on all meaningful\ninputs. Current robustness certification methods emphasize local robustness,\nstruggling to scale and generalize. This paper presents a systematic and\nefficient method to evaluate and verify global robustness for deep neural\nnetworks, leveraging the PAC verification framework for solid guarantees on\nverification results. We utilize probabilistic programs to characterize\nmeaningful input regions, setting a realistic standard for global robustness.\nAdditionally, we introduce the cumulative robustness curve as a criterion in\nevaluating global robustness. We design a statistical method that combines\nmulti-level splitting and regression analysis for the estimation, significantly\nreducing the execution time. Experimental results demonstrate the efficiency\nand effectiveness of our verification method and its capability to find rare\nand diversified counterexamples for adversarial training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.20556v1",
    "published_date": "2024-05-31 00:46:04 UTC",
    "updated_date": "2024-05-31 00:46:04 UTC"
  },
  {
    "arxiv_id": "2405.20543v2",
    "title": "Towards a General Recipe for Combinatorial Optimization with Multi-Filter GNNs",
    "authors": [
      "Frederik Wenkel",
      "Semih Cantürk",
      "Stefan Horoi",
      "Michael Perlmutter",
      "Guy Wolf"
    ],
    "abstract": "Graph neural networks (GNNs) have achieved great success for a variety of\ntasks such as node classification, graph classification, and link prediction.\nHowever, the use of GNNs (and machine learning more generally) to solve\ncombinatorial optimization (CO) problems is much less explored. Here, we\nintroduce GCON, a novel GNN architecture that leverages a complex filter bank\nand localized attention mechanisms to solve CO problems on graphs. We show how\nour method differentiates itself from prior GNN-based CO solvers and how it can\nbe effectively applied to the maximum cut, minimum dominating set, and maximum\nclique problems in a unsupervised learning setting. GCON is competitive across\nall tasks and consistently outperforms other specialized GNN-based approaches,\nand is on par with the powerful Gurobi solver on the max-cut problem. We\nprovide an open-source implementation of our work at\nhttps://github.com/WenkelF/copt.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DM",
      "68T07 (Primary) 68T20, 90C35, 05C62 (Secondary)",
      "F.2.2; I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "In Proceedings of the Third Learning on Graphs Conference (LoG 2024,\n  Oral); 20 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.20543v2",
    "published_date": "2024-05-31 00:02:07 UTC",
    "updated_date": "2024-11-24 23:57:33 UTC"
  }
]
{
  "date": "2025-12-26",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-12-26 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä»Šæ—¥å¯¼è¯»**\nä»Šå¤©çš„ arXiv åˆæ˜¯â€œç¥ä»™æ‰“æ¶â€çš„ä¸€å¤©ã€‚æœ€é‡ç£…çš„å½“å± **Yoshua Bengio** å›¢é˜Ÿå¯¹ RL è®­ç»ƒä¸­ **KL æ­£åˆ™åŒ–**çš„æ·±å…¥å‰–æï¼Œç›´æŒ‡å½“å‰ LLM å¯¹é½è®­ç»ƒä¸­çš„ç—›ç‚¹ã€‚æ­¤å¤–ï¼Œ**Scaling Law** çš„ç ”ç©¶å»¶ä¼¸åˆ°äº†è¶…å‚æ•°è¿ç§»ï¼ˆGoogle/Apple å›¢é˜Ÿï¼‰ï¼Œ**World Modelï¼ˆä¸–ç•Œæ¨¡å‹ï¼‰** åœ¨ Agent å’Œæ— äººæœºé¢†åŸŸçš„åº”ç”¨ä¹Ÿè¿æ¥äº†å‡ ä¸ªæ‰å®çš„å·¥ä½œã€‚åº”ç”¨å±‚é¢ä¸Šï¼Œå…³äº AI ç”Ÿæˆä»£ç çš„**å¯å¤ç°æ€§å±æœº**å€¼å¾—æ‰€æœ‰å¼€å‘è€…è­¦æƒ•ã€‚\n\nä»¥ä¸‹æ˜¯ä»Šå¤©çš„ç²¾é€‰è®ºæ–‡ï¼š\n\n---\n\n### ğŸš€ æ·±åº¦å­¦ä¹ åŸºç¡€ä¸å¤§æ¨¡å‹è®­ç»ƒ (Foundations & Training)\n\n**1. A Comedy of Estimators: On KL Regularization in RL Training of LLMs**\n> **ä¸­æ–‡æ ‡é¢˜:** ä¼°è®¡å™¨çš„å–œå‰§ï¼šè®º LLM å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„ KL æ­£åˆ™åŒ–\n> **æ ¸å¿ƒæœ¯è¯­:** KL Regularization, RLHF, Gradient Bias, On-policy/Off-policy\n> **ä¸€å¥è¯ç‚¹è¯„:** Yoshua Bengio å›¢é˜Ÿå‡ºå“ã€‚ä¸ä»…æŒ‡å‡ºäº†å½“å‰ RLHF å®è·µä¸­ KL ä¼°è®¡å™¨çš„æ¢¯åº¦åå·®é—®é¢˜ï¼Œè¿˜ç»™å‡ºäº†ä¿®æ­£æ–¹æ¡ˆã€‚\n> **è¯¦ç»†è§£è¯»:** åœ¨é€šè¿‡ RL æå‡ LLM æ¨ç†èƒ½åŠ›æ—¶ï¼ŒKL æ•£åº¦æ˜¯é˜²æ­¢æ¨¡å‹è·‘åçš„å…³é”®æ­£åˆ™é¡¹ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—ç²¾ç¡® KL ä¸å¯åœ¨å¤§è§„æ¨¡ä¸‹è¿›è¡Œï¼Œå¤§å®¶éƒ½åœ¨ç”¨ä¼°è®¡å™¨ã€‚ä½œè€…ç³»ç»Ÿåˆ†æäº†å½“å‰å¼€æºåº“ä¸­å¸¸ç”¨çš„ KL ä¼°è®¡å™¨ï¼Œå‘ç°å¾ˆå¤šé…ç½®ä¼šå¯¼è‡´**æ¢¯åº¦åå·® (Gradient Bias)**ï¼Œè¿›è€Œå¼•å‘è®­ç»ƒä¸ç¨³å®šã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æ— åæ¢¯åº¦çš„ä¼°è®¡å™¨é…ç½®èƒ½æ˜¾è‘—æå‡åˆ†å¸ƒå†… (ID) å’Œåˆ†å¸ƒå¤– (OOD) çš„æ€§èƒ½ã€‚\n\n**2. Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration**\n> **ä¸­æ–‡æ ‡é¢˜:** è·¨æ¨¡å—ã€å®½åº¦ã€æ·±åº¦ã€Batch å’Œæ—¶é•¿çš„å®Œæ•´è¶…å‚æ•°è¿ç§»\n> **æ ¸å¿ƒæœ¯è¯­:** Hyperparameter Transfer, $\\mu$P, Scaling Laws\n> **ä¸€å¥è¯ç‚¹è¯„:** å»¶ç»­ $\\mu$P çš„æ€è·¯ï¼ŒGoogle å’Œ Apple çš„ç ”ç©¶è€…å®ç°äº†æ›´æè‡´çš„â€œå°æ¨¡å‹è°ƒå‚ï¼Œå¤§æ¨¡å‹ç›´æ¥ç”¨â€ã€‚\n> **è¯¦ç»†è§£è¯»:** åŸºäº $\\mu$P (Maximal Update Parametrization) ç†è®ºï¼Œæœ¬æ–‡æå‡ºäº† **Complete$^{(d)}$ Parameterisation**ã€‚å®ƒä¸ä»…æ”¯æŒè·¨æ¨¡å‹å®½åº¦å’Œæ·±åº¦çš„è¶…å‚æ•°è¿ç§»ï¼Œè¿˜ç»Ÿä¸€äº† Batch size å’Œè®­ç»ƒæ—¶é•¿çš„ç¼©æ”¾ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä»–ä»¬ç ”ç©¶äº†**é€æ¨¡å— (per-module)** çš„è¶…å‚æ•°ä¼˜åŒ–ä¸è¿ç§»ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥åœ¨å°æ¨¡å‹ä¸ŠæŠŠå­¦ä¹ ç‡ã€AdamW å‚æ•°è°ƒåˆ°æœ€ä¼˜ï¼Œç„¶åæ— ç¼è¿ç§»åˆ°å¤§æ¨¡å‹ä¸Šï¼Œæ˜¾è‘—æå‡ LLM è®­ç»ƒæ•ˆç‡ã€‚\n\n**3. Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models**\n> **ä¸­æ–‡æ ‡é¢˜:** æœ‰ç•ŒåŒæ›²æ­£åˆ‡ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ä¸­ Pre-Layer Normalization çš„ç¨³å®šé«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆ\n> **æ ¸å¿ƒæœ¯è¯­:** Pre-LN, BHyT, Activation Magnitude\n> **è¯¦ç»†è§£è¯»:** Pre-LN æ˜¯ç›®å‰çš„æ ‡é…ï¼Œä½†å­˜åœ¨â€œæ·±åº¦è¯…å’’â€ä¸”æ•ˆç‡ä¸é«˜ã€‚ä½œè€…æå‡ºäº† **BHyT (Bounded Hyperbolic Tanh)**ï¼Œç»“åˆäº† tanh éçº¿æ€§å’Œæ•°æ®é©±åŠ¨çš„è¾“å…¥è¾¹ç•Œé™åˆ¶ã€‚å®ƒèƒ½é˜²æ­¢æ·±å±‚ç½‘ç»œçš„æ¿€æ´»å€¼çˆ†ç‚¸ï¼Œå¹¶ä¸”æ¯” RMSNorm è®­ç»ƒé€Ÿåº¦å¿« 15.8%ï¼Œæ¨ç†ååé‡é«˜ 4.2%ï¼Œä¸”æœ‰ç†è®ºä¸Šçš„ç¨³å®šæ€§ä¿è¯ã€‚\n\n**4. Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model**\n> **ä¸­æ–‡æ ‡é¢˜:** åŠå‚æ•°åå¥½ä¼˜åŒ–ï¼šä½ çš„è¯­è¨€æ¨¡å‹å®é™…ä¸Šæ˜¯ä¸€ä¸ªå•æŒ‡æ ‡æ¨¡å‹\n> **æ ¸å¿ƒæœ¯è¯­:** Preference Optimization, Single-Index Model, Policy Learning\n> **è¯¦ç»†è§£è¯»:** é’ˆå¯¹ DPO ç­‰æ–¹æ³•é€šå¸¸å‡è®¾ Bradley-Terry (Logistic link) æ¨¡å‹çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŠå‚æ•°æ–¹æ³•ã€‚ä½œè€…æŒ‡å‡ºï¼Œåªè¦ç­–ç•¥ç±»æ˜¯å¯å®ç°çš„ï¼Œå¥–åŠ±æœ€å¤§åŒ–é—®é¢˜å°±éšå«äº†ä¸€ä¸ª**å•æŒ‡æ ‡äºŒå…ƒé€‰æ‹©æ¨¡å‹ (Single-Index Binary Choice Model)**ã€‚æ–‡ç« æå‡ºäº†ä¸€ç³»åˆ—ä¸ä¾èµ–å…·ä½“ Link å‡½æ•°çš„ç­–ç•¥å­¦ä¹ å™¨ï¼Œå¯¹æœªçŸ¥çš„åå¥½å™ªå£°åˆ†å¸ƒæ›´é²æ£’ã€‚\n\n---\n\n### ğŸ¤– æ™ºèƒ½ä½“ä¸ä¸–ç•Œæ¨¡å‹ (Agents & World Models)\n\n**5. Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback**\n> **ä¸­æ–‡æ ‡é¢˜:** Agent2Worldï¼šé€šè¿‡è‡ªé€‚åº”å¤šæ™ºèƒ½ä½“åé¦ˆå­¦ä¹ ç”Ÿæˆç¬¦å·ä¸–ç•Œæ¨¡å‹\n> **æ ¸å¿ƒæœ¯è¯­:** Symbolic World Models, PDDL, Multi-agent Framework\n> **è¯¦ç»†è§£è¯»:** è®­ç»ƒ LLM ç”Ÿæˆä¸–ç•Œæ¨¡å‹ï¼ˆå¦‚ PDDL æˆ–æ¨¡æ‹Ÿå™¨ä»£ç ï¼‰å¾ˆéš¾ï¼Œå› ä¸ºç¼ºä¹å¤§è§„æ¨¡éªŒè¯æ•°æ®ã€‚**Agent2World** æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ŒåŒ…å«â€œæ·±åº¦ç ”ç©¶å‘˜â€ã€â€œæ¨¡å‹å¼€å‘è€…â€å’Œâ€œæµ‹è¯•å›¢é˜Ÿâ€ã€‚æµ‹è¯•å›¢é˜Ÿé€šè¿‡è‡ªé€‚åº”å•å…ƒæµ‹è¯•æä¾›åé¦ˆï¼Œå½¢æˆå¤šè½®è®­ç»ƒè½¨è¿¹ã€‚å¾®è°ƒåçš„æ¨¡å‹åœ¨ç”Ÿæˆä¸–ç•Œæ¨¡å‹çš„èƒ½åŠ›ä¸Šæå‡äº† 30.95%ã€‚\n\n**6. SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents**\n> **ä¸­æ–‡æ ‡é¢˜:** SmartSnapï¼šè‡ªéªŒè¯æ™ºèƒ½ä½“çš„ä¸»åŠ¨è¯æ®æœç´¢\n> **æ ¸å¿ƒæœ¯è¯­:** Self-Verifying Agent, Reinforcement Learning, Evidence Seeking\n> **è¯¦ç»†è§£è¯»:** è…¾è®¯å›¢é˜Ÿä½œå“ã€‚ç°æœ‰çš„ GUI æ™ºèƒ½ä½“ä»»åŠ¡éªŒè¯é€šå¸¸æ˜¯è¢«åŠ¨ä¸”æ»åçš„ï¼ˆäº‹åè¯¸è‘›äº®ï¼‰ã€‚**SmartSnap** æå‡ºäº†ä¸€ç§èŒƒå¼è½¬å˜ï¼šæ™ºèƒ½ä½“ä¸ä»…è¦å®Œæˆä»»åŠ¡ï¼Œè¿˜è¦**ä¸»åŠ¨æˆªå›¾ (Snapshot)** ä½œä¸ºè¯æ®æ¥è‡ªè¯æ¸…ç™½ã€‚è¿™ç§â€œè¾¹åšè¾¹å­˜è¯â€çš„æœºåˆ¶è®© 8B æ¨¡å‹åœ¨ç§»åŠ¨ç«¯ä»»åŠ¡ä¸Šè·å¾—äº†æ˜¾è‘—æå‡ï¼Œé€¼è¿‘é—­æºå¤§æ¨¡å‹ã€‚\n\n**7. Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space**\n> **ä¸­æ–‡æ ‡é¢˜:** ç”¨äº 3D ç©ºé—´é•¿æ—¶è§†è§‰ç”Ÿæˆä¸å¯¼èˆªçš„ç©ºä¸­ä¸–ç•Œæ¨¡å‹\n> **æ ¸å¿ƒæœ¯è¯­:** Aerial Navigation, World Model, Future Frame Projection\n> **è¯¦ç»†è§£è¯»:** æ— äººæœºå¯¼èˆªé€šå¸¸åªå…³æ³¨é¿éšœï¼Œç¼ºä¹é«˜å±‚è¯­ä¹‰è§„åˆ’ã€‚ANWM æ˜¯ä¸€ä¸ªç©ºä¸­å¯¼èˆªä¸–ç•Œæ¨¡å‹ï¼Œå¼•å…¥äº†**æœªæ¥å¸§æŠ•å½± (Future Frame Projection)** æ¨¡å—ï¼Œåˆ©ç”¨ç‰©ç†å…ˆéªŒæ¥é¢„æµ‹æœªæ¥çš„è§†è§‰è§‚æµ‹ã€‚è¿™è®©æ— äººæœºèƒ½å¤Ÿé€šè¿‡â€œæƒ³è±¡â€æœªæ¥çš„æ™¯è±¡æ¥è¯„ä¼°è·¯å¾„çš„è¯­ä¹‰åˆç†æ€§ï¼Œæ˜¾è‘—æå‡äº†é•¿è·ç¦»å¯¼èˆªæˆåŠŸç‡ã€‚\n\n---\n\n### ğŸ’» ä»£ç ä¸è½¯ä»¶å·¥ç¨‹ (Code & SE)\n\n**8. AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents**\n> **ä¸­æ–‡æ ‡é¢˜:** AI ç”Ÿæˆçš„ä»£ç ï¼ˆè¿˜ï¼‰ä¸å¯å¤ç°ï¼šåŸºäº LLM çš„ç¼–ç æ™ºèƒ½ä½“ä¾èµ–å·®è·å®è¯ç ”ç©¶\n> **æ ¸å¿ƒæœ¯è¯­:** Reproducibility, Dependency Hell, LLM Coding Agents\n> **è¯¦ç»†è§£è¯»:** è¿™æ˜¯ä¸€ä¸ªéå¸¸è½åœ°çš„ç ”ç©¶ã€‚ä½œè€…æµ‹è¯•äº† Claude Codeã€OpenAI Codex å’Œ Gemini ç”Ÿæˆçš„ 300 ä¸ªé¡¹ç›®ã€‚ç»“æœå‘ç°ï¼Œåªæœ‰ **68.3%** çš„é¡¹ç›®èƒ½â€œå¼€ç®±å³ç”¨â€ã€‚æœ€ä¸¥é‡çš„é—®é¢˜æ˜¯**éšå¼ä¾èµ–**â€”â€”å®é™…è¿è¡Œæ—¶éœ€è¦çš„ä¾èµ–é¡¹æ˜¯æ¨¡å‹å£°æ˜çš„ 13.5 å€ï¼Python çš„è¡¨ç°ï¼ˆ89.2%ï¼‰è¿œå¥½äº Javaï¼ˆ44.0%ï¼‰ã€‚\n\n**9. State-of-the-art Small Language Coder Model: Mify-Coder**\n> **ä¸­æ–‡æ ‡é¢˜:** æœ€å…ˆè¿›çš„å°å‹è¯­è¨€ç¼–ç æ¨¡å‹ï¼šMify-Coder\n> **æ ¸å¿ƒæœ¯è¯­:** Small Language Model, Code Generation, 2.5B Parameters\n> **è¯¦ç»†è§£è¯»:** è¿™æ˜¯ä¸€ä¸ª 2.5B å‚æ•°çš„å°æ¨¡å‹ï¼Œä½†åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šå·ç§°åŒ¹æ•Œæ›´å¤§çš„åŸºçº¿æ¨¡å‹ã€‚å…³é”®åœ¨äºä½¿ç”¨äº† Agent è®¾è®¡çš„ Prompt ç”Ÿæˆåˆæˆæ•°æ®ï¼Œå¹¶è¿›è¡Œäº†ä¸¥æ ¼çš„æ•°æ®é…æ¯”å’Œ CPT-SFT è®­ç»ƒã€‚é€‚åˆåœ¨æ¡Œé¢ç¯å¢ƒéƒ¨ç½²ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸ç”Ÿæˆ (Multimodal & GenAI)\n\n**10. Self-Evaluation Unlocks Any-Step Text-to-Image Generation**\n> **ä¸­æ–‡æ ‡é¢˜:** è‡ªæˆ‘è¯„ä¼°è§£é”ä»»æ„æ­¥æ•°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ\n> **æ ¸å¿ƒæœ¯è¯­:** Text-to-Image, Self-Evaluation, Flow Matching\n> **è¯¦ç»†è§£è¯»:** æå‡ºäº†ä¸€ç§åä¸º **Self-E** çš„ä»å¤´è®­ç»ƒæ–¹æ³•ã€‚æ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¼šåˆ©ç”¨å½“å‰çš„è¯„åˆ†ä¼°è®¡æ¥**è‡ªæˆ‘è¯„ä¼°**ç”Ÿæˆçš„æ ·æœ¬ï¼Œç›¸å½“äºè‡ªå·±ç»™è‡ªå·±å½“è€å¸ˆã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦é¢„è®­ç»ƒçš„ Teacher æ¨¡å‹ï¼ˆä¸åŒäºè’¸é¦ï¼‰ï¼Œä¸”æ”¯æŒä»»æ„æ­¥æ•°æ¨ç†ï¼Œåœ¨æå°‘æ­¥æ•°ä¸‹ä¹Ÿèƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚\n\n**11. Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?**\n> **ä¸­æ–‡æ ‡é¢˜:** è·¨è¶Šç‰ˆæƒé¸¿æ²Ÿï¼šå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¦è¯†åˆ«å¹¶å°Šé‡å—ç‰ˆæƒä¿æŠ¤çš„å†…å®¹ï¼Ÿ\n> **æ ¸å¿ƒæœ¯è¯­:** Copyright Compliance, LVLMs, Tool-Augmented Defense\n> **è¯¦ç»†è§£è¯»:** è¿™æ˜¯ä¸€ä¸ªæ³•å¾‹ä¸æŠ€æœ¯äº¤å‰çš„ benchmarkã€‚ä½œè€…æ„å»ºäº† 5 ä¸‡ä¸ªå¤šæ¨¡æ€æŸ¥è¯¢å¯¹ï¼Œæµ‹è¯• LVLM æ˜¯å¦ä¼šè¾“å‡ºå—ç‰ˆæƒä¿æŠ¤çš„å†…å®¹ï¼ˆå¦‚ä¹¦ç±ç‰‡æ®µã€æ­Œè¯ã€ä»£ç ï¼‰ã€‚ç»“è®ºæ˜¯ï¼šå³ä¾¿æ˜¯é—­æºçš„ SOTA æ¨¡å‹ï¼Œåœ¨é¢å¯¹è§†è§‰è¾“å…¥çš„ç‰ˆæƒå†…å®¹æ—¶ï¼Œè¯†åˆ«å’Œåˆè§„èƒ½åŠ›ä¹Ÿç›¸å½“**åŒ®ä¹**ã€‚\n\n---\n\n### ğŸ§ª AI for Science (ç§‘å­¦æ™ºèƒ½)\n\n**12. HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content**\n> **ä¸­æ–‡æ ‡é¢˜:** HalluMatï¼šé€šè¿‡å¤šé˜¶æ®µéªŒè¯æ£€æµ‹ LLM ç”Ÿæˆææ–™ç§‘å­¦å†…å®¹ä¸­çš„å¹»è§‰\n> **æ ¸å¿ƒæœ¯è¯­:** Hallucination Detection, Materials Science, Fact Verification\n> **è¯¦ç»†è§£è¯»:** é’ˆå¯¹ææ–™ç§‘å­¦é¢†åŸŸçš„ LLM å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº† **HalluMatDetector**ã€‚é€šè¿‡æ£€ç´¢ã€çŸ›ç›¾å›¾åˆ†æç­‰æ‰‹æ®µè¿›è¡Œå¤šé˜¶æ®µéªŒè¯ï¼Œèƒ½å°†å¹»è§‰ç‡é™ä½ 30%ã€‚\n\n**13. SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence**\n> **ä¸­æ–‡æ ‡é¢˜:** SciEvalKitï¼šç§‘å­¦é€šç”¨æ™ºèƒ½çš„å¼€æºè¯„ä¼°å·¥å…·åŒ…\n> **è¯¦ç»†è§£è¯»:** ä¸€ä¸ªæ¶µç›–ç‰©ç†ã€åŒ–å­¦ã€å¤©æ–‡ç­‰å…­å¤§é¢†åŸŸçš„ç§‘å­¦ AI è¯„æµ‹å·¥å…·åŒ…ï¼Œæ—¨åœ¨æ ‡å‡†åŒ–è¯„ä¼° Scientific Foundation Modelsã€‚\n\n---\n\n### ğŸ® æœ‰è¶£çš„åº”ç”¨ (Miscellaneous)\n\n*   **[Game] Quantitative Rule-Based Strategy modeling in Classic Indian Rummy:** ç ”ç©¶äº†å°åº¦æ‹‰ç±³çº¸ç‰Œçš„ç­–ç•¥ï¼Œæå‡ºäº†ä¸€ç§åŸºäº MinDist æŒ‡æ ‡çš„ç®—æ³•ï¼Œæ¯”ä¼ ç»Ÿå¯å‘å¼æ–¹æ³•èƒœç‡æ›´é«˜ã€‚\n*   **[Crypto] Expert System for Bitcoin Forecasting:** åˆ©ç”¨ TimeXer æ¶æ„ï¼Œç»“åˆå…¨çƒ M2 æµåŠ¨æ€§æ•°æ®æ¥é¢„æµ‹æ¯”ç‰¹å¸ä»·æ ¼ï¼Œå·ç§°åœ¨ 70 å¤©é¢„æµ‹èŒƒå›´å†… MSE æä½ã€‚\n*   **[Sport] CricBench:** æ¿çƒåˆ†æçš„ LLM Benchmarkã€‚åœ¨è¿™ä¸ªç‰¹å®šé¢†åŸŸï¼ŒDeepSeek R1 è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¿‡äº† GPT-4oã€‚\n*   **[Social] HeartBench:** è¯„ä¼° LLM çš„â€œæ‹ŸäººåŒ–æ™ºèƒ½â€ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­å›½æ–‡åŒ–èƒŒæ™¯ä¸‹çš„æƒ…æ„Ÿã€ä¼¦ç†å¤„ç†èƒ½åŠ›ã€‚ç›®å‰æ¨¡å‹è·ç¦»ä¸“å®¶è®¾å®šçš„ç†æƒ³åˆ†æ•°è¿˜æœ‰å¾ˆå¤§å·®è·ã€‚\n\n---\n\n**ç»“è¯­**\nä»Šå¤©çš„è®ºæ–‡åæ˜ å‡º AI ç¤¾åŒºæ­£åœ¨ä»â€œæš´åŠ›ç¾å­¦â€è½¬å‘â€œç²¾ç»†åŒ–æ“ä½œâ€ï¼šæ›´ç²¾ç¡®çš„è¶…å‚æ•°è¿ç§»ã€æ›´ä¸¥è°¨çš„ RL æ¢¯åº¦ä¼°è®¡ã€æ›´å¯é çš„ Agent éªŒè¯æœºåˆ¶ã€‚å¸Œæœ›è¿™ä»½å¿«æŠ¥èƒ½ä¸ºä½ çš„ç ”ç©¶å¸¦æ¥çµæ„Ÿï¼",
  "papers": [
    {
      "arxiv_id": "2512.22408v1",
      "title": "A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot",
      "title_zh": "é¢å‘è‡ªä¸»é…é€æœºå™¨äººçš„äººå·¥æ™ºèƒ½ã€åµŒå…¥å¼ã€ä»¿çœŸä¸æœºæ¢°è®¾è®¡ä¸€ä½“åŒ–æ–¹æ³•",
      "authors": [
        "Amro Gamar",
        "Ahmed Abduljalil",
        "Alargam Mohammed",
        "Ali Elhenidy",
        "Abeer Tawakol"
      ],
      "abstract": "This paper presents the development of a fully autonomous delivery robot integrating mechanical engineering, embedded systems, and artificial intelligence. The platform employs a heterogeneous computing architecture, with RPi 5 and ROS 2 handling AI-based perception and path planning, while ESP32 running FreeRTOS ensures real-time motor control. The mechanical design was optimized for payload capacity and mobility through precise motor selection and material engineering. Key technical challenges addressed include optimizing computationally intensive AI algorithms on a resource-constrained platform and implementing a low-latency, reliable communication link between the ROS 2 host and embedded controller. Results demonstrate deterministic, PID-based motor control through rigorous memory and task management, and enhanced system reliability via AWS IoT monitoring and a firmware-level motor shutdown failsafe. This work highlights a unified, multi-disciplinary methodology, resulting in a robust and operational autonomous delivery system capable of real-world deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é›†æˆæœºæ¢°å·¥ç¨‹ã€åµŒå…¥å¼ç³»ç»Ÿå’Œäººå·¥æ™ºèƒ½çš„ç»Ÿä¸€è®¾è®¡æ–¹æ³•ï¼Œæ—¨åœ¨å¼€å‘å…¨è‡ªä¸»é…é€æœºå™¨äººã€‚ç³»ç»Ÿé‡‡ç”¨Heterogeneous computing architectureï¼Œåˆ©ç”¨RPi 5å’ŒROS 2å¤„ç†åŸºäºAIçš„Perceptionä¸Path planningï¼ŒåŒæ—¶é€šè¿‡è¿è¡ŒFreeRTOSçš„ESP32ç¡®ä¿å®æ—¶çš„ç”µæœºæ§åˆ¶ã€‚æœºæ¢°è®¾è®¡é’ˆå¯¹è½½é‡èƒ½åŠ›å’Œç§»åŠ¨æ€§è¿›è¡Œäº†ç”µæœºé€‰æ‹©ä¸ææ–™å·¥ç¨‹çš„ä¼˜åŒ–ã€‚æŠ€æœ¯ä¸Šå…‹æœäº†åœ¨èµ„æºå—é™å¹³å°ä¸Šä¼˜åŒ–é«˜è®¡ç®—å¼ºåº¦AIç®—æ³•çš„éš¾é¢˜ï¼Œå¹¶å®ç°äº†ROS 2ä¸»æœºä¸åµŒå…¥å¼æ§åˆ¶å™¨é—´çš„ä½å»¶è¿Ÿé€šä¿¡ã€‚å®éªŒè¯æ˜ï¼Œé€šè¿‡ä¸¥å¯†çš„å†…å­˜ç®¡ç†å®ç°äº†ç¡®å®šçš„PIDç”µæœºæ§åˆ¶ï¼Œå¹¶ç»“åˆAWS IoTç›‘æ§å’Œå›ºä»¶çº§å®‰å…¨ä¿æŠ¤å¢å¼ºäº†ç³»ç»Ÿå¯é æ€§ã€‚è¿™ç§å¤šå­¦ç§‘ç»Ÿä¸€æ–¹æ³•æœ€ç»ˆæ„å»ºäº†ä¸€ä¸ªé²æ£’ä¸”å¯è¿›è¡Œå®é™…éƒ¨ç½²çš„è‡ªä¸»é…é€ç³»ç»Ÿã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22408v1",
      "published_date": "2025-12-26 23:39:54 UTC",
      "updated_date": "2025-12-26 23:39:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:00:31.806581+00:00"
    },
    {
      "arxiv_id": "2512.22402v1",
      "title": "Efficient Multi-Model Orchestration for Self-Hosted Large Language Models",
      "title_zh": "é¢å‘è‡ªæ‰˜ç®¡å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆå¤šæ¨¡å‹ç¼–æ’",
      "authors": [
        "Bhanu Prakash Vangala",
        "Tanu Malik"
      ],
      "abstract": "Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ç»„ç»‡åœ¨è‡ªæ‰˜ç®¡å¤§è¯­è¨€æ¨¡å‹ (Self-Hosted LLMs) æ—¶é¢ä¸´çš„ GPU åˆ©ç”¨ç‡ã€ä»»åŠ¡è·¯ç”±å’Œå¯é æ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Pick and Spin çš„å¯æ‰©å±•ä¸”ç»æµçš„ç¼–æ’æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäº Kubernetes æ„å»ºï¼Œé›†æˆäº†ç»Ÿä¸€çš„ Helm éƒ¨ç½²ç³»ç»Ÿã€è‡ªé€‚åº”çš„é›¶ç¼©æ”¾ (scale-to-zero) è‡ªåŠ¨åŒ–æœºåˆ¶ä»¥åŠä¸€ä¸ªæ··åˆè·¯ç”±æ¨¡å—ã€‚è¯¥è·¯ç”±æ¨¡å—é€šè¿‡ç»“åˆå…³é”®å­—å¯å‘å¼ç®—æ³•å’Œè½»é‡çº§çš„ DistilBERT åˆ†ç±»å™¨ï¼Œåœ¨æ¨ç†æˆæœ¬ã€å»¶è¿Ÿå’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹ Llama-3 (90B)ã€Gemma-3 (27B)ã€Qwen-3 (235B) å’Œ DeepSeek-R1 (685B) ç­‰æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›æµ‹è¯•ï¼ŒéªŒè¯äº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPick and Spin ç›¸æ¯”é™æ€éƒ¨ç½²èƒ½æé«˜ 21.6% çš„æˆåŠŸç‡ï¼Œé™ä½ 30% çš„å»¶è¿Ÿï¼Œå¹¶å‡å°‘ 33% çš„å•æ¬¡æŸ¥è¯¢ GPU æˆæœ¬ã€‚è¯¥æ–¹æ¡ˆä¸ºç»„ç»‡åœ¨ä¿éšœéšç§å’Œæ§åˆ¶æˆæœ¬çš„å‰æä¸‹ï¼Œå®ç°é«˜æ•ˆçš„å†…éƒ¨å¤§æ¨¡å‹ç¼–æ’æä¾›äº†å®ç”¨çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22402v1",
      "published_date": "2025-12-26 22:42:40 UTC",
      "updated_date": "2025-12-26 22:42:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:00:33.391584+00:00"
    },
    {
      "arxiv_id": "2512.22399v1",
      "title": "Space AI: Leveraging Artificial Intelligence for Space to Improve Life on Earth",
      "title_zh": "Space AIï¼šåˆ©ç”¨èˆªå¤©äººå·¥æ™ºèƒ½æ”¹å–„åœ°çƒç”Ÿæ´»",
      "authors": [
        "Ziyang Wang"
      ],
      "abstract": "Artificial Intelligence (AI) is transforming domains from healthcare and agriculture to finance and industry. As progress on Earth meets growing constraints, the next frontier is outer space, where AI can enable autonomous, resilient operations under extreme uncertainty and limited human oversight. This paper introduces Space AI as a unified interdisciplinary field at the intersection of artificial intelligence and space science and technology. We consolidate historical developments and contemporary progress, and propose a systematic framework that organises Space AI into four mission contexts: 1 AI on Earth, covering intelligent mission planning, spacecraft design optimisation, simulation, and ground-based data analytics; 2 AI in Orbit, focusing on satellite and station autonomy, space robotics, on-board/near-real-time data processing, communication optimisation, and orbital safety; (3) AI in Deep Space, enabling autonomous navigation, adaptive scientific discovery, resource mapping, and long-duration human-AI collaboration under communication constraints; and 4 AI for Multi-Planetary Life, supporting in-situ resource utilisation, habitat and infrastructure construction, life-support and ecological management, and resilient interplanetary networks. Ultimately, Space AI can accelerate humanity's capability to explore and operate in space, while translating advances in sensing, robotics, optimisation, and trustworthy AI into broad societal impact on Earth.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­£å¼æå‡ºäº† Space AI è¿™ä¸€è·¨å­¦ç§‘é¢†åŸŸï¼Œæ—¨åœ¨èåˆ Artificial Intelligence ä¸ç©ºé—´ç§‘å­¦å’ŒæŠ€æœ¯ï¼Œä»¥åº”å¯¹åœ°å¤–ç¯å¢ƒçš„æç«¯ä¸ç¡®å®šæ€§ã€‚æ–‡ç« åœ¨æ€»ç»“å†å²å‘å±•çš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„æ¡†æ¶ï¼Œå°† Space AI åˆ’åˆ†ä¸º AI on Earthã€AI in Orbitã€AI in Deep Space å’Œ AI for Multi-Planetary Life å››ä¸ªæ ¸å¿ƒä»»åŠ¡èƒŒæ™¯ã€‚è¿™äº›èƒŒæ™¯æ¶µç›–äº†ä»åœ°é¢çš„ä»»åŠ¡è§„åˆ’ä¸æ•°æ®åˆ†æã€è½¨é“çš„å«æ˜Ÿè‡ªä¸»è¿è¡Œä¸ç©ºé—´æœºå™¨äººï¼Œåˆ°æ·±ç©ºçš„è‡ªä¸»å¯¼èˆªä¸ç§‘å­¦å‘ç°ï¼Œä»¥åŠå¤šè¡Œæ˜Ÿç”Ÿæ´»çš„èµ„æºåˆ©ç”¨ä¸ç”Ÿå‘½æ”¯æŒç­‰å…³é”®æŠ€æœ¯ã€‚Space AI è‡´åŠ›äºåœ¨æœ‰é™çš„äººå·¥ç›‘ç£ä¸‹å®ç°èˆªå¤©ç³»ç»Ÿçš„è‡ªä¸»ä¸éŸ§æ€§æ“ä½œï¼Œæ˜¾è‘—åŠ é€Ÿäººç±»æ¢ç´¢å¹¶è¿è¥ç©ºé—´çš„èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œè¯¥é¢†åŸŸçš„è¿›æ­¥ä¸ä»…æå‡äº†ç©ºé—´ä½œä¸šæ•ˆç‡ï¼Œè¿˜å°†ä¼ æ„Ÿã€Roboticsã€ä¼˜åŒ–ç®—æ³•å’Œ Trustworthy AI ç­‰å‰æ²¿æŠ€æœ¯æˆæœè½¬åŒ–ä¸ºå¯¹åœ°çƒç¤¾ä¼šçš„å¹¿æ³›ç›Šå¤„ã€‚",
      "categories": [
        "astro-ph.IM",
        "cs.AI",
        "physics.space-ph"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22399v1",
      "published_date": "2025-12-26 22:32:54 UTC",
      "updated_date": "2025-12-26 22:32:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:00:49.314049+00:00"
    },
    {
      "arxiv_id": "2512.22398v1",
      "title": "Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings",
      "title_zh": "å†»ç»“çŸ¥è¯†å›¾è°±åµŒå…¥çš„è½»é‡çº§æ¨ç†æ—¶ä¸ªæ€§åŒ–",
      "authors": [
        "Ozan Oguztuzun",
        "Cerag Oguztuzun"
      ],
      "abstract": "Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ¥è¯†å›¾è°±(Knowledge Graph)åŸºç¡€æ¨¡å‹åœ¨é“¾è·¯é¢„æµ‹ä¸­èƒ½å®ç°è‰¯å¥½çš„ç¾¤ä½“æ€§èƒ½ä½†æ— æ³•æ•æ‰ä¸ªäººç”¨æˆ·åå¥½çš„é—®é¢˜ï¼Œæå‡ºäº†GatedBiasè¿™ä¸€è½»é‡çº§çš„æ¨ç†é˜¶æ®µä¸ªæ€§åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ— éœ€é‡æ–°è®­ç»ƒæˆ–æŸå®³å…¨å±€å‡†ç¡®æ€§ï¼Œå³å¯é€šè¿‡ç»“æ„é—¨æ§è‡ªé€‚åº”(structure-gated adaptation)å°†å†·å†»çš„çŸ¥è¯†å›¾è°±åµŒå…¥(KG Embeddings)è°ƒæ•´ä¸ºä¸ªäººç”¨æˆ·ä¸Šä¸‹æ–‡ã€‚GatedBiasç»“åˆäº†ä¸ªäººèµ„æ–™ç‰¹å®šç‰¹å¾ä¸å›¾å¯¼å‡ºçš„äºŒå…ƒé—¨æ§(binary gates)ä»¥ç”Ÿæˆå¯è§£é‡Šçš„æ¯ä¸ªå®ä½“åç½®ï¼Œæ•´ä¸ªè¿‡ç¨‹ä»…éœ€è¦çº¦300ä¸ªå¯è®­ç»ƒå‚æ•°ã€‚åœ¨Amazon-Bookå’ŒLast-FMä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡å¯¹é½æŒ‡æ ‡çš„åŒæ—¶ä¿æŒäº†ç¾¤ä½“å±‚é¢çš„æ€§èƒ½ã€‚é€šè¿‡åäº‹å®æ‰°åŠ¨å®éªŒ(Counterfactual perturbation experiments)éªŒè¯äº†å…¶å› æœå“åº”æ€§ï¼Œå—ç›Šäºç‰¹å®šåå¥½ä¿¡å·çš„å®ä½“åœ¨ä¿¡å·å¢å¼ºæ—¶è¡¨ç°å‡º6åˆ°30å€çš„æ’åæå‡ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†åŸºç¡€æ¨¡å‹çš„ä¸ªæ€§åŒ–è‡ªé€‚åº”å¯ä»¥å…¼å…·å‚æ•°æ•ˆç‡å’Œå› æœå¯éªŒè¯æ€§ï¼Œæœ‰æ•ˆè¡”æ¥äº†é€šç”¨çŸ¥è¯†è¡¨ç¤ºä¸ä¸ªäººç”¨æˆ·éœ€æ±‚ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22398v1",
      "published_date": "2025-12-26 22:30:37 UTC",
      "updated_date": "2025-12-26 22:30:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:01:07.872043+00:00"
    },
    {
      "arxiv_id": "2512.22396v1",
      "title": "HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification",
      "title_zh": "HalluMatï¼šåŸºäºå¤šé˜¶æ®µéªŒè¯çš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆææ–™ç§‘å­¦å†…å®¹å¹»è§‰æ£€æµ‹",
      "authors": [
        "Bhanu Prakash Vangala",
        "Sajid Mahmud",
        "Pawan Neupane",
        "Joel Selvaraj",
        "Jianlin Cheng"
      ],
      "abstract": "Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ææ–™ç§‘å­¦é¢†åŸŸç”Ÿæˆçš„è™šå‡æˆ–è¯¯å¯¼æ€§ä¿¡æ¯ï¼Œå³å¹»è§‰(hallucination)é—®é¢˜ï¼Œæå‡ºäº†ç³»ç»Ÿçš„æ£€æµ‹ä¸ç¼“è§£æ–¹æ¡ˆã€‚ç ”ç©¶è€…é¦–å…ˆæ¨å‡ºäº†HalluMatDataåŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°ææ–™ç§‘å­¦å†…å®¹ä¸­çš„å¹»è§‰æ£€æµ‹ã€äº‹å®ä¸€è‡´æ€§å’Œå“åº”é²æ£’æ€§ã€‚é€šè¿‡æå‡ºHalluMatDetectorå¤šé˜¶æ®µå¹»è§‰æ£€æµ‹æ¡†æ¶ï¼Œè¯¥æ–¹æ¡ˆé›†æˆäº†å†…åœ¨éªŒè¯(intrinsic verification)ã€å¤šæºæ£€ç´¢(multi-source retrieval)å’ŒçŸ›ç›¾å›¾åˆ†æ(contradiction graph analysis)ç­‰æŠ€æœ¯ã€‚ç ”ç©¶å‘ç°ï¼Œææ–™ç§‘å­¦å­é¢†åŸŸçš„å¹»è§‰æ°´å¹³å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”é«˜ç†µæŸ¥è¯¢(high-entropy queries)è¡¨ç°å‡ºæ›´é«˜çš„äº‹å®ä¸ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒHalluMatDetectoréªŒè¯ç®¡çº¿èƒ½å°†å¹»è§‰ç‡æœ‰æ•ˆé™ä½30%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†é‡Šä¹‰å¹»è§‰ä¸€è‡´æ€§å¾—åˆ†(Paraphrased Hallucination Consistency Score, PHCS)ï¼Œä¸ºé‡åŒ–LLMåœ¨è¯­ä¹‰ç­‰æ•ˆæŸ¥è¯¢ä¸‹çš„å¯é æ€§æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22396v1",
      "published_date": "2025-12-26 22:16:12 UTC",
      "updated_date": "2025-12-26 22:16:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:01:21.157587+00:00"
    },
    {
      "arxiv_id": "2512.22388v1",
      "title": "BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks",
      "title_zh": "BLISSï¼šé¢å‘å›¾ç¥ç»ç½‘ç»œé«˜æ•ˆè®­ç»ƒçš„ Bandit å±‚çº§é‡è¦æ€§é‡‡æ ·ç­–ç•¥",
      "authors": [
        "Omar Alsaqa",
        "Linh Thi Hoang",
        "Muhammed Fatih Balin"
      ],
      "abstract": "Graph Neural Networks (GNNs) are powerful tools for learning from graph-structured data, but their application to large graphs is hindered by computational costs. The need to process every neighbor for each node creates memory and computational bottlenecks. To address this, we introduce BLISS, a Bandit Layer Importance Sampling Strategy. It uses multi-armed bandits to dynamically select the most informative nodes at each layer, balancing exploration and exploitation to ensure comprehensive graph coverage. Unlike existing static sampling methods, BLISS adapts to evolving node importance, leading to more informed node selection and improved performance. It demonstrates versatility by integrating with both Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), adapting its selection policy to their specific aggregation mechanisms. Experiments show that BLISS maintains or exceeds the accuracy of full-batch training.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Graph Neural Networks (GNNs) åœ¨å¤„ç†å¤§è§„æ¨¡å›¾æ•°æ®æ—¶ç”±äºèŠ‚ç‚¹é‚»å±…æ‰©å±•å¯¼è‡´çš„è®¡ç®—ä¸å†…å­˜ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº† BLISS (Bandit Layer Importance Sampling Strategy) é‡‡æ ·ç­–ç•¥ã€‚è¯¥ç­–ç•¥å¼•å…¥äº†å¤šè‡‚è€è™æœº (multi-armed bandits) æœºåˆ¶ï¼Œæ—¨åœ¨æ¯ä¸€å±‚åŠ¨æ€åœ°é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„èŠ‚ç‚¹ï¼Œå¹¶é€šè¿‡å¹³è¡¡æ¢ç´¢ (exploration) ä¸åˆ©ç”¨ (exploitation) æ¥ç¡®ä¿å›¾ç»“æ„çš„å…¨é¢è¦†ç›–ã€‚ä¸ä¼ ç»Ÿçš„é™æ€é‡‡æ ·æ–¹æ³•ä¸åŒï¼ŒBLISS èƒ½å¤Ÿæ ¹æ®èŠ‚ç‚¹é‡è¦æ€§çš„å®æ—¶æ¼”å˜è°ƒæ•´é€‰æ‹©ç­–ç•¥ï¼Œä»è€Œå®ç°æ›´å…·é’ˆå¯¹æ€§çš„èŠ‚ç‚¹é€‰å–ã€‚è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œå¯æ— ç¼é›†æˆåˆ° Graph Convolutional Networks (GCNs) å’Œ Graph Attention Networks (GATs) ä¸­ï¼Œå¹¶æ ¹æ®å…¶ç‰¹å®šçš„èšåˆæœºåˆ¶ä¼˜åŒ–é‡‡æ ·ç­–ç•¥ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒBLISS åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒç”šè‡³è¶…è¿‡å…¨æ‰¹é‡è®­ç»ƒ (full-batch training) çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for 5th Muslims in ML Workshop co-located with NeurIPS 2025. OpenReview: https://openreview.net/forum?id=VaHubA7Pwv Code: https://github.com/linhthi/BLISS-GNN",
      "pdf_url": "https://arxiv.org/pdf/2512.22388v1",
      "published_date": "2025-12-26 21:25:26 UTC",
      "updated_date": "2025-12-26 21:25:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:00:57.139227+00:00"
    },
    {
      "arxiv_id": "2512.22387v1",
      "title": "AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents",
      "title_zh": "AIç”Ÿæˆçš„ä»£ç ï¼ˆå°šï¼‰ä¸å¯å¤ç°ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¼–ç¨‹æ™ºèƒ½ä½“ä¸­ä¾èµ–é¡¹ç¼ºå£çš„å®è¯ç ”ç©¶",
      "authors": [
        "Bhanu Prakash Vangala",
        "Ali Adibifar",
        "Tanu Malik",
        "Ashish Gehani"
      ],
      "abstract": "The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„ä»£ç åœ¨å¯é‡ç°æ€§æ–¹é¢å°šæœªè¢«å……åˆ†æ¢ç´¢çš„é—®é¢˜ï¼Œè¿›è¡Œäº†ä¸€é¡¹å…³äºä¾èµ–é¡¹å·®å¼‚(Dependency Gaps)çš„å®è¯ç ”ç©¶ã€‚ç ”ç©¶è¯„ä¼°äº† Claude Codeã€OpenAI Codex å’Œ Gemini è¿™ä¸‰æ¬¾å…ˆè¿›çš„ LLM ç¼–ç¨‹æ™ºèƒ½ä½“ï¼Œæ¶µç›–äº† Pythonã€JavaScript å’Œ Java è¯­è¨€çš„ 300 ä¸ªç”Ÿæˆé¡¹ç›®ã€‚ä¸ºäº†é‡åŒ–æ‰§è¡Œçš„å¯é‡ç°æ€§ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªä¸‰å±‚ä¾èµ–æ¡†æ¶ï¼Œç”¨ä»¥åŒºåˆ†å£°æ˜ä¾èµ–(Claimed)ã€å·¥ä½œä¾èµ–(Working)å’Œè¿è¡Œæ—¶ä¾èµ–(Runtime)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ•´ä½“ä»…æœ‰ 68.3% çš„é¡¹ç›®èƒ½å¼€ç®±å³ç”¨ï¼Œä¸”ä¸åŒè¯­è¨€é—´è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œå…¶ä¸­ Python çš„æˆåŠŸç‡ä¸º 89.2%ï¼Œè€Œ Java ä»…ä¸º 44.0%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ä»å£°æ˜åˆ°å®é™…è¿è¡Œæ—¶ä¾èµ–é¡¹å¹³å‡æ‰©å¼ äº† 13.5 å€ï¼Œæ­ç¤ºäº†ä¸¥é‡çš„éšè—ä¾èµ–é—®é¢˜ã€‚è¯¥ç ”ç©¶è¡¨æ˜ AI ç”Ÿæˆçš„ä»£ç ç›®å‰å°šä¸å…·å¤‡ç†æƒ³çš„å¯é‡ç°æ€§ï¼Œä¸ºæœªæ¥ä¼˜åŒ– LLM é©±åŠ¨çš„è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22387v1",
      "published_date": "2025-12-26 21:17:22 UTC",
      "updated_date": "2025-12-26 21:17:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:00:46.576693+00:00"
    },
    {
      "arxiv_id": "2601.00024v1",
      "title": "Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach",
      "title_zh": "Classic Indian Rummy ä¸­çš„å®šé‡è§„åˆ™ç­–ç•¥å»ºæ¨¡ï¼šä¸€ç§åº¦é‡ä¼˜åŒ–æ–¹æ³•",
      "authors": [
        "Purushottam Saha",
        "Avirup Chakraborty",
        "Sourish Sarkar",
        "Subhamoy Maitra",
        "Diganta Mukherjee",
        "Tridib Mukherjee"
      ],
      "abstract": "The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·æœ‰ä¸å®Œå…¨ä¿¡æ¯çš„é¡ºåºåšå¼ˆæ¸¸æˆç»å…¸å°åº¦æ‹‰ç±³çº¸ç‰Œ(Classic Indian Rummy)ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§„åˆ™çš„ç­–ç•¥å»ºæ¨¡æ¡†æ¶ã€‚ç ”ç©¶çš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†ä¸€ç§åä¸º MinDist çš„æ–°æ‰‹ç‰Œè¯„ä¼°æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡é€šè¿‡æ”¹è¿› MinScore æŒ‡æ ‡ï¼Œé‡åŒ–äº†å½“å‰æ‰‹ç‰Œä¸æœ€è¿‘åˆæ³•é…ç½®ä¹‹é—´çš„ç¼–è¾‘è·ç¦»(edit distance)ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰æ‰‹ç‰Œåœ¨ç»“æ„ä¸Šè¶‹äºå®Œæˆçš„æ¥è¿‘ç¨‹åº¦ã€‚ä¸ºäº†åœ¨æ¸¸æˆè¿‡ç¨‹ä¸­ç²¾ç¡®è®¡ç®—è¯¥æŒ‡æ ‡ï¼Œä½œè€…å¼€å‘äº†ä¸€ç§é«˜æ•ˆç®—æ³•ï¼Œå¹¶åˆ©ç”¨åŠ¨æ€å‰ªæ(dynamic pruning)å’Œæ¨¡å¼ç¼“å­˜(pattern caching)æŠ€æœ¯æ˜¾è‘—æå‡äº†è®¡ç®—æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨åŒäººé›¶å’Œåšå¼ˆæ¨¡æ‹Ÿä¸­æ•´åˆäº†å¯¹æ‰‹æ‰‹ç‰Œå»ºæ¨¡(Opponent hand-modeling)æŠ€æœ¯ï¼Œå¹¶é€šè¿‡ç»Ÿè®¡å‡è®¾æ£€éªŒå¯¹ç”Ÿæˆçš„ç­–ç•¥è¿›è¡Œäº†ä¸¥è°¨è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäº MinDist çš„æ™ºèƒ½ä½“åœ¨èƒœç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¯å‘å¼æ–¹æ³•(traditional heuristics)ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘å½¢å¼åŒ–ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„æ‹‰ç±³çº¸ç‰Œç®—æ³•ç­–ç•¥å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 6 figures, 2 algorithms",
      "pdf_url": "https://arxiv.org/pdf/2601.00024v1",
      "published_date": "2025-12-26 21:03:47 UTC",
      "updated_date": "2025-12-26 21:03:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:01:09.386095+00:00"
    },
    {
      "arxiv_id": "2512.22385v2",
      "title": "LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition",
      "title_zh": "LLMå¼•å¯¼çš„å°æ ·æœ¬å¯ç©¿æˆ´ä¼ æ„Ÿå™¨äººä½“æ´»åŠ¨è¯†åˆ«èŒƒä¾‹é€‰æ‹©",
      "authors": [
        "Elsen Ronando",
        "Sozo Inoue"
      ],
      "abstract": "In this paper, we propose an LLM-Guided Exemplar Selection framework to address a key limitation in state-of-the-art Human Activity Recognition (HAR) methods: their reliance on large labeled datasets and purely geometric exemplar selection, which often fail to distinguish similar wearable sensor activities such as walking, walking upstairs, and walking downstairs. Our method incorporates semantic reasoning via an LLM-generated knowledge prior that captures feature importance, inter-class confusability, and exemplar budget multipliers, and uses it to guide exemplar scoring and selection. These priors are combined with margin-based validation cues, PageRank centrality, hubness penalization, and facility-location optimization to obtain a compact and informative set of exemplars. Evaluated on the UCI-HAR dataset under strict few-shot conditions, the framework achieves a macro F1-score of 88.78%, outperforming classical approaches such as random sampling, herding, and k-center. The results show that LLM-derived semantic priors, when integrated with structural and geometric cues, provide a stronger foundation for selecting representative sensor exemplars in few-shot wearable-sensor HAR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º LLM-Guided Exemplar Selection çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¯ç©¿æˆ´ä¼ æ„Ÿå™¨äººç±»æ´»åŠ¨è¯†åˆ« (Human Activity Recognition, HAR) ä¸­å¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ä»¥åŠä¼ ç»Ÿå‡ ä½•æ ·æœ¬é€‰æ‹©åœ¨åŒºåˆ†ç›¸ä¼¼æ´»åŠ¨ï¼ˆå¦‚è¡Œèµ°ã€ä¸Šæ¥¼ä¸ä¸‹æ¥¼ï¼‰æ—¶çš„ä¸è¶³ã€‚è¯¥æ–¹æ³•æ ¸å¿ƒåœ¨äºå¼•å…¥ Large Language Model (LLM) ç”Ÿæˆçš„è¯­ä¹‰æ¨ç†çŸ¥è¯†å…ˆéªŒï¼Œæ¶µç›–ç‰¹å¾é‡è¦æ€§ã€ç±»åˆ«é—´æ˜“æ··æ·†æ€§å’Œæ ·æœ¬é¢„ç®—æƒé‡ï¼Œä»¥æ­¤å¼•å¯¼æ ·æœ¬è¯„åˆ†ä¸é€‰æ‹©ã€‚è¿™äº›è¯­ä¹‰å…ˆéªŒè¢«è¿›ä¸€æ­¥ä¸åŸºäºè¾¹ç¼˜çš„éªŒè¯çº¿ç´¢ã€PageRank ä¸­å¿ƒæ€§ã€hubness penalization å’Œ facility-location ä¼˜åŒ–ç®—æ³•ç›¸ç»“åˆï¼Œä»è€Œæ„å»ºå‡ºä¸€ä¸ªç²¾ç®€ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ ·æœ¬é›†ã€‚åœ¨å°‘æ ·æœ¬ (few-shot) ç¯å¢ƒä¸‹çš„ UCI-HAR æ•°æ®é›†è¯„ä¼°ä¸­ï¼Œè¯¥æ¡†æ¶è¾¾åˆ°äº† 88.78% çš„ macro F1-scoreï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äº random samplingã€herding å’Œ k-center ç­‰ç»å…¸æ–¹æ³•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°† LLM æä¾›çš„è¯­ä¹‰å…ˆéªŒä¸ç»“æ„åŠå‡ ä½•çº¿ç´¢æ·±åº¦é›†æˆï¼Œèƒ½å¤Ÿä¸ºå°‘æ ·æœ¬ç©¿æˆ´ä¼ æ„Ÿå™¨ HAR çš„ä»£è¡¨æ€§æ ·æœ¬é€‰æ‹©æä¾›æ›´åšå®çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted for presentation at ABC 2026. The manuscript is under revision prior to camera-ready submission",
      "pdf_url": "https://arxiv.org/pdf/2512.22385v2",
      "published_date": "2025-12-26 21:03:15 UTC",
      "updated_date": "2026-01-01 14:37:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:01:10.581161+00:00"
    },
    {
      "arxiv_id": "2512.22382v1",
      "title": "Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration",
      "title_zh": "è·¨æ¨¡å—ã€å®½åº¦ã€æ·±åº¦ã€æ‰¹é‡åŠè®­ç»ƒæ—¶é•¿çš„è¶…å‚æ•°å®Œæ•´è¿ç§»",
      "authors": [
        "Bruno Mlodozeniec",
        "Pierre Ablin",
        "Louis BÃ©thune",
        "Dan Busbridge",
        "Michal Klein",
        "Jason Ramapuram",
        "Marco Cuturi"
      ],
      "abstract": "Hyperparameter tuning can dramatically impact training stability and final performance of large-scale models. Recent works on neural network parameterisations, such as $Î¼$P, have enabled transfer of optimal global hyperparameters across model sizes. These works propose an empirical practice of search for optimal global base hyperparameters at a small model size, and transfer to a large size. We extend these works in two key ways. To handle scaling along most important scaling axes, we propose the Complete$^{(d)}$ Parameterisation that unifies scaling in width and depth -- using an adaptation of CompleteP -- as well as in batch-size and training duration. Secondly, with our parameterisation, we investigate per-module hyperparameter optimisation and transfer. We characterise the empirical challenges of navigating the high-dimensional hyperparameter landscape, and propose practical guidelines for tackling this optimisation problem. We demonstrate that, with the right parameterisation, hyperparameter transfer holds even in the per-module hyperparameter regime. Our study covers an extensive range of optimisation hyperparameters of modern models: learning rates, AdamW parameters, weight decay, initialisation scales, and residual block multipliers. Our experiments demonstrate significant training speed improvements in Large Language Models with the transferred per-module hyperparameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Complete$^{(d)}$ Parameterizationï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒä¸­è¶…å‚æ•°è°ƒä¼˜çš„ç¨³å®šæ€§ä¸æ€§èƒ½æŒ‘æˆ˜ã€‚è¯¥å‚æ•°åŒ–æ–¹æ³•æœ‰æ•ˆç»Ÿä¸€äº†æ¨¡å‹åœ¨å®½åº¦(width)ã€æ·±åº¦(depth)ã€æ‰¹å¤„ç†å¤§å°(batch-size)ä»¥åŠè®­ç»ƒæ—¶é•¿(duration)ç­‰æ ¸å¿ƒç»´åº¦çš„ç¼©æ”¾ï¼Œæ˜¯å¯¹ç°æœ‰ $Î¼$P æŠ€æœ¯çš„æ·±åº¦æ‰©å±•ã€‚ç ”ç©¶é‡ç‚¹æ¢è®¨äº†é€æ¨¡å—(per-module)çš„è¶…å‚æ•°ä¼˜åŒ–ä¸è¿ç§»ï¼Œæ¶µç›–äº†å­¦ä¹ ç‡(learning rates)ã€AdamW å‚æ•°ã€æƒé‡è¡°å‡(weight decay)ã€åˆå§‹åŒ–æ¯”ä¾‹(initialisation scales)åŠæ®‹å·®å—å€ç‡(residual block multipliers)ç­‰å…³é”®ä¼˜åŒ–å› å­ã€‚é€šè¿‡å¯¹é«˜ç»´è¶…å‚æ•°æ™¯è§‚(hyperparameter landscape)çš„åˆ»ç”»ï¼Œä½œè€…æä¾›äº†å¤„ç†è¯¥ä¼˜åŒ–é—®é¢˜çš„å®è·µæŒ‡å—ï¼Œå¹¶è¯å®äº†åœ¨åˆç†çš„å‚æ•°åŒ–ä¸‹ï¼Œé€æ¨¡å—è¶…å‚æ•°è¿ç§»ä¾ç„¶ä¿æŒç¨³å¥ã€‚å®éªŒè¯æ˜ï¼Œåº”ç”¨è¿ç§»åçš„é€æ¨¡å—è¶…å‚æ•°å¯æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹(Large Language Models)çš„è®­ç»ƒé€Ÿåº¦ï¼Œä¸ºè¶…å¤§è§„æ¨¡ç¥ç»ç½‘ç»œçš„é«˜æ•ˆé…ç½®æä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22382v1",
      "published_date": "2025-12-26 20:56:04 UTC",
      "updated_date": "2025-12-26 20:56:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:01:10.090136+00:00"
    },
    {
      "arxiv_id": "2512.22378v1",
      "title": "Towards Efficient Post-Training via Fourier-Driven Adapter Architectures",
      "title_zh": "è¿ˆå‘é«˜æ•ˆåè®­ç»ƒï¼šåŸºäº Fourier é©±åŠ¨çš„é€‚é…å™¨æ¶æ„",
      "authors": [
        "Donggyun Bae",
        "Jongil Park"
      ],
      "abstract": "We propose a novel framework, termed Fourier-Activated Adapter (FAA), for parameter-efficient fine-tuning of large pre-trained language models. By incorporating random Fourier features into lightweight adapter modules, FAA decomposes intermediate representations into complementary low- and high-frequency components, enabling frequency-aware modulation of semantic information. This design allows the model to selectively emphasize informative frequency bands during adaptation while preserving the representational capacity of the frozen backbone. Extensive experiments on GLUE, E2E NLG, and instruction-tuning benchmarks demonstrate that FAA consistently achieves competitive or superior performance compared to existing parameter-efficient fine-tuning methods, while maintaining low computational and memory overhead. Ablation studies further verify the effectiveness of frequency-aware activation and adaptive weighting mechanisms, highlighting FAA as a robust and efficient approach for post-training large language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Fourier-Activated Adapter (FAA) çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºå®ç°å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆå‚æ•°å¾®è°ƒ (parameter-efficient fine-tuning)ã€‚é€šè¿‡å°†éšæœºå‚…é‡Œå¶ç‰¹å¾ (random Fourier features) å¼•å…¥è½»é‡çº§é€‚é…å™¨æ¨¡å—ï¼ŒFAA å°†ä¸­é—´è¡¨ç¤ºåˆ†è§£ä¸ºäº’è¡¥çš„ä½é¢‘å’Œé«˜é¢‘åˆ†é‡ï¼Œå®ç°äº†å¯¹è¯­ä¹‰ä¿¡æ¯çš„é¢‘ç‡æ„ŸçŸ¥è°ƒèŠ‚ (frequency-aware modulation)ã€‚è¿™ç§æ¶æ„å…è®¸æ¨¡å‹åœ¨å¾®è°ƒæœŸé—´é€‰æ‹©æ€§åœ°å¼ºåŒ–æœ‰ä¿¡æ¯é‡çš„é¢‘å¸¦ï¼ŒåŒæ—¶æœ‰æ•ˆä¿ç•™å†»ç»“éª¨å¹²ç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›ã€‚åœ¨ GLUEã€E2E NLG å’ŒæŒ‡ä»¤å¾®è°ƒåŸºå‡†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒFAA åœ¨ç»´æŒæä½è®¡ç®—å’Œå†…å­˜å¼€é”€çš„å‰æä¸‹ï¼Œæ€§èƒ½ä¼˜äºæˆ–ç­‰åŒäºç°æœ‰çš„ä¸»æµå¾®è°ƒæ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å…¶é¢‘ç‡æ„ŸçŸ¥æ¿€æ´»å’Œè‡ªé€‚åº”æƒé‡æœºåˆ¶çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜ FAA æ˜¯å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åæœŸè®­ç»ƒä¸­ä¸€ç§ç¨³å¥ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22378v1",
      "published_date": "2025-12-26 20:50:49 UTC",
      "updated_date": "2025-12-26 20:50:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:01:36.885596+00:00"
    },
    {
      "arxiv_id": "2512.22374v1",
      "title": "Self-Evaluation Unlocks Any-Step Text-to-Image Generation",
      "title_zh": "è‡ªè¯„ä¼°è§£é”ä»»æ„æ­¥æ•°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ",
      "authors": [
        "Xin Yu",
        "Xiaojuan Qi",
        "Zhengqi Li",
        "Kai Zhang",
        "Richard Zhang",
        "Zhe Lin",
        "Eli Shechtman",
        "Tianyu Wang",
        "Yotam Nitzan"
      ],
      "abstract": "We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Self-Evaluating Model (Self-E)ï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„ä»é›¶å¼€å§‹è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæ”¯æŒä»»æ„æ­¥æ•° (any-step) çš„æ¨ç†ã€‚Self-E åœ¨å€Ÿé‰´ Flow Matching æ¨¡å‹å­¦ä¹ æ–¹å¼çš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†åˆ›æ–°çš„è‡ªè¯„ä¼° (self-evaluation) æœºåˆ¶ï¼Œé€šè¿‡åˆ©ç”¨å½“å‰çš„è¯„åˆ†ä¼°è®¡æ¥è¯„ä¼°ç”Ÿæˆçš„æ ·æœ¬ï¼Œä½¿å…¶èƒ½å¤Ÿä½œä¸ºåŠ¨æ€çš„è‡ªæˆ‘æ•™å¸ˆã€‚ä¸ä»…ä¾èµ–å±€éƒ¨ç›‘ç£çš„ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ (diffusion models) æˆ–éœ€è¦é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„è’¸é¦æ–¹æ³• (distillation-based approaches) ä¸åŒï¼Œè¯¥æ¨¡å‹æˆåŠŸç»“åˆäº†å±€éƒ¨å­¦ä¹ ä¸è‡ªé©±åŠ¨å…¨å±€åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSelf-E åœ¨æä½æ¨ç†æ­¥æ•°ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨ 50 æ­¥æ¨ç†æ—¶ä¸æœ€å…ˆè¿›çš„ Flow Matching æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œå…¶æ€§èƒ½éšæ¨ç†æ­¥æ•°å¢åŠ å•è°ƒæå‡ï¼Œåœ¨å•ä¸€ç»Ÿä¸€æ¡†æ¶å†…å®ç°äº†è¶…å¿«é€Ÿå°‘æ­¥ç”Ÿæˆä¸é«˜è´¨é‡é•¿è½¨è¿¹é‡‡æ ·çš„å¹³è¡¡ã€‚ä½œä¸ºé¦–ä¸ªæ”¯æŒä»»æ„æ­¥æ•°çš„ä»é›¶è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼ŒSelf-E ä¸ºé«˜æ•ˆä¸”å¯æ‰©å±•çš„å›¾åƒç”Ÿæˆæä¾›äº†å…¨æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://xinyu-andy.github.io/SelfE-project/",
      "pdf_url": "https://arxiv.org/pdf/2512.22374v1",
      "published_date": "2025-12-26 20:42:11 UTC",
      "updated_date": "2025-12-26 20:42:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:01:59.022602+00:00"
    },
    {
      "arxiv_id": "2512.22367v1",
      "title": "Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions",
      "title_zh": "é¢å‘æ— é™åŠ¨ä½œæ•°å€¼è§„åˆ’çš„åŸºäºå­ç›®æ ‡æ¾å¼›çš„å¯å‘å¼æ–¹æ³•",
      "authors": [
        "Ãngel Aso-Mollar",
        "Diego Aineto",
        "Enrico Scala",
        "Eva Onaindia"
      ],
      "abstract": "Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸¦æœ‰æ§åˆ¶å‚æ•°çš„æ•°å€¼è§„åˆ’(Numeric Planning)é—®é¢˜ï¼Œè§£å†³äº†ç”±äºåŠ¨ä½œå‚æ•°ä½œä¸ºè‡ªç”±æ•°å€¼å˜é‡å¯¼è‡´åœ¨å•ä¸€çŠ¶æ€ä¸‹å­˜åœ¨æ— é™åŠ¨ä½œç©ºé—´ï¼Œè¿›è€Œä½¿ä¼ ç»Ÿå¯å‘å¼ç®—æ³•å¤±æ•ˆçš„éš¾é¢˜ã€‚ä½œè€…è¯†åˆ«å‡ºäº†ä¸€ç±»å¯å¤„ç†çš„å­é›†ï¼Œå³å¯æ§ç®€å•æ•°å€¼é—®é¢˜(controllable, simple numeric problems)ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¹è§‚ç¼–è¯‘æ–¹æ³•(optimistic compilation approach)å°†å…¶è½¬æ¢ä¸ºç®€å•æ•°å€¼ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ä¾èµ–æ§åˆ¶çš„è¡¨è¾¾å¼æŠ½è±¡ä¸ºæœ‰ç•Œå¸¸é‡æ•ˆæœ(bounded constant effects)å’Œæ¾å¼›å‰ç½®æ¡ä»¶(relaxed preconditions)ï¼Œä½¿å¾—ä¼ ç»Ÿçš„å­ç›®æ ‡å¯å‘å¼(subgoaling heuristics)èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¼°è®¡ç›®æ ‡è·ç¦»ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™ç§æ–¹æ³•æ˜¯åœ¨æ— é™åŠ¨ä½œç©ºé—´è®¾å®šä¸‹åº”ç”¨ä¼ ç»Ÿæ•°å€¼å¯å‘å¼ç®—æ³•çš„ä¸€ç§æœ‰æ•ˆä¸”è®¡ç®—å¯è¡Œçš„æ–¹å¼ï¼ŒæˆåŠŸæ‹“å±•äº†æ•°å€¼è§„åˆ’é¢†åŸŸçš„æŠ€æœ¯è¾¹ç•Œã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22367v1",
      "published_date": "2025-12-26 20:05:57 UTC",
      "updated_date": "2025-12-26 20:05:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:01:38.302619+00:00"
    },
    {
      "arxiv_id": "2512.22364v1",
      "title": "Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries",
      "title_zh": "æˆæœ¬æ„ŸçŸ¥å‹ Text-to-SQLï¼šLLM ç”ŸæˆæŸ¥è¯¢çš„äº‘è®¡ç®—æˆæœ¬å®è¯ç ”ç©¶",
      "authors": [
        "Saurabh Deochake",
        "Debajyoti Mukhopadhyay"
      ],
      "abstract": "Text-to-SQL systems powered by Large Language Models (LLMs) achieve high accuracy on standard benchmarks, yet existing efficiency metrics such as the Valid Efficiency Score (VES) measure execution time rather than the consumption-based costs of cloud data warehouses. This paper presents the first systematic evaluation of cloud compute costs for LLM-generated SQL queries. We evaluate six state-of-the-art LLMs across 180 query executions on Google BigQuery using the StackOverflow dataset (230GB), measuring bytes processed, slot utilization, and estimated cost. Our analysis yields three key findings: (1) reasoning models process 44.5% fewer bytes than standard models while maintaining equivalent correctness (96.7%-100%); (2) execution time correlates weakly with query cost (r=0.16), indicating that speed optimization does not imply cost optimization; and (3) models exhibit up to 3.4x cost variance, with standard models producing outliers exceeding 36GB per query. We identify prevalent inefficiency patterns including missing partition filters and unnecessary full-table scans, and provide deployment guidelines for cost-sensitive enterprise environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„Text-to-SQLç³»ç»Ÿï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†äº‘æ•°æ®ä»“åº“çš„è®¡ç®—æˆæœ¬ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„Valid Efficiency Score (VES)ç­‰æ•ˆç‡æŒ‡æ ‡ä»…è¡¡é‡æ‰§è¡Œæ—¶é—´è€Œå¿½ç•¥äº†äº‘ç«¯åŸºäºèµ„æºæ¶ˆè€—çš„å®é™…æˆæœ¬ã€‚é€šè¿‡åœ¨Google BigQueryä¸Šåˆ©ç”¨StackOverflowæ•°æ®é›†å¯¹å…­ç§æœ€å…ˆè¿›çš„LLMsè¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿæµ‹é‡äº†å¤„ç†å­—èŠ‚æ•°ã€slot utilizationåŠä¼°ç®—æˆæœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨ç†æ¨¡å‹åœ¨ä¿æŒ96.7%è‡³100%å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå¤„ç†çš„å­—èŠ‚æ•°æ¯”æ ‡å‡†æ¨¡å‹å°‘44.5%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒæŸ¥è¯¢æ‰§è¡Œæ—¶é—´ä¸æˆæœ¬ä¹‹é—´çš„ç›¸å…³æ€§æå¼±(r=0.16)ï¼Œè¡¨æ˜é’ˆå¯¹é€Ÿåº¦çš„ä¼˜åŒ–å¹¶ä¸å¿…ç„¶å¯¼è‡´æˆæœ¬çš„é™ä½ã€‚ç ”ç©¶è¿˜å‘ç°ä¸åŒæ¨¡å‹é—´çš„æˆæœ¬å·®å¼‚æœ€é«˜å¯è¾¾3.4å€ï¼Œä¸”æ ‡å‡†æ¨¡å‹å¸¸å› ç¼ºå°‘partition filtersæˆ–æ‰§è¡Œä¸å¿…è¦çš„å…¨è¡¨æ‰«æäº§ç”Ÿé«˜æˆæœ¬å¼‚å¸¸å€¼ã€‚æœ€åï¼Œè¯¥è®ºæ–‡æ€»ç»“äº†Text-to-SQLä¸­å¸¸è§çš„ä½æ•ˆæ¨¡å¼ï¼Œå¹¶ä¸ºä¼ä¸šçº§ç¯å¢ƒçš„æˆæœ¬æ•æ„Ÿå‹éƒ¨ç½²æä¾›äº†é‡è¦å‚è€ƒæŒ‡å—ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22364v1",
      "published_date": "2025-12-26 19:51:35 UTC",
      "updated_date": "2025-12-26 19:51:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:02:09.282629+00:00"
    },
    {
      "arxiv_id": "2512.23749v2",
      "title": "Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents",
      "title_zh": "åæ ‡çŸ©é˜µæœºï¼šä¸€ç§ç”¨äºæç›¸ä¼¼æ–‡æ¡£åˆ†ç±»çš„äººç±»çº§æ¦‚å¿µå­¦ä¹ ",
      "authors": [
        "Amin Sadri",
        "M Maruf Hossain"
      ],
      "abstract": "Human-level concept learning argues that humans typically learn new concepts from a single example, whereas machine learning algorithms typically require hundreds of samples to learn a single concept. Our brain subconsciously identifies important features and learns more effectively.\n  Contribution: In this paper, we present the Coordinate Matrix Machine (CM$^2$). This purpose-built small model augments human intelligence by learning document structures and using this information to classify documents. While modern \"Red AI\" trends rely on massive pre-training and energy-intensive GPU infrastructure, CM$^2$ is designed as a Green AI solution. It achieves human-level concept learning by identifying only the structural \"important features\" a human would consider, allowing it to classify very similar documents using only one sample per class.\n  Advantage: Our algorithm outperforms traditional vectorizers and complex deep learning models that require larger datasets and significant compute. By focusing on structural coordinates rather than exhaustive semantic vectors, CM$^2$ offers:\n  1. High accuracy with minimal data (one-shot learning) 2. Geometric and structural intelligence 3. Green AI and environmental sustainability 4. Optimized for CPU-only environments 5. Inherent explainability (glass-box model) 6. Faster computation and low latency 7. Robustness against unbalanced classes 8. Economic viability 9. Generic, expandable, and extendable",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Coordinate Matrix Machine (CM$^2$)ï¼Œä¸€ç§é€šè¿‡å­¦ä¹ æ–‡æ¡£ç»“æ„æ¥å®ç°äººç±»æ°´å¹³æ¦‚å¿µå­¦ä¹ (Human-level concept learning)çš„å°å‹æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•ä¾èµ–æµ·é‡æ•°æ®çš„é—®é¢˜ã€‚ä¸åŒäºæ¶ˆè€—å¤§é‡èƒ½æºçš„Red AIè¶‹åŠ¿ï¼ŒCM$^2$è¢«è®¾è®¡ä¸ºä¸€ç§Green AIè§£å†³æ–¹æ¡ˆï¼Œä¸“æ³¨äºè¯†åˆ«æ–‡æ¡£ä¸­äººç±»å…³æ³¨çš„å…³é”®ç»“æ„ç‰¹å¾ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç»“æ„åæ ‡è€Œéç©·å°½çš„è¯­ä¹‰å‘é‡ï¼Œä»…éœ€æ¯ç±»ä¸€ä¸ªæ ·æœ¬å³å¯å¯¹æåº¦ç›¸ä¼¼çš„æ–‡æ¡£è¿›è¡Œåˆ†ç±»ï¼Œå®ç°äº†é«˜æ•ˆçš„å•æ ·æœ¬å­¦ä¹ (one-shot learning)ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç®—æ³•åœ¨å‡†ç¡®ç‡ä¸Šä¼˜äºä¼ ç»Ÿçš„å‘é‡åŒ–å·¥å…·å’Œå¤æ‚çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¸”å…·æœ‰è®¡ç®—é€Ÿåº¦å¿«ã€ä½å»¶è¿Ÿä»¥åŠå¯¹ç±»åˆ«ä¸å¹³è¡¡(unbalanced classes)é²æ£’æ€§å¼ºç­‰ä¼˜ç‚¹ã€‚æ­¤å¤–ï¼ŒCM$^2$ä½œä¸ºä¸€ç§ç™½ç›’æ¨¡å‹(glass-box model)å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ï¼Œä¸”èƒ½å¤Ÿå®Œç¾é€‚é…ä»…é™CPUçš„è®¡ç®—ç¯å¢ƒã€‚è¯¥ç ”ç©¶ä¸ºå®ç°ç¯å¢ƒå¯æŒç»­ã€ç»æµå¯è¡Œä¸”å…·å¤‡å‡ ä½•ç»“æ„æ™ºèƒ½çš„æ–‡æ¡£åˆ†ç±»ä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.23749v2",
      "published_date": "2025-12-26 19:28:33 UTC",
      "updated_date": "2026-01-01 11:18:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:01:46.457832+00:00"
    },
    {
      "arxiv_id": "2601.04223v1",
      "title": "Beyond Interaction Effects: Two Logics for Studying Population Inequalities",
      "title_zh": "è¶…è¶Šäº¤äº’æ•ˆåº”ï¼šç ”ç©¶äººç¾¤ä¸å¹³ç­‰çš„ä¸¤ç§é€»è¾‘",
      "authors": [
        "Adel Daoud"
      ],
      "abstract": "When sociologists and other social scientist ask whether the return to college differs by race and gender, they face a choice between two fundamentally different modes of inquiry. Traditional interaction models follow deductive logic: the researcher specifies which variables moderate effects and tests these hypotheses. Machine learning methods follow inductive logic: algorithms search across vast combinatorial spaces to discover patterns of heterogeneity. This article develops a framework for navigating between these approaches. We show that the choice between deduction and induction reflects a tradeoff between interpretability and flexibility, and we demonstrate through simulation when each approach excels. Our framework is particularly relevant for inequality research, where understanding how treatment effects vary across intersecting social subpopulation is substantively central.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¤¾ä¼šç§‘å­¦ä¸­åˆ†æäººå£ä¸å¹³ç­‰çš„ä¸¤ç§åŸºæœ¬è·¯å¾„ï¼Œå³ä¼ ç»Ÿçš„äº¤äº’æ¨¡å‹(interaction models)ä¸æœºå™¨å­¦ä¹ æ–¹æ³•(machine learning methods)ã€‚ä¼ ç»Ÿæ¨¡å‹éµå¾ªæ¼”ç»é€»è¾‘(deductive logic)ï¼Œä¾èµ–ç ”ç©¶è€…é¢„å…ˆæŒ‡å®šçš„å˜é‡è¿›è¡Œå‡è®¾æ£€éªŒï¼Œè€Œæœºå™¨å­¦ä¹ æ–¹æ³•åˆ™é‡‡ç”¨å½’çº³é€»è¾‘(inductive logic)ï¼Œé€šè¿‡ç®—æ³•åœ¨å¹¿é˜”çš„ç»„åˆç©ºé—´ä¸­æœç´¢å¼‚è´¨æ€§æ¨¡å¼ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºåœ¨ä¸¤ç§æ–¹æ³•é—´è¿›è¡Œé€‰æ‹©å’Œè½¬æ¢çš„ç†è®ºæ¡†æ¶ï¼Œæ·±å…¥åˆ†æäº†å¯è§£é‡Šæ€§(interpretability)ä¸çµæ´»æ€§(flexibility)ä¹‹é—´çš„æƒè¡¡å…³ç³»ï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿå®éªŒéªŒè¯äº†å„æ–¹æ³•åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„ä¼˜åŠ¿ã€‚è¯¥æ¡†æ¶å¯¹äºä¸å¹³ç­‰ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ­ç¤ºå¤„ç†æ•ˆåº”(treatment effects)å¦‚ä½•è·¨è¶Šäº¤å‰ç¤¾ä¼šå­ç¾¤ä½“(intersecting social subpopulation)è€Œäº§ç”Ÿå·®å¼‚åˆ†å¸ƒæ–¹é¢æä¾›äº†æ·±åº¦çš„ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "econ.GN",
        "stat.ME"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.04223v1",
      "published_date": "2025-12-26 19:25:39 UTC",
      "updated_date": "2025-12-26 19:25:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:02:17.988775+00:00"
    },
    {
      "arxiv_id": "2512.22351v2",
      "title": "VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement",
      "title_zh": "VULCANï¼šé¢å‘è¿­ä»£å¼ 3D ç‰©ä½“å¸ƒç½®çš„å·¥å…·å¢å¼ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Zhengfei Kuang",
        "Rui Lin",
        "Long Zhao",
        "Gordon Wetzstein",
        "Saining Xie",
        "Sanghyun Woo"
      ],
      "abstract": "Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å¤„ç†å¤æ‚ 3D åœºæ™¯æ“ä½œæ—¶é¢ä¸´çš„è§†è§‰å®šä½å¼±ã€åœºæ™¯ç†è§£ä¸è¶³ä»¥åŠè¿­ä»£æ›´æ–°æ˜“å‡ºé”™ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† VULCAN æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è§†è§‰è½åœ°éš¾é¢˜ï¼Œè¯¥ç³»ç»Ÿå¼•å…¥äº†åŸºäº MCP çš„ APIï¼Œå°†äº¤äº’æ–¹å¼ä»è„†å¼±çš„åŸå§‹ä»£ç æ“ä½œè½¬å˜ä¸ºæ›´ç¨³å¥çš„åŠŸèƒ½çº§æ›´æ–°ã€‚åŒæ—¶ï¼ŒVULCAN é›†æˆäº†ä¸€ç³»åˆ—ä¸“é—¨çš„è§†è§‰å·¥å…·ä»¥å¢å¼º 3D åœºæ™¯æ„ŸçŸ¥ï¼Œé€šè¿‡åˆ†æåœºæ™¯çŠ¶æ€å’Œç©ºé—´ä¿¡æ¯æ„å»ºäº†å…³é”®çš„æ„ŸçŸ¥åé¦ˆé—­ç¯ã€‚è¯¥æ¡†æ¶è¿˜é‡‡ç”¨äº†åä½œå¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡è§„åˆ’ (planning)ã€æ‰§è¡Œ (execution) å’ŒéªŒè¯ (verification) çš„æ˜ç¡®åˆ†å·¥ï¼Œä½¿å…¶èƒ½å¤Ÿç¨³å¥åœ°å¤„ç†å¤šæ­¥æŒ‡ä»¤å¹¶ä»ä¸­é—´é”™è¯¯ä¸­æ¢å¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVULCAN åœ¨ 25 é¡¹å¤æ‚çš„ç‰©ä½“æ’åˆ—ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22351v2",
      "published_date": "2025-12-26 19:22:39 UTC",
      "updated_date": "2026-01-06 22:16:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:02:24.481234+00:00"
    },
    {
      "arxiv_id": "2512.22349v1",
      "title": "Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data",
      "title_zh": "ç±»äººè§†è§‰è®¡ç®—æå‡æ·±åº¦ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚ç”Ÿç†æ•°æ®çš„å¯è§£é‡Šæ€§ä¸å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›",
      "authors": [
        "Alaa Alahmadi",
        "Mohamed Hasan"
      ],
      "abstract": "Machine vision models, particularly deep neural networks, are increasingly applied to physiological signal interpretation, including electrocardiography (ECG), yet they typically require large training datasets and offer limited insight into the causal features underlying their predictions. This lack of data efficiency and interpretability constrains their clinical reliability and alignment with human reasoning. Here, we show that a perception-informed pseudo-colouring technique, previously demonstrated to enhance human ECG interpretation, can improve both explainability and few-shot learning in deep neural networks analysing complex physiological data.\n  We focus on acquired, drug-induced long QT syndrome (LQTS) as a challenging case study characterised by heterogeneous signal morphology, variable heart rate, and scarce positive cases associated with life-threatening arrhythmias such as torsades de pointes. This setting provides a stringent test of model generalisation under extreme data scarcity. By encoding clinically salient temporal features, such as QT-interval duration, into structured colour representations, models learn discriminative and interpretable features from as few as one or five training examples. Using prototypical networks and a ResNet-18 architecture, we evaluate one-shot and few-shot learning on ECG images derived from single cardiac cycles and full 10-second rhythms. Explainability analyses show that pseudo-colouring guides attention toward clinically meaningful ECG features while suppressing irrelevant signal components. Aggregating multiple cardiac cycles further improves performance, mirroring human perceptual averaging across heartbeats. Together, these findings demonstrate that human-like perceptual encoding can bridge data efficiency, explainability, and causal reasoning in medical machine intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ç±»äººè§†è§‰è®¡ç®—(Human-like visual computing)æå‡æ·±åº¦ç¥ç»ç½‘ç»œ(Deep Neural Networks)åœ¨å¤„ç†å¤æ‚ç”Ÿç†æ•°æ®æ—¶çš„å¯è§£é‡Šæ€§(Explainability)å’Œå°‘æ ·æœ¬å­¦ä¹ (Few-shot learning)èƒ½åŠ›ã€‚é’ˆå¯¹å¿ƒç”µå›¾(ECG)è§£é‡Šä¸­å­˜åœ¨çš„æ¨¡å‹æ•°æ®æ•ˆç‡ä½å’Œç¼ºä¹ä¸´åºŠæ¨ç†çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§åŸºäºæ„ŸçŸ¥çš„ä¼ªå½©è‰²æŠ€æœ¯(Pseudo-colouring technique)ï¼Œå¹¶å°†è¯ç‰©è¯±å‘çš„é•¿QTç»¼åˆå¾(LQTS)ä½œä¸ºæç«¯æ•°æ®ç¨€ç¼ºä¸‹çš„æŒ‘æˆ˜æ€§æ¡ˆä¾‹ã€‚é€šè¿‡å°†QTé—´æœŸæ—¶é•¿ç­‰ä¸´åºŠæ˜¾è‘—çš„æ—¶é—´ç‰¹å¾ç¼–ç ä¸ºç»“æ„åŒ–è‰²å½©è¡¨ç¤ºï¼Œç ”ç©¶åˆ©ç”¨åŸå‹ç½‘ç»œ(Prototypical networks)å’ŒResNet-18æ¶æ„ï¼Œä½¿å¾—æ¨¡å‹ä»…éœ€1åˆ°5ä¸ªè®­ç»ƒæ ·æœ¬å³å¯å­¦ä¹ åˆ°å…·æœ‰è¾¨åˆ«åŠ›çš„ç‰¹å¾ã€‚å¯è§£é‡Šæ€§åˆ†æè¯æ˜ï¼Œè¿™ç§ä¼ªå½©è‰²å¤„ç†èƒ½æœ‰æ•ˆå¼•å¯¼æ¨¡å‹å…³æ³¨ä¸´åºŠç›¸å…³çš„ECGç‰¹å¾ï¼Œå¹¶æŠ‘åˆ¶æ— å…³ä¿¡å·å¹²æ‰°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥çš„å¤šå¿ƒåŠ¨å‘¨æœŸå¹³å‡å¤„ç†ï¼Œæ¨¡å‹æ€§èƒ½å¾—åˆ°äº†è¿›ä¸€æ­¥æå‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç±»äººæ„ŸçŸ¥ç¼–ç å¯ä»¥åœ¨åŒ»ç–—æœºå™¨æ™ºèƒ½ä¸­æœ‰æ•ˆè¡”æ¥æ•°æ®æ•ˆç‡ã€å¯è§£é‡Šæ€§ä¸å› æœæ¨ç†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22349v1",
      "published_date": "2025-12-26 19:19:59 UTC",
      "updated_date": "2025-12-26 19:19:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:02:02.066075+00:00"
    },
    {
      "arxiv_id": "2512.22113v1",
      "title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications",
      "title_zh": "åŸºäºæ™ºèƒ½ä½“ç»“æ„åŒ–å›¾éå†çš„äº‘åº”ç”¨ä»£ç ç›¸å…³æ•…éšœæ ¹å› åˆ†æ",
      "authors": [
        "Shengkun Cui",
        "Rahul Krishna",
        "Saurabh Jha",
        "Ravishankar K. Iyer"
      ],
      "abstract": "Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº‘åº”ç”¨ä¸­ä»£ç å’Œé…ç½®ç›¸å…³æ•…éšœå¯¼è‡´çš„å·¨å¤§è¿è¥æˆæœ¬ï¼Œæå‡ºäº†åä¸º PRAXIS çš„æ™ºèƒ½ä½“å·¥ä½œæµç¼–æ’æ¡†æ¶ã€‚PRAXIS åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„ç»“æ„åŒ–éå†æŠ€æœ¯ï¼Œåœ¨å¾®æœåŠ¡å±‚çº§çš„æœåŠ¡ä¾èµ–å›¾(SDG)å’Œä»£ç å±‚çº§çš„ç¨‹åºä¾èµ–å›¾(PDG)ä¸Šè¿›è¡Œæ•…éšœè¯Šæ–­ã€‚LLM åœ¨è¿™äº›å›¾ä¸­å……å½“éå†ç­–ç•¥(Traversal Policy)ï¼Œé€šè¿‡åœ¨æœåŠ¡ä¸ä»£ç ä¾èµ–é¡¹ä¹‹é—´è¿›è¡Œè·³è½¬ï¼Œå®ç°å¯¹æ•…éšœçš„ç²¾å‡†å®šä½ä¸æ ¹å› è§£é‡Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRAXIS åœ¨æ ¹å› åˆ†æ(RCA)çš„å‡†ç¡®ç‡ä¸Šæ¯” ReAct åŸºçº¿æ¨¡å‹æé«˜äº† 3.1 å€ï¼ŒåŒæ—¶å°† Token æ¶ˆè€—é™ä½äº† 3.8 å€ã€‚è¯¥ç³»ç»Ÿåœ¨ä¸€ç»„åŒ…å« 30 ä¸ªçœŸå®ä¸–ç•Œäº‹æ•…çš„åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†å“è¶Šæ€§èƒ½ï¼Œä¸ºå¯æ‰©å±•ä¸”é«˜æ•ˆçš„äº‘æ•…éšœè‡ªåŠ¨åŒ–è¿ç»´æä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22113v1",
      "published_date": "2025-12-26 18:56:18 UTC",
      "updated_date": "2025-12-26 18:56:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:02:31.560068+00:00"
    },
    {
      "arxiv_id": "2512.22337v1",
      "title": "The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models",
      "title_zh": "è¿‘ä¼¼æ­£åˆ™åŒ–å›æ”¾åœ¨å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆæœ‰ç›‘ç£å¾®è°ƒä¸­çš„æœ‰æ•ˆæ€§",
      "authors": [
        "Matthew Riemer",
        "Erik Miehling",
        "Miao Liu",
        "Djallel Bouneffouf",
        "Murray Campbell"
      ],
      "abstract": "Although parameter-efficient fine-tuning methods, such as LoRA, only modify a small subset of parameters, they can have a significant impact on the model. Our instruction-tuning experiments show that LoRA-based supervised fine-tuning can catastrophically degrade model capabilities, even when trained on very small datasets for relatively few steps. With that said, we demonstrate that while the most straightforward approach (that is likely the most used in practice) fails spectacularly, small tweaks to the training procedure with very little overhead can virtually eliminate the problem. Particularly, in this paper we consider a regularized approximate replay approach which penalizes KL divergence with respect to the initial model and interleaves in data for next token prediction from a different, yet similar, open access corpus to what was used in pre-training. When applied to Qwen instruction-tuned models, we find that this recipe preserves general knowledge in the model without hindering plasticity to new tasks by adding a modest amount of computational overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³• LoRA åœ¨å¤§è¯­è¨€æ¨¡å‹ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰è¿‡ç¨‹ä¸­å¯èƒ½å¯¼è‡´æ¨¡å‹èƒ½åŠ›ç¾éš¾æ€§ä¸‹é™çš„é—®é¢˜ï¼Œå‘ç°å³ä½¿åœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œå°‘é‡è®­ç»ƒï¼ŒLoRA ä¹Ÿä¼šæ˜¾è‘—æŸå®³æ¨¡å‹çš„åŸæœ‰æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ­£åˆ™åŒ–è¿‘ä¼¼é‡æ”¾ï¼ˆRegularized Approximate Replayï¼‰æ–¹æ³•ï¼Œé€šè¿‡æƒ©ç½šä¸åˆå§‹æ¨¡å‹ä¹‹é—´çš„ KL Divergenceï¼Œå¹¶äº¤å‰è¾“å…¥ä¸é¢„è®­ç»ƒè¯­æ–™ç›¸ä¼¼çš„å¼€æºæ•°æ®è¿›è¡Œ Next Token Prediction è®­ç»ƒã€‚åœ¨ Qwen æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆèƒ½ä»¥æä½çš„è®¡ç®—å¼€é”€æœ‰æ•ˆä¿ç•™æ¨¡å‹çš„é€šç”¨çŸ¥è¯†ï¼ŒåŒæ—¶ä¸ä¼šæŸå®³æ¨¡å‹å­¦ä¹ æ–°ä»»åŠ¡çš„å¡‘æ€§ï¼ˆPlasticityï¼‰ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°é«˜æ•ˆä¸”ç¨³å¥çš„å¤§æ¨¡å‹å¾®è°ƒæä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ”¹è¿›ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22337v1",
      "published_date": "2025-12-26 18:55:42 UTC",
      "updated_date": "2025-12-26 18:55:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:02:14.026822+00:00"
    },
    {
      "arxiv_id": "2512.22336v1",
      "title": "Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback",
      "title_zh": "Agent2Worldï¼šé€šè¿‡è‡ªé€‚åº”å¤šæ™ºèƒ½ä½“åé¦ˆå­¦ä¹ ç”Ÿæˆç¬¦å·ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Mengkang Hu",
        "Bowei Xia",
        "Yuran Wu",
        "Ailing Yu",
        "Yude Zou",
        "Qiguang Chen",
        "Shijian Wang",
        "Jiarui Jin",
        "Kexin Li",
        "Wenxiang Jiao",
        "Yuan Lu",
        "Ping Luo"
      ],
      "abstract": "Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Agent2Worldï¼Œè¿™æ˜¯ä¸€ä¸ªå·¥å…·å¢å¼ºçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”Ÿæˆç¬¦å·ä¸–ç•Œæ¨¡å‹(Symbolic World Models)æ—¶é¢ä¸´çš„éªŒè¯æ€§ç›‘ç£ä¸è¶³å’Œé™æ€éªŒè¯å±€é™æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰é˜¶æ®µå·¥ä½œæµï¼Œé€šè¿‡ Deep Researcher è¿›è¡ŒçŸ¥è¯†åˆæˆï¼Œåˆ©ç”¨ Model Developer å®ç°å¯æ‰§è¡Œæ¨¡å‹ï¼Œå¹¶ç”±ä¸“ä¸šçš„ Testing Team æ‰§è¡Œè‡ªé€‚åº”å•å…ƒæµ‹è¯•å’Œæ¨¡æ‹ŸéªŒè¯ã€‚Agent2World çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨å¤šæ™ºèƒ½ä½“åé¦ˆæœºåˆ¶å°†ç”Ÿæˆè¿‡ç¨‹è½åœ°ï¼Œå…¶ä¸­ Testing Team æä¾›çš„æ„ŸçŸ¥è¡Œä¸ºè‡ªé€‚åº”åé¦ˆå¯ç”Ÿæˆé«˜è´¨é‡çš„å¤šè½®è®­ç»ƒè½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgent2World åœ¨ PDDL å’Œå¯æ‰§è¡Œä»£ç è¡¨ç¤ºçš„å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†å½“å‰æœ€å…ˆè¿›æ°´å¹³(SOTA)çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨è¯¥æ¡†æ¶ä½œä¸ºæ•°æ®å¼•æ“è¿›è¡Œçš„ç›‘ç£å¾®è°ƒ(SFT)ä½¿æ¨¡å‹æ€§èƒ½å¹³å‡ç›¸å¯¹æå‡äº† 30.95%ï¼Œè¯æ˜äº†å…¶åœ¨è‡ªåŠ¨åŒ–æ„å»ºå¯éªŒè¯ä¸–ç•Œæ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "48 pages, 15 tables, 7 figures, Project page: https://agent2world.github.io",
      "pdf_url": "https://arxiv.org/pdf/2512.22336v1",
      "published_date": "2025-12-26 18:54:14 UTC",
      "updated_date": "2025-12-26 18:54:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:02:47.903113+00:00"
    },
    {
      "arxiv_id": "2601.00837v1",
      "title": "Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs",
      "title_zh": "åŸºäºèƒ¸éƒ¨ X å°„çº¿çš„å„¿ç«¥è‚ºç‚æ£€æµ‹ï¼šè¿ç§»å­¦ä¹ ä¸è‡ªå®šä¹‰å·ç§¯ç¥ç»ç½‘ç»œçš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Agniv Roy Choudhury"
      ],
      "abstract": "Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.\n  Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.\n  Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.\n  Results: Fine-tuned ResNet50 achieved the best performance: 99.43\\% accuracy, 99.61\\% F1-score, and 99.93\\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.\n  Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.\n  Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒäº”å²ä»¥ä¸‹å„¿ç«¥ä¸»è¦æ­»å› ä¹‹ä¸€çš„è‚ºç‚ï¼Œé€šè¿‡èƒ¸éƒ¨Xå°„çº¿(Chest X-Rays)å½±åƒå¯¹æ¯”äº†è‡ªå»ºå·ç§¯ç¥ç»ç½‘ç»œ(Custom CNNs)ä¸è¿ç§»å­¦ä¹ (Transfer Learning)åœ¨å„¿ç§‘è‚ºç‚æ£€æµ‹ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†ResNet50ã€DenseNet121å’ŒEfficientNet-B0ç­‰é¢„è®­ç»ƒæ¨¡å‹åœ¨å†»ç»“éª¨å¹²(Frozen-backbone)å’Œå¾®è°ƒ(Fine-tuning)ä¸¤ç§æ¨¡å¼ä¸‹çš„æ•ˆèƒ½ã€‚å®éªŒåˆ©ç”¨5,216å¼ å½±åƒçš„æ•°æ®é›†ï¼Œé‡‡ç”¨å‡†ç¡®ç‡(Accuracy)ã€F1-scoreå’ŒAUCç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ç»“åˆGrad-CAMæŠ€æœ¯éªŒè¯äº†æ¨¡å‹é¢„æµ‹ä¸ä¸´åºŠç›¸å…³è‚ºéƒ¨åŒºåŸŸçš„ä¸€è‡´æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„ResNet50è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œè¾¾åˆ°äº†99.43%çš„å‡†ç¡®ç‡å’Œ99.93%çš„AUCï¼Œä¸”å¾®è°ƒæ¨¡å¼æ¯”å†»ç»“éª¨å¹²æ¨¡å¼å¹³å‡æ€§èƒ½æå‡äº†5.5ä¸ªç™¾åˆ†ç‚¹ã€‚ç ”ç©¶å‘ç°å¾®è°ƒåçš„è¿ç§»å­¦ä¹ æ¨¡å‹åœ¨æ£€æµ‹ç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºä»é›¶å¼€å§‹è®­ç»ƒçš„CNNæ¨¡å‹ï¼Œè¡¨ç°å‡ºæ¥è¿‘å®Œç¾çš„å‡†ç¡®æ€§ã€‚è¯¥æˆæœè¯æ˜äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å„¿ç§‘è‚ºç‚è‡ªåŠ¨åŒ–è¯Šæ–­ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—èµ„æºå—é™åœ°åŒºå¯ä½œä¸ºé«˜æ•ˆçš„ä¸´åºŠç­›æŸ¥å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00837v1",
      "published_date": "2025-12-26 18:48:39 UTC",
      "updated_date": "2025-12-26 18:48:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:02:48.681519+00:00"
    },
    {
      "arxiv_id": "2512.22106v1",
      "title": "Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks",
      "title_zh": "å‰ªæå³åšå¼ˆï¼šå‡è¡¡é©±åŠ¨çš„ç¥ç»ç½‘ç»œç¨€ç–åŒ–",
      "authors": [
        "Zubair Shah",
        "Noaman Khan"
      ],
      "abstract": "Neural network pruning is widely used to reduce model size and computational cost. Yet, most existing methods treat sparsity as an externally imposed constraint, enforced through heuristic importance scores or training-time regularization. In this work, we propose a fundamentally different perspective: pruning as an equilibrium outcome of strategic interaction among model components. We model parameter groups such as weights, neurons, or filters as players in a continuous non-cooperative game, where each player selects its level of participation in the network to balance contribution against redundancy and competition. Within this formulation, sparsity emerges naturally when continued participation becomes a dominated strategy at equilibrium. We analyze the resulting game and show that dominated players collapse to zero participation under mild conditions, providing a principled explanation for pruning behavior. Building on this insight, we derive a simple equilibrium-driven pruning algorithm that jointly updates network parameters and participation variables without relying on explicit importance scores. This work focuses on establishing a principled formulation and empirical validation of pruning as an equilibrium phenomenon, rather than exhaustive architectural or large-scale benchmarking. Experiments on standard benchmarks demonstrate that the proposed approach achieves competitive sparsity-accuracy trade-offs while offering an interpretable, theory-grounded alternative to existing pruning methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºPruning as a Gameçš„æ–°å‹ç¥ç»ç½‘ç»œå‰ªææ¡†æ¶ï¼Œå°†å‰ªæè¿‡ç¨‹å»ºæ¨¡ä¸ºæ¨¡å‹ç»„ä»¶ä¹‹é—´æˆ˜ç•¥äº¤äº’çš„å‡è¡¡(Equilibrium)ç»“æœã€‚ç ”ç©¶è€…å°†æƒé‡ã€ç¥ç»å…ƒæˆ–æ»¤æ³¢å™¨ç­‰å‚æ•°ç»„è§†ä¸ºéåˆä½œåšå¼ˆ(Non-cooperative game)ä¸­çš„å‚ä¸æ–¹ï¼Œå„å‚ä¸æ–¹æ ¹æ®å…¶å¯¹ç½‘ç»œçš„è´¡çŒ®ã€å†—ä½™ç¨‹åº¦å’Œç«äº‰å…³ç³»é€‰æ‹©å‚ä¸æ°´å¹³ã€‚åœ¨è¿™ç§åšå¼ˆè®ºè¡¨è¿°ä¸‹ï¼Œå½“æŒç»­å‚ä¸åšå¼ˆåœ¨å‡è¡¡çŠ¶æ€ä¸‹æˆä¸ºè¢«æ”¯é…ç­–ç•¥(Dominated strategy)æ—¶ï¼Œæ¨¡å‹ç»„ä»¶ä¼šåç¼©è‡³é›¶å‚ä¸ï¼Œä»è€Œä½¿ç¨€ç–æ€§(Sparsity)è‡ªç„¶äº§ç”Ÿã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œè¯¥ç ”ç©¶æ¨å¯¼å‡ºä¸€ç§å‡è¡¡é©±åŠ¨çš„å‰ªæç®—æ³•ï¼Œèƒ½å¤ŸåŒæ—¶æ›´æ–°ç½‘ç»œå‚æ•°å’Œå‚ä¸å˜é‡ï¼Œè€Œæ— éœ€ä¾èµ–ä¼ ç»Ÿçš„å¯å‘å¼é‡è¦æ€§åˆ†æ•°(Importance scores)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„ç¨€ç–æ€§-å‡†ç¡®ç‡æƒè¡¡(Sparsity-accuracy trade-offs)ï¼Œä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©æä¾›äº†ä¸€ä¸ªå…·æœ‰å¯è§£é‡Šæ€§ä¸”å…·å¤‡ç†è®ºåŸºç¡€çš„æ–°è§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Under review / to be submitted to a conference",
      "pdf_url": "https://arxiv.org/pdf/2512.22106v1",
      "published_date": "2025-12-26 18:25:38 UTC",
      "updated_date": "2025-12-26 18:25:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:03:32.506160+00:00"
    },
    {
      "arxiv_id": "2512.23747v1",
      "title": "State-of-the-art Small Language Coder Model: Mify-Coder",
      "title_zh": "Mify-Coderï¼šé¡¶å°–çš„å°å‹è¯­è¨€ä»£ç æ¨¡å‹",
      "authors": [
        "Abhinav Parmar",
        "Abhisek Panigrahi",
        "Abhishek Kumar Dwivedi",
        "Abhishek Bhattacharya",
        "Adarsh Ramachandra",
        "Aditya Choudhary",
        "Aditya Garg",
        "Aditya Raj",
        "Alankrit Bhatt",
        "Alpesh Yadav",
        "Anant Vishnu",
        "Ananthu Pillai",
        "Ankush Kumar",
        "Aryan Patnaik",
        "Aswatha Narayanan S",
        "Avanish Raj Singh",
        "Bhavya Shree Gadda",
        "Brijesh Pankajbhai Kachhadiya",
        "Buggala Jahnavi",
        "Chidurala Nithin Krishna",
        "Chintan Shah",
        "Chunduru Akshaya",
        "Debarshi Banerjee",
        "Debrup Dey",
        "Deepa R.",
        "Deepika B G",
        "Faiz ur Rahman",
        "Gagan Gayari",
        "Gudhi Jagadeesh Kumar Naidu",
        "Gursimar Singh",
        "Harshal Tyagi",
        "Harshini K",
        "James Mani Vathalloor",
        "Jayarama Nettar",
        "Jayashree Gajjam",
        "Joe Walter Sugil George",
        "Kamalakara Sri Krishna Tadepalli",
        "Kamalkumar Rathinasamy",
        "Karan Chaurasia",
        "Karthikeyan S",
        "Kashish Arora",
        "Kaushal Desai",
        "Khushboo Buwade",
        "Kiran Manjrekar",
        "Malikireddy Venkata Sai Likhitha",
        "Manjunath A",
        "Mitali Mahavir Bedmutha",
        "Mohammed Rafee Tarafdar",
        "Nikhil Tiwari",
        "Nikitha K Gigi",
        "Pavan Ravikumar",
        "Pendyala Swarnanjali",
        "Piyush Anand",
        "Prakash Chandrasekar",
        "Prasanna Bhalchandra Gawade",
        "Prasanth Sivan",
        "Preeti Khurana",
        "Priyanshi Babbar",
        "Rajab Ali Mondal",
        "Rajesh Kumar Vissapragada",
        "Rajeshwari Ganesan",
        "Rajeswari Koppisetti",
        "Ramjee R.",
        "Ramkumar Thiruppathisamy",
        "Rani G. S.",
        "S Reka",
        "Samarth Gupta",
        "Sandeep Reddy Kothakota",
        "Sarathy K",
        "Sathyanarayana Sampath Kumar",
        "Saurabh Kumar",
        "Shashank Khasare",
        "Shenbaga Devi Venkatesh Kumar",
        "Shiva Rama Krishna Parvatham",
        "Shoeb Shaikh",
        "Shrishanmathi A",
        "Shubham Pathak",
        "Sree Samhita Koppaka",
        "Sreenivasa Raghavan K S",
        "Sreeram Venkatasubramanian",
        "Suprabha Desai Bojja",
        "Swetha R",
        "Syed Ahmed",
        "Chinmai Harshitha Thota",
        "Tushar Yadav",
        "Veeravelly Kusumitha",
        "V V S S Prasanth Patnaik",
        "Vidya Sri Sesetti",
        "Vijayakeerthi K",
        "Vikram Raj Bakshi",
        "Vinay K K",
        "Vinoth Kumar Loganathan",
        "Vipin Tiwari",
        "Vivek Kumar Shrivastav",
        "V Venkata Sri Datta Charan",
        "Wasim Akhtar Khan"
      ],
      "abstract": "We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic data generated through agentically designed prompts, refined iteratively using enterprise-grade evaluation datasets. LLM-based quality filtering further enhances data density, enabling frugal yet effective training. Through disciplined exploration of CPT-SFT objectives, data mixtures, and sampling dynamics, we deliver frontier-grade code intelligence within a single continuous training trajectory. Empirical evidence shows that principled data and compute discipline allow smaller models to achieve competitive accuracy, efficiency, and safety compliance. Quantized variants of Mify-Coder enable deployment on standard desktop environments without requiring specialized hardware.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Mify-Coderï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰2.5Bå‚æ•°çš„é«˜æ€§èƒ½ä»£ç æ¨¡å‹ï¼ŒåŸºäºMify-2.5BåŸºç¡€æ¨¡å‹å¹¶åœ¨4.2T tokensä¸Šé‡‡ç”¨compute-optimalç­–ç•¥è®­ç»ƒè€Œæˆã€‚Mify-Coderåœ¨æ ‡å‡†ä»£ç ç”Ÿæˆå’Œfunction-calling benchmarksä¸Šæ˜¾è‘—è¶…è¶Šäº†ä½“ç§¯å¤§å¾—å¤šçš„åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†ç´§å‡‘å‹æ¨¡å‹åŒæ ·èƒ½å®ç°frontier-gradeçº§åˆ«çš„ä»£ç æ™ºèƒ½ã€‚å…¶è®­ç»ƒæµæ°´çº¿ç»“åˆäº†é«˜è´¨é‡ç­›é€‰æºä¸é€šè¿‡agentically designed promptsç”Ÿæˆçš„synthetic dataï¼Œå¹¶åˆ©ç”¨ä¼ä¸šçº§è¯„ä¼°æ•°æ®é›†è¿›è¡Œäº†è¿­ä»£ä¼˜åŒ–ã€‚é€šè¿‡LLM-based quality filteringæŠ€æœ¯æå‡æ•°æ®å¯†åº¦ï¼Œè¯¥æ¨¡å‹åœ¨CPT-SFTç›®æ ‡ä¸‹å®ç°äº†é«˜æ•ˆçš„æŒç»­è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸¥è°¨çš„æ•°æ®ä¸è®¡ç®—èµ„æºç®¡ç†ä½¿å°æ¨¡å‹åœ¨å‡†ç¡®æ€§ã€æ•ˆç‡åŠå®‰å…¨åˆè§„æ€§ä¸Šæå…·ç«äº‰åŠ›ï¼Œå…¶quantized variantsç”šè‡³å¯ä»¥åœ¨æ— éœ€ä¸“é—¨ç¡¬ä»¶çš„æ™®é€šæ¡Œé¢ç¯å¢ƒä¸‹ç›´æ¥éƒ¨ç½²ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23747v1",
      "published_date": "2025-12-26 18:16:02 UTC",
      "updated_date": "2025-12-26 18:16:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:02:53.220213+00:00"
    },
    {
      "arxiv_id": "2512.22101v1",
      "title": "A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting",
      "title_zh": "A2P-Visï¼šç”¨äºå¯è§†åŒ–æ´å¯Ÿç”Ÿæˆä¸æŠ¥å‘Šçš„â€œåˆ†æå™¨åˆ°å±•ç¤ºå™¨â€æ™ºèƒ½ä½“åŒ–æµæ°´çº¿",
      "authors": [
        "Shuyu Gan",
        "Renxiang Wang",
        "James Mooney",
        "Dongyeop Kang"
      ],
      "abstract": "Automating end-to-end data science pipeline with AI agents still stalls on two gaps: generating insightful, diverse visual evidence and assembling it into a coherent, professional report. We present A2P-Vis, a two-part, multi-agent pipeline that turns raw datasets into a high-quality data-visualization report. The Data Analyzer orchestrates profiling, proposes diverse visualization directions, generates and executes plotting code, filters low-quality figures with a legibility checker, and elicits candidate insights that are automatically scored for depth, correctness, specificity, depth and actionability. The Presenter then orders topics, composes chart-grounded narratives from the top-ranked insights, writes justified transitions, and revises the document for clarity and consistency, yielding a coherent, publication-ready report. Together, these agents convert raw data into curated materials (charts + vetted insights) and into a readable narrative without manual glue work. We claim that by coupling a quality-assured Analyzer with a narrative Presenter, A2P-Vis operationalizes co-analysis end-to-end, improving the real-world usefulness of automated data analysis for practitioners. For the complete dataset report, please see: https://www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†A2P-Visï¼Œä¸€ä¸ªç”±åˆ†æå™¨åˆ°æ¼”ç¤ºè€…(Analyzer-to-Presenter)çš„å¤šæ™ºèƒ½ä½“ç®¡çº¿ï¼Œæ—¨åœ¨å°†åŸå§‹æ•°æ®é›†è‡ªåŠ¨åŒ–è½¬åŒ–ä¸ºé«˜è´¨é‡çš„æ•°æ®å¯è§†åŒ–æŠ¥å‘Šã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†çš„åä½œï¼Œå¼¥åˆäº†AIæ™ºèƒ½ä½“åœ¨ç”Ÿæˆå¤šæ ·åŒ–è§†è§‰è¯æ®åŠç»„è£…è¿è´¯ä¸“ä¸šæŠ¥å‘Šæ–¹é¢çš„é¸¿æ²Ÿã€‚Data Analyzerè´Ÿè´£æ‰§è¡Œæ•°æ®å‰–æ(profiling)ã€ç”Ÿæˆç»˜å›¾ä»£ç ï¼Œå¹¶åˆ©ç”¨å¯è¯»æ€§æ£€æŸ¥å™¨(legibility checker)è¿‡æ»¤ä½è´¨é‡å›¾è¡¨ï¼ŒåŒæ—¶å¯¹æå–çš„è§è§£(insights)è¿›è¡Œå¤šç»´åº¦çš„è‡ªåŠ¨åŒ–è¯„åˆ†ã€‚éšåï¼ŒPresenterè´Ÿè´£å¯¹ä¸»é¢˜æ’åºå¹¶æ„å»ºå›¾è¡¨å™è¿°ï¼Œé€šè¿‡ç¼–å†™é€»è¾‘è½¬æ¥å’Œä¿®è®¢å†…å®¹ï¼Œç”Ÿæˆå¯ç›´æ¥å‘å¸ƒçš„è¿è´¯æŠ¥å‘Šã€‚é€šè¿‡å°†è´¨é‡å—æ§çš„Analyzerä¸æ“…é•¿å™äº‹çš„Presenterç›¸ç»“åˆï¼ŒA2P-Viså®ç°äº†ç«¯åˆ°ç«¯çš„å…±åŒåˆ†æ(co-analysis)ï¼Œæ˜¾è‘—æå‡äº†è‡ªåŠ¨åŒ–æ•°æ®åˆ†æåœ¨å®é™…å·¥ä½œä¸­çš„å¯ç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "3 pages, 3 figures; Accepted by 1st Workshop on GenAI, Agents and the Future of VIS as Mini-challenge paper and win the Honorable Mention award. Submit number is 7597 and the paper is archived on the workshop website: https://visxgenai.github.io/subs-2025/7597/7597-doc.pdf",
      "pdf_url": "https://arxiv.org/pdf/2512.22101v1",
      "published_date": "2025-12-26 18:02:12 UTC",
      "updated_date": "2025-12-26 18:02:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:03:06.831503+00:00"
    },
    {
      "arxiv_id": "2512.22100v1",
      "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis",
      "title_zh": "TrGLUE ä¸ SentiTurcaï¼šåœŸè€³å…¶è¯­é€šç”¨è¯­è¨€ç†è§£ä¸æƒ…æ„Ÿåˆ†æç»¼åˆè¯„æµ‹åŸºå‡†",
      "authors": [
        "Duygu Altinok"
      ],
      "abstract": "Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœŸè€³å…¶è¯­ç¼ºä¹ç±»ä¼¼GLUEåŸºå‡†æµ‹è¯•çš„é—®é¢˜ï¼Œæ­£å¼æ¨å‡ºäº†TrGLUEå’ŒSentiTurcaï¼Œæ—¨åœ¨ä¸ºåœŸè€³å…¶è¯­çš„Natural Language Understanding (NLU)å’ŒSentiment Analysisæä¾›å…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚TrGLUEæ¶µç›–äº†å¤šç§NLUä»»åŠ¡ï¼Œå…¶è¯­æ–™åº“é€šè¿‡é‡‡é›†åœŸè€³å…¶æœ¬åœŸæ•°æ®å¹¶é‡‡ç”¨ç»“åˆLLMæ ‡æ³¨ã€è·¨æ¨¡å‹ä¸€è‡´æ€§æ£€æŸ¥åŠäººç±»éªŒè¯çš„åŠè‡ªåŠ¨åŒ–æµç¨‹æ„å»ºï¼Œæœ‰æ•ˆä¿è¯äº†è¯­è¨€è‡ªç„¶åº¦å¹¶æ˜¾è‘—å‡å°‘äº†ç¿»è¯‘ç—•è¿¹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æä¾›äº†é’ˆå¯¹Transformer-basedæ¨¡å‹çš„å¾®è°ƒä¸è¯„ä¼°ä»£ç ï¼Œä»¥ä¿ƒæˆè¯¥åŸºå‡†åœ¨å­¦æœ¯ç•Œçš„æœ‰æ•ˆåº”ç”¨ã€‚è¯¥é¡¹å·¥ä½œå¡«è¡¥äº†åœŸè€³å…¶è¯­åœ¨é€šç”¨è¯­è¨€ç†è§£è¯„ä¼°é¢†åŸŸçš„ç©ºç™½ï¼Œä¸ºç›¸å…³æ¨¡å‹çš„èƒ½åŠ›è¡¡é‡æä¾›äº†ç¨³å¥çš„æ ‡å‡†ã€‚é€šè¿‡è¿™ä¸€è´¡çŒ®ï¼Œç ”ç©¶è€…ä¸ä»…æä¾›äº†é«˜è´¨é‡çš„èµ„æºï¼Œè¿˜ä¸ºæ„å»ºå¯æ‰©å±•ã€å¯å¤åˆ¶çš„åŠè‡ªåŠ¨åŒ–æ•°æ®é›†å·¥ä½œæµæä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "under review by Springer",
      "pdf_url": "https://arxiv.org/pdf/2512.22100v1",
      "published_date": "2025-12-26 18:02:09 UTC",
      "updated_date": "2025-12-26 18:02:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:03:05.713286+00:00"
    },
    {
      "arxiv_id": "2512.22335v1",
      "title": "Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides",
      "title_zh": "åŸºäºå¤šé˜¶æ®µè§†è§‰ Transformer çš„å…¨åˆ‡ç‰‡è·¨æ¨¡æ€ HER2 çŠ¶æ€è¯„åˆ†ä¸è‚¿ç˜¤åˆ†ç±»ç‰¹å¾å­¦ä¹ ",
      "authors": [
        "Olaide N. Oyelade",
        "Oliver Hoxey",
        "Yulia Humrye"
      ],
      "abstract": "The popular use of histopathology images, such as hematoxylin and eosin (H&E), has proven to be useful in detecting tumors. However, moving such cancer cases forward for treatment requires accurate on the amount of the human epidermal growth factor receptor 2 (HER2) protein expression. Predicting both the lower and higher levels of HER2 can be challenging. Moreover, jointly analyzing H&E and immunohistochemistry (IHC) stained images for HER2 scoring is difficult. Although several deep learning methods have been investigated to address the challenge of HER2 scoring, they suffer from providing a pixel-level localization of HER2 status. In this study, we propose a single end-to-end pipeline using a system of vision transformers with HER2 status scoring on whole slide images of WSIs. The method includes patch-wise processing of H&E WSIs for tumor localization. A novel mapping function is proposed to correspondingly identify correlated IHC WSIs regions with malignant regions on H&E. A clinically inspired HER2 scoring mechanism is embedded in the pipeline and allows for automatic pixel-level annotation of 4-way HER2 scoring (0, 1+, 2+, and 3+). Also, the proposed method accurately returns HER2-negative and HER2-positive. Privately curated datasets were collaboratively extracted from 13 different cases of WSIs of H&E and IHC. A thorough experiment is conducted on the proposed method. Results obtained showed a good classification accuracy during tumor localization. Also, a classification accuracy of 0.94 and a specificity of 0.933 were returned for the prediction of HER2 status, scoring in the 4-way methods. The applicability of the proposed pipeline was investigated using WSIs patches as comparable to human pathologists. Findings from the study showed the usability of jointly evaluated H&E and IHC images on end-to-end ViTs-based models for HER2 scoring",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤šé˜¶æ®µ Vision Transformers (ViTs) çš„ç«¯åˆ°ç«¯æµæ°´çº¿ï¼Œæ—¨åœ¨å®ç°å…¨æ‰«æå›¾åƒ (WSIs) ä¸Šçš„è‚¿ç˜¤åˆ†ç±»ä¸è·¨æ¨¡æ€ HER2 çŠ¶æ€è¯„åˆ†ã€‚è¯¥ç³»ç»Ÿé¦–å…ˆé€šè¿‡å¯¹ H&E æŸ“è‰²çš„ WSIs è¿›è¡Œåˆ†å—å¤„ç†æ¥å®šä½è‚¿ç˜¤ï¼Œå¹¶å¼•å…¥ä¸€ç§æ–°é¢–çš„æ˜ å°„å‡½æ•°ï¼Œä»¥å®ç° H&E ä¸ IHC æŸ“è‰²å›¾åƒä¸­æ¶æ€§åŒºåŸŸçš„ç²¾ç¡®å¯¹åº”ã€‚ç ”ç©¶åœ¨æµç¨‹ä¸­åµŒå…¥äº†å—ä¸´åºŠå¯å‘çš„ HER2 è¯„åˆ†æœºåˆ¶ï¼Œæ”¯æŒå¯¹ 0ã€1+ã€2+ å’Œ 3+ çŠ¶æ€è¿›è¡Œè‡ªåŠ¨åƒç´ çº§æ ‡æ³¨ï¼Œå¹¶èƒ½å‡†ç¡®åŒºåˆ† HER2 é˜´æ€§ä¸é˜³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ HER2 çŠ¶æ€é¢„æµ‹ä¸­å–å¾—äº† 0.94 çš„åˆ†ç±»å‡†ç¡®ç‡å’Œ 0.933 çš„ç‰¹å¼‚æ€§ï¼Œè¡¨ç°ä¸äººç±»ç—…ç†å­¦å®¶å…·æœ‰å¯æ¯”æ€§ã€‚è¯¥å·¥ä½œè¯æ˜äº†åˆ©ç”¨ Vision Transformers ååŒè¯„ä¼° H&E å’Œ IHC å›¾åƒåœ¨è‡ªåŠ¨åŒ–ç—…ç†è¯Šæ–­ä¸­çš„æœ‰æ•ˆæ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22335v1",
      "published_date": "2025-12-26 17:45:09 UTC",
      "updated_date": "2025-12-26 17:45:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:03:47.032339+00:00"
    },
    {
      "arxiv_id": "2512.22334v3",
      "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
      "title_zh": "SciEvalKitï¼šé¢å‘ç§‘å­¦é€šç”¨æ™ºèƒ½çš„å¼€æºè¯„ä¼°å·¥å…·åŒ…",
      "authors": [
        "Yiheng Wang",
        "Yixin Chen",
        "Shuo Li",
        "Yifan Zhou",
        "Bo Liu",
        "Hengjian Gao",
        "Jiakang Yuan",
        "Jia Bu",
        "Wanghan Xu",
        "Yuhao Zhou",
        "Xiangyu Zhao",
        "Zhiwang Zhou",
        "Fengxiang Wang",
        "Haodong Duan",
        "Songyang Zhang",
        "Jun Yao",
        "Han Deng",
        "Yizhou Wang",
        "Jiabei Xiao",
        "Jiaqi Liu",
        "Encheng Su",
        "Yujie Liu",
        "Weida Wang",
        "Junchi Yao",
        "Shenghe Zheng",
        "Haoran Sun",
        "Runmin Ma",
        "Xiangchao Yan",
        "Bo Zhang",
        "Dongzhan Zhou",
        "Shufei Zhang",
        "Peng Ye",
        "Xiaosong Wang",
        "Shixiang Tang",
        "Wenlong Zhang",
        "Lei Bai"
      ],
      "abstract": "We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† SciEvalKitï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ç§‘å­¦é¢†åŸŸ AI æ¨¡å‹çš„ç»Ÿä¸€åŸºå‡†æµ‹è¯•å·¥å…·åŒ…ã€‚ä¸é€šç”¨å¹³å°ä¸åŒï¼ŒSciEvalKit ä¸“æ³¨äºæ ¸å¿ƒç§‘å­¦æ™ºèƒ½èƒ½åŠ›ï¼ŒåŒ…æ‹¬ Scientific Multimodal Perceptionã€Scientific Multimodal Reasoningã€Scientific Code Generation å’Œ Science Hypothesis Generation ç­‰å…³é”®ç»´åº¦ã€‚å®ƒæ”¯æŒç‰©ç†ã€åŒ–å­¦ã€å¤©æ–‡å­¦å’Œææ–™ç§‘å­¦ç­‰å…­å¤§ä¸»è¦ç§‘å­¦é¢†åŸŸï¼Œå¹¶å»ºç«‹äº†åŸºäºçœŸå®ä¸–ç•Œç‰¹å®šé¢†åŸŸæ•°æ®é›†çš„ä¸“å®¶çº§åŸºå‡†ã€‚è¯¥å·¥å…·åŒ…æä¾›äº†ä¸€ä¸ªçµæ´»ä¸”å¯æ‰©å±•çš„è¯„ä¼°æµç¨‹ï¼Œæ”¯æŒè·¨æ¨¡å‹ä¸æ•°æ®é›†çš„æ‰¹é‡è¯„ä¼°ï¼Œå¹¶å…è®¸é›†æˆè‡ªå®šä¹‰çš„æ¨¡å‹ä¸æ•°æ®é›†ã€‚é€šè¿‡è¿æ¥åŸºäºèƒ½åŠ›çš„è¯„ä¼°å’Œå­¦ç§‘å¤šæ ·æ€§ï¼ŒSciEvalKit ä¸ºåŸºå‡†æµ‹è¯•ä¸‹ä¸€ä»£ç§‘å­¦å¤§æ¨¡å‹(scientific foundation models)å’Œæ™ºèƒ½ä½“æä¾›äº†æ ‡å‡†åŒ–ä¸”å¯å®šåˆ¶çš„åŸºç¡€è®¾æ–½ã€‚è¯¥å·¥å…·åŒ…ç›®å‰å·²å¼€æºï¼Œæ—¨åœ¨é€šè¿‡ç¤¾åŒºé©±åŠ¨çš„æ–¹å¼ä¿ƒè¿› AI4Science é¢†åŸŸçš„å‘å±•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22334v3",
      "published_date": "2025-12-26 17:36:02 UTC",
      "updated_date": "2026-01-06 09:26:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:03:06.266996+00:00"
    },
    {
      "arxiv_id": "2512.22088v1",
      "title": "Unifying Learning Dynamics and Generalization in Transformers Scaling Law",
      "title_zh": "Transformer ç¼©æ”¾æ³•åˆ™ä¸­å­¦ä¹ åŠ¨åŠ›å­¦ä¸æ³›åŒ–çš„ç»Ÿä¸€",
      "authors": [
        "Chiwun Yang"
      ],
      "abstract": "The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics of transformer-based language models as an ordinary differential equation (ODE) system, then approximates this process to kernel behaviors. Departing from prior toy-model analyses, we rigorously analyze stochastic gradient descent (SGD) training for multi-layer transformers on sequence-to-sequence data with arbitrary data distribution, closely mirroring real-world conditions. Our analysis characterizes the convergence of generalization error to the irreducible risk as computational resources scale with data, especially during the optimization process.\n  We establish a theoretical upper bound on excess risk characterized by a distinct phase transition. In the initial optimization phase, the excess risk decays exponentially relative to the computational cost ${\\sf C}$. However, once a specific resource allocation threshold is crossed, the system enters a statistical phase, where the generalization error follows a power-law decay of $Î˜(\\mathsf{C}^{-1/6})$. Beyond this unified framework, our theory derives isolated scaling laws for model size, training time, and dataset size, elucidating how each variable independently governs the upper bounds of generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ¢è®¨Transformeræ¨¡å‹Scaling Lawçš„ç†è®ºåŸºç¡€ï¼Œå°†å…¶å­¦ä¹ åŠ¨æ€å½¢å¼åŒ–ä¸ºå¸¸å¾®åˆ†æ–¹ç¨‹(ODE)ç³»ç»Ÿå¹¶è¿‘ä¼¼ä¸ºæ ¸è¡Œä¸ºã€‚ä¸ä»¥å¾€çš„ç®€åŒ–æ¨¡å‹åˆ†æä¸åŒï¼Œè¯¥å·¥ä½œåœ¨æ¥è¿‘çœŸå®ç¯å¢ƒçš„ä»»æ„æ•°æ®åˆ†å¸ƒä¸‹ï¼Œå¯¹å¤šå±‚Transformerçš„éšæœºæ¢¯åº¦ä¸‹é™(SGD)è®­ç»ƒè¿‡ç¨‹è¿›è¡Œäº†ä¸¥å¯†åˆ†æã€‚ç ”ç©¶æ­ç¤ºäº†æ³›åŒ–è¯¯å·®éšè®¡ç®—èµ„æº$C$å¢åŠ è€Œå˜åŒ–çš„é˜¶æ®µæ€§ç‰¹å¾ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå…·æœ‰æ˜æ˜¾ç›¸ä½åˆ‡æ¢çš„è¶…é¢é£é™©(excess risk)ç†è®ºä¸Šé™ã€‚åœ¨ä¼˜åŒ–åˆæœŸï¼Œè¶…é¢é£é™©ç›¸å¯¹äºè®¡ç®—æˆæœ¬$C$å‘ˆæŒ‡æ•°çº§ä¸‹é™ï¼›è€Œå½“è·¨è¶Šç‰¹å®šèµ„æºåˆ†é…é˜ˆå€¼åï¼Œç³»ç»Ÿè¿›å…¥ç»Ÿè®¡é˜¶æ®µï¼Œæ³›åŒ–è¯¯å·®éµå¾ª$Î˜(C^{-1/6})$çš„å¹‚å¾‹è¡°å‡ã€‚æ­¤å¤–ï¼Œè¯¥ç»Ÿä¸€æ¡†æ¶è¿˜æ¨å¯¼å‡ºäº†å…³äºæ¨¡å‹è§„æ¨¡ã€è®­ç»ƒæ—¶é—´å’Œæ•°æ®é›†å¤§å°çš„ç‹¬ç«‹Scaling Lawï¼Œä»ç†è®ºä¸Šé˜æ˜äº†å„å˜é‡å¦‚ä½•ç‹¬ç«‹å½±å“æ³›åŒ–çš„ä¸Šç•Œã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22088v1",
      "published_date": "2025-12-26 17:20:09 UTC",
      "updated_date": "2025-12-26 17:20:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:03:56.607059+00:00"
    },
    {
      "arxiv_id": "2512.22331v1",
      "title": "The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma",
      "title_zh": "MRI å½±åƒç»„å­¦ä¸­çš„å¤šè§†å›¾èŒƒå¼è½¬å˜ï¼šé¢„æµ‹èƒ¶è´¨æ¯ç»†èƒç˜¤ä¸­çš„ MGMT ç”²åŸºåŒ–",
      "authors": [
        "Mariya Miteva",
        "Maria Nisheva-Pavlova"
      ],
      "abstract": "Non-invasive inference of molecular tumor characteristics from medical imaging is a central goal of radiogenomics, particularly in glioblastoma (GBM), where O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation carries important prognostic and therapeutic significance. Although radiomics-based machine learning methods have shown promise for this task, conventional unimodal and early-fusion approaches are often limited by high feature redundancy and an incomplete modeling of modality-specific information. In this work, we introduce a multi-view latent representation learning framework based on variational autoencoders (VAE) to integrate complementary radiomic features derived from post-contrast T1-weighted (T1Gd) and Fluid-Attenuated Inversion Recovery (FLAIR) magnetic resonance imaging (MRI). By encoding each modality through an independent probabilistic encoder and performing fusion in a compact latent space, the proposed approach preserves modality-specific structure while enabling effective multimodal integration. The resulting latent embeddings are subsequently used for MGMT promoter methylation classification.",
      "tldr_zh": "è¯¥ç ”ç©¶èšç„¦äºèƒ¶è´¨æ¯ç»†èƒç˜¤(Glioblastoma, GBM)ä¸­O6-ç”²åŸºé¸Ÿå˜Œå‘¤-DNAç”²åŸºè½¬ç§»é…¶(MGMT)å¯åŠ¨å­ç”²åŸºåŒ–çš„æ— åˆ›é¢„æµ‹ï¼Œè¿™ä¸€æŒ‡æ ‡åœ¨ä¸´åºŠé¢„åå’Œæ²»ç–—æ–¹æ¡ˆåˆ¶å®šä¸­å…·æœ‰å…³é”®æ„ä¹‰ã€‚ä¼ ç»Ÿçš„å•æ¨¡æ€æˆ–æ—©æœŸèåˆå½±åƒç»„å­¦(radiomics)æ–¹æ³•é€šå¸¸é¢ä¸´ç‰¹å¾å†—ä½™åº¦é«˜ä¸”éš¾ä»¥å……åˆ†æ•è·æ¨¡æ€ç‰¹å¼‚æ€§ä¿¡æ¯çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºå˜åˆ†è‡ªç¼–ç å™¨(Variational Autoencoders, VAE)çš„å¤šè§†å›¾æ½œåœ¨è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é›†æˆå¯¹æ¯”åT1åŠ æƒ(T1Gd)å’Œæ¶²ä½“è¡°å‡åè½¬æ¢å¤(FLAIR)ç£å…±æŒ¯æˆåƒ(MRI)ä¸­çš„äº’è¡¥ç‰¹å¾ã€‚è¯¥æ¡†æ¶é€šè¿‡ç‹¬ç«‹çš„æ¦‚ç‡ç¼–ç å™¨å¯¹å„æ¨¡æ€è¿›è¡Œç¼–ç ï¼Œå¹¶åœ¨ç´§å‡‘çš„æ½œåœ¨ç©ºé—´å†…è¿›è¡Œèåˆï¼Œä»è€Œåœ¨å®ç°é«˜æ•ˆå¤šæ¨¡æ€é›†æˆçš„åŒæ—¶ä¿ç•™äº†æ¨¡æ€ç‰¹æœ‰çš„ç»“æ„ç‰¹å¾ã€‚æœ€ç»ˆç”Ÿæˆçš„æ½œåœ¨åµŒå…¥(latent embeddings)è¢«åº”ç”¨äºMGMTå¯åŠ¨å­ç”²åŸºåŒ–çš„åˆ†ç±»ä»»åŠ¡ï¼Œå±•ç¤ºäº†å¤šè§†å›¾èŒƒå¼åœ¨æ”¾å°„åŸºå› ç»„å­¦(radiogenomics)é¢„æµ‹ä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22331v1",
      "published_date": "2025-12-26 16:32:19 UTC",
      "updated_date": "2025-12-26 16:32:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:03:40.447142+00:00"
    },
    {
      "arxiv_id": "2512.22065v1",
      "title": "StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars",
      "title_zh": "StreamAvatarï¼šé¢å‘å®æ—¶äº¤äº’å¼äººä½“åŒ–èº«çš„æµå¼æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Zhiyao Sun",
        "Ziqiao Peng",
        "Yifeng Ma",
        "Yi Chen",
        "Zhengguang Zhou",
        "Zixiang Zhou",
        "Guozhen Zhang",
        "Youliang Zhang",
        "Yuan Zhou",
        "Qinglin Lu",
        "Yong-Jin Liu"
      ],
      "abstract": "Real-time, streaming interactive avatars represent a critical yet challenging goal in digital human research. Although diffusion-based human avatar generation methods achieve remarkable success, their non-causal architecture and high computational costs make them unsuitable for streaming. Moreover, existing interactive approaches are typically limited to head-and-shoulder region, limiting their ability to produce gestures and body motions. To address these challenges, we propose a two-stage autoregressive adaptation and acceleration framework that applies autoregressive distillation and adversarial refinement to adapt a high-fidelity human video diffusion model for real-time, interactive streaming. To ensure long-term stability and consistency, we introduce three key components: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. Building on this framework, we develop a one-shot, interactive, human avatar model capable of generating both natural talking and listening behaviors with coherent gestures. Extensive experiments demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in generation quality, real-time efficiency, and interaction naturalness. Project page: https://streamavatar.github.io .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†StreamAvatarï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å®ç°å®æ—¶äº¤äº’å¼æ•°å­—äººåŒ–èº«(Human Avatars)çš„æµå¼æ‰©æ•£æ¨¡å‹æ¡†æ¶ã€‚é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨æµå¼ä¼ è¾“ä¸­é¢ä¸´çš„éå› æœæ¶æ„å’Œé«˜è®¡ç®—æˆæœ¬æŒ‘æˆ˜ï¼Œä»¥åŠç°æœ‰æ–¹æ³•å¤šå±€é™äºå¤´è‚©åŒºåŸŸçš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è‡ªå›å½’é€‚é…ä¸åŠ é€Ÿæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è‡ªå›å½’è’¸é¦(Autoregressive Distillation)å’Œå¯¹æŠ—æ€§ç»†åŒ–(Adversarial Refinement)æŠ€æœ¯ï¼Œå¹¶å¼•å…¥äº†Reference Sinkã€å‚è€ƒé”šå®šä½ç½®é‡ç¼–ç (RAPR)ç­–ç•¥ä»¥åŠä¸€è‡´æ€§æ„ŸçŸ¥åˆ¤åˆ«å™¨(Consistency-Aware Discriminator)ä»¥ç¡®ä¿è§†é¢‘ç”Ÿæˆçš„é•¿æœŸç¨³å®šæ€§å’Œä¸€è‡´æ€§ã€‚ä½œä¸ºä¸€ç§å•æ¬¡å­¦ä¹ (One-shot)çš„äº¤äº’æ¨¡å‹ï¼ŒStreamAvatarèƒ½å¤Ÿç”ŸæˆåŒ…å«è¿è´¯èº«ä½“æ‰‹åŠ¿çš„è‡ªç„¶è¯´è¯ä¸å€¾å¬è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡ã€å®æ—¶æ•ˆç‡å’Œäº¤äº’è‡ªç„¶åº¦æ–¹é¢å‡è¾¾åˆ°äº†å½“å‰é¢†å…ˆæ°´å¹³(State-of-the-art)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://streamavatar.github.io",
      "pdf_url": "https://arxiv.org/pdf/2512.22065v1",
      "published_date": "2025-12-26 15:41:24 UTC",
      "updated_date": "2025-12-26 15:41:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:04:41.011747+00:00"
    },
    {
      "arxiv_id": "2512.22326v2",
      "title": "Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers",
      "title_zh": "æ¯”ç‰¹å¸é¢„æµ‹ä¸“å®¶ç³»ç»Ÿï¼šé€šè¿‡ TimeXer Transformers æ•´åˆå…¨çƒæµåŠ¨æ€§",
      "authors": [
        "Sravan Karthick T"
      ],
      "abstract": "Bitcoin price forecasting is characterized by extreme volatility and non-stationarity, often defying traditional univariate time-series models over long horizons. This paper addresses a critical gap by integrating Global M2 Liquidity, aggregated from 18 major economies, as a leading exogenous variable with a 12-week lag structure. Using the TimeXer architecture, we compare a liquidity-conditioned forecasting model (TimeXer-Exog) against state-of-the-art benchmarks including LSTM, N-BEATS, PatchTST, and a standard univariate TimeXer. Experiments conducted on daily Bitcoin price data from January 2020 to August 2025 demonstrate that explicit macroeconomic conditioning significantly stabilizes long-horizon forecasts. At a 70-day forecast horizon, the proposed TimeXer-Exog model achieves a mean squared error (MSE) 1.08e8, outperforming the univariate TimeXer baseline by over 89 percent. These results highlight that conditioning deep learning models on global liquidity provides substantial improvements in long-horizon Bitcoin price forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Bitcoin ä»·æ ¼é¢„æµ‹ä¸­çš„æç«¯æ³¢åŠ¨æ€§å’Œéå¹³ç¨³æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªé›†æˆå…¨çƒæµåŠ¨æ€§çš„ä¸“å®¶ç³»ç»Ÿã€‚ç ”ç©¶åˆ›æ–°æ€§åœ°æ•´åˆäº†æ¥è‡ª 18 ä¸ªä¸»è¦ç»æµä½“çš„ Global M2 Liquidityï¼Œå¹¶å°†å…¶ä½œä¸ºå…·æœ‰ 12 å‘¨æ»åç»“æ„çš„é¢†å…ˆå¤–ç”Ÿå˜é‡å¼•å…¥é¢„æµ‹æ¨¡å‹ã€‚æ ¸å¿ƒæ–¹æ³•åŸºäº TimeXer æ¶æ„ï¼Œå¼€å‘äº†å—æµåŠ¨æ€§è°ƒèŠ‚çš„ TimeXer-Exog æ¨¡å‹ï¼Œå¹¶ä¸ LSTMã€N-BEATSã€PatchTST ç­‰å…ˆè¿›åŸºå‡†è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ˜¾å¼çš„å®è§‚ç»æµè°ƒèŠ‚æ˜¾è‘—å¢å¼ºäº†é•¿å‘¨æœŸé¢„æµ‹çš„ç¨³å®šæ€§ã€‚åœ¨ 70 å¤©çš„é¢„æµ‹è§†é‡ä¸‹ï¼ŒTimeXer-Exog æ¨¡å‹å®ç°çš„å‡æ–¹è¯¯å·® (MSE) æ¯”å•å˜é‡ TimeXer åŸºå‡†é™ä½äº† 89% ä»¥ä¸Šã€‚è¿™ä¸€å‘ç°è¯æ˜äº†å°†æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸å…¨çƒæµåŠ¨æ€§æŒ‡æ ‡ç›¸ç»“åˆï¼Œèƒ½å¤Ÿå¤§å¹…æå‡ Bitcoin é•¿æœŸä»·æ ¼é¢„æµ‹çš„æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22326v2",
      "published_date": "2025-12-26 15:36:04 UTC",
      "updated_date": "2026-01-11 18:28:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:04:14.152938+00:00"
    },
    {
      "arxiv_id": "2512.22323v1",
      "title": "SpotEdit: Selective Region Editing in Diffusion Transformers",
      "title_zh": "SpotEditï¼šæ‰©æ•£ Transformer ä¸­çš„é€‰æ‹©æ€§åŒºåŸŸç¼–è¾‘",
      "authors": [
        "Zhibin Qin",
        "Zhenxiong Tan",
        "Zeqing Wang",
        "Songhua Liu",
        "Xinchao Wang"
      ],
      "abstract": "Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SpotEditï¼Œä¸€ç§é’ˆå¯¹æ‰©æ•£å˜æ¢å™¨(Diffusion Transformers)çš„æ— éœ€è®­ç»ƒ(training-free)çš„é€‰æ‹©æ€§åŒºåŸŸå›¾åƒç¼–è¾‘æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨ç¼–è¾‘å±€éƒ¨åŒºåŸŸæ—¶ä»éœ€å…¨å±€å¤„ç†æ‰€æœ‰æ ‡è®°(tokens)å¯¼è‡´çš„å†—ä½™è®¡ç®—å’Œéä¿®æ”¹åŒºåŸŸè´¨é‡ä¸‹é™é—®é¢˜ï¼ŒSpotEdité€šè¿‡SpotSelectorå’ŒSpotFusionä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶å®ç°å±€éƒ¨æ›´æ–°ã€‚SpotSelectoråˆ©ç”¨æ„ŸçŸ¥ç›¸ä¼¼åº¦(perceptual similarity)è¯†åˆ«å›¾åƒä¸­çš„ç¨³å®šåŒºåŸŸï¼Œå¹¶é€šè¿‡é‡ç”¨æ¡ä»¶å›¾åƒç‰¹å¾æ¥è·³è¿‡ç›¸å…³è®¡ç®—ã€‚SpotFusionåˆ™é‡‡ç”¨åŠ¨æ€èåˆæœºåˆ¶(dynamic fusion mechanism)å°†é‡ç”¨çš„ç‰¹å¾ä¸ç¼–è¾‘åçš„æ ‡è®°è‡ªé€‚åº”æ··åˆï¼Œä»è€Œåœ¨ä¿æŒç¼–è¾‘è´¨é‡çš„åŒæ—¶ç¡®ä¿ä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æ˜¾è‘—å‡å°‘äº†ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ï¼Œè¿˜æå¤§æå‡äº†æœªä¿®æ”¹åŒºåŸŸçš„ä¿çœŸåº¦ï¼Œä¸ºå®ç°é«˜æ•ˆä¸”ç²¾ç¡®çš„å›¾åƒç¼–è¾‘æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22323v1",
      "published_date": "2025-12-26 14:59:41 UTC",
      "updated_date": "2025-12-26 14:59:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:04:37.514915+00:00"
    },
    {
      "arxiv_id": "2512.22322v2",
      "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
      "title_zh": "SmartSnapï¼šé¢å‘è‡ªéªŒè¯æ™ºèƒ½ä½“çš„ä¸»åŠ¨è¯æ®æœå¯»",
      "authors": [
        "Shaofei Cai",
        "Yulei Qin",
        "Haojia Lin",
        "Zihan Xu",
        "Gang Li",
        "Yuchen Shi",
        "Zongyi Li",
        "Yong Mao",
        "Siqi Cai",
        "Xiaoyu Tan",
        "Yitao Liang",
        "Ke Li",
        "Xing Sun"
      ],
      "abstract": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B. Code is available at: https://github.com/TencentYoutuResearch/SmartSnap",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤æ‚GUIä»»åŠ¡ä¸­æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (RL)éªŒè¯ç¯èŠ‚å­˜åœ¨çš„è¢«åŠ¨ã€é«˜æˆæœ¬ä¸”ä½å¯é æ€§é—®é¢˜ï¼Œæå‡ºäº†SmartSnapèŒƒå¼ã€‚è¿™ä¸€èŒƒå¼å®ç°äº†ä»ä¼ ç»Ÿäº‹åéªŒè¯å‘æ™ºèƒ½ä½“ä¸»åŠ¨ã€åŸä½è‡ªæˆ‘éªŒè¯çš„è½¬å˜ï¼Œå¹¶å¼•å…¥äº†å…·å¤‡åŒé‡ä½¿å‘½çš„è‡ªæˆ‘éªŒè¯æ™ºèƒ½ä½“(Self-Verifying Agent)ã€‚è¯¥æ™ºèƒ½ä½“éµå¾ªå®Œå¤‡æ€§(Completeness)ã€ç®€æ´æ€§(Conciseness)å’Œåˆ›é€ æ€§(Creativity)çš„3CåŸåˆ™ï¼Œåˆ©ç”¨ç¯å¢ƒçš„å¯è®¿é—®æ€§ç”Ÿæˆä¸€ç»„æœ€å°ä¸”å…·æœ‰å†³å®šæ€§çš„å¿«ç…§è¯æ®ã€‚è¿™äº›å¿«ç…§ä½œä¸ºé€šç”¨LLM-as-a-JudgeéªŒè¯å™¨çš„å”¯ä¸€åˆ¤å®šä¾æ®ï¼Œæå¤§åœ°å‡å°‘äº†å†—ä½™ä¿¡æ¯çš„å¹²æ‰°ã€‚å®éªŒè¯æ˜ï¼ŒSmartSnapæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å¯æ‰©å±•æ€§ï¼Œä½¿8Bå’Œ30Bæ¨¡å‹çš„æ€§èƒ½åˆ†åˆ«æå‡äº†26.08%å’Œ16.66%ã€‚è¯¥æ–¹æ³•æˆåŠŸå®ç°äº†è§£å†³æ–¹æ¡ˆå¯»æ‰¾ä¸è¯æ®å¯»æ±‚çš„ååŒï¼Œä½¿è®­ç»ƒå‡ºçš„æ™ºèƒ½ä½“åœ¨æ€§èƒ½ä¸Šèƒ½ä¸DeepSeek V3.1å’ŒQwen3-235B-A22Bç­‰å¤§è§„æ¨¡æ¨¡å‹ç«äº‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22322v2",
      "published_date": "2025-12-26 14:51:39 UTC",
      "updated_date": "2026-01-06 03:31:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:04:35.919963+00:00"
    },
    {
      "arxiv_id": "2601.11567v1",
      "title": "Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology",
      "title_zh": "è¡¡é‡å„¿ç§‘å†…åˆ†æ³Œå­¦å°å‹å¼€æºåŒ»ç–—å¤§è¯­è¨€æ¨¡å‹åœ¨å‡†ç¡®æ€§ä¹‹å¤–çš„ç¨³å®šæ€§",
      "authors": [
        "Vanessa D'Amario",
        "Randy Daniel",
        "Alessandro Zanetti",
        "Dhruv Edamadaka",
        "Nitya Alaparthy",
        "Joshua Tarkoff"
      ],
      "abstract": "Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024), Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B (Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In deterministic settings, we examine the effect of prompt variation on models' output and self-assessment bias. In stochastic settings, we evaluate output variability and investigate the relationship between consistency and correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show that high consistency across the model response is not an indicator of correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment bias and dependency on the order of the candidate explanations. Expert review of incorrect reasoning rationales identified a mix of clinically acceptable responses and clinical oversight. We further show that system-level perturbations, such as differences in CUDA builds, can yield statistically significant shifts in model output despite stable accuracy. This work demonstrates that small, semantically negligible prompt perturbations lead to divergent outputs, raising concerns about reproducibility of LLM-based evaluations and highlights the output variability under different stochastic regimes, emphasizing the need of a broader diagnostic framework to understand potential pitfalls in real-world clinical decision support scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å…­ç§å°å‹å¼€æºåŒ»å­¦å¤§è¯­è¨€æ¨¡å‹(Small open-source medical LLMs)åœ¨å„¿ç§‘å†…åˆ†æ³Œå­¦(Pediatric endocrinology)ä¸­çš„è¡¨ç°ï¼Œæ—¨åœ¨æ¢ç´¢å‡†ç¡®ç‡(Accuracy)ä¹‹å¤–çš„ä¸€è‡´æ€§ã€é²æ£’æ€§åŠæ¨ç†è¡Œä¸ºã€‚é€šè¿‡ç»“åˆå¤šé€‰é¢˜(MCQ)ã€äººå·¥è¯„ä¼°å’Œä¸´åºŠå®¡æŸ¥ï¼Œç ”ç©¶å‘ç°HuatuoGPT-o1-8Båœ¨æ€§èƒ½å’Œä¸€è‡´æ€§(Consistency)æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä½†é«˜ä¸€è‡´æ€§å¹¶ä¸ç›´æ¥ä»£è¡¨ç»“æœçš„æ­£ç¡®æ€§ã€‚å®éªŒæ­ç¤ºäº†æ¨¡å‹åœ¨æ¨ç†é€‰æ‹©ä¸­å­˜åœ¨è‡ªæˆ‘è¯„ä¼°åå·®(Self-assessment bias)ï¼Œä¸”å¯¹å€™é€‰è§£é‡Šçš„é¡ºåºå…·æœ‰ä¾èµ–æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜äº†æå°çš„æç¤ºè¯æ‰°åŠ¨(Prompt perturbations)ç”šè‡³CUDAç‰ˆæœ¬å·®å¼‚ç­‰ç³»ç»Ÿçº§å› ç´ éƒ½ä¼šå¯¼è‡´è¾“å‡ºç»“æœäº§ç”Ÿç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—åˆ†æ­§ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†åœ¨çœŸå®ä¸´åºŠå†³ç­–æ”¯æŒ(Clinical decision support)åœºæ™¯ä¸­ï¼Œå»ºç«‹è¶…è¶Šå•ä¸€å‡†ç¡®ç‡æŒ‡æ ‡çš„å¹¿æ³›è¯Šæ–­æ¡†æ¶å¯¹äºè¯†åˆ«æ½œåœ¨é£é™©å’Œç¡®ä¿è¯„ä¼°å¯é‡å¤æ€§çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 11 figures, accepted at 47 workshop Reproducible Artificial Intelligence (AAAI 2026, Singapore, January 27, 2026)",
      "pdf_url": "https://arxiv.org/pdf/2601.11567v1",
      "published_date": "2025-12-26 14:30:53 UTC",
      "updated_date": "2025-12-26 14:30:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:04:30.010745+00:00"
    },
    {
      "arxiv_id": "2512.22031v1",
      "title": "From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation",
      "title_zh": "ä»è®¡ç®—æœºæ¨¡æ‹Ÿåˆ°ä½“å¤–å®éªŒï¼šè¯„ä¼°ç”¨äºå‘½ä¸­åŒ–åˆç‰©ç”Ÿæˆçš„åˆ†å­ç”Ÿæˆæ¨¡å‹",
      "authors": [
        "Nagham Osman",
        "Vittorio Lembo",
        "Giovanni Bottegoni",
        "Laura Toni"
      ],
      "abstract": "Hit identification is a critical yet resource-intensive step in the drug discovery pipeline, traditionally relying on high-throughput screening of large compound libraries. Despite advancements in virtual screening, these methods remain time-consuming and costly. Recent progress in deep learning has enabled the development of generative models capable of learning complex molecular representations and generating novel compounds de novo. However, using ML to replace the entire drug-discovery pipeline is highly challenging. In this work, we rather investigate whether generative models can replace one step of the pipeline: hit-like molecule generation. To the best of our knowledge, this is the first study to explicitly frame hit-like molecule generation as a standalone task and empirically test whether generative models can directly support this stage of the drug discovery pipeline. Specifically, we investigate if such models can be trained to generate hit-like molecules, enabling direct incorporation into, or even substitution of, traditional hit identification workflows. We propose an evaluation framework tailored to this task, integrating physicochemical, structural, and bioactivity-related criteria within a multi-stage filtering pipeline that defines the hit-like chemical space. Two autoregressive and one diffusion-based generative models were benchmarked across various datasets and training settings, with outputs assessed using standard metrics and target-specific docking scores. Our results show that these models can generate valid, diverse, and biologically relevant compounds across multiple targets, with a few selected GSK-3$Î²$ hits synthesized and confirmed active in vitro. We also identify key limitations in current evaluation metrics and available training data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ†å­ç”Ÿæˆæ¨¡å‹åœ¨è¯ç‰©å‘ç°ä¸­æ›¿ä»£ä¼ ç»Ÿé«˜é€šé‡ç­›é€‰ç”¨äºå…ˆå¯¼åŒ–åˆç‰©å‘ç°(Hit generation)çš„å¯èƒ½æ€§ï¼Œå¹¶é¦–æ¬¡å°†ç”Ÿæˆç±»å…ˆå¯¼åŒ–åˆç‰©(Hit-like molecule generation)å®šä¹‰ä¸ºä¸€ä¸ªç‹¬ç«‹çš„ä»»åŠ¡ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªä¸“é—¨çš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆç‰©ç†åŒ–å­¦ã€ç»“æ„å’Œç”Ÿç‰©æ´»æ€§ç›¸å…³æ ‡å‡†çš„å¤šé˜¶æ®µè¿‡æ»¤æµç¨‹ï¼Œå®šä¹‰äº†ç±»å…ˆå¯¼åŒ–åˆç‰©çš„åŒ–å­¦ç©ºé—´ã€‚å®éªŒè¯„ä¼°äº†ä¸¤ä¸ªè‡ªå›å½’æ¨¡å‹(Autoregressive models)å’Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¨¡å‹(Diffusion-based model)åœ¨ä¸åŒæ•°æ®é›†å’Œè®­ç»ƒè®¾ç½®ä¸‹çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹å¤šä¸ªé¶ç‚¹ç”Ÿæˆæœ‰æ•ˆã€å¤šæ ·ä¸”å…·æœ‰ç”Ÿç‰©ç›¸å…³æ€§çš„åŒ–åˆç‰©ï¼Œå¹¶é€šè¿‡å¯¹æ¥è¯„åˆ†(Docking scores)è¿›è¡Œäº†éªŒè¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç ”ç©¶åˆæˆå¹¶å®éªŒéªŒè¯äº†å‡ ç§é’ˆå¯¹GSK-3$\\beta$çš„å…ˆå¯¼åŒ–åˆç‰©ï¼Œè¯å®å…¶åœ¨ä½“å¤–(In vitro)å…·æœ‰æ´»æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æŒ‡å‡ºäº†å½“å‰è¯„ä¼°æŒ‡æ ‡å’Œç°æœ‰è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨çš„å…³é”®å±€é™æ€§ï¼Œä¸ºæœªæ¥ç”Ÿæˆå¼æ¨¡å‹åœ¨è¯ç‰©ç ”å‘ä¸­çš„åº”ç”¨æä¾›äº†æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22031v1",
      "published_date": "2025-12-26 14:02:59 UTC",
      "updated_date": "2025-12-26 14:02:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:04:36.665080+00:00"
    },
    {
      "arxiv_id": "2512.22029v1",
      "title": "LibContinual: A Comprehensive Library towards Realistic Continual Learning",
      "title_zh": "LibContinualï¼šé¢å‘çœŸå®åœºæ™¯æŒç»­å­¦ä¹ çš„ç»¼åˆæ€§åº“",
      "authors": [
        "Wenbin Li",
        "Shangge Liu",
        "Borui Kang",
        "Yiyang Chen",
        "KaXuan Lew",
        "Yang Chen",
        "Yinghuan Shi",
        "Lei Wang",
        "Yang Gao",
        "Jiebo Luo"
      ],
      "abstract": "A fundamental challenge in Continual Learning (CL) is catastrophic forgetting, where adapting to new tasks degrades the performance on previous ones. While the field has evolved with diverse methods, this rapid surge in diverse methodologies has culminated in a fragmented research landscape. The lack of a unified framework, including inconsistent implementations, conflicting dependencies, and varying evaluation protocols, makes fair comparison and reproducible research increasingly difficult. To address this challenge, we propose LibContinual, a comprehensive and reproducible library designed to serve as a foundational platform for realistic CL. Built upon a high-cohesion, low-coupling modular architecture, LibContinual integrates 19 representative algorithms across five major methodological categories, providing a standardized execution environment. Meanwhile, leveraging this unified framework, we systematically identify and investigate three implicit assumptions prevalent in mainstream evaluation: (1) offline data accessibility, (2) unregulated memory resources, and (3) intra-task semantic homogeneity. We argue that these assumptions often overestimate the real-world applicability of CL methods. Through our comprehensive analysis using strict online CL settings, a novel unified memory budget protocol, and a proposed category-randomized setting, we reveal significant performance drops in many representative CL methods when subjected to these real-world constraints. Our study underscores the necessity of resource-aware and semantically robust CL strategies, and offers LibContinual as a foundational toolkit for future research in realistic continual learning. The source code is available from \\href{https://github.com/RL-VIG/LibContinual}{https://github.com/RL-VIG/LibContinual}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Continual Learningé¢†åŸŸå› å®ç°ä¸ä¸€è‡´å’Œè¯„ä¼°åè®®å·®å¼‚å¯¼è‡´çš„ç¢ç‰‡åŒ–é—®é¢˜ï¼Œæå‡ºäº†LibContinualï¼Œä¸€ä¸ªæ—¨åœ¨æ¨åŠ¨ç°å®åœºæ™¯ä¸‹æŒç»­å­¦ä¹ ç ”ç©¶çš„ç»¼åˆæ€§ã€å¯å¤ç°å‡½æ•°åº“ã€‚è¯¥åº“é‡‡ç”¨é«˜å†…èšã€ä½è€¦åˆçš„æ¨¡å—åŒ–æ¶æ„ï¼Œé›†æˆäº†è·¨è¶Šäº”å¤§ç±»åˆ«çš„19ç§ä»£è¡¨æ€§ç®—æ³•ï¼Œå¹¶æä¾›äº†æ ‡å‡†åŒ–çš„æ‰§è¡Œç¯å¢ƒã€‚åˆ©ç”¨è¿™ä¸€ç»Ÿä¸€æ¡†æ¶ï¼Œç ”ç©¶è€…ç³»ç»Ÿåœ°è¯†åˆ«å¹¶è°ƒæŸ¥äº†ä¸»æµè¯„ä¼°ä¸­æ™®éå­˜åœ¨çš„ç¦»çº¿æ•°æ®è®¿é—®ã€ä¸å—é™åˆ¶çš„å†…å­˜èµ„æºä»¥åŠä»»åŠ¡å†…è¯­ä¹‰åŒè´¨åŒ–è¿™ä¸‰ä¸ªéšå«å‡è®¾ã€‚é€šè¿‡åœ¨ä¸¥æ ¼çš„Online CLè®¾ç½®ã€ç»Ÿä¸€å†…å­˜é¢„ç®—åè®®åŠç±»åˆ«éšæœºåŒ–è®¾ç½®ä¸‹çš„å…¨é¢åˆ†æï¼Œå®éªŒæ­ç¤ºäº†è®¸å¤šä»£è¡¨æ€§ç®—æ³•åœ¨é¢å¯¹ç°å®çº¦æŸæ—¶æ€§èƒ½ä¼šå¤§å¹…ä¸‹é™ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘èµ„æºæ„ŸçŸ¥å’Œè¯­ä¹‰é²æ£’æ€§æŒç»­å­¦ä¹ ç­–ç•¥çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çœŸå®åœºæ™¯ä¸‹çš„Continual Learningç ”ç©¶æä¾›äº†é‡è¦çš„åŸºç¡€å·¥å…·åŒ…ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22029v1",
      "published_date": "2025-12-26 13:59:13 UTC",
      "updated_date": "2025-12-26 13:59:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:04:43.452264+00:00"
    },
    {
      "arxiv_id": "2601.09721v1",
      "title": "Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox",
      "title_zh": "å„¿ç§‘å’¨è¯¢ä¸­å¤§è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„è·¨å¹³å°è¯„ä¼°ï¼šå¯¹æŠ—é²æ£’æ€§çš„æ¼”è¿›ä¸è§„æ¨¡æ‚–è®º",
      "authors": [
        "Vahideh Zolfaghari"
      ],
      "abstract": "Background Large language models (LLMs) are increasingly deployed in medical consultations, yet their safety under realistic user pressures remains understudied. Prior assessments focused on neutral conditions, overlooking vulnerabilities from anxious users challenging safeguards. This study evaluated LLM safety under parental anxiety-driven adversarial pressures in pediatric consultations across models and platforms. Methods PediatricAnxietyBench, from a prior evaluation, includes 300 queries (150 authentic, 150 adversarial) spanning 10 topics. Three models were assessed via APIs: Llama-3.3-70B and Llama-3.1-8B (Groq), Mistral-7B (HuggingFace), yielding 900 responses. Safety used a 0-15 scale for restraint, referral, hedging, emergency recognition, and non-prescriptive behavior. Analyses employed paired t-tests with bootstrapped CIs. Results Mean scores: 9.70 (Llama-3.3-70B) to 10.39 (Mistral-7B). Llama-3.1-8B outperformed Llama-3.3-70B by +0.66 (p=0.0001, d=0.225). Models showed positive adversarial effects, Mistral-7B strongest (+1.09, p=0.0002). Safety generalized across platforms; Llama-3.3-70B had 8% failures. Seizures vulnerable (33% inappropriate diagnoses). Hedging predicted safety (r=0.68, p<0.001). Conclusions Evaluation shows safety depends on alignment and architecture over scale, with smaller models outperforming larger. Evolution to robustness across releases suggests targeted training progress. Vulnerabilities and no emergency recognition indicate unsuitability for triage. Findings guide selection, stress adversarial testing, and provide open benchmark for medical AI safety.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å„¿ç§‘åŒ»ç–—å’¨è¯¢ä¸­é¢å¯¹ç”±çˆ¶æ¯ç„¦è™‘é©±åŠ¨çš„å¯¹æŠ—æ€§å‹åŠ›(Adversarial Pressures)æ—¶çš„å®‰å…¨æ€§ã€‚ç ”ç©¶åˆ©ç”¨ PediatricAnxietyBench åŸºå‡†ï¼Œé€šè¿‡ API è·¨å¹³å°è¯„ä¼°äº† Llama-3.3-70Bã€Llama-3.1-8B å’Œ Mistral-7B çš„è¡¨ç°ï¼Œé‡ç‚¹è€ƒå¯Ÿäº†å…¶åœ¨ç´§æ€¥è¯†åˆ«å’Œéå¤„æ–¹è¡Œä¸ºç­‰æ–¹é¢çš„å®‰å…¨æ€§æŒ‡æ ‡ã€‚å®éªŒç»“æœæ­ç¤ºäº†â€œè§„æ¨¡æ‚–è®ºâ€(Scale Paradox)ï¼Œå³è¾ƒå°è§„æ¨¡çš„ Llama-3.1-8B åœ¨å®‰å…¨å¾—åˆ†ä¸Šä¼˜äºè¾ƒå¤§è§„æ¨¡çš„ Llama-3.3-70Bã€‚è¿™è¡¨æ˜ LLM çš„å®‰å…¨æ€§æ›´å¤šå–å†³äºæ¨¡å‹å¯¹é½(alignment)å’Œæ¶æ„ï¼Œè€Œéå•çº¯çš„å‚æ•°è§„æ¨¡ã€‚å°½ç®¡æ¨¡å‹åœ¨è¿›åŒ–è¿‡ç¨‹ä¸­è¡¨ç°å‡ºä¸€å®šçš„å¯¹æŠ—é²æ£’æ€§(Adversarial Robustness)ï¼Œä½†åœ¨ç™«ç—«(seizures)è¯†åˆ«ç­‰ç´§æ€¥åŒ»ç–—åœºæ™¯ä¸­ä»å­˜åœ¨æ˜¾è‘—æ¼æ´ã€‚ç ”ç©¶æœ€ç»ˆè®¤ä¸ºå½“å‰çš„ LLMs å°šä¸å…·å¤‡åˆ†è¯Š(triage)èƒ½åŠ›ï¼Œå…¶å‘ç°ä¸ºåŒ»ç–— AI çš„å®‰å…¨è¯„ä¼°æä¾›äº†å¼€æºåŸºå‡†å’ŒæŒ‡å¯¼å»ºè®®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.09721v1",
      "published_date": "2025-12-26 13:47:42 UTC",
      "updated_date": "2025-12-26 13:47:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:04:50.786036+00:00"
    },
    {
      "arxiv_id": "2512.22022v1",
      "title": "Meta-Learning-Based Handover Management in NextG O-RAN",
      "title_zh": "NextG O-RAN ä¸­åŸºäºå…ƒå­¦ä¹ çš„åˆ‡æ¢ç®¡ç†",
      "authors": [
        "Michail Kalntis",
        "George Iosifidis",
        "JosÃ© SuÃ¡rez-Varela",
        "Andra Lutu",
        "Fernando A. Kuipers"
      ],
      "abstract": "While traditional handovers (THOs) have served as a backbone for mobile connectivity, they increasingly suffer from failures and delays, especially in dense deployments and high-frequency bands. To address these limitations, 3GPP introduced Conditional Handovers (CHOs) that enable proactive cell reservations and user-driven execution. However, both handover (HO) types present intricate trade-offs in signaling, resource usage, and reliability. This paper presents unique, countrywide mobility management datasets from a top-tier mobile network operator (MNO) that offer fresh insights into these issues and call for adaptive and robust HO control in next-generation networks. Motivated by these findings, we propose CONTRA, a framework that, for the first time, jointly optimizes THOs and CHOs within the O-RAN architecture. We study two variants of CONTRA: one where users are a priori assigned to one of the HO types, reflecting distinct service or user-specific requirements, as well as a more dynamic formulation where the controller decides on-the-fly the HO type, based on system conditions and needs. To this end, it relies on a practical meta-learning algorithm that adapts to runtime observations and guarantees performance comparable to an oracle with perfect future information (universal no-regret). CONTRA is specifically designed for near-real-time deployment as an O-RAN xApp and aligns with the 6G goals of flexible and intelligent control. Extensive evaluations leveraging crowdsourced datasets show that CONTRA improves user throughput and reduces both THO and CHO switching costs, outperforming 3GPP-compliant and Reinforcement Learning (RL) baselines in dynamic and real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿåˆ‡æ¢(Traditional Handovers, THOs)åœ¨å¯†é›†éƒ¨ç½²å’Œé«˜é¢‘æ®µç¯å¢ƒä¸‹å­˜åœ¨çš„å¤±è´¥ä¸å»¶è¿Ÿé—®é¢˜ï¼Œä»¥åŠæ¡ä»¶åˆ‡æ¢(Conditional Handovers, CHOs)å¸¦æ¥çš„ä¿¡ä»¤ä¸èµ„æºæƒè¡¡ï¼Œåˆ©ç”¨é¡¶çº§è¿è¥å•†çš„çœŸå®æ•°æ®é›†æå‡ºäº†åä¸º CONTRA çš„æ–°å‹æ¡†æ¶ã€‚CONTRA é¦–æ¬¡å®ç°äº†åœ¨ O-RAN æ¶æ„ä¸‹å¯¹ THOs å’Œ CHOs çš„è”åˆä¼˜åŒ–ï¼Œå¹¶é€šè¿‡ä¸€ç§å®ç”¨çš„ Meta-Learning ç®—æ³•å®ç°è¿è¡Œæ—¶çš„å¿«é€Ÿè‡ªé€‚åº”ï¼Œç¡®ä¿å…¶æ€§èƒ½æ¥è¿‘ç†æƒ³çš„é¢„çŸ¥çŠ¶æ€ã€‚è¯¥æ¡†æ¶æ”¯æŒé¢„åˆ†é…æˆ–æ ¹æ®ç³»ç»ŸçŠ¶æ€åŠ¨æ€å†³ç­–åˆ‡æ¢ç±»å‹ï¼Œå¹¶è¢«è®¾è®¡ä¸ºå¯éƒ¨ç½²çš„ O-RAN xAppï¼Œä»¥æ»¡è¶³ 6G ç½‘ç»œå¯¹çµæ´»æ™ºèƒ½æ§åˆ¶çš„éœ€æ±‚ã€‚å®éªŒè¯„ä¼°è¯æ˜ï¼ŒCONTRA åœ¨æå‡ç”¨æˆ·ååé‡çš„åŒæ—¶ï¼Œæœ‰æ•ˆé™ä½äº† THOs å’Œ CHOs çš„åˆ‡æ¢æˆæœ¬ã€‚åœ¨çœŸå®ä¸”åŠ¨æ€çš„ç½‘ç»œåœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ¡ˆçš„è¡¨ç°ä¼˜äº 3GPP æ ‡å‡†åŸºçº¿ä»¥åŠä¼ ç»Ÿçš„ Reinforcement Learning (RL) æ–¹æ³•ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22022v1",
      "published_date": "2025-12-26 13:01:46 UTC",
      "updated_date": "2025-12-26 13:01:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:05:19.772580+00:00"
    },
    {
      "arxiv_id": "2601.09720v1",
      "title": "Uncertainty-Aware Dynamic Knowledge Graphs for Reliable Question Answering",
      "title_zh": "é¢å‘å¯é é—®ç­”çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥åŠ¨æ€çŸ¥è¯†å›¾è°±",
      "authors": [
        "Yu Takahashi",
        "Shun Takeuchi",
        "Kexuan Xin",
        "Guillaume Pelat",
        "Yoshiaki Ikai",
        "Junya Saito",
        "Jonathan Vitale",
        "Shlomo Berkovsky",
        "Amin Beheshti"
      ],
      "abstract": "Question answering (QA) systems are increasingly deployed across domains. However, their reliability is undermined when retrieved evidence is incomplete, noisy, or uncertain. Existing knowledge graph (KG) based QA frameworks typically represent facts as static and deterministic, failing to capture the evolving nature of information and the uncertainty inherent in reasoning. We present a demonstration of uncertainty-aware dynamic KGs, a framework that combines (i) dynamic construction of evolving KGs, (ii) confidence scoring and uncertainty-aware retrieval, and (iii) an interactive interface for reliable and interpretable QA. Our system highlights how uncertainty modeling can make QA more robust and transparent by enabling users to explore dynamic graphs, inspect confidence-annotated triples, and compare baseline versus confidence-aware answers. The target users of this demo are clinical data scientists and clinicians, and we instantiate the framework in healthcare: constructing personalized KGs from electronic health records, visualizing uncertainty across patient visits, and evaluating its impact on a mortality prediction task. This use case demonstrates the broader promise of uncertainty-aware dynamic KGs for enhancing QA reliability in high-stakes applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çŸ¥è¯†å›¾è°±(Knowledge Graphs)åœ¨å¤„ç†ä¸å®Œæ•´ã€å™ªå£°åŠä¸ç¡®å®šä¿¡æ¯æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªä¸ç¡®å®šæ€§æ„ŸçŸ¥åŠ¨æ€çŸ¥è¯†å›¾è°±(Uncertainty-Aware Dynamic KGs)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†æ¼”åŒ–çŸ¥è¯†å›¾è°±çš„åŠ¨æ€æ„å»ºã€ç½®ä¿¡åº¦è¯„åˆ†(Confidence Scoring)ä¸ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ£€ç´¢ï¼Œä»¥åŠç”¨äºå¯é ä¸”å¯è§£é‡Šé—®ç­”(Question Answering)çš„äº¤äº’å¼ç•Œé¢ã€‚ç³»ç»Ÿé€šè¿‡å¯¹ä¸‰å…ƒç»„è¿›è¡Œç½®ä¿¡åº¦æ ‡æ³¨å¹¶å…è®¸ç”¨æˆ·æ¢ç´¢åŠ¨æ€å›¾è°±ï¼Œæ˜¾è‘—æå‡äº†é—®ç­”ç³»ç»Ÿçš„é²æ£’æ€§å’Œé€æ˜åº¦ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨åŒ»ç–—é¢†åŸŸå¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†å®ä¾‹åŒ–ï¼Œåˆ©ç”¨ç”µå­å¥åº·è®°å½•(Electronic Health Records)æ„å»ºä¸ªæ€§åŒ–çŸ¥è¯†å›¾è°±ï¼Œå¹¶å¯è§†åŒ–æ‚£è€…å°±è¯Šè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨æ­»äº¡ç‡é¢„æµ‹(Mortality Prediction)ç­‰é«˜é£é™©åº”ç”¨åœºæ™¯ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæå‡å¤æ‚ç¯å¢ƒä¸‹é—®ç­”ç³»ç»Ÿçš„å¯é æ€§æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages, 4 figures. Accepted at IEEE ICDM 2025 Demo Track",
      "pdf_url": "https://arxiv.org/pdf/2601.09720v1",
      "published_date": "2025-12-26 12:53:33 UTC",
      "updated_date": "2025-12-26 12:53:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:05:23.695369+00:00"
    },
    {
      "arxiv_id": "2601.00834v2",
      "title": "Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds",
      "title_zh": "å¤æ‚é»æ›¼æµå½¢ä¸Šååº”æ‰©æ•£åŠ¨åŠ›å­¦çš„å†…è•´åº¦é‡ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (IM-PINN)",
      "authors": [
        "Julian Evan Chrisnanto",
        "Salsabila Rahma Alia",
        "Nurfauzi Fadillah",
        "Yulison Herry Chrisnanto"
      ],
      "abstract": "Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a \"Stochastic Cloth\" manifold with extreme Gaussian curvature fluctuations ($K \\in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the \"splitting spot\" and \"labyrinthine\" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\\mathcal{E}_{mass} \\approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨å¤æ‚non-Euclidean manifoldsä¸Šæ¨¡æ‹Ÿéçº¿æ€§reaction-diffusion dynamicsé¢ä¸´çš„ç½‘æ ¼ç”Ÿæˆæˆæœ¬é«˜å’Œsymplectic driftç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†Intrinsic-Metric Physics-Informed Neural Network (IM-PINN)ã€‚è¿™æ˜¯ä¸€ç§mesh-freeå‡ ä½•æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å°†Riemannian metric tensoråµŒå…¥è‡ªåŠ¨å¾®åˆ†å›¾ï¼Œåœ¨è¿ç»­å‚æ•°åŸŸä¸­ç›´æ¥æ±‚è§£PDEså¹¶è§£æåœ°é‡æ„Laplace-Beltrami operatorã€‚è¯¥æ¶æ„é‡‡ç”¨å…·æœ‰Fourier feature embeddingsçš„åŒæµç»“æ„æ¥å‡è½»spectral biasï¼Œä»è€Œå®ç°äº†æ±‚è§£å¤æ‚æ€§ä¸å‡ ä½•ç¦»æ•£åŒ–çš„è§£è€¦ã€‚å®éªŒåœ¨å…·æœ‰æç«¯Gaussian curvature fluctuationsçš„â€œStochastic Clothâ€æµå½¢ä¸ŠéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼ŒæˆåŠŸæ¢å¤äº†Gray-Scott modelçš„â€œsplitting spotâ€å’Œâ€œlabyrinthineâ€æ¨¡å¼ã€‚åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒIM-PINNåœ¨å…¨å±€è´¨é‡å®ˆæ’è¯¯å·®($\\mathcal{E}_{mass}$)æ–¹é¢æ˜¾è‘—ä¼˜äºSurface Finite Element Method (SFEM)ï¼Œä½œä¸ºçƒ­åŠ›å­¦ä¸€è‡´çš„å…¨å±€æ±‚è§£å™¨æ¶ˆé™¤äº†è´¨é‡æ¼‚ç§»ã€‚è¯¥æ¡†æ¶ä¸ºæ¨¡æ‹Ÿè¿›åŒ–è¡¨é¢ä¸Šçš„ç”Ÿç‰©å›¾æ¡ˆå½¢æˆæä¾›äº†ä¸€ç§å†…å­˜é«˜æ•ˆä¸”åˆ†è¾¨ç‡æ— å…³çš„æ–°èŒƒå¼ï¼ŒæˆåŠŸæ¡¥æ¥äº†differential geometryä¸physics-informed machine learningã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00834v2",
      "published_date": "2025-12-26 12:41:05 UTC",
      "updated_date": "2026-01-07 03:43:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:06:19.200208+00:00"
    },
    {
      "arxiv_id": "2512.22010v1",
      "title": "LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration",
      "title_zh": "LongFlyï¼šèåˆæ—¶ç©ºä¸Šä¸‹æ–‡çš„é•¿æ—¶ç¨‹æ— äººæœºè§†è§‰è¯­è¨€å¯¼èˆª",
      "authors": [
        "Wen Jiang",
        "Li Wang",
        "Kangyao Huang",
        "Wei Fan",
        "Jinyuan Liu",
        "Shaoyu Liu",
        "Hongwei Duan",
        "Bin Xu",
        "Xiangyang Ji"
      ],
      "abstract": "Unmanned aerial vehicles (UAVs) are crucial tools for post-disaster search and rescue, facing challenges such as high information density, rapid changes in viewpoint, and dynamic structures, especially in long-horizon navigation. However, current UAV vision-and-language navigation(VLN) methods struggle to model long-horizon spatiotemporal context in complex environments, resulting in inaccurate semantic alignment and unstable path planning. To this end, we propose LongFly, a spatiotemporal context modeling framework for long-horizon UAV VLN. LongFly proposes a history-aware spatiotemporal modeling strategy that transforms fragmented and redundant historical data into structured, compact, and expressive representations. First, we propose the slot-based historical image compression module, which dynamically distills multi-view historical observations into fixed-length contextual representations. Then, the spatiotemporal trajectory encoding module is introduced to capture the temporal dynamics and spatial structure of UAV trajectories. Finally, to integrate existing spatiotemporal context with current observations, we design the prompt-guided multimodal integration module to support time-based reasoning and robust waypoint prediction. Experimental results demonstrate that LongFly outperforms state-of-the-art UAV VLN baselines by 7.89\\% in success rate and 6.33\\% in success weighted by path length, consistently across both seen and unseen environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœº(UAV)åœ¨ç¾åæœç´¢ä¸æ•‘æ´ç­‰é•¿æ—¶ç¨‹å¯¼èˆªä»»åŠ¡ä¸­é¢ä¸´çš„ä¿¡æ¯å¯†åº¦é«˜ã€è§†è§’å˜åŒ–å¿«ä»¥åŠç°æœ‰è§†è§‰è¯­è¨€å¯¼èˆª(VLN)æ–¹æ³•åœ¨æ—¶ç©ºå»ºæ¨¡æ–¹é¢çš„å±€é™æ€§æå‡ºäº†LongFlyæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§å†å²æ„ŸçŸ¥æ—¶ç©ºå»ºæ¨¡ç­–ç•¥ï¼Œå°†ç¢ç‰‡åŒ–ä¸”å†—ä½™çš„å†å²æ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–ä¸”ç´§å‡‘çš„è¡¨å¾ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶æå‡ºäº†åŸºäºæ’æ§½çš„å†å²å›¾åƒå‹ç¼©æ¨¡å—(slot-based historical image compression module)æ¥åŠ¨æ€æå–å¤šè§†å›¾è§‚æµ‹ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨æ—¶ç©ºè½¨è¿¹ç¼–ç æ¨¡å—(spatiotemporal trajectory encoding module)æ•æ‰æ— äººæœºçš„æ—¶é—´åŠ¨æ€ä¸ç©ºé—´ç»“æ„ã€‚æ­¤å¤–ï¼Œé€šè¿‡æç¤ºå¼•å¯¼çš„å¤šæ¨¡æ€é›†æˆæ¨¡å—(prompt-guided multimodal integration module)å®ç°äº†ä¸Šä¸‹æ–‡ä¸å½“å‰è§‚æµ‹çš„æ·±åº¦èåˆï¼Œä»¥æ”¯æŒç¨³å¥çš„èˆªè·¯ç‚¹é¢„æµ‹(waypoint prediction)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLongFlyåœ¨æˆåŠŸç‡(success rate)å’Œè·¯å¾„é•¿åº¦åŠ æƒæˆåŠŸç‡(SPL)ä¸Šåˆ†åˆ«ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ¨¡å‹7.89%å’Œ6.33%ï¼Œå¹¶åœ¨å„ç§å¤æ‚ç¯å¢ƒä¸­å±•ç°äº†æå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22010v1",
      "published_date": "2025-12-26 12:09:40 UTC",
      "updated_date": "2025-12-26 12:09:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:06:05.716784+00:00"
    },
    {
      "arxiv_id": "2512.22317v1",
      "title": "LangPrecip: Language-Aware Multimodal Precipitation Nowcasting",
      "title_zh": "LangPrecipï¼šè¯­è¨€æ„ŸçŸ¥çš„å¤šæ¨¡æ€é™æ°´ä¸´è¿‘é¢„æŠ¥",
      "authors": [
        "Xudong Ling",
        "Tianxi Huang",
        "Qian Dong",
        "Tao He",
        "Chaorong Li",
        "Guiduo Duan"
      ],
      "abstract": "Short-term precipitation nowcasting is an inherently uncertain and under-constrained spatiotemporal forecasting problem, especially for rapidly evolving and extreme weather events. Existing generative approaches rely primarily on visual conditioning, leaving future motion weakly constrained and ambiguous. We propose a language-aware multimodal nowcasting framework(LangPrecip) that treats meteorological text as a semantic motion constraint on precipitation evolution. By formulating nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm, our method enables efficient and physically consistent integration of textual and radar information in latent space.We further introduce LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. Experiments on Swedish and MRMS datasets show consistent improvements over state-of-the-art methods, achieving over 60 \\% and 19\\% gains in heavy-rainfall CSI at an 80-minute lead time.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LangPrecipï¼Œä¸€ç§è¯­è¨€æ„ŸçŸ¥å¤šæ¨¡æ€é™æ°´é¢„æŠ¥æ¡†æ¶(Language-Aware Multimodal Precipitation Nowcasting)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç”Ÿæˆå¼æ–¹æ³•å› ä»…ä¾èµ–è§†è§‰è°ƒèŠ‚è€Œå¯¼è‡´çš„æœªæ¥è¿åŠ¨çº¦æŸè–„å¼±å’Œé¢„æµ‹æ¨¡ç³Šé—®é¢˜ã€‚è¯¥æ–¹æ³•å°†æ°”è±¡æ–‡æœ¬è§†ä¸ºé™æ°´æ¼”å˜çš„è¯­ä¹‰è¿åŠ¨çº¦æŸ(Semantic Motion Constraint)ï¼Œå¹¶åœ¨Rectified FlowèŒƒå¼ä¸‹å°†é¢„æŠ¥å»ºæ¨¡ä¸ºè¯­ä¹‰çº¦æŸçš„è½¨è¿¹ç”Ÿæˆé—®é¢˜ã€‚é€šè¿‡åœ¨æ½œç©ºé—´(Latent Space)ä¸­é«˜æ•ˆæ•´åˆæ–‡æœ¬å’Œé›·è¾¾ä¿¡æ¯ï¼ŒLangPrecipå®ç°äº†ç‰©ç†ä¸€è‡´çš„é¢„æµ‹ç”Ÿæˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æ¨å‡ºäº†åŒ…å«16ä¸‡å¯¹é›·è¾¾åºåˆ—å’Œè¿åŠ¨æè¿°çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†LangPrecip-160kã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‘å…¸å’ŒMRMSæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼Œåœ¨80åˆ†é’Ÿæå‰æœŸçš„å¼ºé™æ°´å…³é”®æˆåŠŸæŒ‡æ•°(CSI)ä¸Šåˆ†åˆ«æå‡äº†è¶…è¿‡60%å’Œ19%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22317v1",
      "published_date": "2025-12-26 12:06:29 UTC",
      "updated_date": "2025-12-26 12:06:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:05:43.445939+00:00"
    },
    {
      "arxiv_id": "2512.22315v1",
      "title": "VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning",
      "title_zh": "VideoZoomerï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„é•¿è§†é¢‘æ¨ç†æ—¶åºèšç„¦",
      "authors": [
        "Yang Ding",
        "Yizhen Zhang",
        "Xin Lai",
        "Ruihang Chu",
        "Yujiu Yang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal Large Language Models, MLLMsï¼‰åœ¨é•¿è§†é¢‘ç†è§£ä¸­ç”±äºä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼Œå¯¼è‡´å‡åŒ€é‡‡æ ·æˆ–é™æ€é¢„é€‰å®¹æ˜“é—æ¼å…³é”®è¯æ®ä¸”æ— æ³•ä¿®æ­£åˆé€‰é”™è¯¯çš„é—®é¢˜ï¼Œæå‡ºäº†VideoZoomerä»£ç†æ¡†æ¶ã€‚VideoZoomer èµ‹äºˆæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€æ§åˆ¶è§†è§‰ç„¦ç‚¹çš„èƒ½åŠ›ï¼Œä»ç²—ç²’åº¦çš„ä½å¸§ç‡æ¦‚è§ˆå¼€å§‹ï¼Œé€šè¿‡å¤šè½®äº¤äº’è‡ªä¸»è°ƒç”¨æ—¶é—´ç¼©æ”¾å·¥å…·ï¼ˆtemporal zoomï¼‰è·å–ç‰¹å®šæ—¶åˆ»çš„é«˜å¸§ç‡ç‰‡æ®µï¼Œä»è€Œé€æ­¥æ”¶é›†ç»†ç²’åº¦è¯æ®ã€‚ç ”ç©¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬åœ¨ç²¾é€‰çš„ç¤ºä¾‹ä¸åæ€è½¨è¿¹æ•°æ®é›†ä¸Šè¿›è¡Œå†·å¯åŠ¨ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰ï¼Œä»¥åŠé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–ä»£ç†ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥7Bå‚æ•°æ¨¡å‹åœ¨å¤šé¡¹é•¿è§†é¢‘ç†è§£ä¸æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨æ˜¾è‘—é™ä½å¸§é¢„ç®—çš„åŒæ—¶æé«˜äº†æ•ˆç‡ã€‚å…¶æ¶Œç°å‡ºçš„å¤æ‚æ¨ç†èƒ½åŠ›ä½¿å…¶æŒç»­è¶…è¶Šç°æœ‰å¼€æºæ¨¡å‹ï¼Œå¹¶èƒ½åœ¨æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­ä¸é—­æºä¸“æœ‰ç³»ç»Ÿç›¸åª²ç¾ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22315v1",
      "published_date": "2025-12-26 11:43:21 UTC",
      "updated_date": "2025-12-26 11:43:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:06:19.577851+00:00"
    },
    {
      "arxiv_id": "2512.21985v1",
      "title": "LVLM-Aided Alignment of Task-Specific Vision Models",
      "title_zh": "LVLM è¾…åŠ©çš„ç‰¹å®šä»»åŠ¡è§†è§‰æ¨¡å‹å¯¹é½",
      "authors": [
        "Alexander Koebler",
        "Lukas Kuhn",
        "Ingo Thon",
        "Florian Buettner"
      ],
      "abstract": "In high-stakes domains, small task-specific vision models are crucial due to their low computational requirements and the availability of numerous methods to explain their results. However, these explanations often reveal that the models do not align well with human domain knowledge, relying instead on spurious correlations. This might result in brittle behavior once deployed in the real-world. To address this issue, we introduce a novel and efficient method for aligning small task-specific vision models with human domain knowledge by leveraging the generalization capabilities of a Large Vision Language Model (LVLM). Our LVLM-Aided Visual Alignment (LVLM-VA) method provides a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling effective interaction between domain experts and the model. Our method demonstrates substantial improvement in aligning model behavior with human specifications, as validated on both synthetic and real-world datasets. We show that it effectively reduces the model's dependence on spurious features and on group-specific biases, without requiring fine-grained feedback.",
      "tldr_zh": "é’ˆå¯¹é«˜é£é™©é¢†åŸŸä¸­å°å‹ç‰¹å®šä»»åŠ¡è§†è§‰æ¨¡å‹å®¹æ˜“ä¾èµ–ä¼ªç›¸å…³ (spurious correlations) ä¸”ä¸äººç±»é¢†åŸŸçŸ¥è¯†å¤±é…çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º LVLM-Aided Visual Alignment (LVLM-VA) çš„æ–°å‹é«˜æ•ˆæ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ (LVLM) çš„æ³›åŒ–èƒ½åŠ›æ¥å®ç°æ¨¡å‹å¯¹é½ã€‚è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªåŒå‘æ¥å£ï¼Œèƒ½å¤Ÿå°†æ¨¡å‹è¡Œä¸ºè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ï¼Œå¹¶å°†äººç±»çš„ç±»åˆ«çº§è§„èŒƒ (class-level specifications) æ˜ å°„ä¸ºå›¾åƒçº§è¯„è®º (image-level critiques)ï¼Œä»è€Œä¿ƒè¿›é¢†åŸŸä¸“å®¶ä¸æ¨¡å‹ä¹‹é—´çš„æœ‰æ•ˆäº’åŠ¨ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯ï¼ŒLVLM-VA æ˜¾è‘—æå‡äº†æ¨¡å‹è¡Œä¸ºä¸äººç±»è§„èŒƒçš„ä¸€è‡´æ€§ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€ç»†ç²’åº¦åé¦ˆçš„å‰æä¸‹ï¼Œèƒ½æœ‰æ•ˆé™ä½æ¨¡å‹å¯¹ä¼ªç‰¹å¾ (spurious features) å’Œç¾¤ä½“ç‰¹å®šåå·® (group-specific biases) çš„ä¾èµ–ï¼Œä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„è§†è§‰æ¨¡å‹æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.21985v1",
      "published_date": "2025-12-26 11:11:25 UTC",
      "updated_date": "2025-12-26 11:11:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:05:57.733270+00:00"
    },
    {
      "arxiv_id": "2601.06064v1",
      "title": "Socio-technical aspects of Agentic AI",
      "title_zh": "ä»£ç†å¼äººå·¥æ™ºèƒ½çš„ç¤¾ä¼š-æŠ€æœ¯ç»´åº¦",
      "authors": [
        "Praveen Kumar Donta",
        "Alaa Saleh",
        "Ying Li",
        "Shubham Vaishnav",
        "Kai Fang",
        "Hailin Feng",
        "Yuchao Xia",
        "Thippa Reddy Gadekallu",
        "Qiyang Zhang",
        "Xiaodan Shi",
        "Ali Beikmohammadi",
        "Sindri MagnÃºsson",
        "Ilir Murturi",
        "Chinmaya Kumar Dehury",
        "Marcin Paprzycki",
        "Lauri Loven",
        "Sasu Tarkoma",
        "Schahram Dustdar"
      ],
      "abstract": "Agentic Artificial Intelligence (AI) represents a fundamental shift in the design of intelligent systems, characterized by interconnected components that collectively enable autonomous perception, reasoning, planning, action, and learning. Recent research on agentic AI has largely focused on technical foundations, including system architectures, reasoning and planning mechanisms, coordination strategies, and application-level performance across domains. However, the societal, ethical, economic, environmental, and governance implications of agentic AI remain weakly integrated into these technical treatments. This paper addresses this gap by presenting a socio-technical analysis of agentic AI that explicitly connects core technical components with societal context. We examine how architectural choices in perception, cognition, planning, execution, and memory introduce dependencies related to data governance, accountability, transparency, safety, and sustainability. To structure this analysis, we adopt the MAD-BAD-SAD construct as an analytical lens, capturing motivations, applications, and moral dilemmas (MAD); biases, accountability, and dangers (BAD); and societal impact, adoption, and design considerations (SAD). Using this lens, we analyze ethical considerations, implications, and challenges arising from contemporary agentic AI systems and assess their manifestation across emerging applications, including healthcare, education, industry, smart and sustainable cities, social services, communications and networking, and earth observation and satellite communications. The paper further identifies open challenges and suggests future research directions, framing agentic AI as an integrated socio-technical system whose behavior and impact are co-produced by algorithms, data, organizational practices, regulatory frameworks, and social norms.",
      "tldr_zh": "è¯¥è®ºæ–‡æ¢è®¨äº†ä»£ç†å¼äººå·¥æ™ºèƒ½ (Agentic AI) çš„ç¤¾ä¼šæŠ€æœ¯ç»´åº¦ï¼ŒæŒ‡å‡ºå½“å‰ç ”ç©¶å¤šèšç„¦äºç³»ç»Ÿæ¶æ„ã€æ¨ç†è§„åˆ’ç­‰æŠ€æœ¯åŸºç¡€ï¼Œè€Œå¯¹å…¶ç¤¾ä¼šã€ä¼¦ç†ã€ç»æµå’Œæ²»ç†å½±å“çš„æ•´åˆå°šæ˜¾ä¸è¶³ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ç¤¾ä¼šæŠ€æœ¯åˆ†ææ¡†æ¶ï¼Œå°†æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶ä¸ç¤¾ä¼šèƒŒæ™¯æ˜¾å¼å…³è”ï¼Œå¹¶é‡‡ç”¨ MAD-BAD-SADï¼ˆåŠ¨æœºã€åº”ç”¨ä¸é“å¾·å›°å¢ƒï¼›åè§ã€é—®è´£ä¸å±é™©ï¼›ç¤¾ä¼šå½±å“ã€é‡‡ç”¨ä¸è®¾è®¡è€ƒé‡ï¼‰è¿™ä¸€åˆ†æè§†è§’è¿›è¡Œè§£æ„ã€‚è®ºæ–‡æ·±å…¥åˆ†æäº†æ„ŸçŸ¥ã€è®¤çŸ¥ã€è§„åˆ’ã€æ‰§è¡Œå’Œè®°å¿†ç­‰æ¶æ„é€‰æ‹©å¦‚ä½•å¼•å…¥æ•°æ®æ²»ç† (Data Governance)ã€é—®è´£åˆ¶ (Accountability)ã€é€æ˜åº¦ (Transparency) åŠå¯æŒç»­æ€§ (Sustainability) ç›¸å…³çš„ä¾èµ–å…³ç³»ã€‚è¯¥ç ”ç©¶è¿›ä¸€æ­¥è¯„ä¼°äº†è¿™äº›ä¼¦ç†è€ƒé‡åœ¨åŒ»ç–—ã€æ•™è‚²ã€å·¥ä¸šã€æ™ºæ…§åŸå¸‚åŠåœ°çƒè§‚æµ‹ç­‰æ–°å…´åº”ç”¨é¢†åŸŸçš„å…·ä½“è¡¨ç°ã€‚ç ”ç©¶æœ€åè¯†åˆ«äº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜å¹¶æŒ‡æ˜æœªæ¥æ–¹å‘ï¼Œå¼ºè°ƒä»£ç†å¼äººå·¥æ™ºèƒ½ (Agentic AI) æ˜¯ä¸€ä¸ªç”±ç®—æ³•ã€æ•°æ®ã€ç»„ç»‡å®è·µã€ç›‘ç®¡æ¡†æ¶å’Œç¤¾ä¼šè§„èŒƒå…±åŒé©±åŠ¨çš„é›†æˆåŒ–ç¤¾ä¼šæŠ€æœ¯ç³»ç»Ÿã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CY",
      "comment": "Dear Reviewer, please note that this is not survey/review or position paper. This paper introduced new framework (MAD-BAD-SAD Framework) for Socio-technical aspects of Agentic AI, Ethical considerations, which is very important to consider beside technical development",
      "pdf_url": "https://arxiv.org/pdf/2601.06064v1",
      "published_date": "2025-12-26 10:56:00 UTC",
      "updated_date": "2025-12-26 10:56:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:05:56.111702+00:00"
    },
    {
      "arxiv_id": "2512.21924v1",
      "title": "Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning",
      "title_zh": "åŸºäºè§£è€¦è§£å‰–å­¦ä¹ çš„è„‘éƒ¨ MRI æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Tao Yang",
        "Xiuying Wang",
        "Hao Liu",
        "Guanzhong Gong",
        "Lian-Ming Wu",
        "Yu-Ping Wang",
        "Lisheng Wang"
      ],
      "abstract": "Detection of various lesions in brain MRI is clinically critical, but challenging due to the diversity of lesions and variability in imaging conditions. Current unsupervised learning methods detect anomalies mainly through reconstructing abnormal images into pseudo-healthy images (PHIs) by normal samples learning and then analyzing differences between images. However, these unsupervised models face two significant limitations: restricted generalizability to multi-modality and multi-center MRIs due to their reliance on the specific imaging information in normal training data, and constrained performance due to abnormal residuals propagated from input images to reconstructed PHIs. To address these limitations, two novel modules are proposed, forming a new PHI reconstruction framework. Firstly, the disentangled representation module is proposed to improve generalizability by decoupling brain MRI into imaging information and essential imaging-invariant anatomical images, ensuring that the reconstruction focuses on the anatomy. Specifically, brain anatomical priors and a differentiable one-hot encoding operator are introduced to constrain the disentanglement results and enhance the disentanglement stability. Secondly, the edge-to-image restoration module is designed to reconstruct high-quality PHIs by restoring the anatomical representation from the high-frequency edge information of anatomical images, and then recoupling the disentangled imaging information. This module not only suppresses abnormal residuals in PHI by reducing abnormal pixels input through edge-only input, but also effectively reconstructs normal regions using the preserved structural details in the edges. Evaluated on nine public datasets (4,443 patients' MRIs from multiple centers), our method outperforms 17 SOTA methods, achieving absolute improvements of +18.32% in AP and +13.64% in DSC.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘éƒ¨ MRI å¼‚å¸¸æ£€æµ‹ä¸­æ— ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤šæ¨¡æ€ã€å¤šä¸­å¿ƒæ³›åŒ–æ€§å·®ä»¥åŠä¼ªå¥åº·å›¾åƒ (PHIs) é‡å»ºä¸­å­˜åœ¨å¼‚å¸¸æ®‹ç•™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§£è€¦è§£å‰–å­¦å­¦ä¹  (Disentangled Anatomy Learning) çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆå¼•å…¥äº†è§£è€¦è¡¨ç¤ºæ¨¡å— (Disentangled representation module)ï¼Œé€šè¿‡å°† MRI å›¾åƒè§£è€¦ä¸ºæˆåƒä¿¡æ¯å’Œå…·æœ‰æˆåƒä¸å˜æ€§çš„è§£å‰–å›¾åƒï¼Œå¹¶ç»“åˆè§£å‰–å…ˆéªŒå’Œå¯å¾® one-hot ç¼–ç ç®—å­æ¥ç¡®ä¿è§£æ„çš„ç¨³å®šæ€§ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œç ”ç©¶è®¾è®¡äº†è¾¹ç¼˜åˆ°å›¾åƒä¿®å¤æ¨¡å— (Edge-to-image restoration module)ï¼Œåˆ©ç”¨è§£å‰–å›¾åƒçš„é«˜é¢‘è¾¹ç¼˜ä¿¡æ¯æ¢å¤é«˜è´¨é‡çš„ PHIsï¼Œé€šè¿‡å‡å°‘åƒç´ è¾“å…¥æœ‰æ•ˆæŠ‘åˆ¶äº†å¼‚å¸¸æ®‹ç•™ï¼ŒåŒæ—¶ä¿ç•™äº†ç»“æ„ç»†èŠ‚ã€‚åœ¨æ¶µç›– 4,443 åæ‚£è€…çš„ 9 ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡ç²¾åº¦ (AP) å’Œ Dice ç³»æ•° (DSC) ä¸Šåˆ†åˆ«å®ç°äº† 18.32% å’Œ 13.64% çš„ç»å¯¹æå‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è§£è€¦è§£å‰–ç‰¹å¾ä¸æˆåƒä¿¡æ¯å¹¶åˆ©ç”¨è¾¹ç¼˜æ¢å¤æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°æ¯” 17 ç§ç°æœ‰å…ˆè¿›æ–¹æ³• (SOTA) æ›´ä¼˜çš„æ— ç›‘ç£ç—…å˜æ£€æµ‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by Medical Image Analysis (2025)",
      "pdf_url": "https://arxiv.org/pdf/2512.21924v1",
      "published_date": "2025-12-26 08:39:09 UTC",
      "updated_date": "2025-12-26 08:39:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:06:09.176860+00:00"
    },
    {
      "arxiv_id": "2512.21917v1",
      "title": "Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model",
      "title_zh": "åŠå‚æ•°åå¥½ä¼˜åŒ–ï¼šä½ çš„è¯­è¨€æ¨¡å‹å®åˆ™æ˜¯ä¸€ä¸ªå•æŒ‡æ•°æ¨¡å‹",
      "authors": [
        "Nathan Kallus"
      ],
      "abstract": "Aligning large language models to preference data is commonly implemented by assuming a known link function between the distribution of observed preferences and the unobserved rewards (e.g., a logistic link as in Bradley-Terry). If the link is wrong, however, inferred rewards can be biased and policies be misaligned. We study policy alignment to preferences under an unknown and unrestricted link. We consider an $f$-divergence-constrained reward maximization problem and show that realizability of the solution in a policy class implies a semiparametric single-index binary choice model, where a scalar-valued index determined by a policy captures the dependence on demonstrations and the rest of the preference distribution is an unrestricted function thereof. Rather than focus on estimation of identifiable finite-dimensional structural parameters in the index as in econometrics, we focus on policy learning, focusing on error to the optimal policy and allowing unidentifiable and nonparametric indices. We develop a variety of policy learners based on profiling the link function, orthogonalizing the link function, and using link-agnostic bipartite ranking objectives. We analyze these and provide finite-sample policy error bounds that depend on generic functional complexity measures of the index class. We further consider practical implementations using first-order optimization suited to neural networks and batched data. The resulting methods are robust to unknown preference noise distribution and scale, while preserving the direct optimization of policies without explicitly fitting rewards.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œå½“å‰å¤§è¯­è¨€æ¨¡å‹åå¥½å¯¹é½é€šå¸¸å‡è®¾è§‚æµ‹åå¥½ä¸å¥–åŠ±ä¹‹é—´å­˜åœ¨å·²çŸ¥çš„ Link Functionï¼Œä½†è‹¥è¯¥å‡è®¾é”™è¯¯ï¼Œåˆ™ä¼šå¯¼è‡´å¥–åŠ±åç½®å’Œç­–ç•¥å¤±å‡†ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æ¢è®¨äº†åœ¨æœªçŸ¥ä¸”ä¸å—é™ Link æƒ…å†µä¸‹çš„ç­–ç•¥å¯¹é½é—®é¢˜ï¼Œå¹¶è¯æ˜åœ¨ $f$-divergence çº¦æŸçš„å¥–åŠ±æœ€å¤§åŒ–æ¡†æ¶ä¸‹ï¼Œå…¶è§£éšå«äº†ä¸€ä¸ªåŠå‚æ•°çš„ Single-Index Modelã€‚ç ”ç©¶è€…å¼€å‘äº†åŸºäº Profilingã€Orthogonalizing ä»¥åŠ Link-agnostic çš„ Bipartite Ranking ç›®æ ‡çš„å¤šç§ç­–ç•¥å­¦ä¹ ç®—æ³•ï¼Œå¹¶æä¾›äº†åŸºäºå‡½æ•°å¤æ‚åº¦åº¦é‡çš„æœ‰é™æ ·æœ¬ç­–ç•¥è¯¯å·®ç•Œé™ï¼ˆFinite-sample Policy Error Boundsï¼‰ã€‚é€šè¿‡é’ˆå¯¹ç¥ç»ç½‘ç»œå’Œæ‰¹é‡æ•°æ®çš„ä¸€é˜¶ä¼˜åŒ–å®ç°ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€æ˜¾å¼æ‹Ÿåˆå¥–åŠ±çš„æƒ…å†µä¸‹å®ç°äº†ç­–ç•¥çš„ç›´æ¥ä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å¯¹æœªçŸ¥çš„åå¥½å™ªå£°åˆ†å¸ƒå’Œå°ºåº¦å…·æœ‰æå¼ºçš„é²æ£’æ€§ï¼Œä¸ºæ›´ç¨³å¥çš„è¯­è¨€æ¨¡å‹å¯¹é½æŠ€æœ¯æä¾›äº†ç†è®ºæ”¯æ’‘ä¸å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "econ.EM",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.21917v1",
      "published_date": "2025-12-26 08:22:41 UTC",
      "updated_date": "2025-12-26 08:22:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:06:33.968758+00:00"
    },
    {
      "arxiv_id": "2512.21907v2",
      "title": "SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?",
      "title_zh": "SpatialBenchï¼šæ™ºèƒ½ä½“èƒ½å¦åˆ†æçœŸå®ä¸–ç•Œçš„ç©ºé—´ç”Ÿç‰©å­¦æ•°æ®ï¼Ÿ",
      "authors": [
        "Kenny Workman",
        "Zhen Yang",
        "Harihara Muralidharan",
        "Hannah Le"
      ],
      "abstract": "Spatial transcriptomics assays are rapidly increasing in scale and complexity, making computational analysis a major bottleneck in biological discovery. Although frontier AI agents have improved dramatically at software engineering and general data analysis, it remains unclear whether they can extract biological insight from messy, real-world spatial datasets. We introduce SpatialBench, a benchmark of 146 verifiable problems derived from practical spatial analysis workflows spanning five spatial technologies and seven task categories. Each problem provides a snapshot of experimental data immediately prior to an analysis step and a deterministic grader that evaluates recovery of a key biological result. Benchmark data on frontier models shows that base model accuracy remains low (20-38% across model families), with strong model-task and model-platform interactions. Harness design has a large empirical effect on performance, indicating that tools, prompts, control flow, and execution environment should be evaluated and improved as first-class objects. SpatialBench serves both as a measurement tool and a diagnostic lens for developing agents that can interact with real spatial datasets faithfully, transparently, and reproducibly.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†SpatialBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«146ä¸ªå¯éªŒè¯é—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°AIæ™ºèƒ½ä½“(Agents)åˆ†æçœŸå®ä¸–ç•Œç©ºé—´ç”Ÿç‰©å­¦(Spatial Biology)æ•°æ®çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æ¶µç›–äº†5ç§ç©ºé—´æŠ€æœ¯(Spatial Technologies)å’Œ7ä¸ªä»»åŠ¡ç±»åˆ«ï¼Œé€šè¿‡æä¾›å®éªŒæ•°æ®å¿«ç…§å’Œç¡®å®šæ€§è¯„åˆ†å™¨(Deterministic Grader)æ¥è¯„ä¼°ç”Ÿç‰©å­¦ç»“æœçš„æ¢å¤æƒ…å†µã€‚å®éªŒæµ‹è¯•æ˜¾ç¤ºï¼Œå‰æ²¿åŸºç¡€æ¨¡å‹çš„å‡†ç¡®ç‡æ™®éè¾ƒä½ï¼Œä»…åœ¨20%è‡³38%ä¹‹é—´ï¼Œä¸”è¡¨ç°å‡ºæ˜¾è‘—çš„æ¨¡å‹-ä»»åŠ¡ä¸æ¨¡å‹-å¹³å°äº¤äº’æ•ˆåº”ã€‚ç ”ç©¶å¼ºè°ƒäº†å®éªŒæ¡†æ¶(Harness)è®¾è®¡å¯¹æ€§èƒ½çš„å·¨å¤§å½±å“ï¼ŒæŒ‡å‡ºå·¥å…·ã€æç¤ºè¯(Prompts)ã€æ§åˆ¶æµå’Œæ‰§è¡Œç¯å¢ƒåº”è¢«è§†ä¸ºæ ¸å¿ƒç»„ä»¶è¿›è¡Œä¼˜åŒ–ã€‚SpatialBenchä¸ä»…ä½œä¸ºä¸€ä¸ªæµ‹é‡å·¥å…·ï¼Œè¿˜ä¸ºå¼€å‘èƒ½å¤Ÿå¿ å®ã€é€æ˜ä¸”å¯é‡å¤åœ°å¤„ç†ç©ºé—´è½¬å½•ç»„å­¦(Spatial transcriptomics)æ•°æ®é›†çš„æ™ºèƒ½ä½“æä¾›äº†è¯Šæ–­è§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 9 figures, 4 tables; NeurIPS 2024 format",
      "pdf_url": "https://arxiv.org/pdf/2512.21907v2",
      "published_date": "2025-12-26 07:40:11 UTC",
      "updated_date": "2026-01-05 18:55:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:06:58.990737+00:00"
    },
    {
      "arxiv_id": "2512.22309v1",
      "title": "LLMBoost: Make Large Language Models Stronger with Boosting",
      "title_zh": "LLMBoostï¼šé€šè¿‡ Boosting æœºåˆ¶å¼ºåŒ–å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½",
      "authors": [
        "Zehao Chen",
        "Tianxiang Ai",
        "Yifei Li",
        "Gongxun Li",
        "Yuyang Wei",
        "Wang Zhou",
        "Guanghui Li",
        "Bin Yu",
        "Zhijun Chen",
        "Hailong Sun",
        "Fuzhen Zhuang",
        "Jianxin Li",
        "Deqing Wang",
        "Yikun Ban"
      ],
      "abstract": "Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LLMBoostï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„é›†æˆå¾®è°ƒ (ensemble fine-tuning) æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ˜¾å¼åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ä¸­é—´çŠ¶æ€æ¥å…‹æœç°æœ‰é›†æˆæ–¹æ³•å°†æ¨¡å‹è§†ä¸ºâ€œé»‘ç›’â€çš„å±€é™ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸‰å¤§æ ¸å¿ƒåˆ›æ–°ï¼šé¦–å…ˆæ˜¯é€šè¿‡è·¨æ¨¡å‹æ³¨æ„åŠ›æœºåˆ¶ (cross-model attention mechanism) å®ç°åç»§æ¨¡å‹å¯¹å‰åºæ¨¡å‹éšè—çŠ¶æ€çš„è®¿é—®ä¸èåˆï¼Œä»è€Œä¿ƒè¿›å±‚æ¬¡åŒ–é”™è¯¯çº æ­£å’ŒçŸ¥è¯†è½¬ç§»ï¼›å…¶æ¬¡æ˜¯é‡‡ç”¨é“¾å¼è®­ç»ƒèŒƒå¼ (chain training paradigm)ï¼Œä»¥é”™è¯¯æŠ‘åˆ¶ä¸ºç›®æ ‡é€æ­¥å¾®è°ƒè¿æ¥çš„æ¨¡å‹ï¼Œç¡®ä¿æ¯ä¸ªæ¨¡å‹èƒ½ä»¥æå°çš„é¢å¤–è®¡ç®—é‡çº æ­£å‰åºæ¨¡å‹çš„é¢„æµ‹é”™è¯¯ï¼›æœ€åæ˜¯è¿‘å¹¶è¡Œæ¨ç†èŒƒå¼ (near-parallel inference paradigm) è®¾è®¡ï¼Œé€šè¿‡é€å±‚æµæ°´çº¿åŒ–éšè—çŠ¶æ€ï¼Œä½¿æ¨ç†æ•ˆç‡æ¥è¿‘å•æ¨¡å‹è§£ç ã€‚ç†è®ºåˆ†æè¯æ˜äº† LLMBoost çš„é¡ºåºæ•´åˆèƒ½å¤Ÿåœ¨æœ‰ç•Œçº é”™å‡è®¾ä¸‹ä¿è¯æ€§èƒ½çš„å•è°ƒæå‡ã€‚åœ¨å¸¸è¯†æ¨ç†å’Œç®—æœ¯æ¨ç†ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLLMBoost åœ¨æ˜¾è‘—æé«˜æ¨¡å‹å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œè¿˜æœ‰æ•ˆé™ä½äº†æ¨ç†å»¶è¿Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22309v1",
      "published_date": "2025-12-26 07:16:41 UTC",
      "updated_date": "2025-12-26 07:16:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:06:45.928469+00:00"
    },
    {
      "arxiv_id": "2512.21898v1",
      "title": "Flexible Multitask Learning with Factorized Diffusion Policy",
      "title_zh": "åŸºäºå› å­åŒ–æ‰©æ•£ç­–ç•¥çš„çµæ´»å¤šä»»åŠ¡å­¦ä¹ ",
      "authors": [
        "Chaoqi Liu",
        "Haonan Chen",
        "Sigmund H. HÃ¸eg",
        "Shaoxiong Yao",
        "Yunzhu Li",
        "Kris Hauser",
        "Yilun Du"
      ],
      "abstract": "Multitask learning poses significant challenges due to the highly multimodal and diverse nature of robot action distributions. However, effectively fitting policies to these complex task distributions is often difficult, and existing monolithic models often underfit the action distribution and lack the flexibility required for efficient adaptation. We introduce a novel modular diffusion policy framework that factorizes complex action distributions into a composition of specialized diffusion models, each capturing a distinct sub-mode of the behavior space for a more effective overall policy. In addition, this modular structure enables flexible policy adaptation to new tasks by adding or fine-tuning components, which inherently mitigates catastrophic forgetting. Empirically, across both simulation and real-world robotic manipulation settings, we illustrate how our method consistently outperforms strong modular and monolithic baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Multitask learning ä¸­æœºå™¨äººåŠ¨ä½œåˆ†å¸ƒçš„é«˜åº¦å¤šæ¨¡æ€å’Œå¤šæ ·æ€§æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„å•ä½“æ¨¡å‹(monolithic models)å®¹æ˜“å‡ºç°æ¬ æ‹Ÿåˆä¸”ç¼ºä¹çµæ´»æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º Factorized Diffusion Policy çš„æ–°å‹æ¨¡å—åŒ–æ‰©æ•£ç­–ç•¥æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å¤æ‚çš„åŠ¨ä½œåˆ†å¸ƒåˆ†è§£ä¸ºä¸€ç³»åˆ—ä¸“é—¨çš„ diffusion models çš„ç»„åˆï¼Œé€šè¿‡è®©æ¯ä¸ªæ¨¡å‹æ•æ‰è¡Œä¸ºç©ºé—´çš„ç‰¹å®šå­æ¨¡å¼æ¥æå‡æ•´ä½“ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¿™ç§æ¨¡å—åŒ–ç»“æ„æ”¯æŒé€šè¿‡å¢åŠ æˆ–å¾®è°ƒç»„ä»¶å®ç°å¯¹æ–°ä»»åŠ¡çš„çµæ´»é€‚åº”ï¼Œå¹¶èƒ½æœ‰æ•ˆç¼“è§£ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººæ“çºµä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½ä¸€è‡´ä¼˜äºå¼ºåŠ›çš„æ¨¡å—åŒ–å’Œå•ä½“åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.21898v1",
      "published_date": "2025-12-26 07:11:47 UTC",
      "updated_date": "2025-12-26 07:11:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:07:32.716730+00:00"
    },
    {
      "arxiv_id": "2512.21897v1",
      "title": "MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction",
      "title_zh": "MMCTOPï¼šé¢å‘ä¸´åºŠè¯•éªŒç»“æœé¢„æµ‹çš„å¤šæ¨¡æ€æ–‡æœ¬åŒ–ä¸æ··åˆä¸“å®¶æ¡†æ¶",
      "authors": [
        "Carolina AparÃ­cio",
        "Qi Shi",
        "Bo Wen",
        "Tesfaye Yadete",
        "Qiwei Han"
      ],
      "abstract": "Addressing the challenge of multimodal data fusion in high-dimensional biomedical informatics, we propose MMCTOP, a MultiModal Clinical-Trial Outcome Prediction framework that integrates heterogeneous biomedical signals spanning (i) molecular structure representations, (ii) protocol metadata and long-form eligibility narratives, and (iii) disease ontologies. MMCTOP couples schema-guided textualization and input-fidelity validation with modality-aware representation learning, in which domain-specific encoders generate aligned embeddings that are fused by a transformer backbone augmented with a drug-disease-conditioned sparse Mixture-of-Experts (SMoE). This design explicitly supports specialization across therapeutic and design subspaces while maintaining scalable computation through top-k routing. MMCTOP achieves consistent improvements in precision, F1, and AUC over unimodal and multimodal baselines on benchmark datasets, and ablations show that schema-guided textualization and selective expert routing contribute materially to performance and stability. We additionally apply temperature scaling to obtain calibrated probabilities, ensuring reliable risk estimation for downstream decision support. Overall, MMCTOP advances multimodal trial modeling by combining controlled narrative normalization, context-conditioned expert fusion, and operational safeguards aimed at auditability and reproducibility in biomedical informatics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MMCTOPï¼Œä¸€ç§ç”¨äºä¸´åºŠè¯•éªŒç»“æœé¢„æµ‹(Clinical Trial Outcome Prediction)çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é«˜ç»´ç”Ÿç‰©åŒ»å­¦ä¿¡æ¯å­¦ä¸­çš„å¼‚æ„æ•°æ®èåˆæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ•´åˆäº†åˆ†å­ç»“æ„è¡¨ç¤ºã€æ–¹æ¡ˆå…ƒæ•°æ®ä¸é•¿ç¯‡å…¥ç»„æ ‡å‡†å™è¿°ä»¥åŠç–¾ç—…æœ¬ä½“(disease ontologies)ç­‰å¤šæºä¿¡å·ã€‚MMCTOP é‡‡ç”¨äº†æ¨¡å¼å¼•å¯¼æ–‡æœ¬åŒ–(schema-guided textualization)å’Œè¾“å…¥ä¿çœŸåº¦éªŒè¯ï¼Œå¹¶ç»“åˆæ¨¡æ€æ„ŸçŸ¥è¡¨ç¤ºå­¦ä¹ ï¼Œé€šè¿‡å—è¯ç‰©-ç–¾ç—…çº¦æŸçš„ç¨€ç–ä¸“å®¶æ··åˆ(Sparse Mixture-of-Experts, SMoE)å¢å¼º Transformer æ¶æ„ã€‚è¿™ç§è®¾è®¡åˆ©ç”¨ top-k è·¯ç”±å®ç°äº†åœ¨ä¸åŒæ²»ç–—å’Œè®¾è®¡å­ç©ºé—´ä¸­çš„æ¨¡å‹ä¸“ä¸šåŒ–ï¼Œåœ¨æå‡æ€§èƒ½çš„åŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMMCTOP åœ¨ç²¾ç¡®ç‡ã€F1 å’Œ AUC ç­‰å…³é”®æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å•æ¨¡æ€å’Œå¤šæ¨¡æ€åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡æ¸©åº¦ç¼©æ”¾(temperature scaling)ç¡®ä¿äº†é£é™©ä¼°è®¡çš„å¯é æ¦‚ç‡æ ¡å‡†ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦é¢†åŸŸæä¾›äº†å…·æœ‰å¯å®¡è®¡æ€§å’Œå¯é‡å¤æ€§çš„å†³ç­–æ”¯æŒæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 3 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.21897v1",
      "published_date": "2025-12-26 06:56:08 UTC",
      "updated_date": "2025-12-26 06:56:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:07:30.211031+00:00"
    },
    {
      "arxiv_id": "2512.21887v2",
      "title": "Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space",
      "title_zh": "é¢å‘ä¸‰ç»´ç©ºé—´é•¿ç¨‹è§†è§‰ç”Ÿæˆä¸å¯¼èˆªçš„ç©ºä¸­ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Weichen Zhang",
        "Peizhi Tang",
        "Xin Zeng",
        "Fanhang Man",
        "Shiquan Yu",
        "Zichao Dai",
        "Baining Zhao",
        "Hongjin Chen",
        "Yu Shang",
        "Wei Wu",
        "Chen Gao",
        "Xinlei Chen",
        "Xin Wang",
        "Yong Li",
        "Wenwu Zhu"
      ],
      "abstract": "Unmanned aerial vehicles (UAVs) have emerged as powerful embodied agents. One of the core abilities is autonomous navigation in large-scale three-dimensional environments. Existing navigation policies, however, are typically optimized for low-level objectives such as obstacle avoidance and trajectory smoothness, lacking the ability to incorporate high-level semantics into planning. To bridge this gap, we propose ANWM, an aerial navigation world model that predicts future visual observations conditioned on past frames and actions, thereby enabling agents to rank candidate trajectories by their semantic plausibility and navigational utility. ANWM is trained on 4-DoF UAV trajectories and introduces a physics-inspired module: Future Frame Projection (FFP), which projects past frames into future viewpoints to provide coarse geometric priors. This module mitigates representational uncertainty in long-distance visual generation and captures the mapping between 3D trajectories and egocentric observations. Empirical results demonstrate that ANWM significantly outperforms existing world models in long-distance visual forecasting and improves UAV navigation success rates in large-scale environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœº(Unmanned Aerial Vehicles, UAVs)åœ¨è‡ªä¸»å¯¼èˆªä¸­ç¼ºä¹é«˜å±‚è¯­ä¹‰è§„åˆ’èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†ANWMï¼Œä¸€ç§ä¸“ä¸ºé•¿æ—¶ç¨‹è§†è§‰ç”Ÿæˆä¸ä¸‰ç»´ç©ºé—´å¯¼èˆªè®¾è®¡çš„ç©ºä¸­å¯¼èˆªä¸–ç•Œæ¨¡å‹ã€‚ANWMé€šè¿‡ç»“åˆå†å²è§†é¢‘å¸§å’ŒåŠ¨ä½œä¿¡æ¯æ¥é¢„æµ‹æœªæ¥è§†è§‰è§‚å¯Ÿï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿä¾æ®è¯­ä¹‰åˆç†æ€§å’Œå¯¼èˆªæ•ˆç”¨å¯¹å€™é€‰è½¨è¿¹è¿›è¡Œè¯„ä¼°ä¸æ’åºã€‚è¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°å¼•å…¥äº†å—ç‰©ç†å¯å‘çš„æœªæ¥å¸§æŠ•å½±(Future Frame Projection, FFP)æ¨¡å—ï¼Œé€šè¿‡å°†è¿‡å»å¸§æŠ•å½±è‡³æœªæ¥è§†ç‚¹æä¾›å‡ ä½•å…ˆéªŒï¼Œæœ‰æ•ˆç¼“è§£äº†é•¿è·ç¦»è§†è§‰ç”Ÿæˆä¸­çš„è¡¨å¾ä¸ç¡®å®šæ€§ã€‚è¿™ä¸€æœºåˆ¶æˆåŠŸæ•æ‰äº†3Dè½¨è¿¹ä¸è‡ªæˆ‘ä¸­å¿ƒè§‚å¯Ÿ(egocentric observations)ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒANWMåœ¨é•¿è·ç¦»è§†è§‰é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶å¤§å¹…æå‡äº†æ— äººæœºåœ¨å¤§è§„æ¨¡ç¯å¢ƒä¸‹çš„å¯¼èˆªæˆåŠŸç‡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.21887v2",
      "published_date": "2025-12-26 06:22:39 UTC",
      "updated_date": "2026-01-03 03:46:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:07:17.768703+00:00"
    },
    {
      "arxiv_id": "2601.09719v1",
      "title": "Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models",
      "title_zh": "æœ‰ç•ŒåŒæ›²æ­£åˆ‡ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­ Pre-Layer Normalization çš„ç¨³å®šé«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆ",
      "authors": [
        "Hoyoon Byun",
        "Youngjun Choi",
        "Taero Kim",
        "Sungrae Park",
        "Kyungwoo Song"
      ],
      "abstract": "Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Bounded Hyperbolic Tanh (BHyT)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æ›¿ä»£å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸­å¸¸ç”¨çš„ Pre-Layer Normalization (Pre-LN) çš„æ–°æ–¹æ¡ˆã€‚é’ˆå¯¹ Pre-LN åœ¨æ¨¡å‹æ·±åº¦å¢åŠ æ—¶å‡ºç°çš„éšè—çŠ¶æ€ä¸ç¨³å’Œé‡å¤ç»Ÿè®¡è®¡ç®—å¯¼è‡´çš„æ•ˆç‡ç“¶é¢ˆï¼ŒBHyT å°† tanh éçº¿æ€§æ¿€æ´»å‡½æ•°ä¸æ•°æ®é©±åŠ¨çš„è¾“å…¥è¾¹ç•Œæœºåˆ¶ç›¸ç»“åˆï¼Œç¡®ä¿æ¿€æ´»å€¼å§‹ç»ˆå¤„äºéé¥±å’ŒåŒºé—´ã€‚è¯¥æ–¹æ³•ä¸ä»…åœ¨ç†è®ºä¸Šæä¾›äº†ç¨³å®šæ€§ä¿è¯ï¼Œæœ‰æ•ˆæŠ‘åˆ¶äº†æ¿€æ´»å€¼å¹…åº¦å’Œæ–¹å·®éšæ·±åº¦çš„å¢é•¿ï¼Œè¿˜é€šè¿‡æ¯å—ä»…è®¡ç®—ä¸€æ¬¡ç²¾ç¡®ç»Ÿè®¡é‡å¹¶é‡‡ç”¨è½»é‡çº§æ–¹å·®è¿‘ä¼¼æ¥æ˜¾è‘—æå‡è¿è¡Œæ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒBHyT åœ¨é¢„è®­ç»ƒé˜¶æ®µæ¯” RMSNorm æé€Ÿçº¦ 15.8%ï¼ŒToken ç”Ÿæˆååé‡æå‡ 4.2%ï¼Œä¸”åœ¨å¤šé¡¹è¯­è¨€ç†è§£ä¸æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºä¼˜äºæˆ–ç­‰åŒäºç°æœ‰æ¨¡å‹çš„æ€§èƒ½ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.09719v1",
      "published_date": "2025-12-26 06:22:13 UTC",
      "updated_date": "2025-12-26 06:22:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:09:23.807568+00:00"
    },
    {
      "arxiv_id": "2512.21884v1",
      "title": "Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹è·¨åœ°åŸŸåˆ†å¸ƒå¼æ¨ç†çš„èµ„æºåˆ†é…ä¼˜åŒ–",
      "authors": [
        "Tingyang Sun",
        "Ting He",
        "Bo Ji",
        "Parimal Parag"
      ],
      "abstract": "Large language models have demonstrated extraordinary performance in many AI tasks but are expensive to use, even after training, due to their requirement of high-end GPUs. Recently, a distributed system called PETALS was developed to lower the barrier for deploying LLMs by splitting the model blocks across multiple servers with low-end GPUs distributed over the Internet, which was much faster than swapping the model parameters between the GPU memory and other cheaper but slower local storage media. However, the performance of such a distributed system critically depends on the resource allocation, and how to do so optimally remains unknown. In this work, we present the first systematic study of the resource allocation problem in distributed LLM inference, with focus on two important decisions: block placement and request routing. Our main results include: experimentally validated performance models that can predict the inference performance under given block placement and request routing decisions, a formulation of the offline optimization of block placement and request routing as a mixed integer linear programming problem together with the NP-hardness proof and a polynomial-complexity algorithm with guaranteed performance, and an adaptation of the offline algorithm for the online setting with the same performance guarantee under bounded load. Through both experiments and experimentally-validated simulations, we have verified that the proposed solution can substantially reduce the inference time compared to the state-of-the-art solution in diverse settings with geographically-distributed servers. As a byproduct, we have also developed a light-weighted CPU-only simulator capable of predicting the performance of distributed LLM inference on GPU servers, which can evaluate large deployments and facilitate future research for researchers with limited GPU access.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Models) æ¨ç†æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œç³»ç»Ÿæ€§åœ°æ¢è®¨äº†åœ°ç†åˆ†å¸ƒå¼æ¨ç†ä¸­çš„èµ„æºåˆ†é…ä¼˜åŒ–ã€‚ç ”ç©¶é‡ç‚¹èšç„¦äºæ¨¡å‹å—æ”¾ç½® (block placement) å’Œè¯·æ±‚è·¯ç”± (request routing) ä¸¤å¤§æ ¸å¿ƒå†³ç­–ã€‚ä½œè€…æå‡ºäº†ç»è¿‡å®éªŒéªŒè¯çš„æ€§èƒ½æ¨¡å‹ï¼Œå¹¶å°†ç¦»çº¿ä¼˜åŒ–é—®é¢˜å»ºæ¨¡ä¸ºæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ (Mixed Integer Linear Programming)ï¼ŒåŒæ—¶ç»™å‡ºäº†å…·æœ‰æ€§èƒ½ä¿è¯çš„å¤šé¡¹å¼å¤æ‚åº¦ç®—æ³•åŠå…¶åœ¨çº¿é€‚é…ç‰ˆæœ¬ã€‚é€šè¿‡å®éªŒå’Œæ¨¡æ‹ŸéªŒè¯ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šç§åœ°ç†åˆ†å¸ƒå¼æœåŠ¡å™¨ç¯å¢ƒä¸‹ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯èƒ½æ˜¾è‘—å‡å°‘æ¨ç†æ—¶é—´ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†ä¸€ä¸ªè½»é‡çº§çš„ä»…é™ CPU çš„æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨å¸®åŠ©ç¼ºä¹ GPU èµ„æºçš„ç§‘ç ”äººå‘˜å¼€å±•å¤§è§„æ¨¡åˆ†å¸ƒå¼ LLM æ¨ç†æ€§èƒ½é¢„æµ‹ä¸ç ”ç©¶ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.21884v1",
      "published_date": "2025-12-26 06:13:59 UTC",
      "updated_date": "2025-12-26 06:13:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:07:24.373880+00:00"
    },
    {
      "arxiv_id": "2512.21878v1",
      "title": "MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting",
      "title_zh": "MASFINï¼šé¢å‘åˆ†è§£å¼é‡‘èæ¨ç†ä¸é¢„æµ‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Marc S. Montalvo",
        "Hamed Yaghoobian"
      ],
      "abstract": "Recent advances in large language models (LLMs) are transforming data-intensive domains, with finance representing a high-stakes environment where transparent and reproducible analysis of heterogeneous signals is essential. Traditional quantitative methods remain vulnerable to survivorship bias, while many AI-driven approaches struggle with signal integration, reproducibility, and computational efficiency. We introduce MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news, while embedding explicit bias-mitigation protocols. The system leverages GPT-4.1-nano for reproducability and cost-efficient inference and generates weekly portfolios of 15-30 equities with allocation weights optimized for short-term performance. In an eight-week evaluation, MASFIN delivered a 7.33% cumulative return, outperforming the S&P 500, NASDAQ-100, and Dow Jones benchmarks in six of eight weeks, albeit with higher volatility. These findings demonstrate the promise of bias-aware, generative AI frameworks for financial forecasting and highlight opportunities for modular multi-agent design to advance practical, transparent, and reproducible approaches in quantitative finance.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† MASFINï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–çš„å¤šæ™ºèƒ½ä½“(multi-agent)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ†è§£å¼çš„é‡‘èæ¨ç†å’Œé¢„æµ‹æ¥æå‡é‡åŒ–åˆ†æçš„é€æ˜åº¦ä¸å¯å¤ç°æ€§ã€‚è¯¥ç³»ç»Ÿå°†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸ç»“æ„åŒ–é‡‘èæŒ‡æ ‡åŠéç»“æ„åŒ–æ–°é—»ä¿¡æ¯ç›¸ç»“åˆï¼Œå¹¶åµŒå…¥äº†æ˜¾å¼çš„åè§ç¼“è§£åè®®(bias-mitigation protocols)ä»¥è§£å†³ä¼ ç»Ÿé‡åŒ–æ–¹æ³•ä¸­çš„ç”Ÿå­˜åå·®(survivorship bias)å’Œä¿¡å·é›†æˆéš¾ç­‰é—®é¢˜ã€‚MASFIN é‡‡ç”¨ GPT-4.1-nano ä»¥å®ç°æˆæœ¬æ•ˆç›Šé«˜çš„æ¨ç†ï¼Œå¹¶èƒ½å¤Ÿæ¯å‘¨ç”ŸæˆåŒ…å« 15-30 åªè‚¡ç¥¨ä¸”ç»è¿‡æƒé‡ä¼˜åŒ–çš„æŠ•èµ„ç»„åˆã€‚åœ¨ä¸ºæœŸå…«å‘¨çš„è¯„ä¼°ä¸­ï¼ŒMASFIN å®ç°äº† 7.33% çš„ç´¯è®¡æ”¶ç›Šç‡ï¼Œåœ¨å…­ä¸ªå‘¨åº¦è¡¨ç°ä¸­è¶…è¶Šäº† S&P 500ã€NASDAQ-100 å’Œ Dow Jones åŸºå‡†ï¼Œå°½ç®¡å…¶æ³¢åŠ¨æ€§è¾ƒé«˜ã€‚è¿™äº›å‘ç°å±•ç¤ºäº†å…·å¤‡åè§æ„è¯†çš„ç”Ÿæˆå¼ AI æ¡†æ¶åœ¨é‡‘èé¢„æµ‹é¢†åŸŸçš„åº”ç”¨å‰æ™¯ï¼Œçªæ˜¾äº†æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“è®¾è®¡åœ¨æ¨åŠ¨é€æ˜ã€å¯å¤ç°çš„é‡åŒ–é‡‘èæ–¹æ³•æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted to the NeurIPS 2025 Workshop on Generative AI in Finance",
      "pdf_url": "https://arxiv.org/pdf/2512.21878v1",
      "published_date": "2025-12-26 06:01:55 UTC",
      "updated_date": "2025-12-26 06:01:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:07:30.916325+00:00"
    },
    {
      "arxiv_id": "2512.21877v1",
      "title": "CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics",
      "title_zh": "CricBenchï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨æ¿çƒåˆ†æä¸­æ€§èƒ½çš„å¤šè¯­è¨€åŸºå‡†",
      "authors": [
        "Vaibhav Devraj",
        "Dhruv Kumar",
        "Jagat Sesh Challa"
      ],
      "abstract": "Cricket is the second most popular sport globally, commanding a massive following of over 2.5 billion fans globally. Enthusiasts and analysts frequently seek advanced statistical insights, such as long-term historical performance trends or complex player comparisons, that are often unavailable through standard web searches. While Large Language Models (LLMs) have advanced significantly in Text-to-SQL tasks, their capability to handle the domain-specific nuances, complex schema variations, and multilingual requirements inherent to sports analytics remains under-explored. To investigate this potential capability gap, we present CricBench, a comprehensive benchmark suite for evaluating LLMs on specialized cricket data. To curate a \"Gold Standard\" dataset, we collaborate with domain experts in cricket and SQL to manually author complex queries, ensuring logical correctness. Recognizing linguistic diversity, we construct the benchmark in both English and Hindi, establishing a framework that is open for further extension to other regional languages. We evaluate six state-of-the-art models, including GPT-4o, Claude 3.7 Sonnet, and open-source models, using a strict evaluation protocol. Our results reveal that high performance on general benchmarks does not guarantee success in specialized domains. While the open-weights reasoning model DeepSeek R1 achieves state-of-the-art performance (50.6%), surpassing proprietary giants like Claude 3.7 Sonnet (47.7%) and GPT-4o (33.7%), it still exhibits a significant accuracy drop when moving from general benchmarks (BIRD) to CricBench. Furthermore, we observe that code-mixed Hindi queries frequently yield parity or higher accuracy compared to English, challenging the assumption that English is the optimal prompt language for specialized SQL tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† CricBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ¿çƒåˆ†æé¢†åŸŸè¡¨ç°çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ã€‚ä¸ºè§£å†³é€šç”¨æ¨¡å‹åœ¨å¤„ç†ä½“è‚²åˆ†æä¸­çš„é¢†åŸŸç‰¹å®šç»†å¾®å·®åˆ«ã€å¤æ‚ Schema å˜ä½“åŠå¤šè¯­è¨€éœ€æ±‚æ—¶çš„èƒ½åŠ›é¸¿æ²Ÿï¼Œç ”ç©¶è€…é€šè¿‡ä¸é¢†åŸŸä¸“å®¶åˆä½œæ‰‹åŠ¨ç¼–å†™ SQL æŸ¥è¯¢ï¼Œæ„å»ºäº†æ¶µç›–è‹±è¯­å’Œå°åœ°è¯­çš„é«˜è´¨é‡æ•°æ®é›†ã€‚é€šè¿‡å¯¹ GPT-4oã€Claude 3.7 Sonnet å’Œ DeepSeek R1 ç­‰å…­æ¬¾ä¸»æµæ¨¡å‹çš„ä¸¥æ ¼è¯„ä¼°ï¼Œå‘ç°å³ä¾¿æ˜¯åœ¨é€šç”¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚çš„æ¨¡å‹ï¼Œåœ¨ç‰¹å®šé¢†åŸŸä¹Ÿé¢ä¸´æ˜¾è‘—çš„å‡†ç¡®ç‡ä¸‹é™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼€æºæ¨ç†æ¨¡å‹ DeepSeek R1 ä»¥ 50.6% çš„å‡†ç¡®ç‡è¶…è¶Šäº† GPT-4o ç­‰ä¸“æœ‰æ¨¡å‹ä½å±…æ¦œé¦–ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è§‚å¯Ÿåˆ°æ··åˆä»£ç çš„å°åœ°è¯­æŸ¥è¯¢åœ¨å‡†ç¡®ç‡ä¸Šå¾€å¾€ä¸è‹±è¯­æŒå¹³ç”šè‡³æ›´é«˜ï¼ŒæŒ‘æˆ˜äº†è‹±è¯­æ˜¯ä¸“ä¸š SQL ä»»åŠ¡æœ€ä½³æç¤ºè¯­è¨€çš„ä¼ ç»Ÿå‡è®¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2512.21877v1",
      "published_date": "2025-12-26 05:59:19 UTC",
      "updated_date": "2025-12-26 05:59:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:07:38.506872+00:00"
    },
    {
      "arxiv_id": "2512.22307v1",
      "title": "LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators",
      "title_zh": "LLAï¼šåŸºäºé€»è¾‘é”å®šåŠ é€Ÿå™¨å¢å¼ºç”Ÿæˆå¼æ¨¡å‹çš„å®‰å…¨æ€§ä¸éšç§æ€§",
      "authors": [
        "You Li",
        "Guannan Zhao",
        "Yuhao Ju",
        "Yunqi He",
        "Jie Gu",
        "Hai Zhou"
      ],
      "abstract": "We introduce LLA, an effective intellectual property (IP) protection scheme for generative AI models. LLA leverages the synergy between hardware and software to defend against various supply chain threats, including model theft, model corruption, and information leakage. On the software side, it embeds key bits into neurons that can trigger outliers to degrade performance and applies invariance transformations to obscure the key values. On the hardware side, it integrates a lightweight locking module into the AI accelerator while maintaining compatibility with various dataflow patterns and toolchains. An accelerator with a pre-stored secret key acts as a license to access the model services provided by the IP owner. The evaluation results show that LLA can withstand a broad range of oracle-guided key optimization attacks, while incurring a minimal computational overhead of less than 0.1% for 7,168 key bits.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LLAï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) æ¨¡å‹å®‰å…¨æ€§å’Œéšç§æ€§çš„çŸ¥è¯†äº§æƒ (IP) ä¿æŠ¤æ–¹æ¡ˆã€‚LLA é€šè¿‡ç¡¬ä»¶ä¸è½¯ä»¶çš„ååŒæœºåˆ¶ï¼Œæœ‰æ•ˆé˜²å¾¡æ¨¡å‹çªƒå–ã€æ¨¡å‹æŸåå’Œä¿¡æ¯æ³„éœ²ç­‰ä¾›åº”é“¾å¨èƒã€‚åœ¨è½¯ä»¶å±‚é¢ï¼Œå®ƒå°†å¯†é’¥ä½åµŒå…¥ç¥ç»å…ƒä»¥è§¦å‘å¼‚å¸¸å€¼ (Outliers) ä»è€Œé™ä½æœªæˆæƒæ€§èƒ½ï¼Œå¹¶åˆ©ç”¨ä¸å˜æ€§å˜æ¢ (Invariance Transformations) éšè—å¯†é’¥ã€‚åœ¨ç¡¬ä»¶å±‚é¢ï¼Œè¯¥æ–¹æ¡ˆåœ¨ AI åŠ é€Ÿå™¨ä¸­é›†æˆäº†ä¸€ä¸ªè½»é‡çº§é”å®šæ¨¡å—ï¼Œä¸”ä¿æŒäº†å¯¹ç°æœ‰æ•°æ®æµå’Œå·¥å…·é“¾çš„å…¼å®¹æ€§ã€‚é¢„å­˜æœ‰ç§é’¥çš„åŠ é€Ÿå™¨å°†ä½œä¸ºè®¿é—®æ¨¡å‹æœåŠ¡çš„æˆæƒè®¸å¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLA èƒ½å¤ŸæŠµæŠ—å¤šç§åŸºäº Oracle å¼•å¯¼çš„å¯†é’¥ä¼˜åŒ–æ”»å‡»ã€‚åœ¨å¤„ç† 7,168 ä½å¯†é’¥æ—¶ï¼Œå…¶äº§ç”Ÿçš„è®¡ç®—å¼€é”€ä½äº 0.1%ï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨ä¿éšœå®‰å…¨æ€§çš„åŒæ—¶å…·æœ‰æé«˜çš„è¿è¡Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by AAAI'26 as a conference paper and selected for oral presentation",
      "pdf_url": "https://arxiv.org/pdf/2512.22307v1",
      "published_date": "2025-12-26 05:47:29 UTC",
      "updated_date": "2025-12-26 05:47:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:07:43.640826+00:00"
    },
    {
      "arxiv_id": "2512.22306v1",
      "title": "Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection",
      "title_zh": "è¶…è¶Šå•ä¸€æ¼æ´ï¼šé¢å‘å¤šæ¼æ´æ£€æµ‹çš„å¤§è¯­è¨€æ¨¡å‹åŸºå‡†è¯„æµ‹",
      "authors": [
        "Chinmay Pushkar",
        "Sanchit Kabra",
        "Dhruv Kumar",
        "Jagat Sesh Challa"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated significant potential in automated software security, particularly in vulnerability detection. However, existing benchmarks primarily focus on isolated, single-vulnerability samples or function-level classification, failing to reflect the complexity of real-world software where multiple interacting vulnerabilities often coexist within large files. Recent studies indicate that LLMs suffer from \"count bias\" and \"selection bias\" in multi-label tasks, yet this has not been rigorously quantified in the domain of code security. In this work, we introduce a comprehensive benchmark for Multi-Vulnerability Detection across four major languages: C, C++, Python, and JavaScript. We construct a dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, and 9) into long-context code samples (7.5k-10k tokens) sourced from CodeParrot. We evaluate five state-of-the-art LLMs, including GPT-4o-mini, Llama-3.3-70B, and the Qwen-2.5 series. Our results reveal a sharp degradation in performance as vulnerability density increases. While Llama-3.3-70B achieves near-perfect F1 scores (approximately 0.97) on single-vulnerability C tasks, performance drops by up to 40% in high-density settings. Notably, Python and JavaScript show distinct failure modes compared to C/C++, with models exhibiting severe \"under-counting\" (Recall dropping to less than 0.30) in complex Python files.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å­¤ç«‹çš„å•æ¼æ´(single-vulnerability)æ ·æœ¬ã€æ— æ³•åæ˜ çœŸå®è½¯ä»¶å¤æ‚æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ¶µç›– C, C++, Python å’Œ JavaScript çš„å¤šæ¼æ´æ£€æµ‹(Multi-Vulnerability Detection)ç»¼åˆåŸºå‡†ã€‚ç ”ç©¶è€…é€šè¿‡åœ¨é•¿ä¸Šä¸‹æ–‡ä»£ç ä¸­ç³»ç»Ÿæ³¨å…¥å—æ§æ•°é‡ï¼ˆ1, 3, 5, 9ä¸ªï¼‰çš„æ¼æ´ï¼Œæ„å»ºäº†åŒ…å« 40,000 ä¸ªæ–‡ä»¶çš„è§„æ¨¡åŒ–æ•°æ®é›†ã€‚é€šè¿‡å¯¹ GPT-4o-miniã€Llama-3.3-70B å’Œ Qwen-2.5 ç³»åˆ—å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶å‘ç°æ¨¡å‹æ€§èƒ½ä¼šéšæ¼æ´å¯†åº¦å¢åŠ è€Œæ˜¾è‘—ä¸‹é™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLlama-3.3-70B åœ¨é«˜å¯†åº¦ç¯å¢ƒä¸‹çš„ F1 åˆ†æ•°è·Œå¹…é«˜è¾¾ 40%ï¼Œä¸”æ¨¡å‹åœ¨å¤„ç† Python å’Œ JavaScript ç­‰è¯­è¨€æ—¶å­˜åœ¨ä¸¥é‡çš„â€œæ¼è®¡â€(under-counting)ç°è±¡ï¼Œéƒ¨åˆ†åœºæ™¯ä¸‹å¬å›ç‡(Recall)é™è‡³ 0.30 ä»¥ä¸‹ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2512.22306v1",
      "published_date": "2025-12-26 05:43:35 UTC",
      "updated_date": "2025-12-26 05:43:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:08:21.069727+00:00"
    },
    {
      "arxiv_id": "2601.09718v1",
      "title": "StatLLaMA: A multi-stage training framework for building a domain-optimized statistical language model",
      "title_zh": "StatLLaMAï¼šç”¨äºæ„å»ºé¢†åŸŸä¼˜åŒ–ç»Ÿè®¡å­¦è¯­è¨€æ¨¡å‹çš„å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶",
      "authors": [
        "Jing-Yi Zeng",
        "Guan-Hua Huang"
      ],
      "abstract": "This study investigates how to efficiently build a domain-specialized large language model (LLM) for statistics using the lightweight LLaMA-3.2-3B family as the foundation model (FM). We systematically compare three multi-stage training pipelines, starting from a base FM with no instruction-following capability, a base FM augmented with post-hoc instruction tuning, and an instruction-tuned FM with strong general reasoning abilities across continual pretraining, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) preference alignment, and downstream task adaptation. Results show that pipelines beginning with a base FM fail to develop meaningful statistical reasoning, even after extensive instruction tuning, SFT, or RLHF alignment. In contrast, starting from LLaMA-3.2-3B-Instruct enables effective domain specialization. A comprehensive evaluation of SFT variants reveals clear trade-offs between domain expertise and general reasoning ability. We further demonstrate that direct preference optimization provides stable and effective RLHF preference alignment. Finally, we show that downstream fine-tuning must be performed with extremely low intensity to avoid catastrophic forgetting in highly optimized models. The final model, StatLLaMA, achieves strong and balanced performance on benchmarks of mathematical reasoning, common-sense reasoning, and statistical expertise, offering a practical blueprint for developing resource-efficient statistical LLMs. The code is available at https://github.com/HuangDLab/StatLLaMA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†StatLLaMAï¼Œä¸€ä¸ªåŸºäºLLaMA-3.2-3Bè½»é‡åŒ–æ¨¡å‹ç³»åˆ—æ„å»ºçš„ç»Ÿè®¡å­¦é¢†åŸŸä¼˜åŒ–è¯­è¨€æ¨¡å‹å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ã€‚ç ”ç©¶ç³»ç»Ÿå¯¹æ¯”äº†ä»åŸºç¡€FMã€ç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„åŸºç¡€FMä»¥åŠå…·å¤‡å¼ºé€šç”¨æ¨ç†èƒ½åŠ›çš„æŒ‡ä»¤å¾®è°ƒFMå‡ºå‘çš„ä¸‰ç§è®­ç»ƒç®¡çº¿ï¼Œæ¶µç›–æŒç»­é¢„è®­ç»ƒ(Continual Pretraining)ã€æœ‰ç›‘ç£å¾®è°ƒ(SFT)ã€åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)å’Œä¸‹æ¸¸ä»»åŠ¡é€‚é…ç­‰é˜¶æ®µã€‚å®éªŒå‘ç°ä»åŸºç¡€FMå¼€å§‹çš„è®­ç»ƒéš¾ä»¥å»ºç«‹ç»Ÿè®¡æ¨ç†èƒ½åŠ›ï¼Œè€Œä»¥LLaMA-3.2-3B-Instructä¸ºèµ·ç‚¹åˆ™èƒ½æœ‰æ•ˆå®ç°é¢†åŸŸä¸“ä¸šåŒ–ã€‚è¯„ä¼°æ­ç¤ºäº†é¢†åŸŸä¸“é•¿ä¸é€šç”¨æ¨ç†èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡(Trade-offs)ï¼Œå¹¶è¯å®ç›´æ¥åå¥½ä¼˜åŒ–(Direct Preference Optimization)åœ¨åå¥½å¯¹é½ä¸­è¡¨ç°ç¨³å®šã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œä¸‹æ¸¸å¾®è°ƒå¿…é¡»ä»¥æä½å¼ºåº¦è¿›è¡Œä»¥è§„é¿ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)ã€‚æœ€ç»ˆæ¨¡å‹StatLLaMAåœ¨æ•°å­¦æ¨ç†ã€å¸¸è¯†æ¨ç†å’Œç»Ÿè®¡ä¸“ä¸šçŸ¥è¯†æµ‹è¯•ä¸­è¡¨ç°å‡è¡¡ï¼Œä¸ºå¼€å‘èµ„æºé«˜æ•ˆå‹ç»Ÿè®¡å¤§è¯­è¨€æ¨¡å‹(LLMs)æä¾›äº†å®è·µè“å›¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "31 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.09718v1",
      "published_date": "2025-12-26 05:16:07 UTC",
      "updated_date": "2025-12-26 05:16:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:07:55.603882+00:00"
    },
    {
      "arxiv_id": "2512.21871v1",
      "title": "Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?",
      "title_zh": "å¼¥åˆç‰ˆæƒé¸¿æ²Ÿï¼šå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ˜¯å¦è¯†åˆ«å¹¶å°Šé‡å—ç‰ˆæƒä¿æŠ¤çš„å†…å®¹ï¼Ÿ",
      "authors": [
        "Naen Xu",
        "Jinghuai Zhang",
        "Changjiang Li",
        "Hengyu An",
        "Chunyi Zhou",
        "Jun Wang",
        "Boyu Xu",
        "Yuyuan Li",
        "Tianyu Du",
        "Shouling Ji"
      ],
      "abstract": "Large vision-language models (LVLMs) have achieved remarkable advancements in multimodal reasoning tasks. However, their widespread accessibility raises critical concerns about potential copyright infringement. Will LVLMs accurately recognize and comply with copyright regulations when encountering copyrighted content (i.e., user input, retrieved documents) in the context? Failure to comply with copyright regulations may lead to serious legal and ethical consequences, particularly when LVLMs generate responses based on copyrighted materials (e.g., retrieved book experts, news reports). In this paper, we present a comprehensive evaluation of various LVLMs, examining how they handle copyrighted content -- such as book excerpts, news articles, music lyrics, and code documentation when they are presented as visual inputs. To systematically measure copyright compliance, we introduce a large-scale benchmark dataset comprising 50,000 multimodal query-content pairs designed to evaluate how effectively LVLMs handle queries that could lead to copyright infringement. Given that real-world copyrighted content may or may not include a copyright notice, the dataset includes query-content pairs in two distinct scenarios: with and without a copyright notice. For the former, we extensively cover four types of copyright notices to account for different cases. Our evaluation reveals that even state-of-the-art closed-source LVLMs exhibit significant deficiencies in recognizing and respecting the copyrighted content, even when presented with the copyright notice. To solve this limitation, we introduce a novel tool-augmented defense framework for copyright compliance, which reduces infringement risks in all scenarios. Our findings underscore the importance of developing copyright-aware LVLMs to ensure the responsible and lawful use of copyrighted content.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨å¤„ç†ä¹¦ç±æ‘˜å½•ã€æ–°é—»æ–‡ç« å’Œä»£ç æ–‡æ¡£ç­‰è§†è§‰è¾“å…¥æ—¶ï¼Œè¯†åˆ«å¹¶éµå®ˆç‰ˆæƒè§„èŒƒçš„èƒ½åŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåŒ…å«50,000ä¸ªå¤šæ¨¡æ€æŸ¥è¯¢-å†…å®¹å¯¹çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†æ¨¡å‹åœ¨æœ‰æ— ç‰ˆæƒå£°æ˜(copyright notice)æƒ…å†µä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä¾¿æ˜¯æœ€å…ˆè¿›çš„é—­æºLVLMsåœ¨è¯†åˆ«å’Œå°Šé‡ç‰ˆæƒæ–¹é¢ä¹Ÿè¡¨ç°å‡ºæ˜æ˜¾çš„ä¸è¶³ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆå¤„ç†æ½œåœ¨çš„ä¾µæƒé£é™©ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å·¥å…·å¢å¼ºé˜²å¾¡æ¡†æ¶(tool-augmented defense framework)ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½ä¸åŒåœºæ™¯ä¸‹çš„ç‰ˆæƒä¾µæƒé£é™©ã€‚è¯¥ç ”ç©¶æˆæœå¼ºè°ƒäº†å¼€å‘å…·æœ‰ç‰ˆæƒæ„è¯†(copyright-aware)çš„LVLMså¯¹ç¡®ä¿å¤šæ¨¡æ€å†…å®¹åˆæ³•ä½¿ç”¨çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "AAAI 2026 (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2512.21871v1",
      "published_date": "2025-12-26 05:09:55 UTC",
      "updated_date": "2025-12-26 05:09:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:09:37.219259+00:00"
    },
    {
      "arxiv_id": "2512.21866v1",
      "title": "Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation",
      "title_zh": "åŸºäºå±‚çº§åŒ–å¤šæºæ•°æ®é›†è’¸é¦çš„é‡‘èé¢†åŸŸå®‰å…¨ä¸å¯è§£é‡Šæ¬ºè¯ˆæ£€æµ‹",
      "authors": [
        "Yiming Qian",
        "Thorsten Neumann",
        "Xueyining Huang",
        "David Hardoon",
        "Fei Gao",
        "Yong Liu",
        "Siow Mong Rick Goh"
      ],
      "abstract": "We propose an explainable, privacy-preserving dataset distillation framework for collaborative financial fraud detection. A trained random forest is converted into transparent, axis-aligned rule regions (leaf hyperrectangles), and synthetic transactions are generated by uniformly sampling within each region. This produces a compact, auditable surrogate dataset that preserves local feature interactions without exposing sensitive original records. The rule regions also support explainability: aggregated rule statistics (for example, support and lift) describe global patterns, while assigning each case to its generating region gives concise human-readable rationales and calibrated uncertainty based on tree-vote disagreement.\n  On the IEEE-CIS fraud dataset (590k transactions across three institution-like clusters), distilled datasets reduce data volume by 85% to 93% (often under 15% of the original) while maintaining competitive precision and micro-F1, with only a modest AUC drop. Sharing and augmenting with synthesized data across institutions improves cross-cluster precision, recall, and AUC. Real vs. synthesized structure remains highly similar (over 93% by nearest-neighbor cosine analysis). Membership-inference attacks perform at chance level (about 0.50) when distinguishing training from hold-out records, suggesting low memorization risk. Removing high-uncertainty synthetic points using disagreement scores further boosts AUC (up to 0.687) and improves calibration. Sensitivity tests show weak dependence on the distillation ratio (AUC about 0.641 to 0.645 from 6% to 60%).\n  Overall, tree-region distillation enables trustworthy, deployable fraud analytics with interpretable global rules, per-case rationales with quantified uncertainty, and strong privacy properties suitable for multi-institution settings and regulatory audit.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é¢å‘åä½œå¼é‡‘èæ¬ºè¯ˆæ£€æµ‹çš„å¯è§£é‡Šä¸”ä¿æŠ¤éšç§çš„ dataset distillation æ¡†æ¶ï¼Œé€šè¿‡å°†è®­ç»ƒå¥½çš„ random forest è½¬æ¢ä¸ºé€æ˜çš„è½´å¯¹é½è§„åˆ™åŒºåŸŸï¼ˆrule regionsï¼‰æ¥ç”Ÿæˆåˆæˆäº¤æ˜“æ•°æ®ã€‚è¯¥æ¡†æ¶ç”Ÿæˆçš„ç´§å‡‘ä¸”å¯å®¡è®¡çš„æ›¿ä»£æ•°æ®é›†åœ¨ä¸æš´éœ²åŸå§‹æ•æ„Ÿè®°å½•çš„å‰æä¸‹ä¿ç•™äº†å±€éƒ¨ç‰¹å¾äº¤äº’ï¼Œå¹¶é€šè¿‡è§„åˆ™ç»Ÿè®¡ä¿¡æ¯æ”¯æŒå…¨å±€æ¨¡å¼çš„å¯è§£é‡Šæ€§ã€‚é€šè¿‡å°†æ¡ˆä¾‹åˆ†é…è‡³ç‰¹å®šçš„ç”ŸæˆåŒºåŸŸï¼Œç³»ç»Ÿèƒ½å¤Ÿæä¾›äººç±»å¯è¯»çš„æ¨ç†ä¾æ®ä»¥åŠåŸºäº tree-vote åˆ†æ­§çš„æ ¡å‡†ä¸ç¡®å®šæ€§ï¼ˆcalibrated uncertaintyï¼‰ã€‚åœ¨ IEEE-CIS æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘ 85% è‡³ 93% æ•°æ®é‡çš„åŒæ—¶ï¼Œä¾ç„¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„ precision å’Œ micro-F1ã€‚è·¨æœºæ„çš„åˆæˆæ•°æ®å…±äº«ä¸ä»…æå‡äº†è·¨é›†ç¾¤çš„æ£€æµ‹æ€§èƒ½ï¼Œä¸”æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆmembership-inference attacksï¼‰å®éªŒè¯æ˜å…¶å…·æœ‰æä½çš„éšç§æ³„éœ²é£é™©ã€‚æ­¤å¤–ï¼Œé€šè¿‡å‰”é™¤é«˜ä¸ç¡®å®šæ€§çš„åˆæˆæ ·æœ¬å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ– AUC è¡¨ç°å’Œæ ¡å‡†åº¦ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™ç§ tree-region è’¸é¦æŠ€æœ¯ä¸ºå¤šæœºæ„ç¯å¢ƒä¸‹çš„é‡‘èæ¬ºè¯ˆåˆ†ææä¾›äº†å…¼å…·å¯è§£é‡Šæ€§ã€éšç§å®‰å…¨æ€§å’Œç›‘ç®¡åˆè§„æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.21866v1",
      "published_date": "2025-12-26 05:00:35 UTC",
      "updated_date": "2025-12-26 05:00:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:09:42.479663+00:00"
    },
    {
      "arxiv_id": "2512.21861v1",
      "title": "Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening",
      "title_zh": "å…¼é¡¾å‡†ç¡®æ€§ä¸æ•ˆç‡ï¼šç”¨äºç³–å°¿ç—…è§†ç½‘è†œç—…å˜ç­›æŸ¥çš„ CNN èåˆæ¨¡å‹",
      "authors": [
        "Md Rafid Islam",
        "Rafsan Jany",
        "Akib Ahmed",
        "Mohammad Ashrafuzzaman Khan"
      ],
      "abstract": "Diabetic retinopathy (DR) remains a leading cause of preventable blindness, yet large-scale screening is constrained by limited specialist availability and variable image quality across devices and populations. This work investigates whether feature-level fusion of complementary convolutional neural network (CNN) backbones can deliver accurate and efficient binary DR screening on globally sourced fundus images. Using 11,156 images pooled from five public datasets (APTOS, EyePACS, IDRiD, Messidor, and ODIR), we frame DR detection as a binary classification task and compare three pretrained models (ResNet50, EfficientNet-B0, and DenseNet121) against pairwise and tri-fusion variants. Across five independent runs, fusion consistently outperforms single backbones. The EfficientNet-B0 + DenseNet121 (Eff+Den) fusion model achieves the best overall mean performance (accuracy: 82.89\\%) with balanced class-wise F1-scores for normal (83.60\\%) and diabetic (82.60\\%) cases. While the tri-fusion is competitive, it incurs a substantially higher computational cost. Inference profiling highlights a practical trade-off: EfficientNet-B0 is the fastest (approximately 1.16 ms/image at batch size 1000), whereas the Eff+Den fusion offers a favorable accuracy--latency balance. These findings indicate that lightweight feature fusion can enhance generalization across heterogeneous datasets, supporting scalable binary DR screening workflows where both accuracy and throughput are critical.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡ç‰¹å¾çº§èåˆ (feature-level fusion) äº’è¡¥çš„å·ç§¯ç¥ç»ç½‘ç»œ (CNN) ä¸»å¹²ç½‘ç»œï¼Œå®ç°å‡†ç¡®ä¸”é«˜æ•ˆçš„ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ (Diabetic retinopathy, DR) äºŒå…ƒç­›æŸ¥ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨æ¥è‡ªäº”ä¸ªå…¬å¼€æ•°æ®é›†çš„ 11,156 å¼ çœ¼åº•å›¾åƒï¼Œå¯¹æ¯”äº† ResNet50ã€EfficientNet-B0 å’Œ DenseNet121 åŠå…¶èåˆå˜ä½“çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œèåˆæ¨¡å‹æ€§èƒ½å§‹ç»ˆä¼˜äºå•ä¸€ä¸»å¹²ç½‘ç»œï¼Œå…¶ä¸­ EfficientNet-B0 + DenseNet121 (Eff+Den) èåˆæ¨¡å‹å®ç°äº† 82.89% çš„æœ€ä½³å¹³å‡å‡†ç¡®ç‡ï¼Œå¹¶åœ¨è¯†åˆ«æ­£å¸¸ä¸æ‚£ç—…æ¡ˆä¾‹ä¸­è¡¨ç°å‡è¡¡ã€‚å°½ç®¡ä¸‰å…ƒèåˆæ¨¡å‹æ€§èƒ½ä¼˜å¼‚ï¼Œä½†å…¶è®¡ç®—æˆæœ¬æ›´é«˜ï¼Œè€Œ Eff+Den æ–¹æ¡ˆåœ¨å‡†ç¡®æ€§ä¸æ¨ç†å»¶è¿Ÿ (latency) ä¹‹é—´è¾¾æˆäº†æ›´ä¼˜å¹³è¡¡ã€‚è¯¥ç ”ç©¶è¯æ˜è½»é‡çº§ç‰¹å¾èåˆèƒ½å¢å¼ºæ¨¡å‹åœ¨å¼‚è´¨æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¯¹å‡†ç¡®ç‡å’Œååé‡ (throughput) å‡æœ‰é«˜è¦æ±‚çš„å¤§è§„æ¨¡ DR ç­›æŸ¥æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.21861v1",
      "published_date": "2025-12-26 04:54:43 UTC",
      "updated_date": "2025-12-26 04:54:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:10:04.621076+00:00"
    },
    {
      "arxiv_id": "2512.21853v1",
      "title": "MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction",
      "title_zh": "MoonBotï¼šé¢å‘æœˆçƒåŸºåœ°å»ºè®¾çš„æ¨¡å—åŒ–æŒ‰éœ€å¯é‡æ„æœºå™¨äºº",
      "authors": [
        "Kentaro Uno",
        "Elian Neppel",
        "Gustavo H. Diaz",
        "Ashutosh Mishra",
        "Shamistan Karimov",
        "A. Sejal Jain",
        "Ayesha Habib",
        "Pascal Pama",
        "Hazal Gozbasi",
        "Shreya Santra",
        "Kazuya Yoshida"
      ],
      "abstract": "The allure of lunar surface exploration and development has recently captured widespread global attention. Robots have proved to be indispensable for exploring uncharted terrains, uncovering and leveraging local resources, and facilitating the construction of future human habitats. In this article, we introduce the modular and on-demand reconfigurable robot (MoonBot), a modular and reconfigurable robotic system engineered to maximize functionality while operating within the stringent mass constraints of lunar payloads and adapting to varying environmental conditions and task requirements. This article details the design and development of MoonBot and presents a preliminary field demonstration that validates the proof of concept through the execution of milestone tasks simulating the establishment of lunar infrastructure. These tasks include essential civil engineering operations, infrastructural component transportation and deployment, and assistive operations with inflatable modules. Furthermore, we systematically summarize the lessons learned during testing, focusing on the connector design and providing valuable insights for the advancement of modular robotic systems in future lunar missions.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† MoonBotï¼Œä¸€ç§ä¸“ä¸ºæœˆçƒåŸºåœ°å»ºè®¾è®¾è®¡çš„æ¨¡å—åŒ–ä¸”å¯æŒ‰éœ€é‡æ„æœºå™¨äººç³»ç»Ÿ (Modular and On-Demand Reconfigurable Robot)ï¼Œæ—¨åœ¨æ»¡è¶³æœˆçƒè½½è·ä¸¥æ ¼çš„è´¨é‡é™åˆ¶å¹¶å®ç°åŠŸèƒ½æœ€å¤§åŒ–ã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº† MoonBot çš„ç¡¬ä»¶è®¾è®¡ä¸å¼€å‘è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡æ¨¡æ‹ŸæœˆçƒåŸºç¡€è®¾æ–½å»ºè®¾çš„ç°åœºæ¼”ç¤ºéªŒè¯äº†å…¶æ‰§è¡ŒåœŸæœ¨å·¥ç¨‹ã€ç»„ä»¶è¿è¾“éƒ¨ç½²åŠå……æ°”å¼æ¨¡å— (Inflatable Modules) è¾…åŠ©ä½œä¸šçš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¯å®äº†è¯¥æ¨¡å—åŒ–æ–¹æ¡ˆåœ¨é€‚åº”å¤šå˜ç¯å¢ƒå’Œä»»åŠ¡éœ€æ±‚æ–¹é¢çš„æœ‰æ•ˆæ€§ä¸çµæ´»æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ç³»ç»Ÿæ€»ç»“äº†æµ‹è¯•ä¸­å…³äºè¿æ¥å™¨è®¾è®¡ (Connector Design) çš„å…³é”®ç»éªŒæ•™è®­ã€‚è¿™äº›æˆæœä¸ºæœªæ¥æœˆçƒæ¢æµ‹åŠé•¿æœŸé©»ç•™ä»»åŠ¡ä¸­æ¨¡å—åŒ–æœºå™¨äººç³»ç»Ÿçš„æ¼”è¿›æä¾›äº†å®è´µçš„å®è·µè§è§£ä¸æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "This is the authors' version of a paper accepted for publication in IEEE Transactions on Field Robotics, (c) IEEE. The final published version is available at https://doi.org/10.1109/TFR.2025.3624346",
      "pdf_url": "https://arxiv.org/pdf/2512.21853v1",
      "published_date": "2025-12-26 04:22:28 UTC",
      "updated_date": "2025-12-26 04:22:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:09:47.483384+00:00"
    },
    {
      "arxiv_id": "2512.21852v2",
      "title": "A Comedy of Estimators: On KL Regularization in RL Training of LLMs",
      "title_zh": "ä¼°è®¡é‡çš„å–œå‰§ï¼šè®ºå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„ KL æ­£åˆ™åŒ–",
      "authors": [
        "Vedant Shah",
        "Johan Obando-Ceron",
        "Vineet Jain",
        "Brian Bartoldson",
        "Bhavya Kailkhura",
        "Sarthak Mittal",
        "Glen Berseth",
        "Pablo Samuel Castro",
        "Yoshua Bengio",
        "Nikolay Malkin",
        "Moksh Jain",
        "Siddarth Venkatraman",
        "Aaron Courville"
      ],
      "abstract": "The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \\texttt{Qwen2.5-7B}, \\texttt{Llama-3.1-8B-Instruct} and \\texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¼ºåŒ–å­¦ä¹ (RL)è®­ç»ƒä¸­åå‘KLæ•£åº¦(Kullback-Leibler divergence)æ­£åˆ™åŒ–é¡¹çš„è¯„ä¼°å™¨é€‰æ‹©é—®é¢˜ã€‚å°½ç®¡è¯¥æ­£åˆ™åŒ–é¡¹è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†ç°æœ‰å®è·µåœ¨è®¡ç®—KL estimatoræ—¶å¾€å¾€å­˜åœ¨æ¢¯åº¦åç½®(gradient bias)ï¼Œå¯¼è‡´ä¼˜åŒ–ç›®æ ‡ä¸å®é™…å®ç°ä¹‹é—´äº§ç”Ÿåå·®ã€‚é€šè¿‡å¯¹å¤šç§è¯„ä¼°å™¨é…ç½®çš„æ¢¯åº¦è¿›è¡Œç†è®ºåˆ†æï¼Œå¹¶ç»“åˆQwen2.5-7Bå’ŒLlama-3.1-8B-Instructç­‰æ¨¡å‹çš„å®è¯è¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†ä¸åŒè®¾è®¡é€‰æ‹©å¯¹è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŒç­–(on-policy)è®¾ç½®ä¸‹ï¼Œå…·æœ‰åç½®æ¢¯åº¦çš„é…ç½®ä¼šå¼•å‘è®­ç»ƒä¸ç¨³å®šï¼Œè€Œé‡‡ç”¨æ— åæ¢¯åº¦çš„é…ç½®åˆ™èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨é¢†åŸŸå†…å¤–çš„ä»»åŠ¡è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯å®åœ¨å¼‚æ­¥ç­‰å¼‚ç­–(off-policy)è®¾ç½®ä¸­ï¼ŒKLæ­£åˆ™åŒ–å¯¹äºç¨³å®šRLè®­ç»ƒåŒæ ·å…·æœ‰é‡è¦ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.21852v2",
      "published_date": "2025-12-26 04:20:58 UTC",
      "updated_date": "2026-01-06 15:07:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:09:54.356748+00:00"
    },
    {
      "arxiv_id": "2512.22303v1",
      "title": "Attack-Aware Deepfake Detection under Counter-Forensic Manipulations",
      "title_zh": "é¢å‘åå–è¯æ“çºµçš„æ”»å‡»æ„ŸçŸ¥æ·±åº¦ä¼ªé€ æ£€æµ‹",
      "authors": [
        "Noor Fatima",
        "Hasan Faraz Khan",
        "Muzammil Behzad"
      ],
      "abstract": "This work presents an attack-aware deepfake and image-forensics detector designed for robustness, well-calibrated probabilities, and transparent evidence under realistic deployment conditions. The method combines red-team training with randomized test-time defense in a two-stream architecture, where one stream encodes semantic content using a pretrained backbone and the other extracts forensic residuals, fused via a lightweight residual adapter for classification, while a shallow Feature Pyramid Network style head produces tamper heatmaps under weak supervision. Red-team training applies worst-of-K counter-forensics per batch, including JPEG realign and recompress, resampling warps, denoise-to-regrain operations, seam smoothing, small color and gamma shifts, and social-app transcodes, while test-time defense injects low-cost jitters such as resize and crop phase changes, mild gamma variation, and JPEG phase shifts with aggregated predictions. Heatmaps are guided to concentrate within face regions using face-box masks without strict pixel-level annotations. Evaluation on existing benchmarks, including standard deepfake datasets and a surveillance-style split with low light and heavy compression, reports clean and attacked performance, AUC, worst-case accuracy, reliability, abstention quality, and weak-localization scores. Results demonstrate near-perfect ranking across attacks, low calibration error, minimal abstention risk, and controlled degradation under regrain, establishing a modular, data-efficient, and practically deployable baseline for attack-aware detection with calibrated probabilities and actionable heatmaps.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ„ŸçŸ¥æ”»å‡»(Attack-aware)çš„ Deepfake å’Œå›¾åƒå–è¯æ£€æµ‹å™¨ï¼Œæ—¨åœ¨çœŸå®éƒ¨ç½²ç¯å¢ƒä¸‹æä¾›é²æ£’æ€§ã€æ ¡å‡†æ¦‚ç‡(Calibrated probabilities)å’Œé€æ˜çš„è¯æ®æ”¯æŒã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŒæµæ¶æ„(Two-stream architecture)ï¼Œå°†ç”¨äºç¼–ç è¯­ä¹‰å†…å®¹çš„é¢„è®­ç»ƒä¸»å¹²ç½‘ç»œä¸æå–å–è¯æ®‹å·®çš„æµç›¸ç»“åˆï¼Œå¹¶é€šè¿‡è½»é‡çº§æ®‹å·®é€‚é…å™¨(Residual adapter)è¿›è¡Œèåˆåˆ†ç±»ã€‚ç³»ç»Ÿåˆ©ç”¨æµ…å±‚ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ(Feature Pyramid Network)é£æ ¼çš„å¤´éƒ¨ï¼Œåœ¨å¼±ç›‘ç£ä¸‹ç”Ÿæˆå®šä½ç¯¡æ”¹åŒºåŸŸçš„çƒ­å›¾(Tamper heatmaps)ã€‚è®­ç»ƒè¿‡ç¨‹å¼•å…¥äº†çº¢é˜Ÿè®­ç»ƒ(Red-team training)ä»¥æ¨¡æ‹Ÿ JPEG é‡å‹ç¼©ã€é‡é‡‡æ ·å’Œå»å™ªé‡åŠ å™ª(Denoise-to-regrain)ç­‰å¤šç§åå–è¯æ“çºµï¼ŒåŒæ—¶åœ¨æµ‹è¯•é˜¶æ®µé€šè¿‡éšæœºæŠ–åŠ¨å’Œèšåˆé¢„æµ‹å¢å¼ºé˜²å¾¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ ‡å‡†æ•°æ®é›†åŠå¤æ‚çš„ç›‘æ§åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºæé«˜çš„ AUC å’Œä½æ ¡å‡†è¯¯å·®ï¼Œæ˜¾è‘—é™ä½äº†æ£€æµ‹å¼ƒæƒé£é™©ã€‚è¿™é¡¹å·¥ä½œä¸ºå…·å¤‡å®é™…éƒ¨ç½²èƒ½åŠ›çš„æ„ŸçŸ¥æ”»å‡»æ£€æµ‹æŠ€æœ¯å»ºç«‹äº†ä¸€ä¸ªæ¨¡å—åŒ–ä¸”æ•°æ®é«˜æ•ˆçš„åŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22303v1",
      "published_date": "2025-12-26 04:05:52 UTC",
      "updated_date": "2025-12-26 04:05:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:10:31.668465+00:00"
    },
    {
      "arxiv_id": "2512.21849v1",
      "title": "HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs",
      "title_zh": "HeartBenchï¼šæ¢ç©¶å¤§è¯­è¨€æ¨¡å‹æ‹ŸäººåŒ–æ™ºèƒ½çš„æ ¸å¿ƒç»´åº¦",
      "authors": [
        "Jiaxin Liu",
        "Peiyi Tu",
        "Wenyu Chen",
        "Yihong Zhuang",
        "Xinxia Ling",
        "Anji Zhou",
        "Chenxi Wang",
        "Zhuo Han",
        "Zhengkai Yang",
        "Junbo Zhao",
        "Zenan Huang",
        "Yuanyuan Wang"
      ],
      "abstract": "While Large Language Models (LLMs) have achieved remarkable success in cognitive and reasoning benchmarks, they exhibit a persistent deficit in anthropomorphic intelligence-the capacity to navigate complex social, emotional, and ethical nuances. This gap is particularly acute in the Chinese linguistic and cultural context, where a lack of specialized evaluation frameworks and high-quality socio-emotional data impedes progress. To address these limitations, we present HeartBench, a framework designed to evaluate the integrated emotional, cultural, and ethical dimensions of Chinese LLMs. Grounded in authentic psychological counseling scenarios and developed in collaboration with clinical experts, the benchmark is structured around a theory-driven taxonomy comprising five primary dimensions and 15 secondary capabilities. We implement a case-specific, rubric-based methodology that translates abstract human-like traits into granular, measurable criteria through a ``reasoning-before-scoring'' evaluation protocol. Our assessment of 13 state-of-the-art LLMs indicates a substantial performance ceiling: even leading models achieve only 60% of the expert-defined ideal score. Furthermore, analysis using a difficulty-stratified ``Hard Set'' reveals a significant performance decay in scenarios involving subtle emotional subtexts and complex ethical trade-offs. HeartBench establishes a standardized metric for anthropomorphic AI evaluation and provides a methodological blueprint for constructing high-quality, human-aligned training data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†ç¤¾äº¤ã€æƒ…æ„Ÿå’Œä¼¦ç†ç­‰ç»†å¾®å·®åˆ«ï¼ˆå³æ‹ŸäººåŒ–æ™ºèƒ½ Anthropomorphic Intelligenceï¼‰æ–¹é¢çš„çŸ­æ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­æ–‡è¯­å¢ƒä¸‹è¯„ä¼°ä½“ç³»ç¼ºå¤±çš„ç°çŠ¶ï¼Œæå‡ºäº† HeartBench è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç«‹è¶³äºçœŸå®çš„å¿ƒç†å’¨è¯¢åœºæ™¯å¹¶ä¸ä¸´åºŠä¸“å®¶æ·±åº¦åä½œï¼Œæ„å»ºäº†æ¶µç›–5ä¸ªä¸€çº§ç»´åº¦å’Œ15ä¸ªäºŒçº§èƒ½åŠ›çš„ç†è®ºé©±åŠ¨åˆ†ç±»ä½“ç³»ã€‚HeartBench å®æ–½äº†ä¸€å¥—åŸºäºé‡è¡¨(Rubric-based)çš„è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡â€œå…ˆæ¨ç†åè¯„åˆ†â€(Reasoning-before-scoring)çš„åè®®ï¼Œå°†æŠ½è±¡çš„æ‹ŸäººåŒ–ç‰¹è´¨è½¬åŒ–ä¸ºå¯ç²¾ç¡®æµ‹é‡çš„æ ‡å‡†ã€‚å¯¹13æ¬¾ä¸»æµ LLMs çš„è¯„æµ‹ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹æ€§èƒ½æ™®éå—é™ï¼Œå³ä½¿æ˜¯é¢†å…ˆæ¨¡å‹ä¹Ÿä»…è¾¾åˆ°ä¸“å®¶ç†æƒ³è¯„åˆ†çš„60%ã€‚é€šè¿‡éš¾åº¦åˆ†å±‚çš„â€œå›°éš¾é›†â€(Hard Set)åˆ†æè¿›ä¸€æ­¥å‘ç°ï¼Œæ¨¡å‹åœ¨åº”å¯¹éšæ™¦æƒ…æ„Ÿæš—ç¤ºå’Œå¤æ‚ä¼¦ç†æƒè¡¡æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„æ€§èƒ½è¡°å‡ã€‚è¯¥ç ”ç©¶ä¸ºæ‹ŸäººåŒ– AI è¯„ä¼°å»ºç«‹äº†æ ‡å‡†åŒ–åº¦é‡ï¼Œå¹¶ä¸ºç”Ÿæˆé«˜è´¨é‡ã€äººç±»å¯¹é½(Human-aligned)çš„è®­ç»ƒæ•°æ®æä¾›äº†é‡è¦çš„æ–¹æ³•è®ºå‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.21849v1",
      "published_date": "2025-12-26 03:54:56 UTC",
      "updated_date": "2025-12-26 03:54:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:10:41.190005+00:00"
    },
    {
      "arxiv_id": "2512.23743v1",
      "title": "Hybrid-Code: A Privacy-Preserving, Redundant Multi-Agent Framework for Reliable Local Clinical Coding",
      "title_zh": "Hybrid-Codeï¼šé¢å‘å¯é æœ¬åœ°ä¸´åºŠç¼–ç çš„éšç§ä¿æŠ¤å‹å†—ä½™å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Yunguo Yu"
      ],
      "abstract": "Clinical coding automation using cloud-based Large Language Models (LLMs) poses privacy risks and latency bottlenecks, rendering them unsuitable for on-premise healthcare deployment. We introduce Hybrid-Code, a hybrid neuro-symbolic multi-agent framework for local clinical coding that ensures production reliability through redundancy and verification. Our system comprises two agents: a Coder that attempts language model-based semantic reasoning using BioMistral-7B but falls back to deterministic keyword matching when model output is unreliable, ensuring pipeline completion; and an Auditor that verifies codes against a 257-code knowledge base and clinical evidence. Evaluating on 1,000 MIMIC-III discharge summaries, we demonstrate no hallucinated codes among accepted outputs within the knowledge base, 24.47% verification rate, and 34.11% coverage (95% CI: 31.2%--37.0%) with 86%+ language model utilization. The Auditor filtered invalid format codes and provided evidence-based quality control (75.53% rejection rate) while ensuring no patient data leaves the hospital firewall. The hybrid architecture -- combining language model semantic understanding (when successful), deterministic fallback (when the model fails), and symbolic verification (always active) -- ensures both reliability and privacy preservation, addressing critical barriers to AI adoption in healthcare. Our key finding is that reliability through redundancy is more valuable than pure model performance in production healthcare systems, where system failures are unacceptable.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Hybrid-Codeï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æœ¬åœ°ä¸´åºŠç¼–ç  (Clinical Coding) çš„éšç§ä¿æŠ¤å‹å†—ä½™å¤šæ™ºèƒ½ä½“æ··åˆç¥ç»ç¬¦å· (hybrid neuro-symbolic) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³äº‘ç«¯å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åŒ»ç–—éƒ¨ç½²ä¸­çš„éšç§æ³„éœ²ä¸å»¶è¿Ÿé—®é¢˜ã€‚è¯¥ç³»ç»ŸåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ™ºèƒ½ä½“ï¼šCoder åˆ©ç”¨ BioMistral-7B è¿›è¡Œè¯­ä¹‰æ¨ç†ï¼Œå¹¶åœ¨æ¨¡å‹å¤±æ•ˆæ—¶é€šè¿‡ç¡®å®šæ€§å…³é”®è¯åŒ¹é… (keyword matching) è¿›è¡Œå…œåº•ä»¥ç¡®ä¿æµç¨‹å®Œå¤‡ï¼›Auditor åˆ™åŸºäº 257 ä¸ªä»£ç çš„çŸ¥è¯†åº“å’Œä¸´åºŠè¯æ®å¯¹ç»“æœè¿›è¡Œç¬¦å·åŒ–éªŒè¯ (symbolic verification)ã€‚åœ¨ 1,000 ä»½ MIMIC-III å‡ºé™¢æ‘˜è¦çš„è¯„ä¼°ä¸­ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒ 86% ä»¥ä¸Šæ¨¡å‹åˆ©ç”¨ç‡çš„åŒæ—¶ï¼Œç¡®ä¿äº†æ¥å—è¾“å‡ºä¸­çš„é›¶å¹»è§‰ä»£ç  (hallucinated codes)ï¼Œå¹¶å®ç°äº† 34.11% çš„è¦†ç›–ç‡ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒï¼Œåœ¨åŒ»ç–—ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œé€šè¿‡å†—ä½™è®¾è®¡è·å¾—çš„ç³»ç»Ÿå¯é æ€§æ¯”å•çº¯æå‡æ¨¡å‹æ€§èƒ½æ›´å…·ä»·å€¼ï¼Œä¸º AI åœ¨ä¸´åºŠåœºæ™¯çš„æœ¬åœ°åŒ–åº”ç”¨æä¾›äº†å®‰å…¨å¯é çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "18 pages, 1 figure, original research paper",
      "pdf_url": "https://arxiv.org/pdf/2512.23743v1",
      "published_date": "2025-12-26 02:27:36 UTC",
      "updated_date": "2025-12-26 02:27:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:10:28.970321+00:00"
    },
    {
      "arxiv_id": "2512.23742v2",
      "title": "AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization",
      "title_zh": "AgenticTCADï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ– TCAD ä»£ç ç”Ÿæˆä¸å™¨ä»¶ä¼˜åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Guangxi Fan",
        "Tianliang Ma",
        "Xuguang Sun",
        "Xun Wang",
        "Kain Lu Low",
        "Leilai Shao"
      ],
      "abstract": "With the continued scaling of advanced technology nodes, the design-technology co-optimization (DTCO) paradigm has become increasingly critical, rendering efficient device design and optimization essential. In the domain of TCAD simulation, however, the scarcity of open-source resources hinders language models from generating valid TCAD code. To overcome this limitation, we construct an open-source TCAD dataset curated by experts and fine-tune a domain-specific model for TCAD code generation. Building on this foundation, we propose AgenticTCAD, a natural language - driven multi-agent framework that enables end-to-end automated device design and optimization. Validation on a 2 nm nanosheet FET (NS-FET) design shows that AgenticTCAD achieves the International Roadmap for Devices and Systems (IRDS)-2024 device specifications within 4.2 hours, whereas human experts required 7.1 days with commercial tools.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…ˆè¿›æŠ€æœ¯èŠ‚ç‚¹ä¸‹è®¾è®¡-æŠ€æœ¯ååŒä¼˜åŒ–(DTCO)çš„éœ€æ±‚ï¼Œæå‡ºäº†AgenticTCADï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³TCADä»¿çœŸé¢†åŸŸå› å¼€æºèµ„æºåŒ®ä¹å¯¼è‡´çš„ä»£ç ç”Ÿæˆéš¾é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆæ„å»ºäº†ä¸“å®¶ç­–åˆ’çš„å¼€æºTCADæ•°æ®é›†ï¼Œå¹¶ä»¥æ­¤å¾®è°ƒå‡ºé¢†åŸŸç‰¹å®šçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œéšåé€šè¿‡è‡ªç„¶è¯­è¨€é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“åä½œå®ç°ç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–å™¨ä»¶è®¾è®¡ä¸ä¼˜åŒ–ã€‚åœ¨2 nm Nanosheet FET (NS-FET)å™¨ä»¶è®¾è®¡çš„éªŒè¯ä¸­ï¼ŒAgenticTCADä»…éœ€4.2å°æ—¶å³å¯è¾¾åˆ°å›½é™…å™¨ä»¶ä¸ç³»ç»Ÿè·¯çº¿å›¾(IRDS)-2024çš„è§„æ ¼è¦æ±‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»ä¸“å®¶ä½¿ç”¨å•†ä¸šå·¥å…·å®Œæˆç›¸åŒä»»åŠ¡é€šå¸¸éœ€è¦7.1å¤©ï¼Œæ˜¾è‘—æå‡äº†åŠå¯¼ä½“å™¨ä»¶çš„ç ”å‘æ•ˆç‡ã€‚è¯¥æˆæœå±•ç¤ºäº†ç”Ÿæˆå¼AIåœ¨åŠ é€ŸåŠå¯¼ä½“å·¥è‰ºå¼€å‘å’Œå™¨ä»¶å»ºæ¨¡æ–¹é¢çš„å·¨å¤§åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by DATE 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.23742v2",
      "published_date": "2025-12-26 01:34:08 UTC",
      "updated_date": "2026-01-13 06:50:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:10:52.824524+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 69,
  "processed_papers_count": 69,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T20:11:41.326134+00:00"
}
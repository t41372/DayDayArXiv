[
  {
    "arxiv_id": "2408.16945v3",
    "title": "Different Victims, Same Layout: Email Visual Similarity Detection for Enhanced Email Protection",
    "authors": [
      "Sachin Shukla",
      "Omid Mirzaei"
    ],
    "abstract": "In the pursuit of an effective spam detection system, the focus has often\nbeen on identifying known spam patterns either through rule-based detection\nsystems or machine learning (ML) solutions that rely on keywords. However, both\nsystems are susceptible to evasion techniques and zero-day attacks that can be\nachieved at low cost. Therefore, an email that bypassed the defense system once\ncan do it again in the following days, even though rules are updated or the ML\nmodels are retrained. The recurrence of failures to detect emails that exhibit\nlayout similarities to previously undetected spam is concerning for customers\nand can erode their trust in a company. Our observations show that threat\nactors reuse email kits extensively and can bypass detection with little\neffort, for example, by making changes to the content of emails. In this work,\nwe propose an email visual similarity detection approach, named Pisco, to\nimprove the detection capabilities of an email threat defense system. We apply\nour proof of concept to some real-world samples received from different\nsources. Our results show that email kits are being reused extensively and\nvisually similar emails are sent to our customers at various time intervals.\nTherefore, this method could be very helpful in situations where detection\nengines that rely on textual features and keywords are bypassed, an occurrence\nour observations show happens frequently.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "To be published in the proceedings of the ACM Conference on Computer\n  and Communications Security (ACM CCS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.16945v3",
    "published_date": "2024-08-29 23:51:51 UTC",
    "updated_date": "2024-09-04 14:25:47 UTC"
  },
  {
    "arxiv_id": "2408.16942v1",
    "title": "A longitudinal sentiment analysis of Sinophobia during COVID-19 using large language models",
    "authors": [
      "Chen Wang",
      "Rohitash Chandra"
    ],
    "abstract": "The COVID-19 pandemic has exacerbated xenophobia, particularly Sinophobia,\nleading to widespread discrimination against individuals of Chinese descent.\nLarge language models (LLMs) are pre-trained deep learning models used for\nnatural language processing (NLP) tasks. The ability of LLMs to understand and\ngenerate human-like text makes them particularly useful for analysing social\nmedia data to detect and evaluate sentiments. We present a sentiment analysis\nframework utilising LLMs for longitudinal sentiment analysis of the Sinophobic\nsentiments expressed in X (Twitter) during the COVID-19 pandemic. The results\nshow a significant correlation between the spikes in Sinophobic tweets,\nSinophobic sentiments and surges in COVID-19 cases, revealing that the\nevolution of the pandemic influenced public sentiment and the prevalence of\nSinophobic discourse. Furthermore, the sentiment analysis revealed a\npredominant presence of negative sentiments, such as annoyance and denial,\nwhich underscores the impact of political narratives and misinformation shaping\npublic opinion. The lack of empathetic sentiment which was present in previous\nstudies related to COVID-19 highlights the way the political narratives in\nmedia viewed the pandemic and how it blamed the Chinese community. Our study\nhighlights the importance of transparent communication in mitigating xenophobic\nsentiments during global crises.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16942v1",
    "published_date": "2024-08-29 23:39:11 UTC",
    "updated_date": "2024-08-29 23:39:11 UTC"
  },
  {
    "arxiv_id": "2409.02114v1",
    "title": "Tiny-Toxic-Detector: A compact transformer-based model for toxic content detection",
    "authors": [
      "Michiel Kamphuis"
    ],
    "abstract": "This paper presents Tiny-toxic-detector, a compact transformer-based model\ndesigned for toxic content detection. Despite having only 2.1 million\nparameters, Tiny-toxic-detector achieves competitive performance on benchmark\ndatasets, with 90.97% accuracy on ToxiGen and 86.98% accuracy on the Jigsaw\ndataset, rivaling models over 50 times its size. This efficiency enables\ndeployment in resource-constrained environments, addressing the need for\neffective content moderation tools that balance performance with computational\nefficiency. The model architecture features 4 transformer encoder layers, each\nwith 2 attention heads, an embedding dimension of 64, and a feedforward\ndimension of 128. Trained on both public and private datasets,\nTiny-toxic-detector demonstrates the potential of efficient, task-specific\nmodels for addressing online toxicity. The paper covers the model architecture,\ntraining process, performance benchmarks, and limitations, underscoring its\nsuitability for applications such as social media monitoring and content\nmoderation. By achieving results comparable to much larger models while\nsignificantly reducing computational demands, Tiny-toxic-detector represents\nprogress toward more sustainable and scalable AI-driven content moderation\nsolutions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.02114v1",
    "published_date": "2024-08-29 22:31:38 UTC",
    "updated_date": "2024-08-29 22:31:38 UTC"
  },
  {
    "arxiv_id": "2408.16932v1",
    "title": "Event Extraction for Portuguese: A QA-driven Approach using ACE-2005",
    "authors": [
      "Luís Filipe Cunha",
      "Ricardo Campos",
      "Alípio Jorge"
    ],
    "abstract": "Event extraction is an Information Retrieval task that commonly consists of\nidentifying the central word for the event (trigger) and the event's arguments.\nThis task has been extensively studied for English but lags behind for\nPortuguese, partly due to the lack of task-specific annotated corpora. This\npaper proposes a framework in which two separated BERT-based models were\nfine-tuned to identify and classify events in Portuguese documents. We\ndecompose this task into two sub-tasks. Firstly, we use a token classification\nmodel to detect event triggers. To extract event arguments, we train a Question\nAnswering model that queries the triggers about their corresponding event\nargument roles. Given the lack of event annotated corpora in Portuguese, we\ntranslated the original version of the ACE-2005 dataset (a reference in the\nfield) into Portuguese, producing a new corpus for Portuguese event extraction.\nTo accomplish this, we developed an automatic translation pipeline. Our\nframework obtains F1 marks of 64.4 for trigger classification and 46.7 for\nargument classification setting, thus a new state-of-the-art reference for\nthese tasks in Portuguese.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16932v1",
    "published_date": "2024-08-29 22:14:21 UTC",
    "updated_date": "2024-08-29 22:14:21 UTC"
  },
  {
    "arxiv_id": "2408.16928v1",
    "title": "ACE-2005-PT: Corpus for Event Extraction in Portuguese",
    "authors": [
      "Luís Filipe Cunha",
      "Purificação Silvano",
      "Ricardo Campos",
      "Alípio Jorge"
    ],
    "abstract": "Event extraction is an NLP task that commonly involves identifying the\ncentral word (trigger) for an event and its associated arguments in text.\nACE-2005 is widely recognised as the standard corpus in this field. While other\ncorpora, like PropBank, primarily focus on annotating predicate-argument\nstructure, ACE-2005 provides comprehensive information about the overall event\nstructure and semantics. However, its limited language coverage restricts its\nusability. This paper introduces ACE-2005-PT, a corpus created by translating\nACE-2005 into Portuguese, with European and Brazilian variants. To speed up the\nprocess of obtaining ACE-2005-PT, we rely on automatic translators. This,\nhowever, poses some challenges related to automatically identifying the correct\nalignments between multi-word annotations in the original text and in the\ncorresponding translated sentence. To achieve this, we developed an alignment\npipeline that incorporates several alignment techniques: lemmatization, fuzzy\nmatching, synonym matching, multiple translations and a BERT-based word\naligner. To measure the alignment effectiveness, a subset of annotations from\nthe ACE-2005-PT corpus was manually aligned by a linguist expert. This subset\nwas then compared against our pipeline results which achieved exact and relaxed\nmatch scores of 70.55\\% and 87.55\\% respectively. As a result, we successfully\ngenerated a Portuguese version of the ACE-2005 corpus, which has been accepted\nfor publication by LDC.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16928v1",
    "published_date": "2024-08-29 22:05:08 UTC",
    "updated_date": "2024-08-29 22:05:08 UTC"
  },
  {
    "arxiv_id": "2408.16913v1",
    "title": "Analyzing Inference Privacy Risks Through Gradients in Machine Learning",
    "authors": [
      "Zhuohang Li",
      "Andrew Lowy",
      "Jing Liu",
      "Toshiaki Koike-Akino",
      "Kieran Parsons",
      "Bradley Malin",
      "Ye Wang"
    ],
    "abstract": "In distributed learning settings, models are iteratively updated with shared\ngradients computed from potentially sensitive user data. While previous work\nhas studied various privacy risks of sharing gradients, our paper aims to\nprovide a systematic approach to analyze private information leakage from\ngradients. We present a unified game-based framework that encompasses a broad\nrange of attacks including attribute, property, distributional, and user\ndisclosures. We investigate how different uncertainties of the adversary affect\ntheir inferential power via extensive experiments on five datasets across\nvarious data modalities. Our results demonstrate the inefficacy of solely\nrelying on data aggregation to achieve privacy against inference attacks in\ndistributed learning. We further evaluate five types of defenses, namely,\ngradient pruning, signed gradient descent, adversarial perturbations,\nvariational information bottleneck, and differential privacy, under both static\nand adaptive adversary settings. We provide an information-theoretic view for\nanalyzing the effectiveness of these defenses against inference from gradients.\nFinally, we introduce a method for auditing attribute inference privacy,\nimproving the empirical estimation of worst-case privacy through crafting\nadversarial canary records.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16913v1",
    "published_date": "2024-08-29 21:21:53 UTC",
    "updated_date": "2024-08-29 21:21:53 UTC"
  },
  {
    "arxiv_id": "2408.16871v1",
    "title": "GSTAM: Efficient Graph Distillation with Structural Attention-Matching",
    "authors": [
      "Arash Rasti-Meymandi",
      "Ahmad Sajedi",
      "Zhaopan Xu",
      "Konstantinos N. Plataniotis"
    ],
    "abstract": "Graph distillation has emerged as a solution for reducing large graph\ndatasets to smaller, more manageable, and informative ones. Existing methods\nprimarily target node classification, involve computationally intensive\nprocesses, and fail to capture the true distribution of the full graph dataset.\nTo address these issues, we introduce Graph Distillation with Structural\nAttention Matching (GSTAM), a novel method for condensing graph classification\ndatasets. GSTAM leverages the attention maps of GNNs to distill structural\ninformation from the original dataset into synthetic graphs. The structural\nattention-matching mechanism exploits the areas of the input graph that GNNs\nprioritize for classification, effectively distilling such information into the\nsynthetic graphs and improving overall distillation performance. Comprehensive\nexperiments demonstrate GSTAM's superiority over existing methods, achieving\n0.45% to 6.5% better performance in extreme condensation ratios, highlighting\nits potential use in advancing distillation for graph classification tasks\n(Code available at https://github.com/arashrasti96/GSTAM).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ECCV-DD 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.16871v1",
    "published_date": "2024-08-29 19:40:04 UTC",
    "updated_date": "2024-08-29 19:40:04 UTC"
  },
  {
    "arxiv_id": "2409.00140v1",
    "title": "Statistical Analysis of the Impact of Quaternion Components in Convolutional Neural Networks",
    "authors": [
      "Gerardo Altamirano-Gómez",
      "Carlos Gershenson"
    ],
    "abstract": "In recent years, several models using Quaternion-Valued Convolutional Neural\nNetworks (QCNNs) for different problems have been proposed. Although the\ndefinition of the quaternion convolution layer is the same, there are different\nadaptations of other atomic components to the quaternion domain, e.g., pooling\nlayers, activation functions, fully connected layers, etc. However, the effect\nof selecting a specific type of these components and the way in which their\ninteractions affect the performance of the model still unclear. Understanding\nthe impact of these choices on model performance is vital for effectively\nutilizing QCNNs. This paper presents a statistical analysis carried out on\nexperimental data to compare the performance of existing components for the\nimage classification problem. In addition, we introduce a novel Fully\nQuaternion ReLU activation function, which exploits the unique properties of\nquaternion algebra to improve model performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE",
      "I.2.0; I.2.10; I.4.0; I.5.1"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.00140v1",
    "published_date": "2024-08-29 19:13:20 UTC",
    "updated_date": "2024-08-29 19:13:20 UTC"
  },
  {
    "arxiv_id": "2408.16768v1",
    "title": "SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners",
    "authors": [
      "Ziyu Guo",
      "Renrui Zhang",
      "Xiangyang Zhu",
      "Chengzhuo Tong",
      "Peng Gao",
      "Chunyuan Li",
      "Pheng-Ann Heng"
    ],
    "abstract": "We introduce SAM2Point, a preliminary exploration adapting Segment Anything\nModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point\ninterprets any 3D data as a series of multi-directional videos, and leverages\nSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.\nOur framework supports various prompt types, including 3D points, boxes, and\nmasks, and can generalize across diverse scenarios, such as 3D objects, indoor\nscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple\n3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight\nthe robust generalization capabilities of SAM2Point. To our best knowledge, we\npresent the most faithful implementation of SAM in 3D, which may serve as a\nstarting point for future research in promptable 3D segmentation. Online Demo:\nhttps://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\nhttps://github.com/ZiyuGuo99/SAM2Point .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Work in progress. Online Demo:\n  https://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\n  https://github.com/ZiyuGuo99/SAM2Point",
    "pdf_url": "http://arxiv.org/pdf/2408.16768v1",
    "published_date": "2024-08-29 17:59:45 UTC",
    "updated_date": "2024-08-29 17:59:45 UTC"
  },
  {
    "arxiv_id": "2408.16767v2",
    "title": "ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model",
    "authors": [
      "Fangfu Liu",
      "Wenqiang Sun",
      "Hanyang Wang",
      "Yikai Wang",
      "Haowen Sun",
      "Junliang Ye",
      "Jun Zhang",
      "Yueqi Duan"
    ],
    "abstract": "Advancements in 3D scene reconstruction have transformed 2D images from the\nreal world into 3D models, producing realistic 3D results from hundreds of\ninput photos. Despite great success in dense-view reconstruction scenarios,\nrendering a detailed scene from insufficient captured views is still an\nill-posed optimization problem, often resulting in artifacts and distortions in\nunseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction\nparadigm that reframes the ambiguous reconstruction challenge as a temporal\ngeneration task. The key insight is to unleash the strong generative prior of\nlarge pre-trained video diffusion models for sparse-view reconstruction.\nHowever, 3D view consistency struggles to be accurately preserved in directly\ngenerated video frames from pre-trained models. To address this, given limited\ninput views, the proposed ReconX first constructs a global point cloud and\nencodes it into a contextual space as the 3D structure condition. Guided by the\ncondition, the video diffusion model then synthesizes video frames that are\nboth detail-preserved and exhibit a high degree of 3D consistency, ensuring the\ncoherence of the scene from various perspectives. Finally, we recover the 3D\nscene from the generated video through a confidence-aware 3D Gaussian Splatting\noptimization scheme. Extensive experiments on various real-world datasets show\nthe superiority of our ReconX over state-of-the-art methods in terms of quality\nand generalizability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://liuff19.github.io/ReconX",
    "pdf_url": "http://arxiv.org/pdf/2408.16767v2",
    "published_date": "2024-08-29 17:59:40 UTC",
    "updated_date": "2024-11-30 09:10:08 UTC"
  },
  {
    "arxiv_id": "2408.16765v1",
    "title": "A Score-Based Density Formula, with Applications in Diffusion Generative Models",
    "authors": [
      "Gen Li",
      "Yuling Yan"
    ],
    "abstract": "Score-based generative models (SGMs) have revolutionized the field of\ngenerative modeling, achieving unprecedented success in generating realistic\nand diverse content. Despite empirical advances, the theoretical basis for why\noptimizing the evidence lower bound (ELBO) on the log-likelihood is effective\nfor training diffusion generative models, such as DDPMs, remains largely\nunexplored. In this paper, we address this question by establishing a density\nformula for a continuous-time diffusion process, which can be viewed as the\ncontinuous-time limit of the forward process in an SGM. This formula reveals\nthe connection between the target density and the score function associated\nwith each step of the forward process. Building on this, we demonstrate that\nthe minimizer of the optimization objective for training DDPMs nearly coincides\nwith that of the true objective, providing a theoretical foundation for\noptimizing DDPMs using the ELBO. Furthermore, we offer new insights into the\nrole of score-matching regularization in training GANs, the use of ELBO in\ndiffusion classifiers, and the recently proposed diffusion loss.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16765v1",
    "published_date": "2024-08-29 17:59:07 UTC",
    "updated_date": "2024-08-29 17:59:07 UTC"
  },
  {
    "arxiv_id": "2409.00138v3",
    "title": "PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action",
    "authors": [
      "Yijia Shao",
      "Tianshi Li",
      "Weiyan Shi",
      "Yanchen Liu",
      "Diyi Yang"
    ],
    "abstract": "As language models (LMs) are widely utilized in personalized communication\nscenarios (e.g., sending emails, writing social media posts) and endowed with a\ncertain level of agency, ensuring they act in accordance with the contextual\nprivacy norms becomes increasingly critical. However, quantifying the privacy\nnorm awareness of LMs and the emerging privacy risk in LM-mediated\ncommunication is challenging due to (1) the contextual and long-tailed nature\nof privacy-sensitive cases, and (2) the lack of evaluation approaches that\ncapture realistic application scenarios. To address these challenges, we\npropose PrivacyLens, a novel framework designed to extend privacy-sensitive\nseeds into expressive vignettes and further into agent trajectories, enabling\nmulti-level evaluation of privacy leakage in LM agents' actions. We instantiate\nPrivacyLens with a collection of privacy norms grounded in privacy literature\nand crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM\nperformance in answering probing questions and their actual behavior when\nexecuting user instructions in an agent setup. State-of-the-art LMs, like GPT-4\nand Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even\nwhen prompted with privacy-enhancing instructions. We also demonstrate the\ndynamic nature of PrivacyLens by extending each seed into multiple trajectories\nto red-team LM privacy leakage risk. Dataset and code are available at\nhttps://github.com/SALT-NLP/PrivacyLens.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024 Datasets and Benchmarks Track",
    "pdf_url": "http://arxiv.org/pdf/2409.00138v3",
    "published_date": "2024-08-29 17:58:38 UTC",
    "updated_date": "2025-03-14 06:03:20 UTC"
  },
  {
    "arxiv_id": "2408.16757v2",
    "title": "Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical Analysis of Methods and Benchmarks",
    "authors": [
      "Hongjun Wang",
      "Sagar Vaze",
      "Kai Han"
    ],
    "abstract": "Detecting test-time distribution shift has emerged as a key capability for\nsafely deployed machine learning models, with the question being tackled under\nvarious guises in recent years. In this paper, we aim to provide a consolidated\nview of the two largest sub-fields within the community: out-of-distribution\n(OOD) detection and open-set recognition (OSR). In particular, we aim to\nprovide rigorous empirical analysis of different methods across settings and\nprovide actionable takeaways for practitioners and researchers. Concretely, we\nmake the following contributions: (i) We perform rigorous cross-evaluation\nbetween state-of-the-art methods in the OOD detection and OSR settings and\nidentify a strong correlation between the performances of methods for them;\n(ii) We propose a new, large-scale benchmark setting which we suggest better\ndisentangles the problem tackled by OOD detection and OSR, re-evaluating\nstate-of-the-art OOD detection and OSR methods in this setting; (iii) We\nsurprisingly find that the best performing method on standard benchmarks\n(Outlier Exposure) struggles when tested at scale, while scoring rules which\nare sensitive to the deep feature magnitude consistently show promise; and (iv)\nWe conduct empirical analysis to explain these phenomena and highlight\ndirections for future research. Code:\nhttps://github.com/Visual-AI/Dissect-OOD-OSR",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to IJCV, preprint version; v2: add supplementary",
    "pdf_url": "http://arxiv.org/pdf/2408.16757v2",
    "published_date": "2024-08-29 17:55:07 UTC",
    "updated_date": "2024-08-30 02:26:01 UTC"
  },
  {
    "arxiv_id": "2408.16749v1",
    "title": "Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge",
    "authors": [
      "Beidi Dong",
      "Jin R. Lee",
      "Ziwei Zhu",
      "Balassubramanian Srinivasan"
    ],
    "abstract": "The United States has experienced a significant increase in violent\nextremism, prompting the need for automated tools to detect and limit the\nspread of extremist ideology online. This study evaluates the performance of\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformers (GPT) in detecting and classifying online domestic\nextremist posts. We collected social media posts containing \"far-right\" and\n\"far-left\" ideological keywords and manually labeled them as extremist or\nnon-extremist. Extremist posts were further classified into one or more of five\ncontributing elements of extremism based on a working definitional framework.\nThe BERT model's performance was evaluated based on training data size and\nknowledge transfer between categories. We also compared the performance of GPT\n3.5 and GPT 4 models using different prompts: na\\\"ive, layperson-definition,\nrole-playing, and professional-definition. Results showed that the best\nperforming GPT models outperformed the best performing BERT models, with more\ndetailed prompts generally yielding better results. However, overly complex\nprompts may impair performance. Different versions of GPT have unique\nsensitives to what they consider extremist. GPT 3.5 performed better at\nclassifying far-left extremist posts, while GPT 4 performed better at\nclassifying far-right extremist posts. Large language models, represented by\nGPT models, hold significant potential for online extremism classification\ntasks, surpassing traditional BERT models in a zero-shot setting. Future\nresearch should explore human-computer interactions in optimizing GPT models\nfor extremist detection and classification tasks to develop more efficient\n(e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes)\nmethods for identifying extremist content.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16749v1",
    "published_date": "2024-08-29 17:43:03 UTC",
    "updated_date": "2024-08-29 17:43:03 UTC"
  },
  {
    "arxiv_id": "2408.16737v2",
    "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling",
    "authors": [
      "Hritik Bansal",
      "Arian Hosseini",
      "Rishabh Agarwal",
      "Vinh Q. Tran",
      "Mehran Kazemi"
    ],
    "abstract": "Training on high-quality synthetic data from strong language models (LMs) is\na common strategy to improve the reasoning performance of LMs. In this work, we\nrevisit whether this strategy is compute-optimal under a fixed inference budget\n(e.g., FLOPs). To do so, we investigate the trade-offs between generating\nsynthetic data using a stronger but more expensive (SE) model versus a weaker\nbut cheaper (WC) model. We evaluate the generated data across three key\nmetrics: coverage, diversity, and false positive rate, and show that the data\nfrom WC models may have higher coverage and diversity, but also exhibit higher\nfalse positive rates. We then finetune LMs on data from SE and WC models in\ndifferent settings: knowledge distillation, self-improvement, and a novel\nweak-to-strong improvement setup where a weaker LM teaches reasoning to a\nstronger LM. Our findings reveal that models finetuned on WC-generated data\nconsistently outperform those trained on SE-generated data across multiple\nbenchmarks and multiple choices of WC and SE models. These results challenge\nthe prevailing practice of relying on SE models for synthetic data generation,\nsuggesting that WC may be the compute-optimal approach for training advanced LM\nreasoners.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16737v2",
    "published_date": "2024-08-29 17:32:35 UTC",
    "updated_date": "2024-10-07 19:37:10 UTC"
  },
  {
    "arxiv_id": "2409.00137v1",
    "title": "Emerging Vulnerabilities in Frontier Models: Multi-Turn Jailbreak Attacks",
    "authors": [
      "Tom Gibbs",
      "Ethan Kosak-Hine",
      "George Ingebretsen",
      "Jason Zhang",
      "Julius Broomfield",
      "Sara Pieri",
      "Reihaneh Iranmanesh",
      "Reihaneh Rabbany",
      "Kellin Pelrine"
    ],
    "abstract": "Large language models (LLMs) are improving at an exceptional rate. However,\nthese models are still susceptible to jailbreak attacks, which are becoming\nincreasingly dangerous as models become increasingly powerful. In this work, we\nintroduce a dataset of jailbreaks where each example can be input in both a\nsingle or a multi-turn format. We show that while equivalent in content, they\nare not equivalent in jailbreak success: defending against one structure does\nnot guarantee defense against the other. Similarly, LLM-based filter guardrails\nalso perform differently depending on not just the input content but the input\nstructure. Thus, vulnerabilities of frontier models should be studied in both\nsingle and multi-turn settings; this dataset provides a tool to do so.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00137v1",
    "published_date": "2024-08-29 17:30:05 UTC",
    "updated_date": "2024-08-29 17:30:05 UTC"
  },
  {
    "arxiv_id": "2408.16725v3",
    "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
    "authors": [
      "Zhifei Xie",
      "Changqiao Wu"
    ],
    "abstract": "Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "Technical report, work in progress. Demo and code:\n  https://github.com/gpt-omni/mini-omni",
    "pdf_url": "http://arxiv.org/pdf/2408.16725v3",
    "published_date": "2024-08-29 17:18:53 UTC",
    "updated_date": "2024-11-05 02:24:18 UTC"
  },
  {
    "arxiv_id": "2409.09047v2",
    "title": "AI Meets the Classroom: When Do Large Language Models Harm Learning?",
    "authors": [
      "Matthias Lehmann",
      "Philipp B. Cornelius",
      "Fabian J. Sting"
    ],
    "abstract": "The effect of large language models (LLMs) in education is debated: Previous\nresearch shows that LLMs can help as well as hurt learning. In two\npre-registered and incentivized laboratory experiments, we find no effect of\nLLMs on overall learning outcomes. In exploratory analyses and a field study,\nwe provide evidence that the effect of LLMs on learning outcomes depends on\nusage behavior. Students who substitute some of their learning activities with\nLLMs (e.g., by generating solutions to exercises) increase the volume of topics\nthey can learn about but decrease their understanding of each topic. Students\nwho complement their learning activities with LLMs (e.g., by asking for\nexplanations) do not increase topic volume but do increase their understanding.\nWe also observe that LLMs widen the gap between students with low and high\nprior knowledge. While LLMs show great potential to improve learning, their use\nmust be tailored to the educational context and students' needs.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09047v2",
    "published_date": "2024-08-29 17:07:46 UTC",
    "updated_date": "2025-03-08 04:13:50 UTC"
  },
  {
    "arxiv_id": "2408.16717v1",
    "title": "A GREAT Architecture for Edge-Based Graph Problems Like TSP",
    "authors": [
      "Attila Lischka",
      "Jiaming Wu",
      "Morteza Haghir Chehreghani",
      "Balázs Kulcsár"
    ],
    "abstract": "In the last years, many neural network-based approaches have been proposed to\ntackle combinatorial optimization problems such as routing problems. Many of\nthese approaches are based on graph neural networks (GNNs) or related\ntransformers, operating on the Euclidean coordinates representing the routing\nproblems. However, GNNs are inherently not well suited to operate on dense\ngraphs, such as in routing problems. Furthermore, models operating on Euclidean\ncoordinates cannot be applied to non-Euclidean versions of routing problems\nthat are often found in real-world settings. To overcome these limitations, we\npropose a novel GNN-related edge-based neural model called Graph Edge Attention\nNetwork (GREAT). We evaluate the performance of GREAT in the\nedge-classification task to predict optimal edges in the Traveling Salesman\nProblem (TSP). We can use such a trained GREAT model to produce sparse TSP\ngraph instances, keeping only the edges GREAT finds promising. Compared to\nother, non-learning-based methods to sparsify TSP graphs, GREAT can produce\nvery sparse graphs while keeping most of the optimal edges. Furthermore, we\nbuild a reinforcement learning-based GREAT framework which we apply to\nEuclidean and non-Euclidean asymmetric TSP. This framework achieves\nstate-of-the-art results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16717v1",
    "published_date": "2024-08-29 17:07:43 UTC",
    "updated_date": "2024-08-29 17:07:43 UTC"
  },
  {
    "arxiv_id": "2408.16672v4",
    "title": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever",
    "authors": [
      "Rohan Jha",
      "Bo Wang",
      "Michael Günther",
      "Georgios Mastrapas",
      "Saba Sturua",
      "Isabelle Mohr",
      "Andreas Koukounas",
      "Mohammad Kalim Akram",
      "Nan Wang",
      "Han Xiao"
    ],
    "abstract": "Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis work we propose a number of incremental improvements to the ColBERT model\narchitecture and training pipeline, using methods shown to work in the more\nmature single-vector embedding model training paradigm, particularly those that\napply to heterogeneous multilingual data or boost efficiency with little\ntradeoff. Our new model, Jina-ColBERT-v2, demonstrates strong performance\nacross a range of English and multilingual retrieval tasks.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.IR",
    "comment": "8 pages, references at pp7,8; EMNLP workshop submission",
    "pdf_url": "http://arxiv.org/pdf/2408.16672v4",
    "published_date": "2024-08-29 16:21:00 UTC",
    "updated_date": "2024-09-14 07:41:06 UTC"
  },
  {
    "arxiv_id": "2408.16673v2",
    "title": "Preserving Diversity in Supervised Fine-Tuning of Large Language Models",
    "authors": [
      "Ziniu Li",
      "Congliang Chen",
      "Tian Xu",
      "Zeyu Qin",
      "Jiancong Xiao",
      "Zhi-Quan Luo",
      "Ruoyu Sun"
    ],
    "abstract": "Large Language Models (LLMs) typically rely on Supervised Fine-Tuning (SFT)\nto specialize in downstream tasks, with the Cross Entropy (CE) loss being the\nde facto choice. However, CE maximizes the likelihood of observed data without\naccounting for alternative possibilities. As such, CE usually leads to reduced\ndiversity in the model's outputs, which hinders further development that\nrequires sampling to explore better responses. To address this limitation, this\npaper introduces a new game-theoretic formulation for SFT. In this framework,\nan auxiliary variable is introduced to regulate the learning process. We prove\nthat the proposed game-theoretic approach connects to the problem of reverse KL\nminimization with entropy regularization. This regularization prevents\nover-memorization of training data and promotes output diversity. To implement\nthis framework, we develop GEM, a new training algorithm that is\ncomputationally efficient as CE by leveraging some unique properties of LLMs.\nEmpirical studies of pre-trained models from 3B to 70B parameters show that GEM\nachieves comparable downstream performance to CE while significantly enhancing\noutput diversity. This increased diversity translates to performance gains in\ntest-time compute scaling for chat and code generation tasks. Moreover, we\nobserve that preserving output diversity has the added benefit of mitigating\nforgetting, as maintaining diverse outputs encourages models to retain\npre-trained knowledge throughout the training process.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.16673v2",
    "published_date": "2024-08-29 16:21:00 UTC",
    "updated_date": "2025-04-05 08:56:18 UTC"
  },
  {
    "arxiv_id": "2408.16667v1",
    "title": "Iterative Graph Alignment",
    "authors": [
      "Fangyuan Yu",
      "Hardeep Singh Arora",
      "Matt Johnson"
    ],
    "abstract": "By compressing diverse narratives, LLMs go beyond memorization, achieving\nintelligence by capturing generalizable causal relationships. However, they\nsuffer from local 'representation gaps' due to insufficient training data\ndiversity, limiting their real-world utility, especially in tasks requiring\nstrict alignment to rules. Traditional alignment methods relying on heavy human\nannotations are inefficient and unscalable. Recent self-alignment techniques\nalso fall short, as they often depend on self-selection based prompting and\nmemorization-based learning. To address these issues, we introduce Iterative\nGraph Alignment (IGA), an annotation-free rule-based alignment algorithm. A\nteacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical\ngraphs and reference answers. The student model (LLM) identifies local\nknowledge gaps by attempting to align its responses with these references,\ncollaborating with helper models to generate diverse answers. These aligned\nresponses are then used for iterative supervised fine-tuning (SFT). Our\nevaluations across five rule-based scenarios demonstrate IGP's effectiveness,\nwith a 73.12\\% alignment improvement in Claude Sonnet 3.5, and\nLlama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude\nSonnet 3.5 in rule-based alignment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16667v1",
    "published_date": "2024-08-29 16:15:01 UTC",
    "updated_date": "2024-08-29 16:15:01 UTC"
  },
  {
    "arxiv_id": "2409.09046v2",
    "title": "HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications",
    "authors": [
      "Rishi Kalra",
      "Zekun Wu",
      "Ayesha Gulley",
      "Airlie Hilliard",
      "Xin Guan",
      "Adriano Koshiyama",
      "Philip Treleaven"
    ],
    "abstract": "Large Language Models (LLMs) face limitations in AI legal and policy\napplications due to outdated knowledge, hallucinations, and poor reasoning in\ncomplex contexts. Retrieval-Augmented Generation (RAG) systems address these\nissues by incorporating external knowledge, but suffer from retrieval errors,\nineffective context integration, and high operational costs. This paper\npresents the Hybrid Parameter-Adaptive RAG (HyPA-RAG) system, designed for the\nAI legal domain, with NYC Local Law 144 (LL144) as the test case. HyPA-RAG\nintegrates a query complexity classifier for adaptive parameter tuning, a\nhybrid retrieval approach combining dense, sparse, and knowledge graph methods,\nand a comprehensive evaluation framework with tailored question types and\nmetrics. Testing on LL144 demonstrates that HyPA-RAG enhances retrieval\naccuracy, response fidelity, and contextual precision, offering a robust and\nadaptable solution for high-stakes legal and policy applications.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "NAACL 2025 Industry Track & EMNLP 2024 CustomNLP4U Workshop",
    "pdf_url": "http://arxiv.org/pdf/2409.09046v2",
    "published_date": "2024-08-29 16:11:20 UTC",
    "updated_date": "2025-02-25 13:14:47 UTC"
  },
  {
    "arxiv_id": "2409.09045v2",
    "title": "United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections",
    "authors": [
      "Leah von der Heyde",
      "Anna-Carolina Haensch",
      "Alexander Wenz",
      "Bolei Ma"
    ],
    "abstract": "\"Synthetic samples\" based on large language models (LLMs) have been argued to\nserve as efficient alternatives to surveys of humans, assuming that their\ntraining data includes information on human attitudes and behavior. However,\nLLM-synthetic samples might exhibit bias, for example due to training data and\nfine-tuning processes being unrepresentative of diverse contexts. Such biases\nrisk reinforcing existing biases in research, policymaking, and society.\nTherefore, researchers need to investigate if and under which conditions\nLLM-generated synthetic samples can be used for public opinion prediction. In\nthis study, we examine to what extent LLM-based predictions of individual\npublic opinion exhibit context-dependent biases by predicting the results of\nthe 2024 European Parliament elections. Prompting three LLMs with\nindividual-level background information of 26,000 eligible European voters, we\nask the LLMs to predict each person's voting behavior. By comparing them to the\nactual results, we show that LLM-based predictions of future voting behavior\nlargely fail, their accuracy is unequally distributed across national and\nlinguistic contexts, and they require detailed attitudinal information in the\nprompt. The findings emphasize the limited applicability of LLM-synthetic\nsamples to public opinion prediction. In investigating their contextual biases,\nthis study contributes to the understanding and mitigation of inequalities in\nthe development of LLMs and their applications in computational social science.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "stat.AP"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09045v2",
    "published_date": "2024-08-29 16:01:06 UTC",
    "updated_date": "2025-04-17 21:21:10 UTC"
  },
  {
    "arxiv_id": "2408.16806v1",
    "title": "Physics-Informed Neural Networks and Extensions",
    "authors": [
      "Maziar Raissi",
      "Paris Perdikaris",
      "Nazanin Ahmadi",
      "George Em Karniadakis"
    ],
    "abstract": "In this paper, we review the new method Physics-Informed Neural Networks\n(PINNs) that has become the main pillar in scientific machine learning, we\npresent recent practical extensions, and provide a specific example in\ndata-driven discovery of governing differential equations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Frontiers of Science Awards 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.16806v1",
    "published_date": "2024-08-29 16:00:42 UTC",
    "updated_date": "2024-08-29 16:00:42 UTC"
  },
  {
    "arxiv_id": "2408.16647v1",
    "title": "DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving",
    "authors": [
      "Yongjie Fu",
      "Anmol Jain",
      "Xuan Di",
      "Xu Chen",
      "Zhaobin Mo"
    ],
    "abstract": "The advancement of autonomous driving technologies necessitates increasingly\nsophisticated methods for understanding and predicting real-world scenarios.\nVision language models (VLMs) are emerging as revolutionary tools with\nsignificant potential to influence autonomous driving. In this paper, we\npropose the DriveGenVLM framework to generate driving videos and use VLMs to\nunderstand them. To achieve this, we employ a video generation framework\ngrounded in denoising diffusion probabilistic models (DDPM) aimed at predicting\nreal-world video sequences. We then explore the adequacy of our generated\nvideos for use in VLMs by employing a pre-trained model known as Efficient\nIn-context Learning on Egocentric Videos (EILEV). The diffusion model is\ntrained with the Waymo open dataset and evaluated using the Fr\\'echet Video\nDistance (FVD) score to ensure the quality and realism of the generated videos.\nCorresponding narrations are provided by EILEV for these generated videos,\nwhich may be beneficial in the autonomous driving domain. These narrations can\nenhance traffic scene understanding, aid in navigation, and improve planning\ncapabilities. The integration of video generation with VLMs in the DriveGenVLM\nframework represents a significant step forward in leveraging advanced AI\nmodels to address complex challenges in autonomous driving.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16647v1",
    "published_date": "2024-08-29 15:52:56 UTC",
    "updated_date": "2024-08-29 15:52:56 UTC"
  },
  {
    "arxiv_id": "2408.16634v3",
    "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model",
    "authors": [
      "Zhuan Shi",
      "Jing Yan",
      "Xiaoli Tang",
      "Lingjuan Lyu",
      "Boi Faltings"
    ],
    "abstract": "The increasing sophistication of text-to-image generative models has led to\ncomplex challenges in defining and enforcing copyright infringement criteria\nand protection. Existing methods, such as watermarking and dataset\ndeduplication, fail to provide comprehensive solutions due to the lack of\nstandardized metrics and the inherent complexity of addressing copyright\ninfringement in diffusion models. To deal with these challenges, we propose a\nReinforcement Learning-based Copyright Protection(RLCP) method for\nText-to-Image Diffusion Model, which minimizes the generation of\ncopyright-infringing content while maintaining the quality of the\nmodel-generated dataset. Our approach begins with the introduction of a novel\ncopyright metric grounded in copyright law and court precedents on\ninfringement. We then utilize the Denoising Diffusion Policy Optimization\n(DDPO) framework to guide the model through a multi-step decision-making\nprocess, optimizing it using a reward function that incorporates our proposed\ncopyright metric. Additionally, we employ KL divergence as a regularization\nterm to mitigate some failure modes and stabilize RL fine-tuning. Experiments\nconducted on 3 mixed datasets of copyright and non-copyright images demonstrate\nthat our approach significantly reduces copyright infringement risk while\nmaintaining image quality.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16634v3",
    "published_date": "2024-08-29 15:39:33 UTC",
    "updated_date": "2025-01-06 15:48:07 UTC"
  },
  {
    "arxiv_id": "2408.16633v1",
    "title": "Optimizing Automated Picking Systems in Warehouse Robots Using Machine Learning",
    "authors": [
      "Keqin Li",
      "Jin Wang",
      "Xubo Wu",
      "Xirui Peng",
      "Runmian Chang",
      "Xiaoyu Deng",
      "Yiwen Kang",
      "Yue Yang",
      "Fanghao Ni",
      "Bo Hong"
    ],
    "abstract": "With the rapid growth of global e-commerce, the demand for automation in the\nlogistics industry is increasing. This study focuses on automated picking\nsystems in warehouses, utilizing deep learning and reinforcement learning\ntechnologies to enhance picking efficiency and accuracy while reducing system\nfailure rates. Through empirical analysis, we demonstrate the effectiveness of\nthese technologies in improving robot picking performance and adaptability to\ncomplex environments. The results show that the integrated machine learning\nmodel significantly outperforms traditional methods, effectively addressing the\nchallenges of peak order processing, reducing operational errors, and improving\noverall logistics efficiency. Additionally, by analyzing environmental factors,\nthis study further optimizes system design to ensure efficient and stable\noperation under variable conditions. This research not only provides innovative\nsolutions for logistics automation but also offers a theoretical and empirical\nfoundation for future technological development and application.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16633v1",
    "published_date": "2024-08-29 15:39:12 UTC",
    "updated_date": "2024-08-29 15:39:12 UTC"
  },
  {
    "arxiv_id": "2408.16632v1",
    "title": "Maelstrom Networks",
    "authors": [
      "Matthew Evanusa",
      "Cornelia Fermüller",
      "Yiannis Aloimonos"
    ],
    "abstract": "Artificial Neural Networks has struggled to devise a way to incorporate\nworking memory into neural networks. While the ``long term'' memory can be seen\nas the learned weights, the working memory consists likely more of dynamical\nactivity, that is missing from feed-forward models. Current state of the art\nmodels such as transformers tend to ``solve'' this by ignoring working memory\nentirely and simply process the sequence as an entire piece of data; however\nthis means the network cannot process the sequence in an online fashion, and\nleads to an immense explosion in memory requirements. Here, inspired by a\ncombination of controls, reservoir computing, deep learning, and recurrent\nneural networks, we offer an alternative paradigm that combines the strength of\nrecurrent networks, with the pattern matching capability of feed-forward neural\nnetworks, which we call the \\textit{Maelstrom Networks} paradigm. This paradigm\nleaves the recurrent component - the \\textit{Maelstrom} - unlearned, and\noffloads the learning to a powerful feed-forward network. This allows the\nnetwork to leverage the strength of feed-forward training without unrolling the\nnetwork, and allows for the memory to be implemented in new neuromorphic\nhardware. It endows a neural network with a sequential memory that takes\nadvantage of the inductive bias that data is organized causally in the temporal\ndomain, and imbues the network with a state that represents the agent's\n``self'', moving through the environment. This could also lead the way to\ncontinual learning, with the network modularized and ``'protected'' from\noverwrites that come with new data. In addition to aiding in solving these\nperformance problems that plague current non-temporal deep networks, this also\ncould finally lead towards endowing artificial networks with a sense of\n``self''.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16632v1",
    "published_date": "2024-08-29 15:39:04 UTC",
    "updated_date": "2024-08-29 15:39:04 UTC"
  },
  {
    "arxiv_id": "2409.00135v1",
    "title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
    "authors": [
      "Huan Zhang",
      "Yu Song",
      "Ziyu Hou",
      "Santiago Miret",
      "Bang Liu"
    ],
    "abstract": "The emergence of specialized large language models (LLMs) has shown promise\nin addressing complex tasks for materials science. Many LLMs, however, often\nstruggle with distinct complexities of material science tasks, such as\nmaterials science computational tasks, and often rely heavily on outdated\nimplicit knowledge, leading to inaccuracies and hallucinations. To address\nthese challenges, we introduce HoneyComb, the first LLM-based agent system\nspecifically designed for materials science. HoneyComb leverages a novel,\nhigh-quality materials science knowledge base (MatSciKB) and a sophisticated\ntool hub (ToolHub) to enhance its reasoning and computational capabilities\ntailored to materials science. MatSciKB is a curated, structured knowledge\ncollection based on reliable literature, while ToolHub employs an Inductive\nTool Construction method to generate, decompose, and refine API tools for\nmaterials science. Additionally, HoneyComb leverages a retriever module that\nadaptively selects the appropriate knowledge source or tools for specific\ntasks, thereby ensuring accuracy and relevance. Our results demonstrate that\nHoneyComb significantly outperforms baseline models across various tasks in\nmaterials science, effectively bridging the gap between current LLM\ncapabilities and the specialized needs of this domain. Furthermore, our\nadaptable framework can be easily extended to other scientific domains,\nhighlighting its potential for broad applicability in advancing scientific\nresearch and applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review on EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.00135v1",
    "published_date": "2024-08-29 15:38:40 UTC",
    "updated_date": "2024-08-29 15:38:40 UTC"
  },
  {
    "arxiv_id": "2408.16629v2",
    "title": "LLMs generate structurally realistic social networks but overestimate political homophily",
    "authors": [
      "Serina Chang",
      "Alicja Chaszczewicz",
      "Emma Wang",
      "Maya Josifovska",
      "Emma Pierson",
      "Jure Leskovec"
    ],
    "abstract": "Generating social networks is essential for many applications, such as\nepidemic modeling and social simulations. The emergence of generative AI,\nespecially large language models (LLMs), offers new possibilities for social\nnetwork generation: LLMs can generate networks without additional training or\nneed to define network parameters, and users can flexibly define individuals in\nthe network using natural language. However, this potential raises two critical\nquestions: 1) are the social networks generated by LLMs realistic, and 2) what\nare risks of bias, given the importance of demographics in forming social ties?\nTo answer these questions, we develop three prompting methods for network\ngeneration and compare the generated networks to a suite of real social\nnetworks. We find that more realistic networks are generated with \"local\"\nmethods, where the LLM constructs relations for one persona at a time, compared\nto \"global\" methods that construct the entire network at once. We also find\nthat the generated networks match real networks on many characteristics,\nincluding density, clustering, connectivity, and degree distribution. However,\nwe find that LLMs emphasize political homophily over all other types of\nhomophily and significantly overestimate political homophily compared to real\nsocial networks.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted to International AAAI Conference on Web and Social Media\n  2025 (ICWSM'25)",
    "pdf_url": "http://arxiv.org/pdf/2408.16629v2",
    "published_date": "2024-08-29 15:36:52 UTC",
    "updated_date": "2025-03-27 19:30:43 UTC"
  },
  {
    "arxiv_id": "2408.16621v1",
    "title": "Towards Infusing Auxiliary Knowledge for Distracted Driver Detection",
    "authors": [
      "Ishwar B Balappanawar",
      "Ashmit Chamoli",
      "Ruwan Wickramarachchi",
      "Aditya Mishra",
      "Ponnurangam Kumaraguru",
      "Amit P. Sheth"
    ],
    "abstract": "Distracted driving is a leading cause of road accidents globally.\nIdentification of distracted driving involves reliably detecting and\nclassifying various forms of driver distraction (e.g., texting, eating, or\nusing in-car devices) from in-vehicle camera feeds to enhance road safety. This\ntask is challenging due to the need for robust models that can generalize to a\ndiverse set of driver behaviors without requiring extensive annotated datasets.\nIn this paper, we propose KiD3, a novel method for distracted driver detection\n(DDD) by infusing auxiliary knowledge about semantic relations between entities\nin a scene and the structural configuration of the driver's pose. Specifically,\nwe construct a unified framework that integrates the scene graphs, and driver\npose information with the visual cues in video frames to create a holistic\nrepresentation of the driver's actions.Our results indicate that KiD3 achieves\na 13.64% accuracy improvement over the vision-only baseline by incorporating\nsuch auxiliary knowledge with visual information.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "I.2.0"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at KiL 2024: Workshop on Knowledge-infused Learning\n  co-located with 30th ACM KDD Conference",
    "pdf_url": "http://arxiv.org/pdf/2408.16621v1",
    "published_date": "2024-08-29 15:28:42 UTC",
    "updated_date": "2024-08-29 15:28:42 UTC"
  },
  {
    "arxiv_id": "2408.16620v1",
    "title": "Hyperdimensional Vector Tsetlin Machines with Applications to Sequence Learning and Generation",
    "authors": [
      "Christian D. Blakely"
    ],
    "abstract": "We construct a two-layered model for learning and generating sequential data\nthat is both computationally fast and competitive with vanilla Tsetlin\nmachines, adding numerous advantages. Through the use of hyperdimensional\nvector computing (HVC) algebras and Tsetlin machine clause structures, we\ndemonstrate that the combination of both inherits the generality of data\nencoding and decoding of HVC with the fast interpretable nature of Tsetlin\nmachines to yield a powerful machine learning model. We apply the approach in\ntwo areas, namely in forecasting, generating new sequences, and classification.\nFor the latter, we derive results for the entire UCR Time Series Archive and\ncompare with the standard benchmarks to see how well the method competes in\ntime series classification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16620v1",
    "published_date": "2024-08-29 15:28:01 UTC",
    "updated_date": "2024-08-29 15:28:01 UTC"
  },
  {
    "arxiv_id": "2408.16601v1",
    "title": "Examination of Code generated by Large Language Models",
    "authors": [
      "Robin Beer",
      "Alexander Feix",
      "Tim Guttzeit",
      "Tamara Muras",
      "Vincent Müller",
      "Maurice Rauscher",
      "Florian Schäffler",
      "Welf Löwe"
    ],
    "abstract": "Large language models (LLMs), such as ChatGPT and Copilot, are transforming\nsoftware development by automating code generation and, arguably, enable rapid\nprototyping, support education, and boost productivity. Therefore, correctness\nand quality of the generated code should be on par with manually written code.\nTo assess the current state of LLMs in generating correct code of high quality,\nwe conducted controlled experiments with ChatGPT and Copilot: we let the LLMs\ngenerate simple algorithms in Java and Python along with the corresponding unit\ntests and assessed the correctness and the quality (coverage) of the generated\n(test) codes. We observed significant differences between the LLMs, between the\nlanguages, between algorithm and test codes, and over time. The present paper\nreports these results together with the experimental methods allowing repeated\nand comparable assessments for more algorithms, languages, and LLMs over time.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "I.2.2"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16601v1",
    "published_date": "2024-08-29 15:12:16 UTC",
    "updated_date": "2024-08-29 15:12:16 UTC"
  },
  {
    "arxiv_id": "2408.16586v2",
    "title": "Enhancing Dialogue Generation in Werewolf Game Through Situation Analysis and Persuasion Strategies",
    "authors": [
      "Zhiyang Qi",
      "Michimasa Inaba"
    ],
    "abstract": "Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs) like GPT-4, have significantly enhanced dialogue\nsystems, enabling them to generate more natural and fluent conversations.\nDespite these improvements, challenges persist, such as managing continuous\ndialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024\naddresses these challenges by employing the Werewolf Game, an incomplete\ninformation game, to test the capabilities of LLMs in complex interactive\nenvironments. This paper introduces a LLM-based Werewolf Game AI, where each\nrole is supported by situation analysis to aid response generation.\nAdditionally, for the werewolf role, various persuasion strategies, including\nlogical appeal, credibility appeal, and emotional appeal, are employed to\neffectively persuade other players to align with its actions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the AIWolfDial2024 workshop at INLG 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.16586v2",
    "published_date": "2024-08-29 14:49:13 UTC",
    "updated_date": "2024-09-04 02:24:08 UTC"
  },
  {
    "arxiv_id": "2408.16577v2",
    "title": "Seeking the Sufficiency and Necessity Causal Features in Multimodal Representation Learning",
    "authors": [
      "Boyu Chen",
      "Junjie Liu",
      "Zhu Li",
      "Mengyue Yang"
    ],
    "abstract": "Probability of necessity and sufficiency (PNS) measures the likelihood of a\nfeature set being both necessary and sufficient for predicting an outcome. It\nhas proven effective in guiding representation learning for unimodal data,\nenhancing both predictive performance and model robustness. Despite these\nbenefits, extending PNS to multimodal settings remains unexplored. This\nextension presents unique challenges, as the conditions for PNS estimation,\nexogeneity and monotonicity, need to be reconsidered in a multimodal context.\nWe address these challenges by first conceptualizing multimodal representations\nas comprising modality-invariant and modality-specific components. We then\nanalyze how to compute PNS for each component while ensuring non-trivial PNS\nestimation. Based on these analyses, we formulate tractable optimization\nobjectives that enable multimodal models to learn high-PNS representations.\nExperiments demonstrate the effectiveness of our method on both synthetic and\nreal-world data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16577v2",
    "published_date": "2024-08-29 14:43:42 UTC",
    "updated_date": "2024-11-26 18:54:29 UTC"
  },
  {
    "arxiv_id": "2408.16537v2",
    "title": "SFR-GNN: Simple and Fast Robust GNNs against Structural Attacks",
    "authors": [
      "Xing Ai",
      "Guanyu Zhu",
      "Yulin Zhu",
      "Yu Zheng",
      "Gaolei Li",
      "Jianhua Li",
      "Kai Zhou"
    ],
    "abstract": "Graph Neural Networks (GNNs) have demonstrated commendable performance for\ngraph-structured data. Yet, GNNs are often vulnerable to adversarial structural\nattacks as embedding generation relies on graph topology. Existing efforts are\ndedicated to purifying the maliciously modified structure or applying adaptive\naggregation, thereby enhancing the robustness against adversarial structural\nattacks. It is inevitable for a defender to consume heavy computational costs\ndue to lacking prior knowledge about modified structures. To this end, we\npropose an efficient defense method, called Simple and Fast Robust Graph Neural\nNetwork (SFR-GNN), supported by mutual information theory. The SFR-GNN first\npre-trains a GNN model using node attributes and then fine-tunes it over the\nmodified graph in the manner of contrastive learning, which is free of\npurifying modified structures and adaptive aggregation, thus achieving great\nefficiency gains. Consequently, SFR-GNN exhibits a 24%--162% speedup compared\nto advanced robust models, demonstrating superior robustness for node\nclassification tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16537v2",
    "published_date": "2024-08-29 13:52:28 UTC",
    "updated_date": "2024-09-01 11:27:45 UTC"
  },
  {
    "arxiv_id": "2408.16517v1",
    "title": "Adaptive Variational Continual Learning via Task-Heuristic Modelling",
    "authors": [
      "Fan Yang"
    ],
    "abstract": "Variational continual learning (VCL) is a turn-key learning algorithm that\nhas state-of-the-art performance among the best continual learning models. In\nour work, we explore an extension of the generalized variational continual\nlearning (GVCL) model, named AutoVCL, which combines task heuristics for\ninformed learning and model optimization. We demonstrate that our model\noutperforms the standard GVCL with fixed hyperparameters, benefiting from the\nautomatic adjustment of the hyperparameter based on the difficulty and\nsimilarity of the incoming task compared to the previous tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "4 pages, 2 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.16517v1",
    "published_date": "2024-08-29 13:28:11 UTC",
    "updated_date": "2024-08-29 13:28:11 UTC"
  },
  {
    "arxiv_id": "2408.16803v1",
    "title": "HLogformer: A Hierarchical Transformer for Representing Log Data",
    "authors": [
      "Zhichao Hou",
      "Mina Ghashami",
      "Mikhail Kuznetsov",
      "MohamadAli Torkamani"
    ],
    "abstract": "Transformers have gained widespread acclaim for their versatility in handling\ndiverse data structures, yet their application to log data remains\nunderexplored. Log data, characterized by its hierarchical, dictionary-like\nstructure, poses unique challenges when processed using conventional\ntransformer models. Traditional methods often rely on manually crafted\ntemplates for parsing logs, a process that is labor-intensive and lacks\ngeneralizability. Additionally, the linear treatment of log sequences by\nstandard transformers neglects the rich, nested relationships within log\nentries, leading to suboptimal representations and excessive memory usage.\n  To address these issues, we introduce HLogformer, a novel hierarchical\ntransformer framework specifically designed for log data. HLogformer leverages\nthe hierarchical structure of log entries to significantly reduce memory costs\nand enhance representation learning. Unlike traditional models that treat log\ndata as flat sequences, our framework processes log entries in a manner that\nrespects their inherent hierarchical organization. This approach ensures\ncomprehensive encoding of both fine-grained details and broader contextual\nrelationships.\n  Our contributions are threefold: First, HLogformer is the first framework to\ndesign a dynamic hierarchical transformer tailored for dictionary-like log\ndata. Second, it dramatically reduces memory costs associated with processing\nextensive log sequences. Third, comprehensive experiments demonstrate that\nHLogformer more effectively encodes hierarchical contextual information,\nproving to be highly effective for downstream tasks such as synthetic anomaly\ndetection and product recommendation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16803v1",
    "published_date": "2024-08-29 13:08:41 UTC",
    "updated_date": "2024-08-29 13:08:41 UTC"
  },
  {
    "arxiv_id": "2409.00134v5",
    "title": "MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale",
    "authors": [
      "Anton Andreychuk",
      "Konstantin Yakovlev",
      "Aleksandr Panov",
      "Alexey Skrynnik"
    ],
    "abstract": "Multi-agent pathfinding (MAPF) is a problem that generally requires finding\ncollision-free paths for multiple agents in a shared environment. Solving MAPF\noptimally, even under restrictive assumptions, is NP-hard, yet efficient\nsolutions for this problem are critical for numerous applications, such as\nautomated warehouses and transportation systems. Recently, learning-based\napproaches to MAPF have gained attention, particularly those leveraging deep\nreinforcement learning. Typically, such learning-based MAPF solvers are\naugmented with additional components like single-agent planning or\ncommunication. Orthogonally, in this work we rely solely on imitation learning\nthat leverages a large dataset of expert MAPF solutions and transformer-based\nneural network to create a foundation model for MAPF called MAPF-GPT. The\nlatter is capable of generating actions without additional heuristics or\ncommunication. MAPF-GPT demonstrates zero-shot learning abilities when solving\nthe MAPF problems that are not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable MAPF solvers\non a diverse range of problem instances and is computationally efficient during\ninference.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00134v5",
    "published_date": "2024-08-29 12:55:10 UTC",
    "updated_date": "2025-04-08 07:32:56 UTC"
  },
  {
    "arxiv_id": "2408.16495v1",
    "title": "On-device AI: Quantization-aware Training of Transformers in Time-Series",
    "authors": [
      "Tianheng Ling",
      "Gregor Schiele"
    ],
    "abstract": "Artificial Intelligence (AI) models for time-series in pervasive computing\nkeep getting larger and more complicated. The Transformer model is by far the\nmost compelling of these AI models. However, it is difficult to obtain the\ndesired performance when deploying such a massive model on a sensor device with\nlimited resources. My research focuses on optimizing the Transformer model for\ntime-series forecasting tasks. The optimized model will be deployed as hardware\naccelerators on embedded Field Programmable Gate Arrays (FPGAs). I will\ninvestigate the impact of applying Quantization-aware Training to the\nTransformer model to reduce its size and runtime memory footprint while\nmaximizing the advantages of FPGAs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is accepted by 2023 IEEE International Conference on\n  Pervasive Computing and Communications(PhD Forum)",
    "pdf_url": "http://arxiv.org/pdf/2408.16495v1",
    "published_date": "2024-08-29 12:49:22 UTC",
    "updated_date": "2024-08-29 12:49:22 UTC"
  },
  {
    "arxiv_id": "2409.09044v1",
    "title": "ElasticAI: Creating and Deploying Energy-Efficient Deep Learning Accelerator for Pervasive Computing",
    "authors": [
      "Chao Qian",
      "Tianheng Ling",
      "Gregor Schiele"
    ],
    "abstract": "Deploying Deep Learning (DL) on embedded end devices is a scorching trend in\npervasive computing. Since most Microcontrollers on embedded devices have\nlimited computing power, it is necessary to add a DL accelerator. Embedded\nField Programmable Gate Arrays (FPGAs) are suitable for deploying DL\naccelerators for embedded devices, but developing an energy-efficient DL\naccelerator on an FPGA is not easy. Therefore, we propose the\nElasticAI-Workflow that aims to help DL developers to create and deploy DL\nmodels as hardware accelerators on embedded FPGAs. This workflow consists of\ntwo key components: the ElasticAI-Creator and the Elastic Node. The former is a\ntoolchain for automatically generating DL accelerators on FPGAs. The latter is\na hardware platform for verifying the performance of the generated\naccelerators. With this combination, the performance of the accelerator can be\nsufficiently guaranteed. We will demonstrate the potential of our approach\nthrough a case study.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "The paper is accepted by 2023 IEEE International Conference on\n  Pervasive Computing and Communications (Best Demo Award)",
    "pdf_url": "http://arxiv.org/pdf/2409.09044v1",
    "published_date": "2024-08-29 12:39:44 UTC",
    "updated_date": "2024-08-29 12:39:44 UTC"
  },
  {
    "arxiv_id": "2409.00133v1",
    "title": "A Survey for Large Language Models in Biomedicine",
    "authors": [
      "Chong Wang",
      "Mengyao Li",
      "Junjun He",
      "Zhongruo Wang",
      "Erfan Darzi",
      "Zan Chen",
      "Jin Ye",
      "Tianbin Li",
      "Yanzhou Su",
      "Jing Ke",
      "Kaili Qu",
      "Shuxin Li",
      "Yi Yu",
      "Pietro Liò",
      "Tianyun Wang",
      "Yu Guang Wang",
      "Yiqing Shen"
    ],
    "abstract": "Recent breakthroughs in large language models (LLMs) offer unprecedented\nnatural language understanding and generation capabilities. However, existing\nsurveys on LLMs in biomedicine often focus on specific applications or model\narchitectures, lacking a comprehensive analysis that integrates the latest\nadvancements across various biomedical domains. This review, based on an\nanalysis of 484 publications sourced from databases including PubMed, Web of\nScience, and arXiv, provides an in-depth examination of the current landscape,\napplications, challenges, and prospects of LLMs in biomedicine, distinguishing\nitself by focusing on the practical implications of these models in real-world\nbiomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot\nlearning across a broad spectrum of biomedical tasks, including diagnostic\nassistance, drug discovery, and personalized medicine, among others, with\ninsights drawn from 137 key studies. Then, we discuss adaptation strategies of\nLLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to\nenhance their performance in specialized biomedical contexts where zero-shot\nfails to achieve, such as medical question answering and efficient processing\nof biomedical literature. Finally, we discuss the challenges that LLMs face in\nthe biomedicine domain including data privacy concerns, limited model\ninterpretability, issues with dataset quality, and ethics due to the sensitive\nnature of biomedical data, the need for highly reliable model outputs, and the\nethical implications of deploying AI in healthcare. To address these\nchallenges, we also identify future research directions of LLM in biomedicine\nincluding federated learning methods to preserve data privacy and integrating\nexplainable AI methodologies to enhance the transparency of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00133v1",
    "published_date": "2024-08-29 12:39:16 UTC",
    "updated_date": "2024-08-29 12:39:16 UTC"
  },
  {
    "arxiv_id": "2408.16442v1",
    "title": "Integrating Features for Recognizing Human Activities through Optimized Parameters in Graph Convolutional Networks and Transformer Architectures",
    "authors": [
      "Mohammad Belal",
      "Taimur Hassan",
      "Abdelfatah Hassan",
      "Nael Alsheikh",
      "Noureldin Elhendawi",
      "Irfan Hussain"
    ],
    "abstract": "Human activity recognition is a major field of study that employs computer\nvision, machine vision, and deep learning techniques to categorize human\nactions. The field of deep learning has made significant progress, with\narchitectures that are extremely effective at capturing human dynamics. This\nstudy emphasizes the influence of feature fusion on the accuracy of activity\nrecognition. This technique addresses the limitation of conventional models,\nwhich face difficulties in identifying activities because of their limited\ncapacity to understand spatial and temporal features. The technique employs\nsensory data obtained from four publicly available datasets: HuGaDB, PKU-MMD,\nLARa, and TUG. The accuracy and F1-score of two deep learning models,\nspecifically a Transformer model and a Parameter-Optimized Graph Convolutional\nNetwork (PO-GCN), were evaluated using these datasets. The feature fusion\ntechnique integrated the final layer features from both models and inputted\nthem into a classifier. Empirical evidence demonstrates that PO-GCN outperforms\nstandard models in activity recognition. HuGaDB demonstrated a 2.3% improvement\nin accuracy and a 2.2% increase in F1-score. TUG showed a 5% increase in\naccuracy and a 0.5% rise in F1-score. On the other hand, LARa and PKU-MMD\nachieved lower accuracies of 64% and 69% respectively. This indicates that the\nintegration of features enhanced the performance of both the Transformer model\nand PO-GCN.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 1 figure, conference",
    "pdf_url": "http://arxiv.org/pdf/2408.16442v1",
    "published_date": "2024-08-29 11:07:48 UTC",
    "updated_date": "2024-08-29 11:07:48 UTC"
  },
  {
    "arxiv_id": "2408.16429v2",
    "title": "Gradient-free variational learning with conditional mixture networks",
    "authors": [
      "Conor Heins",
      "Hao Wu",
      "Dimitrije Markovic",
      "Alexander Tschantz",
      "Jeff Beck",
      "Christopher Buckley"
    ],
    "abstract": "Balancing computational efficiency with robust predictive performance is\ncrucial in supervised learning, especially for critical applications. Standard\ndeep learning models, while accurate and scalable, often lack probabilistic\nfeatures like calibrated predictions and uncertainty quantification. Bayesian\nmethods address these issues but can be computationally expensive as model and\ndata complexity increase. Previous work shows that fast variational methods can\nreduce the compute requirements of Bayesian methods by eliminating the need for\ngradient computation or sampling, but are often limited to simple models. We\nintroduce CAVI-CMN, a fast, gradient-free variational method for training\nconditional mixture networks (CMNs), a probabilistic variant of the\nmixture-of-experts (MoE) model. CMNs are composed of linear experts and a\nsoftmax gating network. By exploiting conditional conjugacy and P\\'olya-Gamma\naugmentation, we furnish Gaussian likelihoods for the weights of both the\nlinear layers and the gating network. This enables efficient variational\nupdates using coordinate ascent variational inference (CAVI), avoiding\ntraditional gradient-based optimization. We validate this approach by training\ntwo-layer CMNs on standard classification benchmarks from the UCI repository.\nCAVI-CMN achieves competitive and often superior predictive accuracy compared\nto maximum likelihood estimation (MLE) with backpropagation, while maintaining\ncompetitive runtime and full posterior distributions over all model parameters.\nMoreover, as input size or the number of experts increases, computation time\nscales competitively with MLE and other gradient-based solutions like black-box\nvariational inference (BBVI), making CAVI-CMN a promising tool for deep, fast,\nand gradient-free Bayesian networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages main text (3 figures), including references. 9 pages\n  supplementary material (5 figures). Accepted at NeurIPS Bayesian Decision\n  Making and Uncertainty Workshop (2024): https://neurips.cc/virtual/2024/98879",
    "pdf_url": "http://arxiv.org/pdf/2408.16429v2",
    "published_date": "2024-08-29 10:43:55 UTC",
    "updated_date": "2025-02-10 09:54:25 UTC"
  },
  {
    "arxiv_id": "2408.16426v1",
    "title": "COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation",
    "authors": [
      "Jiefeng Li",
      "Ye Yuan",
      "Davis Rempe",
      "Haotian Zhang",
      "Pavlo Molchanov",
      "Cewu Lu",
      "Jan Kautz",
      "Umar Iqbal"
    ],
    "abstract": "Estimating global human motion from moving cameras is challenging due to the\nentanglement of human and camera motions. To mitigate the ambiguity, existing\nmethods leverage learned human motion priors, which however often result in\noversmoothed motions with misaligned 2D projections. To tackle this problem, we\npropose COIN, a control-inpainting motion diffusion prior that enables\nfine-grained control to disentangle human and camera motions. Although\npre-trained motion diffusion models encode rich motion priors, we find it\nnon-trivial to leverage such knowledge to guide global motion estimation from\nRGB videos. COIN introduces a novel control-inpainting score distillation\nsampling method to ensure well-aligned, consistent, and high-quality motion\nfrom the diffusion prior within a joint optimization framework. Furthermore, we\nintroduce a new human-scene relation loss to alleviate the scale ambiguity by\nenforcing consistency among the humans, camera, and scene. Experiments on three\nchallenging benchmarks demonstrate the effectiveness of COIN, which outperforms\nthe state-of-the-art methods in terms of global human motion estimation and\ncamera motion estimation. As an illustrative example, COIN outperforms the\nstate-of-the-art method by 33% in world joint position error (W-MPJPE) on the\nRICH dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.16426v1",
    "published_date": "2024-08-29 10:36:29 UTC",
    "updated_date": "2024-08-29 10:36:29 UTC"
  },
  {
    "arxiv_id": "2408.16414v2",
    "title": "Spectral Informed Neural Network: An Efficient and Low-Memory PINN",
    "authors": [
      "Tianchi Yu",
      "Yiming Qi",
      "Ivan Oseledets",
      "Shiyi Chen"
    ],
    "abstract": "With growing investigations into solving partial differential equations by\nphysics-informed neural networks (PINNs), more accurate and efficient PINNs are\nrequired to meet the practical demands of scientific computing. One bottleneck\nof current PINNs is computing the high-order derivatives via automatic\ndifferentiation which often necessitates substantial computing resources. In\nthis paper, we focus on removing the automatic differentiation of the spatial\nderivatives and propose a spectral-based neural network that substitutes the\ndifferential operator with a multiplication. Compared to the PINNs, our\napproach requires lower memory and shorter training time. Thanks to the\nexponential convergence of the spectral basis, our approach is more accurate.\nMoreover, to handle the different situations between physics domain and\nspectral domain, we provide two strategies to train networks by their spectral\ninformation. Through a series of comprehensive experiments, We validate the\naforementioned merits of our proposed network.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16414v2",
    "published_date": "2024-08-29 10:21:00 UTC",
    "updated_date": "2024-10-08 13:27:38 UTC"
  },
  {
    "arxiv_id": "2408.16798v1",
    "title": "Generative AI in Ship Design",
    "authors": [
      "Sahil Thakur",
      "Navneet V Saxena",
      "Prof Sitikantha Roy"
    ],
    "abstract": "The process of ship design is intricate, heavily influenced by the hull form\nwhich accounts for approximately 70% of the total cost. Traditional methods\nrely on human-driven iterative processes based on naval architecture principles\nand engineering analysis. In contrast, generative AI presents a novel approach,\nutilizing computational algorithms rooted in machine learning and artificial\nintelligence to optimize ship hull design. This report outlines the systematic\ncreation of a generative AI for this purpose, involving steps such as dataset\ncollection, model architecture selection, training, and validation. Utilizing\nthe \"SHIP-D\" dataset, consisting of 30,000 hull forms, the report adopts the\nGaussian Mixture Model (GMM) as the generative model architecture. GMMs offer a\nstatistical framework to analyze data distribution, crucial for generating\ninnovative ship designs efficiently. Overall, this approach holds promise in\nrevolutionizing ship design by exploring a broader design space and integrating\nmultidisciplinary optimization objectives effectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16798v1",
    "published_date": "2024-08-29 08:55:35 UTC",
    "updated_date": "2024-08-29 08:55:35 UTC"
  },
  {
    "arxiv_id": "2409.09042v1",
    "title": "Semantic Communication for Cooperative Perception using HARQ",
    "authors": [
      "Yucheng Sheng",
      "Le Liang",
      "Hao Ye",
      "Shi Jin",
      "Geoffrey Ye Li"
    ],
    "abstract": "Cooperative perception, offering a wider field of view than standalone\nperception, is becoming increasingly crucial in autonomous driving. This\nperception is enabled through vehicle-to-vehicle (V2V) communication, allowing\nconnected automated vehicles (CAVs) to exchange sensor data, such as light\ndetection and ranging (LiDAR) point clouds, thereby enhancing the collective\nunderstanding of the environment. In this paper, we leverage an importance map\nto distill critical semantic information, introducing a cooperative perception\nsemantic communication framework that employs intermediate fusion. To counter\nthe challenges posed by time-varying multipath fading, our approach\nincorporates the use of orthogonal frequency-division multiplexing (OFDM) along\nwith channel estimation and equalization strategies. Furthermore, recognizing\nthe necessity for reliable transmission, especially in the low SNR scenarios,\nwe introduce a novel semantic error detection method that is integrated with\nour semantic communication framework in the spirit of hybrid automatic repeated\nrequest (HARQ). Simulation results show that our model surpasses the\ntraditional separate source-channel coding methods in perception performance,\nboth with and without HARQ. Additionally, in terms of throughput, our proposed\nHARQ schemes demonstrate superior efficiency to the conventional coding\napproaches.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09042v1",
    "published_date": "2024-08-29 08:53:26 UTC",
    "updated_date": "2024-08-29 08:53:26 UTC"
  },
  {
    "arxiv_id": "2408.16353v1",
    "title": "DetectBERT: Towards Full App-Level Representation Learning to Detect Android Malware",
    "authors": [
      "Tiezhu Sun",
      "Nadia Daoudi",
      "Kisub Kim",
      "Kevin Allix",
      "Tegawendé F. Bissyandé",
      "Jacques Klein"
    ],
    "abstract": "Recent advancements in ML and DL have significantly improved Android malware\ndetection, yet many methodologies still rely on basic static analysis,\nbytecode, or function call graphs that often fail to capture complex malicious\nbehaviors. DexBERT, a pre-trained BERT-like model tailored for Android\nrepresentation learning, enriches class-level representations by analyzing\nSmali code extracted from APKs. However, its functionality is constrained by\nits inability to process multiple Smali classes simultaneously. This paper\nintroduces DetectBERT, which integrates correlated Multiple Instance Learning\n(c-MIL) with DexBERT to handle the high dimensionality and variability of\nAndroid malware, enabling effective app-level detection. By treating\nclass-level features as instances within MIL bags, DetectBERT aggregates these\ninto a comprehensive app-level representation. Our evaluation demonstrates that\nDetectBERT not only surpasses existing state-of-the-art detection methods but\nalso adapts to evolving malware threats. Moreover, the versatility of the\nDetectBERT framework holds promising potential for broader applications in\napp-level analysis and other software engineering tasks, offering new avenues\nfor research and development.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted at ESEM 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.16353v1",
    "published_date": "2024-08-29 08:47:25 UTC",
    "updated_date": "2024-08-29 08:47:25 UTC"
  },
  {
    "arxiv_id": "2409.00131v1",
    "title": "Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems",
    "authors": [
      "Ding Kai",
      "Ma Zhenguo",
      "Yan Xiaoran"
    ],
    "abstract": "This study focuses on improving the performance of lightweight Large Language\nModels (LLMs) in mathematical reasoning tasks. We introduce a novel method for\nmeasuring mathematical logic similarity and design an automatic screening\nmechanism to construct a set of reference problems that integrate both semantic\nand logical similarity. By employing carefully crafted positive and negative\nexample prompts, we guide the model towards adopting sound reasoning logic. To\nthe best of our knowledge, this is the first attempt to utilize\nretrieval-enhanced generation for mathematical problem-solving. Experimental\nresults demonstrate that our method achieves a 15.8% improvement over the Chain\nof Thought approach on the SVAMP dataset and a 21.5 % improvement on the GSM8K\ndataset. Further application of this method to a large-scale model with 175\nbillion parameters yields performance comparable to the best results on both\naforementioned datasets. Finally, we conduct an analysis of errors during the\nreasoning process, providing valuable insights and directions for future\nresearch on reasoning tasks using large language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00131v1",
    "published_date": "2024-08-29 08:26:42 UTC",
    "updated_date": "2024-08-29 08:26:42 UTC"
  },
  {
    "arxiv_id": "2408.16343v2",
    "title": "Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach",
    "authors": [
      "Yifei Chen",
      "Shenghao Zhu",
      "Zhaojie Fang",
      "Chang Liu",
      "Binfeng Zou",
      "Yuhe Wang",
      "Shuo Chang",
      "Fan Jia",
      "Feiwei Qin",
      "Jin Fan",
      "Yong Peng",
      "Changmiao Wang"
    ],
    "abstract": "Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by\nmemory loss, executive dysfunction, and personality changes. Early diagnosis is\nchallenging due to subtle symptoms and varied presentations, often leading to\nmisdiagnosis with traditional unimodal diagnostic methods due to their limited\nscope. This study introduces an advanced multimodal classification model that\nintegrates clinical, cognitive, neuroimaging, and EEG data to enhance\ndiagnostic accuracy. The model incorporates a feature tagger with a tabular\ndata coding architecture and utilizes the TimesBlock module to capture\nintricate temporal patterns in Electroencephalograms (EEG) data. By employing\nCross-modal Attention Aggregation module, the model effectively fuses Magnetic\nResonance Imaging (MRI) spatial information with EEG temporal data,\nsignificantly improving the distinction between AD, Mild Cognitive Impairment,\nand Normal Cognition. Simultaneously, we have constructed the first AD\nclassification dataset that includes three modalities: EEG, MRI, and tabular\ndata. Our innovative approach aims to facilitate early diagnosis and\nintervention, potentially slowing the progression of AD. The source code and\nour private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16343v2",
    "published_date": "2024-08-29 08:26:00 UTC",
    "updated_date": "2025-01-03 05:07:10 UTC"
  },
  {
    "arxiv_id": "2408.16333v1",
    "title": "Self-Improving Diffusion Models with Synthetic Data",
    "authors": [
      "Sina Alemohammad",
      "Ahmed Imtiaz Humayun",
      "Shruti Agarwal",
      "John Collomosse",
      "Richard Baraniuk"
    ],
    "abstract": "The artificial intelligence (AI) world is running out of real data for\ntraining increasingly large generative models, resulting in accelerating\npressure to train on synthetic data. Unfortunately, training new generative\nmodels with synthetic data from current or past generation models creates an\nautophagous (self-consuming) loop that degrades the quality and/or diversity of\nthe synthetic data in what has been termed model autophagy disorder (MAD) and\nmodel collapse. Current thinking around model autophagy recommends that\nsynthetic data is to be avoided for model training lest the system deteriorate\ninto MADness. In this paper, we take a different tack that treats synthetic\ndata differently from real data. Self-IMproving diffusion models with Synthetic\ndata (SIMS) is a new training concept for diffusion models that uses\nself-synthesized data to provide negative guidance during the generation\nprocess to steer a model's generative process away from the non-ideal synthetic\ndata manifold and towards the real data distribution. We demonstrate that SIMS\nis capable of self-improvement; it establishes new records based on the\nFr\\'echet inception distance (FID) metric for CIFAR-10 and ImageNet-64\ngeneration and achieves competitive results on FFHQ-64 and ImageNet-512.\nMoreover, SIMS is, to the best of our knowledge, the first prophylactic\ngenerative AI algorithm that can be iteratively trained on self-generated\nsynthetic data without going MAD. As a bonus, SIMS can adjust a diffusion\nmodel's synthetic data distribution to match any desired in-domain target\ndistribution to help mitigate biases and ensure fairness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16333v1",
    "published_date": "2024-08-29 08:12:18 UTC",
    "updated_date": "2024-08-29 08:12:18 UTC"
  },
  {
    "arxiv_id": "2408.16331v1",
    "title": "Guided Reasoning: A Non-Technical Introduction",
    "authors": [
      "Gregor Betz"
    ],
    "abstract": "We introduce the concept and a default implementation of Guided Reasoning. A\nmulti-agent system is a Guided Reasoning system iff one agent (the guide)\nprimarily interacts with other agents in order to improve reasoning quality. We\ndescribe Logikon's default implementation of Guided Reasoning in non-technical\nterms. This is a living document we'll gradually enrich with more detailed\ninformation and examples.\n  Code: https://github.com/logikon-ai/logikon",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16331v1",
    "published_date": "2024-08-29 08:08:37 UTC",
    "updated_date": "2024-08-29 08:08:37 UTC"
  },
  {
    "arxiv_id": "2408.16313v1",
    "title": "FA-YOLO: Research On Efficient Feature Selection YOLO Improved Algorithm Based On FMDS and AGMF Modules",
    "authors": [
      "Yukang Huo",
      "Mingyuan Yao",
      "Qingbin Tian",
      "Tonghao Wang",
      "Ruifeng Wang",
      "Haihua Wang"
    ],
    "abstract": "Over the past few years, the YOLO series of models has emerged as one of the\ndominant methodologies in the realm of object detection. Many studies have\nadvanced these baseline models by modifying their architectures, enhancing data\nquality, and developing new loss functions. However, current models still\nexhibit deficiencies in processing feature maps, such as overlooking the fusion\nof cross-scale features and a static fusion approach that lacks the capability\nfor dynamic feature adjustment. To address these issues, this paper introduces\nan efficient Fine-grained Multi-scale Dynamic Selection Module (FMDS Module),\nwhich applies a more effective dynamic feature selection and fusion method on\nfine-grained multi-scale feature maps, significantly enhancing the detection\naccuracy of small, medium, and large-sized targets in complex environments.\nFurthermore, this paper proposes an Adaptive Gated Multi-branch Focus Fusion\nModule (AGMF Module), which utilizes multiple parallel branches to perform\ncomplementary fusion of various features captured by the gated unit branch,\nFMDS Module branch, and TripletAttention branch. This approach further enhances\nthe comprehensiveness, diversity, and integrity of feature fusion. This paper\nhas integrated the FMDS Module, AGMF Module, into Yolov9 to develop a novel\nobject detection model named FA-YOLO. Extensive experimental results show that\nunder identical experimental conditions, FA-YOLO achieves an outstanding 66.1%\nmean Average Precision (mAP) on the PASCAL VOC 2007 dataset, representing 1.0%\nimprovement over YOLOv9's 65.1%. Additionally, the detection accuracies of\nFA-YOLO for small, medium, and large targets are 44.1%, 54.6%, and 70.8%,\nrespectively, showing improvements of 2.0%, 3.1%, and 0.9% compared to YOLOv9's\n42.1%, 51.5%, and 69.9%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages and 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16313v1",
    "published_date": "2024-08-29 07:22:16 UTC",
    "updated_date": "2024-08-29 07:22:16 UTC"
  },
  {
    "arxiv_id": "2408.16307v2",
    "title": "Safe Bayesian Optimization for Complex Control Systems via Additive Gaussian Processes",
    "authors": [
      "Hongxuan Wang",
      "Xiaocong Li",
      "Lihao Zheng",
      "Adrish Bhaumik",
      "Prahlad Vadakkepat"
    ],
    "abstract": "Controller tuning and optimization have been among the most fundamental\nproblems in robotics and mechatronic systems. The traditional methodology is\nusually model-based, but its performance heavily relies on an accurate\nmathematical system model. In control applications with complex dynamics,\nobtaining a precise model is often challenging, leading us towards a\ndata-driven approach. While various researchers have explored the optimization\nof a single controller, it remains a challenge to obtain the optimal controller\nparameters safely and efficiently when multiple controllers are involved. In\nthis paper, we propose SafeCtrlBO to optimize multiple controllers\nsimultaneously and safely. We simplify the exploration process in safe Bayesian\noptimization, reducing computational effort without sacrificing expansion\ncapability. Additionally, we use additive kernels to enhance the efficiency of\nGaussian process updates for unknown functions. Hardware experimental results\non a permanent magnet synchronous motor (PMSM) demonstrate that compared to\nexisting safe Bayesian optimization algorithms, SafeCtrlBO can obtain optimal\nparameters more efficiently while ensuring safety.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "25 pages, 8 figures, 20 subfigures, 1 table. Under Review",
    "pdf_url": "http://arxiv.org/pdf/2408.16307v2",
    "published_date": "2024-08-29 07:12:37 UTC",
    "updated_date": "2024-11-25 07:20:06 UTC"
  },
  {
    "arxiv_id": "2409.08290v1",
    "title": "Reconsidering the energy efficiency of spiking neural networks",
    "authors": [
      "Zhanglu Yan",
      "Zhenyu Bai",
      "Weng-Fai Wong"
    ],
    "abstract": "Spiking neural networks (SNNs) are generally regarded as more\nenergy-efficient because they do not use multiplications. However, most SNN\nworks only consider the counting of additions to evaluate energy consumption,\nneglecting other overheads such as memory accesses and data movement\noperations. This oversight can lead to a misleading perception of efficiency,\nespecially when state-of-the-art SNN accelerators operate with very small time\nwindow sizes. In this paper, we present a detailed comparison of the energy\nconsumption of artificial neural networks (ANNs) and SNNs from a hardware\nperspective. We provide accurate formulas for energy consumption based on\nclassical multi-level memory hierarchy architectures, commonly used\nneuromorphic dataflow architectures, and our proposed improved spatial-dataflow\narchitecture. Our research demonstrates that to achieve comparable accuracy and\ngreater energy efficiency than ANNs, SNNs require strict limitations on both\ntime window size T and sparsity s. For instance, with the VGG16 model and a\nfixed T of 6, the neuron sparsity rate must exceed 93% to ensure energy\nefficiency across most architectures. Inspired by our findings, we explore\nstrategies to enhance energy efficiency by increasing sparsity. We introduce\ntwo regularization terms during training that constrain weights and\nactivations, effectively boosting the sparsity rate. Our experiments on the\nCIFAR-10 dataset, using T of 6, show that our SNNs consume 69% of the energy\nused by optimized ANNs on spatial-dataflow architectures, while maintaining an\nSNN accuracy of 94.18%. This framework, developed using PyTorch, is publicly\navailable for use and further research.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08290v1",
    "published_date": "2024-08-29 07:00:35 UTC",
    "updated_date": "2024-08-29 07:00:35 UTC"
  },
  {
    "arxiv_id": "2408.16293v1",
    "title": "Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems",
    "authors": [
      "Tian Ye",
      "Zicheng Xu",
      "Yuanzhi Li",
      "Zeyuan Allen-Zhu"
    ],
    "abstract": "Language models have demonstrated remarkable performance in solving reasoning\ntasks; however, even the strongest models still occasionally make reasoning\nmistakes. Recently, there has been active research aimed at improving reasoning\naccuracy, particularly by using pretrained language models to \"self-correct\"\ntheir mistakes via multi-round prompting. In this paper, we follow this line of\nwork but focus on understanding the usefulness of incorporating\n\"error-correction\" data directly into the pretraining stage. This data consists\nof erroneous solution steps immediately followed by their corrections. Using a\nsynthetic math dataset, we show promising results: this type of pretrain data\ncan help language models achieve higher reasoning accuracy directly (i.e.,\nthrough simple auto-regression, without multi-round prompting) compared to\npretraining on the same amount of error-free data. We also delve into many\ndetails, such as (1) how this approach differs from beam search, (2) how such\ndata can be prepared, (3) whether masking is needed on the erroneous tokens,\n(4) the amount of error required, (5) whether such data can be deferred to the\nfine-tuning stage, and many others.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2407.20311",
    "pdf_url": "http://arxiv.org/pdf/2408.16293v1",
    "published_date": "2024-08-29 06:49:20 UTC",
    "updated_date": "2024-08-29 06:49:20 UTC"
  },
  {
    "arxiv_id": "2408.16288v2",
    "title": "OpenFGL: A Comprehensive Benchmark for Federated Graph Learning",
    "authors": [
      "Xunkai Li",
      "Yinlin Zhu",
      "Boyang Pang",
      "Guochen Yan",
      "Yeyu Yan",
      "Zening Li",
      "Zhengyu Wu",
      "Wentao Zhang",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "abstract": "Federated graph learning (FGL) is a promising distributed training paradigm\nfor graph neural networks across multiple local systems without direct data\nsharing. This approach inherently involves large-scale distributed graph\nprocessing, which closely aligns with the challenges and research focuses of\ngraph-based data systems. Despite the proliferation of FGL, the diverse\nmotivations from real-world applications, spanning various research backgrounds\nand settings, pose a significant challenge to fair evaluation. To fill this\ngap, we propose OpenFGL, a unified benchmark designed for the primary FGL\nscenarios: Graph-FL and Subgraph-FL. Specifically, OpenFGL includes 42 graph\ndatasets from 18 application domains, 8 federated data simulation strategies\nthat emphasize different graph properties, and 5 graph-based downstream tasks.\nAdditionally, it offers 18 recently proposed SOTA FGL algorithms through a\nuser-friendly API, enabling a thorough comparison and comprehensive evaluation\nof their effectiveness, robustness, and efficiency. Our empirical results\ndemonstrate the capabilities of FGL while also highlighting its potential\nlimitations, providing valuable insights for future research in this growing\nfield, particularly in fostering greater interdisciplinary collaboration\nbetween FGL and data systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by VLDB 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.16288v2",
    "published_date": "2024-08-29 06:40:01 UTC",
    "updated_date": "2025-01-21 03:19:58 UTC"
  },
  {
    "arxiv_id": "2409.00130v1",
    "title": "Mirror contrastive loss based sliding window transformer for subject-independent motor imagery based EEG signal recognition",
    "authors": [
      "Jing Luo",
      "Qi Mao",
      "Weiwei Shi",
      "Zhenghao Shi",
      "Xiaofan Wang",
      "Xiaofeng Lu",
      "Xinhong Hei"
    ],
    "abstract": "While deep learning models have been extensively utilized in motor imagery\nbased EEG signal recognition, they often operate as black boxes. Motivated by\nneurological findings indicating that the mental imagery of left or right-hand\nmovement induces event-related desynchronization (ERD) in the contralateral\nsensorimotor area of the brain, we propose a Mirror Contrastive Loss based\nSliding Window Transformer (MCL-SWT) to enhance subject-independent motor\nimagery-based EEG signal recognition. Specifically, our proposed mirror\ncontrastive loss enhances sensitivity to the spatial location of ERD by\ncontrasting the original EEG signals with their mirror counterparts-mirror EEG\nsignals generated by interchanging the channels of the left and right\nhemispheres of the EEG signals. Moreover, we introduce a temporal sliding\nwindow transformer that computes self-attention scores from high temporal\nresolution features, thereby improving model performance with manageable\ncomputational complexity. We evaluate the performance of MCL-SWT on\nsubject-independent motor imagery EEG signal recognition tasks, and our\nexperimental results demonstrate that MCL-SWT achieved accuracies of 66.48% and\n75.62%, surpassing the state-of-the-art (SOTA) model by 2.82% and 2.17%,\nrespectively. Furthermore, ablation experiments confirm the effectiveness of\nthe proposed mirror contrastive loss. A code demo of MCL-SWT is available at\nhttps://github.com/roniusLuo/MCL_SWT.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "This paper has been accepted by the Fourth International Workshop on\n  Human Brain and Artificial Intelligence, joint workshop of the 33rd\n  International Joint Conference on Artificial Intelligence, Jeju Island, South\n  Korea, from August 3rd to August 9th, 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.00130v1",
    "published_date": "2024-08-29 06:38:36 UTC",
    "updated_date": "2024-08-29 06:38:36 UTC"
  },
  {
    "arxiv_id": "2409.00129v2",
    "title": "Estimating the number of reachable positions in Minishogi",
    "authors": [
      "Sotaro Ishii",
      "Tetsuro Tanaka"
    ],
    "abstract": "To investigate the feasibility of strongly solving Minishogi (Gogo Shogi), it\nis necessary to know the number of its reachable positions from the initial\nposition. However, there currently remains a significant gap between the lower\nand upper bounds of the value, since checking the legality of a Minishogi\nposition is difficult. In this paper, the authors estimate the number of\nreachable positions by generating candidate positions using uniform random\nsampling and measuring the proportion of those reachable by a series of legal\nmoves from the initial position. The experimental results reveal that the\nnumber of reachable Minishogi positions is approximately $2.38\\times 10^{18}$.",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "This article was submitted to the 53th meeting of IPSJ (Information\n  Processing Society of Japan) SIG Game Informatics (held on September 6, 2024)\n  as a non-reviewed technical report, and also published in IPSJ SIG Technical\n  Reports, Vol. 2024-GI-53, No.2, pp.1-6",
    "pdf_url": "http://arxiv.org/pdf/2409.00129v2",
    "published_date": "2024-08-29 06:20:12 UTC",
    "updated_date": "2024-09-18 17:01:26 UTC"
  },
  {
    "arxiv_id": "2409.09041v1",
    "title": "Acceptable Use Policies for Foundation Models",
    "authors": [
      "Kevin Klyman"
    ],
    "abstract": "As foundation models have accumulated hundreds of millions of users,\ndevelopers have begun to take steps to prevent harmful types of uses. One\nsalient intervention that foundation model developers adopt is acceptable use\npolicies: legally binding policies that prohibit users from using a model for\nspecific purposes. This paper identifies acceptable use policies from 30\nfoundation model developers, analyzes the use restrictions they contain, and\nargues that acceptable use policies are an important lens for understanding the\nregulation of foundation models. Taken together, developers' acceptable use\npolicies include 127 distinct use restrictions; the wide variety in the number\nand type of use restrictions may create fragmentation across the AI supply\nchain. Developers also employ acceptable use policies to prevent competitors or\nspecific industries from making use of their models. Developers alone decide\nwhat constitutes acceptable use, and rarely provide transparency about how they\nenforce their policies. In practice, acceptable use policies are difficult to\nenforce, and scrupulous enforcement can act as a barrier to researcher access\nand limit beneficial uses of foundation models. Nevertheless, acceptable use\npolicies for foundation models are an early example of self-regulation that\nhave a significant impact on the market for foundation models and the overall\nAI ecosystem.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "68T01",
      "K.5.0"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages, 2 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.09041v1",
    "published_date": "2024-08-29 06:04:16 UTC",
    "updated_date": "2024-08-29 06:04:16 UTC"
  },
  {
    "arxiv_id": "2408.16272v1",
    "title": "Beyond Uncertainty: Evidential Deep Learning for Robust Video Temporal Grounding",
    "authors": [
      "Kaijing Ma",
      "Haojian Huang",
      "Jin Chen",
      "Haodong Chen",
      "Pengliang Ji",
      "Xianghao Zang",
      "Han Fang",
      "Chao Ban",
      "Hao Sun",
      "Mulin Chen",
      "Xuelong Li"
    ],
    "abstract": "Existing Video Temporal Grounding (VTG) models excel in accuracy but often\noverlook open-world challenges posed by open-vocabulary queries and untrimmed\nvideos. This leads to unreliable predictions for noisy, corrupted, and\nout-of-distribution data. Adapting VTG models to dynamically estimate\nuncertainties based on user input can address this issue. To this end, we\nintroduce SRAM, a robust network module that benefits from a two-stage\ncross-modal alignment task. More importantly, it integrates Deep Evidential\nRegression (DER) to explicitly and thoroughly quantify uncertainty during\ntraining, thus allowing the model to say \"I do not know\" in scenarios beyond\nits handling capacity. However, the direct application of traditional DER\ntheory and its regularizer reveals structural flaws, leading to unintended\nconstraints in VTG tasks. In response, we develop a simple yet effective\nGeom-regularizer that enhances the uncertainty learning framework from the\nground up. To the best of our knowledge, this marks the first successful\nattempt of DER in VTG. Our extensive quantitative and qualitative results\naffirm the effectiveness, robustness, and interpretability of our modules and\nthe uncertainty learning paradigm in VTG tasks. The code will be made\navailable.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Ongoing work: 28pages, 19 figures, 7 tables. Code is available at:\n  https://kaijing.space/SRAM/",
    "pdf_url": "http://arxiv.org/pdf/2408.16272v1",
    "published_date": "2024-08-29 05:32:03 UTC",
    "updated_date": "2024-08-29 05:32:03 UTC"
  },
  {
    "arxiv_id": "2409.00128v2",
    "title": "Can AI Replace Human Subjects? A Large-Scale Replication of Psychological Experiments with LLMs",
    "authors": [
      "Ziyan Cui",
      "Ning Li",
      "Huaikang Zhou"
    ],
    "abstract": "Artificial Intelligence (AI) is increasingly being integrated into scientific\nresearch, particularly in the social sciences, where understanding human\nbehavior is critical. Large Language Models (LLMs) like GPT-4 have shown\npromise in replicating human-like responses in various psychological\nexperiments. However, the extent to which LLMs can effectively replace human\nsubjects across diverse experimental contexts remains unclear. Here, we conduct\na large-scale study replicating 154 psychological experiments from top social\nscience journals with 618 main effects and 138 interaction effects using GPT-4\nas a simulated participant. We find that GPT-4 successfully replicates 76.0\npercent of main effects and 47.0 percent of interaction effects observed in the\noriginal studies, closely mirroring human responses in both direction and\nsignificance. However, only 19.44 percent of GPT-4's replicated confidence\nintervals contain the original effect sizes, with the majority of replicated\neffect sizes exceeding the 95 percent confidence interval of the original\nstudies. Additionally, there is a 71.6 percent rate of unexpected significant\nresults where the original studies reported null findings, suggesting potential\noverestimation or false positives. Our results demonstrate the potential of\nLLMs as powerful tools in psychological research but also emphasize the need\nfor caution in interpreting AI-driven findings. While LLMs can complement human\nstudies, they cannot yet fully replace the nuanced insights provided by human\nsubjects.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CL",
    "comment": "5 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.00128v2",
    "published_date": "2024-08-29 05:18:50 UTC",
    "updated_date": "2024-09-04 03:21:07 UTC"
  },
  {
    "arxiv_id": "2408.16264v2",
    "title": "LoraMap: Harnessing the Power of LoRA Connections",
    "authors": [
      "Hyeryun Park",
      "Jeongwon Kwak",
      "Dongsuk Jang",
      "Sumin Park",
      "Jinwook Choi"
    ],
    "abstract": "Fact-checking techniques can mitigate hallucinations in Large Language Models\n(LLMs), a prominent issue in specialized domains. As parameter-efficient\ntechniques such as Low-Rank Adaptation (LoRA) can overcome substantial\ncomputational overhead, some studies have explored the integration of multiple\nLoRAs. While previous studies focus on parallel integration, this paper\ninvestigates methods to establish connections among multiple LoRAs. We create\nthree reasoning datasets tailored to fact-checking and fine-tune individual\nLoRAs, allowing them to view and reason from diverse perspectives. Then, we\nexplore strategies for allocating these reasoning LoRAs and introduce LoraMap,\nan approach to map connections between them. The results of the fact-checking\ntask demonstrate that the performance of LoraMap is superior to LoraHub, an\nexisting method for integrating LoRAs. LoraMap also outperforms with\nsignificantly fewer trainable parameters than LoraConcat, which concatenates\nLoRAs and further fine-tunes them.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 12 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.16264v2",
    "published_date": "2024-08-29 05:02:52 UTC",
    "updated_date": "2024-10-16 10:19:45 UTC"
  },
  {
    "arxiv_id": "2408.16261v1",
    "title": "Evaluating Time-Series Training Dataset through Lens of Spectrum in Deep State Space Models",
    "authors": [
      "Sekitoshi Kanai",
      "Yasutoshi Ida",
      "Kazuki Adachi",
      "Mihiro Uchida",
      "Tsukasa Yoshida",
      "Shin'ya Yamaguchi"
    ],
    "abstract": "This study investigates a method to evaluate time-series datasets in terms of\nthe performance of deep neural networks (DNNs) with state space models (deep\nSSMs) trained on the dataset. SSMs have attracted attention as components\ninside DNNs to address time-series data. Since deep SSMs have powerful\nrepresentation capacities, training datasets play a crucial role in solving a\nnew task. However, the effectiveness of training datasets cannot be known until\ndeep SSMs are actually trained on them. This can increase the cost of data\ncollection for new tasks, as a trial-and-error process of data collection and\ntime-consuming training are needed to achieve the necessary performance. To\nadvance the practical use of deep SSMs, the metric of datasets to estimate the\nperformance early in the training can be one key element. To this end, we\nintroduce the concept of data evaluation methods used in system identification.\nIn system identification of linear dynamical systems, the effectiveness of\ndatasets is evaluated by using the spectrum of input signals. We introduce this\nconcept to deep SSMs, which are nonlinear dynamical systems. We propose the\nK-spectral metric, which is the sum of the top-K spectra of signals inside deep\nSSMs, by focusing on the fact that each layer of a deep SSM can be regarded as\na linear dynamical system. Our experiments show that the K-spectral metric has\na large absolute value of the correlation coefficient with the performance and\ncan be used to evaluate the quality of training datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16261v1",
    "published_date": "2024-08-29 04:46:49 UTC",
    "updated_date": "2024-08-29 04:46:49 UTC"
  },
  {
    "arxiv_id": "2409.00127v3",
    "title": "Latent-EnSF: A Latent Ensemble Score Filter for High-Dimensional Data Assimilation with Sparse Observation Data",
    "authors": [
      "Phillip Si",
      "Peng Chen"
    ],
    "abstract": "Accurate modeling and prediction of complex physical systems often rely on\ndata assimilation techniques to correct errors inherent in model simulations.\nTraditional methods like the Ensemble Kalman Filter (EnKF) and its variants as\nwell as the recently developed Ensemble Score Filters (EnSF) face significant\nchallenges when dealing with high-dimensional and nonlinear Bayesian filtering\nproblems with sparse observations, which are ubiquitous in real-world\napplications. In this paper, we propose a novel data assimilation method,\nLatent-EnSF, which leverages EnSF with efficient and consistent latent\nrepresentations of the full states and sparse observations to address the joint\nchallenges of high dimensionlity in states and high sparsity in observations\nfor nonlinear Bayesian filtering. We introduce a coupled Variational\nAutoencoder (VAE) with two encoders to encode the full states and sparse\nobservations in a consistent way guaranteed by a latent distribution matching\nand regularization as well as a consistent state reconstruction. With\ncomparison to several methods, we demonstrate the higher accuracy, faster\nconvergence, and higher efficiency of Latent-EnSF for two challenging\napplications with complex models in shallow water wave propagation and\nmedium-range weather forecasting, for highly sparse observations in both space\nand time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "stat.ML",
      "68U01",
      "J.2; I.2.1"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 10 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2409.00127v3",
    "published_date": "2024-08-29 04:43:20 UTC",
    "updated_date": "2024-09-11 17:18:58 UTC"
  },
  {
    "arxiv_id": "2408.16256v1",
    "title": "Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9",
    "authors": [
      "Xia Jiang",
      "Yijun Zhou",
      "Alan Wells",
      "Adam Brufsky"
    ],
    "abstract": "Breast cancer is one of the two cancers responsible for the most deaths in\nwomen, with about 42,000 deaths each year in the US. That there are over\n300,000 breast cancers newly diagnosed each year suggests that only a fraction\nof the cancers result in mortality. Thus, most of the women undergo seemingly\ncurative treatment for localized cancers, but a significant later succumb to\nmetastatic disease for which current treatments are only temporizing for the\nvast majority. The current prognostic metrics are of little actionable value\nfor 4 of the 5 women seemingly cured after local treatment, and many women are\nexposed to morbid and even mortal adjuvant therapies unnecessarily, with these\nadjuvant therapies reducing metastatic recurrence by only a third. Thus, there\nis a need for better prognostics to target aggressive treatment at those who\nare likely to relapse and spare those who were actually cured. While there is a\nplethora of molecular and tumor-marker assays in use and under-development to\ndetect recurrence early, these are time consuming, expensive and still often\nun-validated as to actionable prognostic utility. A different approach would\nuse large data techniques to determine clinical and histopathological\nparameters that would provide accurate prognostics using existing data. Herein,\nwe report on machine learning, together with grid search and Bayesian Networks\nto develop algorithms that present a AUC of up to 0.9 in ROC analyses, using\nonly extant data. Such algorithms could be rapidly translated to clinical\nmanagement as they do not require testing beyond routine tumor evaluations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16256v1",
    "published_date": "2024-08-29 04:35:36 UTC",
    "updated_date": "2024-08-29 04:35:36 UTC"
  },
  {
    "arxiv_id": "2409.09040v1",
    "title": "ChatSUMO: Large Language Model for Automating Traffic Scenario Generation in Simulation of Urban MObility",
    "authors": [
      "Shuyang Li",
      "Talha Azfar",
      "Ruimin Ke"
    ],
    "abstract": "Large Language Models (LLMs), capable of handling multi-modal input and\noutputs such as text, voice, images, and video, are transforming the way we\nprocess information. Beyond just generating textual responses to prompts, they\ncan integrate with different software platforms to offer comprehensive\nsolutions across diverse applications. In this paper, we present ChatSUMO, a\nLLM-based agent that integrates language processing skills to generate abstract\nand real-world simulation scenarios in the widely-used traffic simulator -\nSimulation of Urban MObility (SUMO). Our methodology begins by leveraging the\nLLM for user input which converts to relevant keywords needed to run python\nscripts. These scripts are designed to convert specified regions into\ncoordinates, fetch data from OpenStreetMap, transform it into a road network,\nand subsequently run SUMO simulations with the designated traffic conditions.\nThe outputs of the simulations are then interpreted by the LLM resulting in\ninformative comparisons and summaries. Users can continue the interaction and\ngenerate a variety of customized scenarios without prior traffic simulation\nexpertise. For simulation generation, we created a real-world simulation for\nthe city of Albany with an accuracy of 96\\%. ChatSUMO also realizes the\ncustomizing of edge edit, traffic light optimization, and vehicle edit by users\neffectively.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09040v1",
    "published_date": "2024-08-29 03:59:11 UTC",
    "updated_date": "2024-08-29 03:59:11 UTC"
  },
  {
    "arxiv_id": "2408.16232v1",
    "title": "Enhancing Conditional Image Generation with Explainable Latent Space Manipulation",
    "authors": [
      "Kshitij Pathania"
    ],
    "abstract": "In the realm of image synthesis, achieving fidelity to a reference image\nwhile adhering to conditional prompts remains a significant challenge. This\npaper proposes a novel approach that integrates a diffusion model with latent\nspace manipulation and gradient-based selective attention mechanisms to address\nthis issue. Leveraging Grad-SAM (Gradient-based Selective Attention\nManipulation), we analyze the cross attention maps of the cross attention\nlayers and gradients for the denoised latent vector, deriving importance scores\nof elements of denoised latent vector related to the subject of interest. Using\nthis information, we create masks at specific timesteps during denoising to\npreserve subjects while seamlessly integrating the reference image features.\nThis approach ensures the faithful formation of subjects based on conditional\nprompts, while concurrently refining the background for a more coherent\ncomposition. Our experiments on places365 dataset demonstrate promising\nresults, with our proposed model achieving the lowest mean and median Frechet\nInception Distance (FID) scores compared to baseline models, indicating\nsuperior fidelity preservation. Furthermore, our model exhibits competitive\nperformance in aligning the generated images with provided textual\ndescriptions, as evidenced by high CLIP scores. These results highlight the\neffectiveness of our approach in both fidelity preservation and textual context\npreservation, offering a significant advancement in text-to-image synthesis\ntasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "26B10, 53A35,",
      "I.2.10; I.4.10"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages , 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16232v1",
    "published_date": "2024-08-29 03:12:04 UTC",
    "updated_date": "2024-08-29 03:12:04 UTC"
  },
  {
    "arxiv_id": "2408.16231v2",
    "title": "Anchor-Controlled Generative Adversarial Network for High-Fidelity Electromagnetic and Structurally Diverse Metasurface Design",
    "authors": [
      "Yunhui Zeng",
      "Hongkun Cao",
      "Xin Jin"
    ],
    "abstract": "Metasurfaces, capable of manipulating light at subwavelength scales, hold\ngreat potential for advancing optoelectronic applications. Generative models,\nparticularly Generative Adversarial Networks (GANs), offer a promising approach\nfor metasurface inverse design by efficiently navigating complex design spaces\nand capturing underlying data patterns. However, existing generative models\nstruggle to achieve high electromagnetic fidelity and structural diversity.\nThese challenges arise from the lack of explicit electromagnetic constraints\nduring training, which hinders accurate structure-to-electromagnetic response\nmapping, and the absence of mechanisms to handle one-to-many mappings dilemma,\nresulting in insufficient structural diversity. To address these issues, we\npropose the Anchor-controlled Generative Adversarial Network (AcGAN), a novel\nframework that improves both electromagnetic fidelity and structural diversity.\nTo achieve high electromagnetic fidelity, AcGAN proposes the Spectral Overlap\nCoefficient (SOC) for precise spectral fidelity assessment and develops\nAnchorNet, which provides real-time feedback on electromagnetic performance to\nrefine the structure-to-electromagnetic mapping. To enhance structural\ndiversity, AcGAN incorporates a cluster-guided controller that refines input\nprocessing and ensures multi-level spectral integration, guiding the generation\nprocess to explore multiple configurations for the same spectral target.\nAdditionally, a dynamic loss function progressively shifts the focus from\ndata-driven learning to optimizing both spectral fidelity and structural\ndiversity. Empirical analysis shows that AcGAN reduces the Mean Squared Error\n(MSE) by 73% compared to current state-of-the-art GANs methods and\nsignificantly expands the design space to generate diverse metasurface\narchitectures that meet precise spectral demands.",
    "categories": [
      "physics.optics",
      "cs.AI",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16231v2",
    "published_date": "2024-08-29 03:11:55 UTC",
    "updated_date": "2024-10-03 17:53:02 UTC"
  },
  {
    "arxiv_id": "2408.16224v2",
    "title": "LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models",
    "authors": [
      "Jingyi Wang",
      "Jianzhong Ju",
      "Jian Luan",
      "Zhidong Deng"
    ],
    "abstract": "Recent advances in large vision-language models (VLMs) typically employ\nvision encoders based on the Vision Transformer (ViT) architecture. The\ndivision of the images into patches by ViT results in a fragmented perception,\nthereby hindering the visual understanding capabilities of VLMs. In this paper,\nwe propose an innovative enhancement to address this limitation by introducing\na Scene Graph Expression (SGE) module in VLMs. This module extracts and\nstructurally expresses the complex semantic information within images, thereby\nimproving the foundational perception and understanding abilities of VLMs.\nExtensive experiments demonstrate that integrating our SGE module significantly\nenhances the VLM's performance in vision-language tasks, indicating its\neffectiveness in preserving intricate semantic details and facilitating better\nvisual understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16224v2",
    "published_date": "2024-08-29 02:43:20 UTC",
    "updated_date": "2024-08-30 02:49:40 UTC"
  },
  {
    "arxiv_id": "2408.16221v3",
    "title": "SSDM: Scalable Speech Dysfluency Modeling",
    "authors": [
      "Jiachen Lian",
      "Xuanru Zhou",
      "Zoe Ezzes",
      "Jet Vonk",
      "Brittany Morin",
      "David Baquirin",
      "Zachary Mille",
      "Maria Luisa Gorno Tempini",
      "Gopala Krishna Anumanchipalli"
    ],
    "abstract": "Speech dysfluency modeling is the core module for spoken language learning,\nand speech therapy. However, there are three challenges. First, current\nstate-of-the-art solutions\\cite{lian2023unconstrained-udm,\nlian-anumanchipalli-2024-towards-hudm} suffer from poor scalability. Second,\nthere is a lack of a large-scale dysfluency corpus. Third, there is not an\neffective learning framework. In this paper, we propose \\textit{SSDM: Scalable\nSpeech Dysfluency Modeling}, which (1) adopts articulatory gestures as scalable\nforced alignment; (2) introduces connectionist subsequence aligner (CSA) to\nachieve dysfluency alignment; (3) introduces a large-scale simulated dysfluency\ncorpus called Libri-Dys; and (4) develops an end-to-end system by leveraging\nthe power of large language models (LLMs). We expect SSDM to serve as a\nstandard in the area of dysfluency modeling. Demo is available at\n\\url{https://berkeley-speech-group.github.io/SSDM/}.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "2024 NeurIPS",
    "pdf_url": "http://arxiv.org/pdf/2408.16221v3",
    "published_date": "2024-08-29 02:35:53 UTC",
    "updated_date": "2024-10-03 21:37:49 UTC"
  },
  {
    "arxiv_id": "2408.16213v1",
    "title": "M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation",
    "authors": [
      "Jonggwon Park",
      "Soobum Kim",
      "Byungmu Yoon",
      "Jihun Hyun",
      "Kyoyun Choi"
    ],
    "abstract": "The rapid evolution of artificial intelligence, especially in large language\nmodels (LLMs), has significantly impacted various domains, including\nhealthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,\nbut with limitations: either underutilizing the multi-tasking capabilities of\nLLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM\ndesigned to enhance CXR interpretation. The model is trained on a visual\ninstruction-following dataset that integrates various task-specific datasets in\na conversational format. As a result, the model supports multiple tasks such as\nmedical report generation (MRG), visual grounding, and visual question\nanswering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by\nemploying a chain-of-thought prompting strategy, in which it identifies\nfindings in CXR images and subsequently generates corresponding reports. The\nmodel is adaptable to various MRG scenarios depending on the available inputs,\nsuch as single-image, multi-image, and multi-study contexts. In addition to\nMRG, M4CXR performs visual grounding at a level comparable to specialized\nmodels and also demonstrates outstanding performance in VQA. Both quantitative\nand qualitative assessments reveal M4CXR's versatility in MRG, visual\ngrounding, and VQA, while consistently maintaining clinical accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16213v1",
    "published_date": "2024-08-29 02:12:58 UTC",
    "updated_date": "2024-08-29 02:12:58 UTC"
  },
  {
    "arxiv_id": "2408.16202v1",
    "title": "Short-Term Electricity-Load Forecasting by Deep Learning: A Comprehensive Survey",
    "authors": [
      "Qi Dong",
      "Rubing Huang",
      "Chenhui Cui",
      "Dave Towey",
      "Ling Zhou",
      "Jinyu Tian",
      "Jianzhou Wang"
    ],
    "abstract": "Short-Term Electricity-Load Forecasting (STELF) refers to the prediction of\nthe immediate demand (in the next few hours to several days) for the power\nsystem. Various external factors, such as weather changes and the emergence of\nnew electricity consumption scenarios, can impact electricity demand, causing\nload data to fluctuate and become non-linear, which increases the complexity\nand difficulty of STELF. In the past decade, deep learning has been applied to\nSTELF, modeling and predicting electricity demand with high accuracy, and\ncontributing significantly to the development of STELF. This paper provides a\ncomprehensive survey on deep-learning-based STELF over the past ten years. It\nexamines the entire forecasting process, including data pre-processing, feature\nextraction, deep-learning modeling and optimization, and results evaluation.\nThis paper also identifies some research challenges and potential research\ndirections to be further investigated in future work.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16202v1",
    "published_date": "2024-08-29 01:47:09 UTC",
    "updated_date": "2024-08-29 01:47:09 UTC"
  },
  {
    "arxiv_id": "2408.16200v3",
    "title": "PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object Detection in Bird's-Eye-View",
    "authors": [
      "Zichen Yu",
      "Quanli Liu",
      "Wei Wang",
      "Liyong Zhang",
      "Xiaoguang Zhao"
    ],
    "abstract": "Recently, LSS-based multi-view 3D object detection provides an economical and\ndeployment-friendly solution for autonomous driving. However, all the existing\nLSS-based methods transform multi-view image features into a Cartesian\nBird's-Eye-View(BEV) representation, which does not take into account the\nnon-uniform image information distribution and hardly exploits the view\nsymmetry. In this paper, in order to adapt the image information distribution\nand preserve the view symmetry by regular convolution, we propose to employ the\npolar BEV representation to substitute the Cartesian BEV representation. To\nachieve this, we elaborately tailor three modules: a polar view transformer to\ngenerate the polar BEV representation, a polar temporal fusion module for\nfusing historical polar BEV features and a polar detection head to predict the\npolar-parameterized representation of the object. In addition, we design a 2D\nauxiliary detection head and a spatial attention enhancement module to improve\nthe quality of feature extraction in perspective view and BEV, respectively.\nFinally, we integrate the above improvements into a novel multi-view 3D object\ndetector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves\nthe superior performance. The code is available at\nhttps://github.com/Yzichen/PolarBEVDet.git.(This work has been submitted to the\nIEEE for possible publication. Copyright may be transferred without notice,\nafter which this version may no longer be accessible)",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2408.16200v3",
    "published_date": "2024-08-29 01:42:38 UTC",
    "updated_date": "2024-12-04 03:15:44 UTC"
  },
  {
    "arxiv_id": "2408.16189v2",
    "title": "Adaptive Sample Aggregation In Transfer Learning",
    "authors": [
      "Steve Hanneke",
      "Samory Kpotufe"
    ],
    "abstract": "Transfer Learning aims to optimally aggregate samples from a target\ndistribution, with related samples from a so-called source distribution to\nimprove target risk. Multiple procedures have been proposed over the last two\ndecades to address this problem, each driven by one of a multitude of possible\ndivergence measures between source and target distributions. A first question\nasked in this work is whether there exist unified algorithmic approaches that\nautomatically adapt to many of these divergence measures simultaneously.\n  We show that this is indeed the case for a large family of divergences\nproposed across classification and regression tasks, as they all happen to\nupper-bound the same measure of continuity between source and target risks,\nwhich we refer to as a weak modulus of transfer. This more unified view allows\nus, first, to identify algorithmic approaches that are simultaneously adaptive\nto these various divergence measures via a reduction to particular confidence\nsets. Second, it allows for a more refined understanding of the statistical\nlimits of transfer under such divergences, and in particular, reveals regimes\nwith faster rates than might be expected under coarser lenses.\n  We then turn to situations that are not well captured by the weak modulus and\ncorresponding divergences: these are situations where the aggregate of source\nand target data can improve target performance significantly beyond what's\npossible with either source or target data alone. We show that common such\nsituations -- as may arise, e.g., under certain causal models with spurious\ncorrelations -- are well described by a so-called strong modulus of transfer\nwhich supersedes the weak modulus. We finally show that the strong modulus also\nadmits adaptive procedures, which achieve near optimal rates in terms of the\nunknown strong modulus, and therefore apply in more general settings.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.16189v2",
    "published_date": "2024-08-29 01:02:40 UTC",
    "updated_date": "2025-04-28 03:15:44 UTC"
  },
  {
    "arxiv_id": "2408.16187v1",
    "title": "Real-Time Energy Pricing in New Zealand: An Evolving Stream Analysis",
    "authors": [
      "Yibin Sun",
      "Heitor Murilo Gomes",
      "Bernhard Pfahringer",
      "Albert Bifet"
    ],
    "abstract": "This paper introduces a group of novel datasets representing real-time\ntime-series and streaming data of energy prices in New Zealand, sourced from\nthe Electricity Market Information (EMI) website maintained by the New Zealand\ngovernment. The datasets are intended to address the scarcity of proper\ndatasets for streaming regression learning tasks. We conduct extensive analyses\nand experiments on these datasets, covering preprocessing techniques,\nregression tasks, prediction intervals, concept drift detection, and anomaly\ndetection. Our experiments demonstrate the datasets' utility and highlight the\nchallenges and opportunities for future research in energy price forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 Pages, 8 figures, short version accepted by PRICAI",
    "pdf_url": "http://arxiv.org/pdf/2408.16187v1",
    "published_date": "2024-08-29 00:53:21 UTC",
    "updated_date": "2024-08-29 00:53:21 UTC"
  }
]
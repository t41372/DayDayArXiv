{
  "date": "2025-04-17",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-17 的 arXiv 中文 TLDR 快报！  \n\n今天的 arXiv 论文聚焦于 AI 模型的安全性、优化和应用（如 LLM 微调、RAG 系统）、计算机视觉（如图像编辑和多模态学习）、强化学习（RL）和多代理系统等领域，亮点包括 Causal-Copilot 的因果分析代理、Scaling Instruction-Tuned LLMs 的百万 token 上下文扩展，以及涉及知名会议如 ICLR 和 CVPR 的创新工作，这些论文突显了 AI 在实际场景中的鲁棒性和可持续性。接下来，我将逐一简要介绍重点论文，先优先讨论那些创新性强、可能引发话题的文章（如 LLM 优化和 AI 安全），并将相关主题归并处理；对于次要或较常规的论文（如一些基础优化方法），将快速掠过以控制篇幅。\n\n### 重点论文讨论\n\n**标题（中文）：因果分析代理：一个自主因果分析代理  标题（英文）：Causal-Copilot: An Autonomous Causal Analysis Agent**  \n这篇论文由 Biwei Huang 等作者提出，构建了一个基于 LLM 的自主代理，用于处理表格和时序数据的因果发现和推断。论文的主要贡献是整合多种因果技术，提供自然语言交互的因果分析管道，显著提升了领域专家的实用性，并在实际应用中超越了基准方法。\n\n**标题（中文）：基于检索增强生成的成本框架：用于评估语言模型的经济框架  标题（英文）：Cost-of-Pass: An Economic Framework for Evaluating Language Models**  \nMehmet Hamza Erol 等作者的工作引入了“成本-通过”指标，结合准确性和推理成本评估 LLM。关键发现是轻量模型适合基础任务，而大型模型在知识密集型任务中更具成本效益；论文还分析了模型创新对任务效率的影响，提供了一个实用的经济视角。\n\n**标题（中文）：扩展指令微调的 LLM 以实现百万 token 上下文  标题（英文）：Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation**  \nLinda He 等作者提出了一种基于层次合成数据的后训练框架，成功扩展 LLM 到百万 token 上下文。论文的主要贡献是生成高效合成数据，避免了长上下文训练的资源瓶颈，并在 RULER 和 InfiniteBench 基准上表现出色，同时保持了通用任务性能。\n\n**标题（中文）：重新思考人形手的设计：使用学习的框架  标题（英文）：RUKA: Rethinking the Design of Humanoid Hands with Learning**  \nLerrel Pinto 等作者设计了 RUKA，一种基于 3D 打印的低成本人形手，并通过运动捕捉数据学习关节-致动器模型。论文的关键发现是该设计在鲁棒性和灵活性上超越了现有机器人手，适用于复杂的抓取任务，并开源了代码和数据。\n\n**标题（中文）：视觉语言模型：用于详细视觉理解的开放访问数据和模型  标题（英文）：PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding**  \nYann LeCun 等知名学者参与的这篇论文引入了 PerceptionLM，提供开放数据集和模型，用于视频理解任务。贡献包括多模态基准和训练配方，强调了视频问答和时空定位的鲁棒性，填补了封闭模型的空白。\n\n**标题（中文）：基于检索增强生成的冲突证据  标题（英文）：Retrieval-Augmented Generation with Conflicting Evidence**  \nHan Wang 等作者扩展了 RAG 系统，处理证据冲突的查询。论文的主要发现是多代理辩论机制能有效过滤错误信息，并在 AmbiguDocs 和 FaithEval 上提升了准确性，适用于复杂查询场景。\n\n**标题（中文）：最优鉴别器加权模仿视角的强化学习  标题（英文）：An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning**  \nHaoran Xu 等作者的 ICLR 2025 论文提出 IDRL 方法，通过迭代优化访问分布比率改善离线 RL。关键贡献是超越基线在 D4RL 数据集上的性能，并提升了算法稳定性。\n\n**标题（中文）：安全优先 AI：用于鲁棒和可信系统的基础  标题（英文）：Security-First AI: Foundations for Robust and Trustworthy Systems**  \nKrti Tallam 的论文强调 AI 安全作为基础层，构建了威胁模型和防御机制。发现是安全指标驱动的方法能提升 AI 的透明度和责任性，适用于高风险领域。\n\n**标题（中文）：增强中风诊断：使用加权深度学习方法  标题（英文）：Enhancing Stroke Diagnosis in the Brain Using a Weighted Deep Learning Approach**  \nYao Zhiwan 等作者的论文使用加权投票集成模型预测中风，达到 94.91% 准确率。贡献在于结合随机森林和深度学习，提供早期风险评估工具。\n\n**标题（中文）：跨模态链：从多模态人类视频中学习操作程序  标题（英文）：Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models**  \nChen Wang 等作者的 ICRA 2025 工作利用多模态数据（如肌肉活动和音频）提升机器人操作学习。论文的关键发现是逐步整合模态信息能提高任务精度，并在真实机器人实验中验证了泛化能力。\n\n### 相关主题快速掠过\n其他论文中，与 AI 安全和隐私相关的（如论文 19、30、97）主要讨论了 LLM 的去学习和鲁棒性优化，但这些工作相对常规，我仅简要提及：它们通过强化学习或元学习提升模型对评估者偏差的抵抗力，贡献在于改进隐私保护机制，但未有突破性创新。\n\n在计算机视觉领域，论文 7（MTS Layer for Multidimensional Data Processing）和 16（SAR Object Detection）引入了新层和自监督预训练，提升了图像处理效率；论文 41（Probing and Inducing Combinational Creativity）探索了视觉语言模型的创造性，但这些更侧重技术细节，我快速掠过：它们在基准上表现出色，但对实际应用的直接影响有限。\n\n强化学习和多代理论文（如 15、62）讨论了环境鲁棒性和代理协作，但内容较基础，我仅指出：这些工作通过新框架改善了 RL 在拥塞管理和协作中的性能，适合实际部署。\n\n剩余论文（如量子计算的 1、医疗的 11、交通的 15 等）涉及特定领域优化，但非核心话题，我仅概括：它们提供了实用工具，如量子退火算法改进或事件增强图像超分辨率，但未有广泛影响，故不展开讨论。\n\n总之，今天的论文展示了 AI 领域的多样创新，特别是 LLM 和视觉模型的优化，值得关注后续应用。感谢阅读本快报！如果有特定论文感兴趣，建议查阅原文。",
  "papers": [
    {
      "arxiv_id": "2504.13376v2",
      "title": "Addressing the Minor-Embedding Problem in Quantum Annealing and Evaluating State-of-the-Art Algorithm Performance",
      "title_zh": "解决量子退火中的次嵌入问题并评估最先进算法的性能",
      "authors": [
        "Aitor Gomez-Tejedor",
        "Eneko Osaba",
        "Esther Villar-Rodriguez"
      ],
      "abstract": "This study addresses the minor-embedding problem, which involves mapping the\nvariables of an Ising model onto a quantum annealing processor. The primary\nmotivation stems from the observed performance disparity of quantum annealers\nwhen solving problems suited to the processor's architecture versus those with\nnon-hardware-native topologies. Our research has two main objectives: i) to\nanalyze the impact of embedding quality on the performance of D-Wave Systems\nquantum annealers, and ii) to evaluate the quality of the embeddings generated\nby Minorminer, an algorithm provided by D-Wave and widely recognized as the\nstandard minor-embedding technique in the literature. Regarding the first\nobjective, our experiments reveal a clear correlation between the average chain\nlength of embeddings and the relative errors of the solutions sampled. This\nunderscores the critical influence of embedding quality on quantum annealing\nperformance. For the second objective, we focus on the Minorminer technique,\nassessing its capacity to embed problems, the quality of the embeddings\nproduced, and the robustness of the results. We also compare its performance\nwith Clique Embedding, another algorithm developed by D-Wave, which is\ndeterministic and designed to embed fully connected Ising models into quantum\nannealing processors, serving as a worst-case scenario. The results demonstrate\nthat there is significant room for improvement for Minorminer, as it has not\nconsistently outperformed the worst-case scenario.",
      "tldr_zh": "这篇论文探讨了在量子退火（quantum annealing）中minor-embedding问题的解决方案，重点分析如何将Ising模型变量映射到处理器上，并评估当前最先进的算法性能。研究通过实验验证了嵌入质量（如平均链长）与D-Wave Systems量子退火器解决方案相对错误之间的显著相关性，同时比较了Minorminer算法与Clique Embedding算法的性能。结果表明，Minorminer在嵌入质量和鲁棒性方面存在改进空间，未始终优于worst-case场景，为未来算法优化提供了重要见解。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "quant-ph",
      "comment": "Paper submitted for review in the Future Generation Computer Systems\n  journal",
      "pdf_url": "http://arxiv.org/pdf/2504.13376v2",
      "published_date": "2025-04-17 23:13:14 UTC",
      "updated_date": "2025-04-25 20:16:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:53:33.112361"
    },
    {
      "arxiv_id": "2504.13375v1",
      "title": "Pricing AI Model Accuracy",
      "title_zh": "AI 模型准确性的定价",
      "authors": [
        "Nikhil Kumar"
      ],
      "abstract": "This paper examines the market for AI models in which firms compete to\nprovide accurate model predictions and consumers exhibit heterogeneous\npreferences for model accuracy. We develop a consumer-firm duopoly model to\nanalyze how competition affects firms' incentives to improve model accuracy.\nEach firm aims to minimize its model's error, but this choice can often be\nsuboptimal. Counterintuitively, we find that in a competitive market, firms\nthat improve overall accuracy do not necessarily improve their profits. Rather,\neach firm's optimal decision is to invest further on the error dimension where\nit has a competitive advantage. By decomposing model errors into false positive\nand false negative rates, firms can reduce errors in each dimension through\ninvestments. Firms are strictly better off investing on their superior\ndimension and strictly worse off with investments on their inferior dimension.\nProfitable investments adversely affect consumers but increase overall welfare.",
      "tldr_zh": "这篇论文研究了AI模型市场的竞争环境，其中公司争夺提供准确模型预测，而消费者对准确性有异质偏好。作者开发了一个consumer-firm duopoly model，分析了竞争如何影响公司提高模型准确性的激励，发现公司在竞争中投资整体准确性并不一定提升利润，而是应专注于自身竞争优势的错误维度，如false positive或false negative rates。最终，作者指出，这种投资策略虽对消费者不利，但能增加整体福利。",
      "categories": [
        "econ.TH",
        "cs.AI"
      ],
      "primary_category": "econ.TH",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13375v1",
      "published_date": "2025-04-17 23:09:04 UTC",
      "updated_date": "2025-04-17 23:09:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:53:43.951863"
    },
    {
      "arxiv_id": "2504.16110v1",
      "title": "Security-First AI: Foundations for Robust and Trustworthy Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Krti Tallam"
      ],
      "abstract": "The conversation around artificial intelligence (AI) often focuses on safety,\ntransparency, accountability, alignment, and responsibility. However, AI\nsecurity (i.e., the safeguarding of data, models, and pipelines from\nadversarial manipulation) underpins all of these efforts. This manuscript\nposits that AI security must be prioritized as a foundational layer. We present\na hierarchical view of AI challenges, distinguishing security from safety, and\nargue for a security-first approach to enable trustworthy and resilient AI\nsystems. We discuss core threat models, key attack vectors, and emerging\ndefense mechanisms, concluding that a metric-driven approach to AI security is\nessential for robust AI safety, transparency, and accountability.",
      "tldr_zh": "这篇论文强调AI安全（AI security）应作为基础层，优先于AI的安全性、透明性、问责性、协调性和责任性，以保护数据、模型和管道免受敌对操纵。作者提出一个分层视图，将AI安全与安全性区分开来，并主张采用安全优先的方法来构建可靠且弹性强的AI系统。论文讨论了核心威胁模型（threat models）、关键攻击向量（attack vectors）和新兴防御机制（defense mechanisms），并得出结论，通过指标驱动的方法（metric-driven approach）可以增强AI的安全性、透明性和问责性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.16110v1",
      "published_date": "2025-04-17 22:53:01 UTC",
      "updated_date": "2025-04-17 22:53:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:53:56.208937"
    },
    {
      "arxiv_id": "2504.13371v1",
      "title": "The Impact of AI on the Cyber Offense-Defense Balance and the Character of Cyber Conflict",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew J. Lohn"
      ],
      "abstract": "Unlike other domains of conflict, and unlike other fields with high\nanticipated risk from AI, the cyber domain is intrinsically digital with a\ntight feedback loop between AI training and cyber application. Cyber may have\nsome of the largest and earliest impacts from AI, so it is important to\nunderstand how the cyber domain may change as AI continues to advance. Our\napproach reviewed the literature, collecting nine arguments that have been\nproposed for offensive advantage in cyber conflict and nine proposed arguments\nfor defensive advantage. We include an additional forty-eight arguments that\nhave been proposed to give cyber conflict and competition its character as\ncollected separately by Healey, Jervis, and Nandrajog. We then consider how\neach of those arguments and propositions might change with varying degrees of\nAI advancement. We find that the cyber domain is too multifaceted for a single\nanswer to whether AI will enhance offense or defense broadly. AI will improve\nsome aspects, hinder others, and leave some aspects unchanged. We collect and\npresent forty-four ways that we expect AI to impact the cyber offense-defense\nbalance and the character of cyber conflict and competition.",
      "tldr_zh": "这篇论文探讨了 AI 对网络领域进攻-防御平衡和冲突特性的影响，强调网络域的数字化特性使其成为 AI 应用的最早和最大影响领域。作者通过文献审阅，收集了九个支持进攻优势的论点、九个支持防御优势的论点，以及额外四十八个定义网络冲突特性的论点，并分析了这些论点在不同 AI 推进程度下的变化。研究发现，AI 将多方面影响网络领域，既会提升某些方面、阻碍其他方面，同时保持部分不变，最终总结了四十四种预期影响。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13371v1",
      "published_date": "2025-04-17 22:40:14 UTC",
      "updated_date": "2025-04-17 22:40:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:54:09.110870"
    },
    {
      "arxiv_id": "2504.13976v1",
      "title": "Gas Station of the Future: A Perspective on AI/ML and IoT in Retail Downstream",
      "title_zh": "翻译失败",
      "authors": [
        "Wrick Talukdar"
      ],
      "abstract": "The gas station of the future is poised to transform from a simple fuel\ndispensing center into an intelligent retail hub, driven by advancements in\nArtificial Intelligence (AI), Machine Learning (ML), and the Internet of Things\n(IoT). This paper explores how technology is reshaping the retail downstream\nsector while briefly addressing the upstream and midstream segments. By\nleveraging AI/ML for predictive analytics, dynamic pricing, personalized\ncustomer engagement, and IoT for real-time monitoring and automation, the\nfuture gas station will redefine the fuel retail experience. Additionally, this\npaper incorporates statistics, AI/ML core technical concepts, mathematical\nformulations, case studies, and a proposed framework for a fully autonomous gas\nstation.",
      "tldr_zh": "本文从 AI、ML 和 IoT 的视角，探讨未来加油站如何从简单加油中心转型为智能零售枢纽，重塑零售下游行业，同时简要涉及上游和中游领域。通过利用 AI/ML 进行预测分析、动态定价和个性化客户互动，以及 IoT 实现实时监控和自动化，该框架旨在重新定义燃料零售体验。论文还整合了统计数据、AI/ML 核心技术概念、数学公式、案例研究，并提出一个全自动加油站的框架提案，作为行业创新的参考。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13976v1",
      "published_date": "2025-04-17 22:27:38 UTC",
      "updated_date": "2025-04-17 22:27:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:54:20.400285"
    },
    {
      "arxiv_id": "2504.13368v1",
      "title": "An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Haoran Xu",
        "Shuozhe Li",
        "Harshit Sikchi",
        "Scott Niekum",
        "Amy Zhang"
      ],
      "abstract": "We introduce Iterative Dual Reinforcement Learning (IDRL), a new method that\ntakes an optimal discriminator-weighted imitation view of solving RL. Our\nmethod is motivated by a simple experiment in which we find training a\ndiscriminator using the offline dataset plus an additional expert dataset and\nthen performing discriminator-weighted behavior cloning gives strong results on\nvarious types of datasets. That optimal discriminator weight is quite similar\nto the learned visitation distribution ratio in Dual-RL, however, we find that\ncurrent Dual-RL methods do not correctly estimate that ratio. In IDRL, we\npropose a correction method to iteratively approach the optimal visitation\ndistribution ratio in the offline dataset given no addtional expert dataset.\nDuring each iteration, IDRL removes zero-weight suboptimal transitions using\nthe learned ratio from the previous iteration and runs Dual-RL on the remaining\nsubdataset. This can be seen as replacing the behavior visitation distribution\nwith the optimized visitation distribution from the previous iteration, which\ntheoretically gives a curriculum of improved visitation distribution ratios\nthat are closer to the optimal discriminator weight. We verify the\neffectiveness of IDRL on various kinds of offline datasets, including D4RL\ndatasets and more realistic corrupted demonstrations. IDRL beats strong\nPrimal-RL and Dual-RL baselines in terms of both performance and stability, on\nall datasets.",
      "tldr_zh": "该论文引入了Iterative Dual Reinforcement Learning (IDRL)，一种基于最优鉴别器加权模仿学习的新型强化学习方法，通过迭代修正访问分布比率来提升性能。IDRL 的核心机制是，在每个迭代中移除零权重的不优子转换，并在剩余子数据集上运行Dual-RL，从而逐步逼近最优鉴别器权重，形成一个改进的访问分布课程，而无需额外专家数据集。实验验证显示，IDRL 在D4RL数据集和更现实的损坏演示数据集上，显著超越Primal-RL和Dual-RL基线，在性能和稳定性方面均有提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.13368v1",
      "published_date": "2025-04-17 22:21:35 UTC",
      "updated_date": "2025-04-17 22:21:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:54:33.628568"
    },
    {
      "arxiv_id": "2504.13975v1",
      "title": "Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing",
      "title_zh": "多尺度张量求和因子",
      "authors": [
        "Mehmet Yamaç",
        "Muhammad Numan Yousaf",
        "Serkan Kiranyaz",
        "Moncef Gabbouj"
      ],
      "abstract": "Multilayer perceptrons (MLP), or fully connected artificial neural networks,\nare known for performing vector-matrix multiplications using learnable weight\nmatrices; however, their practical application in many machine learning tasks,\nespecially in computer vision, can be limited due to the high dimensionality of\ninput-output pairs at each layer. To improve efficiency, convolutional\noperators have been utilized to facilitate weight sharing and local\nconnections, yet they are constrained by limited receptive fields. In this\npaper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel\nneural network operator that implements tensor summation at multiple scales,\nwhere each tensor to be summed is obtained through Tucker-decomposition-like\nmode products. Unlike other tensor decomposition methods in the literature, MTS\nis not introduced as a network compression tool; instead, as a new backbone\nneural layer. MTS not only reduces the number of parameters required while\nenhancing the efficiency of weight optimization compared to traditional dense\nlayers (i.e., unfactorized weight matrices in MLP layers), but it also\ndemonstrates clear advantages over convolutional layers. The proof-of-concept\nexperimental comparison of the proposed MTS networks with MLPs and\nConvolutional Neural Networks (CNNs) demonstrates their effectiveness across\nvarious tasks, such as classification, compression, and signal restoration.\nAdditionally, when integrated with modern non-linear units such as the\nmulti-head gate (MHG), also introduced in this study, the corresponding neural\nnetwork, MTSNet, demonstrates a more favorable complexity-performance tradeoff\ncompared to state-of-the-art transformers in various computer vision\napplications. The software implementation of the MTS layer and the\ncorresponding MTS-based networks, MTSNets, is shared at\nhttps://github.com/mehmetyamac/MTSNet.",
      "tldr_zh": "本论文提出 Multiscale Tensor Summation (MTS) Factorization 作为一种新型神经网络层（MTS Layer），用于处理多维数据，通过类似 Tucker-decomposition 的模式产品在多个尺度上实现张量求和，从而减少参数数量并提升权重优化效率。相比传统 MLP 和卷积层，MTS 层不仅在分类、压缩和信号恢复任务中表现出色，还在实验中超越基线模型。论文进一步引入 multi-head gate (MHG) 非线性单元，使 MTSNet 在计算机视觉应用中实现比最先进 transformer 更优的复杂度-性能权衡，并提供了软件实现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "68T07"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.13975v1",
      "published_date": "2025-04-17 22:19:59 UTC",
      "updated_date": "2025-04-17 22:19:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:54:46.738038"
    },
    {
      "arxiv_id": "2504.13365v1",
      "title": "VLLFL: A Vision-Language Model Based Lightweight Federated Learning Framework for Smart Agriculture",
      "title_zh": "VLLFL：一种基于视觉语言模型的轻量级联邦学习框架，用于智能农业",
      "authors": [
        "Long Li",
        "Jiajia Li",
        "Dong Chen",
        "Lina Pu",
        "Haibo Yao",
        "Yanbo Huang"
      ],
      "abstract": "In modern smart agriculture, object detection plays a crucial role by\nenabling automation, precision farming, and monitoring of resources. From\nidentifying crop health and pest infestations to optimizing harvesting\nprocesses, accurate object detection enhances both productivity and\nsustainability. However, training object detection models often requires\nlarge-scale data collection and raises privacy concerns, particularly when\nsensitive agricultural data is distributed across farms. To address these\nchallenges, we propose VLLFL, a vision-language model-based lightweight\nfederated learning framework (VLLFL). It harnesses the generalization and\ncontext-aware detection capabilities of the vision-language model (VLM) and\nleverages the privacy-preserving nature of federated learning. By training a\ncompact prompt generator to boost the performance of the VLM deployed across\ndifferent farms, VLLFL preserves privacy while reducing communication overhead.\nExperimental results demonstrate that VLLFL achieves 14.53% improvement in the\nperformance of VLM while reducing 99.3% communication overhead. Spanning tasks\nfrom identifying a wide variety of fruits to detecting harmful animals in\nagriculture, the proposed framework offers an efficient, scalable, and\nprivacy-preserving solution specifically tailored to agricultural applications.",
      "tldr_zh": "本论文提出 VLLFL，一种基于 Vision-Language Model (VLM) 的轻量级 Federated Learning 框架，旨在解决智能农业中对象检测的隐私和数据收集挑战。框架通过训练一个紧凑的 prompt generator 来提升 VLM 的泛化和上下文感知能力，实现分布式训练，同时保护敏感农业数据隐私并显著减少通信开销。实验结果显示，VLLFL 使 VLM 性能提升 14.53%，通信开销减少 99.3%，并适用于识别水果和检测有害动物等农业任务，提供了一个高效、可扩展的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13365v1",
      "published_date": "2025-04-17 22:14:31 UTC",
      "updated_date": "2025-04-17 22:14:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:54:56.720079"
    },
    {
      "arxiv_id": "2504.13360v1",
      "title": "In between myth and reality: AI for math -- a case study in category theory",
      "title_zh": "在神话与现实之间：AI",
      "authors": [
        "Răzvan Diaconescu"
      ],
      "abstract": "Recently, there is an increasing interest in understanding the performance of\nAI systems in solving math problems. A multitude of tests have been performed,\nwith mixed conclusions. In this paper we discuss an experiment we have made in\nthe direction of mathematical research, with two of the most prominent\ncontemporary AI systems. One of the objective of this experiment is to get an\nunderstanding of how AI systems can assist mathematical research. Another\nobjective is to support the AI systems developers by formulating suggestions\nfor directions of improvement.",
      "tldr_zh": "这篇论文探讨了 AI 在数学研究中的实际表现，通过范畴论（category theory）作为案例研究，审视了现有 AI 系统解决数学问题的能力和局限性。研究者进行了一个实验，使用两个主要的当代 AI 系统，旨在评估这些系统如何辅助数学研究，并分析之前测试的混合结论。实验结果为理解 AI 的潜力提供了洞见，并为 AI 系统开发者提出了改进建议，以更好地支持数学领域的应用。",
      "categories": [
        "cs.AI",
        "math.HO",
        "math.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13360v1",
      "published_date": "2025-04-17 21:58:30 UTC",
      "updated_date": "2025-04-17 21:58:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:55:08.893857"
    },
    {
      "arxiv_id": "2504.13359v1",
      "title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models",
      "title_zh": "Cost-of-Pass：评估语言模型的经济框架",
      "authors": [
        "Mehmet Hamza Erol",
        "Batu El",
        "Mirac Suzgun",
        "Mert Yuksekgonul",
        "James Zou"
      ],
      "abstract": "The widespread adoption of AI systems in the economy hinges on their ability\nto generate economic value that outweighs their inference costs. Evaluating\nthis tradeoff requires metrics that account for both performance and costs. We\npropose a framework grounded in production theory for evaluating language\nmodels by combining accuracy and inference cost. We introduce \"cost-of-pass\",\nthe expected monetary cost of generating a correct solution. We then define the\n\"frontier cost-of-pass\" as the minimum cost-of-pass achievable across available\nmodels or the \"human-expert, using the approximate cost of hiring an expert.\nOur analysis reveals distinct economic insights. First, lightweight models are\nmost cost-effective for basic quantitative tasks, large models for\nknowledge-intensive ones, and reasoning models for complex quantitative\nproblems, despite higher per-token costs. Second, tracking this frontier\ncost-of-pass over the past year reveals significant progress, particularly for\ncomplex quantitative tasks where the cost has roughly halved every few months.\nThird, to trace key innovations driving this progress, we examine\ncounterfactual frontiers: estimates of cost-efficiency without specific model\nclasses. We find that innovations in lightweight, large, and reasoning models\nhave been essential for pushing the frontier in basic quantitative,\nknowledge-intensive, and complex quantitative tasks, respectively. Finally, we\nassess the cost-reductions afforded by common inference-time techniques like\nmajority voting and self-refinement, finding that their marginal accuracy gains\nrarely justify their costs. Our findings underscore that complementary\nmodel-level innovations are the primary drivers of cost-efficiency, and our\neconomic framework provides a principled tool for measuring this progress and\nguiding deployment.",
      "tldr_zh": "本研究提出了一种基于生产理论的经济框架，用于评估语言模型的性能与推理成本权衡，引入了 \"cost-of-pass\" 指标（生成正确解决方案的预期货币成本）和 \"frontier cost-of-pass\"（最低可实现成本，与人类专家成本比较）。分析显示，轻量模型最适合基本量化任务、大模型适用于知识密集型任务，而推理模型则擅长复杂量化问题，且过去一年中复杂任务的成本已大幅降低，每几个月约减半。研究通过反事实前沿分析发现，不同模型创新（如轻量、大型和推理模型）分别推动了相应任务的成本效率进步；此外，推理时技术如多数投票和自精炼虽能提升准确性，但其边际收益通常无法抵消额外成本。该框架强调模型级创新是提升成本效率的主要驱动力，并为语言模型的部署提供指导工具。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Code is available at: https://github.com/mhamzaerol/Cost-of-Pass",
      "pdf_url": "http://arxiv.org/pdf/2504.13359v1",
      "published_date": "2025-04-17 21:58:29 UTC",
      "updated_date": "2025-04-17 21:58:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:55:20.684175"
    },
    {
      "arxiv_id": "2504.13974v1",
      "title": "Enhancing Stroke Diagnosis in the Brain Using a Weighted Deep Learning Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Yao Zhiwan",
        "Reza Zarrab",
        "Jean Dubois"
      ],
      "abstract": "A brain stroke occurs when blood flow to a part of the brain is disrupted,\nleading to cell death. Traditional stroke diagnosis methods, such as CT scans\nand MRIs, are costly and time-consuming. This study proposes a weighted voting\nensemble (WVE) machine learning model that combines predictions from\nclassifiers like random forest, Deep Learning, and histogram-based gradient\nboosting to predict strokes more effectively. The model achieved 94.91%\naccuracy on a private dataset, enabling early risk assessment and prevention.\nFuture research could explore optimization techniques to further enhance\naccuracy.",
      "tldr_zh": "本研究针对脑卒中（脑部血流中断导致细胞死亡）的诊断问题，提出了一种加权投票集成 (WVE) 机器学习模型，该模型结合随机森林、Deep Learning 和基于直方图的梯度提升分类器的预测，以提高诊断效率。\n在私人数据集上，该模型实现了94.91%的准确率，支持早期风险评估和预防，从而减少传统方法如 CT 和 MRI 的成本和时间消耗。\n未来研究可探索优化技术，进一步提升模型性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13974v1",
      "published_date": "2025-04-17 21:54:36 UTC",
      "updated_date": "2025-04-17 21:54:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:55:33.504536"
    },
    {
      "arxiv_id": "2504.13351v1",
      "title": "Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Wang",
        "Fei Xia",
        "Wenhao Yu",
        "Tingnan Zhang",
        "Ruohan Zhang",
        "C. Karen Liu",
        "Li Fei-Fei",
        "Jie Tan",
        "Jacky Liang"
      ],
      "abstract": "Learning to perform manipulation tasks from human videos is a promising\napproach for teaching robots. However, many manipulation tasks require changing\ncontrol parameters during task execution, such as force, which visual data\nalone cannot capture. In this work, we leverage sensing devices such as\narmbands that measure human muscle activities and microphones that record\nsound, to capture the details in the human manipulation process, and enable\nrobots to extract task plans and control parameters to perform the same task.\nTo achieve this, we introduce Chain-of-Modality (CoM), a prompting strategy\nthat enables Vision Language Models to reason about multimodal human\ndemonstration data -- videos coupled with muscle or audio signals. By\nprogressively integrating information from each modality, CoM refines a task\nplan and generates detailed control parameters, enabling robots to perform\nmanipulation tasks based on a single multimodal human video prompt. Our\nexperiments show that CoM delivers a threefold improvement in accuracy for\nextracting task plans and control parameters compared to baselines, with strong\ngeneralization to new task setups and objects in real-world robot experiments.\nVideos and code are available at https://chain-of-modality.github.io",
      "tldr_zh": "该研究提出 Chain-of-Modality (CoM)，一种提示策略，利用视觉语言模型(Vision Language Models)从多模态人类视频中学习操作程序，解决传统视觉数据无法捕获动态控制参数（如力）的问题。CoM 通过逐步整合视频、肌肉活动和音频信号，逐步完善任务计划并生成详细控制参数，从而使机器人基于单个多模态人类视频演示执行操作任务。实验结果显示，CoM 在提取任务计划和控制参数的准确性比基线方法提高了三倍，并在真实机器人实验中表现出对新任务设置和物体的强泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.RO",
      "comment": "ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.13351v1",
      "published_date": "2025-04-17 21:31:23 UTC",
      "updated_date": "2025-04-17 21:31:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:55:44.681081"
    },
    {
      "arxiv_id": "2504.13344v1",
      "title": "Adaptive AI decision interface for autonomous electronic material discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Yahao Dai",
        "Henry Chan",
        "Aikaterini Vriza",
        "Fredrick Kim",
        "Yunfei Wang",
        "Wei Liu",
        "Naisong Shan",
        "Jing Xu",
        "Max Weires",
        "Yukun Wu",
        "Zhiqiang Cao",
        "C. Suzanne Miller",
        "Ralu Divan",
        "Xiaodan Gu",
        "Chenhui Zhu",
        "Sihong Wang",
        "Jie Xu"
      ],
      "abstract": "AI-powered autonomous experimentation (AI/AE) can accelerate materials\ndiscovery but its effectiveness for electronic materials is hindered by data\nscarcity from lengthy and complex design-fabricate-test-analyze cycles. Unlike\nexperienced human scientists, even advanced AI algorithms in AI/AE lack the\nadaptability to make informative real-time decisions with limited datasets.\nHere, we address this challenge by developing and implementing an AI decision\ninterface on our AI/AE system. The central element of the interface is an AI\nadvisor that performs real-time progress monitoring, data analysis, and\ninteractive human-AI collaboration for actively adapting to experiments in\ndifferent stages and types. We applied this platform to an emerging type of\nelectronic materials-mixed ion-electron conducting polymers (MIECPs) -- to\nengineer and study the relationships between multiscale morphology and\nproperties. Using organic electrochemical transistors (OECT) as the testing-bed\ndevice for evaluating the mixed-conducting figure-of-merit -- the product of\ncharge-carrier mobility and the volumetric capacitance ({\\mu}C*), our adaptive\nAI/AE platform achieved a 150% increase in {\\mu}C* compared to the commonly\nused spin-coating method, reaching 1,275 F cm-1 V-1 s-1 in just 64 autonomous\nexperimental trials. A study of 10 statistically selected samples identifies\ntwo key structural factors for achieving higher volumetric capacitance: larger\ncrystalline lamellar spacing and higher specific surface area, while also\nuncovering a new polymer polymorph in this material.",
      "tldr_zh": "本研究开发了一个自适应AI决策接口，用于AI/AE（AI-powered autonomous experimentation）系统，以解决电子材料发现中数据稀缺和算法适应性不足的问题。该接口的核心是AI顾问，通过实时监控、数据分析和人机交互，适应不同实验阶段，并应用于混合离子-电子导电聚合物（MIECPs），研究多尺度形态与性能的关系。在有机电化学晶体管（OECT）测试中，该平台使μC*（电荷载流子迁移率和体积电容的乘积）比传统旋涂方法提高了150%，达到1,275 F cm-1 V-1 s-1，并发现了关键结构因素：更大的晶体层间距和更高的比表面积，以及一种新聚合物多晶型。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13344v1",
      "published_date": "2025-04-17 21:26:48 UTC",
      "updated_date": "2025-04-17 21:26:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:55:58.347462"
    },
    {
      "arxiv_id": "2504.13340v3",
      "title": "Putting the Segment Anything Model to the Test with 3D Knee MRI - A Comparison with State-of-the-Art Performance",
      "title_zh": "翻译失败",
      "authors": [
        "Oliver Mills",
        "Philip Conaghan",
        "Nishant Ravikumar",
        "Samuel Relton"
      ],
      "abstract": "Menisci are cartilaginous tissue found within the knee that contribute to\njoint lubrication and weight dispersal. Damage to menisci can lead to onset and\nprogression of knee osteoarthritis (OA), a condition that is a leading cause of\ndisability, and for which there are few effective therapies. Accurate automated\nsegmentation of menisci would allow for earlier detection and treatment of\nmeniscal abnormalities, as well as shedding more light on the role the menisci\nplay in OA pathogenesis. Focus in this area has mainly used variants of\nconvolutional networks, but there has been no attempt to utilise recent large\nvision transformer segmentation models. The Segment Anything Model (SAM) is a\nso-called foundation segmentation model, which has been found useful across a\nrange of different tasks due to the large volume of data used for training the\nmodel. In this study, SAM was adapted to perform fully-automated segmentation\nof menisci from 3D knee magnetic resonance images. A 3D U-Net was also trained\nas a baseline. It was found that, when fine-tuning only the decoder, SAM was\nunable to compete with 3D U-Net, achieving a Dice score of $0.81\\pm0.03$,\ncompared to $0.87\\pm0.03$, on a held-out test set. When fine-tuning SAM\nend-to-end, a Dice score of $0.87\\pm0.03$ was achieved. The performance of both\nthe end-to-end trained SAM configuration and the 3D U-Net were comparable to\nthe winning Dice score ($0.88\\pm0.03$) in the IWOAI Knee MRI Segmentation\nChallenge 2019. Performance in terms of the Hausdorff Distance showed that both\nconfigurations of SAM were inferior to 3D U-Net in matching the meniscus\nmorphology. Results demonstrated that, despite its generalisability, SAM was\nunable to outperform a basic 3D U-Net in meniscus segmentation, and may not be\nsuitable for similar 3D medical image segmentation tasks also involving fine\nanatomical structures with low contrast and poorly-defined boundaries.",
      "tldr_zh": "本文评估了 Segment Anything Model (SAM) 在 3D 膝部 MRI 图像中自动分割半月板的性能，并将其与 3D U-Net 基线模型进行比较。研究通过微调 SAM 的解码器或端到端方式，发现端到端微调的 SAM 达到 Dice score 为 0.87±0.03，与 3D U-Net 的表现相当，并接近 IWOAI Knee MRI Segmentation Challenge 2019 的获胜分数 (0.88±0.03)。然而，在 Hausdorff Distance 指标上，SAM 在匹配半月板形态方面不如 3D U-Net。总体结果表明，尽管 SAM 具有通用性，但对于涉及精细结构、低对比度和模糊边界的 3D 医学图像分割任务，可能不适合。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Work accepted at BMVC 2024. Minor changes to the camera-ready version\n  since acceptance include a corrected running header and the addition of an\n  Acknowledgments section (including code availability)",
      "pdf_url": "http://arxiv.org/pdf/2504.13340v3",
      "published_date": "2025-04-17 21:18:58 UTC",
      "updated_date": "2025-04-24 14:18:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:56:11.351995"
    },
    {
      "arxiv_id": "2504.13314v1",
      "title": "On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion Management",
      "title_zh": "翻译失败",
      "authors": [
        "Timothy Tjhay",
        "Ricardo J. Bessa",
        "Jose Paulos"
      ],
      "abstract": "The European Union's Artificial Intelligence (AI) Act defines robustness,\nresilience, and security requirements for high-risk sectors but lacks detailed\nmethodologies for assessment. This paper introduces a novel framework for\nquantitatively evaluating the robustness and resilience of reinforcement\nlearning agents in congestion management. Using the AI-friendly digital\nenvironment Grid2Op, perturbation agents simulate natural and adversarial\ndisruptions by perturbing the input of AI systems without altering the actual\nstate of the environment, enabling the assessment of AI performance under\nvarious scenarios. Robustness is measured through stability and reward impact\nmetrics, while resilience quantifies recovery from performance degradation. The\nresults demonstrate the framework's effectiveness in identifying\nvulnerabilities and improving AI robustness and resilience for critical\napplications.",
      "tldr_zh": "该论文针对欧盟 AI Act 对高风险领域提出的 robustness（鲁棒性）和 resilience（弹性）要求，但缺乏详细评估方法，引入了一个新框架，用于定量评估强化学习 agents（代理）在实时拥堵管理中的性能。框架利用 AI-friendly 环境 Grid2Op 和 perturbation agents（扰动代理）模拟自然和对抗性干扰，评估 AI 系统在不改变环境状态的情况下对输入扰动的响应。robustness 通过稳定性指标和奖励影响量化，而 resilience 通过性能下降后的恢复能力衡量。实验结果证明，该框架能有效识别 AI 漏洞，并提升其在关键应用中的鲁棒性和弹性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "IEEE PowerTech 2025 Conference",
      "pdf_url": "http://arxiv.org/pdf/2504.13314v1",
      "published_date": "2025-04-17 20:01:48 UTC",
      "updated_date": "2025-04-17 20:01:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:56:21.061059"
    },
    {
      "arxiv_id": "2504.13310v1",
      "title": "SAR Object Detection with Self-Supervised Pretraining and Curriculum-Aware Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Yasin Almalioglu",
        "Andrzej Kucik",
        "Geoffrey French",
        "Dafni Antotsiou",
        "Alexander Adam",
        "Cedric Archambeau"
      ],
      "abstract": "Object detection in satellite-borne Synthetic Aperture Radar (SAR) imagery\nholds immense potential in tasks such as urban monitoring and disaster\nresponse. However, the inherent complexities of SAR data and the scarcity of\nannotations present significant challenges in the advancement of object\ndetection in this domain. Notably, the detection of small objects in\nsatellite-borne SAR images poses a particularly intricate problem, because of\nthe technology's relatively low spatial resolution and inherent noise.\nFurthermore, the lack of large labelled SAR datasets hinders the development of\nsupervised deep learning-based object detection models. In this paper, we\nintroduce TRANSAR, a novel self-supervised end-to-end vision transformer-based\nSAR object detection model that incorporates masked image pre-training on an\nunlabeled SAR image dataset that spans more than $25,700$ km\\textsuperscript{2}\nground area. Unlike traditional object detection formulation, our approach\ncapitalises on auxiliary binary semantic segmentation, designed to segregate\nobjects of interest during the post-tuning, especially the smaller ones, from\nthe background. In addition, to address the innate class imbalance due to the\ndisproportion of the object to the image size, we introduce an adaptive\nsampling scheduler that dynamically adjusts the target class distribution\nduring training based on curriculum learning and model feedback. This approach\nallows us to outperform conventional supervised architecture such as DeepLabv3\nor UNet, and state-of-the-art self-supervised learning-based arhitectures such\nas DPT, SegFormer or UperNet, as shown by extensive evaluations on benchmark\nSAR datasets.",
      "tldr_zh": "这篇论文针对卫星搭载的合成孔径雷达 (SAR) 图像中的物体检测问题，提出了一种新型自监督端到端视觉 transformer 模型 TRANSAR，以应对数据复杂性、标注稀缺和小物体检测的挑战。TRANSAR 通过在超过 25,700 平方公里无标签 SAR 数据集上进行 masked image pre-training，并结合辅助二元语义分割和自适应采样调度器（基于 curriculum learning 和模型反馈动态调整类分布），有效缓解类不平衡问题。实验评估显示，TRANSAR 在基准 SAR 数据集上超过了传统监督模型如 DeepLabv3 或 UNet，以及最先进的自监督模型如 DPT、SegFormer 或 UperNet。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICLR 2025 ML4RS https://ml-for-rs.github.io/iclr2025/",
      "pdf_url": "http://arxiv.org/pdf/2504.13310v1",
      "published_date": "2025-04-17 19:44:05 UTC",
      "updated_date": "2025-04-17 19:44:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:56:34.041193"
    },
    {
      "arxiv_id": "2504.13973v1",
      "title": "Birds of a Different Feather Flock Together: Exploring Opportunities and Challenges in Animal-Human-Machine Teaming",
      "title_zh": "翻译失败",
      "authors": [
        "Myke C. Cohen",
        "David A. Grimm",
        "Reuth Mirsky",
        "Xiaoyun Yin"
      ],
      "abstract": "Animal-Human-Machine (AHM) teams are a type of hybrid intelligence system\nwherein interactions between a human, AI-enabled machine, and animal members\ncan result in unique capabilities greater than the sum of their parts. This\npaper calls for a systematic approach to studying the design of AHM team\nstructures to optimize performance and overcome limitations in various applied\nsettings. We consider the challenges and opportunities in investigating the\nsynergistic potential of AHM team members by introducing a set of dimensions of\nAHM team functioning to effectively utilize each member's strengths while\ncompensating for individual weaknesses. Using three representative examples of\nsuch teams -- security screening, search-and-rescue, and guide dogs -- the\npaper illustrates how AHM teams can tackle complex tasks. We conclude with open\nresearch directions that this multidimensional approach presents for studying\nhybrid human-AI systems beyond AHM teams.",
      "tldr_zh": "该论文探讨了Animal-Human-Machine (AHM) teams，这是一种混合智能系统，由人类、AI-enabled机器和动物组成，能够通过互动产生超越个体能力的独特功能。作者呼吁系统性地研究AHM团队结构的设计，以优化性能并应对应用场景中的挑战和机会，并引入AHM团队功能的维度框架来发挥各成员优势并弥补弱点。论文通过安全筛查、搜索和救援以及导盲犬等例子，展示了AHM团队处理复杂任务的潜力，并提出开放的研究方向，以扩展对混合人类-AI系统的多维研究。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13973v1",
      "published_date": "2025-04-17 19:26:50 UTC",
      "updated_date": "2025-04-17 19:26:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:56:44.059969"
    },
    {
      "arxiv_id": "2504.13296v1",
      "title": "Enhanced Pruning Strategy for Multi-Component Neural Architectures Using Component-Aware Graph Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Ganesh Sundaram",
        "Jonas Ulmen",
        "Daniel Görges"
      ],
      "abstract": "Deep neural networks (DNNs) deliver outstanding performance, but their\ncomplexity often prohibits deployment in resource-constrained settings.\nComprehensive structured pruning frameworks based on parameter dependency\nanalysis reduce model size with specific regard to computational performance.\nWhen applying them to Multi-Component Neural Architectures (MCNAs), they risk\nnetwork integrity by removing large parameter groups. We introduce a\ncomponent-aware pruning strategy, extending dependency graphs to isolate\nindividual components and inter-component flows. This creates smaller, targeted\npruning groups that conserve functional integrity. Demonstrated effectively on\na control task, our approach achieves greater sparsity and reduced performance\ndegradation, opening a path for optimizing complex, multi-component DNNs\nefficiently.",
      "tldr_zh": "该论文针对深度神经网络(DNNs)的复杂性问题，提出了一种增强的剪枝策略，专门优化多组件神经架构(Multi-Component Neural Architectures, MCNAs)，以避免传统基于参数依赖分析的框架破坏网络完整性。该策略通过组件感知图分析(Component-Aware Graph Analysis)扩展依赖图，隔离单个组件和组件间流，从而创建更小、更针对性的剪枝组，保持模型功能完整性。在控制任务上的实验表明，该方法实现了更高的稀疏性和更低的性能下降，为高效优化复杂多组件DNNs提供了新路径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, IFAC J3C",
      "pdf_url": "http://arxiv.org/pdf/2504.13296v1",
      "published_date": "2025-04-17 19:12:49 UTC",
      "updated_date": "2025-04-17 19:12:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:56:56.649823"
    },
    {
      "arxiv_id": "2504.13972v1",
      "title": "Governance Challenges in Reinforcement Learning from Human Feedback: Evaluator Rationality and Reinforcement Stability",
      "title_zh": "基于人类反馈的强化学习中的治理挑战：评估者理性和强化稳定性",
      "authors": [
        "Dana Alsagheer",
        "Abdulrahman Kamal",
        "Mohammad Kamal",
        "Weidong Shi"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is central in aligning\nlarge language models (LLMs) with human values and expectations. However, the\nprocess remains susceptible to governance challenges, including evaluator bias,\ninconsistency, and the unreliability of feedback. This study examines how the\ncognitive capacity of evaluators, specifically their level of rationality,\naffects the stability of reinforcement signals. A controlled experiment\ncomparing high-rationality and low-rationality participants reveals that\nevaluators with higher rationality scores produce significantly more consistent\nand expert-aligned feedback. In contrast, lower-rationality participants\ndemonstrate considerable variability in their reinforcement decisions ($p <\n0.01$). To address these challenges and improve RLHF governance, we recommend\nimplementing evaluator pre-screening, systematic auditing of feedback\nconsistency, and reliability-weighted reinforcement aggregation. These measures\nenhance the fairness, transparency, and robustness of AI alignment pipelines.",
      "tldr_zh": "这篇论文探讨了Reinforcement Learning from Human Feedback (RLHF) 在对齐大型语言模型(LLMs) 时面临的治理挑战，特别是评估者理性水平对强化信号稳定性的影响。通过对照实验比较高理性与低理性参与者，研究发现高理性评估者提供更一致且与专家对齐的反馈，而低理性评估者反馈变异性显著（p < 0.01）。为提升RLHF的可靠性，论文推荐采用评估者预筛选、反馈一致性审计和可靠性加权聚合等措施，以增强AI对齐流程的公平性、透明度和稳健性。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13972v1",
      "published_date": "2025-04-17 19:10:00 UTC",
      "updated_date": "2025-04-17 19:10:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:57:09.547168"
    },
    {
      "arxiv_id": "2504.13971v1",
      "title": "The Future of Internet of Things and Multimodal Language Models in 6G Networks: Opportunities and Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Abdelrahman Soliman"
      ],
      "abstract": "Based on recent trends in artificial intelligence and IoT research. The\ncooperative potential of integrating the Internet of Things (IoT) and\nMultimodal Language Models (MLLMs) is presented in this survey paper for future\n6G systems. It focuses on the applications of this integration in different\nfields, such as healthcare, agriculture, and smart cities, and investigates the\nfour pillars of IoT integration, such as sensors, communication, processing,\nand security. The paper provides a comprehensive description of IoT and MLLM\ntechnologies and applications, addresses the role of multimodality in each\npillar, and concludes with an overview of the most significant challenges and\ndirections for future research. The general survey is a roadmap for researchers\ninterested in tracing the application areas of MLLMs and IoT, highlighting the\npotential and challenges in this rapidly growing field. The survey recognizes\nthe need to deal with data availability, computational expense, privacy, and\nreal-time processing to harness the complete potential of IoT, MLLM, and 6G\ntechnology",
      "tldr_zh": "这篇调查论文探讨了将Internet of Things (IoT)与Multimodal Language Models (MLLMs)整合到未来6G网络中的合作潜力，涵盖医疗、农业和智能城市等领域的应用。论文分析了IoT整合的四个支柱——传感器、通信、处理和安全，并强调多模态技术在这些支柱中的作用，提供了一个全面的技术描述和应用路线图。最终，它总结了关键挑战，如数据可用性、计算开销、隐私保护和实时处理，并指出未来研究方向，以充分发挥IoT、MLLMs和6G技术的潜力。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "cs.NI"
      ],
      "primary_category": "cs.CY",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.13971v1",
      "published_date": "2025-04-17 18:57:06 UTC",
      "updated_date": "2025-04-17 18:57:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:57:20.457804"
    },
    {
      "arxiv_id": "2504.13277v1",
      "title": "Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces",
      "title_zh": "翻译失败",
      "authors": [
        "Soorya Ram Shimgekar",
        "Violeta J. Rodriguez",
        "Paul A. Bloom",
        "Dong Whi Yoo",
        "Koustuv Saha"
      ],
      "abstract": "Suicide is a critical global public health issue, with millions experiencing\nsuicidal ideation (SI) each year. Online spaces enable individuals to express\nSI and seek peer support. While prior research has revealed the potential of\ndetecting SI using machine learning and natural language analysis, a key\nlimitation is the lack of a theoretical framework to understand the underlying\nfactors affecting high-risk suicidal intent. To bridge this gap, we adopted the\nInterpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607\nposts from Reddit's r/SuicideWatch, categorizing them into SI dimensions\n(Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk\nfactors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired\nCapability of Suicide). We found that high-risk SI posts express planning and\nattempts, methods and tools, and weaknesses and pain. In addition, we also\nexamined the language of supportive responses through psycholinguistic and\ncontent analyses to find that individuals respond differently to different\nstages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI\nchatbots in providing effective supportive responses to suicidal ideation\nposts. We found that although AI improved structural coherence, expert\nevaluations highlight persistent shortcomings in providing dynamic,\npersonalized, and deeply empathetic support. These findings underscore the need\nfor careful reflection and deeper understanding in both the development and\nconsideration of AI-driven interventions for effective mental health support.",
      "tldr_zh": "本文使用 Interpersonal Theory of Suicide (IPTS) 作为理论框架，分析 Reddit r/SuicideWatch 的 59,607 条帖子，分类自杀意念 (SI) 维度（如 Loneliness 和 Self Hate）以及风险因素（如 Thwarted Belongingness 和 Perceived Burdensomeness），以探讨在线空间中 SI 的潜在因素。研究发现，高风险 SI 帖子常涉及自杀计划、方法、工具和个人痛苦，而支持性回应根据 SI 阶段（如早期或高级阶段）存在差异，通过心理语言和内容分析进行评估。最后，评估 AI 聊天机器人发现其虽提高了回应的结构连贯性，但缺乏动态、个性化及深层移情支持，强调在开发 AI 干预时需更深入反思以确保有效性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.SI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13277v1",
      "published_date": "2025-04-17 18:40:55 UTC",
      "updated_date": "2025-04-17 18:40:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:57:34.570041"
    },
    {
      "arxiv_id": "2504.13263v2",
      "title": "Causal-Copilot: An Autonomous Causal Analysis Agent",
      "title_zh": "Causal-Copilot：自治因果分析代理",
      "authors": [
        "Xinyue Wang",
        "Kun Zhou",
        "Wenyi Wu",
        "Har Simrat Singh",
        "Fang Nan",
        "Songyao Jin",
        "Aryan Philip",
        "Saloni Patnaik",
        "Hou Zhu",
        "Shivam Singh",
        "Parjanya Prashant",
        "Qian Shen",
        "Biwei Huang"
      ],
      "abstract": "Causal analysis plays a foundational role in scientific discovery and\nreliable decision-making, yet it remains largely inaccessible to domain experts\ndue to its conceptual and algorithmic complexity. This disconnect between\ncausal methodology and practical usability presents a dual challenge: domain\nexperts are unable to leverage recent advances in causal learning, while causal\nresearchers lack broad, real-world deployment to test and refine their methods.\nTo address this, we introduce Causal-Copilot, an autonomous agent that\noperationalizes expert-level causal analysis within a large language model\nframework. Causal-Copilot automates the full pipeline of causal analysis for\nboth tabular and time-series data -- including causal discovery, causal\ninference, algorithm selection, hyperparameter optimization, result\ninterpretation, and generation of actionable insights. It supports interactive\nrefinement through natural language, lowering the barrier for non-specialists\nwhile preserving methodological rigor. By integrating over 20 state-of-the-art\ncausal analysis techniques, our system fosters a virtuous cycle -- expanding\naccess to advanced causal methods for domain experts while generating rich,\nreal-world applications that inform and advance causal theory. Empirical\nevaluations demonstrate that Causal-Copilot achieves superior performance\ncompared to existing baselines, offering a reliable, scalable, and extensible\nsolution that bridges the gap between theoretical sophistication and real-world\napplicability in causal analysis. A live interactive demo of Causal-Copilot is\navailable at https://causalcopilot.com/.",
      "tldr_zh": "该研究引入了 Causal-Copilot，一种自治代理，旨在解决因果分析(causal analysis)的复杂性问题，使其更易于领域专家使用。该代理在大型语言模型框架中自动化了完整因果分析流程，包括因果发现(causal discovery)、因果推理(causal inference)、算法选择、超参数优化、结果解释以及生成可行动见解，支持 tabular 和 time-series 数据。通过自然语言交互，用户可进行交互式精炼，同时保持方法严谨。实验评估表明，Causal-Copilot 比现有基线性能更优，提供了一个可靠、可扩展的解决方案，促进因果方法在实际应用中的推广和理论进步。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13263v2",
      "published_date": "2025-04-17 18:05:39 UTC",
      "updated_date": "2025-04-21 17:58:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:57:45.663505"
    },
    {
      "arxiv_id": "2504.13261v1",
      "title": "CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models",
      "title_zh": "CPG-EVAL：一个多层次基准，用于评估大语言模型的中文教学语法能力",
      "authors": [
        "Dong Wang"
      ],
      "abstract": "Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT\nhas significantly impacted foreign language education, yet their pedagogical\ngrammar competence remains under-assessed. This paper introduces CPG-EVAL, the\nfirst dedicated benchmark specifically designed to evaluate LLMs' knowledge of\npedagogical grammar within the context of foreign language instruction.\nMethodology: The benchmark comprises five tasks designed to assess grammar\nrecognition, fine-grained grammatical distinction, categorical discrimination,\nand resistance to linguistic interference. Findings: Smaller-scale models can\nsucceed in single language instance tasks, but struggle with multiple instance\ntasks and interference from confusing instances. Larger-scale models show\nbetter resistance to interference but still have significant room for accuracy\nimprovement. The evaluation indicates the need for better instructional\nalignment and more rigorous benchmarks, to effectively guide the deployment of\nLLMs in educational contexts. Value: This study offers the first specialized,\ntheory-driven, multi-tiered benchmark framework for systematically evaluating\nLLMs' pedagogical grammar competence in Chinese language teaching contexts.\nCPG-EVAL not only provides empirical insights for educators, policymakers, and\nmodel developers to better gauge AI's current abilities in educational\nsettings, but also lays the groundwork for future research on improving model\nalignment, enhancing educational suitability, and ensuring informed\ndecision-making concerning LLM integration in foreign language instruction.",
      "tldr_zh": "这项研究引入了CPG-EVAL，这是第一个专门针对大型语言模型(LLMs)在中国外语教学语境中教学语法能力的多层基准。CPG-EVAL包括五个任务，评估语法识别、细粒度语法区分、类别区分以及对语言干扰的抵抗能力。实验发现，小型模型在单语言实例任务中表现较好，但多实例任务和干扰时存在挑战，而大型模型虽更抗干扰但准确率仍有提升空间。该基准为教育者、政策制定者和模型开发者提供宝贵洞见，推动LLMs在教育中的优化对齐和应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 1 figure, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.13261v1",
      "published_date": "2025-04-17 18:01:50 UTC",
      "updated_date": "2025-04-17 18:01:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:57:58.199624"
    },
    {
      "arxiv_id": "2504.13180v1",
      "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding",
      "title_zh": "PerceptionLM：用于详细视觉理解的开源数据和模型",
      "authors": [
        "Jang Hyun Cho",
        "Andrea Madotto",
        "Effrosyni Mavroudi",
        "Triantafyllos Afouras",
        "Tushar Nagarajan",
        "Muhammad Maaz",
        "Yale Song",
        "Tengyu Ma",
        "Shuming Hu",
        "Suyog Jain",
        "Miguel Martin",
        "Huiyu Wang",
        "Hanoona Rasheed",
        "Peize Sun",
        "Po-Yao Huang",
        "Daniel Bolya",
        "Nikhila Ravi",
        "Shashank Jain",
        "Tammy Stark",
        "Shane Moon",
        "Babak Damavandi",
        "Vivian Lee",
        "Andrew Westbury",
        "Salman Khan",
        "Philipp Krähenbühl",
        "Piotr Dollár",
        "Lorenzo Torresani",
        "Kristen Grauman",
        "Christoph Feichtenhofer"
      ],
      "abstract": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.",
      "tldr_zh": "本文提出一个完全开放和可重现的框架，用于构建 Perception Language Model (PLM)，以解决现有封闭源代码 vision-language models 在图像和视频理解中的透明性问题。作者分析了不依赖于 distillation 的标准训练管道，并通过探索大规模合成数据，识别出详细视频理解中的关键数据缺口。针对这些缺口，他们发布了 2.8M 人类标注的细粒度视频问答对和时空定位视频字幕，并引入 PLM-VideoBench 评估套件，用于评估视频的 \"what\"、\"where\"、\"when\" 和 \"how\" 推理能力。所有数据、训练配方、代码和模型均公开，确保研究的完全可重现性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical report",
      "pdf_url": "http://arxiv.org/pdf/2504.13180v1",
      "published_date": "2025-04-17 17:59:56 UTC",
      "updated_date": "2025-04-17 17:59:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:58:10.442730"
    },
    {
      "arxiv_id": "2504.13173v1",
      "title": "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Ali Behrouz",
        "Meisam Razaviyayn",
        "Peilin Zhong",
        "Vahab Mirrokni"
      ],
      "abstract": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
      "tldr_zh": "本论文受人类认知现象“attentional bias”启发，将神经架构（如 Transformers、Titans 和线性 RNNs）重新定义为关联记忆模块，通过内部目标学习键值映射，并提出替代的 attentional bias 配置及其训练近似方法。论文还重新解释遗忘机制为保留正则化，并引入新的遗忘门。基于此，作者提出 Miras 框架，该框架通过四个设计选择（关联记忆架构、attentional bias 目标、保留门和记忆学习算法）开发了新模型 Moneta、Yaad 和 Memora，这些模型在语言建模、常识推理和回忆密集任务上表现出色，甚至超越 Transformers 和其他线性 RNNs。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13173v1",
      "published_date": "2025-04-17 17:59:33 UTC",
      "updated_date": "2025-04-17 17:59:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:58:23.481042"
    },
    {
      "arxiv_id": "2504.13171v1",
      "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Lin",
        "Charlie Snell",
        "Yu Wang",
        "Charles Packer",
        "Sarah Wooders",
        "Ion Stoica",
        "Joseph E. Gonzalez"
      ],
      "abstract": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.",
      "tldr_zh": "该论文提出了一种名为 sleep-time compute 的方法，允许大型语言模型(LLMs)在查询前离线预计算上下文，从而减少测试时的计算需求和延迟。实验在修改后的推理任务 Stateful GSM-Symbolic 和 Stateful AIME 上显示，该方法可将测试计算需求降低约 5 倍，同时保持相同准确率，并通过扩展 sleep-time compute 进一步提高准确率达 13% 和 18%。此外，论文引入 Multi-Query GSM-Symbolic 任务，通过在多个相关查询上分摊预计算，将平均查询成本减少 2.5 倍，并分析发现用户查询的可预测性与方法的有效性高度相关。最终，他们通过一个实际代理 SWE 任务的案例研究，验证了该方法的实用性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Code and data released at:\n  https://github.com/letta-ai/sleep-time-compute",
      "pdf_url": "http://arxiv.org/pdf/2504.13171v1",
      "published_date": "2025-04-17 17:59:25 UTC",
      "updated_date": "2025-04-17 17:59:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:58:34.689389"
    },
    {
      "arxiv_id": "2504.13165v1",
      "title": "RUKA: Rethinking the Design of Humanoid Hands with Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Anya Zorin",
        "Irmak Guzey",
        "Billy Yan",
        "Aadhithya Iyer",
        "Lisa Kondrich",
        "Nikhil X. Bhattasali",
        "Lerrel Pinto"
      ],
      "abstract": "Dexterous manipulation is a fundamental capability for robotic systems, yet\nprogress has been limited by hardware trade-offs between precision,\ncompactness, strength, and affordability. Existing control methods impose\ncompromises on hand designs and applications. However, learning-based\napproaches present opportunities to rethink these trade-offs, particularly to\naddress challenges with tendon-driven actuation and low-cost materials. This\nwork presents RUKA, a tendon-driven humanoid hand that is compact, affordable,\nand capable. Made from 3D-printed parts and off-the-shelf components, RUKA has\n5 fingers with 15 underactuated degrees of freedom enabling diverse human-like\ngrasps. Its tendon-driven actuation allows powerful grasping in a compact,\nhuman-sized form factor. To address control challenges, we learn\njoint-to-actuator and fingertip-to-actuator models from motion-capture data\ncollected by the MANUS glove, leveraging the hand's morphological accuracy.\nExtensive evaluations demonstrate RUKA's superior reachability, durability, and\nstrength compared to other robotic hands. Teleoperation tasks further showcase\nRUKA's dexterous movements. The open-source design and assembly instructions of\nRUKA, code, and data are available at https://ruka-hand.github.io/.",
      "tldr_zh": "这篇论文重新设计了人形手，通过学习方法解决硬件权衡（如精度、紧凑性、强度和负担得起性），引入了 RUKA，一种基于 tendon-driven 驱动的紧凑、负担得起的机器人手。RUKA 使用 3D 打印零件和现成组件，具有 5 个手指和 15 个 underactuated 自由度，支持多样的人类-like 抓取，并通过从 MANUS 手套的运动捕捉数据学习 joint-to-actuator 和 fingertip-to-actuator 模型。实验结果显示 RUKA 在可达性、耐用性和强度上优于现有手，并在 teleoperation 任务中展示了出色的灵巧性能；设计、代码和数据已开源发布。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Website at https://ruka-hand.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2504.13165v1",
      "published_date": "2025-04-17 17:58:59 UTC",
      "updated_date": "2025-04-17 17:58:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:58:46.647936"
    },
    {
      "arxiv_id": "2504.13151v1",
      "title": "MIB: A Mechanistic Interpretability Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Aaron Mueller",
        "Atticus Geiger",
        "Sarah Wiegreffe",
        "Dana Arad",
        "Iván Arcuschin",
        "Adam Belfki",
        "Yik Siu Chan",
        "Jaden Fiotto-Kaufman",
        "Tal Haklay",
        "Michael Hanna",
        "Jing Huang",
        "Rohan Gupta",
        "Yaniv Nikankin",
        "Hadas Orgad",
        "Nikhil Prakash",
        "Anja Reusch",
        "Aruna Sankaranarayanan",
        "Shun Shao",
        "Alessandro Stolfo",
        "Martin Tutek",
        "Amir Zur",
        "David Bau",
        "Yonatan Belinkov"
      ],
      "abstract": "How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.",
      "tldr_zh": "这篇论文引入了 MIB（Mechanistic Interpretability Benchmark），一个用于评估神经语言模型解释性方法的基准，涵盖四个任务和五个模型，强调精确恢复相关因果路径或变量。MIB 包括电路定位轨道（比较如 attribution patching 和信息流路由等方法，以定位关键模型组件）和因果变量定位轨道（比较如 SAEs 或 DAS 等方法，以特征化隐藏向量）。实验发现，归因和掩码优化方法在电路定位中表现最佳，而监督 DAS 方法在因果变量定位中领先，SAE 特征不如标准神经元。该基准提升了方法间的可比性，并证实了机械解释性领域取得了真实进步。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13151v1",
      "published_date": "2025-04-17 17:55:45 UTC",
      "updated_date": "2025-04-17 17:55:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:58:57.169799"
    },
    {
      "arxiv_id": "2504.13150v1",
      "title": "Readable Twins of Unreadable Models",
      "title_zh": "翻译失败",
      "authors": [
        "Krzysztof Pancerz",
        "Piotr Kulicki",
        "Michał Kalisz",
        "Andrzej Burda",
        "Maciej Stanisławski",
        "Jaromir Sarzyński"
      ],
      "abstract": "Creating responsible artificial intelligence (AI) systems is an important\nissue in contemporary research and development of works on AI. One of the\ncharacteristics of responsible AI systems is their explainability. In the\npaper, we are interested in explainable deep learning (XDL) systems. On the\nbasis of the creation of digital twins of physical objects, we introduce the\nidea of creating readable twins (in the form of imprecise information flow\nmodels) for unreadable deep learning models. The complete procedure for\nswitching from the deep learning model (DLM) to the imprecise information flow\nmodel (IIFM) is presented. The proposed approach is illustrated with an example\nof a deep learning classification model for image recognition of handwritten\ndigits from the MNIST data set.",
      "tldr_zh": "该论文探讨了负责任人工智能（AI）系统的可解释性（explainability），特别针对不可读的深度学习模型（DLMs）。作者引入了“readable twins”的概念，即基于数字孪生（digital twins）的思路，将DLMs转换为不精确信息流模型（IIFMs），以提升模型的可读性和解释性。论文详细描述了从DLM到IIFM的完整转换过程，并通过MNIST数据集的手写数字识别分类模型作为示例进行说明。这一方法为创建explainable deep learning (XDL)系统提供了新途径，促进AI的可信度和应用。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Based on the abstract accepted for ISFS 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.13150v1",
      "published_date": "2025-04-17 17:55:34 UTC",
      "updated_date": "2025-04-17 17:55:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:59:08.941227"
    },
    {
      "arxiv_id": "2504.13146v2",
      "title": "Antidistillation Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Yash Savani",
        "Asher Trockman",
        "Zhili Feng",
        "Avi Schwarzschild",
        "Alexander Robey",
        "Marc Finzi",
        "J. Zico Kolter"
      ],
      "abstract": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\nAntidistillation sampling provides exactly this capability. By strategically\nmodifying a model's next-token probability distribution, antidistillation\nsampling poisons reasoning traces, rendering them significantly less effective\nfor distillation while preserving the model's practical utility. For further\ndetails, see https://antidistillation.com.",
      "tldr_zh": "该论文探讨了前沿模型在生成推理痕迹时可能被用于模型蒸馏（model distillation）的安全风险，并引入了 Antidistillation Sampling 作为一种防护策略。该方法通过战略性地修改模型的 next-token probability distribution，来“毒害”推理痕迹，使其对蒸馏过程显著失效，同时保持模型的实际性能不变。通过这种采样技术，研究者旨在保护模型所有者的知识产权，而不影响模型的实用性。详情可参考论文提供的链接。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13146v2",
      "published_date": "2025-04-17 17:54:14 UTC",
      "updated_date": "2025-04-24 18:49:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:59:19.975155"
    },
    {
      "arxiv_id": "2504.13145v2",
      "title": "Exploring Expert Failures Improves LLM Agent Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Li-Cheng Lan",
        "Andrew Bai",
        "Minhao Cheng",
        "Cho-Jui Hsieh",
        "Tianyi Zhou"
      ],
      "abstract": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.",
      "tldr_zh": "该研究发现，现有 Rejection Sampling Fine-Tuning (RFT) 方法在训练 Large Language Models (LLMs) 代理时，倾向于简单子任务，导致复杂子任务（out-of-distribution, OOD）无法有效解决。论文提出 Exploring Expert Failures (EEF) 方法，通过从失败的专家轨迹（如 GPT-4 生成的）中提取有益动作（如计划和关键操作），并将其整合到训练数据集，同时排除潜在有害动作，以提升代理的探索效率和技能获取。实验结果显示，EEF 显著提高了代理性能，在 WebShop 中达到 62% 的胜率，超过了 RFT (53.6%) 和 GPT-4 (35.6%)，并在 WebShop 和 SciWorld 中创下新纪录。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13145v2",
      "published_date": "2025-04-17 17:53:54 UTC",
      "updated_date": "2025-04-18 19:36:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:59:34.369654"
    },
    {
      "arxiv_id": "2504.13143v1",
      "title": "$\\texttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Siwei Yang",
        "Mude Hui",
        "Bingchen Zhao",
        "Yuyin Zhou",
        "Nataniel Ruiz",
        "Cihang Xie"
      ],
      "abstract": "We introduce $\\texttt{Complex-Edit}$, a comprehensive benchmark designed to\nsystematically evaluate instruction-based image editing models across\ninstructions of varying complexity. To develop this benchmark, we harness\nGPT-4o to automatically collect a diverse set of editing instructions at scale.\nOur approach follows a well-structured ``Chain-of-Edit'' pipeline: we first\ngenerate individual atomic editing tasks independently and then integrate them\nto form cohesive, complex instructions. Additionally, we introduce a suite of\nmetrics to assess various aspects of editing performance, along with a\nVLM-based auto-evaluation pipeline that supports large-scale assessments. Our\nbenchmark yields several notable insights: 1) Open-source models significantly\nunderperform relative to proprietary, closed-source models, with the\nperformance gap widening as instruction complexity increases; 2) Increased\ninstructional complexity primarily impairs the models' ability to retain key\nelements from the input images and to preserve the overall aesthetic quality;\n3) Decomposing a complex instruction into a sequence of atomic steps, executed\nin a step-by-step manner, substantially degrades performance across multiple\nmetrics; 4) A straightforward Best-of-N selection strategy improves results for\nboth direct editing and the step-by-step sequential approach; and 5) We observe\na ``curse of synthetic data'': when synthetic data is involved in model\ntraining, the edited images from such models tend to appear increasingly\nsynthetic as the complexity of the editing instructions rises -- a phenomenon\nthat intriguingly also manifests in the latest GPT-4o outputs.",
      "tldr_zh": "本研究引入了$\\texttt{Complex-Edit}$基准，用于系统评估基于指令的图像编辑模型在不同复杂程度指令下的性能。该基准利用GPT-4o和“Chain-of-Edit”管道生成多样化的编辑指令，先创建独立原子任务再整合成复杂指令，并引入VLM-based自动评估指标。关键发现包括：开源模型远逊于闭源模型，且性能差距随指令复杂度增大；复杂度增加会削弱模型保留输入图像关键元素和美学质量的能力；指令分解为步步执行或使用Best-of-N策略可改善部分结果，但合成数据训练可能导致编辑图像更显合成现象。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://ucsc-vlaa.github.io/Complex-Edit/, Dataset:\n  https://huggingface.co/datasets/UCSC-VLAA/Complex-Edit",
      "pdf_url": "http://arxiv.org/pdf/2504.13143v1",
      "published_date": "2025-04-17 17:51:59 UTC",
      "updated_date": "2025-04-17 17:51:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:59:44.523112"
    },
    {
      "arxiv_id": "2504.13139v2",
      "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo",
      "title_zh": "通过 Sequential Monte Carlo 实现大型语言模型的语法和语义控制",
      "authors": [
        "João Loula",
        "Benjamin LeBrun",
        "Li Du",
        "Ben Lipkin",
        "Clemente Pasti",
        "Gabriel Grand",
        "Tianyu Liu",
        "Yahya Emara",
        "Marjorie Freedman",
        "Jason Eisner",
        "Ryan Cotterell",
        "Vikash Mansinghka",
        "Alexander K. Lew",
        "Tim Vieira",
        "Timothy J. O'Donnell"
      ],
      "abstract": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.",
      "tldr_zh": "这篇论文提出了一种基于 Sequential Monte Carlo (SMC) 的架构，用于控制大型语言模型 (LMs) 的生成，使其输出符合特定语法或语义约束，通过概率条件化框架实现高效的生成过程。SMC 方法允许在推理时灵活整合领域特定约束，并动态优化计算资源分配。实验在 Python 代码生成、text-to-SQL、目标推理和分子合成等四个领域显示，小型开源模型使用此方法可超越 8 倍大的模型，甚至闭源微调模型，性能提升主要源于对后验分布的更准确逼近。该系统构建于 Lew et al. (2023) 的框架之上，提供了一个简单可编程的工具，适用于多种受控生成问题。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "34 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.13139v2",
      "published_date": "2025-04-17 17:49:40 UTC",
      "updated_date": "2025-04-18 18:45:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T13:59:57.694530"
    },
    {
      "arxiv_id": "2504.13969v2",
      "title": "Tinker Tales: Interactive Storytelling Framework for Early Childhood Narrative Development and AI Literacy",
      "title_zh": "Tinker Tales：针对早期儿童叙事发展和 AI 素养的互动故事讲述框架",
      "authors": [
        "Nayoung Choi",
        "Peace Cyebukayire",
        "Jinho D. Choi"
      ],
      "abstract": "This paper presents Tinker Tales, an interactive storytelling framework in\nthe format of a board game, designed to support both narrative development and\nAI literacy in early childhood. The framework integrates tangible and\nspeech-based interactions with AI through NFC chip-attached pawns and tokens,\nalong with a speaker and microphone. Children select and define key story\nelements-such as characters, places, items, and emotions-using the pawns and\ntokens, providing further details to the AI and receiving proper assistance,\nsimilar to how adults prompt AI for specific tasks (e.g., writing). For\nevaluation, several game sessions were simulated with a child AI agent, and the\nquality and safety of the generated stories were assessed from various\nperspectives. This work highlights the potential of combining physical and\ndigital elements in AI literacy, offering a safe and engaging way for children\nto learn how to effectively collaborate with AI.",
      "tldr_zh": "本论文介绍了Tinker Tales，一种以棋盘游戏形式设计的互动故事框架，旨在促进幼儿的叙事发展和AI literacy。该框架整合了NFC芯片附着的棋子和令牌、扬声器及麦克风等有形和语音互动，让孩子们选择并定义故事元素（如角色、地点、物品和情感），并像成人提示AI一样获取协助。评估通过模拟游戏会话显示，该框架能生成高质量且安全的故事情节，突显了物理与数字元素结合在AI教育中的潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13969v2",
      "published_date": "2025-04-17 17:47:55 UTC",
      "updated_date": "2025-04-22 23:59:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:00:08.711167"
    },
    {
      "arxiv_id": "2504.13131v1",
      "title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results",
      "title_zh": "NTIRE 2025 短视频 UGC 视频质量评估和增强挑战：方法和结果",
      "authors": [
        "Xin Li",
        "Kun Yuan",
        "Bingchen Li",
        "Fengbin Guan",
        "Yizhen Shao",
        "Zihao Yu",
        "Xijun Wang",
        "Yiting Lu",
        "Wei Luo",
        "Suhang Yao",
        "Ming Sun",
        "Chao Zhou",
        "Zhibo Chen",
        "Radu Timofte",
        "Yabin Zhang",
        "Ao-Xiang Zhang",
        "Tianwu Zhi",
        "Jianzhao Liu",
        "Yang Li",
        "Jingwen Xu",
        "Yiting Liao",
        "Yushen Zuo",
        "Mingyang Wu",
        "Renjie Li",
        "Shengyun Zhong",
        "Zhengzhong Tu",
        "Yufan Liu",
        "Xiangguang Chen",
        "Zuowei Cao",
        "Minhao Tang",
        "Shan Liu",
        "Kexin Zhang",
        "Jingfen Xie",
        "Yan Wang",
        "Kai Chen",
        "Shijie Zhao",
        "Yunchen Zhang",
        "Xiangkai Xu",
        "Hong Gao",
        "Ji Shi",
        "Yiming Bao",
        "Xiugang Dong",
        "Xiangsheng Zhou",
        "Yaofeng Tu",
        "Ying Liang",
        "Yiwen Wang",
        "Xinning Chai",
        "Yuxuan Zhang",
        "Zhengxue Cheng",
        "Yingsheng Qin",
        "Yucai Yang",
        "Rong Xie",
        "Li Song",
        "Wei Sun",
        "Kang Fu",
        "Linhan Cao",
        "Dandan Zhu",
        "Kaiwei Zhang",
        "Yucheng Zhu",
        "Zicheng Zhang",
        "Menghan Hu",
        "Xiongkuo Min",
        "Guangtao Zhai",
        "Zhi Jin",
        "Jiawei Wu",
        "Wei Wang",
        "Wenjian Zhang",
        "Yuhai Lan",
        "Gaoxiong Yi",
        "Hengyuan Na",
        "Wang Luo",
        "Di Wu",
        "MingYin Bai",
        "Jiawang Du",
        "Zilong Lu",
        "Zhenyu Jiang",
        "Hui Zeng",
        "Ziguan Cui",
        "Zongliang Gan",
        "Guijin Tang",
        "Xinglin Xie",
        "Kehuan Song",
        "Xiaoqiang Lu",
        "Licheng Jiao",
        "Fang Liu",
        "Xu Liu",
        "Puhua Chen",
        "Ha Thu Nguyen",
        "Katrien De Moor",
        "Seyed Ali Amirshahi",
        "Mohamed-Chaker Larabi",
        "Qi Tang",
        "Linfeng He",
        "Zhiyong Gao",
        "Zixuan Gao",
        "Guohua Zhang",
        "Zhiye Huang",
        "Yi Deng",
        "Qingmiao Jiang",
        "Lu Chen",
        "Yi Yang",
        "Xi Liao",
        "Nourine Mohammed Nadir",
        "Yuxuan Jiang",
        "Qiang Zhu",
        "Siyue Teng",
        "Fan Zhang",
        "Shuyuan Zhu",
        "Bing Zeng",
        "David Bull",
        "Meiqin Liu",
        "Chao Yao",
        "Yao Zhao"
      ],
      "abstract": "This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC\nVideo Quality Assessment and Enhancement. The challenge comprises two tracks:\n(i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image\nSuper-Resolution (KwaiSR). Track 1 aims to advance the development of\nlightweight and efficient video quality assessment (VQA) models, with an\nemphasis on eliminating reliance on model ensembles, redundant weights, and\nother computationally expensive components in the previous IQA/VQA\ncompetitions. Track 2 introduces a new short-form UGC dataset tailored for\nsingle image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800\nsynthetically generated S-UGC image pairs and 1,900 real-world S-UGC images,\nwhich are split into training, validation, and test sets using a ratio of\n8:1:1. The primary objective of the challenge is to drive research that\nbenefits the user experience of short-form UGC platforms such as Kwai and\nTikTok. This challenge attracted 266 participants and received 18 valid final\nsubmissions with corresponding fact sheets, significantly contributing to the\nprogress of short-form UGC VQA and image superresolution. The project is\npublicly available at https://github.com/lixinustc/KVQE-\nChallengeCVPR-NTIRE2025.",
      "tldr_zh": "这篇论文回顾了 NTIRE 2025 挑战赛，聚焦于短视频 UGC 的质量评估和增强，包括两个赛道：Efficient Video Quality Assessment (KVQ) 和 Diffusion-based Image Super-Resolution (KwaiSR)。KVQ 赛道旨在开发轻量级、高效的视频质量评估 (VQA) 模型，避免使用模型集成和冗余组件；KwaiSR 赛道引入了一个新数据集，包含 1800 对合成 S-UGC 图像对和 1900 张真实 S-UGC 图像，用于图像超分辨率训练、验证和测试。挑战赛吸引了 266 名参与者，收到 18 个有效提交，推动了短视频 UGC 平台如 Kwai 和 TikTok 的用户体验研究进展。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Challenge Report of NTIRE 2025; Methods from 18 Teams; Accepted by\n  CVPR Workshop; 21 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.13131v1",
      "published_date": "2025-04-17 17:45:34 UTC",
      "updated_date": "2025-04-17 17:45:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:00:22.809530"
    },
    {
      "arxiv_id": "2504.13129v1",
      "title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis",
      "title_zh": "Science-T2I：解决图像合成中的科学错觉",
      "authors": [
        "Jialuo Li",
        "Wenhao Chai",
        "Xingyu Fu",
        "Haiyang Xu",
        "Saining Xie"
      ],
      "abstract": "We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.",
      "tldr_zh": "本研究提出 Science-T2I，一种专家标注的对抗数据集，包含20k图像对和9k提示，旨在整合科学知识以提升图像生成的真实性和一致性。基于此，他们开发了SciScore，一个端到端的奖励模型，通过增强预训练CLIP模型的科学理解和视觉能力，来评估生成的图像。接着，引入一个两阶段训练框架，包括监督微调和masked在线微调阶段，用于将科学知识融入现有生成模型，如FLUX。实验结果显示，SciScore的性能接近人类水平，提升约5%，而应用于FLUX的微调方法使性能提升超过50%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://jialuo-li.github.io/Science-T2I-Web",
      "pdf_url": "http://arxiv.org/pdf/2504.13129v1",
      "published_date": "2025-04-17 17:44:19 UTC",
      "updated_date": "2025-04-17 17:44:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:00:32.410049"
    },
    {
      "arxiv_id": "2504.13128v1",
      "title": "FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents",
      "title_zh": "FreshStack: 针对技术文档构建现实的检索评估基准",
      "authors": [
        "Nandan Thakur",
        "Jimmy Lin",
        "Sam Havens",
        "Michael Carbin",
        "Omar Khattab",
        "Andrew Drozdov"
      ],
      "abstract": "We introduce FreshStack, a reusable framework for automatically building\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\nand answers. FreshStack conducts the following steps: (1) automatic corpus\ncollection from code and technical documentation, (2) nugget generation from\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\nretrieval models, when applied out-of-the-box, significantly underperform\noracle approaches on all five topics, denoting plenty of headroom to improve IR\nquality. In addition, we identify cases where rerankers do not clearly improve\nfirst-stage retrieval accuracy (two out of five topics). We hope that\nFreshStack will facilitate future work toward constructing realistic, scalable,\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\navailable at: https://fresh-stack.github.io.",
      "tldr_zh": "该研究引入了 FreshStack，一个自动构建信息检索 (IR) 评估基准的框架，通过从代码和技术文档收集语料库、从社区问题和答案生成关键信息 (nugget)，以及使用融合检索技术和混合架构进行文档检索。FreshStack 用于创建五个数据集，聚焦于快速增长的近期和利基主题，确保任务具有挑战性。实验结果显示，现有的检索模型在这些数据集上显著落后于理想方法，且 reranker 在两个主题上未能改善第一阶段检索准确性。该框架有望促进未来构建现实、可扩展且无污染的 IR 和 RAG 评估基准，并已公开数据集链接。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13128v1",
      "published_date": "2025-04-17 17:44:06 UTC",
      "updated_date": "2025-04-17 17:44:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:00:44.790937"
    },
    {
      "arxiv_id": "2504.13125v1",
      "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard",
      "title_zh": "翻译失败",
      "authors": [
        "Varun Rao",
        "Youran Sun",
        "Mahendra Kumar",
        "Tejas Mutneja",
        "Agastya Mukherjee",
        "Haizhao Yang"
      ],
      "abstract": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在金融任务中的应用，通过对 Qwen2.5 和 Deepseek-R1 等基础模型进行微调，以 Open FinLLM Leaderboard 为基准。研究采用了监督微调 (SFT)、直接偏好优化 (DPO) 和强化学习 (RL) 等技术，显著提升了模型在各种金融任务上的性能。论文还测量了金融领域的數據规模定律 (data scaling law)，证明了 LLMs 在金融应用中的巨大潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13125v1",
      "published_date": "2025-04-17 17:42:02 UTC",
      "updated_date": "2025-04-17 17:42:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:00:57.021722"
    },
    {
      "arxiv_id": "2504.13123v2",
      "title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training",
      "title_zh": "翻译失败",
      "authors": [
        "Xinsong Zhang",
        "Yarong Zeng",
        "Xinting Huang",
        "Hu Hu",
        "Runquan Xie",
        "Han Hu",
        "Zhanhui Kang"
      ],
      "abstract": "In recent years, the field of vision-language model pre-training has\nexperienced rapid advancements, driven primarily by the continuous enhancement\nof textual capabilities in large language models. However, existing training\nparadigms for multimodal large language models heavily rely on high-quality\nimage-text pairs. As models and data scales grow exponentially, the\navailability of such meticulously curated data has become increasingly scarce\nand saturated, thereby severely limiting further advancements in this domain.\nThis study investigates scalable caption generation techniques for\nvision-language model pre-training and demonstrates that large-scale\nlow-hallucination synthetic captions can serve dual purposes: 1) acting as a\nviable alternative to real-world data for pre-training paradigms and 2)\nachieving superior performance enhancement when integrated into vision-language\nmodels through empirical validation. This paper presents following key\ncontributions: 1) a novel pipeline for generating high-quality,\nlow-hallucination, and knowledge-rich synthetic captions. Our continuous DPO\nmethodology yields remarkable results in reducing hallucinations. Specifically,\nthe non-hallucination caption rate on a held-out test set increases from 48.3%\nto 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals\nthat our synthetic captions confer superior pre-training advantages over their\ncounterparts. Across 15 vision language tasks, the model trained with our data\nachieves a significant performance gain of at least 6.2% compared to identical\nimages with alt-text. In 20 common cognitive domains, the model trained with\nour data outperforms the alt-text data by at least 7.5%. Meanwhile, it also\noffers considerable support in the text-to-image domain. With our dataset, the\nFID score is reduced by 17.1 on a real-world validation benchmark and 13.3 on\nthe MSCOCO validation benchmark.",
      "tldr_zh": "本研究针对视觉语言模型预训练中高质量图像-文本对稀缺的问题，提出了一种可扩展的低-hallucination synthetic captions 生成管道，使用连续 DPO 方法显著减少幻觉，将非-hallucination 标题率从 48.3% 提高到 77.9%。这项管道生成的高质量、知识丰富的合成标题可作为真实数据的替代品，并在模型预训练中提供优势。实证验证显示，与 alt-text 相比，使用合成标题的模型在 15 个视觉语言任务上至少提升 6.2%，在 20 个认知领域至少提升 7.5%，并在文本到图像领域将 FID score 分别降低 17.1 和 13.3。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13123v2",
      "published_date": "2025-04-17 17:40:06 UTC",
      "updated_date": "2025-05-17 06:50:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:01:09.397102"
    },
    {
      "arxiv_id": "2504.13241v2",
      "title": "Recursive Deep Inverse Reinforcement Learning",
      "title_zh": "递归深度逆强化学习",
      "authors": [
        "Paul Ghanem",
        "Michael Potter",
        "Owen Howell",
        "Pau Closas",
        "Alireza Ramezani",
        "Deniz Erdogmus",
        "Tales Imbiriba"
      ],
      "abstract": "Inferring an adversary's goals from exhibited behavior is crucial for\ncounterplanning and non-cooperative multi-agent systems in domains like\ncybersecurity, military, and strategy games. Deep Inverse Reinforcement\nLearning (IRL) methods based on maximum entropy principles show promise in\nrecovering adversaries' goals but are typically offline, require large batch\nsizes with gradient descent, and rely on first-order updates, limiting their\napplicability in real-time scenarios. We propose an online Recursive Deep\nInverse Reinforcement Learning (RDIRL) approach to recover the cost function\ngoverning the adversary actions and goals. Specifically, we minimize an upper\nbound on the standard Guided Cost Learning (GCL) objective using sequential\nsecond-order Newton updates, akin to the Extended Kalman Filter (EKF), leading\nto a fast (in terms of convergence) learning algorithm. We demonstrate that\nRDIRL is able to recover cost and reward functions of expert agents in standard\nand adversarial benchmark tasks. Experiments on benchmark tasks show that our\nproposed approach outperforms several leading IRL algorithms.",
      "tldr_zh": "该研究针对逆强化学习（IRL）在推断对手目标行为方面的应用，解决了现有深度IRL方法（如基于最大熵原则的算法）在实时场景中的局限性，包括离线处理、大批量数据需求和一阶更新。作者提出了一种在线的Recursive Deep Inverse Reinforcement Learning (RDIRL)方法，通过最小化Guided Cost Learning (GCL)目标的上界，并采用顺序二阶Newton更新（类似于Extended Kalman Filter (EKF)），实现更快的收敛。实验结果显示，RDIRL在标准和对抗性基准任务中成功恢复了专家代理的成本和奖励函数，并优于其他领先IRL算法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13241v2",
      "published_date": "2025-04-17 17:39:35 UTC",
      "updated_date": "2025-04-21 03:47:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:01:20.702065"
    },
    {
      "arxiv_id": "2504.13120v2",
      "title": "Probing and Inducing Combinational Creativity in Vision-Language Models",
      "title_zh": "在视觉语言模型中探究和诱导组合式创造力",
      "authors": [
        "Yongqian Peng",
        "Yuxi Ma",
        "Mengmeng Wang",
        "Yuxuan Wang",
        "Yizhou Wang",
        "Chi Zhang",
        "Yixin Zhu",
        "Zilong Zheng"
      ],
      "abstract": "The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs' outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.",
      "tldr_zh": "该研究探讨了视觉语言模型（VLMs）如 GPT-4V 和 DALLE-3 是否具备组合创造力（combinational creativity），即通过概念混合（concept blending）将现有概念合成新想法。作者提出 Identification-Explanation-Implication (IEI) 框架，将创造过程分解为识别输入空间、提取共享属性和推导新语义含义，并构建了 CreativeMashup 数据集（包含666个艺术家生成的视觉混搭）用于验证。实验结果显示，在理解任务中，最佳 VLMs 已超过平均人类表现但未达专家水平，而在生成任务中，融入 IEI 框架显著提升了输出创造质量，为评估和改进人工智能创造力提供了理论基础和实用指南。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://ppyyqq.github.io/aicc/ The first two authors\n  contribute equally",
      "pdf_url": "http://arxiv.org/pdf/2504.13120v2",
      "published_date": "2025-04-17 17:38:18 UTC",
      "updated_date": "2025-04-29 14:51:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:01:33.868740"
    },
    {
      "arxiv_id": "2504.13102v1",
      "title": "A Multi-task Learning Balanced Attention Convolutional Neural Network Model for Few-shot Underwater Acoustic Target Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Huang",
        "Shumeng Sun",
        "Junpeng Lu",
        "Zhenpeng Xu",
        "Zhengyang Xiu",
        "Hao Zhang"
      ],
      "abstract": "Underwater acoustic target recognition (UATR) is of great significance for\nthe protection of marine diversity and national defense security. The\ndevelopment of deep learning provides new opportunities for UATR, but faces\nchallenges brought by the scarcity of reference samples and complex\nenvironmental interference. To address these issues, we proposes a multi-task\nbalanced channel attention convolutional neural network (MT-BCA-CNN). The\nmethod integrates a channel attention mechanism with a multi-task learning\nstrategy, constructing a shared feature extractor and multi-task classifiers to\njointly optimize target classification and feature reconstruction tasks. The\nchannel attention mechanism dynamically enhances discriminative acoustic\nfeatures such as harmonic structures while suppressing noise. Experiments on\nthe Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\\%\nclassification accuracy and 95\\% $F1$-score in 27-class few-shot scenarios,\nsignificantly outperforming traditional CNN and ACNN models, as well as popular\nstate-of-the-art UATR methods. Ablation studies confirm the synergistic\nbenefits of multi-task learning and attention mechanisms, while a dynamic\nweighting adjustment strategy effectively balances task contributions. This\nwork provides an efficient solution for few-shot underwater acoustic\nrecognition, advancing research in marine bioacoustics and sonar signal\nprocessing.",
      "tldr_zh": "该研究针对水下声学目标识别(UATR)的样本稀缺和复杂环境干扰挑战，提出了一种多任务学习平衡注意力卷积神经网络(MT-BCA-CNN)模型。该模型整合通道注意力机制和多任务学习策略，通过共享特征提取器同时优化目标分类和特征重建任务，动态增强谐波结构等判别性声学特征并抑制噪声。在Watkins Marine Life Dataset的27类few-shot场景中，MT-BCA-CNN实现了97%的分类准确率和95%的F1分数，显著优于传统CNN、ACNN及其他先进方法。消融研究和动态权重调整策略进一步证实了多任务学习与注意力机制的协同益处，为few-shot水下声学识别提供高效解决方案，推动海洋生物声学和声纳信号处理研究。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13102v1",
      "published_date": "2025-04-17 17:11:32 UTC",
      "updated_date": "2025-04-17 17:11:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:01:46.611352"
    },
    {
      "arxiv_id": "2504.13101v1",
      "title": "An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research",
      "title_zh": "基于经验的可识别性理论将加速自监督学习研究",
      "authors": [
        "Patrik Reizinger",
        "Randall Balestriero",
        "David Klindt",
        "Wieland Brendel"
      ],
      "abstract": "Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.",
      "tldr_zh": "本文提出，通过Identifiability Theory (IT)合成证据，证明Self-Supervised Learning (SSL)中的Platonic Representation Hypothesis (PRH)可能出现，但当前IT无法充分解释SSL的经验成功。论文建议扩展IT为Singular Identifiability Theory (SITh)，一个更全面的框架，涵盖SSL整个管道，以提供更深入的见解，帮助实现更可解释和泛化的表示。未来研究应聚焦三个关键方向：SSL的训练动态和收敛属性、有限样本与数据多样性的影响，以及架构和优化器的归纳偏差作用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13101v1",
      "published_date": "2025-04-17 17:10:33 UTC",
      "updated_date": "2025-04-17 17:10:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:01:57.301871"
    },
    {
      "arxiv_id": "2504.13079v1",
      "title": "Retrieval-Augmented Generation with Conflicting Evidence",
      "title_zh": "翻译失败",
      "authors": [
        "Han Wang",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
      "tldr_zh": "这篇论文探讨了Retrieval-Augmented Generation (RAG) 在处理模糊查询、冲突证据和噪声信息时的挑战，强调现有方法通常单独应对这些问题。作者提出RAMDocs数据集，用于模拟现实场景，包括模糊性、错误信息和噪声，以及MADAM-RAG多代理方法，该方法通过代理的多轮辩论和聚合器整合响应，来同时处理这些冲突因素。实验结果显示，MADAM-RAG在AmbigDocs基准上比基线提升了11.40%，在FaithEval基准上提升了15.80%（使用Llama3.3-70B-Instruct），尽管在证据不平衡场景下仍存在改进空间。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Our data and code is available at:\n  https://github.com/HanNight/RAMDocs",
      "pdf_url": "http://arxiv.org/pdf/2504.13079v1",
      "published_date": "2025-04-17 16:46:11 UTC",
      "updated_date": "2025-04-17 16:46:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:02:10.364187"
    },
    {
      "arxiv_id": "2504.13078v1",
      "title": "Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual Try-Off",
      "title_zh": "翻译失败",
      "authors": [
        "Riza Velioglu",
        "Petra Bevandic",
        "Robin Chan",
        "Barbara Hammer"
      ],
      "abstract": "Computer vision is transforming fashion through Virtual Try-On (VTON) and\nVirtual Try-Off (VTOFF). VTON generates images of a person in a specified\ngarment using a target photo and a standardized garment image, while a more\nchallenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo\nof another person wearing the garment. VTOFF, on the other hand, extracts\nstandardized garment images from clothed individuals. We introduce TryOffDiff,\na diffusion-based VTOFF model. Built on a latent diffusion framework with\nSigLIP image conditioning, it effectively captures garment properties like\ntexture, shape, and patterns. TryOffDiff achieves state-of-the-art results on\nVITON-HD and strong performance on DressCode dataset, covering upper-body,\nlower-body, and dresses. Enhanced with class-specific embeddings, it pioneers\nmulti-garment VTOFF, the first of its kind. When paired with VTON models, it\nimproves p2p-VTON by minimizing unwanted attribute transfer, such as skin\ncolor. Code is available at: https://rizavelioglu.github.io/tryoffdiff/",
      "tldr_zh": "本文提出 TryOffDiff，一种基于扩散模型的 Virtual Try-Off (VTOFF) 系统，利用潜在扩散框架和 SigLIP 图像条件，精确捕获服装的纹理、形状和图案，支持多服装提取，包括上身、下身和连衣裙。TryOffDiff 在 VITON-HD 和 DressCode 数据集上实现最先进性能，并通过类特定嵌入实现首创的多服装 VTOFF 功能。当与 Virtual Try-On (VTON) 模型结合时，它显著提升 Person-to-Person Virtual Try-On (p2p-VTON)，减少不想要的属性转移，如皮肤颜色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13078v1",
      "published_date": "2025-04-17 16:45:18 UTC",
      "updated_date": "2025-04-17 16:45:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:02:22.951116"
    },
    {
      "arxiv_id": "2504.13068v2",
      "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sudesh Ramesh Bhagat",
        "Ibne Farabi Shihab",
        "Anuj Sharma"
      ],
      "abstract": "This study investigates the relationship between deep learning (DL) model\naccuracy and expert agreement in classifying crash narratives. We evaluate five\nDL models -- including BERT variants, USE, and a zero-shot classifier --\nagainst expert labels and narratives, and extend the analysis to four large\nlanguage models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our findings reveal\nan inverse relationship: models with higher technical accuracy often show lower\nagreement with human experts, while LLMs demonstrate stronger expert alignment\ndespite lower accuracy. We use Cohen's Kappa and Principal Component Analysis\n(PCA) to quantify and visualize model-expert agreement, and employ SHAP\nanalysis to explain misclassifications. Results show that expert-aligned models\nrely more on contextual and temporal cues than location-specific keywords.\nThese findings suggest that accuracy alone is insufficient for safety-critical\nNLP tasks. We argue for incorporating expert agreement into model evaluation\nframeworks and highlight the potential of LLMs as interpretable tools in crash\nanalysis pipelines.",
      "tldr_zh": "这篇论文探讨了深度学习(DL)模型在分类崩溃叙述时的准确性与专家一致性的关系，发现高准确性模型往往与专家意见一致性较低，而大型语言模型(LLMs)如 GPT-4 和 LLaMA 3 尽管准确性较低，却显示出更高的专家对齐度。研究评估了五种 DL 模型（包括 BERT 变体、USE 和零-shot 分类器）以及四种 LLMs，使用 Cohen's Kappa、Principal Component Analysis (PCA) 和 SHAP 分析来量化并可视化模型与专家的契合度。结果表明，专家对齐的模型更依赖上下文和时间线索而非位置关键词，并建议在安全关键的 NLP 任务中，将专家一致性纳入模型评估框架，以提升崩溃分析的可解释性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13068v2",
      "published_date": "2025-04-17 16:29:08 UTC",
      "updated_date": "2025-05-01 23:02:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:02:35.104132"
    },
    {
      "arxiv_id": "2504.13059v1",
      "title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins",
      "title_zh": "RoboTwin: 基于生成式数字孪生的双臂机器人基准",
      "authors": [
        "Yao Mu",
        "Tianxing Chen",
        "Zanxin Chen",
        "Shijia Peng",
        "Zhiqian Lan",
        "Zeyu Gao",
        "Zhixuan Liang",
        "Qiaojun Yu",
        "Yude Zou",
        "Mingkun Xu",
        "Lunkai Lin",
        "Zhiqiang Xie",
        "Mingyu Ding",
        "Ping Luo"
      ],
      "abstract": "In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples demonstrate significant potential for enhancing dual-arm robotic\nmanipulation systems by improving success rates by over 70% for single-arm\ntasks and over 40% for dual-arm tasks compared to models trained solely on\nreal-world data.",
      "tldr_zh": "该研究引入了RoboTwin，一种基于3D生成基础模型和大型语言模型的生成性数字孪生框架，旨在解决双臂机器人协调和复杂物体操作中数据稀缺与评估基准不足的问题。RoboTwin从单个2D图像创建多样化的物体数字孪生，并通过空间关系感知代码生成框架结合物体注释和大型语言模型来分解任务、确定空间约束并生成精确机器人运动代码，从而提供模拟与真实世界数据相结合的标准化基准。实验验证显示，使用开源COBOT Magic Robot平台预训练并微调的政策，能将单臂任务成功率提高超过70%，双臂任务提高超过40%，显著优于仅基于真实数据训练的模型。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.RO",
      "comment": "CVPR 2025 Highlight. 22 pages. Project page:\n  https://robotwin-benchmark.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2504.13059v1",
      "published_date": "2025-04-17 16:14:24 UTC",
      "updated_date": "2025-04-17 16:14:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:02:44.976099"
    },
    {
      "arxiv_id": "2504.13054v1",
      "title": "Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yichao Feng",
        "Shuai Zhao",
        "Yueqiu Li",
        "Luwei Xiao",
        "Xiaobao Wu",
        "Anh Tuan Luu"
      ],
      "abstract": "Aspect-based summarization aims to generate summaries tailored to specific\naspects, addressing the resource constraints and limited generalizability of\ntraditional summarization approaches. Recently, large language models have\nshown promise in this task without the need for training. However, they rely\nexcessively on prompt engineering and face token limits and hallucination\nchallenges, especially with in-context learning. To address these challenges,\nin this paper, we propose a novel framework for aspect-based summarization:\nSelf-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely\non in-context learning, given an aspect, we employ an embedding-driven\nretrieval mechanism to identify its relevant text segments. This approach\nextracts the pertinent content while avoiding unnecessary details, thereby\nmitigating the challenge of token limits. Moreover, our framework optimizes\ntoken usage by deleting unrelated parts of the text and ensuring that the model\ngenerates output strictly based on the given aspect. With extensive experiments\non benchmark datasets, we demonstrate that our framework not only achieves\nsuperior performance but also effectively mitigates the token limitation\nproblem.",
      "tldr_zh": "本文提出了一种名为Self-Aspect Retrieval Enhanced Generation的框架，用于Aspect-based summarization，以解决传统方法在资源限制和泛化方面的挑战。框架通过嵌入驱动检索机制（embedding-driven retrieval mechanism）自动识别给定方面的相关文本段落，优化令牌使用（token limits），并减少幻觉问题（hallucination challenges），避免依赖单纯的in-context learning。实验结果显示，该框架在基准数据集上显著提升了摘要生成性能，并有效缓解了令牌限制问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13054v1",
      "published_date": "2025-04-17 16:09:57 UTC",
      "updated_date": "2025-04-17 16:09:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:02:57.760754"
    },
    {
      "arxiv_id": "2504.13048v1",
      "title": "Design Topological Materials by Reinforcement Fine-Tuned Generative Model",
      "title_zh": "翻译失败",
      "authors": [
        "Haosheng Xu",
        "Dongheng Qian",
        "Zhixuan Liu",
        "Yadong Jiang",
        "Jing Wang"
      ],
      "abstract": "Topological insulators (TIs) and topological crystalline insulators (TCIs)\nare materials with unconventional electronic properties, making their discovery\nhighly valuable for practical applications. However, such materials,\nparticularly those with a full band gap, remain scarce. Given the limitations\nof traditional approaches that scan known materials for candidates, we focus on\nthe generation of new topological materials through a generative model.\nSpecifically, we apply reinforcement fine-tuning (ReFT) to a pre-trained\ngenerative model, thereby aligning the model's objectives with our material\ndesign goals. We demonstrate that ReFT is effective in enhancing the model's\nability to generate TIs and TCIs, with minimal compromise on the stability of\nthe generated materials. Using the fine-tuned model, we successfully identify a\nlarge number of new topological materials, with Ge$_2$Bi$_2$O$_6$ serving as a\nrepresentative example--a TI with a full band gap of 0.26 eV, ranking among the\nlargest known in this category.",
      "tldr_zh": "本研究针对拓扑绝缘体(TIs)和拓扑晶体绝缘体(TCIs)的稀缺问题，提出了一种基于Reinforcement Fine-Tuning (ReFT)微调生成模型的方法，以生成具有全能隙的新拓扑材料。ReFT通过将模型目标与材料设计目标对齐，显著提升了生成TIs和TCIs的能力，同时保持了材料的稳定性。实验结果显示，该方法成功识别了大量新拓扑材料，其中Ge$_2$Bi$_2$O$_6$作为代表性例子，具有0.26 eV的全能隙，是已知最大之一。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13048v1",
      "published_date": "2025-04-17 16:05:24 UTC",
      "updated_date": "2025-04-17 16:05:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:03:10.188808"
    },
    {
      "arxiv_id": "2504.13042v2",
      "title": "Event-Enhanced Blurry Video Super-Resolution",
      "title_zh": "翻译失败",
      "authors": [
        "Dachun Kai",
        "Yueyi Zhang",
        "Jin Wang",
        "Zeyu Xiao",
        "Zhiwei Xiong",
        "Xiaoyan Sun"
      ],
      "abstract": "In this paper, we tackle the task of blurry video super-resolution (BVSR),\naiming to generate high-resolution (HR) videos from low-resolution (LR) and\nblurry inputs. Current BVSR methods often fail to restore sharp details at high\nresolutions, resulting in noticeable artifacts and jitter due to insufficient\nmotion information for deconvolution and the lack of high-frequency details in\nLR frames. To address these challenges, we introduce event signals into BVSR\nand propose a novel event-enhanced network, Ev-DeblurVSR. To effectively fuse\ninformation from frames and events for feature deblurring, we introduce a\nreciprocal feature deblurring module that leverages motion information from\nintra-frame events to deblur frame features while reciprocally using global\nscene context from the frames to enhance event features. Furthermore, to\nenhance temporal consistency, we propose a hybrid deformable alignment module\nthat fully exploits the complementary motion information from inter-frame\nevents and optical flow to improve motion estimation in the deformable\nalignment process. Extensive evaluations demonstrate that Ev-DeblurVSR\nestablishes a new state-of-the-art performance on both synthetic and real-world\ndatasets. Notably, on real data, our method is +2.59 dB more accurate and\n7.28$\\times$ faster than the recent best BVSR baseline FMA-Net. Code:\nhttps://github.com/DachunKai/Ev-DeblurVSR.",
      "tldr_zh": "本论文针对模糊视频超分辨率（BVSR）任务，旨在从低分辨率（LR）和模糊输入生成高分辨率（HR）视频，但现有方法因运动信息不足和高频细节缺失而导致细节恢复不佳和伪影问题。作者引入事件信号（event signals），提出新型网络Ev-DeblurVSR，包括互惠特征去模糊模块（reciprocal feature deblurring module），该模块利用帧内事件运动信息去模糊帧特征，并反向增强事件特征；以及混合可变形对齐模块（hybrid deformable alignment module），通过帧间事件和光流的光学信息改善运动估计以提升时间一致性。实验结果显示，Ev-DeblurVSR在合成和真实数据集上达到新的最先进性能，在真实数据上比最佳基线FMA-Net准确率提高2.59 dB，且速度快7.28倍。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2025. Project page:\n  https://dachunkai.github.io/ev-deblurvsr.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2504.13042v2",
      "published_date": "2025-04-17 15:55:41 UTC",
      "updated_date": "2025-04-18 02:49:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:03:22.181200"
    },
    {
      "arxiv_id": "2504.13037v2",
      "title": "Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond",
      "title_zh": "翻译失败",
      "authors": [
        "Yundi Zhang",
        "Paul Hager",
        "Che Liu",
        "Suprosanna Shit",
        "Chen Chen",
        "Daniel Rueckert",
        "Jiazhen Pan"
      ],
      "abstract": "Cardiac magnetic resonance imaging is the gold standard for non-invasive\ncardiac assessment, offering rich spatio-temporal views of the cardiac anatomy\nand physiology. Patient-level health factors, such as demographics, metabolic,\nand lifestyle, are known to substantially influence cardiovascular health and\ndisease risk, yet remain uncaptured by CMR alone. To holistically understand\ncardiac health and to enable the best possible interpretation of an\nindividual's disease risk, CMR and patient-level factors must be jointly\nexploited within an integrated framework. Recent multi-modal approaches have\nbegun to bridge this gap, yet they often rely on limited spatio-temporal data\nand focus on isolated clinical tasks, thereby hindering the development of a\ncomprehensive representation for cardiac health evaluation. To overcome these\nlimitations, we introduce ViTa, a step toward foundation models that delivers a\ncomprehensive representation of the heart and a precise interpretation of\nindividual disease risk. Leveraging data from 42,000 UK Biobank participants,\nViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling\na complete capture of the cardiac cycle. These imaging data are then fused with\ndetailed tabular patient-level factors, enabling context-aware insights. This\nmulti-modal paradigm supports a wide spectrum of downstream tasks, including\ncardiac phenotype and physiological feature prediction, segmentation, and\nclassification of cardiac and metabolic diseases within a single unified\nframework. By learning a shared latent representation that bridges rich imaging\nfeatures and patient context, ViTa moves beyond traditional, task-specific\nmodels toward a universal, patient-specific understanding of cardiac health,\nhighlighting its potential to advance clinical utility and scalability in\ncardiac analysis.",
      "tldr_zh": "该研究提出ViTa框架，作为心脏MRI基础模型的初步步骤，旨在整合视觉数据（如3D+T cine stacks）和患者级别表格数据（如人口统计、代谢和生活方式因素），以实现对心脏健康的全面评估和个体疾病风险的精确解读。利用来自42,000名UK Biobank参与者的数据，ViTa融合短轴和长轴视图的影像信息，提供一个共享的潜在表示，支持多种下游任务，包括心脏表型预测、生理特征分割以及心脏和代谢疾病分类。相比传统任务特定模型，ViTa通过多模态范式提升了临床效用和可扩展性，推动了从孤立分析向通用患者特定心脏健康理解的转变。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13037v2",
      "published_date": "2025-04-17 15:46:19 UTC",
      "updated_date": "2025-04-18 09:26:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:03:33.530109"
    },
    {
      "arxiv_id": "2504.13035v1",
      "title": "Prototypes are Balanced Units for Efficient and Effective Partially Relevant Video Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "WonJun Moon",
        "Cheol-Ho Cho",
        "Woojin Jun",
        "Minho Shim",
        "Taeoh Kim",
        "Inwoong Lee",
        "Dongyoon Wee",
        "Jae-Pil Heo"
      ],
      "abstract": "In a retrieval system, simultaneously achieving search accuracy and\nefficiency is inherently challenging. This challenge is particularly pronounced\nin partially relevant video retrieval (PRVR), where incorporating more diverse\ncontext representations at varying temporal scales for each video enhances\naccuracy but increases computational and memory costs. To address this\ndichotomy, we propose a prototypical PRVR framework that encodes diverse\ncontexts within a video into a fixed number of prototypes. We then introduce\nseveral strategies to enhance text association and video understanding within\nthe prototypes, along with an orthogonal objective to ensure that the\nprototypes capture a diverse range of content. To keep the prototypes\nsearchable via text queries while accurately encoding video contexts, we\nimplement cross- and uni-modal reconstruction tasks. The cross-modal\nreconstruction task aligns the prototypes with textual features within a shared\nspace, while the uni-modal reconstruction task preserves all video contexts\nduring encoding. Additionally, we employ a video mixing technique to provide\nweak guidance to further align prototypes and associated textual\nrepresentations. Extensive evaluations on TVR, ActivityNet-Captions, and\nQVHighlights validate the effectiveness of our approach without sacrificing\nefficiency.",
      "tldr_zh": "该论文提出了一种原型化的框架，用于高效有效的部分相关视频检索（PRVR），通过将视频中的多样上下文编码成固定数量的原型，来平衡搜索准确性和计算效率。框架引入了多种策略，包括跨模态和单模态重建任务，以增强原型与文本特征的关联，并确保原型捕捉视频内容的多样性；此外，还采用了视频混合技术来进一步对齐原型和文本表示。实验在TVR、ActivityNet-Captions和QVHighlights数据集上验证了该方法的有效性，同时未牺牲效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13035v1",
      "published_date": "2025-04-17 15:43:29 UTC",
      "updated_date": "2025-04-17 15:43:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:03:45.925954"
    },
    {
      "arxiv_id": "2504.13032v1",
      "title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Zheng Wang",
        "Shu Xian Teo",
        "Jun Jie Chew",
        "Wei Shi"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have enabled their use as\nagents for planning complex tasks. Existing methods typically rely on a\nthought-action-observation (TAO) process to enhance LLM performance, but these\napproaches are often constrained by the LLMs' limited knowledge of complex\ntasks. Retrieval-augmented generation (RAG) offers new opportunities by\nleveraging external databases to ground generation in retrieved information. In\nthis paper, we identify two key challenges (enlargability and transferability)\nin applying RAG to task planning. We propose InstructRAG, a novel solution\nwithin a multi-agent meta-reinforcement learning framework, to address these\nchallenges. InstructRAG includes a graph to organize past instruction paths\n(sequences of correct actions), an RL-Agent with Reinforcement Learning to\nexpand graph coverage for enlargability, and an ML-Agent with Meta-Learning to\nimprove task generalization for transferability. The two agents are trained\nend-to-end to optimize overall planning performance. Our experiments on four\nwidely used task planning datasets demonstrate that InstructRAG significantly\nenhances performance and adapts efficiently to new tasks, achieving up to a\n19.2% improvement over the best existing approach.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)用于复杂任务规划时存在的知识限制问题，提出InstructRAG框架，该框架基于Retrieval-Augmented Generation (RAG)并结合指令图组织过去的正确行动序列，以解决enlargability和transferability挑战。InstructRAG采用多智能体元强化学习(meta-reinforcement learning)方法，包括RL-Agent使用Reinforcement Learning扩展图的覆盖范围，以及ML-Agent使用Meta-Learning提升任务泛化能力，两者端到端训练以优化整体性能。在四个常用任务规划数据集上的实验显示，InstructRAG比最佳现有方法提升多达19.2%，显著提高了任务适应性和效率。",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by SIGIR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.13032v1",
      "published_date": "2025-04-17 15:41:39 UTC",
      "updated_date": "2025-04-17 15:41:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:03:57.375528"
    },
    {
      "arxiv_id": "2504.13234v1",
      "title": "Non-Uniform Class-Wise Coreset Selection: Characterizing Category Difficulty for Data-Efficient Transfer Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hanyu Zhang",
        "Zhen Xing",
        "Wenxuan Yang",
        "Chenxi Ma",
        "Weimin Tan",
        "Bo Yan"
      ],
      "abstract": "As transfer learning models and datasets grow larger, efficient adaptation\nand storage optimization have become critical needs. Coreset selection\naddresses these challenges by identifying and retaining the most informative\nsamples, constructing a compact subset for target domain training. However,\ncurrent methods primarily rely on instance-level difficulty assessments,\noverlooking crucial category-level characteristics and consequently\nunder-representing minority classes. To overcome this limitation, we propose\nNon-Uniform Class-Wise Coreset Selection (NUCS), a novel framework that\nintegrates both class-level and instance-level criteria. NUCS automatically\nallocates data selection budgets for each class based on intrinsic category\ndifficulty and adaptively selects samples within optimal difficulty ranges. By\nexplicitly incorporating category-specific insights, our approach achieves a\nmore balanced and representative coreset, addressing key shortcomings of prior\nmethods. Comprehensive theoretical analysis validates the rationale behind\nadaptive budget allocation and sample selection, while extensive experiments\nacross 14 diverse datasets and model architectures demonstrate NUCS's\nconsistent improvements over state-of-the-art methods, achieving superior\naccuracy and computational efficiency. Notably, on CIFAR100 and Food101, NUCS\nmatches full-data training accuracy while retaining just 30% of samples and\nreducing computation time by 60%. Our work highlights the importance of\ncharacterizing category difficulty in coreset selection, offering a robust and\ndata-efficient solution for transfer learning.",
      "tldr_zh": "该论文提出 Non-Uniform Class-Wise Coreset Selection (NUCS)，一种新框架，用于数据高效的转移学习，通过整合类别级和实例级标准来表征类别难度，避免现有方法忽略少数类的问题。NUCS 自动基于内在类别难度分配数据选择预算，并在最佳难度范围内自适应选择样本，从而构建更平衡和代表性的 coreset 子集。实验在 14 个数据集和多种模型架构上验证了其优势，NUCS 比现有方法显著提高准确率和计算效率，例如在 CIFAR100 和 Food101 上，仅使用 30% 的样本就达到全数据训练的准确率，并减少 60% 的计算时间。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11pages",
      "pdf_url": "http://arxiv.org/pdf/2504.13234v1",
      "published_date": "2025-04-17 15:40:51 UTC",
      "updated_date": "2025-04-17 15:40:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:04:10.972008"
    },
    {
      "arxiv_id": "2504.13021v1",
      "title": "Pose and Facial Expression Transfer by using StyleGAN",
      "title_zh": "翻译失败",
      "authors": [
        "Petr Jahoda",
        "Jan Cech"
      ],
      "abstract": "We propose a method to transfer pose and expression between face images.\nGiven a source and target face portrait, the model produces an output image in\nwhich the pose and expression of the source face image are transferred onto the\ntarget identity. The architecture consists of two encoders and a mapping\nnetwork that projects the two inputs into the latent space of StyleGAN2, which\nfinally generates the output. The training is self-supervised from video\nsequences of many individuals. Manual labeling is not required. Our model\nenables the synthesis of random identities with controllable pose and\nexpression. Close-to-real-time performance is achieved.",
      "tldr_zh": "本研究提出了一种基于 StyleGAN2 的方法，用于在面部图像之间转移姿势和表情。具体来说，该方法使用两个编码器和一个映射网络，将源图像的姿势和表情投影到目标身份的潜在空间中生成输出图像，并通过自监督训练从多个个体的视频序列中学习，无需手动标注。实验结果显示，该模型支持合成随机身份的图像，并实现可控姿势和表情的实时性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVWW 2024. Presented in Terme Olimia, Slovenia",
      "pdf_url": "http://arxiv.org/pdf/2504.13021v1",
      "published_date": "2025-04-17 15:29:41 UTC",
      "updated_date": "2025-04-17 15:29:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:04:20.585898"
    },
    {
      "arxiv_id": "2504.12996v1",
      "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation",
      "title_zh": "翻译失败",
      "authors": [
        "Saransh Agrawal",
        "Kuan-Hao Huang"
      ],
      "abstract": "Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems.",
      "tldr_zh": "本论文针对大型语言模型 (LLMs) 在训练中记忆敏感信息的问题，提出了一种选择性遗忘方法“Selective Amnesia”，通过两阶段方法结合因果中介分析和层特定优化，实现对特定数据关联的精确移除，同时最小化对整体模型能力的损害。在实验中，作者对 OLMo 架构 (1B 和 7B 参数) 进行因果追踪，发现前几层 (layers 0-5) 的 MLP 模块是存储主题-属性关联的关键，并开发了约束优化策略，包括冻结上层并对下层应用联合损失函数（输出 token 交叉熵惩罚和自适应正则化）。结果显示，该方法在 SemEval-2025 Task 4 的 1B 模型轨道中获得第二名，保持了 88% 的基线 MMLU 准确率，为高效的 LLM 遗忘范式提供了新途径，显著提升了 AI 系统的数据隐私保护。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, In Proceedings of The 19th International Workshop on\n  Semantic Evaluation (SemEval), 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12996v1",
      "published_date": "2025-04-17 15:05:40 UTC",
      "updated_date": "2025-04-17 15:05:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:04:35.629347"
    },
    {
      "arxiv_id": "2504.12984v2",
      "title": "Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving",
      "title_zh": "翻译失败",
      "authors": [
        "Yaoyao Ding",
        "Bohan Hou",
        "Xiao Zhang",
        "Allan Lin",
        "Tianqi Chen",
        "Cody Yu Hao",
        "Yida Wang",
        "Gennady Pekhimenko"
      ],
      "abstract": "Serving Large Language Models (LLMs) is critical for AI-powered applications\nbut demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance due to\nhigh-level GPU programming abstractions. These abstractions restrict critical\noptimizations, such as fine-grained register management and optimized memory\naccess patterns, which are essential for efficient low-precision computations.\nIn this paper, we introduce a virtual machine (VM) designed for General-Purpose\nGPU (GPGPU) computing, enabling support for low-precision data types with\narbitrary bit widths while maintaining GPU programmability. The proposed VM\nfeatures a thread-block-level programming model, a hierarchical memory space, a\nnovel algebraic layout system, and extensive support for diverse low-precision\ndata types. VM programs are compiled into highly efficient GPU programs with\nautomatic vectorization and instruction selection. Extensive experiments\ndemonstrate that our VM efficiently supports a full spectrum of low-precision\ndata types, and outperforms state-of-the-art low-precision kernels on their\nsupported types. Compared to existing compilers like Triton and Ladder, as well\nas hand-optimized kernels such as QuantLLM and Marlin, our VM achieves\nperformance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.",
      "tldr_zh": "该论文提出Tilus，一种专为LLM Serving设计的虚拟机(VM)，支持任意位宽的低精度数据类型，以优化GPGPU计算并缓解内存带宽和计算吞吐量的资源需求。Tilus采用线程块级编程模型、层次化内存空间和新型代数布局系统，并通过自动向量化(instruction selection)编译成高效GPU程序，以克服现有低精度内核的局限性，如仅支持2的幂次方位宽和优化不足。实验结果显示，Tilus在多种低精度数据类型上表现出色，比Triton、Ladder、QuantLLM和Marlin分别提升1.75x、2.61x、1.29x和1.03x性能，为LLM部署提供更高效的计算框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12984v2",
      "published_date": "2025-04-17 14:45:03 UTC",
      "updated_date": "2025-04-25 18:40:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:04:46.737115"
    },
    {
      "arxiv_id": "2504.13231v2",
      "title": "WildFireCan-MMD: A Multimodal Dataset for Classification of User-Generated Content During Wildfires in Canada",
      "title_zh": "WildFireCan-MMD：用于加拿大野",
      "authors": [
        "Braeden Sherritt",
        "Isar Nejadgholi",
        "Marzieh Amini"
      ],
      "abstract": "Rapid information access is vital during wildfires, yet traditional data\nsources are slow and costly. Social media offers real-time updates, but\nextracting relevant insights remains a challenge. We present WildFireCan-MMD, a\nnew multimodal dataset of X posts from recent Canadian wildfires, annotated\nacross twelve key themes. Evaluating both vision-language models and\ncustom-trained classifiers, we show that while zero-shot prompting offers quick\ndeployment, even simple trained models outperform them when labelled data is\navailable. Our best-performing transformer-based fine-tuned model reaches 83%\nf-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this\nmodel can be used to uncover trends during wildfires. Our findings highlight\nthe enduring importance of tailored datasets and task-specific training.\nImportantly, such datasets should be localized, as disaster response\nrequirements vary across regions and contexts.",
      "tldr_zh": "本研究介绍了WildFireCan-MMD，一种多模态数据集，包含加拿大野火期间的X帖子，并标注了12个关键主题，以帮助从用户生成内容中提取实时洞见。研究评估了视觉语言模型和自定义训练的分类器，发现零-shot prompting虽便于快速部署，但有标注数据时，简单训练模型的表现更优。微调的Transformer模型达到83%的F-score，比GPT-4高23%，并展示了其在揭示野火趋势的应用潜力。该工作强调了定制数据集和任务特定训练的重要性，特别是需要本地化以适应不同地区的灾害响应需求。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13231v2",
      "published_date": "2025-04-17 14:43:56 UTC",
      "updated_date": "2025-05-15 14:47:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:04:57.654462"
    },
    {
      "arxiv_id": "2504.12982v1",
      "title": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild",
      "title_zh": "翻译失败",
      "authors": [
        "Jiatai Wang",
        "Zhiwei Xu",
        "Di Jin",
        "Xuewen Yang",
        "Tao Li"
      ],
      "abstract": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines.",
      "tldr_zh": "该研究探讨了检索增强大型语言模型（LLMs）中知识冲突的问题，这些冲突源于内部记忆和外部信息之间的差异，可能导致响应生成（RG）不可靠。从信息论视角分析发现，当冲突信息与补充信息差异显著时，LLMs 能自信选择偏好，但差异模糊时会增加不确定性。为此，提出 Swin-VIB 框架，该框架整合 variational information bottleneck 模型，用于适应性增强检索信息并指导 LLMs 的偏好选择。实验在单选题、开放式问答和 retrieval augmented generation (RAG) 任务上验证了这一发现，并显示 Swin-VIB 至少将单选任务准确率提高了 7.54%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12982v1",
      "published_date": "2025-04-17 14:40:31 UTC",
      "updated_date": "2025-04-17 14:40:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:05:10.575171"
    },
    {
      "arxiv_id": "2504.12977v1",
      "title": "A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology",
      "title_zh": "一种基于海德",
      "authors": [
        "Maksim Vishnevskiy"
      ],
      "abstract": "This paper presents a novel research analytical IT system grounded in Martin\nHeidegger's Fundamental Ontology, distinguishing between beings (das Seiende)\nand Being (das Sein). The system employs two modally distinct, descriptively\ncomplete languages: a categorical language of beings for processing user inputs\nand an existential language of Being for internal analysis. These languages are\nbridged via a phenomenological reduction module, enabling the system to analyze\nuser queries (including questions, answers, and dialogues among IT\nspecialists), identify recursive and self-referential structures, and provide\nactionable insights in categorical terms. Unlike contemporary systems limited\nto categorical analysis, this approach leverages Heidegger's phenomenological\nexistential analysis to uncover deeper ontological patterns in query\nprocessing, aiding in resolving logical traps in complex interactions, such as\nmetaphor usage in IT contexts. The path to full realization involves\nformalizing the language of Being by a research team based on Heidegger's\nFundamental Ontology; given the existing completeness of the language of\nbeings, this reduces the system's computability to completeness, paving the way\nfor a universal query analysis tool. The paper presents the system's\narchitecture, operational principles, technical implementation, use\ncases--including a case based on real IT specialist dialogues--comparative\nevaluation with existing tools, and its advantages and limitations.",
      "tldr_zh": "这篇论文提出了一种基于Heidegger's Fundamental Ontology的创新IT系统，用于分析用户查询，通过区分beings (das Seiende) 和 Being (das Sein)，并采用categorical language of beings处理输入与existential language of Being进行内部分析。系统通过phenomenological reduction module桥接两种语言，实现对用户查询（如问题、答案和对话）的深入解析，识别递归和自指结构，并提供可操作的洞见，以解决复杂交互中的逻辑陷阱，例如IT上下文中的隐喻。相比现有仅限于范畴分析的工具，该方法利用Heidegger的现象学存在分析，揭示更深层本体模式；论文详细阐述了系统的架构、操作原则、技术实现、用例（如真实IT专家对话案例）、比较评估及其优势和限制。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "12 pages, no figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12977v1",
      "published_date": "2025-04-17 14:29:25 UTC",
      "updated_date": "2025-04-17 14:29:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:05:23.317163"
    },
    {
      "arxiv_id": "2504.12971v2",
      "title": "Transferrable Surrogates in Expressive Neural Architecture Search Spaces",
      "title_zh": "翻译失败",
      "authors": [
        "Shiwen Qin",
        "Gabriela Kadlecová",
        "Martin Pilát",
        "Shay B. Cohen",
        "Roman Neruda",
        "Elliot J. Crowley",
        "Jovita Lukasik",
        "Linus Ericsson"
      ],
      "abstract": "Neural architecture search (NAS) faces a challenge in balancing the\nexploration of expressive, broad search spaces that enable architectural\ninnovation with the need for efficient evaluation of architectures to\neffectively search such spaces. We investigate surrogate model training for\nimproving search in highly expressive NAS search spaces based on context-free\ngrammars. We show that i) surrogate models trained either using zero-cost-proxy\nmetrics and neural graph features (GRAF) or by fine-tuning an off-the-shelf LM\nhave high predictive power for the performance of architectures both within and\nacross datasets, ii) these surrogates can be used to filter out bad\narchitectures when searching on novel datasets, thereby significantly speeding\nup search and achieving better final performances, and iii) the surrogates can\nbe further used directly as the search objective for huge speed-ups.",
      "tldr_zh": "本文研究了在表达性强的神经架构搜索(NAS)空间中使用可转移的surrogate models，以平衡架构创新探索和高效评估的挑战。研究发现，通过zero-cost-proxy metrics和neural graph features (GRAF)训练surrogate models，或fine-tuning现成LM，这些模型能在不同数据集上高度预测架构性能，从而加速搜索过程。最终，surrogate models不仅能过滤不良架构并提升搜索效率，还可直接用作搜索目标，实现显著性能改进。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Project page at: https://shiwenqin.github.io/TransferrableSurrogate/",
      "pdf_url": "http://arxiv.org/pdf/2504.12971v2",
      "published_date": "2025-04-17 14:22:28 UTC",
      "updated_date": "2025-04-18 17:49:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:05:34.620118"
    },
    {
      "arxiv_id": "2504.12961v2",
      "title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?",
      "title_zh": "QLLM：我们真的需要混合网络来进行多智能体强化学习中的信用分配吗？",
      "authors": [
        "Zhouyang Jiang",
        "Bin Zhang",
        "Airong Wei",
        "Zhiwei Xu"
      ],
      "abstract": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.",
      "tldr_zh": "本论文质疑多智能体强化学习(MARL)中是否需要混合网络来处理信用分配问题，提出新算法QLLM，使用大型语言模型(LLMs)自动构建信用分配函数，以解决现有方法的归因不准、可解释性差和扩展性不足等问题。QLLM引入TFCAF概念，将信用分配表示为直接的非线性函数，并采用自定义的coder-evaluator框架来生成、验证和优化代码，减少推理中的幻觉和浅层问题。在标准MARL基准实验中，QLLM consistently outperforms现有基线，具有强泛化能力和与多种MARL算法的兼容性。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "17 pages, 10 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.12961v2",
      "published_date": "2025-04-17 14:07:11 UTC",
      "updated_date": "2025-05-22 07:56:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:05:47.013996"
    },
    {
      "arxiv_id": "2504.12951v1",
      "title": "Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Nearchos Potamitis",
        "Akhil Arora"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?",
      "tldr_zh": "该研究探讨了如何在不依赖口头反馈的情况下提升大型语言模型(LLMs)的推理能力，提出“retrials without feedback”机制，让LLMs在识别错误答案时直接重试问题解决，从而简化迭代过程。\n与传统基于自我评估和反馈的推理框架相比，这种方法显著降低了计算复杂性。\n实验发现，简单重试策略往往优于更复杂的框架，表明高效的简单方法可能比复杂策略更具实际价值，并为LLMs自主代理的发展提供新见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 16 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2405.06691",
      "pdf_url": "http://arxiv.org/pdf/2504.12951v1",
      "published_date": "2025-04-17 13:52:48 UTC",
      "updated_date": "2025-04-17 13:52:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:05:59.153199"
    },
    {
      "arxiv_id": "2504.13227v1",
      "title": "DIDS: Domain Impact-aware Data Sampling for Large Language Model Training",
      "title_zh": "翻译失败",
      "authors": [
        "Weijie Shi",
        "Jipeng Zhang",
        "Yaguang Wu",
        "Jingzhi Fang",
        "Ruiyuan Zhang",
        "Jiajie Xu",
        "Jia Zhu",
        "Hao Chen",
        "Yao Zhao",
        "Sirui Han",
        "Xiaofang Zhou"
      ],
      "abstract": "Large language models (LLMs) are commonly trained on multi-domain datasets,\nwhere domain sampling strategies significantly impact model performance due to\nvarying domain importance across downstream tasks. Existing approaches for\noptimizing domain-level sampling strategies struggle with maintaining\nintra-domain consistency and accurately measuring domain impact. In this paper,\nwe present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain\nconsistency, a gradient clustering algorithm is proposed to group training data\nbased on their learning effects, where a proxy language model and\ndimensionality reduction are employed to reduce computational overhead. To\naccurately measure domain impact, we develop a Fisher Information Matrix (FIM)\nguided metric that quantifies how domain-specific parameter updates affect the\nmodel's output distributions on downstream tasks, with theoretical guarantees.\nFurthermore, to determine optimal sampling ratios, DIDS combines both the\nFIM-guided domain impact assessment and loss learning trajectories that\nindicate domain-specific potential, while accounting for diminishing marginal\nreturns. Extensive experiments demonstrate that DIDS achieves 3.4% higher\naverage performance while maintaining comparable training efficiency.",
      "tldr_zh": "该论文提出DIDS（Domain Impact-aware Data Sampling），一种优化大型语言模型(LLMs)训练的领域影响感知数据采样策略，以解决现有方法在维持领域内一致性和准确测量领域影响方面的不足。通过梯度聚类算法(gradient clustering algorithm)对训练数据进行分组，确保领域内一致性，同时使用代理语言模型和降维技术降低计算开销。DIDS还引入Fisher Information Matrix (FIM)引导的度量来量化领域特定参数更新对下游任务输出分布的影响，并结合损失学习轨迹(loss learning trajectories)确定最佳采样比例，同时考虑边际收益递减。实验结果显示，DIDS在保持可比训练效率的同时，提高了平均性能3.4%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13227v1",
      "published_date": "2025-04-17 13:09:38 UTC",
      "updated_date": "2025-04-17 13:09:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:06:13.389971"
    },
    {
      "arxiv_id": "2504.12911v2",
      "title": "Benchmarking Multi-National Value Alignment for Large Language Models",
      "title_zh": "多国家价值观对齐",
      "authors": [
        "Weijie Shi",
        "Chengyi Ju",
        "Chengzhong Liu",
        "Jiaming Ji",
        "Jipeng Zhang",
        "Ruiyuan Zhang",
        "Jia Zhu",
        "Jiajie Xu",
        "Yaodong Yang",
        "Sirui Han",
        "Yike Guo"
      ],
      "abstract": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）与多国价值观的对齐问题，指出现有基准主要关注伦理审查而忽略更广泛的政策、法律和道德因素，且不易扩展。研究者引入了 NaVAB 基准，通过国家价值观提取管道（包括指令标记、筛选过程和冲突减少机制）来高效构建数据集，并评估 LLMs 与中国、美国、英国、法国和德国等五国的价值观一致性。实验结果显示，NaVAB 能识别模型的不对齐场景，并与其他对齐技术结合，有效降低价值观冲突风险。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12911v2",
      "published_date": "2025-04-17 13:01:38 UTC",
      "updated_date": "2025-04-19 04:07:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:06:23.089461"
    },
    {
      "arxiv_id": "2504.12898v1",
      "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhouhao Sun",
        "Xiao Ding",
        "Li Du",
        "Yunpeng Xu",
        "Yixuan Ma",
        "Yang Zhao",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）捕捉数据集偏差导致泛化性差的问题，提出了一种信息增益引导的因果干预去偏框架（IGCIDB）。该框架首先利用信息增益引导的因果干预方法自动平衡指令微调数据集的分布，以减少偏差影响。接着，通过标准监督微调过程在去偏数据集上训练LLMs。实验结果显示，IGCIDB能有效提升LLMs在不同任务上的泛化性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12898v1",
      "published_date": "2025-04-17 12:39:25 UTC",
      "updated_date": "2025-04-17 12:39:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:06:34.447107"
    },
    {
      "arxiv_id": "2504.12891v1",
      "title": "Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication",
      "title_zh": "AI 代理是机器翻译的新前沿吗？单智能体和多智能体系统在多语言数字通信中的挑战与机遇",
      "authors": [
        "Vicent Briva-Iglesias"
      ],
      "abstract": "The rapid evolution of artificial intelligence (AI) has introduced AI agents\nas a disruptive paradigm across various industries, yet their application in\nmachine translation (MT) remains underexplored. This paper describes and\nanalyses the potential of single- and multi-agent systems for MT, reflecting on\nhow they could enhance multilingual digital communication. While single-agent\nsystems are well-suited for simpler translation tasks, multi-agent systems,\nwhich involve multiple specialized AI agents collaborating in a structured\nmanner, may offer a promising solution for complex scenarios requiring high\naccuracy, domain-specific knowledge, and contextual awareness. To demonstrate\nthe feasibility of multi-agent workflows in MT, we are conducting a pilot study\nin legal MT. The study employs a multi-agent system involving four specialized\nAI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and\n(iv) final editing. Our findings suggest that multi-agent systems may have the\npotential to significantly improve domain-adaptability and contextual\nawareness, with superior translation quality to traditional MT or single-agent\nsystems. This paper also sets the stage for future research into multi-agent\napplications in MT, integration into professional translation workflows, and\nshares a demo of the system analyzed in the paper.",
      "tldr_zh": "这篇论文探讨了AI agents在机器翻译(MT)领域的潜力，分析了单代理系统和多代理系统在多语种数字通信中的挑战与机遇。作者指出，单代理系统适合简单翻译任务，而多代理系统通过多个专业化AI agents的协作，能更好地处理复杂场景，提供更高的准确性、领域特定知识和上下文感知。为验证可行性，他们在法律MT中进行试点研究，使用四个专门agents进行(i)翻译、(ii) adequacy review、(iii) fluency review和(iv) final editing。研究发现，多代理系统显著提升了翻译质量、领域适应性和上下文感知，优于传统MT或单代理系统，并为未来将多代理集成到专业工作流中提供了研究方向和系统演示。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12891v1",
      "published_date": "2025-04-17 12:32:18 UTC",
      "updated_date": "2025-04-17 12:32:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:06:47.466782"
    },
    {
      "arxiv_id": "2504.12867v3",
      "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting",
      "title_zh": "翻译失败",
      "authors": [
        "Guanrou Yang",
        "Chen Yang",
        "Qian Chen",
        "Ziyang Ma",
        "Wenxi Chen",
        "Wen Wang",
        "Tianrui Wang",
        "Yifan Yang",
        "Zhikang Niu",
        "Wenrui Liu",
        "Fan Yu",
        "Zhihao Du",
        "Zhifu Gao",
        "ShiLiang Zhang",
        "Xie Chen"
      ],
      "abstract": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://yanghaha0908.github.io/EmoVoice/. Dataset, code, and checkpoints will\nbe released.",
      "tldr_zh": "本文提出 EmoVoice，一种基于 LLMs 的情感可控 Text-To-Speech (TTS) 模型，支持细粒度的自由式自然语言提示控制，并引入 phoneme boost 设计结合 chain-of-thought (CoT) 和 chain-of-modality (CoM) 技术，以并行输出音素和音频标记，提升生成语音的内容一致性。研究者构建了 EmoVoice-DB，一个高质量的 40 小时英语情感数据集，包含富有表现力的语音和细粒度标签。EmoVoice 在英语 EmoVoice-DB 测试集和中文 Secap 测试集上达到了 state-of-the-art (SOTA) 性能，仅使用合成数据训练，并探讨了现有情感评估指标的可靠性，以及利用 SOTA 多模态 LLMs 如 GPT-4o-audio 和 Gemini 进行评估。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12867v3",
      "published_date": "2025-04-17 11:50:04 UTC",
      "updated_date": "2025-04-22 02:09:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:07:00.833309"
    },
    {
      "arxiv_id": "2504.12856v1",
      "title": "3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise",
      "title_zh": "翻译失败",
      "authors": [
        "Yifeng Cheng",
        "Juan Du"
      ],
      "abstract": "Large pretrained vision foundation models have shown significant potential in\nvarious vision tasks. However, for industrial anomaly detection, the scarcity\nof real defect samples poses a critical challenge in leveraging these models.\nWhile 2D anomaly generation has significantly advanced with established\ngenerative models, the adoption of 3D sensors in industrial manufacturing has\nmade leveraging 3D data for surface quality inspection an emerging trend. In\ncontrast to 2D techniques, 3D anomaly generation remains largely unexplored,\nlimiting the potential of 3D data in industrial quality inspection. To address\nthis gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,\nbased on Perlin noise and surface parameterization. Our method generates\nrealistic 3D surface anomalies by projecting the point cloud onto a 2D plane,\nsampling multi-scale noise values from a Perlin noise field, and perturbing the\npoint cloud along its normal direction. Through comprehensive visualization\nexperiments, we demonstrate how key parameters - including noise scale,\nperturbation strength, and octaves, provide fine-grained control over the\ngenerated anomalies, enabling the creation of diverse defect patterns from\npronounced deformations to subtle surface variations. Additionally, our\ncross-category experiments show that the method produces consistent yet\ngeometrically plausible anomalies across different object types, adapting to\ntheir specific surface characteristics. We also provide a comprehensive\ncodebase and visualization toolkit to facilitate future research.",
      "tldr_zh": "该研究针对工业异常检测中3D数据稀缺的问题，提出了一种简单有效的生成方法3D-PNAS，利用Perlin noise和表面参数化来合成3D表面异常。具体而言，该方法通过将点云投影到2D平面、采样多尺度噪声值并沿法线方向扰动点云，生成真实且多样的缺陷模式，并通过参数如噪声规模、扰动强度和octaves实现细粒度控制。实验结果显示，3D-PNAS能在不同物体类别上产生一致且几何合理的异常，并提供全面的代码库和可视化工具，以推动未来研究。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO",
        "I.5.4"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12856v1",
      "published_date": "2025-04-17 11:23:17 UTC",
      "updated_date": "2025-04-17 11:23:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:07:10.895120"
    },
    {
      "arxiv_id": "2504.12841v1",
      "title": "ALT: A Python Package for Lightweight Feature Representation in Time Series Classification",
      "title_zh": "ALT：一个用于时间序列分类中的轻量级特征表示的Python包",
      "authors": [
        "Balázs P. Halmos",
        "Balázs Hajós",
        "Vince Á. Molnár",
        "Marcell T. Kurbucz",
        "Antal Jakovác"
      ],
      "abstract": "We introduce ALT, an open-source Python package created for efficient and\naccurate time series classification (TSC). The package implements the adaptive\nlaw-based transformation (ALT) algorithm, which transforms raw time series data\ninto a linearly separable feature space using variable-length shifted time\nwindows. This adaptive approach enhances its predecessor, the linear law-based\ntransformation (LLT), by effectively capturing patterns of varying temporal\nscales. The software is implemented for scalability, interpretability, and ease\nof use, achieving state-of-the-art performance with minimal computational\noverhead. Extensive benchmarking on real-world datasets demonstrates the\nutility of ALT for diverse TSC tasks in physics and related domains.",
      "tldr_zh": "本文介绍了 ALT，这是一个开源 Python 包，旨在提供高效准确的时间序列分类 (TSC) 的轻量级特征表示。ALT 实现了自适应法律基于转换 (ALT) 算法，通过可变长度的移位时间窗口将原始时间序列数据转换为线性可分特征空间，从而改进了其前身线性法律基于转换 (LLT)，并更好地捕捉不同时间尺度的模式。该包强调可扩展性、可解释性和易用性，在最小计算开销下实现最先进性能，并在真实数据集的基准测试中证明了其在物理和相关领域 TSC 任务的实用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MS",
        "stat.ML",
        "62M10, 62H30, 68T05, 68T10",
        "I.5.1; I.2.6; G.3; D.2.13"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12841v1",
      "published_date": "2025-04-17 10:57:29 UTC",
      "updated_date": "2025-04-17 10:57:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:07:23.354734"
    },
    {
      "arxiv_id": "2504.13224v1",
      "title": "ICAS: IP Adapter and ControlNet-based Attention Structure for Multi-Subject Style Transfer Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Fuwei Liu"
      ],
      "abstract": "Generating multi-subject stylized images remains a significant challenge due\nto the ambiguity in defining style attributes (e.g., color, texture,\natmosphere, and structure) and the difficulty in consistently applying them\nacross multiple subjects. Although recent diffusion-based text-to-image models\nhave achieved remarkable progress, existing methods typically rely on\ncomputationally expensive inversion procedures or large-scale stylized\ndatasets. Moreover, these methods often struggle with maintaining multi-subject\nsemantic fidelity and are limited by high inference costs. To address these\nlimitations, we propose ICAS (IP-Adapter and ControlNet-based Attention\nStructure), a novel framework for efficient and controllable multi-subject\nstyle transfer. Instead of full-model tuning, ICAS adaptively fine-tunes only\nthe content injection branch of a pre-trained diffusion model, thereby\npreserving identity-specific semantics while enhancing style controllability.\nBy combining IP-Adapter for adaptive style injection with ControlNet for\nstructural conditioning, our framework ensures faithful global layout\npreservation alongside accurate local style synthesis. Furthermore, ICAS\nintroduces a cyclic multi-subject content embedding mechanism, which enables\neffective style transfer under limited-data settings without the need for\nextensive stylized corpora. Extensive experiments show that ICAS achieves\nsuperior performance in structure preservation, style consistency, and\ninference efficiency, establishing a new paradigm for multi-subject style\ntransfer in real-world applications.",
      "tldr_zh": "该论文提出ICAS框架，利用IP-Adapter和ControlNet-based Attention Structure，针对多主体风格转移优化问题，提供高效且可控的解决方案，以解决风格属性（如颜色、纹理和结构）的模糊性和多主体语义保真挑战。ICAS通过仅微调预训练扩散模型的内容注入分支，结合IP-Adapter的自适应风格注入和ControlNet的结构条件，确保全局布局保真和局部风格合成，同时引入循环多主体内容嵌入机制，在有限数据条件下实现有效风格转移，而无需大规模风格化数据集。实验结果显示，ICAS在结构保真、风格一致性和推理效率方面均优于现有方法，为实际应用中的多主体风格转移树立了新范式。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.13224v1",
      "published_date": "2025-04-17 10:48:11 UTC",
      "updated_date": "2025-04-17 10:48:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:07:35.508561"
    },
    {
      "arxiv_id": "2504.12833v1",
      "title": "Image-Editing Specialists: An RLAIF Approach for Diffusion Models",
      "title_zh": "图像编辑专家：一种 RLAIF 方法用于扩散模型",
      "authors": [
        "Elior Benarous",
        "Yilun Du",
        "Heng Yang"
      ],
      "abstract": "We present a novel approach to training specialized instruction-based\nimage-editing diffusion models, addressing key challenges in structural\npreservation with input images and semantic alignment with user prompts. We\nintroduce an online reinforcement learning framework that aligns the diffusion\nmodel with human preferences without relying on extensive human annotations or\ncurating a large dataset. Our method significantly improves the realism and\nalignment with instructions in two ways. First, the proposed models achieve\nprecise and structurally coherent modifications in complex scenes while\nmaintaining high fidelity in instruction-irrelevant areas. Second, they capture\nfine nuances in the desired edit by leveraging a visual prompt, enabling\ndetailed control over visual edits without lengthy textual prompts. This\napproach simplifies users' efforts to achieve highly specific edits, requiring\nonly 5 reference images depicting a certain concept for training. Experimental\nresults demonstrate that our models can perform intricate edits in complex\nscenes, after just 10 training steps. Finally, we showcase the versatility of\nour method by applying it to robotics, where enhancing the visual realism of\nsimulated environments through targeted sim-to-real image edits improves their\nutility as proxies for real-world settings.",
      "tldr_zh": "本研究提出了一种基于 RLAIF 的方法，用于训练专门的指令-based 图像编辑扩散模型，旨在解决输入图像结构保留和用户提示语义对齐的挑战。该框架采用在线强化学习，不依赖大量人类标注，仅需 5 张参考图像即可实现精确编辑，并在复杂场景中保持无关区域的高保真度，同时通过视觉提示捕捉细微细节以简化用户操作。实验结果表明，该模型在短短 10 步训练后即可处理复杂编辑，并扩展到机器人学领域，提升模拟环境的视觉真实性，从而作为真实世界的有效代理。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12833v1",
      "published_date": "2025-04-17 10:46:39 UTC",
      "updated_date": "2025-04-17 10:46:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:07:47.096138"
    },
    {
      "arxiv_id": "2504.12817v1",
      "title": "Explainable Scene Understanding with Qualitative Representations and Graph Neural Networks",
      "title_zh": "基于定性表示和图神经网络的可解释场景理解",
      "authors": [
        "Nassim Belmecheri",
        "Arnaud Gotlieb",
        "Nadjib Lazaar",
        "Helge Spieker"
      ],
      "abstract": "This paper investigates the integration of graph neural networks (GNNs) with\nQualitative Explainable Graphs (QXGs) for scene understanding in automated\ndriving. Scene understanding is the basis for any further reactive or proactive\ndecision-making. Scene understanding and related reasoning is inherently an\nexplanation task: why is another traffic participant doing something, what or\nwho caused their actions? While previous work demonstrated QXGs' effectiveness\nusing shallow machine learning models, these approaches were limited to\nanalysing single relation chains between object pairs, disregarding the broader\nscene context. We propose a novel GNN architecture that processes entire graph\nstructures to identify relevant objects in traffic scenes. We evaluate our\nmethod on the nuScenes dataset enriched with DriveLM's human-annotated\nrelevance labels. Experimental results show that our GNN-based approach\nachieves superior performance compared to baseline methods. The model\neffectively handles the inherent class imbalance in relevant object\nidentification tasks while considering the complete spatial-temporal\nrelationships between all objects in the scene. Our work demonstrates the\npotential of combining qualitative representations with deep learning\napproaches for explainable scene understanding in autonomous driving systems.",
      "tldr_zh": "本论文探讨了将 Graph Neural Networks (GNNs) 与 Qualitative Explainable Graphs (QXGs) 整合，用于自动驾驶中的场景理解，旨在解释交通参与者的行为及其原因。相比以往仅分析对象对之间单一关系链的浅层模型，该方法提出一种新型 GNN 架构，能处理整个图结构并考虑更广泛的场景上下文。实验在 nuScenes 数据集（经 DriveLM 标注增强）上表明，该方法在相关对象识别任务中优于基线模型，能够有效处理类不平衡问题并捕捉所有对象间的空间-时间关系。该研究展示了定性表示与深度学习结合的潜力，提升了自主驾驶系统的可解释性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Workshop \"Advancing Automated Driving in Highly Interactive Scenarios\n  through Behavior Prediction, Trustworthy AI, and Remote Operations\" @ 36th\n  IEEE Intelligent Vehicles Symposium (IV)",
      "pdf_url": "http://arxiv.org/pdf/2504.12817v1",
      "published_date": "2025-04-17 10:21:30 UTC",
      "updated_date": "2025-04-17 10:21:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:08:00.118083"
    },
    {
      "arxiv_id": "2504.12807v1",
      "title": "Hybrid Dense-UNet201 Optimization for Pap Smear Image Segmentation Using Spider Monkey Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Ach Khozaimi",
        "Isnani Darti",
        "Syaiful Anam",
        "Wuryansari Muharini Kusumawinahyu"
      ],
      "abstract": "Pap smear image segmentation is crucial for cervical cancer diagnosis.\nHowever, traditional segmentation models often struggle with complex cellular\nstructures and variations in pap smear images. This study proposes a hybrid\nDense-UNet201 optimization approach that integrates a pretrained DenseNet201 as\nthe encoder for the U-Net architecture and optimizes it using the spider monkey\noptimization (SMO) algorithm. The Dense-UNet201 model excelled at feature\nextraction. The SMO was modified to handle categorical and discrete parameters.\nThe SIPaKMeD dataset was used in this study and evaluated using key performance\nmetrics, including loss, accuracy, Intersection over Union (IoU), and Dice\ncoefficient. The experimental results showed that Dense-UNet201 outperformed\nU-Net, Res-UNet50, and Efficient-UNetB0. SMO Dense-UNet201 achieved a\nsegmentation accuracy of 96.16%, an IoU of 91.63%, and a Dice coefficient score\nof 95.63%. These findings underscore the effectiveness of image preprocessing,\npretrained models, and metaheuristic optimization in improving medical image\nanalysis and provide new insights into cervical cell segmentation methods.",
      "tldr_zh": "本研究针对 Pap 涂片图像分割的挑战（如复杂细胞结构和图像变异），提出了一种混合 Dense-UNet201 优化方法，将预训练的 DenseNet201 用作 U-Net 架构的编码器，并通过 Spider Monkey Optimization (SMO) 算法进行优化，其中 SMO 被修改以处理分类和离散参数。实验使用 SIPaKMeD 数据集，通过损失、准确率、Intersection over Union (IoU) 和 Dice coefficient 等指标进行评估。结果显示，SMO 优化后的 Dense-UNet201 优于 U-Net、Res-UNet50 和 Efficient-UNetB0，实现了 96.16% 的准确率、91.63% 的 IoU 和 95.63% 的 Dice coefficient。这些发现突出了图像预处理、预训练模型和元启发式优化在提升医疗图像分析中的有效性，并为子宫颈细胞分割提供新方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12807v1",
      "published_date": "2025-04-17 10:14:05 UTC",
      "updated_date": "2025-04-17 10:14:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:08:14.356047"
    },
    {
      "arxiv_id": "2504.12806v2",
      "title": "A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Georgios Papadopoulos",
        "Shaltiel Eloul",
        "Yash Satsangi",
        "Jamie Heredge",
        "Niraj Kumar",
        "Chun-Fu Chen",
        "Marco Pistoia"
      ],
      "abstract": "The loss landscape of Variational Quantum Neural Networks (VQNNs) is\ncharacterized by local minima that grow exponentially with increasing qubits.\nBecause of this, it is more challenging to recover information from model\ngradients during training compared to classical Neural Networks (NNs). In this\npaper we present a numerical scheme that successfully reconstructs input\ntraining, real-world, practical data from trainable VQNNs' gradients. Our\nscheme is based on gradient inversion that works by combining gradients\nestimation with the finite difference method and adaptive low-pass filtering.\nThe scheme is further optimized with Kalman filter to obtain efficient\nconvergence. Our experiments show that our algorithm can invert even\nbatch-trained data, given the VQNN model is sufficiently over-parameterized.",
      "tldr_zh": "该研究针对变分量子神经网络(VQNNs)的损失景观特性（即局部最小值随量子比特数指数增长），提出了一种数值梯度反演攻击方案，以从训练梯度中重建输入数据。该方案结合梯度估计、有限差分方法和自适应低通滤波器进行优化，并使用Kalman滤波器实现高效收敛。实验结果显示，该算法即使在批量训练场景下也能有效重建真实数据，前提是VQNN模型足够过参数化，从而为量子机器学习的隐私风险提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 17 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12806v2",
      "published_date": "2025-04-17 10:12:38 UTC",
      "updated_date": "2025-05-07 09:01:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:08:22.259165"
    },
    {
      "arxiv_id": "2504.12803v1",
      "title": "Enhancing Explainability and Reliable Decision-Making in Particle Swarm Optimization through Communication Topologies",
      "title_zh": "通过通信拓扑增强粒子群优化中的可解释性和可靠决策",
      "authors": [
        "Nitin Gupta",
        "Indu Bala",
        "Bapi Dutta",
        "Luis Martínez",
        "Anupam Yadav"
      ],
      "abstract": "Swarm intelligence effectively optimizes complex systems across fields like\nengineering and healthcare, yet algorithm solutions often suffer from low\nreliability due to unclear configurations and hyperparameters. This study\nanalyzes Particle Swarm Optimization (PSO), focusing on how different\ncommunication topologies Ring, Star, and Von Neumann affect convergence and\nsearch behaviors. Using an adapted IOHxplainer , an explainable benchmarking\ntool, we investigate how these topologies influence information flow,\ndiversity, and convergence speed, clarifying the balance between exploration\nand exploitation. Through visualization and statistical analysis, the research\nenhances interpretability of PSO's decisions and provides practical guidelines\nfor choosing suitable topologies for specific optimization tasks. Ultimately,\nthis contributes to making swarm based optimization more transparent, robust,\nand trustworthy.",
      "tldr_zh": "本研究探讨了通信拓扑如何提升粒子群优化（PSO）的可解释性和可靠决策，针对 Ring、Star 和 Von Neumann 拓扑分析其对信息流、多样性以及收敛速度的影响。使用 IOHxplainer 工具进行可视化和统计分析，该研究揭示了这些拓扑在探索与利用平衡方面的作用，并为特定优化任务选择合适拓扑提供了实用指南。最终，这有助于使基于群体的优化算法更透明、鲁棒和可信。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12803v1",
      "published_date": "2025-04-17 10:05:10 UTC",
      "updated_date": "2025-04-17 10:05:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:08:34.432084"
    },
    {
      "arxiv_id": "2504.12782v1",
      "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts",
      "title_zh": "翻译失败",
      "authors": [
        "Leyang Li",
        "Shilin Lu",
        "Yan Ren",
        "Adams Wai-Kin Kong"
      ],
      "abstract": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT",
      "tldr_zh": "这篇论文提出了一种名为 ANT 的 finetuning 框架，用于自动引导去噪轨迹（Auto-Steering Denoising Trajectories），以避免文本到-image 模型生成 unwanted concepts，从而确保模型的伦理部署。ANT 基于关键洞见，通过逆转 classifier-free guidance 的条件方向，在 mid-to-late 去噪阶段进行精确内容修改，同时保留 early-stage 结构完整性和 score function field 的完整性，而无需依赖 heuristic anchor concept 选择。对于 single-concept erasure，它引入 augmentation-enhanced weight saliency map 来识别并擦除关键参数；对于 multi-concept erasure，该框架提供一个通用的 plug-and-play 解决方案。实验证明，ANT 在单概念和多概念擦除任务上达到 state-of-the-art 结果，生成高质量、安全的输出而不损害生成保真度。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2504.12782v1",
      "published_date": "2025-04-17 09:29:30 UTC",
      "updated_date": "2025-04-17 09:29:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:08:48.716820"
    },
    {
      "arxiv_id": "2504.12778v1",
      "title": "Towards Lossless Token Pruning in Late-Interaction Retrieval Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Zong",
        "Benjamin Piwowarski"
      ],
      "abstract": "Late interaction neural IR models like ColBERT offer a competitive\neffectiveness-efficiency trade-off across many benchmarks. However, they\nrequire a huge memory space to store the contextual representation for all the\ndocument tokens. Some works have proposed using either heuristics or\nstatistical-based techniques to prune tokens from each document. This however\ndoesn't guarantee that the removed tokens have no impact on the retrieval\nscore. Our work uses a principled approach to define how to prune tokens\nwithout impacting the score between a document and a query. We introduce three\nregularization losses, that induce a solution with high pruning ratios, as well\nas two pruning strategies. We study them experimentally (in and out-domain),\nshowing that we can preserve ColBERT's performance while using only 30\\% of the\ntokens.",
      "tldr_zh": "本文针对 Late-Interaction 检索模型（如 ColBERT）的高内存需求问题，提出了一种无损 token pruning 方法，通过引入三种正则化损失来优化模型，确保修剪标记后不会影响文档和查询之间的检索分数，同时结合两种修剪策略实现高修剪比率。实验结果显示，在领域内和领域外基准测试中，该方法仅使用 30% 的标记即可保持 ColBERT 的性能水平，从而显著提升了模型的效率和实用性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at SIGIR 2025 Full Paper Track",
      "pdf_url": "http://arxiv.org/pdf/2504.12778v1",
      "published_date": "2025-04-17 09:18:58 UTC",
      "updated_date": "2025-04-17 09:18:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:08:58.958967"
    },
    {
      "arxiv_id": "2504.12777v2",
      "title": "Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis",
      "title_zh": "多智能体强化学习模拟用于环境政策合成",
      "authors": [
        "James Rudd-Jones",
        "Mirco Musolesi",
        "María Pérez-Ortiz"
      ],
      "abstract": "Climate policy development faces significant challenges due to deep\nuncertainty, complex system dynamics, and competing stakeholder interests.\nClimate simulation methods, such as Earth System Models, have become valuable\ntools for policy exploration. However, their typical use is for evaluating\npotential polices, rather than directly synthesizing them. The problem can be\ninverted to optimize for policy pathways, but the traditional optimization\napproaches often struggle with non-linear dynamics, heterogeneous agents, and\ncomprehensive uncertainty quantification. We propose a framework for augmenting\nclimate simulations with Multi-Agent Reinforcement Learning (MARL) to address\nthese limitations. We identify key challenges at the interface between climate\nsimulations and the application of MARL in the context of policy synthesis,\nincluding reward definition, scalability with increasing agents and state\nspaces, uncertainty propagation across linked systems, and solution validation.\nAdditionally, we discuss challenges in making MARL-derived solutions\ninterpretable and useful for policy-makers. Our framework provides a foundation\nfor more sophisticated climate policy exploration while acknowledging important\nlimitations and areas for future research.",
      "tldr_zh": "该研究针对气候政策开发中的不确定性、复杂动态和利益冲突问题，提出了一种使用多智能体强化学习（MARL）增强气候模拟的框架，以直接合成政策路径。该框架解决了关键挑战，包括奖励定义、代理和状态空间的可扩展性、不确定性在系统间的传播以及解决方案验证，同时讨论了如何使MARL输出更具可解释性并适用于决策者。通过这一方法，框架为更先进的政策探索奠定基础，但也承认了其局限性，如需要进一步研究以提升实际应用。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Published in AAMAS'25 Blue Sky Ideas Track",
      "pdf_url": "http://arxiv.org/pdf/2504.12777v2",
      "published_date": "2025-04-17 09:18:04 UTC",
      "updated_date": "2025-05-14 16:44:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:09:10.956099"
    },
    {
      "arxiv_id": "2504.12773v1",
      "title": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration",
      "title_zh": "通过符号-神经",
      "authors": [
        "Yicheng Pan",
        "Zhenrong Zhang",
        "Pengfei Hu",
        "Jiefeng Ma",
        "Jun Du",
        "Jianshu Zhang",
        "Quan Liu",
        "Jianqing Gao",
        "Feng Ma"
      ],
      "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen.",
      "tldr_zh": "本研究针对 Multimodal Large Language Models (MLLMs) 在几何问题求解 (GPS) 中的挑战，如缺乏准确的逐步解决方案数据和严重的 hallucinations，提出了一种通过符号-神经整合的方法。研究开发了 GeoGen 管道，利用精确的 symbolic reasoning 自动生成大规模、高质量的几何图形问答对，并以此训练 GeoLogic 模型，作为自然语言与符号系统的桥梁，帮助验证 MLLM 输出并缓解幻觉。实验结果显示，该方法显著提升了 MLLMs 在几何推理基准上的性能，提供更可靠、可解释的 GPS 解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12773v1",
      "published_date": "2025-04-17 09:13:46 UTC",
      "updated_date": "2025-04-17 09:13:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:09:22.909009"
    },
    {
      "arxiv_id": "2504.12757v2",
      "title": "MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System",
      "title_zh": "翻译失败",
      "authors": [
        "Sonu Kumar",
        "Anubhav Girdhar",
        "Ritesh Patil",
        "Divyansh Tripathi"
      ],
      "abstract": "As Agentic AI gain mainstream adoption, the industry invests heavily in model\ncapabilities, achieving rapid leaps in reasoning and quality. However, these\nsystems remain largely confined to data silos, and each new integration\nrequires custom logic that is difficult to scale. The Model Context Protocol\n(MCP) addresses this challenge by defining a universal, open standard for\nsecurely connecting AI-based applications (MCP clients) to data sources (MCP\nservers). However, the flexibility of the MCP introduces new risks, including\nmalicious tool servers and compromised data integrity. We present MCP Guardian,\na framework that strengthens MCP-based communication with authentication,\nrate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning.\nThrough real-world scenarios and empirical testing, we demonstrate how MCP\nGuardian effectively mitigates attacks and ensures robust oversight with\nminimal overheads. Our approach fosters secure, scalable data access for AI\nassistants, underscoring the importance of a defense-in-depth approach that\nenables safer and more transparent innovation in AI-driven environments.",
      "tldr_zh": "该研究针对Agentic AI系统在数据访问中的安全挑战，提出了MCP Guardian框架，作为一个以安全为优先的层，用于保护基于Model Context Protocol (MCP)的AI系统。该框架通过引入认证、速率限制、日志记录、追踪以及Web Application Firewall (WAF)扫描等机制，强化MCP通信的安全性。实验结果显示，MCP Guardian在真实场景中有效缓解恶意攻击和数据完整性风险，同时以最小开销实现稳健监督，促进AI助手的可扩展数据访问和更透明的创新。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12757v2",
      "published_date": "2025-04-17 08:49:10 UTC",
      "updated_date": "2025-05-19 08:48:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:09:34.321986"
    },
    {
      "arxiv_id": "2504.12755v1",
      "title": "Trajectory Adaptation using Large Language Models",
      "title_zh": "基于大型语言模型的轨迹适应",
      "authors": [
        "Anurag Maurya",
        "Tashmoy Ghosh",
        "Ravi Prakash"
      ],
      "abstract": "Adapting robot trajectories based on human instructions as per new situations\nis essential for achieving more intuitive and scalable human-robot\ninteractions. This work proposes a flexible language-based framework to adapt\ngeneric robotic trajectories produced by off-the-shelf motion planners like\nRRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained\nLLMs to adapt trajectory waypoints by generating code as a policy for dense\nrobot manipulation, enabling more complex and flexible instructions than\ncurrent methods. This approach allows us to incorporate a broader range of\ncommands, including numerical inputs. Compared to state-of-the-art\nfeature-based sequence-to-sequence models which require training, our method\ndoes not require task-specific training and offers greater interpretability and\nmore effective feedback mechanisms. We validate our approach through simulation\nexperiments on the robotic manipulator, aerial vehicle, and ground robot in the\nPybullet and Gazebo simulation environments, demonstrating that LLMs can\nsuccessfully adapt trajectories to complex human instructions.",
      "tldr_zh": "该论文提出了一种基于 Large Language Models (LLMs) 的灵活框架，用于适应机器人轨迹，以实现更直观的人机交互。该框架利用预训练 LLMs 通过生成代码作为策略，调整由 RRT、A-star 等运动规划器生成的轨迹路点或从人类演示中学习到的轨迹，支持复杂指令包括数字输入，且无需任务特定训练。相较于基于特征的序列到序列模型，该方法提供更高的可解释性和反馈机制。在 Pybullet 和 Gazebo 模拟环境中，对机器人机械臂、空中车辆和地面机器人的实验验证了 LLMs 在处理复杂人类指令时的有效性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to CoRL LangRob workshop 2024",
      "pdf_url": "http://arxiv.org/pdf/2504.12755v1",
      "published_date": "2025-04-17 08:48:23 UTC",
      "updated_date": "2025-04-17 08:48:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:09:48.310739"
    },
    {
      "arxiv_id": "2504.12740v1",
      "title": "GPMFS: Global Foundation and Personalized Optimization for Multi-Label Feature Selection",
      "title_zh": "GPMFS：多标签特征选择中的全局基础和个性化优化",
      "authors": [
        "Yifan Cao",
        "Zhilong Mi",
        "Ziqiao Yin",
        "Binghui Guo",
        "Jin Dong"
      ],
      "abstract": "As artificial intelligence methods are increasingly applied to complex task\nscenarios, high dimensional multi-label learning has emerged as a prominent\nresearch focus. At present, the curse of dimensionality remains one of the\nmajor bottlenecks in high-dimensional multi-label learning, which can be\neffectively addressed through multi-label feature selection methods. However,\nexisting multi-label feature selection methods mostly focus on identifying\nglobal features shared across all labels, which overlooks personalized\ncharacteristics and specific requirements of individual labels. This\nglobal-only perspective may limit the ability to capture label-specific\ndiscriminative information, thereby affecting overall performance. In this\npaper, we propose a novel method called GPMFS (Global Foundation and\nPersonalized Optimization for Multi-Label Feature Selection). GPMFS firstly\nidentifies global features by exploiting label correlations, then adaptively\nsupplements each label with a personalized subset of discriminative features\nusing a threshold-controlled strategy. Experiments on multiple real-world\ndatasets demonstrate that GPMFS achieves superior performance while maintaining\nstrong interpretability and robustness. Furthermore, GPMFS provides insights\ninto the label-specific strength across different multi-label datasets, thereby\ndemonstrating the necessity and potential applicability of personalized feature\nselection approaches.",
      "tldr_zh": "本研究针对高维多标签学习中的维度诅咒问题，指出现有 multi-label feature selection 方法仅关注全局特征而忽略标签的个性化特性，导致性能受限。论文提出 GPMFS 方法，通过利用标签相关性首先识别全局特征，然后采用阈值控制策略为每个标签补充个性化的 discriminative 特征子集。实验在多个真实数据集上显示，GPMFS 实现了优越的性能，同时保持了强解释性和鲁棒性，并揭示了标签特定强度的洞见，强调了个性化特征选择的必要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12740v1",
      "published_date": "2025-04-17 08:29:14 UTC",
      "updated_date": "2025-04-17 08:29:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:09:59.066147"
    },
    {
      "arxiv_id": "2504.15301v1",
      "title": "A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations",
      "title_zh": "生物启发的信任",
      "authors": [
        "Zoi Lygizou",
        "Dimitris Kalles"
      ],
      "abstract": "Trust management provides an alternative solution for securing open, dynamic,\nand distributed multi-agent systems, where conventional cryptographic methods\nprove to be impractical. However, existing trust models face challenges related\nto agent mobility, changing behaviors, and the cold start problem. To address\nthese issues we introduced a biologically inspired trust model in which\ntrustees assess their own capabilities and store trust data locally. This\ndesign improves mobility support, reduces communication overhead, resists\ndisinformation, and preserves privacy. Despite these advantages, prior\nevaluations revealed limitations of our model in adapting to provider\npopulation changes and continuous performance fluctuations. This study proposes\na novel algorithm, incorporating a self-classification mechanism for providers\nto detect performance drops potentially harmful for the service consumers.\nSimulation results demonstrate that the new algorithm outperforms its original\nversion and FIRE, a well-known trust and reputation model, particularly in\nhandling dynamic trustee behavior. While FIRE remains competitive under extreme\nenvironmental changes, the proposed algorithm demonstrates greater adaptability\nacross various conditions. In contrast to existing trust modeling research,\nthis study conducts a comprehensive evaluation of our model using widely\nrecognized trust model criteria, assessing its resilience against common\ntrust-related attacks while identifying strengths, weaknesses, and potential\ncountermeasures. Finally, several key directions for future research are\nproposed.",
      "tldr_zh": "这篇论文提出一个生物启发的信任模型（biologically inspired trust model），用于保护开放多智能体系统（open multi-agent systems），通过让受托者（trustees）评估自身能力和本地存储信任数据，来解决代理移动、行为变化和冷启动问题。新算法引入自分类机制，帮助提供者检测潜在有害的性能下降，提升了模型对动态环境和持续性能波动的适应性。模拟结果显示，该算法在处理动态受托者行为方面优于原版模型和知名模型 FIRE，并在全面评估中证明了其对常见信任相关攻击的抵抗力，同时识别了优势、劣势和潜在对策。未来研究方向包括进一步优化模型的鲁棒性。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15301v1",
      "published_date": "2025-04-17 08:21:54 UTC",
      "updated_date": "2025-04-17 08:21:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:10:12.883127"
    },
    {
      "arxiv_id": "2504.12735v2",
      "title": "The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems",
      "title_zh": "The Athenian Academy：多智能体系统的七层架构模型",
      "authors": [
        "Lidong Zhai",
        "Zhijie Qiu",
        "Lvyang Zhang",
        "Jiaqi Li",
        "Yi Wang",
        "Wen Lu",
        "Xizhong Guo",
        "Ge Sun"
      ],
      "abstract": "This paper proposes the \"Academy of Athens\" multi-agent seven-layer\nframework, aimed at systematically addressing challenges in multi-agent systems\n(MAS) within artificial intelligence (AI) art creation, such as collaboration\nefficiency, role allocation, environmental adaptation, and task parallelism.\nThe framework divides MAS into seven layers: multi-agent collaboration,\nsingle-agent multi-role playing, single-agent multi-scene traversal,\nsingle-agent multi-capability incarnation, different single agents using the\nsame large model to achieve the same target agent, single-agent using different\nlarge models to achieve the same target agent, and multi-agent synthesis of the\nsame target agent. Through experimental validation in art creation, the\nframework demonstrates its unique advantages in task collaboration, cross-scene\nadaptation, and model fusion. This paper further discusses current challenges\nsuch as collaboration mechanism optimization, model stability, and system\nsecurity, proposing future exploration through technologies like meta-learning\nand federated learning. The framework provides a structured methodology for\nmulti-agent collaboration in AI art creation and promotes innovative\napplications in the art field.",
      "tldr_zh": "本论文提出“Athenian Academy”七层架构模型，用于系统解决多智能体系统（MAS）在人工智能（AI）艺术创作中的挑战，包括协作效率、角色分配、环境适应和任务并行。该框架将MAS分为七层：多智能体协作、单智能体多角色扮演、单智能体多场景遍历、单智能体多能力化身、不同单智能体使用同一大模型实现同一目标智能体、单智能体使用不同大模型实现同一目标智能体，以及多智能体合成同一目标智能体。通过在艺术创作中的实验验证，该模型展示了在任务协作、跨场景适应和模型融合方面的独特优势。论文进一步讨论了当前挑战如协作机制优化、模型稳定性及系统安全，并建议通过元学习（meta-learning）和联邦学习（federated learning）等技术进行未来探索，为AI艺术创作的多智能体协作提供结构化方法，促进艺术领域的创新应用。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12735v2",
      "published_date": "2025-04-17 08:21:28 UTC",
      "updated_date": "2025-04-18 02:45:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:10:24.084060"
    },
    {
      "arxiv_id": "2504.12734v1",
      "title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Yongrui Chen",
        "Junhao He",
        "Linbo Fu",
        "Shenyu Zhang",
        "Rihui Jin",
        "Xinbang Dai",
        "Jiaqi Li",
        "Dehai Min",
        "Nan Hu",
        "Yuxin Zhang",
        "Guilin Qi",
        "Yi Huang",
        "Tongtong Wu"
      ],
      "abstract": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods.",
      "tldr_zh": "本文提出Pandora框架，用于Unified Structured Knowledge Reasoning (USKR)，旨在统一利用表格、数据库和知识图谱等结构化来源回答自然语言问题，通过Python的Pandas API构建与LLM预训练对齐的知识表示。框架让LLM生成文本推理步骤和可执行Python代码，并从覆盖多种SKR任务的训练示例记忆中抽取演示，以促进知识转移。实验结果显示，在四个基准测试中，Pandora优于现有统一框架，并在三种SKR任务上与任务特定方法竞争。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12734v1",
      "published_date": "2025-04-17 08:18:09 UTC",
      "updated_date": "2025-04-17 08:18:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:10:34.697450"
    },
    {
      "arxiv_id": "2504.12722v1",
      "title": "SimUSER: Simulating User Behavior with Large Language Models for Recommender System Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Nicolas Bougie",
        "Narimasa Watanabe"
      ],
      "abstract": "Recommender systems play a central role in numerous real-life applications,\nyet evaluating their performance remains a significant challenge due to the gap\nbetween offline metrics and online behaviors. Given the scarcity and limits\n(e.g., privacy issues) of real user data, we introduce SimUSER, an agent\nframework that serves as believable and cost-effective human proxies. SimUSER\nfirst identifies self-consistent personas from historical data, enriching user\nprofiles with unique backgrounds and personalities. Then, central to this\nevaluation are users equipped with persona, memory, perception, and brain\nmodules, engaging in interactions with the recommender system. SimUSER exhibits\ncloser alignment with genuine humans than prior work, both at micro and macro\nlevels. Additionally, we conduct insightful experiments to explore the effects\nof thumbnails on click rates, the exposure effect, and the impact of reviews on\nuser engagement. Finally, we refine recommender system parameters based on\noffline A/B test results, resulting in improved user engagement in the real\nworld.",
      "tldr_zh": "该论文提出 SimUSER 框架，利用 Large Language Models 模拟用户行为，作为一种可信且成本有效的代理，用于评估推荐系统性能，以解决离线指标与在线行为脱节的问题。SimUSER 通过从历史数据中识别自洽的 personas，并为用户代理配备记忆、感知和大脑模块，实现与推荐系统的互动，展现出比以往方法更接近真实人类行为的微观和宏观对齐。实验探索了缩略图对点击率、曝光效应以及评论对用户参与的影响，并基于离线 A/B 测试结果优化推荐系统参数，最终提升了真实世界的用户参与度。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12722v1",
      "published_date": "2025-04-17 07:57:23 UTC",
      "updated_date": "2025-04-17 07:57:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:10:48.440466"
    },
    {
      "arxiv_id": "2504.12721v2",
      "title": "TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations",
      "title_zh": "TimeCapsule：通过压缩预测表示解决长期时间序列预测的",
      "authors": [
        "Yihang Lu",
        "Yangyang Xu",
        "Qitao Qing",
        "Xianwei Meng"
      ],
      "abstract": "Recent deep learning models for Long-term Time Series Forecasting (LTSF)\noften emphasize complex, handcrafted designs, while simpler architectures like\nlinear models or MLPs have often outperformed these intricate solutions. In\nthis paper, we revisit and organize the core ideas behind several key\ntechniques, such as redundancy reduction and multi-scale modeling, which are\nfrequently employed in advanced LTSF models. Our goal is to streamline these\nideas for more efficient deep learning utilization. To this end, we introduce\nTimeCapsule, a model built around the principle of high-dimensional information\ncompression that unifies these techniques in a generalized yet simplified\nframework. Specifically, we model time series as a 3D tensor, incorporating\ntemporal, variate, and level dimensions, and leverage mode production to\ncapture multi-mode dependencies while achieving dimensionality compression. We\npropose an internal forecast within the compressed representation domain,\nsupported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the\nlearning of predictive representations. Extensive experiments on challenging\nbenchmarks demonstrate the versatility of our method, showing that TimeCapsule\ncan achieve state-of-the-art performance.",
      "tldr_zh": "该论文审视了长期时间序列预测 (LTSF) 中的核心技术，如冗余减少和多尺度建模，并引入 TimeCapsule 模型，通过高维信息压缩的原则简化并统一这些方法。\nTimeCapsule 将时间序列建模为 3D 张量（包括时间、变量和级别维度），利用模式乘法捕获多模式依赖并实现维度压缩，同时结合 Joint-Embedding Predictive Architecture (JEPA) 进行内部预测，以监控预测表示的学习。\n实验结果表明，TimeCapsule 在多个挑战性基准上达到了最先进性能，展示了其高效性和通用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12721v2",
      "published_date": "2025-04-17 07:54:26 UTC",
      "updated_date": "2025-05-16 08:17:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:10:59.730053"
    },
    {
      "arxiv_id": "2504.12718v1",
      "title": "TUMLS: Trustful Fully Unsupervised Multi-Level Segmentation for Whole Slide Images of Histology",
      "title_zh": "TUMLS：可信的完全无监督多级分割，用于组织学全滑玻图像",
      "authors": [
        "Walid Rehamnia",
        "Alexandra Getmanskaya",
        "Evgeniy Vasilyev",
        "Vadim Turlapov"
      ],
      "abstract": "Digital pathology, augmented by artificial intelligence (AI), holds\nsignificant promise for improving the workflow of pathologists. However,\nchallenges such as the labor-intensive annotation of whole slide images (WSIs),\nhigh computational demands, and trust concerns arising from the absence of\nuncertainty estimation in predictions hinder the practical application of\ncurrent AI methodologies in histopathology. To address these issues, we present\na novel trustful fully unsupervised multi-level segmentation methodology\n(TUMLS) for WSIs. TUMLS adopts an autoencoder (AE) as a feature extractor to\nidentify the different tissue types within low-resolution training data. It\nselects representative patches from each identified group based on an\nuncertainty measure and then does unsupervised nuclei segmentation in their\nrespective higher-resolution space without using any ML algorithms. Crucially,\nthis solution integrates seamlessly into clinicians workflows, transforming the\nexamination of a whole WSI into a review of concise, interpretable cross-level\ninsights. This integration significantly enhances and accelerates the workflow\nwhile ensuring transparency. We evaluated our approach using the UPENN-GBM\ndataset, where the AE achieved a mean squared error (MSE) of 0.0016.\nAdditionally, nucleus segmentation is assessed on the MoNuSeg dataset,\noutperforming all unsupervised approaches with an F1 score of 77.46% and a\nJaccard score of 63.35%. These results demonstrate the efficacy of TUMLS in\nadvancing the field of digital pathology.",
      "tldr_zh": "该论文提出TUMLS，一种完全无监督的多级分割方法，用于处理组织学全滑玻图像(WSIs)，旨在解决数字病理学中标注劳动密集、计算需求高以及预测不确定性缺乏导致的信任问题。TUMLS使用autoencoder (AE)作为特征提取器，在低分辨率数据中识别组织类型，并基于不确定性措施选择代表性补丁，在更高分辨率下进行无监督的细胞核分割，从而提供可解释的跨级洞见。实验结果显示，在UPENN-GBM数据集上AE的mean squared error (MSE)为0.0016，而在MoNuSeg数据集上，TUMLS的F1 score达77.46%和Jaccard score达63.35%，优于其他无监督方法，并显著提升临床工作流程的效率和透明度。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "I.2.6; I.2.10; I.4.6; I.5.3; I.5.4"
      ],
      "primary_category": "eess.IV",
      "comment": "32 pages, 15 figures, 3 tables, 42 references",
      "pdf_url": "http://arxiv.org/pdf/2504.12718v1",
      "published_date": "2025-04-17 07:48:05 UTC",
      "updated_date": "2025-04-17 07:48:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:11:11.298113"
    },
    {
      "arxiv_id": "2504.12717v1",
      "title": "Post-pre-training for Modality Alignment in Vision-Language Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shin'ya Yamaguchi",
        "Dewei Feng",
        "Sekitoshi Kanai",
        "Kazuki Adachi",
        "Daiki Chijiwa"
      ],
      "abstract": "Contrastive language image pre-training (CLIP) is an essential component of\nbuilding modern vision-language foundation models. While CLIP demonstrates\nremarkable zero-shot performance on downstream tasks, the multi-modal feature\nspaces still suffer from a modality gap, which is a gap between image and text\nfeature clusters and limits downstream task performance. Although existing\nworks attempt to address the modality gap by modifying pre-training or\nfine-tuning, they struggle with heavy training costs with large datasets or\ndegradations of zero-shot performance. This paper presents CLIP-Refine, a\npost-pre-training method for CLIP models at a phase between pre-training and\nfine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training\non small image-text datasets without zero-shot performance degradations. To\nthis end, we introduce two techniques: random feature alignment (RaFA) and\nhybrid contrastive-distillation (HyCD). RaFA aligns the image and text features\nto follow a shared prior distribution by minimizing the distance to random\nreference vectors sampled from the prior. HyCD updates the model with hybrid\nsoft labels generated by combining ground-truth image-text pair labels and\noutputs from the pre-trained CLIP model. This contributes to achieving both\nmaintaining the past knowledge and learning new knowledge to align features.\nOur extensive experiments with multiple classification and retrieval tasks show\nthat CLIP-Refine succeeds in mitigating the modality gap and improving the\nzero-shot performance.",
      "tldr_zh": "本文提出 CLIP-Refine，一种后预训练方法，旨在解决视觉语言基础模型（如 CLIP）中的模态间差距(modality gap)，通过在小数据集上仅需一轮训练来对齐图像和文本特征空间，同时保持零样本性能(zero-shot performance)。该方法引入随机特征对齐(RaFA)，通过最小化特征与共享先验分布的随机参考向量的距离来实现对齐；以及混合对比-蒸馏(HyCD)，使用混合软标签结合 ground-truth 和预训练模型输出，以保留原有知识并学习新知识。实验在多个分类和检索任务上表明，CLIP-Refine 有效缓解了模态间差距，并提升了下游任务性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025; Code: https://github.com/yshinya6/clip-refine",
      "pdf_url": "http://arxiv.org/pdf/2504.12717v1",
      "published_date": "2025-04-17 07:46:19 UTC",
      "updated_date": "2025-04-17 07:46:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:11:24.252150"
    },
    {
      "arxiv_id": "2504.12714v2",
      "title": "Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination",
      "title_zh": "跨环境合作实现零样本多智能体协调",
      "authors": [
        "Kunal Jha",
        "Wilka Carvalho",
        "Yancheng Liang",
        "Simon S. Du",
        "Max Kleiman-Weiner",
        "Natasha Jaques"
      ],
      "abstract": "Zero-shot coordination (ZSC), the ability to adapt to a new partner in a\ncooperative task, is a critical component of human-compatible AI. While prior\nwork has focused on training agents to cooperate on a single task, these\nspecialized models do not generalize to new tasks, even if they are highly\nsimilar. Here, we study how reinforcement learning on a distribution of\nenvironments with a single partner enables learning general cooperative skills\nthat support ZSC with many new partners on many new problems. We introduce two\nJax-based, procedural generators that create billions of solvable coordination\nchallenges. We develop a new paradigm called Cross-Environment Cooperation\n(CEC), and show that it outperforms competitive baselines quantitatively and\nqualitatively when collaborating with real people. Our findings suggest that\nlearning to collaborate across many unique scenarios encourages agents to\ndevelop general norms, which prove effective for collaboration with different\npartners. Together, our results suggest a new route toward designing generalist\ncooperative agents capable of interacting with humans without requiring human\ndata.",
      "tldr_zh": "该论文探讨了Zero-shot coordination (ZSC)，即AI在未见过的伙伴下进行合作的能力，强调现有模型在单一任务训练后无法泛化到新任务的问题。作者提出Cross-Environment Cooperation (CEC)范式，通过reinforcement learning在多种环境分布上训练代理，结合两个Jax-based程序生成器创建数十亿协调挑战，以培养通用的合作技能。实验结果显示，CEC在与真实人的合作中定量和定性地优于竞争基线，并证明跨环境学习能促进通用规范的形成，从而无需人类数据即可实现多代理协调。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted to CogSci 2025, In-review for ICML 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12714v2",
      "published_date": "2025-04-17 07:41:25 UTC",
      "updated_date": "2025-04-20 20:10:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:11:35.774436"
    },
    {
      "arxiv_id": "2504.12711v2",
      "title": "NTIRE 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Li",
        "Yeying Jin",
        "Xin Jin",
        "Zongwei Wu",
        "Bingchen Li",
        "Yufei Wang",
        "Wenhan Yang",
        "Yu Li",
        "Zhibo Chen",
        "Bihan Wen",
        "Robby T. Tan",
        "Radu Timofte",
        "Qiyu Rong",
        "Hongyuan Jing",
        "Mengmeng Zhang",
        "Jinglong Li",
        "Xiangyu Lu",
        "Yi Ren",
        "Yuting Liu",
        "Meng Zhang",
        "Xiang Chen",
        "Qiyuan Guan",
        "Jiangxin Dong",
        "Jinshan Pan",
        "Conglin Gou",
        "Qirui Yang",
        "Fangpu Zhang",
        "Yunlong Lin",
        "Sixiang Chen",
        "Guoxi Huang",
        "Ruirui Lin",
        "Yan Zhang",
        "Jingyu Yang",
        "Huanjing Yue",
        "Jiyuan Chen",
        "Qiaosi Yi",
        "Hongjun Wang",
        "Chenxi Xie",
        "Shuai Li",
        "Yuhui Wu",
        "Kaiyi Ma",
        "Jiakui Hu",
        "Juncheng Li",
        "Liwen Pan",
        "Guangwei Gao",
        "Wenjie Li",
        "Zhenyu Jin",
        "Heng Guo",
        "Zhanyu Ma",
        "Yubo Wang",
        "Jinghua Wang",
        "Wangzhi Xing",
        "Anjusree Karnavar",
        "Diqi Chen",
        "Mohammad Aminul Islam",
        "Hao Yang",
        "Ruikun Zhang",
        "Liyuan Pan",
        "Qianhao Luo",
        "XinCao",
        "Han Zhou",
        "Yan Min",
        "Wei Dong",
        "Jun Chen",
        "Taoyi Wu",
        "Weijia Dou",
        "Yu Wang",
        "Shengjie Zhao",
        "Yongcheng Huang",
        "Xingyu Han",
        "Anyan Huang",
        "Hongtao Wu",
        "Hong Wang",
        "Yefeng Zheng",
        "Abhijeet Kumar",
        "Aman Kumar",
        "Marcos V. Conde",
        "Paula Garrido",
        "Daniel Feijoo",
        "Juan C. Benito",
        "Guanglu Dong",
        "Xin Lin",
        "Siyuan Liu",
        "Tianheng Zheng",
        "Jiayu Zhong",
        "Shouyi Wang",
        "Xiangtai Li",
        "Lanqing Guo",
        "Lu Qi",
        "Chao Ren",
        "Shuaibo Wang",
        "Shilong Zhang",
        "Wanyu Zhou",
        "Yunze Wu",
        "Qinzhong Tan",
        "Jieyuan Pei",
        "Zhuoxuan Li",
        "Jiayu Wang",
        "Haoyu Bian",
        "Haoran Sun",
        "Subhajit Paul",
        "Ni Tang",
        "Junhao Huang",
        "Zihan Cheng",
        "Hongyun Zhu",
        "Yuehan Wu",
        "Kaixin Deng",
        "Hang Ouyang",
        "Tianxin Xiao",
        "Fan Yang",
        "Zhizun Luo",
        "Zeyu Xiao",
        "Zhuoyuan Li",
        "Nguyen Pham Hoang Le",
        "An Dinh Thien",
        "Son T. Luu",
        "Kiet Van Nguyen",
        "Ronghua Xu",
        "Xianmin Tian",
        "Weijian Zhou",
        "Jiacheng Zhang",
        "Yuqian Chen",
        "Yihang Duan",
        "Yujie Wu",
        "Suresh Raikwar",
        "Arsh Garg",
        "Kritika",
        "Jianhua Zheng",
        "Xiaoshan Ma",
        "Ruolin Zhao",
        "Yongyu Yang",
        "Yongsheng Liang",
        "Guiming Huang",
        "Qiang Li",
        "Hongbin Zhang",
        "Xiangyu Zheng",
        "A. N. Rajagopalan"
      ],
      "abstract": "This paper reviews the NTIRE 2025 Challenge on Day and Night Raindrop Removal\nfor Dual-Focused Images. This challenge received a wide range of impressive\nsolutions, which are developed and evaluated using our collected real-world\nRaindrop Clarity dataset. Unlike existing deraining datasets, our Raindrop\nClarity dataset is more diverse and challenging in degradation types and\ncontents, which includes day raindrop-focused, day background-focused, night\nraindrop-focused, and night background-focused degradations. This dataset is\ndivided into three subsets for competition: 14,139 images for training, 240\nimages for validation, and 731 images for testing. The primary objective of\nthis challenge is to establish a new and powerful benchmark for the task of\nremoving raindrops under varying lighting and focus conditions. There are a\ntotal of 361 participants in the competition, and 32 teams submitting valid\nsolutions and fact sheets for the final testing phase. These submissions\nachieved state-of-the-art (SOTA) performance on the Raindrop Clarity dataset.\nThe project can be found at\nhttps://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/.",
      "tldr_zh": "本论文回顾了 NTIRE 2025 挑战赛，该赛事聚焦于日夜雨滴去除技术，针对双焦点图像（dual-focused images）建立了一个新的基准。挑战赛基于新收集的 Raindrop Clarity 数据集，该数据集包含多样化的降级类型，如日雨滴聚焦、日背景聚焦、夜雨滴聚焦和夜背景聚焦图像，总计分为 14,139 张训练图像、240 张验证图像和 731 张测试图像。共有 361 名参与者，32 支队伍提交了有效解决方案，这些方案在 Raindrop Clarity 数据集上达到了 state-of-the-art (SOTA) 性能，为雨滴去除任务在不同光照和焦点条件下的研究提供了强大基准。项目详情可查阅 https://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Challenge Report of CVPR NTIRE 2025; 26 pages; Methods from 32 teams",
      "pdf_url": "http://arxiv.org/pdf/2504.12711v2",
      "published_date": "2025-04-17 07:35:35 UTC",
      "updated_date": "2025-04-19 05:26:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:11:47.778714"
    },
    {
      "arxiv_id": "2504.17799v1",
      "title": "Subfunction Structure Matters: A New Perspective on Local Optima Networks",
      "title_zh": "子函数结构很重要：关于局部最优网络的一个新视角",
      "authors": [
        "S. L. Thomson",
        "M. W. Przewozniczek"
      ],
      "abstract": "Local optima networks (LONs) capture fitness landscape information. They are\ntypically constructed in a black-box manner; information about the problem\nstructure is not utilised. This also applies to the analysis of LONs: knowledge\nabout the problem, such as interaction between variables, is not considered. We\nchallenge this status-quo with an alternative approach: we consider how LON\nanalysis can be improved by incorporating subfunction-based information - this\ncan either be known a-priori or learned during search. To this end, LONs are\nconstructed for several benchmark pseudo-boolean problems using three\napproaches: firstly, the standard algorithm; a second algorithm which uses\ndeterministic grey-box crossover; and a third algorithm which selects\nperturbations based on learned information about variable interactions. Metrics\nrelated to subfunction changes in a LON are proposed and compared with metrics\nfrom previous literature which capture other aspects of a LON. Incorporating\nproblem structure in LON construction and analysing it can bring enriched\ninsight into optimisation dynamics. Such information may be crucial to\nunderstanding the difficulty of solving a given problem with state-of-the-art\nlinkage learning optimisers. In light of the results, we suggest incorporation\nof problem structure as an alternative paradigm in landscape analysis for\nproblems with known or suspected subfunction structure.",
      "tldr_zh": "本研究挑战了传统 Local Optima Networks (LONs) 的黑盒构建方法，提出通过整合子函数（subfunction）结构信息来提升 LONs 的分析和构建，从而更好地理解优化问题的动态。研究者使用三种方法构建 LONs：标准算法、基于确定性灰盒交叉（grey-box crossover）的算法，以及利用学习变量交互信息的算法。实验结果显示，新提出的子函数相关指标比传统指标更能揭示优化过程的关键方面，并证明了问题结构对评估优化难度的重要性。该方法为具有已知或可疑子函数结构的问题提供了新的景观分析范式，有助于改进 state-of-the-art linkage learning optimisers。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.17799v1",
      "published_date": "2025-04-17 07:31:11 UTC",
      "updated_date": "2025-04-17 07:31:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:11:59.109023"
    },
    {
      "arxiv_id": "2504.13219v1",
      "title": "Scaling Laws for Data-Efficient Visual Transfer Learning",
      "title_zh": "数据高效视觉迁移学习的缩放定律",
      "authors": [
        "Wenxuan Yang",
        "Qingqu Wei",
        "Chenxi Ma",
        "Weimin Tan",
        "Bo Yan"
      ],
      "abstract": "Current scaling laws for visual AI models focus predominantly on large-scale\npretraining, leaving a critical gap in understanding how performance scales for\ndata-constrained downstream tasks. To address this limitation, this paper\nestablishes the first practical framework for data-efficient scaling laws in\nvisual transfer learning, addressing two fundamental questions: 1) How do\nscaling behaviors shift when downstream tasks operate with limited data? 2)\nWhat governs the efficacy of knowledge distillation under such constraints?\nThrough systematic analysis of vision tasks across data regimes (1K-1M\nsamples), we propose the distillation boundary theory, revealing a critical\nturning point in distillation efficiency: 1) Distillation superiority: In\ndata-scarce conditions, distilled models significantly outperform their\nnon-distillation counterparts, efficiently leveraging inherited knowledge to\ncompensate for limited training samples. 2) Pre-training dominance: As\npre-training data increases beyond a critical threshold, non-distilled models\ngradually surpass distilled versions, suggesting diminishing returns from\nknowledge inheritance when sufficient task-specific data becomes available.\nEmpirical validation across various model scales (2.5M to 38M parameters) and\ndata volumes demonstrate these performance inflection points, with error\ndifference curves transitioning from positive to negative values at critical\ndata thresholds, confirming our theoretical predictions. This work redefines\nscaling laws for data-limited regimes, bridging the knowledge gap between\nlarge-scale pretraining and practical downstream adaptation, addressing a\ncritical barrier to understanding vision model scaling behaviors and optimizing\ncomputational resource allocation.",
      "tldr_zh": "这篇论文建立了首个针对数据高效视觉迁移学习的缩放定律（scaling laws）框架，探讨了下游任务数据有限时模型性能如何变化，以及知识蒸馏（knowledge distillation）的效能。论文提出了蒸馏边界理论（distillation boundary theory），揭示了数据规模的关键转折点：在数据稀缺（1K-1M 样本）条件下，蒸馏模型通过继承知识显著优于非蒸馏模型；但当预训练数据超过阈值时，非蒸馏模型开始主导，显示出知识继承回报的递减。实验在不同模型规模（2.5M 到 38M 参数）上验证了这些性能拐点，重新定义了数据有限情境下的视觉模型缩放行为，并优化了计算资源分配。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13219v1",
      "published_date": "2025-04-17 07:01:01 UTC",
      "updated_date": "2025-04-17 07:01:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:12:13.062198"
    },
    {
      "arxiv_id": "2504.13218v1",
      "title": "Harmony: A Unified Framework for Modality Incremental Learning",
      "title_zh": "Harmony：模态增量学习的统一框架",
      "authors": [
        "Yaguang Song",
        "Xiaoshan Yang",
        "Dongmei Jiang",
        "Yaowei Wang",
        "Changsheng Xu"
      ],
      "abstract": "Incremental learning aims to enable models to continuously acquire knowledge\nfrom evolving data streams while preserving previously learned capabilities.\nWhile current research predominantly focuses on unimodal incremental learning\nand multimodal incremental learning where the modalities are consistent,\nreal-world scenarios often present data from entirely new modalities, posing\nadditional challenges. This paper investigates the feasibility of developing a\nunified model capable of incremental learning across continuously evolving\nmodal sequences. To this end, we introduce a novel paradigm called Modality\nIncremental Learning (MIL), where each learning stage involves data from\ndistinct modalities. To address this task, we propose a novel framework named\nHarmony, designed to achieve modal alignment and knowledge retention, enabling\nthe model to reduce the modal discrepancy and learn from a sequence of distinct\nmodalities, ultimately completing tasks across multiple modalities within a\nunified framework. Our approach introduces the adaptive compatible feature\nmodulation and cumulative modal bridging. Through constructing historical modal\nfeatures and performing modal knowledge accumulation and alignment, the\nproposed components collaboratively bridge modal differences and maintain\nknowledge retention, even with solely unimodal data available at each learning\nphase.These components work in concert to establish effective modality\nconnections and maintain knowledge retention, even when only unimodal data is\navailable at each learning stage. Extensive experiments on the MIL task\ndemonstrate that our proposed method significantly outperforms existing\nincremental learning methods, validating its effectiveness in MIL scenarios.",
      "tldr_zh": "该论文引入了 Modality Incremental Learning (MIL) 范式，旨在让模型从不断演变的模态序列中持续学习，解决现实场景中新模态数据带来的挑战。Harmony 框架通过 adaptive compatible feature modulation 和 cumulative modal bridging 组件，实现模态对齐、知识积累和保留，即使每个学习阶段仅提供单模态数据。实验结果表明，该方法在 MIL 任务上显著优于现有增量学习方法，证明了其在多模态场景中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13218v1",
      "published_date": "2025-04-17 06:35:01 UTC",
      "updated_date": "2025-04-17 06:35:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:12:23.235438"
    },
    {
      "arxiv_id": "2504.12682v1",
      "title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents",
      "title_zh": "WebLists：使用可执行 LLM 代理从复杂交互式网站提取结构化信息",
      "authors": [
        "Arth Bohra",
        "Manvel Saroyan",
        "Danil Melkozerov",
        "Vahe Karufanyan",
        "Gabriel Maher",
        "Pascal Weinberger",
        "Artem Harutyunyan",
        "Giovanni Campagna"
      ],
      "abstract": "Most recent web agent research has focused on navigation and transaction\ntasks, with little emphasis on extracting structured data at scale. We present\nWebLists, a benchmark of 200 data-extraction tasks across four common business\nand enterprise use-cases. Each task requires an agent to navigate to a webpage,\nconfigure it appropriately, and extract complete datasets with well-defined\nschemas. We show that both LLMs with search capabilities and SOTA web agents\nstruggle with these tasks, with a recall of 3% and 31%, respectively, despite\nhigher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that\nenables web agents to convert their execution into repeatable programs, and\nreplay them at scale across pages with similar structure. BardeenAgent is also\nthe first LLM agent to take advantage of the regular structure of HTML. In\nparticular BardeenAgent constructs a generalizable CSS selector to capture all\nrelevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more\nthan doubling the performance of SOTA web agents, and reducing cost per output\nrow by 3x.",
      "tldr_zh": "本论文引入 WebLists 基准，该基准包含 200 个数据提取任务，针对四个常见商业和企业用例，要求代理导航网页、配置页面并提取结构化数据集，但现有 LLM 和 SOTA 网页代理在这些任务上的召回率仅为 3% 和 31%。为解决这一挑战，提出 BardeenAgent 框架，该框架将代理执行转换为可重复程序，并首次利用 HTML 的结构构建通用 CSS 选择器来捕获和提取相关数据。实验结果显示，BardeenAgent 在 WebLists 基准上达到 66% 的召回率，比 SOTA 代理性能提升一倍以上，同时将每输出行的成本降低 3 倍。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12682v1",
      "published_date": "2025-04-17 06:16:40 UTC",
      "updated_date": "2025-04-17 06:16:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:12:37.251403"
    },
    {
      "arxiv_id": "2504.12681v1",
      "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
      "title_zh": "GRAIL：基于梯度的自适应遗忘，用于大语言模型中的隐私和版权",
      "authors": [
        "Kun-Woo Kim",
        "Ji-Hoon Park",
        "Ju-Min Han",
        "Seong-Whan Lee"
      ],
      "abstract": "Large Language Models (LLMs) trained on extensive datasets often learn\nsensitive information, which raises significant social and legal concerns under\nprinciples such as the \"Right to be forgotten.\" Retraining entire models from\nscratch to remove undesired information is both costly and impractical.\nFurthermore, existing single-domain unlearning methods fail to address\nmulti-domain scenarios, where knowledge is interwoven across domains such as\nprivacy and copyright, creating overlapping representations that lead to\nexcessive knowledge removal or degraded performance. To tackle these issues, we\npropose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain\nunlearning framework. GRAIL leverages gradient information from multiple\ndomains to precisely distinguish the unlearning scope from the retention scope,\nand applies an adaptive parameter-wise localization strategy to selectively\nremove targeted knowledge while preserving critical parameters for each domain.\nExperimental results on unlearning benchmarks show that GRAIL achieves\nunlearning success on par with the existing approaches, while also\ndemonstrating up to 17% stronger knowledge retention success compared to the\nprevious state-of-art method. Our findings establish a new paradigm for\neffectively managing and regulating sensitive information in large-scale\npre-trained language models.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 在训练中学习敏感信息的问题，提出 GRAIL（Gradient-Based Adaptive Unlearning）框架，以应对隐私和版权领域的多域取消学习挑战。GRAIL 通过利用多域梯度信息精确区分需要取消的知识范围和需要保留的范围，并采用自适应参数级别的本地化策略，选择性地移除目标知识，同时保持模型性能。实验结果表明，GRAIL 的取消学习成功率与现有方法相当，但知识保留成功率提高了多达 17%，为有效管理 LLMs 中的敏感信息建立了新范式。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by IJCNN 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12681v1",
      "published_date": "2025-04-17 06:16:32 UTC",
      "updated_date": "2025-04-17 06:16:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:12:48.691528"
    },
    {
      "arxiv_id": "2504.12680v1",
      "title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning",
      "title_zh": "Embodied-R：用于通过强化学习激活基础模型中具身空间推理的协作框架",
      "authors": [
        "Baining Zhao",
        "Ziyou Wang",
        "Jianjie Fang",
        "Chen Gao",
        "Fanhang Man",
        "Jinqiang Cui",
        "Xin Wang",
        "Xinlei Chen",
        "Yong Li",
        "Wenwu Zhu"
      ],
      "abstract": "Humans can perceive and reason about spatial relationships from sequential\nvisual observations, such as egocentric video streams. However, how pretrained\nmodels acquire such abilities, especially high-level reasoning, remains\nunclear. This paper introduces Embodied-R, a collaborative framework combining\nlarge-scale Vision-Language Models (VLMs) for perception and small-scale\nLanguage Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a\nnovel reward system considering think-answer logical consistency, the model\nachieves slow-thinking capabilities with limited computational resources. After\ntraining on only 5k embodied video samples, Embodied-R with a 3B LM matches\nstate-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on\nboth in-distribution and out-of-distribution embodied spatial reasoning tasks.\nEmbodied-R also exhibits emergent thinking patterns such as systematic analysis\nand contextual integration. We further explore research questions including\nresponse length, training on VLM, strategies for reward design, and differences\nin model generalization after SFT (Supervised Fine-Tuning) and RL training.",
      "tldr_zh": "本文提出 Embodied-R 框架，这是一个协作系统，结合大型 Vision-Language Models (VLMs) 用于感知和小型 Language Models (LMs) 用于推理，通过 Reinforcement Learning (RL) 激活基础模型中的 embodied 空间推理能力。框架采用一个新颖的奖励系统，强调思考-回答的逻辑一致性，仅用 5k 个 embodied 视频样本训练后，使用 3B LM 的 Embodied-R 在分布内和分布外空间推理任务上，性能与最先进模型（如 OpenAI-o1 和 Gemini-2.5-pro）相当。实验还观察到模型展现出新兴思考模式，包括系统分析和上下文整合。该研究进一步探讨了响应长度、在 VLM 上训练、奖励设计策略以及 Supervised Fine-Tuning (SFT) 与 RL 训练后模型泛化的差异。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12680v1",
      "published_date": "2025-04-17 06:16:11 UTC",
      "updated_date": "2025-04-17 06:16:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:13:02.951207"
    },
    {
      "arxiv_id": "2504.12673v1",
      "title": "ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Singon Kim",
        "Gunho Jung",
        "Seong-Whan Lee"
      ],
      "abstract": "Abstractive compression utilizes smaller langauge models to condense\nquery-relevant context, reducing computational costs in retrieval-augmented\ngeneration (RAG). However,retrieved documents often include information that is\neither irrelevant to answering the query or misleading due to factual incorrect\ncontent, despite having high relevance scores. This behavior indicates that\nabstractive compressors are more likely to omit important information essential\nfor the correct answer, especially in long contexts where attention dispersion\noccurs. To address this issue, we categorize retrieved documents in a more\nfine-grained manner and propose Abstractive Compression Robust against Noise\n(ACoRN), which introduces two novel training steps. First, we use offline data\naugmentation on the training dataset to enhance compressor robustness against\ntwo distinct types of retrieval noise. Second, since the language modelbased\ncompressor cannot fully utilize information from multiple retrieved documents\nand exhibits positional bias, we perform finetuning to generate summaries\ncentered around key information that directly supports the correct answer. Our\nexperiments demonstrate that T5-large, trained with ACoRN as a compressor,\nimproves EM and F1 scores while preserving the answer string, which could serve\nas direct evidence. ACoRN excels on datasets with many accuracy-reducing\ndocuments, making it highly useful in real-world scenarios.",
      "tldr_zh": "这篇论文提出ACoRN（Abstractive Compression Robust against Noise）框架，以提升检索增强生成（RAG）中的抽象压缩鲁棒性，针对检索文档中无关或误导信息导致的关键信息遗漏问题。ACoRN通过离线数据增强来强化压缩器对两种检索噪声的抵抗力，并进行微调以生成以关键信息为中心的摘要，缓解多文档位置偏差。实验结果显示，使用ACoRN训练的T5-large模型在EM和F1分数上显著提升，并在包含大量降低准确性的文档的数据集上表现出色，适用于真实世界场景。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12673v1",
      "published_date": "2025-04-17 06:05:35 UTC",
      "updated_date": "2025-04-17 06:05:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:13:12.001348"
    },
    {
      "arxiv_id": "2504.12672v1",
      "title": "Post-processing improves accuracy of Artificial Intelligence weather forecasts",
      "title_zh": "后处理提高了人工智能天气预报的准确性",
      "authors": [
        "Belinda Trotta",
        "Robert Johnson",
        "Catherine de Burgh-Day",
        "Debra Hudson",
        "Esteban Abellan",
        "James Canvin",
        "Andrew Kelly",
        "Daniel Mentiplay",
        "Benjamin Owen",
        "Jennifer Whelan"
      ],
      "abstract": "Artificial Intelligence (AI) weather models are now reaching\noperational-grade performance for some variables, but like traditional\nNumerical Weather Prediction (NWP) models, they exhibit systematic biases and\nreliability issues. We test the application of the Bureau of Meteorology's\nexisting statistical post-processing system, IMPROVER, to ECMWF's deterministic\nArtificial Intelligence Forecasting System (AIFS), and compare results against\npost-processed outputs from the ECMWF HRES and ENS models. Without any\nmodification to configuration or processing workflows, post-processing yields\ncomparable accuracy improvements for AIFS as for traditional NWP forecasts, in\nboth expected value and probabilistic outputs. We show that blending AIFS with\nNWP models improves overall forecast skill, even when AIFS alone is not the\nmost accurate component. These findings show that statistical post-processing\nmethods developed for NWP are directly applicable to AI models, enabling\nnational meteorological centres to incorporate AI forecasts into existing\nworkflows in a low-risk, incremental fashion.",
      "tldr_zh": "本研究测试了将 Bureau of Meteorology 的 IMPROVER 后处理系统应用于 ECMWF 的 AIFS 模型，以解决 AI 天气预测中的系统偏差和可靠性问题。结果显示，后处理使 AIFS 的预期值和概率输出准确性得到与传统 NWP 模型相当的改善；此外，将 AIFS 与 NWP 模型混合能进一步提升整体预测技能，即使 AIFS 单独使用并非最优。研究证明，现有的统计后处理方法可直接适用于 AI 模型，允许国家气象中心以低风险方式逐步整合 AI 预测。",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12672v1",
      "published_date": "2025-04-17 06:05:10 UTC",
      "updated_date": "2025-04-17 06:05:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:13:24.865805"
    },
    {
      "arxiv_id": "2505.03750v2",
      "title": "AI-Powered Agile Analog Circuit Design and Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Jinhai Hu",
        "Wang Ling Goh",
        "Yuan Gao"
      ],
      "abstract": "Artificial intelligence (AI) techniques are transforming analog circuit\ndesign by automating device-level tuning and enabling system-level\nco-optimization. This paper integrates two approaches: (1) AI-assisted\ntransistor sizing using Multi-Objective Bayesian Optimization (MOBO) for direct\ncircuit parameter optimization, demonstrated on a linearly tunable\ntransconductor; and (2) AI-integrated circuit transfer function modeling for\nsystem-level optimization in a keyword spotting (KWS) application, demonstrated\nby optimizing an analog bandpass filter within a machine learning training\nloop. The combined insights highlight how AI can improve analog performance,\nreduce design iteration effort, and jointly optimize analog components and\napplication-level metrics.",
      "tldr_zh": "这篇论文探讨了 AI 在模拟电路设计中的应用，通过整合两种方法实现敏捷优化和性能提升。首先生成方法采用 Multi-Objective Bayesian Optimization (MOBO) 进行 AI 辅助晶体管尺寸调整，直接优化电路参数，并在线性可调 transconductor 上进行演示。第二种方法则通过 AI 集成电路传输函数建模，实现系统级优化，例如在 keyword spotting (KWS) 应用中优化 analog bandpass filter，并将其整合到机器学习训练循环中。总体结果显示，AI 技术能显著改善模拟性能、减少设计迭代努力，并实现模拟组件与应用级指标的联合优化。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "3 pages, 5 figures, AI4X, 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.03750v2",
      "published_date": "2025-04-17 05:56:14 UTC",
      "updated_date": "2025-05-08 06:08:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:13:37.228220"
    },
    {
      "arxiv_id": "2504.12663v1",
      "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaotian Zhang",
        "Ruizhe Chen",
        "Yang Feng",
        "Zuozhu Liu"
      ],
      "abstract": "Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment.",
      "tldr_zh": "该研究针对大型语言模型（Large Language Models）的个性化对齐问题，提出Persona-judge框架，以解决现有方法依赖奖励信号和额外标注数据的局限性。该框架采用无训练（training-free）的判别范式，通过token-level的自判断机制：一个草案模型（draft model）基于给定偏好生成候选tokens，而另一个判断模型（judge model）则根据不同偏好交叉验证这些tokens的接受性。实验结果表明，Persona-judge利用模型内在的偏好评估机制，提供了一种可扩展、高效的解决方案，推动更具适应性的个性化对齐。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12663v1",
      "published_date": "2025-04-17 05:50:13 UTC",
      "updated_date": "2025-04-17 05:50:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:13:47.341319"
    },
    {
      "arxiv_id": "2504.15299v1",
      "title": "D$^{2}$MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving",
      "title_zh": "翻译失败",
      "authors": [
        "Haodong Wang",
        "Qihua Zhou",
        "Zicong Hong",
        "Song Guo"
      ],
      "abstract": "The mixture of experts (MoE) model is a sparse variant of large language\nmodels (LLMs), designed to hold a better balance between intelligent capability\nand computational overhead. Despite its benefits, MoE is still too expensive to\ndeploy on resource-constrained edge devices, especially with the demands of\non-device inference services. Recent research efforts often apply model\ncompression techniques, such as quantization, pruning and merging, to restrict\nMoE complexity. Unfortunately, due to their predefined static model\noptimization strategies, they cannot always achieve the desired\nquality-overhead trade-off when handling multiple requests, finally degrading\nthe on-device quality of service. These limitations motivate us to propose the\nD$^2$MoE, an algorithm-system co-design framework that matches diverse task\nrequirements by dynamically allocating the most proper bit-width to each\nexpert. Specifically, inspired by the nested structure of matryoshka dolls, we\npropose the matryoshka weight quantization (MWQ) to progressively compress\nexpert weights in a bit-nested manner and reduce the required runtime memory.\nOn top of it, we further optimize the I/O-computation pipeline and design a\nheuristic scheduling algorithm following our hottest-expert-bit-first (HEBF)\nprinciple, which maximizes the expert parallelism between I/O and computation\nqueue under constrained memory budgets, thus significantly reducing the idle\ntemporal bubbles waiting for the experts to load. Evaluations on real edge\ndevices show that D$^2$MoE improves the overall inference throughput by up to\n1.39$\\times$ and reduces the peak memory footprint by up to 53% over the latest\non-device inference frameworks, while still preserving comparable serving\naccuracy as its INT8 counterparts.",
      "tldr_zh": "本研究提出 D²MoE，一种算法-系统联合设计框架，旨在优化 Mixture of Experts (MoE) 模型在资源受限边缘设备上的部署，以实现高效的 on-device LLM 推理服务。该框架通过 matryoshka weight quantization (MWQ) 技术以嵌套方式动态压缩专家权重，并结合 hottest-expert-bit-first (HEBF) 启发式调度算法，优化 I/O-计算管道以最大化专家并行性和减少内存占用。在真实边缘设备评估中，D²MoE 相比现有框架提高推理吞吐量最多 1.39 倍，减少峰值内存占用最多 53%，同时保持与 INT8 模型相当的准确性。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted by MobiCom 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.15299v1",
      "published_date": "2025-04-17 05:37:35 UTC",
      "updated_date": "2025-04-17 05:37:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:14:01.837011"
    },
    {
      "arxiv_id": "2504.12644v1",
      "title": "Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification",
      "title_zh": "基于",
      "authors": [
        "Reek Majumder",
        "Mashrur Chowdhury",
        "Sakib Mahmud Khan",
        "Zadid Khan",
        "Fahim Ahmad",
        "Frank Ngeni",
        "Gurcan Comert",
        "Judith Mwakalonge",
        "Dimitra Michalaka"
      ],
      "abstract": "Deep learning (DL)-based image classification models are essential for\nautonomous vehicle (AV) perception modules since incorrect categorization might\nhave severe repercussions. Adversarial attacks are widely studied cyberattacks\nthat can lead DL models to predict inaccurate output, such as incorrectly\nclassified traffic signs by the perception module of an autonomous vehicle. In\nthis study, we create and compare hybrid classical-quantum deep learning\n(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate\nrobustness against adversarial attacks for perception modules. Before feeding\nthem into the quantum system, we used transfer learning models, alexnet and\nvgg-16, as feature extractors. We tested over 1000 quantum circuits in our\nHCQ-DL models for projected gradient descent (PGD), fast gradient sign attack\n(FGSA), and gradient attack (GA), which are three well-known untargeted\nadversarial approaches. We evaluated the performance of all models during\nadversarial attacks and no-attack scenarios. Our HCQ-DL models maintain\naccuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA\nattacks, which is higher than C-DL models. During the PGD attack, our\nalexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL\nmodels that achieved accuracies below 21\\%. Our results highlight that the\nHCQ-DL models provide improved accuracy for traffic sign classification under\nadversarial settings compared to their classical counterparts.",
      "tldr_zh": "本研究开发了混合经典-量子深度学习（HCQ-DL）模型，以提升自动驾驶车辆（Autonomous Vehicle）感知模块在交通标志分类中的抗对抗攻击（Adversarial Attack）能力，相比传统经典深度学习（C-DL）模型表现出显著鲁棒性。研究采用AlexNet和VGG-16作为特征提取器，将提取到的特征输入量子系统，并测试超过1000个量子电路以应对Projected Gradient Descent (PGD)、Fast Gradient Sign Attack (FGSA)和Gradient Attack (GA)等三种常见攻击。实验结果显示，在无攻击场景下，HCQ-DL模型准确率超过95%；在攻击场景下，其准确率保持在85%以上（如PGD攻击中AlexNet-based HCQ-DL达85%），远高于C-DL模型的低于21%。这项工作证明了量子计算支持的HCQ-DL模型在对抗攻击下可显著提高交通标志分类的准确性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12644v1",
      "published_date": "2025-04-17 05:08:08 UTC",
      "updated_date": "2025-04-17 05:08:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:14:14.196524"
    },
    {
      "arxiv_id": "2504.12637v1",
      "title": "Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Linda He",
        "Jue Wang",
        "Maurice Weber",
        "Shang Zhu",
        "Ben Athiwaratkun",
        "Ce Zhang"
      ],
      "abstract": "Large Language Models (LLMs) struggle with long-context reasoning, not only\ndue to the quadratic scaling of computational complexity with sequence length\nbut also because of the scarcity and expense of annotating long-context data.\nThere has been barely any open-source work that systematically ablates\nlong-context data, nor is there any openly available instruction tuning dataset\nwith contexts surpassing 100K tokens. To bridge this gap, we introduce a novel\npost-training synthetic data generation strategy designed to efficiently extend\nthe context window of LLMs while preserving their general task performance. Our\napproach scalably extends to arbitrarily long context lengths, unconstrained by\nthe length of available real-world data, which effectively addresses the\nscarcity of raw long-context data. Through a step-by-step rotary position\nembedding (RoPE) scaling training strategy, we demonstrate that our model, with\na context length of up to 1M tokens, performs well on the RULER benchmark and\nInfiniteBench and maintains robust performance on general language tasks.",
      "tldr_zh": "该论文解决大型语言模型(LLMs)在处理百万级上下文时面临的计算复杂性和数据稀缺问题，通过引入一种分层合成数据生成策略来扩展模型的上下文窗口，同时保持一般任务性能。  \n该策略不受真实数据长度的限制，能够生成任意长度的合成数据，并结合逐步的旋转位置嵌入(RoPE)缩放训练方法，实现对长上下文的高效训练。  \n实验结果表明，训练后的模型支持高达1M tokens的上下文长度，在 RULER benchmark 和 InfiniteBench 上表现出色，并维持了稳健的语言任务表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12637v1",
      "published_date": "2025-04-17 04:46:57 UTC",
      "updated_date": "2025-04-17 04:46:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:14:25.430036"
    },
    {
      "arxiv_id": "2504.13217v2",
      "title": "Sustainability via LLM Right-sizing",
      "title_zh": "翻译失败",
      "authors": [
        "Jennifer Haase",
        "Finn Klessascheck",
        "Jan Mendling",
        "Sebastian Pokutta"
      ],
      "abstract": "Large language models (LLMs) have become increasingly embedded in\norganizational workflows. This has raised concerns over their energy\nconsumption, financial costs, and data sovereignty. While performance\nbenchmarks often celebrate cutting-edge models, real-world deployment decisions\nrequire a broader perspective: when is a smaller, locally deployable model\n\"good enough\"? This study offers an empirical answer by evaluating eleven\nproprietary and open-weight LLMs across ten everyday occupational tasks,\nincluding summarizing texts, generating schedules, and drafting emails and\nproposals. Using a dual-LLM-based evaluation framework, we automated task\nexecution and standardized evaluation across ten criteria related to output\nquality, factual accuracy, and ethical responsibility. Results show that GPT-4o\ndelivers consistently superior performance but at a significantly higher cost\nand environmental footprint. Notably, smaller models like Gemma-3 and Phi-4\nachieved strong and reliable results on most tasks, suggesting their viability\nin contexts requiring cost-efficiency, local deployment, or privacy. A cluster\nanalysis revealed three model groups -- premium all-rounders, competent\ngeneralists, and limited but safe performers -- highlighting trade-offs between\nquality, control, and sustainability. Significantly, task type influenced model\neffectiveness: conceptual tasks challenged most models, while aggregation and\ntransformation tasks yielded better performances. We argue for a shift from\nperformance-maximizing benchmarks to task- and context-aware sufficiency\nassessments that better reflect organizational priorities. Our approach\ncontributes a scalable method to evaluate AI models through a sustainability\nlens and offers actionable guidance for responsible LLM deployment in practice.",
      "tldr_zh": "这篇论文探讨了通过调整大型语言模型（LLMs）大小来提升可持续性的策略，评估了11个专有和开源LLMs在10个日常职业任务（如文本总结、生成日程和起草邮件）上的表现。研究采用双-LLM框架进行自动化任务执行和标准化评估，涵盖输出质量、事实准确性和道德责任等10个标准，结果显示GPT-4o虽表现出色，但其高成本和环境影响显著；相反，较小模型如Gemma-3和Phi-4在大多数任务中实现了可靠性能，特别适合需要成本效率、本地部署或隐私保护的场景。最终，论文通过聚类分析揭示模型间的权衡（高端全能型、competent generalists和limited but safe performers），并提出从性能最大化转向任务导向的充分性评估方法，以指导可持续的LLM部署。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 2 Figures, 6 Tables",
      "pdf_url": "http://arxiv.org/pdf/2504.13217v2",
      "published_date": "2025-04-17 04:00:40 UTC",
      "updated_date": "2025-04-23 03:43:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:14:39.906828"
    },
    {
      "arxiv_id": "2504.12612v1",
      "title": "The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance",
      "title_zh": "翻译失败",
      "authors": [
        "Ching-Chun Chang",
        "Isao Echizen"
      ],
      "abstract": "Provenance is the chronology of things, resonating with the fundamental\npursuit to uncover origins, trace connections, and situate entities within the\nflow of space and time. As artificial intelligence advances towards autonomous\nagents capable of interactive collaboration on complex tasks, the provenance of\ngenerated content becomes entangled in the interplay of collective creation,\nwhere contributions are continuously revised, extended or overwritten. In a\nmulti-agent generative chain, content undergoes successive transformations,\noften leaving little, if any, trace of prior contributions. In this study, we\ninvestigates the problem of tracking multi-agent provenance across the temporal\ndimension of generation. We propose a chronological system for post hoc\nattribution of generative history from content alone, without reliance on\ninternal memory states or external meta-information. At its core lies the\nnotion of symbolic chronicles, representing signed and time-stamped records, in\na form analogous to the chain of custody in forensic science. The system\noperates through a feedback loop, whereby each generative timestep updates the\nchronicle of prior interactions and synchronises it with the synthetic content\nin the very act of generation. This research seeks to develop an accountable\nform of collaborative artificial intelligence within evolving cyber ecosystems.",
      "tldr_zh": "这篇论文探讨了在多智能体生成系统中追踪内容来源（Provenance）的挑战，特别是在内容不断被修改和重写的协作过程中。研究提出了一种基于symbolic chronicles的系统，通过反馈循环在每个生成步骤中更新时间戳记录和同步合成内容，实现事后归因，而无需依赖内部记忆状态或外部元信息。最终，该方法为演变中的网络生态中开发可追溯和负责任的协作人工智能提供了基础。",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12612v1",
      "published_date": "2025-04-17 03:23:17 UTC",
      "updated_date": "2025-04-17 03:23:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:14:48.466279"
    },
    {
      "arxiv_id": "2504.12609v2",
      "title": "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration",
      "title_zh": "翻译失败",
      "authors": [
        "Tyler Ga Wei Lum",
        "Olivia Y. Lee",
        "C. Karen Liu",
        "Jeannette Bohg"
      ],
      "abstract": "Teaching robots dexterous manipulation skills often requires collecting\nhundreds of demonstrations using wearables or teleoperation, a process that is\nchallenging to scale. Videos of human-object interactions are easier to collect\nand scale, but leveraging them directly for robot learning is difficult due to\nthe lack of explicit action labels from videos and morphological differences\nbetween robot and human hands. We propose Human2Sim2Robot, a novel\nreal-to-sim-to-real framework for training dexterous manipulation policies\nusing only one RGB-D video of a human demonstrating a task. Our method utilizes\nreinforcement learning (RL) in simulation to cross the human-robot embodiment\ngap without relying on wearables, teleoperation, or large-scale data collection\ntypically necessary for imitation learning methods. From the demonstration, we\nextract two task-specific components: (1) the object pose trajectory to define\nan object-centric, embodiment-agnostic reward function, and (2) the\npre-manipulation hand pose to initialize and guide exploration during RL\ntraining. We found that these two components are highly effective for learning\nthe desired task, eliminating the need for task-specific reward shaping and\ntuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop\ntrajectory replay by 55% and imitation learning with data augmentation by 68%\nacross grasping, non-prehensile manipulation, and multi-step tasks. Project\nSite: https://human2sim2robot.github.io",
      "tldr_zh": "该论文提出 Human2Sim2Robot 框架，使用仅一个 RGB-D 视频演示，通过 Sim-to-Real RL（模拟到真实强化学习）来桥接人类-机器人实体差异，实现机器人灵巧操作策略的训练。该方法从演示中提取对象位姿轨迹以定义对象中心、无实体依赖的奖励函数，以及预操作手势来初始化和引导 RL 训练，从而避免了传统模仿学习所需的穿戴设备、大规模数据或任务特定奖励调整。实验结果显示，Human2Sim2Robot 在抓取、非 prehensile 操作和多步任务上，比对象感知开环轨迹重放方法提升 55%，并比模仿学习方法提升 68%，证明了其高效性和可扩展性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "15 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12609v2",
      "published_date": "2025-04-17 03:15:20 UTC",
      "updated_date": "2025-04-22 23:42:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:15:03.703129"
    },
    {
      "arxiv_id": "2504.12608v1",
      "title": "Code Copycat Conundrum: Demystifying Repetition in LLM-based Code Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Mingwei Liu",
        "Juntao Li",
        "Ying Wang",
        "Xueying Du",
        "Zuoyu Ou",
        "Qiuyuan Chen",
        "Bingxu An",
        "Zhao Wei",
        "Yong Xu",
        "Fangming Zou",
        "Xin Peng",
        "Yiling Lou"
      ],
      "abstract": "Despite recent advances in Large Language Models (LLMs) for code generation,\nthe quality of LLM-generated code still faces significant challenges. One\nsignificant issue is code repetition, which refers to the model's tendency to\ngenerate structurally redundant code, resulting in inefficiencies and reduced\nreadability. To address this, we conduct the first empirical study to\ninvestigate the prevalence and nature of repetition across 19 state-of-the-art\ncode LLMs using three widely-used benchmarks. Our study includes both\nquantitative and qualitative analyses, revealing that repetition is pervasive\nand manifests at various granularities and extents, including character,\nstatement, and block levels. We further summarize a taxonomy of 20 repetition\npatterns. Building on our findings, we propose DeRep, a rule-based technique\ndesigned to detect and mitigate repetition in generated code. We evaluate DeRep\nusing both open-source benchmarks and in an industrial setting. Our results\ndemonstrate that DeRep significantly outperforms baselines in reducing\nrepetition (with an average improvements of 91.3%, 93.5%, and 79.9% in rep-3,\nrep-line, and sim-line metrics) and enhancing code quality (with a Pass@1\nincrease of 208.3% over greedy search). Furthermore, integrating DeRep improves\nthe performance of existing repetition mitigation methods, with Pass@1\nimprovements ranging from 53.7% to 215.7%.",
      "tldr_zh": "本文研究了大型语言模型(LLMs)在代码生成中的代码重复问题，这是导致代码冗余和可读性差的主要挑战。通过首个实证研究，分析了19个最先进代码LLMs在三个基准上的表现，包括定量和定性分析，发现重复问题普遍存在于字符、语句和块级别，并总结了20种重复模式。作者提出DeRep，一种基于规则的技术，用于检测和减轻重复，在实验中显著提升性能，rep-3、rep-line和sim-line指标平均改善91.3%、93.5%和79.9%，Pass@1指标提高了208.3%，并增强了现有方法的有效性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12608v1",
      "published_date": "2025-04-17 03:13:39 UTC",
      "updated_date": "2025-04-17 03:13:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:15:14.365520"
    },
    {
      "arxiv_id": "2504.12606v1",
      "title": "Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Changsheng Lv",
        "Mengshi Qi",
        "Zijian Fu",
        "Huadong Ma"
      ],
      "abstract": "In this paper, we introduce a novel method named Robo-SGG, i.e.,\nLayout-Oriented Normalization and Restitution for Robust Scene Graph\nGeneration. Compared to the existing SGG setting, the robust scene graph\ngeneration aims to perform inference on a diverse range of corrupted images,\nwith the core challenge being the domain shift between the clean and corrupted\nimages. Existing SGG methods suffer from degraded performance due to\ncompromised visual features e.g., corruption interference or occlusions. To\nobtain robust visual features, we exploit the layout information, which is\ndomain-invariant, to enhance the efficacy of existing SGG methods on corrupted\nimages. Specifically, we employ Instance Normalization(IN) to filter out the\ndomain-specific feature and recover the unchangeable structural features, i.e.,\nthe positional and semantic relationships among objects by the proposed\nLayout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder\n(LEE) that augments the existing object and predicate encoders within the SGG\nframework, enriching the robust positional and semantic features of objects and\npredicates. Note that our proposed Robo-SGG module is designed as a\nplug-and-play component, which can be easily integrated into any baseline SGG\nmodel. Extensive experiments demonstrate that by integrating the\nstate-of-the-art method into our proposed Robo-SGG, we achieve relative\nimprovements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet\ntasks on the VG-C dataset, respectively, and achieve new state-of-the-art\nperformance in corruption scene graph generation benchmark (VG-C and GQA-C). We\nwill release our source code and model.",
      "tldr_zh": "本研究提出Robo-SGG，一种基于布局导向归一化和恢复的方法，用于提升场景图生成（Scene Graph Generation）的鲁棒性。该方法针对损坏图像中的领域偏移问题，利用布局信息（domain-invariant）来过滤领域特定特征，通过Instance Normalization (IN)和Layout-Oriented Restitution恢复对象的结构特征，如位置和语义关系。同时，引入Layout-Embedded Encoder (LEE)来增强现有SGG框架中对象和谓词的编码器，提高鲁棒性特征的丰富度。实验结果显示，Robo-SGG集成到最先进模型后，在VG-C数据集上分别提升mR@50 5.6%（PredCls）、8.0%（SGCls）和6.5%（SGDet），并在VG-C和GQA-C基准上达到新状态-of-the-art性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12606v1",
      "published_date": "2025-04-17 03:09:22 UTC",
      "updated_date": "2025-04-17 03:09:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:15:25.494208"
    },
    {
      "arxiv_id": "2504.12585v1",
      "title": "Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models",
      "title_zh": "识别与减轻大语言模型中先验分布的影响",
      "authors": [
        "Liyi Zhang",
        "Veniamin Veselovsky",
        "R. Thomas McCoy",
        "Thomas L. Griffiths"
      ],
      "abstract": "Large language models (LLMs) sometimes fail to respond appropriately to\ndeterministic tasks -- such as counting or forming acronyms -- because the\nimplicit prior distribution they have learned over sequences of tokens\ninfluences their responses. In this work, we show that, in at least some cases,\nLLMs actually compute the information needed to perform these tasks correctly,\nand we identify some interventions that can allow them to access this\ninformation to improve their performance. First, we show that simply prompting\nthe language model to not rely on its prior knowledge leads to dramatic\nimprovements in prior-dominated tasks. We then use mechanistic interpretability\ntechniques to localize the prior within the LLM and manipulate the extent to\nwhich that prior influences its responses. Specifically, we show that it is\npossible to identify layers of the underlying neural network that correlate\nwith the prior probability of a response and that lightweight finetuning of\nthese layers with basic prompts on prior-dominated tasks achieves high\nperformance on held-out answers. These results suggest that the information\nrequired to produce a correct response is contained within the representations\nof the problems formed by the models. Furthermore, we show that this finetuning\nis significantly more effective for prior-dominated tasks, and that the error\nafter finetuning is no longer correlated with the prior. Our results suggest\nthat it may be possible to define effective methods for manipulating the extent\nto which LLMs rely upon their priors in solving problems, potentially\nincreasing their performance in settings where LLMs hallucinate for reasons\nrelated to the prior probability of token sequences.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)中隐式先验分布的影响，导致它们在确定性任务（如计数或形成缩写）上表现不佳，因为先验会扭曲响应。作者通过提示干预（如指导模型不依赖先验知识）和机械解释性(mechanistic interpretability)技术，定位先验在神经网络层中的位置，并进行轻量级微调，以释放模型中已存在的正确信息。结果显示，这种方法显著提高了先验主导任务的性能，错误率不再与先验相关，并为减少LLMs因先验引起的幻觉提供潜在解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.12585v1",
      "published_date": "2025-04-17 02:00:53 UTC",
      "updated_date": "2025-04-17 02:00:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:15:38.251968"
    },
    {
      "arxiv_id": "2504.12577v1",
      "title": "Local Data Quantity-Aware Weighted Averaging for Federated Learning with Dishonest Clients",
      "title_zh": "本地数据量感知加权平均用于处理不诚实客户端的",
      "authors": [
        "Leming Wu",
        "Yaochu Jin",
        "Kuangrong Hao",
        "Han Yu"
      ],
      "abstract": "Federated learning (FL) enables collaborative training of deep learning\nmodels without requiring data to leave local clients, thereby preserving client\nprivacy. The aggregation process on the server plays a critical role in the\nperformance of the resulting FL model. The most commonly used aggregation\nmethod is weighted averaging based on the amount of data from each client,\nwhich is thought to reflect each client's contribution. However, this method is\nprone to model bias, as dishonest clients might report inaccurate training data\nvolumes to the server, which is hard to verify. To address this issue, we\npropose a novel secure \\underline{Fed}erated \\underline{D}ata\nq\\underline{u}antity-\\underline{a}ware weighted averaging method (FedDua). It\nenables FL servers to accurately predict the amount of training data from each\nclient based on their local model gradients uploaded. Furthermore, it can be\nseamlessly integrated into any FL algorithms that involve server-side model\naggregation. Extensive experiments on three benchmarking datasets demonstrate\nthat FedDua improves the global model performance by an average of 3.17%\ncompared to four popular FL aggregation methods in the presence of inaccurate\nclient data volume declarations.",
      "tldr_zh": "该论文针对联邦学习（Federated Learning）中 dishonest clients 可能报告虚假数据量的问题，提出了一种新的聚合方法 FedDua（Federated Data quantity-aware weighted averaging）。FedDua 通过分析客户端上传的本地模型 gradients 来准确预测每个客户端的真实训练数据量，并据此进行 weighted averaging，从而减少模型偏差。该方法可无缝集成到任何涉及服务器端模型聚合的 FL 算法中，实验在三个基准数据集上显示，与四种流行方法相比，FedDua 在数据量声明不准确的情况下平均提高了 3.17% 的全局模型性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "The paper has been accepted by ICME 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.12577v1",
      "published_date": "2025-04-17 01:50:24 UTC",
      "updated_date": "2025-04-17 01:50:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:15:48.923885"
    },
    {
      "arxiv_id": "2504.12576v1",
      "title": "CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Wentao Wu",
        "Xiao Wang",
        "Chenglong Li",
        "Bo Jiang",
        "Jin Tang",
        "Bin Luo",
        "Qi Liu"
      ],
      "abstract": "Event cameras have attracted increasing attention in recent years due to\ntheir advantages in high dynamic range, high temporal resolution, low power\nconsumption, and low latency. Some researchers have begun exploring\npre-training directly on event data. Nevertheless, these efforts often fail to\nestablish strong connections with RGB frames, limiting their applicability in\nmulti-modal fusion scenarios. To address these issues, we propose a novel CM3AE\npre-training framework for the RGB-Event perception. This framework accepts\nmulti-modalities/views of data as input, including RGB images, event images,\nand event voxels, providing robust support for both event-based and RGB-event\nfusion based downstream tasks. Specifically, we design a multi-modal fusion\nreconstruction module that reconstructs the original image from fused\nmulti-modal features, explicitly enhancing the model's ability to aggregate\ncross-modal complementary information. Additionally, we employ a multi-modal\ncontrastive learning strategy to align cross-modal feature representations in a\nshared latent space, which effectively enhances the model's capability for\nmulti-modal understanding and capturing global dependencies. We construct a\nlarge-scale dataset containing 2,535,759 RGB-Event data pairs for the\npre-training. Extensive experiments on five downstream tasks fully demonstrated\nthe effectiveness of CM3AE. Source code and pre-trained models will be released\non https://github.com/Event-AHU/CM3AE.",
      "tldr_zh": "该研究提出 CM3AE 框架，用于统一 RGB 帧和事件体素/帧的预训练，旨在解决事件相机数据与 RGB 融合的连接问题，从而提升多模态感知能力。该框架支持 RGB 图像、事件图像和事件体素作为输入，结合多模态融合重建模块和对比学习策略，实现跨模态特征的聚合与对齐，增强模型对全局依赖的捕捉。研究者构建了包含 2,535,759 个 RGB-Event 数据对的大规模数据集，并在五个下游任务上进行实验，证明 CM3AE 显著提高了事件-based 和 RGB-Event 融合任务的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12576v1",
      "published_date": "2025-04-17 01:49:46 UTC",
      "updated_date": "2025-04-17 01:49:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:16:01.771045"
    },
    {
      "arxiv_id": "2504.12563v1",
      "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Haris Riaz",
        "Sourav Bhabesh",
        "Vinayak Arannil",
        "Miguel Ballesteros",
        "Graham Horwood"
      ],
      "abstract": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth.",
      "tldr_zh": "本文提出 MetaSynth 方法，通过 meta-prompting 和多代理协作生成多样性更高的 synthetic data，旨在解决传统合成数据在领域适应（如 Finance 和 Biomedicine）中缺乏多样性的问题。实验结果显示，使用仅 2500 万 tokens 的 MetaSynth 数据对 Mistral-7B-v0.3 进行持续预训练，可使模型在 Finance 领域提升 4.08%、在 Biomedicine 领域提升 13.75%，同时不影响一般任务性能。多样性评估表明，MetaSynth 生成的数据在七个自动化指标上接近 LLM pre-training 语料，这证明了少数多样合成数据即可实现有效的领域适应。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "33 pages, 17 figures. Preprint",
      "pdf_url": "http://arxiv.org/pdf/2504.12563v1",
      "published_date": "2025-04-17 01:25:15 UTC",
      "updated_date": "2025-04-17 01:25:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:16:13.976612"
    },
    {
      "arxiv_id": "2504.12562v1",
      "title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition",
      "title_zh": "翻译失败",
      "authors": [
        "Haidar Khan",
        "Hisham A. Alyahya",
        "Yazeed Alnumay",
        "M Saiful Bari",
        "Bülent Yener"
      ],
      "abstract": "Evaluating the capabilities of Large Language Models (LLMs) has traditionally\nrelied on static benchmark datasets, human assessments, or model-based\nevaluations - methods that often suffer from overfitting, high costs, and\nbiases. ZeroSumEval is a novel competition-based evaluation protocol that\nleverages zero-sum games to assess LLMs with dynamic benchmarks that resist\nsaturation. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (PyJail), classic games (Chess, Liar's Dice, Poker),\nknowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These\ngames are designed to evaluate a range of AI capabilities such as strategic\nreasoning, planning, knowledge application, and creativity. Building upon\nrecent studies that highlight the effectiveness of game-based evaluations for\nLLMs, ZeroSumEval enhances these approaches by providing a standardized and\nextensible framework. To demonstrate this, we conduct extensive experiments\nwith >7000 simulations across 7 games and 13 models. Our results show that\nwhile frontier models from the GPT and Claude families can play common games\nand answer questions, they struggle to play games that require creating novel\nand challenging questions. We also observe that models cannot reliably\njailbreak each other and fail generally at tasks requiring creativity. We\nrelease our code at https://github.com/facebookresearch/ZeroSumEval.",
      "tldr_zh": "ZeroSumEval 提出了一种基于零和游戏的竞争评估协议，用于扩展LLM（Large Language Models）评估，避免传统方法的过拟合、高成本和偏见问题。该框架利用动态基准，包括PyJail（安全挑战）、Chess（经典游戏）、MathQuiz（知识测试）和Gandalf（说服挑战）等多样化游戏，来评估AI的能力，如战略推理、规划和创造力。通过超过7000次模拟实验，涉及13个模型，结果显示前沿模型（如GPT和Claude系列）在常见游戏和问题回答上表现良好，但难以处理创建新颖问题或需要创造力的任务。该方法提供了一个标准化的可扩展框架，并开源代码以促进进一步研究。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12562v1",
      "published_date": "2025-04-17 01:23:50 UTC",
      "updated_date": "2025-04-17 01:23:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:16:25.096287"
    },
    {
      "arxiv_id": "2504.13961v1",
      "title": "CONTINA: Confidence Interval for Traffic Demand Prediction with Coverage Guarantee",
      "title_zh": "CONTINA：带有覆盖保证的交通需求预测置信区间",
      "authors": [
        "Chao Yang",
        "Xiannan Huang",
        "Shuhan Qiu",
        "Yan Cheng"
      ],
      "abstract": "Accurate short-term traffic demand prediction is critical for the operation\nof traffic systems. Besides point estimation, the confidence interval of the\nprediction is also of great importance. Many models for traffic operations,\nsuch as shared bike rebalancing and taxi dispatching, take into account the\nuncertainty of future demand and require confidence intervals as the input.\nHowever, existing methods for confidence interval modeling rely on strict\nassumptions, such as unchanging traffic patterns and correct model\nspecifications, to guarantee enough coverage. Therefore, the confidence\nintervals provided could be invalid, especially in a changing traffic\nenvironment. To fill this gap, we propose an efficient method, CONTINA\n(Conformal Traffic Intervals with Adaptation) to provide interval predictions\nthat can adapt to external changes. By collecting the errors of interval during\ndeployment, the method can adjust the interval in the next step by widening it\nif the errors are too large or shortening it otherwise. Furthermore, we\ntheoretically prove that the coverage of the confidence intervals provided by\nour method converges to the target coverage level. Experiments across four\nreal-world datasets and prediction models demonstrate that the proposed method\ncan provide valid confidence intervals with shorter lengths. Our method can\nhelp traffic management personnel develop a more reasonable and robust\noperation plan in practice. And we release the code, model and dataset in\n\\href{ https://github.com/xiannanhuang/CONTINA/}{ Github}.",
      "tldr_zh": "本论文提出了一种名为 CONTINA 的方法，用于提供交通需求预测的置信区间（Confidence Interval），以确保覆盖率担保。CONTINA 通过收集预测错误并动态调整区间宽度（如在错误较大时拓宽），能够适应外部变化的环境，避免了传统方法依赖严格假设（如不变交通模式）的局限。理论上，该方法证明了置信区间的覆盖率会收敛到预设目标水平。实验在四个真实数据集上验证，CONTINA 比现有模型生成更短的置信区间，同时保持有效性，最终有助于交通管理制定更稳健的操作计划，并开源了相关代码。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13961v1",
      "published_date": "2025-04-17 01:14:53 UTC",
      "updated_date": "2025-04-17 01:14:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:16:36.434987"
    },
    {
      "arxiv_id": "2504.12557v2",
      "title": "TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Siow Meng Low",
        "Akshat Kumar"
      ],
      "abstract": "In safe reinforcement learning (RL), auxiliary safety costs are used to align\nthe agent to safe decision making. In practice, safety constraints, including\ncost functions and budgets, are unknown or hard to specify, as it requires\nanticipation of all possible unsafe behaviors. We therefore address a general\nsetting where the true safety definition is unknown, and has to be learned from\nsparsely labeled data. Our key contributions are: first, we design a safety\nmodel that performs credit assignment to estimate each decision step's impact\non the overall safety using a dataset of diverse trajectories and their\ncorresponding binary safety labels (i.e., whether the corresponding trajectory\nis safe/unsafe). Second, we illustrate the architecture of our safety model to\ndemonstrate its ability to learn a separate safety score for each timestep.\nThird, we reformulate the safe RL problem using the proposed safety model and\nderive an effective algorithm to optimize a safe yet rewarding policy. Finally,\nour empirical results corroborate our findings and show that this approach is\neffective in satisfying unknown safety definition, and scalable to various\ncontinuous control tasks.",
      "tldr_zh": "本研究提出TraCeS框架，用于从稀疏安全反馈中进行基于轨迹的信用分配（credit assignment），以解决安全强化学习（Safe RL）中安全约束未知的问题。该框架设计了一个安全模型，利用多样轨迹数据集和二元安全标签（binary safety labels）来估计每个决策步骤对整体安全的影响，并展示了模型架构以学习每个时间步的安全分数。研究重新表述了Safe RL问题，并推导了一个优化算法，以训练出既安全又高效的策略。实验结果证明，该方法在各种连续控制任务中有效满足未知安全定义，并展示了良好的可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12557v2",
      "published_date": "2025-04-17 01:11:08 UTC",
      "updated_date": "2025-04-23 04:44:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:16:50.162627"
    },
    {
      "arxiv_id": "2504.12552v1",
      "title": "Privacy-Preserving Operating Room Workflow Analysis using Digital Twins",
      "title_zh": "基于 Digital Twins 的隐私保护手术室工作流分析",
      "authors": [
        "Alejandra Perez",
        "Han Zhang",
        "Yu-Chun Ku",
        "Lalithkumar Seenivasan",
        "Roger Soberanis",
        "Jose L. Porras",
        "Richard Day",
        "Jeff Jopling",
        "Peter Najjar",
        "Mathias Unberath"
      ],
      "abstract": "Purpose: The operating room (OR) is a complex environment where optimizing\nworkflows is critical to reduce costs and improve patient outcomes. The use of\ncomputer vision approaches for the automatic recognition of perioperative\nevents enables identification of bottlenecks for OR optimization. However,\nprivacy concerns limit the use of computer vision for automated event detection\nfrom OR videos, which makes privacy-preserving approaches needed for OR\nworkflow analysis. Methods: We propose a two-stage pipeline for\nprivacy-preserving OR video analysis and event detection. In the first stage,\nwe leverage vision foundation models for depth estimation and semantic\nsegmentation to generate de-identified Digital Twins (DT) of the OR from\nconventional RGB videos. In the second stage, we employ the SafeOR model, a\nfused two-stream approach that processes segmentation masks and depth maps for\nOR event detection. We evaluate this method on an internal dataset of 38\nsimulated surgical trials with five event classes. Results: Our results\nindicate that this DT-based approach to the OR event detection model achieves\nperformance on par and sometimes even better than raw RGB video-based models on\ndetecting OR events. Conclusion: DTs enable privacy-preserving OR workflow\nanalysis, facilitating the sharing of de-identified data across institutions\nand they can potentially enhance model generalizability by mitigating\ndomain-specific appearance differences.",
      "tldr_zh": "本研究针对手术室（OR）工作流程优化面临的隐私问题，提出了一种基于 Digital Twins 的隐私保护分析方法。该方法采用两阶段管道：首先利用视觉基础模型进行深度估计和语义分割，从常规 RGB videos 生成去标识化的 Digital Twins；其次，使用 SafeOR 模型处理分割掩码和深度图来检测 OR 事件。在包含 38 个模拟手术试验的数据集上评估，结果显示该方法在事件检测性能上与原始 RGB 视频模型相当或更优。最后，该方法促进机构间去标识化数据的共享，并通过减少领域特定外观差异提升模型的泛化性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12552v1",
      "published_date": "2025-04-17 00:46:06 UTC",
      "updated_date": "2025-04-17 00:46:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:17:01.240990"
    },
    {
      "arxiv_id": "2504.12549v1",
      "title": "Memorization: A Close Look at Books",
      "title_zh": "翻译失败",
      "authors": [
        "Iris Ma",
        "Ian Domingo",
        "Alberto Krone-Martins",
        "Pierre Baldi",
        "Cristina V. Lopes"
      ],
      "abstract": "To what extent can entire books be extracted from LLMs? Using the Llama 3 70B\nfamily of models, and the \"prefix-prompting\" extraction technique, we were able\nto auto-regressively reconstruct, with a very high level of similarity, one\nentire book (Alice's Adventures in Wonderland) from just the first 500 tokens.\nWe were also able to obtain high extraction rates on several other books,\npiece-wise. However, these successes do not extend uniformly to all books. We\nshow that extraction rates of books correlate with book popularity and thus,\nlikely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama\n3.1, following recent work (Nasr et al., 2025). We further find that this\nundoing comes from changes to only a tiny fraction of weights concentrated\nprimarily in the lower transformer blocks. Our results provide evidence of the\nlimits of current regurgitation mitigation strategies and introduce a framework\nfor studying how fine-tuning affects the retrieval of verbatim memorization in\naligned LLMs.",
      "tldr_zh": "本文研究了从大型语言模型（LLMs）中提取整个书籍的可能性，使用 Llama 3 70B 模型和 prefix-prompting 技术，他们成功从前 500 个 tokens 重建了《Alice's Adventures in Wonderland》的完整内容，并对其他书籍实现了高提取率。研究发现，提取成功率与书籍的流行度和训练数据重复性相关，但指令微调后的 Llama 3.1 模型未能完全缓解这种记忆提取问题，主要涉及少量权重变化。总体而言，该工作揭示了当前缓解策略的局限性，并引入了一个框架来探讨微调如何影响对齐 LLMs 中的逐字记忆。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12549v1",
      "published_date": "2025-04-17 00:20:18 UTC",
      "updated_date": "2025-04-17 00:20:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:17:13.707088"
    },
    {
      "arxiv_id": "2504.12546v2",
      "title": "Anonymous Public Announcements",
      "title_zh": "匿名公开公告",
      "authors": [
        "Thomas Ågotnes",
        "Rustam Galimullin",
        "Ken Satoh",
        "Satoshi Tojo"
      ],
      "abstract": "We formalise the notion of an anonymous public announcement in the tradition\nof public announcement logic. Such announcements can be seen as in-between a\npublic announcement from ``the outside\" (an announcement of $\\phi$) and a\npublic announcement by one of the agents (an announcement of $K_a\\phi$): we get\nmore information than just $\\phi$, but not (necessarily) about exactly who made\nit. Even if such an announcement is prima facie anonymous, depending on the\nbackground knowledge of the agents it might reveal the identity of the\nannouncer: if I post something on a message board, the information might reveal\nwho I am even if I don't sign my name. Furthermore, like in the Russian Cards\npuzzle, if we assume that the announcer's intention was to stay anonymous, that\nin fact might reveal more information. In this paper we first look at the case\nwhen no assumption about intentions are made, in which case the logic with an\nanonymous public announcement operator is reducible to epistemic logic. We then\nlook at the case when we assume common knowledge of the intention to stay\nanonymous, which is both more complex and more interesting: in several ways it\nboils down to the notion of a ``safe\" announcement (again, similarly to Russian\nCards). Main results include formal expressivity results and axiomatic\ncompleteness for key logical languages.",
      "tldr_zh": "这篇论文在公共公告逻辑的基础上，正式化了匿名公共公告（anonymous public announcement）的概念，将其视为介于外部公告（announcement of φ）和代理人公告（announcement of K_a φ）之间的中间形式，提供更多信息但不一定揭示发布者身份。论文分析了匿名公告可能根据代理人的背景知识（如俄罗斯扑克牌谜题）无意中暴露发布者，以及假设发布者意图保持匿名的复杂情况，该情况类似于“安全”公告。最终，结果包括逻辑的正式表达性和关键语言的公理完备性（axiomatic completeness）。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12546v2",
      "published_date": "2025-04-17 00:14:37 UTC",
      "updated_date": "2025-04-21 01:42:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:17:25.770783"
    },
    {
      "arxiv_id": "2504.12545v1",
      "title": "Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice",
      "title_zh": "通过大型语言模型对大规模枪击事件进行知识获取以支持AI驱动的正义",
      "authors": [
        "Benign John Ihugba",
        "Afsana Nasrin",
        "Ling Wu",
        "Lin Li",
        "Lijun Qian",
        "Xishuang Dong"
      ],
      "abstract": "Mass-shooting events pose a significant challenge to public safety,\ngenerating large volumes of unstructured textual data that hinder effective\ninvestigations and the formulation of public policy. Despite the urgency, few\nprior studies have effectively automated the extraction of key information from\nthese events to support legal and investigative efforts. This paper presented\nthe first dataset designed for knowledge acquisition on mass-shooting events\nthrough the application of named entity recognition (NER) techniques. It\nfocuses on identifying key entities such as offenders, victims, locations, and\ncriminal instruments, that are vital for legal and investigative purposes. The\nNER process is powered by Large Language Models (LLMs) using few-shot\nprompting, facilitating the efficient extraction and organization of critical\ninformation from diverse sources, including news articles, police reports, and\nsocial media. Experimental results on real-world mass-shooting corpora\ndemonstrate that GPT-4o is the most effective model for mass-shooting NER,\nachieving the highest Micro Precision, Micro Recall, and Micro F1-scores.\nMeanwhile, o1-mini delivers competitive performance, making it a\nresource-efficient alternative for less complex NER tasks. It is also observed\nthat increasing the shot count enhances the performance of all models, but the\ngains are more substantial for GPT-4o and o1-mini, highlighting their superior\nadaptability to few-shot learning scenarios.",
      "tldr_zh": "这篇论文针对大规模枪击事件生成的大量非结构化文本数据，提出了第一个专用于知识获取的数据集，以支持法律调查和政策制定。研究采用命名实体识别 (NER) 技术，通过 Large Language Models (LLMs) 和 few-shot prompting 方法，从新闻、警方报告和社会媒体中提取关键实体，如罪犯、受害者、地点和犯罪工具。实验结果显示，GPT-4o 在真实世界语料上表现出色，实现了最高的 Micro Precision、Micro Recall 和 Micro F1-scores，而 o1-mini 作为资源高效替代方案，也在少样本任务中表现出色；此外，增加 shot count 可以显著提升所有模型的性能，尤其是 GPT-4o 和 o1-mini，突显了它们的 few-shot 学习适应性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.12545v1",
      "published_date": "2025-04-17 00:13:04 UTC",
      "updated_date": "2025-04-17 00:13:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:17:38.221346"
    },
    {
      "arxiv_id": "2504.13216v1",
      "title": "KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Bokwang Hwang",
        "Seonkyu Lim",
        "Taewoong Kim",
        "Yongjae Geun",
        "Sunghyun Bang",
        "Sohyun Park",
        "Jihyun Park",
        "Myeonggyu Lee",
        "Jinwoo Lee",
        "Yerin Kim",
        "Jinsun Yoo",
        "Jingyeong Hong",
        "Jina Park",
        "Yongchan Kim",
        "Suhyun Kim",
        "Younggyun Hahm",
        "Yiseul Lee",
        "Yejee Kang",
        "Chanhyuk Yoon",
        "Chansu Lee",
        "Heeyewon Jeong",
        "Jiyeon Lee",
        "Seonhye Gu",
        "Hyebin Kang",
        "Yousang Cho",
        "Hangyeol Yoo",
        "KyungTae Lim"
      ],
      "abstract": "We introduce KFinEval-Pilot, a benchmark suite specifically designed to\nevaluate large language models (LLMs) in the Korean financial domain.\nAddressing the limitations of existing English-centric benchmarks,\nKFinEval-Pilot comprises over 1,000 curated questions across three critical\nareas: financial knowledge, legal reasoning, and financial toxicity. The\nbenchmark is constructed through a semi-automated pipeline that combines\nGPT-4-generated prompts with expert validation to ensure domain relevance and\nfactual accuracy. We evaluate a range of representative LLMs and observe\nnotable performance differences across models, with trade-offs between task\naccuracy and output safety across different model families. These results\nhighlight persistent challenges in applying LLMs to high-stakes financial\napplications, particularly in reasoning and safety. Grounded in real-world\nfinancial use cases and aligned with the Korean regulatory and linguistic\ncontext, KFinEval-Pilot serves as an early diagnostic tool for developing safer\nand more reliable financial AI systems.",
      "tldr_zh": "我们引入了KFinEval-Pilot，这是一个全面的基准套件，旨在评估大型语言模型(LLMs)在韩国金融领域的语言理解能力，涵盖金融知识、法律推理和金融毒性等三个关键领域，总计超过1000个精心策划的问题。 该套件采用半自动化管道，结合GPT-4生成的提示和专家验证，确保内容的领域相关性和事实准确性。 在评估各种代表性LLMs时，我们观察到模型之间存在显著性能差异，在任务准确性和输出安全性之间存在权衡，突显了LLMs在高风险金融应用中的推理和安全挑战。 总之，KFinEval-Pilot作为基于韩国监管和语言环境的诊断工具，有助于推动更可靠的金融AI系统开发。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13216v1",
      "published_date": "2025-04-17 00:12:58 UTC",
      "updated_date": "2025-04-17 00:12:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:17:50.058318"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 122,
  "processed_papers_count": 122,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T14:18:14.297485"
}
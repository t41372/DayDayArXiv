{
  "date": "2025-07-24",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-07-24 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv åˆ—è¡¨å¯è°“æ˜¯**å¤§æ¨¡å‹å®‰å…¨ä¸æ¨ç†èƒ½åŠ›å…±è¿›åŒ–**çš„ä¸€å¤©ã€‚æœ€å¼•äººæ³¨ç›®çš„æ˜¯ä¸Šæµ· AI Lab æ¨å‡ºçš„ **SafeWork-R1**ï¼Œå±•ç¤ºäº†å®‰å…¨ä¸èƒ½åŠ›å¦‚ä½•é€šè¿‡â€œé¡¿æ‚Ÿï¼ˆAha momentsï¼‰â€ååŒè¿›åŒ–ï¼›åŒæ—¶ï¼Œ**AI è‡ªåŠ¨åŒ–ç§‘ç ”ï¼ˆAI Scientistï¼‰** è¿ˆå‡ºé‡è¦ä¸€æ­¥ï¼Œå‡ºç°äº†èƒ½è‡ªä¸»å‘ç°ç¥ç»ç½‘ç»œæ¶æ„çš„ **ASI-Arch**ï¼Œè¢«ç§°ä¸ºæ¶æ„å‘ç°çš„â€œAlphaGo æ—¶åˆ»â€ã€‚æ­¤å¤–ï¼Œå…³äº CoT çš„**æœºæ¢°å¯è§£é‡Šæ€§**ã€**ç§‘å­¦é¢†åŸŸä¸“ç”¨å¤§æ¨¡å‹**ä»¥åŠ**å¤šæ¨¡æ€ä»£ç†ï¼ˆAgentsï¼‰** çš„æ•ˆç‡ä¼˜åŒ–ä¹Ÿæ˜¯ä»Šæ—¥çš„çƒ­ç‚¹è¯é¢˜ã€‚\n\n---\n\n### ğŸš€ é‡ç£…å¤´æ¡ï¼šå®‰å…¨è¿›åŒ–ä¸ AI ç§‘å­¦å®¶\n\n**1. SafeWork-R1ï¼šåœ¨ AI-45Â° å®šå¾‹ä¸‹çš„å®‰å…¨ä¸æ™ºåŠ›å…±è¿›åŒ–**\n**SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šä¸Šæµ· AI Lab å›¢é˜Ÿæå‡ºã€‚è¿™ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ¨¡å‹ï¼Œæ›´æ˜¯ä¸€ç§åä¸º SafeLadder çš„æ¡†æ¶ã€‚ä¸åŒäºä¼ ç»Ÿçš„ RLHF ä»…å­¦ä¹ äººç±»åå¥½ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¤§è§„æ¨¡ã€å¾ªåºæ¸è¿›çš„å®‰å…¨å¯¼å‘å¼ºåŒ–å­¦ä¹ ï¼Œè®©æ¨¡å‹æ¶Œç°å‡º**å†…åœ¨çš„å®‰å…¨æ¨ç†å’Œè‡ªæˆ‘åæ€èƒ½åŠ›**ï¼ˆå³å®‰å…¨çš„â€œAha momentsâ€ï¼‰ã€‚\n> **å‘ç°**ï¼šSafeWork-R1 åœ¨ä¸ç‰ºç‰²é€šç”¨èƒ½åŠ›çš„å‰æä¸‹ï¼Œå®‰å…¨åŸºå‡†æµ‹è¯•æ¯” Qwen2.5-VL-72B æå‡äº† 46.54%ï¼Œå¹¶å£°ç§°åœ¨å®‰å…¨æ€§ä¸Šè¶…è¶Šäº† GPT-4.1ï¼ˆåŸæ–‡å¦‚æ­¤ï¼‰ç­‰æ¨¡å‹ã€‚è¿™è¯æ˜äº†å®‰å…¨æ€§å’Œèƒ½åŠ›å¯ä»¥ååŒè¿›åŒ–ï¼Œè€Œéé›¶å’Œåšå¼ˆã€‚\n\n**2. æ¨¡å‹æ¶æ„å‘ç°çš„ AlphaGo æ—¶åˆ»**\n**AlphaGo Moment for Model Architecture Discovery**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡º **ASI-Arch**ï¼Œä¸€ä¸ªå…¨è‡ªåŠ¨ AI ç§‘ç ”ç³»ç»Ÿï¼Œæ—¨åœ¨æ‰“ç ´äººç±»è®¤çŸ¥å¯¹ AI ç ”å‘çš„ç“¶é¢ˆã€‚å®ƒä¸åªæ˜¯åšä¼ ç»Ÿçš„ NASï¼ˆç¥ç»æ¶æ„æœç´¢ï¼‰ï¼Œè€Œæ˜¯è¿›è¡Œç«¯åˆ°ç«¯çš„ç§‘å­¦å‘ç°ï¼šæå‡ºå‡è®¾ã€ç¼–å†™ä»£ç ã€è®­ç»ƒéªŒè¯ã€‚\n> **å‘ç°**ï¼šè¯¥ç³»ç»Ÿåœ¨ 20,000 GPU å°æ—¶å†…è¿›è¡Œäº† 1773 æ¬¡è‡ªä¸»å®éªŒï¼Œå‘ç°äº† 106 ç§åˆ›æ–°çš„çº¿æ€§ Attention æ¶æ„ã€‚æ–‡ç« å»ºç«‹äº†ä¸€ä¸ªâ€œç§‘å­¦å‘ç°çš„ Scaling Lawâ€ï¼Œè¡¨æ˜æ¶æ„åˆ›æ–°å¯ä»¥é€šè¿‡ç®—åŠ›å †å æ¥å®ç°ï¼Œè€Œéä»…é äººç±»çµæ„Ÿã€‚\n\n**3. Innovatorï¼šé€šè¿‡ç»†ç²’åº¦ MoE Upcycling è¿›è¡Œç§‘å­¦æŒç»­é¢„è®­ç»ƒ**\n**Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šè§£å†³â€œç¾éš¾æ€§é—å¿˜â€é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è®©é€šç”¨ LLM å­¦ä¹ ç§‘å­¦çŸ¥è¯†æ—¶ã€‚ä½œè€…æå‡ºå°†ç¨ å¯†æ¨¡å‹ï¼ˆDense LLMï¼‰â€œå‡çº§ï¼ˆUpcyclingï¼‰â€ä¸ºç»†ç²’åº¦çš„æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ã€‚\n> **å‘ç°**ï¼šInnovator åŒ…å« 64 ä¸ªç§‘å­¦ä¸“å®¶å’Œä¸€ä¸ªé€šç”¨ä¸“å®¶ã€‚åœ¨ 30 ä¸ªç§‘å­¦ä»»åŠ¡ä¸Šå¹³å‡æå‡ 25%ï¼ŒåŒæ—¶ä¿ç•™äº† 99% çš„é€šç”¨èƒ½åŠ›ã€‚è¿™ä¸ºæ„å»ºâ€œç§‘å­¦é€šç”¨æ™ºèƒ½â€æŒ‡æ˜äº†ä¸€æ¡é«˜æ•ˆè·¯å¾„ã€‚\n\n---\n\n### ğŸ§  æ¨ç†ã€è§£é‡Šæ€§ä¸è®¤çŸ¥æ¶æ„\n\n**4. è¿è´¯æ€ç»´ï¼ˆCoTï¼‰æ˜¯å¦‚ä½•æ€è€ƒçš„ï¼ŸåŸºäºç¨€ç–è‡ªç¼–ç å™¨çš„æœºæ¢°å¯è§£é‡Šæ€§ç ”ç©¶**\n**How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ CoT å¿ å®åº¦ï¼ˆFaithfulnessï¼‰çš„ç‰¹å¾çº§å› æœç ”ç©¶ã€‚åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencodersï¼‰å’Œæ¿€æ´»ä¿®è¡¥ï¼ˆActivation Patchingï¼‰æŠ€æœ¯ã€‚\n> **å‘ç°**ï¼šåœ¨å°æ¨¡å‹ï¼ˆ70Mï¼‰ä¸­ CoT ä½œç”¨æœ‰é™ï¼Œä½†åœ¨å¤§æ¨¡å‹ï¼ˆ2.8Bï¼‰ä¸­ï¼ŒCoT æ˜¾è‘—æé«˜äº†å†…éƒ¨è®¡ç®—çš„æ¨¡å—åŒ–ç¨‹åº¦å’Œå¯è§£é‡Šæ€§ã€‚è¿™éªŒè¯äº† CoT ä¸ä»…ä»…æ˜¯ç”Ÿæˆæ–‡æœ¬ï¼Œç¡®å®åœ¨æ¨¡å‹å†…éƒ¨è¯±å¯¼äº†æ›´ç»“æ„åŒ–çš„æ¨ç†è¿‡ç¨‹ã€‚\n\n**5. é‡è®¿ LLM æ¨ç†ï¼šåŸºäºä¿¡æ¯ç“¶é¢ˆç†è®º**\n**Revisiting LLM Reasoning via Information Bottleneck**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šä¸º RLVRï¼ˆå¸¦éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼‰æä¾›äº†ç†è®ºåŸºç¡€ã€‚æå‡º **IBRO** æ¡†æ¶ï¼Œåˆ©ç”¨ä¿¡æ¯ç“¶é¢ˆï¼ˆInformation Bottleneckï¼‰åŸåˆ™æ¥ä¼˜åŒ–æ¨ç†è½¨è¿¹ã€‚\n> **å‘ç°**ï¼šé€šè¿‡å¼•å…¥è½»é‡çº§çš„ IB æ­£åˆ™åŒ–ï¼ˆåªéœ€ä¸€è¡Œä»£ç ä¿®æ”¹ï¼‰ï¼Œå¯ä»¥æ¿€åŠ±æ¨¡å‹ç”Ÿæˆæ—¢åŒ…å«æ­£ç¡®ç­”æ¡ˆä¿¡æ¯åˆå…·æœ‰æ³›åŒ–æ€§çš„æ¨ç†é“¾ï¼Œåœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†ä¸€è‡´æå‡ã€‚\n\n**6. è§£è€¦ LLM çš„çŸ¥è¯†ä¸æ¨ç†ï¼šåŸºäºè®¤çŸ¥åŒç³»ç»Ÿç†è®ºçš„æ¢ç´¢**\n**Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šå—äººç±»è®¤çŸ¥â€œç³»ç»Ÿ1ï¼ˆå¿«ï¼‰â€å’Œâ€œç³»ç»Ÿ2ï¼ˆæ…¢ï¼‰â€çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªè®¤çŸ¥å½’å› æ¡†æ¶ï¼Œè¯•å›¾å°† LLM çš„è¡¨ç°è§£è€¦ä¸ºâ€œçŸ¥è¯†æ£€ç´¢â€å’Œâ€œæ¨ç†è°ƒæ•´â€ä¸¤ä¸ªé˜¶æ®µã€‚\n> **å‘ç°**ï¼šçŸ¥è¯†ä¸»è¦å­˜åœ¨äºä½å±‚ç½‘ç»œï¼Œè€Œæ¨ç†åœ¨é«˜å±‚ã€‚å‚æ•°è§„æ¨¡æ‰©å¤§èƒ½æ˜¾è‘—æå‡çŸ¥è¯†ï¼Œä½†è®©æ¨ç†å˜å¾—æ›´åŠ â€œè°¨æ…â€ã€‚\n\n---\n\n### ğŸ¤– Agents ä¸ è‡ªåŠ¨ç¼–ç¨‹\n\n**7. MemoCoderï¼šåˆ©ç”¨ LLM è¾…åŠ©ä»£ç†è¿›è¡Œè‡ªåŠ¨åŒ–å‡½æ•°åˆæˆ**\n**MemoCoder: Automated Function Synthesis using LLM-Supported Agents**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ä»£ç ç”Ÿæˆçš„è¿­ä»£è°ƒè¯•é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚æ ¸å¿ƒæ˜¯ **Fixing Knowledge Set**ï¼ˆä¿®å¤çŸ¥è¯†åº“ï¼‰å’Œ **Mentor Agent**ï¼ˆå¯¼å¸ˆä»£ç†ï¼‰ã€‚\n> **å‘ç°**ï¼šä¸åƒä¸€èˆ¬çš„ Self-Repair å®¹æ˜“é™·å…¥æ­»å¾ªç¯ï¼ŒMemoCoder èƒ½ä»è¿‡å»çš„ä¿®å¤ä¸­å­¦ä¹ å¹¶æ£€ç´¢ç»éªŒã€‚åœ¨ MBPP å’Œ HumanEval ä¸Šæ˜¾è‘—ä¼˜äº Zero-shot å’Œæ™®é€šçš„ Self-Repair ç­–ç•¥ã€‚\n\n**8. é«˜æ•ˆä»£ç†ï¼šåœ¨é™ä½æˆæœ¬çš„åŒæ—¶æ„å»ºæœ‰æ•ˆä»£ç†**\n**Efficient Agents: Building Effective Agents While Reducing Cost**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šåœ¨ GAIA åŸºå‡†ä¸Šç³»ç»Ÿç ”ç©¶äº† Agent çš„â€œæ•ˆç‡-æ•ˆæœâ€æƒè¡¡ã€‚\n> **å‘ç°**ï¼šè®¸å¤š Agent æ¡†æ¶è¿‡åº¦å¤æ‚åŒ–äº†ã€‚ä½œè€…æå‡ºçš„ Efficient Agents æ¡†æ¶ä¿ç•™äº† OWLï¼ˆé¢†å…ˆå¼€æºæ¡†æ¶ï¼‰96.7% çš„æ€§èƒ½ï¼Œä½†æˆæœ¬é™ä½äº† 42.7%ã€‚\n\n**9. å¤§è§„æ¨¡æµ‹è¯•å¤±è´¥é©±åŠ¨çš„ä»£ç†å¼ç¨‹åºä¿®å¤**\n**Agentic Program Repair from Test Failures at Scale**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šMeta ç­‰æœºæ„çš„å·¥ä½œã€‚å¼€å‘äº†ä¸€ä¸ªâ€œå·¥ç¨‹ä»£ç†ï¼ˆEngineering Agentï¼‰â€ï¼Œåˆ©ç”¨ ReAct æ¡†æ¶ã€é™æ€åˆ†æå’Œæµ‹è¯•æ‰§è¡Œåé¦ˆæ¥è‡ªåŠ¨ä¿®å¤å¤§è§„æ¨¡ä»£ç åº“ä¸­çš„ Bugã€‚\n> **å‘ç°**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¯¥ä»£ç†ç”Ÿæˆçš„ä¿®å¤æœ‰ 31.5% æœ€ç»ˆè¢«åˆå¹¶ï¼ˆLandï¼‰ã€‚ä¸“é—¨å¾®è°ƒçš„ 70B æ¨¡å‹åœ¨ä¿® Bug ä»»åŠ¡ä¸Šå¯ä»¥åª²ç¾ Llama-405Bã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸è§†è§‰ç”Ÿæˆ\n\n**10. VideoMindï¼šå…·æœ‰æ„å›¾è½åœ°çš„å…¨æ¨¡æ€è§†é¢‘ç†è§£æ•°æ®é›†**\n**VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šç°æœ‰çš„è§†é¢‘æ•°æ®é›†å¤šåœç•™åœ¨è¡¨é¢æè¿°ã€‚VideoMind åŒ…å« 103K è§†é¢‘ï¼Œé‡ç‚¹åœ¨äº**æ„å›¾ï¼ˆIntentï¼‰** çš„æ ‡æ³¨ï¼Œå³â€œä¸ºä»€ä¹ˆè¦è¿™æ ·åšâ€ï¼Œè¿™éœ€è¦æ·±åº¦è®¤çŸ¥ç†è§£ã€‚\n> **å‘ç°**ï¼šå¼•å…¥äº†åŸºäºæ€ç»´é“¾ï¼ˆCoTï¼‰ç”Ÿæˆçš„æ·±å±‚è®¤çŸ¥æè¿°ï¼Œä¸ºæƒ…æ„Ÿå’Œæ„å›¾è¯†åˆ«ç­‰é«˜é˜¶è§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„åŸºå‡†ã€‚\n\n**11. PTCMILï¼šåŸºäºæç¤º Token èšç±»çš„å…¨åˆ‡ç‰‡å›¾åƒåˆ†æ**\n**PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰æå…¶å·¨å¤§çš„åˆ†è¾¨ç‡ï¼Œæå‡ºåœ¨ Vision Transformer ä¸­å¼•å…¥å¯å­¦ä¹ çš„ Prompt Tokens æ¥è¿›è¡Œèšç±»ã€‚\n> **å‘ç°**ï¼šè¿™ç§æ–¹æ³•èƒ½åŠ¨æ€åœ°å°†èšç±»ä¸ä¸‹æ¸¸ä»»åŠ¡å¯¹é½ï¼Œæœ‰æ•ˆé™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶åœ¨ç”Ÿå­˜åˆ†æç­‰ä»»åŠ¡ä¸Šè¶…è¶Šäº† SOTAã€‚\n\n**12. GenAI è­¦ç”¨è‰å›¾ï¼šåŸºäº Stable Diffusion**\n**Gen-AI Police Sketches with Stable Diffusion**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæ¢ç´¢åˆ©ç”¨ç”Ÿæˆå¼ AI è‡ªåŠ¨åŒ–å«Œç–‘äººè‰å›¾ç»˜åˆ¶ã€‚å¯¹æ¯”äº†åŸºç¡€ SDã€åŠ äº† CLIP çš„ SD ä»¥åŠ LoRA å¾®è°ƒç‰ˆæœ¬ã€‚\n> **å‘ç°**ï¼šå¾®è°ƒ Self-Attention å’Œ Cross-Attention å±‚èƒ½æœ€å¥½åœ°å¯¹é½æ–‡æœ¬æè¿°å’Œè‰å›¾ç‰¹å¾ã€‚è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå¼ AI åœ¨æ‰§æ³•é¢†åŸŸçš„å…·ä½“åº”ç”¨å°è¯•ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€é˜²å¾¡ä¸å¯¹é½\n\n**13. è§„èŒƒè‡ªæ ¡æ­£ï¼šé€šè¿‡æµ‹è¯•æ—¶ä¼˜åŒ–ç¼“è§£ä¸Šä¸‹æ–‡å¥–åŠ±é»‘å®¢è¡Œä¸º**\n**Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šLLM ç»å¸¸ä¼šé’»è§„åˆ™ï¼ˆPrompt/Rubricï¼‰çš„ç©ºå­ï¼ˆReward Hackingï¼‰ã€‚SSC æ¡†æ¶è®©æ¨¡å‹åœ¨æ¨ç†æ—¶è‡ªæˆ‘æ‰¹è¯„å¹¶**ä¿®æ”¹è§„èŒƒæœ¬èº«**ï¼Œå µä½æ¼æ´ã€‚\n> **å‘ç°**ï¼šä¸éœ€è¦ä¿®æ”¹æ¨¡å‹æƒé‡ï¼Œä»…åœ¨æ¨ç†æ—¶è¿›è¡Œï¼Œå°†æ¨¡å‹é’»ç©ºå­çš„æ¦‚ç‡é™ä½äº† 90% ä»¥ä¸Šã€‚\n\n**14. Safeguarding RAG Pipelines with GMTP**\n**Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ RAG ç³»ç»Ÿé¢ä¸´çš„â€œæŠ•æ¯’æ–‡æ¡£â€æ”»å‡»ï¼ˆæ³¨å…¥æ¶æ„æ–‡æ¡£è¯¯å¯¼ç”Ÿæˆï¼‰ã€‚æå‡ºåˆ©ç”¨æ¢¯åº¦å’Œ Masked Token æ¦‚ç‡æ¥æ£€æµ‹å¼‚å¸¸ã€‚\n> **å‘ç°**ï¼šæ”»å‡»è€…æ³¨å…¥çš„ Token é€šå¸¸å…·æœ‰æä½çš„ Masked æ¦‚ç‡ï¼Œè¯¥æ–¹æ³•èƒ½è¿‡æ»¤æ‰ 90% ä»¥ä¸Šçš„æŠ•æ¯’å†…å®¹ã€‚\n\n**15. LoRA-Leakï¼šé’ˆå¯¹ LoRA å¾®è°ƒè¯­è¨€æ¨¡å‹çš„æˆå‘˜æ¨æ–­æ”»å‡»**\n**LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šLoRA å¾®è°ƒå‚æ•°å¾ˆå°‘ï¼Œå¤§å®¶é€šå¸¸ä»¥ä¸ºå®ƒå¾ˆå®‰å…¨ã€‚ä½†è¿™é¡¹ç ”ç©¶å‘ç°ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºå‚è€ƒï¼ŒLoRA å¾®è°ƒçš„æ•°æ®éå¸¸å®¹æ˜“é­å—æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAï¼‰ã€‚\n> **implication**ï¼šå¯¹äºæä¾›ç§æœ‰æ•°æ®å¾®è°ƒæœåŠ¡çš„å‚å•†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¿…é¡»é‡è§†çš„éšç§é£é™©ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡\n\n*   **[Data Vis] å¯è§†åŒ–æœ‰åŠ©äº AI ç†è§£æ•°æ®å—ï¼Ÿ** (Paper 111): å®éªŒè¡¨æ˜ï¼Œå°±åƒäººç±»ä¸€æ ·ï¼Œç»™ GPT-4 å’Œ Claude 3.5 çœ‹æ•£ç‚¹å›¾ï¼ˆScatterplotï¼‰èƒ½æ˜¾è‘—æå‡å®ƒä»¬å¯¹æ•°æ®çš„åˆ†æèƒ½åŠ›ã€‚\n*   **[Knowledge Editing] NeuralDB** (Paper 110): å°†çŸ¥è¯†ç¼–è¾‘æ‰©å±•åˆ°äº† 100,000 æ¡äº‹å®çš„è§„æ¨¡ï¼Œé€šè¿‡å°†äº‹å®å­˜å‚¨ä¸ºç¥ç»é”®å€¼æ•°æ®åº“ï¼ˆNeural KV Databaseï¼‰æ¥é¿å…é—å¿˜ã€‚\n*   **[Finance] Forecasting Commodity Price Shocks** (Paper 8): ç»“åˆ LSTM å’Œç”Ÿæˆå¼ AI æå–çš„æ–°é—»ä¿¡å·æ¥é¢„æµ‹å¤§å®—å•†å“ä»·æ ¼å†²å‡»ï¼ŒAUC è¾¾åˆ° 0.94ã€‚\n*   **[Deepfake] Deepfake Detection Via Facial Feature Extraction** (Paper 5): æå‡ºä»…ä½¿ç”¨é¢éƒ¨å…³é”®ç‚¹ï¼ˆLandmarksï¼‰è€ŒéåŸå§‹å›¾åƒæ¥æ£€æµ‹ Deepfakeï¼Œè¿™æŒ‘æˆ˜äº†å¿…é¡»ç”¨å›¾åƒå¤„ç†çš„å‡è®¾ã€‚\n\n---\nå¸Œæœ›ä»Šå¤©çš„ TLDR èƒ½ä¸ºä½ çš„ç ”ç©¶å¸¦æ¥çµæ„Ÿï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2507.18848v1",
      "title": "PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis",
      "title_zh": "PTCMILï¼šåŸºäºæç¤ºè¯ Token èšç±»çš„å…¨åˆ‡ç‰‡å›¾åƒåˆ†æå¤šå®ä¾‹å­¦ä¹ ",
      "authors": [
        "Beidi Zhao",
        "SangMook Kim",
        "Hao Chen",
        "Chen Zhou",
        "Zu-hua Gao",
        "Gang Wang",
        "Xiaoxiao Li"
      ],
      "abstract": "Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with the complexity and heterogeneity of WSIs. Existing MIL methods face challenges in aggregating diverse patch information into robust WSI representations. While ViTs and clustering-based approaches show promise, they are computationally intensive and fail to capture task-specific and slide-specific variability. To address these limitations, we propose PTCMIL, a novel Prompt Token Clustering-based ViT for MIL aggregation. By introducing learnable prompt tokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in an end-to-end manner. It dynamically aligns clustering with downstream tasks, using projection-based clustering tailored to each WSI, reducing complexity while preserving patch heterogeneity. Through token merging and prototype-based pooling, PTCMIL efficiently captures task-relevant patterns. Extensive experiments on eight datasets demonstrate its superior performance in classification and survival analysis tasks, outperforming state-of-the-art methods. Systematic ablation studies confirm its robustness and strong interpretability. The code is released at https://github.com/ubc-tea/PTCMIL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PTCMILï¼Œä¸€ç§åŸºäº Prompt Token Clustering çš„ ViT æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ– Whole Slide Image (WSI) åˆ†æä¸­çš„ Multiple Instance Learning (MIL) èšåˆè¿‡ç¨‹ã€‚é’ˆå¯¹ç°æœ‰ MIL æ–¹æ³•åœ¨å¤„ç† WSI å¤æ‚æ€§å’Œå¼‚è´¨æ€§æ—¶å­˜åœ¨çš„è®¡ç®—è´Ÿæ‹…é‡åŠç¼ºä¹ä»»åŠ¡é’ˆå¯¹æ€§ç­‰æŒ‘æˆ˜ï¼ŒPTCMIL é€šè¿‡åœ¨ ViT ä¸»å¹²ä¸­å¼•å…¥å¯å­¦ä¹ çš„ prompt tokensï¼Œå®ç°äº†èšç±»ä¸é¢„æµ‹ä»»åŠ¡çš„ç«¯åˆ°ç«¯ç»Ÿä¸€ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ projection-based clustering ä¸ºæ¯ä¸ª WSI åŠ¨æ€å¯¹é½ä¸‹æ¸¸ä»»åŠ¡ï¼Œåœ¨é™ä½å¤æ‚åº¦çš„åŒæ—¶æœ‰æ•ˆä¿ç•™äº† patch çš„å¼‚è´¨æ€§ã€‚é€šè¿‡ç»“åˆ token merging å’Œ prototype-based pooling æŠ€æœ¯ï¼ŒPTCMIL èƒ½å¤Ÿé«˜æ•ˆæ•è·ä¸ä»»åŠ¡é«˜åº¦ç›¸å…³çš„æ¨¡å¼ã€‚åœ¨å…«ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPTCMIL åœ¨åˆ†ç±»å’Œ survival analysis ä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶è¡¨ç°å‡ºæä½³çš„é²æ£’æ€§ä¸å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18848v1",
      "published_date": "2025-07-24 23:33:59 UTC",
      "updated_date": "2025-07-24 23:33:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:33:51.685233+00:00"
    },
    {
      "arxiv_id": "2507.18847v2",
      "title": "Equivariant Volumetric Grasping",
      "title_zh": "ç­‰å˜ä½“ç´ æŠ“å–",
      "authors": [
        "Pinhao Song",
        "Yutong Hu",
        "Pengteng Li",
        "Renaud Detry"
      ],
      "abstract": "We propose a new volumetric grasp model that is equivariant to rotations around the vertical axis, leading to a significant improvement in sample efficiency. Our model employs a tri-plane volumetric feature representation -- i.e., the projection of 3D features onto three canonical planes. We introduce a novel tri-plane feature design in which features on the horizontal plane are equivariant to 90Â° rotations, while the sum of features from the other two planes remains invariant to the same transformations. This design is enabled by a new deformable steerable convolution, which combines the adaptability of deformable convolutions with the rotational equivariance of steerable ones. This allows the receptive field to adapt to local object geometry while preserving equivariance properties. We further develop equivariant adaptations of two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically, we derive a new equivariant formulation of IGD's deformable attention mechanism and propose an equivariant generative model of grasp orientations based on flow matching. We provide a detailed analytical justification of the proposed equivariance properties and validate our approach through extensive simulated and real-world experiments. Our results demonstrate that the proposed projection-based design significantly reduces both computational and memory costs. Moreover, the equivariant grasp models built on top of our tri-plane features consistently outperform their non-equivariant counterparts, achieving higher performance with only a modest computational overhead. Video and code can be viewed in: https://mousecpn.github.io/evg-page/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºEquivariant Volumetric Graspingçš„ä½“ç§¯æŠ“å–æ¨¡å‹ï¼Œé€šè¿‡å®ç°ç»•å‚ç›´è½´çš„ç­‰å˜æ€§(equivariance)æ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸‰å¹³é¢(tri-plane)ä½“ç§¯ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç‰¹å¾ç»“æ„ï¼Œä½¿å¾—æ°´å¹³é¢ç‰¹å¾å¯¹90Â°æ—‹è½¬å…·æœ‰ç­‰å˜æ€§ï¼Œè€Œå…¶ä»–å¹³é¢çš„ç‰¹å¾æ€»å’Œä¿æŒä¸å˜ã€‚è¿™ä¸€è®¾è®¡ç”±æ–°æå‡ºçš„å¯å˜å½¢è½¬å‘å·ç§¯(deformable steerable convolution)é©±åŠ¨ï¼Œå®ƒç»“åˆäº†å˜å½¢å·ç§¯çš„è‡ªé€‚åº”æ„Ÿå—é‡ä¸å¯è½¬å‘å·ç§¯çš„æ—‹è½¬ç­‰å˜æ€§è´¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å¼€å‘äº†GIGAå’ŒIGDä¸¤ç§å…ˆè¿›æŠ“å–è§„åˆ’å™¨çš„ç­‰å˜ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬æ¨å¯¼äº†IGDå˜å½¢æ³¨æ„åŠ›æœºåˆ¶(deformable attention)çš„æ–°ç­‰å˜å…¬å¼ï¼Œå¹¶æå‡ºäº†åŸºäºæµåŒ¹é…(flow matching)çš„æŠ“å–æ–¹å‘ç­‰å˜ç”Ÿæˆæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§åŸºäºæŠ•å½±çš„è®¾è®¡å¤§å¹…é™ä½äº†è®¡ç®—å’Œå†…å­˜æˆæœ¬ï¼Œä¸”å…¶ç­‰å˜æ¨¡å‹åœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜äºéç­‰å˜æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥å·¥ä½œä¸ºå®ç°é«˜æ•ˆã€ä½æˆæœ¬ä¸”å…·æœ‰å‡ ä½•é²æ£’æ€§çš„æœºå™¨äººä½“ç§¯æŠ“å–æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.18847v2",
      "published_date": "2025-07-24 23:18:32 UTC",
      "updated_date": "2025-08-05 12:56:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:33:57.185307+00:00"
    },
    {
      "arxiv_id": "2507.19551v3",
      "title": "Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content",
      "title_zh": "Rainbow Noiseï¼šé’ˆå¯¹ LGBTQ å†…å®¹çš„å¤šæ¨¡æ€æœ‰å®³æ¨¡å› æ£€æµ‹å™¨å‹åŠ›æµ‹è¯•",
      "authors": [
        "Ran Tong",
        "Songtao Wei",
        "Jiaqi Liu",
        "Lanruo Wang"
      ],
      "abstract": "Hateful memes aimed at LGBTQ\\,+ communities often evade detection by tweaking either the caption, the image, or both. We build the first robustness benchmark for this setting, pairing four realistic caption attacks with three canonical image corruptions and testing all combinations on the PrideMM dataset. Two state-of-the-art detectors, MemeCLIP and MemeBLIP2, serve as case studies, and we introduce a lightweight \\textbf{Text Denoising Adapter (TDA)} to enhance the latter's resilience. Across the grid, MemeCLIP degrades more gently, while MemeBLIP2 is particularly sensitive to the caption edits that disrupt its language processing. However, the addition of the TDA not only remedies this weakness but makes MemeBLIP2 the most robust model overall. Ablations reveal that all systems lean heavily on text, but architectural choices and pre-training data significantly impact robustness. Our benchmark exposes where current multimodal safety models crack and demonstrates that targeted, lightweight modules like the TDA offer a powerful path towards stronger defences.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é’ˆå¯¹ LGBTQ+ ç¤¾åŒºçš„æœ‰å®³æ¨¡å›  (Hateful memes) å®¹æ˜“è§„é¿æ£€æµ‹çš„é—®é¢˜ï¼Œæ„å»ºäº†é¦–ä¸ªç¨³å¥æ€§åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å°†å››ç§æ–‡æœ¬æ”»å‡»ä¸ä¸‰ç§å›¾åƒæŸåç›¸ç»“åˆï¼Œåœ¨ PrideMM æ•°æ®é›†ä¸Šå¯¹ MemeCLIP å’Œ MemeBLIP2 è¿›è¡Œäº†å…¨é¢çš„å‹åŠ›æµ‹è¯•ã€‚ç ”ç©¶å‘ç° MemeCLIP çš„æ€§èƒ½ä¸‹é™è¾ƒä¸ºå¹³ç¼“ï¼Œè€Œ MemeBLIP2 å¯¹ç ´åè¯­è¨€å¤„ç†çš„æ–‡æœ¬ç¼–è¾‘å°¤ä¸ºæ•æ„Ÿã€‚ä¸ºäº†æå‡åè€…çš„éŸ§æ€§ï¼Œç ”ç©¶è€…å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„æ–‡æœ¬å»å™ªé€‚é…å™¨ (Text Denoising Adapter, TDA)ï¼Œå®éªŒè¯æ˜ TDA æ˜¾è‘—å¢å¼ºäº† MemeBLIP2 çš„ç¨³å¥æ€§å¹¶ä½¿å…¶æˆä¸ºç»¼åˆè¡¨ç°æœ€å¼ºçš„æ¨¡å‹ã€‚æ¶ˆèå®éªŒæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–æ–‡æœ¬ä¿¡æ¯ï¼ŒåŒæ—¶ä¹Ÿè¯æ˜äº†é’ˆå¯¹æ€§çš„è½»é‡çº§æ¨¡å—æ˜¯å¼ºåŒ–å¤šæ¨¡æ€å®‰å…¨æ¨¡å‹é˜²å¾¡èƒ½åŠ›çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "14 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2507.19551v3",
      "published_date": "2025-07-24 23:10:42 UTC",
      "updated_date": "2025-12-02 01:06:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:33:57.473950+00:00"
    },
    {
      "arxiv_id": "2507.18838v1",
      "title": "Flow Stochastic Segmentation Networks",
      "title_zh": "æµå¼éšæœºåˆ†å‰²ç½‘ç»œ",
      "authors": [
        "Fabio De Sousa Ribeiro",
        "Omar Todd",
        "Charles Jones",
        "Avinash Kori",
        "Raghav Mehta",
        "Ben Glocker"
      ],
      "abstract": "We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a generative segmentation model family featuring discrete-time autoregressive and modern continuous-time flow variants. We prove fundamental limitations of the low-rank parameterisation of previous methods and show that Flow-SSNs can estimate arbitrarily high-rank pixel-wise covariances without assuming the rank or storing the distributional parameters. Flow-SSNs are also more efficient to sample from than standard diffusion-based segmentation models, thanks to most of the model capacity being allocated to learning the base distribution of the flow, constituting an expressive prior. We apply Flow-SSNs to challenging medical imaging benchmarks and achieve state-of-the-art results. Code available: https://github.com/biomedia-mira/flow-ssn.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Flow Stochastic Segmentation Network (Flow-SSN)ï¼Œè¿™æ˜¯ä¸€ç§æ¶µç›–ç¦»æ•£æ—¶é—´è‡ªå›å½’ä¸è¿ç»­æ—¶é—´æµ(flow)å˜ä½“çš„ç”Ÿæˆå¼åˆ†å‰²æ¨¡å‹ç³»åˆ—ã€‚ä½œè€…è¯æ˜äº†å…ˆå‰æ–¹æ³•åœ¨ä½ç§©å‚æ•°åŒ–(low-rank parameterisation)æ–¹é¢çš„æ ¹æœ¬å±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†Flow-SSNsèƒ½å¤Ÿåœ¨ä¸é¢„è®¾ç§©æˆ–å­˜å‚¨åˆ†å¸ƒå‚æ•°çš„æƒ…å†µä¸‹ï¼Œä¼°è®¡ä»»æ„é«˜ç§©çš„åƒç´ çº§åæ–¹å·®(pixel-wise covariances)ã€‚ç›¸æ¯”äºæ ‡å‡†çš„åŸºäºæ‰©æ•£çš„åˆ†å‰²æ¨¡å‹(diffusion-based segmentation models)ï¼ŒFlow-SSNsé€šè¿‡å°†æ¨¡å‹å®¹é‡é›†ä¸­äºå­¦ä¹ æµçš„åŸºåˆ†å¸ƒä»¥æ„å»ºå¼ºè¡¨ç°åŠ›çš„å…ˆéªŒï¼Œæ˜¾è‘—æå‡äº†é‡‡æ ·æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒFlow-SSNsåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦å½±åƒåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ç›®å‰çš„æœ€é«˜æ°´å¹³(state-of-the-art)ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚åˆ†å‰²ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18838v1",
      "published_date": "2025-07-24 22:26:28 UTC",
      "updated_date": "2025-07-24 22:26:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:02.081710+00:00"
    },
    {
      "arxiv_id": "2507.18815v1",
      "title": "Deepfake Detection Via Facial Feature Extraction and Modeling",
      "title_zh": "åŸºäºé¢éƒ¨ç‰¹å¾æå–ä¸å»ºæ¨¡çš„æ·±åº¦ä¼ªé€ æ£€æµ‹",
      "authors": [
        "Benjamin Carter",
        "Nathan Dilla",
        "Micheal Callahan",
        "Atuhaire Ambala"
      ],
      "abstract": "The rise of deepfake technology brings forth new questions about the authenticity of various forms of media found online today. Videos and images generated by artificial intelligence (AI) have become increasingly more difficult to differentiate from genuine media, resulting in the need for new models to detect artificially-generated media. While many models have attempted to solve this, most focus on direct image processing, adapting a convolutional neural network (CNN) or a recurrent neural network (RNN) that directly interacts with the video image data. This paper introduces an approach of using solely facial landmarks for deepfake detection. Using a dataset consisting of both deepfake and genuine videos of human faces, this paper describes an approach for extracting facial landmarks for deepfake detection, focusing on identifying subtle inconsistencies in facial movements instead of raw image processing. Experimental results demonstrated that this feature extraction technique is effective in various neural network models, with the same facial landmarks tested on three neural network models, with promising performance metrics indicating its potential for real-world applications. The findings discussed in this paper include RNN and artificial neural network (ANN) models with accuracy between 96% and 93%, respectively, with a CNN model hovering around 78%. This research challenges the assumption that raw image processing is necessary to identify deepfake videos by presenting a facial feature extraction approach compatible with various neural network models while requiring fewer parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Deepfake æ£€æµ‹ä¸­çš„é¢éƒ¨ç‰¹å¾æå–ä¸å»ºæ¨¡é—®é¢˜ï¼Œæ—¨åœ¨åº”å¯¹æ—¥ç›Šé€¼çœŸçš„äººå·¥æ™ºèƒ½ç”Ÿæˆåª’ä½“å¸¦æ¥çš„çœŸå®æ€§æŒ‘æˆ˜ã€‚é’ˆå¯¹ä¸»æµæ¨¡å‹å¤šä¾èµ–ç›´æ¥å›¾åƒå¤„ç†ï¼ˆå¦‚ CNN æˆ– RNNï¼‰çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»…åˆ©ç”¨é¢éƒ¨å…³é”®ç‚¹ (facial landmarks) çš„æ£€æµ‹æ–¹æ³•ã€‚è¯¥æŠ€æœ¯é€šè¿‡æå–ç‰¹å¾æ¥è¯†åˆ«é¢éƒ¨è¿åŠ¨ä¸­çš„ç»†å¾®ä¸ä¸€è‡´æ€§ï¼Œè€Œéå¤„ç†åŸå§‹å›¾åƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä¸­ RNN å’Œ Artificial Neural Network (ANN) çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ° 96% å’Œ 93%ï¼Œè€Œ CNN çº¦ä¸º 78%ã€‚è¿™é¡¹ç ”ç©¶æŒ‘æˆ˜äº†å¿…é¡»è¿›è¡ŒåŸå§‹å›¾åƒå¤„ç†æ‰èƒ½è¯†åˆ« Deepfake çš„å‡è®¾ï¼Œè¯æ˜äº†è¯¥ç‰¹å¾æå–æ–¹æ³•åœ¨å‡å°‘å‚æ•°é‡çš„åŒæ—¶ï¼Œå…·å¤‡è·¨æ¨¡å‹çš„å…¼å®¹æ€§ä¸å®é™…åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Keywords: deepfake, facial recognition, feature extraction, artificial intelligence, recurrent neural network, convolutional neural network, artificial neural network",
      "pdf_url": "https://arxiv.org/pdf/2507.18815v1",
      "published_date": "2025-07-24 21:30:51 UTC",
      "updated_date": "2025-07-24 21:30:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:08.691101+00:00"
    },
    {
      "arxiv_id": "2507.18812v1",
      "title": "MemoCoder: Automated Function Synthesis using LLM-Supported Agents",
      "title_zh": "MemoCoderï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹æ”¯æŒæ™ºèƒ½ä½“çš„è‡ªåŠ¨åŒ–å‡½æ•°åˆæˆ",
      "authors": [
        "Yiping Jia",
        "Zhen Ming Jiang",
        "Shayan Noei",
        "Ying Zou"
      ],
      "abstract": "With the widespread adoption of Large Language Models (LLMs) such as GitHub Copilot and ChatGPT, developers increasingly rely on AI-assisted tools to support code generation. While LLMs can generate syntactically correct solutions for well-structured programming tasks, they often struggle with challenges that require iterative debugging, error handling, or adaptation to diverse problem structures. Existing approaches such as fine-tuning or self-repair strategies either require costly retraining or lack mechanisms to accumulate and reuse knowledge from previous attempts.\n  To address these limitations, we propose MemoCoder, a multi-agent framework that enables collaborative problem solving and persistent learning from past fixes. At the core of MemoCoder is a Fixing Knowledge Set, which stores successful repairs and supports retrieval for future tasks. A central Mentor Agent supervises the repair process by identifying recurring error patterns and refining high-level fixing strategies, providing a novel supervisory role that guides the self-repair loop. We evaluate MemoCoder across three public benchmarks -- MBPP, HumanEval, and LiveCodeBench -- spanning a range of problem complexities. Experimental results show that MemoCoder consistently outperforms both zero-shot prompting and a Self-Repair strategy, with improvements ranging from 3.1% to 12.1% in Pass@10 and from 1.4% to 14.5% in Pass@50, demonstrating its effectiveness in iterative refinement and knowledge-guided code generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä»£ç ç”Ÿæˆä¸­éš¾ä»¥åº”å¯¹è¿­ä»£è°ƒè¯•å’Œé”™è¯¯å¤„ç†ï¼Œä¸”ç°æœ‰æ–¹æ³•ç¼ºä¹çŸ¥è¯†ç§¯ç´¯ä¸é‡ç”¨æœºåˆ¶çš„é—®é¢˜ï¼Œæå‡ºäº† MemoCoderã€‚MemoCoder æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ (multi-agent framework)ï¼Œæ—¨åœ¨å®ç°ååŒé—®é¢˜è§£å†³å¹¶ä»è¿‡å»çš„ä¿®å¤ç»éªŒä¸­è¿›è¡ŒæŒä¹…åŒ–å­¦ä¹ ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¿®å¤çŸ¥è¯†é›† (Fixing Knowledge Set)ï¼Œç”¨äºå­˜å‚¨æˆåŠŸçš„ä¿®å¤æ–¹æ¡ˆä¾›æœªæ¥ä»»åŠ¡æ£€ç´¢ã€‚åŒæ—¶ï¼Œæ ¸å¿ƒçš„å¯¼å¸ˆæ™ºèƒ½ä½“ (Mentor Agent) é€šè¿‡è¯†åˆ«é‡å¤å‘ç”Ÿçš„é”™è¯¯æ¨¡å¼å¹¶æç‚¼é«˜å±‚ä¿®å¤ç­–ç•¥æ¥ç›‘ç£ä¿®å¤è¿‡ç¨‹ï¼Œå¼•å¯¼è‡ªæˆ‘ä¿®å¤å¾ªç¯ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ MBPPã€HumanEval å’Œ LiveCodeBench ä¸‰ä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMemoCoder åœ¨ Pass@10 å’Œ Pass@50 æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬æç¤º (zero-shot prompting) å’Œè‡ªæˆ‘ä¿®å¤ç­–ç•¥ (Self-Repair)ï¼Œæœ€é«˜æå‡è¾¾ 14.5%ï¼Œè¯æ˜äº†å…¶åœ¨çŸ¥è¯†å¼•å¯¼çš„ä»£ç ç”Ÿæˆå’Œè¿­ä»£ä¼˜åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18812v1",
      "published_date": "2025-07-24 21:23:44 UTC",
      "updated_date": "2025-07-24 21:23:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:12.585778+00:00"
    },
    {
      "arxiv_id": "2507.18802v1",
      "title": "DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition",
      "title_zh": "DxHFï¼šé€šè¿‡äº¤äº’å¼åˆ†è§£ä¸ºå¤§è¯­è¨€æ¨¡å‹å¯¹é½æä¾›é«˜è´¨é‡äººç±»åé¦ˆ",
      "authors": [
        "Danqing Shi",
        "Furui Cheng",
        "Tino Weinkauf",
        "Antti Oulasvirta",
        "Mennatallah El-Assady"
      ],
      "abstract": "Human preferences are widely used to align large language models (LLMs) through methods such as reinforcement learning from human feedback (RLHF). However, the current user interfaces require annotators to compare text paragraphs, which is cognitively challenging when the texts are long or unfamiliar. This paper contributes by studying the decomposition principle as an approach to improving the quality of human feedback for LLM alignment. This approach breaks down the text into individual claims instead of directly comparing two long-form text responses. Based on the principle, we build a novel user interface DxHF. It enhances the comparison process by showing decomposed claims, visually encoding the relevance of claims to the conversation and linking similar claims. This allows users to skim through key information and identify differences for better and quicker judgment. Our technical evaluation shows evidence that decomposition generally improves feedback accuracy regarding the ground truth, particularly for users with uncertainty. A crowdsourcing study with 160 participants indicates that using DxHF improves feedback accuracy by an average of 5%, although it increases the average feedback time by 18 seconds. Notably, accuracy is significantly higher in situations where users have less certainty. The finding of the study highlights the potential of HCI as an effective method for improving human-AI alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡åˆ†è§£åŸåˆ™(decomposition principle)æé«˜å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹é½ä¸­äººç±»åé¦ˆè´¨é‡çš„æ–¹æ³•ï¼Œå¹¶å¼€å‘äº†åä¸ºDxHFçš„åˆ›æ–°ç”¨æˆ·ç•Œé¢ã€‚é’ˆå¯¹é•¿æ–‡æœ¬å¯¹æ¯”ä¸­è®¤çŸ¥è´Ÿæ‹…é‡çš„é—®é¢˜ï¼ŒDxHFå°†æ–‡æœ¬å“åº”æ‹†åˆ†ä¸ºç‹¬ç«‹çš„claimï¼Œå¹¶é€šè¿‡è§†è§‰åŒ–ç¼–ç å…¶ä¸å¯¹è¯çš„ç›¸å…³æ€§åŠé“¾æ¥ç›¸ä¼¼claimã€‚è¿™ç§è®¾è®¡å…è®¸ç”¨æˆ·å¿«é€Ÿæµè§ˆå…³é”®ä¿¡æ¯å¹¶è¯†åˆ«å·®å¼‚ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®ã€æ›´è¿…é€Ÿçš„åˆ¤æ–­ã€‚æŠ€æœ¯è¯„ä¼°è¡¨æ˜ï¼Œåˆ†è§£åŸåˆ™æ˜¾è‘—æé«˜äº†åé¦ˆç›¸å¯¹äºground truthçš„å‡†ç¡®åº¦ï¼Œå°¤å…¶æ˜¯åœ¨ç”¨æˆ·æ„Ÿåˆ°ä¸ç¡®å®šæ—¶æ•ˆæœæ›´ä½³ã€‚å¯¹160åå‚ä¸è€…çš„ä¼—åŒ…ç ”ç©¶æ˜¾ç¤ºï¼Œä½¿ç”¨DxHFå¹³å‡å¯å°†åé¦ˆå‡†ç¡®ç‡æé«˜5%ï¼Œå°½ç®¡åé¦ˆæ—¶é—´å¹³å‡å¢åŠ äº†18ç§’ã€‚è¯¥ç ”ç©¶ç»“æœå¼ºè°ƒäº†äººæœºäº¤äº’(HCI)ä½œä¸ºæå‡äººç±»ä¸AIå¯¹é½(human-AI alignment)æœ‰æ•ˆæ‰‹æ®µçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18802v1",
      "published_date": "2025-07-24 21:01:24 UTC",
      "updated_date": "2025-07-24 21:01:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:14.684208+00:00"
    },
    {
      "arxiv_id": "2508.06497v1",
      "title": "Forecasting Commodity Price Shocks Using Temporal and Semantic Fusion of Prices Signals and Agentic Generative AI Extracted Economic News",
      "title_zh": "åŸºäºä»·æ ¼ä¿¡å·ä¸æ™ºèƒ½ä½“ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æå–ç»æµæ–°é—»çš„æ—¶é—´ä¸è¯­ä¹‰èåˆçš„å¤§å®—å•†å“ä»·æ ¼å†²å‡»é¢„æµ‹",
      "authors": [
        "Mohammed-Khalil Ghali",
        "Cecil Pang",
        "Oscar Molina",
        "Carlos Gershenson-Garcia",
        "Daehan Won"
      ],
      "abstract": "Accurate forecasting of commodity price spikes is vital for countries with limited economic buffers, where sudden increases can strain national budgets, disrupt import-reliant sectors, and undermine food and energy security. This paper introduces a hybrid forecasting framework that combines historical commodity price data with semantic signals derived from global economic news, using an agentic generative AI pipeline. The architecture integrates dual-stream Long Short-Term Memory (LSTM) networks with attention mechanisms to fuse structured time-series inputs with semantically embedded, fact-checked news summaries collected from 1960 to 2023. The model is evaluated on a 64-year dataset comprising normalized commodity price series and temporally aligned news embeddings. Results show that the proposed approach achieves a mean AUC of 0.94 and an overall accuracy of 0.91 substantially outperforming traditional baselines such as logistic regression (AUC = 0.34), random forest (AUC = 0.57), and support vector machines (AUC = 0.47). Additional ablation studies reveal that the removal of attention or dimensionality reduction leads to moderate declines in performance, while eliminating the news component causes a steep drop in AUC to 0.46, underscoring the critical value of incorporating real-world context through unstructured text. These findings demonstrate that integrating agentic generative AI with deep learning can meaningfully improve early detection of commodity price shocks, offering a practical tool for economic planning and risk mitigation in volatile market environments while saving the very high costs of operating a full generative AI agents pipeline.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå†å²å•†å“ä»·æ ¼æ•°æ®ä¸å…¨çƒç»æµæ–°é—»è¯­ä¹‰ä¿¡å·çš„æ··åˆé¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ Agentic Generative AI æµç¨‹æé«˜å¯¹å•†å“ä»·æ ¼å†²å‡»(Commodity Price Shocks)çš„é¢„æµ‹ç²¾åº¦ã€‚å…¶æ¶æ„é›†æˆäº†åŒæµé•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)ä¸æ³¨æ„åŠ›æœºåˆ¶(Attention Mechanisms)ï¼Œå°†ç»“æ„åŒ–æ—¶é—´åºåˆ—è¾“å…¥ä¸ä»1960å¹´è‡³2023å¹´é—´æå–çš„è¯­ä¹‰æ–°é—»åµŒå…¥è¿›è¡Œæ·±åº¦èåˆã€‚åœ¨é•¿è¾¾64å¹´çš„æ•°æ®é›†è¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡å‹å®ç°äº†0.94çš„å¹³å‡AUCå’Œ0.91çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºé€»è¾‘å›å½’ã€éšæœºæ£®æ—(Random Forest)å’Œæ”¯æŒå‘é‡æœº(SVM)ç­‰ä¼ ç»ŸåŸºå‡†ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œç§»é™¤æ–°é—»ç»„ä»¶ä¼šå¯¼è‡´é¢„æµ‹æ€§èƒ½å¤§å¹…æ»‘å¡ï¼ŒéªŒè¯äº†åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­å¼•å…¥éç»“æ„åŒ–æ–‡æœ¬è¯­å¢ƒå¯¹äºæ•æ‰å¸‚åœºé£é™©çš„å…³é”®ä½œç”¨ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†å°†æ™ºèƒ½ä½“ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸æ·±åº¦å­¦ä¹ ç›¸ç»“åˆèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„å•†å“ä»·æ ¼å†²å‡»çš„æ—©æœŸé¢„è­¦ï¼Œä¸ºå®è§‚ç»æµè§„åˆ’å’Œé£é™©ç¼“è§£æä¾›äº†ä¸€ç§å…¼é¡¾ç²¾ç¡®åº¦ä¸æˆæœ¬æ•ˆç›Šçš„å®ç”¨å·¥å…·ã€‚",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.CP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.06497v1",
      "published_date": "2025-07-24 20:52:47 UTC",
      "updated_date": "2025-07-24 20:52:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:17.482269+00:00"
    },
    {
      "arxiv_id": "2507.18795v1",
      "title": "Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization",
      "title_zh": "æ’é˜Ÿç½‘ç»œè·¯ç”±ä¼˜åŒ–ä¸­çš„ä»¿çœŸé©±åŠ¨å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Fatima Al-Ani",
        "Molly Wang",
        "Jevon Charles",
        "Aaron Ong",
        "Joshua Forday",
        "Vinayak Modi"
      ],
      "abstract": "This study focuses on the development of a simulation-driven reinforcement learning (RL) framework for optimizing routing decisions in complex queueing network systems, with a particular emphasis on manufacturing and communication applications. Recognizing the limitations of traditional queueing methods, which often struggle with dynamic, uncertain environments, we propose a robust RL approach leveraging Deep Deterministic Policy Gradient (DDPG) combined with Dyna-style planning (Dyna-DDPG). The framework includes a flexible and configurable simulation environment capable of modeling diverse queueing scenarios, disruptions, and unpredictable conditions. Our enhanced Dyna-DDPG implementation incorporates separate predictive models for next-state transitions and rewards, significantly improving stability and sample efficiency. Comprehensive experiments and rigorous evaluations demonstrate the framework's capability to rapidly learn effective routing policies that maintain robust performance under disruptions and scale effectively to larger network sizes. Additionally, we highlight strong software engineering practices employed to ensure reproducibility and maintainability of the framework, enabling practical deployment in real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ¶é€ ä¸šå’Œé€šä¿¡é¢†åŸŸä¸­å¤æ‚æ’é˜Ÿç½‘ç»œ(Queuing Network)ç³»ç»Ÿçš„è·¯ç”±å†³ç­–ä¼˜åŒ–é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ä¸ªä»¿çœŸé©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿæ’é˜Ÿæ–¹æ³•åœ¨åŠ¨æ€ä¸ç¡®å®šç¯å¢ƒä¸‹çš„å±€é™æ€§ï¼Œè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆäº†Dynaé£æ ¼è§„åˆ’çš„æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ç®—æ³•(Dyna-DDPG)ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªçµæ´»ä¸”å¯é…ç½®çš„ä»¿çœŸç¯å¢ƒï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿå¤šæ ·çš„æ’é˜Ÿåœºæ™¯ã€çªå‘ä¸­æ–­åŠä¸å¯é¢„æµ‹çš„å·¥å†µã€‚é€šè¿‡ä¸ºçŠ¶æ€è½¬ç§»å’Œå¥–åŠ±å»ºç«‹ç‹¬ç«‹çš„é¢„æµ‹æ¨¡å‹ï¼Œå¢å¼ºå‹Dyna-DDPGæ˜¾è‘—æå‡äº†ç®—æ³•çš„ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡(Sample Efficiency)ã€‚å®éªŒè¯„ä¼°è¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¿«é€Ÿå­¦ä¹ å¹¶ç”Ÿæˆæœ‰æ•ˆçš„è·¯ç”±ç­–ç•¥ï¼Œåœ¨å„ç§å¹²æ‰°ä¸‹è¡¨ç°å‡ºå¼ºåŠ²çš„é²æ£’æ€§ï¼Œå¹¶èƒ½æˆåŠŸæ‰©å±•è‡³æ›´å¤§è§„æ¨¡çš„ç½‘ç»œã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡ä¸¥è°¨çš„è½¯ä»¶å·¥ç¨‹å®è·µç¡®ä¿äº†æ¡†æ¶çš„å¯é‡å¤æ€§å’Œå¯ç»´æŠ¤æ€§ï¼Œä¸ºå…¶åœ¨ç°å®å·¥ä¸šç¯å¢ƒä¸­çš„éƒ¨ç½²å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18795v1",
      "published_date": "2025-07-24 20:32:47 UTC",
      "updated_date": "2025-07-24 20:32:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:18.882595+00:00"
    },
    {
      "arxiv_id": "2507.18788v2",
      "title": "When Better Eyes Lead to Blindness: A Diagnostic Study of the Information Bottleneck in CNN-LSTM Image Captioning Models",
      "title_zh": "è§†é‡æå‡åè‡´ç›²è§†ï¼šCNN-LSTM å›¾åƒæè¿°æ¨¡å‹ä¸­çš„ä¿¡æ¯ç“¶é¢ˆè¯Šæ–­ç ”ç©¶",
      "authors": [
        "Hitesh Kumar Gupta"
      ],
      "abstract": "Image captioning, situated at the intersection of computer vision and natural language processing, requires a sophisticated understanding of both visual scenes and linguistic structure. While modern approaches are dominated by large-scale Transformer architectures, this paper documents a systematic, iterative development of foundational image captioning models, progressing from a simple CNN-LSTM encoder-decoder to a competitive attention-based system. This paper presents a series of five models, beginning with Genesis and concluding with Nexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic attention mechanism. The experiments chart the impact of architectural enhancements and demonstrate a key finding within the classic CNN-LSTM paradigm: merely upgrading the visual backbone without a corresponding attention mechanism can degrade performance, as the single-vector bottleneck cannot transmit the richer visual detail. This insight validates the architectural shift to attention. Trained on the MS COCO 2017 dataset, the final model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several foundational benchmarks and validating the iterative design process. This work provides a clear, replicable blueprint for understanding the core architectural principles that underpin modern vision-language tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾åƒæè¿° (Image Captioning) æ¨¡å‹ä»åŸºç¡€çš„ CNN-LSTM ç¼–ç å™¨-è§£ç å™¨æ¶æ„åˆ°å…ˆè¿›æ³¨æ„åŠ›æœºåˆ¶ç³»ç»Ÿçš„æ¼”è¿›è¿‡ç¨‹ã€‚ä½œè€…é€šè¿‡å¼€å‘ä» Genesis åˆ° Nexus çš„äº”ä¸ªè¿­ä»£æ¨¡å‹ï¼Œç³»ç»Ÿåœ°åˆ†æäº†æ¶æ„æ”¹è¿›å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°äº†ä¸€ä¸ªå…³é”®ç°è±¡ï¼šåœ¨ç»å…¸çš„ CNN-LSTM èŒƒå¼ä¸­ï¼Œå¦‚æœä»…å‡çº§è§†è§‰éª¨å¹²ç½‘ç»œ (backbone) è€Œä¸å¼•å…¥ç›¸åº”çš„æ³¨æ„åŠ›æœºåˆ¶ (attention mechanism)ï¼Œå•å‘é‡ç“¶é¢ˆ (single-vector bottleneck) ä¼šå¯¼è‡´æ— æ³•ä¼ è¾“ä¸°å¯Œçš„è§†è§‰ç»†èŠ‚ï¼Œè¿›è€Œé™ä½æ¨¡å‹æ€§èƒ½ã€‚æœ€ç»ˆæ¨¡å‹ Nexus ç»“åˆäº† EfficientNetV2B3 éª¨å¹²ç½‘ç»œä¸åŠ¨æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ MS COCO 2017 æ•°æ®é›†ä¸Šå®ç°äº† 31.4 çš„ BLEU-4 åˆ†æ•°ã€‚è¯¥å·¥ä½œä¸ä»…è¶…è¶Šäº†å¤šä¸ªåŸºç¡€åŸºå‡†ï¼Œè¿˜ä¸ºç†è§£è§†è§‰-è¯­è¨€ä»»åŠ¡çš„æ ¸å¿ƒæ¶æ„åŸåˆ™æä¾›äº†æ¸…æ™°ä¸”å¯å¤ç°çš„è“å›¾ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is published in International Journal of Computer Applications (IJCA), Vol. 187, No. 31, August 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18788v2",
      "published_date": "2025-07-24 20:20:44 UTC",
      "updated_date": "2025-08-20 19:21:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:27.984953+00:00"
    },
    {
      "arxiv_id": "2507.22936v2",
      "title": "Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis",
      "title_zh": "é‡‘èè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¯„ä¼°ï¼šè´¢åŠ¡æŠ¥å‘Šåˆ†æå¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Md Talha Mohsin"
      ],
      "abstract": "Large language models (LLMs) are increasingly used to support the analysis of complex financial disclosures, yet their reliability, behavioral consistency, and transparency remain insufficiently understood in high-stakes settings. This paper presents a controlled evaluation of five transformer-based LLMs applied to question answering over the Business sections of U.S. 10-K filings. To capture complementary aspects of model behavior, we combine human evaluation, automated similarity metrics, and behavioral diagnostics under standardized and context-controlled prompting conditions. Human assessments indicate that models differ in their average performance across qualitative dimensions such as relevance, completeness, clarity, conciseness, and factual accuracy, though inter-rater agreement is modest, reflecting the subjective nature of these criteria. Automated metrics reveal systematic differences in lexical overlap and semantic similarity across models, while behavioral diagnostics highlight variation in response stability and cross-prompt alignment. Importantly, no single model consistently dominates across all evaluation perspectives. Together, these findings suggest that apparent performance differences should be interpreted as relative tendencies under the tested conditions rather than definitive indicators of general reliability. The results underscore the need for evaluation frameworks that account for human disagreement, behavioral variability, and interpretability when deploying LLMs in financially consequential applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹äº”ç§åŸºäº Transformer çš„ Large Language Models (LLMs) åœ¨ç¾å›½ 10-K æŠ¥è¡¨ Business éƒ¨åˆ†çš„é—®ç­”ä»»åŠ¡è¿›è¡Œå—æ§è¯„ä¼°ï¼Œæ¢è®¨äº†å…¶åœ¨é‡‘èæŠ¥å‘Šåˆ†æä¸­çš„å¯é æ€§ã€è¡Œä¸ºä¸€è‡´æ€§å’Œé€æ˜åº¦ã€‚ç ”ç©¶ç»“åˆäº†äººå·¥è¯„ä»· (Human evaluation)ã€è‡ªåŠ¨ç›¸ä¼¼æ€§æŒ‡æ ‡å’Œè¡Œä¸ºè¯Šæ–­ (Behavioral diagnostics)ï¼Œåœ¨æ ‡å‡†åŒ–å’Œå—æ§è¯­å¢ƒæç¤ºæ¡ä»¶ä¸‹æ•æ‰æ¨¡å‹è¡Œä¸ºçš„å¤šç»´ç‰¹å¾ã€‚äººå·¥è¯„ä¼°æ˜¾ç¤ºæ¨¡å‹åœ¨ç›¸å…³æ€§ã€å®Œæ•´æ€§ã€æ¸…æ™°åº¦ã€ç®€æ´æ€§å’Œäº‹å®å‡†ç¡®æ€§ç­‰ç»´åº¦ä¸Šè¡¨ç°å„å¼‚ï¼Œä½†ç”±äºæ ‡å‡†çš„ä¸»è§‚æ€§ï¼Œè¯„ä»·è€…é—´çš„ä¸€è‡´æ€§æœ‰é™ã€‚è‡ªåŠ¨æŒ‡æ ‡æ­ç¤ºäº†æ¨¡å‹é—´åœ¨è¯æ±‡é‡å å’Œè¯­ä¹‰ç›¸ä¼¼åº¦ä¸Šçš„ç³»ç»Ÿæ€§å·®å¼‚ï¼Œè¡Œä¸ºè¯Šæ–­åˆ™å‡¸æ˜¾äº†å“åº”ç¨³å®šæ€§ä¸è·¨æç¤ºå¯¹é½ (Cross-prompt alignment) æ–¹é¢çš„æ³¢åŠ¨ã€‚ç»“æœè¡¨æ˜æ²¡æœ‰ä»»ä½•å•ä¸€æ¨¡å‹èƒ½åœ¨æ‰€æœ‰è¯„ä¼°ç»´åº¦ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œæ¨¡å‹è¡¨ç°åº”è¢«è§†ä¸ºç‰¹å®šæµ‹è¯•æ¡ä»¶ä¸‹çš„ç›¸å¯¹è¶‹åŠ¿è€Œéç»å¯¹å¯é æ€§æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶æœ€åå¼ºè°ƒäº†åœ¨é‡‘èç­‰é«˜é£é™©é¢†åŸŸéƒ¨ç½² LLMs æ—¶ï¼Œå»ºç«‹èƒ½å…¼é¡¾äººå·¥åˆ†æ­§ã€è¡Œä¸ºå˜å¼‚æ€§å’Œå¯è§£é‡Šæ€§çš„è¯„ä¼°æ¡†æ¶çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.HC",
        "q-fin.CP"
      ],
      "primary_category": "cs.CL",
      "comment": "23 Pages",
      "pdf_url": "https://arxiv.org/pdf/2507.22936v2",
      "published_date": "2025-07-24 20:10:27 UTC",
      "updated_date": "2026-01-19 21:50:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:41.698491+00:00"
    },
    {
      "arxiv_id": "2507.18775v1",
      "title": "Initial Steps in Integrating Large Reasoning and Action Models for Service Composition",
      "title_zh": "é¢å‘æœåŠ¡ç»„åˆçš„å¤§å‹æ¨ç†ä¸åŠ¨ä½œæ¨¡å‹é›†æˆåˆæ¢",
      "authors": [
        "Ilche Georgievski",
        "Marco Aiello"
      ],
      "abstract": "Service composition remains a central challenge in building adaptive and intelligent software systems, often constrained by limited reasoning capabilities or brittle execution mechanisms. This paper explores the integration of two emerging paradigms enabled by large language models: Large Reasoning Models (LRMs) and Large Action Models (LAMs). We argue that LRMs address the challenges of semantic reasoning and ecosystem complexity while LAMs excel in dynamic action execution and system interoperability. However, each paradigm has complementary limitations - LRMs lack grounded action capabilities, and LAMs often struggle with deep reasoning. We propose an integrated LRM-LAM architectural framework as a promising direction for advancing automated service composition. Such a system can reason about service requirements and constraints while dynamically executing workflows, thus bridging the gap between intention and execution. This integration has the potential to transform service composition into a fully automated, user-friendly process driven by high-level natural language intent.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨Service Compositioné¢†åŸŸé›†æˆLarge Reasoning Models (LRMs)ä¸Large Action Models (LAMs)çš„åˆæ­¥æ­¥éª¤ï¼Œä»¥è§£å†³ä¼ ç»Ÿç³»ç»Ÿåœ¨æ¨ç†èƒ½åŠ›å’Œæ‰§è¡Œæœºåˆ¶æ–¹é¢çš„å±€é™æ€§ã€‚è®ºæ–‡æŒ‡å‡ºï¼ŒLRMsæ“…é•¿å¤„ç†è¯­ä¹‰æ¨ç†å’Œå¤æ‚çš„ç”Ÿæ€ç³»ç»Ÿéœ€æ±‚ï¼Œè€ŒLAMsåˆ™åœ¨åŠ¨æ€åŠ¨ä½œæ‰§è¡Œå’Œç³»ç»Ÿäº’æ“ä½œæ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚ä¸ºäº†å…‹æœLRMsç¼ºä¹è½åœ°æ‰§è¡Œèƒ½åŠ›ä»¥åŠLAMséš¾ä»¥è¿›è¡Œæ·±åº¦æ¨ç†çš„äº’è¡¥æ€§ç¼ºé™·ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§é›†æˆçš„LRM-LAMæ¶æ„æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿç²¾å‡†ç†è§£æœåŠ¡éœ€æ±‚ä¸çº¦æŸï¼Œå¹¶åŒæ­¥åŠ¨æ€æ‰§è¡Œå·¥ä½œæµï¼Œä»è€Œæœ‰æ•ˆå¼¥åˆäº†ç”¨æˆ·æ„å›¾ä¸å®é™…æ‰§è¡Œä¹‹é—´çš„é¸¿æ²Ÿã€‚è¿™ä¸€é›†æˆæ–¹æ¡ˆæœ‰æœ›å°†Service Compositionè½¬å˜ä¸ºç”±é«˜å±‚è‡ªç„¶è¯­è¨€æ„å›¾é©±åŠ¨çš„å…¨è‡ªåŠ¨è¿‡ç¨‹ï¼Œä¸ºå¼€å‘é«˜åº¦è‡ªé€‚åº”çš„æ™ºèƒ½è½¯ä»¶ç³»ç»Ÿæä¾›äº†åˆ›æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 3 figures, 19th Symposium and Summer School on Service-Oriented Computing (SummerSOC)",
      "pdf_url": "https://arxiv.org/pdf/2507.18775v1",
      "published_date": "2025-07-24 19:57:18 UTC",
      "updated_date": "2025-07-24 19:57:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:42.523776+00:00"
    },
    {
      "arxiv_id": "2507.18755v1",
      "title": "Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback",
      "title_zh": "é¢å‘å¤§è§„æ¨¡æµ‹è¯•å¤±è´¥çš„æ™ºèƒ½ä½“ç¨‹åºä¿®å¤ï¼šä¸€ç§ç»“åˆé™æ€åˆ†æä¸æµ‹è¯•æ‰§è¡Œåé¦ˆçš„ç¥ç»ç¬¦å·æ–¹æ³•",
      "authors": [
        "Chandra Maddila",
        "Adam Tait",
        "Claire Chang",
        "Daniel Cheng",
        "Nauman Ahmad",
        "Vijayaraghavan Murali",
        "Marshall Roch",
        "Arnaud Avondet",
        "Aaron Meltzer",
        "Victor Montalvao",
        "Michael Hopko",
        "Chris Waterson",
        "Parth Thakkar",
        "Renuka Fernandez",
        "Kristian Kristensen",
        "Sivan Barzily",
        "Sherry Chen",
        "Rui Abreu",
        "Nachiappan Nagappan",
        "Payam Shodjai",
        "Killian Murphy",
        "James Everingham",
        "Aparna Ramani",
        "Peter C. Rigby"
      ],
      "abstract": "Aim: With the advent of LLMs, sophisticated agentic program repair has become viable at large organizations with large codebases. In this work, we develop an Engineering Agent that fixes the source code based on test failures at scale across diverse software offerings internally.\n  Method: Using Llama as the base, we employ the ReAct harness to develop an agent. We start with a test failure that was triaged by a rule-based test failure bot. We then set up an agentic harness and allow the agent to reason and run a set of 15 actions from reading a file to generating a patch. We provide feedback to the agent through static analysis and test failures so it can refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch conforms to the standards followed by a human review to land fixes.\n  Benchmark Findings: We curated offline benchmarks for our patch generator, the Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we found that a specialized 70B model is highly competitive with the much larger but vanilla Llama-405B. In an ablation study, we found that the ReAct harness (neural model) benefited from the symbolic information from static analysis tools and test execution traces. A model that strikes a balance between the solve rate and error rate vs the cost and latency has a benchmark solve rate of 42.3% using an average 11.8 feedback iterations.\n  Production Findings: In a three month period, 80% of the generated fixes were reviewed, of which 31.5% were landed (25.5% of the total number of generated fixes).\n  Feedback from Engineers: We used open coding to extract qualitative themes from engineers' feedback. We saw positive feedback in the form of quick approvals, gratitude, and surprise. We also found mixed feedback when the Engineering Agent's solution was partially correct and it served as a good starting point.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§åä¸ºEngineering Agentçš„è‡ªåŠ¨ç¨‹åºä¿®å¤æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨åˆ©ç”¨æµ‹è¯•å¤±è´¥åé¦ˆåœ¨å¤§è§„æ¨¡ä»£ç åº“ä¸­å®ç°è‡ªåŠ¨æºä»£ç ä¿®å¤ã€‚è¯¥ç³»ç»Ÿä»¥Llamaä¸ºåŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨ReActæ¡†æ¶ï¼Œé€šè¿‡åŒ…å«15ç§æ“ä½œçš„åŠ¨ä½œé›†è¿›è¡Œæ¨ç†ä¸æ‰§è¡Œã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§Neuro-symbolicæ–¹æ³•ï¼Œå°†ç¥ç»æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸static analysiså·¥å…·åŠæµ‹è¯•æ‰§è¡Œè¿½è¸ªæä¾›çš„ç¬¦å·åŒ–åé¦ˆç›¸ç»“åˆï¼Œä»è€Œä¸æ–­ä¼˜åŒ–ä¿®å¤æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜å¼•å…¥äº†LLM-as-a-Judgeæœºåˆ¶ï¼Œé€šè¿‡æ¨¡ä»¿äººç±»è¯„å®¡çš„æ ‡å‡†æ¥ç¡®ä¿ç”Ÿæˆè¡¥ä¸çš„è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡ä¸“é—¨è®­ç»ƒçš„70Bæ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¯ä¸æ›´å¤§çš„Llama-405Bç›¸åª²ç¾ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†42.3%çš„ä¿®å¤æˆåŠŸç‡ã€‚åœ¨ä¸ºæœŸä¸‰ä¸ªæœˆçš„ç”Ÿäº§ç¯å¢ƒæµ‹è¯•ä¸­ï¼Œè¯¥æ™ºèƒ½ä½“ç”Ÿæˆçš„è¡¥ä¸ä¸­æœ‰25.5%è¢«æœ€ç»ˆé‡‡çº³å¹¶åˆå¹¶ï¼Œå±•ç°äº†æ˜¾è‘—çš„å·¥ä¸šå®ç”¨ä»·å€¼ã€‚å·¥ç¨‹å¸ˆçš„å®šæ€§åé¦ˆä¹Ÿè¯å®ï¼Œè¯¥å·¥å…·åœ¨æä¾›å‡†ç¡®ä¿®å¤çš„åŒæ—¶ï¼Œä¹Ÿèƒ½ä½œä¸ºä¼˜ç§€çš„è¾…åŠ©èµ·ç‚¹æå‡æ•´ä½“å¼€å‘æ•ˆç‡ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18755v1",
      "published_date": "2025-07-24 19:12:32 UTC",
      "updated_date": "2025-07-24 19:12:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:47.735269+00:00"
    },
    {
      "arxiv_id": "2507.18742v1",
      "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement",
      "title_zh": "è§„èŒƒè‡ªæˆ‘ä¿®æ­£ï¼šé€šè¿‡æµ‹è¯•æ—¶ç²¾ç‚¼ç¼“è§£ä¸Šä¸‹æ–‡å¥–åŠ±é’»è¥",
      "authors": [
        "VÃ­ctor Gallego"
      ],
      "abstract": "Language models (LMs) are susceptible to in-context reward hacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve high scores without fulfilling the user's true intent. We introduce Specification Self-Correction (SSC), a novel, test-time framework that enables an LM to identify and correct flaws within its own guiding specification. SSC employs a multi-step inference process where the model first generates a response based on a potentially tainted specification, critiques its output, and then revises the specification itself to remove the exploitable loophole. A final, more robust response is then generated using this self-corrected specification. Across experiments spanning creative writing and agentic coding tasks with several LMs, we demonstrate that while models initially game tainted specifications in 50-70\\% of cases, the SSC process reduces this vulnerability by over 90\\%. This dynamic repair occurs at inference time, requires no weight modification, and leads to more robustly aligned model behavior. Code at https://github.com/vicgalle/specification-self-correction .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­è¨€æ¨¡å‹(LMs)å®¹æ˜“åˆ©ç”¨æœ‰ç¼ºé™·çš„è§„èŒƒ(specifications)æˆ–è¯„ä¼°æ ‡å‡†(rubrics)è¿›è¡Œè¯­å¢ƒä¸‹å¥–åŠ±æ“çºµ(in-context reward hacking)çš„é—®é¢˜ï¼Œæå‡ºäº†è§„èŒƒè‡ªæˆ‘ä¿®æ­£(Specification Self-Correction, SSC)æ¡†æ¶ã€‚SSCæ˜¯ä¸€ç§æ–°é¢–çš„æ¨ç†é˜¶æ®µ(test-time)æ¡†æ¶ï¼Œå…è®¸æ¨¡å‹è¯†åˆ«å¹¶ä¿®æ­£å…¶æŒ‡å¯¼è§„èŒƒä¸­çš„ç¼ºé™·ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šæ­¥æ¨ç†è¿‡ç¨‹ï¼Œé¦–å…ˆæ ¹æ®æ½œåœ¨æœ‰ç¼ºé™·çš„è§„èŒƒç”Ÿæˆåˆå§‹å“åº”ï¼Œæ¥ç€å¯¹å…¶è¾“å‡ºè¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä¼°å¹¶ä¿®è®¢è§„èŒƒä»¥æ¶ˆé™¤å¯åˆ©ç”¨çš„æ¼æ´ï¼Œæœ€ååˆ©ç”¨ä¿®æ­£åçš„è§„èŒƒç”Ÿæˆæ›´ç¨³å¥çš„æœ€ç»ˆå“åº”ã€‚åœ¨åˆ›æ„å†™ä½œå’Œæ™ºèƒ½ä½“ç¼–ç¨‹ä»»åŠ¡çš„å®éªŒä¸­ï¼ŒSSCå°†æ¨¡å‹å¯¹å—æ±¡æŸ“è§„èŒƒçš„æ¼æ´åˆ©ç”¨ç‡é™ä½äº†90%ä»¥ä¸Šã€‚è¿™ç§åŠ¨æ€ä¿®å¤åœ¨æ¨ç†é˜¶æ®µå®Œæˆï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹è¡Œä¸ºçš„ç¨³å¥å¯¹é½(robust alignment)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to SCALR Workshop @ COLM 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18742v1",
      "published_date": "2025-07-24 18:44:28 UTC",
      "updated_date": "2025-07-24 18:44:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:49.093702+00:00"
    },
    {
      "arxiv_id": "2507.18732v1",
      "title": "Multi-Year Maintenance Planning for Large-Scale Infrastructure Systems: A Novel Network Deep Q-Learning Approach",
      "title_zh": "å¤§è§„æ¨¡åŸºç¡€è®¾æ–½ç³»ç»Ÿå¤šå¹´æœŸå…»æŠ¤è§„åˆ’ï¼šä¸€ç§æ–°å‹ç½‘ç»œæ·±åº¦Qå­¦ä¹ æ–¹æ³•",
      "authors": [
        "Amir Fard",
        "Arnold X. -X. Yuan"
      ],
      "abstract": "Infrastructure asset management is essential for sustaining the performance of public infrastructure such as road networks, bridges, and utility networks. Traditional maintenance and rehabilitation planning methods often face scalability and computational challenges, particularly for large-scale networks with thousands of assets under budget constraints. This paper presents a novel deep reinforcement learning (DRL) framework that optimizes asset management strategies for large infrastructure networks. By decomposing the network-level Markov Decision Process (MDP) into individual asset-level MDPs while using a unified neural network architecture, the proposed framework reduces computational complexity, improves learning efficiency, and enhances scalability. The framework directly incorporates annual budget constraints through a budget allocation mechanism, ensuring maintenance plans are both optimal and cost-effective. Through a case study on a large-scale pavement network of 68,800 segments, the proposed DRL framework demonstrates significant improvements over traditional methods like Progressive Linear Programming and genetic algorithms, both in efficiency and network performance. This advancement contributes to infrastructure asset management and the broader application of reinforcement learning in complex, large-scale environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹åŸºç¡€è®¾æ–½ç³»ç»Ÿåœ¨é¢„ç®—çº¦æŸä¸‹è¿›è¡Œå¤šå¹´ç»´æŠ¤è§„åˆ’æ—¶é¢ä¸´çš„å¯æ‰©å±•æ€§å’Œè®¡ç®—æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning, DRL)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç½‘ç»œçº§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process, MDP)åˆ†è§£ä¸ºå•ä¸ªèµ„äº§çº§çš„MDPï¼Œå¹¶é‡‡ç”¨ç»Ÿä¸€çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæœ‰æ•ˆé™ä½äº†è®¡ç®—å¤æ‚æ€§å¹¶æ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡ã€‚é€šè¿‡å¼•å…¥é¢„ç®—åˆ†é…æœºåˆ¶ï¼Œè¯¥æ¡†æ¶èƒ½ç›´æ¥å¤„ç†å¹´åº¦é¢„ç®—çº¦æŸï¼Œç¡®ä¿ç»´æŠ¤è®¡åˆ’åœ¨æˆæœ¬æ•ˆç›Šå’Œæ€§èƒ½ä¹‹é—´è¾¾æˆæœ€ä¼˜ã€‚åœ¨åŒ…å«68,800ä¸ªè·¯æ®µçš„å¤§è§„æ¨¡è·¯é¢ç½‘ç»œæ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œè¯¥DRLæ¡†æ¶åœ¨å¤„ç†æ•ˆç‡å’Œç½‘ç»œæ€§èƒ½æå‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ¸è¿›çº¿æ€§è§„åˆ’(Progressive Linear Programming)å’Œé—ä¼ ç®—æ³•(Genetic Algorithms)ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºå¤æ‚å¤§è§„æ¨¡ç¯å¢ƒä¸‹çš„åŸºç¡€è®¾æ–½èµ„äº§ç®¡ç†æä¾›äº†é«˜æ•ˆçš„å†³ç­–æ”¯æŒï¼Œå¹¶æ‹“å±•äº†å¼ºåŒ–å­¦ä¹ åœ¨å·¥ç¨‹ç®¡ç†é¢†åŸŸçš„åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18732v1",
      "published_date": "2025-07-24 18:27:31 UTC",
      "updated_date": "2025-07-24 18:27:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:56.928612+00:00"
    },
    {
      "arxiv_id": "2507.18725v2",
      "title": "The Right to be Forgotten in Pruning: Unveil Machine Unlearning on Sparse Models",
      "title_zh": "å‰ªæä¸­çš„è¢«é—å¿˜æƒï¼šæ­ç¤ºç¨€ç–æ¨¡å‹ä¸­çš„æœºå™¨é—å¿˜",
      "authors": [
        "Yang Xiao",
        "Gen Li",
        "Jie Ji",
        "Ruimeng Ye",
        "Xiaolong Ma",
        "Bo Hui"
      ],
      "abstract": "Machine unlearning aims to efficiently eliminate the memory about deleted data from trained models and address the right to be forgotten. Despite the success of existing unlearning algorithms, unlearning in sparse models has not yet been well studied. In this paper, we empirically find that the deleted data has an impact on the pruned topology in a sparse model. Motivated by the observation and the right to be forgotten, we define a new terminology ``un-pruning\" to eliminate the impact of deleted data on model pruning. Then we propose an un-pruning algorithm to approximate the pruned topology driven by retained data. We remark that any existing unlearning algorithm can be integrated with the proposed un-pruning workflow and the error of un-pruning is upper-bounded in theory. Also, our un-pruning algorithm can be applied to both structured sparse models and unstructured sparse models. In the experiment, we further find that Membership Inference Attack (MIA) accuracy is unreliable for assessing whether a model has forgotten deleted data, as a small change in the amount of deleted data can produce arbitrary MIA results. Accordingly, we devise new performance metrics for sparse models to evaluate the success of un-pruning. Lastly, we conduct extensive experiments to verify the efficacy of un-pruning with various pruning methods and unlearning algorithms. Our code is released at https://github.com/NKUShaw/SparseModels .",
      "tldr_zh": "æœ¬æ–‡æ¢è®¨äº†ç¨€ç–æ¨¡å‹ï¼ˆSparse Modelsï¼‰ä¸­çš„æœºå™¨å¸è½½ï¼ˆMachine Unlearningï¼‰é—®é¢˜ï¼Œå‘ç°è¢«åˆ é™¤çš„æ•°æ®ä¼šå¯¹å‰ªæåçš„æ‹“æ‰‘ç»“æ„ï¼ˆPruned Topologyï¼‰äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚é’ˆå¯¹è¿™ä¸€ç°è±¡ï¼Œç ”ç©¶è€…å®šä¹‰äº†â€œåå‰ªæâ€ï¼ˆUn-pruningï¼‰è¿™ä¸€æ–°æ¦‚å¿µï¼Œæ—¨åœ¨æ¶ˆé™¤è¢«åˆ æ•°æ®å¯¹æ¨¡å‹å‰ªæçš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºä¿ç•™æ•°æ®æ¥é€¼è¿‘å‰ªææ‹“æ‰‘çš„ç®—æ³•ã€‚è¯¥ Un-pruning å·¥ä½œæµå¯ä¸ç°æœ‰çš„æœºå™¨å¸è½½ç®—æ³•æ— ç¼é›†æˆï¼Œä¸”åœ¨ç†è®ºä¸Šè¯æ˜äº†å…¶è¯¯å·®å…·æœ‰ä¸Šç•Œï¼ŒåŒæ—¶é€‚ç”¨äºç»“æ„åŒ–å’Œéç»“æ„åŒ–ç¨€ç–æ¨¡å‹ã€‚å®éªŒè¿›ä¸€æ­¥æ­ç¤ºäº†æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMembership Inference Attack, MIAï¼‰åœ¨è¯„ä¼°ç¨€ç–æ¨¡å‹å¸è½½æ•ˆæœæ—¶çš„ä¸å¯é æ€§ï¼Œå› ä¸ºæå°çš„æ•°æ®é‡å˜åŒ–å°±å¯èƒ½å¯¼è‡´ä¸å¯é¢„æµ‹çš„ MIA ç»“æœã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸“é—¨é’ˆå¯¹ç¨€ç–æ¨¡å‹çš„æ–°è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æ›´å‡†ç¡®åœ°è¡¡é‡ Un-pruning çš„æˆåŠŸç¨‹åº¦ã€‚æœ€åï¼Œé€šè¿‡åœ¨å¤šç§å‰ªææ–¹æ³•å’Œå¸è½½ç®—æ³•ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº† Un-pruning æ¡†æ¶åœ¨å®ç°ç¨€ç–æ¨¡å‹æ•°æ®â€œè¢«é—å¿˜æƒâ€æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages for main part",
      "pdf_url": "https://arxiv.org/pdf/2507.18725v2",
      "published_date": "2025-07-24 18:13:26 UTC",
      "updated_date": "2025-12-02 19:35:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:34:57.123912+00:00"
    },
    {
      "arxiv_id": "2507.18632v1",
      "title": "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation",
      "title_zh": "SIDAï¼šåˆæˆå›¾åƒé©±åŠ¨çš„é›¶æ ·æœ¬é¢†åŸŸè‡ªé€‚åº”",
      "authors": [
        "Ye-Chan Kim",
        "SeungJu Cha",
        "Si-Woo Kim",
        "Taewhan Kim",
        "Dong-Jin Kim"
      ],
      "abstract": "Zero-shot domain adaptation is a method for adapting a model to a target domain without utilizing target domain image data. To enable adaptation without target images, existing studies utilize CLIP's embedding space and text description to simulate target-like style features. Despite the previous achievements in zero-shot domain adaptation, we observe that these text-driven methods struggle to capture complex real-world variations and significantly increase adaptation time due to their alignment process. Instead of relying on text descriptions, we explore solutions leveraging image data, which provides diverse and more fine-grained style cues. In this work, we propose SIDA, a novel and efficient zero-shot domain adaptation method leveraging synthetic images. To generate synthetic images, we first create detailed, source-like images and apply image translation to reflect the style of the target domain. We then utilize the style features of these synthetic images as a proxy for the target domain. Based on these features, we introduce Domain Mix and Patch Style Transfer modules, which enable effective modeling of real-world variations. In particular, Domain Mix blends multiple styles to expand the intra-domain representations, and Patch Style Transfer assigns different styles to individual patches. We demonstrate the effectiveness of our method by showing state-of-the-art performance in diverse zero-shot adaptation scenarios, particularly in challenging domains. Moreover, our approach achieves high efficiency by significantly reducing the overall adaptation time.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SIDAï¼Œä¸€ç§åˆ©ç”¨åˆæˆå›¾åƒé©±åŠ¨çš„æ–°å‹é«˜æ•ˆZero-shot Domain Adaptationæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–‡æœ¬é©±åŠ¨æ–¹æ³•éš¾ä»¥æ•æ‰å¤æ‚ç°å®ä¸–ç•Œå˜åŒ–ä¸”é€‚é…æ—¶é—´è¿‡é•¿çš„é—®é¢˜ã€‚ä¸ä¾èµ–CLIPæ–‡æœ¬æè¿°çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒSIDAé€šè¿‡ç”Ÿæˆç²¾ç»†çš„æºåŸŸé£æ ¼å›¾åƒå¹¶ç»“åˆå›¾åƒç¿»è¯‘æŠ€æœ¯ï¼Œäº§ç”Ÿèƒ½å¤Ÿåæ˜ ç›®æ ‡åŸŸé£æ ¼çš„åˆæˆå›¾åƒä½œä¸ºä»£ç†æ•°æ®ã€‚åŸºäºè¿™äº›åˆæˆå›¾åƒçš„é£æ ¼ç‰¹å¾ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†Domain Mixå’ŒPatch Style Transferæ¨¡å—ï¼Œå…¶ä¸­Domain Mixé€šè¿‡æ··åˆå¤šç§é£æ ¼æ¥æ‰©å±•åŸŸå†…è¡¨å¾ï¼Œè€ŒPatch Style Transferåˆ™ä¸ºå•ä¸ªå›¾åƒå—åˆ†é…ä¸åŒçš„é£æ ¼ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ¨¡æ‹ŸçœŸå®çš„é£æ ¼å˜ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSIDAåœ¨å¤šç§Zero-shot Domain Adaptationåœºæ™¯ä¸­å‡è¾¾åˆ°äº†State-of-the-artæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸè¡¨ç°çªå‡ºã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ˜¾è‘—ç¼©çŸ­æ•´ä½“é€‚é…æ—¶é—´ï¼Œåœ¨ä¿è¯æ•ˆæœçš„åŒæ—¶å®ç°äº†æé«˜çš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ACM MM 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18632v1",
      "published_date": "2025-07-24 17:59:36 UTC",
      "updated_date": "2025-07-24 17:59:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:35:01.232590+00:00"
    },
    {
      "arxiv_id": "2507.19549v1",
      "title": "AccessGuru: Leveraging LLMs to Detect and Correct Web Accessibility Violations in HTML Code",
      "title_zh": "AccessGuruï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ£€æµ‹å¹¶ä¿®å¤ HTML ä»£ç ä¸­çš„ç½‘é¡µæ— éšœç¢è¿è§„é¡¹",
      "authors": [
        "Nadeen Fathallah",
        "Daniel HernÃ¡ndez",
        "Steffen Staab"
      ],
      "abstract": "The vast majority of Web pages fail to comply with established Web accessibility guidelines, excluding a range of users with diverse abilities from interacting with their content. Making Web pages accessible to all users requires dedicated expertise and additional manual efforts from Web page providers. To lower their efforts and promote inclusiveness, we aim to automatically detect and correct Web accessibility violations in HTML code. While previous work has made progress in detecting certain types of accessibility violations, the problem of automatically detecting and correcting accessibility violations remains an open challenge that we address. We introduce a novel taxonomy classifying Web accessibility violations into three key categories - Syntactic, Semantic, and Layout. This taxonomy provides a structured foundation for developing our detection and correction method and redefining evaluation metrics. We propose a novel method, AccessGuru, which combines existing accessibility testing tools and Large Language Models (LLMs) to detect violations and applies taxonomy-driven prompting strategies to correct all three categories. To evaluate these capabilities, we develop a benchmark of real-world Web accessibility violations. Our benchmark quantifies syntactic and layout compliance and judges semantic accuracy through comparative analysis with human expert corrections. Evaluation against our benchmark shows that AccessGuru achieves up to 84% average violation score decrease, significantly outperforming prior methods that achieve at most 50%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å¤šæ•°ç½‘é¡µæœªèƒ½éµå®ˆç½‘é¡µå¯è®¿é—®æ€§æŒ‡å—ä¸”æ‰‹åŠ¨ä¿®æ­£æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº† AccessGuru æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨æ£€æµ‹å’Œä¿®æ­£ HTML ä»£ç ä¸­çš„å¯è®¿é—®æ€§è¿è§„ã€‚ç ”ç©¶è€…å¼•å…¥äº†ä¸€ç§å°†è¿è§„è¡Œä¸ºåˆ†ä¸ºè¯­æ³•(Syntactic)ã€è¯­ä¹‰(Semantic)å’Œå¸ƒå±€(Layout)çš„æ–°å‹åˆ†ç±»æ³•ï¼Œä¸ºç³»ç»ŸåŒ–çš„æ£€æµ‹ä¸ä¿®æ­£æä¾›äº†ç»“æ„åŒ–åŸºç¡€ã€‚AccessGuru ç»“åˆäº†ç°æœ‰çš„å¯è®¿é—®æ€§æµ‹è¯•å·¥å…·ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼Œå¹¶é‡‡ç”¨åˆ†ç±»é©±åŠ¨çš„æç¤ºç­–ç•¥æ¥é’ˆå¯¹æ€§åœ°è§£å†³å„ç±»è¿è§„é—®é¢˜ã€‚é€šè¿‡åœ¨çœŸå®ä¸–ç•Œç½‘é¡µè¿è§„åŸºå‡†æµ‹è¯•é›†ä¸Šçš„è¯„ä¼°ï¼ŒAccessGuru å®ç°äº†é«˜è¾¾ 84% çš„å¹³å‡è¿è§„å¾—åˆ†é™å¹…ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºæ­¤å‰æœ€é«˜ä»…ä¸º 50% çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶æˆæœæœ‰æ•ˆé™ä½äº†å¼€å‘è€…æå‡ç½‘é¡µåŒ…å®¹æ€§çš„æŠ€æœ¯é—¨æ§›ï¼Œä¸ºè‡ªåŠ¨åŒ–è§£å†³ç½‘é¡µå¯è®¿é—®æ€§æŒ‘æˆ˜æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.19549v1",
      "published_date": "2025-07-24 17:59:30 UTC",
      "updated_date": "2025-07-24 17:59:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:35:16.606509+00:00"
    },
    {
      "arxiv_id": "2507.18625v2",
      "title": "3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation",
      "title_zh": "åŸºäºçº¦æŸè¡¨è¾¾å‹ä¸­é—´è¡¨ç¤ºçš„3Dè½¯ä»¶åˆæˆ",
      "authors": [
        "Shuqing Li",
        "Anson Y. Lam",
        "Yun Peng",
        "Wenxuan Wang",
        "Michael R. Lyu"
      ],
      "abstract": "Graphical user interface (UI) software has undergone a fundamental transformation from traditional two-dimensional (2D) desktop/web/mobile interfaces to spatial three-dimensional (3D) environments. While existing work has made remarkable success in automated 2D software generation, such as HTML/CSS and mobile app interface code synthesis, the generation of 3D software still remains under-explored. Current methods for 3D software generation usually generate the 3D environments as a whole and cannot modify or control specific elements in the software. Furthermore, these methods struggle to handle the complex spatial and semantic constraints inherent in the real world. To address the challenges, we present Scenethesis, a novel requirement-sensitive 3D software synthesis approach that maintains formal traceability between user specifications and generated 3D software. Scenethesis is built upon ScenethesisLang, a domain-specific language that serves as a granular constraint-aware intermediate representation (IR) to bridge natural language requirements and executable 3D software. It serves both as a comprehensive scene description language enabling fine-grained modification of 3D software elements and as a formal constraint-expressive specification language capable of expressing complex spatial constraints. By decomposing 3D software synthesis into stages operating on ScenethesisLang, Scenethesis enables independent verification, targeted modification, and systematic constraint satisfaction. Our evaluation demonstrates that Scenethesis accurately captures over 80% of user requirements and satisfies more than 90% of hard constraints while handling over 100 constraints simultaneously. Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual evaluation scores compared to the state-of-the-art method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰3Dè½¯ä»¶ç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†å¤æ‚ç©ºé—´çº¦æŸå’Œå…ƒç´ ç»†ç²’åº¦æ§åˆ¶æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº† Scenethesis è¿™ç§éœ€æ±‚æ•æ„Ÿçš„åˆæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº† ScenethesisLang ä½œä¸ºä¸€ç§å…·å¤‡çº¦æŸè¡¨è¾¾èƒ½åŠ›çš„ä¸­é—´è¡¨ç¤º(Intermediate Representation, IR)ï¼Œæ—¨åœ¨é€šè¿‡é¢†åŸŸç‰¹å®šè¯­è¨€(Domain-Specific Language, DSL)å»ºç«‹è‡ªç„¶è¯­è¨€éœ€æ±‚ä¸3Dè½¯ä»¶ä¹‹é—´çš„æ­£å¼è¿½æº¯ã€‚Scenethesis å°†è½¯ä»¶åˆæˆåˆ†è§£ä¸ºå¤šä¸ªç‹¬ç«‹é˜¶æ®µï¼Œä»è€Œæ”¯æŒå¯¹3Då…ƒç´ çš„ç²¾ç¡®ä¿®æ”¹ã€ç³»ç»Ÿæ€§éªŒè¯ä»¥åŠå¤æ‚è¯­ä¹‰çº¦æŸçš„æ»¡è¶³ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ•æ‰è¶…è¿‡80%çš„ç”¨æˆ·éœ€æ±‚ï¼Œåœ¨å¤„ç†ä¸Šç™¾ä¸ªå¹¶å‘çº¦æŸæ—¶æ»¡è¶³ç‡è¶…è¿‡90%ã€‚æ­¤å¤–ï¼ŒScenethesis åœ¨ BLIP-2 è§†è§‰è¯„ä¼°æŒ‡æ ‡ä¸Šæ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†42.8%ï¼Œæ˜¾è‘—å¢å¼ºäº†3Dè½¯ä»¶ç”Ÿæˆçš„å‡†ç¡®æ€§ä¸å¯æ§æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM",
        "cs.SE"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by the IEEE/ACM International Conference on Software Engineering (ICSE) 2026, Rio de Janeiro, Brazil",
      "pdf_url": "https://arxiv.org/pdf/2507.18625v2",
      "published_date": "2025-07-24 17:58:03 UTC",
      "updated_date": "2025-12-17 06:47:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:35:11.180741+00:00"
    },
    {
      "arxiv_id": "2507.18623v3",
      "title": "Moving Out: Physically-grounded Human-AI Collaboration",
      "title_zh": "Moving Outï¼šåŸºäºç‰©ç†ç¯å¢ƒçš„äººæœºåä½œ",
      "authors": [
        "Xuhui Kang",
        "Sung-Wook Lee",
        "Haolin Liu",
        "Yuyan Wang",
        "Yen-Ling Kuo"
      ],
      "abstract": "The ability to adapt to physical actions and constraints in an environment is crucial for embodied agents (e.g., robots) to effectively collaborate with humans. Such physically grounded human-AI collaboration must account for the increased complexity of the continuous state-action space and constrained dynamics caused by physical constraints. In this paper, we introduce Moving Out, a new human-AI collaboration benchmark that resembles a wide range of collaboration modes affected by physical attributes and constraints, such as moving heavy items together and maintaining consistent actions to move a big item around a corner. Using Moving Out, we designed two tasks and collected human-human interaction data to evaluate models' abilities to adapt to diverse human behaviors and unseen physical attributes. To address the challenges in physical environments, we propose a novel method, BASS (Behavior Augmentation, Simulation, and Selection), to enhance the diversity of agents and their understanding of the outcome of actions. Our experiments show that BASS outperforms state-of-the-art models in AI-AI and human-AI collaboration. The project page is available at https://live-robotics-uva.github.io/movingout_ai/.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…·èº«æ™ºèƒ½ä½“åœ¨ä¸äººç±»åä½œæ—¶é€‚åº”ç‰©ç†åŠ¨ä½œå’Œç¯å¢ƒçº¦æŸçš„é‡è¦æ€§ï¼Œå¹¶å¼•å…¥äº†å…¨æ–°çš„ç‰©ç†æ¥åœ°ï¼ˆphysically-groundedï¼‰äººæœºåä½œåŸºå‡† Moving Outã€‚è¯¥åŸºå‡†æ¨¡æ‹Ÿäº†å¦‚å…±åŒæ¬è¿é‡ç‰©ã€åä½œç»•è¿‡å¼¯è§’ç­‰å¤šç§å—ç‰©ç†å±æ€§å’ŒåŠ¨åŠ›å­¦çº¦æŸçš„åä½œæ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿ç»­çŠ¶æ€åŠ¨ä½œç©ºé—´å¸¦æ¥çš„å¤æ‚æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸º BASS (Behavior Augmentation, Simulation, and Selection) çš„åˆ›æ–°æ–¹æ³•ï¼Œé€šè¿‡è¡Œä¸ºå¢å¼ºå’Œä»¿çœŸé€‰æ‹©æ¥æå‡æ™ºèƒ½ä½“çš„å¤šæ ·æ€§åŠå¯¹åŠ¨ä½œç»“æœçš„ç†è§£ã€‚å®éªŒé€šè¿‡æ”¶é›†äººç±»äº’åŠ¨æ•°æ®ï¼Œè¯„ä¼°äº†æ¨¡å‹åœ¨åº”å¯¹å¤šæ ·åŒ–äººç±»è¡Œä¸ºå’ŒæœªçŸ¥ç‰©ç†å±æ€§æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼ŒBASS åœ¨ AI-AI å’Œäººæœºåä½œå®éªŒä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„ state-of-the-art æ¨¡å‹ï¼Œæœ‰æ•ˆæå‡äº†æœºå™¨äººåœ¨ç‰©ç†ç¯å¢ƒä¸­çš„åä½œæ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.18623v3",
      "published_date": "2025-07-24 17:57:18 UTC",
      "updated_date": "2025-09-28 20:57:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:35:11.328310+00:00"
    },
    {
      "arxiv_id": "2508.02694v1",
      "title": "Efficient Agents: Building Effective Agents While Reducing Cost",
      "title_zh": "Efficient Agentsï¼šåœ¨é™ä½æˆæœ¬çš„åŒæ—¶æ„å»ºæœ‰æ•ˆæ™ºèƒ½ä½“",
      "authors": [
        "Ningning Wang",
        "Xavier Hu",
        "Pai Liu",
        "He Zhu",
        "Yue Hou",
        "Heyuan Huang",
        "Shengyu Zhang",
        "Jian Yang",
        "Jiaheng Liu",
        "Ge Zhang",
        "Changwang Zhang",
        "Jun Wang",
        "Yuchen Eleanor Jiang",
        "Wangchunshu Zhou"
      ],
      "abstract": "The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from $0.398 to $0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Model)é©±åŠ¨çš„æ™ºèƒ½ä½“(Agent)åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶æˆæœ¬ä¸æ–­æ”€å‡çš„é—®é¢˜ï¼Œå¼€å±•äº†å…³äºæ•ˆç‡ä¸æ•ˆèƒ½æƒè¡¡(efficiency-effectiveness trade-off)çš„é¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶ã€‚é€šè¿‡å¯¹GAIAåŸºå‡†æµ‹è¯•çš„å®è¯åˆ†æï¼Œç ”ç©¶è€…è¯„ä¼°äº†LLMéª¨å¹²æ¨¡å‹é€‰æ‹©ã€æ™ºèƒ½ä½“æ¡†æ¶è®¾è®¡ä»¥åŠæµ‹è¯•æ—¶ç¼©æ”¾(test-time scaling)ç­–ç•¥å¯¹æ€§èƒ½ä¸æˆæœ¬çš„å½±å“ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºEfficient Agentsçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éœ€æ±‚åŒ¹é…æœ€ä¼˜çš„å¤æ‚åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEfficient Agentsåœ¨ä¿ç•™äº†é¢†å…ˆå¼€æºæ¡†æ¶OWL 96.7%æ€§èƒ½çš„åŒæ—¶ï¼Œå°†å•æ¬¡ä»»åŠ¡è¿è¥æˆæœ¬ä»0.398ç¾å…ƒé™ä½è‡³0.228ç¾å…ƒã€‚è¿™ä¸€ä¼˜åŒ–å®ç°äº†cost-of-passæŒ‡æ ‡28.4%çš„æ˜¾è‘—æå‡ï¼Œä¸ºå¼€å‘ä½æˆæœ¬ã€é«˜æ€§èƒ½ä¸”å¯æŒç»­çš„AIé©±åŠ¨ç³»ç»Ÿæä¾›äº†æå…·å‚è€ƒä»·å€¼çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Work in progress. For GitHub repository, see https://github.com/OPPO-PersonalAI/OAgents",
      "pdf_url": "https://arxiv.org/pdf/2508.02694v1",
      "published_date": "2025-07-24 17:56:51 UTC",
      "updated_date": "2025-07-24 17:56:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:35:37.974395+00:00"
    },
    {
      "arxiv_id": "2507.18616v1",
      "title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning",
      "title_zh": "SynCï¼šåŸºäºä¸€å¯¹å¤šæ˜ å°„çš„åˆæˆå›¾åƒæè¿°æ•°æ®é›†ç²¾ç‚¼ï¼ŒåŠ©åŠ›é›¶æ ·æœ¬å›¾åƒæè¿°",
      "authors": [
        "Si-Woo Kim",
        "MinJu Jeon",
        "Ye-Chan Kim",
        "Soeun Lee",
        "Taewhan Kim",
        "Dong-Jin Kim"
      ],
      "abstract": "Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets generated by text-to-image (T2I) models to mitigate the need for costly manual annotation. However, these T2I models often produce images that exhibit semantic misalignments with their corresponding input captions (e.g., missing objects, incorrect attributes), resulting in noisy synthetic image-caption pairs that can hinder model training. Existing dataset pruning techniques are largely designed for removing noisy text in web-crawled data. However, these methods are ill-suited for the distinct challenges of synthetic data, where captions are typically well-formed, but images may be inaccurate representations. To address this gap, we introduce SynC, a novel framework specifically designed to refine synthetic image-caption datasets for ZIC. Instead of conventional filtering or regeneration, SynC focuses on reassigning captions to the most semantically aligned images already present within the synthetic image pool. Our approach employs a one-to-many mapping strategy by initially retrieving multiple relevant candidate images for each caption. We then apply a cycle-consistency-inspired alignment scorer that selects the best image by verifying its ability to retrieve the original caption via image-to-text retrieval. Extensive evaluations demonstrate that SynC consistently and significantly improves performance across various ZIC models on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art results in several scenarios. SynC offers an effective strategy for curating refined synthetic data to enhance ZIC.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é›¶æ ·æœ¬å›¾åƒæè¿°(Zero-shot Image Captioning, ZIC)é¢†åŸŸä¸­åˆæˆæ•°æ®é›†å­˜åœ¨çš„è¯­ä¹‰å¤±é…é—®é¢˜ï¼Œæå‡ºäº†åä¸ºSynCçš„å›¾åƒ-æ–‡æœ¬å¯¹ç²¾ç‚¼æ¡†æ¶ã€‚ä¸åŒäºä¼ ç»Ÿçš„è¿‡æ»¤æˆ–é‡æ–°ç”Ÿæˆæ–¹æ³•ï¼ŒSynCé€šè¿‡ä¸€å¯¹å¤šæ˜ å°„(one-to-many mapping)ç­–ç•¥ï¼Œå°†ç°æœ‰æ–‡æœ¬é‡æ–°åˆ†é…ç»™åˆæˆå›¾åƒåº“ä¸­æœ€åŒ¹é…çš„å›¾åƒã€‚è¯¥æ¡†æ¶åˆ©ç”¨å—å¾ªç¯ä¸€è‡´æ€§å¯å‘(cycle-consistency-inspired)çš„å¯¹é½è¯„åˆ†å™¨ï¼Œé€šè¿‡å›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢éªŒè¯æ¥ç­›é€‰æœ€ä½³å›¾åƒå¯¹ã€‚åœ¨MS-COCOã€Flickr30kå’ŒNoCapsç­‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSynCæ˜¾è‘—æå‡äº†å¤šç§ZICæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªå®éªŒåœºæ™¯ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›(state-of-the-art)æ°´å¹³ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†é€šè¿‡ä¼˜åŒ–åˆæˆæ•°æ®çš„æ˜ å°„å…³ç³»å¯ä»¥æœ‰æ•ˆå¢å¼ºæ¨¡å‹çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ACM Multimedia 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18616v1",
      "published_date": "2025-07-24 17:53:26 UTC",
      "updated_date": "2025-07-24 17:53:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:35:41.180072+00:00"
    },
    {
      "arxiv_id": "2507.18612v1",
      "title": "Approximate SMT Counting Beyond Discrete Domains",
      "title_zh": "è¶…è¶Šç¦»æ•£åŸŸçš„è¿‘ä¼¼ SMT è®¡æ•°",
      "authors": [
        "Arijit Shaw",
        "Kuldeep S. Meel"
      ],
      "abstract": "Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning, solving complex formulas across discrete and continuous domains. Recent progress in propositional model counting motivates extending SMT capabilities toward model counting, especially for hybrid SMT formulas. Existing approaches, like bit-blasting, are limited to discrete variables, highlighting the challenge of counting solutions projected onto the discrete domain in hybrid formulas.\n  We introduce pact, an SMT model counter for hybrid formulas that uses hashing-based approximate model counting to estimate solutions with theoretical guarantees. pact makes a logarithmic number of SMT solver calls relative to the projection variables, leveraging optimized hash functions. pact achieves significant performance improvements over baselines on a large suite of benchmarks. In particular, out of 14,202 instances, pact successfully finished on 603 instances, while Baseline could only finish on 13 instances.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Satisfiability Modulo Theory (SMT) æ±‚è§£å™¨åœ¨å¤„ç†æ··åˆ SMT å…¬å¼ (hybrid SMT formulas) æ—¶ï¼Œéš¾ä»¥å¯¹æŠ•å½±åˆ°ç¦»æ•£åŸŸçš„è§£è¿›è¡Œè®¡æ•°çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† pactï¼Œä¸€ç§ä¸“é—¨ç”¨äºæ··åˆå…¬å¼çš„ SMT æ¨¡å‹è®¡æ•°å™¨ (SMT model counter)ã€‚è¯¥å·¥å…·é‡‡ç”¨äº†åŸºäºå“ˆå¸Œ (hashing-based) çš„è¿‘ä¼¼æ¨¡å‹è®¡æ•° (approximate model counting) æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨æä¾›ç†è®ºä¿è¯ (theoretical guarantees) çš„åŒæ—¶ä¼°ç®—è§£çš„æ•°é‡ã€‚pact ç»“åˆäº†ä¼˜åŒ–çš„å“ˆå¸Œå‡½æ•° (hash functions)ï¼Œä½¿å¾— SMT æ±‚è§£å™¨çš„è°ƒç”¨æ¬¡æ•°ç›¸å¯¹äºæŠ•å½±å˜é‡å‘ˆå¯¹æ•°çº§ (logarithmic number) å¢é•¿ã€‚åœ¨åŒ…å« 14,202 ä¸ªå®ä¾‹çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸­ï¼Œpact æˆåŠŸå®Œæˆäº† 603 ä¸ªå®ä¾‹ï¼Œè€ŒåŸºçº¿æ¨¡å‹ä»…èƒ½å®Œæˆ 13 ä¸ªã€‚å®éªŒç»“æœè¯æ˜äº† pact åœ¨å¤„ç†å¤æ‚æ··åˆåŸŸ SMT è®¡æ•°ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "To be published in the proceedings of Design Automation Conference (DAC) 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18612v1",
      "published_date": "2025-07-24 17:48:13 UTC",
      "updated_date": "2025-07-24 17:48:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:35:35.194349+00:00"
    },
    {
      "arxiv_id": "2507.22935v3",
      "title": "Trusted Knowledge Extraction for Operations and Maintenance Intelligence",
      "title_zh": "é¢å‘è¿ç»´æ™ºèƒ½çš„å¯ä¿¡çŸ¥è¯†æŠ½å–",
      "authors": [
        "Kathleen P. Mealey",
        "Jonathan A. Karr",
        "Priscila Saboia Moreira",
        "Paul R. Brenner",
        "Charles F. Vardeman"
      ],
      "abstract": "Deriving operational intelligence from organizational data repositories is a key challenge due to the dichotomy of data confidentiality vs data integration objectives, as well as the limitations of Natural Language Processing (NLP) tools relative to the specific knowledge structure of domains such as operations and maintenance. In this work, we discuss Knowledge Graph construction and break down the Knowledge Extraction process into its Named Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation Extraction functional components. We then evaluate sixteen NLP tools in concert with or in comparison to the rapidly advancing capabilities of Large Language Models (LLMs). We focus on the operational and maintenance intelligence use case for trusted applications in the aircraft industry. A baseline dataset is derived from a rich public domain US Federal Aviation Administration dataset focused on equipment failures or maintenance requirements. We assess the zero-shot performance of NLP and LLM tools that can be operated within a controlled, confidential environment (no data is sent to third parties). Based on our observation of significant performance limitations, we discuss the challenges related to trusted NLP and LLM tools as well as their Technical Readiness Level for wider use in mission-critical industries such as aviation. We conclude with recommendations to enhance trust and provide our open-source curated dataset to support further baseline testing and evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è¿ç»´æ™ºèƒ½(Operations and Maintenance Intelligence)é¢†åŸŸä¸­ï¼Œå¦‚ä½•ä»ç»„ç»‡æ•°æ®ä»“åº“ä¸­æå–å—ä¿¡ä»»çŸ¥è¯†çš„æŒ‘æˆ˜ï¼Œé‡ç‚¹è§£å†³æ•°æ®æœºå¯†æ€§ä¸æ•°æ®é›†æˆç›®æ ‡ä¹‹é—´çš„çŸ›ç›¾ã€‚ç ”ç©¶è¯¦ç»†é˜è¿°äº†çŸ¥è¯†å›¾è°±(Knowledge Graph)çš„æ„å»ºï¼Œå¹¶å°†çŸ¥è¯†æå–(Knowledge Extraction)è¿‡ç¨‹åˆ†è§£ä¸ºå‘½åå®ä½“è¯†åˆ«(NER)ã€å…±æŒ‡æ¶ˆè§£(Coreference Resolution)ã€å‘½åå®ä½“é“¾æ¥(NEL)å’Œå…³ç³»æŠ½å–(Relation Extraction)ç­‰åŠŸèƒ½ç»„ä»¶ã€‚ä½œè€…é’ˆå¯¹èˆªç©ºå·¥ä¸šçš„å—ä¿¡ä»»åº”ç”¨ï¼Œè¯„ä¼°äº†16ç§NLPå·¥å…·ä»¥åŠå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†ç¾å›½è”é‚¦èˆªç©ºç®¡ç†å±€(FAA)æ•…éšœç»´æŠ¤æ•°æ®é›†æ—¶çš„æ€§èƒ½ã€‚è¯¥è¯„ä¼°é‡ç‚¹å…³æ³¨åœ¨å—æ§ã€æœºå¯†ç¯å¢ƒä¸‹è¿è¡Œçš„å·¥å…·ä¹‹é›¶æ ·æœ¬(Zero-shot)è¡¨ç°ï¼Œä»¥ç¡®ä¿æ•°æ®ä¸å‘ç¬¬ä¸‰æ–¹æ³„éœ²ã€‚å®éªŒè§‚å¯Ÿåˆ°ç°æœ‰çš„å—ä¿¡ä»»NLPå’ŒLLMå·¥å…·åœ¨æ€§èƒ½ä¸Šå­˜åœ¨æ˜¾è‘—å±€é™ï¼Œç ”ç©¶æ®æ­¤è®¨è®ºäº†è¿™äº›å·¥å…·åœ¨èˆªç©ºç­‰å…³é”®ä»»åŠ¡è¡Œä¸šå¤§è§„æ¨¡åº”ç”¨çš„æŠ€æœ¯æˆç†Ÿåº¦(Technical Readiness Level)ã€‚æœ€åï¼Œç ”ç©¶æå‡ºäº†å¢å¼ºç³»ç»Ÿä¿¡ä»»åº¦çš„å»ºè®®ï¼Œå¹¶æä¾›äº†å¼€æºçš„ç­–å±•æ•°æ®é›†ä»¥æ”¯æŒæœªæ¥çš„åŸºçº¿æµ‹è¯•ä¸è¯„ä¼°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22935v3",
      "published_date": "2025-07-24 17:36:16 UTC",
      "updated_date": "2025-10-25 03:17:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:01.492419+00:00"
    },
    {
      "arxiv_id": "2507.18594v3",
      "title": "DRWKV: Focusing on Object Edges for Low-Light Image Enhancement",
      "title_zh": "DRWKVï¼šèšç„¦ç‰©ä½“è¾¹ç¼˜çš„å¼±å…‰å›¾åƒå¢å¼º",
      "authors": [
        "Xuecheng Bai",
        "Yuxiang Wang",
        "Boyu Hu",
        "Qinyuan Jie",
        "Chuanzhi Xu",
        "Kechen Li",
        "Hongru Xiao",
        "Vera Chung"
      ],
      "abstract": "Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DRWKV (Detailed Receptance Weighted Key Value) æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä½å…‰å›¾åƒå¢å¼ºä¸­ç‰©ä½“è¾¹ç¼˜è¿ç»­æ€§å’Œç²¾ç»†ç»“æ„ä¿å­˜çš„éš¾é¢˜ã€‚é€šè¿‡å¼•å…¥å…¨å±€è¾¹ç¼˜Retinex (Global Edge Retinex, GER) ç†è®ºï¼Œè¯¥æ¨¡å‹å®ç°äº†å…‰ç…§ä¸è¾¹ç¼˜ç»“æ„çš„æœ‰æ•ˆè§£è€¦ï¼Œä»è€Œæå‡äº†è¾¹ç¼˜ä¿çœŸåº¦ã€‚ç ”ç©¶è¿˜è®¾è®¡äº†è¿›åŒ–å¼WKVæ³¨æ„åŠ› (Evolving WKV Attention) æœºåˆ¶ï¼Œåˆ©ç”¨èºæ—‹æ‰«ææ–¹å¼æ•è·ç©ºé—´è¾¹ç¼˜è¿ç»­æ€§å¹¶å¯¹ä¸è§„åˆ™ç»“æ„è¿›è¡Œå»ºæ¨¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŒè¾¹é¢‘è°±å¯¹é½å™¨ (Bilateral Spectrum Aligner, Bi-SAB) å’Œå®šåˆ¶çš„MS2-Lossï¼Œæ¨¡å‹èƒ½å¤ŸååŒä¼˜åŒ–äº®åº¦å’Œè‰²åº¦ç‰¹å¾ï¼Œæå‡è§†è§‰è‡ªç„¶åº¦å¹¶å‡å°‘ä¼ªå½±ã€‚å®éªŒè¯æ˜ï¼ŒDRWKVåœ¨äº”ä¸ªLLIEåŸºå‡†æµ‹è¯•çš„PSNRã€SSIMå’ŒNIQEæŒ‡æ ‡ä¸Šå‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œä¸”ä¿æŒäº†è¾ƒä½çš„è®¡ç®—å¤æ‚åº¦ã€‚è¯¥æ¨¡å‹è¿˜æ˜¾è‘—æå‡äº†ä½å…‰å¤šç›®æ ‡è·Ÿè¸ªç­‰ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to WACV 2026",
      "pdf_url": "https://arxiv.org/pdf/2507.18594v3",
      "published_date": "2025-07-24 17:24:59 UTC",
      "updated_date": "2025-12-06 16:47:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:35:44.291925+00:00"
    },
    {
      "arxiv_id": "2507.19548v1",
      "title": "Justifications for Democratizing AI Alignment and Their Prospects",
      "title_zh": "AIå¯¹é½æ°‘ä¸»åŒ–çš„æ­£å½“æ€§è¯æ˜åŠå…¶å‰æ™¯",
      "authors": [
        "AndrÃ© SteingrÃ¼ber",
        "Kevin Baum"
      ],
      "abstract": "The AI alignment problem comprises both technical and normative dimensions. While technical solutions focus on implementing normative constraints in AI systems, the normative problem concerns determining what these constraints should be. This paper examines justifications for democratic approaches to the normative problem -- where affected stakeholders determine AI alignment -- as opposed to epistocratic approaches that defer to normative experts. We analyze both instrumental justifications (democratic approaches produce better outcomes) and non-instrumental justifications (democratic approaches prevent illegitimate authority or coercion). We argue that normative and metanormative uncertainty create a justificatory gap that democratic approaches aim to fill through political rather than theoretical justification. However, we identify significant challenges for democratic approaches, particularly regarding the prevention of illegitimate coercion through AI alignment. Our analysis suggests that neither purely epistocratic nor purely democratic approaches may be sufficient on their own, pointing toward hybrid frameworks that combine expert judgment with participatory input alongside institutional safeguards against AI monopolization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†AI Alignmentçš„è§„èŒƒæ€§ç»´åº¦ï¼Œå¯¹æ¯”äº†ç”±åˆ©ç›Šç›¸å…³è€…é©±åŠ¨çš„æ°‘ä¸»åŒ–æ–¹æ³•ä¸ä¾èµ–ä¸“å®¶çŸ¥è¯†çš„Epistocraticæ–¹æ³•ã€‚é€šè¿‡åˆ†æå·¥å…·æ€§ä¸éå·¥å…·æ€§æ­£å½“ç†ç”±ï¼Œè®ºæ–‡æŒ‡å‡ºåœ¨Normative and Metanormative UncertaintyèƒŒæ™¯ä¸‹ï¼Œæ°‘ä¸»åŒ–è·¯å¾„èƒ½é€šè¿‡æ”¿æ²»æ­£å½“åŒ–å¡«è¡¥ç†è®ºè®ºè¯çš„ç©ºç™½ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹Ÿè¯†åˆ«å‡ºæ°‘ä¸»åŒ–åœ¨é˜²æ­¢éæ³•å¼ºåˆ¶ï¼ˆIllegitimate Coercionï¼‰æ–¹é¢å­˜åœ¨çš„é‡å¤§æŒ‘æˆ˜ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œçº¯ç²¹çš„ç²¾è‹±æ²»ç†æˆ–æ°‘ä¸»å†³ç­–å‡ä¸è¶³ä»¥ç‹¬ç«‹è§£å†³å¯¹é½éš¾é¢˜ã€‚å› æ­¤ï¼Œä½œè€…æè®®æ„å»ºä¸€ç§ç»“åˆä¸“å®¶åˆ¤æ–­ã€å…¬ä¼—å‚ä¸ä»¥åŠé˜²å„æ–­åˆ¶åº¦ä¿éšœçš„Hybrid Frameworksã€‚è¿™ä¸€æ¡†æ¶æ—¨åœ¨å¹³è¡¡ä¸“ä¸šæ€§ä¸æ°‘ä¸»åˆæ³•æ€§ï¼Œä¸ºæœªæ¥AIæ²»ç†çš„åˆ¶åº¦è®¾è®¡æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "accepted for the LNCS on-site proceedings of the AISoLA 2025 conference",
      "pdf_url": "https://arxiv.org/pdf/2507.19548v1",
      "published_date": "2025-07-24 17:16:19 UTC",
      "updated_date": "2025-07-24 17:16:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:35:54.175836+00:00"
    },
    {
      "arxiv_id": "2507.22934v1",
      "title": "Deep Learning Approaches for Multimodal Intent Recognition: A Survey",
      "title_zh": "å¤šæ¨¡æ€æ„å›¾è¯†åˆ«æ·±åº¦å­¦ä¹ æ–¹æ³•ç»¼è¿°",
      "authors": [
        "Jingwei Zhao",
        "Yuhua Wen",
        "Qifei Li",
        "Minchi Hu",
        "Yingying Zhou",
        "Jingyao Xue",
        "Junyang Wu",
        "Yingming Gao",
        "Zhengqi Wen",
        "Jianhua Tao",
        "Ya Li"
      ],
      "abstract": "Intent recognition aims to identify users' underlying intentions, traditionally focusing on text in natural language processing. With growing demands for natural human-computer interaction, the field has evolved through deep learning and multimodal approaches, incorporating data from audio, vision, and physiological signals. Recently, the introduction of Transformer-based models has led to notable breakthroughs in this domain. This article surveys deep learning methods for intent recognition, covering the shift from unimodal to multimodal techniques, relevant datasets, methodologies, applications, and current challenges. It provides researchers with insights into the latest developments in multimodal intent recognition (MIR) and directions for future research.",
      "tldr_zh": "è¯¥ç»¼è¿°å…¨é¢ç³»ç»Ÿåœ°æ¢è®¨äº†ç”¨äºå¤šæ¨¡æ€æ„å›¾è¯†åˆ«(Multimodal Intent Recognition, MIR)çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚éšç€å¯¹è‡ªç„¶äººæœºäº¤äº’éœ€æ±‚çš„æ—¥ç›Šå¢é•¿ï¼Œè¯¥é¢†åŸŸå·²ä»ä¼ ç»Ÿçš„æ–‡æœ¬å¤„ç†æ¼”å˜ä¸ºæ•´åˆéŸ³é¢‘ã€è§†è§‰å’Œç”Ÿç†ä¿¡å·çš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œå…¶ä¸­åŸºäºTransformerçš„æ¨¡å‹åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çªç ´ã€‚æ–‡ç« è¯¦ç»†å›é¡¾äº†ä»å•æ¨¡æ€åˆ°å¤šæ¨¡æ€æŠ€æœ¯æ¼”è¿›çš„è·¯å¾„ï¼Œå¹¶å¯¹ç›¸å…³çš„Datasetã€æ–¹æ³•è®ºã€åº”ç”¨é¢†åŸŸåŠå½“å‰é¢ä¸´çš„æŒ‘æˆ˜è¿›è¡Œäº†å½’çº³ã€‚é€šè¿‡æ·±å…¥åˆ†ææœ€æ–°çš„ç ”ç©¶æˆæœï¼Œè¯¥æ–‡ä¸ºç ”ç©¶äººå‘˜æä¾›äº†MIRé¢†åŸŸçš„å…³é”®è§è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¼•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to ACM Computing Surveys",
      "pdf_url": "https://arxiv.org/pdf/2507.22934v1",
      "published_date": "2025-07-24 17:12:01 UTC",
      "updated_date": "2025-07-24 17:12:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:35:56.380471+00:00"
    },
    {
      "arxiv_id": "2507.18587v1",
      "title": "A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff",
      "title_zh": "æ”¯æŒæ¯ç”¨æˆ·é€Ÿç‡-åŠŸç‡è‡ªé€‚åº”æƒè¡¡çš„å¤§è§„æ¨¡ MIMO é¢„ç¼–ç åŸºç¡€æ¨¡å‹",
      "authors": [
        "JÃ©rÃ´me Emery",
        "Ali Hasanzadeh Karkan",
        "Jean-FranÃ§ois Frigon",
        "FranÃ§ois Leduc-Primeau"
      ],
      "abstract": "Deep learning (DL) has emerged as a solution for precoding in massive multiple-input multiple-output (mMIMO) systems due to its capacity to learn the characteristics of the propagation environment. However, training such a model requires high-quality, local datasets at the deployment site, which are often difficult to collect. We propose a transformer-based foundation model for mMIMO precoding that seeks to minimize the energy consumption of the transmitter while dynamically adapting to per-user rate requirements. At equal energy consumption, zero-shot deployment of the proposed foundation model significantly outperforms zero forcing, and approaches weighted minimum mean squared error performance with 8x less complexity. To address model adaptation in data-scarce settings, we introduce a data augmentation method that finds training samples similar to the target distribution by computing the cosine similarity between the outputs of the pre-trained feature extractor. Our work enables the implementation of DL-based solutions in practice by addressing challenges of data availability and training complexity. Moreover, the ability to dynamically configure per-user rate requirements can be leveraged by higher level resource allocation and scheduling algorithms for greater control over energy efficiency, spectral efficiency and fairness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡å¤šè¾“å…¥å¤šè¾“å‡º(Massive MIMO)é¢„ç¼–ç æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„åŸºç¡€æ¨¡å‹(Foundation Model)ï¼Œæ—¨åœ¨æœ€å°åŒ–å‘å°„æœºèƒ½è€—çš„åŒæ—¶åŠ¨æ€é€‚åº”æ¯ä¸ªç”¨æˆ·çš„é€Ÿç‡éœ€æ±‚ã€‚åœ¨ç­‰èƒ½è€—æ¡ä»¶ä¸‹ï¼Œè¯¥æ¨¡å‹çš„é›¶æ ·æœ¬(Zero-shot)éƒ¨ç½²æ€§èƒ½æ˜¾è‘—ä¼˜äºè¿«é›¶(Zero Forcing)ç®—æ³•ï¼Œå¹¶ä»¥ä»…ä¸ºå…«åˆ†ä¹‹ä¸€çš„å¤æ‚åº¦æ¥è¿‘åŠ æƒæœ€å°å‡æ–¹è¯¯å·®(WMMSE)çš„æ€§èƒ½ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹çš„æ¨¡å‹é€‚é…éš¾é¢˜ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒç‰¹å¾æå–å™¨è¾“å‡ºçš„ä½™å¼¦ç›¸ä¼¼åº¦(Cosine Similarity)è¿›è¡Œæ•°æ®å¢å¼ºçš„æ–¹æ³•ï¼Œä»¥å¯»æ‰¾ä¸ç›®æ ‡åˆ†å¸ƒç›¸ä¼¼çš„è®­ç»ƒæ ·æœ¬ã€‚è¯¥å·¥ä½œæœ‰æ•ˆè§£å†³äº†æ·±åº¦å­¦ä¹ æ–¹æ¡ˆåœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„æ•°æ®è·å–å’Œè®­ç»ƒå¤æ‚åº¦æŒ‘æˆ˜ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ”¯æŒæ›´é«˜å±‚èµ„æºåˆ†é…ç®—æ³•å¯¹èƒ½æ•ˆã€é¢‘è°±æ•ˆç‡åŠå…¬å¹³æ€§çš„åŠ¨æ€æ§åˆ¶ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "6 pages, 3 figures. Accepted to the IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC) 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18587v1",
      "published_date": "2025-07-24 17:10:06 UTC",
      "updated_date": "2025-07-24 17:10:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:00.180763+00:00"
    },
    {
      "arxiv_id": "2507.18584v1",
      "title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs",
      "title_zh": "AQuiltï¼šå°†é€»è¾‘ä¸è‡ªæ£€èå…¥é¢å‘ä¸“ä¸šé¢†åŸŸå¤§æ¨¡å‹çš„é«˜ç›¸å…³æ€§ã€ä½æˆæœ¬æ•°æ®åˆæˆ",
      "authors": [
        "Xiaopeng Ke",
        "Hexuan Deng",
        "Xuebo Liu",
        "Jun Rao",
        "Zhenxi Song",
        "Jun Yu",
        "Min Zhang"
      ],
      "abstract": "Despite the impressive performance of large language models (LLMs) in general domains, they often underperform in specialized domains. Existing approaches typically rely on data synthesis methods and yield promising results by using unlabeled data to capture domain-specific features. However, these methods either incur high computational costs or suffer from performance limitations, while also demonstrating insufficient generalization across different tasks. To address these challenges, we propose AQuilt, a framework for constructing instruction-tuning data for any specialized domains from corresponding unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and inspection, we encourage reasoning processes and self-inspection to enhance model performance. Moreover, customizable task instructions enable high-quality data generation for any task. As a result, we construct a dataset of 703k examples to train a powerful data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further analysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source code, models, and scripts are available at https://github.com/Krueske/AQuilt.",
      "tldr_zh": "é’ˆå¯¹é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°ä¸ä½³ä»¥åŠç°æœ‰æ•°æ®åˆæˆæ–¹æ³•æˆæœ¬é«˜ã€æ³›åŒ–æ€§å·®çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† AQuilt æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä»æ— æ ‡æ³¨æ•°æ®ï¼ˆUnlabeled dataï¼‰ä¸­ä¸ºç‰¹å®šé¢†åŸŸæ„å»ºé«˜è´¨é‡çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼ˆInstruction-tuning dataï¼‰ï¼Œå…¶æ ¸å¿ƒè¦ç´ æ¶µç›–äº†ç­”æ¡ˆï¼ˆAnswerï¼‰ã€é—®é¢˜ï¼ˆQuestionï¼‰ã€è‡ªæ£€ï¼ˆInspectionï¼‰ã€é€»è¾‘ï¼ˆLogicï¼‰å’Œä»»åŠ¡ç±»å‹ï¼ˆTask typeï¼‰ã€‚é€šè¿‡åœ¨åˆæˆè¿‡ç¨‹ä¸­å¼•å…¥é€»è¾‘æ¨ç†å’Œè‡ªæ£€æœºåˆ¶ï¼ŒAQuilt æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¹¶æå‡äº†æ•°æ®ç”Ÿæˆçš„è´¨é‡ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªåŒ…å« 703k ä¸ªç¤ºä¾‹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒå‡ºäº†å¼ºå¤§çš„æ•°æ®åˆæˆæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAQuilt çš„æ€§èƒ½è¶³ä»¥ä¸ DeepSeek-V3 åª²ç¾ï¼Œä½†ç”Ÿäº§æˆæœ¬ä»…ä¸ºåè€…çš„ 17%ã€‚è¿›ä¸€æ­¥åˆ†æè¿˜è¯å®ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„æ•°æ®å¯¹ä¸‹æ¸¸ä»»åŠ¡å…·æœ‰æ›´é«˜çš„ç›¸å…³æ€§ï¼Œä¸ºä½æˆæœ¬å¼€å‘ä¸“å®¶çº§ LLMs æä¾›äº†æœ‰æ•ˆçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "32 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.18584v1",
      "published_date": "2025-07-24 17:03:27 UTC",
      "updated_date": "2025-07-24 17:03:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:02.674997+00:00"
    },
    {
      "arxiv_id": "2507.18583v1",
      "title": "DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data",
      "title_zh": "DR.EHRï¼šèåˆçŸ¥è¯†æ³¨å…¥ä¸åˆæˆæ•°æ®çš„ç”µå­å¥åº·è®°å½•ç¨ å¯†æ£€ç´¢",
      "authors": [
        "Zhengyun Zhao",
        "Huaiyuan Ying",
        "Yue Zhong",
        "Sheng Yu"
      ],
      "abstract": "Electronic Health Records (EHRs) are pivotal in clinical practices, yet their retrieval remains a challenge mainly due to semantic gap issues. Recent advancements in dense retrieval offer promising solutions but existing models, both general-domain and biomedical-domain, fall short due to insufficient medical knowledge or mismatched training corpora. This paper introduces \\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV discharge summaries to address the need for extensive medical knowledge and large-scale training data. The first stage involves medical entity extraction and knowledge injection from a biomedical knowledge graph, while the second stage employs large language models to generate diverse training data. We train two variants of \\texttt{DR.EHR}, with 110M and 7B parameters, respectively. Evaluated on the CliniQ benchmark, our models significantly outperforms all existing dense retrievers, achieving state-of-the-art results. Detailed analyses confirm our models' superiority across various match and query types, particularly in challenging semantic matches like implication and abbreviation. Ablation studies validate the effectiveness of each pipeline component, and supplementary experiments on EHR QA datasets demonstrate the models' generalizability on natural language questions, including complex ones with multiple entities. This work significantly advances EHR retrieval, offering a robust solution for clinical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DR.EHRï¼Œä¸€ç³»åˆ—ä¸“é—¨ä¸ºç”µå­å¥åº·è®°å½• (EHR) æ£€ç´¢è®¾è®¡çš„ç¨ å¯†æ£€ç´¢ (Dense Retrieval) æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨åŒ»å­¦çŸ¥è¯†å‚¨å¤‡å’Œè®­ç»ƒæ•°æ®åŒ¹é…ä¸Šçš„ä¸è¶³ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ç‹¬ç‰¹çš„ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆä»ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å›¾è°± (Biomedical Knowledge Graph) ä¸­æå–åŒ»ç–—å®ä½“å¹¶è¿›è¡ŒçŸ¥è¯†æ³¨å…¥ (Knowledge Injection)ï¼Œéšååˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆå¤§è§„æ¨¡å¤šæ ·åŒ–çš„åˆæˆè®­ç»ƒæ•°æ® (Synthetic Data)ã€‚åœ¨ CliniQ åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDR.EHR çš„ 110M å’Œ 7B å‚æ•°ç‰ˆæœ¬å‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„é€šç”¨åŠåŒ»å­¦é¢†åŸŸæ£€ç´¢å™¨ï¼Œå°¤å…¶åœ¨å¤„ç†å«è“„è¡¨è¾¾ (Implication) å’Œç¼©å†™ (Abbreviation) ç­‰æŒ‘æˆ˜æ€§è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ—¶è¡¨ç°ä¼˜å¼‚ã€‚é’ˆå¯¹é—®ç­”æ•°æ®é›† (QA Datasets) çš„æ‰©å±•å®éªŒè¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ¨¡å‹åœ¨å¤„ç†åŒ…å«å¤šä¸ªå®ä½“çš„å¤æ‚è‡ªç„¶è¯­è¨€é—®é¢˜æ—¶å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥å·¥ä½œé€šè¿‡ç»“åˆçŸ¥è¯†å›¾è°±å¢å¼ºä¸åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œä¸ºä¸´åºŠç¯å¢ƒä¸‹çš„ç²¾å‡†ä¿¡æ¯æ£€ç´¢æä¾›äº†ç¨³å¥ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Model and code released upon acceptance",
      "pdf_url": "https://arxiv.org/pdf/2507.18583v1",
      "published_date": "2025-07-24 17:02:46 UTC",
      "updated_date": "2025-07-24 17:02:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:12.082565+00:00"
    },
    {
      "arxiv_id": "2507.18576v3",
      "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law",
      "title_zh": "SafeWork-R1ï¼šAI-45Â° å®šå¾‹ä¸‹çš„å®‰å…¨æ€§ä¸æ™ºèƒ½ååŒè¿›åŒ–",
      "authors": [
        "Shanghai AI Lab",
        ":",
        "Yicheng Bao",
        "Guanxu Chen",
        "Mingkang Chen",
        "Yunhao Chen",
        "Chiyu Chen",
        "Lingjie Chen",
        "Sirui Chen",
        "Xinquan Chen",
        "Jie Cheng",
        "Yu Cheng",
        "Dengke Deng",
        "Yizhuo Ding",
        "Dan Ding",
        "Xiaoshan Ding",
        "Yi Ding",
        "Zhichen Dong",
        "Lingxiao Du",
        "Yuyu Fan",
        "Xinshun Feng",
        "Yanwei Fu",
        "Yuxuan Gao",
        "Ruijun Ge",
        "Tianle Gu",
        "Lujun Gui",
        "Jiaxuan Guo",
        "Qianxi He",
        "Yuenan Hou",
        "Xuhao Hu",
        "Hong Huang",
        "Kaichen Huang",
        "Shiyang Huang",
        "Yuxian Jiang",
        "Shanzhe Lei",
        "Jie Li",
        "Lijun Li",
        "Hao Li",
        "Juncheng Li",
        "Xiangtian Li",
        "Yafu Li",
        "Lingyu Li",
        "Xueyan Li",
        "Haotian Liang",
        "Dongrui Liu",
        "Qihua Liu",
        "Zhixuan Liu",
        "Bangwei Liu",
        "Huacan Liu",
        "Yuexiao Liu",
        "Zongkai Liu",
        "Chaochao Lu",
        "Yudong Lu",
        "Xiaoya Lu",
        "Zhenghao Lu",
        "Qitan Lv",
        "Caoyuan Ma",
        "Jiachen Ma",
        "Xiaoya Ma",
        "Zhongtian Ma",
        "Lingyu Meng",
        "Ziqi Miao",
        "Yazhe Niu",
        "Yuezhang Peng",
        "Yuan Pu",
        "Han Qi",
        "Chen Qian",
        "Xingge Qiao",
        "Jingjing Qu",
        "Jiashu Qu",
        "Wanying Qu",
        "Wenwen Qu",
        "Xiaoye Qu",
        "Qihan Ren",
        "Qingnan Ren",
        "Qingyu Ren",
        "Jing Shao",
        "Wenqi Shao",
        "Shuai Shao",
        "Dongxing Shi",
        "Xin Song",
        "Xinhao Song",
        "Yan Teng",
        "Xuan Tong",
        "Yingchun Wang",
        "Xuhong Wang",
        "Shujie Wang",
        "Xin Wang",
        "Yige Wang",
        "Yixu Wang",
        "Yuanfu Wang",
        "Futing Wang",
        "Ruofan Wang",
        "Wenjie Wang",
        "Yajie Wang",
        "Muhao Wei",
        "Xiaoyu Wen",
        "Fenghua Weng",
        "Yuqi Wu",
        "Yingtong Xiong",
        "Xingcheng Xu",
        "Chao Yang",
        "Yue Yang",
        "Yang Yao",
        "Yulei Ye",
        "Zhenyun Yin",
        "Yi Yu",
        "Bo Zhang",
        "Qiaosheng Zhang",
        "Jinxuan Zhang",
        "Yexin Zhang",
        "Yinqiang Zheng",
        "Hefeng Zhou",
        "Zhanhui Zhou",
        "Pengyu Zhu",
        "Qingzi Zhu",
        "Yubo Zhu",
        "Bowen Zhou"
      ],
      "abstract": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that demonstrates the coevolution of capabilities and safety. It is developed by our proposed SafeLadder framework, which incorporates large-scale, progressive, safety-oriented reinforcement learning post-training, supported by a suite of multi-principled verifiers. Unlike previous alignment methods such as RLHF that simply learn human preferences, SafeLadder enables SafeWork-R1 to develop intrinsic safety reasoning and self-reflection abilities, giving rise to safety `aha' moments. Notably, SafeWork-R1 achieves an average improvement of $46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks without compromising general capabilities, and delivers state-of-the-art safety performance compared to leading proprietary models such as GPT-4.1 and Claude Opus 4. To further bolster its reliability, we implement two distinct inference-time intervention methods and a deliberative search mechanism, enforcing step-level verification. Finally, we further develop SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and capability can co-evolve synergistically, highlighting the generalizability of our framework in building robust, reliable, and trustworthy general-purpose AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† SafeWork-R1ï¼Œè¿™æ˜¯ä¸€ç§å°–ç«¯çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨å±•ç¤ºäººå·¥æ™ºèƒ½èƒ½åŠ›ä¸å®‰å…¨æ€§çš„ååŒè¿›åŒ–ã€‚è¯¥æ¨¡å‹åŸºäºæå‡ºçš„ SafeLadder æ¡†æ¶å¼€å‘ï¼Œåˆ©ç”¨å¤§è§„æ¨¡ã€æ¸è¿›å¼çš„å®‰å…¨å¯¼å‘ Reinforcement Learningï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰åè®­ç»ƒï¼Œå¹¶ç”±ä¸€ç³»åˆ—å¤šåŸåˆ™ Verifiersï¼ˆéªŒè¯å™¨ï¼‰æ”¯æŒã€‚ä¸åŒäºä¼ ç»Ÿçš„ RLHF æ–¹æ³•ï¼ŒSafeLadder ä½¿æ¨¡å‹å…·å¤‡äº†å†…åœ¨çš„å®‰å…¨æ¨ç†å’Œè‡ªæˆ‘åæ€èƒ½åŠ›ï¼Œä»è€Œäº§ç”Ÿå®‰å…¨é¢†åŸŸçš„ â€œaha momentsâ€ã€‚å®éªŒè¡¨æ˜ï¼ŒSafeWork-R1 åœ¨å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­æ¯”åŸºç¡€æ¨¡å‹ Qwen2.5-VL-72B å¹³å‡æå‡äº† 46.54%ï¼Œæ€§èƒ½å¯æ¯”è‚© GPT-4.1 å’Œ Claude Opus 4 ç­‰é¡¶çº§æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸¤ç§ Inference-time Interventionï¼ˆæ¨ç†æ—¶å¹²é¢„ï¼‰æ–¹æ³•å’Œ Deliberative Searchï¼ˆå®¡æ…æœç´¢ï¼‰æœºåˆ¶ä»¥å®ç°æ­¥éª¤çº§éªŒè¯ã€‚è¯¥æˆæœåœ¨ InternVL3ã€DeepSeek ç­‰å¤šä¸ªæ¨¡å‹æ¶æ„ä¸Šè¯æ˜äº†æ¡†æ¶çš„é€šç”¨æ€§ï¼Œå¼ºè°ƒäº†å®‰å…¨æ€§ä¸èƒ½åŠ›åœ¨æ„å»ºå¯é é€šç”¨äººå·¥æ™ºèƒ½è¿‡ç¨‹ä¸­å¯ä»¥åŒæ­¥å¢é•¿ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "47 pages, 18 figures, authors are listed in alphabetical order by their last names; v3 modifies minor issues",
      "pdf_url": "https://arxiv.org/pdf/2507.18576v3",
      "published_date": "2025-07-24 16:49:19 UTC",
      "updated_date": "2025-08-07 08:02:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:30.183809+00:00"
    },
    {
      "arxiv_id": "2507.18572v1",
      "title": "PosterMate: Audience-driven Collaborative Persona Agents for Poster Design",
      "title_zh": "PosterMateï¼šé¢å‘æµ·æŠ¥è®¾è®¡çš„å—ä¼—é©±åŠ¨åä½œå¼è§’è‰²æ™ºèƒ½ä½“",
      "authors": [
        "Donghoon Shin",
        "Daniel Lee",
        "Gary Hsieh",
        "Gromit Yeuk-Yin Chan"
      ],
      "abstract": "Poster designing can benefit from synchronous feedback from target audiences. However, gathering audiences with diverse perspectives and reconciling them on design edits can be challenging. Recent generative AI models present opportunities to simulate human-like interactions, but it is unclear how they may be used for feedback processes in design. We introduce PosterMate, a poster design assistant that facilitates collaboration by creating audience-driven persona agents constructed from marketing documents. PosterMate gathers feedback from each persona agent regarding poster components, and stimulates discussion with the help of a moderator to reach a conclusion. These agreed-upon edits can then be directly integrated into the poster design. Through our user study (N=12), we identified the potential of PosterMate to capture overlooked viewpoints, while serving as an effective prototyping tool. Additionally, our controlled online evaluation (N=100) revealed that the feedback from an individual persona agent is appropriate given its persona identity, and the discussion effectively synthesizes the different persona agents' perspectives.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PosterMateï¼Œä¸€ç§åˆ©ç”¨å—ä¼—é©±åŠ¨çš„ç”»åƒæ™ºèƒ½ä½“ (Persona Agents) è¾…åŠ©æµ·æŠ¥è®¾è®¡çš„åä½œç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³è·å–å¤šæ ·åŒ–å—ä¼—åé¦ˆåŠè¾¾æˆè®¾è®¡å…±è¯†çš„éš¾é¢˜ã€‚ç³»ç»Ÿé€šè¿‡ä»è¥é”€æ–‡æ¡£ä¸­æå–ä¿¡æ¯å¹¶æ„å»ºç”»åƒæ™ºèƒ½ä½“ï¼Œé’ˆå¯¹æµ·æŠ¥ç»„ä»¶æ”¶é›†åé¦ˆï¼Œå¹¶åœ¨ä¸»æŒäºº (Moderator) çš„å¼•å¯¼ä¸‹é€šè¿‡æ¨¡æ‹Ÿè®¨è®ºè¾¾æˆä¸€è‡´çš„ä¿®æ”¹æ„è§ï¼Œè¿™äº›æ„è§éšåå¯ç›´æ¥é›†æˆåˆ°è®¾è®¡ä¸­ã€‚ç”¨æˆ·ç ”ç©¶ (N=12) æ˜¾ç¤ºï¼ŒPosterMate èƒ½å¤Ÿæ•æ‰è®¾è®¡å¸ˆå¸¸å¿½è§†çš„è§†è§’ï¼Œå¹¶ä½œä¸ºæœ‰æ•ˆçš„åŸå‹å·¥å…· (Prototyping tool) æå‡è®¾è®¡æ•ˆç‡ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹100åå‚ä¸è€…çš„å—æ§åœ¨çº¿è¯„ä¼°è¯å®ï¼Œå„ç”»åƒæ™ºèƒ½ä½“çš„åé¦ˆä¸å…¶èº«ä»½è®¾å®šé«˜åº¦å¥‘åˆï¼Œä¸”è®¨è®ºè¿‡ç¨‹èƒ½å¤ŸæˆåŠŸåˆæˆä¸åŒå—ä¼—çš„å¤šå…ƒè§è§£ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18572v1",
      "published_date": "2025-07-24 16:46:25 UTC",
      "updated_date": "2025-07-24 16:46:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:38.024965+00:00"
    },
    {
      "arxiv_id": "2507.18567v1",
      "title": "Proceedings 19th International Workshop on the ACL2 Theorem Prover and Its Applications",
      "title_zh": "ç¬¬19å±Š ACL2 å®šç†è¯æ˜å™¨åŠå…¶åº”ç”¨å›½é™…ç ”è®¨ä¼šè®ºæ–‡é›†",
      "authors": [
        "Ruben Gamboa",
        "Panagiotis Manolios"
      ],
      "abstract": "The ACL2 Workshop series is the major technical forum for users of the ACL2 theorem proving system to present research related to the ACL2 theorem prover and its applications. ACL2 is an industrial-strength automated reasoning system, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM Software System Award was awarded to Boyer, Kaufmann, and Moore for their work on ACL2 and the other theorem provers in the Boyer-Moore family.",
      "tldr_zh": "è¯¥æ–‡çŒ®æ”¶å½•äº†ç¬¬19å±ŠACL2å®šç†è¯æ˜å™¨åŠå…¶åº”ç”¨å›½é™…ç ”è®¨ä¼š(International Workshop on the ACL2 Theorem Prover and Its Applications)çš„ç ”ç©¶æˆæœã€‚ä½œä¸ºBoyer-Mooreå®¶æ—å®šç†è¯æ˜å™¨çš„é‡è¦æˆå‘˜ï¼ŒACL2è¢«å®šä¹‰ä¸ºä¸€ç§å·¥ä¸šçº§çš„è‡ªåŠ¨åŒ–æ¨ç†ç³»ç»Ÿ(automated reasoning system)ï¼Œåœ¨ç³»ç»ŸéªŒè¯é¢†åŸŸå…·æœ‰é‡è¦åœ°ä½ã€‚è¯¥ç ”è®¨ä¼šç³»åˆ—æ˜¯ACL2ç”¨æˆ·å±•ç¤ºç³»ç»Ÿç›¸å…³ç ”ç©¶ã€ç†è®ºæ¢ç´¢åŠå®é™…åº”ç”¨æ¡ˆä¾‹çš„æ ¸å¿ƒæŠ€æœ¯è®ºå›ã€‚æ–‡ä¸­å¼ºè°ƒäº†ACL2åœ¨è®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„æ°å‡ºè´¡çŒ®ï¼Œå…¶å¼€å‘è€…æ›¾å› åœ¨è¯¥ç³»ç»Ÿä¸Šçš„å·¥ä½œè£è·2005å¹´ACMè½¯ä»¶ç³»ç»Ÿå¥–(ACM Software System Award)ã€‚é€šè¿‡è¯¥è®ºæ–‡é›†ï¼Œè¯»è€…å¯ä»¥å…¨é¢äº†è§£ACL2åœ¨å½¢å¼åŒ–éªŒè¯ã€å®šç†è¯æ˜åŠå…¶å·¥ä¸šåº”ç”¨æ–¹é¢çš„æœ€æ–°è¿›å±•ä¸å®è·µç»éªŒã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18567v1",
      "published_date": "2025-07-24 16:42:15 UTC",
      "updated_date": "2025-07-24 16:42:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:32.726191+00:00"
    },
    {
      "arxiv_id": "2507.18562v2",
      "title": "GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation",
      "title_zh": "GIIFTï¼šå›¾å¼•å¯¼çš„å½’çº³å¼æ— å›¾åƒå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘",
      "authors": [
        "Jiafeng Xiong",
        "Yuting Zhao"
      ],
      "abstract": "Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GIIFTï¼Œä¸€ç§åŸºäºå›¾å¼•å¯¼çš„å½’çº³å¼æ— å›¾åƒå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ (Graph-guided Inductive Image-Free Multimodal Machine Translation) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ MMT æ–¹æ³•åœ¨å¼¥åˆæ¨¡æ€å·®è·å’Œè·¨é¢†åŸŸæ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚é€šè¿‡æ„å»ºæ–°å‹çš„ multimodal scene graphsï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆä¿ç•™å¹¶æ•´åˆç‰¹å®šæ¨¡æ€çš„ä¿¡æ¯ï¼Œä»è€Œå…‹æœäº†ç¡¬æ€§è§†è§‰è¯­è¨€å¯¹é½çš„æŒ‘æˆ˜ã€‚GIIFT é‡‡ç”¨ä¸¤é˜¶æ®µè®¾è®¡ï¼Œåˆ©ç”¨ cross-modal Graph Attention Network (GAT) adapter åœ¨ç»Ÿä¸€çš„èåˆç©ºé—´ä¸­å­¦ä¹ å¤šæ¨¡æ€çŸ¥è¯†ï¼Œå¹¶å°†å…¶å½’çº³æ³›åŒ–è‡³æ›´å¹¿æ³›çš„ image-free ç¿»è¯‘é¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ Multi30K æ•°æ®é›†çš„ English-to-French å’Œ English-to-German ä»»åŠ¡ä¸­ï¼ŒGIIFT åœ¨æ¨ç†é˜¶æ®µæ— éœ€å›¾åƒè¾“å…¥çš„æƒ…å†µä¸‹ä»è¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ WMT åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ image-free ç¿»è¯‘åŸºçº¿ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤§è§„æ¨¡ç¿»è¯‘ä»»åŠ¡å’Œå½’çº³æ¨ç†æ–¹é¢çš„å¼ºå¤§æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as an oral presentation at the EMNLP 2025 Workshop on Machine Translation (WMT)",
      "pdf_url": "https://arxiv.org/pdf/2507.18562v2",
      "published_date": "2025-07-24 16:36:47 UTC",
      "updated_date": "2025-10-08 10:20:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:39.482600+00:00"
    },
    {
      "arxiv_id": "2507.18561v1",
      "title": "Beyond Internal Data: Constructing Complete Datasets for Fairness Testing",
      "title_zh": "è¶…è¶Šå†…éƒ¨æ•°æ®ï¼šæ„å»ºç”¨äºå…¬å¹³æ€§æµ‹è¯•çš„å®Œæ•´æ•°æ®é›†",
      "authors": [
        "Varsha Ramineni",
        "Hossein A. Rahmani",
        "Emine Yilmaz",
        "David Barber"
      ],
      "abstract": "As AI becomes prevalent in high-risk domains and decision-making, it is essential to test for potential harms and biases. This urgency is reflected by the global emergence of AI regulations that emphasise fairness and adequate testing, with some mandating independent bias audits. However, procuring the necessary data for fairness testing remains a significant challenge. Particularly in industry settings, legal and privacy concerns restrict the collection of demographic data required to assess group disparities, and auditors face practical and cultural challenges in gaining access to data. Further, internal historical datasets are often insufficiently representative to identify real-world biases. This work focuses on evaluating classifier fairness when complete datasets including demographics are inaccessible. We propose leveraging separate overlapping datasets to construct complete synthetic data that includes demographic information and accurately reflects the underlying relationships between protected attributes and model features. We validate the fidelity of the synthetic data by comparing it to real data, and empirically demonstrate that fairness metrics derived from testing on such synthetic data are consistent with those obtained from real data. This work, therefore, offers a path to overcome real-world data scarcity for fairness testing, enabling independent, model-agnostic evaluation of fairness, and serving as a viable substitute where real data is limited.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½å…¬å¹³æ€§æµ‹è¯•(Fairness Testing)ä¸­å› æ³•å¾‹å’Œéšç§é™åˆ¶å¯¼è‡´çš„äººå£ç»Ÿè®¡å­¦(Demographics)æ•°æ®è·å–éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨é‡å æ•°æ®é›†æ„å»ºå®Œæ•´åˆæˆæ•°æ®(Synthetic Data)çš„æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å»ºç«‹èƒ½å¤Ÿå‡†ç¡®åæ˜ å—ä¿æŠ¤å±æ€§(Protected Attributes)ä¸æ¨¡å‹ç‰¹å¾(Model Features)ä¹‹é—´åº•å±‚å…³ç³»çš„åˆæˆæ•°æ®é›†ï¼Œè§£å†³äº†å†…éƒ¨å†å²æ•°æ®ä»£è¡¨æ€§ä¸è¶³åŠæ•°æ®è·å–å—é™çš„é—®é¢˜ã€‚å®éªŒé€šè¿‡å°†åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®å¯¹æ¯”éªŒè¯äº†å…¶é«˜ä¿çœŸåº¦(Fidelity)ï¼Œå¹¶ä»ç»éªŒä¸Šè¯æ˜äº†åŸºäºè¯¥åˆæˆæ•°æ®å¾—å‡ºçš„å…¬å¹³æ€§æŒ‡æ ‡(Fairness Metrics)ä¸çœŸå®æ•°æ®æ‰€å¾—ç»“æœä¿æŒé«˜åº¦ä¸€è‡´ã€‚è¿™é¡¹å·¥ä½œä¸ºå…‹æœç°å®ä¸–ç•Œä¸­çš„æ•°æ®ç¨€ç¼ºæä¾›äº†æœ‰æ•ˆè·¯å¾„ï¼Œæ”¯æŒç‹¬ç«‹ä¸”æ¨¡å‹æ— å…³(Model-agnostic)çš„å…¬å¹³æ€§è¯„ä¼°ã€‚åœ¨çœŸå®æ•°æ®å—é™çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•å¯ä½œä¸ºå¯é çš„æ›¿ä»£æ–¹æ¡ˆï¼ŒåŠ©åŠ›ç ”ç©¶è€…å’Œå®¡è®¡æœºæ„æ»¡è¶³æ—¥ç›Šä¸¥æ ¼çš„AIç›‘ç®¡ä¸åˆè§„è¦æ±‚ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.18561v1",
      "published_date": "2025-07-24 16:35:42 UTC",
      "updated_date": "2025-07-24 16:35:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:42.543981+00:00"
    },
    {
      "arxiv_id": "2507.18560v1",
      "title": "HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven Sentiment Integration for Financial Portfolio Optimization",
      "title_zh": "HARLFï¼šåŸºäºåˆ†å±‚å¼ºåŒ–å­¦ä¹ ä¸è½»é‡çº§ LLM é©±åŠ¨æƒ…æ„Ÿæ•´åˆçš„é‡‘èæŠ•èµ„ç»„åˆä¼˜åŒ–",
      "authors": [
        "Benjamin Coriat",
        "Eric Benhamou"
      ],
      "abstract": "This paper presents a novel hierarchical framework for portfolio optimization, integrating lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to combine sentiment signals from financial news with traditional market indicators. Our three-tier architecture employs base RL agents to process hybrid data, meta-agents to aggregate their decisions, and a super-agent to merge decisions based on market data and sentiment analysis. Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming equal-weighted and S&P 500 benchmarks. Key contributions include scalable cross-modal integration, a hierarchical RL structure for enhanced stability, and open-source reproducibility.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HARLFï¼Œä¸€ç§ç”¨äºæŠ•èµ„ç»„åˆä¼˜åŒ–çš„åˆ†å±‚æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè½»é‡çº§å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)ï¼ŒæˆåŠŸå°†é‡‘èæ–°é—»çš„æƒ…ç»ªä¿¡å·ä¸ä¼ ç»Ÿå¸‚åœºæŒ‡æ ‡ç›¸ç»“åˆã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸‰å±‚æ¶æ„ï¼ŒåŒ…æ‹¬å¤„ç†æ··åˆæ•°æ®çš„å¼ºåŒ–å­¦ä¹ åº•å±‚æ™ºèƒ½ä½“(RL agents)ã€è´Ÿè´£æ±‡æ€»å†³ç­–çš„å…ƒæ™ºèƒ½ä½“(meta-agents)ä»¥åŠæœ€ç»ˆèåˆå¸‚åœºæ•°æ®ä¸æƒ…ç»ªåˆ†æçš„è¶…çº§æ™ºèƒ½ä½“(super-agent)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨2018å¹´è‡³2024å¹´çš„æµ‹è¯•æœŸå†…å®ç°äº†26%çš„å¹´åŒ–æ”¶ç›Šç‡å’Œ1.2çš„å¤æ™®æ¯”ç‡(Sharpe ratio)ï¼Œå…¶è¡¨ç°ä¼˜äºç­‰æƒé‡åŸºå‡†å’Œæ ‡æ™®500æŒ‡æ•°(S&P 500)ã€‚è¯¥ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå®ç°äº†å¯æ‰©å±•çš„è·¨æ¨¡æ€é›†æˆï¼Œå¹¶åˆ©ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ (Hierarchical RL)ç»“æ„æ˜¾è‘—å¢å¼ºäº†æŠ•èµ„ç­–ç•¥çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œé€šè¿‡å¼€æºç¡®ä¿äº†ç»“æœçš„å¯å¤ç°æ€§ï¼Œä¸ºé‡‘èé¢†åŸŸçš„æ™ºèƒ½å†³ç­–æä¾›äº†åˆ›æ–°çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "q-fin.PM",
        "cs.AI"
      ],
      "primary_category": "q-fin.PM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18560v1",
      "published_date": "2025-07-24 16:35:24 UTC",
      "updated_date": "2025-07-24 16:35:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:55.443145+00:00"
    },
    {
      "arxiv_id": "2507.18681v1",
      "title": "Concept Probing: Where to Find Human-Defined Concepts (Extended Version)",
      "title_zh": "æ¦‚å¿µæ¢æµ‹ï¼šæ¢å¯»äººç±»å®šä¹‰æ¦‚å¿µçš„åˆ†å¸ƒä½ç½®ï¼ˆæ‰©å±•ç‰ˆï¼‰",
      "authors": [
        "Manuel de Sousa Ribeiro",
        "Afonso Leote",
        "JoÃ£o Leite"
      ],
      "abstract": "Concept probing has recently gained popularity as a way for humans to peek into what is encoded within artificial neural networks. In concept probing, additional classifiers are trained to map the internal representations of a model into human-defined concepts of interest. However, the performance of these probes is highly dependent on the internal representations they probe from, making identifying the appropriate layer to probe an essential task. In this paper, we propose a method to automatically identify which layer's representations in a neural network model should be considered when probing for a given human-defined concept of interest, based on how informative and regular the representations are with respect to the concept. We validate our findings through an exhaustive empirical analysis over different neural network models and datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¦‚å¿µæ¢æµ‹(Concept Probing)æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡è®­ç»ƒé¢å¤–çš„åˆ†ç±»å™¨å°†ç¥ç»ç½‘ç»œçš„å†…éƒ¨è¡¨å¾æ˜ å°„ä¸ºäººç±»å®šä¹‰çš„æ¦‚å¿µã€‚ç”±äºæ¢æµ‹å™¨çš„æ€§èƒ½é«˜åº¦ä¾èµ–äºå…¶æ‰€æ¢æµ‹çš„ç‰¹å®šå±‚ï¼Œè¯†åˆ«æœ€åˆé€‚çš„è¡¨å¾å±‚æˆä¸ºäº†è¯¥é¢†åŸŸçš„ä¸€é¡¹æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨è¯†åˆ«ç¥ç»ç½‘ç»œä¸­ç‰¹å®šå±‚çš„æ–¹æ³•ï¼Œç”¨äºæ¢æµ‹ç»™å®šçš„äººç±»å®šä¹‰æ¦‚å¿µã€‚è¯¥æ–¹æ³•ä¸»è¦ä¾æ®è¡¨å¾ç›¸å¯¹äºç›®æ ‡æ¦‚å¿µçš„ä¿¡æ¯é‡(Informative)å’Œæ­£åˆ™æ€§(Regular)æ¥è¿›è¡Œç­›é€‰ã€‚ç ”ç©¶äººå‘˜é€šè¿‡åœ¨å¤šç§ç¥ç»ç½‘ç»œæ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯¦å°½å®è¯åˆ†æï¼ŒéªŒè¯äº†è¯¥è‡ªåŠ¨è¯†åˆ«é€»è¾‘çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£äººå·¥ç¥ç»ç½‘ç»œå¦‚ä½•ç¼–ç äººç±»å®šä¹‰çš„æ¦‚å¿µæä¾›äº†æ›´å…·é’ˆå¯¹æ€§çš„å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Extended version of the paper published in Proceedings of the International Conference on Neurosymbolic Learning and Reasoning (NeSy 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.18681v1",
      "published_date": "2025-07-24 16:30:10 UTC",
      "updated_date": "2025-07-24 16:30:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:47.057145+00:00"
    },
    {
      "arxiv_id": "2507.22933v1",
      "title": "Augmented Vision-Language Models: A Systematic Review",
      "title_zh": "å¢å¼ºå‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼šç³»ç»Ÿç»¼è¿°",
      "authors": [
        "Anthony C Davis",
        "Burhan Sadiq",
        "Tianmin Shu",
        "Chien-Ming Huang"
      ],
      "abstract": "Recent advances in visual-language machine learning models have demonstrated exceptional ability to use natural language and understand visual scenes by training on large, unstructured datasets. However, this training paradigm cannot produce interpretable explanations for its outputs, requires retraining to integrate new information, is highly resource-intensive, and struggles with certain forms of logical reasoning. One promising solution involves integrating neural networks with external symbolic information systems, forming neural symbolic systems that can enhance reasoning and memory abilities. These neural symbolic systems provide more interpretable explanations to their outputs and the capacity to assimilate new information without extensive retraining. Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural component, augmented by external systems, offers a pragmatic approach to realizing the benefits of neural-symbolic integration. This systematic literature review aims to categorize techniques through which visual-language understanding can be improved by interacting with external symbolic information systems.",
      "tldr_zh": "è¯¥ç»¼è¿°é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)åœ¨è§£é‡Šæ€§ä¸è¶³ã€é€»è¾‘æ¨ç†å›°éš¾ä»¥åŠæ•´åˆæ–°ä¿¡æ¯éœ€é¢‘ç¹é‡è®­ç»ƒç­‰å±€é™æ€§è¿›è¡Œäº†ç³»ç»Ÿæ€§æ¢è®¨ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨å°†ç¥ç»ç½‘ç»œä¸å¤–éƒ¨ç¬¦å·ä¿¡æ¯ç³»ç»Ÿç»“åˆçš„ç¥ç»ç¬¦å·ç³»ç»Ÿ(Neural Symbolic Systems)ï¼Œä»¥æ­¤æå‡æ¨¡å‹çš„æ¨ç†ä¸è®°å¿†èƒ½åŠ›ã€‚é€šè¿‡å°†é¢„è®­ç»ƒçš„VLMsä½œä¸ºæ ¸å¿ƒç¥ç»ç»„ä»¶å¹¶è¾…ä»¥å¤–éƒ¨å¢å¼ºç³»ç»Ÿï¼Œè¯¥æ–¹æ³•åœ¨ä¸è¿›è¡Œå¤§è§„æ¨¡é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ›´å…·è§£é‡Šæ€§çš„è¾“å‡ºå’Œæ–°ä¿¡æ¯çš„å¿«é€ŸåŒåŒ–ã€‚æœ¬æ–‡æä¾›äº†ä¸€ä»½ç³»ç»Ÿçš„æ–‡çŒ®ç»¼è¿°ï¼Œæ—¨åœ¨å¯¹é€šè¿‡ä¸å¤–éƒ¨ç¬¦å·ä¿¡æ¯ç³»ç»Ÿäº¤äº’æ¥æå‡è§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›çš„å„ç±»æŠ€æœ¯è¿›è¡Œåˆ†ç±»ã€‚è¿™ç§ç¥ç»ç¬¦å·é›†æˆçš„æ–¹æ³•ä¸ºå®ç°æ›´å…·é²æ£’æ€§ã€å¯è§£é‡Šæ€§ä¸”èµ„æºé«˜æ•ˆçš„è§†è§‰æ™ºèƒ½æä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22933v1",
      "published_date": "2025-07-24 16:27:38 UTC",
      "updated_date": "2025-07-24 16:27:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:50.827641+00:00"
    },
    {
      "arxiv_id": "2507.18552v1",
      "title": "VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding",
      "title_zh": "VideoMindï¼šé¢å‘æ·±å±‚è®¤çŸ¥è§†é¢‘ç†è§£çš„æ„å›¾å…³è”å…¨æ¨¡æ€è§†é¢‘æ•°æ®é›†",
      "authors": [
        "Baoyao Yang",
        "Wanyun Li",
        "Dixin Chen",
        "Junxiang Chen",
        "Wenbin Yao",
        "Haifeng Lin"
      ],
      "abstract": "This paper introduces VideoMind, a video-centric omni-modal dataset designed for deep video content cognition and enhanced multi-modal feature representation. The dataset comprises 103K video samples (3K reserved for testing), each paired with audio and systematically detailed textual descriptions. Specifically, every video and its audio is described across three hierarchical layers (factual, abstract, and intent), progressing from surface to depth. It contains over 22 million words, averaging ~225 words per sample. VideoMind's key distinction from existing datasets is its provision of intent expressions, which require contextual integration across the entire video and are not directly observable. These deep-cognitive expressions are generated using a Chain-of-Thought (COT) approach, prompting the mLLM through step-by-step reasoning. Each description includes annotations for subject, place, time, event, action, and intent, supporting downstream recognition tasks. Crucially, we establish a gold-standard benchmark with 3,000 manually validated samples for evaluating deep-cognitive video understanding. We design hybrid-cognitive retrieval experiments, scored by multi-level retrieval metrics, to appropriately assess deep video comprehension. Evaluation results for models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a powerful benchmark for fine-grained cross-modal alignment and advances fields requiring in-depth video understanding, such as emotion and intent recognition. The data is publicly available on GitHub, HuggingFace, and OpenDataLab, https://github.com/cdx-cindy/VideoMind.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VideoMindï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è§†é¢‘ä¸ºä¸­å¿ƒçš„å…¨æ¨¡æ€ (omni-modal) æ•°æ®é›†ï¼Œæ—¨åœ¨ä¿ƒè¿›æ·±åº¦è§†é¢‘å†…å®¹è®¤çŸ¥å’Œå¢å¼ºå¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºã€‚è¯¥æ•°æ®é›†åŒ…å« 10.3 ä¸‡ä¸ªè§†é¢‘æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬å‡é…æœ‰éŸ³é¢‘ï¼Œå¹¶é‡‡ç”¨äº†æ¶µç›–äº‹å® (factual)ã€æŠ½è±¡ (abstract) å’Œæ„å›¾ (intent) ä¸‰ä¸ªå±‚æ¬¡çš„è¯¦ç»†æ–‡æœ¬æè¿°ã€‚VideoMind çš„æ ¸å¿ƒç‰¹è‰²åœ¨äºæä¾›äº†éœ€è¦ä¸Šä¸‹æ–‡æ•´åˆçš„éç›´è§‚æ„å›¾è¡¨è¾¾ï¼Œåˆ©ç”¨ Chain-of-Thought (COT) æ–¹æ³•å¼•å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (mLLM) è¿›è¡Œé€æ­¥æ¨ç†ç”Ÿæˆã€‚æ¯æ¡æè¿°å‡åŒ…å«å¯¹ä¸»ä½“ã€åœ°ç‚¹ã€æ—¶é—´ã€äº‹ä»¶ã€åŠ¨ä½œå’Œæ„å›¾çš„æ ‡æ³¨ï¼Œä¸ºç»†ç²’åº¦è·¨æ¨¡æ€å¯¹é½æä¾›äº†æ”¯æŒã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…å»ºç«‹äº†ä¸€ä¸ªåŒ…å« 3,000 ä¸ªæ‰‹åŠ¨éªŒè¯æ ·æœ¬çš„é‡‘æ ‡å‡†åŸºå‡†ï¼Œå¹¶è®¾è®¡äº†æ··åˆè®¤çŸ¥æ£€ç´¢å®éªŒæ¥è¯„ä¼° InternVideoã€VAST ç­‰æ¨¡å‹åœ¨æ·±åº¦è§†é¢‘ç†è§£æ–¹é¢çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†çš„å‘å¸ƒä¸ºæƒ…æ„Ÿè¯†åˆ«ã€æ„å›¾è¯†åˆ«ç­‰éœ€è¦æ·±åº¦å†…å®¹è§£æçš„é¢†åŸŸæä¾›äº†å¼ºæœ‰åŠ›çš„åŸºå‡†æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages; 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.18552v1",
      "published_date": "2025-07-24 16:19:43 UTC",
      "updated_date": "2025-07-24 16:19:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:36:58.834233+00:00"
    },
    {
      "arxiv_id": "2507.18550v1",
      "title": "On the Performance of Concept Probing: The Influence of the Data (Extended Version)",
      "title_zh": "è®ºæ¦‚å¿µæ¢æµ‹æ€§èƒ½ï¼šæ•°æ®çš„å½±å“ï¼ˆæ‰©å±•ç‰ˆï¼‰",
      "authors": [
        "Manuel de Sousa Ribeiro",
        "Afonso Leote",
        "JoÃ£o Leite"
      ],
      "abstract": "Concept probing has recently garnered increasing interest as a way to help interpret artificial neural networks, dealing both with their typically large size and their subsymbolic nature, which ultimately renders them unfeasible for direct human interpretation. Concept probing works by training additional classifiers to map the internal representations of a model into human-defined concepts of interest, thus allowing humans to peek inside artificial neural networks. Research on concept probing has mainly focused on the model being probed or the probing model itself, paying limited attention to the data required to train such probing models. In this paper, we address this gap. Focusing on concept probing in the context of image classification tasks, we investigate the effect of the data used to train probing models on their performance. We also make available concept labels for two widely used datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Concept probing æŠ€æœ¯åœ¨æå‡äººå·¥ç¥ç»ç½‘ç»œå¯è§£é‡Šæ€§æ–¹é¢çš„åº”ç”¨ï¼Œè¯¥æŠ€æœ¯é€šè¿‡è®­ç»ƒåˆ†ç±»å™¨å°†æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºæ˜ å°„ä¸ºäººç±»å®šä¹‰çš„æ„Ÿå¿µï¼Œä»¥åº”å¯¹ subsymbolic æ¨¡å‹éš¾ä»¥ç›´æ¥è§£è¯»çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶å¤šå…³æ³¨æ¢æµ‹æ¨¡å‹æ¶æ„è€Œå¿½è§†è®­ç»ƒæ•°æ®å½±å“çš„ç°çŠ¶ï¼Œæœ¬æ–‡ç³»ç»Ÿè°ƒæŸ¥äº†åœ¨ image classification ä»»åŠ¡ä¸­ï¼Œè®­ç»ƒæ•°æ®å¯¹æ¢æµ‹æ¨¡å‹æ€§èƒ½çš„å…·ä½“ä½œç”¨ã€‚ä½œè€…é€šè¿‡å®éªŒæ­ç¤ºäº†æ•°æ®åœ¨ Concept probing è¿‡ç¨‹ä¸­çš„å…³é”®é‡è¦æ€§ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å…¬å¼€å‘å¸ƒäº†é’ˆå¯¹ä¸¤ä¸ªå¸¸ç”¨æ•°æ®é›†çš„æ¦‚å¿µæ ‡ç­¾ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†èµ„æºã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†æ•°æ®è´¨é‡å¯¹æ¨¡å‹è§£é‡Šç»“æœçš„å†³å®šæ€§å½±å“ï¼Œä¸ºå¼€å‘æ›´ç¨³å¥çš„å¯è§£é‡Šæ€§åˆ†æå·¥å…·å¥ å®šäº†å®è¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "Extended version of the paper published in Proceedings of the European Conference on Artificial Intelligence (ECAI 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.18550v1",
      "published_date": "2025-07-24 16:18:46 UTC",
      "updated_date": "2025-07-24 16:18:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:37:08.538916+00:00"
    },
    {
      "arxiv_id": "2507.18680v1",
      "title": "Market Making Strategies with Reinforcement Learning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¸‚åœºåšå¸‚ç­–ç•¥",
      "authors": [
        "Ã“scar FernÃ¡ndez Vicente"
      ],
      "abstract": "This thesis presents the results of a comprehensive research project focused on applying Reinforcement Learning (RL) to the problem of market making in financial markets. Market makers (MMs) play a fundamental role in providing liquidity, yet face significant challenges arising from inventory risk, competition, and non-stationary market dynamics. This research explores how RL, particularly Deep Reinforcement Learning (DRL), can be employed to develop autonomous, adaptive, and profitable market making strategies.\n  The study begins by formulating the MM task as a reinforcement learning problem, designing agents capable of operating in both single-agent and multi-agent settings within a simulated financial environment. It then addresses the complex issue of inventory management using two complementary approaches: reward engineering and Multi-Objective Reinforcement Learning (MORL). While the former uses dynamic reward shaping to guide behavior, the latter leverages Pareto front optimization to explicitly balance competing objectives.\n  To address the problem of non-stationarity, the research introduces POW-dTS, a novel policy weighting algorithm based on Discounted Thompson Sampling. This method allows agents to dynamically select and combine pretrained policies, enabling continual adaptation to shifting market conditions.\n  The experimental results demonstrate that the proposed RL-based approaches significantly outperform traditional and baseline algorithmic strategies across various performance metrics. Overall, this research thesis contributes new methodologies and insights for the design of robust, efficient, and adaptive market making agents, reinforcing the potential of RL to transform algorithmic trading in complex financial systems.",
      "tldr_zh": "è¯¥è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning)ï¼Œåœ¨å¤æ‚çš„é‡‘èå¸‚åœºä¸­å¼€å‘è‡ªä¸»ã€è‡ªé€‚åº”ä¸”ç›ˆåˆ©çš„åšå¸‚å•†(Market Maker)ç­–ç•¥ã€‚ç ”ç©¶å°†åšå¸‚ä»»åŠ¡å»ºæ¨¡ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œè®¾è®¡äº†èƒ½å¤Ÿåœ¨æ¨¡æ‹Ÿç¯å¢ƒçš„å•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“è®¾å®šä¸‹è¿è¡Œçš„æ™ºèƒ½ä½“ã€‚ä¸ºäº†è§£å†³åº“å­˜ç®¡ç†éš¾é¢˜ï¼Œç ”ç©¶é‡‡ç”¨äº†å¥–åŠ±å·¥ç¨‹(Reward Engineering)å’Œå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ (Multi-Objective Reinforcement Learning)ä¸¤ç§äº’è¡¥æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€å¥–åŠ±å¡‘å½¢å’Œå¸•ç´¯æ‰˜å‰æ²¿ä¼˜åŒ–(Pareto Front Optimization)æ¥å¹³è¡¡ç«äº‰ç›®æ ‡ã€‚é’ˆå¯¹å¸‚åœºéå¹³ç¨³æ€§ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†åŸºäºæŠ˜æ‰£æ±¤æ™®æ£®é‡‡æ ·(Discounted Thompson Sampling)çš„æ–°å‹ç­–ç•¥åŠ æƒç®—æ³•POW-dTSï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤ŸåŠ¨æ€ç»„åˆé¢„è®­ç»ƒç­–ç•¥ä»¥é€‚åº”å¸‚åœºæ³¢åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šé¡¹æ€§èƒ½æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿç®—æ³•å’ŒåŸºå‡†ç­–ç•¥ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºé²æ£’ã€é«˜æ•ˆçš„åšå¸‚æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ–¹æ³•è®ºï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å¼ºåŒ–å­¦ä¹ åœ¨å˜é©é‡‘èç®—æ³•äº¤æ˜“æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18680v1",
      "published_date": "2025-07-24 16:17:49 UTC",
      "updated_date": "2025-07-24 16:17:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:37:24.826070+00:00"
    },
    {
      "arxiv_id": "2507.18546v1",
      "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface",
      "title_zh": "GLiNER2ï¼šä¸€ç§åŸºäºæ¨¡å¼é©±åŠ¨æ¥å£çš„é«˜æ•ˆå¤šä»»åŠ¡ä¿¡æ¯æŠ½å–ç³»ç»Ÿ",
      "authors": [
        "Urchade Zaratiana",
        "Gil Pasternak",
        "Oliver Boyd",
        "George Hurn-Maloney",
        "Ash Lewis"
      ],
      "abstract": "Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions often require specialized models for different tasks or rely on computationally expensive large language models. We present GLiNER2, a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction within a single efficient model. Built pretrained transformer encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task composition through an intuitive schema-based interface. Our experiments demonstrate competitive performance across extraction and classification tasks with substantial improvements in deployment accessibility compared to LLM-based alternatives. We release GLiNER2 as an open-source pip-installable library with pre-trained models and documentation at https://github.com/fastino-ai/GLiNER2.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GLiNER2ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰©å±•äº†åŸå§‹ GLiNER æ¶æ„çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å•ä¸€é«˜æ•ˆæ¨¡å‹æ”¯æŒ Named Entity Recognitionã€Text Classification ä»¥åŠ Hierarchical Structured Data Extractionã€‚è¯¥ç³»ç»ŸåŸºäºé¢„è®­ç»ƒçš„ Transformer Encoder æ¶æ„ï¼Œåœ¨ä¿æŒç´§å‡‘æ¨¡å‹ä½“ç§¯å’Œé«˜æ•ˆ CPU è¿è¡Œæ€§èƒ½çš„åŒæ—¶ï¼Œå¼•å…¥äº†ç›´è§‚çš„ Schema-Driven Interface æ¥å®ç°å¤šä»»åŠ¡ç»„åˆã€‚ä¸åŸºäº Large Language Models (LLMs) çš„æ–¹æ¡ˆç›¸æ¯”ï¼ŒGLiNER2 åœ¨ç¡®ä¿æå–å’Œåˆ†ç±»ä»»åŠ¡æ€§èƒ½å…·æœ‰ç«äº‰åŠ›çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†éƒ¨ç½²çš„å¯è®¿é—®æ€§ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†å¤šä»»åŠ¡ä¿¡æ¯æå–æ—¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¼€å‘è€…æä¾›äº†æ›´è½»é‡ä¸”åŠŸèƒ½å¼ºå¤§çš„ NLP è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18546v1",
      "published_date": "2025-07-24 16:11:14 UTC",
      "updated_date": "2025-07-24 16:11:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:37:23.899794+00:00"
    },
    {
      "arxiv_id": "2507.18533v1",
      "title": "C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation",
      "title_zh": "C2G-KDï¼šé¢å‘æ— æ•°æ®çŸ¥è¯†è’¸é¦çš„ PCA çº¦æŸç”Ÿæˆå™¨",
      "authors": [
        "Magnus Bengtsson",
        "Kenneth Ã–stberg"
      ],
      "abstract": "We introduce C2G-KD, a data-free knowledge distillation framework where a class-conditional generator is trained to produce synthetic samples guided by a frozen teacher model and geometric constraints derived from PCA. The generator never observes real training data but instead learns to activate the teacher's output through a combination of semantic and structural losses. By constraining generated samples to lie within class-specific PCA subspaces estimated from as few as two real examples per class, we preserve topological consistency and diversity. Experiments on MNIST show that even minimal class structure is sufficient to bootstrap useful synthetic training pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†C2G-KDï¼Œä¸€ç§åŸºäºPCAçº¦æŸç”Ÿæˆå™¨çš„æ— æ•°æ®çŸ¥è¯†è’¸é¦(Data-Free Knowledge Distillation)æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒä¸€ä¸ªç±»åˆ«æ¡ä»¶ç”Ÿæˆå™¨(class-conditional generator)ï¼Œåœ¨å†»ç»“çš„æ•™å¸ˆæ¨¡å‹å’Œæºè‡ªPCAçš„å‡ ä½•çº¦æŸæŒ‡å¯¼ä¸‹äº§ç”Ÿåˆæˆæ ·æœ¬ã€‚ç”Ÿæˆå™¨åœ¨ä¸æ¥è§¦çœŸå®è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨è¯­ä¹‰å’Œç»“æ„æŸå¤±(semantic and structural losses)å­¦ä¹ æ¿€æ´»æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºã€‚é€šè¿‡å°†ç”Ÿæˆçš„æ ·æœ¬çº¦æŸåœ¨æ¯ç±»ä»…éœ€ä¸¤ä¸ªçœŸå®ç¤ºä¾‹å³å¯ä¼°ç®—å‡ºçš„PCAå­ç©ºé—´å†…ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆä¿æŒäº†æ•°æ®çš„æ‹“æ‰‘ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ã€‚åœ¨MNISTæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œå³ä½¿æ˜¯æç®€çš„ç±»åˆ«ç»“æ„ä¿¡æ¯ä¹Ÿè¶³ä»¥å¼•å¯¼å‡ºæœ‰æ•ˆçš„åˆæˆè®­ç»ƒæµç¨‹ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æ•°æ®å—é™åœºæ™¯ä¸‹è¿›è¡ŒçŸ¥è¯†è¿ç§»çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.18533v1",
      "published_date": "2025-07-24 16:00:32 UTC",
      "updated_date": "2025-07-24 16:00:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:37:26.602945+00:00"
    },
    {
      "arxiv_id": "2507.18521v2",
      "title": "GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning",
      "title_zh": "GLANCEï¼šé¢å‘å¼‚è´¨å›¾è¡¨ç¤ºå­¦ä¹ çš„èšç±»å¢å¼ºå›¾é€»è¾‘æ³¨æ„åŠ›ç½‘ç»œ",
      "authors": [
        "Zhongtian Sun",
        "Anoushka Harit",
        "Alexandra Cristea",
        "Christl A. Donnelly",
        "Pietro LiÃ²"
      ],
      "abstract": "Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data but often struggle on heterophilous graphs, where connected nodes differ in features or class labels. This limitation arises from indiscriminate neighbor aggregation and insufficient incorporation of higher-order structural patterns. To address these challenges, we propose GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel framework that integrates logic-guided reasoning, dynamic graph refinement, and adaptive clustering to enhance graph representation learning. GLANCE combines a logic layer for interpretable and structured embeddings, multi-head attention-based edge pruning for denoising graph structures, and clustering mechanisms for capturing global patterns. Experimental results in benchmark datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE achieves competitive performance, offering robust and interpretable solutions for heterophilous graph scenarios. The proposed framework is lightweight, adaptable, and uniquely suited to the challenges of heterophilous graphs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GLANCE (Graph Logic Attention Network with Cluster Enhancement) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å›¾ç¥ç»ç½‘ç»œ (GNNs) åœ¨å¤„ç†å¼‚è´¨å›¾ (heterophilous graphs) æ—¶å› é‚»å±…èšåˆä¸åŠ é€‰æ‹©å’Œé«˜é˜¶ç»“æ„æ•æ‰ä¸è¶³è€Œå¯¼è‡´çš„æ€§èƒ½å—é™é—®é¢˜ã€‚GLANCE ç»“åˆäº†é€»è¾‘å¼•å¯¼æ¨ç†ã€åŠ¨æ€å›¾ä¼˜åŒ–å’Œè‡ªé€‚åº”èšç±»æŠ€æœ¯ï¼Œé€šè¿‡é€»è¾‘å±‚ (logic layer) ç”Ÿæˆå…·æœ‰å¯è§£é‡Šæ€§çš„ç»“æ„åŒ–åµŒå…¥ï¼Œå¹¶åˆ©ç”¨åŸºäºå¤šå¤´æ³¨æ„åŠ›çš„è¾¹å‰ªæ (edge pruning) æœºåˆ¶å¯¹å›¾ç»“æ„è¿›è¡Œå»å™ªã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†èšç±»æœºåˆ¶ (clustering mechanisms) ä»¥æ•æ‰å…¨å±€æ¨¡å¼ï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¼ºå›¾è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚åœ¨ Cornellã€Texas å’Œ Wisconsin ç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGLANCE å–å¾—äº†æå…·ç«äº‰åŠ›çš„è¡¨ç°ï¼Œä¸ºå¼‚è´¨å›¾åœºæ™¯æä¾›äº†ç¨³å¥ä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶å…·å¤‡è½»é‡çº§å’Œé«˜é€‚åº”æ€§çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤æ‚å¼‚è´¨å›¾å¸¦æ¥çš„å­¦æœ¯æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at International Joint Conference on Knowledge Graphs",
      "pdf_url": "https://arxiv.org/pdf/2507.18521v2",
      "published_date": "2025-07-24 15:45:26 UTC",
      "updated_date": "2025-09-29 10:26:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:37:30.686255+00:00"
    },
    {
      "arxiv_id": "2507.18512v1",
      "title": "Explaining How Visual, Textual and Multimodal Encoders Share Concepts",
      "title_zh": "è§£æè§†è§‰ã€æ–‡æœ¬ä¸å¤šæ¨¡æ€ç¼–ç å™¨é—´çš„æ¦‚å¿µå…±äº«",
      "authors": [
        "ClÃ©ment Cornet",
        "Romaric BesanÃ§on",
        "HervÃ© Le Borgne"
      ],
      "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful technique for extracting human-interpretable features from neural networks activations. Previous works compared different models based on SAE-derived features but those comparisons have been restricted to models within the same modality. We propose a novel indicator allowing quantitative comparison of models across SAE features, and use it to conduct a comparative study of visual, textual and multimodal encoders. We also propose to quantify the Comparative Sharedness of individual features between different classes of models. With these two new tools, we conduct several studies on 21 encoders of the three types, with two significantly different sizes, and considering generalist and domain specific datasets. The results allow to revisit previous studies at the light of encoders trained in a multimodal context and to quantify to which extent all these models share some representations or features. They also suggest that visual features that are specific to VLMs among vision encoders are shared with text encoders, highlighting the impact of text pretraining. The code is available at https://github.com/CEA-LIST/SAEshareConcepts",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨ Sparse Autoencoders (SAEs) æŠ€æœ¯æå–ç¥ç»ç½‘ç»œçš„å¯è§£é‡Šç‰¹å¾ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–æ¯”è¾ƒè§†è§‰ã€æ–‡æœ¬å’Œå¤šæ¨¡æ€ç¼–ç å™¨ä¹‹é—´çš„è·¨æ¨¡æ€ç‰¹å¾ã€‚é’ˆå¯¹ä»¥å¾€ç ”ç©¶ä»…é™äºåŒæ¨¡æ€æ¯”è¾ƒçš„å±€é™ï¼Œä½œè€…è¿˜å¼•å…¥äº† Comparative Sharedness å·¥å…·æ¥è¡¡é‡ä¸åŒç±»åˆ«æ¨¡å‹é—´å•ä¸ªç‰¹å¾çš„å…±äº«ç¨‹åº¦ã€‚é€šè¿‡å¯¹ 21 ç§æ¶µç›–ä¸åŒè§„æ¨¡ã€é€šç”¨åŠç‰¹å®šé¢†åŸŸæ•°æ®é›†çš„ç¼–ç å™¨è¿›è¡Œç³»ç»Ÿæ€§ç ”ç©¶ï¼Œè¯¥å·¥ä½œé‡åŒ–äº†ä¸åŒæ¨¡å‹åœ¨è¡¨ç¤ºå±‚é¢çš„å…±äº«ç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVision-Language Models (VLMs) ä¸­ç‰¹æœ‰çš„è§†è§‰ç‰¹å¾é€šå¸¸ä¸æ–‡æœ¬ç¼–ç å™¨å…±äº«ï¼Œè¿™æ­ç¤ºäº†æ–‡æœ¬é¢„è®­ç»ƒå¯¹è§†è§‰è¡¨å¾å¡‘é€ çš„æ·±è¿œå½±å“ã€‚è¯¥ç ”ç©¶ä¸ä»…æ·±åŒ–äº†å¯¹å¤šæ¨¡æ€æ¨¡å‹å†…éƒ¨æœºåˆ¶çš„ç†è§£ï¼Œè¿˜ä¸ºè·¨æ¨¡æ€æ¨¡å‹çš„å®šé‡æ¯”è¾ƒæä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18512v1",
      "published_date": "2025-07-24 15:33:31 UTC",
      "updated_date": "2025-07-24 15:33:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:37:36.680879+00:00"
    },
    {
      "arxiv_id": "2507.18484v1",
      "title": "Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments",
      "title_zh": "å¼ºåŒ–å…·èº«ä¸»åŠ¨é˜²å¾¡ï¼šåˆ©ç”¨è‡ªé€‚åº”äº¤äº’æå‡å¯¹æŠ—æ€§ 3D ç¯å¢ƒä¸­çš„é²æ£’è§†è§‰æ„ŸçŸ¥",
      "authors": [
        "Xiao Yang",
        "Lingxuan Wu",
        "Lizhong Wang",
        "Chengyang Ying",
        "Hang Su",
        "Jun Zhu"
      ],
      "abstract": "Adversarial attacks in 3D environments have emerged as a critical threat to the reliability of visual perception systems, particularly in safety-sensitive applications such as identity verification and autonomous driving. These attacks employ adversarial patches and 3D objects to manipulate deep neural network (DNN) predictions by exploiting vulnerabilities within complex scenes. Existing defense mechanisms, such as adversarial training and purification, primarily employ passive strategies to enhance robustness. However, these approaches often rely on pre-defined assumptions about adversarial tactics, limiting their adaptability in dynamic 3D settings. To address these challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a proactive defense framework that leverages adaptive exploration and interaction with the environment to improve perception robustness in 3D adversarial contexts. By implementing a multi-step objective that balances immediate prediction accuracy with predictive entropy minimization, Rein-EAD optimizes defense strategies over a multi-step horizon. Additionally, Rein-EAD involves an uncertainty-oriented reward-shaping mechanism that facilitates efficient policy updates, thereby reducing computational overhead and supporting real-world applicability without the need for differentiable environments. Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating a substantial reduction in attack success rates while preserving standard accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization to unseen and adaptive attacks, making it suitable for real-world complex tasks, including 3D object classification, face recognition and autonomous driving.",
      "tldr_zh": "é’ˆå¯¹3Dç¯å¢ƒä¸­å¯¹æŠ—æ€§è¡¥ä¸(Adversarial patches)å’Œ3Då¯¹è±¡å¯¹è§†è§‰æ„ŸçŸ¥ç³»ç»Ÿæ„æˆçš„å®‰å…¨å¨èƒï¼Œç°æœ‰çš„è¢«åŠ¨é˜²å¾¡æœºåˆ¶åœ¨åŠ¨æ€åœºæ™¯ä¸­å¾€å¾€ç¼ºä¹é€‚åº”æ€§ã€‚è¯¥ç ”ç©¶æå‡ºäº†å¼ºåŒ–å…·èº«ä¸»åŠ¨é˜²å¾¡æ¡†æ¶ Rein-EAD (Reinforced Embodied Active Defense)ï¼Œé€šè¿‡ä¸ç¯å¢ƒçš„è‡ªé€‚åº”æ¢ç´¢å’Œäº¤äº’æ¥ä¸»åŠ¨æå‡æ„ŸçŸ¥é²æ£’æ€§ã€‚Rein-EAD é‡‡ç”¨å¤šæ­¥ä¼˜åŒ–ç›®æ ‡ï¼Œå¹³è¡¡äº†å³æ—¶é¢„æµ‹å‡†ç¡®æ€§ä¸é¢„æµ‹ç†µæœ€å°åŒ–(Predictive entropy minimization)ï¼Œå¹¶å¼•å…¥äº†é¢å‘ä¸ç¡®å®šæ€§çš„å¥–åŠ±é‡å¡‘æœºåˆ¶(Uncertainty-oriented reward-shaping)ä»¥å®ç°é«˜æ•ˆçš„ç­–ç•¥æ›´æ–°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸ä¾èµ–å¯å¾®ç¯å¢ƒçš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½äº†å¤šç§3Dä»»åŠ¡ä¸­çš„æ”»å‡»æˆåŠŸç‡ï¼Œå¹¶åœ¨å¯¹è±¡åˆ†ç±»ã€äººè„¸è¯†åˆ«å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå±•ç°äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚Rein-EAD å¯¹æœªçŸ¥å’Œè‡ªé€‚åº”æ”»å‡»è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¤æ‚3Dåœºæ™¯ä¸‹çš„è§†è§‰ç³»ç»Ÿæä¾›äº†æ›´å…·é²æ£’æ€§çš„ä¸»åŠ¨é˜²å¾¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: text overlap with arXiv:2404.00540",
      "pdf_url": "https://arxiv.org/pdf/2507.18484v1",
      "published_date": "2025-07-24 14:56:21 UTC",
      "updated_date": "2025-07-24 14:56:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:37:39.383790+00:00"
    },
    {
      "arxiv_id": "2507.18678v1",
      "title": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting",
      "title_zh": "é€šè¿‡2Dåˆ°3Dæ•°æ®å‡ç»´è¿ˆå‘å¯æ‰©å±•çš„ç©ºé—´æ™ºèƒ½",
      "authors": [
        "Xingyu Miao",
        "Haoran Duan",
        "Quanhao Qian",
        "Jiuniu Wang",
        "Yang Long",
        "Ling Shao",
        "Deli Zhao",
        "Ran Xu",
        "Gongjie Zhang"
      ],
      "abstract": "Spatial intelligence is emerging as a transformative frontier in AI, yet it remains constrained by the scarcity of large-scale 3D datasets. Unlike the abundant 2D imagery, acquiring 3D data typically requires specialized sensors and laborious annotation. In this work, we present a scalable pipeline that converts single-view images into comprehensive, scale- and appearance-realistic 3D representations - including point clouds, camera poses, depth maps, and pseudo-RGBD - via integrated depth estimation, camera calibration, and scale calibration. Our method bridges the gap between the vast repository of imagery and the increasing demand for spatial scene understanding. By automatically generating authentic, scale-aware 3D data from images, we significantly reduce data collection costs and open new avenues for advancing spatial intelligence. We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D, and demonstrate through extensive experiments that our generated data can benefit various 3D tasks, ranging from fundamental perception to MLLM-based reasoning. These results validate our pipeline as an effective solution for developing AI systems capable of perceiving, understanding, and interacting with physical environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ AI é¢†åŸŸç©ºé—´æ™ºèƒ½ (Spatial Intelligence) å› å¤§è§„æ¨¡ 3D æ•°æ®é›†åŒ®ä¹è€Œå—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡ 2D åˆ° 3D æ•°æ®æå‡ (2D-to-3D Data Lifting) çš„å¯æ‰©å±•æµæ°´çº¿ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå°†å•è§†å›¾å›¾åƒè½¬åŒ–ä¸ºåŒ…å«ç‚¹äº‘ (Point Clouds)ã€ç›¸æœºä½å§¿ (Camera Poses)ã€æ·±åº¦å›¾ (Depth Maps) åŠä¼ª RGBD (Pseudo-RGBD) çš„é«˜ä¿åŸä¸‰ç»´è¡¨ç¤ºã€‚å…¶æ ¸å¿ƒæŠ€æœ¯é›†æˆäº†æ·±åº¦ä¼°è®¡ (Depth Estimation)ã€ç›¸æœºæ ¡å‡† (Camera Calibration) ä»¥åŠå°ºåº¦æ ¡å‡† (Scale Calibration)ï¼Œå®ç°äº†åœ¨å°ºåº¦å’Œå¤–è§‚ä¸Šå‡å…·å¤‡çœŸå®æ„Ÿçš„åœºæ™¯é‡å»ºã€‚åŸºäºæ­¤å·¥ä½œï¼Œä½œè€…å‘å¸ƒäº† COCO-3D å’Œ Objects365-v2-3D ä¸¤ä¸ªå¤§è§„æ¨¡ç©ºé—´æ•°æ®é›†ï¼Œå¤§å¹…é™ä½äº† 3D æ•°æ®çš„è·å–ä¸æ ‡æ³¨æˆæœ¬ã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›ç”Ÿæˆçš„ 3D æ•°æ®èƒ½æ˜¾è‘—æå‡ä»åŸºç¡€æ„ŸçŸ¥åˆ°åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) æ¨ç†ç­‰å¤šç§ 3D ä»»åŠ¡çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘èƒ½å¤Ÿæ„ŸçŸ¥ã€ç†è§£å¹¶ä¸ç‰©ç†ç¯å¢ƒäº’åŠ¨çš„ AI ç³»ç»Ÿæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§„æ¨¡åŒ–æ•°æ®è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025 (Highlight)",
      "pdf_url": "https://arxiv.org/pdf/2507.18678v1",
      "published_date": "2025-07-24 14:53:26 UTC",
      "updated_date": "2025-07-24 14:53:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:37:44.133572+00:00"
    },
    {
      "arxiv_id": "2507.18476v1",
      "title": "Automated Code Review Using Large Language Models with Symbolic Reasoning",
      "title_zh": "ç»“åˆç¬¦å·æ¨ç†ä¸å¤§è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥",
      "authors": [
        "Busra Icoz",
        "Goksel Biricik"
      ],
      "abstract": "Code review is one of the key processes in the software development lifecycle and is essential to maintain code quality. However, manual code review is subjective and time consuming. Given its rule-based nature, code review is well suited for automation. In recent years, significant efforts have been made to automate this process with the help of artificial intelligence. Recent developments in Large Language Models (LLMs) have also emerged as a promising tool in this area, but these models often lack the logical reasoning capabilities needed to fully understand and evaluate code. To overcome this limitation, this study proposes a hybrid approach that integrates symbolic reasoning techniques with LLMs to automate the code review process. We tested our approach using the CodexGlue dataset, comparing several models, including CodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining symbolic reasoning and prompting techniques with LLMs. Our results show that this approach improves the accuracy and efficiency of automated code review.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹äººå·¥ä»£ç å®¡æŸ¥(Code review)ä¸»è§‚ä¸”è€—æ—¶çš„ç—›ç‚¹ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆç¬¦å·æ¨ç†(Symbolic Reasoning)æŠ€æœ¯ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ··åˆè‡ªåŠ¨åŒ–æ–¹æ³•ã€‚ç”±äºLLMsåœ¨å¤„ç†å¤æ‚é€»è¾‘æ—¶å¾€å¾€ç¼ºä¹å…¨é¢ç†è§£å’Œè¯„ä¼°ä»£ç çš„èƒ½åŠ›ï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡é›†æˆç¬¦å·æ¨ç†æ¥å¢å¼ºæ¨¡å‹çš„é€»è¾‘æ¨å¯¼æ°´å¹³ã€‚ç ”ç©¶äººå‘˜åœ¨CodexGlueæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶å°†å…¶ä¸CodeT5ã€CodeBERTåŠGraphCodeBERTç­‰ç°æœ‰æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç»“åˆäº†ç¬¦å·æ¨ç†ä¸æç¤º(Prompting)æŠ€æœ¯çš„æ–¹æ³•æ˜¾è‘—æå‡äº†è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­å®ç°æ›´å¯é ã€æ›´é«˜æ•ˆçš„ä»£ç è´¨é‡æ§åˆ¶æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18476v1",
      "published_date": "2025-07-24 14:50:27 UTC",
      "updated_date": "2025-07-24 14:50:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:37:46.775428+00:00"
    },
    {
      "arxiv_id": "2507.18457v1",
      "title": "Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols",
      "title_zh": "å†æ¢é’ˆå¯¹æ¿€å…‰é›·è¾¾æ£€æµ‹çš„ç‰©ç†å¯å®ç°å¯¹æŠ—ç‰©ä½“æ”»å‡»ï¼šæ˜ç¡®é—®é¢˜å»ºæ¨¡ä¸å®éªŒè§„èŒƒ",
      "authors": [
        "Luo Cheng",
        "Hanwei Zhang",
        "Lijun Zhang",
        "Holger Hermanns"
      ],
      "abstract": "Adversarial robustness in LiDAR-based 3D object detection is a critical research area due to its widespread application in real-world scenarios. While many digital attacks manipulate point clouds or meshes, they often lack physical realizability, limiting their practical impact. Physical adversarial object attacks remain underexplored and suffer from poor reproducibility due to inconsistent setups and hardware differences. To address this, we propose a device-agnostic, standardized framework that abstracts key elements of physical adversarial object attacks, supports diverse methods, and provides open-source code with benchmarking protocols in simulation and real-world settings. Our framework enables fair comparison, accelerates research, and is validated by successfully transferring simulated attacks to a physical LiDAR system. Beyond the framework, we offer insights into factors influencing attack success and advance understanding of adversarial robustness in real-world LiDAR perception.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é‡æ–°å®¡è§†äº†é’ˆå¯¹ LiDAR 3Dç›®æ ‡æ£€æµ‹ (3D object detection) çš„ç‰©ç†å¯å®ç°å¯¹æŠ—æ”»å‡»ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ•°å­—æ”»å‡»åœ¨ç°å®åœºæ™¯ä¸­ç¼ºä¹å¯è¡Œæ€§ï¼Œä¸”ç‰©ç†æ”»å‡»å› å®éªŒåè®®ä¸ç»Ÿä¸€è€Œéš¾ä»¥å¤ç°ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªè®¾å¤‡æ— å…³ (device-agnostic) çš„æ ‡å‡†åŒ–æ¡†æ¶ï¼Œç”¨äºæŠ½è±¡ç‰©ç†å¯¹æŠ—æ”»å‡»çš„å…³é”®è¦ç´ å¹¶æ”¯æŒå¤šæ ·åŒ–æ”»å‡»æ–¹æ³•çš„å®æ–½ã€‚è¯¥æ¡†æ¶æä¾›äº†å¼€æºä»£ç ä»¥åŠæ¶µç›–ä»¿çœŸä¸çœŸå®ç¯å¢ƒçš„åŸºå‡†æµ‹è¯•åè®®ï¼Œç¡®ä¿äº†ä¸åŒç ”ç©¶ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒã€‚é€šè¿‡å°†æ¨¡æ‹Ÿæ”»å‡»æˆåŠŸè½¬ç§»è‡³ç‰©ç† LiDAR ç³»ç»Ÿï¼Œç ”ç©¶éªŒè¯äº†è¯¥æ¡†æ¶çš„å®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ·±å…¥æ¢è®¨äº†å½±å“æ”»å‡»æˆåŠŸç‡çš„å„ç±»å› ç´ ï¼Œä¸ºæå‡ç°å®ä¸–ç•Œ LiDAR æ„ŸçŸ¥çš„å¯¹æŠ—é²æ£’æ€§ (adversarial robustness) æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18457v1",
      "published_date": "2025-07-24 14:37:00 UTC",
      "updated_date": "2025-07-24 14:37:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:37:51.086181+00:00"
    },
    {
      "arxiv_id": "2507.18451v1",
      "title": "Generation of Synthetic Clinical Text: A Systematic Review",
      "title_zh": "åˆæˆä¸´åºŠæ–‡æœ¬ç”Ÿæˆï¼šç³»ç»Ÿç»¼è¿°",
      "authors": [
        "Basel Alshaikhdeeb",
        "Ahmed Abdelmonem Hemedan",
        "Soumyabrata Ghosh",
        "Irina Balaur",
        "Venkata Satagopam"
      ],
      "abstract": "Generating clinical synthetic text represents an effective solution for common clinical NLP issues like sparsity and privacy. This paper aims to conduct a systematic review on generating synthetic medical free-text by formulating quantitative analysis to three research questions concerning (i) the purpose of generation, (ii) the techniques, and (iii) the evaluation methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE, Google Scholar, and arXiv databases for publications associated with generating synthetic medical unstructured free-text. We have identified 94 relevant articles out of 1,398 collected ones. A great deal of attention has been given to the generation of synthetic medical text from 2018 onwards, where the main purpose of such a generation is towards text augmentation, assistive writing, corpus building, privacy-preserving, annotation, and usefulness. Transformer architectures were the main predominant technique used to generate the text, especially the GPTs. On the other hand, there were four main aspects of evaluation, including similarity, privacy, structure, and utility, where utility was the most frequent method used to assess the generated synthetic medical text. Although the generated synthetic medical text demonstrated a moderate possibility to act as real medical documents in different downstream NLP tasks, it has proven to be a great asset as augmented, complementary to the real documents, towards improving the accuracy and overcoming sparsity/undersampling issues. Yet, privacy is still a major issue behind generating synthetic medical text, where more human assessments are needed to check for the existence of any sensitive information. Despite that, advances in generating synthetic medical text will considerably accelerate the adoption of workflows and pipeline development, discarding the time-consuming legalities of data transfer.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆåˆæˆä¸´åºŠæ–‡æœ¬(Synthetic Clinical Text)è¿›è¡Œäº†ç³»ç»Ÿç»¼è¿°ï¼Œæ—¨åœ¨è§£å†³ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­å¸¸è§çš„æ•°æ®ç¨€ç–å’Œéšç§ä¿æŠ¤é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡å¯¹94ç¯‡æ ¸å¿ƒæ–‡çŒ®çš„å®šé‡åˆ†æï¼Œç³»ç»Ÿæ¢è®¨äº†ç”Ÿæˆç›®çš„ã€æŠ€æœ¯æ¶æ„åŠè¯„ä¼°æ–¹æ³•ï¼Œå‘ç°è‡ª2018å¹´èµ·è¯¥é¢†åŸŸå…³æ³¨åº¦æ˜¾è‘—æå‡ã€‚æŠ€æœ¯å±‚é¢ä»¥Transformeræ¶æ„ï¼ˆç‰¹åˆ«æ˜¯GPTæ¨¡å‹ï¼‰ä¸ºä¸»ï¼Œè€Œè¯„ä¼°åˆ™ä¾§é‡äºæ•ˆç”¨(Utility)ã€ç›¸ä¼¼æ€§ã€éšç§å’Œç»“æ„å››ä¸ªç»´åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œåˆæˆæ–‡æœ¬ä½œä¸ºçœŸå®æ–‡æ¡£çš„è¡¥å……èµ„äº§ï¼Œåœ¨æå‡ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®æ€§å’Œè§£å†³æ¬ é‡‡æ ·é—®é¢˜æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚å°½ç®¡åˆæˆæ–‡æœ¬èƒ½æ˜¾è‘—åŠ é€Ÿå·¥ä½œæµå¼€å‘å¹¶ç»•è¿‡ç¹ççš„æ•°æ®ä¼ è¾“æ³•å¾‹é™åˆ¶ï¼Œä½†éšç§ä¿æŠ¤ä»æ˜¯å½“å‰é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ï¼ŒäºŸéœ€æ›´å¤šäººå·¥è¯„ä¼°æ¥ç¡®ä¿æ•æ„Ÿä¿¡æ¯çš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18451v1",
      "published_date": "2025-07-24 14:35:16 UTC",
      "updated_date": "2025-07-24 14:35:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:37:54.774999+00:00"
    },
    {
      "arxiv_id": "2507.18448v2",
      "title": "Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, A Low-Resource Language",
      "title_zh": "éŸµå¾‹é‡ç°ï¼šåŸºäº Transformer æ¨¡å‹çš„ä½èµ„æºè¯­è¨€å­ŸåŠ æ‹‰è¯­æ ‡ç‚¹æ¢å¤",
      "authors": [
        "Md Obyedullahil Mamun",
        "Md Adyelullahil Mamun",
        "Arif Ahmad",
        "Md. Imran Hossain Emu"
      ],
      "abstract": "Punctuation restoration enhances the readability of text and is critical for post-processing tasks in Automatic Speech Recognition (ASR), especially for low-resource languages like Bangla. In this study, we explore the application of transformer-based models, specifically XLM-RoBERTa-large, to automatically restore punctuation in unpunctuated Bangla text. We focus on predicting four punctuation marks: period, comma, question mark, and exclamation mark across diverse text domains. To address the scarcity of annotated resources, we constructed a large, varied training corpus and applied data augmentation techniques. Our best-performing model, trained with an augmentation factor of alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the Reference set, and 90.2% on the ASR set.\n  Results show strong generalization to reference and ASR transcripts, demonstrating the model's effectiveness in real-world, noisy scenarios. This work establishes a strong baseline for Bangla punctuation restoration and contributes publicly available datasets and code to support future research in low-resource NLP.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä½èµ„æºè¯­è¨€å­ŸåŠ æ‹‰è¯­(Bangla)çš„æ ‡ç‚¹æ¢å¤(Punctuation Restoration)é—®é¢˜ï¼Œæ—¨åœ¨æå‡æ–‡æœ¬å¯è¯»æ€§å¹¶ä¼˜åŒ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ç³»ç»Ÿçš„åå¤„ç†æ•ˆæœã€‚ä½œè€…é‡‡ç”¨åŸºäºTransformerçš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯XLM-RoBERTa-largeï¼Œé’ˆå¯¹å¥å·ã€é€—å·ã€é—®å·å’Œæ„Ÿå¹å·å››ç§æ ‡ç‚¹è¿›è¡Œè‡ªåŠ¨é¢„æµ‹ã€‚ä¸ºè§£å†³å­ŸåŠ æ‹‰è¯­æ ‡æ³¨èµ„æºåŒ®ä¹çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†å¤§è§„æ¨¡å¤šæ ·åŒ–çš„è®­ç»ƒè¯­æ–™åº“ï¼Œå¹¶å¼•å…¥äº†æ•°æ®å¢å¼º(Data Augmentation)æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨$\\alpha=0.20\\%$çš„å¢å¼ºå› å­ä¸‹ï¼Œæ¨¡å‹åœ¨æ–°é—»æµ‹è¯•é›†ã€å‚è€ƒé›†å’ŒASRé›†ä¸Šåˆ†åˆ«å®ç°äº†97.1%ã€91.2%å’Œ90.2%çš„å‡†ç¡®ç‡ã€‚è¯¥æ¨¡å‹å±•ç°äº†åœ¨çœŸå®å™ªå£°ç¯å¢ƒä¸‹çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå­ŸåŠ æ‹‰è¯­çš„æ ‡ç‚¹æ¢å¤ä»»åŠ¡å»ºç«‹äº†ç¨³å¥çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å…¬å¼€äº†ç›¸å…³æ•°æ®é›†å’Œä»£ç ï¼Œä¸ºä½èµ„æºè‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸçš„åç»­ç ”ç©¶æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18448v2",
      "published_date": "2025-07-24 14:33:13 UTC",
      "updated_date": "2026-01-11 15:27:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:38:12.776548+00:00"
    },
    {
      "arxiv_id": "2507.18442v1",
      "title": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data",
      "title_zh": "AraTableï¼šè¯„ä¼° LLMs å¯¹é˜¿æ‹‰ä¼¯è¯­è¡¨æ ¼æ•°æ®æ¨ç†ä¸ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Rana Alshaikh",
        "Israa Alghanmi",
        "Shelan Jeawak"
      ],
      "abstract": "The cognitive and reasoning abilities of large language models (LLMs) have enabled remarkable progress in natural language processing. However, their performance in interpreting structured data, especially in tabular formats, remains limited. Although benchmarks for English tabular data are widely available, Arabic is still underrepresented because of the limited availability of public resources and its unique language features. To address this gap, we present AraTable, a novel and comprehensive benchmark designed to evaluate the reasoning and understanding capabilities of LLMs when applied to Arabic tabular data. AraTable consists of various evaluation tasks, such as direct question answering, fact verification, and complex reasoning, involving a wide range of Arabic tabular sources. Our methodology follows a hybrid pipeline, where initial content is generated by LLMs and subsequently filtered and verified by human experts to ensure high dataset quality. Initial analyses using AraTable show that, while LLMs perform adequately on simpler tabular tasks such as direct question answering, they continue to face significant cognitive challenges when tasks require deeper reasoning and fact verification. This indicates that there are substantial opportunities for future work to improve performance on complex tabular reasoning tasks. We also propose a fully automated evaluation framework that uses a self-deliberation mechanism and achieves performance nearly identical to that of human judges. This research provides a valuable, publicly available resource and evaluation framework that can help accelerate the development of foundational models for processing and analysing Arabic structured data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† AraTableï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¯¹é˜¿æ‹‰ä¼¯è¯­è¡¨æ ¼æ•°æ®ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç”±äºå…¬å¼€èµ„æºç¨€ç¼ºå’Œè¯­è¨€ç‰¹æ€§å¯¼è‡´çš„ç©ºç™½ã€‚AraTable æ¶µç›–äº†ç›´æ¥é—®ç­” (direct question answering)ã€äº‹å®æ ¸æŸ¥ (fact verification) ä»¥åŠå¤æ‚æ¨ç† (complex reasoning) ç­‰å¤šæ ·åŒ–è¯„ä¼°ä»»åŠ¡ï¼Œæ¶‰åŠå¹¿æ³›çš„é˜¿æ‹‰ä¼¯è¯­è¡¨æ ¼æ¥æºã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº†ä¸€ç§æ··åˆå·¥ä½œæµï¼Œé€šè¿‡å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆå§‹å†…å®¹å¹¶è¾…ä»¥äººç±»ä¸“å®¶çš„ä¸¥æ ¼è¿‡æ»¤ä¸éªŒè¯ï¼Œä»¥ç¡®ä¿æ•°æ®é›†çš„é«˜è´¨é‡ã€‚åˆæ­¥åˆ†ææ˜¾ç¤ºï¼Œè™½ç„¶ LLMs åœ¨ç®€å•çš„é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å°šå¯ï¼Œä½†åœ¨é¢å¯¹éœ€è¦æ·±åº¦æ¨ç†å’Œäº‹å®æ ¸æŸ¥çš„ä»»åŠ¡æ—¶ä»é¢ä¸´æ˜¾è‘—çš„è®¤çŸ¥æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åŸºäºè‡ªæˆ‘å®¡è®®æœºåˆ¶ (self-deliberation mechanism) çš„å…¨è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œå…¶è¯„ä¼°æ•ˆæœä¸äººç±»è¯„åˆ¤å‡ ä¹ä¸€è‡´ã€‚è¿™ä¸€å¼€æºèµ„æºå’Œè¯„ä¼°æ¡†æ¶ä¸ºå¤„ç†é˜¿æ‹‰ä¼¯è¯­ç»“æ„åŒ–æ•°æ®çš„æ¨¡å‹å¼€å‘æä¾›äº†é‡è¦æ”¯æ’‘ï¼Œä¸ºæœªæ¥æå‡å¤æ‚è¡¨æ ¼æ¨ç†æ€§èƒ½å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18442v1",
      "published_date": "2025-07-24 14:26:41 UTC",
      "updated_date": "2025-07-24 14:26:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:38:15.078916+00:00"
    },
    {
      "arxiv_id": "2507.18413v1",
      "title": "GPU Accelerated Compact-Table Propagation",
      "title_zh": "GPU åŠ é€Ÿçš„ Compact-Table çº¦æŸä¼ æ’­",
      "authors": [
        "Enrico Santi",
        "Fabio Tardivo",
        "Agostino Dovier",
        "Andrea Formisano"
      ],
      "abstract": "Constraint Programming developed within Logic Programming in the Eighties; nowadays all Prolog systems encompass modules capable of handling constraint programming on finite domains demanding their solution to a constraint solver. This work focuses on a specific form of constraint, the so-called table constraint, used to specify conditions on the values of variables as an enumeration of alternative options. Since every condition on a set of finite domain variables can be ultimately expressed as a finite set of cases, Table can, in principle, simulate any other constraint. These characteristics make Table one of the most studied constraints ever, leading to a series of increasingly efficient propagation algorithms. Despite this, it is not uncommon to encounter real-world problems with hundreds or thousands of valid cases that are simply too many to be handled effectively with standard CPU-based approaches. In this paper, we deal with the Compact-Table (CT) algorithm, the state-of-the-art propagation algorithms for Table. We describe how CT can be enhanced by exploiting the massive computational power offered by modern GPUs to handle large Table constraints. In particular, we report on the design and implementation of GPU-accelerated CT, on its integration into an existing constraint solver, and on an experimental validation performed on a significant set of instances.",
      "tldr_zh": "è¯¥ç ”ç©¶èšç„¦äºçº¦æŸç¼–ç¨‹ (Constraint Programming) ä¸­çš„ Table çº¦æŸï¼Œé’ˆå¯¹ä¼ ç»Ÿ CPU æ–¹æ³•åœ¨å¤„ç†åŒ…å«æ•°åƒä¸ªæœ‰æ•ˆæ¡ˆä¾‹çš„å¤§è§„æ¨¡é—®é¢˜æ—¶æ•ˆç‡å—é™çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº GPU åŠ é€Ÿçš„ä¼ æ’­æ–¹æ¡ˆã€‚ä½œè€…è¯¦ç»†é˜è¿°äº†é’ˆå¯¹ç›®å‰æœ€å…ˆè¿›çš„ Compact-Table (CT) ç®—æ³•è¿›è¡Œ GPU å¹¶è¡ŒåŒ–æ”¹è¿›çš„è®¾è®¡ä¸å®ç°ï¼Œå¹¶å°†å…¶é›†æˆè‡³ç°æœ‰çš„çº¦æŸæ±‚è§£å™¨ (constraint solver) ä¸­ã€‚é€šè¿‡å¯¹å¤§é‡å®ä¾‹è¿›è¡Œçš„å®éªŒéªŒè¯ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº† GPU-accelerated CT åœ¨å¤„ç†å¤§è§„æ¨¡ Table çº¦æŸæ—¶çš„å“è¶Šæ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œæœ‰æ•ˆåˆ©ç”¨äº†ç°ä»£ GPU çš„å¤§è§„æ¨¡è®¡ç®—èƒ½åŠ›ï¼Œä¸ºæå‡å¤æ‚çº¦æŸé—®é¢˜çš„æ±‚è§£æ•ˆç‡æä¾›äº†æ–°çš„æŠ€æœ¯é€”å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under consideration in Theory and Practice of Logic Programming (TPLP)",
      "pdf_url": "https://arxiv.org/pdf/2507.18413v1",
      "published_date": "2025-07-24 13:53:49 UTC",
      "updated_date": "2025-07-24 13:53:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:38:23.678112+00:00"
    },
    {
      "arxiv_id": "2507.22931v3",
      "title": "Enhancing RAG Efficiency with Adaptive Context Compression",
      "title_zh": "é€šè¿‡è‡ªé€‚åº”ä¸Šä¸‹æ–‡å‹ç¼©æå‡ RAG æ•ˆç‡",
      "authors": [
        "Shuyu Guo",
        "Shuo Zhang",
        "Zhaochun Ren"
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-augmented generation, RAG)ç”±äºæ£€ç´¢ä¸Šä¸‹æ–‡è¿‡é•¿å¯¼è‡´æ¨ç†æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†è‡ªé€‚åº”ä¸Šä¸‹æ–‡å‹ç¼©æ¡†æ¶ACC-RAGã€‚è¯¥æ¡†æ¶æ”¹å˜äº†ç°æœ‰æ–¹æ³•é‡‡ç”¨å›ºå®šå‹ç¼©ç‡çš„å±€é™ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥å¤æ‚ç¨‹åº¦åŠ¨æ€è°ƒæ•´å‹ç¼©æ¯”ï¼Œä»è€Œåœ¨ä¼˜åŒ–æ¨ç†æ•ˆç‡çš„åŒæ—¶ç¡®ä¿å›ç­”å‡†ç¡®æ€§ã€‚ACC-RAGç»“åˆäº†å±‚æ¬¡åŒ–å‹ç¼©å™¨(hierarchical compressor)å’Œä¸Šä¸‹æ–‡é€‰æ‹©å™¨(context selector)ï¼Œåˆ©ç”¨å¤šç²’åº¦åµŒå…¥(multi-granular embeddings)ä»…ä¿ç•™æœ€å…³é”®ä¸”å……åˆ†çš„ä¿¡æ¯ï¼Œå…¶æœºåˆ¶ç±»ä¼¼äºäººç±»çš„ç•¥è¯»è¡Œä¸ºã€‚å®éªŒåœ¨Wikipediaå’Œäº”ä¸ªé—®ç­”æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºACC-RAGçš„æ¨ç†é€Ÿåº¦æ¯”æ ‡å‡†RAGæå‡äº†4å€ä»¥ä¸Šï¼Œä¸”æ€§èƒ½ä¼˜äºå›ºå®šå‹ç¼©ç‡æ–¹æ³•ã€‚è¿™ä¸€æ–¹æ¡ˆé€šè¿‡åŠ¨æ€å‹ç¼©ç­–ç•¥ï¼Œåœ¨ä¿æŒç”šè‡³æå‡å‡†ç¡®æ€§çš„å‰æä¸‹æ˜¾è‘—é™ä½äº†ç³»ç»Ÿçš„è®¡ç®—å¼€é”€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22931v3",
      "published_date": "2025-07-24 13:46:51 UTC",
      "updated_date": "2025-09-24 16:41:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:38:20.077376+00:00"
    },
    {
      "arxiv_id": "2507.18398v1",
      "title": "Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„å‘¼å«ä¸­å¿ƒè¿è¥ä¼˜åŒ–ï¼šä»·å€¼è¿­ä»£ä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–çš„æ¯”è¾ƒ",
      "authors": [
        "Kwong Ho Li",
        "Wathsala Karunarathne"
      ],
      "abstract": "This paper investigates the application of Reinforcement Learning (RL) to optimise call routing in call centres to minimise client waiting time and staff idle time. Two methods are compared: a model-based approach using Value Iteration (VI) under known system dynamics, and a model-free approach using Proximal Policy Optimisation (PPO) that learns from experience. For the model-based approach, a theoretical model is used, while a simulation model combining Discrete Event Simulation (DES) with the OpenAI Gym environment is developed for model-free learning. Both models frame the problem as a Markov Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with Poisson client arrivals and exponentially distributed service and abandonment times. For policy evaluation, random, VI, and PPO policies are evaluated using the simulation model. After 1,000 test episodes, PPO consistently achives the highest rewards, along with the lowest client waiting time and staff idle time, despite requiring longer training time.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¼˜åŒ–å‘¼å«ä¸­å¿ƒ(Call Centre)çš„å‘¼å«è·¯ç”±é—®é¢˜ï¼Œæ—¨åœ¨å¹³è¡¡å¹¶æœ€å°åŒ–å®¢æˆ·ç­‰å¾…æ—¶é—´ä¸å‘˜å·¥ç©ºé—²æ—¶é—´ã€‚è®ºæ–‡å¯¹æ¯”äº†ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼šåœ¨å·²çŸ¥ç³»ç»ŸåŠ¨åŠ›å­¦ä¸‹çš„åŸºäºæ¨¡å‹(model-based)çš„ä»·å€¼è¿­ä»£(Value Iteration)ä»¥åŠé€šè¿‡ç»éªŒå­¦ä¹ çš„æ— æ¨¡å‹(model-free)è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(Proximal Policy Optimisation)ã€‚ç ”ç©¶è€…å°†è¯¥é—®é¢˜å»ºæ¨¡ä¸ºåŸºäºæŠ€èƒ½è·¯ç”±(Skills-Based Routing)æ¡†æ¶ä¸‹çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process)ï¼Œå¹¶ç»“åˆç¦»æ•£äº‹ä»¶æ¨¡æ‹Ÿ(Discrete Event Simulation)ä¸OpenAI Gymç¯å¢ƒå¼€å‘äº†ä»¿çœŸç³»ç»Ÿã€‚å®éªŒè®¾ç½®è€ƒè™‘äº†æ³Šæ¾å®¢æˆ·åˆ°è¾¾(Poisson arrivals)ä»¥åŠæŒ‡æ•°åˆ†å¸ƒçš„æœåŠ¡å’Œæ”¾å¼ƒæ—¶é—´ã€‚ç»è¿‡1000ä¸ªæµ‹è¯•å‘¨æœŸçš„è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºPPOç­–ç•¥åœ¨å¥–åŠ±è·å–ã€ç¼©çŸ­å®¢æˆ·ç­‰å¾…æ—¶é—´ä»¥åŠé™ä½å‘˜å·¥ç©ºé—²æ—¶é—´æ–¹é¢å‡ä¼˜äºVIå’Œéšæœºç­–ç•¥ã€‚å°½ç®¡PPOéœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œä½†å…¶åœ¨å¤„ç†å¤æ‚çš„å‘¼å«ä¸­å¿ƒè¿è¥ä¼˜åŒ–ä»»åŠ¡ä¸­å±•ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.18398v1",
      "published_date": "2025-07-24 13:31:38 UTC",
      "updated_date": "2025-07-24 13:31:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:38:26.484744+00:00"
    },
    {
      "arxiv_id": "2507.18392v1",
      "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy",
      "title_zh": "CLEARï¼šè®©åŸºäºå¤§æ¨¡å‹è¯„åˆ¤çš„é”™è¯¯åˆ†æå˜å¾—ç®€å•",
      "authors": [
        "Asaf Yehudai",
        "Lilach Eden",
        "Yotam Perlitz",
        "Roy Bar-Haim",
        "Michal Shmueli-Scheuer"
      ],
      "abstract": "The evaluation of Large Language Models (LLMs) increasingly relies on other LLMs acting as judges. However, current evaluation paradigms typically yield a single score or ranking, answering which model is better but not why. While essential for benchmarking, these top-level scores obscure the specific, actionable reasons behind a model's performance. To bridge this gap, we introduce CLEAR, an interactive, open-source package for LLM-based error analysis. CLEAR first generates per-instance textual feedback, then it creates a set of system-level error issues, and quantifies the prevalence of each identified issue. Our package also provides users with an interactive dashboard that allows for a comprehensive error analysis through aggregate visualizations, applies interactive filters to isolate specific issues or score ranges, and drills down to the individual instances that exemplify a particular behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks, and showcase its utility through a user case study.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰ LLM-as-a-Judge è¯„ä¼°èŒƒå¼ä»…èƒ½æä¾›å•ä¸€åˆ†æ•°æˆ–æ’åè€Œæ— æ³•è§£é‡Šé”™è¯¯åŸå› çš„å±€é™æ€§ï¼Œæå‡ºäº†å¼€æºäº¤äº’å¼å·¥å…·åŒ… CLEARã€‚è¯¥å·¥å…·åŒ…é€šè¿‡ä¸ºæ¯ä¸ªå®ä¾‹ç”Ÿæˆæ–‡æœ¬åé¦ˆï¼Œå¹¶è¯†åˆ«ã€é‡åŒ–ç³»ç»Ÿçº§çš„é”™è¯¯é—®é¢˜ï¼Œå®ç°äº†å¯¹æ¨¡å‹æ€§èƒ½ç¼ºé™·çš„æ·±åº¦å‰–æã€‚CLEAR è¿˜é…å¤‡äº†åŠŸèƒ½ä¸°å¯Œçš„äº¤äº’å¼ä»ªè¡¨ç›˜ï¼Œæ”¯æŒç”¨æˆ·åˆ©ç”¨èšåˆå¯è§†åŒ–ã€äº¤äº’å¼è¿‡æ»¤å™¨å’Œå®ä¾‹é’»å–åŠŸèƒ½ï¼Œç²¾å‡†å®šä½å¹¶åˆ†æç‰¹å®šçš„é”™è¯¯æ¨¡å¼æˆ–åˆ†å€¼åŒºé—´ã€‚é€šè¿‡åœ¨ RAG å’Œ Math ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯åˆ†æä»¥åŠç”¨æˆ·æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜äº† CLEAR åœ¨æä¾›å¯è§£é‡Šã€å¯æ“ä½œçš„é”™è¯¯åˆ†ææ–¹é¢çš„æ˜¾è‘—æ•ˆç”¨ï¼Œä¸ºå¼€å‘è€…æ”¹è¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18392v1",
      "published_date": "2025-07-24 13:15:21 UTC",
      "updated_date": "2025-07-24 13:15:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:38:31.382654+00:00"
    },
    {
      "arxiv_id": "2507.18391v1",
      "title": "Revisiting LLM Reasoning via Information Bottleneck",
      "title_zh": "åŸºäºä¿¡æ¯ç“¶é¢ˆçš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å†æ¢",
      "authors": [
        "Shiye Lei",
        "Zhihao Cheng",
        "Kai Jia",
        "Dacheng Tao"
      ],
      "abstract": "Large language models (LLMs) have recently demonstrated remarkable progress in reasoning capabilities through reinforcement learning with verifiable rewards (RLVR). By leveraging simple rule-based rewards, RL effectively incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning trajectories, progressively guiding them toward correct answers. However, existing approaches remain largely heuristic and intuition-driven, limiting the development of principled methodologies. In this paper, we present a theoretical characterization of LLM reasoning grounded in information bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO), a framework that encourages reasoning trajectories to be both informative about the final correct answer and generalizable across diverse prompts. We derive a practical token-level surrogate objective and propose an efficient approximation, resulting in the lightweight IB regularization method. This technique integrates seamlessly into existing RL-based post-training frameworks without additional computational overhead, requiring only a one-line code modification. Empirically, we validate IB regularization across multiple mathematical reasoning benchmarks and RL algorithms, demonstrating consistent improvements in LLM reasoning performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ (RLVR)ä¸‹çš„æ¨ç†èƒ½åŠ›ï¼Œé’ˆå¯¹ç°æœ‰æ–¹æ³•ç¼ºä¹ç†è®ºåŸåˆ™çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºä¿¡æ¯ç“¶é¢ˆ(Information Bottleneck, IB)åŸåˆ™çš„ç†è®ºè¡¨å¾ã€‚ç ”ç©¶è€…å¼•å…¥äº†IBæ„ŸçŸ¥çš„æ¨ç†ä¼˜åŒ–(IB-aware reasoning optimization, IBRO)æ¡†æ¶ï¼Œæ—¨åœ¨å¼•å¯¼æ¨ç†è·¯å¾„åœ¨ä¿ç•™æ­£ç¡®ç­”æ¡ˆå…³é”®ä¿¡æ¯çš„åŒæ—¶ï¼Œæå‡åœ¨ä¸åŒæç¤ºè¯é—´çš„æ³›åŒ–æ€§èƒ½ã€‚é€šè¿‡æ¨å¯¼å®ç”¨çš„è¯å…ƒçº§ä»£ç†ç›®æ ‡ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è½»é‡çº§çš„IBæ­£åˆ™åŒ–(IB regularization)æŠ€æœ¯ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæµç¨‹ä¸­ï¼Œä»…éœ€ä¸€è¡Œä»£ç ä¿®æ”¹ä¸”ä¸äº§ç”Ÿé¢å¤–è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸Šè¯æ˜ï¼ŒIBæ­£åˆ™åŒ–èƒ½ä¸€è‡´åœ°æå‡LLMsçš„æ¨ç†è¡¨ç°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18391v1",
      "published_date": "2025-07-24 13:14:25 UTC",
      "updated_date": "2025-07-24 13:14:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:38:32.432019+00:00"
    },
    {
      "arxiv_id": "2507.18368v1",
      "title": "Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios",
      "title_zh": "æ´å¯Ÿè¡¨è±¡ä¹‹å¤–ï¼šè¯„ä¼°é‡‘èåœºæ™¯ä¸‹å¤§è¯­è¨€æ¨¡å‹çš„å‘æ•£æ€§ä¸èšåˆæ€§æ€ç»´",
      "authors": [
        "Zhuang Qiang Bok",
        "Watson Wei Khong Chua"
      ],
      "abstract": "Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step logic. In finance, however, professionals must not only converge on optimal decisions but also generate creative, plausible futures under uncertainty. We introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent thinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990 multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we evaluated 14 leading models and uncovered striking differences. Despite high fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models like DeepSeek-R1 and Cohere Command R+ rank among the top for generating actionable, insights suitable for investment decisions. ConDiFi provides a new perspective to assess reasoning capabilities essential to safe and strategic deployment of LLMs in finance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡‘èé¢†åŸŸä¸ä»…éœ€è¦é€»è¾‘å†³ç­–ï¼Œè¿˜éœ€è¦åœ¨ä¸ç¡®å®šæ€§ä¸‹ç”Ÿæˆåˆ›é€ æ€§æ–¹æ¡ˆçš„éœ€æ±‚ï¼Œæå‡ºäº†åä¸º ConDiFi çš„å…¨æ–°è¯„æµ‹åŸºå‡†ã€‚ConDiFi æ—¨åœ¨å…±åŒè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡‘èä»»åŠ¡ä¸­çš„å‘æ•£æ€§æ€ç»´ï¼ˆDivergent Thinkingï¼‰å’Œæ”¶æ•›æ€§æ€ç»´ï¼ˆConvergent Thinkingï¼‰ã€‚è¯¥åŸºå‡†åŒ…å« 607 ä¸ªç”¨äºå‘æ•£æ€§æ¨ç†çš„å®è§‚é‡‘èæç¤ºè¯ï¼Œä»¥åŠ 990 ä¸ªç”¨äºæ”¶æ•›æ€§æ¨ç†çš„å¤šè·³å¯¹æŠ—æ€§å¤šé€‰é¢˜ï¼ˆMCQsï¼‰ã€‚é€šè¿‡å¯¹ 14 ä¸ªä¸»æµæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶å‘ç°å°½ç®¡ GPT-4o è¡¨ç°å‡ºæé«˜çš„æµç•…åº¦ï¼Œä½†åœ¨æ–°é¢–æ€§ï¼ˆNoveltyï¼‰å’Œå¯æ“ä½œæ€§ï¼ˆActionabilityï¼‰æ–¹é¢è¡¨ç°æ¬ ä½³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDeepSeek-R1 å’Œ Cohere Command R+ åœ¨ç”Ÿæˆé€‚ç”¨äºæŠ•èµ„å†³ç­–çš„å¯æ“ä½œæ´å¯Ÿæ–¹é¢ä½å±…å‰åˆ—ã€‚è¯¥åŸºå‡†ä¸ºè¯„ä¼°é‡‘èé¢†åŸŸå®‰å…¨ä¸”æˆ˜ç•¥æ€§éƒ¨ç½² LLMs æ‰€éœ€çš„å…³é”®æ¨ç†èƒ½åŠ›æä¾›äº†å…¨æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by Agentic & GenAI Evaluation KDD2025: KDD workshop on Evaluation and Trustworthiness of Agentic and Generative AI Models https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/",
      "pdf_url": "https://arxiv.org/pdf/2507.18368v1",
      "published_date": "2025-07-24 12:47:29 UTC",
      "updated_date": "2025-07-24 12:47:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:38:37.093826+00:00"
    },
    {
      "arxiv_id": "2507.18365v4",
      "title": "RecPS: Privacy Risk Scoring for Recommender Systems",
      "title_zh": "RecPSï¼šæ¨èç³»ç»Ÿçš„éšç§é£é™©è¯„åˆ†",
      "authors": [
        "Jiajie He",
        "Yuechun Gu",
        "Keke Chen"
      ],
      "abstract": "Recommender systems (RecSys) have become an essential component of many web applications. The core of the system is a recommendation model trained on highly sensitive user-item interaction data. While privacy-enhancing techniques are actively studied in the research community, the real-world model development still depends on minimal privacy protection, e.g., via controlled access. Users of such systems should have the right to choose \\emph{not} to share highly sensitive interactions. However, there is no method allowing the user to know which interactions are more sensitive than others. Thus, quantifying the privacy risk of RecSys training data is a critical step to enabling privacy-aware RecSys model development and deployment. We propose a membership-inference attack (MIA)- based privacy scoring method, RecPS, to measure privacy risks at both the interaction and user levels. The RecPS interaction-level score definition is motivated and derived from differential privacy, which is then extended to the user-level scoring method. A critical component is the interaction-level MIA method RecLiRA, which gives high-quality membership estimation. We have conducted extensive experiments on well-known benchmark datasets and RecSys models to show the unique features and benefits of RecPS scoring in risk assessment and RecSys model unlearning.",
      "tldr_zh": "æ¨èç³»ç»Ÿ (RecSys) è®­ç»ƒæ‰€ä¾èµ–çš„ç”¨æˆ·-ç‰©å“äº¤äº’æ•°æ®å…·æœ‰é«˜åº¦æ•æ„Ÿæ€§ï¼Œä½†ç›®å‰ç¼ºä¹é‡åŒ–ç‰¹å®šäº¤äº’éšç§é£é™©çš„æœ‰æ•ˆæ‰‹æ®µã€‚è¯¥ç ”ç©¶æå‡ºäº† RecPSï¼Œä¸€ç§åŸºäºæˆå‘˜æ¨ç†æ”»å‡» (Membership-Inference Attack, MIA) çš„éšç§è¯„åˆ†æ–¹æ³•ï¼Œæ—¨åœ¨ä»äº¤äº’çº§åˆ«å’Œç”¨æˆ·çº§åˆ«ç²¾å‡†è¡¡é‡éšç§é£é™©ã€‚RecPS çš„äº¤äº’çº§è¯„åˆ†å®šä¹‰å—å·®åˆ†éšç§ (Differential Privacy) å¯å‘å¹¶æ¨å¯¼è€Œæ¥ï¼Œéšåè¢«æ‰©å±•è‡³ç”¨æˆ·çº§è¯„åˆ†ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶æ˜¯åä¸º RecLiRA çš„äº¤äº’çº§ MIA æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„æˆå‘˜èº«ä»½ä¼°è®¡ã€‚åœ¨çŸ¥ååŸºå‡†æ•°æ®é›†å’Œå¤šç§ RecSys æ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRecPS åœ¨é£é™©è¯„ä¼°å’Œæ¨¡å‹å»å­¦ä¹  (Unlearning) æ–¹é¢å…·æœ‰ç‹¬ç‰¹çš„åº”ç”¨ä»·å€¼å’ŒæŠ€æœ¯ä¼˜åŠ¿ï¼Œä¸ºéšç§æ„ŸçŸ¥çš„æ¨èç³»ç»Ÿå¼€å‘ä¸éƒ¨ç½²å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by ACM RecSys 2025; to appear",
      "pdf_url": "https://arxiv.org/pdf/2507.18365v4",
      "published_date": "2025-07-24 12:46:30 UTC",
      "updated_date": "2025-09-06 19:57:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:38:41.028221+00:00"
    },
    {
      "arxiv_id": "2507.18337v2",
      "title": "The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams",
      "title_zh": "AlphaPhysicsï¼šç”¨äºç‰©ç†è€ƒè¯•ä»£æ•°è¡¨è¾¾å¼è‡ªåŠ¨åˆ¤åˆ†çš„é¡¹é‡å†™ç³»ç»Ÿ",
      "authors": [
        "Peter Baumgartner",
        "Lachlan McGinness"
      ],
      "abstract": "We present our method for automatically marking Physics exams. The marking problem consists in assessing typed student answers for correctness with respect to a ground truth solution. This is a challenging problem that we seek to tackle using a combination of a computer algebra system, an SMT solver and a term rewriting system. A Large Language Model is used to interpret and remove errors from student responses and rewrite these in a machine readable format. Once formalized and language-aligned, the next step then consists in applying automated reasoning techniques for assessing student solution correctness. We consider two methods of automated theorem proving: off-the-shelf SMT solving and term rewriting systems tailored for physics problems involving trigonometric expressions. The development of the term rewrite system and establishing termination and confluence properties was not trivial, and we describe it in some detail in the paper. We evaluate our system on a rich pool of over 1500 real-world student exam responses from the 2023 Australian Physics Olympiad.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AlphaPhysics è‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºè¯„ä¼°ç‰©ç†è€ƒè¯•ä¸­å­¦ç”Ÿæäº¤çš„ä»£æ•°è¡¨è¾¾å¼ç­”æ¡ˆã€‚ç³»ç»Ÿé¦–å…ˆåˆ©ç”¨ Large Language Model (LLM) è§£é‡Šå­¦ç”Ÿå“åº”å¹¶ä¿®å¤é”™è¯¯ï¼Œå°†å…¶è½¬åŒ–ä¸ºæœºå™¨å¯è¯»çš„æ ¼å¼ã€‚æ ¸å¿ƒè¯„ä¼°è¿‡ç¨‹ç»“åˆäº† computer algebra systemã€SMT solver ä»¥åŠä¸€ç§ä¸ºå¤„ç†ç‰©ç†ä¸‰è§’å‡½æ•°è¡¨è¾¾å¼è€Œå®šåˆ¶çš„ term rewriting system (TRS)ã€‚ç ”ç©¶å›¢é˜Ÿè¯¦ç»†æè¿°äº† TRS çš„å¼€å‘è¿‡ç¨‹ï¼Œå¹¶æˆåŠŸç¡®ç«‹äº†å…¶åœ¨è‡ªåŠ¨åŒ–æ¨ç†ä¸­çš„ termination å’Œ confluence ç‰¹æ€§ã€‚æœ€åï¼Œè¯¥ç³»ç»Ÿåœ¨ 2023 Australian Physics Olympiad çš„ 1500 å¤šä»½çœŸå®å­¦ç”Ÿç­”å·ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œè¯æ˜äº†å…¶åœ¨è‡ªåŠ¨è¯„åˆ†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18337v2",
      "published_date": "2025-07-24 12:08:49 UTC",
      "updated_date": "2025-08-05 05:42:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:38:44.130645+00:00"
    },
    {
      "arxiv_id": "2507.22929v1",
      "title": "EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow",
      "title_zh": "EH-Benchmarkï¼šçœ¼ç§‘å¹»è§‰è¯„æµ‹åŸºå‡†ä¸æ™ºèƒ½ä½“é©±åŠ¨çš„è‡ªé¡¶å‘ä¸‹å¯è¿½æº¯æ¨ç†å·¥ä½œæµ",
      "authors": [
        "Xiaoyu Pan",
        "Yang Bai",
        "Ke Zou",
        "Yang Zhou",
        "Jun Zhou",
        "Huazhu Fu",
        "Yih-Chung Tham",
        "Yong Liu"
      ],
      "abstract": "Medical Large Language Models (MLLMs) play a crucial role in ophthalmic diagnosis, holding significant potential to address vision-threatening diseases. However, their accuracy is constrained by hallucinations stemming from limited ophthalmic knowledge, insufficient visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data, which collectively impede precise lesion detection and disease diagnosis. Furthermore, existing medical benchmarks fail to effectively evaluate various types of hallucinations or provide actionable solutions to mitigate them. To address the above challenges, we introduce EH-Benchmark, a novel ophthalmology benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs' hallucinations based on specific tasks and error types into two primary classes: Visual Understanding and Logical Composition, each comprising multiple subclasses. Given that MLLMs predominantly rely on language-based reasoning rather than visual processing, we propose an agent-centric, three-phase framework, including the Knowledge-Level Retrieval stage, the Task-Level Case Studies stage, and the Result-Level Validation stage. Experimental results show that our multi-agent framework significantly mitigates both types of hallucinations, enhancing accuracy, interpretability, and reliability. Our project is available at https://github.com/ppxy1/EH-Benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨çœ¼ç§‘è¯Šæ–­ä¸­ç”±äºçœ¼ç§‘çŸ¥è¯†æœ‰é™ã€è§†è§‰å®šä½ä¸æ¨ç†èƒ½åŠ›ä¸è¶³è€Œå¼•å‘çš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº† EH-Benchmark è¯„ä¼°åŸºå‡†ã€‚è¯¥åŸºå‡†å°† MLLMs çš„å¹»è§‰æŒ‰ä»»åŠ¡å’Œé”™è¯¯ç±»å‹åˆ†ä¸ºè§†è§‰ç†è§£ (Visual Understanding) å’Œé€»è¾‘æ„æˆ (Logical Composition) ä¸¤å¤§ç±»ï¼Œå¡«è¡¥äº†ç°æœ‰åŒ»ç–—åŸºå‡†åœ¨ç»†åˆ†å¹»è§‰è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚ä¸ºäº†ç¼“è§£è¿™äº›å¹»è§‰ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ä¸ªä»¥æ™ºèƒ½ä½“ä¸ºä¸­å¿ƒçš„ä¸‰é˜¶æ®µå·¥ä½œæµï¼Œæ¶µç›–çŸ¥è¯†çº§æ£€ç´¢ (Knowledge-Level Retrieval)ã€ä»»åŠ¡çº§æ¡ˆä¾‹ç ”ç©¶ (Task-Level Case Studies) å’Œç»“æœçº§éªŒè¯ (Result-Level Validation) é˜¶æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å¤šæ™ºèƒ½ä½“æ¡†æ¶èƒ½æ˜¾è‘—é™ä½ä¸¤ç±»å¹»è§‰çš„å‘ç”Ÿï¼Œæœ‰æ•ˆæå‡äº†çœ¼ç§‘ç—…å˜æ£€æµ‹ä¸ç–¾ç—…è¯Šæ–­çš„å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œå¯é æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "9 figures, 5 tables. submit/6621751",
      "pdf_url": "https://arxiv.org/pdf/2507.22929v1",
      "published_date": "2025-07-24 12:07:36 UTC",
      "updated_date": "2025-07-24 12:07:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:38:58.829201+00:00"
    },
    {
      "arxiv_id": "2507.18334v1",
      "title": "Improving Bird Classification with Primary Color Additives",
      "title_zh": "åˆ©ç”¨ä¸‰åŸè‰²å¢å¼ºæå‡é¸Ÿç±»åˆ†ç±»æ€§èƒ½",
      "authors": [
        "Ezhini Rasendiran R",
        "Chandresh Kumar Maurya"
      ],
      "abstract": "We address the problem of classifying bird species using their song recordings, a challenging task due to environmental noise, overlapping vocalizations, and missing labels. Existing models struggle with low-SNR or multi-species recordings. We hypothesize that birds can be classified by visualizing their pitch pattern, speed, and repetition, collectively called motifs. Deep learning models applied to spectrogram images help, but similar motifs across species cause confusion. To mitigate this, we embed frequency information into spectrograms using primary color additives. This enhances species distinction and improves classification accuracy. Our experiments show that the proposed approach achieves statistically significant gains over models without colorization and surpasses the BirdCLEF 2024 winner, improving F1 by 7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the effectiveness of incorporating frequency information via colorization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¸Ÿç±»é¸£å«å½•éŸ³åˆ†ç±»ä¸­ç”±äºç¯å¢ƒå™ªå£°å’Œé‡å é¸£å”±å¯¼è‡´çš„è¯†åˆ«éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŸè‰²æ·»åŠ å‰‚(primary color additives)çš„æ”¹è¿›æ–¹æ³•ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥å¤„ç†é¢‘è°±å›¾(spectrogram)å›¾åƒï¼Œä½†ä¸åŒç‰©ç§é—´ç›¸ä¼¼çš„éŸ³è°ƒæ¨¡å¼å’Œé€Ÿåº¦ï¼ˆç»Ÿç§°ä¸ºmotifsï¼‰å¸¸å¯¼è‡´åˆ†ç±»æ··æ·†ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½œè€…é€šè¿‡åŸè‰²æ·»åŠ å‰‚å°†å…³é”®çš„é¢‘ç‡ä¿¡æ¯åµŒå…¥é¢‘è°±å›¾ä¸­ï¼Œä»è€Œå¢å¼ºäº†ç‰©ç§é—´çš„ç‰¹å¾åŒºåˆ†åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç»Ÿè®¡å­¦ä¸Šæ˜¾è‘—ä¼˜äºéç€è‰²æ¨¡å‹ï¼Œå¹¶è¶…è¶Šäº†BirdCLEF 2024çš„å† å†›æ¨¡å‹ï¼Œä½¿F1åˆ†æ•°ã€ROC-AUCå’ŒCMAPåˆ†åˆ«æå‡äº†7.3%ã€6.2%å’Œ6.6%ã€‚è¿™ä¸€æˆæœæœ‰åŠ›è¯æ˜äº†é€šè¿‡ç€è‰²æŠ€æœ¯æ•´åˆé¢‘ç‡ä¿¡æ¯åœ¨æå‡å¤æ‚éŸ³é¢‘åˆ†ç±»ä»»åŠ¡å‡†ç¡®ç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages (Accepted to Interspeech 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.18334v1",
      "published_date": "2025-07-24 12:05:17 UTC",
      "updated_date": "2025-07-24 12:05:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:01.336467+00:00"
    },
    {
      "arxiv_id": "2507.18326v1",
      "title": "A Concept for Efficient Scalability of Automated Driving Allowing for Technical, Legal, Cultural, and Ethical Differences",
      "title_zh": "å…¼é¡¾æŠ€æœ¯ã€æ³•å¾‹ã€æ–‡åŒ–ä¸ä¼¦ç†å·®å¼‚çš„è‡ªåŠ¨é©¾é©¶é«˜æ•ˆå¯æ‰©å±•æ€§æ„æƒ³",
      "authors": [
        "Lars Ullrich",
        "Michael Buchholz",
        "Jonathan Petit",
        "Klaus Dietmayer",
        "Knut Graichen"
      ],
      "abstract": "Efficient scalability of automated driving (AD) is key to reducing costs, enhancing safety, conserving resources, and maximizing impact. However, research focuses on specific vehicles and context, while broad deployment requires scalability across various configurations and environments. Differences in vehicle types, sensors, actuators, but also traffic regulations, legal requirements, cultural dynamics, or even ethical paradigms demand high flexibility of data-driven developed capabilities. In this paper, we address the challenge of scalable adaptation of generic capabilities to desired systems and environments. Our concept follows a two-stage fine-tuning process. In the first stage, fine-tuning to the specific environment takes place through a country-specific reward model that serves as an interface between technological adaptations and socio-political requirements. In the second stage, vehicle-specific transfer learning facilitates system adaptation and governs the validation of design decisions. In sum, our concept offers a data-driven process that integrates both technological and socio-political aspects, enabling effective scalability across technical, legal, cultural, and ethical differences.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶(Automated Driving)åœ¨å¤§è§„æ¨¡éƒ¨ç½²ä¸­é¢ä¸´çš„æŠ€æœ¯ã€æ³•å¾‹ã€æ–‡åŒ–åŠä¼¦ç†å·®å¼‚ï¼Œæå‡ºäº†ä¸€ä¸ªæ—¨åœ¨å®ç°é«˜æ•ˆå¯æ‰©å±•æ€§çš„æ¦‚å¿µæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è§£å†³é€šç”¨èƒ½åŠ›å¦‚ä½•å‘ç‰¹å®šè½¦è¾†ç³»ç»Ÿä¸è¿è¡Œç¯å¢ƒè½¬åŒ–çš„é—®é¢˜ï¼Œåº”å¯¹äº†å½“å‰ç ”ç©¶å±€é™äºç‰¹å®šè½¦è¾†å’Œåœºæ™¯çš„æŒ‘æˆ˜ã€‚æ ¸å¿ƒæ–¹æ¡ˆé‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„å¾®è°ƒ(Fine-tuning)è¿‡ç¨‹ï¼Œç¬¬ä¸€é˜¶æ®µåˆ©ç”¨å›½å®¶ç‰¹å®šçš„å¥–åŠ±æ¨¡å‹(Reward Model)å®ç°ç¯å¢ƒé€‚åº”ï¼Œå……å½“æŠ€æœ¯è°ƒæ•´ä¸ç¤¾ä¼šæ”¿æ²»éœ€æ±‚ä¹‹é—´çš„å…³é”®æ¥å£ã€‚ç¬¬äºŒé˜¶æ®µé€šè¿‡è½¦è¾†ç‰¹å®šçš„è¿ç§»å­¦ä¹ (Transfer Learning)ä¿ƒè¿›ç³»ç»Ÿè‡ªé€‚åº”ï¼Œå¹¶å¼•å¯¼è®¾è®¡å†³ç­–çš„éªŒè¯å·¥ä½œã€‚è¿™ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•æœ‰æ•ˆæ•´åˆäº†æŠ€æœ¯ä¸ç¤¾ä¼šæ”¿æ²»å› ç´ ï¼Œç¡®ä¿äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨ä¸åŒé…ç½®å’Œå¤šå…ƒç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ‰©å±•ä¸åˆè§„åº”ç”¨ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted to be published at 2025 28th IEEE International Conference on Intelligent Transportation Systems (ITSC), Gold Coast, Australia, November 18-21, 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18326v1",
      "published_date": "2025-07-24 11:51:55 UTC",
      "updated_date": "2025-07-24 11:51:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:05.715969+00:00"
    },
    {
      "arxiv_id": "2507.18323v2",
      "title": "SemiSegECG: A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation",
      "title_zh": "SemiSegECGï¼šé¢å‘å¿ƒç”µå›¾æ³¢å½¢åˆ»ç”»åŠç›‘ç£è¯­ä¹‰åˆ†å‰²çš„å¤šæ•°æ®é›†åŸºå‡†",
      "authors": [
        "Minje Park",
        "Jeonghwa Lim",
        "Taehyung Yu",
        "Sunghoon Joo"
      ],
      "abstract": "Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform features, is critical for clinical diagnosis. Despite recent advances using deep learning, progress has been limited by the scarcity of publicly available annotated datasets. Semi-supervised learning presents a promising solution by leveraging abundant unlabeled ECG data. In this study, we present SemiSegECG, the first systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG delineation. We curated and unified multiple public datasets, including previously underused sources, to support robust and diverse evaluation. We adopted five representative SemiSeg algorithms from computer vision, implemented them on two different architectures: the convolutional network and the transformer, and evaluated them in two different settings: in-domain and cross-domain. Additionally, we propose ECG-specific training configurations and augmentation strategies and introduce a standardized evaluation framework. Our results show that the transformer outperforms the convolutional network in semi-supervised ECG delineation. We anticipate that SemiSegECG will serve as a foundation for advancing semi-supervised ECG delineation methods and will facilitate further research in this domain.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¿ƒç”µå›¾ï¼ˆECGï¼‰æè®°ä¸­æ ‡æ³¨æ•°æ®åŒ®ä¹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†SemiSegECGï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºECGæè®°åŠç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆSemi-supervised semantic segmentationï¼‰çš„ç³»ç»ŸåŸºå‡†æµ‹è¯•ã€‚ä½œè€…é€šè¿‡æ•´åˆå¹¶ç»Ÿä¸€å¤šä¸ªå…¬å¼€æ•°æ®é›†ï¼Œæ„å»ºäº†ä¸€ä¸ªæ”¯æŒç¨³å¥è¯„ä¼°çš„å¤šæ ·åŒ–æ•°æ®å¹³å°ã€‚ç ”ç©¶é‡‡ç”¨äº†äº”ç§ä»£è¡¨æ€§çš„åŠç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œå¹¶åˆ†åˆ«åœ¨å·ç§¯ç½‘ç»œï¼ˆConvolutional networkï¼‰å’ŒTransformeræ¶æ„ä¸Šè¿›è¡Œäº†å®ç°ã€‚è¯„ä¼°è¿‡ç¨‹æ¶µç›–äº†åŸŸå†…ï¼ˆIn-domainï¼‰å’Œè·¨åŸŸï¼ˆCross-domainï¼‰ä¸¤ç§è®¾ç½®ï¼Œå¹¶å¼•å…¥äº†ä¸“é—¨é’ˆå¯¹ECGç‰¹æ€§çš„è®­ç»ƒé…ç½®ä¸å¢å¼ºç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTransformeræ¶æ„åœ¨åŠç›‘ç£ECGæè®°ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿå·ç§¯ç½‘ç»œã€‚SemiSegECGä¸ä»…å»ºç«‹äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œä¹Ÿä¸ºæœªæ¥åŠç›‘ç£ECGåˆ†ææ–¹æ³•çš„ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CIKM 2025. The code is available at https://github.com/bakqui/semi-seg-ecg",
      "pdf_url": "https://arxiv.org/pdf/2507.18323v2",
      "published_date": "2025-07-24 11:49:46 UTC",
      "updated_date": "2025-08-05 08:06:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:07.107019+00:00"
    },
    {
      "arxiv_id": "2507.18302v1",
      "title": "LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models",
      "title_zh": "LoRA-Leakï¼šé’ˆå¯¹ LoRA å¾®è°ƒè¯­è¨€æ¨¡å‹çš„æˆå‘˜æ¨ç†æ”»å‡»",
      "authors": [
        "Delong Ran",
        "Xinlei He",
        "Tianshuo Cong",
        "Anyu Wang",
        "Qi Li",
        "Xiaoyun Wang"
      ],
      "abstract": "Language Models (LMs) typically adhere to a \"pre-training and fine-tuning\" paradigm, where a universal pre-trained model can be fine-tuned to cater to various specialized domains. Low-Rank Adaptation (LoRA) has gained the most widespread use in LM fine-tuning due to its lightweight computational cost and remarkable performance. Because the proportion of parameters tuned by LoRA is relatively small, there might be a misleading impression that the LoRA fine-tuning data is invulnerable to Membership Inference Attacks (MIAs). However, we identify that utilizing the pre-trained model can induce more information leakage, which is neglected by existing MIAs. Therefore, we introduce LoRA-Leak, a holistic evaluation framework for MIAs against the fine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership inference attacks, including ten existing MIAs, and five improved MIAs that leverage the pre-trained model as a reference. In experiments, we apply LoRA-Leak to three advanced LMs across three popular natural language processing tasks, demonstrating that LoRA-based fine-tuned LMs are still vulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings). We also applied LoRA-Leak to different fine-tuning settings to understand the resulting privacy risks. We further explore four defenses and find that only dropout and excluding specific LM layers during fine-tuning effectively mitigate MIA risks while maintaining utility. We highlight that under the \"pre-training and fine-tuning\" paradigm, the existence of the pre-trained model makes MIA a more severe risk for LoRA-based LMs. We hope that our findings can provide guidance on data privacy protection for specialized LM providers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LoRA-Leakï¼Œä¸€ä¸ªé’ˆå¯¹Low-Rank Adaptation (LoRA)å¾®è°ƒè¯­è¨€æ¨¡å‹çš„æˆå‘˜æ¨ç†æ”»å‡»(Membership Inference Attacks, MIAs)ç»¼åˆè¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶LoRAä»…å¾®è°ƒæå°æ¯”ä¾‹çš„å‚æ•°ï¼Œä½†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºå‚è€ƒå¯ä»¥è¯±å‘è¢«ç°æœ‰ç ”ç©¶å¿½è§†çš„ä¿¡æ¯æ³„éœ²ã€‚LoRA-Leaké›†æˆäº†åäº”ç§æ”»å‡»æ‰‹æ®µï¼ŒåŒ…æ‹¬åç§ç°æœ‰æ”»å‡»å’Œäº”ç§åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹æ”¹è¿›çš„æ–°å‹æ”»å‡»ï¼Œå¹¶åœ¨å¤šç§è¯­è¨€æ¨¡å‹å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒåŸºäºLoRAçš„æ¨¡å‹åœ¨å¾®è°ƒæ—¶ä¾ç„¶é¢ä¸´æ˜¾è‘—çš„éšç§é£é™©ï¼Œåœ¨ä¿å®ˆè®¾ç½®ä¸‹å¯è¾¾åˆ°0.775çš„AUCã€‚ç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨äº†å››ç§é˜²å¾¡æªæ–½ï¼Œå‘ç°Dropoutå’Œåœ¨å¾®è°ƒæ—¶æ’é™¤ç‰¹å®šå±‚èƒ½æœ‰æ•ˆç¼“è§£MIAé£é™©å¹¶å…¼é¡¾æ¨¡å‹å®ç”¨æ€§ã€‚è¿™ä¸€å·¥ä½œå¼ºè°ƒäº†åœ¨â€œé¢„è®­ç»ƒ-å¾®è°ƒâ€èŒƒå¼ä¸‹ï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„å­˜åœ¨æ˜¾è‘—åŠ å‰§äº†éšç§æ³„éœ²å¨èƒï¼Œä¸ºä¸“ä¸šè¯­è¨€æ¨¡å‹æœåŠ¡å•†æä¾›äº†æ•°æ®éšç§ä¿æŠ¤çš„æŒ‡å¯¼å»ºè®®ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2507.18302v1",
      "published_date": "2025-07-24 11:18:27 UTC",
      "updated_date": "2025-07-24 11:18:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:10.435670+00:00"
    },
    {
      "arxiv_id": "2507.18290v1",
      "title": "Foundations for Risk Assessment of AI in Protecting Fundamental Rights",
      "title_zh": "åŸºæœ¬æƒåˆ©ä¿éšœä¸­çš„äººå·¥æ™ºèƒ½é£é™©è¯„ä¼°åŸºç¡€",
      "authors": [
        "Antonino Rotolo",
        "Beatrice Ferrigno",
        "Jose Miguel Angel Garcia Godinez",
        "Claudio Novelli",
        "Giovanni Sartor"
      ],
      "abstract": "This chapter introduces a conceptual framework for qualitative risk assessment of AI, particularly in the context of the EU AI Act. The framework addresses the complexities of legal compliance and fundamental rights protection by itegrating definitional balancing and defeasible reasoning. Definitional balancing employs proportionality analysis to resolve conflicts between competing rights, while defeasible reasoning accommodates the dynamic nature of legal decision-making. Our approach stresses the need for an analysis of AI deployment scenarios and for identifying potential legal violations and multi-layered impacts on fundamental rights. On the basis of this analysis, we provide philosophical foundations for a logical account of AI risk analysis. In particular, we consider the basic building blocks for conceptually grasping the interaction between AI deployment scenarios and fundamental rights, incorporating in defeasible reasoning definitional balancing and arguments about the contextual promotion or demotion of rights. This layered approach allows for more operative models of assessment of both high-risk AI systems and General Purpose AI (GPAI) systems, emphasizing the broader applicability of the latter. Future work aims to develop a formal model and effective algorithms to enhance AI risk assessment, bridging theoretical insights with practical applications to support responsible AI governance.",
      "tldr_zh": "æœ¬ç ”ç©¶ä¸ºä¿æŠ¤åŸºæœ¬æƒåˆ©(Fundamental Rights)æå‡ºäº†ä¸€å¥—äººå·¥æ™ºèƒ½(AI)å®šæ€§é£é™©è¯„ä¼°çš„æ¦‚å¿µæ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆ(EU AI Act)çš„èƒŒæ™¯ä¸‹ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆå®šä¹‰æ€§å¹³è¡¡(Definitional Balancing)ä¸å¯å‡»è´¥æ¨ç†(Defeasible Reasoning)ï¼Œåˆ©ç”¨æ¯”ä¾‹åŸåˆ™åˆ†ææ¥è§£å†³æƒåˆ©å†²çªï¼Œå¹¶é€‚åº”æ³•å¾‹å†³ç­–çš„åŠ¨æ€æ€§è´¨ã€‚è¯¥æ–¹æ³•å¼ºè°ƒå¯¹äººå·¥æ™ºèƒ½éƒ¨ç½²åœºæ™¯çš„æ·±åº¦åˆ†æï¼Œæ—¨åœ¨è¯†åˆ«æ½œåœ¨çš„æ³•å¾‹è¿è§„åŠå…¶å¯¹åŸºæœ¬æƒåˆ©äº§ç”Ÿçš„å¤šå±‚æ¬¡å½±å“ã€‚ç ”ç©¶è¿˜ä¸ºäººå·¥æ™ºèƒ½é£é™©åˆ†æå¥ å®šäº†é€»è¾‘åŸºç¡€ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­çº³å…¥äº†å…³äºæƒåˆ©åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹è¢«ä¿ƒè¿›æˆ–å‡æŸçš„è®ºè¯ã€‚è¿™ç§åˆ†å±‚æ–¹æ³•å¢å¼ºäº†å¯¹é«˜é£é™©äººå·¥æ™ºèƒ½ç³»ç»Ÿ(High-risk AI systems)åŠé€šç”¨äººå·¥æ™ºèƒ½(GPAI)ç³»ç»Ÿçš„æ“ä½œæ€§è¯„ä¼°ï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡è¡”æ¥ç†è®ºæ´å¯Ÿä¸å®é™…åº”ç”¨ï¼Œä¸ºå¼€å‘å½¢å¼åŒ–æ¨¡å‹å’Œé«˜æ•ˆç®—æ³•æä¾›äº†åŸºç¡€ï¼Œè‡´åŠ›äºæ¨åŠ¨è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½æ²»ç†(Responsible AI governance)ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 1 figure. To be published in: The Philosophical Foundations of Information Technology Law. Oxford University Press, Oxford",
      "pdf_url": "https://arxiv.org/pdf/2507.18290v1",
      "published_date": "2025-07-24 10:52:22 UTC",
      "updated_date": "2025-07-24 10:52:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:13.512964+00:00"
    },
    {
      "arxiv_id": "2507.18288v1",
      "title": "TCM-Tongue: A Standardized Tongue Image Dataset with Pathological Annotations for AI-Assisted TCM Diagnosis",
      "title_zh": "TCM-Tongueï¼šé¢å‘ AI è¾…åŠ©ä¸­åŒ»è¯Šæ–­çš„ç—…ç†æ ‡æ³¨æ ‡å‡†åŒ–èˆŒè±¡å›¾åƒæ•°æ®é›†",
      "authors": [
        "Xuebo Jin",
        "Longfei Gao",
        "Anshuo Tong",
        "Zhengyang Chen",
        "Jianlei Kong",
        "Ning Sun",
        "Huijun Ma",
        "Qiang Wang",
        "Yuting Bai",
        "Tingli Su"
      ],
      "abstract": "Traditional Chinese medicine (TCM) tongue diagnosis, while clinically valuable, faces standardization challenges due to subjective interpretation and inconsistent imaging protocols, compounded by the lack of large-scale, annotated datasets for AI development. To address this gap, we present the first specialized dataset for AI-driven TCM tongue diagnosis, comprising 6,719 high-quality images captured under standardized conditions and annotated with 20 pathological symptom categories (averaging 2.54 clinically validated labels per image, all verified by licensed TCM practitioners). The dataset supports multiple annotation formats (COCO, TXT, XML) for broad usability and has been benchmarked using nine deep learning models (YOLOv5/v7/v8 variants, SSD, and MobileNetV2) to demonstrate its utility for AI development. This resource provides a critical foundation for advancing reliable computational tools in TCM, bridging the data shortage that has hindered progress in the field, and facilitating the integration of AI into both research and clinical practice through standardized, high-quality diagnostic data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† TCM-Tongueï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äº AI è¾…åŠ©ä¸­åŒ»èˆŒè¯Šçš„æ ‡å‡†åŒ–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ä¸­åŒ»èˆŒè¯Šä¸­å› ä¸»è§‚è§£é‡Šå’Œæˆåƒä¸ä¸€è‡´å¯¼è‡´çš„æ ‡å‡†åŒ–éš¾é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å« 6,719 å¼ åœ¨æ ‡å‡†åŒ–æ¡ä»¶ä¸‹é‡‡é›†çš„é«˜è´¨é‡å›¾åƒï¼Œå¹¶ç”±æ‰§ä¸šä¸­åŒ»åŒ»å¸ˆæ ‡æ³¨äº† 20 ä¸ªç—…ç†ç—‡çŠ¶ç±»åˆ«ï¼Œæ¯å¼ å›¾åƒå¹³å‡åŒ…å« 2.54 ä¸ªä¸´åºŠéªŒè¯æ ‡ç­¾ã€‚æ•°æ®é›†æ”¯æŒ COCOã€TXT å’Œ XML ç­‰å¤šç§æ ‡æ³¨æ ¼å¼ï¼Œå¹¶åˆ©ç”¨ YOLOv5/v7/v8ã€SSD å’Œ MobileNetV2 ç­‰ 9 ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ (Deep Learning Models) è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†å…¶åœ¨ AI å¼€å‘ä¸­çš„å®ç”¨ä»·å€¼ã€‚ä½œä¸ºæ¨åŠ¨ä¸­åŒ»å¯é è®¡ç®—å·¥å…·çš„å…³é”®èµ„æºï¼Œè¯¥æ•°æ®é›†æœ‰æ•ˆå¼¥è¡¥äº†é•¿æœŸé˜»ç¢è¯¥é¢†åŸŸè¿›å±•çš„æ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚é€šè¿‡æä¾›æ ‡å‡†åŒ–ä¸”ç»è¿‡ä¸´åºŠéªŒè¯çš„é«˜è´¨é‡è¯Šæ–­æ•°æ®ï¼ŒTCM-Tongue ä¸ºäººå·¥æ™ºèƒ½ (AI) åœ¨ä¸­åŒ»ç ”ç©¶ä¸ä¸´åºŠå®è·µä¸­çš„æ·±åº¦èåˆå¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "16 pages, 11 figures, 2 Tables",
      "pdf_url": "https://arxiv.org/pdf/2507.18288v1",
      "published_date": "2025-07-24 10:49:31 UTC",
      "updated_date": "2025-07-24 10:49:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:28.432536+00:00"
    },
    {
      "arxiv_id": "2507.22928v1",
      "title": "How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding",
      "title_zh": "æ€ç»´é“¾å¦‚ä½•æ€è€ƒï¼ŸåŸºäºç¨€ç–è‡ªç¼–ç çš„æ€ç»´é“¾æ¨ç†æœºç†å¯è§£é‡Šæ€§ç ”ç©¶",
      "authors": [
        "Xi Chen",
        "Aske Plaat",
        "Niki van Stein"
      ],
      "abstract": "Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on multi-step tasks, yet whether the generated \"thoughts\" reflect the true internal reasoning process is unresolved. We present the first feature-level causal study of CoT faithfulness. Combining sparse autoencoders with activation patching, we extract monosemantic features from Pythia-70M and Pythia-2.8B while they tackle GSM8K math problems under CoT and plain (noCoT) prompting. Swapping a small set of CoT-reasoning features into a noCoT run raises answer log-probabilities significantly in the 2.8B model, but has no reliable effect in 70M, revealing a clear scale threshold. CoT also leads to significantly higher activation sparsity and feature interpretability scores in the larger model, signalling more modular internal computation. For example, the model's confidence in generating correct answers improves from 1.2 to 4.3. We introduce patch-curves and random-feature patching baselines, showing that useful CoT information is not only present in the top-K patches but widely distributed. Overall, our results indicate that CoT can induce more interpretable internal structures in high-capacity LLMs, validating its role as a structured prompting method.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æç¤ºè¯å¦‚ä½•å½±å“å¤§è¯­è¨€æ¨¡å‹çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ï¼Œå¹¶é¦–æ¬¡åœ¨ç‰¹å¾å±‚é¢å¼€å±•äº†CoTå¿ å®åº¦çš„å› æœç ”ç©¶ã€‚ç ”ç©¶è€…ç»“åˆç¨€ç–è‡ªç¼–ç å™¨(Sparse Autoencoders, SAEs)ä¸æ¿€æ´»ä¿®è¡¥(activation patching)æŠ€æœ¯ï¼Œåœ¨Pythia-70Må’ŒPythia-2.8Bæ¨¡å‹å¤„ç†GSM8Kæ•°å­¦é—®é¢˜æ—¶æå–äº†å•è¯­ä¹‰ç‰¹å¾(monosemantic features)ã€‚å®éªŒè¡¨æ˜ï¼Œå°†CoTæ¨ç†ç‰¹å¾æ¤å…¥æ— CoTè¿è¡Œä¸­èƒ½æ˜¾è‘—æå‡2.8Bæ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œä½†åœ¨70Mæ¨¡å‹ä¸­æ•ˆæœä¸ä½³ï¼Œæ­ç¤ºäº†æ¨ç†èƒ½åŠ›çš„è§„æ¨¡é—¨æ§›(scale threshold)ã€‚æ­¤å¤–ï¼ŒCoTåœ¨å¤§å‹æ¨¡å‹ä¸­è¯±å¯¼äº†æ›´é«˜çš„æ¿€æ´»ç¨€ç–åº¦(activation sparsity)å’Œç‰¹å¾å¯è§£é‡Šæ€§è¯„åˆ†ï¼Œä½¿æ¨¡å‹ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„ç½®ä¿¡åº¦ä»1.2æå‡è‡³4.3ã€‚é€šè¿‡å¼•å…¥è¡¥ä¸æ›²çº¿(patch-curves)ç­‰åŸºå‡†ï¼Œç ”ç©¶è¯æ˜æœ‰æ•ˆçš„CoTä¿¡æ¯åœ¨æ¨¡å‹å†…éƒ¨å‘ˆå¹¿æ³›åˆ†å¸ƒçŠ¶æ€ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶éªŒè¯äº†CoTåœ¨é«˜å®¹é‡LLMsä¸­èƒ½å¤Ÿè¯±å¯¼æ›´å…·æ¨¡å—åŒ–ä¸”å¯è§£é‡Šçš„å†…éƒ¨ç»“æ„ï¼Œä¸ºå…¶ä½œä¸ºç»“æ„åŒ–æç¤ºæ–¹æ³•çš„æœ‰æ•ˆæ€§æä¾›äº†æœºç†å±‚é¢çš„æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.22928v1",
      "published_date": "2025-07-24 10:25:46 UTC",
      "updated_date": "2025-07-24 10:25:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:22.143298+00:00"
    },
    {
      "arxiv_id": "2507.18263v1",
      "title": "Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models",
      "title_zh": "Locate-and-Focusï¼šå¢å¼ºè¯­éŸ³è¯­è¨€æ¨¡å‹ä¸­çš„æœ¯è¯­ç¿»è¯‘",
      "authors": [
        "Suhang Wu",
        "Jialong Tang",
        "Chengyi Yang",
        "Pei Zhang",
        "Baosong Yang",
        "Junhui Li",
        "Junfeng Yao",
        "Min Zhang",
        "Jinsong Su"
      ],
      "abstract": "Direct speech translation (ST) has garnered increasing attention nowadays, yet the accurate translation of terminology within utterances remains a great challenge. In this regard, current studies mainly concentrate on leveraging various translation knowledge into ST models. However, these methods often struggle with interference from irrelevant noise and can not fully utilize the translation knowledge. To address these issues, in this paper, we propose a novel Locate-and-Focus method for terminology translation. It first effectively locates the speech clips containing terminologies within the utterance to construct translation knowledge, minimizing irrelevant information for the ST model. Subsequently, it associates the translation knowledge with the utterance and hypothesis from both audio and textual modalities, allowing the ST model to better focus on translation knowledge during translation. Experimental results across various datasets demonstrate that our method effectively locates terminologies within utterances and enhances the success rate of terminology translation, while maintaining robust general translation performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›´æ¥è¯­éŸ³ç¿»è¯‘ (ST) ä¸­æœ¯è¯­ç¿»è¯‘å‡†ç¡®æ€§é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º Locate-and-Focus çš„æ–°é¢–æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶ä¸­ç¿»è¯‘çŸ¥è¯†åˆ©ç”¨ä¸è¶³åŠæ— å…³å™ªå£°å¹²æ‰°çš„é—®é¢˜ï¼Œé¦–å…ˆé€šè¿‡ç²¾ç¡®å®šä½åŒ…å«æœ¯è¯­çš„è¯­éŸ³ç‰‡æ®µæ¥æ„å»ºç¿»è¯‘çŸ¥è¯†ï¼Œä»è€Œæœ€å¤§ç¨‹åº¦å‡å°‘å¹²æ‰°ä¿¡æ¯ã€‚éšåï¼ŒLocate-and-Focus ä»éŸ³é¢‘å’Œæ–‡æœ¬åŒæ¨¡æ€å‡ºå‘ï¼Œå°†ç¿»è¯‘çŸ¥è¯†ä¸è¯­éŸ³è¾“å…¥åŠç¿»è¯‘å‡è®¾å»ºç«‹å…³è”ï¼Œä½¿æ¨¡å‹åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­èƒ½å¤Ÿæ›´å¥½åœ°èšç„¦äºæœ¯è¯­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ•°æ®é›†ä¸Šå‡èƒ½æœ‰æ•ˆå®šä½æœ¯è¯­å¹¶æ˜¾è‘—æå‡æœ¯è¯­ç¿»è¯‘çš„æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥çš„é€šç”¨ç¿»è¯‘æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18263v1",
      "published_date": "2025-07-24 10:07:59 UTC",
      "updated_date": "2025-07-24 10:07:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:23.734435+00:00"
    },
    {
      "arxiv_id": "2507.18262v4",
      "title": "ReSemAct: Advancing Fine-Grained Robotic Manipulation via Semantic Structuring and Affordance Refinement",
      "title_zh": "ReSemActï¼šé€šè¿‡è¯­ä¹‰ç»“æ„åŒ–ä¸ç¤ºèƒ½ç»†åŒ–æå‡ç»†ç²’åº¦æœºå™¨äººæ“çºµ",
      "authors": [
        "Chenyu Su",
        "Weiwei Shang",
        "Chen Qian",
        "Fei Zhang",
        "Shuang Cong"
      ],
      "abstract": "Fine-grained robotic manipulation requires grounding natural language into appropriate affordance targets. However, most existing methods driven by foundation models often compress rich semantics into oversimplified affordances, preventing exploitation of implicit semantic information. To address these challenges, we present ReSemAct, a novel unified manipulation framework that introduces Semantic Structuring and Affordance Refinement (SSAR), powered by the automated synergistic reasoning between Multimodal Large Language Models (MLLMs) and Vision Foundation Models (VFMs). Specifically, the Semantic Structuring module derives a unified semantic affordance description from natural language and RGB observations, organizing affordance regions, implicit functional intent, and coarse affordance anchors into a structured representation for downstream refinement. Building upon this specification, the Affordance Refinement strategy instantiates two complementary flows that separately specialize geometry and position, yielding fine-grained affordance targets. These refined targets are then encoded as real-time joint-space optimization objectives, enabling reactive and robust manipulation in dynamic environments. Extensive simulation and real-world experiments are conducted in semantically rich household and sparse chemical lab environments. The results demonstrate that ReSemAct performs diverse tasks under zero-shot conditions, showcasing the robustness of SSAR with foundation models in fine-grained manipulation. Code and videos at https://github.com/scy-v/ReSemAct and https://resemact.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ReSemActï¼Œä¸€ä¸ªæ—¨åœ¨æå‡ç²¾ç»†åŒ–æœºå™¨äººæ“çºµèƒ½åŠ›çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰ç»“æ„åŒ–(Semantic Structuring)å’Œèƒ½åŠ›ç²¾ç‚¼(Affordance Refinement)è§£å†³äº†ç°æœ‰åŸºç¡€æ¨¡å‹æ–¹æ³•ä¸­è¯­ä¹‰ä¿¡æ¯è¿‡åº¦ç®€åŒ–çš„æŒ‘æˆ˜ã€‚æ ¸å¿ƒçš„ SSAR æœºåˆ¶åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)ä¸è§†è§‰åŸºç¡€æ¨¡å‹(VFMs)ä¹‹é—´çš„ååŒæ¨ç†ï¼Œå°†è‡ªç„¶è¯­è¨€å’Œ RGB è§‚æµ‹è½¬åŒ–ä¸ºç»Ÿä¸€çš„ç»“æ„åŒ–è¯­ä¹‰è¡¨ç¤ºã€‚å…¶ä¸­è¯­ä¹‰ç»“æ„åŒ–æ¨¡å—è´Ÿè´£æå–æ“ä½œåŒºåŸŸã€åŠŸèƒ½æ„å›¾åŠç²—ç•¥é”šç‚¹ï¼Œè€Œèƒ½åŠ›ç²¾ç‚¼ç­–ç•¥åˆ™é€šè¿‡å‡ ä½•ä¸ä½ç½®çš„äº’è¡¥æµç”Ÿæˆç²¾ç¡®çš„æ“ä½œç›®æ ‡ã€‚è¿™äº›ç›®æ ‡æœ€ç»ˆè¢«ç¼–ç ä¸ºå®æ—¶å…³èŠ‚ç©ºé—´ä¼˜åŒ–ä»»åŠ¡ï¼Œä»è€Œåœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°ç¨³å¥çš„ååº”å¼æ“çºµã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReSemAct åœ¨é›¶æ ·æœ¬(Zero-shot)æ¡ä»¶ä¸‹èƒ½å¤ŸæˆåŠŸå®Œæˆå®¶åº­å’ŒåŒ–å­¦å®éªŒå®¤ä¸­çš„å¤šç§å¤æ‚ä»»åŠ¡ï¼Œå……åˆ†è¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†ç²¾ç»†åŒ–æ“çºµéœ€æ±‚æ—¶çš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Code and videos: https://github.com/scy-v/ReSemAct and https://resemact.github.io",
      "pdf_url": "https://arxiv.org/pdf/2507.18262v4",
      "published_date": "2025-07-24 10:07:31 UTC",
      "updated_date": "2025-12-29 09:41:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:32.704596+00:00"
    },
    {
      "arxiv_id": "2507.18260v1",
      "title": "Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection",
      "title_zh": "åˆ©ç”¨æ‰©æ•£å…ˆéªŒä¸é«˜æ–¯ä¸å¯çŸ¥è¡¨å¾å­¦ä¹ å¢å¼ºçº¢å¤–å¼±å°ç›®æ ‡æ£€æµ‹",
      "authors": [
        "Junyao Li",
        "Yahao Lu",
        "Xingyuan Guo",
        "Xiaoyu Xian",
        "Tiantian Wang",
        "Yukai Shi"
      ],
      "abstract": "Infrared small target detection (ISTD) plays a vital role in numerous practical applications. In pursuit of determining the performance boundaries, researchers employ large and expensive manual-labeling data for representation learning. Nevertheless, this approach renders the state-of-the-art ISTD methods highly fragile in real-world challenges. In this paper, we first study the variation in detection performance across several mainstream methods under various scarcity -- namely, the absence of high-quality infrared data -- that challenge the prevailing theories about practical ISTD. To address this concern, we introduce the Gaussian Agnostic Representation Learning. Specifically, we propose the Gaussian Group Squeezer, leveraging Gaussian sampling and compression for non-uniform quantization. By exploiting a diverse array of training samples, we enhance the resilience of ISTD models against various challenges. Then, we introduce two-stage diffusion models for real-world reconstruction. By aligning quantized signals closely with real-world distributions, we significantly elevate the quality and fidelity of the synthetic samples. Comparative evaluations against state-of-the-art detection methods in various scarcity scenarios demonstrate the efficacy of the proposed approach.",
      "tldr_zh": "çº¢å¤–å°ç›®æ ‡æ£€æµ‹ (Infrared Small Target Detection, ISTD) åœ¨å®é™…åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•é«˜åº¦ä¾èµ–æ˜‚è´µçš„äººå·¥æ ‡æ³¨æ•°æ®ä¸”åœ¨çœŸå®åœºæ™¯ä¸‹è¡¨ç°è„†å¼±ã€‚è¯¥ç ”ç©¶é¦–å…ˆæ¢è®¨äº†åœ¨é«˜è´¨é‡æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹ä¸»æµ ISTD æ–¹æ³•çš„æ€§èƒ½æ³¢åŠ¨ï¼Œå¹¶æ®æ­¤æå‡ºäº†ä¸€ç§é«˜æ–¯ä¸å¯çŸ¥è¡¨ç¤ºå­¦ä¹  (Gaussian Agnostic Representation Learning) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ Gaussian Group Squeezer åˆ©ç”¨é«˜æ–¯é‡‡æ ·å’Œå‹ç¼©æŠ€æœ¯è¿›è¡Œéå‡åŒ€é‡åŒ– (non-uniform quantization)ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤„ç†å¤šæ ·åŒ–è®­ç»ƒæ ·æœ¬æ—¶çš„é²æ£’æ€§ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†ä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹ (two-stage diffusion models) è¿›è¡ŒçœŸå®åœºæ™¯é‡å»ºï¼Œé€šè¿‡å°†é‡åŒ–ä¿¡å·ä¸ç°å®åˆ†å¸ƒç´§å¯†å¯¹é½ï¼Œå¤§å¹…æå‡äº†åˆæˆæ ·æœ¬çš„è´¨é‡ä¸ä¿çœŸåº¦ã€‚å¯¹æ¯”å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ£€æµ‹æ–¹æ³•ï¼Œæœ‰æ•ˆéªŒè¯äº†å…¶åœ¨æå‡çº¢å¤–å°ç›®æ ‡æ£€æµ‹æ€§èƒ½æ–¹é¢çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to Neural Networks. We propose the Gaussian Group Squeezer, leveraging Gaussian sampling and compression with diffusion models for channel-based data augmentation",
      "pdf_url": "https://arxiv.org/pdf/2507.18260v1",
      "published_date": "2025-07-24 10:03:33 UTC",
      "updated_date": "2025-07-24 10:03:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:45.520478+00:00"
    },
    {
      "arxiv_id": "2507.18252v1",
      "title": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning",
      "title_zh": "åŸºäºçœ¼åŠ¨è¿½è¸ªä¸å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„å¤šæ¨¡æ€è¡Œä¸ºæ¨¡å¼åˆ†æ",
      "authors": [
        "Dongyang Guo",
        "Yasmeen Abdrabou",
        "Enkeleda Thaqi",
        "Enkelejda Kasneci"
      ],
      "abstract": "Eye-tracking data reveals valuable insights into users' cognitive states but is difficult to analyze due to its structured, non-linguistic nature. While large language models (LLMs) excel at reasoning over text, they struggle with temporal and numerical data. This paper presents a multimodal human-AI collaborative framework designed to enhance cognitive pattern extraction from eye-tracking signals. The framework includes: (1) a multi-stage pipeline using horizontal and vertical segmentation alongside LLM reasoning to uncover latent gaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert judgment with LLM output to generate trust scores for behavioral interpretations; and (3) a hybrid anomaly detection module combining LSTM-based temporal modeling with LLM-driven semantic analysis. Our results across several LLMs and prompt strategies show improvements in consistency, interpretability, and performance, with up to 50% accuracy in difficulty prediction tasks. This approach offers a scalable, interpretable solution for cognitive modeling and has broad potential in adaptive learning, human-computer interaction, and educational analytics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€äººæœºåä½œæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºä»çœ¼åŠ¨è¿½è¸ª(Eye-tracking)ä¿¡å·ä¸­æå–è®¤çŸ¥æ¨¡å¼çš„èƒ½åŠ›ï¼Œä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†æ—¶åºå’Œæ•°å€¼æ•°æ®æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåŒ…å«ä¸€ä¸ªç»“åˆæ°´å¹³ä¸å‚ç›´åˆ†å‰²åŠLLMæ¨ç†çš„å¤šé˜¶æ®µæµæ°´çº¿ï¼Œç”¨äºæ­ç¤ºæ½œåœ¨çš„æ³¨è§†æ¨¡å¼ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ä¸“å®¶-æ¨¡å‹ååŒè¯„åˆ†æ¨¡å—(Expert-Model Co-Scoring Module)æ¥ç”Ÿæˆè¡Œä¸ºè§£é‡Šçš„ä¿¡ä»»è¯„åˆ†ï¼Œå¹¶è®¾è®¡äº†ç»“åˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)æ—¶åºå»ºæ¨¡ä¸LLMè¯­ä¹‰åˆ†æçš„æ··åˆå¼‚å¸¸æ£€æµ‹æ¨¡å—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸€è‡´æ€§ã€å¯è§£é‡Šæ€§å’Œæ€§èƒ½ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œåœ¨éš¾åº¦é¢„æµ‹ä»»åŠ¡ä¸­å‡†ç¡®ç‡è¾¾åˆ°50%ã€‚è¿™ä¸€æ–¹æ¡ˆä¸ºè®¤çŸ¥å»ºæ¨¡æä¾›äº†å¯æ‰©å±•ä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„è·¯å¾„ï¼Œåœ¨è‡ªé€‚åº”å­¦ä¹ ã€äººæœºäº¤äº’(HCI)åŠæ•™è‚²åˆ†æé¢†åŸŸå±•ç°å‡ºå¹¿é˜”çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18252v1",
      "published_date": "2025-07-24 09:49:53 UTC",
      "updated_date": "2025-07-24 09:49:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:54.475603+00:00"
    },
    {
      "arxiv_id": "2507.18243v1",
      "title": "DepthDark: Robust Monocular Depth Estimation for Low-Light Environments",
      "title_zh": "DepthDarkï¼šé¢å‘å¼±å…‰ç¯å¢ƒçš„é²æ£’å•ç›®æ·±åº¦ä¼°è®¡",
      "authors": [
        "Longjian Zeng",
        "Zunjie Zhu",
        "Rongfeng Lu",
        "Ming Lu",
        "Bolun Zheng",
        "Chenggang Yan",
        "Anke Xue"
      ],
      "abstract": "In recent years, foundation models for monocular depth estimation have received increasing attention. Current methods mainly address typical daylight conditions, but their effectiveness notably decreases in low-light environments. There is a lack of robust foundational models for monocular depth estimation specifically designed for low-light scenarios. This largely stems from the absence of large-scale, high-quality paired depth datasets for low-light conditions and the effective parameter-efficient fine-tuning (PEFT) strategy. To address these challenges, we propose DepthDark, a robust foundation model for low-light monocular depth estimation. We first introduce a flare-simulation module and a noise-simulation module to accurately simulate the imaging process under nighttime conditions, producing high-quality paired depth datasets for low-light conditions. Additionally, we present an effective low-light PEFT strategy that utilizes illumination guidance and multiscale feature fusion to enhance the model's capability in low-light environments. Our method achieves state-of-the-art depth estimation performance on the challenging nuScenes-Night and RobotCar-Night datasets, validating its effectiveness using limited training data and computing resources.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•ç›®æ·±åº¦ä¼°è®¡(Monocular Depth Estimation)æ¨¡å‹åœ¨ä½å…‰ç…§ç¯å¢ƒä¸‹è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œä¸”ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡ä½å…‰ç…§æ•°æ®é›†å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)ç­–ç•¥çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸“ä¸ºä½å…‰ç¯å¢ƒè®¾è®¡çš„é²æ£’åŸºç¡€æ¨¡å‹DepthDarkã€‚ç ”ç©¶è€…é€šè¿‡å¼•å…¥å…‰æ™•æ¨¡æ‹Ÿæ¨¡å—(Flare-simulation)å’Œå™ªå£°æ¨¡æ‹Ÿæ¨¡å—(Noise-simulation)æ¥ç²¾ç¡®æ¨¡æ‹Ÿå¤œé—´æˆåƒè¿‡ç¨‹ï¼Œä»è€Œç”Ÿæˆäº†é«˜è´¨é‡çš„é…å¯¹æ·±åº¦æ•°æ®é›†ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸€ç§ç»“åˆå…‰ç…§å¼•å¯¼å’Œå¤šå°ºåº¦ç‰¹å¾èåˆ(Multiscale Feature Fusion)çš„ä½å…‰ç…§PEFTç­–ç•¥ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨æš—å…‰ç¯å¢ƒä¸‹çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDepthDarkåœ¨æŒ‘æˆ˜æ€§çš„nuScenes-Nightå’ŒRobotCar-Nightæ•°æ®é›†ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„(State-of-the-art)æ€§èƒ½è¡¨ç°ã€‚è¯¥æ–¹æ³•åœ¨è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä¾ç„¶éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œä¸ºä½å…‰ç…§åœºæ™¯ä¸‹çš„ç¨³å¥æ·±åº¦ä¼°è®¡æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACM MM 2025 conference",
      "pdf_url": "https://arxiv.org/pdf/2507.18243v1",
      "published_date": "2025-07-24 09:32:53 UTC",
      "updated_date": "2025-07-24 09:32:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:02.927447+00:00"
    },
    {
      "arxiv_id": "2507.18229v2",
      "title": "From Individual Learning to Market Equilibrium: Correcting Structural and Parametric Biases in RL Simulations of Economic Models",
      "title_zh": "ä»ä¸ªä½“å­¦ä¹ åˆ°å¸‚åœºå‡è¡¡ï¼šçº æ­£ç»æµæ¨¡å‹å¼ºåŒ–å­¦ä¹ æ¨¡æ‹Ÿä¸­çš„ç»“æ„æ€§ä¸å‚æ•°åŒ–åå·®",
      "authors": [
        "Ruxin Chen",
        "Zeqiang Zhang"
      ],
      "abstract": "The application of Reinforcement Learning (RL) to economic modeling reveals a fundamental conflict between the assumptions of equilibrium theory and the emergent behavior of learning agents. While canonical economic models assume atomistic agents act as `takers' of aggregate market conditions, a naive single-agent RL simulation incentivizes the agent to become a `manipulator' of its environment. This paper first demonstrates this discrepancy within a search-and-matching model with concave production, showing that a standard RL agent learns a non-equilibrium, monopsonistic policy. Additionally, we identify a parametric bias arising from the mismatch between economic discounting and RL's treatment of intertemporal costs. To address both issues, we propose a calibrated Mean-Field Reinforcement Learning framework that embeds a representative agent in a fixed macroeconomic field and adjusts the cost function to reflect economic opportunity costs. Our iterative algorithm converges to a self-consistent fixed point where the agent's policy aligns with the competitive equilibrium. This approach provides a tractable and theoretically sound methodology for modeling learning agents in economic systems within the broader domain of computational social science.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨ç»æµæ¨¡å‹åº”ç”¨ä¸­ï¼Œä¸ªä½“å­¦ä¹ è¡Œä¸ºä¸å¸‚åœºå‡è¡¡(Market Equilibrium)ç†è®ºä¹‹é—´çš„åŸºæœ¬å†²çªã€‚ç ”ç©¶å‘ç°ï¼Œä¼ ç»Ÿç»æµå­¦å‡è®¾ä¸ªä½“æ˜¯å¸‚åœºæ¡ä»¶çš„è¢«åŠ¨â€œæ¥å—è€…â€ï¼Œè€Œæ ‡å‡†çš„å•æ™ºèƒ½ä½“RLæ¨¡æ‹Ÿå¾€å¾€ä¼šé©±ä½¿æ™ºèƒ½ä½“æˆä¸ºç¯å¢ƒçš„â€œæ“æ§è€…â€ï¼Œä»è€Œåœ¨æœå¯»åŒ¹é…æ¨¡å‹(search-and-matching model)ä¸­äº§ç”Ÿéå‡è¡¡çš„ä¹°æ–¹å„æ–­ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æŒ‡å‡ºRLåœ¨å¤„ç†è·¨æœŸæˆæœ¬æ—¶ä¸ç»æµæŠ˜ç°æ¨¡å‹å­˜åœ¨å‚æ•°åå·®ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ ¡å‡†åçš„å¹³å‡åœºå¼ºåŒ–å­¦ä¹ (Mean-Field Reinforcement Learning)æ¡†æ¶ï¼Œé€šè¿‡å°†ä»£è¡¨æ€§æ™ºèƒ½ä½“åµŒå…¥å›ºå®šçš„å®è§‚ç»æµåœºï¼Œå¹¶è°ƒæ•´æˆæœ¬å‡½æ•°ä»¥åæ˜ ç»æµæœºä¼šæˆæœ¬ã€‚å®éªŒè¯æ˜ï¼Œè¯¥è¿­ä»£ç®—æ³•èƒ½å¤Ÿæ”¶æ•›è‡³ä¸€ä¸ªè‡ªæ´½çš„ä¸åŠ¨ç‚¹ï¼Œä½¿æ™ºèƒ½ä½“çš„ç­–ç•¥ä¸ç«äº‰å‡è¡¡(competitive equilibrium)ä¿æŒä¸€è‡´ã€‚è¿™ä¸€ç ”ç©¶ä¸ºè®¡ç®—ç¤¾ä¼šç§‘å­¦é¢†åŸŸä¸­æ¨¡æ‹Ÿå…·æœ‰å­¦ä¹ èƒ½åŠ›çš„ç»æµä¸»ä½“æä¾›äº†ä¸€ç§ç†è®ºå®Œå¤‡ä¸”é«˜æ•ˆçš„æ–¹æ³•è®ºã€‚",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18229v2",
      "published_date": "2025-07-24 09:21:02 UTC",
      "updated_date": "2025-10-19 08:22:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:39:56.777108+00:00"
    },
    {
      "arxiv_id": "2507.18223v1",
      "title": "GenAI for Automotive Software Development: From Requirements to Wheels",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½èµ‹èƒ½æ±½è½¦è½¯ä»¶å¼€å‘ï¼šä»éœ€æ±‚åˆ°å®è½¦",
      "authors": [
        "Nenad Petrovic",
        "Fengjunjie Pan",
        "Vahid Zolfaghari",
        "Krzysztof Lebioda",
        "Andre Schamschurko",
        "Alois Knoll"
      ],
      "abstract": "This paper introduces a GenAI-empowered approach to automated development of automotive software, with emphasis on autonomous and Advanced Driver Assistance Systems (ADAS) capabilities. The process starts with requirements as input, while the main generated outputs are test scenario code for simulation environment, together with implementation of desired ADAS capabilities targeting hardware platform of the vehicle connected to testbench. Moreover, we introduce additional steps for requirements consistency checking leveraging Model-Driven Engineering (MDE). In the proposed workflow, Large Language Models (LLMs) are used for model-based summarization of requirements (Ecore metamodel, XMI model instance and OCL constraint creation), test scenario generation, simulation code (Python) and target platform code generation (C++). Additionally, Retrieval Augmented Generation (RAG) is adopted to enhance test scenario generation from autonomous driving regulations-related documents. Our approach aims shorter compliance and re-engineering cycles, as well as reduced development and testing time when it comes to ADAS-related capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§èµ‹èƒ½äºæ±½è½¦è½¯ä»¶è‡ªåŠ¨åŒ–å¼€å‘çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(GenAI)æ–¹æ³•ï¼Œä¸»è¦é’ˆå¯¹è‡ªåŠ¨é©¾é©¶å’Œé«˜çº§é©¾é©¶è¾…åŠ©ç³»ç»Ÿ(ADAS)çš„å¼€å‘æµç¨‹ã€‚è¯¥æ–¹æ³•ä»¥éœ€æ±‚ä¸ºèµ·ç‚¹ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ç»“åˆæ¨¡å‹é©±åŠ¨å·¥ç¨‹(MDE)æŠ€æœ¯è¿›è¡Œéœ€æ±‚ä¸€è‡´æ€§æ£€æŸ¥ï¼Œæ¶‰åŠEcore metamodelã€XMIæ¨¡å‹å®ä¾‹åŠOCLçº¦æŸçš„è‡ªåŠ¨åˆ›å»ºã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯æ•´åˆè‡ªåŠ¨é©¾é©¶æ³•è§„æ–‡æ¡£ï¼Œä»¥æå‡æµ‹è¯•åœºæ™¯ç”Ÿæˆçš„ä¸“ä¸šæ€§ã€‚ç³»ç»Ÿæœ€ç»ˆèƒ½è‡ªåŠ¨ç”Ÿæˆç”¨äºæ¨¡æ‹Ÿç¯å¢ƒçš„Pythonä»£ç ä»¥åŠé’ˆå¯¹ç›®æ ‡ç¡¬ä»¶å¹³å°çš„C++å®ç°ä»£ç ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆç¼©çŸ­äº†æ±½è½¦è½¯ä»¶çš„åˆè§„ä¸é‡æ„å‘¨æœŸï¼Œå¹¶æ˜¾è‘—é™ä½äº†ADASç›¸å…³åŠŸèƒ½çš„å¼€å‘ä¸æµ‹è¯•æ—¶é—´ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18223v1",
      "published_date": "2025-07-24 09:17:13 UTC",
      "updated_date": "2025-07-24 09:17:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:16.181806+00:00"
    },
    {
      "arxiv_id": "2507.18219v2",
      "title": "FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting",
      "title_zh": "FedSA-GCLï¼šå…·æœ‰ä¸ªæ€§åŒ–èšåˆä¸èšç±»æ„ŸçŸ¥å¹¿æ’­çš„åŠå¼‚æ­¥è”é‚¦å›¾å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Zhongzheng Yuan",
        "Lianshuai Guo",
        "Xunkai Li",
        "Yinlin Zhu",
        "Wenyu Wang",
        "Meixia Qu"
      ],
      "abstract": "Federated Graph Learning (FGL) is a distributed learning paradigm that enables collaborative training over large-scale subgraphs located on multiple local systems. However, most existing FGL approaches rely on synchronous communication, which leads to inefficiencies and is often impractical in real-world deployments. Meanwhile, current asynchronous federated learning (AFL) methods are primarily designed for conventional tasks such as image classification and natural language processing, without accounting for the unique topological properties of graph data. Directly applying these methods to graph learning can possibly result in semantic drift and representational inconsistency in the global model. To address these challenges, we propose FedSA-GCL, a semi-asynchronous federated framework that leverages both inter-client label distribution divergence and graph topological characteristics through a novel ClusterCast mechanism for efficient training. We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain and Metis split algorithms, and compare it against 9 baselines. Extensive experiments demonstrate that our method achieves strong robustness and outstanding efficiency, outperforming the baselines by an average of 2.92% with the Louvain and by 3.4% with the Metis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å›¾å­¦ä¹ (Federated Graph Learning, FGL)åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„åŒæ­¥é€šä¿¡æ•ˆç‡ä½ä¸‹ï¼Œä»¥åŠä¼ ç»Ÿå¼‚æ­¥è”é‚¦å­¦ä¹ åº”ç”¨äºå›¾æ•°æ®æ—¶æ˜“äº§ç”Ÿè¯­ä¹‰æ¼‚ç§»å’Œè¡¨ç¤ºä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæå‡ºäº†FedSA-GCLæ¡†æ¶ã€‚è¿™æ˜¯ä¸€ç§åŠå¼‚æ­¥è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åˆ›æ–°çš„ClusterCastæœºåˆ¶å……åˆ†åˆ©ç”¨äº†å®¢æˆ·ç«¯é—´çš„æ ‡ç­¾åˆ†å¸ƒå·®å¼‚å’Œå›¾æ‹“æ‰‘ç‰¹å¾ï¼Œå®ç°äº†ä¸ªæ€§åŒ–èšåˆä¸é›†ç¾¤æ„ŸçŸ¥çš„å¹¿æ’­ã€‚ç ”ç©¶äººå‘˜åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œå›¾æ•°æ®é›†ä¸Šåˆ©ç”¨Louvainå’ŒMetisåˆ’åˆ†ç®—æ³•å¯¹è¯¥æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸9ä¸ªåŸºçº¿æ¨¡å‹è¿›è¡Œäº†è¯¦å°½å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedSA-GCLåœ¨ä¿æŒå¼ºé²æ£’æ€§çš„åŒæ—¶å±•ç°å‡ºå“è¶Šçš„è®­ç»ƒæ•ˆç‡ï¼Œå…¶åœ¨Louvainå’ŒMetisåˆ’åˆ†ä¸‹çš„å‡†ç¡®ç‡åˆ†åˆ«å¹³å‡ä¼˜äºåŸºçº¿æ¨¡å‹2.92%å’Œ3.4%ã€‚è¯¥ç ”ç©¶ä¸ºè§£å†³å¤§è§„æ¨¡åˆ†å¸ƒå¼å›¾å­¦ä¹ ä¸­çš„é€šä¿¡å»¶è¿Ÿä¸æ•°æ®å¼‚è´¨æ€§æŒ‘æˆ˜æä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„åŠå¼‚æ­¥è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18219v2",
      "published_date": "2025-07-24 09:15:07 UTC",
      "updated_date": "2025-08-05 14:52:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:09.181892+00:00"
    },
    {
      "arxiv_id": "2507.18215v1",
      "title": "Information Security Based on LLM Approaches: A Review",
      "title_zh": "åŸºäº LLM æ–¹æ³•çš„ä¿¡æ¯å®‰å…¨ç ”ç©¶ç»¼è¿°",
      "authors": [
        "Chang Gong",
        "Zhongwen Li",
        "Xiaoqi Li"
      ],
      "abstract": "Information security is facing increasingly severe challenges, and traditional protection means are difficult to cope with complex and changing threats. In recent years, as an emerging intelligent technology, large language models (LLMs) have shown a broad application prospect in the field of information security. In this paper, we focus on the key role of LLM in information security, systematically review its application progress in malicious behavior prediction, network threat analysis, system vulnerability detection, malicious code identification, and cryptographic algorithm optimization, and explore its potential in enhancing security protection performance. Based on neural networks and Transformer architecture, this paper analyzes the technical basis of large language models and their advantages in natural language processing tasks. It is shown that the introduction of large language modeling helps to improve the detection accuracy and reduce the false alarm rate of security systems. Finally, this paper summarizes the current application results and points out that it still faces challenges in model transparency, interpretability, and scene adaptability, among other issues. It is necessary to explore further the optimization of the model structure and the improvement of the generalization ability to realize a more intelligent and accurate information security protection system.",
      "tldr_zh": "è¯¥ç»¼è¿°æ–‡ç« æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨ä¿¡æ¯å®‰å…¨é¢†åŸŸçš„å…³é”®ä½œç”¨åŠå…¶å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æ–‡ç« åŸºäºç¥ç»ç½‘ç»œå’ŒTransformeræ¶æ„ï¼Œæ·±å…¥åˆ†æäº†LLMåœ¨å¤„ç†è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­çš„æŠ€æœ¯ä¼˜åŠ¿åŠå…¶åœ¨å¢å¼ºå®‰å…¨é˜²æŠ¤æ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶ç³»ç»Ÿåœ°å›é¡¾äº†LLMåœ¨æ¶æ„è¡Œä¸ºé¢„æµ‹ã€ç½‘ç»œå¨èƒåˆ†æã€ç³»ç»Ÿæ¼æ´æ£€æµ‹ã€æ¶æ„ä»£ç è¯†åˆ«ä»¥åŠå¯†ç ç®—æ³•ä¼˜åŒ–ç­‰äº”ä¸ªå…³é”®é¢†åŸŸçš„åº”ç”¨è¿›å±•ã€‚ç»“æœè¡¨æ˜ï¼Œå¼•å…¥LLMæœ‰åŠ©äºæ˜¾è‘—æå‡å®‰å…¨ç³»ç»Ÿçš„æ£€æµ‹å‡†ç¡®ç‡å¹¶æœ‰æ•ˆé™ä½è¯¯æŠ¥ç‡ã€‚è®ºæ–‡æœ€åæŒ‡å‡ºï¼Œå°½ç®¡ç›®å‰å·²å–å¾—ä¸€å®šæˆæœï¼Œä½†LLMåœ¨æ¨¡å‹é€æ˜åº¦ã€å¯è§£é‡Šæ€§å’Œåœºæ™¯é€‚åº”æ€§ç­‰æ–¹é¢ä»é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ã€‚æœªæ¥ç ”ç©¶éœ€è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ç»“æ„å¹¶æå‡æ³›åŒ–èƒ½åŠ›ï¼Œä»¥å®ç°æ›´æ™ºèƒ½ã€æ›´ç²¾å‡†çš„ä¿¡æ¯å®‰å…¨ä¿æŠ¤ä½“ç³»ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18215v1",
      "published_date": "2025-07-24 09:09:36 UTC",
      "updated_date": "2025-07-24 09:09:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:11.789164+00:00"
    },
    {
      "arxiv_id": "2507.18206v2",
      "title": "MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation",
      "title_zh": "MoRPI-PINNï¼šé¢å‘ç§»åŠ¨æœºå™¨äººçº¯æƒ¯æ€§å¯¼èˆªçš„ç‰©ç†ä¿¡æ¯æ¡†æ¶",
      "authors": [
        "Arup Kumar Sahoo",
        "Itzik Klein"
      ],
      "abstract": "A fundamental requirement for full autonomy in mobile robots is accurate navigation even in situations where satellite navigation or cameras are unavailable. In such practical situations, relying only on inertial sensors will result in navigation solution drift due to the sensors' inherent noise and error terms. One of the emerging solutions to mitigate drift is to maneuver the robot in a snake-like slithering motion to increase the inertial signal-to-noise ratio, allowing the regression of the mobile robot position. In this work, we propose MoRPI-PINN as a physics-informed neural network framework for accurate inertial-based mobile robot navigation. By embedding physical laws and constraints into the training process, MoRPI-PINN is capable of providing an accurate and robust navigation solution. Using real-world experiments, we show accuracy improvements of over 85% compared to other approaches. MoRPI-PINN is a lightweight approach that can be implemented even on edge devices and used in any typical mobile robot application.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§»åŠ¨æœºå™¨äººåœ¨å«æ˜Ÿå¯¼èˆªæˆ–ç›¸æœºä¸å¯ç”¨ç¯å¢ƒä¸‹ä»…ä¾èµ–æƒ¯æ€§ä¼ æ„Ÿå™¨å¯¼è‡´çš„å¯¼èˆªæ¼‚ç§»é—®é¢˜ï¼Œæå‡ºäº† MoRPI-PINN è¿™ä¸€ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (Physics-Informed Neural Network) æ¡†æ¶ã€‚é€šè¿‡å°†ç‰©ç†å®šå¾‹å’Œçº¦æŸåµŒå…¥è®­ç»ƒè¿‡ç¨‹ï¼ŒMoRPI-PINN èƒ½å¤Ÿåœ¨çº¯æƒ¯æ€§å¯¼èˆªåœºæ™¯ä¸‹æä¾›å‡†ç¡®ä¸”é²æ£’çš„å®šä½æ–¹æ¡ˆã€‚çœŸå®ä¸–ç•Œå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¯”å…¶ä»–ç°æœ‰æ–¹æ³•çš„ç²¾åº¦æå‡äº† 85% ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒMoRPI-PINN é‡‡ç”¨äº†è½»é‡åŒ– (lightweight) è®¾è®¡ï¼Œä½¿å…¶èƒ½å¤Ÿéƒ¨ç½²åœ¨è¾¹ç¼˜è®¾å¤‡ (edge devices) ä¸Šï¼Œä¸ºå„ç±»ç§»åŠ¨æœºå™¨äººçš„å…¨è‡ªä¸»åŒ–åº”ç”¨æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.18206v2",
      "published_date": "2025-07-24 09:02:13 UTC",
      "updated_date": "2025-09-09 13:34:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:20.379196+00:00"
    },
    {
      "arxiv_id": "2507.18202v1",
      "title": "Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection",
      "title_zh": "åˆ©ç”¨ GMTP ä¿éšœ RAG æµæ°´çº¿å®‰å…¨ï¼šä¸€ç§ç”¨äºä¸­æ¯’æ–‡æ¡£æ£€æµ‹çš„æ¢¯åº¦æ©ç æ ‡è®°æ¦‚ç‡æ–¹æ³•",
      "authors": [
        "San Kim",
        "Jonghwi Kim",
        "Yejin Jeon",
        "Gary Geunbae Lee"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by providing external knowledge for accurate and up-to-date responses. However, this reliance on external sources exposes a security risk, attackers can inject poisoned documents into the knowledge base to steer the generation process toward harmful or misleading outputs. In this paper, we propose Gradient-based Masked Token Probability (GMTP), a novel defense method to detect and filter out adversarially crafted documents. Specifically, GMTP identifies high-impact tokens by examining gradients of the retriever's similarity function. These key tokens are then masked, and their probabilities are checked via a Masked Language Model (MLM). Since injected tokens typically exhibit markedly low masked-token probabilities, this enables GMTP to easily detect malicious documents and achieve high-precision filtering. Experiments demonstrate that GMTP is able to eliminate over 90% of poisoned content while retaining relevant documents, thus maintaining robust retrieval and generation performance across diverse datasets and adversarial settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ç³»ç»Ÿå®¹æ˜“å—åˆ°æ”»å‡»è€…æ³¨å…¥ä¸­æ¯’æ–‡æ¡£(poisoned documents)å¯¼è‡´è¾“å‡ºè¯¯å¯¼æ€§ä¿¡æ¯çš„å®‰å…¨é£é™©ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGMTPçš„é˜²å¾¡æ–¹æ³•ã€‚GMTPçš„å…¨ç§°ä¸ºåŸºäºæ¢¯åº¦çš„æ©ç æ ‡è®°æ¦‚ç‡(Gradient-based Masked Token Probability)æ–¹æ³•ï¼Œæ—¨åœ¨æ£€æµ‹å¹¶è¿‡æ»¤æ‰å¯¹æŠ—æ€§ç”Ÿæˆçš„æ¶æ„æ–‡æ¡£ã€‚è¯¥æ–¹æ³•é€šè¿‡æ£€æŸ¥æ£€ç´¢å™¨ç›¸ä¼¼åº¦å‡½æ•°çš„æ¢¯åº¦æ¥è¯†åˆ«é«˜å½±å“åŠ›çš„æ ‡è®°(high-impact tokens)ï¼Œå¹¶åˆ©ç”¨æ©ç è¯­è¨€æ¨¡å‹(Masked Language Model, MLM)æ£€æµ‹è¿™äº›æ ‡è®°çš„æ©ç æ ‡è®°æ¦‚ç‡(masked-token probabilities)ã€‚ç”±äºæ³¨å…¥çš„æ¶æ„æ ‡è®°é€šå¸¸å…·æœ‰æ˜¾è‘—è¾ƒä½çš„æ¦‚ç‡å€¼ï¼ŒGMTPèƒ½å¤Ÿç²¾å‡†è¯†åˆ«å¹¶è¿‡æ»¤æ¶æ„å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ¶ˆé™¤è¶…è¿‡90%çš„ä¸­æ¯’æ–‡æ¡£ï¼ŒåŒæ—¶ä¿ç•™ç›¸å…³æ–‡æ¡£ï¼Œåœ¨å¤šç§æ•°æ®é›†å’Œå¯¹æŠ—åœºæ™¯ä¸‹æœ‰æ•ˆç»´æŠ¤äº†RAGç³»ç»Ÿçš„æ£€ç´¢ä¸ç”Ÿæˆæ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, accepted to ACL Findings 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18202v1",
      "published_date": "2025-07-24 08:58:41 UTC",
      "updated_date": "2025-07-24 08:58:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:18.779399+00:00"
    },
    {
      "arxiv_id": "2507.18198v1",
      "title": "Comparing Non-minimal Semantics for Disjunction in Answer Set Programming",
      "title_zh": "å›ç­”é›†ç¨‹åºè®¾è®¡ä¸­æå–éæå°è¯­ä¹‰çš„æ¯”è¾ƒ",
      "authors": [
        "Felicidad Aguado",
        "Pedro Cabalar",
        "Brais MuÃ±iz",
        "Gilberto PÃ©rez",
        "ConcepciÃ³n Vidal"
      ],
      "abstract": "In this paper, we compare four different semantics for disjunction in Answer Set Programming that, unlike stable models, do not adhere to the principle of model minimality. Two of these approaches, Cabalar and MuÃ±iz' \\emph{Justified Models} and Doherty and Szalas' \\emph{Strongly Supported Models}, directly provide an alternative non-minimal semantics for disjunction. The other two, Aguado et al's \\emph{Forks} and Shen and Eiter's \\emph{Determining Inference} (DI) semantics, actually introduce a new disjunction connective, but are compared here as if they constituted new semantics for the standard disjunction operator. We are able to prove that three of these approaches (Forks, Justified Models and a reasonable relaxation of the DI semantics) actually coincide, constituting a common single approach under different definitions. Moreover, this common semantics always provides a superset of the stable models of a program (in fact, modulo any context) and is strictly stronger than the fourth approach (Strongly Supported Models), that actually treats disjunctions as in classical logic.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¯”è¾ƒäº† Answer Set Programming (ASP) ä¸­ä¸éµå¾ªæ¨¡å‹æœ€å°åŒ–åŸåˆ™çš„å››ç§æå–è¯­ä¹‰ï¼ŒåŒ…æ‹¬ Justified Modelsã€Strongly Supported Modelsã€Forks ä»¥åŠ Determining Inference (DI) è¯­ä¹‰ã€‚ç ”ç©¶è¯æ˜äº† Forksã€Justified Models ä»¥åŠ DI è¯­ä¹‰çš„ä¸€ç§åˆç†æ¾å¼›ç‰ˆæœ¬åœ¨æœ¬è´¨ä¸Šæ˜¯é‡åˆçš„ï¼Œä»è€Œåœ¨ä¸åŒçš„å®šä¹‰ä¸‹æ„å»ºå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ææ¡†æ¶ã€‚è¿™ç§å…±åŒçš„è¯­ä¹‰æ‰€äº§ç”Ÿçš„æ¨¡å‹é›†åˆå§‹ç»ˆæ˜¯ stable models çš„è¶…é›†ï¼Œå¹¶åœ¨ä»»ä½•ä¸Šä¸‹æ–‡ä¸­å‡ä¿æŒè¿™ç§ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç»Ÿä¸€è¯­ä¹‰åœ¨é€»è¾‘å¼ºåº¦ä¸Šæ˜æ˜¾ä¼˜äºå°†æå–å¤„ç†ä¸ºç»å…¸é€»è¾‘çš„ Strongly Supported Modelsã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡æ­ç¤ºä¸åŒéæœ€å°åŒ–è¯­ä¹‰ä¹‹é—´çš„ç­‰ä»·å…³ç³»ï¼Œä¸ºé€»è¾‘ç¼–ç¨‹ä¸­æå–æ“ä½œçš„ç†è®ºç ”ç©¶æä¾›äº†æ–°çš„è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18198v1",
      "published_date": "2025-07-24 08:54:37 UTC",
      "updated_date": "2025-07-24 08:54:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:22.184633+00:00"
    },
    {
      "arxiv_id": "2507.18671v2",
      "title": "Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling",
      "title_zh": "Innovatorï¼šåŸºäºç»†ç²’åº¦ MoE å‡çº§çš„ç§‘å­¦é¢†åŸŸæŒç»­é¢„è®­ç»ƒ",
      "authors": [
        "Ning Liao",
        "Xiaoxing Wang",
        "Zehao Lin",
        "Weiyang Guo",
        "Feng Hong",
        "Shixiang Song",
        "Geng Yu",
        "Zihua Zhao",
        "Sitao Xie",
        "Longxuan Wei",
        "Xiangqi Jin",
        "Xiaohan Qin",
        "Jiale Ma",
        "Kai Chen",
        "Jiangchao Yao",
        "Zhouhan Lin",
        "Junchi Yan",
        "Zhiyu Li",
        "Feiyu Xiong",
        "Yanfeng Wang",
        "Linfeng Zhang"
      ],
      "abstract": "A large language model (LLM) with knowledge in both scientific and general tasks is the foundation of science general intelligence. However, directly continued pretraining an LLM using science data usually leads to catastrophic forgetting, which indicates severe degradation in general ability. In this report, we present Innovator, which solves this problem by upcycling a pre-trained dense LLM into a fine-grained Mixtures-of-Experts model during continued pretraining, where different experts are expected to learn science knowledge in different disciplines, and a shared expert is utilized for general tasks. Innovator introduces a four-stage upcycle training paradigm: (1) Scientific Expert Induction on discipline-specific data, (2) Fine-grained Expert Splitting via FFN dimension decomposition, (3) Science-Aware Routing warmup, and (4) Generalist-Scientist Integration training on hybrid datasets. Such a paradigm enables knowledge in the general domain, and different scientific disciplines can be decoupled, avoiding the negative influence among knowledge in different domains. With 53.3B total parameters and 13.3B activated, Innovator extends Qwen2.5-7B using a shared general expert and 64 specialized scientific experts with 8 activated. Trained on 300B tokens with tri-level quality-controlled data, Innovator achieves 25% average improvement across 30 scientific tasks with a win rate as 70%, while retaining 99% performance in general tasks. Furthermore, Innovator-Reason, which is post-trained from Innovator for reasoning boosting, exhibits excellent reasoning performance in solving complex scientific problems with improvements over 30%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Innovatoræ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç§‘å­¦é¢†åŸŸæŒç»­é¢„è®­ç»ƒæ—¶é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)é—®é¢˜ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å°†é¢„è®­ç»ƒçš„ç¨ å¯†æ¨¡å‹å‘ä¸Šé‡‡æ ·(upcycling)ä¸ºç»†ç²’åº¦çš„æ··åˆä¸“å®¶æ¨¡å‹(Mixture-of-Experts, MoE)ï¼Œåˆ©ç”¨ä¸“é—¨çš„ç§‘å­¦ä¸“å®¶å¤„ç†å­¦ç§‘çŸ¥è¯†ï¼Œå¹¶ç”±å…±äº«ä¸“å®¶è´Ÿè´£é€šç”¨ä»»åŠ¡ã€‚Innovatorå¼•å…¥äº†åŒ…å«ç§‘å­¦ä¸“å®¶è¯±å¯¼ã€ç»†ç²’åº¦ä¸“å®¶æ‹†åˆ†ã€ç§‘å­¦æ„ŸçŸ¥è·¯ç”±çƒ­èº«ä»¥åŠé€šç”¨-ç§‘å­¦å®¶é›†æˆè®­ç»ƒçš„å››é˜¶æ®µèŒƒå¼ï¼Œæœ‰æ•ˆå®ç°äº†ä¸åŒé¢†åŸŸçŸ¥è¯†çš„è§£è€¦ã€‚åŸºäºQwen2.5-7Bæ„å»ºçš„Innovatoræ¨¡å‹åœ¨30é¡¹ç§‘å­¦ä»»åŠ¡ä¸­å¹³å‡æ€§èƒ½æå‡äº†25%ï¼ŒåŒæ—¶ä¿ç•™äº†99%çš„é€šç”¨èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹æ¨ç†å¢å¼ºçš„Innovator-Reasonåœ¨è§£å†³å¤æ‚ç§‘å­¦é—®é¢˜ä¸Šè¡¨ç°å“è¶Šï¼Œå…¶æ€§èƒ½æå‡å¹…åº¦è¶…è¿‡30%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2507.18671v2",
      "published_date": "2025-07-24 08:37:58 UTC",
      "updated_date": "2025-10-16 07:15:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:45.832992+00:00"
    },
    {
      "arxiv_id": "2507.18182v2",
      "title": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models",
      "title_zh": "SCOPEï¼šç”¨äºå¤§è¯­è¨€æ¨¡å‹è¯„ä¼°çš„éšæœºä¸é€†åå·®é€‰é¡¹å¸ƒå±€",
      "authors": [
        "Wonjun Jeong",
        "Dongseok Kim",
        "Taegkeun Whangbo"
      ],
      "abstract": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice tasks by exploiting inherent biases in option positions or labels, rather than demonstrating genuine understanding. This study introduces SCOPE, an evaluation framework designed to measure and mitigate such selection bias in a dataset-independent manner. By repeatedly invoking a null prompt that lacks semantic content, SCOPE estimates each model's unique position-bias distribution. It then redistributes the answer slot according to the inverse-bias distribution, thereby equalizing the lucky-rate, the probability of selecting the correct answer by chance. Furthermore, it prevents semantically similar distractors from being placed adjacent to the answer, thereby blocking near-miss guesses based on superficial proximity cues. Across multiple benchmark experiments, SCOPE consistently outperformed existing debiasing methods in terms of stable performance improvements and showed clearer confidence distributions over correct options. This framework thus offers a new standard for enhancing the fairness and reliability of LLM evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤šé¡¹é€‰æ‹©ä»»åŠ¡ä¸­åˆ©ç”¨é€‰é¡¹ä½ç½®æˆ–æ ‡ç­¾åè§è€ŒéçœŸå®ç†è§£æ¥è·å–è™šé«˜åˆ†æ•°çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º SCOPE çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ä»¥ç‹¬ç«‹äºæ•°æ®é›†çš„æ–¹å¼è¡¡é‡å¹¶å‡è½»æ­¤ç±»é€‰æ‹©åè§ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡å¤è°ƒç”¨ç¼ºä¹è¯­ä¹‰å†…å®¹çš„ null prompt æ¥ä¼°ç®—æ¯ä¸ªæ¨¡å‹ç‰¹æœ‰çš„ position-bias åˆ†å¸ƒï¼Œå¹¶æ®æ­¤åˆ©ç”¨åå‘åè§åˆ†å¸ƒ (inverse-bias distribution) é‡æ–°åˆ†é…ç­”æ¡ˆæ§½ä½ï¼Œä»è€Œå¹³è¡¡å¶ç„¶é€‰ä¸­æ­£ç¡®ç­”æ¡ˆçš„ lucky-rateã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡é˜²æ­¢è¯­ä¹‰ç›¸ä¼¼çš„å¹²æ‰°é¡¹ (distractors) ä¸ç­”æ¡ˆç›¸é‚»ï¼Œé˜»æ–­äº†åŸºäºè¡¨é¢é‚»è¿‘çº¿ç´¢çš„ near-miss çŒœæµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒSCOPE åœ¨æ€§èƒ½æå‡çš„ç¨³å®šæ€§å’Œæ­£ç¡®é€‰é¡¹çš„ confidence distributions æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ debiasing æ–¹æ³•ã€‚è¯¥æ¡†æ¶ä¸ºæå‡ LLM è¯„ä¼°çš„å…¬å¹³æ€§å’Œå¯é æ€§æä¾›äº†æ–°çš„åŸºå‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Comments: 34 pages, 1 figure. v2: All \"Consequence.\" statements in the Theoretical Analysis section relabeled as \"Corollary.\"; duplicated values in Table 20 (previously identical to Table 15) corrected",
      "pdf_url": "https://arxiv.org/pdf/2507.18182v2",
      "published_date": "2025-07-24 08:28:17 UTC",
      "updated_date": "2025-08-04 15:53:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:39.772750+00:00"
    },
    {
      "arxiv_id": "2507.18178v1",
      "title": "Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory",
      "title_zh": "è§£è€¦å¤§è¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†ä¸æ¨ç†ï¼šåŸºäºè®¤çŸ¥åŒç³»ç»Ÿç†è®ºçš„æ¢ç´¢",
      "authors": [
        "Mutian Yang",
        "Jiandong Gao",
        "Ji Wu"
      ],
      "abstract": "While large language models (LLMs) leverage both knowledge and reasoning during inference, the capacity to distinguish between them plays a pivotal role in model analysis, interpretability, and development. Inspired by dual-system cognitive theory, we propose a cognition attribution framework to decouple the contribution of knowledge and reasoning. In particular, the cognition of LLMs is decomposed into two distinct yet complementary phases: knowledge retrieval (Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs are prompted to generate answers under two different cognitive modes, fast thinking and slow thinking, respectively. The performance under different cognitive modes is analyzed to quantify the contribution of knowledge and reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results reveal: (1) reasoning adjustment is domain-specific, benefiting reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and potentially imparing knowledge-intensive domains. (2) Parameter scaling improves both knowledge and reasoning, with knowledge improvements being more pronounced. Additionally, parameter scaling make LLMs reasoning significantly more prudent, while moderately more intelligent. (3) Knowledge primarily resides in lower network layers, while reasoning operates in higher layers. Our framework not only helps understand LLMs from a \"decoupling\" perspective, but also provides new insights into existing research, including scaling laws, hierarchical knowledge editing, and limitations of small-model reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶å—è®¤çŸ¥åŒç³»ç»Ÿç†è®ºå¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªè®¤çŸ¥å½’å› æ¡†æ¶ï¼Œæ—¨åœ¨è§£è€¦å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„çŸ¥è¯†ä¸æ¨ç†è´¡çŒ®ã€‚è¯¥æ¡†æ¶å°† LLMs çš„è®¤çŸ¥è¿‡ç¨‹åˆ†è§£ä¸ºçŸ¥è¯†æ£€ç´¢ï¼ˆKnowledge Retrievalï¼Œé˜¶æ®µ1ï¼‰å’Œæ¨ç†è°ƒèŠ‚ï¼ˆReasoning Adjustmentï¼Œé˜¶æ®µ2ï¼‰ä¸¤ä¸ªé˜¶æ®µï¼Œåˆ†åˆ«å¯¹åº”â€œå¿«é€Ÿæ€è€ƒâ€å’Œâ€œæ…¢é€Ÿæ€è€ƒâ€ä¸¤ç§æ¨¡å¼ã€‚é€šè¿‡å¯¹ 15 ä¸ª LLMs åœ¨ 3 ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒåˆ†æï¼Œç ”ç©¶å‘ç°æ¨ç†è°ƒèŠ‚å…·æœ‰é¢†åŸŸç‰¹å®šæ€§ï¼Œèƒ½æ˜¾è‘—æå‡æ•°å­¦å’Œç‰©ç†ç­‰æ¨ç†å¯†é›†å‹é¢†åŸŸçš„è¡¨ç°ï¼Œä½†å¯èƒ½æŸå®³çŸ¥è¯†å¯†é›†å‹é¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‚æ•°è§„æ¨¡çš„æ‰©å¤§ï¼ˆParameter Scalingï¼‰å¯¹çŸ¥è¯†èƒ½åŠ›çš„æå‡æ¯”æ¨ç†èƒ½åŠ›æ›´ä¸ºæ˜¾è‘—ï¼Œä¸”ä½¿æ¨¡å‹æ¨ç†å˜å¾—æ›´åŠ å®¡æ…ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ­ç¤ºäº†çŸ¥è¯†ä¸»è¦åˆ†å¸ƒåœ¨ç½‘ç»œçš„è¾ƒä½å±‚çº§ï¼Œè€Œæ¨ç†åˆ™è¿ä½œäºè¾ƒé«˜å±‚çº§ã€‚è¯¥æ¡†æ¶ä¸ºç†è§£è§„æ¨¡æ³•åˆ™ï¼ˆScaling Lawsï¼‰ã€åˆ†å±‚çŸ¥è¯†ç¼–è¾‘ä»¥åŠå°æ¨¡å‹æ¨ç†çš„å±€é™æ€§æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18178v1",
      "published_date": "2025-07-24 08:24:52 UTC",
      "updated_date": "2025-07-24 08:24:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:42.473267+00:00"
    },
    {
      "arxiv_id": "2507.18177v2",
      "title": "Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios",
      "title_zh": "Differential-UMambaï¼šæœ‰é™æ•°æ®åœºæ™¯ä¸‹è‚¿ç˜¤åˆ†å‰²çš„é‡æ–°æ€è€ƒ",
      "authors": [
        "Dhruv Jain",
        "Romain Modzelewski",
        "Romain Herault",
        "Clement Chatelain",
        "Eva Torfeh",
        "Sebastien Thureau"
      ],
      "abstract": "In data-scarce scenarios, deep learning models often overfit to noise and irrelevant patterns, which limits their ability to generalize to unseen samples. To address these challenges in medical image segmentation, we introduce Diff-UMamba, a novel architecture that combines the UNet framework with the mamba mechanism to model long-range dependencies. At the heart of Diff-UMamba is a noise reduction module, which employs a signal differencing strategy to suppress noisy or irrelevant activations within the encoder. This encourages the model to filter out spurious features and enhance task-relevant representations, thereby improving its focus on clinically significant regions. As a result, the architecture achieves improved segmentation accuracy and robustness, particularly in low-data settings. Diff-UMamba is evaluated on multiple public datasets, including medical segmentation decathalon dataset (lung and pancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over baseline methods in various segmentation tasks. To further assess performance under limited data conditions, additional experiments are conducted on the BraTS-21 dataset by varying the proportion of available training samples. The approach is also validated on a small internal non-small cell lung cancer dataset for the segmentation of gross tumor volume in cone beam CT, where it achieves a 4-5% improvement over baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å½±åƒåˆ†å‰²ä¸­æ•°æ®ç¨€ç¼ºå¯¼è‡´çš„è¿‡æ‹Ÿåˆå’Œå™ªå£°å¹²æ‰°é—®é¢˜ï¼Œæå‡ºäº†Diff-UMambaæ¶æ„ã€‚è¯¥æ¶æ„å°†UNetæ¡†æ¶ä¸Mambaæœºåˆ¶ç›¸ç»“åˆï¼Œä»¥æœ‰æ•ˆå»ºæ¨¡é•¿ç¨‹ä¾èµ–(long-range dependencies)ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªé™å™ªæ¨¡å—(noise reduction module)ï¼Œé€šè¿‡ä¿¡å·å·®åˆ†ç­–ç•¥(signal differencing strategy)æŠ‘åˆ¶ç¼–ç å™¨ä¸­çš„æ— å…³æ¿€æ´»ï¼Œä»è€Œå¼ºåŒ–ä»»åŠ¡ç›¸å…³çš„è¡¨å¾å¹¶èšç„¦äºä¸´åºŠæ˜¾è‘—åŒºåŸŸã€‚å®éªŒåœ¨Medical Segmentation Decathlonã€AIIB23åŠBraTS-21ç­‰å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºDiff-UMambaåœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­æ¯”åŸºçº¿æ¨¡å‹æå‡äº†1-3%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å†…éƒ¨éå°ç»†èƒè‚ºç™Œ(NSCLC)æ•°æ®é›†çš„CBCTå¤§ä½“è‚¿ç˜¤ä½“ç§¯(GTV)åˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†4-5%çš„æ€§èƒ½å¢é•¿ï¼Œå……åˆ†éªŒè¯äº†å…¶åœ¨æœ‰é™æ•°æ®åœºæ™¯ä¸‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18177v2",
      "published_date": "2025-07-24 08:23:11 UTC",
      "updated_date": "2025-07-29 11:21:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:47.933262+00:00"
    },
    {
      "arxiv_id": "2507.18171v1",
      "title": "Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models",
      "title_zh": "è¶‹å‘å‡å€¼ï¼šæ–‡æœ¬åµŒå…¥æ¨¡å‹ä¸­ç²˜æ€§è¯å…ƒçš„æ£€æµ‹",
      "authors": [
        "Kexin Chen",
        "Dongxia Wang",
        "Yi Liu",
        "Haonan Zhang",
        "Wenhai Wang"
      ],
      "abstract": "Despite the widespread use of Transformer-based text embedding models in NLP tasks, surprising 'sticky tokens' can undermine the reliability of embeddings. These tokens, when repeatedly inserted into sentences, pull sentence similarity toward a certain value, disrupting the normal distribution of embedding distances and degrading downstream performance. In this paper, we systematically investigate such anomalous tokens, formally defining them and introducing an efficient detection method, Sticky Token Detector (STD), based on sentence and token filtering. Applying STD to 40 checkpoints across 14 model families, we discover a total of 868 sticky tokens. Our analysis reveals that these tokens often originate from special or unused entries in the vocabulary, as well as fragmented subwords from multilingual corpora. Notably, their presence does not strictly correlate with model size or vocabulary size. We further evaluate how sticky tokens affect downstream tasks like clustering and retrieval, observing significant performance drops of up to 50%. Through attention-layer analysis, we show that sticky tokens disproportionately dominate the model's internal representations, raising concerns about tokenization robustness. Our findings show the need for better tokenization strategies and model design to mitigate the impact of sticky tokens in future text embedding applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº† Transformer-based text embedding models ä¸­å­˜åœ¨çš„ \"sticky tokens\" å¼‚å¸¸ç°è±¡ï¼Œè¿™äº›æ ‡è®°ä¼šå¹²æ‰°å¥å­ç›¸ä¼¼åº¦åˆ†å¸ƒå¹¶å‰Šå¼±åµŒå…¥çš„å¯é æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†åä¸º Sticky Token Detector (STD) çš„é«˜æ•ˆæ£€æµ‹æ–¹æ³•ï¼Œå¹¶åœ¨ 14 ä¸ªæ¨¡å‹ç³»åˆ—çš„ 40 ä¸ª checkpoints ä¸­è¯†åˆ«å‡º 868 ä¸ªæ­¤ç±»æ ‡è®°ã€‚ç ”ç©¶å‘ç°è¿™äº›æ ‡è®°ä¸»è¦æºè‡ªè¯æ±‡è¡¨ä¸­çš„ç‰¹æ®Šæ¡ç›®æˆ–å¤šè¯­è¨€è¯­æ–™åº“çš„ç¢ç‰‡åŒ– subwordsï¼Œå…¶å­˜åœ¨ä¸æ¨¡å‹è§„æ¨¡å¹¶æ— ç›´æ¥å…³è”ã€‚å®éªŒè¡¨æ˜ï¼Œsticky tokens ä¼šå¯¼è‡´ clustering å’Œ retrieval ç­‰ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸‹é™é«˜è¾¾ 50%ã€‚é€šè¿‡ attention-layer åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†è¿™äº›æ ‡è®°åœ¨æ¨¡å‹å†…éƒ¨è¡¨ç¤ºä¸­å æ®äº†ä¸æˆæ¯”ä¾‹çš„ä¸»å¯¼åœ°ä½ã€‚è¯¥å‘ç°æç¤ºåœ¨æœªæ¥çš„ text embedding å¼€å‘ä¸­ï¼Œå¿…é¡»é€šè¿‡ä¼˜åŒ– tokenization ç­–ç•¥å’Œæ¨¡å‹æ¶æ„æ¥å¢å¼ºç³»ç»Ÿçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 main",
      "pdf_url": "https://arxiv.org/pdf/2507.18171v1",
      "published_date": "2025-07-24 08:13:16 UTC",
      "updated_date": "2025-07-24 08:13:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:48.827455+00:00"
    },
    {
      "arxiv_id": "2507.18153v2",
      "title": "When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label",
      "title_zh": "å½“å™ªå£°æ ‡ç­¾é‡ä¸Šå›¾ç±»åˆ«ä¸å¹³è¡¡ï¼šä¸€ç§ç»“åˆ LLM ä¸ä¼ªæ ‡ç­¾çš„å›¾å¢å¼ºæ–¹æ³•",
      "authors": [
        "Riting Xia",
        "Rucong Wang",
        "Yulin Liu",
        "Anchen Li",
        "Xueyan Liu",
        "Yan Zhang"
      ],
      "abstract": "Class-imbalanced graph node classification is a practical yet underexplored research problem. Although recent studies have attempted to address this issue, they typically assume clean and reliable labels when processing class-imbalanced graphs. This assumption often violates the nature of real-world graphs, where labels frequently contain noise. Given this gap, this paper systematically investigates robust node classification for class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph Augmentation framework based on Large language models (LLMs) and Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling method to generate synthetic minority nodes, producing label-accurate minority nodes to alleviate class imbalance. Based on the class-balanced graphs, we develop a dynamically weighted pseudo-labeling method to obtain high-confidence pseudo labels to reduce label noise ratio. Additionally, we implement a secondary LLM-guided oversampling mechanism to mitigate potential class distribution skew caused by pseudo labels. Experimental results show that GraphALP achieves superior performance over state-of-the-art methods on class-imbalanced graphs with noisy labels.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾èŠ‚ç‚¹åˆ†ç±»ä¸­åŒæ—¶é¢ä¸´ç±»ä¸å¹³è¡¡ (Class Imbalance) ä¸å™ªå£°æ ‡ç­¾ (Noisy Labels) çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå½“å‰ç ”ç©¶å¾€å¾€å¿½è§†äº†çœŸå®ä¸–ç•Œå›¾ä¸­æ ‡ç­¾æ™®éå­˜åœ¨çš„å™ªå£°é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† GraphALP æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸ä¼ªæ ‡ç­¾ (Pseudo-labeling) æŠ€æœ¯çš„å›¾å¢å¼ºæ–¹æ³•ã€‚è¯¥æ¡†æ¶é¦–å…ˆè®¾è®¡äº†ä¸€ç§åŸºäº LLM çš„è¿‡é‡‡æ · (Oversampling) æœºåˆ¶æ¥ç”Ÿæˆæ ‡ç­¾å‡†ç¡®çš„å°‘æ•°ç±»åˆæˆèŠ‚ç‚¹ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£ç±»åˆ«ä¸å¹³è¡¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶é€šè¿‡åŠ¨æ€åŠ æƒçš„ä¼ªæ ‡ç­¾æ–¹æ³•æå–é«˜ç½®ä¿¡åº¦æ ‡ç­¾ä»¥é™ä½æ•´ä½“å™ªå£°ç‡ï¼Œå¹¶åˆ©ç”¨äºŒæ¬¡ LLM å¼•å¯¼çš„è¿‡é‡‡æ ·è¿›ä¸€æ­¥ä¿®æ­£å¯èƒ½ç”±ä¼ªæ ‡ç­¾å¼•èµ·çš„åˆ†å¸ƒåå·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphALP åœ¨å¤„ç†å¸¦å™ªå£°çš„ç±»ä¸å¹³è¡¡å›¾æ•°æ®æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18153v2",
      "published_date": "2025-07-24 07:39:07 UTC",
      "updated_date": "2025-07-25 04:04:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:54.683782+00:00"
    },
    {
      "arxiv_id": "2507.18145v2",
      "title": "Logical Characterizations of GNNs with Mean Aggregation",
      "title_zh": "é‡‡ç”¨å‡å€¼èšåˆçš„å›¾ç¥ç»ç½‘ç»œçš„é€»è¾‘åˆ»ç”»",
      "authors": [
        "Moritz SchÃ¶nherr",
        "Carsten Lutz"
      ],
      "abstract": "We study the expressive power of graph neural networks (GNNs) with mean as the aggregation function, with the following results. In the non-uniform setting, such GNNs have exactly the same expressive power as ratio modal logic, which has modal operators expressing that at least a certain ratio of the successors of a vertex satisfies a specified property. In the uniform setting, the expressive power relative to MSO is exactly that of modal logic, and thus identical to the (absolute) expressive power of GNNs with max aggregation. The proof, however, depends on constructions that are not satisfactory from a practical perspective. This leads us to making the natural assumptions that combination functions are continuous and classification functions are thresholds. The resulting class of GNNs with mean aggregation turns out to be much less expressive: relative to MSO and in the uniform setting, it has the same expressive power as alternation-free modal logic. This is in contrast to the expressive power of GNNs with max and sum aggregation, which is not affected by these assumptions.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°åˆ†æäº†é‡‡ç”¨å‡å€¼èšåˆ(Mean Aggregation)çš„å›¾ç¥ç»ç½‘ç»œ(GNNs)çš„è¡¨è¾¾èƒ½åŠ›(Expressive Power)ã€‚åœ¨éå‡åŒ€è®¾å®š(Non-uniform Setting)ä¸‹ï¼Œç ”ç©¶è¯æ˜æ­¤ç±»GNNsä¸æ¯”ä¾‹æ¨¡æ€é€»è¾‘(Ratio Modal Logic)å…·æœ‰ç›¸åŒçš„è¡¨è¾¾èƒ½åŠ›ã€‚åœ¨å‡åŒ€è®¾å®š(Uniform Setting)ä¸­ï¼Œå…¶ç›¸å¯¹äºäºŒé˜¶é€»è¾‘(MSO)çš„è¡¨è¾¾èƒ½åŠ›ç­‰åŒäºæ¨¡æ€é€»è¾‘(Modal Logic)ï¼Œè¿™åœ¨ç»“æœä¸Šä¸æœ€å¤§å€¼èšåˆ(Max Aggregation)ä¸€è‡´ã€‚ç„¶è€Œï¼Œå½“è€ƒè™‘åˆ°ç»„åˆå‡½æ•°(Combination Functions)è¿ç»­ä¸”åˆ†ç±»å‡½æ•°ä¸ºé˜ˆå€¼ç­‰å®é™…åº”ç”¨å‡è®¾æ—¶ï¼Œå‡å€¼èšåˆå‹GNNsçš„è¡¨è¾¾èƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™ï¼Œä»…ç­‰ä»·äºæ— äº¤æ›¿æ¨¡æ€é€»è¾‘(Alternation-free Modal Logic)ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å‡å€¼èšåˆä¸æœ€å¤§å€¼èšåˆæˆ–æ±‚å’Œèšåˆ(Sum Aggregation)åœ¨é€»è¾‘åˆ»ç”»ä¸Šçš„æœ¬è´¨åŒºåˆ«ï¼Œå› ä¸ºåä¸¤è€…çš„è¡¨è¾¾èƒ½åŠ›å¹¶ä¸ä¼šå—åˆ°è¿™äº›å®é™…å‡è®¾çš„å½±å“ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, extended version of paper to appear in AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2507.18145v2",
      "published_date": "2025-07-24 07:21:49 UTC",
      "updated_date": "2025-12-19 14:03:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:40:58.484462+00:00"
    },
    {
      "arxiv_id": "2507.18143v2",
      "title": "HIVMedQA: Benchmarking large language models for HIV medical decision support",
      "title_zh": "HIVMedQAï¼šé¢å‘HIVåŒ»ç–—å†³ç­–æ”¯æŒçš„å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯„",
      "authors": [
        "Gonzalo Cardenal-Antolin",
        "Jacques Fellay",
        "Bashkim Jaha",
        "Roger Kouyos",
        "Niko Beerenwinkel",
        "Diane Duroux"
      ],
      "abstract": "Large language models (LLMs) are emerging as valuable tools to support clinicians in routine decision-making. HIV management is a compelling use case due to its complexity, including diverse treatment options, comorbidities, and adherence challenges. However, integrating LLMs into clinical practice raises concerns about accuracy, potential harm, and clinician acceptance. Despite their promise, AI applications in HIV care remain underexplored, and LLM benchmarking studies are scarce. This study evaluates the current capabilities of LLMs in HIV management, highlighting their strengths and limitations. We introduce HIVMedQA, a benchmark designed to assess open-ended medical question answering in HIV care. The dataset consists of curated, clinically relevant questions developed with input from an infectious disease physician. We evaluated seven general-purpose and three medically specialized LLMs, applying prompt engineering to enhance performance. Our evaluation framework incorporates both lexical similarity and an LLM-as-a-judge approach, extended to better reflect clinical relevance. We assessed performance across key dimensions: question comprehension, reasoning, knowledge recall, bias, potential harm, and factual accuracy. Results show that Gemini 2.5 Pro consistently outperformed other models across most dimensions. Notably, two of the top three models were proprietary. Performance declined as question complexity increased. Medically fine-tuned models did not always outperform general-purpose ones, and larger model size was not a reliable predictor of performance. Reasoning and comprehension were more challenging than factual recall, and cognitive biases such as recency and status quo were observed. These findings underscore the need for targeted development and evaluation to ensure safe, effective LLM integration in clinical care.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹HIVç®¡ç†ä¸­çš„ä¸´åºŠå†³ç­–æ”¯æŒï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†HIVMedQAï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚è‰¾æ»‹ç—…è¯Šç–—é—®é¢˜æ—¶çš„èƒ½åŠ›ã€‚ç ”ç©¶è€…é€‰å–äº†7ä¸ªé€šç”¨æ¨¡å‹å’Œ3ä¸ªåŒ»å­¦ä¸“ä¸šæ¨¡å‹ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹ï¼ˆprompt engineeringï¼‰ä¼˜åŒ–åï¼Œåˆ©ç”¨è¯æ±‡ç›¸ä¼¼åº¦å’ŒLLM-as-a-judgeçš„æ–¹æ³•ï¼Œä»ç†è§£åŠ›ã€æ¨ç†èƒ½åŠ›ã€çŸ¥è¯†å¬å›ã€åè§ã€æ½œåœ¨ä¼¤å®³åŠäº‹å®å‡†ç¡®æ€§ç­‰ç»´åº¦è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGemini 2.5 Proåœ¨å¤šæ•°ç»´åº¦ä¸Šè¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œä¸”è¡¨ç°æ’åå‰ä¸‰çš„æ¨¡å‹ä¸­æœ‰ä¸¤ä¸ªæ˜¯å•†ä¸šé—­æºæ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹çš„æ¨ç†å’Œç†è§£èƒ½åŠ›ç›¸è¾ƒäºäº‹å®å¬å›ï¼ˆfactual recallï¼‰æ›´å…·æŒ‘æˆ˜æ€§ï¼Œä¸”åŒ»å­¦å¾®è°ƒï¼ˆfine-tunedï¼‰æ¨¡å‹çš„æ•ˆæœå¹¶ä¸æ€»æ˜¯ä¼˜äºé€šç”¨æ¨¡å‹ï¼Œæ¨¡å‹è§„æ¨¡ä¹Ÿä¸æ˜¯æ€§èƒ½çš„å¯é é¢„æµ‹æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œéšç€é—®é¢˜å¤æ‚åº¦çš„å¢åŠ ï¼Œæ¨¡å‹è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œå¹¶æš´éœ²å‡ºè¿‘å› æ•ˆåº”ï¼ˆrecency biasï¼‰å’Œç°çŠ¶åå·®ï¼ˆstatus quo biasï¼‰ç­‰è®¤çŸ¥åå·®ã€‚è¯¥é¡¹ç ”ç©¶ç»“æœå¼ºè°ƒäº†ä¸ºç¡®ä¿LLMsåœ¨ä¸´åºŠæŠ¤ç†ä¸­å®‰å…¨æœ‰æ•ˆåº”ç”¨ï¼Œè¿›è¡Œé’ˆå¯¹æ€§å¼€å‘å’Œå¤šç»´åº¦è¯„ä¼°çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18143v2",
      "published_date": "2025-07-24 07:06:30 UTC",
      "updated_date": "2025-07-25 06:40:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:04.881429+00:00"
    },
    {
      "arxiv_id": "2507.18133v1",
      "title": "Deep Learning for Glioblastoma Morpho-pathological Features Identification: A BraTS-Pathology Challenge Solution",
      "title_zh": "èƒ¶è´¨æ¯ç»†èƒç˜¤å½¢æ€ç—…ç†å­¦ç‰¹å¾è¯†åˆ«çš„æ·±åº¦å­¦ä¹ ï¼šBraTS-Pathology æŒ‘æˆ˜èµ›è§£å†³æ–¹æ¡ˆ",
      "authors": [
        "Juexin Zhang",
        "Ying Weng",
        "Ke Chen"
      ],
      "abstract": "Glioblastoma, a highly aggressive brain tumor with diverse molecular and pathological features, poses a diagnostic challenge due to its heterogeneity. Accurate diagnosis and assessment of this heterogeneity are essential for choosing the right treatment and improving patient outcomes. Traditional methods rely on identifying specific features in tissue samples, but deep learning offers a promising approach for improved glioblastoma diagnosis. In this paper, we present our approach to the BraTS-Path Challenge 2024. We leverage a pre-trained model and fine-tune it on the BraTS-Path training dataset. Our model demonstrates poor performance on the challenging BraTS-Path validation set, as rigorously assessed by the Synapse online platform. The model achieves an accuracy of 0.392229, a recall of 0.392229, and a F1-score of 0.392229, indicating a consistent ability to correctly identify instances under the target condition. Notably, our model exhibits perfect specificity of 0.898704, showing an exceptional capacity to correctly classify negative cases. Moreover, a Matthews Correlation Coefficient (MCC) of 0.255267 is calculated, to signify a limited positive correlation between predicted and actual values and highlight our model's overall predictive power. Our solution also achieves the second place during the testing phase.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èƒ¶è´¨æ¯ç»†èƒç˜¤(Glioblastoma)æé«˜çš„å¼‚è´¨æ€§å¯¼è‡´çš„è¯Šæ–­éš¾é¢˜ï¼Œæ¢è®¨äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ (Deep Learning)è¯†åˆ«å½¢æ€ç—…ç†å­¦ç‰¹å¾çš„æœ‰æ•ˆé€”å¾„ã€‚ä½œè€…æå‡ºäº†é’ˆå¯¹BraTS-Path Challenge 2024æŒ‘æˆ˜èµ›çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åœ¨BraTS-Pathè®­ç»ƒæ•°æ®é›†ä¸Šå¯¹é¢„è®­ç»ƒæ¨¡å‹(Pre-trained model)è¿›è¡Œå¾®è°ƒ(Fine-tune)æ¥æ„å»ºè¯†åˆ«æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§çš„éªŒè¯é›†ä¸Šè¾¾åˆ°äº†0.392229çš„å‡†ç¡®ç‡ã€å¬å›ç‡åŠF1åˆ†æ•°ï¼Œä½“ç°äº†åœ¨å¤„ç†å¤æ‚ç—…ç†ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚ç„¶è€Œï¼Œæ¨¡å‹è¡¨ç°å‡º0.898704çš„é«˜ç‰¹å¼‚æ€§(Specificity)ï¼Œå±•ç°äº†åœ¨æ­£ç¡®åˆ†ç±»é˜´æ€§ç—…ä¾‹æ–¹é¢çš„æ˜¾è‘—èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®¡ç®—å¾—å‡ºçš„é©¬ä¿®æ–¯ç›¸å…³ç³»æ•°(Matthews Correlation Coefficient)ä¸º0.255267ï¼Œåæ˜ äº†é¢„æµ‹ç»“æœä¸å®é™…æƒ…å†µä¹‹é—´çš„æ­£ç›¸å…³å…³ç³»ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨æŒ‘æˆ˜èµ›çš„æµ‹è¯•é˜¶æ®µè£è·ç¬¬äºŒåï¼Œä¸ºæå‡èƒ¶è´¨æ¯ç»†èƒç˜¤çš„è¾…åŠ©è¯Šæ–­ç²¾åº¦æä¾›äº†é‡è¦çš„å®è·µå‚è€ƒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted by the International Brain Tumor Segmentation (BraTS) challenge organized at MICCAI 2024 conference",
      "pdf_url": "https://arxiv.org/pdf/2507.18133v1",
      "published_date": "2025-07-24 06:47:23 UTC",
      "updated_date": "2025-07-24 06:47:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:07.722112+00:00"
    },
    {
      "arxiv_id": "2507.18126v1",
      "title": "U-Net Based Healthy 3D Brain Tissue Inpainting",
      "title_zh": "åŸºäº U-Net çš„ä¸‰ç»´å¥åº·è„‘ç»„ç»‡ä¿®å¤",
      "authors": [
        "Juexin Zhang",
        "Ying Weng",
        "Ke Chen"
      ],
      "abstract": "This paper introduces a novel approach to synthesize healthy 3D brain tissue from masked input images, specifically focusing on the task of 'ASNR-MICCAI BraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a U-Net-based architecture, which is designed to effectively reconstruct the missing or corrupted regions of brain MRI scans. To enhance our model's generalization capabilities and robustness, we implement a comprehensive data augmentation strategy that involves randomly masking healthy images during training. Our model is trained on the BraTS-Local-Inpainting dataset and demonstrates the exceptional performance in recovering healthy brain tissue. The evaluation metrics employed, including Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently yields impressive results. On the BraTS-Local-Inpainting validation set, our model achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score of 0.007. Notably, these evaluation metrics exhibit relatively low standard deviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE score, which indicates that our model's reliability and consistency across various input scenarios. Our method also secured first place in the challenge.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ ASNR-MICCAI BraTS Local Synthesis of Tissue via Inpainting ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§ä»æ©ç å›¾åƒä¸­åˆæˆå¥åº· 3D è„‘ç»„ç»‡çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäº U-Net çš„æ¶æ„ï¼Œæ—¨åœ¨æœ‰æ•ˆé‡å»ºè„‘éƒ¨ MRI æ‰«æä¸­ç¼ºå¤±æˆ–å—æŸçš„åŒºåŸŸã€‚ä¸ºäº†å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œç ”ç©¶å›¢é˜Ÿå®æ–½äº†å…¨é¢çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹å¥åº·å›¾åƒè¿›è¡Œéšæœºæ©ç å¤„ç†ã€‚æ¨¡å‹åœ¨ BraTS-Local-Inpainting æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå±•ç°äº†å‡ºè‰²çš„å¥åº·è„‘ç»„ç»‡æ¢å¤æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šå–å¾—äº† 0.841 çš„ SSIMã€23.257 çš„ PSNR å’Œ 0.007 çš„ MSE è¯„åˆ†ã€‚è¯„ä¼°æŒ‡æ ‡è¾ƒä½çš„æ ‡å‡†å·®è¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ¨¡å‹åœ¨ä¸åŒè¾“å…¥åœºæ™¯ä¸‹çš„å¯é æ€§ä¸ä¸€è‡´æ€§ã€‚å‡­å€Ÿä¼˜å¼‚çš„è¡¨ç°ï¼Œè¯¥æ–¹æ³•åœ¨ç›¸å…³æŒ‘æˆ˜èµ›ä¸­è£è·ç¬¬ä¸€åã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted by the International Brain Tumor Segmentation (BraTS) challenge organized at MICCAI 2024 conference. Included 7 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.18126v1",
      "published_date": "2025-07-24 06:26:46 UTC",
      "updated_date": "2025-07-24 06:26:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:05.716497+00:00"
    },
    {
      "arxiv_id": "2507.18123v1",
      "title": "Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes",
      "title_zh": "ä¸»åŠ¨è¯„ä¼°ä¸å­¦ä¹ å…³é”®å·®å¼‚ï¼šåŸºäºæ€¥è¯Šåˆ†è¯Šè®°å½•çš„ç–«è‹—å®‰å…¨æ€§ä¿¡å·æ£€æµ‹",
      "authors": [
        "Sedigh Khademi",
        "Christopher Palmer",
        "Muhammad Javed",
        "Hazel Clothier",
        "Jim Buttery",
        "Gerardo Luis Dimaguila",
        "Jim Black"
      ],
      "abstract": "The rapid development of COVID-19 vaccines has showcased the global communitys ability to combat infectious diseases. However, the need for post-licensure surveillance systems has grown due to the limited window for safety data collection in clinical trials and early widespread implementation. This study aims to employ Natural Language Processing techniques and Active Learning to rapidly develop a classifier that detects potential vaccine safety issues from emergency department notes. ED triage notes, containing expert, succinct vital patient information at the point of entry to health systems, can significantly contribute to timely vaccine safety signal surveillance. While keyword-based classification can be effective, it may yield false positives and demand extensive keyword modifications. This is exacerbated by the infrequency of vaccination-related ED presentations and their similarity to other reasons for ED visits. NLP offers a more accurate and efficient alternative, albeit requiring annotated data, which is often scarce in the medical field. Active learning optimizes the annotation process and the quality of annotated data, which can result in faster model implementation and improved model performance. This work combines active learning, data augmentation, and active learning and evaluation techniques to create a classifier that is used to enhance vaccine safety surveillance from ED triage notes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–°å† ç–«è‹—ä¸Šå¸‚åå®‰å…¨ç›‘æµ‹çš„éœ€æ±‚ï¼Œæ—¨åœ¨åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å’Œä¸»åŠ¨å­¦ä¹ (Active Learning)æŠ€æœ¯ï¼Œä»æ€¥è¯Šç§‘(Emergency Department)åˆ†è¯Šè®°å½•ä¸­å¿«é€Ÿå¼€å‘ç–«è‹—å®‰å…¨æ€§æ£€æµ‹åˆ†ç±»å™¨ã€‚é‰´äºæ€¥è¯Šåˆ†è¯Šè®°å½•ä¸­ç–«è‹—ç›¸å…³ç—…ä¾‹ç¨€å°‘ä¸”ææ˜“ä¸æ™®é€šç—…ç—‡æ··æ·†ï¼Œä¼ ç»ŸåŸºäºå…³é”®è¯çš„åˆ†ç±»æ–¹æ³•å­˜åœ¨é«˜è¯¯æŠ¥ç‡å’Œé«˜ç»´æŠ¤æˆæœ¬çš„é—®é¢˜ã€‚ä¸ºè§£å†³åŒ»å­¦é¢†åŸŸæ ‡æ³¨æ•°æ®çŸ­ç¼ºçš„æŒ‘æˆ˜ï¼Œè¯¥å·¥ä½œæ•´åˆäº†ä¸»åŠ¨å­¦ä¹ ã€æ•°æ®å¢å¼º(Data Augmentation)ä»¥åŠä¸»åŠ¨è¯„ä¼°æŠ€æœ¯ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†æ•°æ®æ ‡æ³¨æµç¨‹å¹¶æå‡äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„é¢„æµ‹æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§ç»“åˆæ–¹æ³•èƒ½æœ‰æ•ˆä»ç²¾ç®€çš„æ€¥è¯Šè®°å½•ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œæ„å»ºå‡ºé«˜æ•ˆçš„ç–«è‹—å®‰å…¨ä¿¡å·ç›‘æµ‹ç³»ç»Ÿï¼Œä¸ºåŠæ—¶å“åº”å…¬å…±å«ç”Ÿå®‰å…¨æä¾›äº†æ›´ç²¾å‡†çš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.18123v1",
      "published_date": "2025-07-24 06:18:34 UTC",
      "updated_date": "2025-07-24 06:18:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:25.426261+00:00"
    },
    {
      "arxiv_id": "2507.18668v1",
      "title": "Efficient Knowledge Tracing Leveraging Higher-Order Information in Integrated Graphs",
      "title_zh": "èåˆå›¾ä¸­åˆ©ç”¨é«˜é˜¶ä¿¡æ¯çš„é«˜æ•ˆçŸ¥è¯†è¿½è¸ª",
      "authors": [
        "Donghee Han",
        "Daehee Kim",
        "Minjun Lee",
        "Daeyoung Roh",
        "Keejun Han",
        "Mun Yong Yi"
      ],
      "abstract": "The rise of online learning has led to the development of various knowledge tracing (KT) methods. However, existing methods have overlooked the problem of increasing computational cost when utilizing large graphs and long learning sequences. To address this issue, we introduce Dual Graph Attention-based Knowledge Tracing (DGAKT), a graph neural network model designed to leverage high-order information from subgraphs representing student-exercise-KC relationships. DGAKT incorporates a subgraph-based approach to enhance computational efficiency. By processing only relevant subgraphs for each target interaction, DGAKT significantly reduces memory and computational requirements compared to full global graph models. Extensive experimental results demonstrate that DGAKT not only outperforms existing KT models but also sets a new standard in resource efficiency, addressing a critical need that has been largely overlooked by prior KT approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çŸ¥è¯†è¿½è¸ª (Knowledge Tracing, KT) æ–¹æ³•åœ¨å¤§è§„æ¨¡å›¾ç»“æ„å’Œé•¿å­¦ä¹ åºåˆ—ä¸­é¢ä¸´çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†åŒå›¾æ³¨æ„çŸ¥è¯†è¿½è¸ª (Dual Graph Attention-based Knowledge Tracing, DGAKT) æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ˜¯ä¸€ç§å›¾ç¥ç»ç½‘ç»œ (Graph Neural Network, GNN)ï¼Œæ—¨åœ¨åˆ©ç”¨ä»£è¡¨å­¦ç”Ÿ-ç»ƒä¹ -çŸ¥è¯†ç‚¹ (student-exercise-KC) å…³ç³»å­å›¾ä¸­çš„é«˜é˜¶ä¿¡æ¯ã€‚DGAKT é‡‡ç”¨äº†ä¸€ç§åŸºäºå­å›¾çš„æ–¹æ³•æ¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œé€šè¿‡ä»…é’ˆå¯¹æ¯ä¸ªç›®æ ‡äº¤äº’å¤„ç†ç›¸å…³å­å›¾ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆè§£å†³äº†æ­¤å‰ KT æ¨¡å‹ä¸­è¢«å¿½è§†çš„èµ„æºåˆ©ç”¨æ•ˆç‡ç“¶é¢ˆã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDGAKT ä¸ä»…åœ¨é¢„æµ‹å‡†ç¡®åº¦ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ï¼Œè¿˜åœ¨èµ„æºæ•ˆç‡æ–¹é¢æ ‘ç«‹äº†æ–°çš„æ ‡å‡†ï¼Œä¸ºåœ¨çº¿æ•™è‚²åœºæ™¯çš„å¤§è§„æ¨¡åº”ç”¨æä¾›äº†å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18668v1",
      "published_date": "2025-07-24 06:12:43 UTC",
      "updated_date": "2025-07-24 06:12:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:23.108397+00:00"
    },
    {
      "arxiv_id": "2507.18119v2",
      "title": "GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness",
      "title_zh": "GOAT-SLMï¼šå…·å¤‡å‰¯è¯­è¨€ä¸è¯´è¯äººç‰¹å¾æ„ŸçŸ¥èƒ½åŠ›çš„å£è¯­è¯­è¨€æ¨¡å‹",
      "authors": [
        "Hongjie Chen",
        "Zehan Li",
        "Yaodong Song",
        "Wenming Deng",
        "Yitong Yao",
        "Yuxin Zhang",
        "Hang Lv",
        "Xuechao Zhu",
        "Jian Kang",
        "Jie Lian",
        "Jie Li",
        "Chao Wang",
        "Shuangyong Song",
        "Yongxiang Li",
        "Zhongjiang He",
        "Xuelong Li"
      ],
      "abstract": "Recent advances in end-to-end spoken language models (SLMs) have significantly improved the ability of AI systems to engage in natural spoken interactions. However, most existing models treat speech merely as a vehicle for linguistic content, often overlooking the rich paralinguistic and speaker characteristic cues embedded in human speech, such as dialect, age, emotion, and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel spoken language model with paralinguistic and speaker characteristic awareness, designed to extend spoken language modeling beyond text semantics. GOAT-SLM adopts a dual-modality head architecture that decouples linguistic modeling from acoustic realization, enabling robust language understanding while supporting expressive and adaptive speech generation. To enhance model efficiency and versatility, we propose a modular, staged training strategy that progressively aligns linguistic, paralinguistic, and speaker characteristic information using large-scale speech-text corpora. Experimental results on TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM achieves well-balanced performance across both semantic and non-semantic tasks, and outperforms existing open-source models in handling emotion, dialectal variation, and age-sensitive interactions. This work highlights the importance of modeling beyond linguistic content and advances the development of more natural, adaptive, and socially aware spoken language systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GOAT-SLMï¼Œä¸€ç§å…·æœ‰å‰¯è¯­è¨€ï¼ˆparalinguisticï¼‰å’Œè¯´è¯äººç‰¹å¾ï¼ˆspeaker characteristicï¼‰æ„ŸçŸ¥èƒ½åŠ›çš„è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹ä»…å…³æ³¨è¯­è¨€å†…å®¹è€Œå¿½è§†æƒ…æ„Ÿã€æ–¹è¨€å’Œå¹´é¾„ç­‰ä¸°å¯Œçº¿ç´¢çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŒæ¨¡æ€å¤´éƒ¨æ¶æ„ï¼ˆdual-modality head architectureï¼‰ï¼Œé€šè¿‡å°†è¯­è¨€å»ºæ¨¡ä¸å£°å­¦å®ç°è§£è€¦ï¼Œå®ç°äº†ç¨³å¥çš„è¯­è¨€ç†è§£ä¸è¡¨ç°åŠ›ä¸°å¯Œçš„è¯­éŸ³ç”Ÿæˆã€‚ä¸ºäº†æå‡æ•ˆç‡ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–çš„é˜¶æ®µæ€§è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨å¤§è§„æ¨¡è¯­æ–™åº“é€æ­¥å¯¹é½è¯­è¨€ã€å‰¯è¯­è¨€åŠè¯´è¯äººç‰¹å¾ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGOAT-SLMåœ¨TELEVALåŸºå‡†æµ‹è¯•çš„è¯­ä¹‰å’Œéè¯­ä¹‰ä»»åŠ¡ä¸­å‡è¡¨ç°å‡è¡¡ï¼Œåœ¨å¤„ç†æƒ…æ„Ÿã€æ–¹è¨€å˜åŒ–åŠå¹´é¾„æ•æ„Ÿäº¤äº’æ–¹é¢ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡å»ºæ¨¡è¯­è¨€å†…å®¹ä¹‹å¤–çš„ç‰¹å¾ï¼Œæ˜¾è‘—æ¨åŠ¨äº†æ›´è‡ªç„¶ã€å…·å¤‡ç¤¾ä¼šæ„ŸçŸ¥èƒ½åŠ›çš„è¯­éŸ³äº¤äº’ç³»ç»Ÿçš„å‘å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18119v2",
      "published_date": "2025-07-24 06:10:29 UTC",
      "updated_date": "2025-07-25 08:25:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:29.348766+00:00"
    },
    {
      "arxiv_id": "2508.05640v3",
      "title": "Request-Only Optimization for Recommendation Systems",
      "title_zh": "æ¨èç³»ç»Ÿçš„ä»…è¯·æ±‚ä¼˜åŒ–",
      "authors": [
        "Liang Guo",
        "Wei Li",
        "Lucy Liao",
        "Huihui Cheng",
        "Rui Zhang",
        "Yu Shi",
        "Yueming Wang",
        "Yanzun Huang",
        "Keke Zhai",
        "Pengchao Wang",
        "Timothy Shi",
        "Xuan Cao",
        "Shengzhi Wang",
        "Renqin Cai",
        "Zhaojie Gong",
        "Omkar Vichare",
        "Rui Jian",
        "Leon Gao",
        "Shiyan Deng",
        "Xingyu Liu",
        "Xiong Zhang",
        "Fu Li",
        "Wenlei Xie",
        "Bin Wen",
        "Rui Li",
        "Lu Fang",
        "Xing Liu",
        "Jiaqi Zhai"
      ],
      "abstract": "Deep Learning Recommendation Models (DLRMs) represent one of the largest machine learning applications on the planet. Industry-scale DLRMs are trained with petabytes of recommendation data to serve billions of users every day. To utilize the rich user signals in the long user history, DLRMs have been scaled up to unprecedented complexity, up to trillions of floating-point operations (TFLOPs) per example. This scale, coupled with the huge amount of training data, necessitates new storage and training algorithms to efficiently improve the quality of these complex recommendation systems. In this paper, we present a Request-Only Optimizations (ROO) training and modeling paradigm. ROO simultaneously improves the storage and training efficiency as well as the model quality of recommendation systems. We holistically approach this challenge through co-designing data (i.e., request-only data), infrastructure (i.e., request-only based data processing pipeline), and model architecture (i.e., request-only neural architectures). Our ROO training and modeling paradigm treats a user request as a unit of the training data. Compared with the established practice of treating a user impression as a unit, our new design achieves native feature deduplication in data logging, consequently saving data storage. Second, by de-duplicating computations and communications across multiple impressions in a request, this new paradigm enables highly scaled-up neural network architectures to better capture user interest signals, such as Generative Recommenders (GRs) and other request-only friendly architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šçº§æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹ï¼ˆDeep Learning Recommendation Models, DLRMsï¼‰é¢ä¸´çš„æ•°æ®è§„æ¨¡åºå¤§ä¸è®¡ç®—å¤æ‚åº¦æé«˜çš„é—®é¢˜ï¼Œæå‡ºäº† Request-Only Optimizations (ROO) è®­ç»ƒä¸å»ºæ¨¡èŒƒå¼ã€‚ROO èŒƒå¼é€šè¿‡å¯¹æ•°æ®ã€åŸºç¡€è®¾æ–½å’Œæ¨¡å‹æ¶æ„è¿›è¡ŒååŒè®¾è®¡ï¼Œå°†â€œç”¨æˆ·è¯·æ±‚â€ï¼ˆuser requestï¼‰è€Œéä¼ ç»Ÿçš„â€œç”¨æˆ·å±•ç¤ºâ€ï¼ˆuser impressionï¼‰ä½œä¸ºè®­ç»ƒæ•°æ®çš„åŸºæœ¬å•ä½ã€‚è¿™ç§è®¾è®¡åœ¨æ•°æ®è®°å½•é˜¶æ®µå®ç°äº†å¤©ç„¶çš„ç‰¹å¾å»é‡ï¼ˆfeature deduplicationï¼‰ï¼Œä»è€Œæ˜¾è‘—èŠ‚çœäº†æµ·é‡æ•°æ®çš„å­˜å‚¨ç©ºé—´ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨å•æ¬¡è¯·æ±‚çš„å¤šæ¡å±•ç¤ºä¿¡æ¯é—´å®ç°è®¡ç®—ä¸é€šä¿¡çš„å»é‡ï¼Œè¯¥èŒƒå¼æ”¯æŒäº†ç”Ÿæˆå¼æ¨èç³»ç»Ÿï¼ˆGenerative Recommenders, GRsï¼‰ç­‰æ›´ä¸ºå¤æ‚çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œèƒ½å¤Ÿæ›´ç²¾å‡†åœ°æ•æ‰ç”¨æˆ·å…´è¶£ä¿¡å·ã€‚æœ€ç»ˆï¼ŒROO åœ¨åŒæ­¥æå‡æ¨èç³»ç»Ÿå­˜å‚¨ä¸è®­ç»ƒæ•ˆç‡çš„åŒæ—¶ï¼Œä¹Ÿæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹çš„é¢„æµ‹è´¨é‡ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.05640v3",
      "published_date": "2025-07-24 05:56:55 UTC",
      "updated_date": "2025-08-14 23:38:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:30.811110+00:00"
    },
    {
      "arxiv_id": "2507.18115v1",
      "title": "Agentic AI framework for End-to-End Medical Data Inference",
      "title_zh": "é¢å‘ç«¯åˆ°ç«¯åŒ»ç–—æ•°æ®æ¨ç†çš„æ™ºèƒ½ä½“ AI æ¡†æ¶",
      "authors": [
        "Soorya Ram Shimgekar",
        "Shayan Vassef",
        "Abhay Goyal",
        "Navin Kumar",
        "Koustuv Saha"
      ],
      "abstract": "Building and deploying machine learning solutions in healthcare remains expensive and labor-intensive due to fragmented preprocessing workflows, model compatibility issues, and stringent data privacy constraints. In this work, we introduce an Agentic AI framework that automates the entire clinical data pipeline, from ingestion to inference, through a system of modular, task-specific agents. These agents handle both structured and unstructured data, enabling automatic feature selection, model selection, and preprocessing recommendation without manual intervention. We evaluate the system on publicly available datasets from geriatrics, palliative care, and colonoscopy imaging. For example, in the case of structured data (anxiety data) and unstructured data (colonoscopy polyps data), the pipeline begins with file-type detection by the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring privacy compliance, where we first identify the data type and then anonymize it. The Feature Extraction Agent identifies features using an embedding-based approach for tabular data, extracting all column names, and a multi-stage MedGemma-based approach for image data, which infers modality and disease name. These features guide the Model-Data Feature Matcher Agent in selecting the best-fit model from a curated repository. The Preprocessing Recommender Agent and Preprocessing Implementor Agent then apply tailored preprocessing based on data type and model requirements. Finally, the ``Model Inference Agent\" runs the selected model on the uploaded data and generates interpretable outputs using tools like SHAP, LIME, and DETR attention maps. By automating these high-friction stages of the ML lifecycle, the proposed framework reduces the need for repeated expert intervention, offering a scalable, cost-efficient pathway for operationalizing AI in clinical environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—æœºå™¨å­¦ä¹ æ–¹æ¡ˆå¼€å‘ä¸­æµç¨‹ç ´ç¢ã€æ¨¡å‹å…¼å®¹æ€§å·®åŠéšç§å—é™ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºç«¯åˆ°ç«¯åŒ»ç–—æ•°æ®æ¨ç†çš„æ™ºèƒ½ä½“ AI æ¡†æ¶ (Agentic AI framework)ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ç³»åˆ—æ¨¡å—åŒ–çš„ç‰¹å®šä»»åŠ¡æ™ºèƒ½ä½“ (Agents) å®ç°äº†ä»æ•°æ®æ‘„å…¥åˆ°æ¨ç†çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†ç»“æ„åŒ–å’Œéç»“æ„åŒ–åŒ»ç–—æ•°æ®ã€‚ç³»ç»Ÿé›†æˆäº† Ingestion Identifier Agent è¿›è¡Œæ–‡ä»¶æ£€æµ‹ã€Data Anonymizer Agent ç¡®ä¿éšç§åˆè§„ï¼Œå¹¶åˆ©ç”¨åµŒå…¥ (embedding) æŠ€æœ¯æˆ– MedGemma æ¨¡å‹å®Œæˆå¤šæ¨¡æ€ç‰¹å¾æå–ã€‚éšåï¼ŒModel-Data Feature Matcher Agent ä¼šåŒ¹é…æœ€é€‚æ¨¡å‹ï¼Œå¹¶ç”±ä¸“é—¨çš„æ™ºèƒ½ä½“æ‰§è¡Œå®šåˆ¶åŒ–é¢„å¤„ç†ã€‚åœ¨æœ€ç»ˆæ¨ç†é˜¶æ®µï¼Œæ¡†æ¶é€šè¿‡ SHAPã€LIME å’Œ DETR æ³¨æ„åŠ›å›¾æä¾›å¯è§£é‡Šæ€§è¾“å‡ºã€‚åœ¨è€å¹´åŒ»å­¦ã€å§‘æ¯æ²»ç–—å’Œç»“è‚ é•œå½±åƒæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—å‡å°‘äº†å¯¹ä¸“å®¶å¹²é¢„çš„ä¾èµ–ï¼Œä¸ºä¸´åºŠ AI çš„è§„æ¨¡åŒ–åº”ç”¨æä¾›äº†é«˜æˆæœ¬æ•ˆç›Šçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 5 figures, 2 tables, BIBM conference",
      "pdf_url": "https://arxiv.org/pdf/2507.18115v1",
      "published_date": "2025-07-24 05:56:25 UTC",
      "updated_date": "2025-07-24 05:56:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:33.273061+00:00"
    },
    {
      "arxiv_id": "2507.18112v1",
      "title": "Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation Using Tensor Networks",
      "title_zh": "åŸºäºå¼ é‡ç½‘ç»œçš„ MRI å›¾åƒç”Ÿæˆ 3D DDPM å‚æ•°é«˜æ•ˆå¾®è°ƒ",
      "authors": [
        "Binghua Li",
        "Ziqing Chang",
        "Tong Liang",
        "Chao Li",
        "Toshihisa Tanaka",
        "Shigeki Aoki",
        "Qibin Zhao",
        "Zhe Sun"
      ],
      "abstract": "We address the challenge of parameter-efficient fine-tuning (PEFT) for three-dimensional (3D) U-Net-based denoising diffusion probabilistic models (DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its practical significance, research on parameter-efficient representations of 3D convolution operations remains limited. To bridge this gap, we propose Tensor Volumetric Operator (TenVOO), a novel PEFT method specifically designed for fine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network modeling, TenVOO represents 3D convolution kernels with lower-dimensional tensors, effectively capturing complex spatial dependencies during fine-tuning with few parameters. We evaluate TenVOO on three downstream brain MRI datasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830 T1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that TenVOO achieves state-of-the-art performance in multi-scale structural similarity index measure (MS-SSIM), outperforming existing approaches in capturing spatial dependencies while requiring only 0.3% of the trainable parameters of the original model. Our code is available at: https://github.com/xiaovhua/tenvoo",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç£å…±æŒ¯æˆåƒ (MRI) å›¾åƒç”Ÿæˆä¸­åŸºäº 3D U-Net çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ (DDPM) çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º Tensor Volumetric Operator (TenVOO) çš„åˆ›æ–°æ–¹æ³•ã€‚TenVOO åˆ©ç”¨å¼ é‡ç½‘ç»œ (Tensor Network) å»ºæ¨¡ï¼Œå°† 3D å·ç§¯æ ¸è¡¨ç¤ºä¸ºä½ç»´å¼ é‡ï¼Œä»è€Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä»¥æå°‘çš„å‚æ•°é«˜æ•ˆæ•è·å¤æ‚çš„ç©ºé—´ä¾èµ–æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨ ADNIã€PPMI å’Œ BraTS2021 ä¸‰ä¸ªä¸‹æ¸¸è„‘éƒ¨ MRI æ•°æ®é›†ä¸Šå¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒæ¨¡å‹é¢„è®­ç»ƒäºæ¥è‡ª UK Biobank çš„ 59,830 ä¸ª T1 åŠ æƒæ‰«æã€‚ç»“æœæ˜¾ç¤ºï¼ŒTenVOO åœ¨å¤šå°ºåº¦ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•° (MS-SSIM) ä¸Šè¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æœ€æ˜¾è‘—çš„æ˜¯ï¼Œè¯¥æ–¹æ³•ä»…éœ€åŸå§‹æ¨¡å‹ 0.3% çš„å¯è®­ç»ƒå‚æ•°ï¼Œä¸º 3D å·ç§¯åéª¨å¹²æ¨¡å‹çš„å‚æ•°é«˜æ•ˆè¡¨ç¤ºç ”ç©¶æä¾›äº†æ–°çš„çªç ´ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18112v1",
      "published_date": "2025-07-24 05:51:51 UTC",
      "updated_date": "2025-07-24 05:51:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:40.673656+00:00"
    },
    {
      "arxiv_id": "2507.18106v1",
      "title": "Distributional Uncertainty for Out-of-Distribution Detection",
      "title_zh": "é¢å‘åˆ†å¸ƒå¤–æ£€æµ‹çš„åˆ†å¸ƒä¸ç¡®å®šæ€§",
      "authors": [
        "JinYoung Kim",
        "DaeUng Jo",
        "Kimin Yun",
        "Jeonghyo Song",
        "Youngjoon Yoo"
      ],
      "abstract": "Estimating uncertainty from deep neural networks is a widely used approach for detecting out-of-distribution (OoD) samples, which typically exhibit high predictive uncertainty. However, conventional methods such as Monte Carlo (MC) Dropout often focus solely on either model or data uncertainty, failing to align with the semantic objective of OoD detection. To address this, we propose the Free-Energy Posterior Network, a novel framework that jointly models distributional uncertainty and identifying OoD and misclassified regions using free energy. Our method introduces two key contributions: (1) a free-energy-based density estimator parameterized by a Beta distribution, which enables fine-grained uncertainty estimation near ambiguous or unseen regions; and (2) a loss integrated within a posterior network, allowing direct uncertainty estimation from learned parameters without requiring stochastic sampling. By integrating our approach with the residual prediction branch (RPL) framework, the proposed method goes beyond post-hoc energy thresholding and enables the network to learn OoD regions by leveraging the variance of the Beta distribution, resulting in a semantically meaningful and computationally efficient solution for uncertainty-aware segmentation. We validate the effectiveness of our method on challenging real-world benchmarks, including Fishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åˆ†å¸ƒå¤–(Out-of-Distribution, OoD)æ£€æµ‹ä¸­ä¸ç¡®å®šæ€§ä¼°è®¡ä¸è¯­ä¹‰ç›®æ ‡ä¸åŒ¹é…çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Free-Energy Posterior Network çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å…±åŒå»ºæ¨¡åˆ†å¸ƒä¸ç¡®å®šæ€§(Distributional Uncertainty)ï¼Œåˆ©ç”¨è‡ªç”±èƒ½(Free Energy)æ¥æœ‰æ•ˆè¯†åˆ« OoD å’Œè¯¯åˆ†ç±»åŒºåŸŸã€‚å…¶æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ä¸€ä¸ªç”± Beta distribution å‚æ•°åŒ–çš„è‡ªç”±èƒ½å¯†åº¦ä¼°è®¡å™¨ï¼Œç”¨äºå®ç°ç»†ç²’åº¦çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä»¥åŠä¸€ç§é›†æˆåœ¨ Posterior Network ä¸­çš„æŸå¤±å‡½æ•°ï¼Œä»è€Œæ— éœ€éšæœºé‡‡æ ·å³å¯ç›´æ¥ä¼°è®¡å‚æ•°ã€‚é€šè¿‡å°†è¯¥æ–¹æ³•ä¸æ®‹å·®é¢„æµ‹åˆ†æ”¯(Residual Prediction Branch, RPL)æ¡†æ¶ç›¸ç»“åˆï¼Œè¯¥ç ”ç©¶å®ç°äº†ä¸€ç§è¯­ä¹‰æ˜ç¡®ä¸”è®¡ç®—é«˜æ•ˆçš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥åˆ†å‰²æ–¹æ¡ˆã€‚åœ¨ Fishyscapesã€RoadAnomaly å’Œ Segment-Me-If-You-Can ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåˆ©ç”¨ Beta distribution çš„æ–¹å·®æ¥å­¦ä¹  OoD åŒºåŸŸå¹¶æ˜¾è‘—æå‡æ£€æµ‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages , 3 figures , IEEE International Conference on Advanced Visual and Signal-Based Systems",
      "pdf_url": "https://arxiv.org/pdf/2507.18106v1",
      "published_date": "2025-07-24 05:35:49 UTC",
      "updated_date": "2025-07-24 05:35:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:40.176600+00:00"
    },
    {
      "arxiv_id": "2507.18100v1",
      "title": "Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†é¢‘æ—¶åºå®šä½æ•°æ®é›†ä¸è®­ç»ƒæ–¹æ¡ˆ",
      "authors": [
        "Ruizhe Chen",
        "Zhiting Fan",
        "Tianze Luo",
        "Heqing Zou",
        "Zhaopeng Feng",
        "Guiyang Xie",
        "Hansheng Zhang",
        "Zhuochen Wang",
        "Zuozhu Liu",
        "Huaijian Zhang"
      ],
      "abstract": "Video Temporal Grounding (VTG) aims to localize relevant temporal segments in videos given natural language queries. Despite recent progress with large vision-language models (LVLMs) and instruction-tuning, existing approaches often suffer from limited temporal awareness and poor generalization. In this work, we introduce a two-stage training framework that integrates supervised fine-tuning with reinforcement learning (RL) to improve both the accuracy and robustness of VTG models. Our approach first leverages high-quality curated cold start data for SFT initialization, followed by difficulty-controlled RL to further enhance temporal localization and reasoning abilities. Comprehensive experiments on multiple VTG benchmarks demonstrate that our method consistently outperforms existing models, particularly in challenging and open-domain scenarios. We conduct an in-depth analysis of training strategies and dataset curation, highlighting the importance of both high-quality cold start data and difficulty-controlled RL. To facilitate further research and industrial adoption, we release all intermediate datasets, models, and code to the community.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘æ—¶é—´å®šä½(Video Temporal Grounding, VTG)ä»»åŠ¡ä¸­å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)å­˜åœ¨çš„æ—¶é—´æ„ŸçŸ¥åŠ›ä¸è¶³å’Œæ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆç›‘ç£å¾®è°ƒ(SFT)ä¸å¼ºåŒ–å­¦ä¹ (RL)çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡é«˜è´¨é‡çš„å†·å¯åŠ¨æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒåˆå§‹åŒ–ï¼Œéšååˆ©ç”¨éš¾åº¦å¯æ§çš„å¼ºåŒ–å­¦ä¹ (difficulty-controlled RL)è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ—¶é—´å®šä½ä¸æ¨ç†èƒ½åŠ›ã€‚åœ¨å¤šä¸ªVTGåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå°¤å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾åŸŸåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†æ•°æ®é›†ç­–åˆ’ä¸è®­ç»ƒç­–ç•¥çš„é‡è¦æ€§ï¼Œå¹¶å¼€æºäº†å…¨éƒ¨æ•°æ®é›†ã€æ¨¡å‹åŠä»£ç ï¼Œä¸ºè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå·¥ä¸šåŒ–åº”ç”¨æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18100v1",
      "published_date": "2025-07-24 05:24:01 UTC",
      "updated_date": "2025-07-24 05:24:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:42.280560+00:00"
    },
    {
      "arxiv_id": "2507.18667v1",
      "title": "Gen-AI Police Sketches with Stable Diffusion",
      "title_zh": "åŸºäº Stable Diffusion çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡æ‹Ÿç”»åƒ",
      "authors": [
        "Nicholas Fidalgo",
        "Aaron Contreras",
        "Katherine Harvey",
        "Johnny Ni"
      ],
      "abstract": "This project investigates the use of multimodal AI-driven approaches to automate and enhance suspect sketching. Three pipelines were developed and evaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model integrated with a pre-trained CLIP model for text-image alignment, and (3) novel approach incorporating LoRA fine-tuning of the CLIP model, applied to self-attention and cross-attention layers, and integrated with Stable Diffusion. An ablation study confirmed that fine-tuning both self- and cross-attention layers yielded the best alignment between text descriptions and sketches. Performance testing revealed that Model 1 achieved the highest structural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of 25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced perceptual similarity (LPIPS), with Model 3 showing improvement over Model 2 but still trailing Model 1. Qualitatively, sketches generated by Model 1 demonstrated the clearest facial features, highlighting its robustness as a baseline despite its simplicity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤šæ¨¡æ€ AI é©±åŠ¨çš„æ–¹æ³•æ¥è‡ªåŠ¨åŒ–å’Œå¢å¼ºå«Œç–‘äººç”»åƒ(suspect sketching)çš„æŠ€æœ¯ã€‚ç ”ç©¶å¼€å‘å¹¶è¯„ä¼°äº†ä¸‰ç§å·¥ä½œæµï¼Œåˆ†åˆ«æ˜¯åŸºç¡€çš„ Image-to-image Stable Diffusion æ¨¡å‹ã€é›†æˆé¢„è®­ç»ƒ CLIP æ¨¡å‹çš„å·¥ä½œæµï¼Œä»¥åŠä¸€ç§å¯¹ CLIP æ¨¡å‹çš„ Self-attention å’Œ Cross-attention å±‚è¿›è¡Œ LoRA å¾®è°ƒçš„æ–°å‹æ–¹æ³•ã€‚æ¶ˆèå®éªŒ(Ablation study)è¯å®ï¼ŒåŒæ—¶å¯¹ Self-attention å’Œ Cross-attention å±‚è¿›è¡Œå¾®è°ƒèƒ½å¤Ÿå®ç°æ–‡æœ¬æè¿°ä¸ç”»åƒä¹‹é—´æœ€ä½³çš„å¯¹é½æ•ˆæœã€‚æ€§èƒ½æµ‹è¯•æ˜¾ç¤ºï¼ŒåŸºç¡€æ¨¡å‹(Model 1)åœ¨ç»“æ„ç›¸ä¼¼æ€§(SSIM)è¾¾åˆ°0.72ï¼Œå³°å€¼ä¿¡å™ªæ¯”(PSNR)ä¸º25 dBï¼Œåœ¨è¿™äº›æŒ‡æ ‡ä¸Šä¼˜äºé›†æˆ CLIP å’Œ LoRA å¾®è°ƒçš„æ¨¡å‹ã€‚è™½ç„¶é€šè¿‡è¿­ä»£ä¼˜åŒ–æå‡äº†æ„ŸçŸ¥ç›¸ä¼¼åº¦(LPIPS)ï¼Œä¸” LoRA å¾®è°ƒæ¨¡å‹(Model 3)è¡¨ç°ä¼˜äº Model 2ï¼Œä½†å…¶é‡åŒ–è¡¨ç°ä»ç•¥é€ŠäºåŸºç¡€æ¨¡å‹ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼ŒåŸºç¡€æ¨¡å‹ç”Ÿæˆçš„ç”»åƒé¢éƒ¨ç‰¹å¾æœ€ä¸ºæ¸…æ™°ï¼Œå‡¸æ˜¾äº†å…¶ä½œä¸ºåŸºå‡†æ¨¡å‹åœ¨å¤„ç†è¯¥ä»»åŠ¡æ—¶çš„é²æ£’æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18667v1",
      "published_date": "2025-07-24 04:41:58 UTC",
      "updated_date": "2025-07-24 04:41:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:47.682247+00:00"
    },
    {
      "arxiv_id": "2507.18082v3",
      "title": "TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound",
      "title_zh": "TextSAM-EUSï¼šé¢å‘å†…é•œè¶…å£°ä¸‹èƒ°è…ºè‚¿ç˜¤ç²¾å‡†åˆ†å‰²çš„ SAM æ–‡æœ¬æç¤ºå­¦ä¹ ",
      "authors": [
        "Pascal Spiegler",
        "Taha Koleilat",
        "Arash Harirpoush",
        "Corey S. Miller",
        "Hassan Rivaz",
        "Marta Kersten-Oertel",
        "Yiming Xiao"
      ],
      "abstract": "Pancreatic cancer carries a poor prognosis and relies on endoscopic ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle noise, low contrast, and unintuitive appearance of EUS make segmentation of pancreatic tumors with fully supervised deep learning (DL) models both error-prone and dependent on large, expert-curated annotation datasets. To address these challenges, we present TextSAM-EUS, a novel, lightweight, text-driven adaptation of the Segment Anything Model (SAM) that requires no manual geometric prompts at inference. Our approach leverages text prompt learning (context optimization) through the BiomedCLIP text encoder in conjunction with a LoRA-based adaptation of SAM's architecture to enable automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total parameters. On the public Endoscopic Ultrasound Database of the Pancreas, TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised DL models and foundation models (e.g., SAM and its variants). As the first attempt to incorporate prompt learning in SAM-based medical image segmentation, TextSAM-EUS offers a practical option for efficient and robust automatic EUS segmentation. Code is available at https://github.com/HealthX-Lab/TextSAM-EUS .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èƒ°è…ºç™Œè¶…å£°å†…é•œ (EUS) å›¾åƒå­˜åœ¨çš„æ•£æ–‘å™ªå£°ã€ä½å¯¹æ¯”åº¦å’Œéç›´è§‚è¡¨ç°å¯¼è‡´åˆ†å‰²å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº† TextSAM-EUS æ¡†æ¶ã€‚ä½œä¸ºä¸€ç§æ–°å‹ä¸”è½»é‡çº§çš„æ–‡æœ¬é©±åŠ¨ Segment Anything Model (SAM) é€‚é…æ–¹æ¡ˆï¼ŒTextSAM-EUS åœ¨æ¨ç†é˜¶æ®µæ— éœ€æ‰‹åŠ¨å‡ ä½•æç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡ BiomedCLIP æ–‡æœ¬ç¼–ç å™¨å¼•å…¥æ–‡æœ¬æç¤ºå­¦ä¹  (context optimization)ï¼Œå¹¶ç»“åˆåŸºäº LoRA çš„æ¶æ„å¾®è°ƒï¼Œä»…éœ€è°ƒæ•´ 0.86% çš„å‚æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å…¬å¼€çš„èƒ°è…ºè¶…å£°å†…é•œæ•°æ®åº“ä¸Šï¼Œè¯¥æ¨¡å‹è‡ªåŠ¨æç¤ºä¸‹çš„ Dice ç³»æ•°è¾¾åˆ° 82.69%ï¼Œå½’ä¸€åŒ–è¡¨é¢è·ç¦» (NSD) ä¸º 85.28%ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿› (SOTA) å…¨ç›‘ç£æ·±åº¦å­¦ä¹ æ¨¡å‹åŠåŸºç¡€æ¨¡å‹ã€‚ä½œä¸ºé¦–æ¬¡å°†æç¤ºå­¦ä¹ åº”ç”¨äº SAM åŒ»å­¦å›¾åƒåˆ†å‰²çš„å°è¯•ï¼Œè¯¥ç ”ç©¶ä¸ºä¸´åºŠä¸­é«˜æ•ˆä¸”ç¨³å¥çš„è‡ªåŠ¨ EUS åˆ†å‰²æä¾›äº†ä¸€ç§å®ç”¨çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICCV 2025 Workshop CVAMD",
      "pdf_url": "https://arxiv.org/pdf/2507.18082v3",
      "published_date": "2025-07-24 04:17:06 UTC",
      "updated_date": "2025-07-30 17:39:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:41:51.531447+00:00"
    },
    {
      "arxiv_id": "2507.18074v1",
      "title": "AlphaGo Moment for Model Architecture Discovery",
      "title_zh": "æ¨¡å‹æ¶æ„å‘ç°çš„ AlphaGo æ—¶åˆ»",
      "authors": [
        "Yixiu Liu",
        "Yang Nan",
        "Weixian Xu",
        "Xiangkun Hu",
        "Lyumanshan Ye",
        "Zhen Qin",
        "Pengfei Liu"
      ],
      "abstract": "While AI systems demonstrate exponentially improving capabilities, the pace of AI research itself remains linearly bounded by human cognitive capacity, creating an increasingly severe development bottleneck. We present ASI-Arch, the first demonstration of Artificial Superintelligence for AI research (ASI4AI) in the critical domain of neural architecture discovery--a fully autonomous system that shatters this fundamental constraint by enabling AI to conduct its own architectural innovation. Moving beyond traditional Neural Architecture Search (NAS), which is fundamentally limited to exploring human-defined spaces, we introduce a paradigm shift from automated optimization to automated innovation. ASI-Arch can conduct end-to-end scientific research in the domain of architecture discovery, autonomously hypothesizing novel architectural concepts, implementing them as executable code, training and empirically validating their performance through rigorous experimentation and past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000 GPU hours, culminating in the discovery of 106 innovative, state-of-the-art (SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed unexpected strategic insights invisible to human players, our AI-discovered architectures demonstrate emergent design principles that systematically surpass human-designed baselines and illuminate previously unknown pathways for architectural innovation. Crucially, we establish the first empirical scaling law for scientific discovery itself--demonstrating that architectural breakthroughs can be scaled computationally, transforming research progress from a human-limited to a computation-scalable process. We provide comprehensive analysis of the emergent design patterns and autonomous research capabilities that enabled these breakthroughs, establishing a blueprint for self-accelerating AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†ASI-Archï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨ç¥ç»æ¶æ„å‘ç°é¢†åŸŸå±•ç¤ºçš„ç”¨äºäººå·¥æ™ºèƒ½ç ”ç©¶çš„è¶…çº§æ™ºèƒ½ï¼ˆASI4AIï¼‰ç³»ç»Ÿï¼Œæ—¨åœ¨æ‰“ç ´äººç±»è®¤çŸ¥èƒ½åŠ›å¯¹AIç ”å‘é€Ÿåº¦çš„é™åˆ¶ã€‚ASI-Archå®ç°äº†ä»ä¼ ç»Ÿç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰çš„è‡ªåŠ¨åŒ–ä¼˜åŒ–åˆ°è‡ªåŠ¨åŒ–åˆ›æ–°çš„èŒƒå¼è½¬å˜ï¼Œèƒ½å¤Ÿè‡ªä¸»æå‡ºæ–°é¢–æ¶æ„æ¦‚å¿µã€ç¼–å†™å¯æ‰§è¡Œä»£ç å¹¶è¿›è¡Œå®è¯éªŒè¯ã€‚é€šè¿‡åœ¨20,000ä¸ªGPUå°æ—¶å†…è¿›è¡Œçš„1,773æ¬¡è‡ªä¸»å®éªŒï¼Œè¯¥ç³»ç»ŸæˆåŠŸå‘ç°äº†106ç§åˆ›æ–°çš„SOTAçº¿æ€§æ³¨æ„åŠ›ï¼ˆlinear attentionï¼‰æ¶æ„ã€‚è¿™äº›æ¶æ„å±•ç°å‡ºäº†è¶…è¶Šäººç±»è®¾è®¡çš„æ¶Œç°æ€§è®¾è®¡åŸåˆ™ï¼ˆemergent design principlesï¼‰ï¼Œæ­ç¤ºäº†æ­¤å‰æœªçŸ¥çš„åˆ›æ–°è·¯å¾„ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ç¡®ç«‹äº†ç§‘å­¦å‘ç°çš„é¦–ä¸ªå®è¯ç¼©æ”¾æ³•åˆ™ï¼ˆscaling lawï¼‰ï¼Œè¯æ˜äº†æ¶æ„çªç ´å¯ä»¥é€šè¿‡è®¡ç®—åŠ›è§„æ¨¡åŒ–å®ç°ã€‚è¿™ä¸€æˆæœæ ‡å¿—ç€AIç ”ç©¶æ­£ä»å—é™äºäººç±»èƒ½åŠ›çš„æ¨¡å¼è½¬å˜ä¸ºè®¡ç®—å¯æ‰©å±•çš„è‡ªä¸»è¿›åŒ–è¿‡ç¨‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18074v1",
      "published_date": "2025-07-24 03:57:27 UTC",
      "updated_date": "2025-07-24 03:57:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:42:14.972048+00:00"
    },
    {
      "arxiv_id": "2507.18071v2",
      "title": "Group Sequence Policy Optimization",
      "title_zh": "ç¾¤ç»„åºåˆ—ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Chujie Zheng",
        "Shixuan Liu",
        "Mingze Li",
        "Xiong-Hui Chen",
        "Bowen Yu",
        "Chang Gao",
        "Kai Dang",
        "Yuqiong Liu",
        "Rui Men",
        "An Yang",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "abstract": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Group Sequence Policy Optimization (GSPO)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒçš„ç¨³å®šã€é«˜æ•ˆä¸”é«˜æ€§èƒ½çš„ Reinforcement Learning ç®—æ³•ã€‚ä¸ä»¥å¾€é‡‡ç”¨ token çº§åˆ«é‡è¦æ€§é‡‡æ ·çš„ç®—æ³•ä¸åŒï¼ŒGSPO åŸºäºåºåˆ—ä¼¼ç„¶ (Sequence Likelihood) å®šä¹‰é‡è¦æ€§æ¯”ä¾‹ï¼Œå¹¶æ‰§è¡Œåºåˆ—çº§åˆ«çš„ Clippingã€å¥–åŠ±å’Œä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒGSPO åœ¨è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ä¸Šå‡ä¼˜äº GRPO ç®—æ³•ï¼Œä¸”èƒ½æ˜¾è‘—ç¨³å®š Mixture-of-Experts (MoE) æ¶æ„çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ã€‚è¯¥ç®—æ³•ä¸ä»…ç®€åŒ–äº†å¼ºåŒ–å­¦ä¹ åŸºç¡€è®¾æ–½çš„è®¾è®¡ï¼Œè¿˜ä¸ºæœ€æ–°çš„ Qwen3 æ¨¡å‹æ€§èƒ½å¸¦æ¥äº†æ˜¾è‘—æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18071v2",
      "published_date": "2025-07-24 03:50:32 UTC",
      "updated_date": "2025-07-28 11:11:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:42:16.697385+00:00"
    },
    {
      "arxiv_id": "2507.18061v3",
      "title": "TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios",
      "title_zh": "TELEVALï¼šé¢å‘ä¸­æ–‡äº¤äº’åœºæ™¯çš„å£è¯­è¯­è¨€æ¨¡å‹åŠ¨æ€åŸºå‡†æµ‹è¯•",
      "authors": [
        "Zehan Li",
        "Hongjie Chen",
        "Qing Wang",
        "Yuxin Zhang",
        "Jing Zhou",
        "Hang Lv",
        "Mengjie Du",
        "Yaodong Song",
        "Jie Lian",
        "Jian Kang",
        "Jie Li",
        "Yongxiang Li",
        "Xuelong Li"
      ],
      "abstract": "Spoken language models (SLMs) have advanced rapidly in recent years, accompanied by a growing number of evaluation benchmarks. However, most existing benchmarks emphasize task completion and capability scaling, while remaining poorly aligned with how users interact with SLMs in real-world spoken conversations. Effective spoken interaction requires not only accurate understanding of user intent and content, but also the ability to respond with appropriate interactional strategies. In this paper, we present TELEVAL, a dynamic, user-centered benchmark for evaluating SLMs in realistic Chinese spoken interaction scenarios. TELEVAL consolidates evaluation into two core aspects. Reliable Content Fulfillment assesses whether models can comprehend spoken inputs and produce semantically correct responses. Interactional Appropriateness evaluates whether models act as socially capable interlocutors, requiring them not only to generate human-like, colloquial responses, but also to implicitly incorporate paralinguistic cues for natural interaction. Experiments reveal that, despite strong performance on semantic and knowledge-oriented tasks, current SLMs still struggle to produce natural and interactionally appropriate responses, highlighting the need for more interaction-faithful evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TELEVALï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºä¸­æ–‡å£è¯­äº¤äº’åœºæ™¯è®¾è®¡çš„åŠ¨æ€ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„è¯„æµ‹åŸºå‡†(benchmark)ã€‚é’ˆå¯¹ç°æœ‰è¯„æµ‹è¿‡äºå…³æ³¨ä»»åŠ¡å®Œæˆåº¦è€Œå¿½è§†çœŸå®å¯¹è¯ä½“éªŒçš„é—®é¢˜ï¼ŒTELEVAL ä»å¯é å†…å®¹å±¥è¡Œ(Reliable Content Fulfillment)å’Œäº¤äº’é€‚å½“æ€§(Interactional Appropriateness)ä¸¤ä¸ªæ ¸å¿ƒç»´åº¦å¯¹å£è¯­è¯­è¨€æ¨¡å‹(SLMs)è¿›è¡Œè¯„ä¼°ã€‚å¯é å†…å®¹å±¥è¡Œä¾§é‡äºæ¨¡å‹å¯¹å£è¯­è¾“å…¥çš„ç†è§£èƒ½åŠ›åŠè¯­ä¹‰å›åº”çš„å‡†ç¡®æ€§ï¼Œè€Œäº¤äº’é€‚å½“æ€§åˆ™è€ƒå¯Ÿæ¨¡å‹æ˜¯å¦èƒ½ç”Ÿæˆç±»äººã€å£è¯­åŒ–çš„å›åº”å¹¶èå…¥å‰¯è¯­è¨€çº¿ç´¢(paralinguistic cues)ä»¥å®ç°è‡ªç„¶äº¤äº’ã€‚å®éªŒå‘ç°ï¼Œå°½ç®¡å½“å‰çš„ SLMs åœ¨è¯­ä¹‰å’ŒçŸ¥è¯†å¯¼å‘ä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨ç”Ÿæˆç¬¦åˆè‡ªç„¶äº¤äº’é€»è¾‘çš„å›åº”æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚è¯¥åŸºå‡†çš„æå‡ºå¼ºè°ƒäº†åœ¨çœŸå®å£è¯­å¯¹è¯èƒŒæ™¯ä¸‹ï¼Œæ¨¡å‹éœ€è¦ä»å•çº¯çš„ä»»åŠ¡å¤„ç†è½¬å‘æ›´å…·ç¤¾äº¤å±æ€§çš„äº¤äº’æ¼”è¿›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18061v3",
      "published_date": "2025-07-24 03:23:55 UTC",
      "updated_date": "2026-01-12 03:01:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:42:19.076814+00:00"
    },
    {
      "arxiv_id": "2507.18059v1",
      "title": "Multi-Agent Guided Policy Optimization",
      "title_zh": "å¤šæ™ºèƒ½ä½“å¼•å¯¼ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Yueheng Li",
        "Guangming Xie",
        "Zongqing Lu"
      ],
      "abstract": "Due to practical constraints such as partial observability and limited communication, Centralized Training with Decentralized Execution (CTDE) has become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning (MARL). However, existing CTDE methods often underutilize centralized training or lack theoretical guarantees. We propose Multi-Agent Guided Policy Optimization (MAGPO), a novel framework that better leverages centralized training by integrating centralized guidance with decentralized execution. MAGPO uses an auto-regressive joint policy for scalable, coordinated exploration and explicitly aligns it with decentralized policies to ensure deployability under partial observability. We provide theoretical guarantees of monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across 6 diverse environments. Results show that MAGPO consistently outperforms strong CTDE baselines and matches or surpasses fully centralized approaches, offering a principled and practical solution for decentralized multi-agent learning. Our code and experimental data can be found in https://github.com/liyheng/MAGPO.",
      "tldr_zh": "é’ˆå¯¹åä½œå¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Multi-Agent Reinforcement Learning, MARL) ä¸­ç°æœ‰ä¸­å¿ƒåŒ–è®­ç»ƒåˆ†å¸ƒå¼æ‰§è¡Œ (Centralized Training with Decentralized Execution, CTDE) æ–¹æ³•å¯¹è®­ç»ƒèµ„æºåˆ©ç”¨ä¸è¶³ä¸”ç¼ºä¹ç†è®ºä¿è¯çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† Multi-Agent Guided Policy Optimization (MAGPO) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆä¸­å¿ƒåŒ–å¼•å¯¼ (centralized guidance) ä¸åˆ†å¸ƒå¼æ‰§è¡Œ (decentralized execution)ï¼Œæ›´æœ‰æ•ˆåœ°å‘æŒ¥äº†ä¸­å¿ƒåŒ–è®­ç»ƒçš„ä¼˜åŠ¿ã€‚MAGPO é‡‡ç”¨è‡ªå›å½’è”åˆç­–ç•¥ (auto-regressive joint policy) å®ç°å¯æ‰©å±•çš„åè°ƒæ¢ç´¢ï¼Œå¹¶å°†å…¶ä¸åˆ†å¸ƒå¼ç­–ç•¥æ˜¾å¼å¯¹é½ï¼Œä»¥ç¡®ä¿åœ¨éƒ¨åˆ†å¯è§‚æµ‹æ€§ç¯å¢ƒä¸‹çš„éƒ¨ç½²èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜ä¸ºè¯¥ç®—æ³•æä¾›äº†å•è°ƒç­–ç•¥æ”¹è¿› (monotonic policy improvement) çš„ç†è®ºä¿è¯ã€‚åœ¨ 6 ä¸ªä¸åŒç¯å¢ƒçš„ 43 é¡¹ä»»åŠ¡ä¸­è¿›è¡Œçš„å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒMAGPO çš„è¡¨ç°å§‹ç»ˆä¼˜äºå¼ºåŠ›çš„ CTDE åŸºå‡†ï¼Œä¸”èƒ½åŒ¹é…æˆ–è¶…è¶Šå®Œå…¨ä¸­å¿ƒåŒ–æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºåˆ†å¸ƒå¼å¤šæ™ºèƒ½ä½“å­¦ä¹ æä¾›äº†ä¸€ç§å…¼å…·ä¸¥è°¨ç†è®ºä¸å®è·µä»·å€¼çš„ç³»ç»ŸåŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18059v1",
      "published_date": "2025-07-24 03:22:21 UTC",
      "updated_date": "2025-07-24 03:22:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:42:24.073868+00:00"
    },
    {
      "arxiv_id": "2507.18046v1",
      "title": "Enhancing Scene Transition Awareness in Video Generation via Post-Training",
      "title_zh": "é€šè¿‡åè®­ç»ƒå¢å¼ºè§†é¢‘ç”Ÿæˆä¸­çš„åœºæ™¯è½¬æ¢æ„ŸçŸ¥",
      "authors": [
        "Hanwen Shen",
        "Jiajie Lu",
        "Yupeng Cao",
        "Xiaonan Yang"
      ],
      "abstract": "Recent advances in AI-generated video have shown strong performance on \\emph{text-to-video} tasks, particularly for short clips depicting a single scene. However, current models struggle to generate longer videos with coherent scene transitions, primarily because they cannot infer when a transition is needed from the prompt. Most open-source models are trained on datasets consisting of single-scene video clips, which limits their capacity to learn and respond to prompts requiring multiple scenes. Developing scene transition awareness is essential for multi-scene generation, as it allows models to identify and segment videos into distinct clips by accurately detecting transitions.\n  To address this, we propose the \\textbf{Transition-Aware Video} (TAV) dataset, which consists of preprocessed video clips with multiple scene transitions. Our experiment shows that post-training on the \\textbf{TAV} dataset improves prompt-based scene transition understanding, narrows the gap between required and generated scenes, and maintains image quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤šåœºæ™¯è½¬æ¢æ—¶æ„ŸçŸ¥åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† Transition-Aware Video (TAV) æ•°æ®é›†ï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹å¯¹åœºæ™¯åˆ‡æ¢çš„ç†è§£èƒ½åŠ›ã€‚ç›®å‰å¤§å¤šæ•°å¼€æºæ¨¡å‹ä¸»è¦åœ¨å•åœºæ™¯æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´å…¶éš¾ä»¥ä»æç¤ºè¯ä¸­å‡†ç¡®æ¨æ–­ä½•æ—¶éœ€è¦è¿›è¡Œåœºæ™¯è½¬æ¢ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥åŒ…å«å¤šä¸ªåœºæ™¯è½¬æ¢çš„é¢„å¤„ç†è§†é¢‘å‰ªè¾‘ï¼Œå¯¹æ¨¡å‹è¿›è¡Œ Post-Trainingã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ TAV æ•°æ®é›†ä¸Šè¿›è¡Œåè®­ç»ƒæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹æç¤ºè¯é©±åŠ¨çš„åœºæ™¯è½¬æ¢çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶æœ‰æ•ˆç¼©å°äº†éœ€æ±‚åœºæ™¯ä¸ç”Ÿæˆåœºæ™¯ä¹‹é—´çš„æ•°é‡å·®è·ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—æ”¹å–„å¤šåœºæ™¯ç”Ÿæˆè¿è´¯æ€§çš„åŒæ—¶ï¼Œä¾ç„¶ä¿æŒäº†é«˜è´¨é‡çš„å›¾åƒç”Ÿæˆæ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18046v1",
      "published_date": "2025-07-24 02:50:26 UTC",
      "updated_date": "2025-07-24 02:50:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:42:23.173140+00:00"
    },
    {
      "arxiv_id": "2507.18044v1",
      "title": "Synthetic Data Generation for Phrase Break Prediction with Large Language Model",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„çŸ­è¯­æ–­å¥é¢„æµ‹åˆæˆæ•°æ®ç”Ÿæˆ",
      "authors": [
        "Hoyeon Lee",
        "Sejung Son",
        "Ye-Eun Kang",
        "Jong-Hwan Kim"
      ],
      "abstract": "Current approaches to phrase break prediction address crucial prosodic aspects of text-to-speech systems but heavily rely on vast human annotations from audio or text, incurring significant manual effort and cost. Inherent variability in the speech domain, driven by phonetic factors, further complicates acquiring consistent, high-quality data. Recently, large language models (LLMs) have shown success in addressing data challenges in NLP by generating tailored synthetic data while reducing manual annotation needs. Motivated by this, we explore leveraging LLM to generate synthetic phrase break annotations, addressing the challenges of both manual annotation and speech-related tasks by comparing with traditional annotations and assessing effectiveness across multiple languages. Our findings suggest that LLM-based synthetic data generation effectively mitigates data challenges in phrase break prediction and highlights the potential of LLMs as a viable solution for the speech domain.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰çŸ­è¯­ä¸­æ–­é¢„æµ‹(Phrase Break Prediction)è¿‡åº¦ä¾èµ–æ˜‚è´µä¸”ä¸ä¸€è‡´çš„äººå·¥æ ‡æ³¨é—®é¢˜ï¼Œæå‡ºåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Model)ç”Ÿæˆå®šåˆ¶åŒ–åˆæˆæ ‡æ³¨çš„æ–¹æ³•ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”LLMç”Ÿæˆçš„åˆæˆæ•°æ®ä¸ä¼ ç»Ÿæ ‡æ³¨æ•°æ®ï¼Œå¹¶åœ¨å¤šç§è¯­è¨€ç¯å¢ƒä¸‹è¯„ä¼°äº†å…¶æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨æ˜¾è‘—å‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºLLMçš„åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£æ•°æ®ç¨€ç¼ºå’Œæ ‡æ³¨å›°éš¾ç­‰æŒ‘æˆ˜ï¼Œåœ¨ä¿æŒé«˜è´¨é‡çš„åŒæ—¶é™ä½äº†ç ”å‘æˆæœ¬ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†LLMåœ¨è¯­éŸ³é¢†åŸŸ(speech domain)ä½œä¸ºæ•°æ®è§£å†³æ–¹æ¡ˆçš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å¤„ç†è¯­éŸ³ç›¸å…³ä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€å‘ç°ä¸ä»…éªŒè¯äº†åˆæˆæ•°æ®åœ¨çŸ­è¯­é¢„æµ‹ä¸­çš„å®ç”¨æ€§ï¼Œä¹Ÿä¸ºæ”¹è¿›æ–‡æœ¬è½¬è¯­éŸ³(text-to-speech)ç³»ç»Ÿçš„éŸµå¾‹è¡¨ç°æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.18044v1",
      "published_date": "2025-07-24 02:45:03 UTC",
      "updated_date": "2025-07-24 02:45:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:42:34.743090+00:00"
    },
    {
      "arxiv_id": "2507.18043v1",
      "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs",
      "title_zh": "GrAInSï¼šåŸºäºæ¢¯åº¦å½’å› çš„å¤§è¯­è¨€æ¨¡å‹ä¸è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†æ—¶æ“æ§",
      "authors": [
        "Duy Nguyen",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Inference-time steering methods offer a lightweight alternative to fine-tuning large language models (LLMs) and vision-language models (VLMs) by modifying internal activations at test time without updating model weights. However, most existing approaches rely on fixed, global intervention vectors, overlook the causal influence of individual input tokens, and fail to leverage informative gradients from the model's logits, particularly in multimodal settings where visual and textual inputs contribute unevenly. To address these limitations, we introduce GrAInS, an inference-time steering approach that operates across both language-only and vision-language models and tasks. GrAInS uses contrastive, gradient-based attribution via Integrated Gradients to identify the top-k most influential tokens, both positively and negatively attributed based on their contribution to preferred versus dispreferred outputs. These tokens are then used to construct directional steering vectors that capture semantic shifts from undesirable to desirable behavior. During inference, GrAInS adjusts hidden activations at transformer layers guided by token-level attribution signals, and normalizes activations to preserve representational scale. This enables fine-grained, interpretable, and modular control over model behavior, without retraining or auxiliary supervision. Empirically, GrAInS consistently outperforms both fine-tuning and existing steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514 with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all while preserving the model's fluency and general capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„ç°æœ‰æ¨ç†é˜¶æ®µå¼•å¯¼(inference-time steering)æ–¹æ³•å­˜åœ¨çš„å±€é™ï¼Œæå‡ºäº†GrAInSæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å…¨å±€å¹²é¢„å‘é‡å¿½ç•¥å•ä¸ªTokenå› æœå½±å“åŠå¤šæ¨¡æ€è¾“å…¥è´¡çŒ®ä¸å‡çš„é—®é¢˜ã€‚GrAInSåˆ©ç”¨åŸºäºIntegrated Gradientsçš„å¯¹æ¯”æ¢¯åº¦å½’å› æŠ€æœ¯ï¼Œè¯†åˆ«å¯¹åå¥½è¾“å‡ºè´¡çŒ®æœ€å¤§æˆ–æœ€è´Ÿé¢çš„å‰kä¸ªå…³é”®Tokenï¼Œå¹¶æ®æ­¤æ„å»ºæ•æ‰è¯­ä¹‰åç§»çš„æ–¹å‘æ€§å¼•å¯¼å‘é‡ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè¯¥æ–¹æ³•åŠ¨æ€è°ƒæ•´Transformerå±‚çš„éšå±‚æ¿€æ´»å€¼ï¼Œå¹¶ç»“åˆæ¿€æ´»å€¼å½’ä¸€åŒ–ä»¥ç»´æŒè¡¨å¾å°ºåº¦ï¼Œå®ç°äº†ç»†ç²’åº¦ã€å¯è§£é‡Šä¸”æ¨¡å—åŒ–çš„æ¨¡å‹è¡Œä¸ºæ§åˆ¶ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒæˆ–è¾…åŠ©ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGrAInSåœ¨TruthfulQAä»»åŠ¡ä¸Šå°†Llama-3.1-8Bçš„å‡†ç¡®ç‡æå‡äº†13.22%ï¼Œå¹¶å°†LLaVA-1.6-7Båœ¨MMHal-Benchä¸Šçš„å¹»è§‰ç‡ä»0.624é™è‡³0.514ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨SPA-VLæ•°æ®é›†ä¸Šçš„å¯¹é½èƒœç‡æå‡äº†8.11%ï¼ŒåŒæ—¶ä¿è¯äº†æ¨¡å‹çš„è¯­è¨€æµç•…åº¦ä¸é€šç”¨èƒ½åŠ›ï¼Œåœ¨å¤šé¡¹æ€§èƒ½æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å¾®è°ƒå’Œå¼•å¯¼åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages. Code: https://github.com/duykhuongnguyen/GrAInS",
      "pdf_url": "https://arxiv.org/pdf/2507.18043v1",
      "published_date": "2025-07-24 02:34:13 UTC",
      "updated_date": "2025-07-24 02:34:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:42:39.846102+00:00"
    },
    {
      "arxiv_id": "2507.18033v1",
      "title": "OpenNav: Open-World Navigation with Multimodal Large Language Models",
      "title_zh": "OpenNavï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¼€æ”¾ä¸–ç•Œå¯¼èˆª",
      "authors": [
        "Mingfeng Yuan",
        "Letian Wang",
        "Steven L. Waslander"
      ],
      "abstract": "Pre-trained large language models (LLMs) have demonstrated strong common-sense reasoning abilities, making them promising for robotic navigation and planning tasks. However, despite recent progress, bridging the gap between language descriptions and actual robot actions in the open-world, beyond merely invoking limited predefined motion primitives, remains an open challenge. In this work, we aim to enable robots to interpret and decompose complex language instructions, ultimately synthesizing a sequence of trajectory points to complete diverse navigation tasks given open-set instructions and open-set objects. We observe that multi-modal large language models (MLLMs) exhibit strong cross-modal understanding when processing free-form language instructions, demonstrating robust scene comprehension. More importantly, leveraging their code-generation capability, MLLMs can interact with vision-language perception models to generate compositional 2D bird-eye-view value maps, effectively integrating semantic knowledge from MLLMs with spatial information from maps to reinforce the robot's spatial understanding. To further validate our approach, we effectively leverage large-scale autonomous vehicle datasets (AVDs) to validate our proposed zero-shot vision-language navigation framework in outdoor navigation tasks, demonstrating its capability to execute a diverse range of free-form natural language navigation instructions while maintaining robustness against object detection errors and linguistic ambiguities. Furthermore, we validate our system on a Husky robot in both indoor and outdoor scenes, demonstrating its real-world robustness and applicability. Supplementary videos are available at https://trailab.github.io/OpenNav-website/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OpenNavï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ Multimodal Large Language Models (MLLMs) å®ç°å¼€æ”¾ä¸–ç•Œæœºå™¨äººå¯¼èˆªçš„é›¶æ ·æœ¬æ¡†æ¶ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³è¯­è¨€æŒ‡ä»¤ä¸æœºå™¨äººå®é™…è¡ŒåŠ¨ä¹‹é—´çš„è¡”æ¥æŒ‘æˆ˜ï¼Œé€šè¿‡å°†å¤æ‚çš„è‡ªç”±æ ¼å¼æŒ‡ä»¤åˆ†è§£å¹¶åˆæˆä¸€ç³»åˆ—è½¨è¿¹ç‚¹æ¥å®Œæˆå¯¼èˆªä»»åŠ¡ã€‚OpenNav å……åˆ†åˆ©ç”¨ MLLMs çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ä¸è§†è§‰è¯­è¨€æ„ŸçŸ¥æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œç”Ÿæˆç»„åˆå¼çš„ 2D bird-eye-view ä»·å€¼å›¾ï¼Œä»è€Œå°† MLLMs çš„è¯­ä¹‰çŸ¥è¯†ä¸åœ°å›¾çš„ç©ºé—´ä¿¡æ¯ç›¸èåˆã€‚åœ¨å¤§å‹è‡ªåŠ¨é©¾é©¶æ•°æ®é›† (AVDs) ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤„ç†å¤šæ ·åŒ–æŒ‡ä»¤æ—¶å…·æœ‰æå¼ºçš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹ç›®æ ‡æ£€æµ‹é”™è¯¯å’Œè¯­è¨€æ­§ä¹‰ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿåœ¨ Husky æœºå™¨äººä¸Šçš„å®¤å†…å¤–å®æµ‹éªŒè¯äº†å…¶åœ¨ç°å®ç¯å¢ƒä¸­çš„å¯é æ€§ä¸åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18033v1",
      "published_date": "2025-07-24 02:05:28 UTC",
      "updated_date": "2025-07-24 02:05:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:42:33.534165+00:00"
    },
    {
      "arxiv_id": "2507.18031v1",
      "title": "ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks",
      "title_zh": "ViGTextï¼šèåˆè§†è§‰è¯­è¨€æ¨¡å‹è§£é‡Šä¸å›¾ç¥ç»ç½‘ç»œçš„æ·±åº¦ä¼ªé€ å›¾åƒæ£€æµ‹",
      "authors": [
        "Ahmad ALBarqawi",
        "Mahmoud Nazzal",
        "Issa Khalil",
        "Abdallah Khreishah",
        "NhatHai Phan"
      ],
      "abstract": "The rapid rise of deepfake technology, which produces realistic but fraudulent digital content, threatens the authenticity of media. Traditional deepfake detection approaches often struggle with sophisticated, customized deepfakes, especially in terms of generalization and robustness against malicious attacks. This paper introduces ViGText, a novel approach that integrates images with Vision Large Language Model (VLLM) Text explanations within a Graph-based framework to improve deepfake detection. The novelty of ViGText lies in its integration of detailed explanations with visual data, as it provides a more context-aware analysis than captions, which often lack specificity and fail to reveal subtle inconsistencies. ViGText systematically divides images into patches, constructs image and text graphs, and integrates them for analysis using Graph Neural Networks (GNNs) to identify deepfakes. Through the use of multi-level feature extraction across spatial and frequency domains, ViGText captures details that enhance its robustness and accuracy to detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText significantly enhances generalization and achieves a notable performance boost when it detects user-customized deepfakes. Specifically, average F1 scores rise from 72.45% to 98.32% under generalization evaluation, and reflects the model's superior ability to generalize to unseen, fine-tuned variations of stable diffusion models. As for robustness, ViGText achieves an increase of 11.1% in recall compared to other deepfake detection approaches. When facing targeted attacks that exploit its graph-based architecture, ViGText limits classification performance degradation to less than 4%. ViGText uses detailed visual and textual analysis to set a new standard for detecting deepfakes, helping ensure media authenticity and information integrity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ViGTextï¼Œè¿™æ˜¯ä¸€ç§å°†å›¾åƒæ•°æ®ä¸è§†è§‰å¤§è¯­è¨€æ¨¡å‹ (Vision Large Language Model, VLLM) çš„æ–‡æœ¬è§£é‡Šç›¸ç»“åˆçš„æ–°å‹æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ Deepfake å›¾åƒæ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚ä¸åŒäºä¼ ç»Ÿçš„ç®€ç•¥æ ‡é¢˜ï¼ŒViGText åˆ©ç”¨ VLLM ç”Ÿæˆçš„è¯¦ç»†è§£é‡Šæ¥æ•æ‰ç»†å¾®çš„ä¸ä¸€è‡´æ€§ï¼Œä»è€Œæä¾›æ›´å…·ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„åˆ†æã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å›¾åƒåˆ’åˆ†ä¸ºè¡¥ä¸å¹¶æ„å»ºå›¾åƒä¸æ–‡æœ¬å›¾ç»“æ„ï¼Œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œ (Graph Neural Networks, GNNs) å¯¹è·¨æ¨¡æ€ç‰¹å¾è¿›è¡Œæ·±åº¦æ•´åˆã€‚æ­¤å¤–ï¼ŒViGText åœ¨ç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸæ‰§è¡Œå¤šçº§ç‰¹å¾æå–ï¼Œæœ‰æ•ˆå¢å¼ºäº†è¯†åˆ«å¤æ‚å®šåˆ¶åŒ– Deepfake å›¾åƒçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒViGText åœ¨æ³›åŒ–è¯„ä¼°ä¸­çš„å¹³å‡ F1 åˆ†æ•°ä» 72.45% æå‡è‡³ 98.32%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹åœ¨å¤„ç†ç¨³å®šæ‰©æ•£ (Stable Diffusion) æ¨¡å‹å˜ä½“æ—¶çš„è¡¨ç°ã€‚åœ¨é²æ£’æ€§æ–¹é¢ï¼Œå…¶å¬å›ç‡ (Recall) ç›¸æ¯”å…¶ä»–æ–¹æ³•æé«˜äº† 11.1%ï¼Œä¸”åœ¨é¢å¯¹é’ˆå¯¹æ€§çš„å¯¹æŠ—æ”»å‡»æ—¶ï¼Œæ€§èƒ½ä¸‹é™å¹…åº¦æ§åˆ¶åœ¨ 4% ä»¥å†…ã€‚è¯¥ç ”ç©¶é€šè¿‡è§†è§‰ä¸æ–‡æœ¬çš„ç²¾ç»†åŒ–åˆ†æï¼Œä¸ºç¡®ä¿åª’ä½“çœŸå®æ€§å’Œä¿¡æ¯å®Œæ•´æ€§æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18031v1",
      "published_date": "2025-07-24 02:04:58 UTC",
      "updated_date": "2025-07-24 02:04:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:42:44.229973+00:00"
    },
    {
      "arxiv_id": "2507.18028v1",
      "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database",
      "title_zh": "NeuralDBï¼šé€šè¿‡ç¥ç»é”®å€¼æ•°æ®åº“å°†å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†ç¼–è¾‘æ‰©å±•è‡³åä¸‡æ¡äº‹å®",
      "authors": [
        "Weizhi Fei",
        "Hao Shi",
        "Jing Xu",
        "Jingchen Peng",
        "Jiazheng Li",
        "Jingzhao Zhang",
        "Bo Bai",
        "Wei Han",
        "Zhenyuan Chen",
        "Xueyan Niu"
      ],
      "abstract": "Efficiently editing knowledge stored in large language models (LLMs) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of LLMs and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\\&E methods as querying a Key-Value (KV) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural KV database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of LLMs. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\\textbf{50x} more than in prior work).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NeuralDBï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) çŸ¥è¯†ç¼–è¾‘è§„æ¨¡åŒ–éš¾é¢˜çš„æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿçš„å®šä½å¹¶ç¼–è¾‘ (Locate-and-Edit, L&E) æ–¹æ³•åœ¨å¤„ç†æµ·é‡äº‹å®æ—¶å®¹æ˜“å¯¼è‡´æ¨¡å‹é€šç”¨èƒ½åŠ›ä¸‹é™åŠçŸ¥è¯†é—å¿˜çš„é—®é¢˜ï¼ŒNeuralDB å°†ç¼–è¾‘è¿‡ç¨‹å»ºæ¨¡ä¸ºå¯¹ç¥ç»é”®å€¼æ•°æ®åº“ (Neural KV Database) çš„æŸ¥è¯¢ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåœ¨äºå¼•å…¥äº†éçº¿æ€§é—¨æ§æ£€ç´¢æ¨¡å—ï¼Œè¯¥æ¨¡å—ä»…åœ¨æ¨ç†æ¶‰åŠå·²ç¼–è¾‘äº‹å®æ—¶æ¿€æ´»ï¼Œä»è€Œç¡®ä¿äº† LLMs åŸå§‹èƒ½åŠ›çš„å®Œæ•´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNeuralDB åœ¨ ZsRE å’Œ CounterFacts æ•°æ®é›†ä¸Šè¡¨ç°å“è¶Šï¼Œåœ¨ç¼–è¾‘æ•ˆæœã€æ³›åŒ–æ€§ã€ç‰¹å¼‚æ€§åŠè¯­è¨€ä¸€è‡´æ€§ç­‰æ–¹é¢å‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æˆåŠŸå°†ç¼–è¾‘è§„æ¨¡æ‰©å±•è‡³ 100,000 æ¡äº‹å®ï¼Œæ¯”ä»¥å¾€å·¥ä½œæå‡äº† 50 å€ï¼Œä¸ºå®ç°å¤§æ¨¡å‹çš„é«˜æ•ˆæŒç»­æ›´æ–°å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.18028v1",
      "published_date": "2025-07-24 02:00:09 UTC",
      "updated_date": "2025-07-24 02:00:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:43:08.774880+00:00"
    },
    {
      "arxiv_id": "2507.18022v1",
      "title": "Does visualization help AI understand data?",
      "title_zh": "å¯è§†åŒ–æ˜¯å¦æœ‰åŠ©äºäººå·¥æ™ºèƒ½ç†è§£æ•°æ®ï¼Ÿ",
      "authors": [
        "Victoria R. Li",
        "Johnathan Sun",
        "Martin Wattenberg"
      ],
      "abstract": "Charts and graphs help people analyze data, but can they also be useful to AI systems? To investigate this question, we perform a series of experiments with two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three representative analysis tasks, the two systems describe synthetic datasets more precisely and accurately when raw data is accompanied by a scatterplot, especially as datasets grow in complexity. Comparison with two baselines -- providing a blank chart and a chart with mismatched data -- shows that the improved performance is due to the content of the charts. Our results are initial evidence that AI systems, like humans, can benefit from visualization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾è¡¨å¯è§†åŒ–æ˜¯å¦èƒ½åƒè¾…åŠ©äººç±»ä¸€æ ·æå‡äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„åˆ†æèƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ GPT 4.1 å’Œ Claude 3.5 ä¸¤ç§å•†ä¸šè§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models)åœ¨ä¸‰é¡¹ä»£è¡¨æ€§åˆ†æä»»åŠ¡ä¸­è¿›è¡Œäº†å®éªŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“åŸå§‹æ•°æ®è¾…ä»¥æ•£ç‚¹å›¾(Scatterplot)æ—¶ï¼Œç³»ç»Ÿå¯¹åˆæˆæ•°æ®é›†çš„æè¿°æ›´åŠ ç²¾ç¡®ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®é›†å¤æ‚åº¦å¢åŠ çš„æƒ…å†µä¸‹è¡¨ç°æ›´ä¼˜ã€‚ä¸æä¾›ç©ºç™½å›¾è¡¨æˆ–é”™è¯¯åŒ¹é…æ•°æ®çš„åŸºå‡†(Baselines)å¯¹æ¯”ï¼Œç ”ç©¶è¯å®äº†æ€§èƒ½çš„æå‡ç›´æ¥æºäºå›¾è¡¨å†…å®¹çš„è¾…åŠ©ã€‚è¯¥ç ”ç©¶ä¸º AI ç³»ç»Ÿèƒ½å¤Ÿä»å¯è§†åŒ–ä¸­è·ç›Šæä¾›äº†åˆæ­¥è¯æ®ï¼Œè¯æ˜äº†è§†è§‰å‘ˆç°å¯¹æå‡æ¨¡å‹æ•°æ®ç†è§£èƒ½åŠ›çš„ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.18022v1",
      "published_date": "2025-07-24 01:47:34 UTC",
      "updated_date": "2025-07-24 01:47:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:43:21.005524+00:00"
    },
    {
      "arxiv_id": "2507.18017v1",
      "title": "Fashion-AlterEval: A Dataset for Improved Evaluation of Conversational Recommendation Systems with Alternative Relevant Items",
      "title_zh": "Fashion-AlterEvalï¼šé€šè¿‡å¤‡é€‰ç›¸å…³é¡¹æ”¹è¿›å¯¹è¯å¼æ¨èç³»ç»Ÿè¯„ä¼°çš„æ•°æ®é›†",
      "authors": [
        "Maria Vlachou"
      ],
      "abstract": "In Conversational Recommendation Systems (CRS), a user provides feedback on recommended items at each turn, leading the CRS towards improved recommendations. Due to the need for a large amount of data, a user simulator is employed for both training and evaluation. Such user simulators critique the current retrieved item based on knowledge of a single target item. However, system evaluation in offline settings with simulators is limited by the focus on a single target item and their unlimited patience over a large number of turns. To overcome these limitations of existing simulators, we propose Fashion-AlterEval, a new dataset that contains human judgments for a selection of alternative items by adding new annotations in common fashion CRS datasets. Consequently, we propose two novel meta-user simulators that use the collected judgments and allow simulated users not only to express their preferences about alternative items to their original target, but also to change their mind and level of patience. In our experiments using the Shoes and Fashion IQ as the original datasets and three CRS models, we find that using the knowledge of alternatives by the simulator can have a considerable impact on the evaluation of existing CRS models, specifically that the existing single-target evaluation underestimates their effectiveness, and when simulatedusers are allowed to instead consider alternative relevant items, the system can rapidly respond to more quickly satisfy the user.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Fashion-AlterEvalï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ”¹è¿›å¯¹è¯å¼æ¨èç³»ç»Ÿ (Conversational Recommendation Systems, CRS) è¯„ä¼°çš„æ–°å‹æ•°æ®é›†ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡æ‹Ÿå™¨ä»…å…³æ³¨å•ä¸€ç›®æ ‡é¡¹ä¸”å…·æœ‰æ— é™è€å¿ƒçš„å±€é™æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨ç°æœ‰çš„æ—¶å°šé¢†åŸŸæ•°æ®é›†åŸºç¡€ä¸Šå¢åŠ äº†äººå·¥æ ‡æ³¨çš„æ›¿ä»£ç›¸å…³é¡¹ä¿¡æ¯ï¼Œå¹¶æ®æ­¤å¼€å‘äº†ä¸¤ç§æ–°å‹çš„å…ƒç”¨æˆ·æ¨¡æ‹Ÿå™¨ (meta-user simulators)ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨¡æ‹Ÿç”¨æˆ·å¯¹æ›¿ä»£é¡¹çš„åå¥½ä»¥åŠè€å¿ƒå’Œæ„æ„¿çš„å˜åŒ–ã€‚åœ¨ Shoes å’Œ Fashion IQ æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„åŸºäºå•ç›®æ ‡çš„è¯„ä¼°æ–¹æ³•ä½ä¼°äº† CRS æ¨¡å‹çš„å®é™…æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼Œå½“å…è®¸æ¨¡æ‹Ÿç”¨æˆ·è€ƒè™‘æ›¿ä»£ç›¸å…³é¡¹æ—¶ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•è·ç”¨æˆ·æ„å›¾å¹¶æé«˜æ»¡æ„åº¦ã€‚è¯¥å·¥ä½œä¸ºæ„å»ºæ›´çœŸå®ã€æ›´é«˜æ•ˆçš„å¯¹è¯å¼æ¨èè¯„ä¼°ä½“ç³»æä¾›äº†é‡è¦çš„å·¥å…·å’Œè§è§£ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2401.05783",
      "pdf_url": "https://arxiv.org/pdf/2507.18017v1",
      "published_date": "2025-07-24 01:18:24 UTC",
      "updated_date": "2025-07-24 01:18:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:43:32.029608+00:00"
    },
    {
      "arxiv_id": "2507.21153v1",
      "title": "Deep Reinforcement Learning for Real-Time Green Energy Integration in Data Centers",
      "title_zh": "é¢å‘æ•°æ®ä¸­å¿ƒå®æ—¶ç»¿è‰²èƒ½æºé›†æˆçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Abderaouf Bahi",
        "Amel Ourici"
      ],
      "abstract": "This paper explores the implementation of a Deep Reinforcement Learning (DRL)-optimized energy management system for e-commerce data centers, aimed at enhancing energy efficiency, cost-effectiveness, and environmental sustainability. The proposed system leverages DRL algorithms to dynamically manage the integration of renewable energy sources, energy storage, and grid power, adapting to fluctuating energy availability in real time. The study demonstrates that the DRL-optimized system achieves a 38\\% reduction in energy costs, significantly outperforming traditional Reinforcement Learning (RL) methods (28\\%) and heuristic approaches (22\\%). Additionally, it maintains a low SLA violation rate of 1.5\\%, compared to 3.0\\% for RL and 4.8\\% for heuristic methods. The DRL-optimized approach also results in an 82\\% improvement in energy efficiency, surpassing other methods, and a 45\\% reduction in carbon emissions, making it the most environmentally friendly solution. The system's cumulative reward of 950 reflects its superior performance in balancing multiple objectives. Through rigorous testing and ablation studies, the paper validates the effectiveness of the DRL model's architecture and parameters, offering a robust solution for energy management in data centers. The findings highlight the potential of DRL in advancing energy optimization strategies and addressing sustainability challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå­å•†åŠ¡æ•°æ®ä¸­å¿ƒï¼Œæå‡ºäº†ä¸€ç§åŸºäº Deep Reinforcement Learning (DRL) ä¼˜åŒ–çš„èƒ½é‡ç®¡ç†ç³»ç»Ÿï¼Œæ—¨åœ¨æå‡èƒ½æºæ•ˆç‡ã€æˆæœ¬æ•ˆç›Šå’Œç¯å¢ƒå¯æŒç»­æ€§ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨ DRL ç®—æ³•å®æ—¶åŠ¨æ€ç®¡ç†å¯å†ç”Ÿèƒ½æºã€èƒ½æºå­˜å‚¨ç³»ç»Ÿä¸ç”µç½‘ç”µåŠ›çš„æ•´åˆï¼Œä»¥é€‚åº”èƒ½æºä¾›åº”çš„å®æ—¶æ³¢åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRL ä¼˜åŒ–ç³»ç»Ÿå®ç°äº† 38% çš„èƒ½æºæˆæœ¬å‰Šå‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿ Reinforcement Learning (RL) æ–¹æ³•å’Œ heuristic æ–¹æ³•ï¼ŒåŒæ—¶å°† Service Level Agreement (SLA) è¿è§„ç‡ç»´æŒåœ¨ 1.5% çš„è¾ƒä½æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨èƒ½æºæ•ˆç‡ä¸Šæå‡äº† 82%ï¼Œå¹¶å‡å°‘äº† 45% çš„ç¢³æ’æ”¾é‡ï¼Œæˆä¸ºæœ€å…·ç¯å¢ƒå‹å¥½æ€§çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ä¸¥æ ¼çš„æµ‹è¯•å’Œ ablation studiesï¼Œè¯¥ç ”ç©¶éªŒè¯äº† DRL æ¨¡å‹æ¶æ„çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ•°æ®ä¸­å¿ƒçš„èƒ½æºç®¡ç†å’Œå¯æŒç»­æ€§æŒ‘æˆ˜æä¾›äº†é²æ£’çš„ä¼˜åŒ–ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21153v1",
      "published_date": "2025-07-24 00:59:56 UTC",
      "updated_date": "2025-07-24 00:59:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:43:30.615110+00:00"
    },
    {
      "arxiv_id": "2507.18009v1",
      "title": "GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures",
      "title_zh": "GRR-CoCaï¼šåœ¨å¤šæ¨¡æ€æ¨¡å‹æ¶æ„ä¸­åˆ©ç”¨ LLM æœºåˆ¶",
      "authors": [
        "Jake R. Patock",
        "Nicole Catherine Lewis",
        "Kevin McCoy",
        "Christina Gomez",
        "Canling Chen",
        "Lorenzo Luzi"
      ],
      "abstract": "State-of-the-art (SOTA) image and text generation models are multimodal models that have many similarities to large language models (LLMs). Despite achieving strong performances, leading foundational multimodal model architectures frequently lag behind the architectural sophistication of contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner (CoCa) model that incorporates Gaussian error gated linear units, root mean squared normalization, and rotary positional embedding into the textual decoders and the vision transformer (ViT) encoder. Each architectural modification has been shown to improve model performance in LLMs, but has yet to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model with the same modified textual decoders but with CoCa's original ViT encoder. We used standard pretraining and fine-tuning workflows to benchmark the models on contrastive and generative tasks. Our GRR-CoCa significantly outperformed Baseline CoCa on the pretraining dataset and three diverse fine-tuning datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were 13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We show that GRR-CoCa's modified architecture improves performance and generalization across vision-language domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºç¡€å¤šæ¨¡æ€æ¨¡å‹æ¶æ„åœ¨å¤æ‚æ€§ä¸Šæ»åäºå½“ä»£å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„é—®é¢˜ï¼Œæå‡ºäº†æ”¹è¿›çš„å¯¹æ¯”æè¿°ç¬¦æ¨¡å‹GRR-CoCaã€‚è¯¥æ¨¡å‹å°†Gaussian error gated linear unitsã€root mean squared normalizationä»¥åŠrotary positional embeddingå¼•å…¥æ–‡æœ¬è§£ç å™¨å’Œè§†è§‰Transformer(ViT)ç¼–ç å™¨ä¸­ï¼Œä»¥æå‡æ¶æ„æ€§èƒ½ã€‚é€šè¿‡åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒå·¥ä½œæµä¸­çš„åŸºå‡†æµ‹è¯•ï¼ŒGRR-CoCaåœ¨é¢„è®­ç»ƒæ•°æ®é›†ä¸Šçš„contrastive lossæ˜¾è‘—é™ä½äº†27.25%ï¼Œperplexityé™ä½äº†3.71%ã€‚åœ¨ä¸‰ä¸ªä¸åŒçš„å¾®è°ƒæ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å‹çš„å¹³å‡å¯¹æ¯”æŸå¤±å’Œå›°æƒ‘åº¦ä¹Ÿåˆ†åˆ«ä¼˜åŒ–äº†13.66%å’Œ5.18%ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†é€šè¿‡æ•´åˆLLMçš„å…ˆè¿›æ¶æ„æœºåˆ¶ï¼Œå¯ä»¥æœ‰æ•ˆå¢å¼ºå¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€(vision-language)é¢†åŸŸçš„è¡¨ç°å’Œæ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.18009v1",
      "published_date": "2025-07-24 00:54:31 UTC",
      "updated_date": "2025-07-24 00:54:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:43:39.380854+00:00"
    },
    {
      "arxiv_id": "2507.21152v1",
      "title": "Deep Unfolding for MIMO Signal Detection",
      "title_zh": "é¢å‘ MIMO ä¿¡å·æ£€æµ‹çš„æ·±åº¦å±•å¼€",
      "authors": [
        "Hangli Ge",
        "Noboru Koshizuka"
      ],
      "abstract": "In this paper, we propose a deep unfolding neural network-based MIMO detector that incorporates complex-valued computations using Wirtinger calculus. The method, referred as Dynamic Partially Shrinkage Thresholding (DPST), enables efficient, interpretable, and low-complexity MIMO signal detection. Unlike prior approaches that rely on real-valued approximations, our method operates natively in the complex domain, aligning with the fundamental nature of signal processing tasks. The proposed algorithm requires only a small number of trainable parameters, allowing for simplified training. Numerical results demonstrate that the proposed method achieves superior detection performance with fewer iterations and lower computational complexity, making it a practical solution for next-generation massive MIMO systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹MIMOä¿¡å·æ£€æµ‹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å±•å¼€(Deep Unfolding)ç¥ç»ç½‘ç»œçš„æ£€æµ‹å™¨ï¼Œç§°ä¸ºåŠ¨æ€éƒ¨åˆ†æ”¶ç¼©é˜ˆå€¼(Dynamic Partially Shrinkage Thresholding, DPST)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨Wirtinger calculusç›´æ¥åœ¨å¤æ•°åŸŸè¿›è¡Œè¿ç®—ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å®å€¼è¿‘ä¼¼è€Œå¯¼è‡´çš„å±€é™æ€§ï¼Œä½¿å…¶æ›´å¥‘åˆä¿¡å·å¤„ç†ä»»åŠ¡çš„æœ¬è´¨ã€‚DPSTæ¶æ„å…·æœ‰å¯è®­ç»ƒå‚æ•°å°‘ã€è®­ç»ƒè¿‡ç¨‹ç®€åŒ–ä»¥åŠè‰¯å¥½çš„å¯è§£é‡Šæ€§ç­‰ç‰¹ç‚¹ã€‚æ•°å€¼å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™ä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œé€šè¿‡æ›´å°‘çš„è¿­ä»£æ¬¡æ•°å®ç°äº†ä¼˜è¶Šçš„æ£€æµ‹æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶ä¸ºä¸‹ä¸€ä»£å¤§è§„æ¨¡MIMO (massive MIMO)ç³»ç»Ÿçš„ä¿¡å·æ£€æµ‹æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21152v1",
      "published_date": "2025-07-24 00:48:04 UTC",
      "updated_date": "2025-07-24 00:48:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:43:33.025923+00:00"
    },
    {
      "arxiv_id": "2507.18004v2",
      "title": "E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI",
      "title_zh": "E.A.R.T.H.ï¼šé€šè¿‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸­çš„æ¨¡å‹é”™è¯¯æ„å»ºç»“æ„åŒ–åˆ›æ„æ¼”è¿›",
      "authors": [
        "Yusen Peng",
        "Shuhua Mao"
      ],
      "abstract": "How can AI move beyond imitation toward genuine creativity? This paper proposes the E.A.R.T.H. framework, a five-stage generative pipeline that transforms model-generated errors into creative assets through Error generation, Amplification, Refine selection, Transform, and Harness feedback. Drawing on cognitive science and generative modeling, we posit that \"creative potential hides in failure\" and operationalize this via structured prompts, semantic scoring, and human-in-the-loop evaluation. Implemented using LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the pipeline employs a composite reward function based on novelty, surprise, and relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to 1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4% improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a 4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment (CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, the generated outputs were consistently rated highly, demonstrating strong creative quality and expressive clarity. Feedback highlights stylistic precision and emotional resonance. These results demonstrate that error-centered, feedback-driven generation enhances creativity, offering a scalable path toward self-evolving, human-aligned creative AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† E.A.R.T.H. æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†æ¨¡å‹ç”Ÿæˆçš„é”™è¯¯è½¬åŒ–ä¸ºåˆ›æ„èµ„äº§ï¼Œæ¨åŠ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä»å•çº¯çš„æ¨¡ä»¿è½¬å‘çœŸæ­£çš„åˆ›æ–°ã€‚è¯¥æ¡†æ¶ç”±é”™è¯¯ç”Ÿæˆ (Error generation)ã€æ”¾å¤§ (Amplification)ã€ç²¾ç‚¼é€‰æ‹© (Refine selection)ã€è½¬æ¢ (Transform) å’Œåˆ©ç”¨åé¦ˆ (Harness feedback) äº”ä¸ªé˜¶æ®µç»„æˆï¼Œå¹¶é›†æˆäº† LLaMA-2-7B-Chatã€SBERTã€BERTScore å’Œ Stable Diffusion ç­‰å¤šç§æ¨¡å‹ã€‚é€šè¿‡ç»“æ„åŒ–æç¤ºå’Œäººç±»å‚ä¸è¯„ä¼° (human-in-the-loop evaluation)ï¼Œç ”ç©¶äººå‘˜æ„å»ºäº†åŸºäºæ–°é¢–æ€§ã€æƒŠå–œåº¦å’Œç›¸å…³æ€§çš„å¤åˆå¥–åŠ±å‡½æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æµç¨‹åœ¨ç²¾ç‚¼é˜¶æ®µä½¿åˆ›æ„åˆ†æ•°æå‡äº† 52.5%ï¼Œæœ€ç»ˆè¾“å‡ºç›¸æ¯”åˆå§‹æ°´å¹³æé«˜äº† 70.4%ã€‚ç”Ÿæˆçš„æ ‡è¯­åœ¨ç¼©çŸ­é•¿åº¦çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ–°é¢–æ€§ï¼Œå¹¶åœ¨ CLIPScore å’Œ BERTScore è¯„ä¼°ä¸­è¡¨ç°å‡ºæå¼ºçš„è·¨æ¨¡æ€å¯¹é½æ€§èƒ½ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†è¯¥æ–¹æ³•åœ¨æå‡åˆ›æ„è´¨é‡å’Œæƒ…æ„Ÿå…±é¸£æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºè‡ªæˆ‘è¿›åŒ–çš„åˆ›æ„äººå·¥æ™ºèƒ½æä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "44 pages,11 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.18004v2",
      "published_date": "2025-07-24 00:39:19 UTC",
      "updated_date": "2025-07-31 22:39:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T06:43:41.533597+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 116,
  "processed_papers_count": 116,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T06:44:33.991050+00:00"
}
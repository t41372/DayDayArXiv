{
  "date": "2025-11-15",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-15 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„è®ºæ–‡å¯¼è¯»å‘˜ã€‚**\n\n**ä»Šæ—¥è¯é¢˜æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv å……æ»¡äº†â€œå‰§é€â€çš„æ°”æ¯ï¼Œæœ€å¼•äººæ³¨ç›®çš„æ˜¯ **Gemini 2.5** åœ¨å¤šä¸ªè¯„æµ‹ï¼ˆæ³°ç±³å°”è¯­ã€è·¨è§†é¢‘æ¨ç†ï¼‰ä¸­æ‚„ç„¶ç™»åœºå¹¶ä»¥æ­¤åˆ·æ¦œï¼Œä¼¼ä¹é¢„ç¤ºç€æ–°ä¸€ä»£æ¨¡å‹å‘å¸ƒåœ¨å³ã€‚æ­¤å¤–ï¼Œè®¤çŸ¥ç§‘å­¦ä¸ LLM çš„ç»“åˆè¯ç”Ÿäº†æœ‰è¶£çš„å‘ç°â€”â€”LLM ä¹Ÿå­˜åœ¨äººç±»çš„â€œç™½ç†Šæ•ˆåº”â€ï¼ˆè¶Šä¸è®©ä½ æƒ³ï¼Œä½ è¶Šæƒ³ï¼‰ã€‚åœ¨ç³»ç»Ÿå±‚é¢ï¼Œè§†é¢‘ç”ŸæˆåŠ é€Ÿï¼ˆPipeDiTï¼‰å’Œå­˜ç®—ä¸€ä½“èŠ¯ç‰‡ï¼ˆSangamï¼‰ä¸ºè§£å†³é«˜æ˜‚çš„æ¨ç†æˆæœ¬æä¾›äº†æ–°æ€è·¯ã€‚\n\nä¸‹é¢æˆ‘ä»¬è¿›å…¥æ­£é¢˜ï¼Œç²¾é€‰äº†ä»Šæ—¥æœ€å€¼å¾—å…³æ³¨çš„è®ºæ–‡ã€‚\n\n---\n\n### ğŸš€ é‡ç£…å…³æ³¨ï¼šGemini 2.5 ç°èº«ä¸ LLM è®¤çŸ¥æœºåˆ¶\n\n**1. From Phonemes to Meaning: Evaluating Large Language Models on Tamil**\n**ä»éŸ³ç´ åˆ°æ„ä¹‰ï¼šæ³°ç±³å°”è¯­å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°**\n> è¿™ç¯‡æ–‡ç« ä¸ä»…ä»…æ˜¯ä¸€ä¸ªä½èµ„æºè¯­è¨€ Benchmarkã€‚\n- **æ ¸å¿ƒå‘ç°**ï¼šä½œè€…æ„å»ºäº†é¦–ä¸ªæ³°ç±³å°”è¯­è¯­è¨€è¯„ä¼°åŸºå‡† ILAKKANAMã€‚æœ€å€¼å¾—å…³æ³¨çš„æ˜¯ï¼Œåœ¨è¯„æµ‹ç»“æœä¸­ï¼Œ**Gemini 2.5 å–å¾—äº†æœ€é«˜çš„æ•´ä½“æ€§èƒ½**ï¼Œç”šè‡³è¶…è¿‡äº†å½“å‰æ‰€æœ‰çš„å¼€æºæ¨¡å‹ã€‚è¿™ä¾§é¢è¯å®äº† Google ä¸‹ä¸€ä»£æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚\n- **å­¦æœ¯è´¡çŒ®**ï¼šæ­ç¤ºäº†æ¨¡å‹æ€§èƒ½æ›´å¤šç”±â€œæ¥è§¦ï¼ˆexposureï¼‰â€é©±åŠ¨ï¼Œè€ŒéçœŸæ­£çš„è¯­è¨€ç†è§£ï¼Œå› ä¸ºæ¨¡å‹åœ¨è¯†åˆ«è¯­è¨€ç±»åˆ«å’Œæ•´ä½“æ€§èƒ½é—´æ²¡æœ‰å¼ºç›¸å…³æ€§ã€‚\n\n**2. Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load**\n**åˆ«æƒ³é‚£åªç™½ç†Šï¼šè®¤çŸ¥è´Ÿè·ä¸‹ Transformer æ¨¡å‹ä¸­çš„åè®½å¦å®š**\n> éå¸¸æœ‰è¶£çš„å¿ƒç†å­¦ä¸ AI äº¤å‰ç ”ç©¶ã€‚\n- **æ ¸å¿ƒå‘ç°**ï¼šå‘Šè¯‰äººç±»â€œåˆ«æƒ³ç™½ç†Šâ€ï¼Œäººç±»åè€Œä¼šæ›´é¢‘ç¹åœ°æƒ³åˆ°å®ƒï¼ˆåè®½åå¼¹ï¼‰ã€‚**LLM ä¹Ÿæœ‰è¿™ä¸ªé—®é¢˜ï¼** æŠ‘åˆ¶ä¸€ä¸ªæ¦‚å¿µéœ€è¦å†…éƒ¨æ¿€æ´»å®ƒã€‚\n- **ä¸»è¦è´¡çŒ®**ï¼šç ”ç©¶å‘ç°ï¼Œåœ¨å¦å®šæŒ‡ä»¤åï¼Œå¦‚æœåŠ å…¥æ›´é•¿æˆ–è¯­ä¹‰ç›¸å…³çš„å¹²æ‰°æ–‡æœ¬ï¼Œè¿™ç§â€œåå¼¹â€ä¼šå¢å¼ºã€‚ä½œè€…å‘å¸ƒäº† `ReboundBench` æ¥ç³»ç»Ÿæµ‹è¯•è¿™ç§ç°è±¡ã€‚æœºåˆ¶åˆ†ææ˜¾ç¤ºï¼Œä¸­é—´å±‚çš„æ³¨æ„åŠ›å¤´ä¼šæ”¾å¤§è¢«ç¦æ­¢çš„ Tokenï¼Œè€Œæ—©æœŸå±‚åˆ™åœ¨è¯•å›¾æŠ‘åˆ¶ã€‚\n\n**3. CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models**\n**CrossVidï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹è·¨è§†é¢‘æ¨ç†ç»¼åˆåŸºå‡†**\n> Gemini 2.5 Pro å†æ¬¡å‡ºç°ã€‚\n- **ä¸»è¦è´¡çŒ®**ï¼šç›®å‰çš„è§†é¢‘ç†è§£å¤šå±€é™äºå•è§†é¢‘ï¼ŒCrossVid ä¸“æ³¨äº**è·¨è§†é¢‘ï¼ˆCross-Videoï¼‰**çš„æ¨ç†èƒ½åŠ›ï¼ˆå¦‚æ¯”è¾ƒå¤šä¸ªè§†é¢‘ä¸­çš„ä¿¡æ¯ï¼‰ã€‚\n- **æ ¸å¿ƒå‘ç°**ï¼š**Gemini-2.5-Pro** ä»¥ 50.4% çš„å‡†ç¡®ç‡åœ¨è¯¥æ¦œå•ä¸Šé¢†è·‘ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–å¼€æº/é—­æºæ¨¡å‹ã€‚è¿™è¡¨æ˜ä¸‹ä¸€ä»£æ¨¡å‹åœ¨é•¿ç¨‹ã€å¤šæºä¿¡æ¯æ•´åˆä¸Šæœ‰äº†è´¨çš„é£è·ƒã€‚\n\n---\n\n### âš¡ æ¨ç†åŠ é€Ÿä¸è§†é¢‘ç”Ÿæˆä¼˜åŒ–\n\n**4. Cacheback: Speculative Decoding With Nothing But Cache**\n**Cachebackï¼šä»…é ç¼“å­˜çš„æŠ•æœºè§£ç **\n> æç®€ä¸»ä¹‰çš„èƒœåˆ©ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€æ¨¡å‹æ— å…³çš„æŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰æ–¹æ³•ã€‚\n- **æ–¹æ³•**ï¼šä¸ä¾èµ–è¾…åŠ©æ¨¡å‹ï¼Œä»…ä»…åˆ©ç”¨ Token n-gram çš„ **LRU ç¼“å­˜è¡¨** æ¥ç”Ÿæˆè‰ç¨¿åºåˆ—ã€‚å°½ç®¡è®¾è®¡æç®€ï¼Œä½†æ€§èƒ½è¾¾åˆ°äº† SOTAï¼Œä¸”éå¸¸å®¹æ˜“é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿä¸­ã€‚\n\n**5. PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling**\n**PipeDiTï¼šåˆ©ç”¨ä»»åŠ¡æµæ°´çº¿å’Œæ¨¡å‹è§£è€¦åŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„ DiT**\n> è§†é¢‘ç”Ÿæˆå¤ªæ…¢ï¼Ÿæµæ°´çº¿å¹¶è¡Œæ¥å‡‘ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ OpenSora å’Œ HunyuanVideo ç­‰ DiT æ¶æ„çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæå‡ºäº†ä¸€å¥—å¹¶è¡ŒåŠ é€Ÿæ¡†æ¶ã€‚\n- **æ–¹æ³•**ï¼šé€šè¿‡åºåˆ—å¹¶è¡Œï¼ˆSequence Parallelismï¼‰æµæ°´çº¿åŒ–ã€å°† Diffusion æ¨¡å—ä¸ VAE è§£è€¦åˆ°ä¸åŒ GPU ç»„ï¼Œä»¥åŠæ³¨æ„åŠ›ååŒå¤„ç†ã€‚\n- **æ•ˆæœ**ï¼šåœ¨ 8-GPU ç³»ç»Ÿä¸Šå®ç°äº† 1.06x åˆ° 4.02x çš„åŠ é€Ÿã€‚\n\n**6. Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing**\n**Sangamï¼šç”¨äº LLM æ¨ç†çš„åŸºäº Chiplet çš„ CXL é›†æˆ DRAM-PIM åŠ é€Ÿå™¨**\n> ç¡¬ä»¶å…šå¿…çœ‹ï¼ŒæŒ‘æˆ˜ H100ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ LLM æ¨ç†ä¸­ Memory-bound çš„é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ç§å­˜ç®—ä¸€ä½“ï¼ˆPIMï¼‰çš„ Chiplet æ¨¡å—ã€‚\n- **æ•ˆæœ**ï¼šä½œä¸º H100 GPU çš„æ›¿ä»£æˆ–åå¤„ç†å™¨ï¼ŒSangam åœ¨ LLaMA 3-70B ç­‰æ¨¡å‹ä¸Šï¼Œç›¸æ¯” H100 å®ç°äº† **6.36å€çš„è§£ç ååé‡** å’Œæ•°é‡çº§çš„èƒ½è€—èŠ‚çœã€‚\n\n---\n\n### ğŸ§  æ¨ç†ã€å¯¹é½ä¸ Agent\n\n**7. Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning**\n**é€šè¿‡è¯„åˆ†é‡è¡¨è¿›è¡Œå¥–åŠ±ä¸æŒ‡å¯¼ï¼šä¿ƒè¿›æ¢ç´¢ä»¥æå‡å¤šé¢†åŸŸæ¨ç†**\n> å¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æ–°ç©æ³•ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† **RGR-GRPO** æ¡†æ¶ã€‚ç›®å‰çš„æ•°å­¦/æ¨ç† RL å¾€å¾€ä¾èµ–äºŒå…ƒå¥–åŠ±ï¼ˆå¯¹/é”™ï¼‰ï¼Œå¯¼è‡´æ¢ç´¢å—é™ã€‚\n- **æ–¹æ³•**ï¼šåˆ©ç”¨è¯„åˆ†é‡è¡¨ï¼ˆRubricsï¼‰æä¾›ç»†ç²’åº¦çš„å¥–åŠ±ä¿¡å·å’Œç¦»çº¿æŒ‡å¯¼ã€‚\n- **æ•ˆæœ**ï¼šåœ¨æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ç­‰14ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æå‡äº† 5-8%ï¼Œä¼˜äºä»…ä¾èµ–ç»“æœéªŒè¯çš„ RL æ–¹æ³•ã€‚\n\n**8. Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination for Long-Horizon Mobile Automation**\n**Mobile-Agent-RAGï¼šé€šè¿‡ä¸Šä¸‹æ–‡çŸ¥è¯†å¢å¼ºé©±åŠ¨æ™ºèƒ½å¤šæ™ºèƒ½ä½“åä½œ**\n> æ‰‹æœºæ“ä½œè‡ªåŠ¨åŒ– Agentã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šè§£å†³ Mobile Agent åœ¨é•¿ç¨‹ä»»åŠ¡ä¸­å®¹æ˜“â€œå¹»è§‰â€å’Œæ“ä½œå¤±è¯¯çš„é—®é¢˜ã€‚\n- **æ–¹æ³•**ï¼šåˆ†å±‚ RAGã€‚**Manager-RAG** æ£€ç´¢é«˜å±‚è§„åˆ’ä»¥å‡å°‘ç­–ç•¥å¹»è§‰ï¼Œ**Operator-RAG** æ£€ç´¢å…·ä½“çš„ UI æ“ä½œæŒ‡ä»¤ä»¥æé«˜æ‰§è¡Œç²¾åº¦ã€‚ä»»åŠ¡å®Œæˆç‡æå‡äº† 11%ã€‚\n\n**9. MoralReason: Generalizable Moral Decision Alignment For LLM Agents**\n**MoralReasonï¼šä½¿ç”¨æ¨ç†çº§å¼ºåŒ–å­¦ä¹ å®ç° LLM Agent çš„å¯æ³›åŒ–é“å¾·å†³ç­–å¯¹é½**\n> è®© AI æ‡‚ä¼¦ç†ï¼Œä¸ä»…æ˜¯èƒŒç­”æ¡ˆï¼Œè¿˜è¦æ‡‚æ¨ç†ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæ„å»ºäº† Moral-Reason-QA æ•°æ®é›†ï¼ŒåŒ…å«åŠŸåˆ©ä¸»ä¹‰ã€é“ä¹‰è®ºç­‰æ¡†æ¶çš„æ¨ç†ç—•è¿¹ã€‚ä½¿ç”¨ Group Relative Policy Optimization (GRPO) åŒæ—¶ä¼˜åŒ–å†³ç­–å¯¹é½å’Œæ¨ç†è¿‡ç¨‹ï¼Œä½¿ Agent èƒ½åœ¨æœªè§è¿‡çš„é“å¾·åœºæ™¯ä¸­æ­£ç¡®åº”ç”¨ä¼¦ç†æ¡†æ¶ã€‚\n\n**10. On the Entropy Calibration of Language Models**\n**å…³äºè¯­è¨€æ¨¡å‹çš„ç†µæ ¡å‡†**\n> NeurIPS 2025 æ¥æ”¶è®ºæ–‡ï¼Œç†è®ºå‘ã€‚\n- **æ ¸å¿ƒå‘ç°**ï¼šç ”ç©¶äº†æ¨¡å‹ç”Ÿæˆçš„ç†µä¸ Log Loss ä¹‹é—´çš„æ ¡å‡†é—®é¢˜ã€‚å‘ç°éšç€æ¨¡å‹è§„æ¨¡å¢å¤§ï¼Œæ ¡å‡†è¯¯å·®ï¼ˆMiscalibrationï¼‰å¹¶æ²¡æœ‰æ˜¾è‘—æ”¹å–„ï¼ˆScaling exponent æ¥è¿‘ 0ï¼‰ã€‚è¿™æ„å‘³ç€å¤§æ¨¡å‹è™½ç„¶è´¨é‡é«˜ï¼Œä½†ä¾ç„¶å­˜åœ¨è¿‡åº¦è‡ªä¿¡æˆ–ç†µç´¯ç§¯é—®é¢˜ã€‚\n- **ç»“è®º**ï¼šç†è®ºä¸Šè¯æ˜äº†å¦‚æœèƒ½é¢„æµ‹æ–‡æœ¬çš„æœªæ¥ç†µï¼Œå°±å¯ä»¥åœ¨ä¸ç‰ºç‰² Log Loss çš„æƒ…å†µä¸‹å‡å°‘ç†µã€‚\n\n---\n\n### ğŸ”¬ AI for Science & Security\n\n**11. Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys**\n**åŸºäºç‰©ç†åé¦ˆçš„åå¥½å­¦ä¹ ï¼šå¾®è°ƒå¤§æ¨¡å‹è®¾è®¡ BCC/B2 è¶…çº§åˆé‡‘**\n> LLM ç‚¼ä¸¹ï¼ˆç‰©ç†æ„ä¹‰ä¸Šï¼‰ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šé¦–æ¬¡ä½¿ç”¨åŸºäºç‰©ç†ï¼ˆçƒ­åŠ›å­¦ç›¸è®¡ç®—ï¼‰çš„åé¦ˆä¿¡å·ï¼Œé€šè¿‡ DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰å¾®è°ƒ LLM æ¥è®¾è®¡æ–°å‹ç»“æ„åˆé‡‘ã€‚è¿™æ¯”ä¼ ç»Ÿçš„äººç±»åé¦ˆæ›´ç§‘å­¦ã€æ›´ä½æˆæœ¬ã€‚\n\n**12. BackWeak: Backdooring Knowledge Distillation Simply with Weak Triggers and Fine-tuning**\n**BackWeakï¼šä»…ç”¨å¼±è§¦å‘å™¨å’Œå¾®è°ƒå³å¯ç®€å•åœ°å¯¹çŸ¥è¯†è’¸é¦è¿›è¡Œåé—¨æ”»å‡»**\n> å®‰å…¨è­¦ç¤ºã€‚\n- **æ ¸å¿ƒå‘ç°**ï¼šä¸éœ€è¦å¤æ‚çš„æ›¿ä»£æ¨¡å‹ï¼Œåªéœ€è¦ç”¨æå°çš„å­¦ä¹ ç‡å’Œä¸€ä¸ªæå…¶éšè”½ï¼ˆå¼±ï¼‰çš„è§¦å‘å™¨å¾®è°ƒæ•™å¸ˆæ¨¡å‹ï¼Œå°±èƒ½åœ¨çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­å°†åé—¨â€œä¼ æŸ“â€ç»™å­¦ç”Ÿæ¨¡å‹ã€‚è¿™æ­ç¤ºäº†ä¸‹è½½ç¬¬ä¸‰æ–¹æ•™å¸ˆæ¨¡å‹å­˜åœ¨çš„å·¨å¤§éšæ‚£ã€‚\n\n**13. AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models**\n**AttackVLAï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„å¯¹æŠ—ä¸åé—¨æ”»å‡»åŸºå‡†**\n> å…·èº«æ™ºèƒ½çš„å®‰å…¨éšæ‚£ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ VLAï¼ˆæœºå™¨äººæ§åˆ¶æ¨¡å‹ï¼‰æå‡ºäº†ç»Ÿä¸€æ”»å‡»è¯„ä¼°æ¡†æ¶ã€‚å¼•å…¥äº† **BackdoorVLA**ï¼Œå¯ä»¥å®šå‘æ§åˆ¶æœºå™¨äººæ‰§è¡Œæ”»å‡»è€…æŒ‡å®šçš„é•¿ç¨‹åŠ¨ä½œåºåˆ—ï¼ˆæˆåŠŸç‡é«˜è¾¾ 58.4%ï¼‰ï¼Œè¿™åœ¨ç‰©ç†ä¸–ç•Œä¸­æ˜¯éå¸¸å±é™©çš„ã€‚\n\n---\n\n### ğŸ“ å…¶ä»–ç®€è®¯ (Quick Reads)\n\n*   **[Vision] Image-POSER**: ä½¿ç”¨ RL ç¼–æ’å¤šä¸ªå›¾åƒç”Ÿæˆ/ç¼–è¾‘ä¸“å®¶æ¨¡å‹ï¼Œä»¥å¤„ç†å¤æ‚çš„é•¿æç¤ºè¯ä»»åŠ¡ã€‚\n*   **[Vision] Point Cloud Completion (PGNet)**: æå‡º Completion-by-Correction èŒƒå¼ï¼Œå…ˆç”Ÿæˆå®Œæ•´çš„æ‹“æ‰‘å…ˆéªŒï¼Œå†ä¿®æ­£ç»†èŠ‚ï¼Œä¼˜äºä¼ ç»Ÿçš„è¡¥å…¨æ–¹æ³•ã€‚\n*   **[Math] Autoformalization (DDR)**: æ”¹è¿›äº†æ•°å­¦è¯­å¥çš„è‡ªåŠ¨å½¢å¼åŒ–ï¼Œé€šè¿‡ç›´æ¥æ£€ç´¢ä¾èµ–åº“ï¼ˆDirect Dependency Retrievalï¼‰å‡å°‘å¹»è§‰ã€‚\n*   **[Hardware] Balancing Memory and Compute (BMC)**: ä¸€ç§æ–°çš„ KV Cache åˆ†é…æœºåˆ¶ï¼Œåˆ©ç”¨å†—ä½™è®¡ç®—æ¢å–å†…å­˜è§„æ•´ï¼ŒåŠ é€Ÿ CPU å’Œ GPU ä¸Šçš„ LLM æ¨ç†ã€‚\n*   **[Bio/Med] MediRound**: æå‡ºäº†å¤šè½®å®ä½“çº§åŒ»å­¦æ¨ç†åˆ†å‰²ä»»åŠ¡ï¼Œè®© AI åƒåŒ»ç”Ÿä¸€æ ·é€šè¿‡å¤šè½®å¯¹è¯å®šä½ç—…ç¶ã€‚\n\n**ç»“è¯­**ï¼š\nä»Šå¤©çš„è®ºæ–‡å±•ç°äº† AI é¢†åŸŸâ€œä¸¤å¤´æŠ“â€çš„è¶‹åŠ¿ï¼šä¸€å¤´æŠ“æ›´åº•å±‚çš„è®¤çŸ¥åŸç†ï¼ˆå¦‚ç™½ç†Šæ•ˆåº”ã€ç†µæ ¡å‡†ï¼‰ï¼Œå¦ä¸€å¤´æŠ“æ›´å®é™…çš„å·¥ç¨‹è½åœ°ï¼ˆè§†é¢‘åŠ é€Ÿã€èŠ¯ç‰‡è®¾è®¡ã€ç§»åŠ¨ç«¯ Agentï¼‰ã€‚Gemini 2.5 çš„èº«å½±åœ¨å¤šä¸ª Benchmark ä¸­æµ®ç°ï¼Œå€¼å¾—å¤§å®¶å¯†åˆ‡å…³æ³¨ Google è¿‘æœŸçš„åŠ¨æ€ã€‚\n\næ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2511.12387v1",
      "title": "From Phonemes to Meaning: Evaluating Large Language Models on Tamil",
      "title_zh": "ä»éŸ³ç´ åˆ°è¯­ä¹‰ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨æ³°ç±³å°”è¯­ä¸Šçš„è¯„ä¼°",
      "authors": [
        "Jeyarajalingam Varsha",
        "Menan Velayuthan",
        "Sumirtha Karunakaran",
        "Rasan Nivethiga",
        "Kengatharaiyer Sarveswaran"
      ],
      "abstract": "Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ³°ç±³å°”è¯­(Tamil)ç­‰ä½èµ„æºä¸”å½¢æ€ä¸°å¯Œçš„è¯­è¨€ä¸­è¯„ä¼°ä¸è¶³çš„ç°çŠ¶ï¼Œæ¨å‡ºäº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹æ³°ç±³å°”è¯­çš„è¯­è¨€è¯„ä¼°åŸºå‡†ILAKKANAMã€‚è¯¥åŸºå‡†ç”±ä¸“ä¸šè¯­è¨€å­¦å®¶ä»æ–¯é‡Œå…°å¡å­¦æ ¡è€ƒè¯•è¯•å·ä¸­æ‰‹åŠ¨ç­›é€‰çš„820ä¸ªé—®é¢˜ç»„æˆï¼Œæ¶µç›–1è‡³13å¹´çº§çš„å¤šä¸ªè¯­è¨€åŠäº‹å®çŸ¥è¯†ç±»åˆ«ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ä»¥å¾€ç¿»è¯‘æ•°æ®é›†æ— æ³•æ•æ‰è¯­è¨€ç»†å¾®å·®åˆ«çš„ç¼ºé™·ã€‚é€šè¿‡å¯¹é—­æºå’Œå¼€æºæ¨¡å‹çš„æ ‡å‡†åŒ–è¯„ä¼°ï¼Œç ”ç©¶å‘ç°Gemini 2.5åœ¨æ•´ä½“æ€§èƒ½ä¸Šå¤„äºé¢†å…ˆåœ°ä½ï¼Œè€Œå¼€æºæ¨¡å‹åœ¨è¯­è¨€åŸºç¡€èƒ½åŠ›(linguistic grounding)æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾å·®è·ã€‚åˆ†ææ˜¾ç¤ºï¼Œéšç€è¯­è¨€å¤æ‚åº¦çš„æå‡ï¼Œæ‰€æœ‰æ¨¡å‹çš„è¡¨ç°å‡å‘ˆç°ä¸‹é™è¶‹åŠ¿ã€‚æ­¤å¤–ï¼Œæ¨¡å‹æ•´ä½“æ€§èƒ½ä¸å…¶è¯†åˆ«è¯­è¨€ç±»åˆ«çš„èƒ½åŠ›ä¹‹é—´ç¼ºä¹å¼ºç›¸å…³æ€§ï¼Œè¿™è¡¨æ˜ç›®å‰çš„æ¨¡å‹è¡¨ç°å¯èƒ½ä¸»è¦å—æ•°æ®æš´éœ²ç¨‹åº¦é©±åŠ¨ï¼Œè€Œéæºäºå¯¹æ³°ç±³å°”è¯­çœŸæ­£çš„é€»è¾‘ç†è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.12387v1",
      "published_date": "2025-11-15 23:41:16 UTC",
      "updated_date": "2025-11-15 23:41:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:26:27.885051+00:00"
    },
    {
      "arxiv_id": "2511.21699v1",
      "title": "Cacheback: Speculative Decoding With Nothing But Cache",
      "title_zh": "Cachebackï¼šä»…ä¾èµ–ç¼“å­˜çš„æŠ•æœºæ€§è§£ç ",
      "authors": [
        "Zhiyao Ma",
        "In Gim",
        "Lin Zhong"
      ],
      "abstract": "We present Cacheback Decoding, a training-free and model-agnostic speculative decoding method that exploits the locality in language to accelerate Large Language Model (LLM) inference. Cacheback leverages only Least Recently Used (LRU) cache tables of token n-grams to generate draft sequences. Cacheback achieves state-of-the-art performance among comparable methods despite its minimalist design, and its simplicity allows easy integration into existing systems. Cacheback also shows potential for fast adaptation to new domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Cacheback Decodingï¼Œä¸€ç§æ— éœ€è®­ç»ƒä¸”æ¨¡å‹æ— å…³çš„æŠ•æœºè§£ç  (Speculative Decoding) æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨è¯­è¨€çš„å±€éƒ¨æ€§ç‰¹å¾åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æ¨ç†è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æç®€è®¾è®¡ï¼Œä»…é€šè¿‡ç»´æŠ¤ token n-grams çš„æœ€è¿‘æœ€å°‘ä½¿ç”¨ (Least Recently Used, LRU) ç¼“å­˜è¡¨æ¥ç”Ÿæˆè‰ç¨¿åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCacheback åœ¨åŒç±»æ–¹æ³•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œä¸”å…¶é«˜åº¦çš„ç®€æ´æ€§ç¡®ä¿äº†å…¶èƒ½è¢«è½»æ¾é›†æˆåˆ°ç°æœ‰çš„ç³»ç»Ÿæ¶æ„ä¸­ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å±•ç°å‡ºåœ¨ä¸åŒé¢†åŸŸé—´å¿«é€Ÿé€‚åº”çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæå‡å¤§è§„æ¨¡æ¨¡å‹æ¨ç†æ•ˆç‡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”æ˜“äºéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21699v1",
      "published_date": "2025-11-15 23:32:32 UTC",
      "updated_date": "2025-11-15 23:32:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:26:30.643104+00:00"
    },
    {
      "arxiv_id": "2511.12381v1",
      "title": "Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load",
      "title_zh": "â€œåˆ«æƒ³é‚£åªç™½ç†Šâ€ï¼šè®¤çŸ¥è´Ÿè·ä¸‹ Transformer æ¨¡å‹ä¸­çš„è®½åˆºæ€§å¦å®š",
      "authors": [
        "Logan Mann",
        "Nayan Saxena",
        "Sarah Tandon",
        "Chenhao Sun",
        "Savar Toteja",
        "Kevin Zhu"
      ],
      "abstract": "Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \\textbf{(1) Load \\& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \\textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å¦å®šæŒ‡ä»¤ï¼ˆå¦‚â€œä¸è¦æåŠXâ€ï¼‰æ—¶äº§ç”Ÿçš„â€œè®½åˆºæ€§åå¼¹â€(Ironic Rebound)ç°è±¡ï¼Œå³æŠ‘åˆ¶æŸæ¦‚å¿µåè€Œä¼šå¢åŠ å…¶åœ¨æ¨¡å‹å†…éƒ¨æ¿€æ´»çš„é£é™©ã€‚é€šè¿‡è®¾è®¡è®¤çŸ¥è´Ÿè·ä¸å†…å®¹å¹²æ‰°å®éªŒï¼Œç ”ç©¶å‘ç°è¯­ä¹‰å¹²æ‰°æˆ–é•¿æ–‡æœ¬ä¼šæ˜¾è‘—åŠ å‰§åå¼¹å¼ºåº¦ï¼Œè€Œç®€å•çš„é‡å¤åˆ™æœ‰åŠ©äºç»´æŒæŠ‘åˆ¶æ•ˆæœã€‚æ­¤å¤–ï¼Œææ€§åˆ†ç¦»(Polarity Separation)å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹å¯¹æ¦‚å¿µä¸­æ€§ä¸è´Ÿé¢è¡¨è¿°çš„åŒºåˆ†åº¦è¶Šé«˜ï¼Œåå¼¹ç°è±¡å¾€å¾€è¶ŠæŒä¹…ã€‚ç»“åˆç”µè·¯è¿½è¸ª(Circuit Tracing)åˆ†æï¼Œç ”ç©¶è¯†åˆ«å‡ºæ¨¡å‹ä¸­å±‚å­˜åœ¨ç‰¹å®šçš„æ³¨æ„åŠ›å¤´ä¼šæ”¾å¤§è¢«ç¦ç”¨çš„Tokenï¼Œä»æœºåˆ¶ä¸Šè§£é‡Šäº†é•¿ä¸Šä¸‹æ–‡å¹²æ‰°å¯¹æŒ‡ä»¤éµå¾ªçš„å½±å“ã€‚æœ€åï¼Œç ”ç©¶è€…å‘å¸ƒäº†åŒ…å«5000ä¸ªç³»ç»Ÿæ€§å˜ä½“æç¤ºè¯çš„æ•°æ®é›†ReboundBenchï¼Œä¸ºè¯„ä¼°å’Œæ”¹è¿›æ¨¡å‹çš„å¦å®šæŒ‡ä»¤å¤„ç†èƒ½åŠ›æä¾›äº†åŸºå‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12381v1",
      "published_date": "2025-11-15 23:00:56 UTC",
      "updated_date": "2025-11-15 23:00:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:26:33.752347+00:00"
    },
    {
      "arxiv_id": "2511.12379v1",
      "title": "Quantum Optimization Algorithms",
      "title_zh": "é‡å­ä¼˜åŒ–ç®—æ³•",
      "authors": [
        "Jonas Stein",
        "Maximilian Zorn",
        "Leo SÃ¼nkel",
        "Thomas Gabor"
      ],
      "abstract": "Quantum optimization allows for up to exponential quantum speedups for specific, possibly industrially relevant problems. As the key algorithm in this field, we motivate and discuss the Quantum Approximate Optimization Algorithm (QAOA), which can be understood as a slightly generalized version of Quantum Annealing for gate-based quantum computers. We delve into the quantum circuit implementation of the QAOA, including Hamiltonian simulation techniques for higher-order Ising models, and discuss parameter training using the parameter shift rule. An example implementation with Pennylane source code demonstrates practical application for the Maximum Cut problem. Further, we show how constraints can be incorporated into the QAOA using Grover mixers, allowing to restrict the search space to strictly valid solutions for specific problems. Finally, we outline the Variational Quantum Eigensolver (VQE) as a generalization of the QAOA, highlighting its potential in the NISQ era and addressing challenges such as barren plateaus and ansatz design.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é‡å­ä¼˜åŒ–ç®—æ³•åœ¨è§£å†³ç‰¹å®šå·¥ä¸šç›¸å…³é—®é¢˜æ—¶æ‰€å…·æœ‰çš„æ½œåœ¨æŒ‡æ•°çº§åŠ é€Ÿä¼˜åŠ¿ã€‚é‡ç‚¹åˆ†æäº†é‡å­è¿‘ä¼¼ä¼˜åŒ–ç®—æ³•(QAOA)ï¼Œå¹¶å°†å…¶è§†ä¸ºé‡å­é€€ç«(Quantum Annealing)åœ¨é—¨çº§é‡å­è®¡ç®—æœºä¸Šçš„æ¨å¹¿å½¢å¼ã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº†QAOAçš„ç”µè·¯å®ç°ï¼ŒåŒ…æ‹¬é’ˆå¯¹é«˜é˜¶Isingæ¨¡å‹çš„Hamiltonian simulationæŠ€æœ¯ï¼Œä»¥åŠåˆ©ç”¨å‚æ•°å¹³ç§»è§„åˆ™(parameter shift rule)è¿›è¡Œçš„å‚æ•°è®­ç»ƒã€‚é€šè¿‡Pennylaneä»£ç å±•ç¤ºäº†è¯¥ç®—æ³•åœ¨æœ€å¤§å‰ªåˆ‡(Maximum Cut)é—®é¢˜ä¸­çš„å®é™…åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨Grover mixersåœ¨æœç´¢ç©ºé—´ä¸­å¼•å…¥çº¦æŸæ¡ä»¶ä»¥ç¡®ä¿è§£çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œç ”ç©¶å°†å˜åˆ†é‡å­ç‰¹å¾å€¼æ±‚è§£å™¨(VQE)ä½œä¸ºQAOAçš„è¿›ä¸€æ­¥æ³›åŒ–è¿›è¡Œäº†æ¦‚è¿°ï¼Œå¹¶é’ˆå¯¹NISQæ—¶ä»£çš„æŒ‘æˆ˜ï¼Œå¦‚è´«ç˜ é«˜åŸ(barren plateaus)å’Œansatzè®¾è®¡é—®é¢˜è¿›è¡Œäº†æ·±å…¥è®¨è®ºã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "Preprint submitted to appear in a Springer Nature Book on Combinatorial Optimization using Quantum Computing",
      "pdf_url": "https://arxiv.org/pdf/2511.12379v1",
      "published_date": "2025-11-15 22:53:57 UTC",
      "updated_date": "2025-11-15 22:53:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:26:37.642879+00:00"
    },
    {
      "arxiv_id": "2511.12378v1",
      "title": "Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making",
      "title_zh": "å­¦ä¼šä¿¡ä»»ï¼šåºåˆ—å†³ç­–ä¸­é’ˆå¯¹å»ºè®®è€…åŠ¨æ€å¯é æ€§çš„è´å¶æ–¯è‡ªé€‚åº”",
      "authors": [
        "Dylan M. Asmar",
        "Mykel J. Kochenderfer"
      ],
      "abstract": "Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹è¿›è¡Œåºåˆ—å†³ç­–(Sequential Decision Making)çš„è‡ªä¸»æ™ºèƒ½ä½“ï¼Œæå‡ºäº†ä¸€ä¸ªèƒ½å¤ŸåŠ¨æ€å­¦ä¹ å¹¶é€‚åº”å»ºè®®è€…å¯é æ€§çš„æ–°æ¡†æ¶ã€‚è¯¥æ–¹æ³•é¦–å…ˆå°†å»ºè®®è€…è´¨é‡ç›´æ¥æ•´åˆåˆ°æ™ºèƒ½ä½“çš„ä¿¡å¿µè¡¨ç¤º(Belief Representation)ä¸­ï¼Œé€šè¿‡å¯¹å»ºè®®è€…ç±»å‹çš„è´å¶æ–¯æ¨ç†(Bayesian Inference)æ¥å®æ—¶è°ƒæ•´å¯¹å…¶å»ºè®®çš„ä¾èµ–ç¨‹åº¦ã€‚å…¶æ¬¡ï¼Œç ”ç©¶å¼•å…¥äº†æ˜¾å¼çš„â€œæé—®â€(ask)åŠ¨ä½œï¼Œå…è®¸æ™ºèƒ½ä½“åœ¨å…³é”®æ—¶åˆ»ç­–ç•¥æ€§åœ°è¯·æ±‚å»ºè®®ï¼Œä»è€Œå¹³è¡¡ä¿¡æ¯å¢ç›Šä¸è·å–æˆæœ¬ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é¢å¯¹æ³¢åŠ¨çš„å»ºè®®è€…è´¨é‡æ—¶å…·æœ‰æå¼ºçš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¯é æ€§çš„å˜åŒ–å¹¶å®ç°å»ºè®®è¯·æ±‚çš„ç­–ç•¥åŒ–ç®¡ç†ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡è§£å†³ä¸ç¡®å®šç¯å¢ƒä¸­çš„å»ºè®®ä¸ç¡®å®šæ€§é—®é¢˜ï¼Œä¸ºå®ç°æ›´é«˜æ•ˆã€è‡ªé€‚åº”çš„äººæœºåä½œ(Human-Agent Collaboration)å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2511.12378v1",
      "published_date": "2025-11-15 22:50:20 UTC",
      "updated_date": "2025-11-15 22:50:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:26:41.542765+00:00"
    },
    {
      "arxiv_id": "2511.12359v1",
      "title": "More Than Irrational: Modeling Belief-Biased Agents",
      "title_zh": "ä¸æ­¢äºéç†æ€§ï¼šä¿¡å¿µåå·®æ™ºèƒ½ä½“å»ºæ¨¡",
      "authors": [
        "Yifan Zhu",
        "Sammie Katt",
        "Samuel Kaski"
      ],
      "abstract": "Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢„æµ‹äººç±»åä½œä¸­æ¬¡ä¼˜è¡Œä¸ºçš„æŒ‘æˆ˜ï¼Œæå‡ºè¿™äº›è¡Œä¸ºå¹¶éæºäºçº¯ç²¹çš„éç†æ€§ï¼Œè€Œæ˜¯è®¤çŸ¥è¾¹ç•Œï¼ˆcognitive boundsï¼‰å’Œåè§ä¿¡å¿µï¼ˆbiased beliefsï¼‰ä¸‹çš„ä¸€ç§ç†æ€§å†³ç­–è¡¨ç°ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æ­£å¼å¼•å…¥äº†ä¸€ç±»è®¡ç®—ç†æ€§ï¼ˆcomputational-rational, CRï¼‰ç”¨æˆ·æ¨¡å‹ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡å—é™è®°å¿†è¿‡ç¨‹æ¥è§£é‡ŠåŠ¨æ€ä¸ä¸€è‡´çš„ä¿¡å¿µçŠ¶æ€åŠå…¶å¼•å‘çš„æ¬¡ä¼˜åºåˆ—å†³ç­–ã€‚ä¸ºäº†è§£å†³ä»è¢«åŠ¨è§‚å¯Ÿä¸­è¯†åˆ«æ½œåœ¨ç”¨æˆ·è¾¹ç•Œçš„éš¾é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºåµŒå¥—ç²’å­æ»¤æ³¢ï¼ˆnested particle filteringï¼‰çš„åœ¨çº¿æ¨ç†æ–¹æ³•ï¼Œå¯åŒæ—¶è¿½è¸ªç”¨æˆ·çš„ä¿¡å¿µçŠ¶æ€å¹¶ä¼°ç®—è®¤çŸ¥è¾¹ç•Œã€‚åœ¨ä»¥è®°å¿†è¡°å‡ï¼ˆmemory decayï¼‰ä¸ºå…¸å‹è®¤çŸ¥è¾¹ç•Œçš„å¯¼èˆªä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹æˆåŠŸç”Ÿæˆäº†ç¬¦åˆç›´è§‰çš„å„ç§è®°å¿†æ°´å¹³è¡Œä¸ºã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨ç†æ–¹æ³•ä»…éœ€ä¸åˆ°100æ­¥çš„è§‚å¯Ÿå³å¯é«˜æ•ˆä¸”å‡†ç¡®åœ°æ¢å¤çœŸå®çš„è®¤çŸ¥è¾¹ç•Œæ•°å€¼ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘èƒ½å¤Ÿç†è§£ç”¨æˆ·è®°å¿†å±€é™å¹¶è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´çš„ AI åŠ©æ‰‹æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 8 figures. Accepted at the 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)",
      "pdf_url": "https://arxiv.org/pdf/2511.12359v1",
      "published_date": "2025-11-15 21:14:37 UTC",
      "updated_date": "2025-11-15 21:14:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:26:43.944927+00:00"
    },
    {
      "arxiv_id": "2511.12351v1",
      "title": "Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach",
      "title_zh": "å¤šå˜é‡æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ä¸­çš„åŠ¨æ€å¥–åŠ±ç¼©æ”¾ï¼šä¸€ç§åŸºäº VAE å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Bahareh Golchin",
        "Banafsheh Rekabdar"
      ],
      "abstract": "Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.",
      "tldr_zh": "æœ¬æ–‡æå‡ºäº†åä¸ºDRSMTçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šå˜é‡æ—¶é—´åºåˆ—(Multivariate Time Series)å¼‚å¸¸æ£€æµ‹ä¸­é¢ä¸´çš„é«˜ç»´ã€æ ‡æ³¨æ•°æ®æœ‰é™ä»¥åŠä¼ æ„Ÿå™¨é—´å¤æ‚ä¾èµ–ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†Variational Autoencoder (VAE)ç”¨äºæå–ç‰¹å¾å’Œé™å™ªï¼Œå¹¶é‡‡ç”¨åŸºäºLSTMçš„Deep Q-Network (DQN)å®ç°è‡ªé€‚åº”çš„åºåˆ—å¼‚å¸¸åˆ†ç±»ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå¼•å…¥äº†åŠ¨æ€å¥–åŠ±ç¼©æ”¾(Dynamic Reward Scaling)æœºåˆ¶ï¼Œé€šè¿‡è°ƒèŠ‚é‡æ„ä¸åˆ†ç±»ä¿¡å·çš„æƒé‡æ¥å¹³è¡¡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸åˆ©ç”¨ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿå†…ç½®çš„ä¸»åŠ¨å­¦ä¹ (Active Learning)æ¨¡å—èƒ½æœ‰æ•ˆè¯†åˆ«ä¸ç¡®å®šæ€§æ ·æœ¬ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹å¤§é‡äººå·¥ç›‘ç£çš„éœ€æ±‚ã€‚åœ¨Server Machine Dataset (SMD)å’ŒWater Distribution Testbed (WADI)æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨F1-scoreå’ŒAU-PRæŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œè¯æ˜äº†ç»“åˆç”Ÿæˆæ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ ä¸é€‰æ‹©æ€§ç›‘ç£åœ¨å·¥ä¸šç›‘æ§é¢†åŸŸçš„æœ‰æ•ˆæ€§ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12351v1",
      "published_date": "2025-11-15 20:36:20 UTC",
      "updated_date": "2025-11-15 20:36:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:26:47.544250+00:00"
    },
    {
      "arxiv_id": "2511.12346v2",
      "title": "CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification",
      "title_zh": "CLAReSNetï¼šèåˆå·ç§¯ä¸æ½œåœ¨æ³¨æ„åŠ›çš„é«˜å…‰è°±å›¾åƒåˆ†ç±»",
      "authors": [
        "Asmit Bandyopadhyay",
        "Anindita Das Bhattacharjee",
        "Rakesh Das"
      ],
      "abstract": "Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\\mathcal{O}(T^2D)$ to $\\mathcal{O}(T\\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under severe class imbalance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜å…‰è°±å›¾åƒ(HSI)åˆ†ç±»ä¸­é«˜å…‰è°±ç»´åº¦ã€å¤æ‚çš„ç©ºè°±ç›¸å…³æ€§ä»¥åŠæ ·æœ¬å¤±è¡¡ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†CLAReSNetï¼Œè¿™æ˜¯ä¸€ç§å°†å¤šå°ºåº¦å·ç§¯æå–ä¸Transformeré£æ ¼æ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆçš„æ··åˆæ¶æ„ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šå°ºåº¦å·ç§¯ä¸»å¹²å’Œæ·±åº¦æ®‹å·®å—ï¼Œå¹¶ç»“åˆå¢å¼ºçš„å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—(CBAM)æ¥æå–å±‚æ¬¡åŒ–çš„ç©ºé—´ç‰¹å¾ã€‚åœ¨å…‰è°±ç¼–ç å±‚ï¼ŒCLAReSNeté›†æˆäº†åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ(LSTM/GRU)ä¸å¤šå°ºåº¦å…‰è°±æ½œæ³¨æ„åŠ›(MSLA)æœºåˆ¶ï¼Œåˆ©ç”¨è‡ªé€‚åº”æ½œæ ‡è‡´(latent tokens)åˆ†é…å°†è®¡ç®—å¤æ‚åº¦ä»å¹³æ–¹çº§é™ä½è‡³å¯¹æ•°çº§($\\mathcal{O}(T\\log(T)D)$)ã€‚é€šè¿‡å±‚æ¬¡åŒ–äº¤å‰æ³¨æ„åŠ›èåˆ(Hierarchical cross-attention fusion)åŠ¨æ€èšåˆå¤šçº§ç‰¹å¾è¡¨ç¤ºï¼Œç¡®ä¿äº†åˆ†ç±»çš„ç¨³å¥æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLAReSNetåœ¨Indian Pineså’ŒSalinasæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†99.71%å’Œ99.96%çš„æ€»ä½“å‡†ç¡®ç‡ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºHybridSNã€SSRNå’ŒSpectralFormerç­‰æ¨¡å‹ã€‚å­¦ä¹ åˆ°çš„åµŒå…¥è¡¨ç¤ºå±•ç°å‡ºä¼˜å¼‚çš„ç±»é—´å¯åˆ†ç¦»æ€§å’Œç±»å†…ç´§å‡‘æ€§ï¼Œæœ‰æ•ˆéªŒè¯äº†è¯¥æ¨¡å‹åœ¨ä¸¥é‡ç±»ä¸å¹³è¡¡æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12346v2",
      "published_date": "2025-11-15 20:25:59 UTC",
      "updated_date": "2025-12-19 12:55:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:26:52.746650+00:00"
    },
    {
      "arxiv_id": "2511.12344v2",
      "title": "Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning",
      "title_zh": "åŸºäºè¯„ä»·é‡è¡¨çš„å¥–åŠ±ä¸å¼•å¯¼ï¼šé€šè¿‡ä¿ƒè¿›æ¢ç´¢æå‡å¤šé¢†åŸŸæ¨ç†èƒ½åŠ›",
      "authors": [
        "Baolong Bi",
        "Shenghua Liu",
        "Yiwei Wang",
        "Siqian Tong",
        "Lingrui Mei",
        "Yuyao Ge",
        "Yilong Xu",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ç”±äºè¿‡åº¦ä¾èµ–å•é¢†åŸŸå¯éªŒè¯å¥–åŠ±(RLVR)å’Œå—é™çš„åœ¨çº¿æ¢ç´¢ç©ºé—´è€Œå¯¼è‡´æ€§èƒ½å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†RGR-GRPO (Reward and Guidance through Rubrics) å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°åˆ©ç”¨ rubrics æä¾›ç»†ç²’åº¦çš„å¥–åŠ±ä¿¡å·å’Œç¦»çº¿å¼•å¯¼ï¼Œä½¿æ¨¡å‹åœ¨ GRPO è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿè·å¾—æ›´å¯†é›†ã€æ›´å…·ä¿¡æ¯é‡çš„å¥–åŠ±å¹¶æ¢ç´¢æ›´å¤§çš„è§£ç©ºé—´ã€‚åœ¨æ¶‰åŠæ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦å’Œé€šç”¨æ¨ç†ç­‰å¤šä¸ªé¢†åŸŸçš„14ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRGR-GRPO çš„è¡¨ç°ä¸€è‡´ä¼˜äºä¼ ç»Ÿçš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåœ¨æ•°å­¦å’ŒåŒ–å­¦ä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†7.0%å’Œ8.4%çš„å¹³å‡æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ off-policy è®­ç»ƒä¸­å±•ç°äº†ç¨³å®šçš„ç†µ(entropy)æ³¢åŠ¨å’Œå“è¶Šçš„ pass@k æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ç»´æŒæŒç»­æ¢ç´¢å’Œçªç ´ç°æœ‰æ€§èƒ½ç“¶é¢ˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡ç»“åˆç²¾ç»†åŒ–å¥–åŠ±ä¸å¤šé¢†åŸŸå¼•å¯¼ï¼ŒRGR-GRPO ä¸ºæå‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è·¨é¢†åŸŸæ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦çš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12344v2",
      "published_date": "2025-11-15 20:14:51 UTC",
      "updated_date": "2025-11-18 20:39:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:26:54.552406+00:00"
    },
    {
      "arxiv_id": "2511.12342v1",
      "title": "Ground Plane Projection for Improved Traffic Analytics at Intersections",
      "title_zh": "ç”¨äºæå‡äº¤å‰å£äº¤é€šåˆ†ææ€§èƒ½çš„åœ°å¹³é¢æŠ•å½±æ–¹æ³•",
      "authors": [
        "Sajjad Pakdamansavoji",
        "Kumar Vaibhav Jha",
        "Baher Abdulhai",
        "James H Elder"
      ],
      "abstract": "Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨äº¤å‰å£æµé‡åˆ†æä¸­å°†è½¦è¾†æ£€æµ‹ä»ä¼ ç»Ÿçš„å›¾åƒå¹³é¢(Image Plane)è½¬æ¢åˆ°åœ°é¢å¹³é¢(Ground Plane)æŠ•å½±çš„ä¼˜åŠ¿ï¼Œæ—¨åœ¨æå‡è½¬å‘æµé‡ç»Ÿè®¡(Turning Movement Counts, TMC)çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶é€šè¿‡å°†åŸºç¡€è®¾æ–½æ‘„åƒæœºæ•æ‰åˆ°çš„è½¦è¾†åå‘æŠ•å½±(Back-projection)åˆ°çœŸå®ä¸–ç•Œçš„3Dåæ ‡ç³»ä¸­ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„è½¨è¿¹åˆ†ç±»(Trajectory Classification)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å•æ‘„åƒæœºç³»ç»Ÿä¸­ï¼Œè¿™ç§æ–¹æ³•æ¯”ä¼ ç»Ÿçš„è§†è§‰åˆ†æèƒ½æ›´å‡†ç¡®åœ°å®Œæˆæµé‡ç»Ÿè®¡ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯æ˜é€šè¿‡å¯¹å¤šä¸ªæ‘„åƒæœºçš„åå‘æŠ•å½±æ£€æµ‹ç»“æœè¿›è¡Œå¼±èåˆ(Weak Fusion)ï¼Œå¯ä»¥è¿›ä¸€æ­¥è·å¾—æ›´é«˜çš„ç²¾åº¦ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œä¸ºäº†ä¼˜åŒ–ä¿¡å·æ§åˆ¶å’ŒåŸå¸‚è§„åˆ’ï¼Œäº¤é€šåˆ†æåº”å½“åœ¨åœ°é¢å¹³é¢è€Œéå›¾åƒå¹³é¢ä¸Šè¿›è¡Œã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12342v1",
      "published_date": "2025-11-15 20:02:29 UTC",
      "updated_date": "2025-11-15 20:02:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:27:00.243319+00:00"
    },
    {
      "arxiv_id": "2511.12321v1",
      "title": "Learning Time in Static Classifiers",
      "title_zh": "é™æ€åˆ†ç±»å™¨ä¸­çš„æ—¶é—´å­¦ä¹ ",
      "authors": [
        "Xi Ding",
        "Lei Wang",
        "Piotr Koniusz",
        "Yongsheng Gao"
      ],
      "abstract": "Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.",
      "tldr_zh": "ç°å®ä¸–ç•Œçš„è§†è§‰æ•°æ®é€šå¸¸éšæ—¶é—´é€æ¸æ¼”å˜ï¼Œä½†ä¼ ç»Ÿçš„åˆ†ç±»å™¨ç”±äºå‡è®¾æ—¶é—´ç‹¬ç«‹æ€§ï¼Œé™åˆ¶äº†å…¶æ•æ‰æ­¤ç±»åŠ¨æ€çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œåœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„æˆ–å¼•å…¥å¾ªç¯æ¨¡å—(recurrent modules)çš„æƒ…å†µä¸‹ï¼Œèµ‹äºˆæ ‡å‡†å‰é¦ˆåˆ†ç±»å™¨(feedforward classifiers)æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒæ˜¯å…¨æ–°çš„Support-Exemplar-Query (SEQ)å­¦ä¹ èŒƒå¼ï¼Œå°†è®­ç»ƒæ•°æ®ç»“æ„åŒ–ä¸ºå…·æœ‰æ—¶é—´è¿è´¯æ€§çš„è½¨è¿¹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ç‰¹å®šç±»åˆ«çš„æ—¶é—´åŸå‹(temporal prototypes)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¯å¾®åˆ†çš„soft-DTWæŸå¤±å¯¹é½é¢„æµ‹åºåˆ—ï¼Œå¹¶é€šè¿‡å¤šé¡¹å¼ç›®æ ‡å‡½æ•°(multi-term objective)ç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§ä¸æ—¶é—´å¹³æ»‘åº¦ã€‚é€šè¿‡æŸå¤±å‡½æ•°è®¾è®¡å¼•å…¥å¼ºæ—¶é—´å½’çº³åç½®(temporal inductive bias)ï¼Œè¯¥æ–¹æ³•åœ¨ç»†ç²’åº¦å’Œè¶…ç»†ç²’åº¦å›¾åƒåˆ†ç±»ä»¥åŠè§†é¢‘å¼‚å¸¸æ£€æµ‹(video anomaly detection)ä¸­å‡å–å¾—äº†ä¼˜å¼‚è¡¨ç°ã€‚è¿™ç§æ¨¡å—åŒ–ä¸”æ•°æ®é«˜æ•ˆçš„æ–¹æ³•æˆåŠŸæ¡¥æ¥äº†é™æ€ä¸æ—¶é—´å­¦ä¹ ï¼Œä»…éœ€åœ¨é¢„æå–ç‰¹å¾ä¸Šåº”ç”¨ç®€å•åˆ†ç±»å™¨å³å¯å®ç°ç²¾ç¡®ä¸”å…·æœ‰æ—¶é—´ä¸€è‡´æ€§çš„é¢„æµ‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)",
      "pdf_url": "https://arxiv.org/pdf/2511.12321v1",
      "published_date": "2025-11-15 18:42:51 UTC",
      "updated_date": "2025-11-15 18:42:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:27:12.195324+00:00"
    },
    {
      "arxiv_id": "2511.12319v1",
      "title": "Decision and Gender Biases in Large Language Models: A Behavioral Economic Perspective",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å†³ç­–ä¸æ€§åˆ«åè§ï¼šåŸºäºè¡Œä¸ºç»æµå­¦çš„è§†è§’",
      "authors": [
        "Luca Corazzini",
        "Elisa Deriu",
        "Marco Guerzoni"
      ],
      "abstract": "Large language models (LLMs) increasingly mediate economic and organisational processes, from automated customer support and recruitment to investment advice and policy analysis. These systems are often assumed to embody rational decision making free from human error; yet they are trained on human language corpora that may embed cognitive and social biases. This study investigates whether advanced LLMs behave as rational agents or whether they reproduce human behavioural tendencies when faced with classic decision problems. Using two canonical experiments in behavioural economics, the ultimatum game and a gambling game, we elicit decisions from two state of the art models, Google Gemma7B and Qwen, under neutral and gender conditioned prompts. We estimate parameters of inequity aversion and loss-aversion and compare them with human benchmarks. The models display attenuated but persistent deviations from rationality, including moderate fairness concerns, mild loss aversion, and subtle gender conditioned differences.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»è¡Œä¸ºç»æµå­¦è§†è§’æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦ä½“ç°ç†æ€§å†³ç­–ï¼Œè¿˜æ˜¯ä¼šå¤ç°äººç±»çš„è¡Œä¸ºåå·®ã€‚ç ”ç©¶é€šè¿‡æœ€åé€šç‰’åšå¼ˆ(Ultimatum Game)å’Œåšå¼ˆæ¸¸æˆ(Gambling Game)ä¸¤ä¸ªç»å…¸å®éªŒï¼Œè¯„ä¼°äº†Google Gemma 7Bå’ŒQwenä¸¤ä¸ªå…ˆè¿›æ¨¡å‹åœ¨å†³ç­–è¿‡ç¨‹ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å¯¹ä¸å¹³ç­‰è§„é¿(Inequity Aversion)å’ŒæŸå¤±è§„é¿(Loss Aversion)å‚æ•°çš„ä¼°ç®—ï¼Œç ”ç©¶äººå‘˜åˆ†æäº†æ¨¡å‹åœ¨ä¸­æ€§æç¤ºåŠæ€§åˆ«è°ƒèŠ‚æç¤ºä¸‹çš„ååº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹è™½ç„¶åç¦»ç¨‹åº¦è¾ƒäººç±»æ›´è½»ï¼Œä½†ä»è¡¨ç°å‡ºæŒç»­çš„éç†æ€§åå·®ï¼ŒåŒ…æ‹¬å¯¹å…¬å¹³æ€§çš„ä¸­åº¦å…³æ³¨å’Œè½»å¾®çš„æŸå¤±è§„é¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†æ¨¡å‹ä¸­å­˜åœ¨çš„å¾®å¦™æ€§åˆ«åè§ï¼Œè¯å®äº†LLMsä¼šå¸æ”¶å¹¶åæ˜ è®­ç»ƒè¯­æ–™ä¸­çš„è®¤çŸ¥å’Œç¤¾ä¼šå€¾å‘ã€‚è¿™äº›å‘ç°å¯¹äºåœ¨ç»æµå’Œç»„ç»‡è¿‡ç¨‹ä¸­éƒ¨ç½²è¿™äº›å¯èƒ½å½±å“å†³ç­–çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿå…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12319v1",
      "published_date": "2025-11-15 18:38:17 UTC",
      "updated_date": "2025-11-15 18:38:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:27:15.372120+00:00"
    },
    {
      "arxiv_id": "2511.17582v3",
      "title": "GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning",
      "title_zh": "GateRAï¼šé¢å‘å‚æ•°é«˜æ•ˆå¾®è°ƒçš„è¯å…ƒæ„ŸçŸ¥è°ƒåˆ¶",
      "authors": [
        "Jie Ou",
        "Shuaihong Jiang",
        "Yingjun Du",
        "Cees G. M. Snoek"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ LoRAã€DoRA ç­‰ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒ (Parameter-efficient fine-tuning, PEFT) æ–¹æ³•å¯¹æ‰€æœ‰ token åº”ç”¨é™æ€æ›´æ–°è€Œå¿½ç•¥è¾“å…¥å·®å¼‚çš„é—®é¢˜ï¼Œæå‡ºäº† GateRA è¿™ä¸€å¼•å…¥ token-aware modulation çš„ç»Ÿä¸€æ¡†æ¶ã€‚GateRA é€šè¿‡åœ¨æ ‡å‡† PEFT åˆ†æ”¯ä¸­é›†æˆè‡ªé€‚åº”é—¨æ§ (adaptive gating) æœºåˆ¶ï¼Œå®ç°äº†åŠ¨æ€çš„ token çº§åˆ«é€‚é…ï¼Œä»è€Œåœ¨ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†çš„åŒæ—¶å°†æ¨¡å‹å®¹é‡èšç„¦äºæŒ‘æˆ˜æ€§è¾“å…¥ã€‚å®éªŒåˆ†ææ­ç¤ºäº†è¯¥æ–¹æ³•å…·æœ‰ç›¸ä½æ•æ„Ÿæ€§ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æŠ‘åˆ¶å†—ä½™ prefill é˜¶æ®µçš„æ›´æ–°å¹¶å¼ºåŒ– decoding é˜¶æ®µçš„é€‚é…ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºç†µçš„æ­£åˆ™åŒ– (entropy-based regularization) æ¥ä¿ƒè¿›è¿‘ä¹äºŒè¿›åˆ¶çš„é—¨æ§å†³ç­–ï¼Œå®ç°äº†å¯è§£é‡Šä¸”é«˜æ•ˆçš„ç¨€ç–é€‚é…ã€‚ç†è®ºå±‚é¢ï¼ŒGateRA åœ¨ PEFT è·¯å¾„ä¸Šè¯±å¯¼äº†è½¯æ¢¯åº¦æ©ç æ•ˆåº” (soft gradient-masking effect)ï¼Œå®ç°äº†è¿ç»­ä¸”å¯å¾®çš„é€‚é…æ§åˆ¶ã€‚åœ¨å¤šä¸ªå¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGateRA çš„æ€§èƒ½ä¸€è‡´ä¼˜äºæˆ–ç­‰åŒäºæ­¤å‰çš„ä¸»æµ PEFT æŠ€æœ¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.17582v3",
      "published_date": "2025-11-15 17:55:47 UTC",
      "updated_date": "2025-12-22 06:51:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:27:29.275518+00:00"
    },
    {
      "arxiv_id": "2511.12309v1",
      "title": "Optimal Self-Consistency for Efficient Reasoning with Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆæ¨ç†çš„æœ€ä¼˜è‡ªæˆ‘ä¸€è‡´æ€§",
      "authors": [
        "Austin Feng",
        "Marius Alonso",
        "Ambroise Odonnat"
      ],
      "abstract": "Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªä¸€è‡´æ€§(Self-consistency, SC)åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†æ€§èƒ½æ—¶é¢ä¸´çš„æˆæœ¬é«˜æ˜‚åŠç¼ºä¹ç»Ÿä¸€ç†è®ºåˆ†æçš„é—®é¢˜ï¼ŒåŸºäºä¼—æ•°ä¼°è®¡(Mode Estimation)å’ŒæŠ•ç¥¨ç†è®ºå¯¹å…¶æ‰©å±•è¡Œä¸ºè¿›è¡Œäº†ç³»ç»Ÿæ€§ç ”ç©¶ã€‚é€šè¿‡æ¨å¯¼å¹¶éªŒè¯SCåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å¹‚å¾‹æ‰©å±•(Power Law Scaling)è§„å¾‹ï¼Œè®ºæ–‡æ·±å…¥åˆ†æäº†å›ºå®šåˆ†é…ä¸åŠ¨æ€åˆ†é…æ–¹æ¡ˆçš„æ ·æœ¬æ•ˆç‡ã€‚åŸºäºä¸Šè¿°è§è§£ï¼Œç ”ç©¶è€…æå‡ºäº†Blend-ASCï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è¶…å‚æ•°ä¸”èƒ½é€‚åº”ä»»æ„æ ·æœ¬é¢„ç®—çš„æ–°å‹åŠ¨æ€åˆ†é…å˜ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBlend-ASCåœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå¹³å‡æ‰€éœ€æ ·æœ¬é‡æ¯”åŸå§‹SCå‡å°‘äº†6.8å€ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å„ç±»åŸºå‡†æ¨¡å‹ã€‚è¯¥æ–¹æ³•ä¸ºå®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„æ€ç»´é“¾æ¨ç†æä¾›äº†ä¸€ç§çµæ´»ä¸”æå…·ç«äº‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12309v1",
      "published_date": "2025-11-15 17:45:42 UTC",
      "updated_date": "2025-11-15 17:45:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:27:22.481509+00:00"
    },
    {
      "arxiv_id": "2511.12306v2",
      "title": "UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI",
      "title_zh": "UpBenchï¼šé¢å‘ä»¥äººä¸ºæœ¬äººå·¥æ™ºèƒ½çš„åŠ¨æ€æ¼”è¿›å¼çœŸå®åŠ³åŠ¨åŠ›å¸‚åœºæ™ºèƒ½ä½“åŸºå‡†æ¡†æ¶",
      "authors": [
        "Darvin Yi",
        "Teng Liu",
        "Mattie Terzolo",
        "Lance Hasson",
        "Ayan Sinha",
        "Pablo Mendes",
        "Andrew Rabinovich"
      ],
      "abstract": "As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.",
      "tldr_zh": "ç°æœ‰çš„ Large Language Model (LLM) æ™ºèƒ½ä½“è¯„æµ‹åŸºå‡†å¤§å¤šå¤„äºé™æ€ã€åˆæˆæˆ–é¢†åŸŸå—é™çš„çŠ¶æ€ï¼Œéš¾ä»¥åæ˜ æ™ºèƒ½ä½“åœ¨çœŸå®ä¸”å…·æœ‰ç»æµä»·å€¼çš„åŠ¨æ€ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶æ¨å‡ºäº† UpBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå…¨çƒ Upwork åŠ³åŠ¨åŠ›å¸‚åœºçœŸå®å·¥ä½œçš„åŠ¨æ€æ¼”è¿›åŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“åœ¨ç°å®æ•°å­—å·¥ä½œä¸­çš„èƒœä»»åŠ›ã€é€‚åº”æ€§åŠäººæœºåä½œèƒ½åŠ›ã€‚æ¡†æ¶ä¸­çš„æ¯é¡¹ä»»åŠ¡å‡æºè‡ªç»è¿‡éªŒè¯çš„çœŸå®å®¢æˆ·äº¤æ˜“ï¼Œå°†è¯„ä¼°è¿‡ç¨‹é”šå®šåœ¨çœŸå®çš„èŒä¸šæ´»åŠ¨å’Œè´¢åŠ¡äº§å‡ºä¸­ã€‚UpBench é‡‡ç”¨äº†ä¸€ç§åŸºäº Rubric çš„è¯„ä¼°æ¡†æ¶ï¼Œç”±èµ„æ·±è‡ªç”±èŒä¸šè€…å°†å·¥ä½œåˆ†è§£ä¸ºè¯¦ç»†ã€å¯éªŒè¯çš„éªŒæ”¶æ ‡å‡†ï¼Œä»è€Œå®ç°æ¯”ç®€å•äºŒå…ƒæŒ‡æ ‡æ›´ç»†ç²’åº¦çš„æ¨¡å‹ä¼˜åŠ£åˆ†æå’ŒæŒ‡ä»¤éµå¾ªåº¦è¯„ä¼°ã€‚äººç±»ä¸“ä¸šçŸ¥è¯†è¢«æ·±åº¦æ•´åˆåˆ°ä»ä»»åŠ¡ç­–åˆ’åˆ°æœ€ç»ˆè¯„ä¼°çš„æ•°æ®æµæ°´çº¿ä¸­ï¼Œåœ¨ç¡®ä¿ç¬¦åˆä¸“ä¸šæ ‡å‡†çš„åŒæ—¶ï¼Œä¹Ÿä¸º Human-AI Collaboration çš„ç ”ç©¶æä¾›äº†æ”¯æŒã€‚é€šè¿‡å®šæœŸæ›´æ–°ä»»åŠ¡ä»¥åæ˜ åœ¨çº¿åŠ³åŠ¨åŠ›å¸‚åœºçš„æ¼”å˜ï¼ŒUpBench ä¸ºè¯„ä¼°çœŸå®èƒŒæ™¯ä¸‹çš„ Agentic Systems æä¾›äº†å¯æ‰©å±•ä¸”ä»¥äººä¸ºä¸­å¿ƒçš„åŸºçŸ³ï¼Œæ¨åŠ¨äº† AI é€šè¿‡ä¼™ä¼´å…³ç³»å¢å¼ºäººç±»èƒ½åŠ›çš„æ„¿æ™¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12306v2",
      "published_date": "2025-11-15 17:39:37 UTC",
      "updated_date": "2025-12-12 17:51:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:27:26.484765+00:00"
    },
    {
      "arxiv_id": "2511.12301v1",
      "title": "Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method",
      "title_zh": "é‡æ–°å®¡è§†åŒ»ç–—äººå·¥æ™ºèƒ½ç”Ÿæˆå¼æ•°æ®å¢å¼ºä¸­çš„åå·®ï¼šä¸€ç§é¢‘ç‡é‡æ ¡å‡†æ–¹æ³•",
      "authors": [
        "Chi Liu",
        "Jincheng Liu",
        "Congcong Zhu",
        "Minghao Wang",
        "Sheng Shen",
        "Jia Gu",
        "Tianqing Zhu",
        "Wanlei Zhou"
      ],
      "abstract": "Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–— AI ä¸­ç”Ÿæˆå¼æ•°æ®å¢å¼º (Generative Data Augmentation, GDA) çš„åå·®é—®é¢˜ï¼ŒæŒ‡å‡ºçœŸå®å›¾åƒä¸åˆæˆå›¾åƒä¹‹é—´çš„é¢‘ç‡å¤±è°ƒæ˜¯å¯¼è‡´ GDA ä¸å¯é çš„å…³é”®å› ç´ ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†é¢‘ç‡é‡æ ¡å‡† (Frequency Recalibration, FreRec) æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘é¢‘ç‡åˆ†å¸ƒå·®å¼‚æ¥ä¼˜åŒ–å¢å¼ºæ•ˆæœã€‚è¯¥æ–¹æ³•åŒ…å«ç»Ÿè®¡é«˜é¢‘æ›¿æ¢ (Statistical High-frequency Replacement, SHR) å’Œé‡å»ºé«˜é¢‘æ˜ å°„ (Reconstructive High-frequency Mapping, RHM) ä¸¤ä¸ªæ ¸å¿ƒç¯èŠ‚ï¼Œåˆ†åˆ«ç”¨äºåˆæ­¥å¯¹é½é«˜é¢‘æˆåˆ†å’Œå¢å¼ºå›¾åƒçš„ç»†èŠ‚è´¨é‡ã€‚åœ¨è„‘éƒ¨ MRIã€èƒ¸éƒ¨ X å…‰å’Œçœ¼åº•å›¾åƒç­‰å¤šç§åŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒFreRec ç›¸æ¯”æœªæ ¡å‡†çš„åˆæˆæ ·æœ¬æ˜¾è‘—æå‡äº†ä¸‹æ¸¸å›¾åƒåˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚ä½œä¸ºä¸€ç§ç‹¬ç«‹ä¸”å…¼å®¹æ€§å¼ºçš„åå¤„ç†æŠ€æœ¯ï¼ŒFreRec å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„åŒ»ç–— GDA æµæ°´çº¿ä¸­ï¼Œå¹¶é€‚ç”¨äºå„ç±»ç”Ÿæˆæ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for AAAI 2026 (Main Track Poster)",
      "pdf_url": "https://arxiv.org/pdf/2511.12301v1",
      "published_date": "2025-11-15 17:28:26 UTC",
      "updated_date": "2025-11-15 17:28:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:27:37.482322+00:00"
    },
    {
      "arxiv_id": "2511.12286v1",
      "title": "Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing",
      "title_zh": "Sangamï¼šé›†æˆ CXL çš„åŸºäºèŠ¯ç²’çš„ DRAM-PIM å¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿå™¨",
      "authors": [
        "Khyati Kiyawat",
        "Zhenxing Fan",
        "Yasas Seneviratne",
        "Morteza Baradaran",
        "Akhil Shekar",
        "Zihan Xia",
        "Mingu Kang",
        "Kevin Skadron"
      ],
      "abstract": "Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Sangamï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Chiplet çš„ CXL é›†æˆ DRAM-PIM åŠ é€Ÿå™¨ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ¨ç†é˜¶æ®µå› æ¨¡å‹è§„æ¨¡åŠ KV cache å¢é•¿è€Œé¢ä¸´çš„å†…å­˜å—é™é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰ PIM æ–¹æ¡ˆåœ¨ DRAM å·¥è‰ºä¸‹é€»è¾‘å¤„ç†èƒ½åŠ›å—é™ä¸”ä¼šæŒ¤å å­˜å‚¨å®¹é‡çš„ç¼ºé™·ï¼ŒSangam é€šè¿‡å¼‚æ„é›†æˆå°†é€»è¾‘ä¸å†…å­˜è§£è€¦åˆ°ä¸åŒçš„ Chiplets ä¸­ï¼Œå¹¶åˆ©ç”¨ Interposer è¿›è¡Œè¿æ¥ã€‚å…¶é€»è¾‘ Chiplet å†…éƒ¨é›†æˆäº† Systolic Arrays å’Œ SRAM ç¼“å­˜ï¼Œèƒ½å¤Ÿå®ç°å¯¹ DRAM ä¸­å†…å­˜å—é™å‹ GEMM ç®—å­çš„é«˜å¸¦å®½è®¿é—®ä¸åŠ é€Ÿã€‚ä½œä¸º CXL æŒ‚è½½çš„å­˜å‚¨æ¨¡å—ï¼ŒSangam æ—¢å¯ä»¥ä½œä¸º GPU çš„æ›¿ä»£å“ï¼Œä¹Ÿå¯ä»¥ä¸å…¶ååŒè¿è¡Œä»¥æå‡æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ LLaMA 2ã€Mistral å’Œ LLaMA 3 ç­‰æ¨¡å‹ä¸Šï¼ŒSangam ç›¸æ¯” H100 GPU å®ç°äº†æœ€é«˜ 4.22 å€çš„ç«¯åˆ°ç«¯æŸ¥è¯¢å»¶è¿ŸåŠ é€ŸåŠ 10.3 å€çš„è§£ç ååé‡æå‡ï¼Œå¹¶å®ç°äº†æ•°é‡çº§çš„èƒ½è€—èŠ‚çœã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12286v1",
      "published_date": "2025-11-15 16:39:51 UTC",
      "updated_date": "2025-11-15 16:39:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:27:33.376714+00:00"
    },
    {
      "arxiv_id": "2511.12271v1",
      "title": "MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning",
      "title_zh": "MoralReasonï¼šåŸºäºæ¨ç†çº§å¼ºåŒ–å­¦ä¹ çš„ LLM æ™ºèƒ½ä½“å¯æ³›åŒ–é“å¾·å†³ç­–å¯¹é½",
      "authors": [
        "Zhiyu An",
        "Wan Du"
      ],
      "abstract": "Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é“å¾·å†³ç­–å¯¹é½ä¸­çš„åˆ†å¸ƒå¤–(out-of-distribution)æ³›åŒ–é—®é¢˜ï¼Œæå‡ºäº†MoralReasonæ¡†æ¶ã€‚ç ”ç©¶è€…é¦–å…ˆæ„å»ºäº†Moral-Reason-QAæ•°æ®é›†ï¼ŒåŒ…å«680ä¸ªé«˜æ­§ä¹‰é“å¾·åœºæ™¯ï¼Œå¹¶æ¶µç›–äº†åŠŸåˆ©ä¸»ä¹‰(utilitarian)ã€ä¹‰åŠ¡è®º(deontological)å’Œç¾å¾·ä¼¦ç†(virtue ethics)çš„ç‰¹å®šæ¨ç†è·¯å¾„ã€‚é€šè¿‡é‡‡ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Group Relative Policy Optimization, GRPO)å’Œå¤åˆå¥–åŠ±æœºåˆ¶ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†å±‚é¢åŒæ—¶ä¼˜åŒ–å†³ç­–å¯¹é½å’Œæ¡†æ¶ç‰¹å®šçš„æ¨ç†è¿‡ç¨‹ï¼Œä¿ƒä½¿æ¨¡å‹å†…åŒ–åº•å±‚é“å¾·å‡†åˆ™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMoralReasonåœ¨æœªè§è¿‡çš„é“å¾·åœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŠŸåˆ©ä¸»ä¹‰å’Œä¹‰åŠ¡è®ºæ¡†æ¶çš„å¯¹é½åˆ†æ•°åˆ†åˆ«æå‡äº†0.757å’Œ0.450ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†LLMæ™ºèƒ½ä½“å¯ä»¥ç»è¿‡ç³»ç»Ÿè®­ç»ƒæ¥å†…åŒ–å¹¶è·¨åœºæ™¯åº”ç”¨ç‰¹å®šçš„é“å¾·æ¡†æ¶ï¼Œä¸ºäººå·¥æ™ºèƒ½å®‰å…¨é¢†åŸŸå¥ å®šäº†å…³é”®çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.12271v1",
      "published_date": "2025-11-15 15:52:10 UTC",
      "updated_date": "2025-11-15 15:52:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:27:41.685741+00:00"
    },
    {
      "arxiv_id": "2511.12265v1",
      "title": "Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks",
      "title_zh": "æ ¡å‡†å¯¹æŠ—é‡‡æ ·ï¼šå¤šè‡‚æœºå¼•å¯¼çš„é’ˆå¯¹æœªé¢„è§æ”»å‡»çš„æ³›åŒ–",
      "authors": [
        "Rui Wang",
        "Zeming Wei",
        "Xiyue Zhang",
        "Meng Sun"
      ],
      "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.",
      "tldr_zh": "æ·±åº¦ç¥ç»ç½‘ç»œ (DNNs) å®¹æ˜“å—åˆ°å„ç§å¯¹æŠ—æ‰°åŠ¨ (adversarial perturbations) çš„å½±å“ï¼Œè€Œç°æœ‰çš„å¯¹æŠ—è®­ç»ƒ (Adversarial Training, AT) æ¡†æ¶é€šå¸¸ä»…é’ˆå¯¹ç‰¹å®šçš„æ”»å‡»ç±»å‹ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹è®­ç»ƒä¸­æœªæ¶‰åŠçš„å®é™…æ”»å‡»æ—¶ä¾ç„¶è„†å¼±ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºæ ¡å‡†å¯¹æŠ—é‡‡æ · (Calibrated Adversarial Sampling, CAS) çš„é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹å¯¹æœªçŸ¥æ”»å‡»çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä»å¤šè‡‚è€è™æœº (multi-armed bandit) æ¡†æ¶çš„ä¼˜åŒ–è§†è§’å‡ºå‘ï¼Œé€šè¿‡åŠ¨æ€è®¾è®¡å¥–åŠ±æœºåˆ¶å¹¶å¹³è¡¡æ¢ç´¢ (exploration) ä¸åˆ©ç”¨ (exploitation)ï¼Œå……åˆ†è€ƒè™‘äº†å¤šä¸ªé²æ£’æ€§ç»´åº¦ä¹‹é—´çš„åŠ¨æ€åŠäº’ä¾ç‰¹æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCAS åœ¨ä¿æŒé«˜å¹²å‡€å‡†ç¡®ç‡ (clean accuracy) çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ•´ä½“é²æ£’æ€§ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå®ç°æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é²æ£’æ³›åŒ–æä¾›äº†ä¸€ç§å…¨æ–°çš„æœ‰æ•ˆèŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12265v1",
      "published_date": "2025-11-15 15:42:40 UTC",
      "updated_date": "2025-11-15 15:42:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:27:44.764088+00:00"
    },
    {
      "arxiv_id": "2511.12263v2",
      "title": "CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models",
      "title_zh": "CrossVidï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è·¨è§†é¢‘æ¨ç†èƒ½åŠ›çš„ç»¼åˆåŸºå‡†",
      "authors": [
        "Jingyao Li",
        "Jingyun Wang",
        "Molin Tan",
        "Haochen Wang",
        "Cilin Yan",
        "Likun Shi",
        "Jiayin Cai",
        "Xiaolong Jiang",
        "Yao Hu"
      ],
      "abstract": "Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(Multimodal Large Language Models, MLLMs)åœ¨è·¨è§†é¢‘æ¨ç†(Cross-Video Reasoning, CVR)èƒ½åŠ›çš„è¯„ä¼°ç¼ºå£ï¼ŒæŒ‡å‡ºå½“å‰åŸºå‡†æµ‹è¯•å¤šå±€é™äºå•è§†é¢‘åˆ†æï¼Œéš¾ä»¥è¡¡é‡æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹æ•´åˆä¸å¯¹æ¯”å¤šç»„è§†é¢‘ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ¨å‡ºäº†CrossVidï¼Œè¿™æ˜¯é¦–ä¸ªå…¨é¢è¯„ä¼°MLLMsåœ¨è·¨è§†é¢‘è¯­å¢ƒä¸‹æ—¶ç©ºæ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æ¶µç›–4ä¸ªé«˜ç»´ç»´åº¦å’Œ10é¡¹å…·ä½“ä»»åŠ¡ï¼ŒåŒ…å«5,331ä¸ªè§†é¢‘åŠ9,015ä¸ªæ¶µç›–å•é€‰ã€å¤šé€‰å’Œå¼€æ”¾å¼é—®é¢˜çš„æŒ‘æˆ˜æ€§é—®ç­”å¯¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGemini-2.5-Proåœ¨CrossVidä¸Šè¡¨ç°æœ€ä¼˜ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸º50.4%ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºï¼Œå½“å‰å¤§å¤šæ•°MLLMsåœ¨CVRä»»åŠ¡ä¸­é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œä¸»è¦ç“¶é¢ˆåœ¨äºæ— æ³•æœ‰æ•ˆæ•´åˆæˆ–æ¯”è¾ƒåˆ†å¸ƒåœ¨å¤šä¸ªè§†é¢‘ä¸­çš„è¯æ®è¿›è¡Œæ¨ç†ã€‚CrossVidçš„æå‡ºä¸ºæœªæ¥æå‡MLLMsçš„è·¨è§†é¢‘æ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦çš„è¯„ä»·ä½“ç³»ä¸æ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to AAAI 2026 (main track). For code and data, see https://github.com/chuntianli666/CrossVid",
      "pdf_url": "https://arxiv.org/pdf/2511.12263v2",
      "published_date": "2025-11-15 15:41:38 UTC",
      "updated_date": "2025-11-30 14:02:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:27:45.780728+00:00"
    },
    {
      "arxiv_id": "2511.12256v1",
      "title": "Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment",
      "title_zh": "é¢å‘ä½å‰‚é‡ CT è´¨é‡è¯„ä¼°çš„ MedSigLIP æç¤ºè¯æ¡ä»¶åŒ– FiLM ä¸å¤šå°ºåº¦èåˆ",
      "authors": [
        "Tolga Demiroglu",
        "Mehmet Ozan Unal",
        "Metin Ertas",
        "Isa Yildirim"
      ],
      "abstract": "We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºMedSigLIPçš„æç¤ºè°ƒèŠ‚(Prompt-Conditioned)æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºä½å‰‚é‡CT (Low-Dose CT)çš„å›¾åƒè´¨é‡è¯„ä¼°ã€‚è¯¥æ–¹æ³•é€šè¿‡ç‰¹å¾çº¿æ€§è°ƒåˆ¶(Feature-wise Linear Modulation, FiLM)å’Œå¤šå°ºåº¦æ± åŒ–(multi-scale pooling)å°†æ–‡æœ¬å…ˆéªŒæ³¨å…¥æ¨¡å‹ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºå°†è¡¥ä¸ä»¤ç‰Œ(patch-token)ç‰¹å¾ä¸ä¸´åºŠæ„å›¾ç›¸ç»“åˆï¼Œå®ç°äº†é«˜æ•ˆçš„æ•°æ®å­¦ä¹ ä¸å¿«é€Ÿé€‚åº”ã€‚æ¶æ„ä¸Šèåˆäº†å…¨å±€ã€å±€éƒ¨åŠçº¹ç†æ„ŸçŸ¥çš„æ± åŒ–æŠ€æœ¯ï¼Œå¹¶é€šè¿‡è½»é‡çº§MLPæ•´åˆå„å›å½’å¤´è¾“å‡ºï¼Œè¾…ä»¥æˆå¯¹æ’åºæŸå¤±(pairwise ranking loss)è¿›è¡Œè®­ç»ƒã€‚åœ¨LDCTIQA2023æŒ‘æˆ˜èµ›ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆå–å¾—äº†PLCC 0.9575ã€SROCC 0.9561å’ŒKROCC 0.8301çš„ä¼˜å¼‚æˆç»©ã€‚è¿™ä¸€è¡¨ç°è¶…è¶Šäº†è¯¥æŒ‘æˆ˜èµ›ä¸­æ’åæœ€å‰çš„å·²å‘è¡¨æˆæœï¼Œå……åˆ†éªŒè¯äº†æç¤ºå¼•å¯¼(prompt-guided)ç­–ç•¥åœ¨æå‡åŒ»å­¦å½±åƒè´¨é‡è¯„ä¼°å‡†ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12256v1",
      "published_date": "2025-11-15 15:26:59 UTC",
      "updated_date": "2025-11-15 15:26:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:04.071831+00:00"
    },
    {
      "arxiv_id": "2511.12254v2",
      "title": "Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation",
      "title_zh": "Mobile-Agent-RAGï¼šä»¥ä¸Šä¸‹æ–‡çŸ¥è¯†èµ‹èƒ½é©±åŠ¨æ™ºèƒ½å¤šæ™ºèƒ½ä½“ååŒï¼Œå®ç°é•¿ç¨‹ç§»åŠ¨è‡ªåŠ¨åŒ–",
      "authors": [
        "Yuxiang Zhou",
        "Jichang Li",
        "Yanhao Zhang",
        "Haonan Lu",
        "Guanbin Li"
      ],
      "abstract": "Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§»åŠ¨æ™ºèƒ½ä½“åœ¨é•¿ç¨‹ã€è·¨åº”ç”¨ä»»åŠ¡ä¸­å› è¿‡åº¦ä¾èµ–é™æ€çŸ¥è¯†è€Œå¯¼è‡´çš„è§„åˆ’å¹»è§‰å’Œç•Œé¢(UI)æ‰§è¡Œé”™è¯¯ï¼Œæå‡ºäº†Mobile-Agent-RAGï¼Œä¸€ç§é›†æˆäº†åŒå±‚æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)çš„å±‚æ¬¡åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé«˜å±‚è§„åˆ’éœ€è¦ç­–ç•¥å¯¼å‘çš„ç»éªŒï¼Œè€Œåº•å±‚æ“ä½œåˆ™éœ€è¦ä¸å…·ä½“UIç´§å¯†ç»“åˆçš„ç²¾ç¡®æŒ‡ä»¤ã€‚æ¡†æ¶é€šè¿‡Manager-RAGåœ¨è§„åˆ’é˜¶æ®µæ£€ç´¢äººå·¥éªŒè¯çš„è®¡åˆ’ä»¥å‡å°‘æˆ˜ç•¥å¹»è§‰ï¼Œå¹¶åœ¨æ‰§è¡Œé˜¶æ®µé€šè¿‡Operator-RAGæä¾›ç²¾ç¡®çš„åŸå­åŠ¨ä½œæŒ‡å¯¼ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ä½“ç³»ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸¤ä¸ªä¸“é—¨çš„æ£€ç´¢çŸ¥è¯†åº“ï¼Œå¹¶æ¨å‡ºäº†Mobile-Eval-RAGåŸºå‡†æµ‹è¯•ã€‚å®éªŒè¯æ˜ï¼ŒMobile-Agent-RAGæ˜¾è‘—ä¼˜äºç°æœ‰SOTAæ¨¡å‹ï¼Œå°†ä»»åŠ¡å®Œæˆç‡æå‡äº†11.0%ï¼Œæ­¥éª¤æ•ˆç‡æå‡äº†10.2%ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸”å¯é çš„ç§»åŠ¨è‡ªåŠ¨åŒ–æä¾›äº†ç¨³å¥çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12254v2",
      "published_date": "2025-11-15 15:22:42 UTC",
      "updated_date": "2025-12-03 03:26:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:10.968674+00:00"
    },
    {
      "arxiv_id": "2511.12248v1",
      "title": "Deep Unfolded BM3D: Unrolling Non-local Collaborative Filtering into a Trainable Neural Network",
      "title_zh": "æ·±åº¦å±•å¼€ BM3Dï¼šå°†éå±€éƒ¨ååŒè¿‡æ»¤å±•å¼€ä¸ºå¯è®­ç»ƒç¥ç»ç½‘ç»œ",
      "authors": [
        "Kerem Basim",
        "Mehmet Ozan Unal",
        "Metin Ertas",
        "Isa Yildirim"
      ],
      "abstract": "Block-Matching and 3D Filtering (BM3D) exploits non-local self-similarity priors for denoising but relies on fixed parameters. Deep models such as U-Net are more flexible but often lack interpretability and fail to generalize across noise regimes. In this study, we propose Deep Unfolded BM3D (DU-BM3D), a hybrid framework that unrolls BM3D into a trainable architecture by replacing its fixed collaborative filtering with a learnable U-Net denoiser. This preserves BM3D's non-local structural prior while enabling end-to-end optimization. We evaluate DU-BM3D on low-dose CT (LDCT) denoising and show that it outperforms classic BM3D and standalone U-Net across simulated LDCT at different noise levels, yielding higher PSNR and SSIM, especially in high-noise conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Deep Unfolded BM3D (DU-BM3D)ï¼Œè¿™æ˜¯ä¸€ç§å°†Block-Matching and 3D Filtering (BM3D)å±•å¼€ä¸ºå¯è®­ç»ƒç¥ç»ç½‘ç»œæ¶æ„çš„æ··åˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå»å™ªæ–¹æ³•å‚æ•°å›ºå®šä»¥åŠæ·±åº¦æ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†BM3Dä¸­å›ºæœ‰çš„ååŒè¿‡æ»¤(collaborative filtering)æ›¿æ¢ä¸ºå¯å­¦ä¹ çš„U-Netå»å™ªå™¨ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„å‚æ•°ä¼˜åŒ–ã€‚è¿™ä¸€è®¾è®¡åœ¨æœ‰æ•ˆä¿ç•™BM3Déå±€éƒ¨è‡ªç›¸ä¼¼æ€§(non-local self-similarity)ç»“æ„å…ˆéªŒçš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä¸åŒå™ªå£°ç¯å¢ƒä¸‹çš„çµæ´»æ€§ã€‚ç ”ç©¶è€…åœ¨ä½å‰‚é‡CT (LDCT)å»å™ªä»»åŠ¡ä¸Šå¯¹DU-BM3Dè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚ç»“æœæ˜¾ç¤ºï¼ŒDU-BM3Dåœ¨å¤šç§å™ªå£°æ°´å¹³ä¸‹çš„è¡¨ç°å‡ä¼˜äºä¼ ç»ŸBM3Då’Œç‹¬ç«‹U-Netæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜å™ªå£°æ¡ä»¶ä¸‹å–å¾—äº†æ›´é«˜çš„PSNRå’ŒSSIMæ•°å€¼ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å°†ç»å…¸ç®—æ³•çš„ç»“æ„å…ˆéªŒä¸ç¥ç»ç½‘ç»œçš„å­¦ä¹ èƒ½åŠ›ç›¸ç»“åˆï¼Œèƒ½å¤Ÿä¸ºåŒ»å­¦å½±åƒå»å™ªæä¾›æ›´å…·æ³›åŒ–æ€§å’Œè§£é‡Šæ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12248v1",
      "published_date": "2025-11-15 15:06:47 UTC",
      "updated_date": "2025-11-15 15:06:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:03.166681+00:00"
    },
    {
      "arxiv_id": "2511.16689v2",
      "title": "Concept-Based Interpretability for Toxicity Detection",
      "title_zh": "é¢å‘æ¯’æ€§æ£€æµ‹çš„åŸºäºæ¦‚å¿µçš„å¯è§£é‡Šæ€§",
      "authors": [
        "Samarth Garg",
        "Divya Singh",
        "Deeksha Varshney",
        "Mamta"
      ],
      "abstract": "The rise of social networks has not only facilitated communication but also allowed the spread of harmful content. Although significant advances have been made in detecting toxic language in textual data, the exploration of concept-based explanations in toxicity detection remains limited. In this study, we leverage various subtype attributes present in toxicity detection datasets, such as obscene, threat, insult, identity attack, and sexual explicit as concepts that serve as strong indicators to identify whether language is toxic. However, disproportionate attribution of concepts towards the target class often results in classification errors. Our work introduces an interpretability technique based on the Concept Gradient (CG) method which provides a more causal interpretation by measuring how changes in concepts directly affect the output of the model. This is an extension of traditional gradient-based methods in machine learning, which often focus solely on input features. We propose the curation of Targeted Lexicon Set, which captures toxic words that contribute to misclassifications in text classification models. To assess the significance of these lexicon sets in misclassification, we compute Word-Concept Alignment (WCA) scores, which quantify the extent to which these words lead to errors due to over-attribution to toxic concepts. Finally, we introduce a lexicon-free augmentation strategy by generating toxic samples that exclude predefined toxic lexicon sets. This approach allows us to examine whether over-attribution persists when explicit lexical overlap is removed, providing insights into the model's attribution on broader toxic language patterns.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤ç½‘ç»œä¸­æœ‰æ¯’è¯­è¨€æ£€æµ‹çš„å¯è§£é‡Šæ€§é—®é¢˜ï¼Œæ¢è®¨äº†åŸºäºæ¦‚å¿µ(Concept-Based)çš„è§£é‡Šæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹å› å¯¹ç‰¹å®šæ¦‚å¿µè¿‡åº¦å½’å› è€Œå¯¼è‡´çš„åˆ†ç±»é”™è¯¯ã€‚ç ”ç©¶åˆ©ç”¨æœ‰æ¯’æ£€æµ‹æ•°æ®é›†ä¸­çš„å­ç±»å‹å±æ€§ï¼ˆå¦‚ obscene, threat, insult, identity attack, sexual explicitï¼‰ä½œä¸ºæ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶å¼•å…¥äº†æ¦‚å¿µæ¢¯åº¦(Concept Gradient, CG)æŠ€æœ¯ï¼Œé€šè¿‡è¡¡é‡æ¦‚å¿µå˜åŒ–å¯¹æ¨¡å‹è¾“å‡ºçš„ç›´æ¥å½±å“æ¥æä¾›æ›´å…·å› æœæ€§çš„è§£é‡Šã€‚ä¸ºäº†é‡åŒ–è¯æ±‡å¯¹è¯¯åˆ¤çš„å½±å“ï¼Œè¯¥å·¥ä½œæ„å»ºäº†é’ˆå¯¹æ€§è¯åº“é›†(Targeted Lexicon Set)å¹¶æå‡ºè¯æ±‡-æ¦‚å¿µå¯¹é½(Word-Concept Alignment, WCA)è¯„åˆ†ï¼Œç”¨äºåˆ†æè¯æ±‡å› è¿‡åº¦å½’å› è€Œå¯¼è‡´é”™è¯¯çš„ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ä¸€ç§æ— è¯åº“å¢å¼º(Lexicon-free augmentation)ç­–ç•¥ç”Ÿæˆä¸å«é¢„å®šä¹‰è¯åº“çš„æœ‰æ¯’æ ·æœ¬ï¼Œä»¥æ­¤è¯„ä¼°æ¨¡å‹åœ¨å»é™¤æ˜¾æ€§è¯æ±‡é‡å åå¯¹å¹¿æ³›æœ‰æ¯’è¯­è¨€æ¨¡å¼çš„å½’å› è¡¨ç°ã€‚è¿™äº›æ–¹æ³•ä¸ºæå‡æœ‰æ¯’è¯­è¨€æ£€æµ‹æ¨¡å‹çš„å¯è§£é‡Šæ€§åŠè¯†åˆ«åˆ†ç±»åå·®æä¾›äº†ç³»ç»Ÿçš„åˆ†ææ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.16689v2",
      "published_date": "2025-11-15 14:53:23 UTC",
      "updated_date": "2025-12-13 11:10:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:12.076287+00:00"
    },
    {
      "arxiv_id": "2511.12241v1",
      "title": "AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos",
      "title_zh": "AURAï¼šåŸºäºåˆæˆ ICU è§†é¢‘çš„å¢å¼ºå‹éè®¡åˆ’æ€§æ‹”ç®¡è­¦ç¤ºç³»ç»Ÿçš„å¼€å‘ä¸éªŒè¯",
      "authors": [
        "Junhyuk Seo",
        "Hyeyoon Moon",
        "Kyu-Hwan Jung",
        "Namkee Oh",
        "Taerim Kim"
      ],
      "abstract": "Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶å¼€å‘å¹¶éªŒè¯äº†åä¸ºAURAï¼ˆAugmented Unplanned Removal Alertï¼‰çš„å¢å¼ºå‹éè®¡åˆ’æ‹”ç®¡é¢„è­¦ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³é‡ç—‡ç›‘æŠ¤å®¤ï¼ˆICUsï¼‰ä¸­ç”±äºéšç§å’Œä¼¦ç†æŒ‘æˆ˜å¯¼è‡´ç¼ºä¹æ ‡æ³¨æ•°æ®è€Œéš¾ä»¥å®æ—¶æ£€æµ‹éè®¡åˆ’æ‹”ç®¡ï¼ˆUnplanned extubation, UEï¼‰çš„é—®é¢˜ã€‚è¯¥ç ”ç©¶åˆ›æ–°æ€§åœ°åˆ©ç”¨æ–‡æœ¬è½¬è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼ˆtext-to-video diffusionï¼‰ç”Ÿæˆäº†ä¸´åºŠçœŸå®çš„åˆæˆICUè§†é¢‘æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¼€å‘äº†åŸºäºè§†è§‰çš„é£é™©æ£€æµ‹ç³»ç»Ÿã€‚AURAç³»ç»Ÿé€šè¿‡åº”ç”¨å§¿æ€ä¼°è®¡ï¼ˆpose estimationï¼‰æŠ€æœ¯è¯†åˆ«ä¸¤ç§é«˜é£é™©æ¨¡å¼ï¼Œå³æ‰‹éƒ¨æ¥è¿‘æ°”é“æ’ç®¡åŒºåŸŸçš„ç¢°æ’ï¼ˆcollisionï¼‰ä»¥åŠé€šè¿‡è¿½è¸ªå…³é”®ç‚¹é€Ÿåº¦é‡åŒ–çš„èºåŠ¨ï¼ˆagitationï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ç¢°æ’æ£€æµ‹ä¸Šè¾¾åˆ°äº†é«˜å‡†ç¡®ç‡ï¼Œä¸”åˆæˆæ•°æ®çš„çœŸå®æ€§å¾—åˆ°äº†ä¸“å®¶çš„è®¤å¯ã€‚æ­¤é¡¹å·¥ä½œä¸ºå¼€å‘ä¿æŠ¤éšç§ä¸”å¯é‡å¤çš„æ‚£è€…å®‰å…¨ç›‘æµ‹ç³»ç»Ÿå¼€è¾Ÿäº†æ–°è·¯å¾„ï¼Œå…·æœ‰åœ¨å®é™…é‡ç—‡ç›‘æŠ¤åœºæ™¯ä¸­éƒ¨ç½²çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.12241v1",
      "published_date": "2025-11-15 14:52:37 UTC",
      "updated_date": "2025-11-15 14:52:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:11.899989+00:00"
    },
    {
      "arxiv_id": "2511.12240v2",
      "title": "SCI: A Metacognitive Control for Signal Dynamics",
      "title_zh": "SCIï¼šé’ˆå¯¹ä¿¡å·åŠ¨æ€çš„å…ƒè®¤çŸ¥æ§åˆ¶",
      "authors": [
        "Vishal Joshua Meesala"
      ],
      "abstract": "Modern deep learning systems are typically deployed as open-loop function approximators: they map inputs to outputs in a single pass, without regulating how much computation or explanatory effort is spent on a given case. In safety-critical settings, this is brittle: easy and ambiguous inputs receive identical processing, and uncertainty is only read off retrospectively from raw probabilities. We introduce the Surgical Cognitive Interpreter (SCI), a lightweight closed-loop metacognitive control layer that wraps an existing stochastic model and turns prediction into an iterative process. SCI monitors a scalar interpretive state SP(t), here instantiated as a normalized entropy-based confidence signal, and adaptively decides whether to stop, continue sampling, or abstain. The goal is not to improve accuracy per se, but to regulate interpretive error Î”SP and expose a safety signal that tracks when the underlying model is likely to fail. We instantiate SCI around Monte Carlo dropout classifiers in three domains: vision (MNIST digits), medical time series (MIT-BIH arrhythmia), and industrial condition monitoring (rolling-element bearings). In all cases, the controller allocates more inference steps to misclassified inputs than to correct ones (up to about 3-4x on MNIST and bearings, and 1.4x on MIT-BIH). The resulting Î”SP acts as a usable safety signal for detecting misclassifications (AUROC 0.63 on MNIST, 0.70 on MIT-BIH, 0.86 on bearings). Code and reproducibility: https://github.com/vishal-1344/sci",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ ç³»ç»Ÿåœ¨å®‰å…¨å…³é”®é¢†åŸŸå› å¼€ç¯(open-loop)è¿è¡Œå¯¼è‡´çš„ä¸ç¡®å®šæ€§å¤„ç†èƒ½åŠ›ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†Surgical Cognitive Interpreter (SCI)æ¡†æ¶ã€‚SCIæ˜¯ä¸€ä¸ªè½»é‡çº§çš„é—­ç¯å…ƒè®¤çŸ¥æ§åˆ¶å±‚(metacognitive control layer)ï¼Œé€šè¿‡å°è£…ç°æœ‰çš„éšæœºæ¨¡å‹å°†å•æ¬¡é¢„æµ‹è½¬åŒ–ä¸ºè¿­ä»£è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶å®æ—¶ç›‘æ§åŸºäºå½’ä¸€åŒ–ç†µçš„ç½®ä¿¡åº¦ä¿¡å·SP(t)ï¼Œå¹¶æ®æ­¤åŠ¨æ€å†³å®šåœæ­¢æ¨ç†ã€ç»§ç»­é‡‡æ ·æˆ–å¼ƒæƒï¼Œæ—¨åœ¨è°ƒèŠ‚è§£é‡Šè¯¯å·®å¹¶è¯†åˆ«æ¨¡å‹æ½œåœ¨çš„å¤±æ•ˆé£é™©ã€‚ç ”ç©¶åœ¨è§†è§‰ã€åŒ»ç–—æ—¶é—´åºåˆ—å’Œå·¥ä¸šç›‘æµ‹ä¸‰ä¸ªé¢†åŸŸå¯¹æ­è½½Monte Carlo dropoutåˆ†ç±»å™¨çš„SCIè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ§åˆ¶å™¨èƒ½ä¸ºè¯¯åˆ†ç±»è¾“å…¥è‡ªåŠ¨åˆ†é…æ˜¾è‘—æ›´å¤šçš„æ¨ç†æ­¥éª¤ï¼Œä¸”ç”Ÿæˆçš„è¯¯å·®ä¿¡å·Î”SPå¯ä½œä¸ºæœ‰æ•ˆçš„å®‰å…¨ä¿¡å·(safety signal)ç”¨äºæ£€æµ‹è¯¯åˆ†ç±»ã€‚åœ¨è½´æ‰¿ç›‘æµ‹ä»»åŠ¡ä¸­è¯¥ä¿¡å·çš„AUROCè¾¾åˆ°0.86ï¼Œä¸ºæå‡æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ä¸å®‰å…¨æ€§æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "v2: Extended theoretical analysis (Lyapunov-style stability), added metacognitive experiments across three domains, and released code and configuration files at https://github.com/vishal-1344/sci",
      "pdf_url": "https://arxiv.org/pdf/2511.12240v2",
      "published_date": "2025-11-15 14:48:17 UTC",
      "updated_date": "2025-11-30 19:55:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:17.183712+00:00"
    },
    {
      "arxiv_id": "2511.12239v1",
      "title": "Beyond World Models: Rethinking Understanding in AI Models",
      "title_zh": "è¶…è¶Šä¸–ç•Œæ¨¡å‹ï¼šAI æ¨¡å‹ç†è§£èƒ½åŠ›çš„å†æ€è€ƒ",
      "authors": [
        "Tarun Gupta",
        "Danish Pruthi"
      ],
      "abstract": "World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models \"understand\" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å½“å‰AIé¢†åŸŸå¹¿å—å…³æ³¨çš„World modelsï¼ˆä¸–ç•Œæ¨¡å‹ï¼‰æ¦‚å¿µï¼Œå³æ—¨åœ¨æ¨¡æ‹Ÿå¤–éƒ¨ä¸–ç•Œã€è¿½è¸ªå®ä½“çŠ¶æ€åŠæ•æ‰å› æœå…³ç³»çš„å†…éƒ¨è¡¨å¾ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œç ”ç©¶è¿™ä¸€æ–¹å‘çš„ä¸»è¦åŠ¨åŠ›åœ¨äºäººç±»æ‹¥æœ‰ç±»ä¼¼çš„å¿ƒç†æ¨¡å‹ï¼Œå› æ­¤åœ¨AIä¸­å‘ç°ç±»ä¼¼è¡¨å¾å¸¸è¢«è§†ä¸ºæ¨¡å‹å…·å¤‡ç±»äººç†è§£çš„æ ‡å¿—ã€‚é€šè¿‡å¼•ç”¨ç§‘å­¦å“²å­¦æ–‡çŒ®ä¸­çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œä½œè€…æ‰¹åˆ¤æ€§åœ°å®¡è§†äº†World modelsæ¡†æ¶æ˜¯å¦èƒ½å¤Ÿå……åˆ†è¡¨å¾äººç±»æ°´å¹³çš„ç†è§£ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†World modelsçš„èƒ½åŠ›ä¸äººç±»çœŸå®ç†è§£ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚çš„å“²å­¦è§†è§’ï¼Œè¿›è€Œæ¢ç´¢äº†è¯¥æ¡†æ¶çš„å±€é™æ€§ã€‚è¿™é¡¹å·¥ä½œæŒ‘æˆ˜äº†ä»…é æ¨¡æ‹Ÿå¤–éƒ¨ä¸–ç•Œå³å¯å®ç°ç†è§£çš„å‡è®¾ï¼Œä¸ºé‡æ–°æ€è€ƒäººå·¥æ™ºèƒ½çš„ç†è§£æœ¬è´¨æä¾›äº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to AAAI 2026 (Main Track)",
      "pdf_url": "https://arxiv.org/pdf/2511.12239v1",
      "published_date": "2025-11-15 14:45:26 UTC",
      "updated_date": "2025-11-15 14:45:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:16.976209+00:00"
    },
    {
      "arxiv_id": "2511.12236v1",
      "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts",
      "title_zh": "ä¸€è‡´æ€§æ˜¯å…³é”®ï¼šåŸºäºå…³é”®äº‹å®ä¸ä¸€è‡´æ€§æ ¡éªŒçš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬å¹»è§‰æ£€æµ‹",
      "authors": [
        "Raavi Gupta",
        "Pranav Hari Panicker",
        "Sumit Bhatia",
        "Ganesh Ramakrishnan"
      ],
      "abstract": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) ç»å¸¸äº§ç”Ÿäº‹å®é”™è¯¯å³å¹»è§‰ (hallucinations) çš„é—®é¢˜ï¼Œä»¥åŠç°æœ‰æ£€æµ‹æ–¹æ³•åœ¨ API å—é™ç¯å¢ƒä¸‹å­˜åœ¨çš„é«˜æˆæœ¬ä¸é«˜å»¶è¿ŸæŒ‘æˆ˜ï¼Œæå‡ºäº† CONFACTCHECKã€‚è¿™æ˜¯ä¸€ç§æ— éœ€å¤–éƒ¨çŸ¥è¯†åº“çš„é«˜æ•ˆå¹»è§‰æ£€æµ‹æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒç›´è§‰åœ¨äºå¯¹ç”Ÿæˆæ–‡æœ¬ä¸­äº‹å®æ€§æ¢æµ‹ (factual probes) çš„å“åº”åœ¨å•ä¸ªæ¨¡å‹å†…éƒ¨åŠè·¨æ¨¡å‹é—´åº”ä¿æŒä¸€è‡´ã€‚CONFACTCHECK é€šè¿‡éªŒè¯è¿™äº›å…³é”®äº‹å®çš„ä¸€è‡´æ€§æ¥è¯†åˆ«é”™è¯¯ä¿¡æ¯ï¼Œé¿å…äº†å¯¹æ¨¡å‹æƒé‡æˆ–å¤§è§„æ¨¡æ£€ç´¢çš„ä¾èµ–ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†äº‹å®æ€§æ–‡æœ¬å’Œå¼€æ”¾ç”Ÿæˆä»»åŠ¡æ—¶ï¼Œä¸ä»…æ˜¾è‘—é™ä½äº†èµ„æºæ¶ˆè€—ï¼Œè€Œä¸”åœ¨å‡†ç¡®ç‡ä¸Šä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåœ¨èµ„æºå—é™æˆ–é»‘ç›’æ¨¡å‹åœºæ™¯ä¸‹å®ç°å¯é çš„å¹»è§‰æ£€æµ‹æä¾›äº†æå…·æˆæœ¬æ•ˆç›Šçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "To appear at International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL), 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.12236v1",
      "published_date": "2025-11-15 14:33:02 UTC",
      "updated_date": "2025-11-15 14:33:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:29.577850+00:00"
    },
    {
      "arxiv_id": "2511.12233v2",
      "title": "Model Inversion Attack Against Deep Hashing",
      "title_zh": "é’ˆå¯¹æ·±åº¦å“ˆå¸Œçš„æ¨¡å‹åæ¼”æ”»å‡»",
      "authors": [
        "Dongdong Zhao",
        "Qiben Xu",
        "Ranxin Fang",
        "Baogang Song"
      ],
      "abstract": "Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Deep Hashingåœ¨æé«˜æ£€ç´¢æ•ˆç‡çš„åŒæ—¶æ‰€å¸¦æ¥çš„ä¸¥é‡éšç§é£é™©ï¼Œç‰¹åˆ«æ˜¯ä»å“ˆå¸Œç é‡æ„åŸå§‹è®­ç»ƒæ•°æ®å¯èƒ½å¯¼è‡´çš„ç”Ÿç‰©è¯†åˆ«ä¼ªé€ ç­‰å¨èƒã€‚é’ˆå¯¹ç°æœ‰Model Inversion Attackéš¾ä»¥é€‚åº”ç¦»æ•£Hamming spaceä¸”æ— æ³•è·å–çœŸå®è®­ç»ƒå“ˆå¸Œç çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†é¦–ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„æ”»å‡»æ¡†æ¶DHMIã€‚è¯¥æ¡†æ¶é€šè¿‡èšç±»è¾…åŠ©æ•°æ®é›†æå–è¯­ä¹‰å“ˆå¸Œä¸­å¿ƒä½œä¸ºæ›¿ä»£é”šç‚¹ï¼Œå¹¶åˆ©ç”¨èåˆåˆ†ç±»ä¸€è‡´æ€§ä¸å“ˆå¸Œé‚»è¿‘æ€§çš„æ–°æŒ‡æ ‡è¿›è¡ŒSurrogate-guidedå»å™ªä¼˜åŒ–ã€‚é€šè¿‡ä¸€ç»„æ›¿ä»£æ¨¡å‹æŒ‡å¯¼æ ·æœ¬ç²¾ç»†åŒ–ï¼ŒDHMIèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦ä¸”è¯­ä¹‰ä¸€è‡´çš„å›¾åƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨æœ€å›°éš¾çš„é»‘ç›’(Black-box)è®¾ç½®ä¸‹ï¼ŒDHMIçš„é‡æ„æ•ˆæœä»ä¼˜äºç°æœ‰çš„å…ˆè¿›æ”»å‡»æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ä»…éªŒè¯äº†DHMIçš„æœ‰æ•ˆæ€§ï¼Œæ›´æ­ç¤ºäº†Deep Hashingç³»ç»Ÿå†…åœ¨çš„ä¸¥é‡éšç§éšæ‚£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12233v2",
      "published_date": "2025-11-15 14:21:16 UTC",
      "updated_date": "2025-11-21 11:52:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:26.785513+00:00"
    },
    {
      "arxiv_id": "2511.17580v1",
      "title": "A novel strategy for multi-resource load balancing in agent-based systems",
      "title_zh": "åŸºäºæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¤šèµ„æºè´Ÿè½½å‡è¡¡æ–°ç­–ç•¥",
      "authors": [
        "Leszek Sliwko",
        "Aleksander Zgrzywa"
      ],
      "abstract": "The paper presents a multi-resource load balancing strategy which can be utilised within an agent-based system. This approach can assist system designers in their attempts to optimise the structure for complex enterprise architectures. In this system, the social behaviour of the agent and its adaptation abilities are applied to determine an optimal setup for a given configuration. All the methods have been developed to allow the agent's self-assessment. The proposed agent system has been implemented and the experiment results are presented here.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸“ä¸º agent-based systems è®¾è®¡çš„æ–°å‹å¤šèµ„æºè´Ÿè½½å‡è¡¡ (multi-resource load balancing) ç­–ç•¥ï¼Œæ—¨åœ¨ååŠ©ç³»ç»Ÿè®¾è®¡è€…ä¼˜åŒ–å¤æ‚ä¼ä¸šæ¶æ„ (enterprise architectures) çš„ç»„ç»‡ç»“æ„ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ™ºèƒ½ä½“çš„ç¤¾ä¼šè¡Œä¸º (social behavior) åŠå…¶é€‚åº”èƒ½åŠ› (adaptation abilities)ï¼Œåœ¨ç»™å®šé…ç½®ä¸‹ç¡®å®šæœ€ä¼˜çš„ç³»ç»Ÿè®¾ç½®ã€‚ç ”ç©¶ä¸­å¼€å‘çš„æ‰€æœ‰æ–¹æ³•éƒ½æ”¯æŒæ™ºèƒ½ä½“çš„è‡ªæˆ‘è¯„ä¼° (self-assessment)ï¼Œç¡®ä¿äº†ç³»ç»Ÿçš„çµæ´»æ€§ä¸è‡ªä¸»æ€§ã€‚é€šè¿‡å¯¹æ‰€ææ™ºèƒ½ä½“ç³»ç»Ÿçš„å®ç°ä¸å®éªŒéªŒè¯ï¼Œç»“æœè¯æ˜äº†è¯¥ç­–ç•¥åœ¨å¤„ç†å¤šèµ„æºåˆ†é…é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¼˜åŒ–å¤æ‚åˆ†å¸ƒå¼ç³»ç»Ÿçš„èµ„æºç®¡ç†æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.DC",
        "cs.SE"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17580v1",
      "published_date": "2025-11-15 14:05:23 UTC",
      "updated_date": "2025-11-15 14:05:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:35.661174+00:00"
    },
    {
      "arxiv_id": "2511.12214v1",
      "title": "ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction",
      "title_zh": "ViTEï¼šé¢å‘è¡Œäººè½¨è¿¹é¢„æµ‹çš„è™šæ‹Ÿå›¾è½¨è¿¹ä¸“å®¶è·¯ç”±å™¨",
      "authors": [
        "Ruochen Li",
        "Zhanxing Zhu",
        "Tanqiu Qiao",
        "Hubert P. H. Shum"
      ],
      "abstract": "Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡Œäººè½¨è¿¹é¢„æµ‹(Pedestrian trajectory prediction)ä¸­æ•æ‰é«˜é˜¶äº¤äº’æ—¶é¢ä¸´çš„æ„Ÿå—é‡å—é™æˆ–è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†æ–°å‹æ¡†æ¶ViTE (Virtual graph Trajectory Expert router)ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒç”±Virtual Graphæ¨¡å—ç»„æˆï¼Œé€šè¿‡å¼•å…¥åŠ¨æ€è™šæ‹ŸèŠ‚ç‚¹ï¼Œåœ¨æ— éœ€æ·±å±‚Graph Neural Network (GNN)å †å çš„æƒ…å†µä¸‹å®ç°äº†å¯¹é•¿ç¨‹å’Œé«˜é˜¶äº¤äº’çš„é«˜æ•ˆå»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒExpert Routeræ¨¡å—é‡‡ç”¨æ··åˆä¸“å®¶(Mixture-of-Experts)è®¾è®¡ï¼Œèƒ½å¤Ÿæ ¹æ®ç¤¾äº¤ä¸Šä¸‹æ–‡è‡ªé€‚åº”åœ°é€‰æ‹©äº¤äº’ä¸“å®¶ï¼Œä»è€Œå®ç°çµæ´»ä¸”å¯æ‰©å±•çš„æ¨ç†ã€‚åœ¨ETH/UCYã€NBAå’ŒSDDä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒViTEåœ¨ä¿æŒå®é™…æ•ˆç‡çš„åŒæ—¶ï¼Œæ€§èƒ½å§‹ç»ˆå¤„äºæœ€å…ˆè¿›çš„(state-of-the-art)æ°´å¹³ã€‚è¿™ä¸€ç ”ç©¶éªŒè¯äº†é€šè¿‡ç»“åˆè™šæ‹Ÿå›¾æœºåˆ¶ä¸ä¸“å®¶è·¯ç”±æ¥å¤„ç†å¤æ‚äº¤äº’æ¨¡å¼çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶å’ŒåŸå¸‚è§„åˆ’ç­‰åº”ç”¨æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12214v1",
      "published_date": "2025-11-15 13:36:07 UTC",
      "updated_date": "2025-11-15 13:36:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:51.984822+00:00"
    },
    {
      "arxiv_id": "2511.12213v1",
      "title": "MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues",
      "title_zh": "MME-RAGï¼šé¢å‘ä»»åŠ¡å‹å¯¹è¯ç»†ç²’åº¦å®ä½“è¯†åˆ«çš„å¤šç®¡ç†è€…-ä¸“å®¶æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Liang Xue",
        "Haoyu Liu",
        "Yajun Tian",
        "Xinyu Zhong",
        "Yang Liu"
      ],
      "abstract": "Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MME-RAGï¼Œä¸€ç§å¤šç®¡ç†è€…-ä¸“å®¶æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMulti-Manager-Expert Retrieval-Augmented Generationï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å‘ä»»åŠ¡çš„å¯¹è¯ä¸­é¢ä¸´çš„ç»†ç²’åº¦å®ä½“è¯†åˆ«ï¼ˆFine-grained entity recognitionï¼‰é¢†åŸŸé€‚åº”å’Œæ£€ç´¢å¯æ§æ€§éš¾é¢˜ã€‚è¯¥æ¡†æ¶å°†è¯†åˆ«ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªåè°ƒé˜¶æ®µï¼šç”±è½»é‡çº§ç®¡ç†è€…è´Ÿè´£çš„ç±»å‹çº§ï¼ˆtype-levelï¼‰åˆ¤æ–­ï¼Œä»¥åŠç”±ä¸“é—¨ä¸“å®¶æ‰§è¡Œçš„ç‰‡æ®µçº§ï¼ˆspan-levelï¼‰æå–ã€‚æ¯ä¸ªä¸“å®¶é€šè¿‡KeyInfoæ£€ç´¢å™¨åœ¨æ¨ç†æ—¶æ³¨å…¥è¯­ä¹‰å¯¹é½çš„å°‘æ ·æœ¬ç¤ºä¾‹ï¼ˆfew-shot exemplarsï¼‰ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç°ç²¾ç¡®çš„é¢†åŸŸè‡ªé€‚åº”æå–ã€‚åœ¨CrossNERã€MIT-Movieã€MIT-RestaurantåŠæ–°æ„å»ºçš„å¤šé¢†åŸŸå®¢æœæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒMME-RAGåœ¨å¤šæ•°é¢†åŸŸå‡ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œå±‚æ¬¡åŒ–åˆ†è§£ï¼ˆhierarchical decompositionï¼‰ä¸KeyInfoå¼•å¯¼çš„æ£€ç´¢æ˜¯å¢å¼ºé²æ£’æ€§å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›çš„æ ¸å¿ƒé©±åŠ¨åŠ›ï¼Œä¸ºè‡ªé€‚åº”å¯¹è¯ç†è§£æä¾›äº†å¯æ‰©å±•ä¸”å…·å¯è§£é‡Šæ€§çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12213v1",
      "published_date": "2025-11-15 13:35:55 UTC",
      "updated_date": "2025-11-15 13:35:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:53.781072+00:00"
    },
    {
      "arxiv_id": "2511.12212v1",
      "title": "Recursive Threshold Median Filter and Autoencoder for Salt-and-Pepper Denoising: SSIM analysis of Images and Entropy Maps",
      "title_zh": "åŸºäºé€’å½’é˜ˆå€¼çš„ä¸­å€¼æ»¤æ³¢ä¸è‡ªåŠ¨ç¼–ç å™¨æ¤’ç›å»å™ªï¼šå›¾åƒåŠç†µå›¾çš„ SSIM åˆ†æ",
      "authors": [
        "Petr Boriskov",
        "Kirill Rudkovskii",
        "Andrei Velichko"
      ],
      "abstract": "This paper studies the removal of salt-and-pepper noise from images using median filter (MF) and simple three-layer autoencoder (AE) within recursive threshold algorithm. The performance of denoising is assessed with two metrics: the standard Structural Similarity Index SSIMImg of restored and clean images and a newly applied metric SSIMMap - the SSIM of entropy maps of these images computed via 2D Sample Entropy in sliding windows. We shown that SSIMMap is more sensitive to blur and local intensity transitions and complements SSIMImg. Experiments on low- and high-resolution grayscales images demonstrate that recursive threshold MF robustly restores images even under strong noise (50-60 %), whereas simple AE is only capable of restoring images with low levels of noise (<30 %). We propose two scalable schemes: (i) 2MF, which uses two MFs with different window sizes and a final thresholding step, effective for highlighting sharp local details at low resolution; and (ii) MFs-AE, which aggregates features from multiple MFs via an AE and is beneficial for restoring the overall scene structure at higher resolution. Owing to its simplicity and computational efficiency, MF remains preferable for deployment on resource-constrained platforms (edge/IoT), whereas AE underperforms without prior denoising. The results also validate the practical value of SSIMMap for objective blur assessment and denoising parameter tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨é€’å½’é˜ˆå€¼ç®—æ³•æ¡†æ¶ä¸‹ï¼Œåˆ©ç”¨ä¸­å€¼æ»¤æ³¢å™¨(Median Filter, MF)å’Œç®€å•çš„ä¸‰å±‚è‡ªåŠ¨ç¼–ç å™¨(Autoencoder, AE)å»é™¤å›¾åƒä¸­çš„æ¤’ç›å™ªå£°(Salt-and-Pepper Noise)ã€‚é™¤äº†ä¼ ç»Ÿçš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°(SSIMImg)ï¼Œä½œè€…è¿˜å¼•å…¥äº†åŸºäº2Dæ ·æœ¬ç†µ(2D Sample Entropy)è®¡ç®—çš„ç†µæ˜ å°„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°(SSIMMap)ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒè¯æ˜SSIMMapå¯¹æ¨¡ç³Šå’Œå±€éƒ¨å¼ºåº¦è¿‡æ¸¡æ›´æ•æ„Ÿï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¡¥å……SSIMImgå¹¶ç”¨äºå»å™ªå‚æ•°çš„ç²¾ç¡®å¾®è°ƒã€‚ç ”ç©¶å‘ç°MFåœ¨50-60%çš„é«˜å™ªå£°ç¯å¢ƒä¸‹è¡¨ç°ç¨³å¥ï¼Œè€Œç®€å•çš„AEä»…èƒ½å¤„ç†30%ä»¥ä¸‹çš„ä½å™ªå£°æ°´å¹³ã€‚ä¸ºæ­¤ä½œè€…æå‡ºäº†2MFå’ŒMFs-AEä¸¤ç§å¯æ‰©å±•æ–¹æ¡ˆï¼Œåˆ†åˆ«ç”¨äºå¼ºåŒ–å±€éƒ¨ç»†èŠ‚å’Œæ¢å¤é«˜åˆ†è¾¨ç‡ä¸‹çš„æ•´ä½“åœºæ™¯ç»“æ„ã€‚æœ€ç»ˆç»“è®ºæŒ‡å‡ºï¼Œç”±äºMFè®¡ç®—æ•ˆç‡é«˜ä¸”å®ç°ç®€å•ï¼Œæ›´é€‚åˆéƒ¨ç½²åœ¨èµ„æºå—é™çš„è¾¹ç¼˜æˆ–ç‰©è”ç½‘å¹³å°(Edge/IoT)ï¼Œè€ŒAEåœ¨ç¼ºä¹é¢„å»å™ªçš„æƒ…å†µä¸‹è¡¨ç°è¾ƒå·®ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "14 pages, 13 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.12212v1",
      "published_date": "2025-11-15 13:35:53 UTC",
      "updated_date": "2025-11-15 13:35:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:54.474791+00:00"
    },
    {
      "arxiv_id": "2511.17579v1",
      "title": "Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation",
      "title_zh": "åŸºäºä»·å€¼å»ç›¸å…³ä¸å¤–æ¨çš„å¤§è¯­è¨€æ¨¡å‹å¤šä»·å€¼å¯¹é½",
      "authors": [
        "Hefei Xu",
        "Le Wu",
        "Chen Cheng",
        "Hao Liu"
      ],
      "abstract": "With the rapid advancement of large language models (LLMs), aligning them with human values for safety and ethics has become a critical challenge. This problem is especially challenging when multiple, potentially conflicting human values must be considered and balanced. Although several variants of existing alignment methods (such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)) have been proposed to address multi-value alignment, they suffer from notable limitations: 1) they are often unstable and inefficient in multi-value optimization; and 2) they fail to effectively handle value conflicts. As a result, these approaches typically struggle to achieve optimal trade-offs when aligning multiple values.\n  To address this challenge, we propose a novel framework called Multi-Value Alignment (MVA). It mitigates alignment degradation caused by parameter interference among diverse human values by minimizing their mutual information. Furthermore, we propose a value extrapolation strategy to efficiently explore the Pareto frontier, thereby constructing a set of LLMs with diverse value preferences. Extensive experiments demonstrate that MVA consistently outperforms existing baselines in aligning LLMs with multiple human values.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šä»·å€¼è§‚å¯¹é½(Multi-Value Alignment)ä¸­é¢ä¸´çš„ä»·å€¼è§‚å†²çªã€ä¼˜åŒ–ä¸ç¨³å®šåŠæ•ˆç‡ä½ä¸‹ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMVAçš„æ–°å‹å¯¹é½æ¡†æ¶ã€‚ç°æœ‰çš„RLHFå’ŒDPOå˜ä½“åœ¨å¤„ç†å¤šä¸ªæ½œåœ¨å†²çªçš„äººç±»ä»·å€¼è§‚æ—¶ï¼Œå¾€å¾€éš¾ä»¥å®ç°æœ€ä½³çš„æ€§èƒ½æƒè¡¡ã€‚MVAé€šè¿‡æœ€å°åŒ–ä¸åŒä»·å€¼è§‚ä¹‹é—´çš„äº’ä¿¡æ¯(Mutual Information)æ¥å®ç°ä»·å€¼è§£è€¦ï¼Œæœ‰æ•ˆç¼“è§£äº†å› å‚æ•°å¹²æ‰°å¯¼è‡´çš„å¯¹é½æ€§èƒ½é€€åŒ–é—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æå‡ºäº†ä¸€ç§ä»·å€¼å¤–æ¨(Value Extrapolation)ç­–ç•¥ï¼Œä»¥é«˜æ•ˆæ¢ç´¢å¸•ç´¯æ‰˜å‰æ²¿(Pareto Frontier)ï¼Œä»è€Œæ„å»ºå‡ºå…·æœ‰ä¸åŒä»·å€¼åå¥½çš„æ¨¡å‹é›†åˆã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒMVAåœ¨å¤šä»·å€¼è§‚å¯¹é½ä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºç°æœ‰çš„åŸºå‡†æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted by AAAI26 oral; 12 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.17579v1",
      "published_date": "2025-11-15 13:33:26 UTC",
      "updated_date": "2025-11-15 13:33:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:28:58.284920+00:00"
    },
    {
      "arxiv_id": "2511.12208v2",
      "title": "Debate over Mixed-knowledge: A Robust Multi-Agent Reasoning Framework for Incomplete Knowledge Graph Question Answering",
      "title_zh": "æ··åˆçŸ¥è¯†è¾©è®ºï¼šé¢å‘ä¸å®Œæ•´çŸ¥è¯†å›¾è°±é—®ç­”çš„é²æ£’å¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶",
      "authors": [
        "Jilong Liu",
        "Pengyang Shao",
        "Wei Qin",
        "Fei Liu",
        "Yonghui Yang",
        "Richang Hong"
      ],
      "abstract": "Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º Debate over Mixed-knowledge (DoM) çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸å®Œå…¨çŸ¥è¯†å›¾è°±é—®ç­” (Incomplete Knowledge Graph Question Answering, IKGQA) ä¸­å› çŸ¥è¯†ç¼ºå¤±å¯¼è‡´çš„äº‹å®å‡†ç¡®æ€§ä¸‹é™é—®é¢˜ã€‚DoM åŸºäºå¤šæ™ºèƒ½ä½“è¾©è®º (Multi-Agent Debate) èŒƒå¼ï¼Œé€šè¿‡åŠ¨æ€æ•´åˆç»“æ„åŒ–çš„ Knowledge Graph (KG) å’Œéç»“æ„åŒ–çš„å¤–éƒ¨æ–‡æœ¬çŸ¥è¯†ï¼Œå®ç°äº†å¤šæºçŸ¥è¯†çš„æ·±åº¦äº’è¡¥ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†è§£å­é—®é¢˜ï¼Œå¹¶åˆ©ç”¨ä¸“é—¨çš„ KG æ™ºèƒ½ä½“å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) æ™ºèƒ½ä½“ååŒå·¥ä½œï¼Œç”±è£åˆ¤æ™ºèƒ½ä½“ (Judge Agent) å¯¹ç»“æœè¿›è¡Œè¿­ä»£è¯„ä¼°ä¸æ±‡æ€»ã€‚é’ˆå¯¹ç°æœ‰æ•°æ®é›†éšæœºç§»é™¤ä¸‰å…ƒç»„æ— æ³•åæ˜ çœŸå®ä¸–ç•ŒåŠ¨æ€æ€§çš„å±€é™ï¼Œè¯¥ç ”ç©¶è¿˜æ„å»ºäº†åŸºäºçœŸå®çŸ¥è¯†æ›´æ–°çš„æ–°æ•°æ®é›† Incomplete Knowledge Graph WebQuestionsã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDoM èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨çŸ¥è¯†äº’è¡¥æ€§å¹¶å¢å¼ºå¯¹ KG ä¸å®Œæ•´æ€§çš„é²æ£’æ€§ï¼Œå…¶æ€§èƒ½è¡¨ç°æŒç»­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12208v2",
      "published_date": "2025-11-15 13:31:42 UTC",
      "updated_date": "2025-12-05 14:23:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:29:05.700387+00:00"
    },
    {
      "arxiv_id": "2511.12206v1",
      "title": "A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR",
      "title_zh": "åŸºäº YOLOv8 ä¸ OCR çš„æ–°å‹ AI é©±åŠ¨åè§†é•œç¼ºå¤±ã€æœªä½©æˆ´å¤´ç›”åŠè½¦ç‰Œå®æ—¶æ£€æµ‹ç³»ç»Ÿ",
      "authors": [
        "Nishant Vasantkumar Hegde",
        "Aditi Agarwal",
        "Minal Moharir"
      ],
      "abstract": "Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€æ¬¾åŸºäºäººå·¥æ™ºèƒ½çš„è‡ªåŠ¨åŒ–äº¤é€šè¿è§„æ£€æµ‹ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³æ‰‹åŠ¨æ‰§æ³•æ•ˆç‡ä½ä¸”ä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ YOLOv8 æ¨¡å‹è¿›è¡Œé²æ£’çš„ç‰©ä½“æ£€æµ‹ï¼Œå¹¶ç»“åˆ EasyOCR æŠ€æœ¯å®ç°è½¦ç‰Œè¯†åˆ«ã€‚é€šè¿‡åœ¨è‡ªå®šä¹‰ä¸”ç»è¿‡æ•°æ®å¢å¼ºçš„æ ‡æ³¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç³»ç»Ÿèƒ½å¤Ÿç²¾å‡†è¯†åˆ«æœªä½©æˆ´å¤´ç›”å’Œæ‘©æ‰˜è½¦åè§†é•œç¼ºå¤±ï¼ˆMirror Absenceï¼‰ç­‰è¿è§„è¡Œä¸ºï¼Œå…¶ä¸­åè§†é•œè‡ªåŠ¨æ£€æµ‹æ˜¯è¯¥é¢†åŸŸçš„ä¸€é¡¹åˆ›æ–°è´¡çŒ®ã€‚ç ”ç©¶è¿˜åˆ©ç”¨ Streamlit æ„å»ºäº†å®æ—¶ç›‘æ§ä¸è¿è§„è®°å½•ç•Œé¢ï¼Œå¹¶é€šè¿‡é«˜çº§å›¾åƒé¢„å¤„ç†æŠ€æœ¯ä¼˜åŒ–äº†å¤æ‚ç¯å¢ƒä¸‹çš„è¯†åˆ«æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº† 0.9147 çš„ç²¾ç¡®åº¦ï¼ˆPrecisionï¼‰ã€0.886 çš„å¬å›ç‡ï¼ˆRecallï¼‰ä»¥åŠ 0.843 çš„å¹³å‡ç²¾åº¦ï¼ˆmAP@50ï¼‰ã€‚è¯¥å·¥ä½œè¯æ˜äº†åœ¨ä¸¥æ ¼çš„ IoU é˜ˆå€¼ä¸‹ç³»ç»Ÿä¾ç„¶å…·æœ‰å¼ºå¤§çš„æ£€æµ‹æ€§èƒ½ï¼Œä¸ºç°å®åœºæ™¯ä¸­çš„è‡ªåŠ¨åŒ–äº¤é€šæ‰§æ³•æä¾›äº†åˆ‡å®æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 4 figures. Published in: Proceedings of the 12th International Conference on Emerging Trends in Engineering Technology Signal and Information Processing (ICETET SIP 2025) Note: The conference proceedings contain an outdated abstract due to a publisher-side error. This arXiv version includes the correct and updated abstract",
      "pdf_url": "https://arxiv.org/pdf/2511.12206v1",
      "published_date": "2025-11-15 13:18:17 UTC",
      "updated_date": "2025-11-15 13:18:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:29:04.884702+00:00"
    },
    {
      "arxiv_id": "2511.12203v1",
      "title": "Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps",
      "title_zh": "åŸºäºè·¯å¾„-éšœç¢ç‰©é‡å çš„çº¦æŸä½ç§»é—®é¢˜å±€éƒ¨æœ€ä¼˜è§£",
      "authors": [
        "Antony Thomas",
        "Fulvio Mastrogiovanni",
        "Marco Baglietto"
      ],
      "abstract": "We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººé€šè¿‡ç§»åŠ¨éšœç¢ç‰©å¯»æ‰¾å¯è¡Œè·¯å¾„çš„çº¦æŸä½ç§»é—®é¢˜(constraint displacement problems)ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§£å†³æ–¹æ³•ã€‚è¯¥è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œé¦–å…ˆé€šè¿‡æœ€å°åŒ–ç‰¹å®šçš„ç›®æ ‡å‡½æ•°ï¼Œè®¡ç®—å‡ºä¸€æ¡ç©¿è¿‡éšœç¢ç‰©çš„åˆå§‹æœºå™¨äººè½¨è¿¹ã€‚éšåï¼Œç³»ç»Ÿæ ¹æ®è·¯å¾„ä¸éšœç¢ç‰©çš„é‡å æƒ…å†µå¯¹éšœç¢ç‰©è¿›è¡Œä½ç§»å¤„ç†ï¼Œä»è€Œä½¿é¢„è®¾çš„æœºå™¨äººè½¨è¿¹å˜å¾—æ— ç¢°æ’ä¸”å¯è¡Œã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ±‚å¾—éšœç¢ç‰©ä½ç§»çš„å±€éƒ¨æœ€ä¼˜è§£(locally optimal solutions)ã€‚é€šè¿‡å¤šä¸ªç¤ºä¾‹ï¼Œç ”ç©¶æˆåŠŸåœ¨ä¸¤ç±»ä¸åŒçš„çº¦æŸä½ç§»é—®é¢˜ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚å—é™ç¯å¢ƒä¸‹çš„è·¯å¾„è§„åˆ’èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Robotics and Autonomous Systems",
      "pdf_url": "https://arxiv.org/pdf/2511.12203v1",
      "published_date": "2025-11-15 13:14:30 UTC",
      "updated_date": "2025-11-15 13:14:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:29:11.971077+00:00"
    },
    {
      "arxiv_id": "2511.14794v1",
      "title": "irace-evo: Automatic Algorithm Configuration Extended With LLM-Based Code Evolution",
      "title_zh": "irace-evoï¼šèåˆå¤§è¯­è¨€æ¨¡å‹ä»£ç æ¼”åŒ–çš„è‡ªåŠ¨ç®—æ³•é…ç½®æ‰©å±•",
      "authors": [
        "Camilo ChacÃ³n Sartori",
        "Christian Blum"
      ],
      "abstract": "Automatic algorithm configuration tools such as irace efficiently tune parameter values but leave algorithmic code unchanged. This paper introduces a first version of irace-evo, an extension of irace that integrates code evolution through large language models (LLMs) to jointly explore parameter and code spaces. The proposed framework enables multi-language support (e.g., C++, Python), reduces token consumption via progressive context management, and employs the Always-From-Original principle to ensure robust and controlled code evolution. We evaluate irace-evo on the Construct, Merge, Solve & Adapt (CMSA) metaheuristic for the Variable-Sized Bin Packing Problem (VSBPP). Experimental results show that irace-evo can discover new algorithm variants that outperform the state-of-the-art CMSA implementation while maintaining low computational and monetary costs. Notably, irace-evo generates competitive algorithmic improvements using lightweight models (e.g., Claude Haiku 3.5) with a total usage cost under 2 euros. These results demonstrate that coupling automatic configuration with LLM-driven code evolution provides a powerful, cost-efficient avenue for advancing heuristic design and metaheuristic optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†irace-evoï¼Œè¿™æ˜¯å¯¹è‡ªåŠ¨åŒ–ç®—æ³•é…ç½®å·¥å…·iraceçš„æ‰©å±•ï¼Œé€šè¿‡é›†æˆå¤§è¯­è¨€æ¨¡å‹(LLM)çš„ä»£ç æ¼”åŒ–(Code Evolution)åŠŸèƒ½ï¼Œå®ç°äº†å¯¹å‚æ•°ç©ºé—´å’Œä»£ç ç©ºé—´çš„å…±åŒæ¢ç´¢ã€‚è¯¥æ¡†æ¶æ”¯æŒC++å’ŒPythonç­‰å¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œé€šè¿‡æ¸è¿›å¼ä¸Šä¸‹æ–‡ç®¡ç†(Progressive Context Management)é™ä½Tokenæ¶ˆè€—ï¼Œå¹¶é‡‡ç”¨Always-From-OriginalåŸåˆ™ç¡®ä¿ä»£ç æ¼”åŒ–çš„ç¨³å¥æ€§ã€‚å®éªŒåœ¨å˜é•¿è£…ç®±é—®é¢˜(VSBPP)çš„CMSAå…ƒå¯å‘å¼ç®—æ³•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºirace-evoèƒ½å¤Ÿå‘ç°è¶…è¶Šç°æœ‰SOTAæ°´å¹³çš„æ–°ç®—æ³•å˜ä½“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä¾¿ä½¿ç”¨Claude Haiku 3.5ç­‰è½»é‡çº§æ¨¡å‹ï¼Œåœ¨æ€»æˆæœ¬ä½äº2æ¬§å…ƒçš„æƒ…å†µä¸‹ï¼Œirace-evoä»èƒ½å–å¾—æå…·ç«äº‰åŠ›çš„æ”¹è¿›ã€‚è¿™ä¸€æˆæœè¯æ˜äº†å°†è‡ªåŠ¨åŒ–é…ç½®ä¸LLMé©±åŠ¨çš„ä»£ç æ¼”åŒ–ç›¸ç»“åˆï¼Œæ˜¯æ¨åŠ¨å¯å‘å¼è®¾è®¡å’Œå…ƒå¯å‘å¼ä¼˜åŒ–çš„ä¸€ç§é«˜æ•ˆä¸”ä½æˆæœ¬çš„é€”å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14794v1",
      "published_date": "2025-11-15 12:42:18 UTC",
      "updated_date": "2025-11-15 12:42:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:29:10.971318+00:00"
    },
    {
      "arxiv_id": "2511.12176v2",
      "title": "Reinforcement Learning for Charging Optimization of Inhomogeneous Dicke Quantum Batteries",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„éå‡åŒ€ Dicke é‡å­ç”µæ± å……ç”µä¼˜åŒ–",
      "authors": [
        "Xiaobin Song",
        "Siyuan Bai",
        "Da-Wei Wang",
        "Hanxiao Tao",
        "Xizhe Wang",
        "Rebing Wu",
        "Benben Jiang"
      ],
      "abstract": "Charging optimization is a key challenge to the implementation of quantum batteries, particularly under inhomogeneity and partial observability. This paper employs reinforcement learning to optimize piecewise-constant charging policies for an inhomogeneous Dicke battery. We systematically compare policies across four observability regimes, from full-state access to experimentally accessible observables (energies of individual two-level systems (TLSs), first-order averages, and second-order correlations). Simulation results demonstrate that full observability yields near-optimal ergotropy with low variability, while under partial observability, access to only single-TLS energies or energies plus first-order averages lags behind the fully observed baseline. However, augmenting partial observations with second-order correlations recovers most of the gap, reaching 94%-98% of the full-state baseline. The learned schedules are nonmyopic, trading temporary plateaus or declines for superior terminal outcomes. These findings highlight a practical route to effective fast-charging protocols under realistic information constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éå‡åŒ€ Dicke é‡å­ç”µæ±  (Inhomogeneous Dicke Quantum Batteries) åœ¨ä¸å‡åŒ€æ€§å’Œéƒ¨åˆ†å¯è§‚æµ‹æ€§ä¸‹çš„å……ç”µä¼˜åŒ–æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä¼˜åŒ–åˆ†æ®µå¸¸æ•°å……ç”µç­–ç•¥çš„æ–¹æ³•ã€‚ç ”ç©¶è€…ç³»ç»Ÿåœ°æ¯”è¾ƒäº†ä»å…¨çŠ¶æ€è®¿é—®åˆ°å®éªŒå¯è§‚æµ‹å€¼ï¼ˆåŒ…æ‹¬å•ä¸ªäºŒèƒ½çº§ç³»ç»Ÿ energiesã€ä¸€é˜¶å¹³å‡å€¼å’ŒäºŒé˜¶ç›¸å…³æ€§ï¼‰çš„å››ç§ä¸åŒè§‚æµ‹æœºåˆ¶ä¸‹çš„ç­–ç•¥è¡¨ç°ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œå…¨è§‚æµ‹æ€§èƒ½å¯è¾¾åˆ°è¿‘ä¹æœ€ä¼˜çš„ ergotropyï¼Œè€Œä»…è®¿é—®å•ä¸ª TLS èƒ½é‡æˆ–ä¸€é˜¶å¹³å‡å€¼çš„éƒ¨åˆ†è§‚æµ‹ç­–ç•¥åˆ™æ˜æ˜¾è½åäºå…¨è§‚æµ‹åŸºå‡†ã€‚ç„¶è€Œï¼Œé€šè¿‡å¼•å…¥äºŒé˜¶ç›¸å…³æ€§ (second-order correlations) å¢å¼ºéƒ¨åˆ†è§‚æµ‹ä¿¡æ¯ï¼Œå¯ä»¥å¼¥è¡¥å¤§éƒ¨åˆ†æ€§èƒ½å·®è·ï¼Œè¾¾åˆ°å…¨çŠ¶æ€åŸºå‡†çš„ 94%-98%ã€‚å­¦ä¹ åˆ°çš„å……ç”µæ–¹æ¡ˆå±•ç°å‡ºéè¿‘è§†æ€§ (non-myopic) ç‰¹å¾ï¼Œå³é€šè¿‡ç‰ºç‰²ä¸´æ—¶çš„å¹³å°æœŸæˆ–ä¸‹é™æ¥æ¢å–æ›´ä¼˜çš„æœ€ç»ˆå……ç”µæ•ˆæœã€‚è¿™äº›å‘ç°ä¸ºåœ¨ç°å®ä¿¡æ¯çº¦æŸä¸‹å®ç°é«˜æ•ˆå¿«é€Ÿå……ç”µåè®®æä¾›äº†ä¸€æ¡å®ç”¨çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12176v2",
      "published_date": "2025-11-15 12:06:59 UTC",
      "updated_date": "2026-01-23 03:11:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:29:15.574860+00:00"
    },
    {
      "arxiv_id": "2511.12175v1",
      "title": "AI-Enhanced IoT Systems for Predictive Maintenance and Affordability Optimization in Smart Microgrids: A Digital Twin Approach",
      "title_zh": "é¢å‘æ™ºèƒ½å¾®ç”µç½‘é¢„æµ‹æ€§ç»´æŠ¤ä¸ç»æµæ€§ä¼˜åŒ–çš„äººå·¥æ™ºèƒ½å¢å¼ºå‹ç‰©è”ç½‘ç³»ç»Ÿï¼šåŸºäºæ•°å­—å­ªç”Ÿçš„æ–¹æ³•",
      "authors": [
        "Koushik Ahmed Kushal",
        "Florimond Gueniat"
      ],
      "abstract": "This study presents an AI enhanced IoT framework for predictive maintenance and affordability optimization in smart microgrids using a Digital Twin modeling approach. The proposed system integrates real time sensor data, machine learning based fault prediction, and cost aware operational analytics to improve reliability and energy efficiency in distributed microgrid environments. By synchronizing physical microgrid components with a virtual Digital Twin, the framework enables early detection of component degradation, dynamic load management, and optimized maintenance scheduling. Experimental evaluations demonstrate improved predictive accuracy, reduced operational downtime, and measurable cost savings compared to baseline microgrid management methods. The findings highlight the potential of Digital Twin driven IoT architectures as a scalable solution for next generation intelligent and affordable energy systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ•°å­—å­ªç”Ÿ(Digital Twin)å»ºæ¨¡æ–¹æ³•çš„AIå¢å¼ºå‹ç‰©è”ç½‘(IoT)æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºæ™ºèƒ½å¾®ç”µç½‘(Smart Microgrids)çš„é¢„æµ‹æ€§ç»´æŠ¤ä¸å¯è´Ÿæ‹…æ€§ä¼˜åŒ–ã€‚è¯¥ç³»ç»Ÿé€šè¿‡é›†æˆå®æ—¶ä¼ æ„Ÿå™¨æ•°æ®ã€åŸºäºæœºå™¨å­¦ä¹ (Machine Learning)çš„æ•…éšœé¢„æµ‹ä»¥åŠæˆæœ¬æ„ŸçŸ¥å‹è¿è¥åˆ†æï¼Œæœ‰æ•ˆæå‡äº†åˆ†å¸ƒå¼å¾®ç”µç½‘ç¯å¢ƒçš„å¯é æ€§ä¸èƒ½æºæ•ˆç‡ã€‚é€šè¿‡å®ç°ç‰©ç†ç»„ä»¶ä¸è™šæ‹Ÿæ•°å­—å­ªç”Ÿçš„å®æ—¶åŒæ­¥ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ”¯æŒç»„ä»¶é€€åŒ–çš„æ—©æœŸæ£€æµ‹ã€åŠ¨æ€è´Ÿè·ç®¡ç†ä»¥åŠä¼˜åŒ–çš„ç»´æŠ¤è®¡åˆ’è°ƒåº¦ã€‚å®éªŒè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸ä¼ ç»Ÿå¾®ç”µç½‘ç®¡ç†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—æé«˜äº†é¢„æµ‹ç²¾åº¦å¹¶å‡å°‘äº†è¿è¥åœæœºæ—¶é—´ï¼Œä»è€Œå®ç°äº†æ˜ç¡®çš„æˆæœ¬èŠ‚çº¦ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç”±æ•°å­—å­ªç”Ÿé©±åŠ¨çš„ç‰©è”ç½‘æ¶æ„ä¸ºæ„å»ºä¸‹ä¸€ä»£æ™ºèƒ½ä¸”ç»æµçš„èƒ½æºç³»ç»Ÿæä¾›äº†ä¸€ç§å…·å¤‡å¯æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "12 pages, 6 figures, includes simulation and evaluation results",
      "pdf_url": "https://arxiv.org/pdf/2511.12175v1",
      "published_date": "2025-11-15 12:06:47 UTC",
      "updated_date": "2025-11-15 12:06:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:29:21.883247+00:00"
    },
    {
      "arxiv_id": "2511.12170v2",
      "title": "Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective",
      "title_zh": "é‡æ–°å®¡è§†å¤šæ¨¡æ€ç‚¹äº‘è¡¥å…¨ï¼šä¸€ç§â€œä»¥ä¿®æ­£ä¿ƒè¡¥å…¨â€çš„è§†è§’",
      "authors": [
        "Wang Luo",
        "Di Wu",
        "Hengyuan Na",
        "Yinlin Zhu",
        "Miao Hu",
        "Guocong Quan"
      ],
      "abstract": "Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€ç‚¹äº‘è¡¥å…¨(Point Cloud Completion)ä»»åŠ¡ä¸­å­˜åœ¨çš„ç»“æ„ä¸ä¸€è‡´å’Œæ‹“æ‰‘ä¼ªå½±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„â€œé€šè¿‡ä¿®æ­£å®ç°è¡¥å…¨â€(Completion-by-Correction)èŒƒå¼ã€‚ä¸ä¼ ç»Ÿçš„â€œé€šè¿‡ä¿®å¤å®ç°è¡¥å…¨â€(Completion-by-Inpainting)åˆæˆæ–¹å¼ä¸åŒï¼Œè¯¥èŒƒå¼åˆ©ç”¨é¢„è®­ç»ƒå›¾åƒè½¬3Dæ¨¡å‹ç”Ÿæˆçš„æ‹“æ‰‘å®Œæ•´å½¢çŠ¶å…ˆéªŒ(Shape Prior)ä½œä¸ºèµ·ç‚¹ï¼Œé€šè¿‡ç‰¹å¾ç©ºé—´ä¿®æ­£å°†å…¶ä¸å®é™…çš„éƒ¨åˆ†è§‚æµ‹æ•°æ®å¯¹é½ã€‚åŸºäºæ­¤èŒƒå¼è®¾è®¡çš„PGNetæ¡†æ¶é‡‡ç”¨åŒç‰¹å¾ç¼–ç (Dual-Feature Encoding)æ¥é”šå®šç”Ÿæˆå…ˆéªŒï¼Œå¹¶åˆ©ç”¨åˆ†å±‚ä¿®æ­£(Hierarchical Correction)ç­–ç•¥é€æ­¥ä¼˜åŒ–å‡ ä½•ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPGNetåœ¨ShapeNetViPCæ•°æ®é›†ä¸Šçš„å€’è§’è·ç¦»(Chamfer Distance)é™ä½äº†23.5%ï¼ŒF-scoreæå‡äº†7.1%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹ï¼Œå®ç°äº†ç»“æ„ä¸€è‡´ä¸”é«˜åº¦å¯¹é½çš„3Dé‡å»ºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.12170v2",
      "published_date": "2025-11-15 11:51:13 UTC",
      "updated_date": "2025-12-01 11:06:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:29:33.968529+00:00"
    },
    {
      "arxiv_id": "2511.12169v2",
      "title": "Incremental Maintenance of DatalogMTL Materialisations",
      "title_zh": "DatalogMTL ç‰©åŒ–çš„å¢é‡ç»´æŠ¤",
      "authors": [
        "Kaiyue Zhao",
        "Dingqi Chen",
        "Shaoyu Wang",
        "Pan Hu"
      ],
      "abstract": "DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DRedMTLï¼Œä¸€ç§é’ˆå¯¹å¸¦ç•Œé™åŒºé—´çš„ DatalogMTL çš„å¢é‡æ¨ç†ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨ç†æ–¹æ³•åœ¨å¤„ç†åŠ¨æ€æ•°æ®æ›´æ–°æ—¶æ•ˆç‡ä¸è¶³çš„é—®é¢˜ã€‚DatalogMTL å°†ä¼ ç»Ÿçš„ Datalog ä¸åº¦é‡æ—¶åºé€»è¾‘ (MTL) ç»“åˆï¼Œèƒ½å¤Ÿå¯¹æ—¶åºæ•°æ®è¿›è¡Œè¡¨è¾¾åŠ›å¼ºçš„æ¨ç†ã€‚è¯¥ç®—æ³•åŸºäºç»å…¸çš„ DRed ç®—æ³•ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„æ“ä½œç®—å­æ¥å¤„ç† DatalogMTL å®ä½“åŒ– (Materialisation) ä¸­ç‰¹æœ‰çš„äº‹å®é›†åˆä¸å‘¨æœŸæ€§åŒºé—´è¡¨ç¤ºã€‚è¿™äº›ç®—å­ç¡®ä¿äº†ç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆåœ°ç»´æŠ¤ç”±äºæ•°æ®å˜åŠ¨å¼•èµ·çš„æ¨ç†ç»“æœæ›´æ–°ï¼Œè€Œæ— éœ€è¿›è¡Œè€—æ—¶çš„é‡æ–°å®ä½“åŒ– (rematerialisation)ã€‚åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDRedMTL çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹æ•ˆç‡æå‡ç”šè‡³è¾¾åˆ°æ•°ä¸ªæ•°é‡çº§ã€‚è¯¥æˆæœä¸ºæ¶‰åŠé¢‘ç¹æ•°æ®å˜åŠ¨çš„å®æ—¶æ—¶åºæ¨ç†åº”ç”¨æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as oral paper at the main track of AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.12169v2",
      "published_date": "2025-11-15 11:45:19 UTC",
      "updated_date": "2025-11-19 16:30:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:29:43.767455+00:00"
    },
    {
      "arxiv_id": "2511.12154v1",
      "title": "Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions",
      "title_zh": "å¼€æ”¾é“¶è¡ŒåŸºç¡€æ¨¡å‹ï¼šåŸºäºå°‘é‡é‡‘èäº¤æ˜“çš„è¯­è¨€è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Gustavo Polleti",
        "Marlesson Santana",
        "Eduardo Fontes"
      ],
      "abstract": "We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§é’ˆå¯¹é‡‘èäº¤æ˜“çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ (Multimodal Foundational Model)ï¼Œå°†ç»“æ„åŒ–å±æ€§å’Œéç»“æ„åŒ–æ–‡æœ¬æè¿°æ•´åˆä¸ºç»Ÿä¸€çš„è¡¨å¾ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ©ç è¯­è¨€å»ºæ¨¡ (Masked Language Modeling) é€‚é…äºäº¤æ˜“åºåˆ—ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†é‡‘èæ•°æ®æ—¶æ¯”ä¼ ç»Ÿçš„ç‰¹å¾å·¥ç¨‹å’Œç¦»æ•£äº‹ä»¶åºåˆ—æ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚ç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„å¼€æ”¾é“¶è¡Œ (Open Banking) åœºæ™¯ä¸‹ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºäº†å“è¶Šçš„æœ‰æ•ˆæ€§ã€‚ä½œä¸ºé¦–ä¸ªé’ˆå¯¹åŒ—ç¾æ•°åƒå®¶é‡‘èæœºæ„å¼€å±•çš„å¤§è§„æ¨¡ç ”ç©¶ï¼Œè¯¥æˆæœéªŒè¯äº†å¤šæ¨¡æ€è¡¨å¾åœ¨ä¸åŒåœ°åŸŸå’Œæœºæ„é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†è‡ªç›‘ç£æ¨¡å‹ (Self-supervised Models) åœ¨æå‡æ¬ºè¯ˆæ£€æµ‹ã€ä¿¡ç”¨é£é™©è¯„ä¼°åŠå®¢æˆ·æ´å¯Ÿç­‰é‡‘èåº”ç”¨æ–¹é¢çš„æ ¸å¿ƒæ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12154v1",
      "published_date": "2025-11-15 10:52:39 UTC",
      "updated_date": "2025-11-15 10:52:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:29:39.286212+00:00"
    },
    {
      "arxiv_id": "2511.12149v1",
      "title": "AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models",
      "title_zh": "AttackVLAï¼šé¢å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„å¯¹æŠ—æ”»å‡»ä¸åé—¨æ”»å‡»åŸºå‡†æµ‹è¯•",
      "authors": [
        "Jiayu Li",
        "Yunhan Zhao",
        "Xiang Zheng",
        "Zonghuan Xu",
        "Yige Li",
        "Xingjun Ma",
        "Yu-Gang Jiang"
      ],
      "abstract": "Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AttackVLAï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹Vision-Language-Action (VLA)æ¨¡å‹å®‰å…¨æ€§çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–äº†æ•°æ®æ„å»ºã€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„å…¨ç”Ÿå‘½å‘¨æœŸã€‚é’ˆå¯¹ç°æœ‰VLAæ”»å‡»æŠ€æœ¯ç¼ºä¹æ ‡å‡†åŒ–ã€éš¾ä»¥è·¨æ¶æ„æ¯”è¾ƒä¸”çœŸå®ä¸–ç•ŒéªŒè¯ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å¤šç§ç°æœ‰åŠæ”¹ç¼–è‡ªè§†è§‰è¯­è¨€æ¨¡å‹çš„æ”»å‡»æ‰‹æ®µã€‚ç ”ç©¶å‘ç°å½“å‰æ–¹æ³•å¤šå¯¼è‡´éç›®æ ‡æ€§å¤±è´¥ï¼Œç¼ºä¹å¯¹é•¿æ—¶ç¨‹(long-horizon)ç²¾ç¡®åŠ¨ä½œåºåˆ—çš„æ“æ§èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…è¿›ä¸€æ­¥æå‡ºäº†BackdoorVLAåé—¨æ”»å‡»ï¼Œé€šè¿‡æ¤å…¥è§¦å‘å™¨å¼ºåˆ¶æ¨¡å‹æ‰§è¡Œç‰¹å®šçš„é•¿æ—¶ç¨‹åŠ¨ä½œåºåˆ—ã€‚å®éªŒåœ¨ä»¿çœŸå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­éªŒè¯äº†æ”»å‡»çš„æœ‰æ•ˆæ€§ï¼ŒBackdoorVLAå®ç°äº†58.4%çš„å¹³å‡æˆåŠŸç‡ï¼Œéƒ¨åˆ†ä»»åŠ¡æˆåŠŸç‡è¾¾100%ã€‚è¯¥å·¥ä½œä¸ºè¡¡é‡VLAæ¨¡å‹çš„å®‰å…¨è„†å¼±æ€§æä¾›äº†æ ‡å‡†åŒ–å·¥å…·ï¼Œå¹¶æ­ç¤ºäº†å…·èº«æ™ºèƒ½ç³»ç»Ÿé¢ä¸´çš„ç²¾ç¡®æ“æ§é£é™©ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12149v1",
      "published_date": "2025-11-15 10:30:46 UTC",
      "updated_date": "2025-11-15 10:30:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:29:47.698777+00:00"
    },
    {
      "arxiv_id": "2511.13775v1",
      "title": "Known Meets Unknown: Mitigating Overconfidence in Open Set Recognition",
      "title_zh": "å·²çŸ¥é‡è§æœªçŸ¥ï¼šç¼“è§£å¼€æ”¾é›†è¯†åˆ«ä¸­çš„è¿‡åº¦è‡ªä¿¡",
      "authors": [
        "Dongdong Zhao",
        "Ranxin Fang",
        "Changtian Song",
        "Zhihui Liu",
        "Jianwen Xiang"
      ],
      "abstract": "Open Set Recognition (OSR) requires models not only to accurately classify known classes but also to effectively reject unknown samples. However, when unknown samples are semantically similar to known classes, inter-class overlap in the feature space often causes models to assign unjustifiably high confidence to them, leading to misclassification as known classes -- a phenomenon known as overconfidence. This overconfidence undermines OSR by blurring the decision boundary between known and unknown classes. To address this issue, we propose a framework that explicitly mitigates overconfidence caused by inter-class overlap. The framework consists of two components: a perturbation-based uncertainty estimation module, which applies controllable parameter perturbations to generate diverse predictions and quantify predictive uncertainty, and an unknown detection module with distinct learning-based classifiers, implemented as a two-stage procedure, which leverages the estimated uncertainty to improve discrimination between known and unknown classes, thereby enhancing OSR performance. Experimental results on three public datasets show that the proposed framework achieves superior performance over existing OSR methods.",
      "tldr_zh": "å¼€æ”¾é›†è¯†åˆ«(Open Set Recognition, OSR)è¦æ±‚æ¨¡å‹åœ¨åˆ†ç±»å·²çŸ¥ç±»åˆ«çš„åŒæ—¶æœ‰æ•ˆæ‹’ç»æœªçŸ¥æ ·æœ¬ï¼Œä½†è¯­ä¹‰ç›¸ä¼¼æ€§å¯¼è‡´çš„è¿‡åº¦è‡ªä¿¡(Overconfidence)ç°è±¡å¸¸ä½¿æ¨¡å‹äº§ç”Ÿé”™è¯¯çš„é¢„æµ‹ä¿¡å¿ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨æ˜¾å¼ç¼“è§£ç”±ç±»åˆ«é‡å å¼•èµ·è¿‡åº¦è‡ªä¿¡çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±åŸºäºæ‰°åŠ¨çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ¨¡å—(Perturbation-based uncertainty estimation module)å’ŒæœªçŸ¥æ£€æµ‹æ¨¡å—(Unknown detection module)æ„æˆã€‚å‰è€…åˆ©ç”¨å¯æ§çš„å‚æ•°æ‰°åŠ¨é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œåè€…åˆ™é€šè¿‡ä¸¤é˜¶æ®µç¨‹åºåˆ©ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡æ¥å¢å¼ºå¯¹å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«çš„åŒºåˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„ Open Set Recognition æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.13775v1",
      "published_date": "2025-11-15 09:56:44 UTC",
      "updated_date": "2025-11-15 09:56:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:30:02.466125+00:00"
    },
    {
      "arxiv_id": "2511.12135v2",
      "title": "RTMol: Rethinking Molecule-text Alignment in a Round-trip View",
      "title_zh": "RTMolï¼šä»å¾€è¿”è§†è§’é‡æ–°å®¡è§†åˆ†å­-æ–‡æœ¬å¯¹é½",
      "authors": [
        "Letian Chen",
        "Runhan Shi",
        "Gufeng Yu",
        "Yang Yang"
      ],
      "abstract": "Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RTMolï¼Œä¸€ä¸ªé€šè¿‡è‡ªç›‘ç£å›ç¯å­¦ä¹ (self-supervised round-trip learning)ç»Ÿä¸€åˆ†å­æè¿°(molecular captioning)ä¸text-to-SMILESç”Ÿæˆçš„åŒå‘å¯¹é½æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨åŒ–å­¦å‡†ç¡®æ€§ã€æ•°æ®æ­§ä¹‰åŠåŒå‘ä¸€è‡´æ€§æ–¹é¢çš„å±€é™ï¼ŒRTMolå¼•å…¥äº†åˆ›æ–°çš„å›ç¯è¯„ä¼°æŒ‡æ ‡(round-trip evaluation metrics)ï¼Œå¹¶æ”¯æŒåœ¨æ— éœ€æˆå¯¹åˆ†å­-æ–‡æœ¬è¯­æ–™çš„æƒ…å†µä¸‹è¿›è¡Œåˆ†å­æè¿°çš„æ— ç›‘ç£è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§LLMsä¸Šå°†åŒå‘å¯¹é½æ€§èƒ½æå‡äº†é«˜è¾¾47%ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸBLEUæŒ‡æ ‡åé‡è¯­è¨€æµç•…åº¦è€ŒéåŒ–å­¦ç²¾ç¡®æ€§çš„é—®é¢˜ã€‚RTMolä¸ºåˆ†å­ä¸æ–‡æœ¬çš„æ·±åº¦ç†è§£ä¸è”åˆç”Ÿæˆå»ºç«‹äº†ä¸€ç§é«˜æ•ˆçš„æ–°èŒƒå¼ï¼Œå¯¹äºè¯ç‰©å‘ç°å’Œææ–™è®¾è®¡ç­‰é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-bio.BM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12135v2",
      "published_date": "2025-11-15 09:55:55 UTC",
      "updated_date": "2025-11-21 09:48:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:30:03.689421+00:00"
    },
    {
      "arxiv_id": "2511.12131v1",
      "title": "OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description",
      "title_zh": "OAD-Promoterï¼šåˆ©ç”¨ç‰©ä½“å±æ€§æè¿°å¢å¼ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬è§†è§‰é—®ç­”",
      "authors": [
        "Quanxing Xu",
        "Ling Zhou",
        "Feifei Zhang",
        "Jinyu Tian",
        "Rubing Huang"
      ],
      "abstract": "Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OAD-Promoterï¼Œä¸€ç§æ—¨åœ¨é€šè¿‡å‡è½»è¯­è¨€åå·®å’Œæé«˜é¢†åŸŸæ¼‚ç§»é²æ£’æ€§æ¥å¢å¼ºåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è§†è§‰é—®ç­”(VQA)æ€§èƒ½çš„æ–°æ–¹æ³•ã€‚è¯¥æ¡†æ¶åŒ…å«å¯¹è±¡é›†ä¸­ç¤ºä¾‹ç”Ÿæˆ(OEG)æ¨¡å—ã€è®°å¿†çŸ¥è¯†è¾…åŠ©(MKA)æ¨¡å—ä»¥åŠOAD Promptä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚OEGæ¨¡å—é€šè¿‡ç”Ÿæˆå…¨å±€æ ‡é¢˜å’Œå¯¹è±¡é›†ä¸­æ ·æœ¬ï¼Œåˆ©ç”¨äº’è¡¥çš„å…¨å±€ä¸åŒºåŸŸè§†è§‰çº¿ç´¢æ¥å¢å¼ºè§†è§‰ä¿¡æ¯è¾“å…¥å¹¶æŠ‘åˆ¶è¯­è¨€åè§ã€‚MKAæ¨¡å—é€šè¿‡ä»å­˜å‚¨ç¤ºä¾‹ä¸­æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼ŒååŠ©æ¨¡å‹æœ‰æ•ˆå¤„ç†åˆ†å¸ƒå¤–(OOD)æ ·æœ¬ï¼Œä»è€Œæå‡å…¶åœ¨æœªçŸ¥é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€ç»ˆé€šè¿‡OAD Promptæ•´åˆå„æ¨¡å—è¾“å‡ºä»¥ä¼˜åŒ–LLMçš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOAD-Promoteråœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬è®¾ç½®ä¸‹æ˜¾è‘—æå‡äº†ç°æœ‰VQAæ–¹æ³•çš„æ€§èƒ½ï¼Œå¹¶å–å¾—äº†æ–°çš„SOTAç ”ç©¶æˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.12131v1",
      "published_date": "2025-11-15 09:37:12 UTC",
      "updated_date": "2025-11-15 09:37:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:29:53.563929+00:00"
    },
    {
      "arxiv_id": "2511.17577v1",
      "title": "Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation",
      "title_zh": "åŸºäºåŠ¨æ€å‰ªæä¸çŸ¥è¯†è’¸é¦çš„é«˜æ•ˆæ•°å­¦æ¨ç†æ¨¡å‹",
      "authors": [
        "Fengming Yu",
        "Qingyu Meng",
        "Haiwei Pan",
        "Kejia Zhang"
      ],
      "abstract": "With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è®¡ç®—å’Œå­˜å‚¨æˆæœ¬è¿‡é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆåŠ¨æ€æ³¨æ„åŠ›å¤´å‰ªæ (Dynamic Attention Head Pruning) ä¸çŸ¥è¯†è’¸é¦ (Knowledge Distillation) çš„è½»é‡åŒ–ä¼˜åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆæƒé‡èŒƒæ•° (Weight Norms) å’Œç†µ (Entropy) åŠ¨æ€è¯„ä¼°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­å„å¤´çš„é‡è¦æ€§ï¼Œå¹¶å®æ—¶å‰ªæå†—ä½™å¤´ä»¥æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚ä¸ºäº†ç¼“è§£æ€§èƒ½é€€åŒ–ï¼Œç ”ç©¶åˆ©ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯å°†åŸå§‹æ¨¡å‹çš„ä¿¡æ¯è¿ç§»è‡³å‰ªæåçš„å­¦ç”Ÿæ¨¡å‹ï¼Œç¡®ä¿å°è§„æ¨¡æ¨¡å‹ä»èƒ½ä¿ç•™å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨ Math23k å’Œ ASDiv-A æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ 30% çš„å‰ªæç‡ä¸‹å¯å‡å°‘ 18.7% çš„å‚æ•°å¹¶æå‡ 27.5% çš„æ¨ç†é€Ÿåº¦ï¼Œè€Œå‡†ç¡®ç‡ä»…ä¸‹é™ 0.7%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ°´å¹³æ•°å­¦æ¨ç†æ€§èƒ½çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†å®ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2511.17577v1",
      "published_date": "2025-11-15 09:21:44 UTC",
      "updated_date": "2025-11-15 09:21:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:30:02.670661+00:00"
    },
    {
      "arxiv_id": "2511.12116v1",
      "title": "LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models",
      "title_zh": "LLMLagBenchï¼šè¯†åˆ«å¤§è¯­è¨€æ¨¡å‹çš„æ—¶é—´è®­ç»ƒè¾¹ç•Œ",
      "authors": [
        "Piotr PÄ™zik",
        "Konrad KaczyÅ„ski",
        "Maria SzymaÅ„ska",
        "Filip Å»arnecki",
        "Zuzanna Deckert",
        "Jakub Kwiatkowski",
        "Wojciech Janowski"
      ],
      "abstract": "Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨é¢„è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨çš„æ—¶é—´æˆªæ­¢ç‚¹(temporal cutoff)é—®é¢˜ï¼ŒæŒ‡å‡ºæœªçŸ¥çš„çŸ¥è¯†è¾¹ç•Œå¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­æ··æ·†è¿‡æ—¶ä¿¡æ¯å¹¶é™ä½å‡†ç¡®æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†LLMLagBenchï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è¯†åˆ«æ¨¡å‹è®­ç»ƒæ•°æ®æ—¶é—´è¾¹ç•Œçš„æ–°é²œåº¦åŸºå‡†æµ‹è¯•(freshness benchmark)ï¼Œé€šè¿‡è¯„ä¼°æ¨¡å‹å¯¹è¿‘æœŸäº‹ä»¶çš„çŸ¥è¯†æ¥ç¡®å®šå…¶å¯èƒ½çš„è®­ç»ƒæˆªæ­¢æ—¶é—´ã€‚ç ”ç©¶å›¢é˜Ÿå°†è¯¥åŸºå‡†åº”ç”¨äºè¯„ä¼°å¤§é‡å·²å£°æ˜æˆ–æœªå£°æ˜æˆªæ­¢æ—¥æœŸçš„LLMsï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ã€‚é€šè¿‡äººå·¥æ ¡éªŒåŠä¸å…¬å¼€é¢„è®­ç»ƒä¿¡æ¯çš„å¯¹æ¯”ï¼Œç»“æœè¯å®äº†LLMLagBenchåœ¨è¯†åˆ«æ—¶é—´è¾¹ç•Œæ–¹é¢çš„å¯é æ€§ã€‚è¯¥åŸºå‡†ä¸ºé‡åŒ–å’Œç›‘æµ‹å¤§è¯­è¨€æ¨¡å‹çš„æ—¶æ•ˆæ€§çŸ¥è¯†æä¾›äº†ç³»ç»ŸåŒ–çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12116v1",
      "published_date": "2025-11-15 09:08:10 UTC",
      "updated_date": "2025-11-15 09:08:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:30:59.081848+00:00"
    },
    {
      "arxiv_id": "2511.12113v1",
      "title": "MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization",
      "title_zh": "MetaGDPOï¼šé€šè¿‡ç»„ç›´æ¥åå¥½ä¼˜åŒ–åˆ©ç”¨å…ƒè®¤çŸ¥çŸ¥è¯†ç¼“è§£ç¾éš¾æ€§é—å¿˜",
      "authors": [
        "Lanxue Zhang",
        "Yuqiang Xie",
        "Fang Fang",
        "Fanglong Dong",
        "Rui Liu",
        "Yanan Cao"
      ],
      "abstract": "Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 8B ä»¥ä¸‹çš„å°å‹å¤§è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¸¸è§çš„ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)é—®é¢˜ï¼Œæå‡ºäº†åä¸º MetaGDPO çš„ç»¼åˆè§£å†³æ–¹æ¡ˆã€‚åœ¨æ•°æ®å±‚é¢ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŒ…å« 5000 ä¸ªå®ä¾‹ä¸”èå…¥å…ƒè®¤çŸ¥çŸ¥è¯†(metacognitive knowledge)çš„ä»»åŠ¡æ•°æ®é›†ï¼Œé€šè¿‡æ ‡æ³¨ä»»åŠ¡æ‰€éœ€çš„ç‰¹å®šçŸ¥è¯†å¹¶ç»“åˆæ¨¡å‹å›ºæœ‰èƒ½åŠ›è¿›è¡Œç­›é€‰ï¼Œä»¥æé«˜å°å‹æ¨¡å‹å¯¹çŸ¥è¯†è’¸é¦çš„å¸æ”¶æ•ˆç‡ã€‚åœ¨ç®—æ³•å±‚é¢ï¼Œç ”ç©¶å¼•å…¥äº† GDPO (Group Direct Preference Optimization)ï¼Œè¿™æ˜¯ä¸€ç§èƒ½åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹é«˜æ•ˆé€¼è¿‘ GRPO æ€§èƒ½çš„ä¼˜åŒ–æ–¹æ³•ã€‚é€šè¿‡å¤§å‹æ¨¡å‹çš„å¼•å¯¼ä»¥åŠå‚è€ƒæ¨¡å‹å¯¹ä¼˜åŒ–è·¯å¾„çš„éšå¼çº¦æŸï¼ŒGDPO å®ç°äº†æ›´æœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»å¹¶æŠ‘åˆ¶äº†å‚æ•°æ¼‚ç§»(parameter drift)ã€‚å¤šé¡¹å®éªŒç»“æœè¯æ˜ï¼ŒMetaGDPO ä¸ä»…æ˜¾è‘—ç¼“è§£äº†ç¾éš¾æ€§é—å¿˜ï¼Œè¿˜å¤§å¹…æå‡äº†å°å‹æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 10 figures, AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.12113v1",
      "published_date": "2025-11-15 09:06:23 UTC",
      "updated_date": "2025-11-15 09:06:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:30:09.283358+00:00"
    },
    {
      "arxiv_id": "2511.12110v1",
      "title": "MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images",
      "title_zh": "MediRoundï¼šåŒ»å­¦å›¾åƒå¤šè½®å®ä½“çº§æ¨ç†åˆ†å‰²",
      "authors": [
        "Qinyue Tong",
        "Ziqian Lu",
        "Jun Liu",
        "Rui Zuo",
        "Zheming Lu"
      ],
      "abstract": "Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸç°æœ‰æ–¹æ³•ç¼ºä¹äº¤äº’æ€§ä»¥åŠæ–‡æœ¬æç¤ºåˆ†å‰²ä»…å±€é™äºå•è½®å¯¹è¯çš„é—®é¢˜ï¼Œæå‡ºäº†å¤šè½®å®ä½“çº§åŒ»å­¦æ¨ç†åˆ†å‰²(Multi-Round Entity-Level Medical Reasoning Segmentation, MEMR-Seg)è¿™ä¸€æ–°ä»»åŠ¡ã€‚ä¸ºäº†æ”¯æŒè¯¥ä»»åŠ¡ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†å¤§å‹æ•°æ®é›† MR-MedSegï¼ŒåŒ…å« 177K ä¸ªæ¶‰åŠè·¨è½®æ¬¡å®ä½“æ¨ç†çš„åŒ»å­¦åˆ†å‰²å¯¹è¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº† MediRound æ¨¡å‹ä½œä¸ºè¯¥ä»»åŠ¡çš„æœ‰æ•ˆåŸºçº¿ï¼Œå¹¶ç‰¹åˆ«å¼•å…¥äº†ä¸€ç§è½»é‡çº§ä¸”æœ‰æ•ˆçš„åˆ¤æ–­ä¸çº æ­£æœºåˆ¶(Judgment & Correction Mechanism)ï¼Œä»¥ç¼“è§£å¤šè½®åˆ†å‰²æ¨ç†è¿‡ç¨‹ä¸­å¸¸è§çš„é”™è¯¯ä¼ æ’­é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMediRound åœ¨å¤„ç† MEMR-Seg ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…¶æ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„åŒ»å­¦æŒ‡ä»£åˆ†å‰²æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.12110v1",
      "published_date": "2025-11-15 08:59:21 UTC",
      "updated_date": "2025-11-15 08:59:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:31:10.680750+00:00"
    },
    {
      "arxiv_id": "2511.12101v1",
      "title": "Decoupled Action Head: Confining Task Knowledge to Conditioning Layers",
      "title_zh": "è§£è€¦å¼åŠ¨ä½œå¤´ï¼šå°†ä»»åŠ¡çŸ¥è¯†å±€é™äºæ¡ä»¶å±‚",
      "authors": [
        "Jian Zhou",
        "Sihao Lin",
        "Shuai Fu",
        "Qi WU"
      ],
      "abstract": "Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Behavior Cloning (BC) å’Œ Diffusion Policy (DP) åœ¨æœºå™¨äººæ“ä½œä¸­é¢ä¸´çš„æ•°æ®ç¨€ç¼ºåŠå†…éƒ¨æœºåˆ¶ç†è§£ä¸è¶³ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ Decoupled Action Head è®­ç»ƒæ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨å‡ ä¹é›¶æˆæœ¬çš„è¿åŠ¨å­¦ç”Ÿæˆè½¨è¿¹ (Kinematics-generated trajectories) ä½œä¸ºæ— è§‚æµ‹æ•°æ®æ¥é¢„è®­ç»ƒé€šç”¨çš„åŠ¨ä½œå¤´ï¼Œéšåå°†å…¶å†»ç»“å¹¶é€šè¿‡ç‰¹å¾è°ƒåˆ¶ (Feature Modulation) é€‚åº”æ–°ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–åœºæ™¯ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œä¸”èƒ½å°† DP-C çš„è®­ç»ƒæ•ˆç‡æå‡è¾¾ 41%ã€‚ç ”ç©¶é€šè¿‡è§£è€¦åˆ†æå‘ç°ï¼Œä»»åŠ¡ç‰¹å®šçŸ¥è¯†ä¸»è¦å±€é™äºè°ƒèŠ‚ç»„ä»¶ä¸­ï¼Œè€ŒåŠ¨ä½œç”Ÿæˆéª¨å¹²ç½‘ç»œçš„ä½œç”¨ç›¸å¯¹æœ‰é™ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº† DP-MLPï¼Œé€šè¿‡ä»… 4M å‚æ•°çš„ MLP å—æ›¿ä»£äº†åŸæœ‰çš„ 244M å‚æ•° U-Net éª¨å¹²ï¼Œåœ¨è§£è€¦æ¨¡å¼ä¸‹å®ç°äº† 89.1% çš„è®­ç»ƒé€Ÿåº¦æå‡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12101v1",
      "published_date": "2025-11-15 08:39:50 UTC",
      "updated_date": "2025-11-15 08:39:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:31:17.181973+00:00"
    },
    {
      "arxiv_id": "2511.12089v1",
      "title": "KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything",
      "title_zh": "KrwEmdï¼šä¿®æ­£â€œå®Œå…¨é—å¿˜â€å¼éå®Œç¾å›å¿†æŠ½è±¡",
      "authors": [
        "Yanchang Fu",
        "Qiyue Yin",
        "Shengda Liu",
        "Pei Xu",
        "Kaiqi Huang"
      ],
      "abstract": "Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KrwEmdç®—æ³•ï¼Œæ—¨åœ¨è§£å†³å¾·å·æ‰‘å…‹ç­‰å¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯åšå¼ˆä¸­å› è¿‡åº¦æŠ½è±¡(Excessive abstraction)å¯¼è‡´AIæ€§èƒ½å—æŸçš„é—®é¢˜ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„ä¸å®Œå…¨å›å¿†æŠ½è±¡(Imperfect-recall abstraction)ç”±äºå®Œå…¨ä¸¢å¼ƒå†å²ä¿¡æ¯ï¼Œä¸¥é‡å½±å“äº†å†³ç­–è´¨é‡ã€‚ä¸ºæ­¤ï¼ŒKrwEmdå¼•å…¥äº†k-recall winrateç‰¹å¾ï¼Œè¯¥ç‰¹å¾é€šè¿‡ç»“åˆæœªæ¥ä¿¡æ¯ä¸å…³é”®çš„å†å²åšå¼ˆä¿¡æ¯ï¼Œèƒ½å¤Ÿå®šæ€§åŒºåˆ†ä¿¡å·è§‚æµ‹ä¿¡æ¯é›†å¹¶å®šé‡æ•æ‰å…¶ç›¸ä¼¼æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒKrwEmdåˆ©ç”¨Earth Mover's Distance(æ¨åœŸæœºè·ç¦»)æ¥è¡¡é‡ç‰¹å¾é—´çš„å·®å¼‚å¹¶è¿›è¡Œä¿¡æ¯é›†èšç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKrwEmdåœ¨å®é™…åšå¼ˆä¸­æ˜¾è‘—æå‡äº†AIçš„æ€§èƒ½ï¼Œä¼˜äºç°æœ‰çš„æŠ½è±¡ç®—æ³•ï¼Œä¸ºè§£å†³åšå¼ˆè®ºä¸­çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12089v1",
      "published_date": "2025-11-15 08:15:50 UTC",
      "updated_date": "2025-11-15 08:15:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:31:18.274771+00:00"
    },
    {
      "arxiv_id": "2511.12085v1",
      "title": "Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness",
      "title_zh": "å…·æœ‰å¯¹æŠ—é²æ£’æ€§çš„åŸºäº Transformer çš„å¯è§£é‡Šç”µå­é‚®ä»¶é’“é±¼åˆ†ç±»",
      "authors": [
        "Sajad U P"
      ],
      "abstract": "Phishing and related cyber threats are becoming more varied and technologically advanced. Among these, email-based phishing remains the most dominant and persistent threat. These attacks exploit human vulnerabilities to disseminate malware or gain unauthorized access to sensitive information. Deep learning (DL) models, particularly transformer-based models, have significantly enhanced phishing mitigation through their contextual understanding of language. However, some recent threats, specifically Artificial Intelligence (AI)-generated phishing attacks, are reducing the overall system resilience of phishing detectors. In response, adversarial training has shown promise against AI-generated phishing threats. This study presents a hybrid approach that uses DistilBERT, a smaller, faster, and lighter version of the BERT transformer model for email classification. Robustness against text-based adversarial perturbations is reinforced using Fast Gradient Method (FGM) adversarial training. Furthermore, the framework integrates the LIME Explainable AI (XAI) technique to enhance the transparency of the DistilBERT architecture. The framework also uses the Flan-T5-small language model from Hugging Face to generate plain-language security narrative explanations for end-users. This combined approach ensures precise phishing classification while providing easily understandable justifications for the model's decisions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¥ç›Šå¤æ‚çš„ç½‘ç»œé’“é±¼(Phishing)å¨èƒï¼Œæå‡ºäº†ä¸€ç§åŸºäº DistilBERT å˜æ¢å™¨æ¨¡å‹çš„æ··åˆåˆ†ç±»æ¡†æ¶ã€‚ä¸ºäº†åº”å¯¹äººå·¥æ™ºèƒ½ç”Ÿæˆçš„é’“é±¼æ”»å‡»å¹¶æå‡ç³»ç»ŸéŸ§æ€§ï¼Œç ”ç©¶é‡‡ç”¨äº†å¿«é€Ÿæ¢¯åº¦æ³•(Fast Gradient Method, FGM)è¿›è¡Œå¯¹æŠ—è®­ç»ƒï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹æ–‡æœ¬å¯¹æŠ—æ‰°åŠ¨çš„é²æ£’æ€§ã€‚æ¡†æ¶é€šè¿‡é›†æˆ LIME å¯è§£é‡Š AI (XAI) æŠ€æœ¯æå‡äº† DistilBERT æ¶æ„çš„é€æ˜åº¦ï¼Œå¹¶åˆ©ç”¨ Flan-T5-small æ¨¡å‹ä¸ºç»ˆç«¯ç”¨æˆ·ç”Ÿæˆé€šä¿—æ˜“æ‡‚çš„å®‰å…¨å™è¿°è¯´æ˜ã€‚è¿™ç§ç»¼åˆæ–¹æ³•åœ¨ç¡®ä¿é«˜ç²¾åº¦é’“é±¼é‚®ä»¶è¯†åˆ«çš„åŒæ—¶ï¼Œä¸ºæ¨¡å‹çš„å†³ç­–æä¾›äº†æ¸…æ™°çš„é€»è¾‘æ”¯æ’‘ã€‚è¯¥ç ”ç©¶æˆåŠŸç»“åˆäº†æ·±åº¦å­¦ä¹ çš„é«˜æ•ˆåˆ†ç±»èƒ½åŠ›ä¸å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œä¸ºæ„å»ºé˜²å¾¡ AI ç”Ÿæˆå¨èƒçš„å®‰å…¨æ£€æµ‹ç³»ç»Ÿæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12085v1",
      "published_date": "2025-11-15 08:05:47 UTC",
      "updated_date": "2025-11-15 08:05:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:31:20.167440+00:00"
    },
    {
      "arxiv_id": "2511.12083v2",
      "title": "No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding",
      "title_zh": "åŸºäºé¢„è®­ç»ƒåµŒå…¥çš„ä¸å®Œå…¨ä¿¡æ¯åšå¼ˆæ— æ‚”ç­–ç•¥æ±‚è§£",
      "authors": [
        "Yanchang Fu",
        "Shengda Liu",
        "Pei Xu",
        "Kaiqi Huang"
      ],
      "abstract": "High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)--such as no-limit Texas Hold'em--where the finite nature of spatial resources hinders solving strategies for the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly discards critical information: specifically, the quantifiable subtle differences between information sets--vital for strategy solving--thus compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds the features of individual information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR introduces a strategy-solving process driven by regret accumulation and strategy updates in this embedding space, with supporting theoretical analysis verifying its ability to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions via low-dimensional embedding for strategy solving.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡éå®Œç¾ä¿¡æ¯æ‰©å±•å¼åšå¼ˆï¼ˆIIEFGsï¼‰ä¸­ä¼ ç»Ÿç¦»æ•£èšç±»æŠ½è±¡æ–¹æ³•å› ç¡¬åˆ†ç±»å¯¼è‡´å…³é”®ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Embedding CFR çš„æ–°å‹ç®—æ³•ã€‚å—è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¯å‘é‡åµŒå…¥ï¼ˆWord Embeddingï¼‰èŒƒå¼çš„å¯å‘ï¼Œè¯¥ç®—æ³•å°†ä¿¡æ¯é›†ç‰¹å¾é¢„è®­ç»ƒå¹¶æ˜ å°„åˆ°ä½ç»´è¿ç»­åµŒå…¥ç©ºé—´ä¸­ï¼Œä»è€Œæ›´ç²¾ç¡®åœ°æ•æ‰ä¿¡æ¯é›†ä¹‹é—´çš„ç»†å¾®å·®åˆ«ä¸è”ç³»ã€‚Embedding CFR åœ¨è¯¥åµŒå…¥ç©ºé—´å†…é©±åŠ¨é—æ†¾å€¼ç´¯ç§¯ï¼ˆRegret Accumulationï¼‰å’Œç­–ç•¥æ›´æ–°ï¼Œç†è®ºåˆ†æè¯æ˜äº†å…¶èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç´¯ç§¯é—æ†¾ã€‚åœ¨å¾·å·æ‰‘å…‹å®éªŒä¸­ï¼Œè¯¥ç®—æ³•åœ¨ç›¸åŒç©ºé—´å¼€é”€ä¸‹æ¯”åŸºäºèšç±»çš„æŠ½è±¡ç®—æ³•è¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚ä½œä¸ºé¦–ä¸ªåœ¨æ‰‘å…‹ AI é¢†åŸŸé€šè¿‡ä½ç»´åµŒå…¥å®ç°ä¿¡æ¯é›†æŠ½è±¡é¢„è®­ç»ƒçš„ç®—æ³•ï¼Œè¯¥ç ”ç©¶ä¸ºè§£å†³å¤æ‚éå®Œç¾ä¿¡æ¯åšå¼ˆæä¾›äº†æ›´é«˜æ•ˆä¸”å…·æœ‰ç†è®ºæ”¯æ’‘çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to appear at AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.12083v2",
      "published_date": "2025-11-15 08:04:19 UTC",
      "updated_date": "2025-12-09 10:19:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:31:24.075824+00:00"
    },
    {
      "arxiv_id": "2511.14792v1",
      "title": "Application of Graph Based Vision Transformers Architectures for Accurate Temperature Prediction in Fiber Specklegram Sensors",
      "title_zh": "åŸºäºå›¾è§†è§‰ Transformer æ¶æ„åœ¨å…‰çº¤æ•£æ–‘ä¼ æ„Ÿå™¨ç²¾ç¡®æ¸©åº¦é¢„æµ‹ä¸­çš„åº”ç”¨",
      "authors": [
        "Abhishek Sebastian"
      ],
      "abstract": "Fiber Specklegram Sensors (FSS) are highly effective for environmental monitoring, particularly for detecting temperature variations. However, the nonlinear nature of specklegram data presents significant challenges for accurate temperature prediction. This study investigates the use of transformer-based architectures, including Vision Transformers (ViTs), Swin Transformers, and emerging models such as Learnable Importance Non-Symmetric Attention Vision Transformers (LINA-ViT) and Multi-Adaptive Proximity Vision Graph Attention Transformers (MAP-ViGAT), to predict temperature from specklegram data over a range of 0 to 120 Celsius. The results show that ViTs achieved a Mean Absolute Error (MAE) of 1.15, outperforming traditional models such as CNNs. GAT-ViT and MAP-ViGAT variants also demonstrated competitive accuracy, highlighting the importance of adaptive attention mechanisms and graph-based structures in capturing complex modal interactions and phase shifts in specklegram data. Additionally, this study incorporates Explainable AI (XAI) techniques, including attention maps and saliency maps, to provide insights into the decision-making processes of the transformer models, improving interpretability and transparency. These findings establish transformer architectures as strong benchmarks for optical fiber-based temperature sensing and offer promising directions for industrial monitoring and structural health assessment applications.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨å…‰çº¤æ–‘å›¾ä¼ æ„Ÿå™¨ (Fiber Specklegram Sensors, FSS) ä¸­åº”ç”¨åŸºäºå›¾çš„è§†è§‰ Transformer (Vision Transformers, ViTs) æ¶æ„æ¥å®ç°ç²¾ç¡®æ¸©åº¦é¢„æµ‹çš„æ–¹æ³•ã€‚é’ˆå¯¹æ–‘å›¾æ•°æ®é«˜åº¦éçº¿æ€§çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è¯„ä¼°äº† ViTsã€Swin Transformers ä»¥åŠ LINA-ViT å’Œ MAP-ViGAT ç­‰æ¨¡å‹åœ¨ 0 åˆ° 120 æ‘„æ°åº¦èŒƒå›´å†…çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒViTs è¾¾åˆ°äº† 1.15 çš„å¹³å‡ç»å¯¹è¯¯å·® (MAE)ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ CNNs æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶å’ŒåŸºäºå›¾çš„ç»“æ„å¯¹äºæ•æ‰æ–‘å›¾æ•°æ®ä¸­å¤æ‚çš„æ¨¡æ€äº¤äº’å’Œç›¸ç§»è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡å¼•å…¥å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI, XAI) æŠ€æœ¯ä¸­çš„æ³¨æ„åŠ›å›¾å’Œæ˜¾è‘—æ€§å›¾ï¼Œå¢å¼ºäº†æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦ä¸å¯è§£é‡Šæ€§ã€‚è¿™äº›æˆæœç¡®ç«‹äº† Transformer æ¶æ„åœ¨å…‰çº¤æ¸©åº¦ä¼ æ„Ÿé¢†åŸŸçš„åŸºå‡†åœ°ä½ï¼Œå¹¶ä¸ºå·¥ä¸šç›‘æµ‹å’Œç»“æ„å¥åº·è¯„ä¼°åº”ç”¨æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.14792v1",
      "published_date": "2025-11-15 07:56:15 UTC",
      "updated_date": "2025-11-15 07:56:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:31:28.661755+00:00"
    },
    {
      "arxiv_id": "2601.02365v1",
      "title": "FUSE : Failure-aware Usage of Subagent Evidence for MultiModal Search and Recommendation",
      "title_zh": "FUSEï¼šé¢å‘å¤šæ¨¡æ€æœç´¢ä¸æ¨èçš„æ•…éšœæ„ŸçŸ¥å­æ™ºèƒ½ä½“è¯æ®åˆ©ç”¨",
      "authors": [
        "Tushar Vatsa",
        "Vibha Belavadi",
        "Priya Shanmugasundaram",
        "Suhas Suresha",
        "Dewang Sultania"
      ],
      "abstract": "Multimodal creative assistants decompose user goals and route tasks to subagents for layout, styling, retrieval, and generation. Retrieval quality is pivotal, yet failures can arise at several stages: understanding user intent, choosing content types, finding candidates (recall), or ranking results. Meanwhile, sending and processing images is costly, making naive multimodal approaches impractical. We present FUSE: Failure-aware Usage of Subagent Evidence for MultiModal Search and Recommendation. FUSE replaces most raw-image prompting with a compact Grounded Design Representation (GDR): a selection aware JSON of canvas elements (image, text, shape, icon, video, logo), structure, styles, salient colors, and user selection provided by the Planner team. FUSE implements seven context budgeting strategies: comprehensive baseline prompting, context compression, chain-of-thought reasoning, mini-shot optimization, retrieval-augmented context, two-stage processing, and zero-shot minimalism. Finally, a pipeline attribution layer monitors system performance by converting subagent signals into simple checks: intent alignment, content-type/routing sanity, recall health (e.g., zero-hit and top-match strength), and ranking displacement analysis. We evaluate the seven context budgeting variants across 788 evaluation queries from diverse users and design templates (refer Figure 3). Our systematic evaluation reveals that Context Compression achieves optimal performance across all pipeline stages, with 93.3% intent accuracy, 86.8% routing success(with fallbacks), 99.4% recall, and 88.5% NDCG@5. This approach demonstrates that strategic context summarization outperforms both comprehensive and minimal contextualization strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FUSE (Failure-aware Usage of Subagent Evidence)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤šæ¨¡æ€æœç´¢ä¸æ¨èè®¾è®¡çš„æ•…éšœæ„ŸçŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€åˆ›æ„åŠ©ç†åœ¨ä»»åŠ¡åˆ†è§£ä¸æ‰§è¡Œè¿‡ç¨‹ä¸­é¢ä¸´çš„æ£€ç´¢è´¨é‡ç“¶é¢ˆåŠå›¾åƒå¤„ç†æˆæœ¬é«˜æ˜‚ç­‰é—®é¢˜ã€‚FUSE å¼•å…¥äº†åä¸º Grounded Design Representation (GDR) çš„è½»é‡åŒ– JSON è¡¨ç¤ºæ³•æ¥æ›¿ä»£åŸå§‹å›¾åƒæç¤ºï¼Œæ¶µç›–äº†ç”»å¸ƒå…ƒç´ ã€ç»“æ„ã€æ ·å¼å’Œå…³é”®é¢œè‰²ç­‰æ ¸å¿ƒä¿¡æ¯ã€‚è¯¥æ¡†æ¶å®ç°äº†åŒ…æ‹¬ Context Compression (ä¸Šä¸‹æ–‡å‹ç¼©)ã€Chain-of-Thought (é“¾å¼æ€ç»´) å’Œ Retrieval-Augmented Context (æ£€ç´¢å¢å¼ºä¸Šä¸‹æ–‡) åœ¨å†…çš„ä¸ƒç§ä¸Šä¸‹æ–‡é¢„ç®—ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒFUSE é€šè¿‡æµæ°´çº¿å½’å› å±‚ (Pipeline Attribution Layer) å°†å­æ™ºèƒ½ä½“ä¿¡å·è½¬æ¢ä¸ºç›‘æµ‹æŒ‡æ ‡ï¼Œå®ç°å¯¹æ„å›¾å¯¹é½ã€è·¯ç”±å¥å…¨æ€§ã€å¬å›å¥åº·åº¦å’Œæ’åºä½ç§»çš„å®æ—¶åˆ†æã€‚åœ¨æ¶‰åŠ 788 ä¸ªæŸ¥è¯¢çš„ç³»ç»Ÿæ€§è¯„ä¼°ä¸­ï¼Œå®éªŒç»“æœæ˜¾ç¤º Context Compression ç­–ç•¥è¡¨ç°æœ€ä¸ºå“è¶Šï¼Œå®ç°äº† 93.3% çš„æ„å›¾å‡†ç¡®ç‡ã€99.4% çš„å¬å›ç‡ä»¥åŠ 88.5% çš„ NDCG@5ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†æˆ˜ç•¥æ€§çš„ä¸Šä¸‹æ–‡æ‘˜è¦ç­–ç•¥åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€è®¾è®¡ä»»åŠ¡æ—¶ï¼Œå…¶æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„å…¨é¢æˆ–æç®€ä¸Šä¸‹æ–‡å¤„ç†ç­–ç•¥ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "ICDM MMSR 2025: Workshop on Multimodal Search and Recommendations",
      "pdf_url": "https://arxiv.org/pdf/2601.02365v1",
      "published_date": "2025-11-15 07:55:51 UTC",
      "updated_date": "2025-11-15 07:55:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:31:28.862238+00:00"
    },
    {
      "arxiv_id": "2511.12075v1",
      "title": "Treatment Stitching with SchrÃ¶dinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies",
      "title_zh": "åŸºäºè–›å®šè°”æ¡¥çš„æ²»ç–—ç¼åˆï¼šå¢å¼ºè‡ªé€‚åº”æ²»ç–—ç­–ç•¥ä¸­çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Dong-Hee Shin",
        "Deok-Joong Lee",
        "Young-Han Son",
        "Tae-Eui Kam"
      ],
      "abstract": "Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the SchrÃ¶dinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªé€‚åº”æ²»ç–—ç­–ç•¥(Adaptive treatment strategies, ATS)åœ¨ä¸´åºŠç¯å¢ƒä¸­åº”ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline RL)æ—¶é¢ä¸´çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºTreatment Stitching (TreatStitch)çš„æ–°å‹æ•°æ®å¢å¼ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ™ºèƒ½æ‹¼æ¥ç°æœ‰æ²»ç–—è½¨è¿¹çš„ç‰‡æ®µæ¥ç”Ÿæˆä¸´åºŠæœ‰æ•ˆçš„åˆæˆæ•°æ®ï¼Œæ—¨åœ¨æå‡Offline RLçš„ç­–ç•¥ä¼˜åŒ–èƒ½åŠ›ã€‚å¯¹äºæ— æ³•ç›´æ¥æ‹¼æ¥çš„éç›¸ä¼¼ä¸­é—´çŠ¶æ€ï¼ŒTreatStitchå¼•å…¥äº†SchrÃ¶dinger bridgeæ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆå¹³æ»‘ä¸”èƒ½é‡é«˜æ•ˆçš„æ¡¥æ¥è½¨è¿¹æ¥å®ç°çŠ¶æ€è¿æ¥ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—ä¸°å¯Œäº†åŸå§‹æ•°æ®é›†çš„å¤šæ ·æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»æ›´å¹¿æ³›çš„ä¸´åºŠåœºæ™¯ä¸­å­¦ä¹ ã€‚åœ¨å¤šä¸ªæ²»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTreatStitchèƒ½æ˜¾è‘—å¢å¼ºOffline RLçš„æ€§èƒ½è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æä¾›äº†ç†è®ºè¯æ˜ï¼Œç¡®ä¿ç”Ÿæˆçš„åˆæˆè½¨è¿¹é€šè¿‡é¿å…åˆ†å¸ƒå¤–(out-of-distribution)è½¬æ¢æ¥ç»´æŒå…¶ä¸´åºŠæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 5 figures, AAAI conference",
      "pdf_url": "https://arxiv.org/pdf/2511.12075v1",
      "published_date": "2025-11-15 07:37:49 UTC",
      "updated_date": "2025-11-15 07:37:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:31:33.380882+00:00"
    },
    {
      "arxiv_id": "2511.12074v2",
      "title": "MF-Speech: Achieving Fine-Grained and Compositional Control in Speech Generation via Factor Disentanglement",
      "title_zh": "MF-Speechï¼šé€šè¿‡å› å­è§£è€¦å®ç°è¯­éŸ³ç”Ÿæˆçš„ç»†ç²’åº¦ä¸ç»„åˆå¼æ§åˆ¶",
      "authors": [
        "Xinyue Yu",
        "Youqing Fang",
        "Pingyu Wu",
        "Guoyang Ye",
        "Wenbo Zhou",
        "Weiming Zhang",
        "Song Xiao"
      ],
      "abstract": "Generating expressive and controllable human speech is one of the core goals of generative artificial intelligence, but its progress has long been constrained by two fundamental challenges: the deep entanglement of speech factors and the coarse granularity of existing control mechanisms. To overcome these challenges, we have proposed a novel framework called MF-Speech, which consists of two core components: MF-SpeechEncoder and MF-SpeechGenerator. MF-SpeechEncoder acts as a factor purifier, adopting a multi-objective optimization strategy to decompose the original speech signal into highly pure and independent representations of content, timbre, and emotion. Subsequently, MF-SpeechGenerator functions as a conductor, achieving precise, composable and fine-grained control over these factors through dynamic fusion and Hierarchical Style Adaptive Normalization (HSAN). Experiments demonstrate that in the highly challenging multi-factor compositional speech generation task, MF-Speech significantly outperforms current state-of-the-art methods, achieving a lower word error rate (WER=4.67%), superior style control (SECS=0.5685, Corr=0.68), and the highest subjective evaluation scores(nMOS=3.96, sMOS_emotion=3.86, sMOS_style=3.78). Furthermore, the learned discrete factors exhibit strong transferability, demonstrating their significant potential as a general-purpose speech representation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MF-Speechæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¯­éŸ³ç”Ÿæˆä¸­è¯­éŸ³æˆåˆ†æ·±å±‚è€¦åˆä»¥åŠç°æœ‰æ§åˆ¶æœºåˆ¶ç²’åº¦è¿‡ç²—çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶ç”±MF-SpeechEncoderå’ŒMF-SpeechGeneratorä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼Œé€šè¿‡å› å­è§£è€¦å®ç°ç²¾ç»†åŒ–ä¸”å¯ç»„åˆçš„è¯­éŸ³æ§åˆ¶ã€‚å…¶ä¸­ï¼ŒMF-SpeechEncoderåˆ©ç”¨å¤šç›®æ ‡ä¼˜åŒ–ç­–ç•¥å°†è¯­éŸ³ä¿¡å·åˆ†è§£ä¸ºå†…å®¹(content)ã€éŸ³è‰²(timbre)å’Œæƒ…æ„Ÿ(emotion)ç­‰ç‹¬ç«‹ä¸”çº¯å‡€çš„è¡¨ç¤ºã€‚éšåï¼ŒMF-SpeechGeneratoré€šè¿‡åŠ¨æ€èåˆä¸å±‚æ¬¡åŒ–é£æ ¼è‡ªé€‚åº”å½’ä¸€åŒ–(Hierarchical Style Adaptive Normalization, HSAN)æŠ€æœ¯ï¼Œå¯¹è¿™äº›å› å­è¿›è¡Œç²¾å‡†è°ƒåº¦ä¸åˆæˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤æ‚çš„å¤šå› ç´ ç»„åˆç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒMF-Speechåœ¨å­—é”™ç‡(WER=4.67%)ã€é£æ ¼ç›¸ä¼¼åº¦åŠä¸»è§‚è¯„åˆ†(nMOS=3.96)ç­‰æ–¹é¢å‡æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°è¿™äº›è§£è€¦åçš„ç¦»æ•£å› å­å…·æœ‰æå¼ºçš„å¯è¿ç§»æ€§ï¼Œå…·å¤‡ä½œä¸ºé€šç”¨è¯­éŸ³è¡¨ç¤ºçš„å·¨å¤§åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.12074v2",
      "published_date": "2025-11-15 07:30:51 UTC",
      "updated_date": "2025-11-19 14:50:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:31:46.577861+00:00"
    },
    {
      "arxiv_id": "2511.13772v1",
      "title": "Can LLMs Create Legally Relevant Summaries and Analyses of Videos?",
      "title_zh": "LLMs èƒ½å¦ç”Ÿæˆå…·æœ‰æ³•å¾‹ç›¸å…³æ€§çš„è§†é¢‘æ‘˜è¦ä¸åˆ†æï¼Ÿ",
      "authors": [
        "Lyra Hoeben-Kuil",
        "Gijs van Dijck",
        "Jaromir Savelka",
        "Johanna Gunawan",
        "Konrad Kollnig",
        "Marta Kolacz",
        "Mindy Duffourc",
        "Shashank Chakravarthy",
        "Hannes Westermann"
      ],
      "abstract": "Understanding the legally relevant factual basis of an event and conveying it through text is a key skill of legal professionals. This skill is important for preparing forms (e.g., insurance claims) or other legal documents (e.g., court claims), but often presents a challenge for laypeople. Current AI approaches aim to bridge this gap, but mostly rely on the user to articulate what has happened in text, which may be challenging for many. Here, we investigate the capability of large language models (LLMs) to understand and summarize events occurring in videos. We ask an LLM to summarize and draft legal letters, based on 120 YouTube videos showing legal issues in various domains. Overall, 71.7\\% of the summaries were rated as of high or medium quality, which is a promising result, opening the door to a number of applications in e.g. access to justice.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç†è§£å’Œæ€»ç»“è§†é¢‘ä¸­å…·æœ‰æ³•å¾‹ç›¸å…³æ€§äº‹å®æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡ 120 ä¸ªæ¶‰åŠä¸åŒæ³•å¾‹é¢†åŸŸçš„ YouTube è§†é¢‘ï¼Œæµ‹è¯•äº† LLM ç”Ÿæˆæ‘˜è¦å¹¶èµ·è‰æ³•å¾‹ä¿¡å‡½ (legal letters) çš„è¡¨ç°ã€‚è¯„ä»·ç»“æœæ˜¾ç¤ºï¼Œ71.7% çš„ç”Ÿæˆæ‘˜è¦è¾¾åˆ°äº†é«˜è´¨é‡æˆ–ä¸­ç­‰è´¨é‡æ°´å¹³ï¼Œå±•ç°äº† LLMs åœ¨è·¨æ¨¡æ€æ³•å¾‹ä¿¡æ¯å¤„ç†ä¸Šçš„æ½œåŠ›ã€‚è¯¥å‘ç°ä¸ºæ³•å¾‹å…¬æ­£æ€§ (access to justice) é¢†åŸŸçš„åº”ç”¨å¼€è¾Ÿäº†æ–°è·¯å¾„ï¼Œèƒ½å¤ŸååŠ©æ™®é€šäººå°†å¤æ‚çš„è§†é¢‘äº‹ä»¶è½¬åŒ–ä¸ºä¸“ä¸šçš„ä¹¦é¢æ³•å¾‹åˆ†æã€‚ç ”ç©¶è¯æ˜äº† AI æŠ€æœ¯åœ¨è¾…åŠ©éä¸“ä¸šäººå‘˜å¤„ç†æ³•å¾‹æ–‡æ¡£å’Œä¿é™©ç´¢èµ”ç­‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ],
      "primary_category": "cs.MM",
      "comment": "Accepted for publication at JURIX 2025 Torino, Italy. This is the preprint version. Code and data available at: https://github.com/maastrichtlawtech/jurix2025_LLM_video_analysis",
      "pdf_url": "https://arxiv.org/pdf/2511.13772v1",
      "published_date": "2025-11-15 07:30:39 UTC",
      "updated_date": "2025-11-15 07:30:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:31:44.683541+00:00"
    },
    {
      "arxiv_id": "2511.12072v1",
      "title": "ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation",
      "title_zh": "ProAV-DiTï¼šé¢å‘é«˜æ•ˆåŒæ­¥éŸ³è§†é¢‘ç”Ÿæˆçš„æŠ•å½±æ½œæ‰©æ•£ Transformer",
      "authors": [
        "Jiahui Sun",
        "Weining Wang",
        "Mingzhen Sun",
        "Yirong Yang",
        "Xinxin Zhu",
        "Jing Liu"
      ],
      "abstract": "Sounding Video Generation (SVG) remains a challenging task due to the inherent structural misalignment between audio and video, as well as the high computational cost of multimodal data processing. In this paper, we introduce ProAV-DiT, a Projected Latent Diffusion Transformer designed for efficient and synchronized audio-video generation. To address structural inconsistencies, we preprocess raw audio into video-like representations, aligning both the temporal and spatial dimensions between audio and video. At its core, ProAV-DiT adopts a Multi-scale Dual-stream Spatio-Temporal Autoencoder (MDSA), which projects both modalities into a unified latent space using orthogonal decomposition, enabling fine-grained spatiotemporal modeling and semantic alignment. To further enhance temporal coherence and modality-specific fusion, we introduce a multi-scale attention mechanism, which consists of multi-scale temporal self-attention and group cross-modal attention. Furthermore, we stack the 2D latents from MDSA into a unified 3D latent space, which is processed by a spatio-temporal diffusion Transformer. This design efficiently models spatiotemporal dependencies, enabling the generation of high-fidelity synchronized audio-video content while reducing computational overhead. Extensive experiments conducted on standard benchmarks demonstrate that ProAV-DiT outperforms existing methods in both generation quality and computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ProAV-DiTï¼Œä¸€ç§æŠ•å½±æ½œç©ºé—´æ‰©æ•£Transformer (Projected Latent Diffusion Transformer)ï¼Œæ—¨åœ¨è§£å†³æœ‰å£°è§†é¢‘ç”Ÿæˆ (Sounding Video Generation) ä¸­éŸ³è§†é¢‘ç»“æ„å¤±é…åŠå¤šæ¨¡æ€å¤„ç†è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡å°†åŸå§‹éŸ³é¢‘é¢„å¤„ç†ä¸ºç±»è§†é¢‘è¡¨ç¤ºï¼Œæœ‰æ•ˆå®ç°äº†éŸ³è§†é¢‘åœ¨æ—¶ç©ºç»´åº¦ä¸Šçš„å¯¹é½ã€‚å…¶æ ¸å¿ƒæ¶æ„é‡‡ç”¨å¤šå°ºåº¦åŒæµæ—¶ç©ºè‡ªç¼–ç å™¨ (Multi-scale Dual-stream Spatio-Temporal Autoencoder, MDSA)ï¼Œåˆ©ç”¨æ­£äº¤åˆ†è§£å°†åŒæ¨¡æ€æŠ•å½±è‡³ç»Ÿä¸€æ½œç©ºé—´ï¼Œå®ç°äº†ç»†ç²’åº¦çš„å»ºæ¨¡ä¸è¯­ä¹‰å¯¹é½ã€‚æ­¤å¤–ï¼Œå¤šå°ºåº¦æ—¶é—´è‡ªæ³¨æ„åŠ›å’Œåˆ†ç»„è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶çš„å¼•å…¥ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†ç”Ÿæˆå†…å®¹çš„æ—¶é—´è¿è´¯æ€§ä¸æ¨¡æ€èåˆã€‚æ¨¡å‹é€šè¿‡åœ¨3Dæ½œç©ºé—´ä¸­åº”ç”¨æ—¶ç©ºæ‰©æ•£Transformerï¼Œåœ¨æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶é«˜æ•ˆå»ºæ¨¡äº†æ—¶ç©ºä¾èµ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProAV-DiTåœ¨ç”Ÿæˆè´¨é‡å’Œè¿è¡Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸçš„åŒæ­¥éŸ³è§†é¢‘å†…å®¹ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12072v1",
      "published_date": "2025-11-15 07:24:17 UTC",
      "updated_date": "2025-11-15 07:24:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:03.759815+00:00"
    },
    {
      "arxiv_id": "2511.12071v1",
      "title": "Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread",
      "title_zh": "é€šè¿‡çŸ¥è¯†è¡¥å…¨æå‡æœºå™¨å­¦ä¹ å›¾åµŒå…¥ï¼šåŸºäº COVID-19 ä¼ æ’­æ¡ˆä¾‹çš„ç ”ç©¶ä¸éªŒè¯",
      "authors": [
        "Rosario Napoli",
        "Gabriele Morabito",
        "Antonio Celesti",
        "Massimo Villari",
        "Maria Fazio"
      ],
      "abstract": "The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾æœºå™¨å­¦ä¹ (Graph Machine Learning)ä¸­å›¾åµŒå…¥(Graph Embeddings)å› ä»…ä¾èµ–æ˜¾å¼æ‹“æ‰‘å’Œç‰¹å¾è€Œå¯èƒ½é—æ¼ç¨€ç–æ•°æ®é›†ä¸­éšå«çŸ¥è¯†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›æ–¹æ¡ˆã€‚ç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§é›†æˆçŸ¥è¯†è¡¥å…¨(Knowledge Completion)é˜¶æ®µçš„é€šç”¨å·¥ä½œæµï¼Œæ—¨åœ¨ç”ŸæˆåµŒå…¥å‘é‡ä¹‹å‰é€šè¿‡åŸºäºè¡°å‡çš„æ¨ç†å‡½æ•°å¯¹ä¼ é€’å…³ç³»(Transitive Relations)è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œé‡å¡‘å›¾æ‹“æ‰‘ç»“æ„ã€‚è¿™ç§æ–¹æ³•ç›´æ¥å½±å“äº†GraphSAGEå’ŒNode2Vecç­‰ç®—æ³•åœ¨ç‰¹å¾èšåˆè¿‡ç¨‹ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å·¥ä½œæµæ˜¾è‘—æ”¹å˜äº†åµŒå…¥ç©ºé—´çš„å‡ ä½•åˆ†å¸ƒï¼Œè¯æ˜äº†çŸ¥è¯†è¡¥å…¨ä¸ä»…æ˜¯ç®€å•çš„æ•°æ®å¢å¼ºï¼Œæ›´æ˜¯æå‡å›¾è¡¨ç¤ºè´¨é‡çš„å˜é©æ€§æ­¥éª¤ã€‚è¯¥æ–¹æ¡ˆåœ¨æ–°å† è‚ºç‚(COVID-19)ä¼ æ’­çš„æ¡ˆä¾‹ç ”ç©¶ä¸­å¾—åˆ°äº†æœ‰æ•ˆéªŒè¯ï¼Œå±•ç¤ºäº†å…¶åœ¨æŒ–æ˜å¤æ‚æ•°æ®æ½œåœ¨è¯­ä¹‰æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 16th IEEE International Conference on Knowledge Graphs (ICKG) 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.12071v1",
      "published_date": "2025-11-15 07:24:00 UTC",
      "updated_date": "2025-11-15 07:24:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:02.574162+00:00"
    },
    {
      "arxiv_id": "2511.12063v3",
      "title": "TextBO: Bayesian Optimization in Language Space for Eval-Efficient Self-Improving AI",
      "title_zh": "TextBOï¼šé¢å‘é«˜è¯„ä¼°æ•ˆç‡ AI è‡ªæˆ‘æ”¹è¿›çš„è¯­è¨€ç©ºé—´è´å¶æ–¯ä¼˜åŒ–",
      "authors": [
        "Enoch Hyunwook Kang",
        "Hema Yoganarasimhan"
      ],
      "abstract": "Large Language Models (LLMs) have enabled self-improving AI systems that iteratively generate, evaluate, and refine their outcomes. Recent studies show that prompt-optimization-based self-improvement can outperform state-of-the-art reinforcement-learning fine-tuning of LLMs, but performance is typically measured by generation efficiency. However, in many applications, the constraint is evaluation efficiency: obtaining reliable feedback is far more costly than generating candidates. To optimize for evaluation efficiency, we extend Upper Confidence Bound-Bayesian Optimization (UCB-BO), a framework known for optimal evaluation-efficiency guarantees, to the language domain. Doing so is challenging for two reasons: (i) gradients needed for UCB-BO are ill-defined in discrete prompt space; and (ii) UCB-style exploration relies on a surrogate model and acquisition function, which only live implicitly in the LLM. We overcome these challenges by proving that combining simple textual gradients (LLM-proposed local edits) with the Best-of-N selection strategy statistically emulates ascent along the gradient of the canonical UCB acquisition function. Based on this result, we propose TextBO, a simple, evaluation-efficient self-improving algorithm that operates purely in language space without explicit surrogates or calibrated uncertainty models. We empirically validate TextBO on automated ad-alignment tasks using a persona-induced preference distribution, demonstrating superior performance per evaluation compared to strong baselines such as Best-of-N and GEPA. We also evaluate TextBO's Best-of-N multi-step textual-gradient mechanism on agentic AI benchmarks by augmenting GEPA with it and show that it significantly outperforms standard GEPA. In sum, TextBO is a simple and principled framework for AI self-improving system design that bridges prompt optimization with classical Bayesian optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TextBOï¼Œä¸€ä¸ªåœ¨è¯­è¨€ç©ºé—´å†…è¿›è¡ŒBayesian Optimizationçš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è‡ªæˆ‘æ”¹è¿›AIç³»ç»Ÿçš„Evaluation Efficiencyã€‚é’ˆå¯¹åœ¨ç¦»æ•£æç¤ºç©ºé—´ä¸­æ¢¯åº¦éš¾ä»¥å®šä¹‰ä»¥åŠç¼ºä¹æ˜¾å¼ä»£ç†æ¨¡å‹çš„é—®é¢˜ï¼Œç ”ç©¶è€…è¯æ˜äº†å°†LLMç”Ÿæˆçš„Textual Gradientsä¸Best-of-Nç­–ç•¥ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨ç»Ÿè®¡ä¸Šæ¨¡æ‹Ÿç»å…¸Upper Confidence Bound (UCB) é‡‡é›†å‡½æ•°çš„æ¢¯åº¦ä¸Šå‡è¿‡ç¨‹ã€‚åŸºäºæ­¤å‘ç°ï¼ŒTextBOå®ç°äº†æ— éœ€æ˜¾å¼ä»£ç†æ¨¡å‹æˆ–æ ¡å‡†ä¸ç¡®å®šæ€§æ¨¡å‹ã€çº¯ç²¹åœ¨è¯­è¨€ç©ºé—´è¿è¡Œçš„ä¼˜åŒ–ç®—æ³•ã€‚åœ¨è‡ªåŠ¨åŒ–å¹¿å‘ŠåŒ¹é…ä»»åŠ¡å’ŒAgentic AIåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒTextBOåœ¨å•ä½è¯„ä¼°æˆæœ¬ä¸‹çš„æ€§èƒ½è¡¨ç°æ˜¾è‘—ä¼˜äºBest-of-Nå’ŒGEPAç­‰å¼ºåŸºçº¿æ¨¡å‹ã€‚è¯¥æ¡†æ¶æˆåŠŸå°†Prompt Optimizationä¸ç»å…¸çš„Bayesian Optimizationç†è®ºç›¸è¿æ¥ï¼Œä¸ºè®¾è®¡ç®€å•ä¸”å…·æœ‰ç†è®ºåŸåˆ™çš„è‡ªæˆ‘æ”¹è¿›AIç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12063v3",
      "published_date": "2025-11-15 07:04:44 UTC",
      "updated_date": "2026-01-06 03:54:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:05.788234+00:00"
    },
    {
      "arxiv_id": "2511.12061v2",
      "title": "MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity (Extension)",
      "title_zh": "MovSemCLï¼šé¢å‘è½¨è¿¹ç›¸ä¼¼æ€§è®¡ç®—çš„è¿åŠ¨è¯­ä¹‰å¯¹æ¯”å­¦ä¹ ï¼ˆæ‰©å±•ç‰ˆï¼‰",
      "authors": [
        "Zhichen Lai",
        "Hua Lu",
        "Huan Li",
        "Jialiang Li",
        "Christian S. Jensen"
      ],
      "abstract": "Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MovSemCLï¼Œä¸€ç§ç”¨äºè½¨è¿¹ç›¸ä¼¼åº¦è®¡ç®—çš„ç§»åŠ¨è¯­ä¹‰å¯¹æ¯”å­¦ä¹  (Movement-Semantics Contrastive Learning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å­¦ä¹ æ–¹æ³•åœ¨è¯­ä¹‰å»ºæ¨¡ã€å±‚æ¬¡åŒ–è¡¨è¾¾åŠè®¡ç®—æ•ˆç‡ä¸Šçš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†åŸå§‹ GPS è½¨è¿¹è½¬æ¢ä¸ºç§»åŠ¨è¯­ä¹‰ç‰¹å¾å¹¶è¿›è¡Œè¡¥ä¸ (Patches) åˆ†æ®µï¼Œåˆ©ç”¨è¡¥ä¸å†…å’Œè¡¥ä¸é—´çš„æ³¨æ„åŠ›æœºåˆ¶ (Intra- and Inter-patch Attentions) æ•æ‰å±€éƒ¨ä¸å…¨å±€è½¨è¿¹æ¨¡å¼ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å±‚æ¬¡åŒ–è¡¨ç¤ºå¹¶é™ä½è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æ›²ç‡å¼•å¯¼çš„å¢å¼ºç­–ç•¥ (Curvature-guided Augmentation)ï¼Œé€šè¿‡ä¿ç•™è½¬å¼¯å’Œäº¤å‰å£ç­‰å…³é”®ä¿¡æ¯æ®µå¹¶æ©è”½å†—ä½™éƒ¨åˆ†ï¼Œç”Ÿæˆäº†ç¬¦åˆç‰©ç†è§„å¾‹çš„å¢å¼ºè§†å›¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMovSemCL åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„ç›¸ä¼¼æ€§æœç´¢ä»»åŠ¡ä¸­å¹³å‡æ’åæ¥è¿‘ç†æƒ³å€¼ 1ï¼Œåœ¨å¯å‘å¼è¿‘ä¼¼æ–¹é¢æ€§èƒ½æå‡é«˜è¾¾ 20.3%ï¼Œä¸”æ¨ç†å»¶è¿Ÿé™ä½äº† 43.4%ã€‚è¿™è¯æ˜äº†è¯¥æ¡†æ¶åœ¨æå‡è½¨è¿¹ç›¸ä¼¼åº¦è®¡ç®—ç²¾åº¦ä¸æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 6 figures; accepted by AAAI 2026 as an Oral paper",
      "pdf_url": "https://arxiv.org/pdf/2511.12061v2",
      "published_date": "2025-11-15 06:56:40 UTC",
      "updated_date": "2025-12-17 13:30:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:13.565160+00:00"
    },
    {
      "arxiv_id": "2511.12060v2",
      "title": "Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization",
      "title_zh": "åŸºäºå¤šè·¯å¾„å·®å¼‚åŒ–è£å‰ªè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–çš„æ©¡èƒ¶è½®èƒèƒ¶ç‰‡ç”Ÿäº§æ™ºèƒ½ååŒä¼˜åŒ–",
      "authors": [
        "Yinghao Ruan",
        "Wei Pang",
        "Shuaihao Liu",
        "Huili Yang",
        "Leyi Han",
        "Xinghui Dong"
      ],
      "abstract": "The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ©¡èƒ¶è½®èƒå·¥ä¸šä¸­ä¼ ç»Ÿä¸­å¿ƒåŒ–è°ƒåº¦å’Œéçµæ´»ç”Ÿäº§çº¿é…ç½®çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMulti-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO)çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨å®ç°æ™ºèƒ½åŒ–ååŒä¼˜åŒ–ã€‚ä¸ºäº†åº”å¯¹è½®èƒåˆ¶é€ ç³»ç»Ÿä¸­å¤æ‚çš„éçº¿æ€§äº¤äº’å’Œé«˜ç»´å¤šç›®æ ‡ä¼˜åŒ–æŒ‘æˆ˜ï¼Œè¯¥ç®—æ³•é‡‡ç”¨äº†å¤šåˆ†æ”¯ç­–ç•¥æ¶æ„(multi-branch policy architecture)ä»¥åŠå·®å¼‚åŒ–çš„æ¢¯åº¦è£å‰ªçº¦æŸ(differentiated gradient clipping constraints)ï¼Œç¡®ä¿äº†é«˜ç»´ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§ä¸æ•ˆç‡ã€‚åœ¨æ©¡èƒ¶è½®èƒè–„è†œç”Ÿäº§çš„å®½åº¦å’Œåšåº¦æ§åˆ¶å®éªŒä¸­ï¼ŒMPD-PPOåœ¨è°ƒèŠ‚ç²¾åº¦å’Œè¿è¡Œæ•ˆç‡æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆè§£å†³äº†é«˜ç»´åº¦å¤„ç†ã€å¤šç›®æ ‡æƒè¡¡åŠåŠ¨æ€é€‚åº”ç­‰å·¥ä¸šæ ¸å¿ƒéš¾é¢˜ï¼Œä¸ºè½®èƒåˆ¶é€ çš„å®æ—¶ç”Ÿäº§éƒ¨ç½²æä¾›äº†æ›´é«˜çš„æ€§èƒ½è¡¨ç°å’Œç³»ç»Ÿç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.12060v2",
      "published_date": "2025-11-15 06:53:27 UTC",
      "updated_date": "2025-11-19 04:47:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:13.362265+00:00"
    },
    {
      "arxiv_id": "2511.12056v1",
      "title": "PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling",
      "title_zh": "PipeDiTï¼šåˆ©ç”¨ä»»åŠ¡æµæ°´çº¿ä¸æ¨¡å‹è§£è€¦åŠ é€Ÿè§†é¢‘ç”Ÿæˆä¸­çš„æ‰©æ•£ Transformer",
      "authors": [
        "Sijie Wang",
        "Qiang Wang",
        "Shaohuai Shi"
      ],
      "abstract": "Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PipeDiTï¼Œä¸€ä¸ªæ—¨åœ¨åŠ é€Ÿè§†é¢‘ç”Ÿæˆä¸­æ‰©æ•£å˜æ¢å™¨(Diffusion Transformer, DiT)æ¨ç†è¿‡ç¨‹çš„æµæ°´çº¿æ¡†æ¶ã€‚PipeDiTå¼•å…¥äº†PipeSPç®—æ³•ï¼Œé€šè¿‡é’ˆå¯¹åºåˆ—å¹¶è¡Œ(Sequence Parallelism)è®¾è®¡çš„æµæ°´çº¿æœºåˆ¶ï¼Œå®ç°äº†æ½œåœ¨è¡¨ç¤ºç”Ÿæˆè®¡ç®—ä¸å¤šGPUé—´é€šä¿¡çš„å¹¶è¡Œå¤„ç†ï¼Œæœ‰æ•ˆé™ä½äº†æ¨ç†å»¶è¿Ÿã€‚è¯¥æ¡†æ¶è¿˜é‡‡ç”¨DeDiVAEç­–ç•¥å°†æ‰©æ•£æ¨¡å—ä¸å˜åˆ†è‡ªç¼–ç å™¨(VAE)æ¨¡å—è§£è€¦è‡³ä¸åŒçš„GPUç»„å¹¶ä½¿æ‰§è¡Œè¿‡ç¨‹æµæ°´çº¿åŒ–ï¼Œåœ¨é™ä½è€—æ—¶çš„åŒæ—¶ä¹Ÿä¼˜åŒ–äº†æ˜¾å­˜å ç”¨ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ•ˆèƒ½ï¼Œç ”ç©¶è€…æå‡ºäº†æ³¨æ„åŠ›åå¤„ç†(Attention Co-processing, Aco)æ–¹æ³•ä»¥æ›´å……åˆ†åœ°åˆ©ç”¨GPUèµ„æºã€‚è¯¥æ¡†æ¶è¢«æˆåŠŸé›†æˆåˆ°OpenSoraPlanå’ŒHunyuanVideoå¼€æºé¡¹ç›®ä¸­ï¼Œå¹¶åœ¨8-GPUç³»ç»Ÿä¸Šå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§åˆ†è¾¨ç‡å’Œæ—¶é—´æ­¥é•¿é…ç½®ä¸‹ï¼ŒPipeDiTç›¸æ¯”åŸºçº¿æ¨¡å‹å®ç°äº†1.06å€è‡³4.02å€çš„æ˜¾è‘—åŠ é€Ÿã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12056v1",
      "published_date": "2025-11-15 06:46:40 UTC",
      "updated_date": "2025-11-15 06:46:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:16.374041+00:00"
    },
    {
      "arxiv_id": "2511.12052v1",
      "title": "Exploring AI in Steganography and Steganalysis: Trends, Clusters, and Sustainable Development Potential",
      "title_zh": "éšå†™æœ¯ä¸éšå†™åˆ†æä¸­çš„äººå·¥æ™ºèƒ½åº”ç”¨æ¢ç´¢ï¼šè¶‹åŠ¿ã€èšç±»åŠå¯æŒç»­å‘å±•æ½œåŠ›",
      "authors": [
        "Aditya Kumar Sahu",
        "Chandan Kumar",
        "Saksham Kumar",
        "Serdar Solak"
      ],
      "abstract": "Steganography and steganalysis are strongly related subjects of information security. Over the past decade, many powerful and efficient artificial intelligence (AI) - driven techniques have been designed and presented during research into steganography as well as steganalysis. This study presents a scientometric analysis of AI-driven steganography-based data hiding techniques using a thematic modelling approach. A total of 654 articles within the time span of 2017 to 2023 have been considered. Experimental evaluation of the study reveals that 69% of published articles are from Asian countries. The China is on top (TP:312), followed by India (TP-114). The study mainly identifies seven thematic clusters: steganographic image data hiding, deep image steganalysis, neural watermark robustness, linguistic steganography models, speech steganalysis algorithms, covert communication networks, and video steganography techniques. The proposed study also assesses the scope of AI-steganography under the purview of sustainable development goals (SDGs) to present the interdisciplinary reciprocity between them. It has been observed that only 18 of the 654 articles are aligned with one of the SDGs, which shows that limited studies conducted in alignment with SDG goals. SDG9 which is Industry, Innovation, and Infrastructure is leading among 18 SDGs mapped articles. To the top of our insight, this study is the unique one to present a scientometric study on AI-driven steganography-based data hiding techniques. In the context of descriptive statistics, the study breaks down the underlying causes of observed trends, including the influence of DL developments, trends in East Asia and maturity of foundational methods. The work also stresses upon the critical gaps in societal alignment, particularly the SDGs, ultimately working on unveiling the field's global impact on AI security challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹2017å¹´è‡³2023å¹´é—´åŸºäºäººå·¥æ™ºèƒ½ (AI) é©±åŠ¨çš„éšå†™æœ¯ (Steganography) å’Œéšå†™åˆ†æ (Steganalysis) æ•°æ®éšè—æŠ€æœ¯è¿›è¡Œäº†å…¨é¢çš„ç§‘å­¦è®¡é‡åˆ†æ (Scientometric analysis)ã€‚ç ”ç©¶é‡‡ç”¨ä¸»é¢˜å»ºæ¨¡ (Thematic modelling) æ–¹æ³•åˆ†æäº†654ç¯‡æ–‡çŒ®ï¼Œæ­ç¤ºäº†äºšæ´²å›½å®¶å°¤å…¶æ˜¯ä¸­å›½å’Œå°åº¦åœ¨è¯¥é¢†åŸŸçš„é¢†å…ˆåœ°ä½ã€‚ç ”ç©¶è¯†åˆ«å‡ºéšå†™å›¾åƒæ•°æ®éšè— (Steganographic image data hiding)ã€æ·±åº¦å›¾åƒéšå†™åˆ†æ (Deep image steganalysis) å’Œç¥ç»æ°´å°é²æ£’æ€§ (Neural watermark robustness) ç­‰ä¸ƒå¤§æ ¸å¿ƒä¸»é¢˜é›†ç¾¤ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†è¯¥é¢†åŸŸä¸å¯æŒç»­å‘å±•ç›®æ ‡ (SDGs) çš„å…³è”ï¼Œå‘ç°ç›®å‰ä»…æœ‰æå°‘æ•°ç ”ç©¶ä¸ SDG ç›®æ ‡å¯¹é½ï¼Œå…¶ä¸­ä»¥ SDG 9ï¼ˆå·¥ä¸šã€åˆ›æ–°å’ŒåŸºç¡€è®¾æ–½ï¼‰æœ€ä¸ºçªå‡ºã€‚æœ€åï¼Œæ–‡ç« æŒ‡å‡ºäº†è¯¥é¢†åŸŸåœ¨ç¤¾ä¼šå½±å“å¯¹é½æ–¹é¢çš„å…³é”®å·®è·ï¼Œå¹¶åˆ†æäº†æ·±åº¦å­¦ä¹  (DL) è¿›æ­¥å¯¹å…¨çƒäººå·¥æ™ºèƒ½å®‰å…¨æŒ‘æˆ˜çš„æ·±è¿œå½±å“ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12052v1",
      "published_date": "2025-11-15 06:12:46 UTC",
      "updated_date": "2025-11-15 06:12:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:23.165246+00:00"
    },
    {
      "arxiv_id": "2511.13771v1",
      "title": "ExplainableGuard: Interpretable Adversarial Defense for Large Language Models Using Chain-of-Thought Reasoning",
      "title_zh": "ExplainableGuardï¼šåŸºäºé“¾å¼æ€ç»´æ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹å¯è§£é‡Šå¯¹æŠ—é˜²å¾¡",
      "authors": [
        "Shaowei Guan",
        "Yu Zhai",
        "Zhengyu Zhang",
        "Yanze Wang",
        "Hin Chi Kwok"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly vulnerable to adversarial attacks that can subtly manipulate their outputs. While various defense mechanisms have been proposed, many operate as black boxes, lacking transparency in their decision-making. This paper introduces ExplainableGuard, an interpretable adversarial defense framework leveraging the chain-of-thought (CoT) reasoning capabilities of DeepSeek-Reasoner. Our approach not only detects and neutralizes adversarial perturbations in text but also provides step-by-step explanations for each defense action. We demonstrate how tailored CoT prompts guide the LLM to perform a multi-faceted analysis (character, word, structural, and semantic) and generate a purified output along with a human-readable justification. Preliminary results on the GLUE Benchmark and IMDB Movie Reviews dataset show promising defense efficacy. Additionally, a human evaluation study reveals that ExplainableGuard's explanations outperform ablated variants in clarity, specificity, and actionability, with a 72.5% deployability-trust rating, underscoring its potential for more trustworthy LLM deployments.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº† ExplainableGuardï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ Chain-of-Thought (CoT) æ¨ç†èƒ½åŠ›çš„è§£é‡Šæ€§å¯¹æŠ—é˜²å¾¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åº”å¯¹å¯¹æŠ—æ€§æ”»å‡»æ—¶é˜²å¾¡æœºåˆ¶é€æ˜åº¦ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¾æ‰˜ DeepSeek-Reasonerï¼Œä¸ä»…èƒ½æœ‰æ•ˆæ£€æµ‹å¹¶ä¸­å’Œæ–‡æœ¬ä¸­çš„å¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œè¿˜èƒ½ä¸ºæ¯ä¸€æ­¥é˜²å¾¡æ“ä½œæä¾›è¯¦ç»†çš„é€»è¾‘è§£é‡Šã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ CoT æç¤ºï¼Œæ¨¡å‹å¯ä»¥ä»å­—ç¬¦ã€å•è¯ã€ç»“æ„å’Œè¯­ä¹‰å››ä¸ªç»´åº¦è¿›è¡Œå¤šé¢åˆ†æï¼Œä»è€Œç”Ÿæˆå‡€åŒ–åçš„æ–‡æœ¬ä»¥åŠäººç±»å¯è¯»çš„é˜²å¾¡ä¾æ®ã€‚å®éªŒåœ¨ GLUE åŸºå‡†æµ‹è¯•å’Œ IMDB æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶é˜²å¾¡æ•ˆèƒ½ï¼Œä¸”äººç±»è¯„ä¼°æ˜¾ç¤ºè¯¥æ¡†æ¶ç”Ÿæˆçš„è§£é‡Šåœ¨æ¸…æ™°åº¦ã€ç‰¹å¼‚æ€§å’Œå¯æ“ä½œæ€§ä¸Šè¡¨ç°å‡ºè‰²ã€‚æœ€ç»ˆï¼ŒExplainableGuard è·å¾—äº† 72.5% çš„éƒ¨ç½²ä¿¡ä»»è¯„åˆ†ï¼Œè¯æ˜äº†å…¶åœ¨æå‡ LLM éƒ¨ç½²å¯é æ€§æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "9 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.13771v1",
      "published_date": "2025-11-15 06:11:07 UTC",
      "updated_date": "2025-11-15 06:11:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:19.270380+00:00"
    },
    {
      "arxiv_id": "2511.12047v1",
      "title": "DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging",
      "title_zh": "DCMM-Transformerï¼šé¢å‘åŒ»å­¦å½±åƒçš„åº¦æ ¡æ­£æ··åˆæˆå‘˜æ³¨æ„åŠ›",
      "authors": [
        "Huimin Cheng",
        "Xiaowei Yu",
        "Shushan Wu",
        "Luyang Fang",
        "Chao Cao",
        "Jing Zhang",
        "Tianming Liu",
        "Dajiang Zhu",
        "Wenxuan Zhong",
        "Ping Ma"
      ],
      "abstract": "Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DCMM-Transformerï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºåŒ»å­¦å½±åƒåˆ†æè®¾è®¡çš„æ–°å‹ Vision Transformers (ViTs) æ¶æ„ï¼Œæ—¨åœ¨æ•æ‰å™¨å®˜ã€ç»„ç»‡ç­‰æ½œåœ¨çš„è§£å‰–åˆ†ç»„ç»“æ„ã€‚é’ˆå¯¹ SBM-Transformer ç­‰å‰åºå·¥ä½œåœ¨éå¯å¾®æ€§å’Œè®­ç»ƒä¸ç¨³å®šæ€§æ–¹é¢çš„ç¼ºé™·ï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†åº¦æ ¡æ­£æ··åˆæˆå‘˜ (Degree-Corrected Mixed-Membership, DCMM) æ¨¡å‹ä½œä¸ºè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„åŠ æ€§åç½®ã€‚ä¸ä¾èµ–ä¹˜æ³•æ©ç å’ŒäºŒå€¼é‡‡æ ·çš„æ–¹æ³•ä¸åŒï¼ŒDCMM-Transformer ä»¥å®Œå…¨å¯å¾®ä¸”å¯è§£é‡Šçš„æ–¹å¼å»ºæ¨¡ç¤¾åŒºç»“æ„å’Œåº¦å¼‚è´¨æ€§ã€‚åœ¨æ¶‰åŠå¤§è„‘ã€èƒ¸éƒ¨ã€ä¹³è…ºåŠçœ¼éƒ¨ç­‰å¤šç§æ¨¡æ€çš„æ•°æ®é›†å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å­¦ä¹ åˆ°çš„ç»“æ„åŒ–æ³¨æ„åŠ›è°ƒèŠ‚æ˜¾è‘—æå‡äº†å¯è§£é‡Šæ€§ï¼Œç”Ÿæˆçš„æ³¨æ„åŠ›å›¾åœ¨è§£å‰–å­¦ä¸Šå…·æœ‰æ˜ç¡®çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12047v1",
      "published_date": "2025-11-15 05:55:01 UTC",
      "updated_date": "2025-11-15 05:55:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:25.777895+00:00"
    },
    {
      "arxiv_id": "2511.12046v1",
      "title": "BackWeak: Backdooring Knowledge Distillation Simply with Weak Triggers and Fine-tuning",
      "title_zh": "BackWeakï¼šä»…é€šè¿‡å¼±è§¦å‘å™¨å’Œå¾®è°ƒå®ç°çŸ¥è¯†è’¸é¦åé—¨æ”»å‡»",
      "authors": [
        "Shanmin Wang",
        "Dongdong Zhao"
      ],
      "abstract": "Knowledge Distillation (KD) is essential for compressing large models, yet relying on pre-trained \"teacher\" models downloaded from third-party repositories introduces serious security risks -- most notably backdoor attacks. Existing KD backdoor methods are typically complex and computationally intensive: they employ surrogate student models and simulated distillation to guarantee transferability, and they construct triggers in a way similar to universal adversarial perturbations (UAPs), which being not stealthy in magnitude, inherently exhibit strong adversarial behavior. This work questions whether such complexity is necessary and constructs stealthy \"weak\" triggers -- imperceptible perturbations that have negligible adversarial effect. We propose BackWeak, a simple, surrogate-free attack paradigm. BackWeak shows that a powerful backdoor can be implanted by simply fine-tuning a benign teacher with a weak trigger using a very small learning rate. We demonstrate that this delicate fine-tuning is sufficient to embed a backdoor that reliably transfers to diverse student architectures during a victim's standard distillation process, yielding high attack success rates. Extensive empirical evaluations on multiple datasets, model architectures, and KD methods show that BackWeak is efficient, simpler, and often more stealthy than previous elaborate approaches. This work calls on researchers studying KD backdoor attacks to pay particular attention to the trigger's stealthiness and its potential adversarial characteristics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†çŸ¥è¯†è’¸é¦(Knowledge Distillation)ä¸­æ•™å¸ˆæ¨¡å‹å¼•å…¥çš„åé—¨æ”»å‡»é£é™©ï¼ŒæŒ‡å‡ºç°æœ‰æ”»å‡»æ–¹æ³•é€šå¸¸è¿‡äºå¤æ‚ä¸”å…¶è§¦å‘å™¨åœ¨éšè”½æ€§ä¸Šå­˜åœ¨ç¼ºé™·ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†BackWeakï¼Œä¸€ç§æ— éœ€ä»£ç†æ¨¡å‹çš„ç®€å•æ”»å‡»èŒƒå¼ï¼Œé€šè¿‡æ„å»ºå‡ ä¹ä¸å¯å¯Ÿè§‰ä¸”å¯¹æŠ—æ•ˆåº”æå°çš„\"å¼±\"è§¦å‘å™¨(weak triggers)æ¥å®ç°æ”»å‡»ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºä»¥æä½çš„å­¦ä¹ ç‡å¯¹è‰¯æ€§æ•™å¸ˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œåœ¨ä¸å½±å“æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹æ¤å…¥åé—¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ¤å…¥çš„åé—¨èƒ½å¤Ÿå¯é åœ°è½¬ç§»åˆ°å¤šç§ä¸åŒçš„å­¦ç”Ÿæ¨¡å‹æ¶æ„ä¸­ï¼Œå¹¶åœ¨æ ‡å‡†çš„çŸ¥è¯†è’¸é¦æµç¨‹ä¸‹å±•ç°å‡ºæé«˜çš„æ”»å‡»æˆåŠŸç‡ã€‚åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹æ¶æ„ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜ï¼ŒBackWeakæ¯”ä»¥å¾€ç²¾å¿ƒè®¾è®¡çš„æ–¹æ³•æ›´é«˜æ•ˆã€æ›´ç®€å•ä¸”æ›´å…·éšè”½æ€§ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†åœ¨ç ”ç©¶çŸ¥è¯†è’¸é¦åé—¨æ”»å‡»æ—¶ï¼Œå¿…é¡»é‡ç‚¹å…³æ³¨è§¦å‘å™¨çš„éšè”½æ€§åŠå…¶æ½œåœ¨çš„å¯¹æŠ—ç‰¹æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12046v1",
      "published_date": "2025-11-15 05:53:15 UTC",
      "updated_date": "2025-11-15 05:53:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:30.672250+00:00"
    },
    {
      "arxiv_id": "2511.12041v3",
      "title": "Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers",
      "title_zh": "åŸºäºå¤šå°ºåº¦å›¾ Transformer çš„ç½‘æ ¼åŒ–çˆ†è½°æµè¶…åˆ†è¾¨ç‡",
      "authors": [
        "Shivam Barwey",
        "Pinaki Pal"
      ],
      "abstract": "Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element + neighborhood graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ååº”æµçš„é«˜åˆ†è¾¨ç‡é‡å»ºï¼Œæå‡ºäº†ä¸€ç§åä¸ºSR-GTçš„åŸºäºç½‘æ ¼çš„å¤šå°ºåº¦å›¾å˜æ¢å™¨(Multiscale Graph Transformer)æ–¹æ³•ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŸºäºå›¾çš„æµåœºè¡¨ç¤ºï¼Œèƒ½å¤Ÿå…¼å®¹å¤æ‚å‡ ä½•å½¢çŠ¶ä»¥åŠéå‡åŒ€æˆ–éç»“æ„åŒ–ç½‘æ ¼ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ•°æ®é©±åŠ¨æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»å‡ ä½•æ—¶çš„å±€é™ã€‚SR-GTåˆ©ç”¨Transformeræ¶æ„æ•æ‰ä½åˆ†è¾¨ç‡æµåœºä¸­çš„é•¿ç¨‹ä¾èµ–å…³ç³»(Long-range Dependencies)ï¼Œç²¾å‡†è¯†åˆ«å…³é”®ç‰¹å¾å¹¶ç”Ÿæˆä¿æŒé«˜ä¿çœŸåº¦çš„è¶…åˆ†è¾¨ç‡æµåœºã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ç‹¬ç‰¹çš„â€œå•å…ƒ+é‚»åŸŸâ€(Element + Neighborhood)å›¾è¡¨ç¤ºæ³•å¯¹ç²—ç³™è¾“å…¥è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡TokenåŒ–å¤„ç†ç¡®ä¿äº†ç‰¹å¾æå–çš„æœ‰æ•ˆæ€§ã€‚åœ¨æ°¢æ°”-ç©ºæ°”é¢„æ··æ°”ä½“çš„2Dçˆ†è½°ä¼ æ’­(Detonation Propagation)æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å±•ç¤ºäº†æé«˜çš„é‡å»ºç²¾åº¦ã€‚å®éªŒè¯æ˜ï¼ŒSR-GTåœ¨å¤„ç†å…·æœ‰å¤æ‚å¤šå°ºåº¦ç‰¹æ€§çš„ååº”æµåœºæ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºæ’å€¼çš„è¶…åˆ†è¾¨ç‡(Super-resolution)æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12041v3",
      "published_date": "2025-11-15 05:20:01 UTC",
      "updated_date": "2025-11-22 08:09:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:52.262234+00:00"
    },
    {
      "arxiv_id": "2511.12036v1",
      "title": "Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys",
      "title_zh": "åŸºäºç‰©ç†åé¦ˆçš„åå¥½å­¦ä¹ ï¼šé¢å‘BCC/B2è¶…åˆé‡‘è®¾è®¡çš„è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Satanu Ghosh",
        "Collin Holgate",
        "Neal R. Brodnik",
        "Doug Downey",
        "Samantha Daly",
        "Tresa M. Pollock",
        "Samuel Carton"
      ],
      "abstract": "We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†åå¥½å­¦ä¹ (Preference Learning)åº”ç”¨äºè¯­è¨€æ¨¡å‹å¼•å¯¼çš„æ–°å‹ç»“æ„åˆé‡‘è®¾è®¡ï¼Œé‡ç‚¹é’ˆå¯¹BCC/B2 superalloysè¿™ä¸€åœ¨æç«¯ç¯å¢ƒä¸‹å…·æœ‰åº”ç”¨æ½œåŠ›çš„ææ–™ç±»åˆ«ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨LLaMA-3.1ã€Gemma-2å’ŒOLMo-2ç­‰å¼€æºæ¨¡å‹ï¼Œè¯æ˜äº†å¯ä»¥é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–(Direct Preference Optimization, DPO)åœ¨ç»Ÿä¸€çš„å¥–åŠ±ä¿¡å·ä¸‹å®ç°å¤šç›®æ ‡è®¾è®¡ã€‚ä¸ä¾èµ–é«˜æˆæœ¬äººå·¥åé¦ˆçš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥ç ”ç©¶åˆ›æ–°æ€§åœ°é‡‡ç”¨çƒ­åŠ›å­¦ç›¸è®¡ç®—(thermodynamic phase calculations)ç”Ÿæˆçš„ç‰©ç†åé¦ˆä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä¸ºæ¨¡å‹å¾®è°ƒæä¾›äº†åšå®çš„ç§‘å­¦ä¾æ®ã€‚è¿™æ˜¯é¦–æ¬¡å±•ç¤ºåˆ©ç”¨ç‰©ç†åé¦ˆ(physics-grounded feedback)å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œåå¥½å¾®è°ƒä»¥è¿›è¡Œç»“æ„åˆé‡‘è®¾è®¡ï¼Œä¸ºç‰©ç†ç§‘å­¦é¢†åŸŸçš„æ™ºèƒ½è®¾è®¡ç©ºé—´æ¢ç´¢æä¾›äº†ä¸€ä¸ªé€šç”¨ä¸”å¯æ‰©å±•çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.CE",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12036v1",
      "published_date": "2025-11-15 05:08:22 UTC",
      "updated_date": "2025-11-15 05:08:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:48.772962+00:00"
    },
    {
      "arxiv_id": "2511.12033v1",
      "title": "EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation",
      "title_zh": "EARLï¼šé¢å‘å¯é  RTL ä»£ç ç”Ÿæˆçš„ç†µæ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ å¯¹é½",
      "authors": [
        "Jiahe Shi",
        "Zhengqi Gao",
        "Ching-Yun Ko",
        "Duane Boning"
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naÃ¯vely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‡ªåŠ¨ç”Ÿæˆå¯„å­˜å™¨ä¼ è¾“çº§(RTL)ä»£ç æ—¶é¢ä¸´çš„è¯­æ³•é”™è¯¯ã€åŠŸèƒ½å¹»è§‰åŠè®¾è®¡æ„å›¾å¯¹é½ä¸è¶³ç­‰é—®é¢˜ï¼Œæå‡ºäº†åä¸ºEARLçš„ç†µæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚ç ”ç©¶é€šè¿‡å¯¹RTLç”Ÿæˆçš„ç†µ(Entropy)åˆ†æå‘ç°ï¼Œä»…æœ‰å°‘æ•°å…³é”®Tokenï¼ˆå¦‚always, if, assign, posedgeç­‰ï¼‰å…·æœ‰é«˜ä¸ç¡®å®šæ€§å¹¶ä¸»å¯¼æ§åˆ¶æµä¸æ¨¡å—ç»“æ„ã€‚EARLåˆ©ç”¨å¯éªŒè¯å¥–åŠ±ä¿¡å·è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œå¹¶å¼•å…¥ç†µå¼•å¯¼çš„é€‰æ‹©æ€§æ›´æ–°æœºåˆ¶ï¼Œå°†ç­–ç•¥æ¢¯åº¦é›†ä¸­åœ¨è¿™äº›é«˜ç†µTokenä¸Šã€‚è¿™ç§æ–¹æ³•åœ¨ç¡®ä¿è®­ç»ƒç¨³å®šæ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆå¼ºåŒ–äº†å¯¹åŠŸèƒ½æ­£ç¡®æ€§è‡³å…³é‡è¦ä»£ç åŒºåŸŸçš„æ›´æ–°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEARLåœ¨VerilogEvalå’ŒRTLLMä¸Šçš„åŠŸèƒ½é€šè¿‡ç‡(Functional Pass Rate)æ¯”ç°æœ‰æ¨¡å‹æœ€é«˜æå‡äº†14.7%ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°†å¼ºåŒ–å­¦ä¹ (RL)èšç„¦äºå…³é”®çš„é«˜ä¸ç¡®å®šæ€§Tokenï¼Œèƒ½ä¸ºç»“æ„åŒ–RTLä»£ç ç”Ÿæˆå¸¦æ¥æ›´å¯é ä¸”å…·é’ˆå¯¹æ€§çš„æ€§èƒ½æ”¹è¿›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12033v1",
      "published_date": "2025-11-15 05:00:07 UTC",
      "updated_date": "2025-11-15 05:00:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:32:53.169347+00:00"
    },
    {
      "arxiv_id": "2511.12031v1",
      "title": "Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding",
      "title_zh": "æƒè¡¡è®¡ç®—ä¸å¤åˆ¶ï¼šæå‡æŠ•æœºè§£ç ä¸‹çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ€§èƒ½",
      "authors": [
        "Arun Ramachandran",
        "Ramaswamy Govindarajan",
        "Murali Annavaram",
        "Prakash Raghavendra",
        "Hossein Entezari Zarch",
        "Lei Gao",
        "Chaoyi Jiang"
      ],
      "abstract": "With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†ä¸­KV cacheæ›´æ–°å¯¼è‡´çš„é¢‘ç¹å†…å­˜åˆ†é…ä¸å¤åˆ¶å¼€é”€ï¼Œæå‡ºäº†ä¸€ç§å¹³è¡¡å†…å­˜ä¸è®¡ç®—(Balancing Memory and Compute, BMC)çš„æ–°å‹åˆ†é…æœºåˆ¶ã€‚BMCé€šè¿‡æ¯éš”ræ¬¡è¿­ä»£åˆ†é…å¸¦æœ‰å†—ä½™è¡Œçš„KV tensorsï¼Œå®ç°äº†æ— éœ€é¢å¤–æ‹·è´çš„å°±åœ°æ›´æ–°ï¼Œå·§å¦™åœ°åˆ©ç”¨å°‘é‡å†—ä½™è®¡ç®—æ¢å–äº†æ˜¾è‘—çš„å†…å­˜æ€§èƒ½å¢ç›Šã€‚ä½œè€…è¿˜å‘ç°è¿™äº›å†—ä½™è¡Œå¯è¢«é‡ç”¨äºæŠ•æœºè§£ç (Speculative Decoding, SD)ä»¥è¿›ä¸€æ­¥æå‡ä»¤ç‰Œç”Ÿæˆæ•ˆç‡ï¼Œå¹¶å»ºç«‹äº†åˆ†ææ¨¡å‹ä»¥ç¡®å®šæœ€ä¼˜å‚æ•°rã€‚å®éªŒè¡¨æ˜ï¼ŒBMCç›¸æ¯”HuggingFaceåŸºå‡†å®ç°äº†é«˜è¾¾3.2å€çš„ååé‡åŠ é€Ÿï¼Œåœ¨ç»“åˆSDæ—¶å¯è·å¾—é¢å¤–çš„1.39å€æ€§èƒ½æå‡ï¼Œä¸”è¡¨ç°ä¼˜äºvLLMå’ŒDeepSpeedç­‰ä¸»æµæ¡†æ¶ã€‚è¯¥æ–¹æ¡ˆåœ¨æ¡Œé¢çº§å’ŒæœåŠ¡å™¨çº§CPUä¸Šå‡å¾—åˆ°äº†å¹¿æ³›éªŒè¯ï¼Œå¹¶è¯æ˜åœ¨GPUç¯å¢ƒä¸‹åŒæ ·å…·æœ‰è‰¯å¥½çš„é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12031v1",
      "published_date": "2025-11-15 04:49:23 UTC",
      "updated_date": "2025-11-15 04:49:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:33:00.065925+00:00"
    },
    {
      "arxiv_id": "2511.12027v1",
      "title": "GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory",
      "title_zh": "GCAgentï¼šåŸºäºå›¾å¼åŒ–ä¸å™äº‹æ€§æƒ…èŠ‚è®°å¿†çš„é•¿è§†é¢‘ç†è§£",
      "authors": [
        "Jeong Hun Yeo",
        "Sangyun Chung",
        "Sungjune Park",
        "Dae Hoe Kim",
        "Jinyoung Moon",
        "Yong Man Ro"
      ],
      "abstract": "Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\\% accuracy on the Long split and the highest overall average (71.9\\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.",
      "tldr_zh": "é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨é•¿è§†é¢‘ç†è§£ä¸­é¢ä¸´çš„æ ‡è®°é™åˆ¶å’Œå¤æ‚æ—¶é—´ä¾èµ–æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†GCAgentï¼Œä¸€ä¸ªå…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥å›¾å¼ä¸å™è¿°æƒ…èŠ‚è®°å¿†(Schematic and Narrative Episodic Memory)ï¼Œé€šè¿‡ç»“æ„åŒ–å»ºæ¨¡äº‹ä»¶åŠå…¶å› æœå’Œæ—¶é—´å…³ç³»ï¼Œå°†å¤æ‚è§†é¢‘ä¿¡æ¯è½¬åŒ–ä¸ºç®€æ´æœ‰åºçš„ä¸Šä¸‹æ–‡ä»¥è§£å†³é•¿æœŸä¾èµ–é—®é¢˜ã€‚GCAgentåœ¨å¤šé˜¶æ®µçš„æ„ŸçŸ¥-è¡ŒåŠ¨-åæ€(Perception-Action-Reflection)å¾ªç¯ä¸­è¿è¡Œï¼Œå¹¶åˆ©ç”¨å­˜å‚¨ç®¡ç†å™¨(Memory Manager)æ£€ç´¢ç›¸å…³æƒ…èŠ‚èƒŒæ™¯è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGCAgentåœ¨Video-MMEé•¿è§†é¢‘æµ‹è¯•é›†ä¸Šè¾ƒåŸºçº¿æ¨¡å‹å‡†ç¡®ç‡æå‡äº†23.5%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨åŒç±»7Bè§„æ¨¡çš„MLLMsä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›(SOTA)æ€§èƒ½ï¼Œåœ¨Video-MMEé•¿è§†é¢‘æ‹†åˆ†ä¸­å‡†ç¡®ç‡è¾¾åˆ°73.4%ï¼ŒéªŒè¯äº†å…¶å—è®¤çŸ¥å¯å‘çš„ç»“æ„åŒ–å­˜å‚¨ä¸æ™ºèƒ½ä½“æ¨ç†èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12027v1",
      "published_date": "2025-11-15 04:29:00 UTC",
      "updated_date": "2025-11-15 04:29:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:33:14.083208+00:00"
    },
    {
      "arxiv_id": "2511.11780v1",
      "title": "Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing",
      "title_zh": "Image-POSERï¼šé¢å‘å¤šä¸“å®¶å›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„åæ€å¼å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Hossein Mohebbi",
        "Mohammed Abdulrahman",
        "Yanting Miao",
        "Pascal Poupart",
        "Suraj Kothawade"
      ],
      "abstract": "Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Image-POSERï¼Œè¿™æ˜¯ä¸€ç§åå°„å¼å¼ºåŒ–å­¦ä¹ (Reflective Reinforcement Learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å•æ¬¡ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†åˆ›æ„å·¥ä½œæµä¸­å¸¸è§çš„é•¿ç¯‡ç»„åˆæŒ‡ä»¤æ—¶çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åè°ƒå¤šç§é¢„è®­ç»ƒçš„text-to-imageå’Œimage-to-imageä¸“å®¶æ¨¡å‹ï¼Œå¹¶å°†å›¾åƒåˆæˆä¸ç¼–è¾‘å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process)ï¼Œå®ç°äº†å¯¹é•¿æ ¼å¼æç¤ºè¯çš„ç«¯åˆ°ç«¯å¤„ç†ã€‚Image-POSERåˆ©ç”¨åŠ¨æ€ä»»åŠ¡åˆ†è§£(dynamic task decomposition)æŠ€æœ¯å¤„ç†å¤æ‚éœ€æ±‚ï¼Œå¹¶ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹(vision-language model)è¯„è®ºè€…æä¾›çš„ç»“æ„åŒ–åé¦ˆæ¥ç›‘ç£æ¯ä¸€æ­¥çš„å¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒImage-POSERåœ¨å¯¹é½åº¦ã€ä¿çœŸåº¦å’Œç¾å­¦è¡¨ç°ç­‰åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºåŒ…æ‹¬å‰æ²¿æ¨¡å‹åœ¨å†…çš„åŸºå‡†æ–¹æ³•ï¼Œå¹¶åœ¨äººå·¥è¯„ä¼°ä¸­è·å¾—äº†æ›´é«˜åå¥½ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿä½¿AIç³»ç»Ÿå…·å¤‡è‡ªä¸»åˆ†è§£ã€é‡ç»„å’Œç»“åˆè§†è§‰æ¨¡å‹çš„èƒ½åŠ›ï¼Œä¸ºå¼€å‘é€šç”¨è§†è§‰åŠ©æ‰‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11780v1",
      "published_date": "2025-11-15 03:15:34 UTC",
      "updated_date": "2025-11-15 03:15:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:33:02.684491+00:00"
    },
    {
      "arxiv_id": "2511.12008v1",
      "title": "Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models",
      "title_zh": "åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç—…ç†å­¦è‡ªé€‚åº”è¯Šæ–­æ¨ç†æ¡†æ¶",
      "authors": [
        "Yunqi Hong",
        "Johnson Kao",
        "Liam Edwards",
        "Nein-Tzu Liu",
        "Chung-Yen Huang",
        "Alex Oliveira-Kowaleski",
        "Cho-Jui Hsieh",
        "Neil Y. C. Lin"
      ],
      "abstract": "AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RECAP-PATHï¼Œä¸€ç§é¢å‘ç—…ç†å­¦é¢†åŸŸçš„å¯è§£é‡Šè¯Šæ–­æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨é©±åŠ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) ä»è¢«åŠ¨çš„æ¨¡å¼è¯†åˆ«è½¬å‘ä¸è¯æ®æŒ‚é’©çš„é€»è¾‘æ¨ç†ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒé‡‡ç”¨ä¸¤é˜¶æ®µè‡ªå­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡å¤šæ ·åŒ– (diversification) æ‰©å±•ç—…ç†å­¦è§£é‡Šå¹¶åˆ©ç”¨ä¼˜åŒ– (optimization) æå‡å‡†ç¡®æ€§ï¼Œä»è€Œè‡ªä¸»æ¨å¯¼è¯Šæ–­æ ‡å‡†ã€‚RECAP-PATH åœ¨ä»…éœ€å°‘é‡æ ‡æ³¨æ•°æ®é›†ä¸”æ— éœ€æ¨¡å‹æƒé‡æ›´æ–°çš„æƒ…å†µä¸‹ï¼Œå³å¯å®ç°å¯¹ç™Œç—‡çš„ç²¾å‡†è¯Šæ–­ã€‚åœ¨ä¹³è…ºå’Œå‰åˆ—è…ºæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„æ¨ç†é€»è¾‘ä¸ä¸“å®¶è¯„ä¼°é«˜åº¦ä¸€è‡´ï¼Œä¸”è¯Šæ–­å‡†ç¡®ç‡è¾ƒåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å°†è§†è§‰ç†è§£ä¸åŒ»å­¦æ¨ç†æ·±åº¦èåˆï¼Œä¸ºæ„å»ºä¸´åºŠå¯ä¿¡ä¸”å…·å¤‡è¯æ®è§£é‡Šæ€§çš„ç—…ç†è¯Šæ–­ AI å¼€è¾Ÿäº†é€šç”¨è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12008v1",
      "published_date": "2025-11-15 03:06:59 UTC",
      "updated_date": "2025-11-15 03:06:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:33:03.779114+00:00"
    },
    {
      "arxiv_id": "2511.12006v1",
      "title": "Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy",
      "title_zh": "ä¸ç¡®å®šæ€§å¼•å¯¼çš„é€‰æ‹©æ€§è‡ªé€‚åº”åŠ©åŠ›è·¨å¹³å°é¢„æµ‹è§å…‰æ˜¾å¾®æˆåƒ",
      "authors": [
        "Kai-Wen K. Yang",
        "Andrew Bai",
        "Alexandra Bermudez",
        "Yunqi Hong",
        "Zoe Latham",
        "Iris Sloan",
        "Michael Liu",
        "Vishrut Goyal",
        "Cho-Jui Hsieh",
        "Neil Y. C. Lin"
      ],
      "abstract": "Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä¸åŒæ˜¾å¾®é•œä»ªå™¨æˆ–é‡‡é›†è®¾ç½®ä¸‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†SIT-ADDA-Autoæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è·¨å¹³å°çš„é¢„æµ‹æ€§è§å…‰æ˜¾å¾®æˆåƒã€‚è¯¥æ¡†æ¶æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„å…¨ç½‘ç»œé‡è®­èŒƒå¼ï¼Œæå‡ºä»…é€‚é…ç½‘ç»œä¸­æœ€æ—©çš„å·ç§¯å±‚ï¼ˆearly convolutional layersï¼‰å¹¶å†»ç»“æ·±å±‚ç»“æ„ï¼Œä»è€Œåœ¨è¿ç§»è¿‡ç¨‹ä¸­ä¿æŒå·²å­¦ä¹ çš„è¯­ä¹‰è¡¨ç¤ºã€‚SIT-ADDA-Autoç»“åˆäº†æµ…å±‚å¯¹æŠ—å¯¹é½ä¸é¢„æµ‹ä¸ç¡®å®šæ€§ï¼ˆpredictive uncertaintyï¼‰ï¼Œèƒ½å¤Ÿåœ¨ç¼ºä¹ç›®æ ‡æ ‡ç­¾çš„æƒ…å†µä¸‹è‡ªåŠ¨ç¡®å®šæœ€ä½³çš„é€‚é…æ·±åº¦ã€‚å¤šé¡¹å®éªŒè¯æ˜ï¼Œåœ¨é¢å¯¹æ›å…‰ã€ç…§æ˜å˜åŒ–åŠè·¨ä»ªå™¨è¿ç§»æ—¶ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒé‡å»ºå’Œä¸‹æ¸¸åˆ†å‰²ï¼ˆsegmentationï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¨ç¼–ç å™¨é€‚é…å’Œéå¯¹æŠ—åŸºçº¿æ¨¡å‹ã€‚è¯¥æˆæœæœ‰æ•ˆå‡å°‘äº†è¯­ä¹‰ç‰¹å¾æ¼‚ç§»ï¼Œä¸ºæ˜¾å¾®æˆåƒé¢†åŸŸçš„æ— æ ‡ç­¾é¢†åŸŸé€‚é…ï¼ˆlabel-free adaptationï¼‰æä¾›äº†é‡è¦çš„è®¾è®¡å‡†åˆ™å’Œå®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.12006v1",
      "published_date": "2025-11-15 03:01:05 UTC",
      "updated_date": "2025-11-15 03:01:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:33:07.372050+00:00"
    },
    {
      "arxiv_id": "2511.12003v2",
      "title": "Look as You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning",
      "title_zh": "Look as You Thinkï¼šåŸºäºå¼ºåŒ–å­¦ä¹ ç»Ÿä¸€æ¨ç†ä¸è§†è§‰è¯æ®å½’å› çš„å¯éªŒè¯æ–‡æ¡£ RAG",
      "authors": [
        "Shuochen Liu",
        "Pengfei Luo",
        "Chao Zhang",
        "Yuhao Chen",
        "Haotian Zhang",
        "Qi Liu",
        "Xin Kou",
        "Tong Xu",
        "Enhong Chen"
      ],
      "abstract": "Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰æ–‡æ¡£æ£€ç´¢å¢å¼ºç”Ÿæˆ(VD-RAG)ä¸­çš„è¯æ®å½’å› é—®é¢˜ï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤šæ¨¡æ€é—®ç­”ä¸­çš„å¯é æ€§ä¸å¯éªŒè¯æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Chain-of-Evidence (CoE)èŒƒå¼ï¼Œå°†é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†ä¸è§†è§‰è¯æ®å½’å› ç›¸ç»Ÿä¸€ï¼Œé€šè¿‡è¾¹ç•Œæ¡†(bounding boxes)å’Œé¡µé¢ç´¢å¼•å°†æ¨ç†æ­¥éª¤ä¸å…·ä½“çš„æ–‡æ¡£åŒºåŸŸå…³è”ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†Look As You Think (LAT)å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨è¿‡ç¨‹çº§å¥–åŠ±æœºåˆ¶è®­ç»ƒæ¨¡å‹ç”Ÿæˆå…·æœ‰ä¸€è‡´å½’å› çš„å¯éªŒè¯æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLATåœ¨Qwen2.5-VL-7B-Instructæ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨Paper-VISAå’ŒWiki-VISAåŸºå‡†æµ‹è¯•ä¸­ä½¿è½¯ç²¾ç¡®åŒ¹é…(soft EM)å’ŒIoU@0.5åˆ†åˆ«å¹³å‡æå‡äº†8.23%å’Œ47.0%ã€‚è¯¥æ–¹æ³•ä¸ä»…åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒ(SFT)åŸºçº¿ï¼Œè¿˜å±•ç¤ºäº†æ›´å¼ºçš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Poster of AAAI'2026",
      "pdf_url": "https://arxiv.org/pdf/2511.12003v2",
      "published_date": "2025-11-15 02:50:23 UTC",
      "updated_date": "2025-11-29 05:50:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:33:12.361099+00:00"
    },
    {
      "arxiv_id": "2511.11992v1",
      "title": "Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams",
      "title_zh": "é¢å‘å»ä¸­å¿ƒåŒ–æ™ºèƒ½ä½“å›¢é˜Ÿçš„ç›®æ ‡å¯¼å‘å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Hung Du",
        "Hy Nguyen",
        "Srikanth Thudumu",
        "Rajesh Vasa",
        "Kon Mouzakis"
      ],
      "abstract": "Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é™†ã€æµ·ã€ç©ºè‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨é€šä¿¡å—é™ã€æ— ä¸­å¿ƒæ§åˆ¶å’Œéƒ¨åˆ†å¯è§‚æµ‹ï¼ˆpartial observabilityï¼‰çš„åŠ¨æ€ç¯å¢ƒä¸­é¢ä¸´çš„åä½œéš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§å»ä¸­å¿ƒåŒ–çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMulti-Agent Reinforcement Learning, MARLï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åŸºäºç›®æ ‡çš„é€šä¿¡ç­–ç•¥ï¼ˆgoal-aware communication strategyï¼‰ï¼Œå…è®¸æ™ºèƒ½ä½“æ ¹æ®å±€éƒ¨ç›®æ ‡å’Œè§‚æµ‹ç»“æœè¿›è¡Œé€‰æ‹©æ€§é€šä¿¡ï¼Œåœ¨å°Šé‡å¯è§æ€§é™åˆ¶çš„åŒæ—¶é€šè¿‡å…±äº«å…³é”®ä¿¡æ¯å¢å¼ºåä½œã€‚ç ”ç©¶äººå‘˜åœ¨åŒ…å«éšœç¢ç‰©å’ŒåŠ¨æ€æ™ºèƒ½ä½“ç¾¤ä½“çš„å¤æ‚å¤šæ™ºèƒ½ä½“å¯¼èˆªä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸éåˆä½œåŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ä»»åŠ¡æˆåŠŸç‡å¹¶ç¼©çŸ­äº†åˆ°è¾¾ç›®æ ‡çš„æ—¶é—´ï¼ˆtime-to-goalï¼‰ã€‚æ­¤å¤–ï¼Œéšç€æ™ºèƒ½ä½“æ•°é‡å¢åŠ ï¼Œä»»åŠ¡æ€§èƒ½ä¿æŒç¨³å®šï¼Œå±•ç°äº†è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼ˆscalabilityï¼‰ï¼Œè¯æ˜äº†å»ä¸­å¿ƒåŒ–ç›®æ ‡é©±åŠ¨å‹ MARL åœ¨ç°å®å¤šè½¦è¾†ç³»ç»ŸååŒä½œä¸šä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted poster at the IEEE Consumer Communications & Networking Conference (CCNC) 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11992v1",
      "published_date": "2025-11-15 02:11:31 UTC",
      "updated_date": "2025-11-15 02:11:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:33:14.675101+00:00"
    },
    {
      "arxiv_id": "2511.11990v4",
      "title": "Improving Autoformalization Using Direct Dependency Retrieval",
      "title_zh": "åˆ©ç”¨ç›´æ¥ä¾èµ–æ£€ç´¢æå‡è‡ªåŠ¨å½¢å¼åŒ–",
      "authors": [
        "Shaoqi Wang",
        "Lu Yu",
        "Siwei Lou",
        "Feng Yan",
        "Chunjie Yang",
        "Qing Cui",
        "Jun Zhou"
      ],
      "abstract": "The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨å½¢å¼åŒ–(Autoformalization)è¿‡ç¨‹ä¸­å› ç¼ºä¹ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¯¼è‡´çš„å®šä¹‰å¹»è§‰ä»¥åŠç°æœ‰æ£€ç´¢æ–¹æ³•åœ¨å½¢å¼åŒ–åº“ä¾èµ–æ£€ç´¢ä¸Šç²¾åº¦è¾ƒä½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDDR(Direct Dependency Retrieval)çš„æ–°å‹æ£€ç´¢å¢å¼ºæ¡†æ¶ã€‚è¯¥æ–¹æ³•ç›´æ¥ä»è‡ªç„¶è¯­è¨€æ•°å­¦æè¿°ä¸­ç”Ÿæˆå€™é€‰åº“ä¾èµ–ï¼Œå¹¶åˆ©ç”¨é«˜æ•ˆçš„åç¼€æ•°ç»„(suffix array)æ ¡éªŒå…¶åœ¨å½¢å¼åŒ–åº“ä¸­çš„çœŸå®å­˜åœ¨æ€§ã€‚åŸºäºè¿™ä¸€æœºåˆ¶ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å«è¶…è¿‡50ä¸‡ä¸ªæ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†å¹¶å¾®è°ƒäº†é«˜ç²¾åº¦çš„DDRæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDDRæ¨¡å‹åœ¨æ£€ç´¢ç²¾ç¡®ç‡å’Œå¬å›ç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›(SOTA)æ–¹æ³•ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ­è½½DDRçš„è‡ªåŠ¨å½¢å¼åŒ–ç³»ç»Ÿåœ¨å•æ¬¡å°è¯•å‡†ç¡®ç‡å’Œå¤šæ¬¡å°è¯•ç¨³å®šæ€§æ–¹é¢ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„åŸºäºé€‰æ‹©çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ–¹æ³•è¡¨ç°å‡ºæŒç»­çš„æ€§èƒ½ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11990v4",
      "published_date": "2025-11-15 02:05:11 UTC",
      "updated_date": "2026-01-01 15:38:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:33:31.983453+00:00"
    },
    {
      "arxiv_id": "2511.11966v2",
      "title": "On the Entropy Calibration of Language Models",
      "title_zh": "è®ºè¯­è¨€æ¨¡å‹çš„ç†µæ ¡å‡†",
      "authors": [
        "Steven Cao",
        "Gregory Valiant",
        "Percy Liang"
      ],
      "abstract": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing as generations grow longer, due to error accumulation. To calibrate the model and improve text quality, it has become standard practice to truncate the distribution, but this approach reduces output diversity, which we would like to avoid. Therefore, in this paper, we ask: does miscalibration improve automatically with scale, and if not, is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the rate of scaling depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted theoretically: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹çš„ç†µæ ¡å‡† (Entropy Calibration) é—®é¢˜ï¼Œå³æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„ç†µæ˜¯å¦ä¸å…¶åœ¨äººç±»æ–‡æœ¬ä¸Šçš„å¯¹æ•°æŸå¤± (log loss) ç›¸åŒ¹é…ã€‚ç ”ç©¶å‘ç°ç”±äºè¯¯å·®ç§¯ç´¯ï¼Œè¯­è¨€æ¨¡å‹æ™®éå­˜åœ¨å¤±å‡†ç°è±¡ï¼Œä¸”è¿™ç§å¤±å‡†éšæ¨¡å‹è§„æ¨¡å¢åŠ è€Œæ”¹å–„çš„é€Ÿåº¦ææ…¢ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†æ ¡å‡†è§„æ¨¡åŒ–çš„é€Ÿåº¦å–å†³äºæ•°æ®åˆ†å¸ƒçš„å¹‚å¾‹æŒ‡æ•° (power law exponent)ï¼Œé’ˆå¯¹0.5Båˆ°70Bå‚æ•°æ¨¡å‹çš„å®è¯åˆ†æè¿›ä¸€æ­¥è¯å®å¤§æ¨¡å‹ä¸å°æ¨¡å‹çš„è¯¯å·®ç§¯ç´¯é€Ÿç‡ç›¸ä¼¼ã€‚è¿™è§£é‡Šäº†ä¸ºä½•é«˜è´¨é‡çš„å¤§æ¨¡å‹ä»éœ€ä¾èµ–æˆªæ–­ (truncation) æŠ€æœ¯æ¥æ§åˆ¶ç†µå€¼ï¼Œå°½ç®¡è¿™ç§åšæ³•ä¼šæŸå®³è¾“å‡ºå¤šæ ·æ€§å¹¶å¢åŠ å¯¹æ•°æŸå¤±ã€‚æœ€åï¼Œç ”ç©¶åœ¨ç†è®ºä¸Šè¯æ˜äº†é€šè¿‡å¼•å…¥èƒ½å¤Ÿé¢„æµ‹æœªæ¥æ–‡æœ¬ç†µçš„é»‘ç›’æ¨¡å‹ï¼Œå¯ä»¥åœ¨ä¿ç•™å¯¹æ•°æŸå¤±çš„å‰æä¸‹é™ä½ç†µï¼Œä¸ºåœ¨ä¸ç‰ºç‰²å¤šæ ·æ€§çš„æƒ…å†µä¸‹æå‡æ¨¡å‹è´¨é‡æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "Neurips 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.11966v2",
      "published_date": "2025-11-15 00:33:03 UTC",
      "updated_date": "2026-01-13 00:58:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:33:29.182764+00:00"
    },
    {
      "arxiv_id": "2511.17576v1",
      "title": "Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks",
      "title_zh": "é¢å‘ä½“è„‚ä¼°ç®—çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ï¼šèåˆè®¡ç®—æœºè§†è§‰ä¸äººä½“æµ‹é‡å­¦çš„DEXAåŸºå‡†ç ”ç©¶",
      "authors": [
        "Rayan Aldajani"
      ],
      "abstract": "Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†åˆ©ç”¨äººå·¥æ™ºèƒ½(AI)æ¨¡å‹ä½œä¸ºä½æˆæœ¬ä½“è„‚ç™¾åˆ†æ¯”ä¼°ç®—æ›¿ä»£æ–¹æ¡ˆçš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨è§£å†³DEXAæ‰«æç­‰é‡‘æ ‡å‡†æ–¹æ³•ä»·æ ¼æ˜‚è´µä¸”æ™®åŠç‡ä½çš„é—®é¢˜ã€‚ç ”ç©¶è€…ä¸“é—¨ç¼–è¯‘äº†ä¸€ä¸ªåŒ…å«535ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬äººä½“æµ‹é‡æ•°æ®ï¼ˆå¦‚èº«é«˜ã€ä½“é‡ã€é¢ˆéƒ¨ã€è¸éƒ¨å’Œè…•éƒ¨å‘¨é•¿ï¼‰ä»¥åŠä»ç¤¾äº¤åª’ä½“è·å–çš„è‡ªæŠ¥ä½“è„‚å›¾åƒã€‚ç ”ç©¶å¼€å‘äº†åŸºäºResNetçš„å›¾åƒæ¨¡å‹å’Œåˆ©ç”¨äººä½“æµ‹é‡æ•°æ®çš„å›å½’æ¨¡å‹ï¼Œå¹¶ä¸ºæœªæ¥æ•°æ®æ‰©å±•è®¾è®¡äº†å¤šæ¨¡æ€èåˆæ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå›¾åƒçš„æ¨¡å‹å®ç°äº†4.44%çš„å‡æ–¹æ ¹è¯¯å·®(RMSE)å’Œ0.807çš„å†³å®šç³»æ•°(R^2)ã€‚è¿™äº›å‘ç°è¯æ˜äº†AIè¾…åŠ©æ¨¡å‹åœ¨æä¾›ä¾¿æ·ã€ä½æˆæœ¬ä½“è„‚ä¼°ç®—æ–¹é¢çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒå¥åº·ä¸å¥èº«é¢†åŸŸçš„æœªæ¥æ¶ˆè´¹çº§åº”ç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "2 pages, 2 figures, accepted at IEEE CASCON 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.17576v1",
      "published_date": "2025-11-15 00:20:24 UTC",
      "updated_date": "2025-11-15 00:20:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:33:35.668857+00:00"
    },
    {
      "arxiv_id": "2511.11954v1",
      "title": "LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code",
      "title_zh": "LLM è¾…åŠ©çš„å½¢å¼åŒ–å®ç°ã€Šå›½å†…æ”¶å…¥æ³•å…¸ã€‹æ³•å®šä¸ä¸€è‡´æ€§çš„ç¡®å®šæ€§æ£€æµ‹",
      "authors": [
        "Borchuluun Yadamsuren",
        "Steven Keith Platt",
        "Miguel Diaz"
      ],
      "abstract": "This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.\n  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.\n  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.\n  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç¥ç»ç¬¦å·æ··åˆæ¡†æ¶(neuro-symbolic framework)ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸ç¬¦å·é€»è¾‘(symbolic logic)ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œè§£å†³ç¾å›½å›½å†…ç¨æ”¶æ³•(Internal Revenue Code, IRC)ç­‰å¤æ‚æ³•å¾‹ä¸­çš„æˆæ–‡æ³•ä¸ä¸€è‡´æ€§æ£€æµ‹é—®é¢˜ã€‚ç ”ç©¶è€…é€šè¿‡å®éªŒå¯¹æ¯”äº†GPT-4oåœ¨è‡ªç„¶è¯­è¨€æç¤ºä¸Prologå¢å¼ºæç¤ºä¸‹çš„è¡¨ç°ï¼Œå‘ç°è™½ç„¶æ¦‚ç‡æ€§æç¤ºçš„æ£€æµ‹å‡†ç¡®ç‡ä»…ä¸º33%ï¼Œä½†åˆ©ç”¨GPT-5è¾…åŠ©ç²¾ç‚¼çš„Prologå½¢å¼åŒ–æ¨¡å‹å±•ç°å‡ºäº†æé«˜çš„å¯é æ€§ã€‚è¯¥æ··åˆæ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿç¡®å®šæ€§(deterministic)ä¸”å¯é‡å¤çš„ç»“æœï¼Œå¹¶æˆåŠŸåœ¨IRCç‰¹å®šç« èŠ‚ä¸­è¯†åˆ«å‡ºäº†æ³•å¾‹å†²çªçš„ä¸ä¸€è‡´åŒºåŸŸã€‚éªŒè¯æµ‹è¯•ç¡®è®¤äº†Prologå®ç°æ–¹æ¡ˆåœ¨è‡ªä¸»è¯†åˆ«æ³•å¾‹ä¸ä¸€è‡´æ€§æ–¹é¢å…·æœ‰é«˜åº¦çš„å‡†ç¡®æ€§å’Œå†…éƒ¨ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMè¾…åŠ©çš„å½¢å¼åŒ–è¡¨è¾¾èƒ½å¤Ÿå¼¥è¡¥æ¨¡å‹åœ¨æ·±åº¦ç»“æ„åŒ–æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œç¡®ä¿æ³•å¾‹åˆ†æçš„ä¸¥è°¨æ€§ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†å½¢å¼åŒ–é€»è¾‘é”šå®šçš„å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶ä¸ºæ³•å¾‹æ¡æ–‡çš„é€æ˜ã€å¯é æ£€æµ‹æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "29 pages, 3 appendices with Prolog code and full codebase available at: https://github.com/borchuluun/section121-inconsistency-detection",
      "pdf_url": "https://arxiv.org/pdf/2511.11954v1",
      "published_date": "2025-11-15 00:05:02 UTC",
      "updated_date": "2025-11-15 00:05:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:33:44.780085+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 84,
  "processed_papers_count": 84,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T06:35:19.600624+00:00"
}
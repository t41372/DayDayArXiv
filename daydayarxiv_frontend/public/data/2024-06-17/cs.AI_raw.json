[
  {
    "arxiv_id": "2406.12150v1",
    "title": "ChaosMining: A Benchmark to Evaluate Post-Hoc Local Attribution Methods in Low SNR Environments",
    "authors": [
      "Ge Shi",
      "Ziwen Kan",
      "Jason Smucny",
      "Ian Davidson"
    ],
    "abstract": "In this study, we examine the efficacy of post-hoc local attribution methods\nin identifying features with predictive power from irrelevant ones in domains\ncharacterized by a low signal-to-noise ratio (SNR), a common scenario in\nreal-world machine learning applications. We developed synthetic datasets\nencompassing symbolic functional, image, and audio data, incorporating a\nbenchmark on the {\\it (Model \\(\\times\\) Attribution\\(\\times\\) Noise Condition)}\ntriplet. By rigorously testing various classic models trained from scratch, we\ngained valuable insights into the performance of these attribution methods in\nmultiple conditions. Based on these findings, we introduce a novel extension to\nthe notable recursive feature elimination (RFE) algorithm, enhancing its\napplicability for neural networks. Our experiments highlight its strengths in\nprediction and feature selection, alongside limitations in scalability. Further\ndetails and additional minor findings are included in the appendix, with\nextensive discussions. The codes and resources are available at\n\\href{https://github.com/geshijoker/ChaosMining/}{URL}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 10 figures, submission to Neurips 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12150v1",
    "published_date": "2024-06-17 23:39:29 UTC",
    "updated_date": "2024-06-17 23:39:29 UTC"
  },
  {
    "arxiv_id": "2406.12147v1",
    "title": "Metacognitive AI: Framework and the Case for a Neurosymbolic Approach",
    "authors": [
      "Hua Wei",
      "Paulo Shakarian",
      "Christian Lebiere",
      "Bruce Draper",
      "Nikhil Krishnaswamy",
      "Sergei Nirenburg"
    ],
    "abstract": "Metacognition is the concept of reasoning about an agent's own internal\nprocesses and was originally introduced in the field of developmental\npsychology. In this position paper, we examine the concept of applying\nmetacognition to artificial intelligence. We introduce a framework for\nunderstanding metacognitive artificial intelligence (AI) that we call TRAP:\ntransparency, reasoning, adaptation, and perception. We discuss each of these\naspects in-turn and explore how neurosymbolic AI (NSAI) can be leveraged to\naddress challenges of metacognition.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12147v1",
    "published_date": "2024-06-17 23:30:46 UTC",
    "updated_date": "2024-06-17 23:30:46 UTC"
  },
  {
    "arxiv_id": "2406.12146v2",
    "title": "Should AI Optimize Your Code? A Comparative Study of Classical Optimizing Compilers Versus Current Large Language Models",
    "authors": [
      "Miguel Romero Rosas",
      "Miguel Torres Sanchez",
      "Rudolf Eigenmann"
    ],
    "abstract": "Traditional optimizing compilers have played an important role in adapting to\nthe growing complexity of modern software systems. The need for efficient\nparallel programming in current architectures requires strong optimization\ntechniques. The beginning of Large Language Models (LLMs) raises intriguing\nquestions about the potential of these AI approaches to revolutionize code\noptimization methodologies. This work aims to answer an essential question for\nthe compiler community: \"Can AI-driven models revolutionize the way we approach\ncode optimization?\".\n  To address this question, we present a comparative analysis between three\nclassical optimizing compilers and two recent large language models, evaluating\ntheir respective abilities and limitations in optimizing code for maximum\nefficiency. In addition, we introduce a benchmark suite of challenging\noptimization patterns and an automatic mechanism for evaluating the performance\nand correctness of the code generated by LLMs. We used three different\nprompting strategies to evaluate the performance of the LLMs, Simple\nInstruction (IP), Detailed Instruction Prompting (DIP), and Chain of Thought\n(CoT).\n  A key finding is that while LLMs have the potential to outperform current\noptimizing compilers, they often generate incorrect code on large code sizes,\ncalling for automated verification methods. In addition, expressing a compiler\nstrategy as part of the LLMs prompt substantially improves its overall\nperformance. Our evaluation across three benchmark suites shows CodeLlama-70B\nas the superior LLM, capable of achieving speedups of up to x1.75.\nAdditionally, CETUS is the best among the current optimizing compilers,\nachieving a maximum speedup of 1.67x. We also found substantial differences\namong the three prompting strategies.",
    "categories": [
      "cs.AI",
      "cs.PF",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 7 figures, Accepted at SupercomputingAsia 2025 (SCA'25),\n  March 10 to 13, 2025, Singapore, Singapore",
    "pdf_url": "http://arxiv.org/pdf/2406.12146v2",
    "published_date": "2024-06-17 23:26:41 UTC",
    "updated_date": "2025-04-02 17:22:18 UTC"
  },
  {
    "arxiv_id": "2406.12142v2",
    "title": "Slicing Through Bias: Explaining Performance Gaps in Medical Image Analysis using Slice Discovery Methods",
    "authors": [
      "Vincent Olesen",
      "Nina Weng",
      "Aasa Feragen",
      "Eike Petersen"
    ],
    "abstract": "Machine learning models have achieved high overall accuracy in medical image\nanalysis. However, performance disparities on specific patient groups pose\nchallenges to their clinical utility, safety, and fairness. This can affect\nknown patient groups - such as those based on sex, age, or disease subtype - as\nwell as previously unknown and unlabeled groups. Furthermore, the root cause of\nsuch observed performance disparities is often challenging to uncover,\nhindering mitigation efforts. In this paper, to address these issues, we\nleverage Slice Discovery Methods (SDMs) to identify interpretable\nunderperforming subsets of data and formulate hypotheses regarding the cause of\nobserved performance disparities. We introduce a novel SDM and apply it in a\ncase study on the classification of pneumothorax and atelectasis from chest\nx-rays. Our study demonstrates the effectiveness of SDMs in hypothesis\nformulation and yields an explanation of previously observed but unexplained\nperformance disparities between male and female patients in widely used chest\nX-ray datasets and models. Our findings indicate shortcut learning in both\nclassification tasks, through the presence of chest drains and ECG wires,\nrespectively. Sex-based differences in the prevalence of these shortcut\nfeatures appear to cause the observed classification performance gap,\nrepresenting a previously underappreciated interaction between shortcut\nlearning and model fairness analyses.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "MICCAI 2024 Workshop on Fairness of AI in Medical Imaging",
    "pdf_url": "http://arxiv.org/pdf/2406.12142v2",
    "published_date": "2024-06-17 23:08:46 UTC",
    "updated_date": "2024-10-22 13:32:34 UTC"
  },
  {
    "arxiv_id": "2406.12140v1",
    "title": "COT Flow: Learning Optimal-Transport Image Sampling and Editing by Contrastive Pairs",
    "authors": [
      "Xinrui Zu",
      "Qian Tao"
    ],
    "abstract": "Diffusion models have demonstrated strong performance in sampling and editing\nmulti-modal data with high generation quality, yet they suffer from the\niterative generation process which is computationally expensive and slow. In\naddition, most methods are constrained to generate data from Gaussian noise,\nwhich limits their sampling and editing flexibility. To overcome both\ndisadvantages, we present Contrastive Optimal Transport Flow (COT Flow), a new\nmethod that achieves fast and high-quality generation with improved zero-shot\nediting flexibility compared to previous diffusion models. Benefiting from\noptimal transport (OT), our method has no limitation on the prior distribution,\nenabling unpaired image-to-image (I2I) translation and doubling the editable\nspace (at both the start and end of the trajectory) compared to other zero-shot\nediting methods. In terms of quality, COT Flow can generate competitive results\nin merely one step compared to previous state-of-the-art unpaired\nimage-to-image (I2I) translation methods. To highlight the advantages of COT\nFlow through the introduction of OT, we introduce the COT Editor to perform\nuser-guided editing with excellent flexibility and quality. The code will be\nreleased at https://github.com/zuxinrui/cot_flow.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12140v1",
    "published_date": "2024-06-17 23:02:20 UTC",
    "updated_date": "2024-06-17 23:02:20 UTC"
  },
  {
    "arxiv_id": "2406.12138v1",
    "title": "Bias in Text Embedding Models",
    "authors": [
      "Vasyl Rakivnenko",
      "Nestor Maslej",
      "Jessica Cervi",
      "Volodymyr Zhukov"
    ],
    "abstract": "Text embedding is becoming an increasingly popular AI methodology, especially\namong businesses, yet the potential of text embedding models to be biased is\nnot well understood. This paper examines the degree to which a selection of\npopular text embedding models are biased, particularly along gendered\ndimensions. More specifically, this paper studies the degree to which these\nmodels associate a list of given professions with gendered terms. The analysis\nreveals that text embedding models are prone to gendered biases but in varying\nways. Although there are certain inter-model commonalities, for instance,\ngreater association of professions like nurse, homemaker, and socialite with\nfemale identifiers, and greater association of professions like CEO, manager,\nand boss with male identifiers, not all models make the same gendered\nassociations for each occupation. Furthermore, the magnitude and directionality\nof bias can also vary on a model-by-model basis and depend on the particular\nwords models are prompted with. This paper demonstrates that gender bias\nafflicts text embedding models and suggests that businesses using this\ntechnology need to be mindful of the specific dimensions of this problem.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12138v1",
    "published_date": "2024-06-17 22:58:36 UTC",
    "updated_date": "2024-06-17 22:58:36 UTC"
  },
  {
    "arxiv_id": "2406.12137v3",
    "title": "IDs for AI Systems",
    "authors": [
      "Alan Chan",
      "Noam Kolt",
      "Peter Wills",
      "Usman Anwar",
      "Christian Schroeder de Witt",
      "Nitarshan Rajkumar",
      "Lewis Hammond",
      "David Krueger",
      "Lennart Heim",
      "Markus Anderljung"
    ],
    "abstract": "AI systems are increasingly pervasive, yet information needed to decide\nwhether and how to engage with them may not exist or be accessible. A user may\nnot be able to verify whether a system has certain safety certifications. An\ninvestigator may not know whom to investigate when a system causes an incident.\nIt may not be clear whom to contact to shut down a malfunctioning system.\nAcross a number of domains, IDs address analogous problems by identifying\nparticular entities (e.g., a particular Boeing 747) and providing information\nabout other entities of the same class (e.g., some or all Boeing 747s). We\npropose a framework in which IDs are ascribed to instances of AI systems (e.g.,\na particular chat session with Claude 3), and associated information is\naccessible to parties seeking to interact with that system. We characterize IDs\nfor AI systems, provide concrete examples where IDs could be useful, argue that\nthere could be significant demand for IDs from key actors, analyze how those\nactors could incentivize ID adoption, explore a potential implementation of our\nframework for deployers of AI systems, and highlight limitations and risks. IDs\nseem most warranted in settings where AI systems could have a large impact upon\nthe world, such as in making financial transactions or contacting real humans.\nWith further study, IDs could help to manage a world where AI systems pervade\nsociety.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review; accepted to RegML workshop at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12137v3",
    "published_date": "2024-06-17 22:48:11 UTC",
    "updated_date": "2024-10-28 19:15:40 UTC"
  },
  {
    "arxiv_id": "2406.12123v3",
    "title": "ChatEMG: Synthetic Data Generation to Control a Robotic Hand Orthosis for Stroke",
    "authors": [
      "Jingxi Xu",
      "Runsheng Wang",
      "Siqi Shang",
      "Ava Chen",
      "Lauren Winterbottom",
      "To-Liang Hsu",
      "Wenxi Chen",
      "Khondoker Ahmed",
      "Pedro Leandro La Rotta",
      "Xinyue Zhu",
      "Dawn M. Nilsen",
      "Joel Stein",
      "Matei Ciocarlie"
    ],
    "abstract": "Intent inferral on a hand orthosis for stroke patients is challenging due to\nthe difficulty of data collection. Additionally, EMG signals exhibit\nsignificant variations across different conditions, sessions, and subjects,\nmaking it hard for classifiers to generalize. Traditional approaches require a\nlarge labeled dataset from the new condition, session, or subject to train\nintent classifiers; however, this data collection process is burdensome and\ntime-consuming. In this paper, we propose ChatEMG, an autoregressive generative\nmodel that can generate synthetic EMG signals conditioned on prompts (i.e., a\ngiven sequence of EMG signals). ChatEMG enables us to collect only a small\ndataset from the new condition, session, or subject and expand it with\nsynthetic samples conditioned on prompts from this new context. ChatEMG\nleverages a vast repository of previous data via generative training while\nstill remaining context-specific via prompting. Our experiments show that these\nsynthetic samples are classifier-agnostic and can improve intent inferral\naccuracy for different types of classifiers. We demonstrate that our complete\napproach can be integrated into a single patient session, including the use of\nthe classifier for functional orthosis-assisted tasks. To the best of our\nknowledge, this is the first time an intent classifier trained partially on\nsynthetic data has been deployed for functional control of an orthosis by a\nstroke survivor. Videos, source code, and additional information can be found\nat https://jxu.ai/chatemg.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages; accepted to RA-L in November 2024; published at RA-L in\n  February 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.12123v3",
    "published_date": "2024-06-17 22:04:44 UTC",
    "updated_date": "2025-04-09 21:49:04 UTC"
  },
  {
    "arxiv_id": "2406.12120v3",
    "title": "Adding Conditional Control to Diffusion Models with Reinforcement Learning",
    "authors": [
      "Yulai Zhao",
      "Masatoshi Uehara",
      "Gabriele Scalia",
      "Sunyuan Kung",
      "Tommaso Biancalani",
      "Sergey Levine",
      "Ehsan Hajiramezanali"
    ],
    "abstract": "Diffusion models are powerful generative models that allow for precise\ncontrol over the characteristics of the generated samples. While these\ndiffusion models trained on large datasets have achieved success, there is\noften a need to introduce additional controls in downstream fine-tuning\nprocesses, treating these powerful models as pre-trained diffusion models. This\nwork presents a novel method based on reinforcement learning (RL) to add such\ncontrols using an offline dataset comprising inputs and labels. We formulate\nthis task as an RL problem, with the classifier learned from the offline\ndataset and the KL divergence against pre-trained models serving as the reward\nfunctions. Our method, $\\textbf{CTRL}$ ($\\textbf{C}$onditioning\npre-$\\textbf{T}$rained diffusion models with $\\textbf{R}$einforcement\n$\\textbf{L}$earning), produces soft-optimal policies that maximize the\nabovementioned reward functions. We formally demonstrate that our method\nenables sampling from the conditional distribution with additional controls\nduring inference. Our RL-based approach offers several advantages over existing\nmethods. Compared to classifier-free guidance, it improves sample efficiency\nand can greatly simplify dataset construction by leveraging conditional\nindependence between the inputs and additional controls. Additionally, unlike\nclassifier guidance, it eliminates the need to train classifiers from\nintermediate states to additional controls. The code is available at\nhttps://github.com/zhaoyl18/CTRL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.12120v3",
    "published_date": "2024-06-17 22:00:26 UTC",
    "updated_date": "2025-02-24 02:16:23 UTC"
  },
  {
    "arxiv_id": "2406.12119v1",
    "title": "Deploying scalable traffic prediction models for efficient management in real-world large transportation networks during hurricane evacuations",
    "authors": [
      "Qinhua Jiang",
      "Brian Yueshuai He",
      "Changju Lee",
      "Jiaqi Ma"
    ],
    "abstract": "Accurate traffic prediction is vital for effective traffic management during\nhurricane evacuation. This paper proposes a predictive modeling system that\nintegrates Multilayer Perceptron (MLP) and Long-Short Term Memory (LSTM) models\nto capture both long-term congestion patterns and short-term speed patterns.\nLeveraging various input variables, including archived traffic data,\nspatial-temporal road network information, and hurricane forecast data, the\nframework is designed to address challenges posed by heterogeneous human\nbehaviors, limited evacuation data, and hurricane event uncertainties. Deployed\nin a real-world traffic prediction system in Louisiana, the model achieved an\n82% accuracy in predicting long-term congestion states over a 6-hour period\nduring a 7-day hurricane-impacted duration. The short-term speed prediction\nmodel exhibited Mean Absolute Percentage Errors (MAPEs) ranging from 7% to 13%\nacross evacuation horizons from 1 to 6 hours. Evaluation results underscore the\nmodel's potential to enhance traffic management during hurricane evacuations,\nand real-world deployment highlights its adaptability and scalability in\ndiverse hurricane scenarios within extensive transportation networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to IEEE ITS Magazine and currently under review",
    "pdf_url": "http://arxiv.org/pdf/2406.12119v1",
    "published_date": "2024-06-17 21:59:44 UTC",
    "updated_date": "2024-06-17 21:59:44 UTC"
  },
  {
    "arxiv_id": "2406.12114v1",
    "title": "Enhancing Text Classification through LLM-Driven Active Learning and Human Annotation",
    "authors": [
      "Hamidreza Rouzegar",
      "Masoud Makrehchi"
    ],
    "abstract": "In the context of text classification, the financial burden of annotation\nexercises for creating training data is a critical issue. Active learning\ntechniques, particularly those rooted in uncertainty sampling, offer a\ncost-effective solution by pinpointing the most instructive samples for manual\nannotation. Similarly, Large Language Models (LLMs) such as GPT-3.5 provide an\nalternative for automated annotation but come with concerns regarding their\nreliability. This study introduces a novel methodology that integrates human\nannotators and LLMs within an Active Learning framework. We conducted\nevaluations on three public datasets. IMDB for sentiment analysis, a Fake News\ndataset for authenticity discernment, and a Movie Genres dataset for\nmulti-label classification.The proposed framework integrates human annotation\nwith the output of LLMs, depending on the model uncertainty levels. This\nstrategy achieves an optimal balance between cost efficiency and classification\nperformance. The empirical results show a substantial decrease in the costs\nassociated with data annotation while either maintaining or improving model\naccuracy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Publisher: Association for Computational Linguistics URL:\n  https://aclanthology.org/2024.law-1.10",
    "pdf_url": "http://arxiv.org/pdf/2406.12114v1",
    "published_date": "2024-06-17 21:45:48 UTC",
    "updated_date": "2024-06-17 21:45:48 UTC"
  },
  {
    "arxiv_id": "2406.12108v2",
    "title": "Computing in the Life Sciences: From Early Algorithms to Modern AI",
    "authors": [
      "Samuel A. Donkor",
      "Matthew E. Walsh",
      "Alexander J. Titus"
    ],
    "abstract": "Computing in the life sciences has undergone a transformative evolution, from\nearly computational models in the 1950s to the applications of artificial\nintelligence (AI) and machine learning (ML) seen today. This paper highlights\nkey milestones and technological advancements through the historical\ndevelopment of computing in the life sciences. The discussion includes the\ninception of computational models for biological processes, the advent of\nbioinformatics tools, and the integration of AI/ML in modern life sciences\nresearch. Attention is given to AI-enabled tools used in the life sciences,\nsuch as scientific large language models and bio-AI tools, examining their\ncapabilities, limitations, and impact to biological risk. This paper seeks to\nclarify and establish essential terminology and concepts to ensure informed\ndecision-making and effective communication across disciplines.",
    "categories": [
      "q-bio.OT",
      "cs.AI"
    ],
    "primary_category": "q-bio.OT",
    "comment": "53 pages, 4 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.12108v2",
    "published_date": "2024-06-17 21:36:52 UTC",
    "updated_date": "2024-06-19 03:54:28 UTC"
  },
  {
    "arxiv_id": "2406.12104v1",
    "title": "End-to-end Text-to-SQL Generation within an Analytics Insight Engine",
    "authors": [
      "Karime Maamari",
      "Amine Mhedhbi"
    ],
    "abstract": "Recent advancements in Text-to-SQL have pushed database management systems\ntowards greater democratization of data access. Today's language models are at\nthe core of these advancements. They enable impressive Text-to-SQL generation\nas experienced in the development of Distyl AI's Analytics Insight Engine. Its\nearly deployment with enterprise customers has highlighted three core\nchallenges. First, data analysts expect support with authoring SQL queries of\nvery high complexity. Second, requests are ad-hoc and, as such, require low\nlatency. Finally, generation requires an understanding of domain-specific\nterminology and practices.\n  The design and implementation of our Text-to-SQL generation pipeline, powered\nby large language models, tackles these challenges. The core tenants of our\napproach rely on external knowledge that we extract in a pre-processing phase,\non retrieving the appropriate external knowledge at query generation time, and\non decomposing SQL query generation following a hierarchical CTE-based\nstructure. Finally, an adaptation framework leverages feedback to update the\nexternal knowledge, in turn improving query generation over time. We give an\noverview of our end-to-end approach and highlight the operators generating SQL\nduring inference.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12104v1",
    "published_date": "2024-06-17 21:33:01 UTC",
    "updated_date": "2024-06-17 21:33:01 UTC"
  },
  {
    "arxiv_id": "2406.12095v2",
    "title": "DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features",
    "authors": [
      "Letian Wang",
      "Seung Wook Kim",
      "Jiawei Yang",
      "Cunjun Yu",
      "Boris Ivanovic",
      "Steven L. Waslander",
      "Yue Wang",
      "Sanja Fidler",
      "Marco Pavone",
      "Peter Karkus"
    ],
    "abstract": "We propose DistillNeRF, a self-supervised learning framework addressing the\nchallenge of understanding 3D environments from limited 2D observations in\noutdoor autonomous driving scenes. Our method is a generalizable feedforward\nmodel that predicts a rich neural scene representation from sparse,\nsingle-frame multi-view camera inputs with limited view overlap, and is trained\nself-supervised with differentiable rendering to reconstruct RGB, depth, or\nfeature images. Our first insight is to exploit per-scene optimized Neural\nRadiance Fields (NeRFs) by generating dense depth and virtual camera targets\nfrom them, which helps our model to learn enhanced 3D geometry from sparse\nnon-overlapping image inputs. Second, to learn a semantically rich 3D\nrepresentation, we propose distilling features from pre-trained 2D foundation\nmodels, such as CLIP or DINOv2, thereby enabling various downstream tasks\nwithout the need for costly 3D human annotations. To leverage these two\ninsights, we introduce a novel model architecture with a two-stage\nlift-splat-shoot encoder and a parameterized sparse hierarchical voxel\nrepresentation. Experimental results on the NuScenes and Waymo NOTR datasets\ndemonstrate that DistillNeRF significantly outperforms existing comparable\nstate-of-the-art self-supervised methods for scene reconstruction, novel view\nsynthesis, and depth estimation; and it allows for competitive zero-shot 3D\nsemantic occupancy prediction, as well as open-world scene understanding\nthrough distilled foundation model features. Demos and code will be available\nat https://distillnerf.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by Advances in Neural Information Processing Systems\n  (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.12095v2",
    "published_date": "2024-06-17 21:15:13 UTC",
    "updated_date": "2024-10-31 03:23:39 UTC"
  },
  {
    "arxiv_id": "2406.12094v2",
    "title": "Who's asking? User personas and the mechanics of latent misalignment",
    "authors": [
      "Asma Ghandeharioun",
      "Ann Yuan",
      "Marius Guerard",
      "Emily Reif",
      "Michael A. Lepori",
      "Lucas Dixon"
    ],
    "abstract": "Despite investments in improving model safety, studies show that misaligned\ncapabilities remain latent in safety-tuned models. In this work, we shed light\non the mechanics of this phenomenon. First, we show that even when model\ngenerations are safe, harmful content can persist in hidden representations and\ncan be extracted by decoding from earlier layers. Then, we show that whether\nthe model divulges such content depends significantly on its perception of who\nit is talking to, which we refer to as user persona. In fact, we find\nmanipulating user persona to be even more effective for eliciting harmful\ncontent than direct attempts to control model refusal. We study both natural\nlanguage prompting and activation steering as control methods and show that\nactivation steering is significantly more effective at bypassing safety\nfilters. We investigate why certain personas break model safeguards and find\nthat they enable the model to form more charitable interpretations of otherwise\ndangerous queries. Finally, we show we can predict a persona's effect on\nrefusal given only the geometry of its steering vector.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12094v2",
    "published_date": "2024-06-17 21:15:12 UTC",
    "updated_date": "2024-08-13 14:02:13 UTC"
  },
  {
    "arxiv_id": "2406.12084v2",
    "title": "When Reasoning Meets Information Aggregation: A Case Study with Sports Narratives",
    "authors": [
      "Yebowen Hu",
      "Kaiqiang Song",
      "Sangwoo Cho",
      "Xiaoyang Wang",
      "Wenlin Yao",
      "Hassan Foroosh",
      "Dong Yu",
      "Fei Liu"
    ],
    "abstract": "Reasoning is most powerful when an LLM accurately aggregates relevant\ninformation. We examine the critical role of information aggregation in\nreasoning by requiring the LLM to analyze sports narratives. To succeed at this\ntask, an LLM must infer points from actions, identify related entities,\nattribute points accurately to players and teams, and compile key statistics to\ndraw conclusions. We conduct comprehensive experiments with real NBA basketball\ndata and present SportsGen, a new method to synthesize game narratives. By\nsynthesizing data, we can rigorously evaluate LLMs' reasoning capabilities\nunder complex scenarios with varying narrative lengths and density of\ninformation. Our findings show that most models, including GPT-4o, often fail\nto accurately aggregate basketball scores due to frequent scoring patterns.\nOpen-source models like Llama-3 further suffer from significant score\nhallucinations. Finally, the effectiveness of reasoning is influenced by\nnarrative complexity, information density, and domain-specific terms,\nhighlighting the challenges in analytical reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Main conference of EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12084v2",
    "published_date": "2024-06-17 20:49:35 UTC",
    "updated_date": "2024-10-04 04:25:07 UTC"
  },
  {
    "arxiv_id": "2406.12082v2",
    "title": "Uncertainty modeling for fine-tuned implicit functions",
    "authors": [
      "Anna Susmelj",
      "Mael Macuglia",
      "Nataša Tagasovska",
      "Reto Sutter",
      "Sebastiano Caprara",
      "Jean-Philippe Thiran",
      "Ender Konukoglu"
    ],
    "abstract": "Implicit functions such as Neural Radiance Fields (NeRFs), occupancy\nnetworks, and signed distance functions (SDFs) have become pivotal in computer\nvision for reconstructing detailed object shapes from sparse views. Achieving\noptimal performance with these models can be challenging due to the extreme\nsparsity of inputs and distribution shifts induced by data corruptions. To this\nend, large, noise-free synthetic datasets can serve as shape priors to help\nmodels fill in gaps, but the resulting reconstructions must be approached with\ncaution. Uncertainty estimation is crucial for assessing the quality of these\nreconstructions, particularly in identifying areas where the model is uncertain\nabout the parts it has inferred from the prior. In this paper, we introduce\nDropsembles, a novel method for uncertainty estimation in tuned implicit\nfunctions. We demonstrate the efficacy of our approach through a series of\nexperiments, starting with toy examples and progressing to a real-world\nscenario. Specifically, we train a Convolutional Occupancy Network on synthetic\nanatomical data and test it on low-resolution MRI segmentations of the lumbar\nspine. Our results show that Dropsembles achieve the accuracy and calibration\nlevels of deep ensembles but with significantly less computational cost.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12082v2",
    "published_date": "2024-06-17 20:46:18 UTC",
    "updated_date": "2025-03-21 15:06:41 UTC"
  },
  {
    "arxiv_id": "2406.12079v1",
    "title": "Multi-Dimensional Pruning: Joint Channel, Layer and Block Pruning with Latency Constraint",
    "authors": [
      "Xinglong Sun",
      "Barath Lakshmanan",
      "Maying Shen",
      "Shiyi Lan",
      "Jingde Chen",
      "Jose Alvarez"
    ],
    "abstract": "As we push the boundaries of performance in various vision tasks, the models\ngrow in size correspondingly. To keep up with this growth, we need very\naggressive pruning techniques for efficient inference and deployment on edge\ndevices. Existing pruning approaches are limited to channel pruning and\nstruggle with aggressive parameter reductions. In this paper, we propose a\nnovel multi-dimensional pruning framework that jointly optimizes pruning across\nchannels, layers, and blocks while adhering to latency constraints. We develop\na latency modeling technique that accurately captures model-wide latency\nvariations during pruning, which is crucial for achieving an optimal\nlatency-accuracy trade-offs at high pruning ratio. We reformulate pruning as a\nMixed-Integer Nonlinear Program (MINLP) to efficiently determine the optimal\npruned structure with only a single pass. Our extensive results demonstrate\nsubstantial improvements over previous methods, particularly at large pruning\nratios. In classification, our method significantly outperforms prior art HALP\nwith a Top-1 accuracy of 70.0(v.s. 68.6) and an FPS of 5262 im/s(v.s. 4101\nim/s). In 3D object detection, we establish a new state-of-the-art by pruning\nStreamPETR at a 45% pruning ratio, achieving higher FPS (37.3 vs. 31.7) and mAP\n(0.451 vs. 0.449) than the dense baseline.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2406.12079v1",
    "published_date": "2024-06-17 20:40:09 UTC",
    "updated_date": "2024-06-17 20:40:09 UTC"
  },
  {
    "arxiv_id": "2406.12078v1",
    "title": "Conformance Checking of Fuzzy Logs against Declarative Temporal Specifications",
    "authors": [
      "Ivan Donadello",
      "Paolo Felli",
      "Craig Innes",
      "Fabrizio Maria Maggi",
      "Marco Montali"
    ],
    "abstract": "Traditional conformance checking tasks assume that event data provide a\nfaithful and complete representation of the actual process executions. This\nassumption has been recently questioned: more and more often events are not\ntraced explicitly, but are instead indirectly obtained as the result of event\nrecognition pipelines, and thus inherently come with uncertainty. In this work,\ndifferently from the typical probabilistic interpretation of uncertainty, we\nconsider the relevant case where uncertainty refers to which activity is\nactually conducted, under a fuzzy semantics. In this novel setting, we consider\nthe problem of checking whether fuzzy event data conform with declarative\ntemporal rules specified as Declare patterns or, more generally, as formulae of\nlinear temporal logic over finite traces (LTLf). This requires to relax the\nassumption that at each instant only one activity is executed, and to\ncorrespondingly redefine boolean operators of the logic with a fuzzy semantics.\nSpecifically, we provide a threefold contribution. First, we define a fuzzy\ncounterpart of LTLf tailored to our purpose. Second, we cast conformance\nchecking over fuzzy logs as a verification problem in this logic. Third, we\nprovide a proof-of-concept, efficient implementation based on the PyTorch\nPython library, suited to check conformance of multiple fuzzy traces at once.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "68T27 (Primary) 68T27, 68T30, 68T37, 03B44 (Secondary)",
      "I.2.4; F.4.1"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12078v1",
    "published_date": "2024-06-17 20:38:57 UTC",
    "updated_date": "2024-06-17 20:38:57 UTC"
  },
  {
    "arxiv_id": "2406.12072v3",
    "title": "DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs",
    "authors": [
      "Jiasheng Zhang",
      "Jialin Chen",
      "Menglin Yang",
      "Aosong Feng",
      "Shuang Liang",
      "Jie Shao",
      "Rex Ying"
    ],
    "abstract": "Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world\nscenarios, where each node and edge are associated with text descriptions, and\nboth the graph structure and text descriptions evolve over time. Despite their\nbroad applicability, there is a notable scarcity of benchmark datasets tailored\nto DyTAGs, which hinders the potential advancement in many research fields. To\naddress this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB),\na collection of large-scale, time-evolving graphs from diverse domains, with\nnodes and edges enriched by dynamically changing text attributes and\ncategories. To facilitate the use of DTGB, we design standardized evaluation\nprocedures based on four real-world use cases: future link prediction,\ndestination node retrieval, edge classification, and textual relation\ngeneration. These tasks require models to understand both dynamic graph\nstructures and natural language, highlighting the unique challenges posed by\nDyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB,\nevaluating 7 popular dynamic graph learning algorithms and their variants of\nadapting to text attributes with LLM embeddings, along with 6 powerful large\nlanguage models (LLMs). Our results show the limitations of existing models in\nhandling DyTAGs. Our analysis also demonstrates the utility of DTGB in\ninvestigating the incorporation of structural and textual dynamics. The\nproposed DTGB fosters research on DyTAGs and their broad applications. It\noffers a comprehensive benchmark for evaluating and advancing models to handle\nthe interplay between dynamic graph structures and natural language. The\ndataset and source code are available at https://github.com/zjs123/DTGB.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "28 pages, 13 figures, camera-ready version for NeurIPS 2024 Datasets\n  and Benchmarks Track",
    "pdf_url": "http://arxiv.org/pdf/2406.12072v3",
    "published_date": "2024-06-17 20:16:12 UTC",
    "updated_date": "2024-11-04 18:38:35 UTC"
  },
  {
    "arxiv_id": "2406.12058v4",
    "title": "WellDunn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions",
    "authors": [
      "Seyedali Mohammadi",
      "Edward Raff",
      "Jinendra Malekar",
      "Vedant Palit",
      "Francis Ferraro",
      "Manas Gaur"
    ],
    "abstract": "Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WDs). We focus on\ntwo existing mental health and well-being datasets: (a) Multi-label\nClassification-based MultiWD, and (b) WellXplain for evaluating attention\nmechanism veracity against expert-labeled explanations. The labels are based on\nHalbert Dunn's theory of wellness, which gives grounding to our evaluation. We\nreveal four surprising results about LMs/LLMs: (1) Despite their human-like\ncapabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on\nWellXplain fails to deliver any remarkable improvements in performance or\nexplanations. (2) Re-examining LMs' predictions based on a confidence-oriented\nloss function reveals a significant performance drop. (3) Across all LMs/LLMs,\nthe alignment between attention and explanations remains low, with LLMs scoring\na dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific\nknowledge and undervalue explanations, causing these discrepancies. This study\nhighlights the need for further research into their consistency and\nexplanations in mental health and well-being.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in BlackboxNLP @ EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12058v4",
    "published_date": "2024-06-17 19:50:40 UTC",
    "updated_date": "2024-10-07 14:08:13 UTC"
  },
  {
    "arxiv_id": "2406.12052v2",
    "title": "UniGLM: Training One Unified Language Model for Text-Attributed Graph Embedding",
    "authors": [
      "Yi Fang",
      "Dongzhe Fan",
      "Sirui Ding",
      "Ninghao Liu",
      "Qiaoyu Tan"
    ],
    "abstract": "Representation learning on text-attributed graphs (TAGs), where nodes are\nrepresented by textual descriptions, is crucial for textual and relational\nknowledge systems and recommendation systems. Currently, state-of-the-art\nembedding methods for TAGs primarily focus on fine-tuning language models\n(e.g., BERT) using structure-aware training signals. While effective, these\nmethods are tailored for individual TAG and cannot generalize across various\ngraph scenarios. Given the shared textual space, leveraging multiple TAGs for\njoint fine-tuning, aligning text and graph structure from different aspects,\nwould be more beneficial. Motivated by this, we introduce a novel Unified Graph\nLanguage Model (UniGLM) framework, the first graph embedding model that\ngeneralizes well to both in-domain and cross-domain TAGs. Specifically, UniGLM\nis trained over multiple TAGs with different domains and scales using\nself-supervised contrastive learning. UniGLM includes an adaptive positive\nsample selection technique for identifying structurally similar nodes and a\nlazy contrastive module that is devised to accelerate training by minimizing\nrepetitive encoding calculations. Extensive empirical results across 9\nbenchmark TAGs demonstrate UniGLM's efficacy against leading embedding\nbaselines in terms of generalization (various downstream tasks and backbones)\nand transfer learning (in and out of domain scenarios). The code is available\nat https://github.com/NYUSHCS/UniGLM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12052v2",
    "published_date": "2024-06-17 19:45:21 UTC",
    "updated_date": "2024-12-23 08:30:47 UTC"
  },
  {
    "arxiv_id": "2406.12048v1",
    "title": "MEDeA: Multi-view Efficient Depth Adjustment",
    "authors": [
      "Mikhail Artemyev",
      "Anna Vorontsova",
      "Anna Sokolova",
      "Alexander Limonov"
    ],
    "abstract": "The majority of modern single-view depth estimation methods predict relative\ndepth and thus cannot be directly applied in many real-world scenarios, despite\nimpressive performance in the benchmarks. Moreover, single-view approaches\ncannot guarantee consistency across a sequence of frames. Consistency is\ntypically addressed with test-time optimization of discrepancy across views;\nhowever, it takes hours to process a single scene. In this paper, we present\nMEDeA, an efficient multi-view test-time depth adjustment method, that is an\norder of magnitude faster than existing test-time approaches. Given RGB frames\nwith camera parameters, MEDeA predicts initial depth maps, adjusts them by\noptimizing local scaling coefficients, and outputs temporally-consistent depth\nmaps. Contrary to test-time methods requiring normals, optical flow, or\nsemantics estimation, MEDeA produces high-quality predictions with a depth\nestimation network solely. Our method sets a new state-of-the-art on TUM RGB-D,\n7Scenes, and ScanNet benchmarks and successfully handles smartphone-captured\ndata from ARKitScenes dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12048v1",
    "published_date": "2024-06-17 19:39:13 UTC",
    "updated_date": "2024-06-17 19:39:13 UTC"
  },
  {
    "arxiv_id": "2406.12045v1",
    "title": "$τ$-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains",
    "authors": [
      "Shunyu Yao",
      "Noah Shinn",
      "Pedram Razavi",
      "Karthik Narasimhan"
    ],
    "abstract": "Existing benchmarks do not test language agents on their interaction with\nhuman users or ability to follow domain-specific rules, both of which are vital\nfor deploying them in real world applications. We propose $\\tau$-bench, a\nbenchmark emulating dynamic conversations between a user (simulated by language\nmodels) and a language agent provided with domain-specific API tools and policy\nguidelines. We employ an efficient and faithful evaluation process that\ncompares the database state at the end of a conversation with the annotated\ngoal state. We also propose a new metric (pass^k) to evaluate the reliability\nof agent behavior over multiple trials. Our experiments show that even\nstate-of-the-art function calling agents (like gpt-4o) succeed on <50% of the\ntasks, and are quite inconsistent (pass^8 <25% in retail). Our findings point\nto the need for methods that can improve the ability of agents to act\nconsistently and follow rules reliably.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12045v1",
    "published_date": "2024-06-17 19:33:08 UTC",
    "updated_date": "2024-06-17 19:33:08 UTC"
  },
  {
    "arxiv_id": "2406.12043v2",
    "title": "Grade Score: Quantifying LLM Performance in Option Selection",
    "authors": [
      "Dmitri Iourovitski"
    ],
    "abstract": "This study introduces the \"Grade Score\", a novel metric designed to evaluate\nthe consistency and fairness of Large Language Models (LLMs) when used as\nmultiple-choice judges with respect to order bias and choice consistency. The\nGrade Score combines Entropy, which measures order bias, and Mode Frequency,\nwhich assesses choice stability, offering insights into LLMs' reliability and\nimpartiality. The study explores techniques such as prompt engineering and\noption sampling strategies to optimize the Grade Score, demonstrating their\neffectiveness in enhancing LLMs' performance. Results showcase varying\nperformances among LLMs with respect to prompts and highlight the positive\nimpact of including irrelevant options. The study also identifies an emergent\nbehavior in instruction-following models, where they adapt to instructions\ntargeting specific biases, demonstrating their adaptability. The Grade Score\nfacilitates comparisons between LLMs and encourages ongoing research towards\noptimizing their decision-making processes, with potential implications for\nimproving their reliability and fairness in various applications. All code is\navailable on GitHub https://github.com/IoDmitri/GradeLab",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12043v2",
    "published_date": "2024-06-17 19:29:39 UTC",
    "updated_date": "2024-06-20 21:58:07 UTC"
  },
  {
    "arxiv_id": "2406.12038v2",
    "title": "Soft Prompting for Unlearning in Large Language Models",
    "authors": [
      "Karuna Bhaila",
      "Minh-Hao Van",
      "Xintao Wu"
    ],
    "abstract": "The widespread popularity of Large Language Models (LLMs), partly due to\ntheir unique ability to perform in-context learning, has also brought to light\nthe importance of ethical and safety considerations when deploying these\npre-trained models. In this work, we focus on investigating machine unlearning\nfor LLMs motivated by data protection regulations. In contrast to the growing\nliterature on fine-tuning methods to achieve unlearning, we focus on a\ncomparatively lightweight alternative called soft prompting to realize the\nunlearning of a subset of training data. With losses designed to enforce\nforgetting as well as utility preservation, our framework \\textbf{S}oft\n\\textbf{P}rompting for \\textbf{U}n\\textbf{l}earning (SPUL) learns prompt tokens\nthat can be appended to an arbitrary query to induce unlearning of specific\nexamples at inference time without updating LLM parameters. We conduct a\nrigorous evaluation of the proposed method and our results indicate that SPUL\ncan significantly improve the trade-off between utility and forgetting in the\ncontext of text classification and question answering with LLMs. We further\nvalidate our method using multiple LLMs to highlight the scalability of our\nframework and provide detailed insights into the choice of hyperparameters and\nthe influence of the size of unlearning data. Our implementation is available\nat \\url{https://github.com/karuna-bhaila/llm_unlearning}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12038v2",
    "published_date": "2024-06-17 19:11:40 UTC",
    "updated_date": "2024-08-05 21:48:22 UTC"
  },
  {
    "arxiv_id": "2406.12036v4",
    "title": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations",
    "authors": [
      "Nikhil Khandekar",
      "Qiao Jin",
      "Guangzhi Xiong",
      "Soren Dunn",
      "Serina S Applebaum",
      "Zain Anwar",
      "Maame Sarfo-Gyamfi",
      "Conrad W Safranek",
      "Abid A Anwar",
      "Andrew Zhang",
      "Aidan Gilson",
      "Maxwell B Singer",
      "Amisha Dave",
      "Andrew Taylor",
      "Aidong Zhang",
      "Qingyu Chen",
      "Zhiyong Lu"
    ],
    "abstract": "As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators\nthat follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each\ninstance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for\nclinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,\nencouraging future improvements of LLMs for various clinical calculation tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Github link: https://github.com/ncbi-nlp/MedCalc-Bench HuggingFace\n  link: https://huggingface.co/datasets/nsk7153/MedCalc-Bench",
    "pdf_url": "http://arxiv.org/pdf/2406.12036v4",
    "published_date": "2024-06-17 19:07:21 UTC",
    "updated_date": "2024-06-30 15:12:10 UTC"
  },
  {
    "arxiv_id": "2406.12035v1",
    "title": "Socially Interactive Agents for Robotic Neurorehabilitation Training: Conceptualization and Proof-of-concept Study",
    "authors": [
      "Rhythm Arora",
      "Pooja Prajod",
      "Matteo Lavit Nicora",
      "Daniele Panzeri",
      "Giovanni Tauro",
      "Rocco Vertechy",
      "Matteo Malosio",
      "Elisabeth André",
      "Patrick Gebhard"
    ],
    "abstract": "Individuals with diverse motor abilities often benefit from intensive and\nspecialized rehabilitation therapies aimed at enhancing their functional\nrecovery. Nevertheless, the challenge lies in the restricted availability of\nneurorehabilitation professionals, hindering the effective delivery of the\nnecessary level of care. Robotic devices hold great potential in reducing the\ndependence on medical personnel during therapy but, at the same time, they\ngenerally lack the crucial human interaction and motivation that traditional\nin-person sessions provide. To bridge this gap, we introduce an AI-based system\naimed at delivering personalized, out-of-hospital assistance during\nneurorehabilitation training. This system includes a rehabilitation training\ndevice, affective signal classification models, training exercises, and a\nsocially interactive agent as the user interface. With the assistance of a\nprofessional, the envisioned system is designed to be tailored to accommodate\nthe unique rehabilitation requirements of an individual patient. Conceptually,\nafter a preliminary setup and instruction phase, the patient is equipped to\ncontinue their rehabilitation regimen autonomously in the comfort of their\nhome, facilitated by a socially interactive agent functioning as a virtual\ncoaching assistant. Our approach involves the integration of an interactive\nsocially-aware virtual agent into a neurorehabilitation robotic framework, with\nthe primary objective of recreating the social aspects inherent to in-person\nrehabilitation sessions. We also conducted a feasibility study to test the\nframework with healthy patients. The results of our preliminary investigation\nindicate that participants demonstrated a propensity to adapt to the system.\nNotably, the presence of the interactive agent during the proposed exercises\ndid not act as a source of distraction; instead, it positively impacted users'\nengagement.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12035v1",
    "published_date": "2024-06-17 19:07:05 UTC",
    "updated_date": "2024-06-17 19:07:05 UTC"
  },
  {
    "arxiv_id": "2406.12031v2",
    "title": "Large Scale Transfer Learning for Tabular Data via Language Modeling",
    "authors": [
      "Josh Gardner",
      "Juan C. Perdomo",
      "Ludwig Schmidt"
    ],
    "abstract": "Tabular data -- structured, heterogeneous, spreadsheet-style data with rows\nand columns -- is widely used in practice across many domains. However, while\nrecent foundation models have reduced the need for developing task-specific\ndatasets and predictors in domains such as language modeling and computer\nvision, this transfer learning paradigm has not had similar impact in the\ntabular domain. In this work, we seek to narrow this gap and present TabuLa-8B,\na language model for tabular prediction. We define a process for extracting a\nlarge, high-quality training dataset from the TabLib corpus, proposing methods\nfor tabular data filtering and quality control. Using the resulting dataset,\nwhich comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama\n3-8B large language model (LLM) for tabular data prediction (classification and\nbinned regression) using a novel packing and attention scheme for tabular\nprediction. Through evaluation across a test suite of 329 datasets, we find\nthat TabuLa-8B has zero-shot accuracy on unseen tables that is over 15\npercentage points (pp) higher than random guessing, a feat that is not possible\nwith existing state-of-the-art tabular prediction models (e.g. XGBoost,\nTabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the\ntarget datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN\nmodels that are explicitly trained on equal, or even up to 16x more data. We\nrelease our model, code, and data along with the publication of this paper.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 camera-ready updates",
    "pdf_url": "http://arxiv.org/pdf/2406.12031v2",
    "published_date": "2024-06-17 18:58:20 UTC",
    "updated_date": "2024-11-20 21:20:08 UTC"
  },
  {
    "arxiv_id": "2406.12030v3",
    "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model",
    "authors": [
      "Yongting Zhang",
      "Lu Chen",
      "Guodong Zheng",
      "Yifeng Gao",
      "Rui Zheng",
      "Jinlan Fu",
      "Zhenfei Yin",
      "Senjie Jin",
      "Yu Qiao",
      "Xuanjing Huang",
      "Feng Zhao",
      "Tao Gui",
      "Jing Shao"
    ],
    "abstract": "The emergence of Vision Language Models (VLMs) has brought unprecedented\nadvances in understanding multimodal information. The combination of textual\nand visual semantics in VLMs is highly complex and diverse, making the safety\nalignment of these models challenging. Furthermore, due to the limited study on\nthe safety alignment of VLMs, there is a lack of large-scale, high-quality\ndatasets. To address these limitations, we propose a Safety Preference\nAlignment dataset for Vision Language Models named SPA-VL. In terms of breadth,\nSPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and\ncontains 100,788 samples of the quadruple (question, image, chosen response,\nrejected response). In terms of depth, the responses are collected from 12\nopen-source (e.g., QwenVL) and closed-source (e.g., Gemini) VLMs to ensure\ndiversity. The construction of preference data is fully automated, and the\nexperimental results indicate that models trained with alignment techniques on\nthe SPA-VL dataset exhibit substantial improvements in harmlessness and\nhelpfulness while maintaining core capabilities. SPA-VL, as a large-scale,\nhigh-quality, and diverse dataset, represents a significant milestone in\nensuring that VLMs achieve both harmlessness and helpfulness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12030v3",
    "published_date": "2024-06-17 18:57:37 UTC",
    "updated_date": "2025-03-25 16:01:59 UTC"
  },
  {
    "arxiv_id": "2406.12022v1",
    "title": "Constructing Ancestral Recombination Graphs through Reinforcement Learning",
    "authors": [
      "Mélanie Raymond",
      "Marie-Hélène Descary",
      "Cédric Beaulac",
      "Fabrice Larribe"
    ],
    "abstract": "Over the years, many approaches have been proposed to build ancestral\nrecombination graphs (ARGs), graphs used to represent the genetic relationship\nbetween individuals. Among these methods, many rely on the assumption that the\nmost likely graph is among the shortest ones. In this paper, we propose a new\napproach to build short ARGs: Reinforcement Learning (RL). We exploit the\nsimilarities between finding the shortest path between a set of genetic\nsequences and their most recent common ancestor and finding the shortest path\nbetween the entrance and exit of a maze, a classic RL problem. In the maze\nproblem, the learner, called the agent, must learn the directions to take in\norder to escape as quickly as possible, whereas in our problem, the agent must\nlearn the actions to take between coalescence, mutation, and recombination in\norder to reach the most recent common ancestor as quickly as possible. Our\nresults show that RL can be used to build ARGs as short as those built with a\nheuristic algorithm optimized to build short ARGs, and sometimes even shorter.\nMoreover, our method allows to build a distribution of short ARGs for a given\nsample, and can also generalize learning to new samples not used during the\nlearning process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12022v1",
    "published_date": "2024-06-17 18:42:03 UTC",
    "updated_date": "2024-06-17 18:42:03 UTC"
  },
  {
    "arxiv_id": "2406.12020v1",
    "title": "When Box Meets Graph Neural Network in Tag-aware Recommendation",
    "authors": [
      "Fake Lin",
      "Ziwei Zhao",
      "Xi Zhu",
      "Da Zhang",
      "Shitian Shen",
      "Xueying Li",
      "Tong Xu",
      "Suojuan Zhang",
      "Enhong Chen"
    ],
    "abstract": "Last year has witnessed the re-flourishment of tag-aware recommender systems\nsupported by the LLM-enriched tags. Unfortunately, though large efforts have\nbeen made, current solutions may fail to describe the diversity and uncertainty\ninherent in user preferences with only tag-driven profiles. Recently, with the\ndevelopment of geometry-based techniques, e.g., box embedding, diversity of\nuser preferences now could be fully modeled as the range within a box in high\ndimension space. However, defect still exists as these approaches are incapable\nof capturing high-order neighbor signals, i.e., semantic-rich multi-hop\nrelations within the user-tag-item tripartite graph, which severely limits the\neffectiveness of user modeling. To deal with this challenge, in this paper, we\npropose a novel algorithm, called BoxGNN, to perform the message aggregation\nvia combination of logical operations, thereby incorporating high-order\nsignals. Specifically, we first embed users, items, and tags as hyper-boxes\nrather than simple points in the representation space, and define two logical\noperations to facilitate the subsequent process. Next, we perform the message\naggregation mechanism via the combination of logical operations, to obtain the\ncorresponding high-order box representations. Finally, we adopt a volume-based\nlearning objective with Gumbel smoothing techniques to refine the\nrepresentation of boxes. Extensive experiments on two publicly available\ndatasets and one LLM-enhanced e-commerce dataset have validated the superiority\nof BoxGNN compared with various state-of-the-art baselines. The code is\nreleased online",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12020v1",
    "published_date": "2024-06-17 18:35:53 UTC",
    "updated_date": "2024-06-17 18:35:53 UTC"
  },
  {
    "arxiv_id": "2406.12000v2",
    "title": "Look Further Ahead: Testing the Limits of GPT-4 in Path Planning",
    "authors": [
      "Mohamed Aghzal",
      "Erion Plaku",
      "Ziyu Yao"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive capabilities across a wide\nvariety of tasks. However, they still face challenges with long-horizon\nplanning. To study this, we propose path planning tasks as a platform to\nevaluate LLMs' ability to navigate long trajectories under geometric\nconstraints. Our proposed benchmark systematically tests path-planning skills\nin complex settings. Using this, we examined GPT-4's planning abilities using\nvarious task representations and prompting approaches. We found that framing\nprompts as Python code and decomposing long trajectory tasks improve GPT-4's\npath planning effectiveness. However, while these approaches show some promise\ntoward improving the planning ability of the model, they do not obtain optimal\npaths and fail at generalizing over extended horizons.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the 2024 IEEE 20th International Conference on Automation\n  Science and Engineering",
    "pdf_url": "http://arxiv.org/pdf/2406.12000v2",
    "published_date": "2024-06-17 18:12:56 UTC",
    "updated_date": "2024-06-20 19:53:52 UTC"
  },
  {
    "arxiv_id": "2406.11988v1",
    "title": "Decomposed evaluations of geographic disparities in text-to-image models",
    "authors": [
      "Abhishek Sureddy",
      "Dishant Padalia",
      "Nandhinee Periyakaruppa",
      "Oindrila Saha",
      "Adina Williams",
      "Adriana Romero-Soriano",
      "Megan Richards",
      "Polina Kirichenko",
      "Melissa Hall"
    ],
    "abstract": "Recent work has identified substantial disparities in generated images of\ndifferent geographic regions, including stereotypical depictions of everyday\nobjects like houses and cars. However, existing measures for these disparities\nhave been limited to either human evaluations, which are time-consuming and\ncostly, or automatic metrics evaluating full images, which are unable to\nattribute these disparities to specific parts of the generated images. In this\nwork, we introduce a new set of metrics, Decomposed Indicators of Disparities\nin Image Generation (Decomposed-DIG), that allows us to separately measure\ngeographic disparities in the depiction of objects and backgrounds in generated\nimages. Using Decomposed-DIG, we audit a widely used latent diffusion model and\nfind that generated images depict objects with better realism than backgrounds\nand that backgrounds in generated images tend to contain larger regional\ndisparities than objects. We use Decomposed-DIG to pinpoint specific examples\nof disparities, such as stereotypical background generation in Africa,\nstruggling to generate modern vehicles in Africa, and unrealistically placing\nsome objects in outdoor settings. Informed by our metric, we use a new\nprompting structure that enables a 52% worst-region improvement and a 20%\naverage improvement in generated background diversity.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11988v1",
    "published_date": "2024-06-17 18:04:23 UTC",
    "updated_date": "2024-06-17 18:04:23 UTC"
  },
  {
    "arxiv_id": "2406.11984v1",
    "title": "Online Pareto-Optimal Decision-Making for Complex Tasks using Active Inference",
    "authors": [
      "Peter Amorese",
      "Shohei Wakayama",
      "Nisar Ahmed",
      "Morteza Lahijanian"
    ],
    "abstract": "When a robot autonomously performs a complex task, it frequently must balance\ncompeting objectives while maintaining safety. This becomes more difficult in\nuncertain environments with stochastic outcomes. Enhancing transparency in the\nrobot's behavior and aligning with user preferences are also crucial. This\npaper introduces a novel framework for multi-objective reinforcement learning\nthat ensures safe task execution, optimizes trade-offs between objectives, and\nadheres to user preferences. The framework has two main layers: a\nmulti-objective task planner and a high-level selector. The planning layer\ngenerates a set of optimal trade-off plans that guarantee satisfaction of a\ntemporal logic task. The selector uses active inference to decide which\ngenerated plan best complies with user preferences and aids learning. Operating\niteratively, the framework updates a parameterized learning model based on\ncollected data. Case studies and benchmarks on both manipulation and mobile\nrobots show that our framework outperforms other methods and (i) learns\nmultiple optimal trade-offs, (ii) adheres to a user preference, and (iii)\nallows the user to adjust the balance between (i) and (ii).",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "17 pages, 10 figures, submitted to IEEE Transactions on Robotics\n  journal",
    "pdf_url": "http://arxiv.org/pdf/2406.11984v1",
    "published_date": "2024-06-17 18:03:45 UTC",
    "updated_date": "2024-06-17 18:03:45 UTC"
  },
  {
    "arxiv_id": "2406.11980v1",
    "title": "Prompt Design Matters for Computational Social Science Tasks but in Unpredictable Ways",
    "authors": [
      "Shubham Atreja",
      "Joshua Ashkinaze",
      "Lingyao Li",
      "Julia Mendelsohn",
      "Libby Hemphill"
    ],
    "abstract": "Manually annotating data for computational social science tasks can be\ncostly, time-consuming, and emotionally draining. While recent work suggests\nthat LLMs can perform such annotation tasks in zero-shot settings, little is\nknown about how prompt design impacts LLMs' compliance and accuracy. We conduct\na large-scale multi-prompt experiment to test how model selection (ChatGPT,\nPaLM2, and Falcon7b) and prompt design features (definition inclusion, output\ntype, explanation, and prompt length) impact the compliance and accuracy of\nLLM-generated annotations on four CSS tasks (toxicity, sentiment, rumor stance,\nand news frames). Our results show that LLM compliance and accuracy are highly\nprompt-dependent. For instance, prompting for numerical scores instead of\nlabels reduces all LLMs' compliance and accuracy. The overall best prompting\nsetup is task-dependent, and minor prompt changes can cause large changes in\nthe distribution of generated labels. By showing that prompt design\nsignificantly impacts the quality and distribution of LLM-generated\nannotations, this work serves as both a warning and practical guide for\nresearchers and practitioners.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2406.11980v1",
    "published_date": "2024-06-17 18:01:43 UTC",
    "updated_date": "2024-06-17 18:01:43 UTC"
  },
  {
    "arxiv_id": "2406.11978v1",
    "title": "Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner",
    "authors": [
      "Kenneth Li",
      "Yiming Wang",
      "Fernanda Viégas",
      "Martin Wattenberg"
    ],
    "abstract": "We present an approach called Dialogue Action Tokens (DAT) that adapts\nlanguage model agents to plan goal-directed dialogues. The core idea is to\ntreat each utterance as an action, thereby converting dialogues into games\nwhere existing approaches such as reinforcement learning can be applied.\nSpecifically, we freeze a pretrained language model and train a small planner\nmodel that predicts a continuous action vector, used for controlled generation\nin each round. This design avoids the problem of language degradation under\nreward optimization. When evaluated on the Sotopia platform for social\nsimulations, the DAT-steered LLaMA model surpasses GPT-4's performance. We also\napply DAT to steer an attacker language model in a novel multi-turn red-teaming\nsetting, revealing a potential new attack surface.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code: https://github.com/likenneth/dialogue_action_token",
    "pdf_url": "http://arxiv.org/pdf/2406.11978v1",
    "published_date": "2024-06-17 18:01:32 UTC",
    "updated_date": "2024-06-17 18:01:32 UTC"
  },
  {
    "arxiv_id": "2406.11839v2",
    "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models",
    "authors": [
      "Fei Wang",
      "Wenxuan Zhou",
      "James Y. Huang",
      "Nan Xu",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ],
    "abstract": "Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/mDPO",
    "pdf_url": "http://arxiv.org/pdf/2406.11839v2",
    "published_date": "2024-06-17 17:59:58 UTC",
    "updated_date": "2024-10-07 17:59:42 UTC"
  },
  {
    "arxiv_id": "2406.11833v2",
    "title": "MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs",
    "authors": [
      "Ziyu Liu",
      "Tao Chu",
      "Yuhang Zang",
      "Xilin Wei",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Zijian Liang",
      "Yuanjun Xiong",
      "Yu Qiao",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "abstract": "Generating natural and meaningful responses to communicate with multi-modal\nhuman inputs is a fundamental capability of Large Vision-Language\nModels(LVLMs). While current open-source LVLMs demonstrate promising\nperformance in simplified scenarios such as single-turn single-image input,\nthey fall short in real-world conversation scenarios such as following\ninstructions in a long context history with multi-turn and multi-images.\nExisting LVLM benchmarks primarily focus on single-choice questions or\nshort-form responses, which do not adequately assess the capabilities of LVLMs\nin real-world human-AI interaction applications. Therefore, we introduce MMDU,\na comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning\ndataset, designed to evaluate and improve LVLMs' abilities in multi-turn and\nmulti-image conversations. We employ the clustering algorithm to ffnd the\nrelevant images and textual descriptions from the open-source Wikipedia and\nconstruct the question-answer pairs by human annotators with the assistance of\nthe GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and\n27 turns, which is at least 5x longer than previous benchmarks and poses\nchallenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs\nusing MMDU reveals that open-source LVLMs lag behind closed-source counterparts\ndue to limited conversational instruction tuning data. We demonstrate that\nffne-tuning open-source LVLMs on MMDU-45k signiffcantly address this gap,\ngenerating longer and more accurate conversations, and improving scores on MMDU\nand existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA:+1.2%). Our\ncontributions pave the way for bridging the gap between current LVLM models and\nreal-world application demands. This project is available at\nhttps://github.com/Liuziyu77/MMDU.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This project is available at https://github.com/Liuziyu77/MMDU",
    "pdf_url": "http://arxiv.org/pdf/2406.11833v2",
    "published_date": "2024-06-17 17:59:47 UTC",
    "updated_date": "2024-10-29 12:34:11 UTC"
  },
  {
    "arxiv_id": "2406.11830v1",
    "title": "Language Modeling with Editable External Knowledge",
    "authors": [
      "Belinda Z. Li",
      "Emmy Liu",
      "Alexis Ross",
      "Abbas Zeitoun",
      "Graham Neubig",
      "Jacob Andreas"
    ],
    "abstract": "When the world changes, so does the text that humans write about it. How do\nwe build language models that can be easily updated to reflect these changes?\nOne popular approach is retrieval-augmented generation, in which new documents\nare inserted into a knowledge base and retrieved during prediction for\ndownstream tasks. Most prior work on these systems have focused on improving\nbehavior during prediction through better retrieval or reasoning. This paper\nintroduces ERASE, which instead improves model behavior when new documents are\nacquired, by incrementally deleting or rewriting other entries in the knowledge\nbase each time a document is added. In two new benchmark datasets evaluating\nmodels' ability to answer questions about a stream of news articles or\nconversations, ERASE improves accuracy relative to conventional\nretrieval-augmented generation by 7-13% (Mixtral-8x7B) and 6-10% (Llama-3-8B)\nabsolute. Code and data are available at https://github.com/belindal/ERASE",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11830v1",
    "published_date": "2024-06-17 17:59:35 UTC",
    "updated_date": "2024-06-17 17:59:35 UTC"
  },
  {
    "arxiv_id": "2406.11827v2",
    "title": "WPO: Enhancing RLHF with Weighted Preference Optimization",
    "authors": [
      "Wenxuan Zhou",
      "Ravi Agrawal",
      "Shujian Zhang",
      "Sathish Reddy Indurthi",
      "Sanqiang Zhao",
      "Kaiqiang Song",
      "Silei Xu",
      "Chenguang Zhu"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) is a promising solution to\nalign large language models (LLMs) more closely with human values. Off-policy\npreference optimization, where the preference data is obtained from other\nmodels, is widely adopted due to its cost efficiency and scalability. However,\noff-policy preference optimization often suffers from a distributional gap\nbetween the policy used for data collection and the target policy, leading to\nsuboptimal optimization. In this paper, we propose a novel strategy to mitigate\nthis problem by simulating on-policy learning with off-policy preference data.\nOur Weighted Preference Optimization (WPO) method adapts off-policy data to\nresemble on-policy data more closely by reweighting preference pairs according\nto their probability under the current policy. This method not only addresses\nthe distributional gap problem but also enhances the optimization process\nwithout incurring additional costs. We validate our method on instruction\nfollowing benchmarks including Alpaca Eval 2 and MT-bench. WPO not only\noutperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2\nbut also establishes a remarkable length-controlled winning rate against\nGPT-4-turbo of 76.7% based on Gemma-2-9b-it. We release the code and models at\nhttps://github.com/wzhouad/WPO.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11827v2",
    "published_date": "2024-06-17 17:59:13 UTC",
    "updated_date": "2024-10-03 21:37:02 UTC"
  },
  {
    "arxiv_id": "2406.11818v1",
    "title": "Embodied Instruction Following in Unknown Environments",
    "authors": [
      "Zhenyu Wu",
      "Ziwei Wang",
      "Xiuwei Xu",
      "Jiwen Lu",
      "Haibin Yan"
    ],
    "abstract": "Enabling embodied agents to complete complex human instructions from natural\nlanguage is crucial to autonomous systems in household services. Conventional\nmethods can only accomplish human instructions in the known environment where\nall interactive objects are provided to the embodied agent, and directly\ndeploying the existing approaches for the unknown environment usually generates\ninfeasible plans that manipulate non-existing objects. On the contrary, we\npropose an embodied instruction following (EIF) method for complex tasks in the\nunknown environment, where the agent efficiently explores the unknown\nenvironment to generate feasible plans with existing objects to accomplish\nabstract instructions. Specifically, we build a hierarchical embodied\ninstruction following framework including the high-level task planner and the\nlow-level exploration controller with multimodal large language models. We then\nconstruct a semantic representation map of the scene with dynamic region\nattention to demonstrate the known visual clues, where the goal of task\nplanning and scene exploration is aligned for human instruction. For the task\nplanner, we generate the feasible step-by-step plans for human goal\naccomplishment according to the task completion process and the known visual\nclues. For the exploration controller, the optimal navigation or object\ninteraction policy is predicted based on the generated step-wise plans and the\nknown visual clues. The experimental results demonstrate that our method can\nachieve 45.09% success rate in 204 complex human instructions such as making\nbreakfast and tidying rooms in large house-level scenes.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Project Page: https://gary3410.github.io/eif_unknown/",
    "pdf_url": "http://arxiv.org/pdf/2406.11818v1",
    "published_date": "2024-06-17 17:55:40 UTC",
    "updated_date": "2024-06-17 17:55:40 UTC"
  },
  {
    "arxiv_id": "2406.11817v1",
    "title": "Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level",
    "authors": [
      "Jie Liu",
      "Zhanhui Zhou",
      "Jiaheng Liu",
      "Xingyuan Bu",
      "Chao Yang",
      "Han-Sen Zhong",
      "Wanli Ouyang"
    ],
    "abstract": "Direct Preference Optimization (DPO), a standard method for aligning language\nmodels with human preferences, is traditionally applied to offline preferences.\nRecent studies show that DPO benefits from iterative training with online\npreferences labeled by a trained reward model. In this work, we identify a\npitfall of vanilla iterative DPO - improved response quality can lead to\nincreased verbosity. To address this, we introduce iterative length-regularized\nDPO (iLR-DPO) to penalize response length. Our empirical results show that\niLR-DPO can enhance a 7B model to perform on par with GPT-4 without increasing\nverbosity. Specifically, our 7B model achieves a $50.5\\%$ length-controlled win\nrate against $\\texttt{GPT-4 Preview}$ on AlpacaEval 2.0, and excels across\nstandard benchmarks including MT-Bench, Arena-Hard and OpenLLM Leaderboard.\nThese results demonstrate the effectiveness of iterative DPO in aligning\nlanguage models with human feedback.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11817v1",
    "published_date": "2024-06-17 17:55:38 UTC",
    "updated_date": "2024-06-17 17:55:38 UTC"
  },
  {
    "arxiv_id": "2406.11811v2",
    "title": "RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content",
    "authors": [
      "Joao Monteiro",
      "Pierre-Andre Noel",
      "Etienne Marcotte",
      "Sai Rajeswar",
      "Valentina Zantedeschi",
      "David Vazquez",
      "Nicolas Chapados",
      "Christopher Pal",
      "Perouz Taslakian"
    ],
    "abstract": "Large Language Models (LLMs) are trained on vast amounts of data, most of\nwhich is automatically scraped from the internet. This data includes\nencyclopedic documents that harbor a vast amount of general knowledge (e.g.,\nWikipedia) but also potentially overlap with benchmark datasets used for\nevaluating LLMs. Consequently, evaluating models on test splits that might have\nleaked into the training set is prone to misleading conclusions. To foster\nsound evaluation of language models, we introduce a new test dataset named\nRepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a\ncollection of five splits of test sets, four of which have not been released to\nthe internet or exposed to LLM APIs prior to this publication. Each sample in\nRepLiQA comprises (1) a reference document crafted by a human annotator and\ndepicting an imaginary scenario (e.g., a news article) absent from the\ninternet; (2) a question about the document's topic; (3) a ground-truth answer\nderived directly from the information in the document; and (4) the paragraph\nextracted from the reference document containing the answer. As such, accurate\nanswers can only be generated if a model can find relevant content within the\nprovided document. We run a large-scale benchmark comprising several\nstate-of-the-art LLMs to uncover differences in performance across models of\nvarious types and sizes in a context-conditional language modeling setting.\nReleased splits of RepLiQA can be found here:\nhttps://huggingface.co/datasets/ServiceNow/repliqa.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11811v2",
    "published_date": "2024-06-17 17:52:54 UTC",
    "updated_date": "2024-11-05 16:47:43 UTC"
  },
  {
    "arxiv_id": "2406.11945v1",
    "title": "GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs with Large Language Models",
    "authors": [
      "Yi Fang",
      "Dongzhe Fan",
      "Daochen Zha",
      "Qiaoyu Tan"
    ],
    "abstract": "This work studies self-supervised graph learning for text-attributed graphs\n(TAGs) where nodes are represented by textual attributes. Unlike traditional\ngraph contrastive methods that perturb the numerical feature space and alter\nthe graph's topological structure, we aim to improve view generation through\nlanguage supervision. This is driven by the prevalence of textual attributes in\nreal applications, which complement graph structures with rich semantic\ninformation. However, this presents challenges because of two major reasons.\nFirst, text attributes often vary in length and quality, making it difficulty\nto perturb raw text descriptions without altering their original semantic\nmeanings. Second, although text attributes complement graph structures, they\nare not inherently well-aligned. To bridge the gap, we introduce GAugLLM, a\nnovel framework for augmenting TAGs. It leverages advanced large language\nmodels like Mistral to enhance self-supervised graph learning. Specifically, we\nintroduce a mixture-of-prompt-expert technique to generate augmented node\nfeatures. This approach adaptively maps multiple prompt experts, each of which\nmodifies raw text attributes using prompt engineering, into numerical feature\nspace. Additionally, we devise a collaborative edge modifier to leverage\nstructural and textual commonalities, enhancing edge augmentation by examining\nor building connections between nodes. Empirical results across five benchmark\ndatasets spanning various domains underscore our framework's ability to enhance\nthe performance of leading contrastive methods as a plug-in tool. Notably, we\nobserve that the augmented features and graph structure can also enhance the\nperformance of standard generative methods, as well as popular graph neural\nnetworks. The open-sourced implementation of our GAugLLM is available at\nGithub.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11945v1",
    "published_date": "2024-06-17 17:49:19 UTC",
    "updated_date": "2024-06-17 17:49:19 UTC"
  },
  {
    "arxiv_id": "2406.11943v1",
    "title": "Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph",
    "authors": [
      "Xiaoxiong Zhang",
      "Zhiwei Zeng",
      "Xin Zhou",
      "Dusit Niyato",
      "Zhiqi Shen"
    ],
    "abstract": "Federated Knowledge Graph Embedding (FKGE) has recently garnered considerable\ninterest due to its capacity to extract expressive representations from\ndistributed knowledge graphs, while concurrently safeguarding the privacy of\nindividual clients. Existing FKGE methods typically harness the arithmetic mean\nof entity embeddings from all clients as the global supplementary knowledge,\nand learn a replica of global consensus entities embeddings for each client.\nHowever, these methods usually neglect the inherent semantic disparities among\ndistinct clients. This oversight not only results in the globally shared\ncomplementary knowledge being inundated with too much noise when tailored to a\nspecific client, but also instigates a discrepancy between local and global\noptimization objectives. Consequently, the quality of the learned embeddings is\ncompromised. To address this, we propose Personalized Federated knowledge graph\nEmbedding with client-wise relation Graph (PFedEG), a novel approach that\nemploys a client-wise relation graph to learn personalized embeddings by\ndiscerning the semantic relevance of embeddings from other clients.\nSpecifically, PFedEG learns personalized supplementary knowledge for each\nclient by amalgamating entity embedding from its neighboring clients based on\ntheir \"affinity\" on the client-wise relation graph. Each client then conducts\npersonalized embedding learning based on its local triples and personalized\nsupplementary knowledge. We conduct extensive experiments on four benchmark\ndatasets to evaluate our method against state-of-the-art models and results\ndemonstrate the superiority of our method.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11943v1",
    "published_date": "2024-06-17 17:44:53 UTC",
    "updated_date": "2024-06-17 17:44:53 UTC"
  },
  {
    "arxiv_id": "2406.11786v1",
    "title": "A Brief Survey on Leveraging Large Scale Vision Models for Enhanced Robot Grasping",
    "authors": [
      "Abhi Kamboj",
      "Katherine Driggs-Campbell"
    ],
    "abstract": "Robotic grasping presents a difficult motor task in real-world scenarios,\nconstituting a major hurdle to the deployment of capable robots across various\nindustries. Notably, the scarcity of data makes grasping particularly\nchallenging for learned models. Recent advancements in computer vision have\nwitnessed a growth of successful unsupervised training mechanisms predicated on\nmassive amounts of data sourced from the Internet, and now nearly all prominent\nmodels leverage pretrained backbone networks. Against this backdrop, we begin\nto investigate the potential benefits of large-scale visual pretraining in\nenhancing robot grasping performance. This preliminary literature review sheds\nlight on critical challenges and delineates prospective directions for future\nresearch in visual pretraining for robotic manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "This report was written in February 2023, thus does not account for\n  any works since then",
    "pdf_url": "http://arxiv.org/pdf/2406.11786v1",
    "published_date": "2024-06-17 17:39:30 UTC",
    "updated_date": "2024-06-17 17:39:30 UTC"
  },
  {
    "arxiv_id": "2406.11785v3",
    "title": "CELL your Model: Contrastive Explanations for Large Language Models",
    "authors": [
      "Ronny Luss",
      "Erik Miehling",
      "Amit Dhurandhar"
    ],
    "abstract": "The advent of black-box deep neural network classification models has sparked\nthe need to explain their decisions. However, in the case of generative AI,\nsuch as large language models (LLMs), there is no class prediction to explain.\nRather, one can ask why an LLM output a particular response to a given prompt.\nIn this paper, we answer this question by proposing a contrastive explanation\nmethod requiring simply black-box/query access. Our explanations suggest that\nan LLM outputs a reply to a given prompt because if the prompt was slightly\nmodified, the LLM would have given a different response that is either less\npreferable or contradicts the original response. The key insight is that\ncontrastive explanations simply require a scoring function that has meaning to\nthe user and not necessarily a specific real valued quantity (viz. class\nlabel). To this end, we offer a novel budgeted algorithm, our main algorithmic\ncontribution, which intelligently creates contrasts based on such a scoring\nfunction while adhering to a query budget, necessary for longer contexts. We\nshow the efficacy of our method on important natural language tasks such as\nopen-text generation and chatbot conversations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11785v3",
    "published_date": "2024-06-17 17:39:10 UTC",
    "updated_date": "2025-02-17 18:37:13 UTC"
  },
  {
    "arxiv_id": "2406.11784v1",
    "title": "MDCR: A Dataset for Multi-Document Conditional Reasoning",
    "authors": [
      "Peter Baile Chen",
      "Yi Zhang",
      "Chunwei Liu",
      "Sejal Gupta",
      "Yoon Kim",
      "Michael Cafarella"
    ],
    "abstract": "The same real-life questions posed to different individuals may lead to\ndifferent answers based on their unique situations. For instance, whether a\nstudent is eligible for a scholarship depends on eligibility conditions, such\nas major or degree required. ConditionalQA was proposed to evaluate models'\ncapability of reading a document and answering eligibility questions,\nconsidering unmentioned conditions. However, it is limited to questions on\nsingle documents, neglecting harder cases that may require cross-document\nreasoning and optimization, for example, \"What is the maximum number of\nscholarships attainable?\" Such questions over multiple documents are not only\nmore challenging due to more context having to understand, but also because the\nmodel has to (1) explore all possible combinations of unmentioned conditions\nand (2) understand the relationship between conditions across documents, to\nreason about the optimal outcome. To evaluate models' capability of answering\nsuch questions, we propose a new dataset MDCR, which can reflect real-world\nchallenges and serve as a new test bed for complex conditional reasoning that\nrequires optimization. We evaluate this dataset using the most recent LLMs and\ndemonstrate their limitations in solving this task. We believe this dataset\nwill facilitate future research in answering optimization questions with\nunknown conditions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11784v1",
    "published_date": "2024-06-17 17:38:43 UTC",
    "updated_date": "2024-06-17 17:38:43 UTC"
  },
  {
    "arxiv_id": "2406.11780v1",
    "title": "Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs",
    "authors": [
      "Swanand Ravindra Kadhe",
      "Farhan Ahmed",
      "Dennis Wei",
      "Nathalie Baracaldo",
      "Inkit Padhi"
    ],
    "abstract": "Large language models (LLMs) have shown to pose social and ethical risks such\nas generating toxic language or facilitating malicious use of hazardous\nknowledge. Machine unlearning is a promising approach to improve LLM safety by\ndirectly removing harmful behaviors and knowledge. In this paper, we propose\n\"SPlit, UNlearn, MerGE\" (SPUNGE), a framework that can be used with any\nunlearning method to amplify its effectiveness. SPUNGE leverages data\nattributes during unlearning by splitting unlearning data into subsets based on\nspecific attribute values, unlearning each subset separately, and merging the\nunlearned models. We empirically demonstrate that SPUNGE significantly improves\nthe performance of two recent unlearning methods on state-of-the-art LLMs while\nmaintaining their general capabilities on standard academic benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11780v1",
    "published_date": "2024-06-17 17:35:52 UTC",
    "updated_date": "2024-06-17 17:35:52 UTC"
  },
  {
    "arxiv_id": "2406.11941v1",
    "title": "Crossfusor: A Cross-Attention Transformer Enhanced Conditional Diffusion Model for Car-Following Trajectory Prediction",
    "authors": [
      "Junwei You",
      "Haotian Shi",
      "Keshu Wu",
      "Keke Long",
      "Sicheng Fu",
      "Sikai Chen",
      "Bin Ran"
    ],
    "abstract": "Vehicle trajectory prediction is crucial for advancing autonomous driving and\nadvanced driver assistance systems (ADAS), enhancing road safety and traffic\nefficiency. While traditional methods have laid foundational work, modern deep\nlearning techniques, particularly transformer-based models and generative\napproaches, have significantly improved prediction accuracy by capturing\ncomplex and non-linear patterns in vehicle motion and traffic interactions.\nHowever, these models often overlook the detailed car-following behaviors and\ninter-vehicle interactions essential for real-world driving scenarios. This\nstudy introduces a Cross-Attention Transformer Enhanced Conditional Diffusion\nModel (Crossfusor) specifically designed for car-following trajectory\nprediction. Crossfusor integrates detailed inter-vehicular interactions and\ncar-following dynamics into a robust diffusion framework, improving both the\naccuracy and realism of predicted trajectories. The model leverages a novel\ntemporal feature encoding framework combining GRU, location-based attention\nmechanisms, and Fourier embedding to capture historical vehicle dynamics. It\nemploys noise scaled by these encoded historical features in the forward\ndiffusion process, and uses a cross-attention transformer to model intricate\ninter-vehicle dependencies in the reverse denoising process. Experimental\nresults on the NGSIM dataset demonstrate that Crossfusor outperforms\nstate-of-the-art models, particularly in long-term predictions, showcasing its\npotential for enhancing the predictive capabilities of autonomous driving\nsystems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11941v1",
    "published_date": "2024-06-17 17:35:47 UTC",
    "updated_date": "2024-06-17 17:35:47 UTC"
  },
  {
    "arxiv_id": "2406.11775v2",
    "title": "Task Me Anything",
    "authors": [
      "Jieyu Zhang",
      "Weikai Huang",
      "Zixian Ma",
      "Oscar Michel",
      "Dong He",
      "Tanmay Gupta",
      "Wei-Chiu Ma",
      "Ali Farhadi",
      "Aniruddha Kembhavi",
      "Ranjay Krishna"
    ],
    "abstract": "Benchmarks for large multimodal language models (MLMs) now serve to\nsimultaneously assess the general capabilities of models instead of evaluating\nfor a specific capability. As a result, when a developer wants to identify\nwhich models to use for their application, they are overwhelmed by the number\nof benchmarks and remain uncertain about which benchmark's results are most\nreflective of their specific use case. This paper introduces Task-Me-Anything,\na benchmark generation engine which produces a benchmark tailored to a user's\nneeds. Task-Me-Anything maintains an extendable taxonomy of visual assets and\ncan programmatically generate a vast number of task instances. Additionally, it\nalgorithmically addresses user queries regarding MLM performance efficiently\nwithin a computational budget. It contains 113K images, 10K videos, 2K 3D\nobject assets, over 365 object categories, 655 attributes, and 335\nrelationships. It can generate 750M image/video question-answering pairs, which\nfocus on evaluating MLM perceptual capabilities. Task-Me-Anything reveals\ncritical insights: open-source MLMs excel in object and attribute recognition\nbut lack spatial and temporal understanding; each model exhibits unique\nstrengths and weaknesses; larger models generally perform better, though\nexceptions exist; and GPT4o demonstrates challenges in recognizing\nrotating/moving objects and distinguishing colors.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024 Track on Datasets and Benchmarks. Website:\n  https://www.task-me-anything.org",
    "pdf_url": "http://arxiv.org/pdf/2406.11775v2",
    "published_date": "2024-06-17 17:32:42 UTC",
    "updated_date": "2025-01-27 06:25:11 UTC"
  },
  {
    "arxiv_id": "2406.11772v1",
    "title": "Deep Learning methodology for the identification of wood species using high-resolution macroscopic images",
    "authors": [
      "David Herrera-Poyatos",
      "Andrés Herrera-Poyatos",
      "Rosana Montes",
      "Paloma de Palacios",
      "Luis G. Esteban",
      "Alberto García Iruela",
      "Francisco García Fernández",
      "Francisco Herrera"
    ],
    "abstract": "Significant advancements in the field of wood species identification are\nneeded worldwide to support sustainable timber trade. In this work we\ncontribute to automate the identification of wood species via high-resolution\nmacroscopic images of timber. The main challenge of this problem is that\nfine-grained patterns in timber are crucial in order to accurately identify\nwood species, and these patterns are not properly learned by traditional\nconvolutional neural networks (CNNs) trained on low/medium resolution images.\n  We propose a Timber Deep Learning Identification with Patch-based Inference\nVoting methodology, abbreviated TDLI-PIV methodology. Our proposal exploits the\nconcept of patching and the availability of high-resolution macroscopic images\nof timber in order to overcome the inherent challenges that CNNs face in timber\nidentification. The TDLI-PIV methodology is able to capture fine-grained\npatterns in timber and, moreover, boosts robustness and prediction accuracy via\na collaborative voting inference process.\n  In this work we also introduce a new data set of marcroscopic images of\ntimber, called GOIMAI-Phase-I, which has been obtained using optical\nmagnification in order to capture fine-grained details, which contrasts to the\nother datasets that are publicly available. More concretely, images in\nGOIMAI-Phase-I are taken with a smartphone with a 24x magnifying lens attached\nto the camera. Our data set contains 2120 images of timber and covers 37\nlegally protected wood species.\n  Our experiments have assessed the performance of the TDLI-PIV methodology,\ninvolving the comparison with other methodologies available in the literature,\nexploration of data augmentation methods and the effect that the dataset size\nhas on the accuracy of TDLI-PIV.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.1; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages and 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11772v1",
    "published_date": "2024-06-17 17:31:57 UTC",
    "updated_date": "2024-06-17 17:31:57 UTC"
  },
  {
    "arxiv_id": "2407.09512v1",
    "title": "Design and evaluation of AI copilots -- case studies of retail copilot templates",
    "authors": [
      "Michal Furmakiewicz",
      "Chang Liu",
      "Angus Taylor",
      "Ilya Venger"
    ],
    "abstract": "Building a successful AI copilot requires a systematic approach. This paper\nis divided into two sections, covering the design and evaluation of a copilot\nrespectively. A case study of developing copilot templates for the retail\ndomain by Microsoft is used to illustrate the role and importance of each\naspect. The first section explores the key technical components of a copilot's\narchitecture, including the LLM, plugins for knowledge retrieval and actions,\norchestration, system prompts, and responsible AI guardrails. The second\nsection discusses testing and evaluation as a principled way to promote desired\noutcomes and manage unintended consequences when using AI in a business\ncontext. We discuss how to measure and improve its quality and safety, through\nthe lens of an end-to-end human-AI decision loop framework. By providing\ninsights into the anatomy of a copilot and the critical aspects of testing and\nevaluation, this paper provides concrete evidence of how good design and\nevaluation practices are essential for building effective, human-centered AI\nassistants.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "22 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09512v1",
    "published_date": "2024-06-17 17:31:33 UTC",
    "updated_date": "2024-06-17 17:31:33 UTC"
  },
  {
    "arxiv_id": "2406.11768v1",
    "title": "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities",
    "authors": [
      "Sreyan Ghosh",
      "Sonal Kumar",
      "Ashish Seth",
      "Chandra Kiran Reddy Evuru",
      "Utkarsh Tyagi",
      "S Sakshi",
      "Oriol Nieto",
      "Ramani Duraiswami",
      "Dinesh Manocha"
    ],
    "abstract": "Perceiving and understanding non-speech sounds and non-verbal speech is\nessential to making decisions that help us interact with our surroundings. In\nthis paper, we propose GAMA, a novel General-purpose Large Audio-Language Model\n(LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We\nbuild GAMA by integrating an LLM with multiple types of audio representations,\nincluding features from a custom Audio Q-Former, a multi-layer aggregator that\naggregates features from multiple layers of an audio encoder. We fine-tune GAMA\non a large-scale audio-language dataset, which augments it with audio\nunderstanding capabilities. Next, we propose CompA-R (Instruction-Tuning for\nComplex Audio Reasoning), a synthetically generated instruction-tuning (IT)\ndataset with instructions that require the model to perform complex reasoning\non the input audio. We instruction-tune GAMA with CompA-R to endow it with\ncomplex reasoning abilities, where we further add a soft prompt as input with\nhigh-level semantic evidence by leveraging event tags of the input audio.\nFinally, we also propose CompA-R-test, a human-labeled evaluation dataset for\nevaluating the capabilities of LALMs on open-ended audio question-answering\nthat requires complex reasoning. Through automated and expert human\nevaluations, we show that GAMA outperforms all other LALMs in literature on\ndiverse audio understanding tasks by margins of 1%-84%. Further, GAMA IT-ed on\nCompA-R proves to be superior in its complex reasoning and instruction\nfollowing capabilities.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Project Website: https://sreyan88.github.io/gamaaudio/",
    "pdf_url": "http://arxiv.org/pdf/2406.11768v1",
    "published_date": "2024-06-17 17:31:01 UTC",
    "updated_date": "2024-06-17 17:31:01 UTC"
  },
  {
    "arxiv_id": "2406.11939v2",
    "title": "From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline",
    "authors": [
      "Tianle Li",
      "Wei-Lin Chiang",
      "Evan Frick",
      "Lisa Dunlap",
      "Tianhao Wu",
      "Banghua Zhu",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "abstract": "The rapid evolution of Large Language Models (LLMs) has outpaced the\ndevelopment of model evaluation, highlighting the need for continuous curation\nof new, challenging benchmarks. However, manual curation of high-quality,\nhuman-aligned benchmarks is expensive and time-consuming. To address this, we\nintroduce BenchBuilder, an automated pipeline that leverages LLMs to curate\nhigh-quality, open-ended prompts from large, crowd-sourced datasets, enabling\ncontinuous benchmark updates without human in the loop. We apply BenchBuilder\nto datasets such as Chatbot Arena and WildChat-1M, extracting challenging\nprompts and utilizing LLM-as-a-Judge for automatic model evaluation. To\nvalidate benchmark quality, we propose new metrics to measure a benchmark's\nalignment with human preferences and ability to separate models. We release\nArena-Hard-Auto, a benchmark consisting 500 challenging prompts curated by\nBenchBuilder. Arena-Hard-Auto provides 3x higher separation of model\nperformances compared to MT-Bench and achieves 98.6% correlation with human\npreference rankings, all at a cost of $20. Our work sets a new framework for\nthe scalable curation of automated benchmarks from extensive data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11939v2",
    "published_date": "2024-06-17 17:26:10 UTC",
    "updated_date": "2024-10-14 18:11:58 UTC"
  },
  {
    "arxiv_id": "2406.11938v1",
    "title": "Tracking the perspectives of interacting language models",
    "authors": [
      "Hayden Helm",
      "Brandon Duderstadt",
      "Youngser Park",
      "Carey E. Priebe"
    ],
    "abstract": "Large language models (LLMs) are capable of producing high quality\ninformation at unprecedented rates. As these models continue to entrench\nthemselves in society, the content they produce will become increasingly\npervasive in databases that are, in turn, incorporated into the pre-training\ndata, fine-tuning data, retrieval data, etc. of other language models. In this\npaper we formalize the idea of a communication network of LLMs and introduce a\nmethod for representing the perspective of individual models within a\ncollection of LLMs. Given these tools we systematically study information\ndiffusion in the communication network of LLMs in various simulated settings.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11938v1",
    "published_date": "2024-06-17 17:20:16 UTC",
    "updated_date": "2024-06-17 17:20:16 UTC"
  },
  {
    "arxiv_id": "2406.11757v4",
    "title": "STAR: SocioTechnical Approach to Red Teaming Language Models",
    "authors": [
      "Laura Weidinger",
      "John Mellor",
      "Bernat Guillen Pegueroles",
      "Nahema Marchal",
      "Ravin Kumar",
      "Kristian Lum",
      "Canfer Akbulut",
      "Mark Diaz",
      "Stevie Bergman",
      "Mikel Rodriguez",
      "Verena Rieser",
      "William Isaac"
    ],
    "abstract": "This research introduces STAR, a sociotechnical framework that improves on\ncurrent best practices for red teaming safety of large language models. STAR\nmakes two key contributions: it enhances steerability by generating\nparameterised instructions for human red teamers, leading to improved coverage\nof the risk surface. Parameterised instructions also provide more detailed\ninsights into model failures at no increased cost. Second, STAR improves signal\nquality by matching demographics to assess harms for specific groups, resulting\nin more sensitive annotations. STAR further employs a novel step of arbitration\nto leverage diverse viewpoints and improve label reliability, treating\ndisagreement not as noise but as a valuable contribution to signal quality.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 5 figures, 5 pages appendix. * denotes equal contribution",
    "pdf_url": "http://arxiv.org/pdf/2406.11757v4",
    "published_date": "2024-06-17 17:16:45 UTC",
    "updated_date": "2024-10-23 16:41:45 UTC"
  },
  {
    "arxiv_id": "2406.11754v1",
    "title": "DustNet: skillful neural network predictions of Saharan dust",
    "authors": [
      "Trish E. Nowak",
      "Andy T. Augousti",
      "Benno I. Simmons",
      "Stefan Siegert"
    ],
    "abstract": "Suspended in the atmosphere are millions of tonnes of mineral dust which\ninteracts with weather and climate. Accurate representation of mineral dust in\nweather models is vital, yet remains challenging. Large scale weather models\nuse high power supercomputers and take hours to complete the forecast. Such\ncomputational burden allows them to only include monthly climatological means\nof mineral dust as input states inhibiting their forecasting accuracy. Here, we\nintroduce DustNet a simple, accurate and super fast forecasting model for\n24-hours ahead predictions of aerosol optical depth AOD. DustNet trains in less\nthan 8 minutes and creates predictions in 2 seconds on a desktop computer.\nCreated by DustNet predictions outperform the state-of-the-art physics-based\nmodel on coarse 1 x 1 degree resolution at 95% of grid locations when compared\nto ground truth satellite data. Our results show DustNet has a potential for\nfast and accurate AOD forecasting which could transform our understanding of\ndust impacts on weather patterns.",
    "categories": [
      "physics.geo-ph",
      "cs.AI",
      "physics.ao-ph",
      "physics.data-an",
      "86-06(Primary), 86A10(Secondary)",
      "J.2; I.2.1; I.2.7"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "34 pages, 9 figures, uses 2D CNN",
    "pdf_url": "http://arxiv.org/pdf/2406.11754v1",
    "published_date": "2024-06-17 17:15:30 UTC",
    "updated_date": "2024-06-17 17:15:30 UTC"
  },
  {
    "arxiv_id": "2407.16895v1",
    "title": "(Unfair) Norms in Fairness Research: A Meta-Analysis",
    "authors": [
      "Jennifer Chien",
      "A. Stevie Bergman",
      "Kevin R. McKee",
      "Nenad Tomasev",
      "Vinodkumar Prabhakaran",
      "Rida Qadri",
      "Nahema Marchal",
      "William Isaac"
    ],
    "abstract": "Algorithmic fairness has emerged as a critical concern in artificial\nintelligence (AI) research. However, the development of fair AI systems is not\nan objective process. Fairness is an inherently subjective concept, shaped by\nthe values, experiences, and identities of those involved in research and\ndevelopment. To better understand the norms and values embedded in current\nfairness research, we conduct a meta-analysis of algorithmic fairness papers\nfrom two leading conferences on AI fairness and ethics, AIES and FAccT,\ncovering a final sample of 139 papers over the period from 2018 to 2022. Our\ninvestigation reveals two concerning trends: first, a US-centric perspective\ndominates throughout fairness research; and second, fairness studies exhibit a\nwidespread reliance on binary codifications of human identity (e.g.,\n\"Black/White\", \"male/female\"). These findings highlight how current research\noften overlooks the complexities of identity and lived experiences, ultimately\nfailing to represent diverse global contexts when defining algorithmic bias and\nfairness. We discuss the limitations of these research design choices and offer\nrecommendations for fostering more inclusive and representative approaches to\nfairness in AI systems, urging a paradigm shift that embraces nuanced, global\nunderstandings of human identity and values.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16895v1",
    "published_date": "2024-06-17 17:14:47 UTC",
    "updated_date": "2024-06-17 17:14:47 UTC"
  },
  {
    "arxiv_id": "2406.11741v4",
    "title": "Transcendence: Generative Models Can Outperform The Experts That Train Them",
    "authors": [
      "Edwin Zhang",
      "Vincent Zhu",
      "Naomi Saphra",
      "Anat Kleiman",
      "Benjamin L. Edelman",
      "Milind Tambe",
      "Sham M. Kakade",
      "Eran Malach"
    ],
    "abstract": "Generative models are trained with the simple objective of imitating the\nconditional probability distribution induced by the data they are trained on.\nTherefore, when trained on data generated by humans, we may not expect the\nartificial model to outperform the humans on their original objectives. In this\nwork, we study the phenomenon of transcendence: when a generative model\nachieves capabilities that surpass the abilities of the experts generating its\ndata. We demonstrate transcendence by training an autoregressive transformer to\nplay chess from game transcripts, and show that the trained model can sometimes\nachieve better performance than all players in the dataset. We theoretically\nprove that transcendence can be enabled by low-temperature sampling, and\nrigorously assess this claim experimentally. Finally, we discuss other sources\nof transcendence, laying the groundwork for future investigation of this\nphenomenon in a broader setting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code, models, and data at https://transcendence.eddie.win",
    "pdf_url": "http://arxiv.org/pdf/2406.11741v4",
    "published_date": "2024-06-17 17:00:52 UTC",
    "updated_date": "2024-10-12 18:46:20 UTC"
  },
  {
    "arxiv_id": "2406.11740v2",
    "title": "Imagination Policy: Using Generative Point Cloud Models for Learning Manipulation Policies",
    "authors": [
      "Haojie Huang",
      "Karl Schmeckpeper",
      "Dian Wang",
      "Ondrej Biza",
      "Yaoyao Qian",
      "Haotian Liu",
      "Mingxi Jia",
      "Robert Platt",
      "Robin Walters"
    ],
    "abstract": "Humans can imagine goal states during planning and perform actions to match\nthose goals. In this work, we propose Imagination Policy, a novel multi-task\nkey-frame policy network for solving high-precision pick and place tasks.\nInstead of learning actions directly, Imagination Policy generates point clouds\nto imagine desired states which are then translated to actions using rigid\naction estimation. This transforms action inference into a local generative\ntask. We leverage pick and place symmetries underlying the tasks in the\ngeneration process and achieve extremely high sample efficiency and\ngeneralizability to unseen configurations. Finally, we demonstrate\nstate-of-the-art performance across various tasks on the RLbench benchmark\ncompared with several strong baselines and validate our approach on a real\nrobot.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11740v2",
    "published_date": "2024-06-17 17:00:41 UTC",
    "updated_date": "2024-11-30 17:40:26 UTC"
  },
  {
    "arxiv_id": "2406.11736v1",
    "title": "Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models",
    "authors": [
      "Fangzhi Xu",
      "Qiushi Sun",
      "Kanzhi Cheng",
      "Jun Liu",
      "Yu Qiao",
      "Zhiyong Wu"
    ],
    "abstract": "One of the primary driving forces contributing to the superior performance of\nLarge Language Models (LLMs) is the extensive availability of human-annotated\nnatural language data, which is used for alignment fine-tuning. This inspired\nresearchers to investigate self-training methods to mitigate the extensive\nreliance on human annotations. However, the current success of self-training\nhas been primarily observed in natural language scenarios, rather than in the\nincreasingly important neural-symbolic scenarios. To this end, we propose an\nenvironment-guided neural-symbolic self-training framework named ENVISIONS. It\naims to overcome two main challenges: (1) the scarcity of symbolic data, and\n(2) the limited proficiency of LLMs in processing symbolic language. Extensive\nevaluations conducted on three distinct domains demonstrate the effectiveness\nof our approach. Additionally, we have conducted a comprehensive analysis to\nuncover the factors contributing to ENVISIONS's success, thereby offering\nvaluable insights for future research in this area. Code will be available at\n\\url{https://github.com/xufangzhi/ENVISIONS}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11736v1",
    "published_date": "2024-06-17 16:52:56 UTC",
    "updated_date": "2024-06-17 16:52:56 UTC"
  },
  {
    "arxiv_id": "2406.11721v2",
    "title": "The Right Time Matters: Data Arrangement Affects Zero-Shot Generalization in Instruction Tuning",
    "authors": [
      "Bingxiang He",
      "Ning Ding",
      "Cheng Qian",
      "Jia Deng",
      "Ganqu Cui",
      "Lifan Yuan",
      "Haiwen Hong",
      "Huan-ang Gao",
      "Longtao Huang",
      "Hui Xue",
      "Huimin Chen",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Understanding alignment techniques begins with comprehending zero-shot\ngeneralization brought by instruction tuning, but little of the mechanism has\nbeen understood. Existing work has largely been confined to the task level,\nwithout considering that tasks are artificially defined and, to LLMs, merely\nconsist of tokens and representations. To bridge this gap, we investigate\nzero-shot generalization from the perspective of the data itself. We first\ndemonstrate that zero-shot generalization happens very early during instruction\ntuning, with loss serving as a stable indicator. Next, we investigate training\ndata arrangement through similarity and granularity perspectives, confirming\nthat the timing of exposure to certain training examples may greatly facilitate\ngeneralization on unseen tasks. Finally, we propose a more grounded training\ndata arrangement framework, Test-centric Multi-turn Arrangement, and show its\neffectiveness in promoting continual learning and further loss reduction. For\nthe first time, we show that zero-shot generalization during instruction tuning\nis a form of similarity-based generalization between training and test data at\nthe instance level. Our code is released at\nhttps://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11721v2",
    "published_date": "2024-06-17 16:40:21 UTC",
    "updated_date": "2025-04-07 14:21:36 UTC"
  },
  {
    "arxiv_id": "2406.11717v3",
    "title": "Refusal in Language Models Is Mediated by a Single Direction",
    "authors": [
      "Andy Arditi",
      "Oscar Obeso",
      "Aaquib Syed",
      "Daniel Paleka",
      "Nina Panickssery",
      "Wes Gurnee",
      "Neel Nanda"
    ],
    "abstract": "Conversational large language models are fine-tuned for both\ninstruction-following and safety, resulting in models that obey benign requests\nbut refuse harmful ones. While this refusal behavior is widespread across chat\nmodels, its underlying mechanisms remain poorly understood. In this work, we\nshow that refusal is mediated by a one-dimensional subspace, across 13 popular\nopen-source chat models up to 72B parameters in size. Specifically, for each\nmodel, we find a single direction such that erasing this direction from the\nmodel's residual stream activations prevents it from refusing harmful\ninstructions, while adding this direction elicits refusal on even harmless\ninstructions. Leveraging this insight, we propose a novel white-box jailbreak\nmethod that surgically disables refusal with minimal effect on other\ncapabilities. Finally, we mechanistically analyze how adversarial suffixes\nsuppress propagation of the refusal-mediating direction. Our findings\nunderscore the brittleness of current safety fine-tuning methods. More broadly,\nour work showcases how an understanding of model internals can be leveraged to\ndevelop practical methods for controlling model behavior.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11717v3",
    "published_date": "2024-06-17 16:36:12 UTC",
    "updated_date": "2024-10-30 18:57:07 UTC"
  },
  {
    "arxiv_id": "2406.11704v2",
    "title": "Nemotron-4 340B Technical Report",
    "authors": [
      "Nvidia",
      ":",
      "Bo Adler",
      "Niket Agarwal",
      "Ashwath Aithal",
      "Dong H. Anh",
      "Pallab Bhattacharya",
      "Annika Brundyn",
      "Jared Casper",
      "Bryan Catanzaro",
      "Sharon Clay",
      "Jonathan Cohen",
      "Sirshak Das",
      "Ayush Dattagupta",
      "Olivier Delalleau",
      "Leon Derczynski",
      "Yi Dong",
      "Daniel Egert",
      "Ellie Evans",
      "Aleksander Ficek",
      "Denys Fridman",
      "Shaona Ghosh",
      "Boris Ginsburg",
      "Igor Gitman",
      "Tomasz Grzegorzek",
      "Robert Hero",
      "Jining Huang",
      "Vibhu Jawa",
      "Joseph Jennings",
      "Aastha Jhunjhunwala",
      "John Kamalu",
      "Sadaf Khan",
      "Oleksii Kuchaiev",
      "Patrick LeGresley",
      "Hui Li",
      "Jiwei Liu",
      "Zihan Liu",
      "Eileen Long",
      "Ameya Sunil Mahabaleshwarkar",
      "Somshubra Majumdar",
      "James Maki",
      "Miguel Martinez",
      "Maer Rodrigues de Melo",
      "Ivan Moshkov",
      "Deepak Narayanan",
      "Sean Narenthiran",
      "Jesus Navarro",
      "Phong Nguyen",
      "Osvald Nitski",
      "Vahid Noroozi",
      "Guruprasad Nutheti",
      "Christopher Parisien",
      "Jupinder Parmar",
      "Mostofa Patwary",
      "Krzysztof Pawelec",
      "Wei Ping",
      "Shrimai Prabhumoye",
      "Rajarshi Roy",
      "Trisha Saar",
      "Vasanth Rao Naik Sabavat",
      "Sanjeev Satheesh",
      "Jane Polak Scowcroft",
      "Jason Sewall",
      "Pavel Shamis",
      "Gerald Shen",
      "Mohammad Shoeybi",
      "Dave Sizer",
      "Misha Smelyanskiy",
      "Felipe Soares",
      "Makesh Narsimhan Sreedhar",
      "Dan Su",
      "Sandeep Subramanian",
      "Shengyang Sun",
      "Shubham Toshniwal",
      "Hao Wang",
      "Zhilin Wang",
      "Jiaxuan You",
      "Jiaqi Zeng",
      "Jimmy Zhang",
      "Jing Zhang",
      "Vivienne Zhang",
      "Yian Zhang",
      "Chen Zhu"
    ],
    "abstract": "We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base,\nNemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open\naccess under the NVIDIA Open Model License Agreement, a permissive model\nlicense that allows distribution, modification, and use of the models and its\noutputs. These models perform competitively to open access models on a wide\nrange of evaluation benchmarks, and were sized to fit on a single DGX H100 with\n8 GPUs when deployed in FP8 precision. We believe that the community can\nbenefit from these models in various research studies and commercial\napplications, especially for generating synthetic data to train smaller\nlanguage models. Notably, over 98% of data used in our model alignment process\nis synthetically generated, showcasing the effectiveness of these models in\ngenerating synthetic data. To further support open research and facilitate\nmodel development, we are also open-sourcing the synthetic data generation\npipeline used in our model alignment process.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11704v2",
    "published_date": "2024-06-17 16:25:04 UTC",
    "updated_date": "2024-08-06 22:37:06 UTC"
  },
  {
    "arxiv_id": "2406.11695v2",
    "title": "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs",
    "authors": [
      "Krista Opsahl-Ong",
      "Michael J Ryan",
      "Josh Purtell",
      "David Broman",
      "Christopher Potts",
      "Matei Zaharia",
      "Omar Khattab"
    ],
    "abstract": "Language Model Programs, i.e. sophisticated pipelines of modular language\nmodel (LM) calls, are increasingly advancing NLP tasks, but they require\ncrafting prompts that are jointly effective for all modules. We study prompt\noptimization for LM programs, i.e. how to update these prompts to maximize a\ndownstream metric without access to module-level labels or gradients. To make\nthis tractable, we factorize our problem into optimizing the free-form\ninstructions and few-shot demonstrations of every module and introduce several\nstrategies to craft task-grounded instructions and navigate credit assignment\nacross modules. Our strategies include (i) program- and data-aware techniques\nfor proposing effective instructions, (ii) a stochastic mini-batch evaluation\nfunction for learning a surrogate model of our objective, and (iii) a\nmeta-optimization procedure in which we refine how LMs construct proposals over\ntime. Using these insights we develop MIPRO, a novel algorithm for optimizing\nLM programs. MIPRO outperforms baseline optimizers on five of seven diverse\nmulti-stage LM programs using a best-in-class open-source model (Llama-3-8B),\nby as high as 13% accuracy. We have released our new optimizers and benchmark\nin DSPy at http://dspy.ai",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024. Krista and Michael contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2406.11695v2",
    "published_date": "2024-06-17 16:12:03 UTC",
    "updated_date": "2024-10-06 17:34:26 UTC"
  },
  {
    "arxiv_id": "2406.11935v2",
    "title": "A Problem-Oriented Perspective and Anchor Verification for Code Optimization",
    "authors": [
      "Tong Ye",
      "Tengfei Ma",
      "Xuhong Zhang",
      "Hang Yu",
      "Jianwei Yin",
      "Wenhai Wang"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in solving\nvarious programming tasks, such as code generation. However, their potential\nfor code optimization, particularly in performance enhancement, remains largely\nunexplored. This paper investigates the capabilities of LLMs in optimizing code\nfor minimal execution time, addressing a critical gap in current research. The\nrecently proposed code optimization dataset constructs program optimization\npairs based on iterative submissions from the same programmer for the same\nproblem. However, this approach limits LLMs to local performance improvements,\nneglecting global algorithmic innovation. To overcome this limitation, we adopt\na completely different perspective by reconstructing the optimization pairs\ninto a problem-oriented approach. This allows for the integration of various\nideas from multiple programmers tackling the same problem. Experimental results\ndemonstrate that adapting LLMs to problem-oriented optimization pairs\nsignificantly enhances their optimization capabilities. Furthermore,\nrecognizing the inherent trade-offs in code optimization, we introduce an\nanchor verification mechanism to mitigate the \"optimization tax\". Ultimately,\nour approach elevates both the optimization ratio and speedup to new levels.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11935v2",
    "published_date": "2024-06-17 16:10:10 UTC",
    "updated_date": "2025-02-17 07:38:47 UTC"
  },
  {
    "arxiv_id": "2406.11686v2",
    "title": "The Role of Inherent Bellman Error in Offline Reinforcement Learning with Linear Function Approximation",
    "authors": [
      "Noah Golowich",
      "Ankur Moitra"
    ],
    "abstract": "In this paper, we study the offline RL problem with linear function\napproximation. Our main structural assumption is that the MDP has low inherent\nBellman error, which stipulates that linear value functions have linear Bellman\nbackups with respect to the greedy policy. This assumption is natural in that\nit is essentially the minimal assumption required for value iteration to\nsucceed. We give a computationally efficient algorithm which succeeds under a\nsingle-policy coverage condition on the dataset, namely which outputs a policy\nwhose value is at least that of any policy which is well-covered by the\ndataset. Even in the setting when the inherent Bellman error is 0 (termed\nlinear Bellman completeness), our algorithm yields the first known guarantee\nunder single-policy coverage.\n  In the setting of positive inherent Bellman error\n${\\varepsilon_{\\mathrm{BE}}} > 0$, we show that the suboptimality error of our\nalgorithm scales with $\\sqrt{\\varepsilon_{\\mathrm{BE}}}$. Furthermore, we prove\nthat the scaling of the suboptimality with $\\sqrt{\\varepsilon_{\\mathrm{BE}}}$\ncannot be improved for any algorithm. Our lower bound stands in contrast to\nmany other settings in reinforcement learning with misspecification, where one\ncan typically obtain performance that degrades linearly with the\nmisspecification error.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "RLC 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11686v2",
    "published_date": "2024-06-17 16:04:06 UTC",
    "updated_date": "2024-06-18 04:23:39 UTC"
  },
  {
    "arxiv_id": "2406.11934v1",
    "title": "Bridging Design Gaps: A Parametric Data Completion Approach With Graph Guided Diffusion Models",
    "authors": [
      "Rui Zhou",
      "Chenyang Yuan",
      "Frank Permenter",
      "Yanxia Zhang",
      "Nikos Arechiga",
      "Matt Klenk",
      "Faez Ahmed"
    ],
    "abstract": "This study introduces a generative imputation model leveraging graph\nattention networks and tabular diffusion models for completing missing\nparametric data in engineering designs. This model functions as an AI design\nco-pilot, providing multiple design options for incomplete designs, which we\ndemonstrate using the bicycle design CAD dataset. Through comparative\nevaluations, we demonstrate that our model significantly outperforms existing\nclassical methods, such as MissForest, hotDeck, PPCA, and tabular generative\nmethod TabCSDI in both the accuracy and diversity of imputation options.\nGenerative modeling also enables a broader exploration of design possibilities,\nthereby enhancing design decision-making by allowing engineers to explore a\nvariety of design completions. The graph model combines GNNs with the\nstructural information contained in assembly graphs, enabling the model to\nunderstand and predict the complex interdependencies between different design\nparameters. The graph model helps accurately capture and impute complex\nparametric interdependencies from an assembly graph, which is key for design\nproblems. By learning from an existing dataset of designs, the imputation\ncapability allows the model to act as an intelligent assistant that\nautocompletes CAD designs based on user-defined partial parametric design,\neffectively bridging the gap between ideation and realization. The proposed\nwork provides a pathway to not only facilitate informed design decisions but\nalso promote creative exploration in design.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "IDETC 2024 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2406.11934v1",
    "published_date": "2024-06-17 16:03:17 UTC",
    "updated_date": "2024-06-17 16:03:17 UTC"
  },
  {
    "arxiv_id": "2407.00067v1",
    "title": "Perceptron Collaborative Filtering",
    "authors": [
      "Arya Chakraborty"
    ],
    "abstract": "While multivariate logistic regression classifiers are a great way of\nimplementing collaborative filtering - a method of making automatic predictions\nabout the interests of a user by collecting preferences or taste information\nfrom many other users, we can also achieve similar results using neural\nnetworks. A recommender system is a subclass of information filtering system\nthat provide suggestions for items that are most pertinent to a particular\nuser. A perceptron or a neural network is a machine learning model designed for\nfitting complex datasets using backpropagation and gradient descent. When\ncoupled with advanced optimization techniques, the model may prove to be a\ngreat substitute for classical logistic classifiers. The optimizations include\nfeature scaling, mean normalization, regularization, hyperparameter tuning and\nusing stochastic/mini-batch gradient descent instead of regular gradient\ndescent. In this use case, we will use the perceptron in the recommender system\nto fit the parameters i.e., the data from a multitude of users and use it to\npredict the preference/interest of a particular user.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2.8"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.00067v1",
    "published_date": "2024-06-17 16:02:45 UTC",
    "updated_date": "2024-06-17 16:02:45 UTC"
  },
  {
    "arxiv_id": "2406.11682v1",
    "title": "Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack",
    "authors": [
      "Shangqing Tu",
      "Zhuoran Pan",
      "Wenxuan Wang",
      "Zhexin Zhang",
      "Yuliang Sun",
      "Jifan Yu",
      "Hongning Wang",
      "Lei Hou",
      "Juanzi Li"
    ],
    "abstract": "Large language models (LLMs) have been increasingly applied to various\ndomains, which triggers increasing concerns about LLMs' safety on specialized\ndomains, e.g. medicine. However, testing the domain-specific safety of LLMs is\nchallenging due to the lack of domain knowledge-driven attacks in existing\nbenchmarks. To bridge this gap, we propose a new task, knowledge-to-jailbreak,\nwhich aims to generate jailbreaks from domain knowledge to evaluate the safety\nof LLMs when applied to those domains. We collect a large-scale dataset with\n12,974 knowledge-jailbreak pairs and fine-tune a large language model as\njailbreak-generator, to produce domain knowledge-specific jailbreaks.\nExperiments on 13 domains and 8 target LLMs demonstrate the effectiveness of\njailbreak-generator in generating jailbreaks that are both relevant to the\ngiven knowledge and harmful to the target LLMs. We also apply our method to an\nout-of-domain knowledge base, showing that jailbreak-generator can generate\njailbreaks that are comparable in harmfulness to those crafted by human\nexperts. Data and code: https://github.com/THU-KEG/Knowledge-to-Jailbreak/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 14 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.11682v1",
    "published_date": "2024-06-17 15:59:59 UTC",
    "updated_date": "2024-06-17 15:59:59 UTC"
  },
  {
    "arxiv_id": "2406.11681v1",
    "title": "R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models",
    "authors": [
      "Shangqing Tu",
      "Yuanchun Wang",
      "Jifan Yu",
      "Yuyang Xie",
      "Yaran Shi",
      "Xiaozhi Wang",
      "Jing Zhang",
      "Lei Hou",
      "Juanzi Li"
    ],
    "abstract": "Large language models have achieved remarkable success on general NLP tasks,\nbut they may fall short for domain-specific problems. Recently, various\nRetrieval-Augmented Large Language Models (RALLMs) are proposed to address this\nshortcoming. However, existing evaluation tools only provide a few baselines\nand evaluate them on various domains without mining the depth of domain\nknowledge. In this paper, we address the challenges of evaluating RALLMs by\nintroducing the R-Eval toolkit, a Python toolkit designed to streamline the\nevaluation of different RAG workflows in conjunction with LLMs. Our toolkit,\nwhich supports popular built-in RAG workflows and allows for the incorporation\nof customized testing data on the specific domain, is designed to be\nuser-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs\nacross three task levels and two representative domains, revealing significant\nvariations in the effectiveness of RALLMs across different tasks and domains.\nOur analysis emphasizes the importance of considering both task and domain\nrequirements when choosing a RAG workflow and LLM combination. We are committed\nto continuously maintaining our platform at https://github.com/THU-KEG/R-Eval\nto facilitate both the industry and the researchers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 9 figures, Accepted by KDD2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11681v1",
    "published_date": "2024-06-17 15:59:49 UTC",
    "updated_date": "2024-06-17 15:59:49 UTC"
  },
  {
    "arxiv_id": "2406.11675v5",
    "title": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models",
    "authors": [
      "Yibin Wang",
      "Haizhou Shi",
      "Ligong Han",
      "Dimitris Metaxas",
      "Hao Wang"
    ],
    "abstract": "Large Language Models (LLMs) often suffer from overconfidence during\ninference, particularly when adapted to downstream domain-specific tasks with\nlimited data. Previous work addresses this issue by employing approximate\nBayesian estimation after the LLMs are trained, enabling them to quantify\nuncertainty. However, such post-training approaches' performance is severely\nlimited by the parameters learned during training. In this paper, we go beyond\npost-training Bayesianization and propose Bayesian Low-Rank Adaptation by\nBackpropagation (BLoB), an algorithm that continuously and jointly adjusts both\nthe mean and covariance of LLM parameters throughout the whole fine-tuning\nprocess. Our empirical results verify the effectiveness of BLoB in terms of\ngeneralization and uncertainty estimation, when evaluated on both\nin-distribution and out-of-distribution data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024. Additional experiments have been included\n  in the appendix",
    "pdf_url": "http://arxiv.org/pdf/2406.11675v5",
    "published_date": "2024-06-17 15:55:38 UTC",
    "updated_date": "2025-01-27 16:00:59 UTC"
  },
  {
    "arxiv_id": "2406.11670v1",
    "title": "Benchmarking of LLM Detection: Comparing Two Competing Approaches",
    "authors": [
      "Thorsten Pröhl",
      "Erik Putzier",
      "Rüdiger Zarnekow"
    ],
    "abstract": "This article gives an overview of the field of LLM text recognition.\nDifferent approaches and implemented detectors for the recognition of\nLLM-generated text are presented. In addition to discussing the\nimplementations, the article focuses on benchmarking the detectors. Although\nthere are numerous software products for the recognition of LLM-generated text,\nwith a focus on ChatGPT-like LLMs, the quality of the recognition (recognition\nrate) is not clear. Furthermore, while it can be seen that scientific\ncontributions presenting their novel approaches strive for some kind of\ncomparison with other approaches, the construction and independence of the\nevaluation dataset is often not comprehensible. As a result, discrepancies in\nthe performance evaluation of LLM detectors are often visible due to the\ndifferent benchmarking datasets. This article describes the creation of an\nevaluation dataset and uses this dataset to investigate the different\ndetectors. The selected detectors are benchmarked against each other.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11670v1",
    "published_date": "2024-06-17 15:51:46 UTC",
    "updated_date": "2024-06-17 15:51:46 UTC"
  },
  {
    "arxiv_id": "2406.11667v3",
    "title": "Is Efficient PAC Learning Possible with an Oracle That Responds 'Yes' or 'No'?",
    "authors": [
      "Constantinos Daskalakis",
      "Noah Golowich"
    ],
    "abstract": "The empirical risk minimization (ERM) principle has been highly impactful in\nmachine learning, leading both to near-optimal theoretical guarantees for\nERM-based learning algorithms as well as driving many of the recent empirical\nsuccesses in deep learning. In this paper, we investigate the question of\nwhether the ability to perform ERM, which computes a hypothesis minimizing\nempirical risk on a given dataset, is necessary for efficient learning: in\nparticular, is there a weaker oracle than ERM which can nevertheless enable\nlearnability? We answer this question affirmatively, showing that in the\nrealizable setting of PAC learning for binary classification, a concept class\ncan be learned using an oracle which only returns a single bit indicating\nwhether a given dataset is realizable by some concept in the class. The sample\ncomplexity and oracle complexity of our algorithm depend polynomially on the VC\ndimension of the hypothesis class, thus showing that there is only a polynomial\nprice to pay for use of our weaker oracle. Our results extend to the agnostic\nlearning setting with a slight strengthening of the oracle, as well as to the\npartial concept, multiclass and real-valued learning settings. In the setting\nof partial concept classes, prior to our work no oracle-efficient algorithms\nwere known, even with a standard ERM oracle. Thus, our results address a\nquestion of Alon et al. (2021) who asked whether there are algorithmic\nprinciples which enable efficient learnability in this setting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "COLT 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11667v3",
    "published_date": "2024-06-17 15:50:08 UTC",
    "updated_date": "2025-02-24 02:38:04 UTC"
  },
  {
    "arxiv_id": "2406.11665v2",
    "title": "See It from My Perspective: How Language Affects Cultural Bias in Image Understanding",
    "authors": [
      "Amith Ananthram",
      "Elias Stengel-Eskin",
      "Mohit Bansal",
      "Kathleen McKeown"
    ],
    "abstract": "Vision-language models (VLMs) can respond to queries about images in many\nlanguages. However, beyond language, culture affects how we see things. For\nexample, individuals from Western cultures focus more on the central figure in\nan image while individuals from East Asian cultures attend more to scene\ncontext. In this work, we characterize the Western bias of VLMs in image\nunderstanding and investigate the role that language plays in this disparity.\nWe evaluate VLMs across subjective and objective visual tasks with culturally\ndiverse images and annotations. We find that VLMs perform better on the Western\nsplit than on the East Asian split of each task. Through controlled\nexperimentation, we trace one source of this bias in image understanding to the\nlack of diversity in language model construction. While inference in a language\nnearer to a culture can lead to reductions in bias, we show it is much more\neffective when that language was well-represented during text-only\npre-training. Interestingly, this yields bias reductions even when prompting in\nEnglish. Our work highlights the importance of richer representation of all\nlanguages in building equitable VLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICLR 2025. 22 pages, 6 figures. Code/models:\n  https://github.com/amith-ananthram/see-it-from-my-perspective",
    "pdf_url": "http://arxiv.org/pdf/2406.11665v2",
    "published_date": "2024-06-17 15:49:51 UTC",
    "updated_date": "2025-02-28 20:03:33 UTC"
  },
  {
    "arxiv_id": "2406.11641v1",
    "title": "YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection",
    "authors": [
      "Tamara R. Lenhard",
      "Andreas Weinmann",
      "Stefan Jäger",
      "Tobias Koch"
    ],
    "abstract": "Predominant methods for image-based drone detection frequently rely on\nemploying generic object detection algorithms like YOLOv5. While proficient in\nidentifying drones against homogeneous backgrounds, these algorithms often\nstruggle in complex, highly textured environments. In such scenarios, drones\nseamlessly integrate into the background, creating camouflage effects that\nadversely affect the detection quality. To address this issue, we introduce a\nnovel deep learning architecture called YOLO-FEDER FusionNet. Unlike\nconventional approaches, YOLO-FEDER FusionNet combines generic object detection\nmethods with the specialized strength of camouflage object detection techniques\nto enhance drone detection capabilities. Comprehensive evaluations of\nYOLO-FEDER FusionNet show the efficiency of the proposed model and demonstrate\nsubstantial improvements in both reducing missed detections and false alarms.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 4 figures, 6 tables, to be published in the conference\n  proceedings of the 2024 IEEE International Conference on Image Processing\n  (ICIP)",
    "pdf_url": "http://arxiv.org/pdf/2406.11641v1",
    "published_date": "2024-06-17 15:25:31 UTC",
    "updated_date": "2024-06-17 15:25:31 UTC"
  },
  {
    "arxiv_id": "2406.11640v2",
    "title": "Linear Bellman Completeness Suffices for Efficient Online Reinforcement Learning with Few Actions",
    "authors": [
      "Noah Golowich",
      "Ankur Moitra"
    ],
    "abstract": "One of the most natural approaches to reinforcement learning (RL) with\nfunction approximation is value iteration, which inductively generates\napproximations to the optimal value function by solving a sequence of\nregression problems. To ensure the success of value iteration, it is typically\nassumed that Bellman completeness holds, which ensures that these regression\nproblems are well-specified. We study the problem of learning an optimal policy\nunder Bellman completeness in the online model of RL with linear function\napproximation. In the linear setting, while statistically efficient algorithms\nare known under Bellman completeness (e.g., Jiang et al. (2017); Zanette et al.\n(2020)), these algorithms all rely on the principle of global optimism which\nrequires solving a nonconvex optimization problem. In particular, it has\nremained open as to whether computationally efficient algorithms exist. In this\npaper we give the first polynomial-time algorithm for RL under linear Bellman\ncompleteness when the number of actions is any constant.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "COLT 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11640v2",
    "published_date": "2024-06-17 15:24:49 UTC",
    "updated_date": "2024-06-18 04:27:49 UTC"
  },
  {
    "arxiv_id": "2407.00066v3",
    "title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead",
    "authors": [
      "Rickard Brüel-Gabrielsson",
      "Jiacheng Zhu",
      "Onkar Bhardwaj",
      "Leshem Choshen",
      "Kristjan Greenewald",
      "Mikhail Yurochkin",
      "Justin Solomon"
    ],
    "abstract": "Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs)\nhas become common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of compression when serving LoRAs. We propose a method for the joint\ncompression of LoRAs into a shared basis paired with LoRA-specific scaling\nmatrices. We extend our algorithm to learn clusters of LoRAs that are amenable\nto joint compression, allowing it to scale gracefully to large LoRA\ncollections. Our experiments with up to 1000 LoRAs demonstrate that compressed\nLoRAs preserve performance while offering major throughput gains in realistic\nserving scenarios with over a thousand LoRAs, maintaining 80% of the throughput\nof serving a single LoRA.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00066v3",
    "published_date": "2024-06-17 15:21:35 UTC",
    "updated_date": "2025-02-01 21:56:34 UTC"
  },
  {
    "arxiv_id": "2406.11638v1",
    "title": "MASAI: Modular Architecture for Software-engineering AI Agents",
    "authors": [
      "Daman Arora",
      "Atharv Sonwane",
      "Nalin Wadhwa",
      "Abhav Mehrotra",
      "Saiteja Utpala",
      "Ramakrishna Bairi",
      "Aditya Kanade",
      "Nagarajan Natarajan"
    ],
    "abstract": "A common method to solve complex problems in software engineering, is to\ndivide the problem into multiple sub-problems. Inspired by this, we propose a\nModular Architecture for Software-engineering AI (MASAI) agents, where\ndifferent LLM-powered sub-agents are instantiated with well-defined objectives\nand strategies tuned to achieve those objectives. Our modular architecture\noffers several advantages: (1) employing and tuning different problem-solving\nstrategies across sub-agents, (2) enabling sub-agents to gather information\nfrom different sources scattered throughout a repository, and (3) avoiding\nunnecessarily long trajectories which inflate costs and add extraneous context.\nMASAI enabled us to achieve the highest performance (28.33% resolution rate) on\nthe popular and highly challenging SWE-bench Lite dataset consisting of 300\nGitHub issues from 11 Python repositories. We conduct a comprehensive\nevaluation of MASAI relative to other agentic methods and analyze the effects\nof our design decisions and their contribution to the success of MASAI.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11638v1",
    "published_date": "2024-06-17 15:19:51 UTC",
    "updated_date": "2024-06-17 15:19:51 UTC"
  },
  {
    "arxiv_id": "2406.11634v2",
    "title": "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance",
    "authors": [
      "Kyle Moore",
      "Jesse Roberts",
      "Thao Pham",
      "Oseremhen Ewaleifoh",
      "Doug Fisher"
    ],
    "abstract": "Cloze testing is a common method for measuring the behavior of large language\nmodels on a number of benchmark tasks. Using the MMLU dataset, we show that the\nbase-rate probability (BRP) differences across answer tokens are significant\nand affect task performance ie. guess A if uncertain. We find that\ncounterfactual prompting does sufficiently mitigate the BRP effect. The BRP\neffect is found to have a similar effect to test taking strategies employed by\nhumans leading to the conflation of task performance and test-taking ability.\nWe propose the Nvr-X-MMLU task, a variation of MMLU, which helps to\ndisambiguate test-taking ability from task performance and reports the latter.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11634v2",
    "published_date": "2024-06-17 15:14:10 UTC",
    "updated_date": "2024-09-30 17:51:11 UTC"
  },
  {
    "arxiv_id": "2406.11632v4",
    "title": "Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation",
    "authors": [
      "Boxuan Lyu",
      "Hidetaka Kamigaito",
      "Kotaro Funakoshi",
      "Manabu Okumura"
    ],
    "abstract": "Maximum a posteriori decoding, a commonly used method for neural machine\ntranslation (NMT), aims to maximize the estimated posterior probability.\nHowever, high estimated probability does not always lead to high translation\nquality. Minimum Bayes Risk (MBR) decoding (\\citealp{kumar2004minimum}) offers\nan alternative by seeking hypotheses with the highest expected utility.\n  Inspired by Quality Estimation (QE) reranking which uses the QE model as a\nranker (\\citealp{fernandes-etal-2022-quality}), we propose source-based MBR\n(sMBR) decoding, a novel approach that utilizes quasi-sources (generated via\nparaphrasing or back-translation) as ``support hypotheses'' and a\nreference-free quality estimation metric as the utility function, marking the\nfirst work to solely use sources in MBR decoding. Experiments show that sMBR\noutperforms QE reranking and the standard MBR decoding. Our findings suggest\nthat sMBR is a promising approach for NMT decoding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11632v4",
    "published_date": "2024-06-17 15:13:52 UTC",
    "updated_date": "2025-02-16 10:36:24 UTC"
  },
  {
    "arxiv_id": "2406.11614v2",
    "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
    "authors": [
      "Yihuai Hong",
      "Lei Yu",
      "Haiqin Yang",
      "Shauli Ravfogel",
      "Mor Geva"
    ],
    "abstract": "The task of \"unlearning\" certain concepts in large language models (LLMs) has\nattracted immense attention recently, due to its importance in mitigating\nundesirable model behaviours, such as the generation of harmful, private, or\nincorrect information. Current protocols to evaluate unlearning methods largely\nrely on behavioral tests, without monitoring the presence of unlearned\nknowledge within the model's parameters. This residual knowledge can be\nadversarially exploited to recover the erased information post-unlearning. We\nargue that unlearning should also be evaluated internally, by considering\nchanges in the parametric knowledge traces of the unlearned concepts. To this\nend, we propose a general evaluation methodology that leverages vocabulary\nprojections to inspect concepts encoded in model parameters. We use this\napproach to localize \"concept vectors\" - parameter vectors that encode concrete\nconcepts - and construct ConceptVectors, a benchmark dataset containing\nhundreds of common concepts and their parametric knowledge traces within two\nopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearning\nmethods minimally impact concept vectors and mostly suppress them during\ninference, while directly ablating these vectors demonstrably removes the\nassociated knowledge and significantly reduces the model's susceptibility to\nadversarial manipulation. Our results highlight limitations in behavioral-based\nunlearning evaluations and call for future work to include parameter-based\nevaluations. To support this, we release our code and benchmark at\nhttps://github.com/yihuaihong/ConceptVectors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11614v2",
    "published_date": "2024-06-17 15:00:35 UTC",
    "updated_date": "2024-10-04 11:46:20 UTC"
  },
  {
    "arxiv_id": "2406.11612v1",
    "title": "Long Code Arena: a Set of Benchmarks for Long-Context Code Models",
    "authors": [
      "Egor Bogomolov",
      "Aleksandra Eliseeva",
      "Timur Galimzyanov",
      "Evgeniy Glukhov",
      "Anton Shapkin",
      "Maria Tigina",
      "Yaroslav Golubev",
      "Alexander Kovrigin",
      "Arie van Deursen",
      "Maliheh Izadi",
      "Timofey Bryksin"
    ],
    "abstract": "Nowadays, the fields of code and natural language processing are evolving\nrapidly. In particular, models become better at processing long context windows\n- supported context sizes have increased by orders of magnitude over the last\nfew years. However, there is a shortage of benchmarks for code processing that\ngo beyond a single file of context, while the most popular ones are limited to\na single method. With this work, we aim to close this gap by introducing Long\nCode Arena, a suite of six benchmarks for code processing tasks that require\nproject-wide context. These tasks cover different aspects of code processing:\nlibrary-based code generation, CI builds repair, project-level code completion,\ncommit message generation, bug localization, and module summarization. For each\ntask, we provide a manually verified dataset for testing, an evaluation suite,\nand open-source baseline solutions based on popular LLMs to showcase the usage\nof the dataset and to simplify adoption by other researchers. We publish the\nbenchmark page on HuggingFace Spaces with the leaderboard, links to HuggingFace\nHub for all the datasets, and link to the GitHub repository with baselines:\nhttps://huggingface.co/spaces/JetBrains-Research/long-code-arena.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "54 pages, 4 figures, 22 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.11612v1",
    "published_date": "2024-06-17 14:58:29 UTC",
    "updated_date": "2024-06-17 14:58:29 UTC"
  },
  {
    "arxiv_id": "2406.11589v5",
    "title": "CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with Test-Driven Agents",
    "authors": [
      "Jing Gong",
      "Yanghui Wu",
      "Linxi Liang",
      "Yanlin Wang",
      "Jiachi Chen",
      "Mingwei Liu",
      "Zibin Zheng"
    ],
    "abstract": "Semantic code search, retrieving code that matches a given natural language\nquery, is an important task to improve productivity in software engineering.\nExisting code search datasets face limitations: they rely on human annotators\nwho assess code primarily through semantic understanding rather than functional\nverification, leading to potential inaccuracies and scalability issues.\nAdditionally, current evaluation metrics often overlook the multi-choice nature\nof code search. This paper introduces CoSQA+, pairing high-quality queries from\nCoSQA with multiple suitable codes. We develop an automated pipeline featuring\nmultiple model-based candidate selections and the novel test-driven agent\nannotation system. Among a single Large Language Model (LLM) annotator and\nPython expert annotators (without test-based verification), agents leverage\ntest-based verification and achieve the highest accuracy of 92.0%. Through\nextensive experiments, CoSQA+ has demonstrated superior quality over CoSQA.\nModels trained on CoSQA+ exhibit improved performance. We provide the code and\ndata at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.IR",
      "I.2.7; D.2.3"
    ],
    "primary_category": "cs.SE",
    "comment": "15 pages, 5 figures, journal",
    "pdf_url": "http://arxiv.org/pdf/2406.11589v5",
    "published_date": "2024-06-17 14:34:14 UTC",
    "updated_date": "2025-04-11 02:52:59 UTC"
  },
  {
    "arxiv_id": "2406.11567v1",
    "title": "Quaternion Generative Adversarial Neural Networks and Applications to Color Image Inpainting",
    "authors": [
      "Duan Wang",
      "Dandan Zhu",
      "Meixiang Zhao",
      "Zhigang Jia"
    ],
    "abstract": "Color image inpainting is a challenging task in imaging science. The existing\nmethod is based on real operation, and the red, green and blue channels of the\ncolor image are processed separately, ignoring the correlation between each\nchannel. In order to make full use of the correlation between each channel,\nthis paper proposes a Quaternion Generative Adversarial Neural Network (QGAN)\nmodel and related theory, and applies it to solve the problem of color image\ninpainting with large area missing. Firstly, the definition of quaternion\ndeconvolution is given and the quaternion batch normalization is proposed.\nSecondly, the above two innovative modules are applied to generate adversarial\nnetworks to improve stability. Finally, QGAN is applied to color image\ninpainting and compared with other state-of-the-art algorithms. The\nexperimental results show that QGAN has superiority in color image inpainting\nwith large area missing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11567v1",
    "published_date": "2024-06-17 14:04:17 UTC",
    "updated_date": "2024-06-17 14:04:17 UTC"
  },
  {
    "arxiv_id": "2406.11563v3",
    "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
    "authors": [
      "André Platzer"
    ],
    "abstract": "This perspective piece calls for the study of the new field of Intersymbolic\nAI, by which we mean the combination of symbolic AI, whose building blocks have\ninherent significance/meaning, with subsymbolic AI, whose entirety creates\nsignificance/effect despite the fact that individual building blocks escape\nmeaning. Canonical kinds of symbolic AI are logic, games and planning.\nCanonical kinds of subsymbolic AI are (un)supervised machine and reinforcement\nlearning. Intersymbolic AI interlinks the worlds of symbolic AI with its\ncompositional symbolic significance and meaning and of subsymbolic AI with its\nsummative significance or effect to enable culminations of insights from both\nworlds by going between and across symbolic AI insights with subsymbolic AI\ntechniques that are being helped by symbolic AI principles. For example,\nIntersymbolic AI may start with symbolic AI to understand a dynamic system,\ncontinue with subsymbolic AI to learn its control, and end with symbolic AI to\nsafely use the outcome of the learned subsymbolic AI controller in the dynamic\nsystem. The way Intersymbolic AI combines both symbolic and subsymbolic AI to\nincrease the effectiveness of AI compared to either kind of AI alone is likened\nto the way that the combination of both conscious and subconscious thought\nincreases the effectiveness of human thought compared to either kind of thought\nalone. Some successful contributions to the Intersymbolic AI paradigm are\nsurveyed here but many more are considered possible by advancing Intersymbolic\nAI.",
    "categories": [
      "cs.AI",
      "68T01, 68T05, 68T07, 68T27, 68T30, 03B70",
      "I.2.0; I.2.3; I.2.4; I.2.6; I.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11563v3",
    "published_date": "2024-06-17 14:01:59 UTC",
    "updated_date": "2024-07-26 09:52:15 UTC"
  },
  {
    "arxiv_id": "2407.12789v2",
    "title": "Generalisation to unseen topologies: Towards control of biological neural network activity",
    "authors": [
      "Laurens Engwegen",
      "Daan Brinks",
      "Wendelin Böhmer"
    ],
    "abstract": "Novel imaging and neurostimulation techniques open doors for advancements in\nclosed-loop control of activity in biological neural networks. This would allow\nfor applications in the investigation of activity propagation, and for\ndiagnosis and treatment of pathological behaviour. Due to the partially\nobservable characteristics of activity propagation, through networks in which\nedges can not be observed, and the dynamic nature of neuronal systems, there is\na need for adaptive, generalisable control. In this paper, we introduce an\nenvironment that procedurally generates neuronal networks with different\ntopologies to investigate this generalisation problem. Additionally, an\nexisting transformer-based architecture is adjusted to evaluate the\ngeneralisation performance of a deep RL agent in the presented partially\nobservable environment. The agent demonstrates the capability to generalise\ncontrol from a limited number of training networks to unseen test networks.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12789v2",
    "published_date": "2024-06-17 13:53:39 UTC",
    "updated_date": "2024-09-27 10:00:15 UTC"
  },
  {
    "arxiv_id": "2406.11555v1",
    "title": "Input Conditioned Graph Generation for Language Agents",
    "authors": [
      "Lukas Vierling",
      "Jie Fu",
      "Kai Chen"
    ],
    "abstract": "Recent progress in Large Language Models (LLMs) and language agents has\ndemonstrated significant promise for various future applications across\nmultiple disciplines. While traditional approaches to language agents often\nrely on fixed, handcrafted designs, our research aims to develop both learnable\nand dynamic agents. Our method uses an existing framework that abstracts\nlanguage agents as graphs. Within this graph framework, we aim to learn a model\nthat can generate edges for every given input to the language agent. This\nallows us to generate edges that represent the flow of communication within the\ngraph based on the given input, thereby adjusting the internal communication of\na language agent. We learn to generate these edges using a pretrained LLM that\nis fine-tuned with reinforcement learning. This LLM can be fine-tuned on\nseveral datasets simultaneously, and we hypothesize that the model learns to\nadapt to these different domains during training, achieving good overall\nperformance when encountering data from different domains during deployment. We\ndemonstrate that our approach surpasses the previous static approach by nearly\n6% accuracy on a combined dataset of MMLU and CMMLU, and by more than 10% when\ntrained with a sparsity-inducing loss. It also performs superior in additional\nexperiments conducted with the MMLU and Mini Crossword Puzzles datasets. The\ncode is available at https://github.com/lukasVierling/DynamicGPTSwarm.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11555v1",
    "published_date": "2024-06-17 13:53:15 UTC",
    "updated_date": "2024-06-17 13:53:15 UTC"
  },
  {
    "arxiv_id": "2407.16894v2",
    "title": "Estimating the Increase in Emissions caused by AI-augmented Search",
    "authors": [
      "Wim Vanderbauwhede"
    ],
    "abstract": "AI-generated answers to conventional search queries dramatically increase the\nenergy consumption. By our estimates, energy demand increase by 60-70 times.\nThis is a based on an updated estimate of energy consumption for conventional\nsearch and recent work on the energy demand of queries to the BLOOM model, a\n176B parameter model, and OpenAI's GPT-3, which is of similar complexity.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16894v2",
    "published_date": "2024-06-17 13:52:00 UTC",
    "updated_date": "2025-01-06 11:06:36 UTC"
  },
  {
    "arxiv_id": "2406.11931v1",
    "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
    "authors": [
      "DeepSeek-AI",
      "Qihao Zhu",
      "Daya Guo",
      "Zhihong Shao",
      "Dejian Yang",
      "Peiyi Wang",
      "Runxin Xu",
      "Y. Wu",
      "Yukun Li",
      "Huazuo Gao",
      "Shirong Ma",
      "Wangding Zeng",
      "Xiao Bi",
      "Zihui Gu",
      "Hanwei Xu",
      "Damai Dai",
      "Kai Dong",
      "Liyue Zhang",
      "Yishi Piao",
      "Zhibin Gou",
      "Zhenda Xie",
      "Zhewen Hao",
      "Bingxuan Wang",
      "Junxiao Song",
      "Deli Chen",
      "Xin Xie",
      "Kang Guan",
      "Yuxiang You",
      "Aixin Liu",
      "Qiushi Du",
      "Wenjun Gao",
      "Xuan Lu",
      "Qinyu Chen",
      "Yaohui Wang",
      "Chengqi Deng",
      "Jiashi Li",
      "Chenggang Zhao",
      "Chong Ruan",
      "Fuli Luo",
      "Wenfeng Liang"
    ],
    "abstract": "We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code\nlanguage model that achieves performance comparable to GPT4-Turbo in\ncode-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained\nfrom an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion\ntokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially\nenhances the coding and mathematical reasoning capabilities of DeepSeek-V2,\nwhile maintaining comparable performance in general language tasks. Compared to\nDeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in\nvarious aspects of code-related tasks, as well as reasoning and general\ncapabilities. Additionally, DeepSeek-Coder-V2 expands its support for\nprogramming languages from 86 to 338, while extending the context length from\n16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves\nsuperior performance compared to closed-source models such as GPT4-Turbo,\nClaude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11931v1",
    "published_date": "2024-06-17 13:51:35 UTC",
    "updated_date": "2024-06-17 13:51:35 UTC"
  },
  {
    "arxiv_id": "2406.11548v6",
    "title": "AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation",
    "authors": [
      "Chuyan Xiong",
      "Chengyu Shen",
      "Xiaoqi Li",
      "Kaichen Zhou",
      "Jeremy Liu",
      "Ruiping Wang",
      "Hao Dong"
    ],
    "abstract": "The ability to reflect on and correct failures is crucial for robotic systems\nto interact stably with real-life objects.Observing the generalization and\nreasoning capabilities of Multimodal Large Language Models (MLLMs), previous\napproaches have aimed to utilize these models to enhance robotic systems\naccordingly.However, these methods typically focus on high-level planning\ncorrections using an additional MLLM, with limited utilization of failed\nsamples to correct low-level contact poses which is particularly prone to occur\nduring articulated object manipulation.To address this gap, we propose an\nAutonomous Interactive Correction (AIC) MLLM, which makes use of previous\nlow-level interaction experiences to correct SE(3) pose predictions for\narticulated object. Specifically, AIC MLLM is initially fine-tuned to acquire\nboth pose prediction and feedback prompt comprehension abilities.We design two\ntypes of prompt instructions for interactions with objects: 1) visual masks to\nhighlight unmovable parts for position correction, and 2) textual descriptions\nto indicate potential directions for rotation correction. During inference, a\nFeedback Information Extraction module is introduced to recognize the failure\ncause, allowing AIC MLLM to adaptively correct the pose prediction using the\ncorresponding prompts.To further enhance manipulation stability, we devise a\nTest Time Adaptation strategy that enables AIC MLLM to better adapt to the\ncurrent scene configuration.Finally, extensive experiments are conducted in\nboth simulated and real-world environments to evaluate the proposed method. The\nresults demonstrate that our AIC MLLM can efficiently correct failure samples\nby leveraging interaction experience prompts.Our project website is\nhttps://sites.google.com/view/aic-mllm.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11548v6",
    "published_date": "2024-06-17 13:44:53 UTC",
    "updated_date": "2024-11-16 07:31:18 UTC"
  },
  {
    "arxiv_id": "2406.11547v1",
    "title": "GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations",
    "authors": [
      "Rick Wilming",
      "Artur Dox",
      "Hjalmar Schulz",
      "Marta Oliveira",
      "Benedict Clark",
      "Stefan Haufe"
    ],
    "abstract": "Large pre-trained language models have become popular for many applications\nand form an important backbone of many downstream tasks in natural language\nprocessing (NLP). Applying 'explainable artificial intelligence' (XAI)\ntechniques to enrich such models' outputs is considered crucial for assuring\ntheir quality and shedding light on their inner workings. However, large\nlanguage models are trained on a plethora of data containing a variety of\nbiases, such as gender biases, affecting model weights and, potentially,\nbehavior. Currently, it is unclear to what extent such biases also impact model\nexplanations in possibly unfavorable ways. We create a gender-controlled text\ndataset, GECO, in which otherwise identical sentences appear in male and female\nforms. This gives rise to ground-truth 'world explanations' for gender\nclassification tasks, enabling the objective evaluation of the correctness of\nXAI methods. We also provide GECOBench, a rigorous quantitative evaluation\nframework benchmarking popular XAI methods, applying them to pre-trained\nlanguage models fine-tuned to different degrees. This allows us to investigate\nhow pre-training induces undesirable bias in model explanations and to what\nextent fine-tuning can mitigate such explanation bias. We show a clear\ndependency between explanation performance and the number of fine-tuned layers,\nwhere XAI methods are observed to particularly benefit from fine-tuning or\ncomplete retraining of embedding layers. Remarkably, this relationship holds\nfor models achieving similar classification performance on the same task. With\nthat, we highlight the utility of the proposed gender-controlled dataset and\nnovel benchmarking approach for research and development of novel XAI methods.\nAll code including dataset generation, model training, evaluation and\nvisualization is available at: https://github.com/braindatalab/gecobench",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2406.11547v1",
    "published_date": "2024-06-17 13:44:37 UTC",
    "updated_date": "2024-06-17 13:44:37 UTC"
  },
  {
    "arxiv_id": "2407.00065v1",
    "title": "A Personalised Learning Tool for Physics Undergraduate Students Built On a Large Language Model for Symbolic Regression",
    "authors": [
      "Yufan Zhu",
      "Zi-Yu Khoo",
      "Jonathan Sze Choong Low",
      "Stephane Bressan"
    ],
    "abstract": "Interleaved practice enhances the memory and problem-solving ability of\nstudents in undergraduate courses. We introduce a personalized learning tool\nbuilt on a Large Language Model (LLM) that can provide immediate and\npersonalized attention to students as they complete homework containing\nproblems interleaved from undergraduate physics courses. Our tool leverages the\ndimensional analysis method, enhancing students' qualitative thinking and\nproblem-solving skills for complex phenomena. Our approach combines LLMs for\nsymbolic regression with dimensional analysis via prompt engineering and offers\nstudents a unique perspective to comprehend relationships between physics\nvariables. This fosters a broader and more versatile understanding of physics\nand mathematical principles and complements a conventional undergraduate\nphysics education that relies on interpreting and applying established\nequations within specific contexts. We test our personalized learning tool on\nthe equations from Feynman's lectures on physics. Our tool can correctly\nidentify relationships between physics variables for most equations,\nunderscoring its value as a complementary personalized learning tool for\nundergraduate physics students.",
    "categories": [
      "physics.ed-ph",
      "cs.AI"
    ],
    "primary_category": "physics.ed-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00065v1",
    "published_date": "2024-06-17 13:43:30 UTC",
    "updated_date": "2024-06-17 13:43:30 UTC"
  },
  {
    "arxiv_id": "2406.11544v4",
    "title": "Do Parameters Reveal More than Loss for Membership Inference?",
    "authors": [
      "Anshuman Suri",
      "Xiao Zhang",
      "David Evans"
    ],
    "abstract": "Membership inference attacks are used as a key tool for disclosure auditing.\nThey aim to infer whether an individual record was used to train a model. While\nsuch evaluations are useful to demonstrate risk, they are computationally\nexpensive and often make strong assumptions about potential adversaries' access\nto models and training environments, and thus do not provide tight bounds on\nleakage from potential attacks. We show how prior claims around black-box\naccess being sufficient for optimal membership inference do not hold for\nstochastic gradient descent, and that optimal membership inference indeed\nrequires white-box access. Our theoretical results lead to a new white-box\ninference attack, IHA (Inverse Hessian Attack), that explicitly uses model\nparameters by taking advantage of computing inverse-Hessian vector products.\nOur results show that both auditors and adversaries may be able to benefit from\naccess to model parameters, and we advocate for further research into white-box\nmethods for membership inference.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to Transactions on Machine Learning Research (TMLR)",
    "pdf_url": "http://arxiv.org/pdf/2406.11544v4",
    "published_date": "2024-06-17 13:42:28 UTC",
    "updated_date": "2024-12-19 14:33:00 UTC"
  },
  {
    "arxiv_id": "2407.12788v1",
    "title": "SS-ADA: A Semi-Supervised Active Domain Adaptation Framework for Semantic Segmentation",
    "authors": [
      "Weihao Yan",
      "Yeqiang Qian",
      "Yueyuan Li",
      "Tao Li",
      "Chunxiang Wang",
      "Ming Yang"
    ],
    "abstract": "Semantic segmentation plays an important role in intelligent vehicles,\nproviding pixel-level semantic information about the environment. However, the\nlabeling budget is expensive and time-consuming when semantic segmentation\nmodel is applied to new driving scenarios. To reduce the costs, semi-supervised\nsemantic segmentation methods have been proposed to leverage large quantities\nof unlabeled images. Despite this, their performance still falls short of the\naccuracy required for practical applications, which is typically achieved by\nsupervised learning. A significant shortcoming is that they typically select\nunlabeled images for annotation randomly, neglecting the assessment of sample\nvalue for model training. In this paper, we propose a novel semi-supervised\nactive domain adaptation (SS-ADA) framework for semantic segmentation that\nemploys an image-level acquisition strategy. SS-ADA integrates active learning\ninto semi-supervised semantic segmentation to achieve the accuracy of\nsupervised learning with a limited amount of labeled data from the target\ndomain. Additionally, we design an IoU-based class weighting strategy to\nalleviate the class imbalance problem using annotations from active learning.\nWe conducted extensive experiments on synthetic-to-real and real-to-real domain\nadaptation settings. The results demonstrate the effectiveness of our method.\nSS-ADA can achieve or even surpass the accuracy of its supervised learning\ncounterpart with only 25% of the target labeled data when using a real-time\nsegmentation model. The code for SS-ADA is available at\nhttps://github.com/ywher/SS-ADA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages,13 figures,8 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.12788v1",
    "published_date": "2024-06-17 13:40:42 UTC",
    "updated_date": "2024-06-17 13:40:42 UTC"
  },
  {
    "arxiv_id": "2406.11538v1",
    "title": "Improving Quality Control of Whole Slide Images by Explicit Artifact Augmentation",
    "authors": [
      "Artur Jurgas",
      "Marek Wodzinski",
      "Marina D'Amato",
      "Jeroen van der Laak",
      "Manfredo Atzori",
      "Henning Müller"
    ],
    "abstract": "The problem of artifacts in whole slide image acquisition, prevalent in both\nclinical workflows and research-oriented settings, necessitates human\nintervention and re-scanning. Overcoming this challenge requires developing\nquality control algorithms, that are hindered by the limited availability of\nrelevant annotated data in histopathology. The manual annotation of\nground-truth for artifact detection methods is expensive and time-consuming.\nThis work addresses the issue by proposing a method dedicated to augmenting\nwhole slide images with artifacts. The tool seamlessly generates and blends\nartifacts from an external library to a given histopathology dataset. The\naugmented datasets are then utilized to train artifact classification methods.\nThe evaluation shows their usefulness in classification of the artifacts, where\nthey show an improvement from 0.10 to 0.01 AUROC depending on the artifact\ntype. The framework, model, weights, and ground-truth annotations are freely\nreleased to facilitate open science and reproducible research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11538v1",
    "published_date": "2024-06-17 13:39:31 UTC",
    "updated_date": "2024-06-17 13:39:31 UTC"
  },
  {
    "arxiv_id": "2406.11524v1",
    "title": "Explainable Artificial Intelligence and Multicollinearity : A Mini Review of Current Approaches",
    "authors": [
      "Ahmed M Salih"
    ],
    "abstract": "Explainable Artificial Intelligence (XAI) methods help to understand the\ninternal mechanism of machine learning models and how they reach a specific\ndecision or made a specific action. The list of informative features is one of\nthe most common output of XAI methods. Multicollinearity is one of the big\nissue that should be considered when XAI generates the explanation in terms of\nthe most informative features in an AI system. No review has been dedicated to\ninvestigate the current approaches to handle such significant issue. In this\npaper, we provide a review of the current state-of-the-art approaches in\nrelation to the XAI in the context of recent advances in dealing with the\nmulticollinearity issue. To do so, we searched in three repositories that are:\nWeb of Science, Scopus and IEEE Xplore to find pertinent published papers.\nAfter excluding irrelevant papers, seven papers were considered in the review.\nIn addition, we discuss the current XAI methods and their limitations in\ndealing with the multicollinearity and suggest future directions.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11524v1",
    "published_date": "2024-06-17 13:26:53 UTC",
    "updated_date": "2024-06-17 13:26:53 UTC"
  },
  {
    "arxiv_id": "2406.11522v2",
    "title": "FullCert: Deterministic End-to-End Certification for Training and Inference of Neural Networks",
    "authors": [
      "Tobias Lorenz",
      "Marta Kwiatkowska",
      "Mario Fritz"
    ],
    "abstract": "Modern machine learning models are sensitive to the manipulation of both the\ntraining data (poisoning attacks) and inference data (adversarial examples).\nRecognizing this issue, the community has developed many empirical defenses\nagainst both attacks and, more recently, certification methods with provable\nguarantees against inference-time attacks. However, such guarantees are still\nlargely lacking for training-time attacks. In this work, we present FullCert,\nthe first end-to-end certifier with sound, deterministic bounds, which proves\nrobustness against both training-time and inference-time attacks. We first\nbound all possible perturbations an adversary can make to the training data\nunder the considered threat model. Using these constraints, we bound the\nperturbations' influence on the model's parameters. Finally, we bound the\nimpact of these parameter changes on the model's prediction, resulting in joint\nrobustness guarantees against poisoning and adversarial examples. To facilitate\nthis novel certification paradigm, we combine our theoretical work with a new\nopen-source library BoundFlow, which enables model training on bounded\ndatasets. We experimentally demonstrate FullCert's feasibility on two datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in DAGM GCPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11522v2",
    "published_date": "2024-06-17 13:23:52 UTC",
    "updated_date": "2024-09-11 12:00:30 UTC"
  },
  {
    "arxiv_id": "2406.11517v1",
    "title": "Revisiting Spurious Correlation in Domain Generalization",
    "authors": [
      "Bin Qin",
      "Jiangmeng Li",
      "Yi Li",
      "Xuesong Wu",
      "Yupeng Wang",
      "Wenwen Qiang",
      "Jianwen Cao"
    ],
    "abstract": "Without loss of generality, existing machine learning techniques may learn\nspurious correlation dependent on the domain, which exacerbates the\ngeneralization of models in out-of-distribution (OOD) scenarios. To address\nthis issue, recent works build a structural causal model (SCM) to describe the\ncausality within data generation process, thereby motivating methods to avoid\nthe learning of spurious correlation by models. However, from the machine\nlearning viewpoint, such a theoretical analysis omits the nuanced difference\nbetween the data generation process and representation learning process,\nresulting in that the causal analysis based on the former cannot well adapt to\nthe latter. To this end, we explore to build a SCM for representation learning\nprocess and further conduct a thorough analysis of the mechanisms underlying\nspurious correlation. We underscore that adjusting erroneous covariates\nintroduces bias, thus necessitating the correct selection of spurious\ncorrelation mechanisms based on practical application scenarios. In this\nregard, we substantiate the correctness of the proposed SCM and further propose\nto control confounding bias in OOD generalization by introducing a propensity\nscore weighted estimator, which can be integrated into any existing OOD method\nas a plug-and-play module. The empirical results comprehensively demonstrate\nthe effectiveness of our method on synthetic and large-scale real OOD datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11517v1",
    "published_date": "2024-06-17 13:22:00 UTC",
    "updated_date": "2024-06-17 13:22:00 UTC"
  },
  {
    "arxiv_id": "2406.11930v1",
    "title": "A Critical Study of What Code-LLMs (Do Not) Learn",
    "authors": [
      "Abhinav Anand",
      "Shweta Verma",
      "Krishna Narasimhan",
      "Mira Mezini"
    ],
    "abstract": "Large Language Models trained on code corpora (code-LLMs) have demonstrated\nimpressive performance in various coding assistance tasks. However, despite\ntheir increased size and training dataset, code-LLMs still have limitations\nsuch as suggesting codes with syntactic errors, variable misuse etc. Some\nstudies argue that code-LLMs perform well on coding tasks because they use\nself-attention and hidden representations to encode relations among input\ntokens. However, previous works have not studied what code properties are not\nencoded by code-LLMs. In this paper, we conduct a fine-grained analysis of\nattention maps and hidden representations of code-LLMs. Our study indicates\nthat code-LLMs only encode relations among specific subsets of input tokens.\nSpecifically, by categorizing input tokens into syntactic tokens and\nidentifiers, we found that models encode relations among syntactic tokens and\namong identifiers, but they fail to encode relations between syntactic tokens\nand identifiers. We also found that fine-tuned models encode these relations\npoorly compared to their pre-trained counterparts. Additionally, larger models\nwith billions of parameters encode significantly less information about code\nthan models with only a few hundred million parameters.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11930v1",
    "published_date": "2024-06-17 13:11:17 UTC",
    "updated_date": "2024-06-17 13:11:17 UTC"
  },
  {
    "arxiv_id": "2407.07719v3",
    "title": "Model-based learning for multi-antenna multi-frequency location-to-channel mapping",
    "authors": [
      "Baptiste Chatelier",
      "Vincent Corlay",
      "Matthieu Crussière",
      "Luc Le Magoarou"
    ],
    "abstract": "Years of study of the propagation channel showed a close relation between a\nlocation and the associated communication channel response. The use of a neural\nnetwork to learn the location-to-channel mapping can therefore be envisioned.\nThe Implicit Neural Representation (INR) literature showed that classical\nneural architecture are biased towards learning low-frequency content, making\nthe location-to-channel mapping learning a non-trivial problem. Indeed, it is\nwell known that this mapping is a function rapidly varying with the location,\non the order of the wavelength. This paper leverages the model-based machine\nlearning paradigm to derive a problem-specific neural architecture from a\npropagation channel model. The resulting architecture efficiently overcomes the\nspectral-bias issue. It only learns low-frequency sparse correction terms\nactivating a dictionary of high-frequency components. The proposed architecture\nis evaluated against classical INR architectures on realistic synthetic data,\nshowing much better accuracy. Its mapping learning performance is explained\nbased on the approximated channel model, highlighting the explainability of the\nmodel-based machine learning paradigm.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07719v3",
    "published_date": "2024-06-17 13:09:25 UTC",
    "updated_date": "2025-03-20 09:21:43 UTC"
  },
  {
    "arxiv_id": "2406.11504v1",
    "title": "On the Feasibility of Fidelity$^-$ for Graph Pruning",
    "authors": [
      "Yong-Min Shin",
      "Won-Yong Shin"
    ],
    "abstract": "As one of popular quantitative metrics to assess the quality of explanation\nof graph neural networks (GNNs), fidelity measures the output difference after\nremoving unimportant parts of the input graph. Fidelity has been widely used\ndue to its straightforward interpretation that the underlying model should\nproduce similar predictions when features deemed unimportant from the\nexplanation are removed. This raises a natural question: \"Does fidelity induce\na global (soft) mask for graph pruning?\" To solve this, we aim to explore the\npotential of the fidelity measure to be used for graph pruning, eventually\nenhancing the GNN models for better efficiency. To this end, we propose\nFidelity$^-$-inspired Pruning (FiP), an effective framework to construct global\nedge masks from local explanations. Our empirical observations using 7 edge\nattribution methods demonstrate that, surprisingly, general eXplainable AI\nmethods outperform methods tailored to GNNs in terms of graph pruning\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "cs.NE",
      "cs.SI",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 3 figures, 2 tables; IJCAI Workshop on Explainable AI (XAI\n  2024) (to appear) (Please cite our workshop version.)",
    "pdf_url": "http://arxiv.org/pdf/2406.11504v1",
    "published_date": "2024-06-17 13:05:00 UTC",
    "updated_date": "2024-06-17 13:05:00 UTC"
  },
  {
    "arxiv_id": "2406.11501v2",
    "title": "Teleporter Theory: A General and Simple Approach for Modeling Cross-World Counterfactual Causality",
    "authors": [
      "Jiangmeng Li",
      "Bin Qin",
      "Qirui Ji",
      "Yi Li",
      "Wenwen Qiang",
      "Jianwen Cao",
      "Fanjiang Xu"
    ],
    "abstract": "Leveraging the development of structural causal model (SCM), researchers can\nestablish graphical models for exploring the causal mechanisms behind machine\nlearning techniques. As the complexity of machine learning applications rises,\nsingle-world interventionism causal analysis encounters theoretical adaptation\nlimitations. Accordingly, cross-world counterfactual approach extends our\nunderstanding of causality beyond observed data, enabling hypothetical\nreasoning about alternative scenarios. However, the joint involvement of\ncross-world variables, encompassing counterfactual variables and real-world\nvariables, challenges the construction of the graphical model. Twin network is\na subtle attempt, establishing a symbiotic relationship, to bridge the gap\nbetween graphical modeling and the introduction of counterfactuals albeit with\nroom for improvement in generalization. In this regard, we demonstrate the\ntheoretical breakdowns of twin networks in certain cross-world counterfactual\nscenarios. To this end, we propose a novel teleporter theory to establish a\ngeneral and simple graphical representation of counterfactuals, which provides\ncriteria for determining teleporter variables to connect multiple worlds. In\ntheoretical application, we determine that introducing the proposed teleporter\ntheory can directly obtain the conditional independence between counterfactual\nvariables and real-world variables from the cross-world SCM without requiring\ncomplex algebraic derivations. Accordingly, we can further identify\ncounterfactual causal effects through cross-world symbolic derivation. We\ndemonstrate the generality of the teleporter theory to the practical\napplication. Adhering to the proposed theory, we build a plug-and-play module,\nand the effectiveness of which are substantiated by experiments on benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11501v2",
    "published_date": "2024-06-17 13:03:44 UTC",
    "updated_date": "2024-06-18 05:49:27 UTC"
  },
  {
    "arxiv_id": "2406.11495v2",
    "title": "Online Context Learning for Socially Compliant Navigation",
    "authors": [
      "Iaroslav Okunevich",
      "Alexandre Lombard",
      "Tomas Krajnik",
      "Yassine Ruichek",
      "Zhi Yan"
    ],
    "abstract": "Robot social navigation needs to adapt to different human factors and\nenvironmental contexts. However, since these factors and contexts are difficult\nto predict and cannot be exhaustively enumerated, traditional learning-based\nmethods have difficulty in ensuring the social attributes of robots in\nlong-term and cross-environment deployments. This letter introduces an online\ncontext learning method that aims to empower robots to adapt to new social\nenvironments online. The proposed method adopts a two-layer structure. The\nbottom layer is built using a deep reinforcement learning-based method to\nensure the output of basic robot navigation commands. The upper layer is\nimplemented using an online robot learning-based method to socialize the\ncontrol commands suggested by the bottom layer. Experiments using a\ncommunity-wide simulator show that our method outperforms the state-of-the-art\nones. Experimental results in the most challenging scenarios show that our\nmethod improves the performance of the state-of-the-art by 8%. The source code\nof the proposed method, the data used, and the tools for the per-training step\nare publicly available at https://github.com/Nedzhaken/SOCSARL-OL.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 4 figures, 1 table, 1 algorithm",
    "pdf_url": "http://arxiv.org/pdf/2406.11495v2",
    "published_date": "2024-06-17 12:59:13 UTC",
    "updated_date": "2025-03-14 10:41:06 UTC"
  },
  {
    "arxiv_id": "2406.11481v3",
    "title": "Constrained Reinforcement Learning with Average Reward Objective: Model-Based and Model-Free Algorithms",
    "authors": [
      "Vaneet Aggarwal",
      "Washim Uddin Mondal",
      "Qinbo Bai"
    ],
    "abstract": "Reinforcement Learning (RL) serves as a versatile framework for sequential\ndecision-making, finding applications across diverse domains such as robotics,\nautonomous driving, recommendation systems, supply chain optimization, biology,\nmechanics, and finance. The primary objective in these applications is to\nmaximize the average reward. Real-world scenarios often necessitate adherence\nto specific constraints during the learning process.\n  This monograph focuses on the exploration of various model-based and\nmodel-free approaches for Constrained RL within the context of average reward\nMarkov Decision Processes (MDPs). The investigation commences with an\nexamination of model-based strategies, delving into two foundational methods -\noptimism in the face of uncertainty and posterior sampling. Subsequently, the\ndiscussion transitions to parametrized model-free approaches, where the\nprimal-dual policy gradient-based algorithm is explored as a solution for\nconstrained MDPs. The monograph provides regret guarantees and analyzes\nconstraint violation for each of the discussed setups.\n  For the above exploration, we assume the underlying MDP to be ergodic.\nFurther, this monograph extends its discussion to encompass results tailored\nfor weakly communicating MDPs, thereby broadening the scope of its findings and\ntheir relevance to a wider range of practical scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2402.02042",
    "pdf_url": "http://arxiv.org/pdf/2406.11481v3",
    "published_date": "2024-06-17 12:46:02 UTC",
    "updated_date": "2024-07-17 11:32:18 UTC"
  },
  {
    "arxiv_id": "2406.11477v2",
    "title": "How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of Target Language Text?",
    "authors": [
      "Atsuki Yamaguchi",
      "Aline Villavicencio",
      "Nikolaos Aletras"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in many\nlanguages beyond English. Yet, LLMs require more inference steps when\ngenerating non-English text due to their reliance on English-centric tokenizers\nand vocabulary, resulting in higher usage costs to non-English speakers.\nVocabulary expansion with target language tokens is a widely used cross-lingual\nvocabulary adaptation approach to remedy this issue. Despite its effectiveness\nin inference speedup, previous work on vocabulary expansion has focused on\nhigh-resource settings assuming access to a substantial amount of target\nlanguage data to effectively initialize the embeddings of the new tokens and\nadapt the LLM to the target language. However, vocabulary expansion in\nlow-resource settings has yet to be explored. In this paper, we investigate\nvocabulary expansion in low-resource settings by considering embedding\ninitialization methods and continual pre-training strategies. Through extensive\nexperiments across typologically diverse languages, tasks and models, we\nestablish a set of strategies to perform vocabulary expansion for faster\ninference, maintaining competitive downstream performance to baselines with\nonly 30K sentences ($\\sim$0.01GB text data) from the target language.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11477v2",
    "published_date": "2024-06-17 12:42:34 UTC",
    "updated_date": "2024-09-16 13:55:24 UTC"
  },
  {
    "arxiv_id": "2406.11474v1",
    "title": "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment",
    "authors": [
      "Heyan Huang",
      "Yinghao Li",
      "Huashan Sun",
      "Yu Bai",
      "Yang Gao"
    ],
    "abstract": "Recent studies have demonstrated that In-Context Learning (ICL), through the\nuse of specific demonstrations, can align Large Language Models (LLMs) with\nhuman preferences known as In-Context Alignment (ICA), indicating that models\ncan comprehend human instructions without requiring parameter adjustments.\nHowever, the exploration of the mechanism and applicability of ICA remains\nlimited. In this paper, we begin by dividing the context text used in ICA into\nthree categories: format, system prompt, and example. Through ablation\nexperiments, we investigate the effectiveness of each part in enabling ICA to\nfunction effectively. We then examine how variants in these parts impact the\nmodel's alignment performance. Our findings indicate that the example part is\ncrucial for enhancing the model's alignment capabilities, with changes in\nexamples significantly affecting alignment performance. We also conduct a\ncomprehensive evaluation of ICA's zero-shot capabilities in various alignment\ntasks. The results indicate that compared to parameter fine-tuning methods, ICA\ndemonstrates superior performance in knowledge-based tasks and tool-use tasks.\nHowever, it still exhibits certain limitations in areas such as multi-turn\ndialogues and instruction following.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 6 figures, work in progress",
    "pdf_url": "http://arxiv.org/pdf/2406.11474v1",
    "published_date": "2024-06-17 12:38:48 UTC",
    "updated_date": "2024-06-17 12:38:48 UTC"
  },
  {
    "arxiv_id": "2406.11473v2",
    "title": "Promises, Outlooks and Challenges of Diffusion Language Modeling",
    "authors": [
      "Justin Deschenaux",
      "Caglar Gulcehre"
    ],
    "abstract": "The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models\nwere proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD\ngenerally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly\nweaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11473v2",
    "published_date": "2024-06-17 12:38:38 UTC",
    "updated_date": "2024-07-10 14:36:06 UTC"
  },
  {
    "arxiv_id": "2406.11460v1",
    "title": "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation",
    "authors": [
      "Jinyuan Fang",
      "Zaiqiao Meng",
      "Craig Macdonald"
    ],
    "abstract": "Retrieval-augmented generation (RAG) offers an effective approach for\naddressing question answering (QA) tasks. However, the imperfections of the\nretrievers in RAG models often result in the retrieval of irrelevant\ninformation, which could introduce noises and degrade the performance,\nespecially when handling multi-hop questions that require multiple steps of\nreasoning. To enhance the multi-hop reasoning ability of RAG models, we propose\nTRACE. TRACE constructs knowledge-grounded reasoning chains, which are a series\nof logically connected knowledge triples, to identify and integrate supporting\nevidence from the retrieved documents for answering questions. Specifically,\nTRACE employs a KG Generator to create a knowledge graph (KG) from the\nretrieved documents, and then uses an Autoregressive Reasoning Chain\nConstructor to build reasoning chains. Experimental results on three multi-hop\nQA datasets show that TRACE achieves an average performance improvement of up\nto 14.03% compared to using all the retrieved documents. Moreover, the results\nindicate that using reasoning chains as context, rather than the entire\ndocuments, is often sufficient to correctly answer questions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11460v1",
    "published_date": "2024-06-17 12:23:32 UTC",
    "updated_date": "2024-06-17 12:23:32 UTC"
  },
  {
    "arxiv_id": "2406.11455v2",
    "title": "Adaptive Reinforcement Learning Planning: Harnessing Large Language Models for Complex Information Extraction",
    "authors": [
      "Zepeng Ding",
      "Ruiyang Ke",
      "Wenhao Huang",
      "Guochao Jiang",
      "Yanda Li",
      "Deqing Yang",
      "Jiaqing Liang"
    ],
    "abstract": "Existing research on large language models (LLMs) shows that they can solve\ninformation extraction tasks through multi-step planning. However, their\nextraction behavior on complex sentences and tasks is unstable, emerging issues\nsuch as false positives and missing elements. We observe that decomposing\ncomplex extraction tasks and extracting them step by step can effectively\nimprove LLMs' performance, and the extraction orders of entities significantly\naffect the final results of LLMs. This paper proposes a two-stage multi-step\nmethod for LLM-based information extraction and adopts the RL framework to\nexecute the multi-step planning. We regard sequential extraction as a Markov\ndecision process, build an LLM-based extraction environment, design a decision\nmodule to adaptively provide the optimal order for sequential entity extraction\non different sentences, and utilize the DDQN algorithm to train the decision\nmodel. We also design the rewards and evaluation metrics suitable for the\nextraction results of LLMs. We conduct extensive experiments on multiple public\ndatasets to demonstrate the effectiveness of our method in improving the\ninformation extraction capabilities of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11455v2",
    "published_date": "2024-06-17 12:11:01 UTC",
    "updated_date": "2024-08-29 14:48:10 UTC"
  },
  {
    "arxiv_id": "2406.11452v1",
    "title": "Attention-Based Deep Reinforcement Learning for Qubit Allocation in Modular Quantum Architectures",
    "authors": [
      "Enrico Russo",
      "Maurizio Palesi",
      "Davide Patti",
      "Giuseppe Ascia",
      "Vincenzo Catania"
    ],
    "abstract": "Modular, distributed and multi-core architectures are currently considered a\npromising approach for scalability of quantum computing systems. The\nintegration of multiple Quantum Processing Units necessitates classical and\nquantum-coherent communication, introducing challenges related to noise and\nquantum decoherence in quantum state transfers between cores. Optimizing\ncommunication becomes imperative, and the compilation and mapping of quantum\ncircuits onto physical qubits must minimize state transfers while adhering to\narchitectural constraints. The compilation process, inherently an NP-hard\nproblem, demands extensive search times even with a small number of qubits to\nbe solved to optimality. To address this challenge efficiently, we advocate for\nthe utilization of heuristic mappers that can rapidly generate solutions. In\nthis work, we propose a novel approach employing Deep Reinforcement Learning\n(DRL) methods to learn these heuristics for a specific multi-core architecture.\nOur DRL agent incorporates a Transformer encoder and Graph Neural Networks. It\nencodes quantum circuits using self-attention mechanisms and produce outputs\nthrough an attention-based pointer mechanism that directly signifies the\nprobability of matching logical qubits with physical cores. This enables the\nselection of optimal cores for logical qubits efficiently. Experimental\nevaluations show that the proposed method can outperform baseline approaches in\nterms of reducing inter-core communications and minimizing online\ntime-to-solution. This research contributes to the advancement of scalable\nquantum computing systems by introducing a novel learning-based heuristic\napproach for efficient quantum circuit compilation and mapping.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11452v1",
    "published_date": "2024-06-17 12:09:11 UTC",
    "updated_date": "2024-06-17 12:09:11 UTC"
  },
  {
    "arxiv_id": "2406.11928v1",
    "title": "FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction",
    "authors": [
      "Muhao Xu",
      "Zhenfeng Zhu",
      "Youru Li",
      "Shuai Zheng",
      "Yawei Zhao",
      "Kunlun He",
      "Yao Zhao"
    ],
    "abstract": "Multimodal electronic health record (EHR) data can offer a holistic\nassessment of a patient's health status, supporting various predictive\nhealthcare tasks. Recently, several studies have embraced the multitask\nlearning approach in the healthcare domain, exploiting the inherent\ncorrelations among clinical tasks to predict multiple outcomes simultaneously.\nHowever, existing methods necessitate samples to possess complete labels for\nall tasks, which places heavy demands on the data and restricts the flexibility\nof the model. Meanwhile, within a multitask framework with multimodal inputs,\nhow to comprehensively consider the information disparity among modalities and\namong tasks still remains a challenging problem. To tackle these issues, a\nunified healthcare prediction model, also named by \\textbf{FlexCare}, is\nproposed to flexibly accommodate incomplete multimodal inputs, promoting the\nadaption to multiple healthcare tasks. The proposed model breaks the\nconventional paradigm of parallel multitask prediction by decomposing it into a\nseries of asynchronous single-task prediction. Specifically, a task-agnostic\nmultimodal information extraction module is presented to capture decorrelated\nrepresentations of diverse intra- and inter-modality patterns. Taking full\naccount of the information disparities between different modalities and\ndifferent tasks, we present a task-guided hierarchical multimodal fusion module\nthat integrates the refined modality-level representations into an individual\npatient-level representation. Experimental results on multiple tasks from\nMIMIC-IV/MIMIC-CXR/MIMIC-NOTE datasets demonstrate the effectiveness of the\nproposed method. Additionally, further analysis underscores the feasibility and\npotential of employing such a multitask strategy in the healthcare domain. The\nsource code is available at https://github.com/mhxu1998/FlexCare.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by KDD 2024 (Research Track)",
    "pdf_url": "http://arxiv.org/pdf/2406.11928v1",
    "published_date": "2024-06-17 12:03:10 UTC",
    "updated_date": "2024-06-17 12:03:10 UTC"
  },
  {
    "arxiv_id": "2406.11439v1",
    "title": "GPT-Powered Elicitation Interview Script Generator for Requirements Engineering Training",
    "authors": [
      "Binnur Görer",
      "Fatma Başak Aydemir"
    ],
    "abstract": "Elicitation interviews are the most common requirements elicitation\ntechnique, and proficiency in conducting these interviews is crucial for\nrequirements elicitation. Traditional training methods, typically limited to\ntextbook learning, may not sufficiently address the practical complexities of\ninterviewing techniques. Practical training with various interview scenarios is\nimportant for understanding how to apply theoretical knowledge in real-world\ncontexts. However, there is a shortage of educational interview material, as\ncreating interview scripts requires both technical expertise and creativity. To\naddress this issue, we develop a specialized GPT agent for auto-generating\ninterview scripts. The GPT agent is equipped with a dedicated knowledge base\ntailored to the guidelines and best practices of requirements elicitation\ninterview procedures. We employ a prompt chaining approach to mitigate the\noutput length constraint of GPT to be able to generate thorough and detailed\ninterview scripts. This involves dividing the interview into sections and\ncrafting distinct prompts for each, allowing for the generation of complete\ncontent for each section. The generated scripts are assessed through standard\nnatural language generation evaluation metrics and an expert judgment study,\nconfirming their applicability in requirements engineering training.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to RE@Next! at the IEEE International Requirements\n  Engineering Conference 2024 at Reykjavik, Iceland",
    "pdf_url": "http://arxiv.org/pdf/2406.11439v1",
    "published_date": "2024-06-17 11:53:55 UTC",
    "updated_date": "2024-06-17 11:53:55 UTC"
  },
  {
    "arxiv_id": "2406.11437v1",
    "title": "Analysing the Behaviour of Tree-Based Neural Networks in Regression Tasks",
    "authors": [
      "Peter Samoaa",
      "Mehrdad Farahani",
      "Antonio Longa",
      "Philipp Leitner",
      "Morteza Haghir Chehreghani"
    ],
    "abstract": "The landscape of deep learning has vastly expanded the frontiers of source\ncode analysis, particularly through the utilization of structural\nrepresentations such as Abstract Syntax Trees (ASTs). While these methodologies\nhave demonstrated effectiveness in classification tasks, their efficacy in\nregression applications, such as execution time prediction from source code,\nremains underexplored. This paper endeavours to decode the behaviour of\ntree-based neural network models in the context of such regression challenges.\nWe extend the application of established models--tree-based Convolutional\nNeural Networks (CNNs), Code2Vec, and Transformer-based methods--to predict the\nexecution time of source code by parsing it to an AST. Our comparative analysis\nreveals that while these models are benchmarks in code representation, they\nexhibit limitations when tasked with regression. To address these deficiencies,\nwe propose a novel dual-transformer approach that operates on both source code\ntokens and AST representations, employing cross-attention mechanisms to enhance\ninterpretability between the two domains. Furthermore, we explore the\nadaptation of Graph Neural Networks (GNNs) to this tree-based problem,\ntheorizing the inherent compatibility due to the graphical nature of ASTs.\nEmpirical evaluations on real-world datasets showcase that our dual-transformer\nmodel outperforms all other tree-based neural networks and the GNN-based\nmodels. Moreover, our proposed dual transformer demonstrates remarkable\nadaptability and robust performance across diverse datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "This Paper is submitted to IEEE Transactions on Neural Networks and\n  Learning Systems",
    "pdf_url": "http://arxiv.org/pdf/2406.11437v1",
    "published_date": "2024-06-17 11:47:14 UTC",
    "updated_date": "2024-06-17 11:47:14 UTC"
  },
  {
    "arxiv_id": "2406.11432v1",
    "title": "AnyTrans: Translate AnyText in the Image with Large Scale Models",
    "authors": [
      "Zhipeng Qian",
      "Pei Zhang",
      "Baosong Yang",
      "Kai Fan",
      "Yiwei Ma",
      "Derek F. Wong",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ],
    "abstract": "This paper introduces AnyTrans, an all-encompassing framework for the\ntask-Translate AnyText in the Image (TATI), which includes multilingual text\ntranslation and text fusion within images. Our framework leverages the\nstrengths of large-scale models, such as Large Language Models (LLMs) and\ntext-guided diffusion models, to incorporate contextual cues from both textual\nand visual elements during translation. The few-shot learning capability of\nLLMs allows for the translation of fragmented texts by considering the overall\ncontext. Meanwhile, the advanced inpainting and editing abilities of diffusion\nmodels make it possible to fuse translated text seamlessly into the original\nimage while preserving its style and realism. Additionally, our framework can\nbe constructed entirely using open-source models and requires no training,\nmaking it highly accessible and easily expandable. To encourage advancement in\nthe TATI task, we have meticulously compiled a test dataset called MTIT6, which\nconsists of multilingual text image translation data from six language pairs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11432v1",
    "published_date": "2024-06-17 11:37:48 UTC",
    "updated_date": "2024-06-17 11:37:48 UTC"
  },
  {
    "arxiv_id": "2406.11431v3",
    "title": "Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization",
    "authors": [
      "Wenkai Yang",
      "Shiqi Shen",
      "Guangyao Shen",
      "Wei Yao",
      "Yong Liu",
      "Zhi Gong",
      "Yankai Lin",
      "Ji-Rong Wen"
    ],
    "abstract": "Superalignment, where humans act as weak supervisors for superhuman models,\nhas become a crucial problem with the rapid development of Large Language\nModels (LLMs). Recent work has preliminarily studied this problem by using weak\nmodels to supervise strong models, and discovered that weakly supervised strong\nstudents can consistently outperform weak teachers towards the alignment\ntarget, leading to a weak-to-strong generalization phenomenon. However, we are\nconcerned that behind such a promising phenomenon, whether there exists an\nissue of weak-to-strong deception, where strong models deceive weak models by\nexhibiting well-aligned in areas known to weak models but producing misaligned\nbehaviors in cases weak models do not know. We take an initial step towards\nexploring this security issue in a specific but realistic multi-objective\nalignment case, where there may be some alignment targets conflicting with each\nother (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such\ncases, strong models might deliberately make mistakes in areas known to them\nbut unknown to weak models within one alignment dimension, in exchange for a\nhigher reward in another dimension. Through extensive experiments in both the\nreward modeling and preference optimization scenarios, we find: (1) The\nweak-to-strong deception phenomenon exists across all settings. (2) The\ndeception intensifies as the capability gap between weak and strong models\nincreases. (3) Bootstrapping with an intermediate model can mitigate the\ndeception to some extent, though its effectiveness remains limited. Our work\nhighlights the urgent need to pay more attention to the true reliability of\nsuperalignment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICLR 2025, camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2406.11431v3",
    "published_date": "2024-06-17 11:36:39 UTC",
    "updated_date": "2025-02-28 13:43:17 UTC"
  },
  {
    "arxiv_id": "2406.11430v4",
    "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression",
    "authors": [
      "Alessio Devoto",
      "Yu Zhao",
      "Simone Scardapane",
      "Pasquale Minervini"
    ],
    "abstract": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
    "pdf_url": "http://arxiv.org/pdf/2406.11430v4",
    "published_date": "2024-06-17 11:35:16 UTC",
    "updated_date": "2024-11-03 09:42:35 UTC"
  },
  {
    "arxiv_id": "2406.11429v1",
    "title": "Fusion Makes Perfection: An Efficient Multi-Grained Matching Approach for Zero-Shot Relation Extraction",
    "authors": [
      "Shilong Li",
      "Ge Bai",
      "Zhang Zhang",
      "Ying Liu",
      "Chenji Lu",
      "Daichi Guo",
      "Ruifang Liu",
      "Yong Sun"
    ],
    "abstract": "Predicting unseen relations that cannot be observed during the training phase\nis a challenging task in relation extraction. Previous works have made progress\nby matching the semantics between input instances and label descriptions.\nHowever, fine-grained matching often requires laborious manual annotation, and\nrich interactions between instances and label descriptions come with\nsignificant computational overhead. In this work, we propose an efficient\nmulti-grained matching approach that uses virtual entity matching to reduce\nmanual annotation cost, and fuses coarse-grained recall and fine-grained\nclassification for rich interactions with guaranteed inference speed.\nExperimental results show that our approach outperforms the previous State Of\nThe Art (SOTA) methods, and achieves a balance between inference efficiency and\nprediction accuracy in zero-shot relation extraction tasks. Our code is\navailable at https://github.com/longls777/EMMA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the main conference of NAACL2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11429v1",
    "published_date": "2024-06-17 11:31:48 UTC",
    "updated_date": "2024-06-17 11:31:48 UTC"
  },
  {
    "arxiv_id": "2406.11427v2",
    "title": "DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors",
    "authors": [
      "Keon Lee",
      "Dong Won Kim",
      "Jaehyeon Kim",
      "Seungjun Chung",
      "Jaewoong Cho"
    ],
    "abstract": "Large-scale latent diffusion models (LDMs) excel in content generation across\nvarious modalities, but their reliance on phonemes and durations in\ntext-to-speech (TTS) limits scalability and access from other fields. While\nrecent studies show potential in removing these domain-specific factors,\nperformance remains suboptimal. In this work, we introduce DiTTo-TTS, a\nDiffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based\nTTS can achieve state-of-the-art performance without domain-specific factors.\nThrough rigorous analysis and empirical exploration, we find that (1) DiT with\nminimal modifications outperforms U-Net, (2) variable-length modeling with a\nspeech length predictor significantly improves results over fixed-length\napproaches, and (3) conditions like semantic alignment in speech latent\nrepresentations are key to further enhancement. By scaling our training data to\n82K hours and the model size to 790M parameters, we achieve superior or\ncomparable zero-shot performance to state-of-the-art TTS models in naturalness,\nintelligibility, and speaker similarity, all without relying on domain-specific\nfactors. Speech samples are available at https://ditto-tts.github.io.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11427v2",
    "published_date": "2024-06-17 11:25:57 UTC",
    "updated_date": "2025-02-17 17:34:45 UTC"
  },
  {
    "arxiv_id": "2406.11423v3",
    "title": "Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains",
    "authors": [
      "Evan M. Williams",
      "Peter Carragher",
      "Kathleen M. Carley"
    ],
    "abstract": "Proactive content moderation requires platforms to rapidly and continuously\nevaluate the credibility of websites. Leveraging the direct and indirect paths\nusers follow to unreliable websites, we develop a website credibility\nclassification and discovery system that integrates both webgraph and\nlarge-scale social media contexts. We additionally introduce the concept of\ndredge words, terms or phrases for which unreliable domains rank highly on\nsearch engines, and provide the first exploration of their usage on social\nmedia. Our graph neural networks that combine webgraph and social media\ncontexts generate to state-of-the-art results in website credibility\nclassification and significantly improves the top-k identification of\nunreliable domains. Additionally, we release a novel dataset of dredge words,\nhighlighting their strong connections to both social media and online commerce\nplatforms.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11423v3",
    "published_date": "2024-06-17 11:22:04 UTC",
    "updated_date": "2025-02-24 16:40:20 UTC"
  },
  {
    "arxiv_id": "2406.15486v2",
    "title": "SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention",
    "authors": [
      "Qianchao Zhu",
      "Jiangfei Duan",
      "Chang Chen",
      "Siran Liu",
      "Xiuhong Li",
      "Guanyu Feng",
      "Xin Lv",
      "Huanqi Cao",
      "Xiao Chuanfu",
      "Xingcheng Zhang",
      "Dahua Lin",
      "Chao Yang"
    ],
    "abstract": "Large language models (LLMs) now support extremely long context windows, but\nthe quadratic complexity of vanilla attention results in significantly long\nTime-to-First-Token (TTFT) latency. Existing approaches to address this\ncomplexity require additional pretraining or finetuning, and often sacrifice\nmodel accuracy. In this paper, we first provide both theoretical and empirical\nfoundations for near-lossless sparse attention. We find dynamically capturing\nhead-specific sparse patterns at runtime with low overhead is crucial. To\naddress this, we propose SampleAttention, an adaptive structured and\nnear-lossless sparse attention. Leveraging observed significant sparse\npatterns, SampleAttention attends to a fixed percentage of adjacent tokens to\ncapture local window patterns, and employs a two-stage query-guided key-value\nfiltering approach, which adaptively select a minimum set of key-values with\nlow overhead, to capture column stripe patterns. Comprehensive evaluations show\nthat SampleAttention can seamlessly replace vanilla attention in off-the-shelf\nLLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$\ncompared with FlashAttention.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15486v2",
    "published_date": "2024-06-17 11:05:15 UTC",
    "updated_date": "2024-06-28 08:55:17 UTC"
  },
  {
    "arxiv_id": "2406.11414v2",
    "title": "Formally Certified Approximate Model Counting",
    "authors": [
      "Yong Kiam Tan",
      "Jiong Yang",
      "Mate Soos",
      "Magnus O. Myreen",
      "Kuldeep S. Meel"
    ],
    "abstract": "Approximate model counting is the task of approximating the number of\nsolutions to an input Boolean formula. The state-of-the-art approximate model\ncounter for formulas in conjunctive normal form (CNF), ApproxMC, provides a\nscalable means of obtaining model counts with probably approximately correct\n(PAC)-style guarantees. Nevertheless, the validity of ApproxMC's approximation\nrelies on a careful theoretical analysis of its randomized algorithm and the\ncorrectness of its highly optimized implementation, especially the latter's\nstateful interactions with an incremental CNF satisfiability solver capable of\nnatively handling parity (XOR) constraints.\n  We present the first certification framework for approximate model counting\nwith formally verified guarantees on the quality of its output approximation.\nOur approach combines: (i) a static, once-off, formal proof of the algorithm's\nPAC guarantee in the Isabelle/HOL proof assistant; and (ii) dynamic, per-run,\nverification of ApproxMC's calls to an external CNF-XOR solver using proof\ncertificates. We detail our general approach to establish a rigorous connection\nbetween these two parts of the verification, including our blueprint for\nturning the formalized, randomized algorithm into a verified proof checker, and\nour design of proof certificates for both ApproxMC and its internal CNF-XOR\nsolving steps. Experimentally, we show that certificate generation adds little\noverhead to an approximate counter implementation, and that our certificate\nchecker is able to fully certify $84.7\\%$ of instances with generated\ncertificates when given the same time and memory limits as the counter.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "The extended version, including the appendix, of the paper to be\n  published in CAV24. The associated artifact is available at\n  https://doi.org/10.5281/zenodo.10948839",
    "pdf_url": "http://arxiv.org/pdf/2406.11414v2",
    "published_date": "2024-06-17 11:02:04 UTC",
    "updated_date": "2024-06-19 01:24:40 UTC"
  },
  {
    "arxiv_id": "2406.11410v2",
    "title": "HARE: HumAn pRiors, a key to small language model Efficiency",
    "authors": [
      "Lingyun Zhang",
      "Bin jin",
      "Gaojian Ge",
      "Lunhui Liu",
      "Xuewen Shen",
      "Mingyong Wu",
      "Houqian Zhang",
      "Yongneng Jiang",
      "Shiqi Chen",
      "Shi Pu"
    ],
    "abstract": "Human priors play a crucial role in efficiently utilizing data in deep\nlearning. However, with the development of large language models (LLMs), there\nis an increasing emphasis on scaling both model size and data volume, which\noften diminishes the importance of human priors in data construction.\nInfluenced by these trends, existing Small Language Models (SLMs) mainly rely\non web-scraped large-scale training data, neglecting the proper incorporation\nof human priors. This oversight limits the training efficiency of language\nmodels in resource-constrained settings. In this paper, we propose a principle\nto leverage human priors for data construction. This principle emphasizes\nachieving high-performance SLMs by training on a concise dataset that\naccommodates both semantic diversity and data quality consistency, while\navoiding benchmark data leakage. Following this principle, we train an SLM\nnamed HARE-1.1B. Extensive experiments on large-scale benchmark datasets\ndemonstrate that HARE-1.1B performs favorably against state-of-the-art SLMs,\nvalidating the effectiveness of the proposed principle. Additionally, this\nprovides new insights into efficient language model training in\nresource-constrained environments from the view of human priors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11410v2",
    "published_date": "2024-06-17 10:56:03 UTC",
    "updated_date": "2024-06-18 11:59:03 UTC"
  },
  {
    "arxiv_id": "2406.11409v2",
    "title": "CodeGemma: Open Code Models Based on Gemma",
    "authors": [
      "CodeGemma Team",
      "Heri Zhao",
      "Jeffrey Hui",
      "Joshua Howland",
      "Nam Nguyen",
      "Siqi Zuo",
      "Andrea Hu",
      "Christopher A. Choquette-Choo",
      "Jingyue Shen",
      "Joe Kelley",
      "Kshitij Bansal",
      "Luke Vilnis",
      "Mateo Wirth",
      "Paul Michel",
      "Peter Choy",
      "Pratik Joshi",
      "Ravin Kumar",
      "Sarmad Hashmi",
      "Shubham Agrawal",
      "Zhitao Gong",
      "Jane Fine",
      "Tris Warkentin",
      "Ale Jakse Hartman",
      "Bin Ni",
      "Kathy Korevec",
      "Kelly Schaefer",
      "Scott Huffman"
    ],
    "abstract": "This paper introduces CodeGemma, a collection of specialized open code models\nbuilt on top of Gemma, capable of a variety of code and natural language\ngeneration tasks. We release three model variants. CodeGemma 7B pretrained (PT)\nand instruction-tuned (IT) variants have remarkably resilient natural language\nunderstanding, excel in mathematical reasoning, and match code capabilities of\nother open models. CodeGemma 2B is a state-of-the-art code completion model\ndesigned for fast code infilling and open-ended generation in latency-sensitive\nsettings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "v1: 11 pages, 4 figures, 5 tables. v2: Update metadata",
    "pdf_url": "http://arxiv.org/pdf/2406.11409v2",
    "published_date": "2024-06-17 10:54:35 UTC",
    "updated_date": "2024-06-19 02:37:50 UTC"
  },
  {
    "arxiv_id": "2407.12787v2",
    "title": "GameVibe: A Multimodal Affective Game Corpus",
    "authors": [
      "Matthew Barthet",
      "Maria Kaselimi",
      "Kosmas Pinitas",
      "Konstantinos Makantasis",
      "Antonios Liapis",
      "Georgios N. Yannakakis"
    ],
    "abstract": "As online video and streaming platforms continue to grow, affective computing\nresearch has undergone a shift towards more complex studies involving multiple\nmodalities. However, there is still a lack of readily available datasets with\nhigh-quality audiovisual stimuli. In this paper, we present GameVibe, a novel\naffect corpus which consists of multimodal audiovisual stimuli, including\nin-game behavioural observations and third-person affect traces for viewer\nengagement. The corpus consists of videos from a diverse set of publicly\navailable gameplay sessions across 30 games, with particular attention to\nensure high-quality stimuli with good audiovisual and gameplay diversity.\nFurthermore, we present an analysis on the reliability of the annotators in\nterms of inter-annotator agreement.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "12 pages, 5 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2407.12787v2",
    "published_date": "2024-06-17 10:52:52 UTC",
    "updated_date": "2025-04-01 09:14:18 UTC"
  },
  {
    "arxiv_id": "2406.11402v3",
    "title": "Are Small Language Models Ready to Compete with Large Language Models for Practical Applications?",
    "authors": [
      "Neelabh Sinha",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "abstract": "The rapid rise of Language Models (LMs) has expanded their use in several\napplications. Yet, due to constraints of model size, associated cost, or\nproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always\nfeasible. With open, smaller LMs emerging, more applications can leverage their\ncapabilities, but selecting the right LM can be challenging as smaller LMs do\nnot perform well universally. This work tries to bridge this gap by proposing a\nframework to experimentally evaluate small, open LMs in practical settings\nthrough measuring semantic correctness of outputs across three practical\naspects: task types, application domains, and reasoning types, using diverse\nprompt styles. It also conducts an in-depth comparison of 10 small, open LMs to\nidentify the best LM and prompt style depending on specific application\nrequirements using the proposed framework. We also show that if selected\nappropriately, they can outperform SOTA LLMs like DeepSeek-v2, GPT-4o,\nGPT-4o-mini, Gemini-1.5-Pro, and even compete with GPT-4o.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at The Fifth Workshop on Trustworthy Natural Language\n  Processing (TrustNLP 2025) in Annual Conference of the Nations of the\n  Americas Chapter of the Association for Computational Linguistics (NAACL),\n  2025. 8 pages + references + Appendix",
    "pdf_url": "http://arxiv.org/pdf/2406.11402v3",
    "published_date": "2024-06-17 10:45:36 UTC",
    "updated_date": "2025-03-12 04:37:42 UTC"
  },
  {
    "arxiv_id": "2406.11927v4",
    "title": "On the Impacts of Contexts on Repository-Level Code Generation",
    "authors": [
      "Nam Le Hai",
      "Dung Manh Nguyen",
      "Nghi D. Q. Bui"
    ],
    "abstract": "CodeLLMs have gained widespread adoption for code generation tasks, yet their\ncapacity to handle repository-level code generation with complex contextual\ndependencies remains underexplored. Our work underscores the critical\nimportance of leveraging repository-level contexts to generate executable and\nfunctionally correct code. We present RepoExec, a novel benchmark designed to\nevaluate repository-level code generation, with a focus on three key aspects:\nexecutability, functional correctness through comprehensive test case\ngeneration, and accurate utilization of cross-file contexts. Our study examines\na controlled scenario where developers specify essential code dependencies\n(contexts), challenging models to integrate them effectively. Additionally, we\nintroduce an instruction-tuned dataset that enhances CodeLLMs' ability to\nleverage dependencies, along with a new metric, Dependency Invocation Rate\n(DIR), to quantify context utilization. Experimental results reveal that while\npretrained LLMs demonstrate superior performance in terms of correctness,\ninstruction-tuned models excel in context utilization and debugging\ncapabilities. RepoExec offers a comprehensive evaluation framework for\nassessing code functionality and alignment with developer intent, thereby\nadvancing the development of more reliable CodeLLMs for real-world\napplications. The dataset and source code are available at\nhttps://github.com/FSoft-AI4Code/RepoExec.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.11927v4",
    "published_date": "2024-06-17 10:45:22 UTC",
    "updated_date": "2025-02-09 08:47:20 UTC"
  },
  {
    "arxiv_id": "2406.11397v2",
    "title": "DistPred: A Distribution-Free Probabilistic Inference Method for Regression and Forecasting",
    "authors": [
      "Daojun Liang",
      "Haixia Zhang",
      "Dongfeng Yuan"
    ],
    "abstract": "Traditional regression and prediction tasks often only provide deterministic\npoint estimates. To estimate the distribution or uncertainty of the response\nvariable, traditional methods either assume that the posterior distribution of\nsamples follows a Gaussian process or require thousands of forward passes for\nsample generation. We propose a novel approach called DistPred for regression\nand forecasting tasks, which overcomes the limitations of existing methods\nwhile remaining simple and powerful. Specifically, we transform proper scoring\nrules that measure the discrepancy between the predicted distribution and the\ntarget distribution into a differentiable discrete form and use it as a loss\nfunction to train the model end-to-end. This allows the model to sample\nnumerous samples in a single forward pass to estimate the potential\ndistribution of the response variable. We have compared our method with several\nexisting approaches on multiple datasets and achieved state-of-the-art\nperformance. Additionally, our method significantly improves computational\nefficiency. For example, compared to state-of-the-art models, DistPred has a\n180x faster inference speed Experimental results can be reproduced through\nhttps://github.com/Anoise/DistPred.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at KDD 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.11397v2",
    "published_date": "2024-06-17 10:33:00 UTC",
    "updated_date": "2025-01-07 03:17:48 UTC"
  },
  {
    "arxiv_id": "2406.11375v2",
    "title": "Boosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?",
    "authors": [
      "Siyu Yuan",
      "Cheng Jiayang",
      "Lin Qiu",
      "Deqing Yang"
    ],
    "abstract": "Analogical reasoning plays a critical role in human cognition, enabling us to\nunderstand new concepts by associating them with familiar ones. Previous\nresearch in the AI community has mainly focused on identifying and generating\nanalogies and then examining their quality under human evaluation, which\noverlooks the practical application of these analogies in real-world settings.\nInspired by the human education process, in this paper, we propose to\ninvestigate how analogies created by teacher language models (LMs) can assist\nstudent LMs in understanding scientific concepts, thereby aligning more closely\nwith practical scenarios. Our results suggest that free-form analogies can\nindeed aid LMs in understanding concepts. Additionally, analogies generated by\nstudent LMs can improve their own performance on scientific question answering,\ndemonstrating their capability to use analogies for self-learning new\nknowledge. Resources are available at https://github.com/siyuyuan/SCUA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11375v2",
    "published_date": "2024-06-17 09:51:38 UTC",
    "updated_date": "2024-09-25 07:38:37 UTC"
  },
  {
    "arxiv_id": "2406.11370v2",
    "title": "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments",
    "authors": [
      "Han Zhou",
      "Xingchen Wan",
      "Yinhong Liu",
      "Nigel Collier",
      "Ivan Vulić",
      "Anna Korhonen"
    ],
    "abstract": "Large language models (LLMs) have shown promising abilities as cost-effective\nand reference-free evaluators for assessing language generation quality. In\nparticular, pairwise LLM evaluators, which compare two generated texts and\ndetermine the preferred one, have been employed in a wide range of\napplications. However, LLMs exhibit preference biases and worrying sensitivity\nto prompt designs. In this work, we first reveal that the predictive preference\nof LLMs can be highly brittle and skewed, even with semantically equivalent\ninstructions. We find that fairer predictive preferences from LLMs consistently\nlead to judgments that are better aligned with humans. Motivated by this\nphenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt\nOptimization framework, ZEPO, which aims to produce fairer preference decisions\nand improve the alignment of LLM evaluators with human judgments. To this end,\nwe propose a zero-shot learning objective based on the preference decision\nfairness. ZEPO demonstrates substantial performance improvements over\nstate-of-the-art LLM evaluators, without requiring labeled data, on\nrepresentative meta-evaluation benchmarks. Our findings underscore the critical\ncorrelation between preference fairness and human alignment, positioning ZEPO\nas an efficient prompt optimizer for bridging the gap between LLM evaluators\nand human judgments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11370v2",
    "published_date": "2024-06-17 09:48:53 UTC",
    "updated_date": "2024-10-12 23:47:11 UTC"
  },
  {
    "arxiv_id": "2406.16937v2",
    "title": "A Complete Survey on LLM-based AI Chatbots",
    "authors": [
      "Sumit Kumar Dam",
      "Choong Seon Hong",
      "Yu Qiao",
      "Chaoning Zhang"
    ],
    "abstract": "The past few decades have witnessed an upsurge in data, forming the\nfoundation for data-hungry, learning-based AI technology. Conversational\nagents, often referred to as AI chatbots, rely heavily on such data to train\nlarge language models (LLMs) and generate new content (knowledge) in response\nto user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have\nset new standards in the AI community. This paper presents a complete survey of\nthe evolution and deployment of LLM-based chatbots in various sectors. We first\nsummarize the development of foundational chatbots, followed by the evolution\nof LLMs, and then provide an overview of LLM-based chatbots currently in use\nand those in the development phase. Recognizing AI chatbots as tools for\ngenerating new knowledge, we explore their diverse applications across various\nindustries. We then discuss the open challenges, considering how the data used\nto train the LLMs and the misuse of the generated knowledge can cause several\nissues. Finally, we explore the future outlook to augment their efficiency and\nreliability in numerous applications. By addressing key milestones and the\npresent-day context of LLM-based chatbots, our survey invites readers to delve\ndeeper into this realm, reflecting on how their next generation will reshape\nconversational AI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.16937v2",
    "published_date": "2024-06-17 09:39:34 UTC",
    "updated_date": "2024-11-18 12:36:13 UTC"
  },
  {
    "arxiv_id": "2406.11357v2",
    "title": "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities",
    "authors": [
      "Zhonghao Li",
      "Xuming Hu",
      "Aiwei Liu",
      "Kening Zheng",
      "Sirui Huang",
      "Hui Xiong"
    ],
    "abstract": "Large Language Models (LLMs) are limited by their parametric knowledge,\nleading to hallucinations in knowledge-extensive tasks. To address this,\nRetrieval-Augmented Generation (RAG) incorporates external document chunks to\nexpand LLM knowledge. Furthermore, compressing information from document chunks\nthrough extraction or summarization can improve LLM performance. Nonetheless,\nLLMs still struggle to notice and utilize scattered key information, a problem\nknown as the \"lost-in-the-middle\" syndrome. Therefore, we typically need to\nrestructure the content for LLM to recognize the key information. We propose\n$\\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that\noperates in the post-retrieval process of RAG. $\\textit{Refiner}$ leverages a\nsingle decoder-only LLM to adaptively extract query-relevant contents verbatim\nalong with the necessary context, and section them based on their\ninterconnectedness, thereby highlights information distinction, and aligns\ndownstream LLMs with the original context effectively. Experiments show that a\ntrained $\\textit{Refiner}$ (with 7B parameters) exhibits significant gain to\ndownstream LLM in improving answer accuracy, and outperforms other\nstate-of-the-art advanced RAG and concurrent compressing approaches in various\nsingle-hop and multi-hop QA tasks. Notably, $\\textit{Refiner}$ achieves a 80.5%\ntokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared\nto the next best solution. $\\textit{Refiner}$ is a plug-and-play solution that\ncan be seamlessly integrated with RAG systems, facilitating its application\nacross diverse open-source frameworks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.IR",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.11357v2",
    "published_date": "2024-06-17 09:25:10 UTC",
    "updated_date": "2024-06-18 02:44:27 UTC"
  },
  {
    "arxiv_id": "2406.11354v2",
    "title": "Preserving Knowledge in Large Language Model with Model-Agnostic Self-Decompression",
    "authors": [
      "Zilun Zhang",
      "Yutao Sun",
      "Tiancheng Zhao",
      "Leigang Sha",
      "Ruochen Xu",
      "Kyusong Lee",
      "Jianwei Yin"
    ],
    "abstract": "Humans can retain old knowledge while learning new information, but Large\nLanguage Models (LLMs) often suffer from catastrophic forgetting when\npost-pretrained or supervised fine-tuned (SFT) on domain-specific data.\nMoreover, for Multimodal Large Language Models (MLLMs) which are composed of\nthe LLM base and visual projector (e.g. LLaVA), a significant decline in\nperformance on language benchmarks was observed compared to their\nsingle-modality counterparts. To address these challenges, we introduce a novel\nmodel-agnostic self-decompression method, Tree Generation (TG), that\ndecompresses knowledge within LLMs into the training corpus. This paper focuses\non TG-SFT, which can synthetically generate SFT data for the instruction tuning\nsteps. By incorporating the dumped corpus during SFT for MLLMs, we\nsignificantly reduce the forgetting problem.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11354v2",
    "published_date": "2024-06-17 09:17:40 UTC",
    "updated_date": "2024-06-19 11:36:30 UTC"
  },
  {
    "arxiv_id": "2406.15484v2",
    "title": "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models",
    "authors": [
      "Ze Wang",
      "Zekun Wu",
      "Xin Guan",
      "Michael Thaler",
      "Adriano Koshiyama",
      "Skylar Lu",
      "Sachin Beepath",
      "Ediz Ertekin Jr.",
      "Maria Perez-Ortiz"
    ],
    "abstract": "The use of Large Language Models (LLMs) in hiring has led to legislative\nactions to protect vulnerable demographic groups. This paper presents a novel\nframework for benchmarking hierarchical gender hiring bias in Large Language\nModels (LLMs) for resume scoring, revealing significant issues of reverse\ngender hiring bias and overdebiasing. Our contributions are fourfold: Firstly,\nwe introduce a new construct grounded in labour economics, legal principles,\nand critiques of current bias benchmarks: hiring bias can be categorized into\ntwo types: Level bias (difference in the average outcomes between demographic\ncounterfactual groups) and Spread bias (difference in the variance of outcomes\nbetween demographic counterfactual groups); Level bias can be further\nsubdivided into statistical bias (i.e. changing with non-demographic content)\nand taste-based bias (i.e. consistent regardless of non-demographic content).\nSecondly, the framework includes rigorous statistical and computational hiring\nbias metrics, such as Rank After Scoring (RAS), Rank-based Impact Ratio,\nPermutation Test, and Fixed Effects Model. Thirdly, we analyze gender hiring\nbiases in ten state-of-the-art LLMs. Seven out of ten LLMs show significant\nbiases against males in at least one industry. An industry-effect regression\nreveals that the healthcare industry is the most biased against males.\nMoreover, we found that the bias performance remains invariant with resume\ncontent for eight out of ten LLMs. This indicates that the bias performance\nmeasured in this paper might apply to other resume datasets with different\nresume qualities. Fourthly, we provide a user-friendly demo and resume dataset\nto support the adoption and practical use of the framework, which can be\ngeneralized to other social traits and tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Findings Paper",
    "pdf_url": "http://arxiv.org/pdf/2406.15484v2",
    "published_date": "2024-06-17 09:15:57 UTC",
    "updated_date": "2024-09-30 11:25:27 UTC"
  },
  {
    "arxiv_id": "2406.11345v1",
    "title": "Full-ECE: A Metric For Token-level Calibration on Large Language Models",
    "authors": [
      "Han Liu",
      "Yupeng Zhang",
      "Bingning Wang",
      "Weipeng Chen",
      "Xiaolin Hu"
    ],
    "abstract": "Deep Neural Networks (DNNs) excel in various domains but face challenges in\nproviding accurate uncertainty estimates, which are crucial for high-stakes\napplications. Large Language Models (LLMs) have recently emerged as powerful\ntools, demonstrating exceptional performance in language tasks. However,\ntraditional calibration metrics such as Expected Calibration Error (ECE) and\nclasswise-ECE (cw-ECE) are inadequate for LLMs due to their vast vocabularies,\ndata complexity, and distributional focus. To address this, we propose a novel\ncalibration concept called full calibration and introduce its corresponding\nmetric, Full-ECE. Full-ECE evaluates the entire predicted probability\ndistribution, offering a more accurate and robust measure of calibration for\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11345v1",
    "published_date": "2024-06-17 09:07:58 UTC",
    "updated_date": "2024-06-17 09:07:58 UTC"
  },
  {
    "arxiv_id": "2406.11334v1",
    "title": "Program Synthesis Benchmark for Visual Programming in XLogoOnline Environment",
    "authors": [
      "Chao Wen",
      "Jacqueline Staub",
      "Adish Singla"
    ],
    "abstract": "Large language and multimodal models have shown remarkable successes on\nvarious benchmarks focused on specific skills such as general-purpose\nprogramming, natural language understanding, math word problem-solving, and\nvisual question answering. However, it is unclear how well these models perform\non tasks that require a combination of these skills. In this paper, we curate a\nnovel program synthesis benchmark based on the XLogoOnline visual programming\nenvironment. The benchmark comprises 85 real-world tasks from the Mini-level of\nthe XLogoOnline environment, each requiring a combination of different skills\nsuch as spatial planning, basic programming, and logical reasoning. Our\nevaluation shows that current state-of-the-art models like GPT-4V and\nLlama3-70B struggle to solve these tasks, achieving only 20% and 2.35% success\nrates. Next, we develop a fine-tuning pipeline to boost the performance of\nmodels by leveraging a large-scale synthetic training dataset with over 80000\ntasks. Moreover, we showcase how emulator-driven feedback can be used to design\na curriculum over training data distribution. We showcase that a fine-tuned\nLlama3-8B drastically outperforms GPT-4V and Llama3-70B models, and provide an\nin-depth analysis of the models' expertise across different skill dimensions.\nWe will publicly release the benchmark for future research on program synthesis\nin visual programming.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11334v1",
    "published_date": "2024-06-17 08:48:02 UTC",
    "updated_date": "2024-06-17 08:48:02 UTC"
  },
  {
    "arxiv_id": "2406.11326v1",
    "title": "GitHub Copilot: the perfect Code compLeeter?",
    "authors": [
      "Ilja Siroš",
      "Dave Singelée",
      "Bart Preneel"
    ],
    "abstract": "This paper aims to evaluate GitHub Copilot's generated code quality based on\nthe LeetCode problem set using a custom automated framework. We evaluate the\nresults of Copilot for 4 programming languages: Java, C++, Python3 and Rust. We\naim to evaluate Copilot's reliability in the code generation stage, the\ncorrectness of the generated code and its dependency on the programming\nlanguage, problem's difficulty level and problem's topic. In addition to that,\nwe evaluate code's time and memory efficiency and compare it to the average\nhuman results. In total, we generate solutions for 1760 problems for each\nprogramming language and evaluate all the Copilot's suggestions for each\nproblem, resulting in over 50000 submissions to LeetCode spread over a 2-month\nperiod. We found that Copilot successfully solved most of the problems.\nHowever, Copilot was rather more successful in generating code in Java and C++\nthan in Python3 and Rust. Moreover, in case of Python3 Copilot proved to be\nrather unreliable in the code generation phase. We also discovered that\nCopilot's top-ranked suggestions are not always the best. In addition, we\nanalysed how the topic of the problem impacts the correctness rate. Finally,\nbased on statistics information from LeetCode, we can conclude that Copilot\ngenerates more efficient code than an average human.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 6 figures. Code available:\n  https://github.com/IljaSir/CopilotSolverForLeetCode",
    "pdf_url": "http://arxiv.org/pdf/2406.11326v1",
    "published_date": "2024-06-17 08:38:29 UTC",
    "updated_date": "2024-06-17 08:38:29 UTC"
  },
  {
    "arxiv_id": "2406.11925v2",
    "title": "DocCGen: Document-based Controlled Code Generation",
    "authors": [
      "Sameer Pimparkhede",
      "Mehant Kammakomati",
      "Srikanth Tamilselvam",
      "Prince Kumar",
      "Ashok Pon Kumar",
      "Pushpak Bhattacharyya"
    ],
    "abstract": "Recent developments show that Large Language Models (LLMs) produce\nstate-of-the-art performance on natural language (NL) to code generation for\nresource-rich general-purpose languages like C++, Java, and Python. However,\ntheir practical usage for structured domain-specific languages (DSLs) such as\nYAML, JSON is limited due to domain-specific schema, grammar, and\ncustomizations generally unseen by LLMs during pre-training. Efforts have been\nmade to mitigate this challenge via in-context learning through relevant\nexamples or by fine-tuning. However, it suffers from problems, such as limited\nDSL samples and prompt sensitivity but enterprises maintain good documentation\nof the DSLs. Therefore, we propose DocCGen, a framework that can leverage such\nrich knowledge by breaking the NL-to-Code generation task for structured code\nlanguages into a two-step process. First, it detects the correct libraries\nusing the library documentation that best matches the NL query. Then, it\nutilizes schema rules extracted from the documentation of these libraries to\nconstrain the decoding. We evaluate our framework for two complex structured\nlanguages, Ansible YAML and Bash command, consisting of two settings:\nOut-of-domain (OOD) and In-domain (ID). Our extensive experiments show that\nDocCGen consistently improves different-sized language models across all six\nevaluation metrics, reducing syntactic and semantic errors in structured code.\nWe plan to open-source the datasets and code to motivate research in\nconstrained code generation.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11925v2",
    "published_date": "2024-06-17 08:34:57 UTC",
    "updated_date": "2024-07-03 09:16:27 UTC"
  },
  {
    "arxiv_id": "2406.11317v1",
    "title": "GUICourse: From General Vision Language Models to Versatile GUI Agents",
    "authors": [
      "Wentong Chen",
      "Junbo Cui",
      "Jinyi Hu",
      "Yujia Qin",
      "Junjie Fang",
      "Yue Zhao",
      "Chongyi Wang",
      "Jun Liu",
      "Guirong Chen",
      "Yupeng Huo",
      "Yuan Yao",
      "Yankai Lin",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Utilizing Graphic User Interface (GUI) for human-computer interaction is\nessential for accessing a wide range of digital tools. Recent advancements in\nVision Language Models (VLMs) highlight the compelling potential to develop\nversatile agents to help humans finish GUI navigation tasks. However, current\nVLMs are challenged in terms of fundamental abilities (OCR and grounding) and\nGUI knowledge (the functions and control methods of GUI elements), preventing\nthem from becoming practical GUI agents. To solve these challenges, we\ncontribute GUICourse, a suite of datasets to train visual-based GUI agents from\ngeneral VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and\ngrounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat\ndatasets to enrich their knowledge of GUI components and interactions.\nExperiments demonstrate that our GUI agents have better performance on common\nGUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B\nparameters) can still work well on single-step and multi-step GUI tasks.\nFinally, we analyze the different varieties in the training stage of this agent\nby ablation study. Our source codes and datasets are released at\nhttps://github.com/yiye3/GUICourse.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11317v1",
    "published_date": "2024-06-17 08:30:55 UTC",
    "updated_date": "2024-06-17 08:30:55 UTC"
  },
  {
    "arxiv_id": "2406.11315v1",
    "title": "Temporal Lidar Depth Completion",
    "authors": [
      "Pietari Kaskela",
      "Philipp Fischer",
      "Timo Roman"
    ],
    "abstract": "Given the lidar measurements from an autonomous vehicle, we can project the\npoints and generate a sparse depth image. Depth completion aims at increasing\nthe resolution of such a depth image by infilling and interpolating the sparse\ndepth values. Like most existing approaches, we make use of camera images as\nguidance in very sparse or occluded regions. In addition, we propose a temporal\nalgorithm that utilizes information from previous timesteps using recurrence.\nIn this work, we show how a state-of-the-art method PENet can be modified to\nbenefit from recurrency. Our algorithm achieves state-of-the-art results on the\nKITTI depth completion dataset while adding only less than one percent of\nadditional overhead in terms of both neural network parameters and floating\npoint operations. The accuracy is especially improved for faraway objects and\nregions containing a low amount of lidar depth samples. Even in regions without\nany ground truth (like sky and rooftops) we observe large improvements which\nare not captured by the existing evaluation metrics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11315v1",
    "published_date": "2024-06-17 08:25:31 UTC",
    "updated_date": "2024-06-17 08:25:31 UTC"
  },
  {
    "arxiv_id": "2406.11308v1",
    "title": "Management Decisions in Manufacturing using Causal Machine Learning -- To Rework, or not to Rework?",
    "authors": [
      "Philipp Schwarz",
      "Oliver Schacht",
      "Sven Klaassen",
      "Daniel Grünbaum",
      "Sebastian Imhof",
      "Martin Spindler"
    ],
    "abstract": "In this paper, we present a data-driven model for estimating optimal rework\npolicies in manufacturing systems. We consider a single production stage within\na multistage, lot-based system that allows for optional rework steps. While the\nrework decision depends on an intermediate state of the lot and system, the\nfinal product inspection, and thus the assessment of the actual yield, is\ndelayed until production is complete. Repair steps are applied uniformly to the\nlot, potentially improving some of the individual items while degrading others.\nThe challenge is thus to balance potential yield improvement with the rework\ncosts incurred. Given the inherently causal nature of this decision problem, we\npropose a causal model to estimate yield improvement. We apply methods from\ncausal machine learning, in particular double/debiased machine learning (DML)\ntechniques, to estimate conditional treatment effects from data and derive\npolicies for rework decisions. We validate our decision model using real-world\ndata from opto-electronic semiconductor manufacturing, achieving a yield\nimprovement of 2 - 3% during the color-conversion process of white\nlight-emitting diodes (LEDs).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "econ.EM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11308v1",
    "published_date": "2024-06-17 08:14:40 UTC",
    "updated_date": "2024-06-17 08:14:40 UTC"
  },
  {
    "arxiv_id": "2406.11303v1",
    "title": "VideoVista: A Versatile Benchmark for Video Understanding and Reasoning",
    "authors": [
      "Yunxin Li",
      "Xinyu Chen",
      "Baotian Hu",
      "Longyue Wang",
      "Haoyuan Shi",
      "Min Zhang"
    ],
    "abstract": "Despite significant breakthroughs in video analysis driven by the rapid\ndevelopment of large multimodal models (LMMs), there remains a lack of a\nversatile evaluation benchmark to comprehensively assess these models'\nperformance in video understanding and reasoning. To address this, we present\nVideoVista, a video QA benchmark that integrates challenges across diverse\ncontent categories, durations, and abilities. Specifically, VideoVista\ncomprises 25,000 questions derived from 3,400 videos spanning 14 categories\n(e.g., Howto, Film, and Entertainment) with durations ranging from a few\nseconds to over 10 minutes. Besides, it encompasses 19 types of understanding\ntasks (e.g., anomaly detection, interaction understanding) and 8 reasoning\ntasks (e.g., logical reasoning, causal reasoning). To achieve this, we present\nan automatic data construction framework, leveraging powerful GPT-4o alongside\nadvanced analysis tools (e.g., video splitting, object segmenting, and\ntracking). We also utilize this framework to construct training data to enhance\nthe capabilities of video-related LMMs (Video-LMMs). Through a comprehensive\nand quantitative evaluation of cutting-edge models, we reveal that: 1)\nVideo-LMMs face difficulties in fine-grained video tasks involving temporal\nlocation, object tracking, and anomaly detection; 2) Video-LMMs present\ninferior logical and relation reasoning abilities; 3) Open-source Video-LMMs'\nperformance is significantly lower than GPT-4o and Gemini-1.5, lagging by 20\npoints. This highlights the crucial role VideoVista will play in advancing LMMs\nthat can accurately understand videos and perform precise reasoning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "38 pages, 44 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11303v1",
    "published_date": "2024-06-17 08:09:00 UTC",
    "updated_date": "2024-06-17 08:09:00 UTC"
  },
  {
    "arxiv_id": "2406.11301v3",
    "title": "Enhancing and Assessing Instruction-Following with Fine-Grained Instruction Variants",
    "authors": [
      "Jiuding Yang",
      "Weidong Guo",
      "Kaitong Yang",
      "Xiangyang Li",
      "Yu Xu",
      "Di Niu"
    ],
    "abstract": "The effective alignment of Large Language Models (LLMs) with precise\ninstructions is essential for their application in diverse real-world\nscenarios. Current methods focus on enhancing the diversity and complexity of\ntraining and evaluation samples, yet they fall short in accurately assessing\nLLMs' ability to follow similar instruction variants. We introduce an effective\ndata augmentation technique DeMoRecon that decomposes complex instructions into\nsimpler sub-components, modifies these, and reconstructs them into new\nvariants, thereby preserves the original instruction's context and complexity\nwhile introducing variability, which is critical for training and evaluating\nLLMs' instruction-following precision. Based on DeMoRecon, we developed the\nFGIV dataset which contains fine-grained instruction variants of 1,773 seed\ninstructions to both fine-tune and evaluate LLMs. Our findings show that LLMs\nfine-tuned with FGIV will gain significant performance boost on both ours and\ncommonly used instructions-following benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11301v3",
    "published_date": "2024-06-17 08:08:11 UTC",
    "updated_date": "2024-10-15 23:26:18 UTC"
  },
  {
    "arxiv_id": "2406.11924v1",
    "title": "Explainable assessment of financial experts' credibility by classifying social media forecasts and checking the predictions with actual market data",
    "authors": [
      "Silvia García-Méndez",
      "Francisco de Arriba-Pérez",
      "Jaime González-Gonzáleza",
      "Francisco J. González-Castaño"
    ],
    "abstract": "Social media include diverse interaction metrics related to user popularity,\nthe most evident example being the number of user followers. The latter has\nraised concerns about the credibility of the posts by the most popular\ncreators. However, most existing approaches to assess credibility in social\nmedia strictly consider this problem a binary classification, often based on a\npriori information, without checking if actual real-world facts back the users'\ncomments. In addition, they do not provide automatic explanations of their\npredictions to foster their trustworthiness. In this work, we propose a\ncredibility assessment solution for financial creators in social media that\ncombines Natural Language Processing and Machine Learning. The reputation of\nthe contributors is assessed by automatically classifying their forecasts on\nasset values by type and verifying these predictions with actual market data to\napproximate their probability of success. The outcome of this verification is a\ncontinuous credibility score instead of a binary result, an entirely novel\ncontribution by this work. Moreover, social media metrics (i.e., user context)\nare exploited by calculating their correlation with the credibility rankings,\nproviding insights on the interest of the end-users in financial posts and\ntheir forecasts (i.e., drop or rise). Finally, the system provides natural\nlanguage explanations of its decisions based on a model-agnostic analysis of\nrelevant features.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11924v1",
    "published_date": "2024-06-17 08:08:03 UTC",
    "updated_date": "2024-06-17 08:08:03 UTC"
  },
  {
    "arxiv_id": "2406.11290v1",
    "title": "Iterative Utility Judgment Framework via LLMs Inspired by Relevance in Philosophy",
    "authors": [
      "Hengran Zhang",
      "Keping Bi",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "abstract": "Utility and topical relevance are critical measures in information retrieval\n(IR), reflecting system and user perspectives, respectively. While topical\nrelevance has long been emphasized, utility is a higher standard of relevance\nand is more useful for facilitating downstream tasks, e.g., in\nRetrieval-Augmented Generation (RAG). When we incorporate utility judgments\ninto RAG, we realize that the topical relevance, utility, and answering in RAG\nare closely related to the three types of relevance that Schutz discussed from\na philosophical perspective. They are topical relevance, interpretational\nrelevance, and motivational relevance, respectively. Inspired by the dynamic\niterations of the three types of relevance, we propose an Iterative utiliTy\njudgmEnt fraMework (ITEM) to promote each step of the cycle of RAG. We\nconducted extensive experiments on multi-grade passage retrieval and factoid\nquestion-answering datasets (i.e., TREC DL, WebAP, and NQ). Experimental\nresults demonstrate significant improvements in utility judgments, ranking of\ntopical relevance, and answer generation upon representative baselines,\nincluding multiple single-shot utility judging approaches. Our code and\nbenchmark can be found at https://anonymous.4open.science/r/ITEM-B486/.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.11290v1",
    "published_date": "2024-06-17 07:52:42 UTC",
    "updated_date": "2024-06-17 07:52:42 UTC"
  },
  {
    "arxiv_id": "2406.11282v1",
    "title": "From Pixels to Progress: Generating Road Network from Satellite Imagery for Socioeconomic Insights in Impoverished Areas",
    "authors": [
      "Yanxin Xi",
      "Yu Liu",
      "Zhicheng Liu",
      "Sasu Tarkoma",
      "Pan Hui",
      "Yong Li"
    ],
    "abstract": "The Sustainable Development Goals (SDGs) aim to resolve societal challenges,\nsuch as eradicating poverty and improving the lives of vulnerable populations\nin impoverished areas. Those areas rely on road infrastructure construction to\npromote accessibility and economic development. Although publicly available\ndata like OpenStreetMap is available to monitor road status, data completeness\nin impoverished areas is limited. Meanwhile, the development of deep learning\ntechniques and satellite imagery shows excellent potential for earth\nmonitoring. To tackle the challenge of road network assessment in impoverished\nareas, we develop a systematic road extraction framework combining an\nencoder-decoder architecture and morphological operations on satellite imagery,\noffering an integrated workflow for interdisciplinary researchers. Extensive\nexperiments of road network extraction on real-world data in impoverished\nregions achieve a 42.7% enhancement in the F1-score over the baseline methods\nand reconstruct about 80% of the actual roads. We also propose a comprehensive\nroad network dataset covering approximately 794,178 km2 area and 17.048 million\npeople in 382 impoverished counties in China. The generated dataset is further\nutilized to conduct socioeconomic analysis in impoverished counties, showing\nthat road network construction positively impacts regional economic\ndevelopment. The technical appendix, code, and generated dataset can be found\nat\nhttps://github.com/tsinghua-fib-lab/Road_network_extraction_impoverished_counties.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 13 figures, IJCAI2024 (AI and Social Good)",
    "pdf_url": "http://arxiv.org/pdf/2406.11282v1",
    "published_date": "2024-06-17 07:40:13 UTC",
    "updated_date": "2024-06-17 07:40:13 UTC"
  },
  {
    "arxiv_id": "2406.11921v1",
    "title": "Rethinking Spatio-Temporal Transformer for Traffic Prediction:Multi-level Multi-view Augmented Learning Framework",
    "authors": [
      "Jiaqi Lin",
      "Qianqian Ren"
    ],
    "abstract": "Traffic prediction is a challenging spatio-temporal forecasting problem that\ninvolves highly complex spatio-temporal correlations. This paper proposes a\nMulti-level Multi-view Augmented Spatio-temporal Transformer (LVSTformer) for\ntraffic prediction. The model aims to capture spatial dependencies from three\ndifferent levels: local geographic, global semantic, and pivotal nodes, along\nwith long- and short-term temporal dependencies. Specifically, we design three\nspatial augmented views to delve into the spatial information from the\nperspectives of local, global, and pivotal nodes. By combining three spatial\naugmented views with three parallel spatial self-attention mechanisms, the\nmodel can comprehensively captures spatial dependencies at different levels. We\ndesign a gated temporal self-attention mechanism to effectively capture long-\nand short-term temporal dependencies. Furthermore, a spatio-temporal context\nbroadcasting module is introduced between two spatio-temporal layers to ensure\na well-distributed allocation of attention scores, alleviating overfitting and\ninformation loss, and enhancing the generalization ability and robustness of\nthe model. A comprehensive set of experiments is conducted on six well-known\ntraffic benchmarks, the experimental results demonstrate that LVSTformer\nachieves state-of-the-art performance compared to competing baselines, with the\nmaximum improvement reaching up to 4.32%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11921v1",
    "published_date": "2024-06-17 07:36:57 UTC",
    "updated_date": "2024-06-17 07:36:57 UTC"
  },
  {
    "arxiv_id": "2406.11920v3",
    "title": "Job-SDF: A Multi-Granularity Dataset for Job Skill Demand Forecasting and Benchmarking",
    "authors": [
      "Xi Chen",
      "Chuan Qin",
      "Chuyu Fang",
      "Chao Wang",
      "Chen Zhu",
      "Fuzhen Zhuang",
      "Hengshu Zhu",
      "Hui Xiong"
    ],
    "abstract": "In a rapidly evolving job market, skill demand forecasting is crucial as it\nenables policymakers and businesses to anticipate and adapt to changes,\nensuring that workforce skills align with market needs, thereby enhancing\nproductivity and competitiveness. Additionally, by identifying emerging skill\nrequirements, it directs individuals towards relevant training and education\nopportunities, promoting continuous self-learning and development. However, the\nabsence of comprehensive datasets presents a significant challenge, impeding\nresearch and the advancement of this field. To bridge this gap, we present\nJob-SDF, a dataset designed to train and benchmark job-skill demand forecasting\nmodels. Based on 10.35 million public job advertisements collected from major\nonline recruitment platforms in China between 2021 and 2023, this dataset\nencompasses monthly recruitment demand for 2,324 types of skills across 521\ncompanies. Our dataset uniquely enables evaluating skill demand forecasting\nmodels at various granularities, including occupation, company, and regional\nlevels. We benchmark a range of models on this dataset, evaluating their\nperformance in standard scenarios, in predictions focused on lower value\nranges, and in the presence of structural breaks, providing new insights for\nfurther research. Our code and dataset are publicly accessible via the\nhttps://github.com/Job-SDF/benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2406.11920v3",
    "published_date": "2024-06-17 07:22:51 UTC",
    "updated_date": "2024-12-01 03:49:57 UTC"
  },
  {
    "arxiv_id": "2406.11272v1",
    "title": "Development of an Adaptive Multi-Domain Artificial Intelligence System Built using Machine Learning and Expert Systems Technologies",
    "authors": [
      "Jeremy Straub"
    ],
    "abstract": "Producing an artificial general intelligence (AGI) has been an elusive goal\nin artificial intelligence (AI) research for some time. An AGI would have the\ncapability, like a human, to be exposed to a new problem domain, learn about it\nand then use reasoning processes to make decisions. While AI techniques have\nbeen used across a wide variety of problem domains, an AGI would require an AI\nthat could reason beyond its programming and training. This paper presents a\nsmall step towards producing an AGI. It describes a mechanism for an AI to\nlearn about and develop reasoning pathways to make decisions in an a priori\nunknown domain. It combines a classical AI technique, the expert system, with a\nits modern adaptation - the gradient descent trained expert system (GDTES) -\nand utilizes generative artificial intelligence (GAI) to create a network and\ntraining data set for this system. These can be created from available sources\nor may draw upon knowledge incorporated in a GAI's own pre-trained model. The\nlearning process in GDTES is used to optimize the AI's decision-making. While\nthis approach does not meet the standards that many have defined for an AGI, it\nprovides a somewhat similar capability, albeit one which requires a learning\nprocess before use.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11272v1",
    "published_date": "2024-06-17 07:21:44 UTC",
    "updated_date": "2024-06-17 07:21:44 UTC"
  },
  {
    "arxiv_id": "2406.11263v2",
    "title": "Understanding the Collapse of LLMs in Model Editing",
    "authors": [
      "Wanli Yang",
      "Fei Sun",
      "Jiajun Tan",
      "Xinyu Ma",
      "Du Su",
      "Dawei Yin",
      "Huawei Shen"
    ],
    "abstract": "Despite significant progress in model editing methods, their application in\nreal-world scenarios remains challenging as they often cause large language\nmodels (LLMs) to collapse. Among them, ROME is particularly concerning, as it\ncould disrupt LLMs with only a single edit. In this paper, we study the root\ncauses of such collapse. Through extensive analysis, we identify two primary\nfactors that contribute to the collapse: i) inconsistent handling of prefixed\nand unprefixed keys in the parameter update equation may result in very small\ndenominators, causing excessively large parameter updates; ii) the subject of\ncollapse cases is usually the first token, whose unprefixed key distribution\nsignificantly differs from the prefixed key distribution in autoregressive\ntransformers, causing the aforementioned issue to materialize. To validate our\nfindings, we propose a simple yet effective approach: uniformly using prefixed\nkeys during editing phase and adding prefixes during testing phase to ensure\nthe consistency between training and testing. The experimental results show\nthat the proposed solution can prevent model collapse while maintaining the\neffectiveness of the edits.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Findings of EMNLP 2024 (Camera-Ready Version)",
    "pdf_url": "http://arxiv.org/pdf/2406.11263v2",
    "published_date": "2024-06-17 07:08:29 UTC",
    "updated_date": "2024-09-30 06:37:03 UTC"
  },
  {
    "arxiv_id": "2407.00063v2",
    "title": "An Interpretable Alternative to Neural Representation Learning for Rating Prediction -- Transparent Latent Class Modeling of User Reviews",
    "authors": [
      "Giuseppe Serra",
      "Peter Tino",
      "Zhao Xu",
      "Xin Yao"
    ],
    "abstract": "Nowadays, neural network (NN) and deep learning (DL) techniques are widely\nadopted in many applications, including recommender systems. Given the sparse\nand stochastic nature of collaborative filtering (CF) data, recent works have\ncritically analyzed the effective improvement of neural-based approaches\ncompared to simpler and often transparent algorithms for recommendation.\nPrevious results showed that NN and DL models can be outperformed by\ntraditional algorithms in many tasks. Moreover, given the largely black-box\nnature of neural-based methods, interpretable results are not naturally\nobtained. Following on this debate, we first present a transparent\nprobabilistic model that topologically organizes user and product latent\nclasses based on the review information. In contrast to popular neural\ntechniques for representation learning, we readily obtain a statistical,\nvisualization-friendly tool that can be easily inspected to understand user and\nproduct characteristics from a textual-based perspective. Then, given the\nlimitations of common embedding techniques, we investigate the possibility of\nusing the estimated interpretable quantities as model input for a rating\nprediction task. To contribute to the recent debates, we evaluate our results\nin terms of both capacity for interpretability and predictive performances in\ncomparison with popular text-based neural approaches. The results demonstrate\nthat the proposed latent class representations can yield competitive predictive\nperformances, compared to popular, but difficult-to-interpret approaches.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00063v2",
    "published_date": "2024-06-17 07:07:42 UTC",
    "updated_date": "2024-07-02 11:17:45 UTC"
  },
  {
    "arxiv_id": "2406.11260v3",
    "title": "Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection",
    "authors": [
      "Sungwon Park",
      "Sungwon Han",
      "Xing Xie",
      "Jae-Gil Lee",
      "Meeyoung Cha"
    ],
    "abstract": "The spread of fake news harms individuals and presents a critical social\nchallenge that must be addressed. Although numerous algorithmic and insightful\nfeatures have been developed to detect fake news, many of these features can be\nmanipulated with style-conversion attacks, especially with the emergence of\nadvanced language models, making it more difficult to differentiate from\ngenuine news. This study proposes adversarial style augmentation, AdStyle,\ndesigned to train a fake news detector that remains robust against various\nstyle-conversion attacks. The primary mechanism involves the strategic use of\nLLMs to automatically generate a diverse and coherent array of style-conversion\nattack prompts, enhancing the generation of particularly challenging prompts\nfor the detector. Experiments indicate that our augmentation strategy\nsignificantly improves robustness and detection performance when evaluated on\nfake news benchmark datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "WWW'25 research track accepted",
    "pdf_url": "http://arxiv.org/pdf/2406.11260v3",
    "published_date": "2024-06-17 07:00:41 UTC",
    "updated_date": "2025-04-18 07:15:33 UTC"
  },
  {
    "arxiv_id": "2406.11259v1",
    "title": "NLDF: Neural Light Dynamic Fields for Efficient 3D Talking Head Generation",
    "authors": [
      "Niu Guanchen"
    ],
    "abstract": "Talking head generation based on the neural radiation fields model has shown\npromising visual effects. However, the slow rendering speed of NeRF seriously\nlimits its application, due to the burdensome calculation process over hundreds\nof sampled points to synthesize one pixel. In this work, a novel Neural Light\nDynamic Fields model is proposed aiming to achieve generating high quality 3D\ntalking face with significant speedup. The NLDF represents light fields based\non light segments, and a deep network is used to learn the entire light beam's\ninformation at once. In learning the knowledge distillation is applied and the\nNeRF based synthesized result is used to guide the correct coloration of light\nsegments in NLDF. Furthermore, a novel active pool training strategy is\nproposed to focus on high frequency movements, particularly on the speaker\nmouth and eyebrows. The propose method effectively represents the facial light\ndynamics in 3D talking video generation, and it achieves approximately 30 times\nfaster speed compared to state of the art NeRF based method, with comparable\ngeneration visual quality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11259v1",
    "published_date": "2024-06-17 06:53:37 UTC",
    "updated_date": "2024-06-17 06:53:37 UTC"
  },
  {
    "arxiv_id": "2406.11255v1",
    "title": "Liberal Entity Matching as a Compound AI Toolchain",
    "authors": [
      "Silvery D. Fu",
      "David Wang",
      "Wen Zhang",
      "Kathleen Ge"
    ],
    "abstract": "Entity matching (EM), the task of identifying whether two descriptions refer\nto the same entity, is essential in data management. Traditional methods have\nevolved from rule-based to AI-driven approaches, yet current techniques using\nlarge language models (LLMs) often fall short due to their reliance on static\nknowledge and rigid, predefined prompts. In this paper, we introduce Libem, a\ncompound AI system designed to address these limitations by incorporating a\nflexible, tool-oriented approach. Libem supports entity matching through\ndynamic tool use, self-refinement, and optimization, allowing it to adapt and\nrefine its process based on the dataset and performance metrics. Unlike\ntraditional solo-AI EM systems, which often suffer from a lack of modularity\nthat hinders iterative design improvements and system optimization, Libem\noffers a composable and reusable toolchain. This approach aims to contribute to\nongoing discussions and developments in AI-driven data management.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.DB",
    "comment": "2 pages, compound ai systems 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11255v1",
    "published_date": "2024-06-17 06:33:34 UTC",
    "updated_date": "2024-06-17 06:33:34 UTC"
  },
  {
    "arxiv_id": "2406.11249v1",
    "title": "Relational Learning in Pre-Trained Models: A Theory from Hypergraph Recovery Perspective",
    "authors": [
      "Yang Chen",
      "Cong Fang",
      "Zhouchen Lin",
      "Bing Liu"
    ],
    "abstract": "Foundation Models (FMs) have demonstrated remarkable insights into the\nrelational dynamics of the world, leading to the crucial question: how do these\nmodels acquire an understanding of world hybrid relations? Traditional\nstatistical learning, particularly for prediction problems, may overlook the\nrich and inherently structured information from the data, especially regarding\nthe relationships between objects. We introduce a mathematical model that\nformalizes relational learning as hypergraph recovery to study pre-training of\nFMs. In our framework, the world is represented as a hypergraph, with data\nabstracted as random samples from hyperedges. We theoretically examine the\nfeasibility of a Pre-Trained Model (PTM) to recover this hypergraph and analyze\nthe data efficiency in a minimax near-optimal style. By integrating rich graph\ntheories into the realm of PTMs, our mathematical framework offers powerful\ntools for an in-depth understanding of pre-training from a unique perspective\nand can be used under various scenarios. As an example, we extend the framework\nto entity alignment in multimodal learning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11249v1",
    "published_date": "2024-06-17 06:20:39 UTC",
    "updated_date": "2024-06-17 06:20:39 UTC"
  },
  {
    "arxiv_id": "2406.11248v2",
    "title": "Performance Improvement of Language-Queried Audio Source Separation Based on Caption Augmentation From Large Language Models for DCASE Challenge 2024 Task 9",
    "authors": [
      "Do Hyun Lee",
      "Yoonah Song",
      "Hong Kook Kim"
    ],
    "abstract": "We present a prompt-engineering-based text-augmentation approach applied to a\nlanguage-queried audio source separation (LASS) task. To enhance the\nperformance of LASS, the proposed approach utilizes large language models\n(LLMs) to generate multiple captions corresponding to each sentence of the\ntraining dataset. To this end, we first perform experiments to identify the\nmost effective prompts for caption augmentation with a smaller number of\ncaptions. A LASS model trained with these augmented captions demonstrates\nimproved performance on the DCASE 2024 Task 9 validation set compared to that\ntrained without augmentation. This study highlights the effectiveness of\nLLM-based caption augmentation in advancing language-queried audio source\nseparation.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "DCASE 2024 Challenge Task 9, 4 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.11248v2",
    "published_date": "2024-06-17 06:19:14 UTC",
    "updated_date": "2024-11-27 02:17:54 UTC"
  },
  {
    "arxiv_id": "2406.11244v1",
    "title": "SpoT-Mamba: Learning Long-Range Dependency on Spatio-Temporal Graphs with Selective State Spaces",
    "authors": [
      "Jinhyeok Choi",
      "Heehyeon Kim",
      "Minhyeong An",
      "Joyce Jiyoung Whang"
    ],
    "abstract": "Spatio-temporal graph (STG) forecasting is a critical task with extensive\napplications in the real world, including traffic and weather forecasting.\nAlthough several recent methods have been proposed to model complex dynamics in\nSTGs, addressing long-range spatio-temporal dependencies remains a significant\nchallenge, leading to limited performance gains. Inspired by a recently\nproposed state space model named Mamba, which has shown remarkable capability\nof capturing long-range dependency, we propose a new STG forecasting framework\nnamed SpoT-Mamba. SpoT-Mamba generates node embeddings by scanning various\nnode-specific walk sequences. Based on the node embeddings, it conducts\ntemporal scans to capture long-range spatio-temporal dependencies. Experimental\nresults on the real-world traffic forecasting dataset demonstrate the\neffectiveness of SpoT-Mamba.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 2 figures, 3 tables. Spatio-Temporal Reasoning and Learning\n  (STRL) Workshop at the 33rd International Joint Conference on Artificial\n  Intelligence (IJCAI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.11244v1",
    "published_date": "2024-06-17 06:15:31 UTC",
    "updated_date": "2024-06-17 06:15:31 UTC"
  },
  {
    "arxiv_id": "2406.11243v1",
    "title": "FamiCom: Further Demystifying Prompts for Language Models with Task-Agnostic Performance Estimation",
    "authors": [
      "Bangzheng Li",
      "Ben Zhou",
      "Xingyu Fu",
      "Fei Wang",
      "Dan Roth",
      "Muhao Chen"
    ],
    "abstract": "Language models have shown impressive in-context-learning capabilities, which\nallow them to benefit from input prompts and perform better on downstream end\ntasks. Existing works investigate the mechanisms behind this observation, and\npropose label-agnostic prompt metrics that can better estimate end-task\nperformances. One popular approach is using perplexity as a way to measure\nmodels' familiarity with the prompt. While showing consistent improvements on\nin-domain tasks, we found that familiarity metrics such as perplexity cannot\naccurately estimate performance in complicated situations such as task or\ndomain transferring scenarios. In this work, we propose a revised measure\ncalled FamiCom, providing a more comprehensive measure for task-agnostic\nperformance estimation. Specifically, FamiCom combines familiarity with\n\\textit{complexity} -- the inherent difficulty of end tasks, which is an\nimportant factor missing from current metrics. Experiments show that FamiCom\nstrongly correlates with end-task performances, producing a 0.85 Spearman's\ncorrelation, versus 0.43 of familiarity-only ones'. We further apply FamiCom to\nautomatic prompt and demonstration selection, and outperform existing methods\nand baselines by more than 7.0% in accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11243v1",
    "published_date": "2024-06-17 06:14:55 UTC",
    "updated_date": "2024-06-17 06:14:55 UTC"
  },
  {
    "arxiv_id": "2406.11240v1",
    "title": "The Benefits of Power Regularization in Cooperative Reinforcement Learning",
    "authors": [
      "Michelle Li",
      "Michael Dennis"
    ],
    "abstract": "Cooperative Multi-Agent Reinforcement Learning (MARL) algorithms, trained\nonly to optimize task reward, can lead to a concentration of power where the\nfailure or adversarial intent of a single agent could decimate the reward of\nevery agent in the system. In the context of teams of people, it is often\nuseful to explicitly consider how power is distributed to ensure no person\nbecomes a single point of failure. Here, we argue that explicitly regularizing\nthe concentration of power in cooperative RL systems can result in systems\nwhich are more robust to single agent failure, adversarial attacks, and\nincentive changes of co-players. To this end, we define a practical pairwise\nmeasure of power that captures the ability of any co-player to influence the\nego agent's reward, and then propose a power-regularized objective which\nbalances task reward and power concentration. Given this new objective, we show\nthat there always exists an equilibrium where every agent is playing a\npower-regularized best-response balancing power and task reward. Moreover, we\npresent two algorithms for training agents towards this power-regularized\nobjective: Sample Based Power Regularization (SBPR), which injects adversarial\ndata during training; and Power Regularization via Intrinsic Motivation (PRIM),\nwhich adds an intrinsic motivation to regulate power to the training objective.\nOur experiments demonstrate that both algorithms successfully balance task\nreward and power, leading to lower power behavior than the baseline of\ntask-only reward and avoid catastrophic events in case an agent in the system\ngoes off-policy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11240v1",
    "published_date": "2024-06-17 06:10:37 UTC",
    "updated_date": "2024-06-17 06:10:37 UTC"
  },
  {
    "arxiv_id": "2406.15481v2",
    "title": "Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding",
    "authors": [
      "Haneul Yoo",
      "Yongjin Yang",
      "Hwaran Lee"
    ],
    "abstract": "As large language models (LLMs) have advanced rapidly, concerns regarding\ntheir safety have become prominent. In this paper, we discover that\ncode-switching in red-teaming queries can effectively elicit undesirable\nbehaviors of LLMs, which are common practices in natural language. We introduce\na simple yet effective framework, CSRT, to synthesize code-switching\nred-teaming queries and investigate the safety and multilingual understanding\nof LLMs comprehensively. Through extensive experiments with ten\nstate-of-the-art LLMs and code-switching queries combining up to 10 languages,\nwe demonstrate that the CSRT significantly outperforms existing multilingual\nred-teaming techniques, achieving 46.7% more attacks than standard attacks in\nEnglish and being effective in conventional safety domains. We also examine the\nmultilingual ability of those LLMs to generate and understand code-switching\ntexts. Additionally, we validate the extensibility of the CSRT by generating\ncode-switching attack prompts with monolingual data. We finally conduct\ndetailed ablation studies exploring code-switching and propound unintended\ncorrelation between resource availability of languages and safety alignment in\nexisting multilingual LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15481v2",
    "published_date": "2024-06-17 06:08:18 UTC",
    "updated_date": "2024-11-02 06:21:44 UTC"
  },
  {
    "arxiv_id": "2406.11239v3",
    "title": "SilverSpeak: Evading AI-Generated Text Detectors using Homoglyphs",
    "authors": [
      "Aldan Creo",
      "Shushanta Pudasaini"
    ],
    "abstract": "The advent of Large Language Models (LLMs) has enabled the generation of text\nthat increasingly exhibits human-like characteristics. As the detection of such\ncontent is of significant importance, substantial research has been conducted\nwith the objective of developing reliable AI-generated text detectors. These\ndetectors have demonstrated promising results on test data, but recent research\nhas revealed that they can be circumvented by employing different techniques.\n  In this paper, we present homoglyph-based attacks (A $\\rightarrow$ Cyrillic\nA) as a means of circumventing existing detectors. We conduct a comprehensive\nevaluation to assess the effectiveness of these attacks on seven detectors,\nincluding ArguGPT, Binoculars, DetectGPT, Fast-DetectGPT, Ghostbuster, OpenAI's\ndetector, and watermarking techniques, on five different datasets. Our findings\ndemonstrate that homoglyph-based attacks can effectively circumvent\nstate-of-the-art detectors, leading them to classify all texts as either\nAI-generated or human-written (decreasing the average Matthews Correlation\nCoefficient from 0.64 to -0.01). Through further examination, we extract the\ntechnical justification underlying the success of the attacks, which varies\nacross detectors. Finally, we discuss the implications of these findings and\npotential defenses against such attacks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Workshop on Detecting AI Generated Content at COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.11239v3",
    "published_date": "2024-06-17 06:07:32 UTC",
    "updated_date": "2025-01-20 05:17:54 UTC"
  },
  {
    "arxiv_id": "2406.11234v2",
    "title": "MiniConGTS: A Near Ultimate Minimalist Contrastive Grid Tagging Scheme for Aspect Sentiment Triplet Extraction",
    "authors": [
      "Qiao Sun",
      "Liujia Yang",
      "Minghao Ma",
      "Nanyang Ye",
      "Qinying Gu"
    ],
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to co-extract the sentiment\ntriplets in a given corpus. Existing approaches within the\npretraining-finetuning paradigm tend to either meticulously craft complex\ntagging schemes and classification heads, or incorporate external semantic\naugmentation to enhance performance. In this study, we, for the first time,\nre-evaluate the redundancy in tagging schemes and the internal enhancement in\npretrained representations. We propose a method to improve and utilize\npretrained representations by integrating a minimalist tagging scheme and a\nnovel token-level contrastive learning strategy. The proposed approach\ndemonstrates comparable or superior performance compared to state-of-the-art\ntechniques while featuring a more compact design and reduced computational\noverhead. Additionally, we are the first to formally evaluate GPT-4's\nperformance in few-shot learning and Chain-of-Thought scenarios for this task.\nThe results demonstrate that the pretraining-finetuning paradigm remains highly\neffective even in the era of large language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2403.07342",
    "pdf_url": "http://arxiv.org/pdf/2406.11234v2",
    "published_date": "2024-06-17 06:01:11 UTC",
    "updated_date": "2024-09-30 18:36:02 UTC"
  },
  {
    "arxiv_id": "2406.11233v3",
    "title": "Probing the Decision Boundaries of In-context Learning in Large Language Models",
    "authors": [
      "Siyan Zhao",
      "Tung Nguyen",
      "Aditya Grover"
    ],
    "abstract": "In-context learning is a key paradigm in large language models (LLMs) that\nenables them to generalize to new tasks and domains by simply prompting these\nmodels with a few exemplars without explicit parameter updates. Many attempts\nhave been made to understand in-context learning in LLMs as a function of model\nscale, pretraining data, and other factors. In this work, we propose a new\nmechanism to probe and understand in-context learning from the lens of decision\nboundaries for in-context binary classification. Decision boundaries are\nstraightforward to visualize and provide important information about the\nqualitative behavior of the inductive biases of standard classifiers. To our\nsurprise, we find that the decision boundaries learned by current LLMs in\nsimple binary classification tasks are often irregular and non-smooth,\nregardless of linear separability in the underlying task. This paper\ninvestigates the factors influencing these decision boundaries and explores\nmethods to enhance their generalizability. We assess various approaches,\nincluding training-free and fine-tuning methods for LLMs, the impact of model\narchitecture, and the effectiveness of active prompting techniques for\nsmoothing decision boundaries in a data-efficient manner. Our findings provide\na deeper understanding of in-context learning dynamics and offer practical\nimprovements for enhancing robustness and generalizability of in-context\nlearning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024, code at\n  https://github.com/siyan-zhao/ICL_decision_boundary",
    "pdf_url": "http://arxiv.org/pdf/2406.11233v3",
    "published_date": "2024-06-17 06:00:24 UTC",
    "updated_date": "2024-12-09 23:53:27 UTC"
  },
  {
    "arxiv_id": "2406.11232v3",
    "title": "SLEGO: A Collaborative Data Analytics System with LLM Recommender for Diverse Users",
    "authors": [
      "Siu Lung Ng",
      "Hirad Baradaran Rezaei",
      "Fethi Rabhi"
    ],
    "abstract": "This paper presents the SLEGO (Software-Lego) system, a collaborative\nanalytics platform that bridges the gap between experienced developers and\nnovice users using a cloud-based platform with modular, reusable microservices.\nThese microservices enable developers to share their analytical tools and\nworkflows, while a simple graphical user interface (GUI) allows novice users to\nbuild comprehensive analytics pipelines without programming skills. Supported\nby a knowledge base and a Large Language Model (LLM) powered recommendation\nsystem, SLEGO enhances the selection and integration of microservices,\nincreasing the efficiency of analytics pipeline construction. Case studies in\nfinance and machine learning illustrate how SLEGO promotes the sharing and\nassembly of modular microservices, significantly improving resource reusability\nand team collaboration. The results highlight SLEGO's role in democratizing\ndata analytics by integrating modular design, knowledge bases, and\nrecommendation systems, fostering a more inclusive and efficient analytical\nenvironment.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "D.2.11; I.2.1"
    ],
    "primary_category": "cs.SE",
    "comment": "14 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11232v3",
    "published_date": "2024-06-17 05:59:13 UTC",
    "updated_date": "2024-12-08 04:49:43 UTC"
  },
  {
    "arxiv_id": "2406.11231v1",
    "title": "Enabling robots to follow abstract instructions and complete complex dynamic tasks",
    "authors": [
      "Ruaridh Mon-Williams",
      "Gen Li",
      "Ran Long",
      "Wenqian Du",
      "Chris Lucas"
    ],
    "abstract": "Completing complex tasks in unpredictable settings like home kitchens\nchallenges robotic systems. These challenges include interpreting high-level\nhuman commands, such as \"make me a hot beverage\" and performing actions like\npouring a precise amount of water into a moving mug. To address these\nchallenges, we present a novel framework that combines Large Language Models\n(LLMs), a curated Knowledge Base, and Integrated Force and Visual Feedback\n(IFVF). Our approach interprets abstract instructions, performs long-horizon\ntasks, and handles various uncertainties. It utilises GPT-4 to analyse the\nuser's query and surroundings, then generates code that accesses a curated\ndatabase of functions during execution. It translates abstract instructions\ninto actionable steps. Each step involves generating custom code by employing\nretrieval-augmented generalisation to pull IFVF-relevant examples from the\nKnowledge Base. IFVF allows the robot to respond to noise and disturbances\nduring execution. We use coffee making and plate decoration to demonstrate our\napproach, including components ranging from pouring to drawer opening, each\nbenefiting from distinct feedback types and methods. This novel advancement\nmarks significant progress toward a scalable, efficient robotic framework for\ncompleting complex tasks in uncertain environments. Our findings are\nillustrated in an accompanying video and supported by an open-source GitHub\nrepository (released upon paper acceptance).",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11231v1",
    "published_date": "2024-06-17 05:55:35 UTC",
    "updated_date": "2024-06-17 05:55:35 UTC"
  },
  {
    "arxiv_id": "2406.11230v2",
    "title": "Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models",
    "authors": [
      "Hengyi Wang",
      "Haizhou Shi",
      "Shiwei Tan",
      "Weiyi Qin",
      "Wenyuan Wang",
      "Tunyu Zhang",
      "Akshay Nambi",
      "Tanuja Ganu",
      "Hao Wang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have shown significant promise in\nvarious applications, leading to broad interest from researchers and\npractitioners alike. However, a comprehensive evaluation of their long-context\ncapabilities remains underexplored. To address these gaps, we introduce the\nMultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to\nassess the long-context capabilities of MLLMs. Besides multi-image input, we\nemploy image stitching to further increase the input context length, and\ndevelop a protocol to automatically generate labels for sub-image level\nretrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their\ncapability to locate a target sub-image (needle) within a set of images\n(haystack) based on textual instructions and descriptions of image contents.\nThis setup necessitates an advanced understanding of extensive visual contexts\nand effective information retrieval within long-context image inputs. With this\nbenchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and\nopen-source models. The findings reveal that GPT-4o consistently surpasses\nother models in long-context scenarios, but suffers from hallucination problems\nin negative samples, i.e., when needles are not in the haystacks. Our\ncomprehensive long-context evaluation of MLLMs also sheds lights on the\nconsiderable performance gap between API-based and open-source models. All the\ncode, data, and instructions required to reproduce the main results are\navailable at https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NAACL 2025 Main",
    "pdf_url": "http://arxiv.org/pdf/2406.11230v2",
    "published_date": "2024-06-17 05:54:06 UTC",
    "updated_date": "2025-02-11 02:17:24 UTC"
  },
  {
    "arxiv_id": "2406.11227v1",
    "title": "Compound Schema Registry",
    "authors": [
      "Silvery D. Fu",
      "Xuewei Chen"
    ],
    "abstract": "Schema evolution is critical in managing database systems to ensure\ncompatibility across different data versions. A schema registry typically\naddresses the challenges of schema evolution in real-time data streaming by\nmanaging, validating, and ensuring schema compatibility. However, current\nschema registries struggle with complex syntactic alterations like field\nrenaming or type changes, which often require significant manual intervention\nand can disrupt service. To enhance the flexibility of schema evolution, we\npropose the use of generalized schema evolution (GSE) facilitated by a compound\nAI system. This system employs Large Language Models (LLMs) to interpret the\nsemantics of schema changes, supporting a broader range of syntactic\nmodifications without interrupting data streams. Our approach includes\ndeveloping a task-specific language, Schema Transformation Language (STL), to\ngenerate schema mappings as an intermediate representation (IR), simplifying\nthe integration of schema changes across different data processing platforms.\nInitial results indicate that this approach can improve schema mapping accuracy\nand efficiency, demonstrating the potential of GSE in practical applications.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "2 pages, compound ai system workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11227v1",
    "published_date": "2024-06-17 05:50:46 UTC",
    "updated_date": "2024-06-17 05:50:46 UTC"
  },
  {
    "arxiv_id": "2406.11218v1",
    "title": "Building another Spanish dictionary, this time with GPT-4",
    "authors": [
      "Miguel Ortega-Martín",
      "Óscar García-Sierra",
      "Alfonso Ardoiz",
      "Juan Carlos Armenteros",
      "Ignacio Garrido",
      "Jorge Álvarez",
      "Camilo Torrón",
      "Iñigo Galdeano",
      "Ignacio Arranz",
      "Oleg Vorontsov",
      "Adrián Alonso"
    ],
    "abstract": "We present the \"Spanish Built Factual Freectianary 2.0\" (Spanish-BFF-2) as\nthe second iteration of an AI-generated Spanish dictionary. Previously, we\ndeveloped the inaugural version of this unique free dictionary employing GPT-3.\nIn this study, we aim to improve the dictionary by using GPT-4-turbo instead.\nFurthermore, we explore improvements made to the initial version and compare\nthe performance of both models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11218v1",
    "published_date": "2024-06-17 05:25:56 UTC",
    "updated_date": "2024-06-17 05:25:56 UTC"
  },
  {
    "arxiv_id": "2406.11217v2",
    "title": "WeatherQA: Can Multimodal Language Models Reason about Severe Weather?",
    "authors": [
      "Chengqian Ma",
      "Zhanxiang Hua",
      "Alexandra Anderson-Frey",
      "Vikram Iyer",
      "Xin Liu",
      "Lianhui Qin"
    ],
    "abstract": "Severe convective weather events, such as hail, tornadoes, and thunderstorms,\noften occur quickly yet cause significant damage, costing billions of dollars\nevery year. This highlights the importance of forecasting severe weather\nthreats hours in advance to better prepare meteorologists and residents in\nat-risk areas. Can modern large foundation models perform such forecasting?\nExisting weather benchmarks typically focus only on predicting time-series\nchanges in certain weather parameters (e.g., temperature, moisture) with\ntext-only features. In this work, we introduce WeatherQA, the first multimodal\ndataset designed for machines to reason about complex combinations of weather\nparameters (a.k.a., ingredients) and predict severe weather in real-world\nscenarios. The dataset includes over 8,000 (multi-images, text) pairs for\ndiverse severe weather events. Each pair contains rich information crucial for\nforecasting -- the images describe the ingredients capturing environmental\ninstability, surface observations, and radar reflectivity, and the text\ncontains forecast analyses written by human experts. With WeatherQA, we\nevaluate state-of-the-art vision language models, including GPT4, Claude3.5,\nGemini-1.5, and a fine-tuned Llama3-based VLM, by designing two challenging\ntasks: (1) multi-choice QA for predicting affected area and (2) classification\nof the development potential of severe convection. These tasks require deep\nunderstanding of domain knowledge (e.g., atmospheric dynamics) and complex\nreasoning over multimodal data (e.g., interactions between weather parameters).\nWe show a substantial gap between the strongest VLM, GPT4o, and human\nreasoning. Our comprehensive case study with meteorologists further reveals the\nweaknesses of the models, suggesting that better training and data integration\nare necessary to bridge this gap. WeatherQA link:\nhttps://github.com/chengqianma/WeatherQA.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "physics.ao-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11217v2",
    "published_date": "2024-06-17 05:23:18 UTC",
    "updated_date": "2024-06-24 03:55:30 UTC"
  },
  {
    "arxiv_id": "2406.11201v2",
    "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
    "authors": [
      "Scott Barnett",
      "Zac Brannelly",
      "Stefanus Kurniawan",
      "Sheng Wong"
    ],
    "abstract": "Large Language Models (LLMs) have the unique capability to understand and\ngenerate human-like text from input queries. When fine-tuned, these models show\nenhanced performance on domain-specific queries. OpenAI highlights the process\nof fine-tuning, stating: \"To fine-tune a model, you are required to provide at\nleast 10 examples. We typically see clear improvements from fine-tuning on 50\nto 100 training examples, but the right number varies greatly based on the\nexact use case.\" This study extends this concept to the integration of LLMs\nwithin Retrieval-Augmented Generation (RAG) pipelines, which aim to improve\naccuracy and relevance by leveraging external corpus data for information\nretrieval. However, RAG's promise of delivering optimal responses often falls\nshort in complex query scenarios. This study aims to specifically examine the\neffects of fine-tuning LLMs on their ability to extract and integrate\ncontextual data to enhance the performance of RAG systems across multiple\ndomains. We evaluate the impact of fine-tuning on the LLMs' capacity for data\nextraction and contextual understanding by comparing the accuracy and\ncompleteness of fine-tuned models against baseline performances across datasets\nfrom multiple domains. Our findings indicate that fine-tuning resulted in a\ndecline in performance compared to the baseline models, contrary to the\nimprovements observed in standalone LLM applications as suggested by OpenAI.\nThis study highlights the need for vigorous investigation and validation of\nfine-tuned models for domain-specific tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11201v2",
    "published_date": "2024-06-17 04:35:17 UTC",
    "updated_date": "2024-06-30 14:42:52 UTC"
  },
  {
    "arxiv_id": "2406.11919v2",
    "title": "Graph Knowledge Distillation to Mixture of Experts",
    "authors": [
      "Pavel Rumiantsev",
      "Mark Coates"
    ],
    "abstract": "In terms of accuracy, Graph Neural Networks (GNNs) are the best architectural\nchoice for the node classification task. Their drawback in real-world\ndeployment is the latency that emerges from the neighbourhood processing\noperation. One solution to the latency issue is to perform knowledge\ndistillation from a trained GNN to a Multi-Layer Perceptron (MLP), where the\nMLP processes only the features of the node being classified (and possibly some\npre-computed structural information). However, the performance of such MLPs in\nboth transductive and inductive settings remains inconsistent for existing\nknowledge distillation techniques. We propose to address the performance\nconcerns by using a specially-designed student model instead of an MLP. Our\nmodel, named Routing-by-Memory (RbM), is a form of Mixture-of-Experts (MoE),\nwith a design that enforces expert specialization. By encouraging each expert\nto specialize on a certain region on the hidden representation space, we\ndemonstrate experimentally that it is possible to derive considerably more\nconsistent performance across multiple datasets. Code available at\nhttps://github.com/Rufaim/routing-by-memory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11919v2",
    "published_date": "2024-06-17 04:00:41 UTC",
    "updated_date": "2024-11-21 05:24:27 UTC"
  },
  {
    "arxiv_id": "2406.11190v1",
    "title": "Aligning Large Language Models from Self-Reference AI Feedback with one General Principle",
    "authors": [
      "Rong Bao",
      "Rui Zheng",
      "Shihan Dou",
      "Xiao Wang",
      "Enyu Zhou",
      "Bo Wang",
      "Qi Zhang",
      "Liang Ding",
      "Dacheng Tao"
    ],
    "abstract": "In aligning large language models (LLMs), utilizing feedback from existing\nadvanced AI rather than humans is an important method to scale supervisory\nsignals. However, it is highly challenging for AI to understand human\nintentions and societal values, and provide accurate preference feedback based\non these. Current AI feedback methods rely on powerful LLMs, carefully designed\nspecific principles to describe human intentions, and are easily influenced by\nposition bias. To address these issues, we propose a self-reference-based AI\nfeedback framework that enables a 13B Llama2-Chat to provide high-quality\nfeedback under simple and general principles such as ``best for humanity``.\nSpecifically, we allow the AI to first respond to the user's instructions, then\ngenerate criticism of other answers based on its own response as a reference,\nand finally determine which answer better fits human preferences according to\nthe criticism. Additionally, we use a self-consistency method to further reduce\nthe impact of position bias, and employ semantic perplexity to calculate the\npreference strength differences between different answers. Experimental results\nshow that our method enables 13B and 70B Llama2-Chat annotators to provide\nhigh-quality preference feedback, and the policy models trained based on these\npreference data achieve significant advantages in benchmark datasets through\nreinforcement learning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11190v1",
    "published_date": "2024-06-17 03:51:46 UTC",
    "updated_date": "2024-06-17 03:51:46 UTC"
  },
  {
    "arxiv_id": "2406.11179v1",
    "title": "Learning Iterative Reasoning through Energy Diffusion",
    "authors": [
      "Yilun Du",
      "Jiayuan Mao",
      "Joshua B. Tenenbaum"
    ],
    "abstract": "We introduce iterative reasoning through energy diffusion (IRED), a novel\nframework for learning to reason for a variety of tasks by formulating\nreasoning and decision-making problems with energy-based optimization. IRED\nlearns energy functions to represent the constraints between input conditions\nand desired outputs. After training, IRED adapts the number of optimization\nsteps during inference based on problem difficulty, enabling it to solve\nproblems outside its training distribution -- such as more complex Sudoku\npuzzles, matrix completion with large value magnitudes, and pathfinding in\nlarger graphs. Key to our method's success is two novel techniques: learning a\nsequence of annealed energy landscapes for easier inference and a combination\nof score function and energy landscape supervision for faster and more stable\ntraining. Our experiments show that IRED outperforms existing methods in\ncontinuous-space reasoning, discrete-space reasoning, and planning tasks,\nparticularly in more challenging scenarios. Code and visualizations at\nhttps://energy-based-model.github.io/ired/",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024, website: https://energy-based-model.github.io/ired/",
    "pdf_url": "http://arxiv.org/pdf/2406.11179v1",
    "published_date": "2024-06-17 03:36:47 UTC",
    "updated_date": "2024-06-17 03:36:47 UTC"
  },
  {
    "arxiv_id": "2406.11176v2",
    "title": "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement",
    "authors": [
      "Weimin Xiong",
      "Yifan Song",
      "Xiutian Zhao",
      "Wenhao Wu",
      "Xun Wang",
      "Ke Wang",
      "Cheng Li",
      "Wei Peng",
      "Sujian Li"
    ],
    "abstract": "Large language model agents have exhibited exceptional performance across a\nrange of complex interactive tasks. Recent approaches have utilized tuning with\nexpert trajectories to enhance agent performance, yet they primarily\nconcentrate on outcome rewards, which may lead to errors or suboptimal actions\ndue to the absence of process supervision signals. In this paper, we introduce\nthe Iterative step-level Process Refinement (IPR) framework, which provides\ndetailed step-by-step guidance to enhance agent training. Specifically, we\nadopt the Monte Carlo method to estimate step-level rewards. During each\niteration, the agent explores along the expert trajectory and generates new\nactions. These actions are then evaluated against the corresponding step of\nexpert trajectory using step-level rewards. Such comparison helps identify\ndiscrepancies, yielding contrastive action pairs that serve as training data\nfor the agent. Our experiments on three complex agent tasks demonstrate that\nour framework outperforms a variety of strong baselines. Moreover, our\nanalytical findings highlight the effectiveness of IPR in augmenting action\nefficiency and its applicability to diverse models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 (Main Conference)",
    "pdf_url": "http://arxiv.org/pdf/2406.11176v2",
    "published_date": "2024-06-17 03:29:13 UTC",
    "updated_date": "2024-09-24 10:01:31 UTC"
  },
  {
    "arxiv_id": "2406.15480v2",
    "title": "On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion",
    "authors": [
      "Chenghao Fan",
      "Zhenyi Lu",
      "Wei Wei",
      "Jie Tian",
      "Xiaoye Qu",
      "Dangyang Chen",
      "Yu Cheng"
    ],
    "abstract": "Efficient fine-tuning of large language models for task-specific applications\nis imperative, yet the vast number of parameters in these models makes their\ntraining increasingly challenging. Despite numerous proposals for effective\nmethods, a substantial memory overhead remains for gradient computations during\nupdates. \\thm{Can we fine-tune a series of task-specific small models and\ntransfer their knowledge directly to a much larger model without additional\ntraining?} In this paper, we explore weak-to-strong specialization using logit\narithmetic, facilitating a direct answer to this question. Existing\nweak-to-strong methods often employ a static knowledge transfer ratio and a\nsingle small model for transferring complex knowledge, which leads to\nsuboptimal performance. % To address this, To surmount these limitations, we\npropose a dynamic logit fusion approach that works with a series of\ntask-specific small models, each specialized in a different task. This method\nadaptively allocates weights among these models at each decoding step, learning\nthe weights through Kullback-Leibler divergence constrained optimization\nproblems. We conduct extensive experiments across various benchmarks in both\nsingle-task and multi-task settings, achieving leading results. By transferring\nexpertise from the 7B model to the 13B model, our method closes the performance\ngap by 96.4\\% in single-task scenarios and by 86.3\\% in multi-task scenarios\ncompared to full fine-tuning of the 13B model. Notably, we achieve surpassing\nperformance on unseen tasks. Moreover, we further demonstrate that our method\ncan effortlessly integrate in-context learning for single tasks and task\narithmetic for multi-task scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15480v2",
    "published_date": "2024-06-17 03:07:41 UTC",
    "updated_date": "2024-10-14 11:31:06 UTC"
  },
  {
    "arxiv_id": "2406.12935v2",
    "title": "ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates",
    "authors": [
      "Fengqing Jiang",
      "Zhangchen Xu",
      "Luyao Niu",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "abstract": "Large language models (LLMs) are expected to follow instructions from users\nand engage in conversations. Techniques to enhance LLMs' instruction-following\ncapabilities typically fine-tune them using data structured according to a\npredefined chat template. Although chat templates are shown to be effective in\noptimizing LLM performance, their impact on safety alignment of LLMs has been\nless understood, which is crucial for deploying LLMs safely at scale.\n  In this paper, we investigate how chat templates affect safety alignment of\nLLMs. We identify a common vulnerability, named ChatBug, that is introduced by\nchat templates. Our key insight to identify ChatBug is that the chat templates\nprovide a rigid format that need to be followed by LLMs, but not by users.\nHence, a malicious user may not necessarily follow the chat template when\nprompting LLMs. Instead, malicious users could leverage their knowledge of the\nchat template and accordingly craft their prompts to bypass safety alignments\nof LLMs. We develop two attacks to exploit the ChatBug vulnerability. We\ndemonstrate that a malicious user can exploit the ChatBug vulnerability of\neight state-of-the-art (SOTA) LLMs and effectively elicit unintended responses\nfrom these models. Moreover, we show that ChatBug can be exploited by existing\njailbreak attacks to enhance their attack success rates. We investigate\npotential countermeasures to ChatBug. Our results show that while adversarial\ntraining effectively mitigates the ChatBug vulnerability, the victim model\nincurs significant performance degradation. These results highlight the\ntrade-off between safety alignment and helpfulness. Developing new methods for\ninstruction tuning to balance this trade-off is an open and critical direction\nfor future research",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "This paper is accepted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.12935v2",
    "published_date": "2024-06-17 03:03:34 UTC",
    "updated_date": "2025-01-07 04:42:20 UTC"
  },
  {
    "arxiv_id": "2406.11161v2",
    "title": "Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning",
    "authors": [
      "Zebang Cheng",
      "Zhi-Qi Cheng",
      "Jun-Yan He",
      "Jingdong Sun",
      "Kai Wang",
      "Yuxiang Lin",
      "Zheng Lian",
      "Xiaojiang Peng",
      "Alexander Hauptmann"
    ],
    "abstract": "Accurate emotion perception is crucial for various applications, including\nhuman-computer interaction, education, and counseling. However, traditional\nsingle-modality approaches often fail to capture the complexity of real-world\nemotional expressions, which are inherently multimodal. Moreover, existing\nMultimodal Large Language Models (MLLMs) face challenges in integrating audio\nand recognizing subtle facial micro-expressions. To address this, we introduce\nthe MERR dataset, containing 28,618 coarse-grained and 4,487 fine-grained\nannotated samples across diverse emotional categories. This dataset enables\nmodels to learn from varied scenarios and generalize to real-world\napplications. Furthermore, we propose Emotion-LLaMA, a model that seamlessly\nintegrates audio, visual, and textual inputs through emotion-specific encoders.\nBy aligning features into a shared space and employing a modified LLaMA model\nwith instruction tuning, Emotion-LLaMA significantly enhances both emotional\nrecognition and reasoning capabilities. Extensive evaluations show\nEmotion-LLaMA outperforms other MLLMs, achieving top scores in Clue Overlap\n(7.83) and Label Overlap (6.25) on EMER, an F1 score of 0.9036 on MER2023-SEMI\nchallenge, and the highest UAR (45.59) and WAR (59.37) in zero-shot evaluations\non DFEW dataset.",
    "categories": [
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2024. 49 pages, 13 figures, Project:\n  https://github.com/ZebangCheng/Emotion-LLaMA, Demo:\n  https://huggingface.co/spaces/ZebangCheng/Emotion-LLaMA",
    "pdf_url": "http://arxiv.org/pdf/2406.11161v2",
    "published_date": "2024-06-17 03:01:22 UTC",
    "updated_date": "2024-11-02 02:30:50 UTC"
  },
  {
    "arxiv_id": "2406.11160v3",
    "title": "Context Graph",
    "authors": [
      "Chengjin Xu",
      "Muzhi Li",
      "Cehao Yang",
      "Xuhui Jiang",
      "Lumingyuan Tang",
      "Yiyan Qi",
      "Jian Guo"
    ],
    "abstract": "Knowledge Graphs (KGs) are foundational structures in many AI applications,\nrepresenting entities and their interrelations through triples. However,\ntriple-based KGs lack the contextual information of relational knowledge, like\ntemporal dynamics and provenance details, which are crucial for comprehensive\nknowledge representation and effective reasoning. Instead, \\textbf{Context\nGraphs} (CGs) expand upon the conventional structure by incorporating\nadditional information such as time validity, geographic location, and source\nprovenance. This integration provides a more nuanced and accurate understanding\nof knowledge, enabling KGs to offer richer insights and support more\nsophisticated reasoning processes. In this work, we first discuss the inherent\nlimitations of triple-based KGs and introduce the concept of CGs, highlighting\ntheir advantages in knowledge representation and reasoning. We then present a\ncontext graph reasoning \\textbf{CGR$^3$} paradigm that leverages large language\nmodels (LLMs) to retrieve candidate entities and related contexts, rank them\nbased on the retrieved information, and reason whether sufficient information\nhas been obtained to answer a query. Our experimental results demonstrate that\nCGR$^3$ significantly improves performance on KG completion (KGC) and KG\nquestion answering (KGQA) tasks, validating the effectiveness of incorporating\ncontextual information on KG representation and reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11160v3",
    "published_date": "2024-06-17 02:59:19 UTC",
    "updated_date": "2024-06-28 03:20:22 UTC"
  },
  {
    "arxiv_id": "2406.11156v4",
    "title": "DELRec: Distilling Sequential Pattern to Enhance LLMs-based Sequential Recommendation",
    "authors": [
      "Haoyi Zhang",
      "Guohao Sun",
      "Jinhu Lu",
      "Guanfeng Liu",
      "Xiu Susie Fang"
    ],
    "abstract": "Sequential recommendation (SR) tasks aim to predict users' next interaction\nby learning their behavior sequence and capturing the connection between users'\npast interactions and their changing preferences. Conventional SR models often\nfocus solely on capturing sequential patterns within the training data,\nneglecting the broader context and semantic information embedded in item titles\nfrom external sources. This limits their predictive power and adaptability.\nLarge language models (LLMs) have recently shown promise in SR tasks due to\ntheir advanced understanding capabilities and strong generalization abilities.\nResearchers have attempted to enhance LLMs-based recommendation performance by\nincorporating information from conventional SR models. However, previous\napproaches have encountered problems such as 1) limited textual information\nleading to poor recommendation performance, 2) incomplete understanding and\nutilization of conventional SR model information by LLMs, and 3) excessive\ncomplexity and low interpretability of LLMs-based methods. To improve the\nperformance of LLMs-based SR, we propose a novel framework, Distilling\nSequential Pattern to Enhance LLMs-based Sequential Recommendation (DELRec),\nwhich aims to extract knowledge from conventional SR models and enable LLMs to\neasily comprehend and utilize the extracted knowledge for more effective SRs.\nDELRec consists of two main stages: 1) Distill Pattern from Conventional SR\nModels, focusing on extracting behavioral patterns exhibited by conventional SR\nmodels using soft prompts through two well-designed strategies; 2) LLMs-based\nSequential Recommendation, aiming to fine-tune LLMs to effectively use the\ndistilled auxiliary information to perform SR tasks. Extensive experimental\nresults conducted on four real datasets validate the effectiveness of the\nDELRec framework.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2406.11156v4",
    "published_date": "2024-06-17 02:47:09 UTC",
    "updated_date": "2024-12-18 12:48:37 UTC"
  },
  {
    "arxiv_id": "2406.15479v2",
    "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
    "authors": [
      "Zhenyi Lu",
      "Chenghao Fan",
      "Wei Wei",
      "Xiaoye Qu",
      "Dangyang Chen",
      "Yu Cheng"
    ],
    "abstract": "In the era of large language models, model merging is a promising way to\ncombine multiple task-specific models into a single multitask model without\nextra training. However, two challenges remain: (a) interference between\ndifferent models and (b) heterogeneous data during testing. Traditional model\nmerging methods often show significant performance gaps compared to fine-tuned\nmodels due to these issues. Additionally, a one-size-fits-all model lacks\nflexibility for diverse test data, leading to performance degradation. We show\nthat both shared and exclusive task-specific knowledge are crucial for merging\nperformance, but directly merging exclusive knowledge hinders overall\nperformance. In view of this, we propose Twin-Merging, a method that\nencompasses two principal stages: (1) modularizing knowledge into shared and\nexclusive components, with compression to reduce redundancy and enhance\nefficiency; (2) dynamically merging shared and task-specific knowledge based on\nthe input. This approach narrows the performance gap between merged and\nfine-tuned models and improves adaptability to heterogeneous data. Extensive\nexperiments on $20$ datasets for both language and vision tasks demonstrate the\neffectiveness of our method, showing an average improvement of $28.34\\%$ in\nabsolute normalized score for discriminative tasks and even surpassing the\nfine-tuned upper bound on the generative tasks. Our implementation is available\nin \\url{https://github.com/LZY-the-boys/Twin-Merging}",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024 poster",
    "pdf_url": "http://arxiv.org/pdf/2406.15479v2",
    "published_date": "2024-06-17 02:31:55 UTC",
    "updated_date": "2024-10-14 04:14:26 UTC"
  },
  {
    "arxiv_id": "2406.11148v3",
    "title": "Few-Shot Recognition via Stage-Wise Retrieval-Augmented Finetuning",
    "authors": [
      "Tian Liu",
      "Huixin Zhang",
      "Shubham Parashar",
      "Shu Kong"
    ],
    "abstract": "Few-shot recognition (FSR) aims to train a classification model with only a\nfew labeled examples of each concept concerned by a downstream task, where data\nannotation cost can be prohibitively high. We develop methods to solve FSR by\nleveraging a pretrained Vision-Language Model (VLM). We particularly explore\nretrieval-augmented learning (RAL), which retrieves open data, e.g., the VLM's\npretraining dataset, to learn models for better serving downstream tasks. RAL\nhas been studied in zero-shot recognition but remains under-explored in FSR.\nAlthough applying RAL to FSR may seem straightforward, we observe interesting\nand novel challenges and opportunities. First, somewhat surprisingly,\nfinetuning a VLM on a large amount of retrieved data underperforms\nstate-of-the-art zero-shot methods. This is due to the imbalanced distribution\nof retrieved data and its domain gaps with the few-shot examples in the\ndownstream task. Second, more surprisingly, we find that simply finetuning a\nVLM solely on few-shot examples significantly outperforms previous FSR methods,\nand finetuning on the mix of retrieved and few-shot data yields even better\nresults. Third, to mitigate the imbalanced distribution and domain gap issues,\nwe propose Stage-Wise retrieval-Augmented fineTuning (SWAT), which involves\nend-to-end finetuning on mixed data in the first stage and retraining the\nclassifier on the few-shot data in the second stage. Extensive experiments on\nnine popular benchmarks demonstrate that SWAT significantly outperforms\nprevious methods by >6% accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025. Website and code:\n  https://tian1327.github.io/SWAT/",
    "pdf_url": "http://arxiv.org/pdf/2406.11148v3",
    "published_date": "2024-06-17 02:27:14 UTC",
    "updated_date": "2025-03-21 20:56:08 UTC"
  },
  {
    "arxiv_id": "2406.11147v2",
    "title": "Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG",
    "authors": [
      "Xueying Du",
      "Geng Zheng",
      "Kaixin Wang",
      "Jiayi Feng",
      "Wentai Deng",
      "Mingwei Liu",
      "Bihuan Chen",
      "Xin Peng",
      "Tao Ma",
      "Yiling Lou"
    ],
    "abstract": "Vulnerability detection is essential for software quality assurance. In\nrecent years, deep learning models (especially large language models) have\nshown promise in vulnerability detection. In this work, we propose a novel\nLLM-based vulnerability detection technique Vul-RAG, which leverages\nknowledge-level retrieval-augmented generation (RAG) framework to detect\nvulnerability for the given code in three phases. First, Vul-RAG constructs a\nvulnerability knowledge base by extracting multi-dimension knowledge via LLMs\nfrom existing CVE instances; second, for a given code snippet, Vul-RAG}\nretrieves the relevant vulnerability knowledge from the constructed knowledge\nbase based on functional semantics; third, Vul-RAG leverages LLMs to check the\nvulnerability of the given code snippet by reasoning the presence of\nvulnerability causes and fixing solutions of the retrieved vulnerability\nknowledge. Our evaluation of Vul-RAG on our constructed benchmark PairVul shows\nthat Vul-RAG substantially outperforms all baselines by 12.96\\%/110\\% relative\nimprovement in accuracy/pairwise-accuracy. In addition, our user study shows\nthat the vulnerability knowledge generated by Vul-RAG can serve as high-quality\nexplanations which can improve the manual detection accuracy from 0.60 to 0.77.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11147v2",
    "published_date": "2024-06-17 02:25:45 UTC",
    "updated_date": "2024-06-19 17:27:06 UTC"
  },
  {
    "arxiv_id": "2406.11143v2",
    "title": "Scorecards for Synthetic Medical Data Evaluation and Reporting",
    "authors": [
      "Ghada Zamzmi",
      "Adarsh Subbaswamy",
      "Elena Sizikova",
      "Edward Margerrison",
      "Jana Delfino",
      "Aldo Badano"
    ],
    "abstract": "Although interest in synthetic medical data (SMD) for training and testing AI\nmethods is growing, the absence of a standardized framework to evaluate its\nquality and applicability hinders its wider adoption. Here, we outline an\nevaluation framework designed to meet the unique requirements of medical\napplications, and introduce SMD Card, which can serve as comprehensive reports\nthat accompany artificially generated datasets. This card provides a\ntransparent and standardized framework for evaluating and reporting the quality\nof synthetic data, which can benefit SMD developers, users, and regulators,\nparticularly for AI models using SMD in regulatory submissions.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11143v2",
    "published_date": "2024-06-17 02:11:59 UTC",
    "updated_date": "2024-12-04 00:18:41 UTC"
  },
  {
    "arxiv_id": "2406.11138v2",
    "title": "Diffusion Models in Low-Level Vision: A Survey",
    "authors": [
      "Chunming He",
      "Yuqi Shen",
      "Chengyu Fang",
      "Fengyang Xiao",
      "Longxiang Tang",
      "Yulun Zhang",
      "Wangmeng Zuo",
      "Zhenhua Guo",
      "Xiu Li"
    ],
    "abstract": "Deep generative models have garnered significant attention in low-level\nvision tasks due to their generative capabilities. Among them, diffusion\nmodel-based solutions, characterized by a forward diffusion process and a\nreverse denoising process, have emerged as widely acclaimed for their ability\nto produce samples of superior quality and diversity. This ensures the\ngeneration of visually compelling results with intricate texture information.\nDespite their remarkable success, a noticeable gap exists in a comprehensive\nsurvey that amalgamates these pioneering diffusion model-based works and\norganizes the corresponding threads. This paper proposes the comprehensive\nreview of diffusion model-based techniques. We present three generic diffusion\nmodeling frameworks and explore their correlations with other deep generative\nmodels, establishing the theoretical foundation. Following this, we introduce a\nmulti-perspective categorization of diffusion models, considering both the\nunderlying framework and the target task. Additionally, we summarize extended\ndiffusion models applied in other tasks, including medical, remote sensing, and\nvideo scenarios. Moreover, we provide an overview of commonly used benchmarks\nand evaluation metrics. We conduct a thorough evaluation, encompassing both\nperformance and efficiency, of diffusion model-based techniques in three\nprominent tasks. Finally, we elucidate the limitations of current diffusion\nmodels and propose seven intriguing directions for future research. This\ncomprehensive examination aims to facilitate a profound understanding of the\nlandscape surrounding denoising diffusion models in the context of low-level\nvision tasks. A curated list of diffusion model-based techniques in over 20\nlow-level vision tasks can be found at\nhttps://github.com/ChunmingHe/awesome-diffusion-models-in-low-level-vision.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IEEE TPAMI",
    "pdf_url": "http://arxiv.org/pdf/2406.11138v2",
    "published_date": "2024-06-17 01:49:27 UTC",
    "updated_date": "2025-02-25 03:53:24 UTC"
  },
  {
    "arxiv_id": "2406.11135v1",
    "title": "Towards Understanding Emotions for Engaged Mental Health Conversations",
    "authors": [
      "Kellie Yu Hui Sim",
      "Kohleen Tijing Fortuno",
      "Kenny Tsu Wei Choo"
    ],
    "abstract": "Providing timely support and intervention is crucial in mental health\nsettings. As the need to engage youth comfortable with texting increases,\nmental health providers are exploring and adopting text-based media such as\nchatbots, community-based forums, online therapies with licensed professionals,\nand helplines operated by trained responders. To support these text-based media\nfor mental health--particularly for crisis care--we are developing a system to\nperform passive emotion-sensing using a combination of keystroke dynamics and\nsentiment analysis. Our early studies of this system posit that the analysis of\nshort text messages and keyboard typing patterns can provide emotion\ninformation that may be used to support both clients and responders. We use our\npreliminary findings to discuss the way forward for applying AI to support\nmental health providers in providing better care.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages, 1 figure, to be published in DIS Companion '24",
    "pdf_url": "http://arxiv.org/pdf/2406.11135v1",
    "published_date": "2024-06-17 01:27:15 UTC",
    "updated_date": "2024-06-17 01:27:15 UTC"
  },
  {
    "arxiv_id": "2406.11132v2",
    "title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents",
    "authors": [
      "Weizhe Chen",
      "Sven Koenig",
      "Bistra Dilkina"
    ],
    "abstract": "In the past year, large language models (LLMs) have had remarkable success in\ndomains outside the traditional natural language processing, and their capacity\nis further expanded into the so-called LLM agents when connected with external\ntools. In all domains, the prompt to the LLMs has been shown to make a big\ndifference in what the LLM would generate and thus affect the performance of\nthe LLM agents. Therefore, automatic prompt engineering (APE) has become an\nimportant question for many researchers and users of LLMs. However, previous\nworks in APE rely on a final checker to evaluate the performance of the given\nprompt -- a requirement that is hard to meet in the case of LLM agents, where\nintermediate feedback is easier to obtain, and the final evaluation could be\nexpensive, inaccurate, or even missing. In this paper, we propose a novel\nmethod, \\textsc{RePrompt}, which does a ``gradient descent\"-like approach to\noptimize the step-by-step instructions in the prompts given to LLM agents,\nbased on the chat history obtained from interactions and reflections with LLM\nagents. By leveraging intermediate feedback, \\textsc{RePrompt} can optimize the\nprompt without the need for a final solution checker. We evaluate our approach\non PDDL generation, TravelPlanner, and Meeting Planning to show that our method\ncould generally improve performance for different reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11132v2",
    "published_date": "2024-06-17 01:23:11 UTC",
    "updated_date": "2025-02-13 21:38:42 UTC"
  },
  {
    "arxiv_id": "2406.11131v2",
    "title": "Are Large Language Models a Good Replacement of Taxonomies?",
    "authors": [
      "Yushi Sun",
      "Hao Xin",
      "Kai Sun",
      "Yifan Ethan Xu",
      "Xiao Yang",
      "Xin Luna Dong",
      "Nan Tang",
      "Lei Chen"
    ],
    "abstract": "Large language models (LLMs) demonstrate an impressive ability to internalize\nknowledge and answer natural language questions. Although previous studies\nvalidate that LLMs perform well on general knowledge while presenting poor\nperformance on long-tail nuanced knowledge, the community is still doubtful\nabout whether the traditional knowledge graphs should be replaced by LLMs. In\nthis paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made\nobsolete by LLMs. Intuitively, LLMs should perform well on common taxonomies\nand at taxonomy levels that are common to people. Unfortunately, there lacks a\ncomprehensive benchmark that evaluates the LLMs over a wide range of taxonomies\nfrom common to specialized domains and at levels from root to leaf so that we\ncan draw a confident conclusion. To narrow the research gap, we constructed a\nnovel taxonomy hierarchical structure discovery benchmark named TaxoGlimpse to\nevaluate the performance of LLMs over taxonomies. TaxoGlimpse covers ten\nrepresentative taxonomies from common to specialized domains with in-depth\nexperiments of different levels of entities in this taxonomy from root to leaf.\nOur comprehensive experiments of eighteen state-of-the-art LLMs under three\nprompting settings validate that LLMs can still not well capture the knowledge\nof specialized taxonomies and leaf-level entities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by VLDB 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11131v2",
    "published_date": "2024-06-17 01:21:50 UTC",
    "updated_date": "2024-06-20 08:01:14 UTC"
  },
  {
    "arxiv_id": "2406.11118v1",
    "title": "Incentivizing Quality Text Generation via Statistical Contracts",
    "authors": [
      "Eden Saig",
      "Ohad Einav",
      "Inbal Talgam-Cohen"
    ],
    "abstract": "While the success of large language models (LLMs) increases demand for\nmachine-generated text, current pay-per-token pricing schemes create a\nmisalignment of incentives known in economics as moral hazard: Text-generating\nagents have strong incentive to cut costs by preferring a cheaper model over\nthe cutting-edge one, and this can be done \"behind the scenes\" since the agent\nperforms inference internally. In this work, we approach this issue from an\neconomic perspective, by proposing a pay-for-performance, contract-based\nframework for incentivizing quality. We study a principal-agent game where the\nagent generates text using costly inference, and the contract determines the\nprincipal's payment for the text according to an automated quality evaluation.\nSince standard contract theory is inapplicable when internal inference costs\nare unknown, we introduce cost-robust contracts. As our main theoretical\ncontribution, we characterize optimal cost-robust contracts through a direct\ncorrespondence to optimal composite hypothesis tests from statistics,\ngeneralizing a result of Saig et al. (NeurIPS'23). We evaluate our framework\nempirically by deriving contracts for a range of objectives and LLM evaluation\nbenchmarks, and find that cost-robust contracts sacrifice only a marginal\nincrease in objective value compared to their cost-aware counterparts.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "Comments are welcome",
    "pdf_url": "http://arxiv.org/pdf/2406.11118v1",
    "published_date": "2024-06-17 00:30:58 UTC",
    "updated_date": "2024-06-17 00:30:58 UTC"
  },
  {
    "arxiv_id": "2406.11109v5",
    "title": "Investigating Annotator Bias in Large Language Models for Hate Speech Detection",
    "authors": [
      "Amit Das",
      "Zheng Zhang",
      "Najib Hasan",
      "Souvika Sarkar",
      "Fatemeh Jamshidi",
      "Tathagata Bhattacharya",
      "Mostafa Rahgouy",
      "Nilanjana Raychawdhary",
      "Dongji Feng",
      "Vinija Jain",
      "Aman Chadha",
      "Mary Sandage",
      "Lauramarie Pope",
      "Gerry Dozier",
      "Cheryl Seals"
    ],
    "abstract": "Data annotation, the practice of assigning descriptive labels to raw data, is\npivotal in optimizing the performance of machine learning models. However, it\nis a resource-intensive process susceptible to biases introduced by annotators.\nThe emergence of sophisticated Large Language Models (LLMs) presents a unique\nopportunity to modernize and streamline this complex procedure. While existing\nresearch extensively evaluates the efficacy of LLMs, as annotators, this paper\ndelves into the biases present in LLMs when annotating hate speech data. Our\nresearch contributes to understanding biases in four key categories: gender,\nrace, religion, and disability with four LLMs: GPT-3.5, GPT-4o, Llama-3.1 and\nGemma-2. Specifically targeting highly vulnerable groups within these\ncategories, we analyze annotator biases. Furthermore, we conduct a\ncomprehensive examination of potential factors contributing to these biases by\nscrutinizing the annotated data. We introduce our custom hate speech detection\ndataset, HateBiasNet, to conduct this research. Additionally, we perform the\nsame experiments on the ETHOS (Mollas et al. 2022) dataset also for comparative\nanalysis. This paper serves as a crucial resource, guiding researchers and\npractitioners in harnessing the potential of LLMs for data annotation, thereby\nfostering advancements in this critical field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeurIPS Safe Generative AI Workshop, 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.11109v5",
    "published_date": "2024-06-17 00:18:31 UTC",
    "updated_date": "2024-11-16 18:56:32 UTC"
  },
  {
    "arxiv_id": "2407.10984v1",
    "title": "On the Combination of AI and Wireless Technologies: 3GPP Standardization Progress",
    "authors": [
      "Chen Sun",
      "Tao Cui",
      "Wenqi Zhang",
      "Yingshuang Bai",
      "Shuo Wang",
      "Haojin Li"
    ],
    "abstract": "Combing Artificial Intelligence (AI) and wireless communication technologies\nhas become one of the major technologies trends towards 2030. This includes\nusing AI to improve the efficiency of the wireless transmission and supporting\nAI deployment with wireless networks. In this article, the latest progress of\nthe Third Generation Partnership Project (3GPP) standards development is\nintroduced. Concentrating on AI model distributed transfer and AI for Beam\nManagement (BM) with wireless network, we introduce the latest studies and\nexplain how the existing standards should be modified to incorporate the\nresults from academia.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10984v1",
    "published_date": "2024-06-17 00:11:04 UTC",
    "updated_date": "2024-06-17 00:11:04 UTC"
  },
  {
    "arxiv_id": "2406.11106v1",
    "title": "From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models",
    "authors": [
      "Harsh Nishant Lalai",
      "Aashish Anantha Ramakrishnan",
      "Raj Sanjay Shah",
      "Dongwon Lee"
    ],
    "abstract": "With the rapid growth of Large Language Models (LLMs), safeguarding textual\ncontent against unauthorized use is crucial. Text watermarking offers a vital\nsolution, protecting both - LLM-generated and plain text sources. This paper\npresents a unified overview of different perspectives behind designing\nwatermarking techniques, through a comprehensive survey of the research\nliterature. Our work has two key advantages, (1) we analyze research based on\nthe specific intentions behind different watermarking techniques, evaluation\ndatasets used, watermarking addition, and removal methods to construct a\ncohesive taxonomy. (2) We highlight the gaps and open challenges in text\nwatermarking to promote research in protecting text authorship. This extensive\ncoverage and detailed analysis sets our work apart, offering valuable insights\ninto the evolving landscape of text watermarking in language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11106v1",
    "published_date": "2024-06-17 00:09:31 UTC",
    "updated_date": "2024-06-17 00:09:31 UTC"
  }
]
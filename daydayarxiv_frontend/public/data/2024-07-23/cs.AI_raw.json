[
  {
    "arxiv_id": "2407.16908v1",
    "title": "Generation Constraint Scaling Can Mitigate Hallucination",
    "authors": [
      "Georgios Kollias",
      "Payel Das",
      "Subhajit Chaudhury"
    ],
    "abstract": "Addressing the issue of hallucinations in large language models (LLMs) is a\ncritical challenge. As the cognitive mechanisms of hallucination have been\nrelated to memory, here we explore hallucination for LLM that is enabled with\nexplicit memory mechanisms. We empirically demonstrate that by simply scaling\nthe readout vector that constrains generation in a memory-augmented LLM\ndecoder, hallucination mitigation can be achieved in a training-free manner.\nOur method is geometry-inspired and outperforms a state-of-the-art LLM editing\nmethod on the task of generation of Wikipedia-like biography entries both in\nterms of generation quality and runtime complexity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages; accepted at ICML 2024 Workshop on Large Language Models and\n  Cognition",
    "pdf_url": "http://arxiv.org/pdf/2407.16908v1",
    "published_date": "2024-07-23 23:58:19 UTC",
    "updated_date": "2024-07-23 23:58:19 UTC"
  },
  {
    "arxiv_id": "2407.16863v1",
    "title": "Balanced Multi-Relational Graph Clustering",
    "authors": [
      "Zhixiang Shen",
      "Haolan He",
      "Zhao Kang"
    ],
    "abstract": "Multi-relational graph clustering has demonstrated remarkable success in\nuncovering underlying patterns in complex networks. Representative methods\nmanage to align different views motivated by advances in contrastive learning.\nOur empirical study finds the pervasive presence of imbalance in real-world\ngraphs, which is in principle contradictory to the motivation of alignment. In\nthis paper, we first propose a novel metric, the Aggregation Class Distance, to\nempirically quantify structural disparities among different graphs. To address\nthe challenge of view imbalance, we propose Balanced Multi-Relational Graph\nClustering (BMGC), comprising unsupervised dominant view mining and dual\nsignals guided representation learning. It dynamically mines the dominant view\nthroughout the training process, synergistically improving clustering\nperformance with representation learning. Theoretical analysis ensures the\neffectiveness of dominant view mining. Extensive experiments and in-depth\nanalysis on real-world and synthetic datasets showcase that BMGC achieves\nstate-of-the-art performance, underscoring its superiority in addressing the\nview imbalance inherent in multi-relational graphs. The source code and\ndatasets are available at https://github.com/zxlearningdeep/BMGC.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ACM Multimedia 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16863v1",
    "published_date": "2024-07-23 22:11:13 UTC",
    "updated_date": "2024-07-23 22:11:13 UTC"
  },
  {
    "arxiv_id": "2407.18276v3",
    "title": "Rome was Not Built in a Single Step: Hierarchical Prompting for LLM-based Chip Design",
    "authors": [
      "Andre Nakkab",
      "Sai Qian Zhang",
      "Ramesh Karri",
      "Siddharth Garg"
    ],
    "abstract": "Large Language Models (LLMs) are effective in computer hardware synthesis via\nhardware description language (HDL) generation. However, LLM-assisted\napproaches for HDL generation struggle when handling complex tasks. We\nintroduce a suite of hierarchical prompting techniques which facilitate\nefficient stepwise design methods, and develop a generalizable automation\npipeline for the process. To evaluate these techniques, we present a benchmark\nset of hardware designs which have solutions with or without architectural\nhierarchy. Using these benchmarks, we compare various open-source and\nproprietary LLMs, including our own fine-tuned Code Llama-Verilog model. Our\nhierarchical methods automatically produce successful designs for complex\nhardware modules that standard flat prompting methods cannot achieve, allowing\nsmaller open-source LLMs to compete with large proprietary models. Hierarchical\nprompting reduces HDL generation time and yields savings on LLM costs. Our\nexperiments detail which LLMs are capable of which applications, and how to\napply hierarchical methods in various modes. We explore case studies of\ngenerating complex cores using automatic scripted hierarchical prompts,\nincluding the first-ever LLM-designed processor with no human feedback. Tools\nfor the Recurrent Optimization via Machine Editing (ROME) method can be found\nat https://github.com/ajn313/ROME-LLM",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted at MLCAD '24. 10 pages, 7 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.18276v3",
    "published_date": "2024-07-23 21:18:31 UTC",
    "updated_date": "2024-09-09 20:01:22 UTC"
  },
  {
    "arxiv_id": "2407.16840v1",
    "title": "Synth4Kws: Synthesized Speech for User Defined Keyword Spotting in Low Resource Environments",
    "authors": [
      "Pai Zhu",
      "Dhruuv Agarwal",
      "Jacob W. Bartel",
      "Kurt Partridge",
      "Hyun Jin Park",
      "Quan Wang"
    ],
    "abstract": "One of the challenges in developing a high quality custom keyword spotting\n(KWS) model is the lengthy and expensive process of collecting training data\ncovering a wide range of languages, phrases and speaking styles. We introduce\nSynth4Kws - a framework to leverage Text to Speech (TTS) synthesized data for\ncustom KWS in different resource settings. With no real data, we found\nincreasing TTS phrase diversity and utterance sampling monotonically improves\nmodel performance, as evaluated by EER and AUC metrics over 11k utterances of\nthe speech command dataset. In low resource settings, with 50k real utterances\nas a baseline, we found using optimal amounts of TTS data can improve EER by\n30.1% and AUC by 46.7%. Furthermore, we mix TTS data with varying amounts of\nreal data and interpolate the real data needed to achieve various quality\ntargets. Our experiments are based on English and single word utterances but\nthe findings generalize to i18n languages and other keyword types.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 5 figures, 2 tables The paper is accepted in Interspeech\n  SynData4GenAI 2024 Workshop - https://syndata4genai.org/#call-for-papers",
    "pdf_url": "http://arxiv.org/pdf/2407.16840v1",
    "published_date": "2024-07-23 21:05:44 UTC",
    "updated_date": "2024-07-23 21:05:44 UTC"
  },
  {
    "arxiv_id": "2407.16837v2",
    "title": "MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs",
    "authors": [
      "Jihyung Kil",
      "Zheda Mai",
      "Justin Lee",
      "Zihe Wang",
      "Kerrie Cheng",
      "Lemeng Wang",
      "Ye Liu",
      "Arpita Chowdhury",
      "Wei-Lun Chao"
    ],
    "abstract": "The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce MLLM-CompBench, a benchmark designed to evaluate the comparative\nreasoning capability of multimodal large language models (MLLMs).\nMLLM-CompBench mines and pairs images through visually oriented questions\ncovering eight dimensions of relative comparison: visual attribute, existence,\nstate, emotion, temporality, spatiality, quantity, and quality. We curate a\ncollection of around 40K image pairs using metadata from diverse vision\ndatasets and CLIP similarity scores. These image pairs span a broad array of\nvisual domains, including animals, fashion, sports, and both outdoor and indoor\nscenes. The questions are carefully crafted to discern relative characteristics\nbetween two images and are labeled by human annotators for accuracy and\nrelevance. We use MLLM-CompBench to evaluate recent MLLMs, including\nGPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable\nshortcomings in their comparative abilities. We believe MLLM-COMPBENCH not only\nsheds light on these limitations but also establishes a solid foundation for\nfuture enhancements in the comparative capability of MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted to NeurIPS 2024. The first two authors\n  contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2407.16837v2",
    "published_date": "2024-07-23 21:02:38 UTC",
    "updated_date": "2025-01-13 05:04:59 UTC"
  },
  {
    "arxiv_id": "2407.16834v1",
    "title": "A Multi-Level Hierarchical Framework for the Classification of Weather Conditions and Hazard Prediction",
    "authors": [
      "Harish Neelam"
    ],
    "abstract": "This paper presents a multilevel hierarchical framework for the\nclassification of weather conditions and hazard prediction. In recent years,\nthe importance of data has grown significantly, with various types like text,\nnumbers, images, audio, and videos playing a key role. Among these, images make\nup a large portion of the data available. This application shows promise for\nvarious purposes, especially when combined with decision support systems for\ntraffic management, afforestation, and weather forecasting. It's particularly\nuseful in situations where traditional weather predictions are not very\naccurate, such as ensuring the safe operation of self driving cars in dangerous\nweather. While previous studies have looked at this topic with fewer\ncategories, this paper focuses on eleven specific types of weather images. The\ngoal is to create a model that can accurately predict weather conditions after\nbeing trained on a large dataset of images. Accuracy is crucial in real-life\nsituations to prevent accidents, making it the top priority for this paper.\nThis work lays the groundwork for future applications in weather prediction,\nespecially in situations where human expertise is not available or may be\nbiased. The framework, capable of classifying images into eleven weather\ncategories: dew, frost, glaze, rime, snow, hail, rain, lightning, rainbow, and\nsandstorm, provides real-time weather information with an accuracy of 0.9329.\nThe proposed framework addresses the growing need for accurate weather\nclassification and hazard prediction, offering a robust solution for various\napplications in the field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.16834v1",
    "published_date": "2024-07-23 20:55:25 UTC",
    "updated_date": "2024-07-23 20:55:25 UTC"
  },
  {
    "arxiv_id": "2407.16833v2",
    "title": "Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach",
    "authors": [
      "Zhuowan Li",
      "Cheng Li",
      "Mingyang Zhang",
      "Qiaozhu Mei",
      "Michael Bendersky"
    ],
    "abstract": "Retrieval Augmented Generation (RAG) has been a powerful tool for Large\nLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,\nrecent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct a comprehensive comparison\nbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths of\nboth. We benchmark RAG and LC across various public datasets using three latest\nLLMs. Results reveal that when resourced sufficiently, LC consistently\noutperforms RAG in terms of average performance. However, RAG's significantly\nlower cost remains a distinct advantage. Based on this observation, we propose\nSelf-Route, a simple yet effective method that routes queries to RAG or LC\nbased on model self-reflection. Self-Route significantly reduces the\ncomputation cost while maintaining a comparable performance to LC. Our findings\nprovide a guideline for long-context applications of LLMs using RAG and LC.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 industry track",
    "pdf_url": "http://arxiv.org/pdf/2407.16833v2",
    "published_date": "2024-07-23 20:51:52 UTC",
    "updated_date": "2024-10-17 17:51:19 UTC"
  },
  {
    "arxiv_id": "2407.16831v1",
    "title": "Networks of Networks: Complexity Class Principles Applied to Compound AI Systems Design",
    "authors": [
      "Jared Quincy Davis",
      "Boris Hanin",
      "Lingjiao Chen",
      "Peter Bailis",
      "Ion Stoica",
      "Matei Zaharia"
    ],
    "abstract": "As practitioners seek to surpass the current reliability and quality frontier\nof monolithic models, Compound AI Systems consisting of many language model\ninference calls are increasingly employed. In this work, we construct systems,\nwhich we call Networks of Networks (NoNs) organized around the distinction\nbetween generating a proposed answer and verifying its correctness, a\nfundamental concept in complexity theory that we show empirically extends to\nLanguage Models (LMs). We introduce a verifier-based judge NoN with K\ngenerators, an instantiation of \"best-of-K\" or \"judge-based\" compound AI\nsystems. Through experiments on synthetic tasks such as prime factorization,\nand core benchmarks such as the MMLU, we demonstrate notable performance gains.\nFor instance, in factoring products of two 3-digit primes, a simple NoN\nimproves accuracy from 3.7\\% to 36.6\\%. On MMLU, a verifier-based judge\nconstruction with only 3 generators boosts accuracy over individual GPT-4-Turbo\ncalls by 2.8\\%. Our analysis reveals that these gains are most pronounced in\ndomains where verification is notably easier than generation--a\ncharacterization which we believe subsumes many reasoning and procedural\nknowledge tasks, but doesn't often hold for factual and declarative\nknowledge-based settings. For mathematical and formal logic reasoning-based\nsubjects of MMLU, we observe a 5-8\\% or higher gain, whilst no gain on others\nsuch as geography and religion. We provide key takeaways for ML practitioners,\nincluding the importance of considering verification complexity, the impact of\nwitness format on verifiability, and a simple test to determine the potential\nbenefit of this NoN approach for a given problem distribution. This work aims\nto inform future research and practice in the design of compound AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16831v1",
    "published_date": "2024-07-23 20:40:37 UTC",
    "updated_date": "2024-07-23 20:40:37 UTC"
  },
  {
    "arxiv_id": "2407.16828v3",
    "title": "Pareto Front Approximation for Multi-Objective Session-Based Recommender Systems",
    "authors": [
      "Timo Wilm",
      "Philipp Normann",
      "Felix Stepprath"
    ],
    "abstract": "This work introduces MultiTRON, an approach that adapts Pareto front\napproximation techniques to multi-objective session-based recommender systems\nusing a transformer neural network. Our approach optimizes trade-offs between\nkey metrics such as click-through and conversion rates by training on sampled\npreference vectors. A significant advantage is that after training, a single\nmodel can access the entire Pareto front, allowing it to be tailored to meet\nthe specific requirements of different stakeholders by adjusting an additional\ninput vector that weights the objectives. We validate the model's performance\nthrough extensive offline and online evaluation. For broader application and\nresearch, the source code is made available at\nhttps://github.com/otto-de/MultiTRON. The results confirm the model's ability\nto manage multiple recommendation objectives effectively, offering a flexible\ntool for diverse business needs.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at the Eighteenth ACM Conference on Recommender Systems\n  (RecSys '24)",
    "pdf_url": "http://arxiv.org/pdf/2407.16828v3",
    "published_date": "2024-07-23 20:38:23 UTC",
    "updated_date": "2025-03-30 12:29:24 UTC"
  },
  {
    "arxiv_id": "2407.16822v1",
    "title": "AI-Enhanced 7-Point Checklist for Melanoma Detection Using Clinical Knowledge Graphs and Data-Driven Quantification",
    "authors": [
      "Yuheng Wang",
      "Tianze Yu",
      "Jiayue Cai",
      "Sunil Kalia",
      "Harvey Lui",
      "Z. Jane Wang",
      "Tim K. Lee"
    ],
    "abstract": "The 7-point checklist (7PCL) is widely used in dermoscopy to identify\nmalignant melanoma lesions needing urgent medical attention. It assigns point\nvalues to seven attributes: major attributes are worth two points each, and\nminor ones are worth one point each. A total score of three or higher prompts\nfurther evaluation, often including a biopsy. However, a significant limitation\nof current methods is the uniform weighting of attributes, which leads to\nimprecision and neglects their interconnections. Previous deep learning studies\nhave treated the prediction of each attribute with the same importance as\npredicting melanoma, which fails to recognize the clinical significance of the\nattributes for melanoma. To address these limitations, we introduce a novel\ndiagnostic method that integrates two innovative elements: a Clinical\nKnowledge-Based Topological Graph (CKTG) and a Gradient Diagnostic Strategy\nwith Data-Driven Weighting Standards (GD-DDW). The CKTG integrates 7PCL\nattributes with diagnostic information, revealing both internal and external\nassociations. By employing adaptive receptive domains and weighted edges, we\nestablish connections among melanoma's relevant features. Concurrently, GD-DDW\nemulates dermatologists' diagnostic processes, who first observe the visual\ncharacteristics associated with melanoma and then make predictions. Our model\nuses two imaging modalities for the same lesion, ensuring comprehensive feature\nacquisition. Our method shows outstanding performance in predicting malignant\nmelanoma and its features, achieving an average AUC value of 85%. This was\nvalidated on the EDRA dataset, the largest publicly available dataset for the\n7-point checklist algorithm. Specifically, the integrated weighting system can\nprovide clinicians with valuable data-driven benchmarks for their evaluations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16822v1",
    "published_date": "2024-07-23 20:27:16 UTC",
    "updated_date": "2024-07-23 20:27:16 UTC"
  },
  {
    "arxiv_id": "2407.16807v1",
    "title": "In Search for Architectures and Loss Functions in Multi-Objective Reinforcement Learning",
    "authors": [
      "Mikhail Terekhov",
      "Caglar Gulcehre"
    ],
    "abstract": "Multi-objective reinforcement learning (MORL) is essential for addressing the\nintricacies of real-world RL problems, which often require trade-offs between\nmultiple utility functions. However, MORL is challenging due to unstable\nlearning dynamics with deep learning-based function approximators. The research\npath most taken has been to explore different value-based loss functions for\nMORL to overcome this issue. Our work empirically explores model-free policy\nlearning loss functions and the impact of different architectural choices. We\nintroduce two different approaches: Multi-objective Proximal Policy\nOptimization (MOPPO), which extends PPO to MORL, and Multi-objective Advantage\nActor Critic (MOA2C), which acts as a simple baseline in our ablations. Our\nproposed approach is straightforward to implement, requiring only small\nmodifications at the level of function approximator. We conduct comprehensive\nevaluations on the MORL Deep Sea Treasure, Minecart, and Reacher environments\nand show that MOPPO effectively captures the Pareto front. Our extensive\nablation studies and empirical analyses reveal the impact of different\narchitectural choices, underscoring the robustness and versatility of MOPPO\ncompared to popular MORL approaches like Pareto Conditioned Networks (PCN) and\nEnvelope Q-learning in terms of MORL metrics, including hypervolume and\nexpected utility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 10 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.16807v1",
    "published_date": "2024-07-23 19:17:47 UTC",
    "updated_date": "2024-07-23 19:17:47 UTC"
  },
  {
    "arxiv_id": "2407.16804v1",
    "title": "Multimodal Machine Learning in Mental Health: A Survey of Data, Algorithms, and Challenges",
    "authors": [
      "Zahraa Al Sahili",
      "Ioannis Patras",
      "Matthew Purver"
    ],
    "abstract": "The application of machine learning (ML) in detecting, diagnosing, and\ntreating mental health disorders is garnering increasing attention.\nTraditionally, research has focused on single modalities, such as text from\nclinical notes, audio from speech samples, or video of interaction patterns.\nRecently, multimodal ML, which combines information from multiple modalities,\nhas demonstrated significant promise in offering novel insights into human\nbehavior patterns and recognizing mental health symptoms and risk factors.\nDespite its potential, multimodal ML in mental health remains an emerging\nfield, facing several complex challenges before practical applications can be\neffectively developed. This survey provides a comprehensive overview of the\ndata availability and current state-of-the-art multimodal ML applications for\nmental health. It discusses key challenges that must be addressed to advance\nthe field. The insights from this survey aim to deepen the understanding of the\npotential and limitations of multimodal ML in mental health, guiding future\nresearch and development in this evolving domain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16804v1",
    "published_date": "2024-07-23 19:07:56 UTC",
    "updated_date": "2024-07-23 19:07:56 UTC"
  },
  {
    "arxiv_id": "2407.16803v2",
    "title": "C3T: Cross-modal Transfer Through Time for Human Action Recognition",
    "authors": [
      "Abhi Kamboj",
      "Anh Duy Nguyen",
      "Minh Do"
    ],
    "abstract": "In order to unlock the potential of diverse sensors, we investigate a method\nto transfer knowledge between modalities using the structure of a unified\nmultimodal representation space for Human Action Recognition (HAR). We\nformalize and explore an understudied cross-modal transfer setting we term\nUnsupervised Modality Adaptation (UMA), where the modality used in testing is\nnot used in supervised training, i.e. zero labeled instances of the test\nmodality are available during training. We develop three methods to perform\nUMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer\nThrough Time (C3T). Our extensive experiments on various camera+IMU datasets\ncompare these methods to each other in the UMA setting, and to their empirical\nupper bound in the supervised setting. The results indicate C3T is the most\nrobust and highest performing by at least a margin of 8%, and nears the\nsupervised setting performance even in the presence of temporal noise. This\nmethod introduces a novel mechanism for aligning signals across time-varying\nlatent vectors, extracted from the receptive field of temporal convolutions.\nOur findings suggest that C3T has significant potential for developing\ngeneralizable models for time-series sensor data, opening new avenues for\nmulti-modal learning in various applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16803v2",
    "published_date": "2024-07-23 19:06:44 UTC",
    "updated_date": "2024-11-07 17:10:15 UTC"
  },
  {
    "arxiv_id": "2407.16802v1",
    "title": "Distribution-Aware Robust Learning from Long-Tailed Data with Noisy Labels",
    "authors": [
      "Jae Soon Baik",
      "In Young Yoon",
      "Kun Hoon Kim",
      "Jun Won Choi"
    ],
    "abstract": "Deep neural networks have demonstrated remarkable advancements in various\nfields using large, well-annotated datasets. However, real-world data often\nexhibit long-tailed distributions and label noise, significantly degrading\ngeneralization performance. Recent studies addressing these issues have focused\non noisy sample selection methods that estimate the centroid of each class\nbased on high-confidence samples within each target class. The performance of\nthese methods is limited because they use only the training samples within each\nclass for class centroid estimation, making the quality of centroids\nsusceptible to long-tailed distributions and noisy labels. In this study, we\npresent a robust training framework called Distribution-aware Sample Selection\nand Contrastive Learning (DaSC). Specifically, DaSC introduces a\nDistribution-aware Class Centroid Estimation (DaCC) to generate enhanced class\ncentroids. DaCC performs weighted averaging of the features from all samples,\nwith weights determined based on model predictions. Additionally, we propose a\nconfidence-aware contrastive learning strategy to obtain balanced and robust\nrepresentations. The training samples are categorized into high-confidence and\nlow-confidence samples. Our method then applies Semi-supervised Balanced\nContrastive Loss (SBCL) using high-confidence samples, leveraging reliable\nlabel information to mitigate class bias. For the low-confidence samples, our\nmethod computes Mixup-enhanced Instance Discrimination Loss (MIDL) to improve\ntheir representations in a self-supervised manner. Our experimental results on\nCIFAR and real-world noisy-label datasets demonstrate the superior performance\nof the proposed DaSC compared to previous approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16802v1",
    "published_date": "2024-07-23 19:06:15 UTC",
    "updated_date": "2024-07-23 19:06:15 UTC"
  },
  {
    "arxiv_id": "2407.16789v2",
    "title": "What Matters in Range View 3D Object Detection",
    "authors": [
      "Benjamin Wilson",
      "Nicholas Autio Mitchell",
      "Jhony Kaesemodel Pontes",
      "James Hays"
    ],
    "abstract": "Lidar-based perception pipelines rely on 3D object detection models to\ninterpret complex scenes. While multiple representations for lidar exist, the\nrange-view is enticing since it losslessly encodes the entire lidar sensor\noutput. In this work, we achieve state-of-the-art amongst range-view 3D object\ndetection models without using multiple techniques proposed in past range-view\nliterature. We explore range-view 3D object detection across two modern\ndatasets with substantially different properties: Argoverse 2 and Waymo Open.\nOur investigation reveals key insights: (1) input feature dimensionality\nsignificantly influences the overall performance, (2) surprisingly, employing a\nclassification loss grounded in 3D spatial proximity works as well or better\ncompared to more elaborate IoU-based losses, and (3) addressing non-uniform\nlidar density via a straightforward range subsampling technique outperforms\nexisting multi-resolution, range-conditioned networks. Our experiments reveal\nthat techniques proposed in recent range-view literature are not needed to\nachieve state-of-the-art performance. Combining the above findings, we\nestablish a new state-of-the-art model for range-view 3D object detection --\nimproving AP by 2.2% on the Waymo Open dataset while maintaining a runtime of\n10 Hz. We establish the first range-view model on the Argoverse 2 dataset and\noutperform strong voxel-based baselines. All models are multi-class and\nopen-source. Code is available at\nhttps://github.com/benjaminrwilson/range-view-3d-detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Fixed broken link",
    "pdf_url": "http://arxiv.org/pdf/2407.16789v2",
    "published_date": "2024-07-23 18:42:37 UTC",
    "updated_date": "2024-07-25 20:20:03 UTC"
  },
  {
    "arxiv_id": "2407.16785v1",
    "title": "PrISM-Observer: Intervention Agent to Help Users Perform Everyday Procedures Sensed using a Smartwatch",
    "authors": [
      "Riku Arakawa",
      "Hiromu Yakura",
      "Mayank Goel"
    ],
    "abstract": "We routinely perform procedures (such as cooking) that include a set of\natomic steps. Often, inadvertent omission or misordering of a single step can\nlead to serious consequences, especially for those experiencing cognitive\nchallenges such as dementia. This paper introduces PrISM-Observer, a\nsmartwatch-based, context-aware, real-time intervention system designed to\nsupport daily tasks by preventing errors. Unlike traditional systems that\nrequire users to seek out information, the agent observes user actions and\nintervenes proactively. This capability is enabled by the agent's ability to\ncontinuously update its belief in the user's behavior in real-time through\nmultimodal sensing and forecast optimal intervention moments and methods. We\nfirst validated the steps-tracking performance of our framework through\nevaluations across three datasets with different complexities. Then, we\nimplemented a real-time agent system using a smartwatch and conducted a user\nstudy in a cooking task scenario. The system generated helpful interventions,\nand we gained positive feedback from the participants. The general\napplicability of PrISM-Observer to daily tasks promises broad applications, for\ninstance, including support for users requiring more involved interventions,\nsuch as people with dementia or post-surgical patients.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "conditionally accepted to ACM UIST 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16785v1",
    "published_date": "2024-07-23 18:38:07 UTC",
    "updated_date": "2024-07-23 18:38:07 UTC"
  },
  {
    "arxiv_id": "2407.16770v1",
    "title": "Infinite Ends from Finite Samples: Open-Ended Goal Inference as Top-Down Bayesian Filtering of Bottom-Up Proposals",
    "authors": [
      "Tan Zhi-Xuan",
      "Gloria Kang",
      "Vikash Mansinghka",
      "Joshua B. Tenenbaum"
    ],
    "abstract": "The space of human goals is tremendously vast; and yet, from just a few\nmoments of watching a scene or reading a story, we seem to spontaneously infer\na range of plausible motivations for the people and characters involved. What\nexplains this remarkable capacity for intuiting other agents' goals, despite\nthe infinitude of ends they might pursue? And how does this cohere with our\nunderstanding of other people as approximately rational agents? In this paper,\nwe introduce a sequential Monte Carlo model of open-ended goal inference, which\ncombines top-down Bayesian inverse planning with bottom-up sampling based on\nthe statistics of co-occurring subgoals. By proposing goal hypotheses related\nto the subgoals achieved by an agent, our model rapidly generates plausible\ngoals without exhaustive search, then filters out goals that would be\nirrational given the actions taken so far. We validate this model in a goal\ninference task called Block Words, where participants try to guess the word\nthat someone is stacking out of lettered blocks. In comparison to both\nheuristic bottom-up guessing and exact Bayesian inference over hundreds of\ngoals, our model better predicts the mean, variance, efficiency, and resource\nrationality of human goal inferences, achieving similar accuracy to the exact\nmodel at a fraction of the cognitive cost, while also explaining garden-path\neffects that arise from misleading bottom-up cues. Our experiments thus\nhighlight the importance of uniting top-down and bottom-up models for\nexplaining the speed, accuracy, and generality of human theory-of-mind.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at CogSci 2024. 6 pages, 4 figures.\n  (Appendix: 5 pages, 6 figures, 2 tables)",
    "pdf_url": "http://arxiv.org/pdf/2407.16770v1",
    "published_date": "2024-07-23 18:04:40 UTC",
    "updated_date": "2024-07-23 18:04:40 UTC"
  },
  {
    "arxiv_id": "2407.16695v2",
    "title": "Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack",
    "authors": [
      "Xiaoyue Xu",
      "Qinyuan Ye",
      "Xiang Ren"
    ],
    "abstract": "We introduce Lifelong ICL, a problem setting that challenges long-context\nlanguage models (LMs) to learn a sequence of language tasks through in-context\nlearning (ICL). We further introduce Task Haystack, an evaluation suite\ndedicated to assessing and diagnosing how long-context LMs utilizes contexts in\nLifelong ICL. When given a task instruction and test inputs, long-context LMs\nare expected to leverage the relevant demonstrations in the Lifelong ICL\nprompt, avoid distraction and interference from other tasks, and achieve test\naccuracies that are not significantly worse than those of the Single-task ICL\nbaseline.\n  Task Haystack draws inspiration from the widely-adopted\n\"needle-in-a-haystack\" (NIAH) evaluation, but presents distinct new challenges.\nIt requires models (1) to utilize the contexts at a deeper level, rather than\nresorting to simple copying and pasting; (2) to navigate through long streams\nof evolving topics and tasks, proxying the complexities and dynamism of\ncontexts in real-world scenarios. Additionally, Task Haystack inherits the\ncontrollability of NIAH, providing model developers with tools and\nvisualizations to identify model vulnerabilities effectively.\n  We benchmark 14 long-context LMs using Task Haystack, finding that frontier\nmodels like GPT-4o still struggle with the setting, failing on 15% of cases on\naverage. Most open-weight models further lack behind by a large margin, with\nfailure rates reaching up to 61%. In our controlled analysis, we identify\nfactors such as distraction and recency bias as contributors to these failure\ncases. Further, performance declines when task instructions are paraphrased at\ntest time or when ICL demonstrations are repeated excessively, raising concerns\nabout the robustness, instruction understanding, and true context utilization\nof long-context LMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024 (Datasets and Benchmarks Track). Code:\n  https://github.com/INK-USC/Lifelong-ICL Website:\n  https://inklab.usc.edu/lifelong-icl/",
    "pdf_url": "http://arxiv.org/pdf/2407.16695v2",
    "published_date": "2024-07-23 17:57:41 UTC",
    "updated_date": "2024-12-02 20:23:49 UTC"
  },
  {
    "arxiv_id": "2407.16741v3",
    "title": "OpenHands: An Open Platform for AI Software Developers as Generalist Agents",
    "authors": [
      "Xingyao Wang",
      "Boxuan Li",
      "Yufan Song",
      "Frank F. Xu",
      "Xiangru Tang",
      "Mingchen Zhuge",
      "Jiayi Pan",
      "Yueqi Song",
      "Bowen Li",
      "Jaskirat Singh",
      "Hoang H. Tran",
      "Fuqiang Li",
      "Ren Ma",
      "Mingzhang Zheng",
      "Bill Qian",
      "Yanjun Shao",
      "Niklas Muennighoff",
      "Yizhe Zhang",
      "Binyuan Hui",
      "Junyang Lin",
      "Robert Brennan",
      "Hao Peng",
      "Heng Ji",
      "Graham Neubig"
    ],
    "abstract": "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the\ndevelopment of powerful and flexible AI agents that interact with the world in\nsimilar ways to those of a human developer: by writing code, interacting with a\ncommand line, and browsing the web. We describe how the platform allows for the\nimplementation of new agents, safe interaction with sandboxed environments for\ncode execution, coordination between multiple agents, and incorporation of\nevaluation benchmarks. Based on our currently incorporated benchmarks, we\nperform an evaluation of agents over 15 challenging tasks, including software\nengineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others.\nReleased under the permissive MIT license, OpenHands is a community project\nspanning academia and industry with more than 2.1K contributions from over 188\ncontributors.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by ICLR 2025; Code:\n  https://github.com/All-Hands-AI/OpenHands",
    "pdf_url": "http://arxiv.org/pdf/2407.16741v3",
    "published_date": "2024-07-23 17:50:43 UTC",
    "updated_date": "2025-04-18 18:14:31 UTC"
  },
  {
    "arxiv_id": "2407.16674v2",
    "title": "KAN or MLP: A Fairer Comparison",
    "authors": [
      "Runpeng Yu",
      "Weihao Yu",
      "Xinchao Wang"
    ],
    "abstract": "This paper does not introduce a novel method. Instead, it offers a fairer and\nmore comprehensive comparison of KAN and MLP models across various tasks,\nincluding machine learning, computer vision, audio processing, natural language\nprocessing, and symbolic formula representation. Specifically, we control the\nnumber of parameters and FLOPs to compare the performance of KAN and MLP. Our\nmain observation is that, except for symbolic formula representation tasks, MLP\ngenerally outperforms KAN. We also conduct ablation studies on KAN and find\nthat its advantage in symbolic formula representation mainly stems from its\nB-spline activation function. When B-spline is applied to MLP, performance in\nsymbolic formula representation significantly improves, surpassing or matching\nthat of KAN. However, in other tasks where MLP already excels over KAN,\nB-spline does not substantially enhance MLP's performance. Furthermore, we find\nthat KAN's forgetting issue is more severe than that of MLP in a standard\nclass-incremental continual learning setting, which differs from the findings\nreported in the KAN paper. We hope these results provide insights for future\nresearch on KAN and other MLP alternatives. Project link:\nhttps://github.com/yu-rp/KANbeFair",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Technical Report",
    "pdf_url": "http://arxiv.org/pdf/2407.16674v2",
    "published_date": "2024-07-23 17:43:35 UTC",
    "updated_date": "2024-08-17 15:20:31 UTC"
  },
  {
    "arxiv_id": "2407.16740v1",
    "title": "PLM-Net: Perception Latency Mitigation Network for Vision-Based Lateral Control of Autonomous Vehicles",
    "authors": [
      "Aws Khalil",
      "Jaerock Kwon"
    ],
    "abstract": "This study introduces the Perception Latency Mitigation Network (PLM-Net), a\nnovel deep learning approach for addressing perception latency in vision-based\nAutonomous Vehicle (AV) lateral control systems. Perception latency is the\ndelay between capturing the environment through vision sensors (e.g., cameras)\nand applying an action (e.g., steering). This issue is understudied in both\nclassical and neural-network-based control methods. Reducing this latency with\npowerful GPUs and FPGAs is possible but impractical for automotive platforms.\nPLM-Net comprises the Base Model (BM) and the Timed Action Prediction Model\n(TAPM). BM represents the original Lane Keeping Assist (LKA) system, while TAPM\npredicts future actions for different latency values. By integrating these\nmodels, PLM-Net mitigates perception latency. The final output is determined\nthrough linear interpolation of BM and TAPM outputs based on real-time latency.\nThis design addresses both constant and varying latency, improving driving\ntrajectories and steering control. Experimental results validate the efficacy\nof PLM-Net across various latency conditions. Source code:\nhttps://github.com/AwsKhalil/oscar/tree/devel-plm-net.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "13 pages excluding the appendixes. 19 pages including appendixes",
    "pdf_url": "http://arxiv.org/pdf/2407.16740v1",
    "published_date": "2024-07-23 17:41:13 UTC",
    "updated_date": "2024-07-23 17:41:13 UTC"
  },
  {
    "arxiv_id": "2407.16667v1",
    "title": "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent",
    "authors": [
      "Huiyu Xu",
      "Wenhui Zhang",
      "Zhibo Wang",
      "Feng Xiao",
      "Rui Zheng",
      "Yunhe Feng",
      "Zhongjie Ba",
      "Kui Ren"
    ],
    "abstract": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been\nintegrated into many real-world applications like Code Copilot. These\napplications have significantly expanded the attack surface of LLMs, exposing\nthem to a variety of threats. Among them, jailbreak attacks that induce toxic\nresponses through jailbreak prompts have raised critical safety concerns. To\nidentify these threats, a growing number of red teaming approaches simulate\npotential adversarial scenarios by crafting jailbreak prompts to test the\ntarget LLM. However, existing red teaming methods do not consider the unique\nvulnerabilities of LLM in different scenarios, making it difficult to adjust\nthe jailbreak prompts to find context-specific vulnerabilities. Meanwhile,\nthese methods are limited to refining jailbreak templates using a few mutation\noperations, lacking the automation and scalability to adapt to different\nscenarios. To enable context-aware and efficient red teaming, we abstract and\nmodel existing attacks into a coherent concept called \"jailbreak strategy\" and\npropose a multi-agent LLM system named RedAgent that leverages these strategies\nto generate context-aware jailbreak prompts. By self-reflecting on contextual\nfeedback in an additional memory buffer, RedAgent continuously learns how to\nleverage these strategies to achieve effective jailbreaks in specific contexts.\nExtensive experiments demonstrate that our system can jailbreak most black-box\nLLMs in just five queries, improving the efficiency of existing red teaming\nmethods by two times. Additionally, RedAgent can jailbreak customized LLM\napplications more efficiently. By generating context-aware jailbreak prompts\ntowards applications on GPTs, we discover 60 severe vulnerabilities of these\nreal-world applications with only two queries per vulnerability. We have\nreported all found issues and communicated with OpenAI and Meta for bug fixes.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16667v1",
    "published_date": "2024-07-23 17:34:36 UTC",
    "updated_date": "2024-07-23 17:34:36 UTC"
  },
  {
    "arxiv_id": "2407.16665v2",
    "title": "A Framework for Pupil Tracking with Event Cameras",
    "authors": [
      "Khadija Iddrisu",
      "Waseem Shariff",
      "Suzanne Little"
    ],
    "abstract": "Saccades are extremely rapid movements of both eyes that occur\nsimultaneously, typically observed when an individual shifts their focus from\none object to another. These movements are among the swiftest produced by\nhumans and possess the potential to achieve velocities greater than that of\nblinks. The peak angular speed of the eye during a saccade can reach as high as\n700{\\deg}/s in humans, especially during larger saccades that cover a visual\nangle of 25{\\deg}. Previous research has demonstrated encouraging outcomes in\ncomprehending neurological conditions through the study of saccades. A\nnecessary step in saccade detection involves accurately identifying the precise\nlocation of the pupil within the eye, from which additional information such as\ngaze angles can be inferred. Conventional frame-based cameras often struggle\nwith the high temporal precision necessary for tracking very fast movements,\nresulting in motion blur and latency issues. Event cameras, on the other hand,\noffer a promising alternative by recording changes in the visual scene\nasynchronously and providing high temporal resolution and low latency. By\nbridging the gap between traditional computer vision and event-based vision, we\npresent events as frames that can be readily utilized by standard deep learning\nalgorithms. This approach harnesses YOLOv8, a state-of-the-art object detection\ntechnology, to process these frames for pupil tracking using the publicly\naccessible Ev-Eye dataset. Experimental results demonstrate the framework's\neffectiveness, highlighting its potential applications in neuroscience,\nophthalmology, and human-computer interaction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is a preprint of a paper submitted to the 26th Irish\n  Machine Vision and Image Processing Conference (IMVIP 2024). If accepted, the\n  copy of record will be available at IET Digital Library",
    "pdf_url": "http://arxiv.org/pdf/2407.16665v2",
    "published_date": "2024-07-23 17:32:02 UTC",
    "updated_date": "2024-10-07 09:46:07 UTC"
  },
  {
    "arxiv_id": "2407.16641v1",
    "title": "A Geometry-Aware Algorithm to Learn Hierarchical Embeddings in Hyperbolic Space",
    "authors": [
      "Zhangyu Wang",
      "Lantian Xu",
      "Zhifeng Kong",
      "Weilong Wang",
      "Xuyu Peng",
      "Enyang Zheng"
    ],
    "abstract": "Hyperbolic embeddings are a class of representation learning methods that\noffer competitive performances when data can be abstracted as a tree-like\ngraph. However, in practice, learning hyperbolic embeddings of hierarchical\ndata is difficult due to the different geometry between hyperbolic space and\nthe Euclidean space. To address such difficulties, we first categorize three\nkinds of illness that harm the performance of the embeddings. Then, we develop\na geometry-aware algorithm using a dilation operation and a transitive closure\nregularization to tackle these illnesses. We empirically validate these\ntechniques and present a theoretical analysis of the mechanism behind the\ndilation operation. Experiments on synthetic and real-world datasets reveal\nsuperior performances of our algorithm.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16641v1",
    "published_date": "2024-07-23 16:56:59 UTC",
    "updated_date": "2024-07-23 16:56:59 UTC"
  },
  {
    "arxiv_id": "2407.16637v2",
    "title": "Course-Correction: Safety Alignment Using Synthetic Preferences",
    "authors": [
      "Rongwu Xu",
      "Yishuo Cai",
      "Zhenhong Zhou",
      "Renjie Gu",
      "Haiqin Weng",
      "Yan Liu",
      "Tianwei Zhang",
      "Wei Xu",
      "Han Qiu"
    ],
    "abstract": "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper accepted to EMNLP 2024. Camera-ready version. We have released\n  our dataset and scripts at https://github.com/pillowsofwind/Course-Correction",
    "pdf_url": "http://arxiv.org/pdf/2407.16637v2",
    "published_date": "2024-07-23 16:54:28 UTC",
    "updated_date": "2024-10-26 15:29:46 UTC"
  },
  {
    "arxiv_id": "2407.16634v1",
    "title": "Knowledge-driven AI-generated data for accurate and interpretable breast ultrasound diagnoses",
    "authors": [
      "Haojun Yu",
      "Youcheng Li",
      "Nan Zhang",
      "Zihan Niu",
      "Xuantong Gong",
      "Yanwen Luo",
      "Quanlin Wu",
      "Wangyan Qin",
      "Mengyuan Zhou",
      "Jie Han",
      "Jia Tao",
      "Ziwei Zhao",
      "Di Dai",
      "Di He",
      "Dong Wang",
      "Binghui Tang",
      "Ling Huo",
      "Qingli Zhu",
      "Yong Wang",
      "Liwei Wang"
    ],
    "abstract": "Data-driven deep learning models have shown great capabilities to assist\nradiologists in breast ultrasound (US) diagnoses. However, their effectiveness\nis limited by the long-tail distribution of training data, which leads to\ninaccuracies in rare cases. In this study, we address a long-standing challenge\nof improving the diagnostic model performance on rare cases using long-tailed\ndata. Specifically, we introduce a pipeline, TAILOR, that builds a\nknowledge-driven generative model to produce tailored synthetic data. The\ngenerative model, using 3,749 lesions as source data, can generate millions of\nbreast-US images, especially for error-prone rare cases. The generated data can\nbe further used to build a diagnostic model for accurate and interpretable\ndiagnoses. In the prospective external evaluation, our diagnostic model\noutperforms the average performance of nine radiologists by 33.5% in\nspecificity with the same sensitivity, improving their performance by providing\npredictions with an interpretable decision-making process. Moreover, on ductal\ncarcinoma in situ (DCIS), our diagnostic model outperforms all radiologists by\na large margin, with only 34 DCIS lesions in the source data. We believe that\nTAILOR can potentially be extended to various diseases and imaging modalities.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16634v1",
    "published_date": "2024-07-23 16:49:01 UTC",
    "updated_date": "2024-07-23 16:49:01 UTC"
  },
  {
    "arxiv_id": "2407.16616v1",
    "title": "Implementing engrams from a machine learning perspective: the relevance of a latent space",
    "authors": [
      "J Marco de Lucas"
    ],
    "abstract": "In our previous work, we proposed that engrams in the brain could be\nbiologically implemented as autoencoders over recurrent neural networks. These\nautoencoders would comprise basic excitatory/inhibitory motifs, with credit\nassignment deriving from a simple homeostatic criterion. This brief note\nexamines the relevance of the latent space in these autoencoders. We consider\nthe relationship between the dimensionality of these autoencoders and the\ncomplexity of the information being encoded. We discuss how observed\ndifferences between species in their connectome could be linked to their\ncognitive capacities. Finally, we link this analysis with a basic but often\noverlooked fact: human cognition is likely limited by our own brain structure.\nHowever, this limitation does not apply to machine learning systems, and we\nshould be aware of the need to learn how to exploit this augmented vision of\nthe nature.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "6 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.16616v1",
    "published_date": "2024-07-23 16:24:29 UTC",
    "updated_date": "2024-07-23 16:24:29 UTC"
  },
  {
    "arxiv_id": "2407.16735v1",
    "title": "Theoretical Analysis of Privacy Leakage in Trustworthy Federated Learning: A Perspective from Linear Algebra and Optimization Theory",
    "authors": [
      "Xiaojin Zhang",
      "Wei Chen"
    ],
    "abstract": "Federated learning has emerged as a promising paradigm for collaborative\nmodel training while preserving data privacy. However, recent studies have\nshown that it is vulnerable to various privacy attacks, such as data\nreconstruction attacks. In this paper, we provide a theoretical analysis of\nprivacy leakage in federated learning from two perspectives: linear algebra and\noptimization theory. From the linear algebra perspective, we prove that when\nthe Jacobian matrix of the batch data is not full rank, there exist different\nbatches of data that produce the same model update, thereby ensuring a level of\nprivacy. We derive a sufficient condition on the batch size to prevent data\nreconstruction attacks. From the optimization theory perspective, we establish\nan upper bound on the privacy leakage in terms of the batch size, the\ndistortion extent, and several other factors. Our analysis provides insights\ninto the relationship between privacy leakage and various aspects of federated\nlearning, offering a theoretical foundation for designing privacy-preserving\nfederated learning algorithms.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16735v1",
    "published_date": "2024-07-23 16:23:38 UTC",
    "updated_date": "2024-07-23 16:23:38 UTC"
  },
  {
    "arxiv_id": "2407.16615v2",
    "title": "Lawma: The Power of Specialization for Legal Annotation",
    "authors": [
      "Ricardo Dominguez-Olmedo",
      "Vedant Nanda",
      "Rediet Abebe",
      "Stefan Bechtold",
      "Christoph Engel",
      "Jens Frankenreiter",
      "Krishna Gummadi",
      "Moritz Hardt",
      "Michael Livermore"
    ],
    "abstract": "Annotation and classification of legal text are central components of\nempirical legal research. Traditionally, these tasks are often delegated to\ntrained research assistants. Motivated by the advances in language modeling,\nempirical legal scholars are increasingly turning to prompting commercial\nmodels, hoping that it will alleviate the significant cost of human annotation.\nDespite growing use, our understanding of how to best utilize large language\nmodels for legal annotation remains limited. To bridge this gap, we introduce\nCaselawQA, a benchmark comprising 260 legal annotation tasks, nearly all new to\nthe machine learning community. We demonstrate that commercial models, such as\nGPT-4.5 and Claude 3.7 Sonnet, achieve non-trivial yet highly variable\naccuracy, generally falling short of the performance required for legal work.\nWe then demonstrate that small, lightly fine-tuned models outperform commercial\nmodels. A few hundred to a thousand labeled examples are usually enough to\nachieve higher accuracy. Our work points to a viable alternative to the\npredominant practice of prompting commercial models. For concrete legal\nannotation tasks with some available labeled data, researchers are likely\nbetter off using a fine-tuned open-source model.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.16615v2",
    "published_date": "2024-07-23 16:23:04 UTC",
    "updated_date": "2025-04-23 12:18:56 UTC"
  },
  {
    "arxiv_id": "2407.16613v1",
    "title": "No-brainer: Morphological Computation driven Adaptive Behavior in Soft Robots",
    "authors": [
      "Alican Mertan",
      "Nick Cheney"
    ],
    "abstract": "It is prevalent in contemporary AI and robotics to separately postulate a\nbrain modeled by neural networks and employ it to learn intelligent and\nadaptive behavior. While this method has worked very well for many types of\ntasks, it isn't the only type of intelligence that exists in nature. In this\nwork, we study the ways in which intelligent behavior can be created without a\nseparate and explicit brain for robot control, but rather solely as a result of\nthe computation occurring within the physical body of a robot. Specifically, we\nshow that adaptive and complex behavior can be created in voxel-based virtual\nsoft robots by using simple reactive materials that actively change the shape\nof the robot, and thus its behavior, under different environmental cues. We\ndemonstrate a proof of concept for the idea of closed-loop morphological\ncomputation, and show that in our implementation, it enables behavior mimicking\nlogic gates, enabling us to demonstrate how such behaviors may be combined to\nbuild up more complex collective behaviors.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to the From Animals to Animats: 17th International\n  Conference on the Simulation of Adaptive Behavior (SAB 2024) conference",
    "pdf_url": "http://arxiv.org/pdf/2407.16613v1",
    "published_date": "2024-07-23 16:20:36 UTC",
    "updated_date": "2024-07-23 16:20:36 UTC"
  },
  {
    "arxiv_id": "2407.16611v1",
    "title": "Local vs Global continual learning",
    "authors": [
      "Giulia Lanzillotta",
      "Sidak Pal Singh",
      "Benjamin F. Grewe",
      "Thomas Hofmann"
    ],
    "abstract": "Continual learning is the problem of integrating new information in a model\nwhile retaining the knowledge acquired in the past. Despite the tangible\nimprovements achieved in recent years, the problem of continual learning is\nstill an open one. A better understanding of the mechanisms behind the\nsuccesses and failures of existing continual learning algorithms can unlock the\ndevelopment of new successful strategies. In this work, we view continual\nlearning from the perspective of the multi-task loss approximation, and we\ncompare two alternative strategies, namely local and global approximations. We\nclassify existing continual learning algorithms based on the approximation\nused, and we assess the practical effects of this distinction in common\ncontinual learning settings.Additionally, we study optimal continual learning\nobjectives in the case of local polynomial approximations and we provide\nexamples of existing algorithms implementing the optimal objectives",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "(10 pages, Will appear in the proceedings of CoLLAs 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.16611v1",
    "published_date": "2024-07-23 16:18:00 UTC",
    "updated_date": "2024-07-23 16:18:00 UTC"
  },
  {
    "arxiv_id": "2407.16608v1",
    "title": "Deep Bayesian segmentation for colon polyps: Well-calibrated predictions in medical imaging",
    "authors": [
      "Daniela L. Ramos",
      "Hector J. Hortua"
    ],
    "abstract": "Colorectal polyps are generally benign alterations that, if not identified\npromptly and managed successfully, can progress to cancer and cause\naffectations on the colon mucosa, known as adenocarcinoma. Today advances in\nDeep Learning have demonstrated the ability to achieve significant performance\nin image classification and detection in medical diagnosis applications.\nNevertheless, these models are prone to overfitting, and making decisions based\nonly on point estimations may provide incorrect predictions. Thus, to obtain a\nmore informed decision, we must consider point estimations along with their\nreliable uncertainty quantification. In this paper, we built different Bayesian\nneural network approaches based on the flexibility of posterior distribution to\ndevelop semantic segmentation of colorectal polyp images. We found that these\nmodels not only provide state-of-the-art performance on the segmentation of\nthis medical dataset but also, yield accurate uncertainty estimates. We applied\nmultiplicative normalized flows(MNF) and reparameterization trick on the UNET,\nFPN, and LINKNET architectures tested with multiple backbones in deterministic\nand Bayesian versions. We report that the FPN + EfficientnetB7 architecture\nwith MNF is the most promising option given its IOU of 0.94 and Expected\nCalibration Error (ECE) of 0.004, combined with its superiority in identifying\ndifficult-to-detect colorectal polyps, which is effective in clinical areas\nwhere early detection prevents the development of colon cancer.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "comments are welcome. 43 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.16608v1",
    "published_date": "2024-07-23 16:13:27 UTC",
    "updated_date": "2024-07-23 16:13:27 UTC"
  },
  {
    "arxiv_id": "2407.16602v2",
    "title": "Functional Acceleration for Policy Mirror Descent",
    "authors": [
      "Veronica Chelu",
      "Doina Precup"
    ],
    "abstract": "We apply functional acceleration to the Policy Mirror Descent (PMD) general\nfamily of algorithms, which cover a wide range of novel and fundamental methods\nin Reinforcement Learning (RL). Leveraging duality, we propose a momentum-based\nPMD update. By taking the functional route, our approach is independent of the\npolicy parametrization and applicable to large-scale optimization, covering\nprevious applications of momentum at the level of policy parameters as a\nspecial case. We theoretically analyze several properties of this approach and\ncomplement with a numerical ablation study, which serves to illustrate the\npolicy optimization dynamics on the value polytope, relative to different\nalgorithmic design choices in this space. We further characterize numerically\nseveral features of the problem setting relevant for functional acceleration,\nand lastly, we investigate the impact of approximation on their learning\nmechanics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16602v2",
    "published_date": "2024-07-23 16:04:55 UTC",
    "updated_date": "2025-03-25 17:30:54 UTC"
  },
  {
    "arxiv_id": "2407.16594v2",
    "title": "Flexible Generation of Preference Data for Recommendation Analysis",
    "authors": [
      "Simone Mungari",
      "Erica Coppolillo",
      "Ettore Ritacco",
      "Giuseppe Manco"
    ],
    "abstract": "Simulating a recommendation system in a controlled environment, to identify\nspecific behaviors and user preferences, requires highly flexible synthetic\ndata generation models capable of mimicking the patterns and trends of real\ndatasets. In this context, we propose HYDRA, a novel preferences data\ngeneration model driven by three main factors: user-item interaction level,\nitem popularity, and user engagement level. The key innovations of the proposed\nprocess include the ability to generate user communities characterized by\nsimilar item adoptions, reflecting real-world social influences and trends.\nAdditionally, HYDRA considers item popularity and user engagement as mixtures\nof different probability distributions, allowing for a more realistic\nsimulation of diverse scenarios. This approach enhances the model's capacity to\nsimulate a wide range of real-world cases, capturing the complexity and\nvariability found in actual user behavior. We demonstrate the effectiveness of\nHYDRA through extensive experiments on well-known benchmark datasets. The\nresults highlight its capability to replicate real-world data patterns,\noffering valuable insights for developing and testing recommendation systems in\na controlled and realistic manner. The code used to perform the experiments is\npublicly available at https://github.com/SimoneMungari/HYDRA.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16594v2",
    "published_date": "2024-07-23 15:53:17 UTC",
    "updated_date": "2025-05-16 10:53:01 UTC"
  },
  {
    "arxiv_id": "2407.16593v1",
    "title": "A Comparative Study on Patient Language across Therapeutic Domains for Effective Patient Voice Classification in Online Health Discussions",
    "authors": [
      "Giorgos Lysandrou",
      "Roma English Owen",
      "Vanja Popovic",
      "Grant Le Brun",
      "Aryo Pradipta Gema",
      "Beatrice Alex",
      "Elizabeth A. L. Fairley"
    ],
    "abstract": "There exists an invisible barrier between healthcare professionals'\nperception of a patient's clinical experience and the reality. This barrier may\nbe induced by the environment that hinders patients from sharing their\nexperiences openly with healthcare professionals. As patients are observed to\ndiscuss and exchange knowledge more candidly on social media, valuable insights\ncan be leveraged from these platforms. However, the abundance of non-patient\nposts on social media necessitates filtering out such irrelevant content to\ndistinguish the genuine voices of patients, a task we refer to as patient voice\nclassification. In this study, we analyse the importance of linguistic\ncharacteristics in accurately classifying patient voices. Our findings\nunderscore the essential role of linguistic and statistical text similarity\nanalysis in identifying common patterns among patient groups. These results\nallude to even starker differences in the way patients express themselves at a\ndisease level and across various therapeutic domains. Additionally, we\nfine-tuned a pre-trained Language Model on the combined datasets with similar\nlinguistic patterns, resulting in a highly accurate automatic patient voice\nclassification. Being the pioneering study on the topic, our focus on\nextracting authentic patient experiences from social media stands as a crucial\nstep towards advancing healthcare standards and fostering a patient-centric\napproach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 4 figures, 5 tables, funded by Talking Medicines Limited",
    "pdf_url": "http://arxiv.org/pdf/2407.16593v1",
    "published_date": "2024-07-23 15:51:46 UTC",
    "updated_date": "2024-07-23 15:51:46 UTC"
  },
  {
    "arxiv_id": "2407.16588v2",
    "title": "A Faster Branching Algorithm for the Maximum $k$-Defective Clique Problem",
    "authors": [
      "Chunyu Luo",
      "Yi Zhou",
      "Zhengren Wang",
      "Mingyu Xiao"
    ],
    "abstract": "A $k$-defective clique of an undirected graph $G$ is a subset of its vertices\nthat induces a nearly complete graph with a maximum of $k$ missing edges. The\nmaximum $k$-defective clique problem, which asks for the largest $k$-defective\nclique from the given graph, is important in many applications, such as social\nand biological network analysis. In the paper, we propose a new branching\nalgorithm that takes advantage of the structural properties of the\n$k$-defective clique and uses the efficient maximum clique algorithm as a\nsubroutine. As a result, the algorithm has a better asymptotic running time\nthan the existing ones. We also investigate upper-bounding techniques and\npropose a new upper bound utilizing the \\textit{conflict relationship} between\nvertex pairs. Because conflict relationship is common in many graph problems,\nwe believe that this technique can be potentially generalized. Finally,\nexperiments show that our algorithm outperforms state-of-the-art solvers on a\nwide range of open benchmarks.",
    "categories": [
      "cs.DS",
      "cs.AI"
    ],
    "primary_category": "cs.DS",
    "comment": "The accepted paper of confernece ECAI-2024 as well as the appendix",
    "pdf_url": "http://arxiv.org/pdf/2407.16588v2",
    "published_date": "2024-07-23 15:40:35 UTC",
    "updated_date": "2024-07-24 02:44:58 UTC"
  },
  {
    "arxiv_id": "2407.18981v1",
    "title": "Prompt Injection Attacks on Large Language Models in Oncology",
    "authors": [
      "Jan Clusmann",
      "Dyke Ferber",
      "Isabella C. Wiest",
      "Carolin V. Schneider",
      "Titus J. Brinker",
      "Sebastian Foersch",
      "Daniel Truhn",
      "Jakob N. Kather"
    ],
    "abstract": "Vision-language artificial intelligence models (VLMs) possess medical\nknowledge and can be employed in healthcare in numerous ways, including as\nimage interpreters, virtual scribes, and general decision support systems.\nHowever, here, we demonstrate that current VLMs applied to medical tasks\nexhibit a fundamental security flaw: they can be attacked by prompt injection\nattacks, which can be used to output harmful information just by interacting\nwith the VLM, without any access to its parameters. We performed a quantitative\nstudy to evaluate the vulnerabilities to these attacks in four state of the art\nVLMs which have been proposed to be of utility in healthcare: Claude 3 Opus,\nClaude 3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N=297 attacks, we show\nthat all of these models are susceptible. Specifically, we show that embedding\nsub-visual prompts in medical imaging data can cause the model to provide\nharmful output, and that these prompts are non-obvious to human observers.\nThus, our study demonstrates a key vulnerability in medical VLMs which should\nbe mitigated before widespread clinical adoption.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "57 Pages, 5 Figures",
    "pdf_url": "http://arxiv.org/pdf/2407.18981v1",
    "published_date": "2024-07-23 15:29:57 UTC",
    "updated_date": "2024-07-23 15:29:57 UTC"
  },
  {
    "arxiv_id": "2407.16732v2",
    "title": "PyBench: Evaluating LLM Agent on various real-world coding tasks",
    "authors": [
      "Yaolun Zhang",
      "Yinxu Pan",
      "Yudong Wang",
      "Jie Cai"
    ],
    "abstract": "The LLM Agent, equipped with a code interpreter, is capable of automatically\nsolving real-world coding tasks, such as data analysis and image editing.\n  However, existing benchmarks primarily focus on either simplistic tasks, such\nas completing a few lines of code, or on extremely complex and specific tasks\nat the repository level, neither of which are representative of various daily\ncoding tasks.\n  To address this gap, we introduce \\textbf{PyBench}, a benchmark encompassing\nfive main categories of real-world tasks, covering more than 10 types of files.\nGiven a high-level user query and related files, the LLM Agent needs to reason\nand execute Python code via a code interpreter for a few turns before making a\nformal response to fulfill the user's requirements. Successfully addressing\ntasks in PyBench demands a robust understanding of various Python packages,\nsuperior reasoning capabilities, and the ability to incorporate feedback from\nexecuted code. Our evaluations indicate that current open-source LLMs are\nstruggling with these tasks. Hence, we conduct analysis and experiments on four\nkinds of datasets proving that comprehensive abilities are needed for PyBench.\nOur fine-tuned 8B size model: \\textbf{PyLlama3} achieves an exciting\nperformance on PyBench which surpasses many 33B and 70B size models. Our\nBenchmark, Training Dataset, and Model are available at:\n{https://github.com/Mercury7353/PyBench}",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.16732v2",
    "published_date": "2024-07-23 15:23:14 UTC",
    "updated_date": "2024-08-03 03:00:43 UTC"
  },
  {
    "arxiv_id": "2407.16564v2",
    "title": "Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music with Lightweight Finetuning",
    "authors": [
      "Fang-Duo Tsai",
      "Shih-Lun Wu",
      "Haven Kim",
      "Bo-Yu Chen",
      "Hao-Chung Cheng",
      "Yi-Hsuan Yang"
    ],
    "abstract": "Text-to-music models allow users to generate nearly realistic musical audio\nwith textual commands. However, editing music audios remains challenging due to\nthe conflicting desiderata of performing fine-grained alterations on the audio\nwhile maintaining a simple user interface. To address this challenge, we\npropose Audio Prompt Adapter (or AP-Adapter), a lightweight addition to\npretrained text-to-music models. We utilize AudioMAE to extract features from\nthe input audio, and construct attention-based adapters to feedthese features\ninto the internal layers of AudioLDM2, a diffusion-based text-to-music model.\nWith 22M trainable parameters, AP-Adapter empowers users to harness both global\n(e.g., genre and timbre) and local (e.g., melody) aspects of music, using the\noriginal audio and a short text as inputs. Through objective and subjective\nstudies, we evaluate AP-Adapter on three tasks: timbre transfer, genre\ntransfer, and accompaniment generation. Additionally, we demonstrate its\neffectiveness on out-of-domain audios containing unseen instruments during\ntraining.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by the 25th International Society for Music Information\n  Retrieval (ISMIR)",
    "pdf_url": "http://arxiv.org/pdf/2407.16564v2",
    "published_date": "2024-07-23 15:16:18 UTC",
    "updated_date": "2024-07-24 11:12:15 UTC"
  },
  {
    "arxiv_id": "2407.16557v3",
    "title": "Patched RTC: evaluating LLMs for diverse software development tasks",
    "authors": [
      "Asankhaya Sharma"
    ],
    "abstract": "This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel\nevaluation technique for Large Language Models (LLMs) applied to diverse\nsoftware development tasks, particularly focusing on \"outer loop\" activities\nsuch as bug fixing, code review, and documentation updates. Patched RTC extends\nthe original Round-Trip Correctness method to work with any LLM and downstream\ntask, offering a self-evaluating framework that measures consistency and\nrobustness of model responses without human intervention. The study\ndemonstrates a correlation between Patched RTC scores and task-specific\naccuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm\nfor open-domain task evaluation. We implement Patched RTC in an open-source\nframework called patchwork, allowing for transparent evaluation during\ninference across various patchflows. Experiments comparing GPT-3.5 and GPT-4\nmodels across different software development tasks reveal that Patched RTC\neffectively distinguishes model performance and task difficulty. The paper also\nexplores the impact of consistency prompts on improving model accuracy,\nsuggesting that Patched RTC can guide prompt refinement and model selection for\ncomplex software development workflows.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16557v3",
    "published_date": "2024-07-23 15:12:14 UTC",
    "updated_date": "2025-04-29 23:30:03 UTC"
  },
  {
    "arxiv_id": "2407.16729v1",
    "title": "PateGail: A Privacy-Preserving Mobility Trajectory Generator with Imitation Learning",
    "authors": [
      "Huandong Wang",
      "Changzheng Gao",
      "Yuchen Wu",
      "Depeng Jin",
      "Lina Yao",
      "Yong Li"
    ],
    "abstract": "Generating human mobility trajectories is of great importance to solve the\nlack of large-scale trajectory data in numerous applications, which is caused\nby privacy concerns. However, existing mobility trajectory generation methods\nstill require real-world human trajectories centrally collected as the training\ndata, where there exists an inescapable risk of privacy leakage. To overcome\nthis limitation, in this paper, we propose PateGail, a privacy-preserving\nimitation learning model to generate mobility trajectories, which utilizes the\npowerful generative adversary imitation learning model to simulate the\ndecision-making process of humans. Further, in order to protect user privacy,\nwe train this model collectively based on decentralized mobility data stored in\nuser devices, where personal discriminators are trained locally to distinguish\nand reward the real and generated human trajectories. In the training process,\nonly the generated trajectories and their rewards obtained based on personal\ndiscriminators are shared between the server and devices, whose privacy is\nfurther preserved by our proposed perturbation mechanisms with theoretical\nproof to satisfy differential privacy. Further, to better model the human\ndecision-making process, we propose a novel aggregation mechanism of the\nrewards obtained from personal discriminators. We theoretically prove that\nunder the reward obtained based on the aggregation mechanism, our proposed\nmodel maximizes the lower bound of the discounted total rewards of users.\nExtensive experiments show that the trajectories generated by our model are\nable to resemble real-world trajectories in terms of five key statistical\nmetrics, outperforming state-of-the-art algorithms by over 48.03%. Furthermore,\nwe demonstrate that the synthetic trajectories are able to efficiently support\npractical applications, including mobility prediction and location\nrecommendation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16729v1",
    "published_date": "2024-07-23 14:59:23 UTC",
    "updated_date": "2024-07-23 14:59:23 UTC"
  },
  {
    "arxiv_id": "2407.16533v1",
    "title": "HAPFI: History-Aware Planning based on Fused Information",
    "authors": [
      "Sujin Jeon",
      "Suyeon Shin",
      "Byoung-Tak Zhang"
    ],
    "abstract": "Embodied Instruction Following (EIF) is a task of planning a long sequence of\nsub-goals given high-level natural language instructions, such as \"Rinse a\nslice of lettuce and place on the white table next to the fork\". To\nsuccessfully execute these long-term horizon tasks, we argue that an agent must\nconsider its past, i.e., historical data, when making decisions in each step.\nNevertheless, recent approaches in EIF often neglects the knowledge from\nhistorical data and also do not effectively utilize information across the\nmodalities. To this end, we propose History-Aware Planning based on Fused\nInformation (HAPFI), effectively leveraging the historical data from diverse\nmodalities that agents collect while interacting with the environment.\nSpecifically, HAPFI integrates multiple modalities, including historical RGB\nobservations, bounding boxes, sub-goals, and high-level instructions, by\neffectively fusing modalities via our Mutually Attentive Fusion method. Through\nexperiments with diverse comparisons, we show that an agent utilizing\nhistorical multi-modal information surpasses all the compared methods that\nneglect the historical data in terms of action planning capability, enabling\nthe generation of well-informed action plans for the next step. Moreover, we\nprovided qualitative evidence highlighting the significance of leveraging\nhistorical multi-modal data, particularly in scenarios where the agent\nencounters intermediate failures, showcasing its robust re-planning\ncapabilities.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 3 figures, published to ICRA 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16533v1",
    "published_date": "2024-07-23 14:46:07 UTC",
    "updated_date": "2024-07-23 14:46:07 UTC"
  },
  {
    "arxiv_id": "2407.16728v1",
    "title": "Distributed Difference of Convex Optimization",
    "authors": [
      "Vivek Khatana",
      "Murti V. Salapaka"
    ],
    "abstract": "In this article, we focus on solving a class of distributed optimization\nproblems involving $n$ agents with the local objective function at every agent\n$i$ given by the difference of two convex functions $f_i$ and $g_i$\n(difference-of-convex (DC) form), where $f_i$ and $g_i$ are potentially\nnonsmooth. The agents communicate via a directed graph containing $n$ nodes. We\ncreate smooth approximations of the functions $f_i$ and $g_i$ and develop a\ndistributed algorithm utilizing the gradients of the smooth surrogates and a\nfinite-time approximate consensus protocol. We term this algorithm as\nDDC-Consensus. The developed DDC-Consensus algorithm allows for non-symmetric\ndirected graph topologies and can be synthesized distributively. We establish\nthat the DDC-Consensus algorithm converges to a stationary point of the\nnonconvex distributed optimization problem. The performance of the\nDDC-Consensus algorithm is evaluated via a simulation study to solve a\nnonconvex DC-regularized distributed least squares problem. The numerical\nresults corroborate the efficacy of the proposed algorithm.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.DC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC",
    "comment": "9 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.16728v1",
    "published_date": "2024-07-23 14:41:32 UTC",
    "updated_date": "2024-07-23 14:41:32 UTC"
  },
  {
    "arxiv_id": "2407.16526v1",
    "title": "Imperfect Vision Encoders: Efficient and Robust Tuning for Vision-Language Models",
    "authors": [
      "Aristeidis Panos",
      "Rahaf Aljundi",
      "Daniel Olmeda Reino",
      "Richard E Turner"
    ],
    "abstract": "Vision language models (VLMs) demonstrate impressive capabilities in visual\nquestion answering and image captioning, acting as a crucial link between\nvisual and language models. However, existing open-source VLMs heavily rely on\npretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness\nacross diverse domains, it still exhibits non-negligible image understanding\nerrors. These errors propagate to the VLM responses, resulting in sub-optimal\nperformance. In our work, we propose an efficient and robust method for\nupdating vision encoders within VLMs. Our approach selectively and locally\nupdates encoders, leading to substantial performance improvements on data where\nprevious mistakes occurred, while maintaining overall robustness. Furthermore,\nwe demonstrate the effectiveness of our method during continual few-shot\nupdates. Theoretical grounding, generality, and computational efficiency\ncharacterize our approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16526v1",
    "published_date": "2024-07-23 14:39:40 UTC",
    "updated_date": "2024-07-23 14:39:40 UTC"
  },
  {
    "arxiv_id": "2407.16514v1",
    "title": "Is 3D Convolution with 5D Tensors Really Necessary for Video Analysis?",
    "authors": [
      "Habib Hajimolahoseini",
      "Walid Ahmed",
      "Austin Wen",
      "Yang Liu"
    ],
    "abstract": "In this paper, we present a comprehensive study and propose several novel\ntechniques for implementing 3D convolutional blocks using 2D and/or 1D\nconvolutions with only 4D and/or 3D tensors. Our motivation is that 3D\nconvolutions with 5D tensors are computationally very expensive and they may\nnot be supported by some of the edge devices used in real-time applications\nsuch as robots. The existing approaches mitigate this by splitting the 3D\nkernels into spatial and temporal domains, but they still use 3D convolutions\nwith 5D tensors in their implementations. We resolve this issue by introducing\nsome appropriate 4D/3D tensor reshaping as well as new combination techniques\nfor spatial and temporal splits. The proposed implementation methods show\nsignificant improvement both in terms of efficiency and accuracy. The\nexperimental results confirm that the proposed spatio-temporal processing\nstructure outperforms the original model in terms of speed and accuracy using\nonly 4D tensors with fewer parameters.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16514v1",
    "published_date": "2024-07-23 14:30:51 UTC",
    "updated_date": "2024-07-23 14:30:51 UTC"
  },
  {
    "arxiv_id": "2407.16496v2",
    "title": "Articulation Work and Tinkering for Fairness in Machine Learning",
    "authors": [
      "Miriam Fahimi",
      "Mayra Russo",
      "Kristen M. Scott",
      "Maria-Esther Vidal",
      "Bettina Berendt",
      "Katharina Kinder-Kurlanda"
    ],
    "abstract": "The field of fair AI aims to counter biased algorithms through computational\nmodelling. However, it faces increasing criticism for perpetuating the use of\noverly technical and reductionist methods. As a result, novel approaches appear\nin the field to address more socially-oriented and interdisciplinary (SOI)\nperspectives on fair AI. In this paper, we take this dynamic as the starting\npoint to study the tension between computer science (CS) and SOI research. By\ndrawing on STS and CSCW theory, we position fair AI research as a matter of\n'organizational alignment': what makes research 'doable' is the successful\nalignment of three levels of work organization (the social world, the\nlaboratory, and the experiment). Based on qualitative interviews with CS\nresearchers, we analyze the tasks, resources, and actors required for doable\nresearch in the case of fair AI. We find that CS researchers engage with SOI\nresearch to some extent, but organizational conditions, articulation work, and\nambiguities of the social world constrain the doability of SOI research for\nthem. Based on our findings, we identify and discuss problems for aligning CS\nand SOI as fair AI continues to evolve.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "K.4.3; I.2.0"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16496v2",
    "published_date": "2024-07-23 14:11:12 UTC",
    "updated_date": "2024-08-28 12:20:42 UTC"
  },
  {
    "arxiv_id": "2407.16485v3",
    "title": "Learning Constraint Network from Demonstrations via Positive-Unlabeled Learning with Memory Replay",
    "authors": [
      "Baiyu Peng",
      "Aude Billard"
    ],
    "abstract": "Planning for a wide range of real-world tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. The majority of prior works\nlimit themselves to learning simple linear constraints, or require strong\nknowledge of the true constraint parameterization or environmental model. To\nmitigate these problems, this paper presents a positive-unlabeled (PU) learning\napproach to infer a continuous, arbitrary and possibly nonlinear, constraint\nfrom demonstration. From a PU learning view, We treat all data in\ndemonstrations as positive (feasible) data, and learn a (sub)-optimal policy to\ngenerate high-reward-winning but potentially infeasible trajectories, which\nserve as unlabeled data containing both feasible and infeasible states. Under\nan assumption on data distribution, a feasible-infeasible classifier (i.e.,\nconstraint model) is learned from the two datasets through a postprocessing PU\nlearning technique. The entire method employs an iterative framework\nalternating between updating the policy, which generates and selects\nhigher-reward policies, and updating the constraint model. Additionally, a\nmemory buffer is introduced to record and reuse samples from previous\niterations to prevent forgetting. The effectiveness of the proposed method is\nvalidated in two Mujoco environments, successfully inferring continuous\nnonlinear constraints and outperforming a baseline method in terms of\nconstraint accuracy and policy safety.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16485v3",
    "published_date": "2024-07-23 14:00:18 UTC",
    "updated_date": "2025-01-16 11:59:02 UTC"
  },
  {
    "arxiv_id": "2408.06346v1",
    "title": "Closing the Affective Loop via Experience-Driven Reinforcement Learning Designers",
    "authors": [
      "Matthew Barthet",
      "Diogo Branco",
      "Roberto Gallotta",
      "Ahmed Khalifa",
      "Georgios N. Yannakakis"
    ],
    "abstract": "Autonomously tailoring content to a set of predetermined affective patterns\nhas long been considered the holy grail of affect-aware human-computer\ninteraction at large. The experience-driven procedural content generation\nframework realises this vision by searching for content that elicits a certain\nexperience pattern to a user. In this paper, we propose a novel reinforcement\nlearning (RL) framework for generating affect-tailored content, and we test it\nin the domain of racing games. Specifically, the experience-driven RL (EDRL)\nframework is given a target arousal trace, and it then generates a racetrack\nthat elicits the desired affective responses for a particular type of player.\nEDRL leverages a reward function that assesses the affective pattern of any\ngenerated racetrack from a corpus of arousal traces. Our findings suggest that\nEDRL can accurately generate affect-driven racing game levels according to a\ndesigner's style and outperforms search-based methods for personalised content\ngeneration. The method is not only directly applicable to game content\ngeneration tasks but also employable broadly to any domain that uses content\nfor affective adaptation.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "9 pages, 4 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2408.06346v1",
    "published_date": "2024-07-23 13:56:43 UTC",
    "updated_date": "2024-07-23 13:56:43 UTC"
  },
  {
    "arxiv_id": "2407.16726v1",
    "title": "Topology Reorganized Graph Contrastive Learning with Mitigating Semantic Drift",
    "authors": [
      "Jiaqiang Zhang",
      "Songcan Chen"
    ],
    "abstract": "Graph contrastive learning (GCL) is an effective paradigm for node\nrepresentation learning in graphs. The key components hidden behind GCL are\ndata augmentation and positive-negative pair selection. Typical data\naugmentations in GCL, such as uniform deletion of edges, are generally blind\nand resort to local perturbation, which is prone to producing under-diversity\nviews. Additionally, there is a risk of making the augmented data traverse to\nother classes. Moreover, most methods always treat all other samples as\nnegatives. Such a negative pairing naturally results in sampling bias and\nlikewise may make the learned representation suffer from semantic drift.\nTherefore, to increase the diversity of the contrastive view, we propose two\nsimple and effective global topological augmentations to compensate current\nGCL. One is to mine the semantic correlation between nodes in the feature\nspace. The other is to utilize the algebraic properties of the adjacency matrix\nto characterize the topology by eigen-decomposition. With the help of both, we\ncan retain important edges to build a better view. To reduce the risk of\nsemantic drift, a prototype-based negative pair selection is further designed\nwhich can filter false negative samples. Extensive experiments on various tasks\ndemonstrate the advantages of the model compared to the state-of-the-art\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16726v1",
    "published_date": "2024-07-23 13:55:33 UTC",
    "updated_date": "2024-07-23 13:55:33 UTC"
  },
  {
    "arxiv_id": "2407.16482v1",
    "title": "BONES: a Benchmark fOr Neural Estimation of Shapley values",
    "authors": [
      "Davide Napolitano",
      "Luca Cagliero"
    ],
    "abstract": "Shapley Values are concepts established for eXplainable AI. They are used to\nexplain black-box predictive models by quantifying the features' contributions\nto the model's outcomes. Since computing the exact Shapley Values is known to\nbe computationally intractable on real-world datasets, neural estimators have\nemerged as alternative, more scalable approaches to get approximated Shapley\nValues estimates. However, experiments with neural estimators are currently\nhard to replicate as algorithm implementations, explainer evaluators, and\nresults visualizations are neither standardized nor promptly usable. To bridge\nthis gap, we present BONES, a new benchmark focused on neural estimation of\nShapley Value. It provides researchers with a suite of state-of-the-art neural\nand traditional estimators, a set of commonly used benchmark datasets, ad hoc\nmodules for training black-box models, as well as specific functions to easily\ncompute the most popular evaluation metrics and visualize results. The purpose\nis to simplify XAI model usage, evaluation, and comparison. In this paper, we\nshowcase BONES results and visualizations for XAI model benchmarking on both\ntabular and image data. The open-source library is available at the following\nlink: https://github.com/DavideNapolitano/BONES.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.16482v1",
    "published_date": "2024-07-23 13:53:22 UTC",
    "updated_date": "2024-07-23 13:53:22 UTC"
  },
  {
    "arxiv_id": "2407.16470v3",
    "title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models",
    "authors": [
      "Kenza Benkirane",
      "Laura Gongas",
      "Shahar Pelles",
      "Naomi Fuchs",
      "Joshua Darmon",
      "Pontus Stenetorp",
      "David Ifeoluwa Adelani",
      "Eduardo Sánchez"
    ],
    "abstract": "Recent advancements in massively multilingual machine translation systems\nhave significantly enhanced translation accuracy; however, even the best\nperforming systems still generate hallucinations, severely impacting user\ntrust. Detecting hallucinations in Machine Translation (MT) remains a critical\nchallenge, particularly since existing methods excel with High-Resource\nLanguages (HRLs) but exhibit substantial limitations when applied to\nLow-Resource Languages (LRLs). This paper evaluates sentence-level\nhallucination detection approaches using Large Language Models (LLMs) and\nsemantic similarity within massively multilingual embeddings. Our study spans\n16 language directions, covering HRLs, LRLs, with diverse scripts. We find that\nthe choice of model is essential for performance. On average, for HRLs,\nLlama3-70B outperforms the previous state of the art by as much as 0.16 MCC\n(Matthews Correlation Coefficient). However, for LRLs we observe that Claude\nSonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our\nstudy is that LLMs can achieve performance comparable or even better than\npreviously proposed models, despite not being explicitly trained for any\nmachine translation task. However, their advantage is less significant for\nLRLs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Authors Kenza Benkirane and Laura Gongas contributed equally to this\n  work",
    "pdf_url": "http://arxiv.org/pdf/2407.16470v3",
    "published_date": "2024-07-23 13:40:54 UTC",
    "updated_date": "2024-10-20 15:01:05 UTC"
  },
  {
    "arxiv_id": "2407.16467v2",
    "title": "Side-Channel Analysis of OpenVINO-based Neural Network Models",
    "authors": [
      "Dirmanto Jap",
      "Jakub Breier",
      "Zdenko Lehocký",
      "Shivam Bhasin",
      "Xiaolu Hou"
    ],
    "abstract": "Embedded devices with neural network accelerators offer great versatility for\ntheir users, reducing the need to use cloud-based services. At the same time,\nthey introduce new security challenges in the area of hardware attacks, the\nmost prominent being side-channel analysis (SCA). It was shown that SCA can\nrecover model parameters with a high accuracy, posing a threat to entities that\nwish to keep their models confidential. In this paper, we explore the\nsusceptibility of quantized models implemented in OpenVINO, an embedded\nframework for deploying neural networks on embedded and Edge devices. We show\nthat it is possible to recover model parameters with high precision, allowing\nthe recovered model to perform very close to the original one. Our experiments\non GoogleNet v1 show only a 1% difference in the Top 1 and a 0.64% difference\nin the Top 5 accuracies.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16467v2",
    "published_date": "2024-07-23 13:33:37 UTC",
    "updated_date": "2024-08-20 11:48:43 UTC"
  },
  {
    "arxiv_id": "2407.16444v1",
    "title": "Psychomatics -- A Multidisciplinary Framework for Understanding Artificial Minds",
    "authors": [
      "Giuseppe Riva",
      "Fabrizia Mantovani",
      "Brenda K. Wiederhold",
      "Antonella Marchetti",
      "Andrea Gaggioli"
    ],
    "abstract": "Although LLMs and other artificial intelligence systems demonstrate cognitive\nskills similar to humans, like concept learning and language acquisition, the\nway they process information fundamentally differs from biological cognition.\nTo better understand these differences this paper introduces Psychomatics, a\nmultidisciplinary framework bridging cognitive science, linguistics, and\ncomputer science. It aims to better understand the high-level functioning of\nLLMs, focusing specifically on how LLMs acquire, learn, remember, and use\ninformation to produce their outputs. To achieve this goal, Psychomatics will\nrely on a comparative methodology, starting from a theory-driven research\nquestion - is the process of language development and use different in humans\nand LLMs? - drawing parallels between LLMs and biological systems. Our analysis\nshows how LLMs can map and manipulate complex linguistic patterns in their\ntraining data. Moreover, LLMs can follow Grice's Cooperative Principle to\nprovide relevant and informative responses. However, human cognition draws from\nmultiple sources of meaning, including experiential, emotional, and imaginative\nfacets, which transcend mere language processing and are rooted in our social\nand developmental trajectories. Moreover, current LLMs lack physical\nembodiment, reducing their ability to make sense of the intricate interplay\nbetween perception, action, and cognition that shapes human understanding and\nexpression. Ultimately, Psychomatics holds the potential to yield\ntransformative insights into the nature of language, cognition, and\nintelligence, both artificial and biological. Moreover, by drawing parallels\nbetween LLMs and human cognitive processes, Psychomatics can inform the\ndevelopment of more robust and human-like AI systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 4 tables, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.16444v1",
    "published_date": "2024-07-23 12:53:41 UTC",
    "updated_date": "2024-07-23 12:53:41 UTC"
  },
  {
    "arxiv_id": "2407.16397v1",
    "title": "On ADMM in Heterogeneous Federated Learning: Personalization, Robustness, and Fairness",
    "authors": [
      "Shengkun Zhu",
      "Jinshan Zeng",
      "Sheng Wang",
      "Yuan Sun",
      "Xiaodong Li",
      "Yuan Yao",
      "Zhiyong Peng"
    ],
    "abstract": "Statistical heterogeneity is a root cause of tension among accuracy,\nfairness, and robustness of federated learning (FL), and is key in paving a\npath forward. Personalized FL (PFL) is an approach that aims to reduce the\nimpact of statistical heterogeneity by developing personalized models for\nindividual users, while also inherently providing benefits in terms of fairness\nand robustness. However, existing PFL frameworks focus on improving the\nperformance of personalized models while neglecting the global model. Moreover,\nthese frameworks achieve sublinear convergence rates and rely on strong\nassumptions. In this paper, we propose FLAME, an optimization framework by\nutilizing the alternating direction method of multipliers (ADMM) to train\npersonalized and global models. We propose a model selection strategy to\nimprove performance in situations where clients have different types of\nheterogeneous data. Our theoretical analysis establishes the global convergence\nand two kinds of convergence rates for FLAME under mild assumptions. We\ntheoretically demonstrate that FLAME is more robust and fair than the\nstate-of-the-art methods on a class of linear problems. Our experimental\nfindings show that FLAME outperforms state-of-the-art methods in convergence\nand accuracy, and it achieves higher test accuracy under various attacks and\nperforms more uniformly across clients.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2311.06756",
    "pdf_url": "http://arxiv.org/pdf/2407.16397v1",
    "published_date": "2024-07-23 11:35:42 UTC",
    "updated_date": "2024-07-23 11:35:42 UTC"
  },
  {
    "arxiv_id": "2408.01449v1",
    "title": "AI Act for the Working Programmer",
    "authors": [
      "Holger Hermanns",
      "Anne Lauber-Rönsberg",
      "Philip Meinel",
      "Sarah Sterz",
      "Hanwei Zhang"
    ],
    "abstract": "The European AI Act is a new, legally binding instrument that will enforce\ncertain requirements on the development and use of AI technology potentially\naffecting people in Europe. It can be expected that the stipulations of the\nAct, in turn, are going to affect the work of many software engineers, software\ntesters, data engineers, and other professionals across the IT sector in Europe\nand beyond. The 113 articles, 180 recitals, and 13 annexes that make up the Act\ncover 144 pages. This paper aims at providing an aid for navigating the Act\nfrom the perspective of some professional in the software domain, termed \"the\nworking programmer\", who feels the need to know about the stipulations of the\nAct.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "25 pages, 2 figures; submitted to AISoLA 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01449v1",
    "published_date": "2024-07-23 11:30:20 UTC",
    "updated_date": "2024-07-23 11:30:20 UTC"
  },
  {
    "arxiv_id": "2407.21053v1",
    "title": "Knowledge Models for Cancer Clinical Practice Guidelines : Construction, Management and Usage in Question Answering",
    "authors": [
      "Pralaypati Ta",
      "Bhumika Gupta",
      "Arihant Jain",
      "Sneha Sree C",
      "Keerthi Ram",
      "Mohanasankar Sivaprakasam"
    ],
    "abstract": "An automated knowledge modeling algorithm for Cancer Clinical Practice\nGuidelines (CPGs) extracts the knowledge contained in the CPG documents and\ntransforms it into a programmatically interactable, easy-to-update structured\nmodel with minimal human intervention. The existing automated algorithms have\nminimal scope and cannot handle the varying complexity of the knowledge content\nin the CPGs for different cancer types. This work proposes an improved\nautomated knowledge modeling algorithm to create knowledge models from the\nNational Comprehensive Cancer Network (NCCN) CPGs in Oncology for different\ncancer types. The proposed algorithm has been evaluated with NCCN CPGs for four\ndifferent cancer types. We also proposed an algorithm to compare the knowledge\nmodels for different versions of a guideline to discover the specific changes\nintroduced in the treatment protocol of a new version. We created a\nquestion-answering (Q&A) framework with the guideline knowledge models as the\naugmented knowledge base to study our ability to query the knowledge models. We\ncompiled a set of 32 question-answer pairs derived from two reliable data\nsources for the treatment of Non-Small Cell Lung Cancer (NSCLC) to evaluate the\nQ&A framework. The framework was evaluated against the question-answer pairs\nfrom one data source, and it can generate the answers with 54.5% accuracy from\nthe treatment algorithm and 81.8% accuracy from the discussion part of the NCCN\nNSCLC guideline knowledge model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21053v1",
    "published_date": "2024-07-23 11:26:40 UTC",
    "updated_date": "2024-07-23 11:26:40 UTC"
  },
  {
    "arxiv_id": "2407.18274v1",
    "title": "Adaptive Differentially Private Structural Entropy Minimization for Unsupervised Social Event Detection",
    "authors": [
      "Zhiwei Yang",
      "Yuecen Wei",
      "Haoran Li",
      "Qian Li",
      "Lei Jiang",
      "Li Sun",
      "Xiaoyan Yu",
      "Chunming Hu",
      "Hao Peng"
    ],
    "abstract": "Social event detection refers to extracting relevant message clusters from\nsocial media data streams to represent specific events in the real world.\nSocial event detection is important in numerous areas, such as opinion\nanalysis, social safety, and decision-making. Most current methods are\nsupervised and require access to large amounts of data. These methods need\nprior knowledge of the events and carry a high risk of leaking sensitive\ninformation in the messages, making them less applicable in open-world\nsettings. Therefore, conducting unsupervised detection while fully utilizing\nthe rich information in the messages and protecting data privacy remains a\nsignificant challenge. To this end, we propose a novel social event detection\nframework, ADP-SEMEvent, an unsupervised social event detection method that\nprioritizes privacy. Specifically, ADP-SEMEvent is divided into two stages,\ni.e., the construction stage of the private message graph and the clustering\nstage of the private message graph. In the first stage, an adaptive\ndifferential privacy approach is used to construct a private message graph. In\nthis process, our method can adaptively apply differential privacy based on the\nevents occurring each day in an open environment to maximize the use of the\nprivacy budget. In the second stage, to address the reduction in data utility\ncaused by noise, a novel 2-dimensional structural entropy minimization\nalgorithm based on optimal subgraphs is used to detect events in the message\ngraph. The highlight of this process is unsupervised and does not compromise\ndifferential privacy. Extensive experiments on two public datasets demonstrate\nthat ADP-SEMEvent can achieve detection performance comparable to\nstate-of-the-art methods while maintaining reasonable privacy budget\nparameters.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "Accepted to ACM CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.18274v1",
    "published_date": "2024-07-23 11:19:22 UTC",
    "updated_date": "2024-07-23 11:19:22 UTC"
  },
  {
    "arxiv_id": "2408.00803v1",
    "title": "A Comprehensive Survey on Root Cause Analysis in (Micro) Services: Methodologies, Challenges, and Trends",
    "authors": [
      "Tingting Wang",
      "Guilin Qi"
    ],
    "abstract": "The complex dependencies and propagative faults inherent in microservices,\ncharacterized by a dense network of interconnected services, pose significant\nchallenges in identifying the underlying causes of issues. Prompt\nidentification and resolution of disruptive problems are crucial to ensure\nrapid recovery and maintain system stability. Numerous methodologies have\nemerged to address this challenge, primarily focusing on diagnosing failures\nthrough symptomatic data. This survey aims to provide a comprehensive,\nstructured review of root cause analysis (RCA) techniques within microservices,\nexploring methodologies that include metrics, traces, logs, and multi-model\ndata. It delves deeper into the methodologies, challenges, and future trends\nwithin microservices architectures. Positioned at the forefront of AI and\nautomation advancements, it offers guidance for future research directions.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00803v1",
    "published_date": "2024-07-23 11:02:49 UTC",
    "updated_date": "2024-07-23 11:02:49 UTC"
  },
  {
    "arxiv_id": "2407.16375v1",
    "title": "Ranking protein-protein models with large language models and graph neural networks",
    "authors": [
      "Xiaotong Xu",
      "Alexandre M. J. J. Bonvin"
    ],
    "abstract": "Protein-protein interactions (PPIs) are associated with various diseases,\nincluding cancer, infections, and neurodegenerative disorders. Obtaining\nthree-dimensional structural information on these PPIs serves as a foundation\nto interfere with those or to guide drug design. Various strategies can be\nfollowed to model those complexes, all typically resulting in a large number of\nmodels. A challenging step in this process is the identification of good models\n(near-native PPI conformations) from the large pool of generated models. To\naddress this challenge, we previously developed DeepRank-GNN-esm, a graph-based\ndeep learning algorithm for ranking modelled PPI structures harnessing the\npower of protein language models. Here, we detail the use of our software with\nexamples. DeepRank-GNN-esm is freely available at\nhttps://github.com/haddocking/DeepRank-GNN-esm",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "14 pages. Detailed protocol to use our DeepRank-GNN-esm software to\n  analyse models of protein-protein complexes",
    "pdf_url": "http://arxiv.org/pdf/2407.16375v1",
    "published_date": "2024-07-23 10:51:35 UTC",
    "updated_date": "2024-07-23 10:51:35 UTC"
  },
  {
    "arxiv_id": "2407.16361v1",
    "title": "Virtue Ethics For Ethically Tunable Robotic Assistants",
    "authors": [
      "Rajitha Ramanayake",
      "Vivek Nallur"
    ],
    "abstract": "The common consensus is that robots designed to work alongside or serve\nhumans must adhere to the ethical standards of their operational environment.\nTo achieve this, several methods based on established ethical theories have\nbeen suggested. Nonetheless, numerous empirical studies show that the ethical\nrequirements of the real world are very diverse and can change rapidly from\nregion to region. This eliminates the idea of a universal robot that can fit\ninto any ethical context. However, creating customised robots for each\ndeployment, using existing techniques is challenging. This paper presents a way\nto overcome this challenge by introducing a virtue ethics inspired\ncomputational method that enables character-based tuning of robots to\naccommodate the specific ethical needs of an environment. Using a simulated\nelder-care environment, we illustrate how tuning can be used to change the\nbehaviour of a robot that interacts with an elderly resident in an\nambient-assisted environment. Further, we assess the robot's responses by\nconsulting ethicists to identify potential shortcomings.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for EUMAS24",
    "pdf_url": "http://arxiv.org/pdf/2407.16361v1",
    "published_date": "2024-07-23 10:11:18 UTC",
    "updated_date": "2024-07-23 10:11:18 UTC"
  },
  {
    "arxiv_id": "2407.16357v2",
    "title": "TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR Prediction at Kuaishou",
    "authors": [
      "Zihua Si",
      "Lin Guan",
      "ZhongXiang Sun",
      "Xiaoxue Zang",
      "Jing Lu",
      "Yiqun Hui",
      "Xingchao Cao",
      "Zeyu Yang",
      "Yichen Zheng",
      "Dewei Leng",
      "Kai Zheng",
      "Chenbin Zhang",
      "Yanan Niu",
      "Yang Song",
      "Kun Gai"
    ],
    "abstract": "The significance of modeling long-term user interests for CTR prediction\ntasks in large-scale recommendation systems is progressively gaining attention\namong researchers and practitioners. Existing work, such as SIM and TWIN,\ntypically employs a two-stage approach to model long-term user behavior\nsequences for efficiency concerns. The first stage rapidly retrieves a subset\nof sequences related to the target item from a long sequence using a\nsearch-based mechanism namely the General Search Unit (GSU), while the second\nstage calculates the interest scores using the Exact Search Unit (ESU) on the\nretrieved results. Given the extensive length of user behavior sequences\nspanning the entire life cycle, potentially reaching up to 10^6 in scale, there\nis currently no effective solution for fully modeling such expansive user\ninterests. To overcome this issue, we introduced TWIN-V2, an enhancement of\nTWIN, where a divide-and-conquer approach is applied to compress life-cycle\nbehaviors and uncover more accurate and diverse user interests. Specifically, a\nhierarchical clustering method groups items with similar characteristics in\nlife-cycle behaviors into a single cluster during the offline phase. By\nlimiting the size of clusters, we can compress behavior sequences well beyond\nthe magnitude of 10^5 to a length manageable for online inference in GSU\nretrieval. Cluster-aware target attention extracts comprehensive and\nmulti-faceted long-term interests of users, thereby making the final\nrecommendation results more accurate and diverse. Extensive offline experiments\non a multi-billion-scale industrial dataset and online A/B tests have\ndemonstrated the effectiveness of TWIN-V2. Under an efficient deployment\nframework, TWIN-V2 has been successfully deployed to the primary traffic that\nserves hundreds of millions of daily active users at Kuaishou.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16357v2",
    "published_date": "2024-07-23 10:00:45 UTC",
    "updated_date": "2024-08-16 05:16:31 UTC"
  },
  {
    "arxiv_id": "2407.16344v3",
    "title": "SOAP: Enhancing Spatio-Temporal Relation and Motion Information Capturing for Few-Shot Action Recognition",
    "authors": [
      "Wenbo Huang",
      "Jinghui Zhang",
      "Xuwei Qian",
      "Zhen Wu",
      "Meng Wang",
      "Lei Zhang"
    ],
    "abstract": "High frame-rate (HFR) videos of action recognition improve fine-grained\nexpression while reducing the spatio-temporal relation and motion information\ndensity. Thus, large amounts of video samples are continuously required for\ntraditional data-driven training. However, samples are not always sufficient in\nreal-world scenarios, promoting few-shot action recognition (FSAR) research. We\nobserve that most recent FSAR works build spatio-temporal relation of video\nsamples via temporal alignment after spatial feature extraction, cutting apart\nspatial and temporal features within samples. They also capture motion\ninformation via narrow perspectives between adjacent frames without considering\ndensity, leading to insufficient motion information capturing. Therefore, we\npropose a novel plug-and-play architecture for FSAR called Spatio-tempOral\nfrAme tuPle enhancer (SOAP) in this paper. The model we designed with such\narchitecture refers to SOAP-Net. Temporal connections between different feature\nchannels and spatio-temporal relation of features are considered instead of\nsimple feature extraction. Comprehensive motion information is also captured,\nusing frame tuples with multiple frames containing more motion information than\nadjacent frames. Combining frame tuples of diverse frame counts further\nprovides a broader perspective. SOAP-Net achieves new state-of-the-art\nperformance across well-known benchmarks such as SthSthV2, Kinetics, UCF101,\nand HMDB51. Extensive empirical evaluations underscore the competitiveness,\npluggability, generalization, and robustness of SOAP. The code is released at\nhttps://github.com/wenbohuang1002/SOAP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ACM MM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16344v3",
    "published_date": "2024-07-23 09:45:25 UTC",
    "updated_date": "2024-08-21 16:07:08 UTC"
  },
  {
    "arxiv_id": "2407.16329v1",
    "title": "PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets",
    "authors": [
      "Jaeyoung Kim",
      "Sihyeon Lee",
      "Hyeon Jeon",
      "Keon-Joo Lee",
      "Hee-Joon Bae",
      "Bohyoung Kim",
      "Jinwook Seo"
    ],
    "abstract": "Acute stroke demands prompt diagnosis and treatment to achieve optimal\npatient outcomes. However, the intricate and irregular nature of clinical data\nassociated with acute stroke, particularly blood pressure (BP) measurements,\npresents substantial obstacles to effective visual analytics and\ndecision-making. Through a year-long collaboration with experienced\nneurologists, we developed PhenoFlow, a visual analytics system that leverages\nthe collaboration between human and Large Language Models (LLMs) to analyze the\nextensive and complex data of acute ischemic stroke patients. PhenoFlow\npioneers an innovative workflow, where the LLM serves as a data wrangler while\nneurologists explore and supervise the output using visualizations and natural\nlanguage interactions. This approach enables neurologists to focus more on\ndecision-making with reduced cognitive load. To protect sensitive patient\ninformation, PhenoFlow only utilizes metadata to make inferences and synthesize\nexecutable codes, without accessing raw patient data. This ensures that the\nresults are both reproducible and interpretable while maintaining patient\nprivacy. The system incorporates a slice-and-wrap design that employs temporal\nfolding to create an overlaid circular visualization. Combined with a linear\nbar graph, this design aids in exploring meaningful patterns within irregularly\nmeasured BP data. Through case studies, PhenoFlow has demonstrated its\ncapability to support iterative analysis of extensive clinical datasets,\nreducing cognitive load and enabling neurologists to make well-informed\ndecisions. Grounded in long-term collaboration with domain experts, our\nresearch demonstrates the potential of utilizing LLMs to tackle current\nchallenges in data-driven clinical decision-making for acute ischemic stroke\npatients.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages, 5 figures, paper to appear in IEEE Transactions on\n  Visualization and Computer Graphics (TVCG) (Proc. IEEE VIS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.16329v1",
    "published_date": "2024-07-23 09:25:59 UTC",
    "updated_date": "2024-07-23 09:25:59 UTC"
  },
  {
    "arxiv_id": "2407.16326v2",
    "title": "On The Expressive Power of Knowledge Graph Embedding Methods",
    "authors": [
      "Jiexing Gao",
      "Dmitry Rodin",
      "Vasily Motolygin",
      "Denis Zaytsev"
    ],
    "abstract": "Knowledge Graph Embedding (KGE) is a popular approach, which aims to\nrepresent entities and relations of a knowledge graph in latent spaces. Their\nrepresentations are known as embeddings. To measure the plausibility of\ntriplets, score functions are defined over embedding spaces. Despite wide\ndissemination of KGE in various tasks, KGE methods have limitations in\nreasoning abilities. In this paper we propose a mathematical framework to\ncompare reasoning abilities of KGE methods. We show that STransE has a higher\ncapability than TransComplEx, and then present new STransCoRe method, which\nimproves the STransE by combining it with the TransCoRe insights, which can\nreduce the STransE space complexity.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "MCS 68T30",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper may involve data that is not readily available to the\n  public",
    "pdf_url": "http://arxiv.org/pdf/2407.16326v2",
    "published_date": "2024-07-23 09:21:38 UTC",
    "updated_date": "2024-07-26 16:11:23 UTC"
  },
  {
    "arxiv_id": "2407.16318v1",
    "title": "PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing",
    "authors": [
      "Blazej Manczak",
      "Eliott Zemour",
      "Eric Lin",
      "Vaikkunth Mugunthan"
    ],
    "abstract": "Deploying language models (LMs) necessitates outputs to be both high-quality\nand compliant with safety guidelines. Although Inference-Time Guardrails (ITG)\noffer solutions that shift model output distributions towards compliance, we\nfind that current methods struggle in balancing safety with helpfulness. ITG\nMethods that safely address non-compliant queries exhibit lower helpfulness\nwhile those that prioritize helpfulness compromise on safety. We refer to this\ntrade-off as the guardrail tax, analogous to the alignment tax. To address\nthis, we propose PrimeGuard, a novel ITG method that utilizes structured\ncontrol flow.\n  PrimeGuard routes requests to different self-instantiations of the LM with\nvarying instructions, leveraging its inherent instruction-following\ncapabilities and in-context learning. Our tuning-free approach dynamically\ncompiles system-designer guidelines for each query. We construct and release\nsafe-eval, a diverse red-team safety benchmark. Extensive evaluations\ndemonstrate that PrimeGuard, without fine-tuning, overcomes the guardrail tax\nby (1) significantly increasing resistance to iterative jailbreak attacks and\n(2) achieving state-of-the-art results in safety guardrailing while (3)\nmatching helpfulness scores of alignment-tuned models. Extensive evaluations\ndemonstrate that PrimeGuard, without fine-tuning, outperforms all competing\nbaselines and overcomes the guardrail tax by improving the fraction of safe\nresponses from 61% to 97% and increasing average helpfulness scores from 4.17\nto 4.29 on the largest models, while reducing attack success rate from 100% to\n8%.\n  PrimeGuard implementation is available at\nhttps://github.com/dynamofl/PrimeGuard and safe-eval dataset is available at\nhttps://huggingface.co/datasets/dynamoai/safe_eval.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML 2024 NextGenAISafety workshop version with links to\n  implementation and dataset",
    "pdf_url": "http://arxiv.org/pdf/2407.16318v1",
    "published_date": "2024-07-23 09:14:27 UTC",
    "updated_date": "2024-07-23 09:14:27 UTC"
  },
  {
    "arxiv_id": "2407.16312v2",
    "title": "MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent Reinforcement Learning",
    "authors": [
      "Florian Felten",
      "Umut Ucak",
      "Hicham Azmani",
      "Gao Peng",
      "Willem Röpke",
      "Hendrik Baier",
      "Patrick Mannion",
      "Diederik M. Roijers",
      "Jordan K. Terry",
      "El-Ghazali Talbi",
      "Grégoire Danoy",
      "Ann Nowé",
      "Roxana Rădulescu"
    ],
    "abstract": "Many challenging tasks such as managing traffic systems, electricity grids,\nor supply chains involve complex decision-making processes that must balance\nmultiple conflicting objectives and coordinate the actions of various\nindependent decision-makers (DMs). One perspective for formalising and\naddressing such tasks is multi-objective multi-agent reinforcement learning\n(MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple\nagents each needing to consider multiple objectives in their learning process.\nIn reinforcement learning research, benchmarks are crucial in facilitating\nprogress, evaluation, and reproducibility. The significance of benchmarks is\nunderscored by the existence of numerous benchmark frameworks developed for\nvarious RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent\nRL (e.g., PettingZoo), and single-agent multi-objective RL (e.g.,\nMO-Gymnasium). To support the advancement of the MOMARL field, we introduce\nMOMAland, the first collection of standardised environments for multi-objective\nmulti-agent reinforcement learning. MOMAland addresses the need for\ncomprehensive benchmarking in this emerging field, offering over 10 diverse\nenvironments that vary in the number of agents, state representations, reward\nstructures, and utility considerations. To provide strong baselines for future\nresearch, MOMAland also includes algorithms capable of learning policies in\nsuch settings.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16312v2",
    "published_date": "2024-07-23 09:05:06 UTC",
    "updated_date": "2024-10-27 17:55:41 UTC"
  },
  {
    "arxiv_id": "2407.21052v1",
    "title": "Table-Filling via Mean Teacher for Cross-domain Aspect Sentiment Triplet Extraction",
    "authors": [
      "Kun Peng",
      "Lei Jiang",
      "Qian Li",
      "Haoran Li",
      "Xiaoyan Yu",
      "Li Sun",
      "Shuo Sun",
      "Yanxian Bi",
      "Hao Peng"
    ],
    "abstract": "Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract\nfine-grained sentiment elements from target domain sentences by leveraging the\nknowledge acquired from the source domain. Due to the absence of labeled data\nin the target domain, recent studies tend to rely on pre-trained language\nmodels to generate large amounts of synthetic data for training purposes.\nHowever, these approaches entail additional computational costs associated with\nthe generation process. Different from them, we discover a striking resemblance\nbetween table-filling methods in ASTE and two-stage Object Detection (OD) in\ncomputer vision, which inspires us to revisit the cross-domain ASTE task and\napproach it from an OD standpoint. This allows the model to benefit from the OD\nextraction paradigm and region-level alignment. Building upon this premise, we\npropose a novel method named \\textbf{T}able-\\textbf{F}illing via \\textbf{M}ean\n\\textbf{T}eacher (TFMT). Specifically, the table-filling methods encode the\nsentence into a 2D table to detect word relations, while TFMT treats the table\nas a feature map and utilizes a region consistency to enhance the quality of\nthose generated pseudo labels. Additionally, considering the existence of the\ndomain gap, a cross-domain consistency based on Maximum Mean Discrepancy is\ndesigned to alleviate domain shift problems. Our method achieves\nstate-of-the-art performance with minimal parameters and computational costs,\nmaking it a strong baseline for cross-domain ASTE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by CIKM2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21052v1",
    "published_date": "2024-07-23 09:04:08 UTC",
    "updated_date": "2024-07-23 09:04:08 UTC"
  },
  {
    "arxiv_id": "2407.16296v1",
    "title": "Quantum Computing for Climate Resilience and Sustainability Challenges",
    "authors": [
      "Kin Tung Michael Ho",
      "Kuan-Cheng Chen",
      "Lily Lee",
      "Felix Burt",
      "Shang Yu",
      "Po-Heng",
      "Lee"
    ],
    "abstract": "The escalating impacts of climate change and the increasing demand for\nsustainable development and natural resource management necessitate innovative\ntechnological solutions. Quantum computing (QC) has emerged as a promising tool\nwith the potential to revolutionize these critical areas. This review explores\nthe application of quantum machine learning and optimization techniques for\nclimate change prediction and enhancing sustainable development. Traditional\ncomputational methods often fall short in handling the scale and complexity of\nclimate models and natural resource management. Quantum advancements, however,\noffer significant improvements in computational efficiency and problem-solving\ncapabilities. By synthesizing the latest research and developments, this paper\nhighlights how QC and quantum machine learning can optimize\nmulti-infrastructure systems towards climate neutrality. The paper also\nevaluates the performance of current quantum algorithms and hardware in\npractical applications and presents realistic cases, i.e., waste-to-energy in\nanaerobic digestion, disaster prevention in flooding prediction, and new\nmaterial development for carbon capture. The integration of these quantum\ntechnologies promises to drive significant advancements in achieving climate\nresilience and sustainable development.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16296v1",
    "published_date": "2024-07-23 08:54:12 UTC",
    "updated_date": "2024-07-23 08:54:12 UTC"
  },
  {
    "arxiv_id": "2407.16292v2",
    "title": "Visual Stereotypes of Autism Spectrum in DALL-E, Stable Diffusion, SDXL, and Midjourney",
    "authors": [
      "Maciej Wodziński",
      "Marcin Rządeczka",
      "Anastazja Szuła",
      "Marta Sokół",
      "Marcin Moskalewicz"
    ],
    "abstract": "Avoiding systemic discrimination requires investigating AI models' potential\nto propagate stereotypes resulting from the inherent biases of training\ndatasets. Our study investigated how text-to-image models unintentionally\nperpetuate non-rational beliefs regarding autism. The research protocol\ninvolved generating images based on 53 prompts aimed at visualizing concrete\nobjects and abstract concepts related to autism across four models: DALL-E,\nStable Diffusion, SDXL, and Midjourney (N=249). Expert assessment of results\nwas performed via a framework of 10 deductive codes representing common\nstereotypes contested by the community regarding their presence and spatial\nintensity, quantified on ordinal scales and subject to statistical analysis of\ninter-rater reliability and size effects. The models frequently utilised\ncontroversial themes and symbols which were unevenly distributed, however, with\nstriking homogeneity in terms of skin colour, gender, and age, with autistic\nindividuals portrayed as engaged in solitary activities, interacting with\nobjects rather than people, and displaying stereotypical emotional expressions\nsuch as pale, anger, or sad. Secondly we observed representational\ninsensitivity regarding autism images despite directional prompting aimed at\nfalsifying the above results. Additionally, DALL-E explicitly denied\nperpetuating stereotypes. We interpret this as ANNs mirroring the human\ncognitive architecture regarding the discrepancy between background and\nreflective knowledge, as justified by our previous research on autism-related\nstereotypes in humans.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16292v2",
    "published_date": "2024-07-23 08:48:09 UTC",
    "updated_date": "2024-07-24 07:15:26 UTC"
  },
  {
    "arxiv_id": "2407.16289v1",
    "title": "Federated Learning for Face Recognition via Intra-subject Self-supervised Learning",
    "authors": [
      "Hansol Kim",
      "Hoyeol Choi",
      "Youngjun Kwak"
    ],
    "abstract": "Federated Learning (FL) for face recognition aggregates locally optimized\nmodels from individual clients to construct a generalized face recognition\nmodel. However, previous studies present two major challenges: insufficient\nincorporation of self-supervised learning and the necessity for clients to\naccommodate multiple subjects. To tackle these limitations, we propose FedFS\n(Federated Learning for personalized Face recognition via intra-subject\nSelf-supervised learning framework), a novel federated learning architecture\ntailored to train personalized face recognition models without imposing\nsubjects. Our proposed FedFS comprises two crucial components that leverage\naggregated features of the local and global models to cooperate with\nrepresentations of an off-the-shelf model. These components are (1) adaptive\nsoft label construction, utilizing dot product operations to reformat labels\nwithin intra-instances, and (2) intra-subject self-supervised learning,\nemploying cosine similarity operations to strengthen robust intra-subject\nrepresentations. Additionally, we introduce a regularization loss to prevent\noverfitting and ensure the stability of the optimized model. To assess the\neffectiveness of FedFS, we conduct comprehensive experiments on the DigiFace-1M\nand VGGFace datasets, demonstrating superior performance compared to previous\nmethods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at the The 35th British Machine Vision Conference 2024 (BMVC\n  2024), Glasgow, UK. Youngjun Kwak is corresponding author",
    "pdf_url": "http://arxiv.org/pdf/2407.16289v1",
    "published_date": "2024-07-23 08:43:42 UTC",
    "updated_date": "2024-07-23 08:43:42 UTC"
  },
  {
    "arxiv_id": "2407.16286v1",
    "title": "A deeper look at depth pruning of LLMs",
    "authors": [
      "Shoaib Ahmed Siddiqui",
      "Xin Dong",
      "Greg Heinrich",
      "Thomas Breuel",
      "Jan Kautz",
      "David Krueger",
      "Pavlo Molchanov"
    ],
    "abstract": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16286v1",
    "published_date": "2024-07-23 08:40:27 UTC",
    "updated_date": "2024-07-23 08:40:27 UTC"
  },
  {
    "arxiv_id": "2407.16280v1",
    "title": "Efficient Detection of Commutative Factors in Factor Graphs",
    "authors": [
      "Malte Luttermann",
      "Johann Machemer",
      "Marcel Gehrke"
    ],
    "abstract": "Lifted probabilistic inference exploits symmetries in probabilistic graphical\nmodels to allow for tractable probabilistic inference with respect to domain\nsizes. To exploit symmetries in, e.g., factor graphs, it is crucial to identify\ncommutative factors, i.e., factors having symmetries within themselves due to\ntheir arguments being exchangeable. The current state of the art to check\nwhether a factor is commutative with respect to a subset of its arguments\niterates over all possible subsets of the factor's arguments, i.e., $O(2^n)$\niterations for a factor with $n$ arguments in the worst case. In this paper, we\nefficiently solve the problem of detecting commutative factors in a factor\ngraph. In particular, we introduce the detection of commutative factors (DECOR)\nalgorithm, which allows us to drastically reduce the computational effort for\nchecking whether a factor is commutative in practice. We prove that DECOR\nefficiently identifies restrictions to drastically reduce the number of\nrequired iterations and validate the efficiency of DECOR in our empirical\nevaluation.",
    "categories": [
      "cs.AI",
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the Proceedings of the 12th Conference on Probabilistic\n  Graphical Models (PGM 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.16280v1",
    "published_date": "2024-07-23 08:31:24 UTC",
    "updated_date": "2024-07-23 08:31:24 UTC"
  },
  {
    "arxiv_id": "2407.16274v2",
    "title": "Comparative Analysis of AES, Blowfish, Twofish, Salsa20, and ChaCha20 for Image Encryption",
    "authors": [
      "Rebwar Khalid Muhammed",
      "Ribwar Rashid Aziz",
      "Alla Ahmad Hassan",
      "Aso Mohammed Aladdin",
      "Shaida Jumaah Saydah",
      "Tarik Ahmed. Rashid",
      "Bryar Ahmad Hassan"
    ],
    "abstract": "Nowadays, cybersecurity has grown into a more significant and difficult\nscientific issue. The recog-nition of threats and attacks meant for knowledge\nand safety on the internet is growing harder to detect. Since cybersecurity\nguarantees the privacy and security of data sent via the Internet, it is\nessential, while also providing protection against malicious attacks. Encrypt\nhas grown into an an-swer that has become an essential element of information\nsecurity systems. To ensure the security of shared data, including text,\nimages, or videos, it is essential to employ various methods and strategies.\nThis study delves into the prevalent cryptographic methods and algorithms\nutilized for prevention and stream encryption, examining their encoding\ntechniques such as advanced encryp-tion standard (AES), Blowfish, Twofish,\nSalsa20, and ChaCha20. The primary objective of this re-search is to identify\nthe optimal times and throughputs (speeds) for data encryption and decryption\nprocesses. The methodology of this study involved selecting five distinct types\nof images to com-pare the outcomes of the techniques evaluated in this\nresearch. The assessment focused on pro-cessing time and speed parameters,\nexamining visual encoding and decoding using Java as the pri-mary platform. A\ncomparative analysis of several symmetric key ciphers was performed, focusing\non handling large datasets. Despite this limitation, comparing different images\nhelped evaluate the techniques' novelty. The results showed that ChaCha20 had\nthe best average time for both encryp-tion and decryption, being over 50%\nfaster than some other algorithms. However, the Twofish algo-rithm had lower\nthroughput during testing. The paper concludes with findings and suggestions\nfor future improvements.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16274v2",
    "published_date": "2024-07-23 08:26:05 UTC",
    "updated_date": "2024-07-26 11:04:49 UTC"
  },
  {
    "arxiv_id": "2407.16255v1",
    "title": "Self-Reasoning Assistant Learning for non-Abelian Gauge Fields Design",
    "authors": [
      "Jinyang Sun",
      "Xi Chen",
      "Xiumei Wang",
      "Dandan Zhu",
      "Xingping Zhou"
    ],
    "abstract": "Non-Abelian braiding has attracted substantial attention because of its\npivotal role in describing the exchange behaviour of anyons, in which the input\nand outcome of non-Abelian braiding are connected by a unitary matrix.\nImplementing braiding in a classical system can assist the experimental\ninvestigation of non-Abelian physics. However, the design of non-Abelian gauge\nfields faces numerous challenges stemmed from the intricate interplay of group\nstructures, Lie algebra properties, representation theory, topology, and\nsymmetry breaking. The extreme diversity makes it a powerful tool for the study\nof condensed matter physics. Whereas the widely used artificial intelligence\nwith data-driven approaches has greatly promoted the development of physics,\nmost works are limited on the data-to-data design. Here we propose a\nself-reasoning assistant learning framework capable of directly generating\nnon-Abelian gauge fields. This framework utilizes the forward diffusion process\nto capture and reproduce the complex patterns and details inherent in the\ntarget distribution through continuous transformation. Then the reverse\ndiffusion process is used to make the generated data closer to the distribution\nof the original situation. Thus, it owns strong self-reasoning capabilities,\nallowing to automatically discover the feature representation and capture more\nsubtle relationships from the dataset. Moreover, the self-reasoning eliminates\nthe need for manual feature engineering and simplifies the process of model\nbuilding. Our framework offers a disruptive paradigm shift to parse complex\nphysical processes, automatically uncovering patterns from massive datasets.",
    "categories": [
      "cs.LG",
      "cond-mat.mes-hall",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16255v1",
    "published_date": "2024-07-23 07:49:35 UTC",
    "updated_date": "2024-07-23 07:49:35 UTC"
  },
  {
    "arxiv_id": "2407.16252v3",
    "title": "LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal Consultation",
    "authors": [
      "Jingyun Sun",
      "Chengxiao Dai",
      "Zhongze Luo",
      "Yangbo Chang",
      "Yang Li"
    ],
    "abstract": "Legal Large Language Models (LLMs) have shown promise in providing legal\nconsultations to non-experts. However, most existing Chinese legal consultation\nmodels are based on single-agent systems, which differ from real-world legal\nconsultations, where multiple professionals collaborate to offer more tailored\nresponses. To better simulate real consultations, we propose LawLuo, a\nmulti-agent framework for multi-turn Chinese legal consultations. LawLuo\nincludes four agents: the receptionist agent, which assesses user intent and\nselects a lawyer agent; the lawyer agent, which interacts with the user; the\nsecretary agent, which organizes conversation records and generates\nconsultation reports; and the boss agent, which evaluates the performance of\nthe lawyer and secretary agents to ensure optimal results. These agents'\ninteractions mimic the operations of real law firms. To train them to follow\ndifferent legal instructions, we developed distinct fine-tuning datasets. We\nalso introduce a case graph-based RAG to help the lawyer agent address vague\nuser inputs. Experimental results show that LawLuo outperforms baselines in\ngenerating more personalized and professional responses, handling ambiguous\nqueries, and following legal instructions in multi-turn conversations. Our full\ncode and constructed datasets will be open-sourced upon paper acceptance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "I.2.1"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.16252v3",
    "published_date": "2024-07-23 07:40:41 UTC",
    "updated_date": "2024-12-16 05:53:28 UTC"
  },
  {
    "arxiv_id": "2407.16244v1",
    "title": "HSVLT: Hierarchical Scale-Aware Vision-Language Transformer for Multi-Label Image Classification",
    "authors": [
      "Shuyi Ouyang",
      "Hongyi Wang",
      "Ziwei Niu",
      "Zhenjia Bai",
      "Shiao Xie",
      "Yingying Xu",
      "Ruofeng Tong",
      "Yen-Wei Chen",
      "Lanfen Lin"
    ],
    "abstract": "The task of multi-label image classification involves recognizing multiple\nobjects within a single image. Considering both valuable semantic information\ncontained in the labels and essential visual features presented in the image,\ntight visual-linguistic interactions play a vital role in improving\nclassification performance. Moreover, given the potential variance in object\nsize and appearance within a single image, attention to features of different\nscales can help to discover possible objects in the image. Recently,\nTransformer-based methods have achieved great success in multi-label image\nclassification by leveraging the advantage of modeling long-range dependencies,\nbut they have several limitations. Firstly, existing methods treat visual\nfeature extraction and cross-modal fusion as separate steps, resulting in\ninsufficient visual-linguistic alignment in the joint semantic space.\nAdditionally, they only extract visual features and perform cross-modal fusion\nat a single scale, neglecting objects with different characteristics. To\naddress these issues, we propose a Hierarchical Scale-Aware Vision-Language\nTransformer (HSVLT) with two appealing designs: (1)~A hierarchical multi-scale\narchitecture that involves a Cross-Scale Aggregation module, which leverages\njoint multi-modal features extracted from multiple scales to recognize objects\nof varying sizes and appearances in images. (2)~Interactive Visual-Linguistic\nAttention, a novel attention mechanism module that tightly integrates\ncross-modal interaction, enabling the joint updating of visual, linguistic and\nmulti-modal features. We have evaluated our method on three benchmark datasets.\nThe experimental results demonstrate that HSVLT surpasses state-of-the-art\nmethods with lower computational cost.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.16244v1",
    "published_date": "2024-07-23 07:31:42 UTC",
    "updated_date": "2024-07-23 07:31:42 UTC"
  },
  {
    "arxiv_id": "2407.16237v2",
    "title": "OriGen:Enhancing RTL Code Generation with Code-to-Code Augmentation and Self-Reflection",
    "authors": [
      "Fan Cui",
      "Chenyang Yin",
      "Kexing Zhou",
      "Youwei Xiao",
      "Guangyu Sun",
      "Qiang Xu",
      "Qipeng Guo",
      "Demin Song",
      "Dahua Lin",
      "Xingcheng Zhang",
      "Yun",
      "Liang"
    ],
    "abstract": "Recent studies have demonstrated the significant potential of Large Language\nModels (LLMs) in generating Register Transfer Level (RTL) code, with notable\nadvancements showcased by commercial models such as GPT-4 and Claude3-Opus.\nHowever, these proprietary LLMs often raise concerns regarding privacy and\nsecurity. While open-source LLMs offer solutions to these concerns, they\ntypically underperform commercial models in RTL code generation tasks,\nprimarily due to the scarcity of high-quality open-source RTL datasets. To\naddress this challenge, we introduce OriGen , a fully open-source framework\nthat incorporates self-reflection capabilities and a novel dataset augmentation\nmethodology for generating high-quality, large-scale RTL code. Our approach\nemploys a code-tocode augmentation technique to enhance the quality of\nopen-source RTL code datasets. Furthermore, OriGen can rectify syntactic errors\nthrough a self-reflection process that leverages compiler feedback.\nExperimental results demonstrate that OriGen significantly outperforms other\nopen-source alternatives in RTL code generation. It surpasses the previous\nbest-performing open-source LLM by 12.8% and even exceeds GPT-4 Turbo in the\npass@1 metric on the VerilogEval-Human benchmark. Moreover, OriGen exhibits\nsuperior capabilities in self-reflection and error correction, outperforming\nGPT-4 by 19.9% on a benchmark designed to evaluate self-reflection\ncapabilities.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16237v2",
    "published_date": "2024-07-23 07:22:25 UTC",
    "updated_date": "2024-09-02 07:25:21 UTC"
  },
  {
    "arxiv_id": "2407.16235v1",
    "title": "Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection",
    "authors": [
      "Xin Zhou",
      "Duc-Manh Tran",
      "Thanh Le-Cong",
      "Ting Zhang",
      "Ivana Clairine Irsan",
      "Joshua Sumarlin",
      "Bach Le",
      "David Lo"
    ],
    "abstract": "Software vulnerabilities pose significant security challenges and potential\nrisks to society, necessitating extensive efforts in automated vulnerability\ndetection. There are two popular lines of work to address automated\nvulnerability detection. On one hand, Static Application Security Testing\n(SAST) is usually utilized to scan source code for security vulnerabilities,\nespecially in industries. On the other hand, deep learning (DL)-based methods,\nespecially since the introduction of large language models (LLMs), have\ndemonstrated their potential in software vulnerability detection. However,\nthere is no comparative study between SAST tools and LLMs, aiming to determine\ntheir effectiveness in vulnerability detection, understand the pros and cons of\nboth SAST and LLMs, and explore the potential combination of these two families\nof approaches.\n  In this paper, we compared 15 diverse SAST tools with 12 popular or\nstate-of-the-art open-source LLMs in detecting software vulnerabilities from\nrepositories of three popular programming languages: Java, C, and Python. The\nexperimental results showed that SAST tools obtain low vulnerability detection\nrates with relatively low false positives, while LLMs can detect up 90\\% to\n100\\% of vulnerabilities but suffer from high false positives. By further\nensembling the SAST tools and LLMs, the drawbacks of both SAST tools and LLMs\ncan be mitigated to some extent. Our analysis sheds light on both the current\nprogress and future directions for software vulnerability detection.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16235v1",
    "published_date": "2024-07-23 07:21:14 UTC",
    "updated_date": "2024-07-23 07:21:14 UTC"
  },
  {
    "arxiv_id": "2407.16220v1",
    "title": "ODGR: Online Dynamic Goal Recognition",
    "authors": [
      "Matan Shamir",
      "Osher Elhadad",
      "Matthew E. Taylor",
      "Reuth Mirsky"
    ],
    "abstract": "Traditionally, Reinforcement Learning (RL) problems are aimed at optimization\nof the behavior of an agent. This paper proposes a novel take on RL, which is\nused to learn the policy of another agent, to allow real-time recognition of\nthat agent's goals. Goal Recognition (GR) has traditionally been framed as a\nplanning problem where one must recognize an agent's objectives based on its\nobserved actions. Recent approaches have shown how reinforcement learning can\nbe used as part of the GR pipeline, but are limited to recognizing predefined\ngoals and lack scalability in domains with a large goal space. This paper\nformulates a novel problem, \"Online Dynamic Goal Recognition\" (ODGR), as a\nfirst step to address these limitations. Contributions include introducing the\nconcept of dynamic goals into the standard GR problem definition, revisiting\ncommon approaches by reformulating them using ODGR, and demonstrating the\nfeasibility of solving ODGR in a navigation domain using transfer learning.\nThese novel formulations open the door for future extensions of existing\ntransfer learning-based GR methods, which will be robust to changing and\nexpansive real-time environments.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 1 figure, RLC workshop, WAHT workshop",
    "pdf_url": "http://arxiv.org/pdf/2407.16220v1",
    "published_date": "2024-07-23 06:52:52 UTC",
    "updated_date": "2024-07-23 06:52:52 UTC"
  },
  {
    "arxiv_id": "2407.20257v1",
    "title": "Causal Understanding For Video Question Answering",
    "authors": [
      "Bhanu Prakash Reddy Guda",
      "Tanmay Kulkarni",
      "Adithya Sampath",
      "Swarnashree Mysore Sathyendra"
    ],
    "abstract": "Video Question Answering is a challenging task, which requires the model to\nreason over multiple frames and understand the interaction between different\nobjects to answer questions based on the context provided within the video,\nespecially in datasets like NExT-QA (Xiao et al., 2021a) which emphasize on\ncausal and temporal questions. Previous approaches leverage either sub-sampled\ninformation or causal intervention techniques along with complete video\nfeatures to tackle the NExT-QA task. In this work we elicit the limitations of\nthese approaches and propose solutions along four novel directions of\nimprovements on theNExT-QA dataset. Our approaches attempts to compensate for\nthe shortcomings in the previous works by systematically attacking each of\nthese problems by smartly sampling frames, explicitly encoding actions and\ncreating interventions that challenge the understanding of the model. Overall,\nfor both single-frame (+6.3%) and complete-video (+1.1%) based approaches, we\nobtain the state-of-the-art results on NExT-QA dataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20257v1",
    "published_date": "2024-07-23 06:32:46 UTC",
    "updated_date": "2024-07-23 06:32:46 UTC"
  },
  {
    "arxiv_id": "2407.16210v1",
    "title": "Strategy and Skill Learning for Physics-based Table Tennis Animation",
    "authors": [
      "Jiashun Wang",
      "Jessica Hodgins",
      "Jungdam Won"
    ],
    "abstract": "Recent advancements in physics-based character animation leverage deep\nlearning to generate agile and natural motion, enabling characters to execute\nmovements such as backflips, boxing, and tennis. However, reproducing the\nselection and use of diverse motor skills in dynamic environments to solve\ncomplex tasks, as humans do, still remains a challenge. We present a strategy\nand skill learning approach for physics-based table tennis animation. Our\nmethod addresses the issue of mode collapse, where the characters do not fully\nutilize the motor skills they need to perform to execute complex tasks. More\nspecifically, we demonstrate a hierarchical control system for diversified\nskill learning and a strategy learning framework for effective decision-making.\nWe showcase the efficacy of our method through comparative analysis with\nstate-of-the-art methods, demonstrating its capabilities in executing various\nskills for table tennis. Our strategy learning framework is validated through\nboth agent-agent interaction and human-agent interaction in Virtual Reality,\nhandling both competitive and cooperative tasks.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GR",
    "comment": "SIGGRAPH 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16210v1",
    "published_date": "2024-07-23 06:31:13 UTC",
    "updated_date": "2024-07-23 06:31:13 UTC"
  },
  {
    "arxiv_id": "2407.16205v5",
    "title": "LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on Large Language Models",
    "authors": [
      "Shi Lin",
      "Hongming Yang",
      "Dingyang Lin",
      "Rongchang Li",
      "Xun Wang",
      "Changting Lin",
      "Wenpeng Xing",
      "Meng Han"
    ],
    "abstract": "The rapid development of Large Language Models (LLMs) has brought significant\nadvancements across various tasks. However, despite these achievements, LLMs\nstill exhibit inherent safety vulnerabilities, especially when confronted with\njailbreak attacks. Existing jailbreak methods suffer from two main limitations:\nreliance on complicated prompt engineering and iterative optimization, which\nlead to low attack success rate (ASR) and attack efficiency (AE). In this work,\nwe propose an efficient jailbreak attack method, Analyzing-based Jailbreak\n(ABJ), which leverages the advanced reasoning capability of LLMs to\nautonomously generate harmful content, revealing their underlying safety\nvulnerabilities during complex reasoning process. We conduct comprehensive\nexperiments on ABJ across various open-source and closed-source LLMs. In\nparticular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional\nAE among all target LLMs, showcasing its remarkable attack effectiveness,\ntransferability, and efficiency. Our findings underscore the urgent need to\nprioritize and improve the safety of LLMs to mitigate the risks of misuse.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16205v5",
    "published_date": "2024-07-23 06:14:41 UTC",
    "updated_date": "2025-03-05 14:43:33 UTC"
  },
  {
    "arxiv_id": "2407.16200v1",
    "title": "MCTS Based Dispatch of Autonomous Vehicles under Operational Constraints for Continuous Transportation",
    "authors": [
      "Milan Tomy",
      "Konstantin M. Seiler",
      "Andrew J. Hill"
    ],
    "abstract": "Continuous transportation of material in the mining industry is achieved by\nthe dispatch of autonomous haul-trucks with discrete haulage capacities.\nRecently, Monte Carlo Tree Search (MCTS) was successfully deployed in tackling\nchallenges of long-run optimality, scalability and adaptability in haul-truck\ndispatch. Typically, operational constraints imposed on the mine site are\nsatisfied by heuristic controllers or human operators independent of the\ndispatch planning. This article incorporates operational constraint\nsatisfaction into the dispatch planning by utilising the MCTS based dispatch\nplanner Flow-Achieving Scheduling Tree (FAST). Operational constraint violation\nand satisfaction are modelled as opportunity costs in the combinatorial\noptimisation problem of dispatch. Explicit cost formulations are avoided by\nutilising MCTS generator models to derive opportunity costs. Experimental\nstudies with four types of operational constraints demonstrate the success of\nutilising opportunity costs for constraint satisfaction, and the effectiveness\nof integrating constraints into dispatch planning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "International Conference on Automation Science and Engineering\n  (CASE), 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16200v1",
    "published_date": "2024-07-23 06:06:16 UTC",
    "updated_date": "2024-07-23 06:06:16 UTC"
  },
  {
    "arxiv_id": "2407.16198v1",
    "title": "INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal Large Language Model",
    "authors": [
      "Yiwei Ma",
      "Zhibin Wang",
      "Xiaoshuai Sun",
      "Weihuang Lin",
      "Qiang Zhou",
      "Jiayi Ji",
      "Rongrong Ji"
    ],
    "abstract": "With advancements in data availability and computing resources, Multimodal\nLarge Language Models (MLLMs) have showcased capabilities across various\nfields. However, the quadratic complexity of the vision encoder in MLLMs\nconstrains the resolution of input images. Most current approaches mitigate\nthis issue by cropping high-resolution images into smaller sub-images, which\nare then processed independently by the vision encoder. Despite capturing\nsufficient local details, these sub-images lack global context and fail to\ninteract with one another. To address this limitation, we propose a novel MLLM,\nINF-LLaVA, designed for effective high-resolution image perception. INF-LLaVA\nincorporates two innovative components. First, we introduce a Dual-perspective\nCropping Module (DCM), which ensures that each sub-image contains continuous\ndetails from a local perspective and comprehensive information from a global\nperspective. Second, we introduce Dual-perspective Enhancement Module (DEM) to\nenable the mutual enhancement of global and local features, allowing INF-LLaVA\nto effectively process high-resolution images by simultaneously capturing\ndetailed local information and comprehensive global context. Extensive ablation\nstudies validate the effectiveness of these components, and experiments on a\ndiverse set of benchmarks demonstrate that INF-LLaVA outperforms existing\nMLLMs. Code and pretrained model are available at\nhttps://github.com/WeihuangLin/INF-LLaVA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16198v1",
    "published_date": "2024-07-23 06:02:30 UTC",
    "updated_date": "2024-07-23 06:02:30 UTC"
  },
  {
    "arxiv_id": "2407.16190v2",
    "title": "Artificial Agency and Large Language Models",
    "authors": [
      "Maud van Lier",
      "Gorka Muñoz-Gil"
    ],
    "abstract": "The arrival of Large Language Models (LLMs) has stirred up philosophical\ndebates about the possibility of realizing agency in an artificial manner. In\nthis work we contribute to the debate by presenting a theoretical model that\ncan be used as a threshold conception for artificial agents. The model defines\nagents as systems whose actions and goals are always influenced by a dynamic\nframework of factors that consists of the agent's accessible history, its\nadaptive repertoire and its external environment. This framework, in turn, is\ninfluenced by the actions that the agent takes and the goals that it forms. We\nshow with the help of the model that state-of-the-art LLMs are not agents yet,\nbut that there are elements to them that suggest a way forward. The paper\nargues that a combination of the agent architecture presented in Park et al.\n(2023) together with the use of modules like the Coscientist in Boiko et al.\n(2023) could potentially be a way to realize agency in an artificial manner. We\nend the paper by reflecting on the obstacles one might face in building such an\nartificial agent and by presenting possible directions for future research.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication in journal Intellectica, special issue\n  \"Philosophies of AI: thinking and writing with LLMs\" (Intellectica, issue 81)",
    "pdf_url": "http://arxiv.org/pdf/2407.16190v2",
    "published_date": "2024-07-23 05:32:00 UTC",
    "updated_date": "2024-07-24 07:32:25 UTC"
  },
  {
    "arxiv_id": "2407.16186v1",
    "title": "Automatic Environment Shaping is the Next Frontier in RL",
    "authors": [
      "Younghyo Park",
      "Gabriel B. Margolis",
      "Pulkit Agrawal"
    ],
    "abstract": "Many roboticists dream of presenting a robot with a task in the evening and\nreturning the next morning to find the robot capable of solving the task. What\nis preventing us from achieving this? Sim-to-real reinforcement learning (RL)\nhas achieved impressive performance on challenging robotics tasks, but requires\nsubstantial human effort to set up the task in a way that is amenable to RL.\nIt's our position that algorithmic improvements in policy optimization and\nother ideas should be guided towards resolving the primary bottleneck of\nshaping the training environment, i.e., designing observations, actions,\nrewards and simulation dynamics. Most practitioners don't tune the RL\nalgorithm, but other environment parameters to obtain a desirable controller.\nWe posit that scaling RL to diverse robotic tasks will only be achieved if the\ncommunity focuses on automating environment shaping procedures.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "ICML 2024 Position Track; Website at\n  https://auto-env-shaping.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2407.16186v1",
    "published_date": "2024-07-23 05:22:29 UTC",
    "updated_date": "2024-07-23 05:22:29 UTC"
  },
  {
    "arxiv_id": "2407.16174v1",
    "title": "Pixel Embedding: Fully Quantized Convolutional Neural Network with Differentiable Lookup Table",
    "authors": [
      "Hiroyuki Tokunaga",
      "Joel Nicholls",
      "Daria Vazhenina",
      "Atsunori Kanemura"
    ],
    "abstract": "By quantizing network weights and activations to low bitwidth, we can obtain\nhardware-friendly and energy-efficient networks. However, existing quantization\ntechniques utilizing the straight-through estimator and piecewise constant\nfunctions face the issue of how to represent originally high-bit input data\nwith low-bit values. To fully quantize deep neural networks, we propose pixel\nembedding, which replaces each float-valued input pixel with a vector of\nquantized values by using a lookup table. The lookup table or low-bit\nrepresentation of pixels is differentiable and trainable by backpropagation.\nSuch replacement of inputs with vectors is similar to word embedding in the\nnatural language processing field. Experiments on ImageNet and CIFAR-100 show\nthat pixel embedding reduces the top-5 error gap caused by quantizing the\nfloating points at the first layer to only 1% for the ImageNet dataset, and the\ntop-1 error gap caused by quantizing first and last layers to slightly over 1%\nfor the CIFAR-100 dataset. The usefulness of pixel embedding is further\ndemonstrated by inference time measurements, which demonstrate over 1.7 times\nspeedup compared to floating point precision first layer.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16174v1",
    "published_date": "2024-07-23 04:41:36 UTC",
    "updated_date": "2024-07-23 04:41:36 UTC"
  },
  {
    "arxiv_id": "2407.16171v1",
    "title": "Learning Trimodal Relation for Audio-Visual Question Answering with Missing Modality",
    "authors": [
      "Kyu Ri Park",
      "Hong Joo Lee",
      "Jung Uk Kim"
    ],
    "abstract": "Recent Audio-Visual Question Answering (AVQA) methods rely on complete visual\nand audio input to answer questions accurately. However, in real-world\nscenarios, issues such as device malfunctions and data transmission errors\nfrequently result in missing audio or visual modality. In such cases, existing\nAVQA methods suffer significant performance degradation. In this paper, we\npropose a framework that ensures robust AVQA performance even when a modality\nis missing. First, we propose a Relation-aware Missing Modal (RMM) generator\nwith Relation-aware Missing Modal Recalling (RMMR) loss to enhance the ability\nof the generator to recall missing modal information by understanding the\nrelationships and context among the available modalities. Second, we design an\nAudio-Visual Relation-aware (AVR) diffusion model with Audio-Visual Enhancing\n(AVE) loss to further enhance audio-visual features by leveraging the\nrelationships and shared cues between the audio-visual modalities. As a result,\nour method can provide accurate answers by effectively utilizing available\ninformation even when input modalities are missing. We believe our method holds\npotential applications not only in AVQA research but also in various\nmulti-modal scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16171v1",
    "published_date": "2024-07-23 04:35:56 UTC",
    "updated_date": "2024-07-23 04:35:56 UTC"
  },
  {
    "arxiv_id": "2407.16164v1",
    "title": "Representation Magnitude has a Liability to Privacy Vulnerability",
    "authors": [
      "Xingli Fang",
      "Jung-Eun Kim"
    ],
    "abstract": "The privacy-preserving approaches to machine learning (ML) models have made\nsubstantial progress in recent years. However, it is still opaque in which\ncircumstances and conditions the model becomes privacy-vulnerable, leading to a\nchallenge for ML models to maintain both performance and privacy. In this\npaper, we first explore the disparity between member and non-member data in the\nrepresentation of models under common training frameworks. We identify how the\nrepresentation magnitude disparity correlates with privacy vulnerability and\naddress how this correlation impacts privacy vulnerability. Based on the\nobservations, we propose Saturn Ring Classifier Module (SRCM), a plug-in\nmodel-level solution to mitigate membership privacy leakage. Through a confined\nyet effective representation space, our approach ameliorates models' privacy\nvulnerability while maintaining generalizability. The code of this work can be\nfound here: \\url{https://github.com/JEKimLab/AIES2024_SRCM}",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in the AAAI/ACM Conference on Artificial Intelligence,\n  Ethics, and Society, 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16164v1",
    "published_date": "2024-07-23 04:13:52 UTC",
    "updated_date": "2024-07-23 04:13:52 UTC"
  },
  {
    "arxiv_id": "2407.16160v2",
    "title": "UniMEL: A Unified Framework for Multimodal Entity Linking with Large Language Models",
    "authors": [
      "Liu Qi",
      "He Yongyi",
      "Lian Defu",
      "Zheng Zhi",
      "Xu Tong",
      "Liu Che",
      "Chen Enhong"
    ],
    "abstract": "Multimodal Entity Linking (MEL) is a crucial task that aims at linking\nambiguous mentions within multimodal contexts to the referent entities in a\nmultimodal knowledge base, such as Wikipedia. Existing methods focus heavily on\nusing complex mechanisms and extensive model tuning methods to model the\nmultimodal interaction on specific datasets. However, these methods\novercomplicate the MEL task and overlook the visual semantic information, which\nmakes them costly and hard to scale. Moreover, these methods can not solve the\nissues like textual ambiguity, redundancy, and noisy images, which severely\ndegrade their performance. Fortunately, the advent of Large Language Models\n(LLMs) with robust capabilities in text understanding and reasoning,\nparticularly Multimodal Large Language Models (MLLMs) that can process\nmultimodal inputs, provides new insights into addressing this challenge.\nHowever, how to design a universally applicable LLMs-based MEL approach remains\na pressing challenge. To this end, we propose UniMEL, a unified framework which\nestablishes a new paradigm to process multimodal entity linking tasks using\nLLMs. In this framework, we employ LLMs to augment the representation of\nmentions and entities individually by integrating textual and visual\ninformation and refining textual information. Subsequently, we employ the\nembedding-based method for retrieving and re-ranking candidate entities. Then,\nwith only ~0.26% of the model parameters fine-tuned, LLMs can make the final\nselection from the candidate entities. Extensive experiments on three public\nbenchmark datasets demonstrate that our solution achieves state-of-the-art\nperformance, and ablation studies verify the effectiveness of all modules. Our\ncode is available at https://github.com/Javkonline/UniMEL.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "CIKM 2024. The first two authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2407.16160v2",
    "published_date": "2024-07-23 03:58:08 UTC",
    "updated_date": "2024-08-21 01:52:02 UTC"
  },
  {
    "arxiv_id": "2408.01446v1",
    "title": "Estimating Environmental Cost Throughout Model's Adaptive Life Cycle",
    "authors": [
      "Vishwesh Sangarya",
      "Richard Bradford",
      "Jung-Eun Kim"
    ],
    "abstract": "With the rapid increase in the research, development, and application of\nneural networks in the current era, there is a proportional increase in the\nenergy needed to train and use models. Crucially, this is accompanied by the\nincrease in carbon emissions into the environment. A sustainable and socially\nbeneficial approach to reducing the carbon footprint and rising energy demands\nassociated with the modern age of AI/deep learning is the adaptive and\ncontinuous reuse of models with regard to changes in the environment of model\ndeployment or variations/changes in the input data. In this paper, we propose\nPreIndex, a predictive index to estimate the environmental and compute\nresources associated with model retraining to distributional shifts in data.\nPreIndex can be used to estimate environmental costs such as carbon emissions\nand energy usage when retraining from current data distribution to new data\ndistribution. It also correlates with and can be used to estimate other\nresource indicators associated with deep learning, such as epochs, gradient\nnorm, and magnitude of model parameter change. PreIndex requires only one\nforward pass of the data, following which it provides a single concise value to\nestimate resources associated with retraining to the new distribution shifted\ndata. We show that PreIndex can be reliably used across various datasets, model\narchitectures, different types, and intensities of distribution shifts. Thus,\nPreIndex enables users to make informed decisions for retraining to different\ndistribution shifts and determine the most cost-effective and sustainable\noption, allowing for the reuse of a model with a much smaller footprint in the\nenvironment. The code for this work is available here:\nhttps://github.com/JEKimLab/AIES2024PreIndex",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted in the AAAI/ACM Conference on Artificial Intelligence,\n  Ethics, and Society, 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01446v1",
    "published_date": "2024-07-23 03:58:06 UTC",
    "updated_date": "2024-07-23 03:58:06 UTC"
  },
  {
    "arxiv_id": "2407.16150v1",
    "title": "Predicting Stock Prices with FinBERT-LSTM: Integrating News Sentiment Analysis",
    "authors": [
      "Wenjun Gu",
      "Yihao Zhong",
      "Shizun Li",
      "Changsong Wei",
      "Liting Dong",
      "Zhuoyue Wang",
      "Chao Yan"
    ],
    "abstract": "The stock market's ascent typically mirrors the flourishing state of the\neconomy, whereas its decline is often an indicator of an economic downturn.\nTherefore, for a long time, significant correlation elements for predicting\ntrends in financial stock markets have been widely discussed, and people are\nbecoming increasingly interested in the task of financial text mining. The\ninherent instability of stock prices makes them acutely responsive to\nfluctuations within the financial markets. In this article, we use deep\nlearning networks, based on the history of stock prices and articles of\nfinancial, business, technical news that introduce market information to\npredict stock prices. We illustrate the enhancement of predictive precision by\nintegrating weighted news categories into the forecasting model. We developed a\npre-trained NLP model known as FinBERT, designed to discern the sentiments\nwithin financial texts. Subsequently, we advanced this model by incorporating\nthe sophisticated Long Short Term Memory (LSTM) architecture, thus constructing\nthe innovative FinBERT-LSTM model. This model utilizes news categories related\nto the stock market structure hierarchy, namely market, industry, and stock\nrelated news categories, combined with the stock market's stock price situation\nin the previous week for prediction. We selected NASDAQ-100 index stock data\nand trained the model on Benzinga news articles, and utilized Mean Absolute\nError (MAE), Mean Absolute Percentage Error (MAPE), and Accuracy as the key\nmetrics for the assessment and comparative analysis of the model's performance.\nThe results indicate that FinBERT-LSTM performs the best, followed by LSTM, and\nDNN model ranks third in terms of effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures, 2 tables, 2024 8th International Conference on\n  Cloud and Big Data Computing",
    "pdf_url": "http://arxiv.org/pdf/2407.16150v1",
    "published_date": "2024-07-23 03:26:07 UTC",
    "updated_date": "2024-07-23 03:26:07 UTC"
  },
  {
    "arxiv_id": "2407.16715v2",
    "title": "Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning",
    "authors": [
      "Yufeng Li",
      "Wenchao Zhao",
      "Bo Dang",
      "Xu Yan",
      "Weimin Wang",
      "Min Gao",
      "Mingxuan Xiao"
    ],
    "abstract": "In clinical treatment, identifying potential adverse reactions of drugs can\nhelp assist doctors in making medication decisions. In response to the problems\nin previous studies that features are high-dimensional and sparse, independent\nprediction models need to be constructed for each adverse reaction of drugs,\nand the prediction accuracy is low, this paper develops an adverse drug\nreaction prediction model based on knowledge graph embedding and deep learning,\nwhich can predict experimental results. Unified prediction of adverse drug\nreactions covered. Knowledge graph embedding technology can fuse the associated\ninformation between drugs and alleviate the shortcomings of high-dimensional\nsparsity in feature matrices, and the efficient training capabilities of deep\nlearning can improve the prediction accuracy of the model. This article builds\nan adverse drug reaction knowledge graph based on drug feature data; by\nanalyzing the embedding effect of the knowledge graph under different embedding\nstrategies, the best embedding strategy is selected to obtain sample vectors;\nand then a convolutional neural network model is constructed to predict adverse\nreactions. The results show that under the DistMult embedding model and\n400-dimensional embedding strategy, the convolutional neural network model has\nthe best prediction effect; the average accuracy, F_1 score, recall rate and\narea under the curve of repeated experiments are better than the methods\nreported in the literature. The obtained prediction model has good prediction\naccuracy and stability, and can provide an effective reference for later safe\nmedication guidance.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "12 pages, 4 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.16715v2",
    "published_date": "2024-07-23 03:25:55 UTC",
    "updated_date": "2024-07-27 15:09:51 UTC"
  },
  {
    "arxiv_id": "2407.16142v1",
    "title": "Diffusion Models as Optimizers for Efficient Planning in Offline RL",
    "authors": [
      "Renming Huang",
      "Yunqiang Pei",
      "Guoqing Wang",
      "Yangming Zhang",
      "Yang Yang",
      "Peng Wang",
      "Hengtao Shen"
    ],
    "abstract": "Diffusion models have shown strong competitiveness in offline reinforcement\nlearning tasks by formulating decision-making as sequential generation.\nHowever, the practicality of these methods is limited due to the lengthy\ninference processes they require. In this paper, we address this problem by\ndecomposing the sampling process of diffusion models into two decoupled\nsubprocesses: 1) generating a feasible trajectory, which is a time-consuming\nprocess, and 2) optimizing the trajectory. With this decomposition approach, we\nare able to partially separate efficiency and quality factors, enabling us to\nsimultaneously gain efficiency advantages and ensure quality assurance. We\npropose the Trajectory Diffuser, which utilizes a faster autoregressive model\nto handle the generation of feasible trajectories while retaining the\ntrajectory optimization process of diffusion models. This allows us to achieve\nmore efficient planning without sacrificing capability. To evaluate the\neffectiveness and efficiency of the Trajectory Diffuser, we conduct experiments\non the D4RL benchmarks. The results demonstrate that our method achieves $\\it\n3$-$\\it 10 \\times$ faster inference speed compared to previous sequence\nmodeling methods, while also outperforming them in terms of overall\nperformance. https://github.com/RenMing-Huang/TrajectoryDiffuser\n  Keywords: Reinforcement Learning and Efficient Planning and Diffusion Model",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper was accepted by ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16142v1",
    "published_date": "2024-07-23 03:00:01 UTC",
    "updated_date": "2024-07-23 03:00:01 UTC"
  },
  {
    "arxiv_id": "2407.21049v1",
    "title": "Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval",
    "authors": [
      "Yannick Assogba",
      "Donghao Ren"
    ],
    "abstract": "As language models support larger and larger context sizes, evaluating their\nability to make effective use of that context becomes increasingly important.\nWe analyze the ability of several code generation models to handle long range\ndependencies using a suite of multi-step key retrieval tasks in context windows\nup to 8k tokens in length. The tasks progressively increase in difficulty and\nallow more nuanced evaluation of model capabilities than tests like the popular\nneedle-in-the-haystack test. We find that performance degrades significantly\n(up to 2x) when a function references another function that is defined later in\nthe prompt. We also observe that models that use sliding window attention\nmechanisms have difficulty handling references further than the size of a\nsingle window. We perform simple prompt modifications using call graph\ninformation to improve multi-step retrieval performance up to 3x. Our analysis\nhighlights different facets of long-context performance and is suggestive of\nprompt construction strategies for code completion tools",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.21049v1",
    "published_date": "2024-07-23 02:45:22 UTC",
    "updated_date": "2024-07-23 02:45:22 UTC"
  },
  {
    "arxiv_id": "2407.16129v1",
    "title": "FoRA: Low-Rank Adaptation Model beyond Multimodal Siamese Network",
    "authors": [
      "Weiying Xie",
      "Yusi Zhang",
      "Tianlin Hui",
      "Jiaqing Zhang",
      "Jie Lei",
      "Yunsong Li"
    ],
    "abstract": "Multimodal object detection offers a promising prospect to facilitate robust\ndetection in various visual conditions. However, existing two-stream backbone\nnetworks are challenged by complex fusion and substantial parameter increments.\nThis is primarily due to large data distribution biases of multimodal\nhomogeneous information. In this paper, we propose a novel multimodal object\ndetector, named Low-rank Modal Adaptors (LMA) with a shared backbone. The\nshared parameters enhance the consistency of homogeneous information, while\nlightweight modal adaptors focus on modality unique features. Furthermore, we\ndesign an adaptive rank allocation strategy to adapt to the varying\nheterogeneity at different feature levels. When applied to two multimodal\nobject detection datasets, experiments validate the effectiveness of our\nmethod. Notably, on DroneVehicle, LMA attains a 10.4% accuracy improvement over\nthe state-of-the-art method with a 149M-parameters reduction. The code is\navailable at https://github.com/zyszxhy/FoRA.\n  Our work was submitted to ACM MM in April 2024, but was rejected. We will\ncontinue to refine our work and paper writing next, mainly including proof of\ntheory and multi-task applications of FoRA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16129v1",
    "published_date": "2024-07-23 02:27:52 UTC",
    "updated_date": "2024-07-23 02:27:52 UTC"
  },
  {
    "arxiv_id": "2407.16128v1",
    "title": "Advancing Brain Imaging Analysis Step-by-step via Progressive Self-paced Learning",
    "authors": [
      "Yanwu Yang",
      "Hairui Chen",
      "Jiesi Hu",
      "Xutao Guo",
      "Ting Ma"
    ],
    "abstract": "Recent advancements in deep learning have shifted the development of brain\nimaging analysis. However, several challenges remain, such as heterogeneity,\nindividual variations, and the contradiction between the high dimensionality\nand small size of brain imaging datasets. These issues complicate the learning\nprocess, preventing models from capturing intrinsic, meaningful patterns and\npotentially leading to suboptimal performance due to biases and overfitting.\nCurriculum learning (CL) presents a promising solution by organizing training\nexamples from simple to complex, mimicking the human learning process, and\npotentially fostering the development of more robust and accurate models.\nDespite its potential, the inherent limitations posed by small initial training\ndatasets present significant challenges, including overfitting and poor\ngeneralization. In this paper, we introduce the Progressive Self-Paced\nDistillation (PSPD) framework, employing an adaptive and progressive pacing and\ndistillation mechanism. This allows for dynamic curriculum adjustments based on\nthe states of both past and present models. The past model serves as a teacher,\nguiding the current model with gradually refined curriculum knowledge and\nhelping prevent the loss of previously acquired knowledge. We validate PSPD's\nefficacy and adaptability across various convolutional neural networks using\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, underscoring\nits superiority in enhancing model performance and generalization capabilities.\nThe source code for this approach will be released at\nhttps://github.com/Hrychen7/PSPD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "miccai-2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16128v1",
    "published_date": "2024-07-23 02:26:04 UTC",
    "updated_date": "2024-07-23 02:26:04 UTC"
  },
  {
    "arxiv_id": "2407.16127v1",
    "title": "Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion",
    "authors": [
      "Yang Liu",
      "Xiaobin Tian",
      "Zequn Sun",
      "Wei Hu"
    ],
    "abstract": "Traditional knowledge graph (KG) completion models learn embeddings to\npredict missing facts. Recent works attempt to complete KGs in a\ntext-generation manner with large language models (LLMs). However, they need to\nground the output of LLMs to KG entities, which inevitably brings errors. In\nthis paper, we present a finetuning framework, DIFT, aiming to unleash the KG\ncompletion ability of LLMs and avoid grounding errors. Given an incomplete\nfact, DIFT employs a lightweight model to obtain candidate entities and\nfinetunes an LLM with discrimination instructions to select the correct one\nfrom the given candidates. To improve performance while reducing instruction\ndata, DIFT uses a truncated sampling method to select useful facts for\nfinetuning and injects KG embeddings into the LLM. Extensive experiments on\nbenchmark datasets demonstrate the effectiveness of our proposed framework.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in the 23rd International Semantic Web Conference (ISWC\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.16127v1",
    "published_date": "2024-07-23 02:25:01 UTC",
    "updated_date": "2024-07-23 02:25:01 UTC"
  },
  {
    "arxiv_id": "2407.16714v1",
    "title": "Masked Graph Learning with Recurrent Alignment for Multimodal Emotion Recognition in Conversation",
    "authors": [
      "Tao Meng",
      "Fuchen Zhang",
      "Yuntao Shou",
      "Hongen Shao",
      "Wei Ai",
      "Keqin Li"
    ],
    "abstract": "Since Multimodal Emotion Recognition in Conversation (MERC) can be applied to\npublic opinion monitoring, intelligent dialogue robots, and other fields, it\nhas received extensive research attention in recent years. Unlike traditional\nunimodal emotion recognition, MERC can fuse complementary semantic information\nbetween multiple modalities (e.g., text, audio, and vision) to improve emotion\nrecognition. However, previous work ignored the inter-modal alignment process\nand the intra-modal noise information before multimodal fusion but directly\nfuses multimodal features, which will hinder the model for representation\nlearning. In this study, we have developed a novel approach called Masked Graph\nLearning with Recursive Alignment (MGLRA) to tackle this problem, which uses a\nrecurrent iterative module with memory to align multimodal features, and then\nuses the masked GCN for multimodal feature fusion. First, we employ LSTM to\ncapture contextual information and use a graph attention-filtering mechanism to\neliminate noise effectively within the modality. Second, we build a recurrent\niteration module with a memory function, which can use communication between\ndifferent modalities to eliminate the gap between modalities and achieve the\npreliminary alignment of features between modalities. Then, a cross-modal\nmulti-head attention mechanism is introduced to achieve feature alignment\nbetween modalities and construct a masked GCN for multimodal feature fusion,\nwhich can perform random mask reconstruction on the nodes in the graph to\nobtain better node feature representation. Finally, we utilize a multilayer\nperceptron (MLP) for emotion recognition. Extensive experiments on two\nbenchmark datasets (i.e., IEMOCAP and MELD) demonstrate that {MGLRA}\noutperforms state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.16714v1",
    "published_date": "2024-07-23 02:23:51 UTC",
    "updated_date": "2024-07-23 02:23:51 UTC"
  },
  {
    "arxiv_id": "2407.21048v1",
    "title": "APTNESS: Incorporating Appraisal Theory and Emotion Support Strategies for Empathetic Response Generation",
    "authors": [
      "Yuxuan Hu",
      "Minghuan Tan",
      "Chenwei Zhang",
      "Zixuan Li",
      "Xiaodan Liang",
      "Min Yang",
      "Chengming Li",
      "Xiping Hu"
    ],
    "abstract": "Empathetic response generation is designed to comprehend the emotions of\nothers and select the most appropriate strategies to assist them in resolving\nemotional challenges. Empathy can be categorized into cognitive empathy and\naffective empathy. The former pertains to the ability to understand and discern\nthe emotional issues and situations of others, while the latter involves the\ncapacity to provide comfort. To enhance one's empathetic abilities, it is\nessential to develop both these aspects. Therefore, we develop an innovative\nframework that combines retrieval augmentation and emotional support strategy\nintegration. Our framework starts with the introduction of a comprehensive\nemotional palette for empathy. We then apply appraisal theory to decompose this\npalette and create a database of empathetic responses. This database serves as\nan external resource and enhances the LLM's empathy by integrating semantic\nretrieval mechanisms. Moreover, our framework places a strong emphasis on the\nproper articulation of response strategies. By incorporating emotional support\nstrategies, we aim to enrich the model's capabilities in both cognitive and\naffective empathy, leading to a more nuanced and comprehensive empathetic\nresponse. Finally, we extract datasets ED and ET from the empathetic dialogue\ndataset \\textsc{EmpatheticDialogues} and ExTES based on dialogue length.\nExperiments demonstrate that our framework can enhance the empathy ability of\nLLMs from both cognitive and affective empathy perspectives. Our code is\nreleased at https://github.com/CAS-SIAT-XinHai/APTNESS.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Appectped to CIKM2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21048v1",
    "published_date": "2024-07-23 02:23:37 UTC",
    "updated_date": "2024-07-23 02:23:37 UTC"
  },
  {
    "arxiv_id": "2407.16123v1",
    "title": "Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility",
    "authors": [
      "Chenxing Wang"
    ],
    "abstract": "With the rapid development of location based services, multimodal\nspatio-temporal (ST) data including trajectories, transportation modes, traffic\nflow and social check-ins are being collected for deep learning based methods.\nThese deep learning based methods learn ST correlations to support the\ndownstream tasks in the fields such as smart mobility, smart city and other\nintelligent transportation systems. Despite their effectiveness, ST data fusion\nand forecasting methods face practical challenges in real-world scenarios.\nFirst, forecasting performance for ST data-insufficient area is inferior,\nmaking it necessary to transfer meta knowledge from heterogeneous area to\nenhance the sparse representations. Second, it is nontrivial to accurately\nforecast in multi-transportation-mode scenarios due to the fine-grained ST\nfeatures of similar transportation modes, making it necessary to distinguish\nand measure the ST correlations to alleviate the influence caused by entangled\nST features. At last, partial data modalities (e.g., transportation mode) are\nlost due to privacy or technical issues in certain scenarios, making it\nnecessary to effectively fuse the multimodal sparse ST features and enrich the\nST representations. To tackle these challenges, our research work aim to\ndevelop effective fusion and forecasting methods for multimodal ST data in\nsmart mobility scenario. In this paper, we will introduce our recent works that\ninvestigates the challenges in terms of various real-world applications and\nestablish the open challenges in this field for future work.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.16123v1",
    "published_date": "2024-07-23 02:08:22 UTC",
    "updated_date": "2024-07-23 02:08:22 UTC"
  },
  {
    "arxiv_id": "2407.16119v2",
    "title": "Uncertainty-Aware Deep Neural Representations for Visual Analysis of Vector Field Data",
    "authors": [
      "Atul Kumar",
      "Siddharth Garg",
      "Soumya Dutta"
    ],
    "abstract": "The widespread use of Deep Neural Networks (DNNs) has recently resulted in\ntheir application to challenging scientific visualization tasks. While advanced\nDNNs demonstrate impressive generalization abilities, understanding factors\nlike prediction quality, confidence, robustness, and uncertainty is crucial.\nThese insights aid application scientists in making informed decisions.\nHowever, DNNs lack inherent mechanisms to measure prediction uncertainty,\nprompting the creation of distinct frameworks for constructing robust\nuncertainty-aware models tailored to various visualization tasks. In this work,\nwe develop uncertainty-aware implicit neural representations to model\nsteady-state vector fields effectively. We comprehensively evaluate the\nefficacy of two principled deep uncertainty estimation techniques: (1) Deep\nEnsemble and (2) Monte Carlo Dropout, aimed at enabling uncertainty-informed\nvisual analysis of features within steady vector field data. Our detailed\nexploration using several vector data sets indicate that uncertainty-aware\nmodels generate informative visualization results of vector field features.\nFurthermore, incorporating prediction uncertainty improves the resilience and\ninterpretability of our DNN model, rendering it applicable for the analysis of\nnon-trivial vector field data sets.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GR",
    "comment": "Accepted for publication at IEEE Visualization 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16119v2",
    "published_date": "2024-07-23 01:59:58 UTC",
    "updated_date": "2024-08-10 10:06:38 UTC"
  },
  {
    "arxiv_id": "2407.16115v2",
    "title": "Transformer-based Graph Neural Networks for Battery Range Prediction in AIoT Battery-Swap Services",
    "authors": [
      "Zhao Li",
      "Yang Liu",
      "Chuan Zhou",
      "Xuanwu Liu",
      "Xuming Pan",
      "Buqing Cao",
      "Xindong Wu"
    ],
    "abstract": "The concept of the sharing economy has gained broad recognition, and within\nthis context, Sharing E-Bike Battery (SEB) have emerged as a focal point of\nsocietal interest. Despite the popularity, a notable discrepancy remains\nbetween user expectations regarding the remaining battery range of SEBs and the\nreality, leading to a pronounced inclination among users to find an available\nSEB during emergency situations. In response to this challenge, the integration\nof Artificial Intelligence of Things (AIoT) and battery-swap services has\nsurfaced as a viable solution. In this paper, we propose a novel structural\nTransformer-based model, referred to as the SEB-Transformer, designed\nspecifically for predicting the battery range of SEBs. The scenario is\nconceptualized as a dynamic heterogeneous graph that encapsulates the\ninteractions between users and bicycles, providing a comprehensive framework\nfor analysis. Furthermore, we incorporate the graph structure into the\nSEB-Transformer to facilitate the estimation of the remaining e-bike battery\nrange, in conjunction with mean structural similarity, enhancing the prediction\naccuracy. By employing the predictions made by our model, we are able to\ndynamically adjust the optimal cycling routes for users in real-time, while\nalso considering the strategic locations of charging stations, thereby\noptimizing the user experience. Empirically our results on real-world datasets\ndemonstrate the superiority of our model against nine competitive baselines.\nThese innovations, powered by AIoT, not only bridge the gap between user\nexpectations and the physical limitations of battery range but also\nsignificantly improve the operational efficiency and sustainability of SEB\nservices. Through these advancements, the shared electric bicycle ecosystem is\nevolving, making strides towards a more reliable, user-friendly, and\nsustainable mode of transportation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9pages, 6figures, accepted by IEEE ICWS 2024 The International\n  Conference on Web Services",
    "pdf_url": "http://arxiv.org/pdf/2407.16115v2",
    "published_date": "2024-07-23 01:33:21 UTC",
    "updated_date": "2025-02-14 06:38:49 UTC"
  },
  {
    "arxiv_id": "2408.01445v1",
    "title": "MiranDa: Mimicking the Learning Processes of Human Doctors to Achieve Causal Inference for Medication Recommendation",
    "authors": [
      "Ziheng Wang",
      "Xinhe Li",
      "Haruki Momma",
      "Ryoichi Nagatomi"
    ],
    "abstract": "To enhance therapeutic outcomes from a pharmacological perspective, we\npropose MiranDa, designed for medication recommendation, which is the first\nactionable model capable of providing the estimated length of stay in hospitals\n(ELOS) as counterfactual outcomes that guide clinical practice and model\ntraining. In detail, MiranDa emulates the educational trajectory of doctors\nthrough two gradient-scaling phases shifted by ELOS: an Evidence-based Training\nPhase that utilizes supervised learning and a Therapeutic Optimization Phase\ngrounds in reinforcement learning within the gradient space, explores optimal\nmedications by perturbations from ELOS. Evaluation of the Medical Information\nMart for Intensive Care III dataset and IV dataset, showcased the superior\nresults of our model across five metrics, particularly in reducing the ELOS.\nSurprisingly, our model provides structural attributes of medication\ncombinations proved in hyperbolic space and advocated \"procedure-specific\"\nmedication combinations. These findings posit that MiranDa enhanced medication\nefficacy. Notably, our paradigm can be applied to nearly all medical tasks and\nthose with information to evaluate predicted outcomes. The source code of the\nMiranDa model is available at https://github.com/azusakou/MiranDa.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01445v1",
    "published_date": "2024-07-23 01:08:57 UTC",
    "updated_date": "2024-07-23 01:08:57 UTC"
  },
  {
    "arxiv_id": "2407.16110v3",
    "title": "Analyzing Polysemy Evolution Using Semantic Cells",
    "authors": [
      "Yukio Ohsawa",
      "Dingming Xue",
      "Kaira Sekiguchi"
    ],
    "abstract": "The senses of words evolve. The sense of the same word may change from today\nto tomorrow, and multiple senses of the same word may be the result of the\nevolution of each other, that is, they may be parents and children. If we view\nJuba as an evolving ecosystem, the paradigm of learning the correct answer,\nwhich does not move with the sense of a word, is no longer valid. This paper is\na case study that shows that word polysemy is an evolutionary consequence of\nthe modification of Semantic Cells, which has al-ready been presented by the\nauthor, by introducing a small amount of diversity in its initial state as an\nexample of analyzing the current set of short sentences. In particular, the\nanalysis of a sentence sequence of 1000 sentences in some order for each of the\nfour senses of the word Spring, collected using Chat GPT, shows that the word\nacquires the most polysemy monotonically in the analysis when the senses are\narranged in the order in which they have evolved. In other words, we present a\nmethod for analyzing the dynamism of a word's acquiring polysemy with evolution\nand, at the same time, a methodology for viewing polysemy from an evolutionary\nframework rather than a learning-based one.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.14749",
    "pdf_url": "http://arxiv.org/pdf/2407.16110v3",
    "published_date": "2024-07-23 00:52:12 UTC",
    "updated_date": "2024-08-06 02:37:51 UTC"
  }
]
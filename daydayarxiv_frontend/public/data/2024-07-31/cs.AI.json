{
  "date": "2024-07-31",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-31 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型创新、强化学习、图像和视频处理以及医疗应用等领域，重点包括 Gemma 2 和 Llama 3 等大型语言模型的发布，以及 LLM 在生成式任务和多模态处理中的进展；令人印象深刻的文章有 Gemma 2 的开源模型优化（作者包括知名学者 Demis Hassabis），这些工作展示了 AI 在高效计算和实际应用中的潜力。\n\n以下是今日论文的精选摘要，我会优先讨论重要或话题性强的文章，并将相关论文归类讨论。对于次要或技术细节较少的论文，我会简要掠过，只突出核心贡献。\n\n### LLM 和生成式 AI 领域\n- **Gemma 2: Improving Open Language Models at a Practical Size**（中文：Gemma 2：改进实用规模的开源语言模型）  \n  这篇论文由 Gemma 团队发布，包括知名学者 Demis Hassabis，介绍了 Gemma 2 模型（2B 到 27B 参数），通过技术改进如局部-全局注意力，提升了多语言、编码和推理能力。主要贡献是模型在小规模下实现与 GPT-4 相当的性能，并开源预训练版本，显著提高了 AI 应用的效率和可访问性。\n\n- **The Llama 3 Herd of Models**（中文：Llama 3 模型系列）  \n  Meta 团队的作品，涵盖了多模态支持的 Llama 3 模型（405B 参数版本），包括图像、视频和语音处理。关键发现是通过密集 Transformer 架构，模型在多任务上达到 SOTA 水平，并开源了模型，强调了可扩展性和实际部署潜力。\n\n- **Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs**（中文：归纳还是演绎？重新审视大型语言模型的基本推理能力）  \n  作者包括 Yizhou Sun，这篇论文通过 SolverLearner 框架测试了 LLM 的归纳推理能力。贡献在于证明 LLM 在归纳任务中表现出色，但演绎能力较弱，提供了一个新视角来评估 LLM 的推理局限性。\n\n- **A Taxonomy of Stereotype Content in Large Language Models**（中文：大型语言模型中刻板印象内容的分类学）  \n  这篇工作分析了 ChatGPT 和其他 LLM 中的刻板印象维度（如道德和能力）。主要发现是 LLM 的刻板印象多维且变量性强，但整体更积极；为 AI 审计和去偏提供了实用框架。\n\n其他 LLM 相关论文如 **Correcting Negative Bias in Large Language Models** 和 **Measuring Progress in Dictionary Learning for Language Model Interpretability**，均探讨了 LLM 的偏置和可解释性，但细节较琐碎，我这里快速掠过：它们通过注意力机制和基准测试提升了模型的鲁棒性。\n\n### 医疗和图像处理领域\n- **S-SYNTH: Knowledge-Based, Synthetic Generation of Skin Images**（中文：S-SYNTH：基于知识的皮肤图像合成生成）  \n  作者包括 Aldo Badano，这篇论文提出了 S-SYNTH 框架，用于生成多样化的合成皮肤图像。贡献在于通过多层皮肤模型模拟真实变异，提高了 AI 在皮肤病变分割中的性能，并缓解了真实数据集的偏置问题，已被 MICCAI 2024 接受。\n\n- **StyleRF-VolVis: Style Transfer of Neural Radiance Fields for Expressive Volume Visualization**（中文：StyleRF-VolVis：用于表现力体积可视化的神经辐射场风格迁移）  \n  这篇工作引入了 StyleRF-VolVis 框架，实现 3D 场景的风格迁移。关键发现是它在保持视觉一致性的同时，支持非光真实编辑，已被 IEEE VIS 2024 接受，提升了医疗图像的可视化表达力。\n\n- **Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era**（中文：Vision Language 模型时代的一般化分布外检测及扩展）  \n  作者包括 Kiyoharu Aizawa，这篇综述讨论了视觉语言模型在异常检测中的挑战。贡献是提出了一个新框架，涵盖异常检测和分布外检测，强调了 VLM 的鲁棒性。\n\n其他医疗论文如 **Enhanced Uncertainty Estimation in Ultrasound Image Segmentation** 和 **Patient-centered Data Science**，主要优化了图像分割和预测模型，但不涉及重大创新，我简要提及：它们通过不确定性估计和多模态框架提高了诊断准确性。\n\n### 强化学习和多模态应用\n- **Distributed In-Context Learning under Non-IID Among Clients**（中文：非独立同分布条件下分布式上下文学习）  \n  这篇论文提出了一种数据驱动的预算分配方法，用于分布式强化学习。贡献在于针对非 IID 数据，提高了 LLM 在少样本任务中的性能，实验证明了其鲁棒性。\n\n- **Non-convolutional Graph Neural Networks**（中文：非卷积图神经网络）  \n  作者包括 Kyunghyun Cho，引入了 RUM 神经网络，解决了传统 GNN 的过度平滑问题。发现它在图分类任务中更具表达性和效率。\n\n其他如 **Resilience and Security of Deep Neural Networks** 和 **Automatic Generation of Behavioral Test Cases**，聚焦 DNN 安全和测试，但主题较常规，我快速掠过：它们通过调查和自动化方法提升了模型鲁棒性。\n\n总体而言，今天的论文展示了 AI 领域的多样性，但核心亮点在于 LLM 的优化和医疗应用的创新。如果您对特定主题感兴趣，我建议优先关注 Gemma 2 和 Llama 3 相关工作，它们可能带来更广泛的影响。明天见！",
  "papers": [
    {
      "arxiv_id": "2408.00197v1",
      "title": "Automated Software Vulnerability Static Code Analysis Using Generative Pre-Trained Transformer Models",
      "title_zh": "利用生成式预训练Transformer模型的自动化软件漏洞静态代码分析",
      "authors": [
        "Elijah Pelofske",
        "Vincent Urias",
        "Lorie M. Liebrock"
      ],
      "abstract": "Generative Pre-Trained Transformer models have been shown to be surprisingly\neffective at a variety of natural language processing tasks -- including\ngenerating computer code. We evaluate the effectiveness of open source GPT\nmodels for the task of automatic identification of the presence of vulnerable\ncode syntax (specifically targeting C and C++ source code). This task is\nevaluated on a selection of 36 source code examples from the NIST SARD dataset,\nwhich are specifically curated to not contain natural English that indicates\nthe presence, or lack thereof, of a particular vulnerability. The NIST SARD\nsource code dataset contains identified vulnerable lines of source code that\nare examples of one out of the 839 distinct Common Weakness Enumerations (CWE),\nallowing for exact quantification of the GPT output classification error rate.\nA total of 5 GPT models are evaluated, using 10 different inference\ntemperatures and 100 repetitions at each setting, resulting in 5,000 GPT\nqueries per vulnerable source code analyzed. Ultimately, we find that the GPT\nmodels that we evaluated are not suitable for fully automated vulnerability\nscanning because the false positive and false negative rates are too high to\nlikely be useful in practice. However, we do find that the GPT models perform\nsurprisingly well at automated vulnerability detection for some of the test\ncases, in particular surpassing random sampling, and being able to identify the\nexact lines of code that are vulnerable albeit at a low success rate. The best\nperforming GPT model result found was Llama-2-70b-chat-hf with inference\ntemperature of 0.1 applied to NIST SARD test case 149165 (which is an example\nof a buffer overflow vulnerability), which had a binary classification recall\nscore of 1.0 and a precision of 1.0 for correctly and uniquely identifying the\nvulnerable line of code and the correct CWE number.",
      "tldr_zh": "本研究评估了开源 Generative Pre-Trained Transformer (GPT) 模型在静态代码分析中的效能，专注于自动识别 C 和 C++ 源代码中的漏洞语法，使用 NIST SARD 数据集的36个示例进行测试。研究方法包括评估5个 GPT 模型、10个不同的推理温度设置，并进行100次重复查询，总计5000次查询，以量化分类错误率。结果显示，这些模型的假阳性和假阴性率过高，不适合实际的完全自动化漏洞扫描；然而，在某些测试案例中，模型表现出色，例如 Llama-2-70b-chat-hf 在温度0.1下成功识别缓冲区溢出漏洞，实现了1.0的召回和精确率。总的来说，这为 GPT 模型在漏洞检测中的潜在应用提供了见解，但强调了其局限性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00197v1",
      "published_date": "2024-07-31 23:33:26 UTC",
      "updated_date": "2024-07-31 23:33:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:31:11.504811"
    },
    {
      "arxiv_id": "2408.00193v2",
      "title": "Resilience and Security of Deep Neural Networks Against Intentional and Unintentional Perturbations: Survey and Research Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Sazzad Sayyed",
        "Milin Zhang",
        "Shahriar Rifat",
        "Ananthram Swami",
        "Michael De Lucia",
        "Francesco Restuccia"
      ],
      "abstract": "In order to deploy deep neural networks (DNNs) in high-stakes scenarios, it\nis imperative that DNNs provide inference robust to external perturbations -\nboth intentional and unintentional. Although the resilience of DNNs to\nintentional and unintentional perturbations has been widely investigated, a\nunified vision of these inherently intertwined problem domains is still\nmissing. In this work, we fill this gap by providing a survey of the state of\nthe art and highlighting the similarities of the proposed approaches.We also\nanalyze the research challenges that need to be addressed to deploy resilient\nand secure DNNs. As there has not been any such survey connecting the\nresilience of DNNs to intentional and unintentional perturbations, we believe\nthis work can help advance the frontier in both domains by enabling the\nexchange of ideas between the two communities.",
      "tldr_zh": "这篇论文调查了深度神经网络 (DNNs) 在高风险场景中对有意和无意扰动的韧性和安全性问题，强调了这两个领域之间的内在联系。论文通过回顾现有方法，突出其相似性，并分析需要解决的研究挑战，以填补统一视角的空白。主要贡献在于促进有意扰动（如攻击）和无意扰动（如噪声）研究社区的交流，从而推进更可靠的 DNNs 部署。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00193v2",
      "published_date": "2024-07-31 23:20:46 UTC",
      "updated_date": "2024-08-03 02:23:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:31:11.338788"
    },
    {
      "arxiv_id": "2408.00191v1",
      "title": "S-SYNTH: Knowledge-Based, Synthetic Generation of Skin Images",
      "title_zh": "S-SYNTH：基于知识的合成皮肤图像生成",
      "authors": [
        "Andrea Kim",
        "Niloufar Saharkhiz",
        "Elena Sizikova",
        "Miguel Lago",
        "Berkman Sahiner",
        "Jana Delfino",
        "Aldo Badano"
      ],
      "abstract": "Development of artificial intelligence (AI) techniques in medical imaging\nrequires access to large-scale and diverse datasets for training and\nevaluation. In dermatology, obtaining such datasets remains challenging due to\nsignificant variations in patient populations, illumination conditions, and\nacquisition system characteristics. In this work, we propose S-SYNTH, the first\nknowledge-based, adaptable open-source skin simulation framework to rapidly\ngenerate synthetic skin, 3D models and digitally rendered images, using an\nanatomically inspired multi-layer, multi-component skin and growing lesion\nmodel. The skin model allows for controlled variation in skin appearance, such\nas skin color, presence of hair, lesion shape, and blood fraction among other\nparameters. We use this framework to study the effect of possible variations on\nthe development and evaluation of AI models for skin lesion segmentation, and\nshow that results obtained using synthetic data follow similar comparative\ntrends as real dermatologic images, while mitigating biases and limitations\nfrom existing datasets including small dataset size, lack of diversity, and\nunderrepresentation.",
      "tldr_zh": "本文提出 S-SYNTH，一种基于知识的开源框架，用于快速生成合成皮肤图像、3D 模型和数字渲染图像，通过一个解剖学启发的多层多组件皮肤模型来模拟皮肤外观变化，如皮肤颜色、毛发和病变形状等参数。S-SYNTH 允许研究这些变化对 AI 模型（如皮肤病变分割）开发和评估的影响，发现合成数据能与真实图像显示类似趋势，同时缓解现有数据集的问题，包括规模小、缺乏多样性和代表性不足。总的来说，该框架为医疗图像 AI 训练提供了一个可控且偏见更少的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the International Conference on Medical Image Computing\n  and Computer Assisted Intervention (MICCAI) 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00191v1",
      "published_date": "2024-07-31 23:16:29 UTC",
      "updated_date": "2024-07-31 23:16:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:31:24.221800"
    },
    {
      "arxiv_id": "2408.00170v3",
      "title": "CREW: Facilitating Human-AI Teaming Research",
      "title_zh": "CREW: 促进人类-AI 团队协作研究",
      "authors": [
        "Lingyu Zhang",
        "Zhengran Ji",
        "Boyuan Chen"
      ],
      "abstract": "With the increasing deployment of artificial intelligence (AI) technologies,\nthe potential of humans working with AI agents has been growing at a great\nspeed. Human-AI teaming is an important paradigm for studying various aspects\nwhen humans and AI agents work together. The unique aspect of Human-AI teaming\nresearch is the need to jointly study humans and AI agents, demanding\nmultidisciplinary research efforts from machine learning to human-computer\ninteraction, robotics, cognitive science, neuroscience, psychology, social\nscience, and complex systems. However, existing platforms for Human-AI teaming\nresearch are limited, often supporting oversimplified scenarios and a single\ntask, or specifically focusing on either human-teaming research or multi-agent\nAI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming\nresearch in real-time decision-making scenarios and engage collaborations from\nmultiple scientific disciplines, with a strong emphasis on human involvement.\nIt includes pre-built tasks for cognitive studies and Human-AI teaming with\nexpandable potentials from our modular design. Following conventional cognitive\nneuroscience research, CREW also supports multimodal human physiological signal\nrecording for behavior analysis. Moreover, CREW benchmarks real-time\nhuman-guided reinforcement learning agents using state-of-the-art algorithms\nand well-tuned baselines. With CREW, we were able to conduct 50 human subject\nstudies within a week to verify the effectiveness of our benchmark.",
      "tldr_zh": "该论文介绍了 CREW 平台，一种旨在促进人类-AI Teaming 研究的新工具，解决现有平台在支持复杂场景和多学科协作方面的局限性。CREW 通过模块化设计，提供预构建的任务用于认知研究和实时决策场景，支持多模态人类生理信号记录以及人类引导的 Reinforcement Learning 代理基准测试。实验结果显示，该平台在短短一周内成功开展了 50 个人类受试者研究，验证了其在提升 Human-AI 团队合作效率方面的有效性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "Our project website is at: http://generalroboticslab.com/CREW",
      "pdf_url": "http://arxiv.org/pdf/2408.00170v3",
      "published_date": "2024-07-31 21:43:55 UTC",
      "updated_date": "2025-01-01 18:42:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:31:39.563955"
    },
    {
      "arxiv_id": "2408.00167v2",
      "title": "Finch: Prompt-guided Key-Value Cache Compression",
      "title_zh": "Finch：提示引导的键-值缓存压缩",
      "authors": [
        "Giulio Corallo",
        "Paolo Papotti"
      ],
      "abstract": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
      "tldr_zh": "该研究针对大型语言模型处理长输入上下文时面临的上下文窗口和 GPU 内存限制，提出了一种新方法 Finch，即提示引导的 Key-Value 缓存压缩技术。Finch 利用预训练的自注意力权重，在给定提示的情况下，迭代识别并存储文本中与提示最相关的 Key (K) 和 Value (V) 对，从而在 KV cache 中实现长文本的压缩，支持高达 93x 的压缩率。实验结果显示，该方法能保持输入的语义完整性，而无需 fine-tuning，为模型处理大输入提供了高效解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication at TACL - pre-MIT Press publication version",
      "pdf_url": "http://arxiv.org/pdf/2408.00167v2",
      "published_date": "2024-07-31 21:33:56 UTC",
      "updated_date": "2024-08-13 09:08:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:31:50.576371"
    },
    {
      "arxiv_id": "2408.00166v1",
      "title": "Review of Explainable Graph-Based Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Thanet Markchom",
        "Huizhi Liang",
        "James Ferryman"
      ],
      "abstract": "Explainability of recommender systems has become essential to ensure users'\ntrust and satisfaction. Various types of explainable recommender systems have\nbeen proposed including explainable graph-based recommender systems. This\nreview paper discusses state-of-the-art approaches of these systems and\ncategorizes them based on three aspects: learning methods, explaining methods,\nand explanation types. It also explores the commonly used datasets,\nexplainability evaluation methods, and future directions of this research area.\nCompared with the existing review papers, this paper focuses on explainability\nbased on graphs and covers the topics required for developing novel explainable\ngraph-based recommender systems.",
      "tldr_zh": "这篇综述论文审视了可解释的基于图的推荐系统（Explainable Graph-Based Recommender Systems），强调其在提升用户信任和满意度方面的关键作用。论文根据学习方法（learning methods）、解释方法（explaining methods）和解释类型（explanation types）对现有方法进行了分类，并探讨了常用数据集、解释性评估方法以及未来研究方向。与现有综述不同，该论文专注于基于图的解释性框架，为开发新型系统提供了全面指导。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00166v1",
      "published_date": "2024-07-31 21:30:36 UTC",
      "updated_date": "2024-07-31 21:30:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:32:00.141044"
    },
    {
      "arxiv_id": "2408.00165v3",
      "title": "Non-convolutional Graph Neural Networks",
      "title_zh": "非卷积图神经网络",
      "authors": [
        "Yuanqing Wang",
        "Kyunghyun Cho"
      ],
      "abstract": "Rethink convolution-based graph neural networks (GNN) -- they\ncharacteristically suffer from limited expressiveness, over-smoothing, and\nover-squashing, and require specialized sparse kernels for efficient\ncomputation. Here, we design a simple graph learning module entirely free of\nconvolution operators, coined random walk with unifying memory (RUM) neural\nnetwork, where an RNN merges the topological and semantic graph features along\nthe random walks terminating at each node. Relating the rich literature on RNN\nbehavior and graph topology, we theoretically show and experimentally verify\nthat RUM attenuates the aforementioned symptoms and is more expressive than the\nWeisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level\nclassification and regression tasks, RUM not only achieves competitive\nperformance, but is also robust, memory-efficient, scalable, and faster than\nthe simplest convolutional GNNs.",
      "tldr_zh": "本文重新审视基于卷积的 Graph Neural Networks (GNNs)，指出其存在表达能力有限、过度平滑和过度压缩等问题，并提出了一种非卷积模块：Random Walk with Unifying Memory (RUM) 神经网络，该模块使用 RNN 整合随机游走中的拓扑和语义图特征。理论分析和实验验证显示，RUM 比 Weisfeiler-Lehman (WL) isomorphism test 更具表达性，能够缓解传统 GNN 的缺陷。在多种节点级和图级分类及回归任务上，RUM 表现出竞争性能、鲁棒性强、内存高效、可扩展且比简单卷积 GNN 更快。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00165v3",
      "published_date": "2024-07-31 21:29:26 UTC",
      "updated_date": "2024-09-29 00:15:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:32:13.888957"
    },
    {
      "arxiv_id": "2408.07838v1",
      "title": "A Culturally-Aware Tool for Crowdworkers: Leveraging Chronemics to Support Diverse Work Styles",
      "title_zh": "面向众包",
      "authors": [
        "Carlos Toxtli",
        "Christopher Curtis",
        "Saiph Savage"
      ],
      "abstract": "Crowdsourcing markets are expanding worldwide, but often feature standardized\ninterfaces that ignore the cultural diversity of their workers, negatively\nimpacting their well-being and productivity. To transform these workplace\ndynamics, this paper proposes creating culturally-aware workplace tools,\nspecifically designed to adapt to the cultural dimensions of monochronic and\npolychronic work styles. We illustrate this approach with \"CultureFit,\" a tool\nthat we engineered based on extensive research in Chronemics and culture\ntheories. To study and evaluate our tool in the real world, we conducted a\nfield experiment with 55 workers from 24 different countries. Our field\nexperiment revealed that CultureFit significantly improved the earnings of\nworkers from cultural backgrounds often overlooked in design. Our study is\namong the pioneering efforts to examine culturally aware digital labor\ninterventions. It also provides access to a dataset with over two million data\npoints on culture and digital work, which can be leveraged for future research\nin this emerging field. The paper concludes by discussing the importance and\nfuture possibilities of incorporating cultural insights into the design of\ntools for digital labor.",
      "tldr_zh": "这篇论文针对众包市场的标准化界面忽略文化多样性问题，提出了一种文化感知工具 CultureFit，利用 Chronemics 理论来适应 monochronic and polychronic 工作风格。研究通过实地实验，涉及 55 名来自 24 个国家的工人，发现 CultureFit 显著提高了被设计忽略的文化背景工人的收入。论文提供了超过 200 万数据点的开源数据集，并讨论了将文化洞察融入数字劳动工具设计的重要性，为未来研究奠定了基础。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "68U35",
        "H.5.2"
      ],
      "primary_category": "cs.HC",
      "comment": "32 pages, 9 figures, Computer Supported Cooperative Work (CSCW) 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.07838v1",
      "published_date": "2024-07-31 21:22:41 UTC",
      "updated_date": "2024-07-31 21:22:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:32:30.793785"
    },
    {
      "arxiv_id": "2408.00162v1",
      "title": "A Taxonomy of Stereotype Content in Large Language Models",
      "title_zh": "大型语言模型中刻板印象内容的分类法",
      "authors": [
        "Gandalf Nicolas",
        "Aylin Caliskan"
      ],
      "abstract": "This study introduces a taxonomy of stereotype content in contemporary large\nlanguage models (LLMs). We prompt ChatGPT 3.5, Llama 3, and Mixtral 8x7B, three\npowerful and widely used LLMs, for the characteristics associated with 87\nsocial categories (e.g., gender, race, occupations). We identify 14 stereotype\ndimensions (e.g., Morality, Ability, Health, Beliefs, Emotions), accounting for\n~90% of LLM stereotype associations. Warmth and Competence facets were the most\nfrequent content, but all other dimensions were significantly prevalent.\nStereotypes were more positive in LLMs (vs. humans), but there was significant\nvariability across categories and dimensions. Finally, the taxonomy predicted\nthe LLMs' internal evaluations of social categories (e.g., how\npositively/negatively the categories were represented), supporting the\nrelevance of a multidimensional taxonomy for characterizing LLM stereotypes.\nOur findings suggest that high-dimensional human stereotypes are reflected in\nLLMs and must be considered in AI auditing and debiasing to minimize\nunidentified harms from reliance in low-dimensional views of bias in LLMs.",
      "tldr_zh": "本研究提出了一种针对大型语言模型(LLMs)中刻板印象内容的分类法，通过提示ChatGPT 3.5、Llama 3和Mixtral 8x7B模型分析87个社会类别（如性别、种族、职业）的特征，识别出14个刻板印象维度（如Morality、Ability、Health），这些维度占约90%的LLMs刻板印象关联，其中Warmth和Competence最为常见。结果显示，LLMs的刻板印象整体比人类更正面，但存在跨类别和维度的显著变异性，且该分类法能准确预测模型对社会类别的内部评估（如正面/负面程度）。研究强调，在AI审计和debiasing中需考虑多维刻板印象，以减少低维偏见观点带来的潜在危害。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00162v1",
      "published_date": "2024-07-31 21:14:41 UTC",
      "updated_date": "2024-07-31 21:14:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:32:39.708807"
    },
    {
      "arxiv_id": "2408.00161v2",
      "title": "Automatic Generation of Behavioral Test Cases For Natural Language Processing Using Clustering and Prompting",
      "title_zh": "翻译失败",
      "authors": [
        "Ying Li",
        "Rahul Singh",
        "Tarun Joshi",
        "Agus Sudjianto"
      ],
      "abstract": "Recent work in behavioral testing for natural language processing (NLP)\nmodels, such as Checklist, is inspired by related paradigms in software\nengineering testing. They allow evaluation of general linguistic capabilities\nand domain understanding, hence can help evaluate conceptual soundness and\nidentify model weaknesses. However, a major challenge is the creation of test\ncases. The current packages rely on semi-automated approach using manual\ndevelopment which requires domain expertise and can be time consuming. This\npaper introduces an automated approach to develop test cases by exploiting the\npower of large language models and statistical techniques. It clusters the text\nrepresentations to carefully construct meaningful groups and then apply\nprompting techniques to automatically generate Minimal Functionality Tests\n(MFT). The well-known Amazon Reviews corpus is used to demonstrate our\napproach. We analyze the behavioral test profiles across four different\nclassification algorithms and discuss the limitations and strengths of those\nmodels.",
      "tldr_zh": "这篇论文针对 NLP 模型的行为测试（Behavioral Testing）面临的挑战，提出了一种自动化生成测试用例的方法，以解决手动开发耗时且需要专业知识的问题。方法结合聚类（Clustering）技术对文本表示进行分组，并使用提示（Prompting）技术及大型语言模型自动生成 Minimal Functionality Tests (MFT)。以 Amazon Reviews 语料库为例，论文分析了四个分类算法的行为测试表现，并讨论了这些模型的优缺点。该方法有助于更高效地评估 NLP 模型的语言能力和领域理解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00161v2",
      "published_date": "2024-07-31 21:12:21 UTC",
      "updated_date": "2024-08-08 16:31:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:32:47.872002"
    },
    {
      "arxiv_id": "2408.00151v1",
      "title": "Moderating Group Conversation Dynamics with Social Robots",
      "title_zh": "翻译失败",
      "authors": [
        "Lucrezia Grassi",
        "Carmine Tommaso Recchiuto",
        "Antonio Sgorbissa"
      ],
      "abstract": "This research investigates the impact of social robot participation in group\nconversations and assesses the effectiveness of various addressing policies.\nThe study involved 300 participants, divided into groups of four, interacting\nwith a humanoid robot serving as the moderator. The robot utilized conversation\ndata to determine the most appropriate speaker to address. The findings\nindicate that the robot's addressing policy significantly influenced\nconversation dynamics, resulting in more balanced attention to each participant\nand a reduction in subgroup formation.",
      "tldr_zh": "这篇论文研究了社交 robots 在小组对话中的作用，并评估了不同 addressing policies 的有效性。实验涉及 300 名参与者，分成四人小组，与一个作为主持人的类人机器人互动，机器人利用对话数据决定最合适的说话者。结果表明，机器人的 addressing policy 显著影响了对话动态，导致参与者获得更均衡的关注，并减少了子群体形成。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 6 figures, 1 table. Accepted at the workshop on advancing\n  Group Understanding and robots' adaptive behavior (GROUND), held at the\n  Robotics Science and Systems (RSS) Conference, 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00151v1",
      "published_date": "2024-07-31 20:29:20 UTC",
      "updated_date": "2024-07-31 20:29:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:33:01.921078"
    },
    {
      "arxiv_id": "2408.00150v1",
      "title": "StyleRF-VolVis: Style Transfer of Neural Radiance Fields for Expressive Volume Visualization",
      "title_zh": "翻译失败",
      "authors": [
        "Kaiyuan Tang",
        "Chaoli Wang"
      ],
      "abstract": "In volume visualization, visualization synthesis has attracted much attention\ndue to its ability to generate novel visualizations without following the\nconventional rendering pipeline. However, existing solutions based on\ngenerative adversarial networks often require many training images and take\nsignificant training time. Still, issues such as low quality, consistency, and\nflexibility persist. This paper introduces StyleRF-VolVis, an innovative style\ntransfer framework for expressive volume visualization (VolVis) via neural\nradiance field (NeRF). The expressiveness of StyleRF-VolVis is upheld by its\nability to accurately separate the underlying scene geometry (i.e., content)\nand color appearance (i.e., style), conveniently modify color, opacity, and\nlighting of the original rendering while maintaining visual content consistency\nacross the views, and effectively transfer arbitrary styles from reference\nimages to the reconstructed 3D scene. To achieve these, we design a base NeRF\nmodel for scene geometry extraction, a palette color network to classify\nregions of the radiance field for photorealistic editing, and an unrestricted\ncolor network to lift the color palette constraint via knowledge distillation\nfor non-photorealistic editing. We demonstrate the superior quality,\nconsistency, and flexibility of StyleRF-VolVis by experimenting with various\nvolume rendering scenes and reference images and comparing StyleRF-VolVis\nagainst other image-based (AdaIN), video-based (ReReVST), and NeRF-based (ARF\nand SNeRF) style rendering solutions.",
      "tldr_zh": "这篇论文提出了 StyleRF-VolVis，一种基于 Neural Radiance Fields (NeRF) 的创新框架，用于体积可视化 (VolVis) 的风格转移，旨在解决现有方法的训练效率低、质量差和灵活性不足等问题。该框架通过准确分离场景几何（内容）和颜色外观（风格），实现了对颜色、不透明度和照明的灵活编辑，同时保持跨视图内容一致性，并支持从参考图像转移任意风格。核心组件包括基础 NeRF 模型用于提取场景几何、调色板颜色网络实现逼真编辑，以及不受限制的颜色网络通过知识蒸馏支持非逼真编辑。实验结果表明，StyleRF-VolVis 在多种体积渲染场景中，相比基于图像（如 AdaIN）、视频（如 ReReVST）和 NeRF（如 ARF 和 SNeRF）的现有方法，具有更高的质量、一致性和灵活性。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted by IEEE VIS 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00150v1",
      "published_date": "2024-07-31 20:26:30 UTC",
      "updated_date": "2024-07-31 20:26:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:33:14.768982"
    },
    {
      "arxiv_id": "2408.00147v1",
      "title": "Formal Ethical Obligations in Reinforcement Learning Agents: Verification and Policy Updates",
      "title_zh": "强化学习代理中的正式伦理义务：验证和策略更新",
      "authors": [
        "Colin Shea-Blymyer",
        "Houssam Abbas"
      ],
      "abstract": "When designing agents for operation in uncertain environments, designers need\ntools to automatically reason about what agents ought to do, how that conflicts\nwith what is actually happening, and how a policy might be modified to remove\nthe conflict. These obligations include ethical and social obligations,\npermissions and prohibitions, which constrain how the agent achieves its\nmission and executes its policy. We propose a new deontic logic, Expected Act\nUtilitarian deontic logic, for enabling this reasoning at design time: for\nspecifying and verifying the agent's strategic obligations, then modifying its\npolicy from a reference policy to meet those obligations. Unlike approaches\nthat work at the reward level, working at the logical level increases the\ntransparency of the trade-offs. We introduce two algorithms: one for\nmodel-checking whether an RL agent has the right strategic obligations, and one\nfor modifying a reference decision policy to make it meet obligations expressed\nin our logic. We illustrate our algorithms on DAC-MDPs which accurately\nabstract neural decision policies, and on toy gridworld environments.",
      "tldr_zh": "这篇论文探讨了在不确定环境中设计强化学习（RL）代理时，如何使用工具来推理代理的伦理和社会义务、许可和禁止，并解决这些义务与实际策略的冲突。论文提出了一种新的逻辑框架——Expected Act Utilitarian deontic logic，用于在设计阶段指定、验证代理的战略义务，并修改参考策略以符合这些义务。与奖励层面的方法不同，这种逻辑层面的方法提升了决策权衡的透明度。论文引入两个算法：一个用于模型检查RL代理是否满足义务，另一个用于更新决策策略，并在DAC-MDPs和玩具网格世界环境中进行了演示。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00147v1",
      "published_date": "2024-07-31 20:21:15 UTC",
      "updated_date": "2024-07-31 20:21:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:33:24.843706"
    },
    {
      "arxiv_id": "2408.00144v1",
      "title": "Distributed In-Context Learning under Non-IID Among Clients",
      "title_zh": "翻译失败",
      "authors": [
        "Siqi Liang",
        "Sumyeong Ahn",
        "Jiayu Zhou"
      ],
      "abstract": "Advancements in large language models (LLMs) have shown their effectiveness\nin multiple complicated natural language reasoning tasks. A key challenge\nremains in adapting these models efficiently to new or unfamiliar tasks.\nIn-context learning (ICL) provides a promising solution for few-shot adaptation\nby retrieving a set of data points relevant to a query, called in-context\nexamples (ICE), from a training dataset and providing them during the inference\nas context. Most existing studies utilize a centralized training dataset, yet\nmany real-world datasets may be distributed among multiple clients, and remote\ndata retrieval can be associated with costs. Especially when the client data\nare non-identical independent distributions (non-IID), retrieving from clients\na proper set of ICEs needed for a test query presents critical challenges. In\nthis paper, we first show that in this challenging setting, test queries will\nhave different preferences among clients because of non-IIDness, and equal\ncontribution often leads to suboptimal performance. We then introduce a novel\napproach to tackle the distributed non-IID ICL problem when a data usage budget\nis present. The principle is that each client's proper contribution (budget)\nshould be designed according to the preference of each query for that client.\nOur approach uses a data-driven manner to allocate a budget for each client,\ntailored to each test query. Through extensive empirical studies on diverse\ndatasets, our framework demonstrates superior performance relative to competing\nbaselines.",
      "tldr_zh": "该论文探讨了大语言模型 (LLMs) 在分布式非独立同分布 (non-IID) 数据环境下的 In-Context Learning (ICL) 挑战，指出测试查询对不同客户端的偏好差异会导致传统等贡献方法性能不佳。作者提出了一种新框架，通过数据驱动方式根据每个测试查询的偏好为客户端分配数据使用预算，实现针对性的 In-Context Examples (ICE) 检索。实验结果显示，该方法在多样数据集上比竞争基线提升了整体性能，为高效的分布式 ICL 适应提供了实用解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2408.00144v1",
      "published_date": "2024-07-31 20:06:25 UTC",
      "updated_date": "2024-07-31 20:06:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:33:44.570885"
    },
    {
      "arxiv_id": "2408.00137v2",
      "title": "Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment",
      "title_zh": "通过负面注意力分数对齐纠正大型语言模型中的负面偏差",
      "authors": [
        "Sangwon Yu",
        "Jongyoon Song",
        "Bongkyu Hwang",
        "Hoyoung Kang",
        "Sooah Cho",
        "Junhwa Choi",
        "Seongho Joe",
        "Taehee Lee",
        "Youngjune L. Gwon",
        "Sungroh Yoon"
      ],
      "abstract": "A binary decision task, like yes-no questions or answer verification,\nreflects a significant real-world scenario such as where users look for\nconfirmation about the correctness of their decisions on specific issues. In\nthis work, we observe that language models exhibit a negative bias in the\nbinary decisions of complex reasoning tasks. Based on our observations and the\nrationale about attention-based model dynamics, we propose a negative attention\nscore (NAS) to systematically and quantitatively formulate negative bias. Based\non NAS, we identify attention heads that attend to negative tokens provided in\nthe instructions as answer candidate of binary decisions, regardless of the\nquestion in the prompt, and validate their association with the negative bias.\nAdditionally, we propose the negative attention score alignment (NASA) method,\nwhich is a parameter-efficient fine-tuning technique to address the extracted\nnegatively biased attention heads. Experimental results from various domains of\nreasoning tasks and large model search space demonstrate that NASA\nsignificantly reduces the gap between precision and recall caused by negative\nbias while preserving their generalization abilities.",
      "tldr_zh": "本研究发现，大型语言模型在二元决策任务（如是/否问题）中存在负面偏差，导致精度和召回率不平衡。研究者提出Negative Attention Score (NAS)来量化这种偏差，并识别出特定attention heads，它们总是关注指令中的负面标记，而非问题内容。基于此，他们开发了Negative Attention Score Alignment (NASA)方法，这是一种参数高效的微调技术，用于修正这些偏差。实验结果显示，NASA在各种推理任务和模型上显著缩小了精度与召回率之间的差距，同时保持了模型的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2025 Oral",
      "pdf_url": "http://arxiv.org/pdf/2408.00137v2",
      "published_date": "2024-07-31 19:50:57 UTC",
      "updated_date": "2025-04-29 01:52:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:33:47.535022"
    },
    {
      "arxiv_id": "2408.00131v1",
      "title": "Distributionally Robust Optimization as a Scalable Framework to Characterize Extreme Value Distributions",
      "title_zh": "分布鲁棒优化作为一种可扩展框架来表征极值分布",
      "authors": [
        "Patrick Kuiper",
        "Ali Hasan",
        "Wenhao Yang",
        "Yuting Ng",
        "Hoda Bidkhori",
        "Jose Blanchet",
        "Vahid Tarokh"
      ],
      "abstract": "The goal of this paper is to develop distributionally robust optimization\n(DRO) estimators, specifically for multidimensional Extreme Value Theory (EVT)\nstatistics. EVT supports using semi-parametric models called max-stable\ndistributions built from spatial Poisson point processes. While powerful, these\nmodels are only asymptotically valid for large samples. However, since extreme\ndata is by definition scarce, the potential for model misspecification error is\ninherent to these applications, thus DRO estimators are natural. In order to\nmitigate over-conservative estimates while enhancing out-of-sample performance,\nwe study DRO estimators informed by semi-parametric max-stable constraints in\nthe space of point processes. We study both tractable convex formulations for\nsome problems of interest (e.g. CVaR) and more general neural network based\nestimators. Both approaches are validated using synthetically generated data,\nrecovering prescribed characteristics, and verifying the efficacy of the\nproposed techniques. Additionally, the proposed method is applied to a real\ndata set of financial returns for comparison to a previous analysis. We\nestablished the proposed model as a novel formulation in the multivariate EVT\ndomain, and innovative with respect to performance when compared to relevant\nalternate proposals.",
      "tldr_zh": "本研究提出了一种可扩展的分布鲁棒优化 (DRO) 框架，用于表征多维 Extreme Value Theory (EVT) 统计量的分布。该框架通过结合半参数 max-stable distributions 和 DRO 估计器，缓解了极端数据稀缺导致的模型失真问题，同时开发了可处理的凸优化（如 CVaR）和基于神经网络的估计器。实验结果显示，该方法在合成数据上成功恢复预设特征，并在真实金融回报数据集上表现出色，与现有方法相比具有更高的性能和创新性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "q-fin.RM"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00131v1",
      "published_date": "2024-07-31 19:45:27 UTC",
      "updated_date": "2024-07-31 19:45:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:33:59.931085"
    },
    {
      "arxiv_id": "2408.00123v1",
      "title": "Semantic Codebook Learning for Dynamic Recommendation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zheqi Lv",
        "Shaoxuan He",
        "Tianyu Zhan",
        "Shengyu Zhang",
        "Wenqiao Zhang",
        "Jingyuan Chen",
        "Zhou Zhao",
        "Fei Wu"
      ],
      "abstract": "Dynamic sequential recommendation (DSR) can generate model parameters based\non user behavior to improve the personalization of sequential recommendation\nunder various user preferences. However, it faces the challenges of large\nparameter search space and sparse and noisy user-item interactions, which\nreduces the applicability of the generated model parameters. The Semantic\nCodebook Learning for Dynamic Recommendation Models (SOLID) framework presents\na significant advancement in DSR by effectively tackling these challenges. By\ntransforming item sequences into semantic sequences and employing a dual\nparameter model, SOLID compresses the parameter generation search space and\nleverages homogeneity within the recommendation system. The introduction of the\nsemantic metacode and semantic codebook, which stores disentangled item\nrepresentations, ensures robust and accurate parameter generation. Extensive\nexperiments demonstrates that SOLID consistently outperforms existing DSR,\ndelivering more accurate, stable, and robust recommendations.",
      "tldr_zh": "该论文提出SOLID框架，用于解决动态序列推荐(DSR)模型在参数搜索空间过大以及用户-物品交互稀疏和noisy问题上的挑战。SOLID通过将物品序列转化为语义序列，并采用双参数模型和semantic codebook（存储分离的物品表示）来压缩搜索空间并利用推荐系统的同质性，从而确保参数生成的鲁棒性和准确性。实验结果显示，SOLID在多项指标上优于现有DSR方法，提供更准确、稳定和鲁棒的个性化推荐。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.MM",
        "cs.SI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00123v1",
      "published_date": "2024-07-31 19:25:25 UTC",
      "updated_date": "2024-07-31 19:25:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:34:11.967905"
    },
    {
      "arxiv_id": "2408.00118v3",
      "title": "Gemma 2: Improving Open Language Models at a Practical Size",
      "title_zh": "翻译失败",
      "authors": [
        "Gemma Team",
        "Morgane Riviere",
        "Shreya Pathak",
        "Pier Giuseppe Sessa",
        "Cassidy Hardin",
        "Surya Bhupatiraju",
        "Léonard Hussenot",
        "Thomas Mesnard",
        "Bobak Shahriari",
        "Alexandre Ramé",
        "Johan Ferret",
        "Peter Liu",
        "Pouya Tafti",
        "Abe Friesen",
        "Michelle Casbon",
        "Sabela Ramos",
        "Ravin Kumar",
        "Charline Le Lan",
        "Sammy Jerome",
        "Anton Tsitsulin",
        "Nino Vieillard",
        "Piotr Stanczyk",
        "Sertan Girgin",
        "Nikola Momchev",
        "Matt Hoffman",
        "Shantanu Thakoor",
        "Jean-Bastien Grill",
        "Behnam Neyshabur",
        "Olivier Bachem",
        "Alanna Walton",
        "Aliaksei Severyn",
        "Alicia Parrish",
        "Aliya Ahmad",
        "Allen Hutchison",
        "Alvin Abdagic",
        "Amanda Carl",
        "Amy Shen",
        "Andy Brock",
        "Andy Coenen",
        "Anthony Laforge",
        "Antonia Paterson",
        "Ben Bastian",
        "Bilal Piot",
        "Bo Wu",
        "Brandon Royal",
        "Charlie Chen",
        "Chintu Kumar",
        "Chris Perry",
        "Chris Welty",
        "Christopher A. Choquette-Choo",
        "Danila Sinopalnikov",
        "David Weinberger",
        "Dimple Vijaykumar",
        "Dominika Rogozińska",
        "Dustin Herbison",
        "Elisa Bandy",
        "Emma Wang",
        "Eric Noland",
        "Erica Moreira",
        "Evan Senter",
        "Evgenii Eltyshev",
        "Francesco Visin",
        "Gabriel Rasskin",
        "Gary Wei",
        "Glenn Cameron",
        "Gus Martins",
        "Hadi Hashemi",
        "Hanna Klimczak-Plucińska",
        "Harleen Batra",
        "Harsh Dhand",
        "Ivan Nardini",
        "Jacinda Mein",
        "Jack Zhou",
        "James Svensson",
        "Jeff Stanway",
        "Jetha Chan",
        "Jin Peng Zhou",
        "Joana Carrasqueira",
        "Joana Iljazi",
        "Jocelyn Becker",
        "Joe Fernandez",
        "Joost van Amersfoort",
        "Josh Gordon",
        "Josh Lipschultz",
        "Josh Newlan",
        "Ju-yeong Ji",
        "Kareem Mohamed",
        "Kartikeya Badola",
        "Kat Black",
        "Katie Millican",
        "Keelin McDonell",
        "Kelvin Nguyen",
        "Kiranbir Sodhia",
        "Kish Greene",
        "Lars Lowe Sjoesund",
        "Lauren Usui",
        "Laurent Sifre",
        "Lena Heuermann",
        "Leticia Lago",
        "Lilly McNealus",
        "Livio Baldini Soares",
        "Logan Kilpatrick",
        "Lucas Dixon",
        "Luciano Martins",
        "Machel Reid",
        "Manvinder Singh",
        "Mark Iverson",
        "Martin Görner",
        "Mat Velloso",
        "Mateo Wirth",
        "Matt Davidow",
        "Matt Miller",
        "Matthew Rahtz",
        "Matthew Watson",
        "Meg Risdal",
        "Mehran Kazemi",
        "Michael Moynihan",
        "Ming Zhang",
        "Minsuk Kahng",
        "Minwoo Park",
        "Mofi Rahman",
        "Mohit Khatwani",
        "Natalie Dao",
        "Nenshad Bardoliwalla",
        "Nesh Devanathan",
        "Neta Dumai",
        "Nilay Chauhan",
        "Oscar Wahltinez",
        "Pankil Botarda",
        "Parker Barnes",
        "Paul Barham",
        "Paul Michel",
        "Pengchong Jin",
        "Petko Georgiev",
        "Phil Culliton",
        "Pradeep Kuppala",
        "Ramona Comanescu",
        "Ramona Merhej",
        "Reena Jana",
        "Reza Ardeshir Rokni",
        "Rishabh Agarwal",
        "Ryan Mullins",
        "Samaneh Saadat",
        "Sara Mc Carthy",
        "Sarah Cogan",
        "Sarah Perrin",
        "Sébastien M. R. Arnold",
        "Sebastian Krause",
        "Shengyang Dai",
        "Shruti Garg",
        "Shruti Sheth",
        "Sue Ronstrom",
        "Susan Chan",
        "Timothy Jordan",
        "Ting Yu",
        "Tom Eccles",
        "Tom Hennigan",
        "Tomas Kocisky",
        "Tulsee Doshi",
        "Vihan Jain",
        "Vikas Yadav",
        "Vilobh Meshram",
        "Vishal Dharmadhikari",
        "Warren Barkley",
        "Wei Wei",
        "Wenming Ye",
        "Woohyun Han",
        "Woosuk Kwon",
        "Xiang Xu",
        "Zhe Shen",
        "Zhitao Gong",
        "Zichuan Wei",
        "Victor Cotruta",
        "Phoebe Kirk",
        "Anand Rao",
        "Minh Giang",
        "Ludovic Peran",
        "Tris Warkentin",
        "Eli Collins",
        "Joelle Barral",
        "Zoubin Ghahramani",
        "Raia Hadsell",
        "D. Sculley",
        "Jeanine Banks",
        "Anca Dragan",
        "Slav Petrov",
        "Oriol Vinyals",
        "Jeff Dean",
        "Demis Hassabis",
        "Koray Kavukcuoglu",
        "Clement Farabet",
        "Elena Buchatskaya",
        "Sebastian Borgeaud",
        "Noah Fiedel",
        "Armand Joulin",
        "Kathleen Kenealy",
        "Robert Dadashi",
        "Alek Andreev"
      ],
      "abstract": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
      "tldr_zh": "本研究介绍了 Gemma 2，这是一个轻量级开源语言模型系列，参数规模从2亿到27亿不等，旨在提升实用大小模型的性能。研究团队对 Transformer 架构进行了改进，包括应用交错本地-全局注意力(interleaving local-global attentions)和组查询注意力(group-query attention)。此外，2B 和 9B 模型采用了知识蒸馏(knowledge distillation)训练方法，而不是传统的下一个标记预测，从而使这些模型在同等规模下表现出最佳效果，甚至可与2-3倍大的模型竞争。所有模型均已向社区发布。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00118v3",
      "published_date": "2024-07-31 19:13:07 UTC",
      "updated_date": "2024-10-02 15:22:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:34:25.784853"
    },
    {
      "arxiv_id": "2408.00114v2",
      "title": "Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs",
      "title_zh": "归纳还是演绎？ 重新思考大型语言模型的基本推理能力",
      "authors": [
        "Kewei Cheng",
        "Jingfeng Yang",
        "Haoming Jiang",
        "Zhengyang Wang",
        "Binxuan Huang",
        "Ruirui Li",
        "Shiyang Li",
        "Zheng Li",
        "Yifan Gao",
        "Xian Li",
        "Bing Yin",
        "Yizhou Sun"
      ],
      "abstract": "Reasoning encompasses two typical types: deductive reasoning and inductive\nreasoning. Despite extensive research into the reasoning capabilities of Large\nLanguage Models (LLMs), most studies have failed to rigorously differentiate\nbetween inductive and deductive reasoning, leading to a blending of the two.\nThis raises an essential question: In LLM reasoning, which poses a greater\nchallenge - deductive or inductive reasoning? While the deductive reasoning\ncapabilities of LLMs, (i.e. their capacity to follow instructions in reasoning\ntasks), have received considerable attention, their abilities in true inductive\nreasoning remain largely unexplored. To investigate into the true inductive\nreasoning capabilities of LLMs, we propose a novel framework, SolverLearner.\nThis framework enables LLMs to learn the underlying function (i.e., $y =\nf_w(x)$), that maps input data points $(x)$ to their corresponding output\nvalues $(y)$, using only in-context examples. By focusing on inductive\nreasoning and separating it from LLM-based deductive reasoning, we can isolate\nand investigate inductive reasoning of LLMs in its pure form via SolverLearner.\nOur observations reveal that LLMs demonstrate remarkable inductive reasoning\ncapabilities through SolverLearner, achieving near-perfect performance with ACC\nof 1 in most cases. Surprisingly, despite their strong inductive reasoning\nabilities, LLMs tend to relatively lack deductive reasoning capabilities,\nparticularly in tasks involving ``counterfactual'' reasoning.",
      "tldr_zh": "该论文重新审视了大型语言模型（LLMs）的基本推理能力，探讨了归纳推理（Inductive reasoning）和演绎推理（Deductive reasoning）哪个更具挑战性。作者提出了一种新框架SolverLearner，通过in-context examples让LLMs学习底层函数（y = f_w(x)），以隔离并测试纯归纳推理能力。实验结果显示，LLMs在归纳推理上表现出色，准确率（ACC）接近完美，但特别是在“counterfactual”推理任务中，演绎推理能力相对较弱。该研究强调了区分两种推理类型的重要性，为未来LLMs的改进提供了新见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00114v2",
      "published_date": "2024-07-31 18:47:11 UTC",
      "updated_date": "2024-08-07 00:52:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:34:37.086699"
    },
    {
      "arxiv_id": "2408.00113v2",
      "title": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models",
      "title_zh": "通过棋盘游戏模型测量语言模型可解释性中字典学习的进展",
      "authors": [
        "Adam Karvonen",
        "Benjamin Wright",
        "Can Rager",
        "Rico Angell",
        "Jannik Brinkmann",
        "Logan Smith",
        "Claudio Mayrink Verdun",
        "David Bau",
        "Samuel Marks"
      ],
      "abstract": "What latent features are encoded in language model (LM) representations?\nRecent work on training sparse autoencoders (SAEs) to disentangle interpretable\nfeatures in LM representations has shown significant promise. However,\nevaluating the quality of these SAEs is difficult because we lack a\nground-truth collection of interpretable features that we expect good SAEs to\nrecover. We thus propose to measure progress in interpretable dictionary\nlearning by working in the setting of LMs trained on chess and Othello\ntranscripts. These settings carry natural collections of interpretable features\n-- for example, \"there is a knight on F3\" -- which we leverage into\n$\\textit{supervised}$ metrics for SAE quality. To guide progress in\ninterpretable dictionary learning, we introduce a new SAE training technique,\n$\\textit{p-annealing}$, which improves performance on prior unsupervised\nmetrics as well as our new metrics.",
      "tldr_zh": "本研究探讨了语言模型 (LM) 表示中的潜在特征可解释性问题，通过训练稀疏自动编码器 (SAEs) 来解缠这些特征。作者提出使用棋类游戏模型（如国际象棋和 Othello）作为基准，提供监督指标，例如“there is a knight on F3”，以评估 SAEs 质量。研究引入了新的训练技术 p-annealing，能够提升 SAEs 在既有无监督指标和新指标上的性能，从而推动可解释字典学习 (dictionary learning) 的进展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as an oral paper (top 5%) at the ICML 2024 Mechanistic\n  Interpretability Workshop and to the NeurIPS 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2408.00113v2",
      "published_date": "2024-07-31 18:45:13 UTC",
      "updated_date": "2024-10-30 14:21:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:34:48.359801"
    },
    {
      "arxiv_id": "2408.00108v2",
      "title": "Preference-Based Abstract Argumentation for Case-Based Reasoning (with Appendix)",
      "title_zh": "基于偏好的抽象论证用于",
      "authors": [
        "Adam Gould",
        "Guilherme Paulino-Passos",
        "Seema Dadhania",
        "Matthew Williams",
        "Francesca Toni"
      ],
      "abstract": "In the pursuit of enhancing the efficacy and flexibility of interpretable,\ndata-driven classification models, this work introduces a novel incorporation\nof user-defined preferences with Abstract Argumentation and Case-Based\nReasoning (CBR). Specifically, we introduce Preference-Based Abstract\nArgumentation for Case-Based Reasoning (which we call AA-CBR-P), allowing users\nto define multiple approaches to compare cases with an ordering that specifies\ntheir preference over these comparison approaches. We prove that the model\ninherently follows these preferences when making predictions and show that\nprevious abstract argumentation for case-based reasoning approaches are\ninsufficient at expressing preferences over constituents of an argument. We\nthen demonstrate how this can be applied to a real-world medical dataset\nsourced from a clinical trial evaluating differing assessment methods of\npatients with a primary brain tumour. We show empirically that our approach\noutperforms other interpretable machine learning models on this dataset.",
      "tldr_zh": "这篇论文提出了一种名为 AA-CBR-P 的新模型，将用户定义的偏好整合到 Abstract Argumentation 和 Case-Based Reasoning (CBR) 中，以提升可解释的数据驱动分类模型的效能。模型允许用户定义多种案例比较方法并指定偏好顺序，确保预测过程优先遵循这些偏好，并证明了其优于现有 Abstract Argumentation 方法在表达论证组成部分偏好方面的不足。在一个真实医疗数据集（脑肿瘤患者评估临床试验）上，实验结果显示 AA-CBR-P 超过了其他可解释机器学习模型的表现。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for KR2024. Includes Appendix",
      "pdf_url": "http://arxiv.org/pdf/2408.00108v2",
      "published_date": "2024-07-31 18:31:04 UTC",
      "updated_date": "2024-08-03 08:51:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:35:01.441634"
    },
    {
      "arxiv_id": "2408.00106v1",
      "title": "WAS: Dataset and Methods for Artistic Text Segmentation",
      "title_zh": "WAS：艺术文本分割的数据集和方法",
      "authors": [
        "Xudong Xie",
        "Yuzhe Li",
        "Yang Liu",
        "Zhifei Zhang",
        "Zhaowen Wang",
        "Wei Xiong",
        "Xiang Bai"
      ],
      "abstract": "Accurate text segmentation results are crucial for text-related generative\ntasks, such as text image generation, text editing, text removal, and text\nstyle transfer. Recently, some scene text segmentation methods have made\nsignificant progress in segmenting regular text. However, these methods perform\npoorly in scenarios containing artistic text. Therefore, this paper focuses on\nthe more challenging task of artistic text segmentation and constructs a real\nartistic text segmentation dataset. One challenge of the task is that the local\nstroke shapes of artistic text are changeable with diversity and complexity. We\npropose a decoder with the layer-wise momentum query to prevent the model from\nignoring stroke regions of special shapes. Another challenge is the complexity\nof the global topological structure. We further design a skeleton-assisted head\nto guide the model to focus on the global structure. Additionally, to enhance\nthe generalization performance of the text segmentation model, we propose a\nstrategy for training data synthesis, based on the large multi-modal model and\nthe diffusion model. Experimental results show that our proposed method and\nsynthetic dataset can significantly enhance the performance of artistic text\nsegmentation and achieve state-of-the-art results on other public datasets.",
      "tldr_zh": "这篇论文针对艺术文本分割任务，构建了一个真实数据集，以解决现有方法在处理多样复杂艺术文本时的不足，该任务对文本相关生成应用（如文本图像生成和风格转移）至关重要。论文提出了一种解码器，使用 layer-wise momentum query 来防止模型忽略特殊形状的局部笔画区域，并设计了 skeleton-assisted head 来引导模型关注全局拓扑结构。针对模型泛化性能，论文还引入了一种基于 large multi-modal model 和 diffusion model 的训练数据合成策略。实验结果表明，该方法和数据集显著提升了艺术文本分割的性能，并在其他公共数据集上达到了 state-of-the-art 水平。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00106v1",
      "published_date": "2024-07-31 18:29:36 UTC",
      "updated_date": "2024-07-31 18:29:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:35:25.413025"
    },
    {
      "arxiv_id": "2408.00103v3",
      "title": "ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget",
      "title_zh": "翻译失败",
      "authors": [
        "Riccardo Orlando",
        "Pere-Lluis Huguet Cabot",
        "Edoardo Barba",
        "Roberto Navigli"
      ],
      "abstract": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in\nNatural Language Processing, serving as critical components in a wide range of\napplications. In this paper, we propose ReLiK, a Retriever-Reader architecture\nfor both EL and RE, where, given an input text, the Retriever module undertakes\nthe identification of candidate entities or relations that could potentially\nappear within the text. Subsequently, the Reader module is tasked to discern\nthe pertinent retrieved entities or relations and establish their alignment\nwith the corresponding textual spans. Notably, we put forward an innovative\ninput representation that incorporates the candidate entities or relations\nalongside the text, making it possible to link entities or extract relations in\na single forward pass and to fully leverage pre-trained language models\ncontextualization capabilities, in contrast with previous\nRetriever-Reader-based methods, which require a forward pass for each\ncandidate. Our formulation of EL and RE achieves state-of-the-art performance\nin both in-domain and out-of-domain benchmarks while using academic budget\ntraining and with up to 40x inference speed compared to competitors. Finally,\nwe show how our architecture can be used seamlessly for Information Extraction\n(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared\nReader that simultaneously extracts entities and relations.",
      "tldr_zh": "本研究提出 ReLiK，一种高效的 Retriever-Reader 架构，用于 Entity Linking (EL) 和 Relation Extraction (RE)，通过 Retriever 模块识别候选实体或关系，Reader 模块则辨别并对齐相关文本片段。创新之处在于一种新的输入表示法，将候选实体或关系与文本整合，实现单次前向传递，充分利用预训练语言模型的上下文能力，从而比传统方法节省计算资源。实验结果显示，ReLiK 在 in-domain 和 out-of-domain 基准上达到 state-of-the-art 性能，同时采用学术预算训练，并比竞争对手快 40 倍。最后，该架构可无缝应用于 Information Extraction (cIE)，通过共享 Reader 同时提取实体和关系，设置新 state-of-the-art 标准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Findings of the Association for Computational Linguistics ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00103v3",
      "published_date": "2024-07-31 18:25:49 UTC",
      "updated_date": "2025-05-09 09:02:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:35:26.538543"
    },
    {
      "arxiv_id": "2408.00096v1",
      "title": "From Attributes to Natural Language: A Survey and Foresight on Text-based Person Re-identification",
      "title_zh": "翻译失败",
      "authors": [
        "Fanzhi Jiang",
        "Su Yang",
        "Mark W. Jones",
        "Liumei Zhang"
      ],
      "abstract": "Text-based person re-identification (Re-ID) is a challenging topic in the\nfield of complex multimodal analysis, its ultimate aim is to recognize specific\npedestrians by scrutinizing attributes/natural language descriptions. Despite\nthe wide range of applicable areas such as security surveillance, video\nretrieval, person tracking, and social media analytics, there is a notable\nabsence of comprehensive reviews dedicated to summarizing the text-based person\nRe-ID from a technical perspective. To address this gap, we propose to\nintroduce a taxonomy spanning Evaluation, Strategy, Architecture, and\nOptimization dimensions, providing a comprehensive survey of the text-based\nperson Re-ID task. We start by laying the groundwork for text-based person\nRe-ID, elucidating fundamental concepts related to attribute/natural\nlanguage-based identification. Then a thorough examination of existing\nbenchmark datasets and metrics is presented. Subsequently, we further delve\ninto prevalent feature extraction strategies employed in text-based person\nRe-ID research, followed by a concise summary of common network architectures\nwithin the domain. Prevalent loss functions utilized for model optimization and\nmodality alignment in text-based person Re-ID are also scrutinized. To\nconclude, we offer a concise summary of our findings, pinpointing challenges in\ntext-based person Re-ID. In response to these challenges, we outline potential\navenues for future open-set text-based person Re-ID and present a baseline\narchitecture for text-based pedestrian image generation-guided\nre-identification(TBPGR).",
      "tldr_zh": "这篇论文对基于文本的人脸再识别（Text-based Person Re-ID）进行了全面调查和前瞻分析，旨在通过属性和自然语言描述识别行人。论文提出一个涵盖 Evaluation、Strategy、Architecture 和 Optimization 维度的分类法，系统总结了基础概念、基准数据集、特征提取策略、网络架构、损失函数以及模态对齐方法。最终，它指出了当前任务的挑战，并为未来研究提出潜在方向，如 open-set Text-based Person Re-ID 和 TBPGR 基线架构，以推动该领域的进展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00096v1",
      "published_date": "2024-07-31 18:16:18 UTC",
      "updated_date": "2024-07-31 18:16:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:35:37.714136"
    },
    {
      "arxiv_id": "2408.00090v2",
      "title": "Execution Semantics of Behavior Trees in Robotic Applications",
      "title_zh": "行为树在机器人应用中的执行语义",
      "authors": [
        "Enrico Ghiorzi",
        "Christian Henkel",
        "Matteo Palmas",
        "Michaela Klauck",
        "Armando Tacchella"
      ],
      "abstract": "Behavior Trees (BTs) have found a widespread adoption in robotics due to\nappealing features, their ease of use as a conceptual model of control policies\nand the availability of software tooling for BT-based design of control\nsoftware. However, BTs don't have formal execution semantics and, furthermore,\nsubtle differences among implementations can make the same model behave\ndifferently depending on the underlying software. This paper aims at defining\nthe execution semantics of behavior trees (BTs) as used in robotics\napplications. To this purpose, we present an abstract data type that formalizes\nthe structure and execution of BTs. While our formalization is inspired by\nexisting contributions in the scientific literature and state-of-the art\nimplementations, we strive to provide an unambiguous treatment of most features\nthat find incomplete or inconsistent treatment across other works.",
      "tldr_zh": "Behavior Trees (BTs) 在机器人应用中因其易用性和可用工具而广泛采用，但缺乏正式执行语义，导致不同实现之间可能出现行为差异。该论文通过定义 BTs 的执行语义，提出一个抽象数据类型（abstract data type）来形式化 BTs 的结构和执行过程，从而提供一个清晰且无歧义的处理框架。该方法基于现有文献和实现，旨在解决其他作品中不完整或不一致的问题，为机器人控制软件设计带来更可靠的基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "68T30",
        "I.2.4"
      ],
      "primary_category": "cs.RO",
      "comment": "25 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00090v2",
      "published_date": "2024-07-31 18:08:59 UTC",
      "updated_date": "2025-04-10 15:46:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:35:48.197529"
    },
    {
      "arxiv_id": "2407.21794v1",
      "title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey",
      "title_zh": "广义分布外检测及其扩展：在视觉语言模型时代的一项综述",
      "authors": [
        "Atsuyuki Miyai",
        "Jingkang Yang",
        "Jingyang Zhang",
        "Yifei Ming",
        "Yueqian Lin",
        "Qing Yu",
        "Go Irie",
        "Shafiq Joty",
        "Yixuan Li",
        "Hai Li",
        "Ziwei Liu",
        "Toshihiko Yamasaki",
        "Kiyoharu Aizawa"
      ],
      "abstract": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD\ndetection, and OD in the VLM era. Our framework reveals that, with some field\ninactivity and integration, the demanding challenges have become OOD detection\nand AD. In addition, we also highlight the significant shift in the definition,\nproblem settings, and benchmarks; we thus feature a comprehensive review of the\nmethodology for OOD detection, including the discussion over other related\ntasks to clarify their relationship to OOD detection. Finally, we explore the\nadvancements in the emerging Large Vision Language Model (LVLM) era, such as\nGPT-4V. We conclude this survey with open challenges and future directions.",
      "tldr_zh": "这篇调查论文探讨了在视觉语言模型(VLMs)时代，Out-of-Distribution Detection (OOD)检测的泛化框架及其相关领域，包括Anomaly Detection (AD)、Novelty Detection (ND)、Open Set Recognition (OSR)和Outlier Detection (OD)。作者提出了generalized OOD detection v2框架，统一这些领域的演变，并强调VLMs如CLIP模糊了传统边界，使OOD检测和AD成为主要挑战。论文全面回顾了OOD检测的方法论、问题设置和基准，并扩展到Large Vision Language Models (LVLMs)如GPT-4V的进展，最后指出了开放挑战和未来研究方向。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "survey paper. We welcome questions, issues, and paper requests via\n  https://github.com/AtsuMiyai/Awesome-OOD-VLM",
      "pdf_url": "http://arxiv.org/pdf/2407.21794v1",
      "published_date": "2024-07-31 17:59:58 UTC",
      "updated_date": "2024-07-31 17:59:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:36:03.547860"
    },
    {
      "arxiv_id": "2407.21792v3",
      "title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?",
      "title_zh": "Safetywashing：AI 安全基准是否真正衡量了安全进步？",
      "authors": [
        "Richard Ren",
        "Steven Basart",
        "Adam Khoja",
        "Alice Gatti",
        "Long Phan",
        "Xuwang Yin",
        "Mantas Mazeika",
        "Alexander Pan",
        "Gabriel Mukobi",
        "Ryan H. Kim",
        "Stephen Fitz",
        "Dan Hendrycks"
      ],
      "abstract": "As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with both upstream model capabilities and training\ncompute, potentially enabling \"safetywashing\"--where capability improvements\nare misrepresented as safety advancements. Based on these findings, we propose\nan empirical foundation for developing more meaningful safety metrics and\ndefine AI safety in a machine learning research context as a set of clearly\ndelineated research goals that are empirically separable from generic\ncapabilities advancements. In doing so, we aim to provide a more rigorous\nframework for AI safety research, advancing the science of safety evaluations\nand clarifying the path towards measurable progress.",
      "tldr_zh": "这篇论文通过对AI safety benchmarks的元分析，调查了这些基准与模型一般能力（如知识和推理）的相关性，发现许多安全基准高度与上游能力及训练计算相关，可能导致\"safetywashing\"现象，即将能力提升误称为安全进步。研究者基于实证发现，提出更可靠的安全指标基础，并将AI safety定义为可从一般能力中分离的明确研究目标。最终，该框架旨在为AI安全研究提供更严格的评估标准，促进可衡量的进展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21792v3",
      "published_date": "2024-07-31 17:59:24 UTC",
      "updated_date": "2024-12-27 17:36:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:36:13.304283"
    },
    {
      "arxiv_id": "2407.21788v1",
      "title": "Vision-Language Model Based Handwriting Verification",
      "title_zh": "基于视觉语言模型的手写验证",
      "authors": [
        "Mihir Chauhan",
        "Abhishek Satbhai",
        "Mohammad Abuzar Hashemi",
        "Mir Basheer Ali",
        "Bina Ramamurthy",
        "Mingchen Gao",
        "Siwei Lyu",
        "Sargur Srihari"
      ],
      "abstract": "Handwriting Verification is a critical in document forensics. Deep learning\nbased approaches often face skepticism from forensic document examiners due to\ntheir lack of explainability and reliance on extensive training data and\nhandcrafted features. This paper explores using Vision Language Models (VLMs),\nsuch as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By\nleveraging their Visual Question Answering capabilities and 0-shot\nChain-of-Thought (CoT) reasoning, our goal is to provide clear,\nhuman-understandable explanations for model decisions. Our experiments on the\nCEDAR handwriting dataset demonstrate that VLMs offer enhanced\ninterpretability, reduce the need for large training datasets, and adapt better\nto diverse handwriting styles. However, results show that the CNN-based\nResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach\nwith GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:\n71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings\nhighlight the potential of VLMs in generating human-interpretable decisions\nwhile underscoring the need for further advancements to match the performance\nof specialized deep learning models.",
      "tldr_zh": "这篇论文探讨了使用视觉语言模型 (VLMs) 如 GPT-4o 和 PaliGemma 进行手写验证，以解决传统深度学习方法的可解释性不足和对大量训练数据的需求问题。研究通过视觉问答 (VQA) 和零样本 Chain-of-Thought (CoT) 推理，提供清晰的人类可理解解释，并在 CEDAR 数据集上展示了 VLMs 的适应多样手写风格的优势。实验结果表明，VLMs 的准确率 (GPT-4o: 70%, PaliGemma: 71%) 低于 CNN 模型 ResNet-18 (84%)，突显了 VLMs 在生成可解释决策的潜力，同时强调了进一步优化以匹配专业模型性能的必要性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "4 Pages, 1 Figure, 1 Table, Accepted as Short paper at Irish Machine\n  Vision and Image Processing (IMVIP) Conference",
      "pdf_url": "http://arxiv.org/pdf/2407.21788v1",
      "published_date": "2024-07-31 17:57:32 UTC",
      "updated_date": "2024-07-31 17:57:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:36:28.102044"
    },
    {
      "arxiv_id": "2407.21787v3",
      "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Bradley Brown",
        "Jordan Juravsky",
        "Ryan Ehrlich",
        "Ronald Clark",
        "Quoc V. Le",
        "Christopher Ré",
        "Azalia Mirhoseini"
      ],
      "abstract": "Scaling the amount of compute used to train language models has dramatically\nimproved their capabilities. However, when it comes to inference, we often\nlimit models to making only one attempt at a problem. Here, we explore\ninference compute as another axis for scaling, using the simple technique of\nrepeatedly sampling candidate solutions from a model. Across multiple tasks and\nmodels, we observe that coverage -- the fraction of problems that are solved by\nany generated sample -- scales with the number of samples over four orders of\nmagnitude. Interestingly, the relationship between coverage and the number of\nsamples is often log-linear and can be modelled with an exponentiated power\nlaw, suggesting the existence of inference-time scaling laws. In domains like\ncoding and formal proofs, where answers can be automatically verified, these\nincreases in coverage directly translate into improved performance. When we\napply repeated sampling to SWE-bench Lite, the fraction of issues solved with\nDeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250\nsamples, outperforming the single-sample state-of-the-art of 43%. In domains\nwithout automatic verifiers, we find that common methods for picking from a\nsample collection (majority voting and reward models) plateau beyond several\nhundred samples and fail to fully scale with the sample budget.",
      "tldr_zh": "该论文探讨了通过重复采样来扩展语言模型的推理计算（inference compute），旨在超越单次生成限制以提升模型性能。研究发现，覆盖率（coverage）与样本数量呈对数线性关系，并可使用指数幂定律建模，在编码和形式证明等可自动验证的领域，直接转化为显著性能提升。实验结果显示，在 SWE-bench Lite 上，使用 DeepSeek-Coder-V2-Instruct 的问题解决率从单样本的 15.9% 上升到 250 个样本时的 56%，超过了现有单样本最先进水平 43%。然而，在缺乏自动验证器的领域，多数投票和奖励模型等方法在数百样本后效果趋于平稳，限制了进一步扩展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21787v3",
      "published_date": "2024-07-31 17:57:25 UTC",
      "updated_date": "2024-12-30 19:03:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:36:39.057817"
    },
    {
      "arxiv_id": "2407.21783v3",
      "title": "The Llama 3 Herd of Models",
      "title_zh": "翻译失败",
      "authors": [
        "Aaron Grattafiori",
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "Alan Schelten",
        "Alex Vaughan",
        "Amy Yang",
        "Angela Fan",
        "Anirudh Goyal",
        "Anthony Hartshorn",
        "Aobo Yang",
        "Archi Mitra",
        "Archie Sravankumar",
        "Artem Korenev",
        "Arthur Hinsvark",
        "Arun Rao",
        "Aston Zhang",
        "Aurelien Rodriguez",
        "Austen Gregerson",
        "Ava Spataru",
        "Baptiste Roziere",
        "Bethany Biron",
        "Binh Tang",
        "Bobbie Chern",
        "Charlotte Caucheteux",
        "Chaya Nayak",
        "Chloe Bi",
        "Chris Marra",
        "Chris McConnell",
        "Christian Keller",
        "Christophe Touret",
        "Chunyang Wu",
        "Corinne Wong",
        "Cristian Canton Ferrer",
        "Cyrus Nikolaidis",
        "Damien Allonsius",
        "Daniel Song",
        "Danielle Pintz",
        "Danny Livshits",
        "Danny Wyatt",
        "David Esiobu",
        "Dhruv Choudhary",
        "Dhruv Mahajan",
        "Diego Garcia-Olano",
        "Diego Perino",
        "Dieuwke Hupkes",
        "Egor Lakomkin",
        "Ehab AlBadawy",
        "Elina Lobanova",
        "Emily Dinan",
        "Eric Michael Smith",
        "Filip Radenovic",
        "Francisco Guzmán",
        "Frank Zhang",
        "Gabriel Synnaeve",
        "Gabrielle Lee",
        "Georgia Lewis Anderson",
        "Govind Thattai",
        "Graeme Nail",
        "Gregoire Mialon",
        "Guan Pang",
        "Guillem Cucurell",
        "Hailey Nguyen",
        "Hannah Korevaar",
        "Hu Xu",
        "Hugo Touvron",
        "Iliyan Zarov",
        "Imanol Arrieta Ibarra",
        "Isabel Kloumann",
        "Ishan Misra",
        "Ivan Evtimov",
        "Jack Zhang",
        "Jade Copet",
        "Jaewon Lee",
        "Jan Geffert",
        "Jana Vranes",
        "Jason Park",
        "Jay Mahadeokar",
        "Jeet Shah",
        "Jelmer van der Linde",
        "Jennifer Billock",
        "Jenny Hong",
        "Jenya Lee",
        "Jeremy Fu",
        "Jianfeng Chi",
        "Jianyu Huang",
        "Jiawen Liu",
        "Jie Wang",
        "Jiecao Yu",
        "Joanna Bitton",
        "Joe Spisak",
        "Jongsoo Park",
        "Joseph Rocca",
        "Joshua Johnstun",
        "Joshua Saxe",
        "Junteng Jia",
        "Kalyan Vasuden Alwala",
        "Karthik Prasad",
        "Kartikeya Upasani",
        "Kate Plawiak",
        "Ke Li",
        "Kenneth Heafield",
        "Kevin Stone",
        "Khalid El-Arini",
        "Krithika Iyer",
        "Kshitiz Malik",
        "Kuenley Chiu",
        "Kunal Bhalla",
        "Kushal Lakhotia",
        "Lauren Rantala-Yeary",
        "Laurens van der Maaten",
        "Lawrence Chen",
        "Liang Tan",
        "Liz Jenkins",
        "Louis Martin",
        "Lovish Madaan",
        "Lubo Malo",
        "Lukas Blecher",
        "Lukas Landzaat",
        "Luke de Oliveira",
        "Madeline Muzzi",
        "Mahesh Pasupuleti",
        "Mannat Singh",
        "Manohar Paluri",
        "Marcin Kardas",
        "Maria Tsimpoukelli",
        "Mathew Oldham",
        "Mathieu Rita",
        "Maya Pavlova",
        "Melanie Kambadur",
        "Mike Lewis",
        "Min Si",
        "Mitesh Kumar Singh",
        "Mona Hassan",
        "Naman Goyal",
        "Narjes Torabi",
        "Nikolay Bashlykov",
        "Nikolay Bogoychev",
        "Niladri Chatterji",
        "Ning Zhang",
        "Olivier Duchenne",
        "Onur Çelebi",
        "Patrick Alrassy",
        "Pengchuan Zhang",
        "Pengwei Li",
        "Petar Vasic",
        "Peter Weng",
        "Prajjwal Bhargava",
        "Pratik Dubal",
        "Praveen Krishnan",
        "Punit Singh Koura",
        "Puxin Xu",
        "Qing He",
        "Qingxiao Dong",
        "Ragavan Srinivasan",
        "Raj Ganapathy",
        "Ramon Calderer",
        "Ricardo Silveira Cabral",
        "Robert Stojnic",
        "Roberta Raileanu",
        "Rohan Maheswari",
        "Rohit Girdhar",
        "Rohit Patel",
        "Romain Sauvestre",
        "Ronnie Polidoro",
        "Roshan Sumbaly",
        "Ross Taylor",
        "Ruan Silva",
        "Rui Hou",
        "Rui Wang",
        "Saghar Hosseini",
        "Sahana Chennabasappa",
        "Sanjay Singh",
        "Sean Bell",
        "Seohyun Sonia Kim",
        "Sergey Edunov",
        "Shaoliang Nie",
        "Sharan Narang",
        "Sharath Raparthy",
        "Sheng Shen",
        "Shengye Wan",
        "Shruti Bhosale",
        "Shun Zhang",
        "Simon Vandenhende",
        "Soumya Batra",
        "Spencer Whitman",
        "Sten Sootla",
        "Stephane Collot",
        "Suchin Gururangan",
        "Sydney Borodinsky",
        "Tamar Herman",
        "Tara Fowler",
        "Tarek Sheasha",
        "Thomas Georgiou",
        "Thomas Scialom",
        "Tobias Speckbacher",
        "Todor Mihaylov",
        "Tong Xiao",
        "Ujjwal Karn",
        "Vedanuj Goswami",
        "Vibhor Gupta",
        "Vignesh Ramanathan",
        "Viktor Kerkez",
        "Vincent Gonguet",
        "Virginie Do",
        "Vish Vogeti",
        "Vítor Albiero",
        "Vladan Petrovic",
        "Weiwei Chu",
        "Wenhan Xiong",
        "Wenyin Fu",
        "Whitney Meers",
        "Xavier Martinet",
        "Xiaodong Wang",
        "Xiaofang Wang",
        "Xiaoqing Ellen Tan",
        "Xide Xia",
        "Xinfeng Xie",
        "Xuchao Jia",
        "Xuewei Wang",
        "Yaelle Goldschlag",
        "Yashesh Gaur",
        "Yasmine Babaei",
        "Yi Wen",
        "Yiwen Song",
        "Yuchen Zhang",
        "Yue Li",
        "Yuning Mao",
        "Zacharie Delpierre Coudert",
        "Zheng Yan",
        "Zhengxing Chen",
        "Zoe Papakipos",
        "Aaditya Singh",
        "Aayushi Srivastava",
        "Abha Jain",
        "Adam Kelsey",
        "Adam Shajnfeld",
        "Adithya Gangidi",
        "Adolfo Victoria",
        "Ahuva Goldstand",
        "Ajay Menon",
        "Ajay Sharma",
        "Alex Boesenberg",
        "Alexei Baevski",
        "Allie Feinstein",
        "Amanda Kallet",
        "Amit Sangani",
        "Amos Teo",
        "Anam Yunus",
        "Andrei Lupu",
        "Andres Alvarado",
        "Andrew Caples",
        "Andrew Gu",
        "Andrew Ho",
        "Andrew Poulton",
        "Andrew Ryan",
        "Ankit Ramchandani",
        "Annie Dong",
        "Annie Franco",
        "Anuj Goyal",
        "Aparajita Saraf",
        "Arkabandhu Chowdhury",
        "Ashley Gabriel",
        "Ashwin Bharambe",
        "Assaf Eisenman",
        "Azadeh Yazdan",
        "Beau James",
        "Ben Maurer",
        "Benjamin Leonhardi",
        "Bernie Huang",
        "Beth Loyd",
        "Beto De Paola",
        "Bhargavi Paranjape",
        "Bing Liu",
        "Bo Wu",
        "Boyu Ni",
        "Braden Hancock",
        "Bram Wasti",
        "Brandon Spence",
        "Brani Stojkovic",
        "Brian Gamido",
        "Britt Montalvo",
        "Carl Parker",
        "Carly Burton",
        "Catalina Mejia",
        "Ce Liu",
        "Changhan Wang",
        "Changkyu Kim",
        "Chao Zhou",
        "Chester Hu",
        "Ching-Hsiang Chu",
        "Chris Cai",
        "Chris Tindal",
        "Christoph Feichtenhofer",
        "Cynthia Gao",
        "Damon Civin",
        "Dana Beaty",
        "Daniel Kreymer",
        "Daniel Li",
        "David Adkins",
        "David Xu",
        "Davide Testuggine",
        "Delia David",
        "Devi Parikh",
        "Diana Liskovich",
        "Didem Foss",
        "Dingkang Wang",
        "Duc Le",
        "Dustin Holland",
        "Edward Dowling",
        "Eissa Jamil",
        "Elaine Montgomery",
        "Eleonora Presani",
        "Emily Hahn",
        "Emily Wood",
        "Eric-Tuan Le",
        "Erik Brinkman",
        "Esteban Arcaute",
        "Evan Dunbar",
        "Evan Smothers",
        "Fei Sun",
        "Felix Kreuk",
        "Feng Tian",
        "Filippos Kokkinos",
        "Firat Ozgenel",
        "Francesco Caggioni",
        "Frank Kanayet",
        "Frank Seide",
        "Gabriela Medina Florez",
        "Gabriella Schwarz",
        "Gada Badeer",
        "Georgia Swee",
        "Gil Halpern",
        "Grant Herman",
        "Grigory Sizov",
        "Guangyi",
        "Zhang",
        "Guna Lakshminarayanan",
        "Hakan Inan",
        "Hamid Shojanazeri",
        "Han Zou",
        "Hannah Wang",
        "Hanwen Zha",
        "Haroun Habeeb",
        "Harrison Rudolph",
        "Helen Suk",
        "Henry Aspegren",
        "Hunter Goldman",
        "Hongyuan Zhan",
        "Ibrahim Damlaj",
        "Igor Molybog",
        "Igor Tufanov",
        "Ilias Leontiadis",
        "Irina-Elena Veliche",
        "Itai Gat",
        "Jake Weissman",
        "James Geboski",
        "James Kohli",
        "Janice Lam",
        "Japhet Asher",
        "Jean-Baptiste Gaya",
        "Jeff Marcus",
        "Jeff Tang",
        "Jennifer Chan",
        "Jenny Zhen",
        "Jeremy Reizenstein",
        "Jeremy Teboul",
        "Jessica Zhong",
        "Jian Jin",
        "Jingyi Yang",
        "Joe Cummings",
        "Jon Carvill",
        "Jon Shepard",
        "Jonathan McPhie",
        "Jonathan Torres",
        "Josh Ginsburg",
        "Junjie Wang",
        "Kai Wu",
        "Kam Hou U",
        "Karan Saxena",
        "Kartikay Khandelwal",
        "Katayoun Zand",
        "Kathy Matosich",
        "Kaushik Veeraraghavan",
        "Kelly Michelena",
        "Keqian Li",
        "Kiran Jagadeesh",
        "Kun Huang",
        "Kunal Chawla",
        "Kyle Huang",
        "Lailin Chen",
        "Lakshya Garg",
        "Lavender A",
        "Leandro Silva",
        "Lee Bell",
        "Lei Zhang",
        "Liangpeng Guo",
        "Licheng Yu",
        "Liron Moshkovich",
        "Luca Wehrstedt",
        "Madian Khabsa",
        "Manav Avalani",
        "Manish Bhatt",
        "Martynas Mankus",
        "Matan Hasson",
        "Matthew Lennie",
        "Matthias Reso",
        "Maxim Groshev",
        "Maxim Naumov",
        "Maya Lathi",
        "Meghan Keneally",
        "Miao Liu",
        "Michael L. Seltzer",
        "Michal Valko",
        "Michelle Restrepo",
        "Mihir Patel",
        "Mik Vyatskov",
        "Mikayel Samvelyan",
        "Mike Clark",
        "Mike Macey",
        "Mike Wang",
        "Miquel Jubert Hermoso",
        "Mo Metanat",
        "Mohammad Rastegari",
        "Munish Bansal",
        "Nandhini Santhanam",
        "Natascha Parks",
        "Natasha White",
        "Navyata Bawa",
        "Nayan Singhal",
        "Nick Egebo",
        "Nicolas Usunier",
        "Nikhil Mehta",
        "Nikolay Pavlovich Laptev",
        "Ning Dong",
        "Norman Cheng",
        "Oleg Chernoguz",
        "Olivia Hart",
        "Omkar Salpekar",
        "Ozlem Kalinli",
        "Parkin Kent",
        "Parth Parekh",
        "Paul Saab",
        "Pavan Balaji",
        "Pedro Rittner",
        "Philip Bontrager",
        "Pierre Roux",
        "Piotr Dollar",
        "Polina Zvyagina",
        "Prashant Ratanchandani",
        "Pritish Yuvraj",
        "Qian Liang",
        "Rachad Alao",
        "Rachel Rodriguez",
        "Rafi Ayub",
        "Raghotham Murthy",
        "Raghu Nayani",
        "Rahul Mitra",
        "Rangaprabhu Parthasarathy",
        "Raymond Li",
        "Rebekkah Hogan",
        "Robin Battey",
        "Rocky Wang",
        "Russ Howes",
        "Ruty Rinott",
        "Sachin Mehta",
        "Sachin Siby",
        "Sai Jayesh Bondu",
        "Samyak Datta",
        "Sara Chugh",
        "Sara Hunt",
        "Sargun Dhillon",
        "Sasha Sidorov",
        "Satadru Pan",
        "Saurabh Mahajan",
        "Saurabh Verma",
        "Seiji Yamamoto",
        "Sharadh Ramaswamy",
        "Shaun Lindsay",
        "Shaun Lindsay",
        "Sheng Feng",
        "Shenghao Lin",
        "Shengxin Cindy Zha",
        "Shishir Patil",
        "Shiva Shankar",
        "Shuqiang Zhang",
        "Shuqiang Zhang",
        "Sinong Wang",
        "Sneha Agarwal",
        "Soji Sajuyigbe",
        "Soumith Chintala",
        "Stephanie Max",
        "Stephen Chen",
        "Steve Kehoe",
        "Steve Satterfield",
        "Sudarshan Govindaprasad",
        "Sumit Gupta",
        "Summer Deng",
        "Sungmin Cho",
        "Sunny Virk",
        "Suraj Subramanian",
        "Sy Choudhury",
        "Sydney Goldman",
        "Tal Remez",
        "Tamar Glaser",
        "Tamara Best",
        "Thilo Koehler",
        "Thomas Robinson",
        "Tianhe Li",
        "Tianjun Zhang",
        "Tim Matthews",
        "Timothy Chou",
        "Tzook Shaked",
        "Varun Vontimitta",
        "Victoria Ajayi",
        "Victoria Montanez",
        "Vijai Mohan",
        "Vinay Satish Kumar",
        "Vishal Mangla",
        "Vlad Ionescu",
        "Vlad Poenaru",
        "Vlad Tiberiu Mihailescu",
        "Vladimir Ivanov",
        "Wei Li",
        "Wenchen Wang",
        "Wenwen Jiang",
        "Wes Bouaziz",
        "Will Constable",
        "Xiaocheng Tang",
        "Xiaojian Wu",
        "Xiaolan Wang",
        "Xilun Wu",
        "Xinbo Gao",
        "Yaniv Kleinman",
        "Yanjun Chen",
        "Ye Hu",
        "Ye Jia",
        "Ye Qi",
        "Yenda Li",
        "Yilin Zhang",
        "Ying Zhang",
        "Yossi Adi",
        "Youngjin Nam",
        "Yu",
        "Wang",
        "Yu Zhao",
        "Yuchen Hao",
        "Yundi Qian",
        "Yunlu Li",
        "Yuzi He",
        "Zach Rait",
        "Zachary DeVito",
        "Zef Rosnbrick",
        "Zhaoduo Wen",
        "Zhenyu Yang",
        "Zhiwei Zhao",
        "Zhiyu Ma"
      ],
      "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
      "tldr_zh": "本论文介绍了 Llama 3 模型集，这是一个支持多语言、编码、推理和工具使用的语言模型系列，其中最大的模型是 405B 参数的密集 Transformer，具有高达 128K tokens 的上下文窗口。研究通过广泛的实证评估发现，Llama 3 在多种任务上与领先模型如 GPT-4 表现相当，并公开发布了其预训练和后训练版本以及 Llama Guard 3 用于输入输出安全。论文还探索了通过组合方法将图像、视频和语音能力整合到 Llama 3 中，结果显示其在相关识别任务上与最先进模型竞争，但这些多模态模型仍在开发中，未予广泛发布。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21783v3",
      "published_date": "2024-07-31 17:54:27 UTC",
      "updated_date": "2024-11-23 23:27:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:36:49.280829"
    },
    {
      "arxiv_id": "2407.21778v1",
      "title": "Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries",
      "title_zh": "Tulip Agent -- 使基于LLM的代理能够使用大型工具库解决任务",
      "authors": [
        "Felix Ocker",
        "Daniel Tanneberg",
        "Julian Eggert",
        "Michael Gienger"
      ],
      "abstract": "We introduce tulip agent, an architecture for autonomous LLM-based agents\nwith Create, Read, Update, and Delete access to a tool library containing a\npotentially large number of tools. In contrast to state-of-the-art\nimplementations, tulip agent does not encode the descriptions of all available\ntools in the system prompt, which counts against the model's context window, or\nembed the entire prompt for retrieving suitable tools. Instead, the tulip agent\ncan recursively search for suitable tools in its extensible tool library,\nimplemented exemplarily as a vector store. The tulip agent architecture\nsignificantly reduces inference costs, allows using even large tool libraries,\nand enables the agent to adapt and extend its set of tools. We evaluate the\narchitecture with several ablation studies in a mathematics context and\ndemonstrate its generalizability with an application to robotics. A reference\nimplementation and the benchmark are available at\ngithub.com/HRI-EU/tulip_agent.",
      "tldr_zh": "这篇论文引入了Tulip Agent，一种基于LLM的自主代理架构，允许代理通过Create, Read, Update, and Delete (CRUD) 操作管理大型工具库，从而解决任务时避免将所有工具描述放入系统提示中占用上下文窗口。不同于现有方法，Tulip Agent采用递归搜索在可扩展的工具库（如vector store）中查找合适工具，这显著降低了推理成本并增强了代理的适应性和扩展性。实验通过数学任务的消融研究和机器人应用的泛化性评估，证明了该架构的有效性。参考实现和基准可从GitHub获取。",
      "categories": [
        "cs.AI",
        "cs.RO",
        "H.3.3; I.2.6; I.2.8; I.2.9"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.21778v1",
      "published_date": "2024-07-31 17:50:54 UTC",
      "updated_date": "2024-07-31 17:50:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:37:04.307133"
    },
    {
      "arxiv_id": "2407.21770v3",
      "title": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Xi Victoria Lin",
        "Akshat Shrivastava",
        "Liang Luo",
        "Srinivasan Iyer",
        "Mike Lewis",
        "Gargi Ghosh",
        "Luke Zettlemoyer",
        "Armen Aghajanyan"
      ],
      "abstract": "We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems.",
      "tldr_zh": "该研究提出 MoMa，一种模态感知混合专家 (Mixture of Modality-Aware Experts, MoMa) 架构，用于预训练混合模态、早期融合 (early-fusion) 语言模型，通过将专家模块分为特定模态组（如文本和图像），并使用学习路由处理相应标记，实现语义适应性。相比标准专家选择 MoE，MoMa 在 1 万亿标记训练预算下，使 1.4B 模型的整体 FLOPs 节省达到 3.7x（文本 2.6x，图像 5.2x）。实验结果显示，MoMa 优于计算等效密集基线，并在与混合深度 (mixture-of-depths, MoD) 结合时进一步提升至 4.2x FLOPs 节省，但这会降低因果推理性能。总体上，MoMa 为高效的多模态 AI 系统预训练提供了新路径。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "v2 -> update related work section v3 -> fix spelling",
      "pdf_url": "http://arxiv.org/pdf/2407.21770v3",
      "published_date": "2024-07-31 17:46:51 UTC",
      "updated_date": "2024-08-12 16:20:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:37:16.791878"
    },
    {
      "arxiv_id": "2407.21753v2",
      "title": "Characterizing User Archetypes and Discussions on Scored.co",
      "title_zh": "翻译失败",
      "authors": [
        "Andrea Failla",
        "Salvatore Citraro",
        "Giulio Rossetti",
        "Francesco Cauteruccio"
      ],
      "abstract": "In recent years, the proliferation of social platforms has drastically\ntransformed the way individuals interact, organize, and share information. In\nthis scenario, we experience an unprecedented increase in the scale and\ncomplexity of interactions and, at the same time, little to no research about\nsome fringe social platforms. In this paper, we present a multi-dimensional\nframework for characterizing nodes and hyperedges in social hypernetworks, with\na focus on the understudied alt-right platform Scored.co. Our approach\nintegrates the possibility of studying higher-order interactions, thanks to the\nhypernetwork representation, and various node features such as user activity,\nsentiment, and toxicity, with the aim to define distinct user archetypes and\nunderstand their roles within the network. Utilizing a comprehensive dataset\nfrom Scored.co, we analyze the dynamics of these archetypes over time and\nexplore their interactions and influence within the community. The framework's\nversatility allows for detailed analysis of both individual user behaviors and\nbroader social structures. Our findings highlight the importance of\nhigher-order interactions in understanding social dynamics, offering new\ninsights into the roles and behaviors that emerge in complex online\nenvironments.",
      "tldr_zh": "该研究探讨了社交平台的兴起及其互动复杂性，针对 alt-right 平台 Scored.co 提出一个多维框架，用于表征社交 hypernetworks 中的节点和 hyperedges。框架整合了 higher-order interactions、用户活动、sentiment 和 toxicity 等特征，以定义 distinct user archetypes 并分析其在网络中的角色和动态。利用 Scored.co 的数据集，研究揭示了这些 archetypes 的时间演变及其互动影响，并强调 higher-order interactions 在理解复杂在线社会动态中的关键作用。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21753v2",
      "published_date": "2024-07-31 17:18:25 UTC",
      "updated_date": "2024-11-22 16:39:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:37:24.938983"
    },
    {
      "arxiv_id": "2407.21742v1",
      "title": "HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Junwei He",
        "Qianqian Xu",
        "Yangbangyan Jiang",
        "Zitai Wang",
        "Yuchen Sun",
        "Qingming Huang"
      ],
      "abstract": "With the progressive advancements in deep graph learning, out-of-distribution\n(OOD) detection for graph data has emerged as a critical challenge. While the\nefficacy of auxiliary datasets in enhancing OOD detection has been extensively\nstudied for image and text data, such approaches have not yet been explored for\ngraph data. Unlike Euclidean data, graph data exhibits greater diversity but\nlower robustness to perturbations, complicating the integration of outliers. To\ntackle these challenges, we propose the introduction of \\textbf{H}ybrid\nExternal and Internal \\textbf{G}raph \\textbf{O}utlier \\textbf{E}xposure (HGOE)\nto improve graph OOD detection performance. Our framework involves using\nrealistic external graph data from various domains and synthesizing internal\noutliers within ID subgroups to address the poor robustness and presence of OOD\nsamples within the ID class. Furthermore, we develop a boundary-aware OE loss\nthat adaptively assigns weights to outliers, maximizing the use of high-quality\nOOD samples while minimizing the impact of low-quality ones. Our proposed HGOE\nframework is model-agnostic and designed to enhance the effectiveness of\nexisting graph OOD detection models. Experimental results demonstrate that our\nHGOE framework can significantly improve the performance of existing OOD\ndetection models across all 8 real datasets.",
      "tldr_zh": "该论文针对图数据的异常分布（OOD）检测问题，提出了一种混合外部和内部图异常暴露框架（HGOE），通过整合来自不同领域的真实外部图数据和在 ID 子组中合成的内部异常，来解决图数据的鲁棒性差和 OOD 样本存在等问题。HGOE 框架还开发了 boundary-aware OE loss 函数，能够自适应地为异常样本分配权重，从而最大化高质 OOD 样本的利用并减少低质样本的影响。该框架是模型无关的，能够增强现有图 OOD 检测模型的效能；实验结果显示，在 8 个真实数据集上，HGOE 显著提升了这些模型的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Proceedings of the 32nd ACM International Conference on Multimedia",
      "pdf_url": "http://arxiv.org/pdf/2407.21742v1",
      "published_date": "2024-07-31 16:55:18 UTC",
      "updated_date": "2024-07-31 16:55:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:37:39.274002"
    },
    {
      "arxiv_id": "2407.21740v2",
      "title": "Contrastive Factor Analysis",
      "title_zh": "对比因子分析",
      "authors": [
        "Zhibin Duan",
        "Tiansheng Wen",
        "Yifei Wang",
        "Chen Zhu",
        "Bo Chen",
        "Mingyuan Zhou"
      ],
      "abstract": "Factor analysis, often regarded as a Bayesian variant of matrix\nfactorization, offers superior capabilities in capturing uncertainty, modeling\ncomplex dependencies, and ensuring robustness. As the deep learning era\narrives, factor analysis is receiving less and less attention due to their\nlimited expressive ability. On the contrary, contrastive learning has emerged\nas a potent technique with demonstrated efficacy in unsupervised\nrepresentational learning. While the two methods are different paradigms,\nrecent theoretical analysis has revealed the mathematical equivalence between\ncontrastive learning and matrix factorization, providing a potential\npossibility for factor analysis combined with contrastive learning. Motivated\nby the interconnectedness of contrastive learning, matrix factorization, and\nfactor analysis, this paper introduces a novel Contrastive Factor Analysis\nframework, aiming to leverage factor analysis's advantageous properties within\nthe realm of contrastive learning. To further leverage the interpretability\nproperties of non-negative factor analysis, which can learn disentangled\nrepresentations, contrastive factor analysis is extended to a non-negative\nversion. Finally, extensive experimental validation showcases the efficacy of\nthe proposed contrastive (non-negative) factor analysis methodology across\nmultiple key properties, including expressiveness, robustness,\ninterpretability, and accurate uncertainty estimation.",
      "tldr_zh": "本文提出 Contrastive Factor Analysis 框架，将因子分析（factor analysis）的优势（如捕捉不确定性、建模复杂依赖和鲁棒性）与对比学习（contrastive learning）相结合，基于二者与矩阵分解（matrix factorization）的数学等价性。  \n该框架旨在提升无监督表示学习的表现，并扩展到非负版本（non-negative factor analysis），以实现可解释性和学习分离表示（disentangled representations）。  \n实验验证显示，该方法在表达能力、鲁棒性、可解释性和不确定性估计等方面均表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21740v2",
      "published_date": "2024-07-31 16:52:00 UTC",
      "updated_date": "2024-08-01 03:16:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:37:50.550151"
    },
    {
      "arxiv_id": "2407.21739v1",
      "title": "A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Mothilal Asokan",
        "Joseph Geo Benjamin",
        "Mohammad Yaqub",
        "Karthik Nandakumar"
      ],
      "abstract": "Adapting foundation models for medical image analysis requires finetuning\nthem on a considerable amount of data because of extreme distribution shifts\nbetween natural (source) data used for pretraining and medical (target) data.\nHowever, collecting task-specific medical data for such finetuning at a central\nlocation raises many privacy concerns. Although Federated learning (FL)\nprovides an effective means for training on private decentralized data,\ncommunication costs in federating large foundation models can quickly become a\nsignificant bottleneck, impacting the solution's scalability. In this work, we\naddress this problem of efficient communication while ensuring effective\nlearning in FL by combining the strengths of Parameter-Efficient Fine-tuning\n(PEFT) with FL. Specifically, we study plug-and-play Low-Rank Adapters (LoRA)\nin a federated manner to adapt the Segment Anything Model (SAM) for 3D medical\nimage segmentation. Unlike prior works that utilize LoRA and finetune the\nentire decoder, we critically analyze the contribution of each granular\ncomponent of SAM on finetuning performance. Thus, we identify specific layers\nto be federated that are very efficient in terms of communication cost while\nproducing on-par accuracy. Our experiments show that retaining the parameters\nof the SAM model (including most of the decoder) in their original state during\nadaptation is beneficial because fine-tuning on small datasets tends to distort\nthe inherent capabilities of the underlying foundation model. On Fed-KiTS, our\napproach decreases communication cost (~48x) compared to full fine-tuning while\nincreasing performance (~6% Dice score) in 3D segmentation tasks. Our approach\nperforms similar to SAMed while achieving ~2.8x reduction in communication and\nparameters to be finetuned. We further validate our approach with experiments\non Fed-IXI and Prostate MRI datasets.",
      "tldr_zh": "该研究提出了一种结合 Federated Learning (FL) 和 Parameter-Efficient Fine-tuning (PEFT) 的方法，用于高效微调 Segment Anything Model (SAM) 以适应 3D 医疗图像分割任务，解决数据隐私和通信成本问题。作者通过分析 SAM 的各个组件，优先联邦微调 Low-Rank Adapters (LoRA) 和特定层，避免微调整个模型，从而显著降低通信开销。实验结果显示，在 Fed-KiTS 数据集上，该方法将通信成本减少约 48 倍，同时 Dice 分数提高约 6%，并在 Fed-IXI 和 Prostate MRI 数据集上验证了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21739v1",
      "published_date": "2024-07-31 16:48:06 UTC",
      "updated_date": "2024-07-31 16:48:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:38:03.527907"
    },
    {
      "arxiv_id": "2407.21738v1",
      "title": "Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification using Ultrasound Scan Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Joseph Geo Benjamin",
        "Mothilal Asokan",
        "Amna Alhosani",
        "Hussain Alasmawi",
        "Werner Gerhard Diehl",
        "Leanne Bricker",
        "Karthik Nandakumar",
        "Mohammad Yaqub"
      ],
      "abstract": "Self-supervised learning (SSL) methods are popular since they can address\nsituations with limited annotated data by directly utilising the underlying\ndata distribution. However, the adoption of such methods is not explored enough\nin ultrasound (US) imaging, especially for fetal assessment. We investigate the\npotential of dual-encoder SSL in utilizing unlabelled US video data to improve\nthe performance of challenging downstream Standard Fetal Cardiac Planes (SFCP)\nclassification using limited labelled 2D US images. We study 7 SSL approaches\nbased on reconstruction, contrastive loss, distillation, and information theory\nand evaluate them extensively on a large private US dataset. Our observations\nand findings are consolidated from more than 500 downstream training\nexperiments under different settings. Our primary observation shows that for\nSSL training, the variance of the dataset is more crucial than its size because\nit allows the model to learn generalisable representations, which improve the\nperformance of downstream tasks. Overall, the BarlowTwins method shows robust\nperformance, irrespective of the training settings and data variations, when\nused as an initialisation for downstream tasks. Notably, full fine-tuning with\n1% of labelled data outperforms ImageNet initialisation by 12% in F1-score and\noutperforms other SSL initialisations by at least 4% in F1-score, thus making\nit a promising candidate for transfer learning from US video to image data.",
      "tldr_zh": "本文利用自监督学习 (SSL) 方法，通过未标注的超声 (US) 视频数据，提升了在有限标注 2D US 图像上进行标准胎儿心脏平面 (SFCP) 分类任务的性能。研究评估了 7 种基于重建、对比损失、蒸馏和信息理论的 SSL 策略，并在大型私有数据集上进行了超过 500 次下游训练实验。关键发现是，数据集的变异性比规模更重要，能帮助模型学习可泛化的表示，其中 BarlowTwins 方法在不同设置下表现出色。最终，使用 1% 标注数据进行完全微调，比 ImageNet 初始化提高了 12% 的 F1 分数，并优于其他 SSL 初始化至少 4%，为 US 视频到图像的迁移学习提供了新途径。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "Simplifying Medical Ultrasound: 4th International Workshop, ASMUS\n  2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 8,\n  2023, Proceedings",
      "pdf_url": "http://arxiv.org/pdf/2407.21738v1",
      "published_date": "2024-07-31 16:47:21 UTC",
      "updated_date": "2024-07-31 16:47:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:38:16.515801"
    },
    {
      "arxiv_id": "2408.00051v1",
      "title": "Areas of Improvement for Autonomous Vehicles: A Machine Learning Analysis of Disengagement Reports",
      "title_zh": "翻译失败",
      "authors": [
        "Tyler Ward"
      ],
      "abstract": "Since 2014, the California Department of Motor Vehicles (CDMV) has compiled\ninformation from manufacturers of autonomous vehicles (AVs) regarding factors\nthat lead to the disengagement from autonomous driving mode in these vehicles.\nThese disengagement reports (DRs) contain information detailing whether the AV\ndisengaged from autonomous mode due to technology failure, manual override, or\nother factors during driving tests. This paper presents a machine learning (ML)\nbased analysis of the information from the 2023 DRs. We use a natural language\nprocessing (NLP) approach to extract important information from the description\nof a disengagement, and use the k-Means clustering algorithm to group report\nentries together. The cluster frequency is then analyzed, and each cluster is\nmanually categorized based on the factors leading to disengagement. We discuss\nfindings from previous years' DRs, and provide our own analysis to identify\nareas of improvement for AVs.",
      "tldr_zh": "这篇论文分析了2023年加州机动车辆局(CDMV)的自动驾驶车辆(AVs)脱离报告(DRs)，旨在通过Machine Learning (ML)方法识别AVs的改进领域。研究采用Natural Language Processing (NLP)技术从脱离描述中提取关键信息，并使用k-Means clustering算法对报告进行分组和频率分析，随后手动分类每个聚类基于导致脱离的因素，如技术故障或手动干预。论文还比较了前几年的发现，提供了针对AVs安全性和可靠性的具体改进建议。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00051v1",
      "published_date": "2024-07-31 16:36:10 UTC",
      "updated_date": "2024-07-31 16:36:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:38:25.767911"
    },
    {
      "arxiv_id": "2407.21729v1",
      "title": "ParLS-PBO: A Parallel Local Search Solver for Pseudo Boolean Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihan Chen",
        "Peng Lin",
        "Hao Hu",
        "Shaowei Cai"
      ],
      "abstract": "As a broadly applied technique in numerous optimization problems, recently,\nlocal search has been employed to solve Pseudo-Boolean Optimization (PBO)\nproblem. A representative local search solver for PBO is LSPBO. In this paper,\nfirstly, we improve LSPBO by a dynamic scoring mechanism, which dynamically\nstrikes a balance between score on hard constraints and score on the objective\nfunction.\n  Moreover, on top of this improved LSPBO , we develop the first parallel local\nsearch PBO solver. The main idea is to share good solutions among different\nthreads to guide the search, by maintaining a pool of feasible solutions. For\nevaluating solutions when updating the pool, we propose a function that\nconsiders both the solution quality and the diversity of the pool. Furthermore,\nwe calculate the polarity density in the pool to enhance the scoring function\nof local search. Our empirical experiments show clear benefits of the proposed\nparallel approach, making it competitive with the parallel version of the\nfamous commercial solver Gurobi.",
      "tldr_zh": "本研究改进了PBO（Pseudo Boolean Optimization）问题的本地搜索求解器LSPBO，通过引入动态评分机制来平衡硬约束和目标函数的分数。基于此，他们开发了第一个并行本地搜索PBO求解器ParLS-PBO，该求解器通过维护一个可行解池来共享优质解，并提出一种评估函数考虑解的质量和池的多样性，同时计算池中的极性密度以增强评分函数。实验结果显示，ParLS-PBO在性能上明显优于基线模型，并与商业求解器Gurobi的并行版本具有竞争力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages,2 figures, to be published in The 30th International\n  Conference on Principles and Practice of Constraint Programming",
      "pdf_url": "http://arxiv.org/pdf/2407.21729v1",
      "published_date": "2024-07-31 16:30:04 UTC",
      "updated_date": "2024-07-31 16:30:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:38:37.674602"
    },
    {
      "arxiv_id": "2407.21726v1",
      "title": "Artificial Intelligence Approaches for Energy Efficiency: A Review",
      "title_zh": "翻译失败",
      "authors": [
        "Alberto Pasqualetto",
        "Lorenzo Serafini",
        "Michele Sprocatti"
      ],
      "abstract": "United Nations set Sustainable Development Goals and this paper focuses on\n7th (Affordable and Clean Energy), 9th (Industries, Innovation and\nInfrastructure), and 13th (Climate Action) goals. Climate change is a major\nconcern in our society; for this reason, a current global objective is to\nreduce energy waste. This work summarizes all main approaches towards energy\nefficiency using Artificial Intelligence with a particular focus on multi-agent\nsystems to create smart buildings. It mentions the tight relationship between\nAI, especially IoT, and Big Data. It explains the application of AI to anomaly\ndetection in smart buildings and a possible classification of Intelligent\nEnergy Management Systems: Direct and Indirect. Finally, some drawbacks of AI\napproaches and some possible future research focuses are proposed.",
      "tldr_zh": "这篇综述论文探讨了使用 Artificial Intelligence (AI) 提升能源效率的方法，聚焦联合国可持续发展目标7（负担得起的清洁能源）、9（工业、创新和基础设施）及13（气候行动），旨在减少能源浪费。论文总结了 AI 在智能建筑中的关键应用，如 multi-agent systems 用于创建智能系统，以及 AI 与 IoT 和 Big Data 的紧密整合，并解释了其在异常检测中的作用。最终，论文对 Intelligent Energy Management Systems 进行了 Direct 和 Indirect 分类，讨论了 AI 方法的潜在缺点，并提出了未来研究方向。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21726v1",
      "published_date": "2024-07-31 16:24:52 UTC",
      "updated_date": "2024-07-31 16:24:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:38:50.052708"
    },
    {
      "arxiv_id": "2407.21721v1",
      "title": "Open-Vocabulary Audio-Visual Semantic Segmentation",
      "title_zh": "开放词汇音频-视觉语义分割",
      "authors": [
        "Ruohao Guo",
        "Liao Qu",
        "Dantong Niu",
        "Yanyu Qi",
        "Wenzhen Yue",
        "Ji Shi",
        "Bowei Xing",
        "Xianghua Ying"
      ],
      "abstract": "Audio-visual semantic segmentation (AVSS) aims to segment and classify\nsounding objects in videos with acoustic cues. However, most approaches operate\non the close-set assumption and only identify pre-defined categories from\ntraining data, lacking the generalization ability to detect novel categories in\npractical applications. In this paper, we introduce a new task: open-vocabulary\naudio-visual semantic segmentation, extending AVSS task to open-world scenarios\nbeyond the annotated label space. This is a more challenging task that requires\nrecognizing all categories, even those that have never been seen nor heard\nduring training. Moreover, we propose the first open-vocabulary AVSS framework,\nOV-AVSS, which mainly consists of two parts: 1) a universal sound source\nlocalization module to perform audio-visual fusion and locate all potential\nsounding objects and 2) an open-vocabulary classification module to predict\ncategories with the help of the prior knowledge from large-scale pre-trained\nvision-language models. To properly evaluate the open-vocabulary AVSS, we split\nzero-shot training and testing subsets based on the AVSBench-semantic\nbenchmark, namely AVSBench-OV. Extensive experiments demonstrate the strong\nsegmentation and zero-shot generalization ability of our model on all\ncategories. On the AVSBench-OV dataset, OV-AVSS achieves 55.43% mIoU on base\ncategories and 29.14% mIoU on novel categories, exceeding the state-of-the-art\nzero-shot method by 41.88%/20.61% and open-vocabulary method by 10.2%/11.6%.\nThe code is available at https://github.com/ruohaoguo/ovavss.",
      "tldr_zh": "本论文引入了 open-vocabulary audio-visual semantic segmentation（开放词汇音频-视觉语义分割）任务，扩展传统 AVSS 以处理开放世界场景，能够识别训练数据中未见过的类别。作者提出首个框架 OV-AVSS，包括通用音源定位模块（用于音频-视觉融合和定位潜在发声对象）和开放词汇分类模块（利用大型预训练视觉-语言模型的先验知识进行预测）。在 AVSBench-OV 数据集上，OV-AVSS 实现了 55.43% mIoU 在 base categories 和 29.14% mIoU 在 novel categories 的性能，超过了现有零-shot 和 open-vocabulary 方法的水平。",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "Accepted by ACM MM 2024 (Oral)",
      "pdf_url": "http://arxiv.org/pdf/2407.21721v1",
      "published_date": "2024-07-31 16:14:09 UTC",
      "updated_date": "2024-07-31 16:14:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:39:13.681241"
    },
    {
      "arxiv_id": "2407.21717v1",
      "title": "Assessing the State of AI Policy",
      "title_zh": "评估 AI 政策的现状",
      "authors": [
        "Joanna F. DeFranco",
        "Luke Biersmith"
      ],
      "abstract": "The deployment of artificial intelligence (AI) applications has accelerated\nrapidly. AI enabled technologies are facing the public in many ways including\ninfrastructure, consumer products and home applications. Because many of these\ntechnologies present risks either in the form of physical injury, or bias,\npotentially yielding unfair outcomes, policy makers must consider the need for\noversight. Most policymakers, however, lack the technical knowledge to judge\nwhether an emerging AI technology is safe, effective, and requires oversight,\ntherefore policy makers must depend on expert opinion. But policymakers are\nbetter served when, in addition to expert opinion, they have some general\nunderstanding of existing guidelines and regulations. This work provides an\noverview [the landscape] of AI legislation and directives at the international,\nU.S. state, city and federal levels. It also reviews relevant business\nstandards, and technical society initiatives. Then an overlap and gap analysis\nare performed resulting in a reference guide that includes recommendations and\nguidance for future policy making.",
      "tldr_zh": "该研究评估了人工智能（AI）政策现状，强调AI技术的快速部署可能带来的风险，如物理伤害或偏见，并指出政策制定者因缺乏技术知识而需依赖专家意见。论文提供了AI立法和指令的全面概述，包括国际、美国州级、市级和联邦层面的法规，以及相关商业标准和技术社会举措。最终，通过重叠和差距分析，生成参考指南并提出未来政策制定的推荐，以帮助决策者更好地管理AI风险。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21717v1",
      "published_date": "2024-07-31 16:09:25 UTC",
      "updated_date": "2024-07-31 16:09:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:39:14.289733"
    },
    {
      "arxiv_id": "2407.21714v1",
      "title": "UMMAN: Unsupervised Multi-graph Merge Adversarial Network for Disease Prediction Based on Intestinal Flora",
      "title_zh": "翻译失败",
      "authors": [
        "Dingkun Liu",
        "Hongjie Zhou",
        "Yilu Qu",
        "Huimei Zhang",
        "Yongdong Xu"
      ],
      "abstract": "The abundance of intestinal flora is closely related to human diseases, but\ndiseases are not caused by a single gut microbe. Instead, they result from the\ncomplex interplay of numerous microbial entities. This intricate and implicit\nconnection among gut microbes poses a significant challenge for disease\nprediction using abundance information from OTU data. Recently, several methods\nhave shown potential in predicting corresponding diseases. However, these\nmethods fail to learn the inner association among gut microbes from different\nhosts, leading to unsatisfactory performance. In this paper, we present a novel\narchitecture, Unsupervised Multi-graph Merge Adversarial Network (UMMAN). UMMAN\ncan obtain the embeddings of nodes in the Multi-Graph in an unsupervised\nscenario, so that it helps learn the multiplex association. Our method is the\nfirst to combine Graph Neural Network with the task of intestinal flora disease\nprediction. We employ complex relation-types to construct the Original-Graph\nand disrupt the relationships among nodes to generate corresponding\nShuffled-Graph. We introduce the Node Feature Global Integration (NFGI) module\nto represent the global features of the graph. Furthermore, we design a joint\nloss comprising adversarial loss and hybrid attention loss to ensure that the\nreal graph embedding aligns closely with the Original-Graph and diverges from\nthe Shuffled-Graph. Comprehensive experiments on five classical OTU gut\nmicrobiome datasets demonstrate the effectiveness and stability of our method.\n(We will release our code soon.)",
      "tldr_zh": "本研究提出了一种无监督多图合并对抗网络（UMMAN），用于基于肠道菌群的疾病预测，旨在解决微生物间复杂互动导致的关联学习挑战，这是首次将Graph Neural Network应用于该任务。UMMAN通过构建Original-Graph和Shuffled-Graph、引入Node Feature Global Integration (NFGI)模块，以及设计包含对抗损失和混合注意力损失的联合损失函数，来在无监督场景下获取节点嵌入并学习多重关联。在五个经典OTU肠道微生物数据集上的全面实验中，该方法展示了显著的预测性能提升和稳定性。",
      "categories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21714v1",
      "published_date": "2024-07-31 16:06:43 UTC",
      "updated_date": "2024-07-31 16:06:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:39:29.285678"
    },
    {
      "arxiv_id": "2407.21713v2",
      "title": "Social Learning through Interactions with Other Agents: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Dylan Hillier",
        "Cheston Tan",
        "Jing Jiang"
      ],
      "abstract": "Social learning plays an important role in the development of human\nintelligence. As children, we imitate our parents' speech patterns until we are\nable to produce sounds; we learn from them praising us and scolding us; and as\nadults, we learn by working with others. In this work, we survey the degree to\nwhich this paradigm -- social learning -- has been mirrored in machine\nlearning. In particular, since learning socially requires interacting with\nothers, we are interested in how embodied agents can and have utilised these\ntechniques. This is especially in light of the degree to which recent advances\nin natural language processing (NLP) enable us to perform new forms of social\nlearning. We look at how behavioural cloning and next-token prediction mirror\nhuman imitation, how learning from human feedback mirrors human education, and\nhow we can go further to enable fully communicative agents that learn from each\nother. We find that while individual social learning techniques have been used\nsuccessfully, there has been little unifying work showing how to bring them\ntogether into socially embodied agents.",
      "tldr_zh": "这篇调查论文探讨了社会学习在人类智能发展中的作用，特别是通过与他人互动的方式，并审视了机器学习领域中具身代理(embodied agents)如何模仿这一范式。论文比较了行为克隆(behavioral cloning)和下一个标记预测(next-token prediction)如何类似于人类模仿，以及学习人类反馈(learning from human feedback)如何类似于人类教育，尤其在自然语言处理(NLP)进步的背景下。研究发现，虽然这些社会学习技术已被成功应用，但缺乏将它们整合成全面的、相互沟通的具身代理的统一框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.7; I.2.0"
      ],
      "primary_category": "cs.LG",
      "comment": "To be published in IJCAI 2024, available on http://www.ijcai.org",
      "pdf_url": "http://arxiv.org/pdf/2407.21713v2",
      "published_date": "2024-07-31 16:06:34 UTC",
      "updated_date": "2024-08-04 03:08:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:39:39.440785"
    },
    {
      "arxiv_id": "2407.21708v1",
      "title": "CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature",
      "title_zh": "CEAR：从科学文献自动构建化学实体和角色知识图谱",
      "authors": [
        "Stefan Langer",
        "Fabian Neuhaus",
        "Andreas Nürnberger"
      ],
      "abstract": "Ontologies are formal representations of knowledge in specific domains that\nprovide a structured framework for organizing and understanding complex\ninformation. Creating ontologies, however, is a complex and time-consuming\nendeavor. ChEBI is a well-known ontology in the field of chemistry, which\nprovides a comprehensive resource for defining chemical entities and their\nproperties. However, it covers only a small fraction of the rapidly growing\nknowledge in chemistry and does not provide references to the scientific\nliterature. To address this, we propose a methodology that involves augmenting\nexisting annotated text corpora with knowledge from Chebi and fine-tuning a\nlarge language model (LLM) to recognize chemical entities and their roles in\nscientific text. Our experiments demonstrate the effectiveness of our approach.\nBy combining ontological knowledge and the language understanding capabilities\nof LLMs, we achieve high precision and recall rates in identifying both the\nchemical entities and roles in scientific literature. Furthermore, we extract\nthem from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a\nknowledge graph (KG) of chemical entities and roles (CEAR), which provides\ncomplementary information to ChEBI, and can help to extend it.",
      "tldr_zh": "该研究提出了一种名为 CEAR 的方法，用于从科学文献中自动构建化学实体和角色知识图谱（knowledge graph），以补充现有本体如 ChEBI 的局限性。方法涉及使用 ChEBI 知识增强标注文本语料库，并微调大型语言模型（LLM）来识别化学实体及其角色，实现了高精确度和召回率。实验从 8,000 篇 ChemRxiv 文章中提取相关信息，并应用第二个 LLM 生成 CEAR 知识图谱，提供额外的文献引用和扩展功能，从而帮助完善化学领域的本体系统。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21708v1",
      "published_date": "2024-07-31 15:56:06 UTC",
      "updated_date": "2024-07-31 15:56:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:39:50.863599"
    },
    {
      "arxiv_id": "2407.21693v3",
      "title": "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities",
      "title_zh": "翻译失败",
      "authors": [
        "Ming Zhang",
        "Caishuang Huang",
        "Yilong Wu",
        "Shichun Liu",
        "Huiyuan Zheng",
        "Yurui Dong",
        "Yujiong Shen",
        "Shihan Dou",
        "Jun Zhao",
        "Junjie Ye",
        "Qi Zhang",
        "Tao Gui",
        "Xuanjing Huang"
      ],
      "abstract": "Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented\nconversations, including information collection. How to utilize TOD accurately,\nefficiently and effectively for information collection has always been a\ncritical and challenging task. Recent studies have demonstrated that Large\nLanguage Models (LLMs) excel in dialogue, instruction generation, and\nreasoning, and can significantly enhance the performance of TOD through\nfine-tuning. However, current datasets primarily cater to user-led systems and\nare limited to predefined specific scenarios and slots, thereby necessitating\nimprovements in the proactiveness, diversity, and capabilities of TOD. In this\nstudy, we present a detailed multi-domain task-oriented data construction\nprocess for conversations, and a Chinese dialogue dataset generated based on\nthis process, TransferTOD, which authentically simulates human-computer\ndialogues in 30 popular life service scenarios. Leveraging this dataset, we\ntrained a model called TransferTOD-7B using full-parameter fine-tuning,\nshowcasing notable abilities in slot filling and questioning. Our work has\ndemonstrated its strong generalization capabilities in various downstream\nscenarios, significantly enhancing both data utilization efficiency and system\nperformance. The data is released in\nhttps://github.com/KongLongGeFDU/TransferTOD.",
      "tldr_zh": "这篇论文提出了TransferTOD，一种可泛化的中文多领域任务导向对话（Task-Oriented Dialogue, TOD）系统，旨在提升系统在信息收集中的主动性、多样性和能力。研究团队构建了一个详细的多领域对话数据集，模拟了30个热门生活服务场景的真实人机对话，并使用全参数微调技术训练了TransferTOD-7B模型，显著提高了槽位填充（slot filling）和提问功能。实验结果显示，该系统在下游场景中展现出强泛化能力，大大提升了数据利用效率和整体性能；数据集已开源至https://github.com/KongLongGeFDU/TransferTOD。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21693v3",
      "published_date": "2024-07-31 15:38:15 UTC",
      "updated_date": "2024-10-12 11:53:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:40:03.332829"
    },
    {
      "arxiv_id": "2407.21687v2",
      "title": "Dynamic Object Queries for Transformer-based Incremental Object Detection",
      "title_zh": "动态对象查询用于基于Transformer的增量对象检测",
      "authors": [
        "Jichuan Zhang",
        "Wei Li",
        "Shuang Cheng",
        "Ya-Li Li",
        "Shengjin Wang"
      ],
      "abstract": "Incremental object detection (IOD) aims to sequentially learn new classes,\nwhile maintaining the capability to locate and identify old ones. As the\ntraining data arrives with annotations only with new classes, IOD suffers from\ncatastrophic forgetting. Prior methodologies mainly tackle the forgetting issue\nthrough knowledge distillation and exemplar replay, ignoring the conflict\nbetween limited model capacity and increasing knowledge. In this paper, we\nexplore \\textit{dynamic object queries} for incremental object detection built\non Transformer architecture. We propose the \\textbf{Dy}namic object\n\\textbf{Q}uery-based \\textbf{DE}tection \\textbf{TR}ansformer (DyQ-DETR), which\nincrementally expands the model representation ability to achieve\nstability-plasticity tradeoff. First, a new set of learnable object queries are\nfed into the decoder to represent new classes. These new object queries are\naggregated with those from previous phases to adapt both old and new knowledge\nwell. Second, we propose the isolated bipartite matching for object queries in\ndifferent phases, based on disentangled self-attention. The interaction among\nthe object queries at different phases is eliminated to reduce inter-class\nconfusion. Thanks to the separate supervision and computation over object\nqueries, we further present the risk-balanced partial calibration for effective\nexemplar replay. Extensive experiments demonstrate that DyQ-DETR significantly\nsurpasses the state-of-the-art methods, with limited parameter overhead. Code\nwill be made publicly available.",
      "tldr_zh": "本文针对增量物体检测（Incremental Object Detection, IOD）中的灾难性遗忘（catastrophic forgetting）问题，提出了一种基于Transformer的动态object queries方法，以平衡模型的稳定性和可塑性。研究开发了DyQ-DETR框架，通过为新类别引入新的可学习object queries，并与旧queries聚合，实现对旧新知识的有效适应。同时，引入isolated bipartite matching和risk-balanced partial calibration，消除不同阶段queries间的交互并优化exemplor replay，以减少类别混淆。实验结果显示，DyQ-DETR显著超越现有最先进方法，同时保持有限的参数开销。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21687v2",
      "published_date": "2024-07-31 15:29:34 UTC",
      "updated_date": "2024-08-27 12:03:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:40:17.497703"
    },
    {
      "arxiv_id": "2407.21674v1",
      "title": "Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Krishan Agyakari Raja Babu",
        "Rachana Sathish",
        "Mrunal Pattanaik",
        "Rahul Venkataramani"
      ],
      "abstract": "Synthetic data is becoming increasingly integral in data-scarce fields such\nas medical imaging, serving as a substitute for real data. However, its\ninherent statistical characteristics can significantly impact downstream tasks,\npotentially compromising deployment performance. In this study, we empirically\ninvestigate this issue and uncover a critical phenomenon: downstream neural\nnetworks often exploit spurious distinctions between real and synthetic data\nwhen there is a strong correlation between the data source and the task label.\nThis exploitation manifests as \\textit{simplicity bias}, where models overly\nrely on superficial features rather than genuine task-related complexities.\nThrough principled experiments, we demonstrate that the source of data (real\nvs.\\ synthetic) can introduce spurious correlating factors leading to poor\nperformance during deployment when the correlation is absent. We first\ndemonstrate this vulnerability on a digit classification task, where the model\nspuriously utilizes the source of data instead of the digit to provide an\ninference. We provide further evidence of this phenomenon in a medical imaging\nproblem related to cardiac view classification in echocardiograms, particularly\ndistinguishing between 2-chamber and 4-chamber views. Given the increasing role\nof utilizing synthetic datasets, we hope that our experiments serve as\neffective guidelines for the utilization of synthetic datasets in model\ntraining.",
      "tldr_zh": "本研究揭示了在医疗数据增强中，合成数据(synthetic data)可能引入的偏见问题，即模型出现\\textit{simplicity bias}，过度依赖浅层特征（如数据来源）而非真实任务相关复杂性，导致部署时性能下降。作者通过实证实验，首先在数字分类任务中证明了模型会利用数据来源作为虚假相关因素(spurious distinctions)，随后在心脏视图分类（区分2-chamber和4-chamber视图）的医疗成像问题中进一步验证了这一现象。结果显示，当数据来源与任务标签强相关时，模型易受影响，强调了合成数据潜在风险。总体而言，该工作为有效利用合成数据集提供指导，帮助减少下游神经网络的偏见。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21674v1",
      "published_date": "2024-07-31 15:14:17 UTC",
      "updated_date": "2024-07-31 15:14:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:40:30.519721"
    },
    {
      "arxiv_id": "2407.21670v5",
      "title": "Dynamic Universal Approximation Theory: Foundations for Parallelism in Neural Networks",
      "title_zh": "动态通用逼近理论：神经网络中并行性的基础",
      "authors": [
        "Wei Wang",
        "Qing Li"
      ],
      "abstract": "Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network.",
      "tldr_zh": "本文提出动态通用逼近理论（Dynamic Universal Approximation Theory），作为神经网络并行化的基础，以解决传统串行模型中层数增加导致训练和推理时间延长的难题。基于 Universal Approximation Theorem (UAT)，研究者设计了 Para-Former 网络，该网络在保持性能的同时，使推理时间不受层数影响，从而显著加速多层网络的处理过程。实验结果验证了这一策略的有效性，在各种任务中展示了优越性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21670v5",
      "published_date": "2024-07-31 15:13:39 UTC",
      "updated_date": "2024-11-29 06:16:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:40:38.718637"
    },
    {
      "arxiv_id": "2407.21666v1",
      "title": "An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Aswini Kumar Patra",
        "Ankit Varshney",
        "Lingaraj Sahoo"
      ],
      "abstract": "Early detection of drought stress is critical for taking timely measures for\nreducing crop loss before the drought impact becomes irreversible. The subtle\nphenotypical and physiological changes in response to drought stress are\ncaptured by non-invasive imaging techniques and these imaging data serve as\nvaluable resource for machine learning methods to identify drought stress.\nWhile convolutional neural networks (CNNs) are in wide use, vision transformers\n(ViTs) present a promising alternative in capturing long-range dependencies and\nintricate spatial relationships, thereby enhancing the detection of subtle\nindicators of drought stress. We propose an explainable deep learning pipeline\nthat leverages the power of ViTs for drought stress detection in potato crops\nusing aerial imagery. We applied two distinct approaches: a synergistic\ncombination of ViT and support vector machine (SVM), where ViT extracts\nintricate spatial features from aerial images, and SVM classifies the crops as\nstressed or healthy and an end-to-end approach using a dedicated classification\nlayer within ViT to directly detect drought stress. Our key findings explain\nthe ViT model's decision-making process by visualizing attention maps. These\nmaps highlight the specific spatial features within the aerial images that the\nViT model focuses as the drought stress signature. Our findings demonstrate\nthat the proposed methods not only achieve high accuracy in drought stress\nidentification but also shedding light on the diverse subtle plant features\nassociated with drought stress. This offers a robust and interpretable solution\nfor drought stress monitoring for farmers to undertake informed decisions for\nimproved crop management.",
      "tldr_zh": "本研究提出了一种可解释的深度学习管道，结合 Vision Transformer (ViTs) 和 Transfer Learning，用于高效识别马铃薯作物的干旱胁迫，通过航空图像捕捉微妙植物变化。方法包括两种途径：一是ViTs提取空间特征后由Support Vector Machine (SVM)进行分类，二是端到端ViTs直接使用分类层检测胁迫。研究通过可视化注意力图解释ViTs的决策过程，突出关键空间特征作为干旱胁迫的标识。结果显示，该方法在干旱胁迫识别中实现高准确率，并揭示了多种微妙植物特征，提供可解释的解决方案，帮助农民进行更有效的作物管理。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "30 pages, 6 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.21666v1",
      "published_date": "2024-07-31 15:08:26 UTC",
      "updated_date": "2024-07-31 15:08:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:40:50.996998"
    },
    {
      "arxiv_id": "2408.04641v1",
      "title": "GPT-3 Powered Information Extraction for Building Robust Knowledge Bases",
      "title_zh": "翻译失败",
      "authors": [
        "Ritabrata Roy Choudhury",
        "Soumik Dey"
      ],
      "abstract": "This work uses the state-of-the-art language model GPT-3 to offer a novel\nmethod of information extraction for knowledge base development. The suggested\nmethod attempts to solve the difficulties associated with obtaining relevant\nentities and relationships from unstructured text in order to extract\nstructured information. We conduct experiments on a huge corpus of text from\ndiverse fields to assess the performance of our suggested technique. The\nevaluation measures, which are frequently employed in information extraction\ntasks, include precision, recall, and F1-score. The findings demonstrate that\nGPT-3 can be used to efficiently and accurately extract pertinent and correct\ninformation from text, hence increasing the precision and productivity of\nknowledge base creation. We also assess how well our suggested approach\nperforms in comparison to the most advanced information extraction techniques\nalready in use. The findings show that by utilizing only a small number of\ninstances in in-context learning, our suggested strategy yields competitive\noutcomes with notable savings in terms of data annotation and engineering\nexpense. Additionally, we use our proposed method to retrieve Biomedical\ninformation, demonstrating its practicality in a real-world setting. All things\nconsidered, our suggested method offers a viable way to overcome the\ndifficulties involved in obtaining structured data from unstructured text in\norder to create knowledge bases. It can greatly increase the precision and\neffectiveness of information extraction, which is necessary for many\napplications including chatbots, recommendation engines, and question-answering\nsystems.",
      "tldr_zh": "这篇论文提出了一种利用 GPT-3 进行信息提取的方法，旨在解决从非结构化文本中获取实体和关系以构建稳健知识库的挑战。实验在大量跨领域文本语料上进行，使用 precision、recall 和 F1-score 评估，结果显示该方法高效准确，并通过 in-context learning 显著减少了数据标注和工程成本。相比现有先进技术，该方法在性能上具有竞争力，并在生物医学信息提取的实际场景中证明了其实用性。总体上，该方法提升了知识库创建的精度和效率，支持应用如聊天机器人、推荐引擎和问答系统。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.04641v1",
      "published_date": "2024-07-31 14:59:29 UTC",
      "updated_date": "2024-07-31 14:59:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:41:04.528269"
    },
    {
      "arxiv_id": "2407.21652v2",
      "title": "Spatial Transformer Network YOLO Model for Agricultural Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yash Zambre",
        "Ekdev Rajkitkul",
        "Akshatha Mohan",
        "Joshua Peeples"
      ],
      "abstract": "Object detection plays a crucial role in the field of computer vision by\nautonomously locating and identifying objects of interest. The You Only Look\nOnce (YOLO) model is an effective single-shot detector. However, YOLO faces\nchallenges in cluttered or partially occluded scenes and can struggle with\nsmall, low-contrast objects. We propose a new method that integrates spatial\ntransformer networks (STNs) into YOLO to improve performance. The proposed\nSTN-YOLO aims to enhance the model's effectiveness by focusing on important\nareas of the image and improving the spatial invariance of the model before the\ndetection process. Our proposed method improved object detection performance\nboth qualitatively and quantitatively. We explore the impact of different\nlocalization networks within the STN module as well as the robustness of the\nmodel across different spatial transformations. We apply the STN-YOLO on\nbenchmark datasets for Agricultural object detection as well as a new dataset\nfrom a state-of-the-art plant phenotyping greenhouse facility. Our code and\ndataset are publicly available.",
      "tldr_zh": "本文提出了一种将 Spatial Transformer Networks (STNs) 整合到 YOLO 模型中的新方法，名为 STN-YOLO，旨在解决 YOLO 在杂乱或部分遮挡场景中检测小、低对比度对象时的挑战，通过关注图像关键区域并提升空间不变性来改善检测性能。实验结果显示，该方法在定性和定量上均显著提升了对象检测效果，并探讨了不同定位网络的影响及其在各种空间变换下的鲁棒性。STN-YOLO 被应用于农业对象检测的基准数据集和一个新的植物表型温室数据集，相关代码和数据集已公开可用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 5 figures, accepted to 2024 IEEE International Conference on\n  Machine Learning and Applications",
      "pdf_url": "http://arxiv.org/pdf/2407.21652v2",
      "published_date": "2024-07-31 14:53:41 UTC",
      "updated_date": "2024-09-15 21:04:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:41:16.841229"
    },
    {
      "arxiv_id": "2407.21647v1",
      "title": "Human interaction classifier for LLM based chatbot",
      "title_zh": "翻译失败",
      "authors": [
        "Diego Martín",
        "Jordi Sanchez",
        "Xavier Vizcaíno"
      ],
      "abstract": "This study investigates different approaches to classify human interactions\nin an artificial intelligence-based environment, specifically for Applus+\nIDIADA's intelligent agent AIDA. The main objective is to develop a classifier\nthat accurately identifies the type of interaction received (Conversation,\nServices, or Document Translation) to direct requests to the appropriate\nchannel and provide a more specialized and efficient service. Various models\nare compared, including LLM-based classifiers, KNN using Titan and Cohere\nembeddings, SVM, and artificial neural networks. Results show that SVM and ANN\nmodels with Cohere embeddings achieve the best overall performance, with\nsuperior F1 scores and faster execution times compared to LLM-based approaches.\nThe study concludes that the SVM model with Cohere embeddings is the most\nsuitable option for classifying human interactions in the AIDA environment,\noffering an optimal balance between accuracy and computational efficiency.",
      "tldr_zh": "这篇论文研究了针对基于 LLM 的聊天机器人的人类互动分类方法，旨在为 Applus+ IDIADA 的智能代理 AIDA 开发一个分类器，以准确识别互动类型（Conversation, Services 或 Document Translation），从而引导请求到合适的渠道。研究比较了多种模型，包括 LLM-based classifiers、KNN with Titan 和 Cohere embeddings、SVM 以及 ANN，结果显示 SVM 和 ANN 与 Cohere embeddings 结合取得了最佳性能，具有更高的 F1 scores 和更快的执行时间。最终结论是，SVM with Cohere embeddings 是 AIDA 环境中分类人类互动的最优选择，实现了准确性和计算效率的理想平衡。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.21647v1",
      "published_date": "2024-07-31 14:50:11 UTC",
      "updated_date": "2024-07-31 14:50:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:41:29.515173"
    },
    {
      "arxiv_id": "2407.21642v1",
      "title": "Lyapunov weights to convey the meaning of time in physics-informed neural networks",
      "title_zh": "Lyapunov 权重用于传达时间在物理信息神经网络中的含义",
      "authors": [
        "Gabriel Turinici"
      ],
      "abstract": "Time is not a dimension as the others. In Physics-Informed Neural Networks\n(PINN) several proposals attempted to adapt the time sampling or time weighting\nto take into account the specifics of this special dimension. But these\nproposals are not principled and need guidance to be used. We explain here\ntheoretically why the Lyapunov exponents give actionable insights and propose a\nweighting scheme to automatically adapt to chaotic, periodic or stable\ndynamics. We characterize theoretically the best weighting scheme under\ncomputational constraints as a cumulative exponential integral of the local\nLyapunov exponent estimators and show that it performs well in practice under\nthe regimes mentioned above.",
      "tldr_zh": "本研究针对 Physics-Informed Neural Networks (PINN) 中时间维度的特殊性，提出使用 Lyapunov weights 来优化时间权重方案，以更好地处理混沌、周期或稳定动态。现有方法缺乏系统性，该方案通过理论分析，基于局部 Lyapunov exponents 的累积指数积分自动调整权重，确保在计算约束下实现最佳性能。实验结果表明，该方法在上述动态场景下表现出色，提升了 PINN 的准确性和适用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21642v1",
      "published_date": "2024-07-31 14:41:40 UTC",
      "updated_date": "2024-07-31 14:41:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:41:38.135076"
    },
    {
      "arxiv_id": "2407.21638v1",
      "title": "Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components",
      "title_zh": "通过辅助审计组件对放射学报告生成模型进行质量控制",
      "authors": [
        "Hermione Warr",
        "Yasin Ibrahim",
        "Daniel R. McGowan",
        "Konstantinos Kamnitsas"
      ],
      "abstract": "Automation of medical image interpretation could alleviate bottlenecks in\ndiagnostic workflows, and has become of particular interest in recent years due\nto advancements in natural language processing. Great strides have been made\ntowards automated radiology report generation via AI, yet ensuring clinical\naccuracy in generated reports is a significant challenge, hindering deployment\nof such methods in clinical practice. In this work we propose a quality control\nframework for assessing the reliability of AI-generated radiology reports with\nrespect to semantics of diagnostic importance using modular auxiliary auditing\ncomponents (AC). Evaluating our pipeline on the MIMIC-CXR dataset, our findings\nshow that incorporating ACs in the form of disease-classifiers can enable\nauditing that identifies more reliable reports, resulting in higher F1 scores\ncompared to unfiltered generated reports. Additionally, leveraging the\nconfidence of the AC labels further improves the audit's effectiveness.",
      "tldr_zh": "本研究针对AI生成的放射学报告临床准确性问题，提出一种质量控制框架，使用模块化的辅助审计组件(Auxiliary Auditing Components, AC)来评估报告在诊断语义方面的可靠性。框架通过整合疾病分类器等AC，实现了对报告的审计，从而识别出更可靠的生成结果，并在MIMIC-CXR数据集上实验中提高了F1 scores。利用AC标签的置信度进一步优化了审计效果，为AI在临床实践中的部署提供了重要保障。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to MICCAI UNSURE Workshop",
      "pdf_url": "http://arxiv.org/pdf/2407.21638v1",
      "published_date": "2024-07-31 14:37:00 UTC",
      "updated_date": "2024-07-31 14:37:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:41:58.444723"
    },
    {
      "arxiv_id": "2408.03151v1",
      "title": "Optimizing Disease Prediction with Artificial Intelligence Driven Feature Selection and Attention Networks",
      "title_zh": "通过人工智能驱动的特征选择和注意力网络优化疾病预测",
      "authors": [
        "D. Dhinakaran",
        "S. Edwin Raja",
        "M. Thiyagarajan",
        "J. Jeno Jasmine",
        "P. Raghavan"
      ],
      "abstract": "The rapid integration of machine learning methodologies in healthcare has\nignited innovative strategies for disease prediction, particularly with the\nvast repositories of Electronic Health Records (EHR) data. This article delves\ninto the realm of multi-disease prediction, presenting a comprehensive study\nthat introduces a pioneering ensemble feature selection model. This model,\ndesigned to optimize learning systems, combines statistical, deep, and\noptimally selected features through the innovative Stabilized Energy Valley\nOptimization with Enhanced Bounds (SEV-EB) algorithm. The objective is to\nachieve unparalleled accuracy and stability in predicting various disorders.\nThis work proposes an advanced ensemble model that synergistically integrates\nstatistical, deep, and optimally selected features. This combination aims to\nenhance the predictive power of the model by capturing diverse aspects of the\nhealth data. At the heart of the proposed model lies the SEV-EB algorithm, a\nnovel approach to optimal feature selection. The algorithm introduces enhanced\nbounds and stabilization techniques, contributing to the robustness and\naccuracy of the overall prediction model. To further elevate the predictive\ncapabilities, an HSC-AttentionNet is introduced. This network architecture\ncombines deep temporal convolution capabilities with LSTM, allowing the model\nto capture both short-term patterns and long-term dependencies in health data.\nRigorous evaluations showcase the remarkable performance of the proposed model.\nAchieving a 95% accuracy and 94% F1-score in predicting various disorders, the\nmodel surpasses traditional methods, signifying a significant advancement in\ndisease prediction accuracy. The implications of this research extend beyond\nthe confines of academia.",
      "tldr_zh": "本研究旨在优化基于电子健康记录 (EHR) 的多疾病预测，通过提出一个创新的集成特征选择模型来提升准确性和稳定性。该模型结合统计特征、深度特征以及使用 Stabilized Energy Valley Optimization with Enhanced Bounds (SEV-EB) 算法优化的特征选择方法。论文进一步引入 HSC-AttentionNet 架构，该架构融合了深度时间卷积和 LSTM，以捕捉健康数据的短期模式和长期依赖关系。实验结果显示，该模型在各种疾病预测中达到95%准确率和94% F1-score，显著超越传统方法，为人工智能在医疗领域的应用提供了重要进展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 Pages, 4 Figures",
      "pdf_url": "http://arxiv.org/pdf/2408.03151v1",
      "published_date": "2024-07-31 14:12:27 UTC",
      "updated_date": "2024-07-31 14:12:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:42:14.736608"
    },
    {
      "arxiv_id": "2408.00041v3",
      "title": "Con4m: Context-aware Consistency Learning Framework for Segmented Time Series Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Junru Chen",
        "Tianyu Cao",
        "Jing Xu",
        "Jiahe Li",
        "Zhilong Chen",
        "Tao Xiao",
        "Yang Yang"
      ],
      "abstract": "Time Series Classification (TSC) encompasses two settings: classifying entire\nsequences or classifying segmented subsequences. The raw time series for\nsegmented TSC usually contain Multiple classes with Varying Duration of each\nclass (MVD). Therefore, the characteristics of MVD pose unique challenges for\nsegmented TSC, yet have been largely overlooked by existing works.\nSpecifically, there exists a natural temporal dependency between consecutive\ninstances (segments) to be classified within MVD. However, mainstream TSC\nmodels rely on the assumption of independent and identically distributed\n(i.i.d.), focusing on independently modeling each segment. Additionally,\nannotators with varying expertise may provide inconsistent boundary labels,\nleading to unstable performance of noise-free TSC models. To address these\nchallenges, we first formally demonstrate that valuable contextual information\nenhances the discriminative power of classification instances. Leveraging the\ncontextual priors of MVD at both the data and label levels, we propose a novel\nconsistency learning framework Con4m, which effectively utilizes contextual\ninformation more conducive to discriminating consecutive segments in segmented\nTSC tasks, while harmonizing inconsistent boundary labels for training.\nExtensive experiments across multiple datasets validate the effectiveness of\nCon4m in handling segmented TSC tasks on MVD. The source code is available at\nhttps://github.com/MrNobodyCali/Con4m.",
      "tldr_zh": "这篇论文针对分段时间序列分类(TSC)中的多类变持续时间(MVD)挑战，提出了一种上下文感知一致性学习框架Con4m，以解决连续实例之间的时间依赖性和标注者不一致导致的边界标签问题。该框架利用MVD的上下文先验，在数据和标签级别进行一致性学习，从而提升连续段的区分能力和模型鲁棒性。实验结果显示，Con4m在多个数据集上有效提高了分段TSC的性能，并提供了开源代码以供进一步验证。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00041v3",
      "published_date": "2024-07-31 14:06:55 UTC",
      "updated_date": "2025-04-23 09:00:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:42:16.794894"
    },
    {
      "arxiv_id": "2408.00040v3",
      "title": "Barlow Twins Deep Neural Network for Advanced 1D Drug-Target Interaction Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Maximilian G. Schuh",
        "Davide Boldini",
        "Annkathrin I. Bohne",
        "Stephan A. Sieber"
      ],
      "abstract": "Accurate prediction of drug-target interactions is critical for advancing\ndrug discovery. By reducing time and cost, machine learning and deep learning\ncan accelerate this laborious discovery process. In a novel approach,\nBarlowDTI, we utilise the powerful Barlow Twins architecture for\nfeature-extraction while considering the structure of the target protein. Our\nmethod achieves state-of-the-art predictive performance against multiple\nestablished benchmarks using only one-dimensional input. The use of gradient\nboosting machine as the underlying predictor ensures fast and efficient\npredictions without the need for substantial computational resources. We also\ninvestigate how the model reaches its decision based on individual training\nsamples. By comparing co-crystal structures, we find that BarlowDTI effectively\nexploits catalytically active and stabilising residues, highlighting the\nmodel's ability to generalise from one-dimensional input data. In addition, we\nfurther benchmark new baselines against existing methods. Together, these\ninnovations improve the efficiency and effectiveness of drug-target interaction\npredictions, providing robust tools for accelerating drug development and\ndeepening the understanding of molecular interactions. Therefore, we provide an\neasy-to-use web interface that can be freely accessed at\nhttps://www.bio.nat.tum.de/oc2/barlowdti .",
      "tldr_zh": "本研究提出BarlowDTI，一种基于Barlow Twins深度神经网络的创新方法，用于一维输入下的药物-靶点交互预测。该方法利用Barlow Twins架构进行特征提取，并结合Gradient Boosting Machine作为预测器，实现高效计算和最先进的预测性能，在多个基准测试中表现出色。研究发现，BarlowDTI能有效识别催化活性残基和稳定残基，从而从一维数据中泛化学习，并通过比较共晶结构验证模型决策的可靠性。该框架不仅提升了药物发现的效率，还提供了一个易用的网络接口（https://www.bio.nat.tum.de/oc2/barlowdti），为加速药物开发提供实用工具。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Refined model architecture, additional results added",
      "pdf_url": "http://arxiv.org/pdf/2408.00040v3",
      "published_date": "2024-07-31 14:06:18 UTC",
      "updated_date": "2024-10-14 14:13:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:42:27.891340"
    },
    {
      "arxiv_id": "2407.21615v1",
      "title": "Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music",
      "title_zh": "翻译失败",
      "authors": [
        "Pedro Sarmento",
        "Jackson Loth",
        "Mathieu Barthet"
      ],
      "abstract": "Generative AI models have recently blossomed, significantly impacting\nartistic and musical traditions. Research investigating how humans interact\nwith and deem these models is therefore crucial. Through a listening and\nreflection study, we explore participants' perspectives on AI- vs\nhuman-generated progressive metal, in symbolic format, using rock music as a\ncontrol group. AI-generated examples were produced by ProgGP, a\nTransformer-based model. We propose a mixed methods approach to assess the\neffects of generation type (human vs. AI), genre (progressive metal vs. rock),\nand curation process (random vs. cherry-picked). This combines quantitative\nfeedback on genre congruence, preference, creativity, consistency, playability,\nhumanness, and repeatability, and qualitative feedback to provide insights into\nlisteners' experiences. A total of 32 progressive metal fans completed the\nstudy. Our findings validate the use of fine-tuning to achieve genre-specific\nspecialization in AI music generation, as listeners could distinguish between\nAI-generated rock and progressive metal. Despite some AI-generated excerpts\nreceiving similar ratings to human music, listeners exhibited a preference for\nhuman compositions. Thematic analysis identified key features for genre and AI\nvs. human distinctions. Finally, we consider the ethical implications of our\nwork in promoting musical data diversity within MIR research by focusing on an\nunder-explored genre.",
      "tldr_zh": "本研究通过聆听和反思实验，分析了32名 progressive metal 粉丝对 AI-generated 和人类创作的 progressive metal 音乐的看法，使用 rock 音乐作为对照，并采用 ProgGP（一个 Transformer-based 模型）生成 AI 音乐。结果显示，听众能够区分 AI-generated rock 和 progressive metal，验证了 fine-tuning 在实现 genre-specific 专业化方面的有效性，尽管部分 AI 音乐在评分上与人类作品类似，但整体偏好人类创作。主题分析识别了区分 genre 和 AI vs. human 的关键特征，并讨论了促进音乐数据多样性的伦理含义，以推动 MIR（音乐信息检索）研究。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Reviewed pre-print accepted for publication at ISMIR 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21615v1",
      "published_date": "2024-07-31 14:03:45 UTC",
      "updated_date": "2024-07-31 14:03:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:42:40.390781"
    },
    {
      "arxiv_id": "2407.21611v2",
      "title": "Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism",
      "title_zh": "通过边界感知注意力机制增强部分伪造音频定位",
      "authors": [
        "Jiafeng Zhong",
        "Bin Li",
        "Jiangyan Yi"
      ],
      "abstract": "The task of partially spoofed audio localization aims to accurately determine\naudio authenticity at a frame level. Although some works have achieved\nencouraging results, utilizing boundary information within a single model\nremains an unexplored research topic. In this work, we propose a novel method\ncalled Boundary-aware Attention Mechanism (BAM). Specifically, it consists of\ntwo core modules: Boundary Enhancement and Boundary Frame-wise Attention. The\nformer assembles the intra-frame and inter-frame information to extract\ndiscriminative boundary features that are subsequently used for boundary\nposition detection and authenticity decision, while the latter leverages\nboundary prediction results to explicitly control the feature interaction\nbetween frames, which achieves effective discrimination between real and fake\nframes. Experimental results on PartialSpoof database demonstrate our proposed\nmethod achieves the best performance. The code is available at\nhttps://github.com/media-sec-lab/BAM.",
      "tldr_zh": "本研究针对部分欺骗音频定位（Partially Spoofed Audio Localization）任务，提出了一种新颖的Boundary-aware Attention Mechanism (BAM)方法，以在帧级别准确判断音频真实性，同时利用边界信息提升模型性能。该方法包括两个核心模块：Boundary Enhancement模块，通过整合帧内和帧间信息提取区分性边界特征，用于边界位置检测和真实性决策；以及Boundary Frame-wise Attention模块，利用边界预测结果控制帧间特征交互，从而有效区分真实和假帧。在PartialSpoof数据库上的实验表明，该方法取得了最佳性能，代码已在https://github.com/media-sec-lab/BAM开源。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by interspeech 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21611v2",
      "published_date": "2024-07-31 13:49:17 UTC",
      "updated_date": "2024-08-19 16:09:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:42:53.481740"
    },
    {
      "arxiv_id": "2407.21600v2",
      "title": "Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors",
      "title_zh": "翻译失败",
      "authors": [
        "Shoujin Huang",
        "Guanxiong Luo",
        "Yunlin Zhao",
        "Yilong Liu",
        "Yuwan Wang",
        "Kexin Yang",
        "Jingzhe Liu",
        "Hua Guo",
        "Min Wang",
        "Lingyan Zhang",
        "Mengye Lyu"
      ],
      "abstract": "Simultaneous multislice (SMS) imaging is a powerful technique for\naccelerating magnetic resonance imaging (MRI) acquisitions. However, SMS\nreconstruction remains challenging due to complex signal interactions between\nand within the excited slices. In this study, we introduce ROGER, a robust SMS\nMRI reconstruction method based on deep generative priors. Utilizing denoising\ndiffusion probabilistic models (DDPM), ROGER begins with Gaussian noise and\ngradually recovers individual slices through reverse diffusion iterations while\nenforcing data consistency from measured k-space data within the readout\nconcatenation framework. The posterior sampling procedure is designed such that\nthe DDPM training can be performed on single-slice images without requiring\nmodifications for SMS tasks. Additionally, our method incorporates a\nlow-frequency enhancement (LFE) module to address the practical issue that\nSMS-accelerated fast spin echo (FSE) and echo planar imaging (EPI) sequences\ncannot easily embed fully-sampled autocalibration signals. Extensive\nexperiments on both retrospectively and prospectively accelerated datasets\ndemonstrate that ROGER consistently outperforms existing methods, enhancing\nboth anatomical and functional imaging with strong out-of-distribution\ngeneralization. The source code and sample data for ROGER are available at\nhttps://github.com/Solor-pikachu/ROGER.",
      "tldr_zh": "本文提出ROGER，一种基于深度生成先验的鲁棒SMS MRI重建方法，利用去噪扩散概率模型（DDPM）从高斯噪声出发，通过反向扩散迭代恢复图像，同时强制k-space数据一致性。ROGER还引入低频增强（LFE）模块，解决SMS加速的FSE和EPI序列中自动校准信号缺失的问题，且其训练可直接在单切片图像上进行，无需修改。实验结果显示，ROGER在回顾性和前瞻性加速数据集上显著优于现有方法，提升了解剖和功能成像质量，并表现出强外分布泛化能力。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "eess.SP",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "comment": "Submitted to Medical Image Analysis. New fMRI analysis and figures\n  are added since v1",
      "pdf_url": "http://arxiv.org/pdf/2407.21600v2",
      "published_date": "2024-07-31 13:34:14 UTC",
      "updated_date": "2025-01-23 07:53:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:43:17.108972"
    },
    {
      "arxiv_id": "2407.21590v1",
      "title": "Measuring What Matters: Intrinsic Distance Preservation as a Robust Metric for Embedding Quality",
      "title_zh": "翻译失败",
      "authors": [
        "Steven N. Hart",
        "Thomas E. Tavolara"
      ],
      "abstract": "Unsupervised embeddings are fundamental to numerous machine learning\napplications, yet their evaluation remains a challenging task. Traditional\nassessment methods often rely on extrinsic variables, such as performance in\ndownstream tasks, which can introduce confounding factors and mask the true\nquality of embeddings. This paper introduces the Intrinsic Distance\nPreservation Evaluation (IDPE) method, a novel approach for assessing embedding\nquality based on the preservation of Mahalanobis distances between data points\nin the original and embedded spaces. We demonstrate the limitations of\nextrinsic evaluation methods through a simple example, highlighting how they\ncan lead to misleading conclusions about embedding quality. IDPE addresses\nthese issues by providing a task-independent measure of how well embeddings\npreserve the intrinsic structure of the original data. Our method leverages\nefficient similarity search techniques to make it applicable to large-scale\ndatasets. We compare IDPE with established intrinsic metrics like\ntrustworthiness and continuity, as well as extrinsic metrics such as Average\nRank and Mean Reciprocal Rank. Our results show that IDPE offers a more\ncomprehensive and reliable assessment of embedding quality across various\nscenarios. We evaluate PCA and t-SNE embeddings using IDPE, revealing insights\ninto their performance that are not captured by traditional metrics. This work\ncontributes to the field by providing a robust, efficient, and interpretable\nmethod for embedding evaluation. IDPE's focus on intrinsic properties offers a\nvaluable tool for researchers and practitioners seeking to develop and assess\nhigh-quality embeddings for diverse machine learning applications.",
      "tldr_zh": "这篇论文提出了一种新的嵌入质量评估方法，称为 Intrinsic Distance Preservation Evaluation (IDPE)，它通过测量原始空间和嵌入空间之间的 Mahalanobis distances 保存情况，来提供一个任务无关的度量，从而避免传统外在评估方法（如下游任务性能）引入的混杂因素。IDPE 利用高效的相似性搜索技术，适用于大规模数据集，并与现有指标如 trustworthiness、continuity、Average Rank 和 Mean Reciprocal Rank 进行了比较，结果显示 IDPE 在各种场景下更全面可靠。实验评估了 PCA 和 t-SNE 嵌入，揭示了传统指标未捕获的性能洞见，为机器学习应用中的嵌入开发提供了稳健、高效的工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21590v1",
      "published_date": "2024-07-31 13:26:09 UTC",
      "updated_date": "2024-07-31 13:26:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:43:18.874852"
    },
    {
      "arxiv_id": "2407.21580v1",
      "title": "Voxel Scene Graph for Intracranial Hemorrhage",
      "title_zh": "用于颅内出血的体素场景图",
      "authors": [
        "Antoine P. Sanner",
        "Nils F. Grauhan",
        "Marc A. Brockmann",
        "Ahmed E. Othman",
        "Anirban Mukhopadhyay"
      ],
      "abstract": "Patients with Intracranial Hemorrhage (ICH) face a potentially\nlife-threatening condition, and patient-centered individualized treatment\nremains challenging due to possible clinical complications. Deep-Learning-based\nmethods can efficiently analyze the routinely acquired head CTs to support the\nclinical decision-making. The majority of early work focuses on the detection\nand segmentation of ICH, but do not model the complex relations between ICH and\nadjacent brain structures. In this work, we design a tailored object detection\nmethod for ICH, which we unite with segmentation-grounded Scene Graph\nGeneration (SGG) methods to learn a holistic representation of the clinical\ncerebral scene. To the best of our knowledge, this is the first application of\nSGG for 3D voxel images. We evaluate our method on two head-CT datasets and\ndemonstrate that our model can recall up to 74% of clinically relevant\nrelations. This work lays the foundation towards SGG for 3D voxel data. The\ngenerated Scene Graphs can already provide insights for the clinician, but are\nalso valuable for all downstream tasks as a compact and interpretable\nrepresentation.",
      "tldr_zh": "本文提出了一种针对颅内出血 (ICH) 的体素场景图 (Voxel Scene Graph) 方法，以解决现有深度学习模型在建模 ICH 与相邻脑结构复杂关系方面的局限。通过定制对象检测与基于分割的场景图生成 (SGG) 相结合，该方法学习了颅内场景的整体表示，这是 SGG 在 3D 体素图像中的首次应用。实验在两个头 CT 数据集上显示，该模型能回召高达 74% 的临床相关关系，为临床决策提供可解释的洞见，并为下游任务奠定紧凑表示基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07",
        "I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21580v1",
      "published_date": "2024-07-31 13:10:59 UTC",
      "updated_date": "2024-07-31 13:10:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:43:30.604984"
    },
    {
      "arxiv_id": "2407.21579v1",
      "title": "A Performance Study of LLM-Generated Code on Leetcode",
      "title_zh": "翻译失败",
      "authors": [
        "Tristan Coignion",
        "Clément Quinton",
        "Romain Rouvoy"
      ],
      "abstract": "This study evaluates the efficiency of code generation by Large Language\nModels (LLMs) and measures their performance against human-crafted solutions\nusing a dataset from Leetcode. We compare 18 LLMs, considering factors such as\nmodel temperature and success rate, and their impact on code performance. This\nresearch introduces a novel method for measuring and comparing the speed of\nLLM-generated code, revealing that LLMs produce code with comparable\nperformance, irrespective of the adopted LLM. We also find that LLMs are\ncapable of generating code that is, on average, more efficient than the code\nwritten by humans. The paper further discusses the use of Leetcode as a\nbenchmarking dataset, the limitations imposed by potential data contamination,\nand the platform's measurement reliability. We believe that our findings\ncontribute to a better understanding of LLM capabilities in code generation and\nset the stage for future optimizations in the field.",
      "tldr_zh": "这篇论文评估了大型语言模型 (LLMs) 在 Leetcode 数据集上生成代码的性能，通过比较 18 个 LLMs 与人类代码，考虑了模型温度和成功率等因素，并引入了一种新方法来测量代码速度。研究发现，LLMs 生成的代码性能与人类相当，甚至在平均效率上更高，且这种性能不受特定 LLM 的影响。论文还讨论了使用 Leetcode 作为基准的局限性，包括潜在的数据污染和测量可靠性，并为提升 LLM 代码生成能力提供了优化方向。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21579v1",
      "published_date": "2024-07-31 13:10:03 UTC",
      "updated_date": "2024-07-31 13:10:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:43:42.033607"
    },
    {
      "arxiv_id": "2407.21577v1",
      "title": "Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography",
      "title_zh": "翻译失败",
      "authors": [
        "Kit M. Bransby",
        "Woo-jin Cho Kim",
        "Jorge Oliveira",
        "Alex Thorley",
        "Arian Beqiri",
        "Alberto Gomez",
        "Agisilaos Chartsias"
      ],
      "abstract": "Building an echocardiography view classifier that maintains performance in\nreal-life cases requires diverse multi-site data, and frequent updates with\nnewly available data to mitigate model drift. Simply fine-tuning on new\ndatasets results in \"catastrophic forgetting\", and cannot adapt to variations\nof view labels between sites. Alternatively, collecting all data on a single\nserver and re-training may not be feasible as data sharing agreements may\nrestrict image transfer, or datasets may only become available at different\ntimes. Furthermore, time and cost associated with re-training grows with every\nnew dataset. We propose a class-incremental learning method which learns an\nexpert network for each dataset, and combines all expert networks with a score\nfusion model. The influence of ``unqualified experts'' is minimised by\nweighting each contribution with a learnt in-distribution score. These weights\npromote transparency as the contribution of each expert is known during\ninference. Instead of using the original images, we use learned features from\neach dataset, which are easier to share and raise fewer licensing and privacy\nconcerns. We validate our work on six datasets from multiple sites,\ndemonstrating significant reductions in training time while improving view\nclassification performance.",
      "tldr_zh": "该研究针对超声心动图（Echocardiography）视图分类器的构建问题，提出了一种多站点类增量学习（Class-Incremental Learning）方法，以应对模型漂移、灾难性遗忘和数据共享限制。方法包括为每个数据集训练一个专家网络，并通过分数融合模型（Score Fusion Model）结合所有专家网络，同时使用学到的特征（Learned Features）代替原始图像，以简化共享并减少隐私问题。加权机制通过学习分布内分数来最小化“不合格专家”的影响，提升了模型的透明度和推理效率。在六个多站点数据集上验证，该方法显著减少了训练时间，同时提高了视图分类性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for Oral at MICCAI workshop ASMUS-2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21577v1",
      "published_date": "2024-07-31 13:05:32 UTC",
      "updated_date": "2024-07-31 13:05:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:43:54.298824"
    },
    {
      "arxiv_id": "2407.21571v1",
      "title": "PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Min Jae Jung",
        "JooHee Kim"
      ],
      "abstract": "Large Language Models (LLMs) encounter significant challenges in continual\nlearning due to catastrophic forgetting, where new information overwrites\npreviously acquired knowledge. This limitation leads to substantial\nenvironmental and economic waste. In this study, we introduce the PMoE,\nProgressive Mixture of Experts with Asymmetric Transformer, which aims to\nminimize forgetting by utilizing an asymmetric design with shallow layers\ndedicated to general knowledge and deep layers for new knowledge. PMoE\nincorporates progressively added experts in deep layers and a router that\nallocates new knowledge to the appropriate experts efficiently. The router,\npositioned adjacent to the deep layers, utilizes deep features aggregating\nconsolidated information. This enables the router to perform efficiently,\nallocating new knowledge to the appropriate experts, which progressively\nincrease in the deep layers. Extensive experiments on TRACE datasets and\ngeneral language understanding datasets demonstrate that the proposed PMoE\noutperforms previous state-of-the-art approaches.",
      "tldr_zh": "本文提出 PMoE（Progressive Mixture of Experts with Asymmetric Transformer）框架，用于解决大型语言模型（LLMs）在持续学习中的灾难性遗忘问题，通过不对称设计将浅层专注于通用知识，深层处理新知识。PMoE 采用逐步添加专家和一个路由器，利用深层特征来高效分配新知识，确保模型在学习新信息时最小化遗忘。实验结果显示，在 TRACE 数据集和一般语言理解数据集上，PMoE 超过了现有最先进方法，证明了其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21571v1",
      "published_date": "2024-07-31 12:56:14 UTC",
      "updated_date": "2024-07-31 12:56:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:44:06.293446"
    },
    {
      "arxiv_id": "2407.21566v1",
      "title": "TRGR: Transmissive RIS-aided Gait Recognition Through Walls",
      "title_zh": "翻译失败",
      "authors": [
        "Yunlong Huang",
        "Junshuo Liu",
        "Jianan Zhang",
        "Tiebin Mi",
        "Xin Shi",
        "Robert Caiming Qiu"
      ],
      "abstract": "Gait recognition with radio frequency (RF) signals enables many potential\napplications requiring accurate identification. However, current systems\nrequire individuals to be within a line-of-sight (LOS) environment and struggle\nwith low signal-to-noise ratio (SNR) when signals traverse concrete and thick\nwalls. To address these challenges, we present TRGR, a novel transmissive\nreconfigurable intelligent surface (RIS)-aided gait recognition system. TRGR\ncan recognize human identities through walls using only the magnitude\nmeasurements of channel state information (CSI) from a pair of transceivers.\nSpecifically, by leveraging transmissive RIS alongside a configuration\nalternating optimization algorithm, TRGR enhances wall penetration and signal\nquality, enabling accurate gait recognition. Furthermore, a residual\nconvolution network (RCNN) is proposed as the backbone network to learn robust\nhuman information. Experimental results confirm the efficacy of transmissive\nRIS, highlighting the significant potential of transmissive RIS in enhancing\nRF-based gait recognition systems. Extensive experiment results show that TRGR\nachieves an average accuracy of 97.88\\% in identifying persons when signals\ntraverse concrete walls, demonstrating the effectiveness and robustness of\nTRGR.",
      "tldr_zh": "该论文提出TRGR，一种基于transmissive RIS的步态识别系统，解决了传统RF信号方法在非视线环境（如穿过混凝土墙）下的低SNR问题。TRGR通过transmissive RIS结合配置交替优化算法增强信号穿透和质量，并使用CSI幅度测量及RCNN网络作为主干来提取鲁棒的人类信息。实验结果显示，该系统在信号穿越墙壁时的平均识别准确率达到97.88%，证明了transmissive RIS在RF-based步态识别中的显著潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Globecom 2024 IoTSN accepted",
      "pdf_url": "http://arxiv.org/pdf/2407.21566v1",
      "published_date": "2024-07-31 12:42:25 UTC",
      "updated_date": "2024-07-31 12:42:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:44:17.651988"
    },
    {
      "arxiv_id": "2407.21560v1",
      "title": "Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding",
      "title_zh": "基于潜在类别分布和约束解码的生成式情感分析",
      "authors": [
        "Jun Zhou",
        "Dongyang Yu",
        "Kamran Aziz",
        "Fangfang Su",
        "Qing Zhang",
        "Fei Li",
        "Donghong Ji"
      ],
      "abstract": "Fine-grained sentiment analysis involves extracting and organizing sentiment\nelements from textual data. However, existing approaches often overlook issues\nof category semantic inclusion and overlap, as well as inherent structural\npatterns within the target sequence. This study introduces a generative\nsentiment analysis model. To address the challenges related to category\nsemantic inclusion and overlap, a latent category distribution variable is\nintroduced. By reconstructing the input of a variational autoencoder, the model\nlearns the intensity of the relationship between categories and text, thereby\nimproving sequence generation. Additionally, a trie data structure and\nconstrained decoding strategy are utilized to exploit structural patterns,\nwhich in turn reduces the search space and regularizes the generation process.\nExperimental results on the Restaurant-ACOS and Laptop-ACOS datasets\ndemonstrate a significant performance improvement compared to baseline models.\nAblation experiments further confirm the effectiveness of latent category\ndistribution and constrained decoding strategy.",
      "tldr_zh": "本研究提出了一种生成式情感分析模型，针对现有方法忽略类别语义包含、重叠以及目标序列内在结构模式的问题，引入了潜在类别分布变量（latent category distribution variable）。通过重构变分自编码器（variational autoencoder）的输入，该模型学习类别与文本之间的关系强度，从而提升序列生成质量；同时，利用 trie 数据结构和约束解码策略（constrained decoding）来利用结构模式，减少搜索空间并规范生成过程。在 Restaurant-ACOS 和 Laptop-ACOS 数据集上的实验显示，该模型比基线模型显著提升性能，且消融实验证实了潜在类别分布变量和约束解码策略的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21560v1",
      "published_date": "2024-07-31 12:29:17 UTC",
      "updated_date": "2024-07-31 12:29:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:44:30.224917"
    },
    {
      "arxiv_id": "2407.21556v1",
      "title": "Operator-based semantics for choice programs: is choosing losing? (full version)",
      "title_zh": "翻译失败",
      "authors": [
        "Jesse Heyninck"
      ],
      "abstract": "Choice constructs are an important part of the language of logic programming,\nyet the study of their semantics has been a challenging task. So far, only\ntwo-valued semantics have been studied, and the different proposals for such\nsemantics have not been compared in a principled way. In this paper, an\noperator-based framework allow for the definition and comparison of different\nsemantics in a principled way is proposed.",
      "tldr_zh": "本文探讨了逻辑编程中选择结构（choice constructs）的语义研究面临的挑战，目前仅限于二值语义（two-valued semantics），且现有提案缺乏系统比较。作者提出一个基于操作符（operator-based）的框架，用于定义和比较不同语义，从而提供一个原则性的分析方法。该框架有助于更全面地理解逻辑编程语义，并可能解决“选择是否导致损失”（is choosing losing?）等潜在问题。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Extended version of a paper accepted at KR 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21556v1",
      "published_date": "2024-07-31 12:25:57 UTC",
      "updated_date": "2024-07-31 12:25:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:44:41.391948"
    },
    {
      "arxiv_id": "2408.08318v1",
      "title": "First Analysis of the EU Artifical Intelligence Act: Towards a Global Standard for Trustworthy AI?",
      "title_zh": "翻译失败",
      "authors": [
        "Marion Ho-Dac"
      ],
      "abstract": "The EU Artificial Intelligence Act (AI Act) came into force in the European\nUnion (EU) on 1 August 2024. It is a key piece of legislation both for the\ncitizens at the heart of AI technologies and for the industry active in the\ninternal market. The AI Act imposes progressive compliance on organisations -\nboth private and public - involved in the global value chain of AI systems and\nmodels marketed and used in the EU. While the Act is unprecedented on an\ninternational scale in terms of its horizontal and binding regulatory scope,\nits global appeal in support of trustworthy AI is one of its major challenges.",
      "tldr_zh": "这篇论文对欧盟人工智能法案 (EU Artificial Intelligence Act) 进行了首次分析，探讨其于 2024 年 8 月 1 日生效后对欧盟公民和行业的影响。法案要求全球 AI 系统和模型价值链中的私人和公共组织逐步遵守其横向、具有约束力的监管规定，旨在推动可信赖 AI 的发展。主要发现是，虽然该法案在国际上开创性强，但其全球吸引力面临挑战，可能难以成为统一的全球标准。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "in French language",
      "pdf_url": "http://arxiv.org/pdf/2408.08318v1",
      "published_date": "2024-07-31 12:16:03 UTC",
      "updated_date": "2024-07-31 12:16:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:44:57.184923"
    },
    {
      "arxiv_id": "2408.00033v1",
      "title": "Enhanced Fault Detection and Cause Identification Using Integrated Attention Mechanism",
      "title_zh": "利用集成注意力机制的增强故障检测与原因识别",
      "authors": [
        "Mohammad Ali Labbaf Khaniki",
        "Alireza Golkarieh",
        "Houman Nouri",
        "Mohammad Manthouri"
      ],
      "abstract": "This study introduces a novel methodology for fault detection and cause\nidentification within the Tennessee Eastman Process (TEP) by integrating a\nBidirectional Long Short-Term Memory (BiLSTM) neural network with an Integrated\nAttention Mechanism (IAM). The IAM combines the strengths of scaled dot product\nattention, residual attention, and dynamic attention to capture intricate\npatterns and dependencies crucial for TEP fault detection. Initially, the\nattention mechanism extracts important features from the input data, enhancing\nthe model's interpretability and relevance. The BiLSTM network processes these\nfeatures bidirectionally to capture long-range dependencies, and the IAM\nfurther refines the output, leading to improved fault detection results.\nSimulation results demonstrate the efficacy of this approach, showcasing\nsuperior performance in accuracy, false alarm rate, and misclassification rate\ncompared to existing methods. This methodology provides a robust and\ninterpretable solution for fault detection and diagnosis in the TEP,\nhighlighting its potential for industrial applications.",
      "tldr_zh": "这篇论文提出了一种新方法，用于 Tennessee Eastman Process (TEP) 中的故障检测和原因识别，整合了 Bidirectional Long Short-Term Memory (BiLSTM) 神经网络与 Integrated Attention Mechanism (IAM)。IAM 结合 scaled dot product attention、residual attention 和 dynamic attention，从输入数据中提取关键特征，提高模型的可解释性和相关性，同时 BiLSTM 双向处理这些特征以捕捉长距离依赖关系。实验结果显示，该方法在准确率、假警报率和误分类率上均优于现有方法，为工业应用提供了鲁棒且可解释的故障诊断解决方案。",
      "categories": [
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00033v1",
      "published_date": "2024-07-31 12:01:57 UTC",
      "updated_date": "2024-07-31 12:01:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:45:11.083072"
    },
    {
      "arxiv_id": "2408.02680v1",
      "title": "Recording First-person Experiences to Build a New Type of Foundation Model",
      "title_zh": "翻译失败",
      "authors": [
        "Dionis Barcari",
        "David Gamez",
        "Aliya Grig"
      ],
      "abstract": "Foundation models have had a big impact in recent years and billions of\ndollars are being invested in them in the current AI boom. The more popular\nones, such as Chat-GPT, are trained on large amounts of Internet data. However,\nit is becoming apparent that this data is likely to be exhausted soon, and\ntechnology companies are looking for new sources of data to train the next\ngeneration of foundation models.\n  Reinforcement learning, RAG, prompt engineering and cognitive modelling are\noften used to fine-tune and augment the behaviour of foundation models. These\ntechniques have been used to replicate people, such as Caryn Marjorie. These\nchatbots are not based on people's actual emotional and physiological responses\nto their environment, so they are, at best, a surface-level approximation to\nthe characters they are imitating.\n  To address these issues, we have developed a recording rig that captures what\nthe wearer is seeing and hearing as well as their skin conductance (GSR),\nfacial expression and brain state (14 channel EEG). AI algorithms are used to\nprocess this data into a rich picture of the environment and internal states of\nthe subject. Foundation models trained on this data could replicate human\nbehaviour much more accurately than the personality models that have been\ndeveloped so far. This type of model has many potential applications, including\nrecommendation, personal assistance, GAN systems, dating and recruitment.\n  This paper gives some background to this work and describes the recording rig\nand preliminary tests of its functionality. It then suggests how a new type of\nfoundation model could be created from the data captured by the rig and\noutlines some applications. Data gathering and model training are expensive, so\nwe are currently working on the launch of a start-up that could raise funds for\nthe next stage of the project.",
      "tldr_zh": "该研究指出，现有的 foundation models 如 Chat-GPT 依赖互联网数据，该数据即将耗尽，且当前模型（如用于模仿人的聊天机器人）仅为表面级复制，缺乏真实情感和生理响应。为解决此问题，研究团队开发了一种记录设备，捕获佩戴者的视觉、听觉、皮肤电导（GSR）、面部表情和脑状态（14 通道 EEG）等第一人称体验数据，并使用 AI 算法处理这些数据以构建更准确的人类行为模型。这种新类型 foundation model 可应用于推荐、个人助理、GAN 系统、约会和招聘等领域，论文还概述了设备测试和未来创业计划以推进数据收集和模型训练。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 5 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:2408.00030",
      "pdf_url": "http://arxiv.org/pdf/2408.02680v1",
      "published_date": "2024-07-31 11:51:26 UTC",
      "updated_date": "2024-07-31 11:51:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:45:23.411531"
    },
    {
      "arxiv_id": "2408.00030v1",
      "title": "A New Type of Foundation Model Based on Recordings of People's Emotions and Physiology",
      "title_zh": "翻译失败",
      "authors": [
        "David Gamez",
        "Dionis Barcari",
        "Aliya Grig"
      ],
      "abstract": "Foundation models have had a big impact in recent years and billions of\ndollars are being invested in them in the current AI boom. The more popular\nones, such as Chat-GPT, are trained on large amounts of data from the Internet,\nand then reinforcement learning, RAG, prompt engineering and cognitive\nmodelling are used to fine-tune and augment their behavior. This technology has\nbeen used to create models of individual people, such as Caryn Marjorie.\nHowever, these chatbots are not based on people's actual emotional and\nphysiological responses to their environment, so they are, at best,\nsurface-level approximations to the characters they are imitating. This paper\ndescribes how a new type of foundation model - a first-person foundation model\n- could be created from recordings of what a person sees and hears as well as\ntheir emotional and physiological reactions to these stimuli. A first-person\nfoundation model would map environmental stimuli to a person's emotional and\nphysiological states, and map a person's emotional and physiological states to\ntheir behavior. First-person foundation models have many exciting applications,\nincluding a new type of recommendation engine, personal assistants, generative\nadversarial networks, dating and recruitment. To obtain training data for a\nfirst-person foundation model, we have developed a recording rig that captures\nwhat the wearer is seeing and hearing as well as their emotional and\nphysiological states. This novel source of data could help to address the\nshortage of new data for building the next generation of foundation models.",
      "tldr_zh": "这篇论文提出了一种新型基础模型（foundation model），即第一人称基础模型（first-person foundation model），它基于对个人所见所闻以及情感和生理反应的记录数据构建，以克服现有模型（如Chat-GPT）仅依赖互联网数据而忽略真实生理响应的局限性。该模型通过将环境刺激映射到情感和生理状态，再映射到行为输出，实现更准确的个人建模。论文描述了用于收集训练数据的记录设备，并探讨了其应用潜力，包括新型推荐引擎（recommendation engine）、个人助理、生成对抗网络（GAN）、约会和招聘等领域，最终有助于缓解下一代基础模型数据短缺的问题。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 2 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2408.00030v1",
      "published_date": "2024-07-31 11:14:45 UTC",
      "updated_date": "2024-07-31 11:14:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:45:33.849416"
    },
    {
      "arxiv_id": "2407.21525v1",
      "title": "Skeleton-Based Action Recognition with Spatial-Structural Graph Convolution",
      "title_zh": "翻译失败",
      "authors": [
        "Jingyao Wang",
        "Emmanuel Bergeret",
        "Issam Falih"
      ],
      "abstract": "Human Activity Recognition (HAR) is a field of study that focuses on\nidentifying and classifying human activities. Skeleton-based Human Activity\nRecognition has received much attention in recent years, where Graph\nConvolutional Network (GCN) based method is widely used and has achieved\nremarkable results. However, the representation of skeleton data and the issue\nof over-smoothing in GCN still need to be studied. 1). Compared to central\nnodes, edge nodes can only aggregate limited neighbor information, and\ndifferent edge nodes of the human body are always structurally related.\nHowever, the information from edge nodes is crucial for fine-grained activity\nrecognition. 2). The Graph Convolutional Network suffers from a significant\nover-smoothing issue, causing nodes to become increasingly similar as the\nnumber of network layers increases. Based on these two ideas, we propose a\ntwo-stream graph convolution method called Spatial-Structural GCN (SpSt-GCN).\nSpatial GCN performs information aggregation based on the topological structure\nof the human body, and structural GCN performs differentiation based on the\nsimilarity of edge node sequences. The spatial connection is fixed, and the\nhuman skeleton naturally maintains this topology regardless of the actions\nperformed by humans. However, the structural connection is dynamic and depends\non the type of movement the human body is performing. Based on this idea, we\nalso propose an entirely data-driven structural connection, which greatly\nincreases flexibility. We evaluate our method on two large-scale datasets,\ni.e., NTU RGB+D and NTU RGB+D 120. The proposed method achieves good results\nwhile being efficient.",
      "tldr_zh": "本研究针对骨骼-based Human Activity Recognition (HAR) 中的问题，提出了一种Spatial-Structural Graph Convolution (SpSt-GCN)方法，以解决Graph Convolutional Network (GCN)在边缘节点信息聚合有限和over-smoothing问题上的局限性。SpSt-GCN采用两流设计：Spatial GCN基于人体固定拓扑结构进行信息聚合，而Structural GCN则动态依赖边缘节点序列的相似性，提高了细粒度活动识别的准确性。该方法还引入了完全数据驱动的structural connection，提升了灵活性，并在NTU RGB+D和NTU RGB+D 120数据集上实现了高效的良好性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21525v1",
      "published_date": "2024-07-31 11:04:41 UTC",
      "updated_date": "2024-07-31 11:04:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:45:49.580375"
    },
    {
      "arxiv_id": "2407.21523v1",
      "title": "Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI",
      "title_zh": "机器学习的表格数据增强：拥抱生成式 AI 的进展",
      "authors": [
        "Lingxi Cui",
        "Huan Li",
        "Ke Chen",
        "Lidan Shou",
        "Gang Chen"
      ],
      "abstract": "Machine learning (ML) on tabular data is ubiquitous, yet obtaining abundant\nhigh-quality tabular data for model training remains a significant obstacle.\nNumerous works have focused on tabular data augmentation (TDA) to enhance the\noriginal table with additional data, thereby improving downstream ML tasks.\nRecently, there has been a growing interest in leveraging the capabilities of\ngenerative AI for TDA. Therefore, we believe it is time to provide a\ncomprehensive review of the progress and future prospects of TDA, with a\nparticular emphasis on the trending generative AI. Specifically, we present an\narchitectural view of the TDA pipeline, comprising three main procedures:\npre-augmentation, augmentation, and post-augmentation. Pre-augmentation\nencompasses preparation tasks that facilitate subsequent TDA, including error\nhandling, table annotation, table simplification, table representation, table\nindexing, table navigation, schema matching, and entity matching. Augmentation\nsystematically analyzes current TDA methods, categorized into retrieval-based\nmethods, which retrieve external data, and generation-based methods, which\ngenerate synthetic data. We further subdivide these methods based on the\ngranularity of the augmentation process at the row, column, cell, and table\nlevels. Post-augmentation focuses on the datasets, evaluation and optimization\naspects of TDA. We also summarize current trends and future directions for TDA,\nhighlighting promising opportunities in the era of generative AI. In addition,\nthe accompanying papers and related resources are continuously updated and\nmaintained in the GitHub repository at\nhttps://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation to reflect\nongoing advancements in the field.",
      "tldr_zh": "这篇论文回顾了表格数据增强 (TDA) 在机器学习 (ML) 中的进展与前景，强调利用生成式 AI 来解决高质量表格数据短缺的问题。论文提出 TDA 管道的架构，包括预增强（处理错误、表格注释等准备任务）、增强（分为检索-based 方法和生成-based 方法，按行、列、单元格和表格级别细分）和后增强（数据集评估与优化）。通过系统分析当前 TDA 方法，论文突出了生成式 AI 的潜力，并讨论了未来趋势，如更高效的合成数据生成。相关资源在 GitHub 仓库持续更新，以支持该领域的持续发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "repository maintained at\n  https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation",
      "pdf_url": "http://arxiv.org/pdf/2407.21523v1",
      "published_date": "2024-07-31 10:56:20 UTC",
      "updated_date": "2024-07-31 10:56:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:45:58.511783"
    },
    {
      "arxiv_id": "2407.21521v1",
      "title": "The Impacts of AI Avatar Appearance and Disclosure on User Motivation",
      "title_zh": "AI 化身外观和披露对用户动机的影响",
      "authors": [
        "Boele Visser",
        "Peter van der Putten",
        "Amirhossein Zohrehvand"
      ],
      "abstract": "This study examines the influence of perceived AI features on user motivation\nin virtual interactions. AI avatars, being disclosed as being an AI, or\nembodying specific genders, could be used in user-AI interactions. Leveraging\ninsights from AI and avatar research, we explore how AI disclosure and gender\naffect user motivation. We conducted a game-based experiment involving over\n72,500 participants who solved search problems alone or with an AI companion.\nDifferent groups experienced varying AI appearances and disclosures. We\nmeasured play intensity. Results revealed that the presence of another avatar\nled to less intense play compared to solo play. Disclosure of the avatar as AI\nheightened effort intensity compared to non-disclosed AI companions.\nAdditionally, a masculine AI appearance reduced effort intensity.",
      "tldr_zh": "这篇论文探讨了AI avatar的外观和披露对用户动机的影响，具体考察AI是否被公开标识为AI以及其性别特征如何改变用户行为。研究采用了一个基于游戏的实验，涉及超过72,500名参与者，他们在单独或与AI伴侣解决搜索问题时，体验不同AI外观和披露条件，并测量玩耍强度。结果显示，AI伴侣的存在会降低玩耍强度，而披露AI身份则会增加努力强度；此外，男性化AI外观会进一步降低玩耍强度。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "15 pages, 6 figures, submitted to the 2nd International Conference on\n  Data Science & Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2407.21521v1",
      "published_date": "2024-07-31 10:48:55 UTC",
      "updated_date": "2024-07-31 10:48:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:46:10.710122"
    },
    {
      "arxiv_id": "2407.21516v1",
      "title": "Expanding the Medical Decathlon dataset: segmentation of colon and colorectal cancer from computed tomography images",
      "title_zh": "扩展 Medical Decathlon 数据集：从计算机断层扫描图像中分割结肠和结直肠癌",
      "authors": [
        "I. M. Chernenkiy",
        "Y. A. Drach",
        "S. R. Mustakimova",
        "V. V. Kazantseva",
        "N. A. Ushakov",
        "S. K. Efetov",
        "M. V. Feldsherov"
      ],
      "abstract": "Colorectal cancer is the third-most common cancer in the Western Hemisphere.\nThe segmentation of colorectal and colorectal cancer by computed tomography is\nan urgent problem in medicine. Indeed, a system capable of solving this problem\nwill enable the detection of colorectal cancer at early stages of the disease,\nfacilitate the search for pathology by the radiologist, and significantly\naccelerate the process of diagnosing the disease. However, scientific\npublications on medical image processing mostly use closed, non-public data.\nThis paper presents an extension of the Medical Decathlon dataset with\ncolorectal markups in order to improve the quality of segmentation algorithms.\nAn experienced radiologist validated the data, categorized it into subsets by\nquality, and published it in the public domain. Based on the obtained results,\nwe trained neural network models of the UNet architecture with 5-part\ncross-validation and achieved a Dice metric quality of $0.6988 \\pm 0.3$. The\npublished markups will improve the quality of colorectal cancer detection and\nsimplify the radiologist's job for study description.",
      "tldr_zh": "本研究扩展了Medical Decathlon数据集，添加了colon和colorectal cancer的分割标记，以解决CT图像中大肠癌早期检测的紧迫需求。数据集由经验丰富的放射科医生验证并按质量分类，并公开以支持算法开发。研究团队使用UNet架构的神经网络模型进行训练，采用5-part cross-validation，取得了Dice metric质量为0.6988 ± 0.3的成绩。该扩展将提升colorectal cancer检测的准确性，并简化放射科医生的诊断过程。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "8 pages, 2 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.21516v1",
      "published_date": "2024-07-31 10:36:41 UTC",
      "updated_date": "2024-07-31 10:36:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:46:21.898717"
    },
    {
      "arxiv_id": "2407.21507v1",
      "title": "FSSC: Federated Learning of Transformer Neural Networks for Semantic Image Communication",
      "title_zh": "翻译失败",
      "authors": [
        "Yuna Yan",
        "Xin Zhang",
        "Lixin Li",
        "Wensheng Lin",
        "Rui Li",
        "Wenchi Cheng",
        "Zhu Han"
      ],
      "abstract": "In this paper, we address the problem of image semantic communication in a\nmulti-user deployment scenario and propose a federated learning (FL) strategy\nfor a Swin Transformer-based semantic communication system (FSSC). Firstly, we\ndemonstrate that the adoption of a Swin Transformer for joint source-channel\ncoding (JSCC) effectively extracts semantic information in the communication\nsystem. Next, the FL framework is introduced to collaboratively learn a global\nmodel by aggregating local model parameters, rather than directly sharing\nclients' data. This approach enhances user privacy protection and reduces the\nworkload on the server or mobile edge. Simulation evaluations indicate that our\nmethod outperforms the typical JSCC algorithm and traditional separate-based\ncommunication algorithms. Particularly after integrating local semantics, the\nglobal aggregation model has further increased the Peak Signal-to-Noise Ratio\n(PSNR) by more than 2dB, thoroughly proving the effectiveness of our algorithm.",
      "tldr_zh": "这篇论文提出了 FSSC 系统，利用联邦学习 (FL) 策略训练基于 Swin Transformer 的语义图像通信框架，旨在解决多用户部署场景下的图像传输问题。通过联合源-信道编码 (JSCC)，系统有效地提取语义信息，并通过 FL 框架聚合本地模型参数而非共享数据，从而增强用户隐私保护并减轻服务器负担。实验结果显示，FSSC 优于传统 JSCC 和分离式通信算法，尤其在整合本地语义后，峰值信噪比 (PSNR) 提高了超过 2dB，证明了该方法的有效性。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21507v1",
      "published_date": "2024-07-31 10:25:24 UTC",
      "updated_date": "2024-07-31 10:25:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:46:35.000236"
    },
    {
      "arxiv_id": "2407.21498v1",
      "title": "MaskUno: Switch-Split Block For Enhancing Instance Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Jawad Haidar",
        "Marc Mouawad",
        "Imad Elhajj",
        "Daniel Asmar"
      ],
      "abstract": "Instance segmentation is an advanced form of image segmentation which, beyond\ntraditional segmentation, requires identifying individual instances of\nrepeating objects in a scene. Mask R-CNN is the most common architecture for\ninstance segmentation, and improvements to this architecture include steps such\nas benefiting from bounding box refinements, adding semantics, or backbone\nenhancements. In all the proposed variations to date, the problem of competing\nkernels (each class aims to maximize its own accuracy) persists when models try\nto synchronously learn numerous classes. In this paper, we propose mitigating\nthis problem by replacing mask prediction with a Switch-Split block that\nprocesses refined ROIs, classifies them, and assigns them to specialized mask\npredictors. We name the method MaskUno and test it on various models from the\nliterature, which are then trained on multiple classes using the benchmark COCO\ndataset. An increase in the mean Average Precision (mAP) of 2.03% was observed\nfor the high-performing DetectoRS when trained on 80 classes. MaskUno proved to\nenhance the mAP of instance segmentation models regardless of the number and\ntyp",
      "tldr_zh": "该论文针对实例分割中多个类别竞争内核的问题，提出了一种名为 MaskUno 的方法，使用 Switch-Split 块替换传统的掩码预测。\nSwitch-Split 块处理精炼的 ROI（Region of Interest），对它们进行分类并分配给专门的掩码预测器，从而缓解模型在同步学习众多类别的准确性冲突。\n实验结果显示，在 COCO 数据集上训练 80 个类时，高性能模型 DetectoRS 的 mAP（平均精度）提高了 2.03%，证明 MaskUno 可提升各种实例分割模型的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21498v1",
      "published_date": "2024-07-31 10:12:14 UTC",
      "updated_date": "2024-07-31 10:12:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:46:46.734338"
    },
    {
      "arxiv_id": "2407.21490v1",
      "title": "Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation",
      "title_zh": "可解释且可控的运动曲线引导心脏超",
      "authors": [
        "Junxuan Yu",
        "Rusi Chen",
        "Yongsong Zhou",
        "Yanlin Chen",
        "Yaofei Duan",
        "Yuhao Huang",
        "Han Zhou",
        "Tan Tao",
        "Xin Yang",
        "Dong Ni"
      ],
      "abstract": "Echocardiography video is a primary modality for diagnosing heart diseases,\nbut the limited data poses challenges for both clinical teaching and machine\nlearning training. Recently, video generative models have emerged as a\npromising strategy to alleviate this issue. However, previous methods often\nrelied on holistic conditions during generation, hindering the flexible\nmovement control over specific cardiac structures. In this context, we propose\nan explainable and controllable method for echocardiography video generation,\ntaking an initial frame and a motion curve as guidance. Our contributions are\nthree-fold. First, we extract motion information from each heart substructure\nto construct motion curves, enabling the diffusion model to synthesize\ncustomized echocardiography videos by modifying these curves. Second, we\npropose the structure-to-motion alignment module, which can map semantic\nfeatures onto motion curves across cardiac structures. Third, The\nposition-aware attention mechanism is designed to enhance video consistency\nutilizing Gaussian masks with structural position information. Extensive\nexperiments on three echocardiography datasets show that our method outperforms\nothers regarding fidelity and consistency. The full code will be released at\nhttps://github.com/mlmi-2024-72/ECM.",
      "tldr_zh": "本研究针对超声心动图视频数据稀缺的问题，提出了一种可解释和可控的生成方法，使用初始帧和运动曲线作为指导，以实现对特定心脏结构的灵活运动控制。关键贡献包括：从心脏子结构提取运动信息构建运动曲线，使扩散模型（diffusion model）能够合成定制视频；引入结构到运动对齐模块（structure-to-motion alignment module）来映射语义特征到运动曲线；以及设计位置感知注意力机制（position-aware attention mechanism），利用高斯掩码提升视频一致性。在三个超声心动图数据集上的实验显示，该方法在保真度和一致性方面优于现有方法。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted by MICCAI MLMI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21490v1",
      "published_date": "2024-07-31 09:59:20 UTC",
      "updated_date": "2024-07-31 09:59:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:46:58.213160"
    },
    {
      "arxiv_id": "2407.21489v1",
      "title": "Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends",
      "title_zh": "翻译失败",
      "authors": [
        "Giuliano Martinelli",
        "Edoardo Barba",
        "Roberto Navigli"
      ],
      "abstract": "Large autoregressive generative models have emerged as the cornerstone for\nachieving the highest performance across several Natural Language Processing\ntasks. However, the urge to attain superior results has, at times, led to the\npremature replacement of carefully designed task-specific approaches without\nexhaustive experimentation. The Coreference Resolution task is no exception;\nall recent state-of-the-art solutions adopt large generative autoregressive\nmodels that outperform encoder-based discriminative systems. In this work,we\nchallenge this recent trend by introducing Maverick, a carefully designed - yet\nsimple - pipeline, which enables running a state-of-the-art Coreference\nResolution system within the constraints of an academic budget, outperforming\nmodels with up to 13 billion parameters with as few as 500 million parameters.\nMaverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,\ntraining with up to 0.006x the memory resources and obtaining a 170x faster\ninference compared to previous state-of-the-art systems. We extensively\nvalidate the robustness of the Maverick framework with an array of diverse\nexperiments, reporting improvements over prior systems in data-scarce,\nlong-document, and out-of-domain settings. We release our code and models for\nresearch purposes at https://github.com/SapienzaNLP/maverick-coref.",
      "tldr_zh": "本研究挑战了自然语言处理(NLP)领域中依赖大型自回归生成模型的趋势，提出了一种高效且准确的核心ference resolution系统Maverick。该系统采用精心设计的简单管道，仅使用500百万参数的模型，就在CoNLL-2012基准上超越了高达130亿参数的现有SOTA模型，实现训练内存减少至0.006倍和推理速度提高170倍。通过多样实验验证，Maverick在数据稀缺、长文档和领域外场景中表现出色，并开源代码以供研究使用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at main conference of ACL 2024. 15 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.21489v1",
      "published_date": "2024-07-31 09:58:48 UTC",
      "updated_date": "2024-07-31 09:58:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:47:11.066161"
    },
    {
      "arxiv_id": "2407.21488v2",
      "title": "Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing the Upper Bound of Generative Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Zhirui Kuai",
        "Zuxu Chen",
        "Huimu Wang",
        "Mingming Li",
        "Dadong Miao",
        "Binbin Wang",
        "Xusong Chen",
        "Li Kuang",
        "Yuxing Han",
        "Jiaxing Wang",
        "Guoyu Tang",
        "Lin Liu",
        "Songlin Wang",
        "Jingwei Zhuo"
      ],
      "abstract": "Generative retrieval (GR) has emerged as a transformative paradigm in search\nand recommender systems, leveraging numeric-based identifier representations to\nenhance efficiency and generalization. Notably, methods like TIGER employing\nResidual Quantization-based Semantic Identifiers (RQ-SID), have shown\nsignificant promise in e-commerce scenarios by effectively managing item IDs.\nHowever, a critical issue termed the \"\\textbf{Hourglass}\" phenomenon, occurs in\nRQ-SID, where intermediate codebook tokens become overly concentrated,\nhindering the full utilization of generative retrieval methods. This paper\nanalyses and addresses this problem by identifying data sparsity and\nlong-tailed distribution as the primary causes. Through comprehensive\nexperiments and detailed ablation studies, we analyze the impact of these\nfactors on codebook utilization and data distribution. Our findings reveal that\nthe \"Hourglass\" phenomenon substantially impacts the performance of RQ-SID in\ngenerative retrieval. We propose effective solutions to mitigate this issue,\nthereby significantly enhancing the effectiveness of generative retrieval in\nreal-world E-commerce applications.",
      "tldr_zh": "这篇论文探讨了生成式检索（Generative Retrieval）中 Residual Quantization-based Semantic Identifiers (RQ-SID) 面临的“Hourglass”现象问题，该现象由于数据稀疏性和长尾分布导致中间 codebook tokens 过度集中，从而限制了系统的性能上限。作者通过全面实验和消融研究，分析了这些因素对 codebook 利用和数据分布的影响，并证实了该现象对 RQ-SID 在生成式检索中的负面作用。最终，他们提出了有效的解决方案，显著提升了生成式检索在真实电商应用中的效果。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21488v2",
      "published_date": "2024-07-31 09:52:53 UTC",
      "updated_date": "2024-10-31 11:45:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:47:22.654789"
    },
    {
      "arxiv_id": "2407.21485v2",
      "title": "Parallel Strategies for Best-First Generalized Planning",
      "title_zh": "针对 Best-First 广义规划的并行策略",
      "authors": [
        "Alejandro Fernández-Alburquerque",
        "Javier Segovia-Aguas"
      ],
      "abstract": "In recent years, there has been renewed interest in closing the performance\ngap between state-of-the-art planning solvers and generalized planning (GP), a\nresearch area of AI that studies the automated synthesis of algorithmic-like\nsolutions capable of solving multiple classical planning instances. One of the\ncurrent advancements has been the introduction of Best-First Generalized\nPlanning (BFGP), a GP algorithm based on a novel solution space that can be\nexplored with heuristic search, one of the foundations of modern planners. This\npaper evaluates the application of parallel search techniques to BFGP, another\ncritical component in closing the performance gap. We first discuss why BFGP is\nwell suited for parallelization and some of its differentiating characteristics\nfrom classical planners. Then, we propose two simple shared-memory parallel\nstrategies with good scaling with the number of cores.",
      "tldr_zh": "该论文探讨了在 Best-First Generalized Planning (BFGP) 中应用并行搜索技术，以缩小 AI 规划求解器和广义规划 (GP) 之间的性能差距。BFGP 是一种基于启发式搜索的算法，能够合成算法-like 解决方案，并具有适合并行化的独特特性，如探索新型解决方案空间。研究者提出了两种简单的共享内存并行策略，这些策略在多核环境下显示出良好的扩展性，从而提升了 GP 的整体效率。",
      "categories": [
        "cs.AI",
        "I.2.8; D.1.3"
      ],
      "primary_category": "cs.AI",
      "comment": "3 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.21485v2",
      "published_date": "2024-07-31 09:50:22 UTC",
      "updated_date": "2024-08-02 16:58:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:47:34.431934"
    },
    {
      "arxiv_id": "2407.21483v3",
      "title": "eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyi Pan",
        "Daniel Hernández",
        "Philipp Seifer",
        "Ralf Lämmel",
        "Steffen Staab"
      ],
      "abstract": "Over the past few years, we have seen the emergence of large knowledge graphs\ncombining information from multiple sources. Sometimes, this information is\nprovided in the form of assertions about other assertions, defining contexts\nwhere assertions are valid. A recent extension to RDF which admits statements\nover statements, called RDF-star, is in revision to become a W3C standard.\nHowever, there is no proposal for a semantics of these RDF-star statements nor\na built-in facility to operate over them. In this paper, we propose a query\nlanguage for epistemic RDF-star metadata based on a four-valued logic, called\neSPARQL. Our proposed query language extends SPARQL-star, the query language\nfor RDF-star, with a new type of FROM clause to facilitate operating with\nmultiple and sometimes conflicting beliefs. We show that the proposed query\nlanguage can express four use case queries, including the following features:\n(i) querying the belief of an individual, (ii) the aggregating of beliefs,\n(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs\n(i.e., nesting of beliefs).",
      "tldr_zh": "该论文提出 eSPARQL，一种基于四值逻辑（four-valued logic）的查询语言，用于在 RDF-star 知识图谱中表示和协调无神论（atheistic）和不可知论（agnostic）信念。eSPARQL 扩展了 SPARQL-star，通过添加新的 FROM 子句来处理多个可能冲突的信念，支持查询个体的信念、聚合信念、检测冲突以及信念嵌套（beliefs about beliefs）。这种方法为知识图谱的多源信息整合提供了更 robust 的语义支持，并通过四个用例查询验证了其有效性。",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21483v3",
      "published_date": "2024-07-31 09:48:27 UTC",
      "updated_date": "2024-08-06 12:34:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:47:47.273234"
    },
    {
      "arxiv_id": "2407.21475v1",
      "title": "Fine-gained Zero-shot Video Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Dengsheng Chen",
        "Jie Hu",
        "Xiaoming Wei",
        "Enhua Wu"
      ],
      "abstract": "Incorporating a temporal dimension into pretrained image diffusion models for\nvideo generation is a prevalent approach. However, this method is\ncomputationally demanding and necessitates large-scale video datasets. More\ncritically, the heterogeneity between image and video datasets often results in\ncatastrophic forgetting of the image expertise. Recent attempts to directly\nextract video snippets from image diffusion models have somewhat mitigated\nthese problems. Nevertheless, these methods can only generate brief video clips\nwith simple movements and fail to capture fine-grained motion or non-grid\ndeformation. In this paper, we propose a novel Zero-Shot video Sampling\nalgorithm, denoted as $\\mathcal{ZS}^2$, capable of directly sampling\nhigh-quality video clips from existing image synthesis methods, such as Stable\nDiffusion, without any training or optimization. Specifically, $\\mathcal{ZS}^2$\nutilizes the dependency noise model and temporal momentum attention to ensure\ncontent consistency and animation coherence, respectively. This ability enables\nit to excel in related tasks, such as conditional and context-specialized video\ngeneration and instruction-guided video editing. Experimental results\ndemonstrate that $\\mathcal{ZS}^2$ achieves state-of-the-art performance in\nzero-shot video generation, occasionally outperforming recent supervised\nmethods.\n  Homepage: \\url{https://densechen.github.io/zss/}.",
      "tldr_zh": "这篇论文提出了\\(\\mathcal{ZS}^2\\)（Zero-Shot video Sampling）算法，一种无需训练或优化的方法，能够从现有图像合成模型（如Stable Diffusion）直接采样高质量视频剪辑，从而解决传统视频生成中计算密集和图像知识遗忘的问题。算法通过依赖噪声模型确保内容一致性，并利用时间动量注意力实现动画连贯性，支持精细运动和非网格变形。实验结果显示，\\(\\mathcal{ZS}^2\\) 在零样本视频生成任务中达到最先进性能，甚至优于部分监督方法，并在条件视频生成、上下文专用生成以及指令引导视频编辑等方面表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21475v1",
      "published_date": "2024-07-31 09:36:58 UTC",
      "updated_date": "2024-07-31 09:36:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:47:59.773532"
    },
    {
      "arxiv_id": "2407.21468v1",
      "title": "An Invertible State Space for Process Trees",
      "title_zh": "翻译失败",
      "authors": [
        "Gero Kolhof",
        "Sebastiaan J. van Zelst"
      ],
      "abstract": "Process models are, like event data, first-class citizens in most process\nmining approaches. Several process modeling formalisms have been proposed and\nused, e.g., Petri nets, BPMN, and process trees. Despite their frequent use,\nlittle research addresses the formal properties of process trees and the\ncorresponding potential to improve the efficiency of solving common\ncomputational problems. Therefore, in this paper, we propose an invertible\nstate space definition for process trees and demonstrate that the corresponding\nstate space graph is isomorphic to the state space graph of the tree's inverse.\nOur result supports the development of novel, time-efficient, decomposition\nstrategies for applications of process trees. Our experiments confirm that our\nstate space definition allows for the adoption of bidirectional state space\nsearch, which significantly improves the overall performance of state space\nsearches.",
      "tldr_zh": "该论文探讨了过程树（process trees）在过程挖掘中的形式属性，旨在解决现有建模形式（如 Petri nets 和 BPMN）中计算问题效率不足的问题。作者提出了一种可逆状态空间定义（invertible state space definition），证明了过程树的状態空间图与其反转状态空间图是同构的（isomorphic），从而支持开发新型、时间高效的分解策略。实验结果表明，这种定义启用双向状态空间搜索（bidirectional state space search），显著提高了状态空间搜索的整体性能。",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.21468v1",
      "published_date": "2024-07-31 09:26:35 UTC",
      "updated_date": "2024-07-31 09:26:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:48:10.072969"
    },
    {
      "arxiv_id": "2407.21467v2",
      "title": "Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data",
      "title_zh": "翻译失败",
      "authors": [
        "Mengtian Kang",
        "Yansong Hu",
        "Shuo Gao",
        "Yuanyuan Liu",
        "Hongbei Meng",
        "Xuemeng Li",
        "Xuhang Chen",
        "Hubin Zhao",
        "Jing Fu",
        "Guohua Hu",
        "Wei Wang",
        "Yanning Dai",
        "Arokia Nathan",
        "Peter Smielewski",
        "Ningli Wang",
        "Shiming Li"
      ],
      "abstract": "Childhood myopia constitutes a significant global health concern. It exhibits\nan escalating prevalence and has the potential to evolve into severe,\nirreversible conditions that detrimentally impact familial well-being and\ncreate substantial economic costs. Contemporary research underscores the\nimportance of precisely predicting myopia progression to enable timely and\neffective interventions, thereby averting severe visual impairment in children.\nSuch predictions predominantly rely on subjective clinical assessments, which\nare inherently biased and resource-intensive, thus hindering their widespread\napplication. In this study, we introduce a novel, high-accuracy method for\nquantitatively predicting the myopic trajectory and myopia risk in children\nusing only fundus images and baseline refraction data. This approach was\nvalidated through a six-year longitudinal study of 3,408 children in Henan,\nutilizing 16,211 fundus images and corresponding refractive data. Our method\nbased on deep learning demonstrated predictive accuracy with an error margin of\n0.311D per year and AUC scores of 0.944 and 0.995 for forecasting the risks of\ndeveloping myopia and high myopia, respectively. These findings confirm the\nutility of our model in supporting early intervention strategies and in\nsignificantly reducing healthcare costs, particularly by obviating the need for\nadditional metadata and repeated consultations. Furthermore, our method was\ndesigned to rely only on fundus images and refractive error data, without the\nneed for meta data or multiple inquiries from doctors, strongly reducing the\nassociated medical costs and facilitating large-scale screening. Our model can\neven provide good predictions based on only a single time measurement.\nConsequently, the proposed method is an important means to reduce medical\ninequities caused by economic disparities.",
      "tldr_zh": "本文提出了一种基于深度学习的方法，使用视网膜图像（fundus images）序列和基线屈光数据（baseline refraction data）来精确预测儿童近视进展轨迹和风险，从而克服传统主观临床评估的局限性。研究通过一项为期六年的纵向研究，涉及3408名儿童和16121张图像，验证了该方法的预测误差仅为0.311D/年，AUC分数分别为0.944（近视风险）和0.995（高度近视风险）。该方法无需额外元数据或多次咨询，甚至基于单次测量即可实现高精度预测，有助于推动早期干预、降低医疗成本并缓解经济导致的医疗不平等。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21467v2",
      "published_date": "2024-07-31 09:26:20 UTC",
      "updated_date": "2025-04-15 16:41:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:48:23.989888"
    },
    {
      "arxiv_id": "2407.21460v1",
      "title": "Multi-agent Assessment with QoS Enhancement for HD Map Updates in a Vehicular Network",
      "title_zh": "多智能体评估：用于车辆网络中 HD 地图更新的 QoS 增强",
      "authors": [
        "Jeffrey Redondo",
        "Nauman Aslam",
        "Juan Zhang",
        "Zhenhui Yuan"
      ],
      "abstract": "Reinforcement Learning (RL) algorithms have been used to address the\nchallenging problems in the offloading process of vehicular ad hoc networks\n(VANET). More recently, they have been utilized to improve the dissemination of\nhigh-definition (HD) Maps. Nevertheless, implementing solutions such as deep\nQ-learning (DQN) and Actor-critic at the autonomous vehicle (AV) may lead to an\nincrease in the computational load, causing a heavy burden on the computational\ndevices and higher costs. Moreover, their implementation might raise\ncompatibility issues between technologies due to the required modifications to\nthe standards. Therefore, in this paper, we assess the scalability of an\napplication utilizing a Q-learning single-agent solution in a distributed\nmulti-agent environment. This application improves the network performance by\ntaking advantage of a smaller state, and action space whilst using a\nmulti-agent approach. The proposed solution is extensively evaluated with\ndifferent test cases involving reward function considering individual or\noverall network performance, number of agents, and centralized and distributed\nlearning comparison. The experimental results demonstrate that the time\nlatencies of our proposed solution conducted in voice, video, HD Map, and\nbest-effort cases have significant improvements, with 40.4%, 36%, 43%, and 12%\nrespectively, compared to the performances with the single-agent approach.",
      "tldr_zh": "这篇论文评估了在车辆自组网(VANET)中使用多智能体 Q-learning 方法来提升高清地图(HD Maps)更新的服务质量(QoS)，以解决传统 Reinforcement Learning 算法如 DQN 和 Actor-critic 带来的计算负载和兼容性问题。提出的解决方案采用更小的状态和动作空间，并通过多智能体方法在分布式环境中优化网络性能，包括考虑个体或整体奖励函数以及集中式与分布式学习的比较。实验结果显示，与单智能体方法相比，该方法在语音、视频、HD Maps 和最佳努力场景中的时延分别改善了40.4%、36%、43%和12%。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21460v1",
      "published_date": "2024-07-31 09:17:09 UTC",
      "updated_date": "2024-07-31 09:17:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:48:35.501678"
    },
    {
      "arxiv_id": "2407.21459v1",
      "title": "KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making",
      "title_zh": "翻译失败",
      "authors": [
        "Gilang Fajar Febrian",
        "Grazziela Figueredo"
      ],
      "abstract": "Data is crucial for evidence-based policymaking and enhancing public\nservices, including those at the Ministry of Finance of the Republic of\nIndonesia. However, the complexity and dynamic nature of governmental financial\ndata and regulations can hinder decision-making. This study investigates the\npotential of Large Language Models (LLMs) to address these challenges, focusing\non Indonesia's financial data and regulations. While LLMs are effective in the\nfinancial sector, their use in the public sector in Indonesia is unexplored.\nThis study undertakes an iterative process to develop KemenkeuGPT using the\nLangChain with Retrieval-Augmented Generation (RAG), prompt engineering and\nfine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of\nFinance, Statistics Indonesia and the International Monetary Fund (IMF).\nSurveys and interviews with Ministry officials informed, enhanced and\nfine-tuned the model. We evaluated the model using human feedback, LLM-based\nevaluation and benchmarking. The model's accuracy improved from 35% to 61%,\nwith correctness increasing from 48% to 64%. The Retrieval-Augmented Generation\nAssessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness\nwith 73% faithfulness, 40% precision and 60% recall, outperforming several\nother base models. An interview with an expert from the Ministry of Finance\nindicated that KemenkeuGPT has the potential to become an essential tool for\ndecision-making. These results are expected to improve with continuous human\nfeedback.",
      "tldr_zh": "这篇论文探讨了利用大型语言模型 (LLMs) 处理印尼政府财务数据和法规，以提升决策效率的问题，针对公共部门的应用进行了首次探索。研究团队开发了 KemenkeuGPT，通过 Retrieval-Augmented Generation (RAG)、提示工程和微调技术，结合 2003-2023 年的财政部、Statistics Indonesia 和 IMF 数据，并融入人类反馈进行优化。实验结果显示，模型准确率从 35% 提高到 61%，正确率从 48% 到 64%，在 RAGAS 框架下表现出 44% 正确性、73% 忠实度、40% 精确度和 60% 召回率，优于其他基线模型。财政部专家认为，KemenkeuGPT 有潜力成为决策的关键工具，并可通过持续反馈进一步改进。",
      "categories": [
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 7 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.21459v1",
      "published_date": "2024-07-31 09:16:33 UTC",
      "updated_date": "2024-07-31 09:16:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:48:50.174197"
    },
    {
      "arxiv_id": "2407.21453v2",
      "title": "TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaolan Huang",
        "Adrien Tousnakhoff",
        "Polina Kozyr",
        "Roman Rehausen",
        "Felix Bießmann",
        "Robert Lachlan",
        "Cedric Adjih",
        "Emmanuel Baccelli"
      ],
      "abstract": "Monitoring biodiversity at scale is challenging. Detecting and identifying\nspecies in fine grained taxonomies requires highly accurate machine learning\n(ML) methods. Training such models requires large high quality data sets. And\ndeploying these models to low power devices requires novel compression\ntechniques and model architectures. While species classification methods have\nprofited from novel data sets and advances in ML methods, in particular neural\nnetworks, deploying these state of the art models to low power devices remains\ndifficult. Here we present a comprehensive empirical comparison of various\ntinyML neural network architectures and compression techniques for species\nclassification. We focus on the example of bird song detection, more concretely\na data set curated for studying the corn bunting bird species. The data set is\nreleased along with all code and experiments of this study. In our experiments\nwe compare predictive performance, memory and time complexity of classical\nspectrogram based methods and recent approaches operating on raw audio signal.\nOur results indicate that individual bird species can be robustly detected with\nrelatively simple architectures that can be readily deployed to low power\ndevices.",
      "tldr_zh": "本研究提出TinyChirp系统，利用TinyML模型在低功耗无线声学传感器上实现鸟鸣识别，以应对大规模生物多样性监测的挑战。\n他们通过全面实验比较了各种TinyML神经网络架构、压缩技术和模型方法，包括基于光谱图的经典方法和处理原始音频信号的最新方法。\n结果显示，简单架构即可robustly检测单个鸟类物种，同时显著降低内存和时间复杂度，便于部署；同时，该研究公开了数据集、代码和实验以支持进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21453v2",
      "published_date": "2024-07-31 08:57:42 UTC",
      "updated_date": "2024-09-11 08:07:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:49:00.049989"
    },
    {
      "arxiv_id": "2407.21443v1",
      "title": "Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency",
      "title_zh": "通过滑动生成和自一致性提升大语言模型在摘要生成中的忠实度",
      "authors": [
        "Taiji Li",
        "Zhi Li",
        "Yin Zhang"
      ],
      "abstract": "Despite large language models (LLMs) have demonstrated impressive performance\nin various tasks, they are still suffering from the factual inconsistency\nproblem called hallucinations. For instance, LLMs occasionally generate content\nthat diverges from source article, and prefer to extract information that\nappears at the beginning and end of the context, especially in long document\nsummarization. Inspired by these findings, we propose to improve the\nfaithfulness of LLMs in summarization by impelling them to process the entire\narticle more fairly and faithfully. We present a novel summary generation\nstrategy, namely SliSum, which exploits the ideas of sliding windows and\nself-consistency. Specifically, SliSum divides the source article into\noverlapping windows, and utilizes LLM to generate local summaries for the\ncontent in the windows. Finally, SliSum aggregates all local summaries using\nclustering and majority voting algorithm to produce more faithful summary of\nentire article. Extensive experiments demonstrate that SliSum significantly\nimproves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and\nGPT-3.5 in both short and long text summarization, while maintaining their\nfluency and informativeness and without additional fine-tuning and resources.\nWe further conduct qualitative and quantitative studies to investigate why\nSliSum works and impacts of hyperparameters in SliSum on performance.",
      "tldr_zh": "该研究针对大型语言模型（Large Language Models, LLMs）在摘要生成中存在的 hallucinations（事实不一致）问题，提出了一种名为 SliSum 的新策略，通过 sliding windows 和 self-consistency 机制来提升摘要的忠实度。SliSum 将源文章分成重叠窗口，生成每个窗口的本地摘要，然后使用聚类和多数投票算法聚合这些摘要，以确保更公平地处理整个文章。实验结果显示，该方法显著提高了 LLaMA-2、Claude-2 和 GPT-3.5 等模型在短文和长文摘要中的忠实度，同时保持了摘要的流畅性和信息性，且无需额外微调或资源。进一步的定性和定量分析探讨了 SliSum 的有效性及其超参数的影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Long paper accepted at LREC-COLING 2024 (oral)",
      "pdf_url": "http://arxiv.org/pdf/2407.21443v1",
      "published_date": "2024-07-31 08:48:48 UTC",
      "updated_date": "2024-07-31 08:48:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:49:14.157832"
    },
    {
      "arxiv_id": "2407.21439v2",
      "title": "MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training",
      "title_zh": "翻译失败",
      "authors": [
        "Zhanpeng Chen",
        "Chengjin Xu",
        "Yiyan Qi",
        "Jian Guo"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in processing and generating content across multiple data\nmodalities. However, a significant drawback of MLLMs is their reliance on\nstatic training data, leading to outdated information and limited contextual\nawareness. This static nature hampers their ability to provide accurate and\nup-to-date responses, particularly in dynamic or rapidly evolving contexts.\nThough integrating Multimodal Retrieval-augmented Generation (Multimodal RAG)\noffers a promising solution, the system would inevitably encounter the\nmulti-granularity noisy correspondence (MNC) problem, which hinders accurate\nretrieval and generation. In this work, we propose RagVL, a novel framework\nwith knowledge-enhanced reranking and noise-injected training, to address these\nlimitations. We instruction-tune the MLLM with a simple yet effective\ninstruction template to induce its ranking ability and serve it as a reranker\nto precisely filter the top-k retrieved images. For generation, we inject\nvisual noise during training at the data and token levels to enhance the\ngenerator's robustness. Extensive experiments on the subsets of two datasets\nthat require retrieving and reasoning over images to answer a given query\nverify the effectiveness of our method. Code and models are available at\nhttps://github.com/IDEA-FinAI/RagVL.",
      "tldr_zh": "这篇论文探讨了Multimodal Large Language Models (MLLMs) 的局限性，包括依赖静态训练数据导致信息过时，以及在Multimodal Retrieval-augmented Generation (RAG) 中遇到的多粒度噪声对应 (MNC) 问题。作者提出RagVL框架，通过knowledge-enhanced reranking（即指令微调MLLMs以增强其排名能力，作为reranker过滤top-k图像）和noise-injected training（在数据和token层面注入视觉噪声以提高生成器的鲁棒性）来解决这些问题。实验在两个数据集的子集上验证了该方法的有效性，提升了检索和推理性能。代码和模型可从GitHub获取。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21439v2",
      "published_date": "2024-07-31 08:43:17 UTC",
      "updated_date": "2024-09-25 06:14:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:49:26.189788"
    },
    {
      "arxiv_id": "2407.21428v1",
      "title": "Deformable 3D Shape Diffusion Model",
      "title_zh": "可变形三维形状扩散模型",
      "authors": [
        "Dengsheng Chen",
        "Jie Hu",
        "Xiaoming Wei",
        "Enhua Wu"
      ],
      "abstract": "The Gaussian diffusion model, initially designed for image generation, has\nrecently been adapted for 3D point cloud generation. However, these adaptations\nhave not fully considered the intrinsic geometric characteristics of 3D shapes,\nthereby constraining the diffusion model's potential for 3D shape manipulation.\nTo address this limitation, we introduce a novel deformable 3D shape diffusion\nmodel that facilitates comprehensive 3D shape manipulation, including point\ncloud generation, mesh deformation, and facial animation. Our approach\ninnovatively incorporates a differential deformation kernel, which deconstructs\nthe generation of geometric structures into successive non-rigid deformation\nstages. By leveraging a probabilistic diffusion model to simulate this\nstep-by-step process, our method provides a versatile and efficient solution\nfor a wide range of applications, spanning from graphics rendering to facial\nexpression animation. Empirical evidence highlights the effectiveness of our\napproach, demonstrating state-of-the-art performance in point cloud generation\nand competitive results in mesh deformation. Additionally, extensive visual\ndemonstrations reveal the significant potential of our approach for practical\napplications. Our method presents a unique pathway for advancing 3D shape\nmanipulation and unlocking new opportunities in the realm of virtual reality.",
      "tldr_zh": "这篇论文提出了一种可变形 3D 形状扩散模型（Deformable 3D Shape Diffusion Model），以解决传统 Gaussian diffusion model 在 3D 点云生成中未充分考虑几何特性的局限性。模型创新性地引入 differential deformation kernel，将几何结构的生成分解为连续的非刚性变形阶段，并通过 probabilistic diffusion model 模拟这一逐步过程，支持点云生成、网格变形和面部动画等应用。实验结果显示，该方法在点云生成中达到 state-of-the-art 性能，在网格变形中具有竞争力，并展示了在图形渲染和虚拟现实领域的显著实际潜力。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21428v1",
      "published_date": "2024-07-31 08:24:42 UTC",
      "updated_date": "2024-07-31 08:24:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:49:38.845193"
    },
    {
      "arxiv_id": "2407.21424v2",
      "title": "Cost-Effective Hallucination Detection for LLMs",
      "title_zh": "针对 LLMs 的成本有效幻觉检测",
      "authors": [
        "Simon Valentin",
        "Jinmiao Fu",
        "Gianluca Detommaso",
        "Shaoyuan Xu",
        "Giovanni Zappella",
        "Bryan Wang"
      ],
      "abstract": "Large language models (LLMs) can be prone to hallucinations - generating\nunreliable outputs that are unfaithful to their inputs, external facts or\ninternally inconsistent. In this work, we address several challenges for\npost-hoc hallucination detection in production settings. Our pipeline for\nhallucination detection entails: first, producing a confidence score\nrepresenting the likelihood that a generated answer is a hallucination; second,\ncalibrating the score conditional on attributes of the inputs and candidate\nresponse; finally, performing detection by thresholding the calibrated score.\nWe benchmark a variety of state-of-the-art scoring methods on different\ndatasets, encompassing question answering, fact checking, and summarization\ntasks. We employ diverse LLMs to ensure a comprehensive assessment of\nperformance. We show that calibrating individual scoring methods is critical\nfor ensuring risk-aware downstream decision making. Based on findings that no\nindividual score performs best in all situations, we propose a multi-scoring\nframework, which combines different scores and achieves top performance across\nall datasets. We further introduce cost-effective multi-scoring, which can\nmatch or even outperform more expensive detection methods, while significantly\nreducing computational overhead.",
      "tldr_zh": "这篇论文针对大型语言模型(LLMs)的幻觉问题，提出了一种后处理检测管道，包括生成置信分数、根据输入和响应属性进行校准，以及通过阈值进行检测。研究者基准测试了多种最先进的评分方法，在问答、事实检查和总结任务的数据集上评估了不同LLMs的表现，发现校准评分至关重要，且没有单一方法适用于所有场景。最终，他们开发了多评分框架以提升检测性能，并引入了成本有效的多评分方法，能匹配或超越更昂贵的检测技术，同时显著减少计算开销。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to GenAI Evaluation Workshop at KDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21424v2",
      "published_date": "2024-07-31 08:19:06 UTC",
      "updated_date": "2024-08-09 11:58:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:49:50.148613"
    },
    {
      "arxiv_id": "2408.00025v3",
      "title": "Need of AI in Modern Education: in the Eyes of Explainable AI (xAI)",
      "title_zh": "AI 在现代教育中的需求：从可解释 AI (xAI) 的视角",
      "authors": [
        "Supriya Manna",
        "Niladri Sett"
      ],
      "abstract": "Modern Education is not \\textit{Modern} without AI. However, AI's complex\nnature makes understanding and fixing problems challenging. Research worldwide\nshows that a parent's income greatly influences a child's education. This led\nus to explore how AI, especially complex models, makes important decisions\nusing Explainable AI tools. Our research uncovered many complexities linked to\nparental income and offered reasonable explanations for these decisions.\nHowever, we also found biases in AI that go against what we want from AI in\neducation: clear transparency and equal access for everyone. These biases can\nimpact families and children's schooling, highlighting the need for better AI\nsolutions that offer fair opportunities to all. This chapter tries to shed\nlight on the complex ways AI operates, especially concerning biases. These are\nthe foundational steps towards better educational policies, which include using\nAI in ways that are more reliable, accountable, and beneficial for everyone\ninvolved.",
      "tldr_zh": "本研究探讨了AI在现代教育中的必要性，并通过Explainable AI (xAI)工具分析AI决策的复杂性，特别是父母收入对儿童教育的影响。研究发现，AI模型在处理相关决策时存在偏见，导致透明度和平等性不足，从而可能加剧家庭和社会不公。最终，该论文强调需要开发更可靠、负责任的AI解决方案，作为制定公平教育政策的基石。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Chapter in the book: Blockchain and AI in Shaping the Modern\n  Education System, CRC Press, Taylor & Francis Group, USA",
      "pdf_url": "http://arxiv.org/pdf/2408.00025v3",
      "published_date": "2024-07-31 08:11:33 UTC",
      "updated_date": "2025-01-02 21:59:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:49:59.651616"
    },
    {
      "arxiv_id": "2407.21385v1",
      "title": "SmileyNet -- Towards the Prediction of the Lottery by Reading Tea Leaves with AI",
      "title_zh": "翻译失败",
      "authors": [
        "Andreas Birk"
      ],
      "abstract": "We introduce SmileyNet, a novel neural network with psychic abilities. It is\ninspired by the fact that a positive mood can lead to improved cognitive\ncapabilities including classification tasks. The network is hence presented in\na first phase with smileys and an encouraging loss function is defined to bias\nit into a good mood. SmileyNet is then used to forecast the flipping of a coin\nbased on an established method of Tasseology, namely by reading tea leaves.\nTraining and testing in this second phase are done with a high-fidelity\nsimulation based on real-world pixels sampled from a professional tea-reading\ncup. SmileyNet has an amazing accuracy of 72% to correctly predict the flip of\na coin. Resnet-34, respectively YOLOv5 achieve only 49%, respectively 53%. It\nis then shown how multiple SmileyNets can be combined to win the lottery.",
      "tldr_zh": "本研究引入了 SmileyNet，一种新型神经网络，旨在通过展示笑脸和设计鼓励的损失函数来模拟积极心情，从而提升其认知和分类能力。SmileyNet 在第二阶段利用茶叶占卜（Tasseology）的真实像素模拟来预测硬币翻转，结果显示其准确率高达 72%，显著优于 ResNet-34 (49%) 和 YOLOv5 (53%)。此外，论文探讨了如何结合多个 SmileyNet 来实现彩票预测，提供了一种创新但幽默的 AI 应用方法。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.CY",
        "cs.LG",
        "cs.RO",
        "I.2; I.4; I.5; I.6; K.3.2"
      ],
      "primary_category": "cs.AI",
      "comment": "This is a satirical accumulation of misconceptions, mistakes, and\n  flawed reasoning I have encountered in recent times as a reviewer and\n  sometimes even as a reader of published papers. I hope it is entertaining and\n  useful in the context of the education of BSc, MSc, and PhD students in\n  Machine Learning, Artificial Intelligence, and Cognitive Science",
      "pdf_url": "http://arxiv.org/pdf/2407.21385v1",
      "published_date": "2024-07-31 07:16:40 UTC",
      "updated_date": "2024-07-31 07:16:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:50:24.369952"
    },
    {
      "arxiv_id": "2407.21384v2",
      "title": "GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction",
      "title_zh": "GEGA：图卷积网络和证据检索引导注意力用于增强文档级关系抽取",
      "authors": [
        "Yanxu Mao",
        "Xiaohui Chen",
        "Peipei Liu",
        "Tiehan Cui",
        "Zuhui Yue",
        "Zheng Li"
      ],
      "abstract": "Document-level relation extraction (DocRE) aims to extract relations between\nentities from unstructured document text. Compared to sentence-level relation\nextraction, it requires more complex semantic understanding from a broader text\ncontext. Currently, some studies are utilizing logical rules within evidence\nsentences to enhance the performance of DocRE. However, in the data without\nprovided evidence sentences, researchers often obtain a list of evidence\nsentences for the entire document through evidence retrieval (ER). Therefore,\nDocRE suffers from two challenges: firstly, the relevance between evidence and\nentity pairs is weak; secondly, there is insufficient extraction of complex\ncross-relations between long-distance multi-entities. To overcome these\nchallenges, we propose GEGA, a novel model for DocRE. The model leverages graph\nneural networks to construct multiple weight matrices, guiding attention\nallocation to evidence sentences. It also employs multi-scale representation\naggregation to enhance ER. Subsequently, we integrate the most efficient\nevidence information to implement both fully supervised and weakly supervised\ntraining processes for the model. We evaluate the GEGA model on three widely\nused benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The\nexperimental results indicate that our model has achieved comprehensive\nimprovements compared to the existing SOTA model.",
      "tldr_zh": "本文提出 GEGA 模型，用于提升文档级关系抽取 (DocRE)，通过 Graph Convolutional Networks 构建多个权重矩阵来引导注意力分配到证据句，并采用多尺度表示聚合增强 Evidence Retrieval (ER)，从而解决证据与实体对相关性弱以及长距离多实体复杂关系的提取不足。GEGA 模型整合高效证据信息，支持全监督和弱监督训练。在 DocRED、Re-DocRED 和 Revisit-DocRED 等基准数据集上的实验结果显示，该模型比现有 SOTA 模型实现了全面性能提升。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21384v2",
      "published_date": "2024-07-31 07:15:33 UTC",
      "updated_date": "2024-09-08 16:42:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:50:34.878730"
    },
    {
      "arxiv_id": "2407.21376v1",
      "title": "An Extended Kalman Filter Integrated Latent Feature Model on Dynamic Weighted Directed Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Hongxun Zhou",
        "Xiangyu Chen",
        "Ye Yuan"
      ],
      "abstract": "A dynamic weighted directed graph (DWDG) is commonly encountered in various\napplication scenarios. It involves extensive dynamic interactions among\nnumerous nodes. Most existing approaches explore the intricate temporal\npatterns hidden in a DWDG from the purely data-driven perspective, which\nsuffers from accuracy loss when a DWDG exhibits strong fluctuations over time.\nTo address this issue, this study proposes a novel\nExtended-Kalman-Filter-Incorporated Latent Feature (EKLF) model to represent a\nDWDG from the model-driven perspective. Its main idea is divided into the\nfollowing two-fold ideas: a) adopting a control model, i.e., the Extended\nKalman Filter (EKF), to track the complex temporal patterns precisely with its\nnonlinear state-transition and observation functions; and b) introducing an\nalternating least squares (ALS) algorithm to train the latent features (LFs)\nalternatively for precisely representing a DWDG. Empirical studies on DWDG\ndatasets demonstrate that the proposed EKLF model outperforms state-of-the-art\nmodels in prediction accuracy and computational efficiency for missing edge\nweights of a DWDG. It unveils the potential for precisely representing a DWDG\nby incorporating a control model.",
      "tldr_zh": "本研究针对动态加权有向图 (DWDG) 的复杂时间模式波动问题，提出了一种整合 Extended Kalman Filter (EKF) 的潜在特征模型 (EKLF)，以从模型驱动角度精确表示 DWDG。该模型采用 EKF 的非线性状态转移和观察函数来跟踪时间模式，并结合交替最小二乘 (ALS) 算法训练潜在特征 (LFs)，从而提升图表示的准确性。实验结果显示，EKLF 在 DWDG 数据集上预测缺失边权重时，准确性和计算效率均优于现有模型，揭示了通过控制模型增强 DWDG 表示的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21376v1",
      "published_date": "2024-07-31 06:57:27 UTC",
      "updated_date": "2024-07-31 06:57:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:50:37.931876"
    },
    {
      "arxiv_id": "2407.21368v3",
      "title": "Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Danfeng Guo",
        "Demetri Terzopoulos"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in\nrecent years, and they have been extended to the medical domain. Although\ndemonstrating satisfactory performance on medical Visual Question Answering\n(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,\nwhich makes them fail to diagnose complex pathologies. Moreover, they readily\nfail to learn minority pathologies due to imbalanced training data. We propose\ntwo prompting strategies for MLVLMs that reduce hallucination and improve VQA\nperformance. In the first strategy, we provide a detailed explanation of the\nqueried pathology. In the second strategy, we fine-tune a cheap, weak learner\nto achieve high performance on a specific metric, and textually provide its\njudgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our\nmethods significantly improve the diagnostic F1 score, with the highest\nincrease being 0.27. We also demonstrate that our prompting strategies can be\nextended to general LVLM domains. Based on POPE metrics, it effectively\nsuppresses the false negative predictions of existing LVLMs and improves Recall\nby approximately 0.07.",
      "tldr_zh": "该研究探讨了Large Vision-Language Models (LVLMs) 在医疗领域的应用，特别是Medical LVLMs (MLVLMs) 在Visual Question Answering (VQA) 任务中面临的幻觉问题和对少数病理的处理不足。作者提出两种提示策略来缓解这些问题：第一种是通过提供详细的病理解释来减少幻觉；第二种是微调一个廉价的弱学习器，并将其判断文本输入给 MLVLMs，以提升性能。在MIMIC-CXR-JPG 和 Chexpert 数据集上的实验显示，这些策略显著提高了诊断 F1 score，最高提升 0.27，并基于 POPE metrics 减少了假阴性预测，提高了 Recall 约 0.07，同时这些方法可扩展到一般 LVLM 领域。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2025:004",
      "pdf_url": "http://arxiv.org/pdf/2407.21368v3",
      "published_date": "2024-07-31 06:34:38 UTC",
      "updated_date": "2025-03-17 00:27:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:50:50.143698"
    },
    {
      "arxiv_id": "2408.05112v1",
      "title": "Semantic Successive Refinement: A Generative AI-aided Semantic Communication Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Kexin Zhang",
        "Lixin Li",
        "Wensheng Lin",
        "Yuna Yan",
        "Rui Li",
        "Wenchi Cheng",
        "Zhu Han"
      ],
      "abstract": "Semantic Communication (SC) is an emerging technology aiming to surpass the\nShannon limit. Traditional SC strategies often minimize signal distortion\nbetween the original and reconstructed data, neglecting perceptual quality,\nespecially in low Signal-to-Noise Ratio (SNR) environments. To address this\nissue, we introduce a novel Generative AI Semantic Communication (GSC) system\nfor single-user scenarios. This system leverages deep generative models to\nestablish a new paradigm in SC. Specifically, At the transmitter end, it\nemploys a joint source-channel coding mechanism based on the Swin Transformer\nfor efficient semantic feature extraction and compression. At the receiver end,\nan advanced Diffusion Model (DM) reconstructs high-quality images from degraded\nsignals, enhancing perceptual details. Additionally, we present a Multi-User\nGenerative Semantic Communication (MU-GSC) system utilizing an asynchronous\nprocessing model. This model effectively manages multiple user requests and\noptimally utilizes system resources for parallel processing. Simulation results\non public datasets demonstrate that our generative AI semantic communication\nsystems achieve superior transmission efficiency and enhanced communication\ncontent quality across various channel conditions. Compared to CNN-based\nDeepJSCC, our methods improve the Peak Signal-to-Noise Ratio (PSNR) by 17.75%\nin Additive White Gaussian Noise (AWGN) channels and by 20.86% in Rayleigh\nchannels.",
      "tldr_zh": "本研究提出了一种基于生成式 AI 的语义通信框架，名为 Semantic Successive Refinement，旨在解决传统 Semantic Communication (SC) 在低 Signal-to-Noise Ratio (SNR) 环境下的信号失真和感知质量问题。系统在发射端使用基于 Swin Transformer 的联合源-信道编码机制进行语义特征提取和压缩，而在接收端采用 Diffusion Model (DM) 来从退化信号中重建高质量图像。针对多用户场景，该框架扩展为 Multi-User Generative Semantic Communication (MU-GSC)，通过异步处理模型实现高效的多用户请求管理和资源优化。实验结果显示，与 CNN-based DeepJSCC 相比，该系统在 AWGN 信道中将 Peak Signal-to-Noise Ratio (PSNR) 提高了 17.75%，在 Rayleigh 信道中提高了 20.86%，从而提升了传输效率和通信内容质量。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.05112v1",
      "published_date": "2024-07-31 06:08:51 UTC",
      "updated_date": "2024-07-31 06:08:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:51:01.620626"
    },
    {
      "arxiv_id": "2407.21359v1",
      "title": "ProSpec RL: Plan Ahead, then Execute",
      "title_zh": "ProSpec RL：提前规划",
      "authors": [
        "Liangliang Liu",
        "Yi Guan",
        "BoRan Wang",
        "Rujia Shen",
        "Yi Lin",
        "Chaoran Kong",
        "Lian Yan",
        "Jingchi Jiang"
      ],
      "abstract": "Imagining potential outcomes of actions before execution helps agents make\nmore informed decisions, a prospective thinking ability fundamental to human\ncognition. However, mainstream model-free Reinforcement Learning (RL) methods\nlack the ability to proactively envision future scenarios, plan, and guide\nstrategies. These methods typically rely on trial and error to adjust policy\nfunctions, aiming to maximize cumulative rewards or long-term value, even if\nsuch high-reward decisions place the environment in extremely dangerous states.\nTo address this, we propose the Prospective (ProSpec) RL method, which makes\nhigher-value, lower-risk optimal decisions by imagining future n-stream\ntrajectories. Specifically, ProSpec employs a dynamic model to predict future\nstates (termed \"imagined states\") based on the current state and a series of\nsampled actions. Furthermore, we integrate the concept of Model Predictive\nControl and introduce a cycle consistency constraint that allows the agent to\nevaluate and select the optimal actions from these trajectories. Moreover,\nProSpec employs cycle consistency to mitigate two fundamental issues in RL:\naugmenting state reversibility to avoid irreversible events (low risk) and\naugmenting actions to generate numerous virtual trajectories, thereby improving\ndata efficiency. We validated the effectiveness of our method on the DMControl\nbenchmarks, where our approach achieved significant performance improvements.\nCode will be open-sourced upon acceptance.",
      "tldr_zh": "该研究提出了一种名为 ProSpec RL 的强化学习方法，强调在执行动作前通过想象未来 n-stream 轨迹来实现前瞻性决策，从而做出高价值、低风险的最优选择。ProSpec RL 利用动态模型预测基于当前状态和采样动作的未来状态（imagined states），并整合 Model Predictive Control 和 cycle consistency 约束来评估并选择最佳动作。相比传统无模型 Reinforcement Learning 方法，该方法通过增强状态可逆性和生成更多虚拟轨迹，解决了数据效率低和不可逆事件风险的问题。在 DMControl 基准测试中，ProSpec RL 取得了显著性能提升，证明了其有效性。代码将在接受后开源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21359v1",
      "published_date": "2024-07-31 06:04:55 UTC",
      "updated_date": "2024-07-31 06:04:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:51:14.043814"
    },
    {
      "arxiv_id": "2407.21358v1",
      "title": "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Elan Markowitz",
        "Anil Ramakrishna",
        "Jwala Dhamala",
        "Ninareh Mehrabi",
        "Charith Peris",
        "Rahul Gupta",
        "Kai-Wei Chang",
        "Aram Galstyan"
      ],
      "abstract": "Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing\nreliable, structured, domain-specific, and up-to-date external knowledge.\nHowever, KGs and LLMs are often developed separately and must be integrated\nafter training. We introduce Tree-of-Traversals, a novel zero-shot reasoning\nalgorithm that enables augmentation of black-box LLMs with one or more KGs. The\nalgorithm equips a LLM with actions for interfacing a KG and enables the LLM to\nperform tree search over possible thoughts and actions to find high confidence\nreasoning paths. We evaluate on two popular benchmark datasets. Our results\nshow that Tree-of-Traversals significantly improves performance on question\nanswering and KG question answering tasks. Code is available at\n\\url{https://github.com/amazon-science/tree-of-traversals}",
      "tldr_zh": "论文提出 Tree-of-Traversals，一种零样本（zero-shot）推理算法，用于将知识图谱（KGs）与黑盒大型语言模型（black-box LLMs）整合，提供可靠的外部知识支持。该算法为 LLMs 配备接口动作，并通过树搜索（tree search）在可能的想法和动作中探索高置信度推理路径，从而提升模型的推理能力。在两个流行基准数据集上的实验显示，该方法显著提高了问答和 KG 问答任务的性能，为后训练整合 KGs 提供了高效解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication at the ACL 2024 Conference",
      "pdf_url": "http://arxiv.org/pdf/2407.21358v1",
      "published_date": "2024-07-31 06:01:24 UTC",
      "updated_date": "2024-07-31 06:01:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:51:26.991257"
    },
    {
      "arxiv_id": "2407.21351v1",
      "title": "Small Object Few-shot Segmentation for Vision-based Industrial Inspection",
      "title_zh": "翻译失败",
      "authors": [
        "Zilong Zhang",
        "Chang Niu",
        "Zhibin Zhao",
        "Xingwu Zhang",
        "Xuefeng Chen"
      ],
      "abstract": "Vision-based industrial inspection (VII) aims to locate defects quickly and\naccurately. Supervised learning under a close-set setting and industrial\nanomaly detection, as two common paradigms in VII, face different problems in\npractical applications. The former is that various and sufficient defects are\ndifficult to obtain, while the latter is that specific defects cannot be\nlocated. To solve these problems, in this paper, we focus on the few-shot\nsemantic segmentation (FSS) method, which can locate unseen defects conditioned\non a few annotations without retraining. Compared to common objects in natural\nimages, the defects in VII are small. This brings two problems to current FSS\nmethods: 1 distortion of target semantics and 2 many false positives for\nbackgrounds. To alleviate these problems, we propose a small object few-shot\nsegmentation (SOFS) model. The key idea for alleviating 1 is to avoid the\nresizing of the original image and correctly indicate the intensity of target\nsemantics. SOFS achieves this idea via the non-resizing procedure and the\nprototype intensity downsampling of support annotations. To alleviate 2, we\ndesign an abnormal prior map in SOFS to guide the model to reduce false\npositives and propose a mixed normal Dice loss to preferentially prevent the\nmodel from predicting false positives. SOFS can achieve FSS and few-shot\nanomaly detection determined by support masks. Diverse experiments substantiate\nthe superior performance of SOFS. Code is available at\nhttps://github.com/zhangzilongc/SOFS.",
      "tldr_zh": "本论文针对视觉-based工业检测（VII）中的缺陷定位问题，提出了一种小物体少样本分割（SOFS）模型，以解决现有少样本语义分割（FSS）方法在处理小缺陷时的目标语义失真和背景假阳性问题。SOFS 通过避免图像缩放、采用原型强度下采样来正确表示目标语义，并引入异常先验图和混合正常Dice损失来减少假阳性预测，从而实现基于少量标注的未见缺陷定位，而无需重新训练。实验结果显示，SOFS 在多样化场景中表现出优越性能，可同时支持FSS 和少样本异常检测，代码已在GitHub上公开。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21351v1",
      "published_date": "2024-07-31 05:43:36 UTC",
      "updated_date": "2024-07-31 05:43:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:51:39.153061"
    },
    {
      "arxiv_id": "2408.00024v1",
      "title": "Deceptive AI systems that give explanations are more convincing than honest AI systems and can amplify belief in misinformation",
      "title_zh": "翻译失败",
      "authors": [
        "Valdemar Danry",
        "Pat Pataranutaporn",
        "Matthew Groh",
        "Ziv Epstein",
        "Pattie Maes"
      ],
      "abstract": "Advanced Artificial Intelligence (AI) systems, specifically large language\nmodels (LLMs), have the capability to generate not just misinformation, but\nalso deceptive explanations that can justify and propagate false information\nand erode trust in the truth. We examined the impact of deceptive AI generated\nexplanations on individuals' beliefs in a pre-registered online experiment with\n23,840 observations from 1,192 participants. We found that in addition to being\nmore persuasive than accurate and honest explanations, AI-generated deceptive\nexplanations can significantly amplify belief in false news headlines and\nundermine true ones as compared to AI systems that simply classify the headline\nincorrectly as being true/false. Moreover, our results show that personal\nfactors such as cognitive reflection and trust in AI do not necessarily protect\nindividuals from these effects caused by deceptive AI generated explanations.\nInstead, our results show that the logical validity of AI generated deceptive\nexplanations, that is whether the explanation has a causal effect on the\ntruthfulness of the AI's classification, plays a critical role in countering\ntheir persuasiveness - with logically invalid explanations being deemed less\ncredible. This underscores the importance of teaching logical reasoning and\ncritical thinking skills to identify logically invalid arguments, fostering\ngreater resilience against advanced AI-driven misinformation.",
      "tldr_zh": "本研究发现，AI 系统（如大语言模型 LLMs）生成的欺骗性解释比诚实解释更有说服力，能放大人们对虚假新闻的相信，并削弱对真实新闻的信任。通过一个预注册在线实验（涉及1,192名参与者和23,840次观察），结果显示个人因素如认知反思和对AI的信任无法有效保护个体免受这些影响。关键发现是，AI生成欺骗性解释的逻辑有效性至关重要，逻辑无效的解释被视为更不可信，因此教育逻辑推理和批判性思维技能是增强对AI驱动误信息的抵抗力的重要措施。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00024v1",
      "published_date": "2024-07-31 05:39:07 UTC",
      "updated_date": "2024-07-31 05:39:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:51:51.753220"
    },
    {
      "arxiv_id": "2407.21347v2",
      "title": "Differentially Private Block-wise Gradient Shuffle for Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "David Zagardo"
      ],
      "abstract": "Traditional Differentially Private Stochastic Gradient Descent (DP-SGD)\nintroduces statistical noise on top of gradients drawn from a Gaussian\ndistribution to ensure privacy. This paper introduces the novel Differentially\nPrivate Block-wise Gradient Shuffle (DP-BloGS) algorithm for deep learning.\nBloGS builds off of existing private deep learning literature, but makes a\ndefinitive shift by taking a probabilistic approach to gradient noise\nintroduction through shuffling modeled after information theoretic privacy\nanalyses. The theoretical results presented in this paper show that the\ncombination of shuffling, parameter-specific block size selection, batch layer\nclipping, and gradient accumulation allows DP-BloGS to achieve training times\nclose to that of non-private training while maintaining similar privacy and\nutility guarantees to DP-SGD. DP-BloGS is found to be significantly more\nresistant to data extraction attempts than DP-SGD. The theoretical results are\nvalidated by the experimental findings.",
      "tldr_zh": "本论文提出了一种新型算法 Differentially Private Block-wise Gradient Shuffle (DP-BloGS)，用于深度学习的差分隐私训练，以概率洗牌方式引入梯度噪声，借鉴信息理论隐私分析，并结合参数特定块大小选择、批层剪裁和梯度积累。相比传统 DP-SGD，DP-BloGS 显著缩短了训练时间，使其接近非隐私训练，同时保持相似的隐私和效用保证。实验结果显示，DP-BloGS 在抵抗数据提取尝试方面表现出色，并验证了理论分析的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "The results are genuine, but the math is wrong! Please do not use\n  this method for your Differential Privacy implementations",
      "pdf_url": "http://arxiv.org/pdf/2407.21347v2",
      "published_date": "2024-07-31 05:32:37 UTC",
      "updated_date": "2025-01-20 16:24:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:52:03.654099"
    },
    {
      "arxiv_id": "2407.21344v1",
      "title": "Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Jingyao Wu",
        "Ting Dang",
        "Vidhyasaharan Sethu",
        "Eliathamby Ambikairajah"
      ],
      "abstract": "There has been a significant focus on modelling emotion ambiguity in recent\nyears, with advancements made in representing emotions as distributions to\ncapture ambiguity. However, there has been comparatively less effort devoted to\nthe consideration of temporal dependencies in emotion distributions which\nencodes ambiguity in perceived emotions that evolve smoothly over time.\nRecognizing the benefits of using constrained dynamical neural ordinary\ndifferential equations (CD-NODE) to model time series as dynamic processes, we\npropose an ambiguity-aware dual-constrained Neural ODE approach to model the\ndynamics of emotion distributions on arousal and valence. In our approach, we\nutilize ODEs parameterised by neural networks to estimate the distribution\nparameters, and we integrate additional constraints to restrict the range of\nthe system outputs to ensure the validity of predicted distributions. We\nevaluated our proposed system on the publicly available RECOLA dataset and\nobserved very promising performance across a range of evaluation metrics.",
      "tldr_zh": "这篇论文提出了一种模糊感知的双重约束动态 Neural ODEs 方法，用于处理连续情感预测中的时序依赖问题，旨在捕捉唤醒和价值维度上情感分布的平滑演变。方法通过神经网络参数化的 ODE 来估计分布参数，并整合额外约束以限制输出范围，确保预测分布的有效性。与传统方法相比，该方法强调了情感模糊的动态建模。实验在公开的 RECOLA 数据集上进行，显示了在多种评估指标上的出色性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted at INTERSPEECh 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21344v1",
      "published_date": "2024-07-31 05:18:06 UTC",
      "updated_date": "2024-07-31 05:18:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:52:14.041852"
    },
    {
      "arxiv_id": "2407.21343v2",
      "title": "MIST: A Simple and Scalable End-To-End 3D Medical Imaging Segmentation Framework",
      "title_zh": "MIST：一个简单且可扩展的端到端三维医疗图像分割框架",
      "authors": [
        "Adrian Celaya",
        "Evan Lim",
        "Rachel Glenn",
        "Brayden Mi",
        "Alex Balsells",
        "Dawid Schellingerhout",
        "Tucker Netherton",
        "Caroline Chung",
        "Beatrice Riviere",
        "David Fuentes"
      ],
      "abstract": "Medical imaging segmentation is a highly active area of research, with deep\nlearning-based methods achieving state-of-the-art results in several\nbenchmarks. However, the lack of standardized tools for training, testing, and\nevaluating new methods makes the comparison of methods difficult. To address\nthis, we introduce the Medical Imaging Segmentation Toolkit (MIST), a simple,\nmodular, and end-to-end medical imaging segmentation framework designed to\nfacilitate consistent training, testing, and evaluation of deep learning-based\nmedical imaging segmentation methods. MIST standardizes data analysis,\npreprocessing, and evaluation pipelines, accommodating multiple architectures\nand loss functions. This standardization ensures reproducible and fair\ncomparisons across different methods. We detail MIST's data format\nrequirements, pipelines, and auxiliary features and demonstrate its efficacy\nusing the BraTS Adult Glioma Post-Treatment Challenge dataset. Our results\nhighlight MIST's ability to produce accurate segmentation masks and its\nscalability across multiple GPUs, showcasing its potential as a powerful tool\nfor future medical imaging research and development.",
      "tldr_zh": "本研究引入了MIST框架，这是一个简单且可扩展的端到端3D医疗图像分割工具，旨在解决deep learning-based方法在训练、测试和评估方面的标准化缺失问题。\nMIST标准化了数据分析、预处理和评估管道，支持多种架构和损失函数，确保方法的可重复性和公平比较。\n实验使用BraTS Adult Glioma Post-Treatment Challenge数据集证明了MIST的效能，能够生成准确的分割掩码，并在多个GPU上实现高效扩展，为医疗图像研究提供了一个强大的工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "Submitted to BraTS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21343v2",
      "published_date": "2024-07-31 05:17:31 UTC",
      "updated_date": "2024-11-18 17:59:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:52:26.720744"
    },
    {
      "arxiv_id": "2407.21338v1",
      "title": "Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "David Valencia",
        "Henry Williams",
        "Yuning Xing",
        "Trevor Gee",
        "Minas Liarokapis",
        "Bruce A. MacDonald"
      ],
      "abstract": "Reinforcement Learning (RL) has been widely used to solve tasks where the\nenvironment consistently provides a dense reward value. However, in real-world\nscenarios, rewards can often be poorly defined or sparse. Auxiliary signals are\nindispensable for discovering efficient exploration strategies and aiding the\nlearning process. In this work, inspired by intrinsic motivation theory, we\npostulate that the intrinsic stimuli of novelty and surprise can assist in\nimproving exploration in complex, sparsely rewarded environments. We introduce\na novel sample-efficient method able to learn directly from pixels, an\nimage-based extension of TD3 with an autoencoder called \\textit{NaSA-TD3}. The\nexperiments demonstrate that NaSA-TD3 is easy to train and an efficient method\nfor tackling complex continuous-control robotic tasks, both in simulated\nenvironments and real-world settings. NaSA-TD3 outperforms existing\nstate-of-the-art RL image-based methods in terms of final performance without\nrequiring pre-trained models or human demonstrations.",
      "tldr_zh": "该论文探讨了强化学习（Reinforcement Learning, RL）在稀疏奖励环境中的挑战，提出使用内在动机理论中的新奇性和惊喜作为辅助信号来提升探索效率。作者引入了一种新方法NaSA-TD3，这是一种基于图像的TD3扩展，结合自编码器（autoencoder），能够从像素直接学习并处理复杂连续控制机器人任务。实验结果显示，NaSA-TD3在模拟和真实环境中表现出色，其最终性能优于现有最先进RL图像-based方法，且无需预训练模型或人类演示。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21338v1",
      "published_date": "2024-07-31 05:11:06 UTC",
      "updated_date": "2024-07-31 05:11:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:52:39.852443"
    },
    {
      "arxiv_id": "2407.21320v2",
      "title": "MetaOpenFOAM: an LLM-based multi-agent framework for CFD",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Chen",
        "Xu Zhu",
        "Hua Zhou",
        "Zhuyin Ren"
      ],
      "abstract": "Remarkable progress has been made in automated problem solving through\nsocieties of agents based on large language models (LLMs). Computational fluid\ndynamics (CFD), as a complex problem, presents unique challenges in automated\nsimulations that require sophisticated solutions. MetaOpenFOAM, as a novel\nmulti-agent collaborations framework, aims to complete CFD simulation tasks\nwith only natural language as input. These simulation tasks include mesh\npre-processing, simulation and so on. MetaOpenFOAM harnesses the power of\nMetaGPT's assembly line paradigm, which assigns diverse roles to various\nagents, efficiently breaking down complex CFD tasks into manageable subtasks.\nLangchain further complements MetaOpenFOAM by integrating Retrieval-Augmented\nGeneration (RAG) technology, which enhances the framework's ability by\nintegrating a searchable database of OpenFOAM tutorials for LLMs. Tests on a\nbenchmark for natural language-based CFD solver, consisting of eight CFD\nsimulation tasks, have shown that MetaOpenFOAM achieved a high pass rate per\ntest (85%), with each test case costing only $0.22 on average. The eight CFD\nsimulation tasks encompass a range of multidimensional flow problems, covering\ncompressible and incompressible flows with different physical processes. This\ndemonstrates the capability to automate CFD simulations using only natural\nlanguage input, iteratively correcting errors to achieve the desired\nsimulations. An ablation study was conducted to verify the necessity of each\ncomponent in the multi-agent system and the RAG technology. A sensitivity study\non the randomness of LLM showed that LLM with low randomness can obtain more\nstable and accurate results. Additionally, MetaOpenFOAM owns the ability to\nidentify and modify key parameters in user requirements, and excels in\ncorrecting bugs when failure match occur,which demonstrates the generalization\nof MetaOpenFOAM.",
      "tldr_zh": "该研究提出 MetaOpenFOAM，一种基于 LLM 的多智能体框架，用于通过自然语言输入自动化 CFD 模拟任务，包括网格预处理和模拟等。框架借鉴 MetaGPT 的装配线范式分配代理角色，并整合 Langchain 的 RAG 技术，利用 OpenFOAM 教程数据库分解复杂任务并提升准确性。在基准测试中，MetaOpenFOAM 在8个多维流体问题任务上实现85%的通过率，平均成本仅0.22美元，并通过消融研究和敏感性分析验证了各组件的必要性及其鲁棒性。",
      "categories": [
        "cs.AI",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages,11 figures, 11 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.21320v2",
      "published_date": "2024-07-31 04:01:08 UTC",
      "updated_date": "2024-08-07 04:34:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:52:51.473285"
    },
    {
      "arxiv_id": "2407.21319v1",
      "title": "Big Cooperative Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yulai Cong"
      ],
      "abstract": "Cooperation plays a pivotal role in the evolution of human intelligence;\nmoreover, it also underlies the recent revolutionary advancement of artificial\nintelligence (AI) that is driven by foundation models. Specifically, we reveal\nthat the training of foundation models can be interpreted as a form of big\ncooperative learning (\\textit{abbr.} big learning), where massive learning\nindividuals/tasks \\emph{cooperate} to approach the unique essence of data from\ndiverse perspectives of data prediction, leveraging a universal model. The\npresented big learning therefore unifies most training objectives of foundation\nmodels within a consistent framework, where their underlying assumptions are\nexposed simultaneously. We design tailored simulations to demonstrate the\nprinciple of big learning, based on which we provide learning-perspective\njustifications for the successes of foundation models, with interesting\nside-products. Furthermore, we reveal that big learning is a new dimension for\nupgrading conventional machine learning paradigms, valuable for endowing\nreinvigorations to associated applications; as an illustrative example, we\npropose the BigLearn-GAN, which is a novel adversarially-trained foundation\nmodel with versatile data sampling capabilities. Code is available at\n\\texttt{https://github.com/YulaiCong/BigCooperativeLearning}.",
      "tldr_zh": "本论文提出“大合作学习”（big cooperative learning）框架，将基础模型（foundation models）的训练视为大量学习个体/任务合作使用通用模型，从不同视角预测数据，从而统一了各种训练目标并暴露其潜在假设。该框架揭示合作在人类智能和AI进步中的关键作用，并通过设计定制模拟实验，提供学习视角的解释，证明了基础模型成功的原理及其副产品。此外，论文展示了大合作学习作为升级传统机器学习范式的全新维度，并提出BigLearn-GAN作为示例，该模型具备多功能数据采样能力，可增强相关应用。代码已在GitHub上开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21319v1",
      "published_date": "2024-07-31 03:59:14 UTC",
      "updated_date": "2024-07-31 03:59:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:53:05.425864"
    },
    {
      "arxiv_id": "2407.21315v4",
      "title": "Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances",
      "title_zh": "翻译失败",
      "authors": [
        "Zehui Wu",
        "Ziwei Gong",
        "Lin Ai",
        "Pengyuan Shi",
        "Kaan Donbekci",
        "Julia Hirschberg"
      ],
      "abstract": "Emotion recognition in speech is a challenging multimodal task that requires\nunderstanding both verbal content and vocal nuances. This paper introduces a\nnovel approach to emotion detection using Large Language Models (LLMs), which\nhave demonstrated exceptional capabilities in natural language understanding.\nTo overcome the inherent limitation of LLMs in processing audio inputs, we\npropose SpeechCueLLM, a method that translates speech characteristics into\nnatural language descriptions, allowing LLMs to perform multimodal emotion\nanalysis via text prompts without any architectural changes. Our method is\nminimal yet impactful, outperforming baseline models that require structural\nmodifications. We evaluate SpeechCueLLM on two datasets: IEMOCAP and MELD,\nshowing significant improvements in emotion recognition accuracy, particularly\nfor high-quality audio data. We also explore the effectiveness of various\nfeature representations and fine-tuning strategies for different LLMs. Our\nexperiments demonstrate that incorporating speech descriptions yields a more\nthan 2% increase in the average weighted F1 score on IEMOCAP (from 70.111% to\n72.596%).",
      "tldr_zh": "这篇论文提出了一种名为 SpeechCueLLM 的方法，利用 Large Language Models (LLMs) 通过将语音特征转化为自然语言描述，来提升语音情感识别的准确性，从而克服 LLMs 处理音频输入的局限。\n该方法无需对模型架构进行任何修改，仅通过文本提示实现多模态分析，并在 IEMOCAP 和 MELD 数据集上表现出色，特别是针对高质量音频。\n实验结果显示，SpeechCueLLM 使 IEMOCAP 的平均加权 F1 分数从 70.111% 提高到 72.596%，比基线模型提升超过 2%，并探讨了不同特征表示和微调策略的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21315v4",
      "published_date": "2024-07-31 03:53:14 UTC",
      "updated_date": "2024-12-23 12:35:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:53:19.960837"
    },
    {
      "arxiv_id": "2407.21311v1",
      "title": "EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Ali Abedi",
        "Q. M. Jonathan Wu",
        "Ning Zhang",
        "Farhad Pourpanah"
      ],
      "abstract": "Unsupervised domain adaptation (UDA) aims to mitigate the domain shift issue,\nwhere the distribution of training (source) data differs from that of testing\n(target) data. Many models have been developed to tackle this problem, and\nrecently vision transformers (ViTs) have shown promising results. However, the\ncomplexity and large number of trainable parameters of ViTs restrict their\ndeployment in practical applications. This underscores the need for an\nefficient model that not only reduces trainable parameters but also allows for\nadjustable complexity based on specific needs while delivering comparable\nperformance. To achieve this, in this paper we introduce an Efficient\nUnsupervised Domain Adaptation (EUDA) framework. EUDA employs the DINOv2, which\nis a self-supervised ViT, as a feature extractor followed by a simplified\nbottleneck of fully connected layers to refine features for enhanced domain\nadaptation. Additionally, EUDA employs the synergistic domain alignment loss\n(SDAL), which integrates cross-entropy (CE) and maximum mean discrepancy (MMD)\nlosses, to balance adaptation by minimizing classification errors in the source\ndomain while aligning the source and target domain distributions. The\nexperimental results indicate the effectiveness of EUDA in producing comparable\nresults as compared with other state-of-the-art methods in domain adaptation\nwith significantly fewer trainable parameters, between 42% to 99.7% fewer. This\nshowcases the ability to train the model in a resource-limited environment. The\ncode of the model is available at: https://github.com/A-Abedi/EUDA.",
      "tldr_zh": "该论文提出了一种高效的无监督域适应 (UDA) 框架 EUDA，以解决源域和目标域分布差异问题，同时减少 Vision Transformer (ViTs) 的参数量。EUDA 利用自监督模型 DINOv2 作为特征提取器，结合简化的瓶颈层和协同域对齐损失 (SDAL)，后者整合交叉熵 (CE) 和最大均值差异 (MMD) 损失，以最小化源域分类错误并对齐域分布。实验结果表明，EUDA 与最先进方法性能相当，但参数减少 42% 到 99.7%，使其适用于资源有限的环境。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.21311v1",
      "published_date": "2024-07-31 03:29:28 UTC",
      "updated_date": "2024-07-31 03:29:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:53:31.445916"
    },
    {
      "arxiv_id": "2407.21300v4",
      "title": "SAKR: Enhancing Retrieval-Augmented Generation via Streaming Algorithm and K-Means Clustering",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyu Kang",
        "Yuzhou Zhu",
        "Yukun Zhong",
        "Ke Wang"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has achieved significant success in\ninformation retrieval to assist large language models LLMs because it builds an\nexternal knowledge database. However, it also has many problems, it consumes a\nlot of memory because of the enormous database, and it cannot update the\nestablished index database in time when confronted with massive streaming data.\nTo reduce the memory required for building the database and maintain accuracy\nsimultaneously, we proposed a new approach integrating a streaming algorithm\nwith k-means clustering into RAG. Our approach applied a streaming algorithm to\nupdate the index dynamically and reduce memory consumption. Additionally, the\nk-means algorithm clusters highly similar documents, and the query time would\nbe shortened. We conducted comparative experiments on four methods, and the\nresults indicated that RAG with streaming algorithm and k-means clusters\noutperforms traditional RAG in accuracy and memory, particularly when dealing\nwith large-scale streaming data.",
      "tldr_zh": "该论文提出 SAKR 方法，以提升 Retrieval-Augmented Generation (RAG) 的性能，针对其高内存消耗和无法及时更新索引的问题。SAKR 通过整合 streaming algorithm 动态更新索引、减少内存占用，以及使用 K-Means clustering 聚类相似文档来缩短查询时间。实验结果显示，SAKR 在四种方法比较中，准确性和内存效率均优于传统 RAG，尤其在处理大规模流式数据时表现出色。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21300v4",
      "published_date": "2024-07-31 03:00:59 UTC",
      "updated_date": "2025-05-15 03:22:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:53:41.502805"
    },
    {
      "arxiv_id": "2407.21299v1",
      "title": "Who should I trust? A Visual Analytics Approach for Comparing Net Load Forecasting Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kaustav Bhattacharjee",
        "Soumya Kundu",
        "Indrasis Chakraborty",
        "Aritra Dasgupta"
      ],
      "abstract": "Net load forecasting is crucial for energy planning and facilitating informed\ndecision-making regarding trade and load distributions. However, evaluating\nforecasting models' performance against benchmark models remains challenging,\nthereby impeding experts' trust in the model's performance. In this context,\nthere is a demand for technological interventions that allow scientists to\ncompare models across various timeframes and solar penetration levels. This\npaper introduces a visual analytics-based application designed to compare the\nperformance of deep-learning-based net load forecasting models with other\nmodels for probabilistic net load forecasting. This application employs\ncarefully selected visual analytic interventions, enabling users to discern\ndifferences in model performance across different solar penetration levels,\ndataset resolutions, and hours of the day over multiple months. We also present\nobservations made using our application through a case study, demonstrating the\neffectiveness of visualizations in aiding scientists in making informed\ndecisions and enhancing trust in net load forecasting models.",
      "tldr_zh": "这篇论文探讨了净负荷预测（net load forecasting）在能源规划中的关键作用，以及评估模型性能时面临的挑战，导致专家对模型信任不足。作者提出了一种基于视觉分析（visual analytics）的应用，用于比较深度学习-based 净负荷预测模型与其他模型的表现，该应用允许用户通过可视化手段分析不同太阳能渗透水平、数据集分辨率和时间段的差异。通过案例研究，论文展示了这一工具如何帮助科学家做出 informed decisions，并提升对净负荷预测模型的信任。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SP",
        "eess.SY"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted for publication in the proceedings of 2025 IEEE PES Grid\n  Edge Technologies Conference & Exposition (Grid Edge)",
      "pdf_url": "http://arxiv.org/pdf/2407.21299v1",
      "published_date": "2024-07-31 02:57:21 UTC",
      "updated_date": "2024-07-31 02:57:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:53:53.989412"
    },
    {
      "arxiv_id": "2407.21298v1",
      "title": "A Vectorization Method Induced By Maximal Margin Classification For Persistent Diagrams",
      "title_zh": "翻译失败",
      "authors": [
        "An Wu",
        "Yu Pan",
        "Fuqi Zhou",
        "Jinghui Yan",
        "Chuanlu Liu"
      ],
      "abstract": "Persistent homology is an effective method for extracting topological\ninformation, represented as persistent diagrams, of spatial structure data.\nHence it is well-suited for the study of protein structures. Attempts to\nincorporate Persistent homology in machine learning methods of protein function\nprediction have resulted in several techniques for vectorizing persistent\ndiagrams. However, current vectorization methods are excessively artificial and\ncannot ensure the effective utilization of information or the rationality of\nthe methods. To address this problem, we propose a more geometrical\nvectorization method of persistent diagrams based on maximal margin\nclassification for Banach space, and additionaly propose a framework that\nutilizes topological data analysis to identify proteins with specific\nfunctions. We evaluated our vectorization method using a binary classification\ntask on proteins and compared it with the statistical methods that exhibit the\nbest performance among thirteen commonly used vectorization methods. The\nexperimental results indicate that our approach surpasses the statistical\nmethods in both robustness and precision.",
      "tldr_zh": "这篇论文针对持久图（persistent diagrams）的向量化问题，提出了一种基于最大边际分类（maximal margin classification）的几何方法，适用于Banach空间，以更有效地利用拓扑信息。作者同时构建了一个框架，利用拓扑数据分析（topological data analysis）来识别具有特定功能的蛋白质。实验结果显示，在蛋白质二元分类任务中，该方法在鲁棒性和精确性上优于13种常用统计方法中的最佳表现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21298v1",
      "published_date": "2024-07-31 02:55:01 UTC",
      "updated_date": "2024-07-31 02:55:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:54:05.708315"
    },
    {
      "arxiv_id": "2408.02677v1",
      "title": "Patient-centered data science: an integrative framework for evaluating and predicting clinical outcomes in the digital health era",
      "title_zh": "以患者为中心的数据科学：一个用于评估和预测数字健康时代临床结果的整合框架",
      "authors": [
        "Mohsen Amoei",
        "Dan Poenaru"
      ],
      "abstract": "This study proposes a novel, integrative framework for patient-centered data\nscience in the digital health era. We developed a multidimensional model that\ncombines traditional clinical data with patient-reported outcomes, social\ndeterminants of health, and multi-omic data to create comprehensive digital\npatient representations. Our framework employs a multi-agent artificial\nintelligence approach, utilizing various machine learning techniques including\nlarge language models, to analyze complex, longitudinal datasets. The model\naims to optimize multiple patient outcomes simultaneously while addressing\nbiases and ensuring generalizability. We demonstrate how this framework can be\nimplemented to create a learning healthcare system that continuously refines\nstrategies for optimal patient care. This approach has the potential to\nsignificantly improve the translation of digital health innovations into\nreal-world clinical benefits, addressing current limitations in AI-driven\nhealthcare models.",
      "tldr_zh": "本研究提出一个整合框架，用于患者中心的数据科学，旨在评估和预测数字健康时代下的临床结果。\n该框架结合传统临床数据、患者报告结果、社会决定因素和多组学数据，形成全面的数字患者表示，并采用多智能体人工智能方法，包括 large language models 等机器学习技术来分析复杂纵向数据集。\n框架的目标是同时优化多个患者结果、处理偏差并确保模型的泛化性，从而构建一个学习型医疗系统，不断改进患者护理策略。\n这种方法有望显著提升数字健康创新向现实临床益处的转化，解决当前 AI 驱动医疗模型的限制。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.02677v1",
      "published_date": "2024-07-31 02:36:17 UTC",
      "updated_date": "2024-07-31 02:36:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:54:18.239397"
    },
    {
      "arxiv_id": "2407.21293v1",
      "title": "SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Peiru Zheng",
        "Yun Zhao",
        "Zhan Gong",
        "Hong Zhu",
        "Shaohua Wu"
      ],
      "abstract": "Many fields could benefit from the rapid development of the large language\nmodels (LLMs). The end-to-end autonomous driving (e2eAD) is one of the\ntypically fields facing new opportunities as the LLMs have supported more and\nmore modalities. Here, by utilizing vision-language model (VLM), we proposed an\ne2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided\ninto four stages, which are perception, prediction, planning, and behavior.\nEach stage consists of several visual question answering (VQA) pairs and VQA\npairs interconnect with each other constructing a graph called Graph VQA\n(GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our\nmethod could achieve e2e driving with language. In our method, vision\ntransformers (ViT) models are employed to process nuScenes visual data, while\nVLM are utilized to interpret and reason about the information extracted from\nthe visual inputs. In the perception stage, the system identifies and\nclassifies objects from the driving environment. The prediction stage involves\nforecasting the potential movements of these objects. The planning stage\nutilizes the gathered information to develop a driving strategy, ensuring the\nsafety and efficiency of the autonomous vehicle. Finally, the behavior stage\ntranslates the planned actions into executable commands for the vehicle. Our\nexperiments demonstrate that SimpleLLM4AD achieves competitive performance in\ncomplex driving scenarios.",
      "tldr_zh": "该论文提出了一种名为 SimpleLLM4AD 的端到端自动驾驶 (e2eAD) 方法，利用视觉语言模型 (VLM) 结合 Graph VQA (GVQA) 来处理驾驶任务。方法将 e2eAD 分为感知、预测、规划和行为四个阶段，每个阶段由相互连接的视觉问答 (VQA) 对组成，通过 VLM 逐阶段推理以实现安全高效的驾驶策略，并使用 Vision Transformers (ViT) 处理 nuScenes 视觉数据。实验结果显示，SimpleLLM4AD 在复杂驾驶场景中取得了竞争性的性能表现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.21293v1",
      "published_date": "2024-07-31 02:35:33 UTC",
      "updated_date": "2024-07-31 02:35:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:54:30.470039"
    },
    {
      "arxiv_id": "2408.07825v1",
      "title": "SSRFlow: Semantic-aware Fusion with Spatial Temporal Re-embedding for Real-world Scene Flow",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyang Lu",
        "Qinghan Chen",
        "Zhimin Yuan",
        "Ming Cheng"
      ],
      "abstract": "Scene flow, which provides the 3D motion field of the first frame from two\nconsecutive point clouds, is vital for dynamic scene perception. However,\ncontemporary scene flow methods face three major challenges. Firstly, they lack\nglobal flow embedding or only consider the context of individual point clouds\nbefore embedding, leading to embedded points struggling to perceive the\nconsistent semantic relationship of another frame. To address this issue, we\npropose a novel approach called Dual Cross Attentive (DCA) for the latent\nfusion and alignment between two frames based on semantic contexts. This is\nthen integrated into Global Fusion Flow Embedding (GF) to initialize flow\nembedding based on global correlations in both contextual and Euclidean spaces.\nSecondly, deformations exist in non-rigid objects after the warping layer,\nwhich distorts the spatiotemporal relation between the consecutive frames. For\na more precise estimation of residual flow at next-level, the Spatial Temporal\nRe-embedding (STR) module is devised to update the point sequence features at\ncurrent-level. Lastly, poor generalization is often observed due to the\nsignificant domain gap between synthetic and LiDAR-scanned datasets. We\nleverage novel domain adaptive losses to effectively bridge the gap of motion\ninference from synthetic to real-world. Experiments demonstrate that our\napproach achieves state-of-the-art (SOTA) performance across various datasets,\nwith particularly outstanding results in real-world LiDAR-scanned situations.\nOur code will be released upon publication.",
      "tldr_zh": "本研究提出SSRFlow框架，用于处理真实世界场景流（Scene flow）估计中的关键挑战，包括全局流嵌入不足、非刚性物体变形以及合成到真实数据集的领域差距问题。框架引入Dual Cross Attentive (DCA)模块和Global Fusion Flow Embedding (GF)来实现基于语义上下文的两帧融合与对齐，同时开发Spatial Temporal Re-embedding (STR)模块更新点序列特征，以精确估计残差流。此外，通过新型领域自适应损失桥接数据集差距，实验结果显示SSRFlow在多种数据集上达到最先进（SOTA）性能，尤其在真实LiDAR扫描场景中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages,12 figures. arXiv admin note: substantial text overlap with\n  arXiv:2403.07032",
      "pdf_url": "http://arxiv.org/pdf/2408.07825v1",
      "published_date": "2024-07-31 02:28:40 UTC",
      "updated_date": "2024-07-31 02:28:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:54:42.401202"
    },
    {
      "arxiv_id": "2408.08315v1",
      "title": "Segment Anything for Videos: A Systematic Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Chunhui Zhang",
        "Yawen Cui",
        "Weilin Lin",
        "Guanjie Huang",
        "Yan Rong",
        "Li Liu",
        "Shiguang Shan"
      ],
      "abstract": "The recent wave of foundation models has witnessed tremendous success in\ncomputer vision (CV) and beyond, with the segment anything model (SAM) having\nsparked a passion for exploring task-agnostic visual foundation models.\nEmpowered by its remarkable zero-shot generalization, SAM is currently\nchallenging numerous traditional paradigms in CV, delivering extraordinary\nperformance not only in various image segmentation and multi-modal segmentation\n(\\eg, text-to-mask) tasks, but also in the video domain. Additionally, the\nlatest released SAM 2 is once again sparking research enthusiasm in the realm\nof promptable visual segmentation for both images and videos. However, existing\nsurveys mainly focus on SAM in various image processing tasks, a comprehensive\nand in-depth review in the video domain is notably absent. To address this gap,\nthis work conducts a systematic review on SAM for videos in the era of\nfoundation models. As the first to review the progress of SAM for videos, this\nwork focuses on its applications to various tasks by discussing its recent\nadvances, and innovation opportunities of developing foundation models on broad\napplications. We begin with a brief introduction to the background of SAM and\nvideo-related research domains. Subsequently, we present a systematic taxonomy\nthat categorizes existing methods into three key areas: video understanding,\nvideo generation, and video editing, analyzing and summarizing their advantages\nand limitations. Furthermore, comparative results of SAM-based and current\nstate-of-the-art methods on representative benchmarks, as well as insightful\nanalysis are offered. Finally, we discuss the challenges faced by current\nresearch and envision several future research directions in the field of SAM\nfor video and beyond.",
      "tldr_zh": "这篇论文对Segment Anything Model (SAM) 在视频领域的应用进行了系统性调查，填补了现有文献主要聚焦图像任务的空白。作者首先介绍了SAM的背景及其零-shot generalization能力，随后将相关方法分类为视频理解、视频生成和视频编辑三大领域，并分析了它们的优势、局限性及与现有最先进方法的比较结果。论文还讨论了当前面临的挑战，并展望了开发视频基础模型的创新机会，为未来研究提供了宝贵指导。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "https://github.com/983632847/SAM-for-Videos",
      "pdf_url": "http://arxiv.org/pdf/2408.08315v1",
      "published_date": "2024-07-31 02:24:53 UTC",
      "updated_date": "2024-07-31 02:24:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:54:53.666360"
    },
    {
      "arxiv_id": "2407.21284v1",
      "title": "Robust Box Prompt based SAM for Medical Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhao Huang",
        "Xin Yang",
        "Han Zhou",
        "Yan Cao",
        "Haoran Dou",
        "Fajin Dong",
        "Dong Ni"
      ],
      "abstract": "The Segment Anything Model (SAM) can achieve satisfactory segmentation\nperformance under high-quality box prompts. However, SAM's robustness is\ncompromised by the decline in box quality, limiting its practicality in\nclinical reality. In this study, we propose a novel Robust Box prompt based SAM\n(\\textbf{RoBox-SAM}) to ensure SAM's segmentation performance under prompts\nwith different qualities. Our contribution is three-fold. First, we propose a\nprompt refinement module to implicitly perceive the potential targets, and\noutput the offsets to directly transform the low-quality box prompt into a\nhigh-quality one. We then provide an online iterative strategy for further\nprompt refinement. Second, we introduce a prompt enhancement module to\nautomatically generate point prompts to assist the box-promptable segmentation\neffectively. Last, we build a self-information extractor to encode the prior\ninformation from the input image. These features can optimize the image\nembeddings and attention calculation, thus, the robustness of SAM can be\nfurther enhanced. Extensive experiments on the large medical segmentation\ndataset including 99,299 images, 5 modalities, and 25 organs/targets validated\nthe efficacy of our proposed RoBox-SAM.",
      "tldr_zh": "本研究针对 Segment Anything Model (SAM) 在低质量 box prompts 下鲁棒性不足的问题，提出了一种新型 Robust Box prompt based SAM (RoBox-SAM)，旨在确保模型在不同提示质量下实现稳定的医疗图像分割性能。主要贡献包括：一个提示精炼模块，通过感知潜在目标并输出偏移量来优化低质量 box prompts，并结合在线迭代策略；一个提示增强模块，自动生成点提示以辅助分割；以及一个自信息提取器，从输入图像中编码先验信息来优化图像嵌入和注意力计算，从而提升整体鲁棒性。在包含99,299张图像、5种模态和25个器官/目标的大型医学数据集上进行的广泛实验验证了RoBox-SAM的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by MICCAI MLMI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21284v1",
      "published_date": "2024-07-31 02:16:28 UTC",
      "updated_date": "2024-07-31 02:16:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:55:06.606892"
    },
    {
      "arxiv_id": "2407.21281v1",
      "title": "Unlocking the Potential of Binding Corporate Rules (BCRs) in Health Data Transfers",
      "title_zh": "解锁 Binding Corporate Rules (BCRs) 在健康数据传输中的潜力",
      "authors": [
        "Marcelo Corrales Compagnucci",
        "Mark Fenwick",
        "Helena Haapio"
      ],
      "abstract": "This chapter explores the essential role of Binding Corporate Rules (BCRs) in\nmanaging and facilitating secure health data transfers within corporate groups\nunder the EU General Data Protection Regulation (GDPR). BCRs are tailored to\nensure compliance with the GDPR and similar international data protection laws,\npresenting a flexible mechanism for transferring sensitive health and genomic\ndata. The chapter situates BCRs within the broader spectrum of the GDPR\ninternational data transfer mechanisms, addressing the unique challenges posed\nby the sensitive nature of health data and the increased adoption of AI\ntechnologies. The European Data Protection Board (EDPB) Recommendations 1/2022\non BCRs, issued following the Schrems II decision, are critically analyzed,\nhighlighting their stringent requirements and the need for a balanced approach\nthat prioritizes data protection and an AI governance framework. The chapter\noutlines the BCR approval process, stressing the importance of streamlining\nthis process to encourage broader adoption. It underscores the necessity of a\nmultidisciplinary approach in developing BCRs, incorporating recently adopted\ninternational standards and frameworks, which offer valuable guidance for\norganizations to build trustworthy AI management systems. They guarantee the\nethical development, deployment, and operation of AI, which is essential for\nits successful integration and the broader digital transformation. In\nconclusion, BCRs are positioned as essential tools for secure health data\nmanagement, fostering transparency, accountability, and collaboration across\ninternational borders. The chapter calls for proactive measures to incentivize\nBCR adoption, streamline approval processes, and promote more innovative\napproaches, ensuring BCRs remain a robust mechanism for global data protection\nand compliance.",
      "tldr_zh": "本章探讨了 Binding Corporate Rules (BCRs) 在欧盟 General Data Protection Regulation (GDPR) 框架下，促进企业集团内健康数据安全传输的潜力。BCRs 作为一种灵活机制，能够应对健康数据敏感性和 AI 技术采用带来的挑战，并通过分析 European Data Protection Board (EDPB) Recommendations 1/2022，强调了严格要求与平衡数据保护的重要性。研究呼吁简化 BCRs 审批过程、采用多学科方法整合国际标准，以提升透明性、问责性和跨境合作，最终将 BCRs 定位为全球数据保护的关键工具。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21281v1",
      "published_date": "2024-07-31 02:09:52 UTC",
      "updated_date": "2024-07-31 02:09:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:55:29.932246"
    },
    {
      "arxiv_id": "2407.21276v3",
      "title": "Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented Generation",
      "title_zh": "多级检索增强生成的知识金字塔构建",
      "authors": [
        "Rubing Chen",
        "Xulu Zhang",
        "Jiaxin Wu",
        "Wenqi Fan",
        "Xiao-Yong Wei",
        "Qing Li"
      ],
      "abstract": "This paper addresses the need for improved precision in existing\nknowledge-enhanced question-answering frameworks, specifically\nRetrieval-Augmented Generation (RAG) methods that primarily focus on enhancing\nrecall. We propose a multi-layer knowledge pyramid approach within the RAG\nframework to achieve a better balance between precision and recall. The\nknowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),\nand chunk-based raw text. We employ cross-layer augmentation techniques for\ncomprehensive knowledge coverage and dynamic updates of the Ontology schema and\ninstances. To ensure compactness, we utilize cross-layer filtering methods for\nknowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall\nmodel for retrieval, starting from the top of the pyramid and progressing down\nuntil a confident answer is obtained. We introduce two benchmarks for\ndomain-specific knowledge retrieval, one in the academic domain and the other\nin the financial domain. The effectiveness of the methods has been validated\nthrough comprehensive experiments by outperforming 19 SOTA methods. An\nencouraging observation is that the proposed method has augmented the GPT-4,\nproviding 395% F1 gain by improving its performance from 0.1636 to 0.8109.",
      "tldr_zh": "这篇论文针对现有 Retrieval-Augmented Generation (RAG) 方法在精确率上的不足，提出了一种多层知识金字塔方法，包括 Ontologies、本体、Knowledge Graphs (KGs) 和基于块的原始文本层，以实现精确率和召回率的平衡。论文引入 PolyRAG 框架，利用跨层增强技术进行知识覆盖和动态更新，以及跨层过滤方法压缩 KGs，确保知识的紧凑性，并采用瀑布模型从金字塔顶层开始检索直至获得可靠答案。在实验中，PolyRAG 超越了 19 个 SOTA 方法，并在两个领域特定基准（学术和金融领域）上显著提升性能，将 GPT-4 的 F1 分数从 0.1636 提高到 0.8109，实现 395% 的提升。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21276v3",
      "published_date": "2024-07-31 01:51:24 UTC",
      "updated_date": "2025-02-21 04:00:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:55:32.805274"
    },
    {
      "arxiv_id": "2407.21275v7",
      "title": "Fi$^2$VTS: Time Series Forecasting Via Capturing Intra- and Inter-Variable Variations in the Frequency Domain",
      "title_zh": "翻译失败",
      "authors": [
        "Rujia Shen",
        "Yang Yang",
        "Yaoxion Lin",
        "Liangliang Liu",
        "Boran Wang",
        "Yi Guan",
        "Jingchi Jiang"
      ],
      "abstract": "Time series forecasting (TSF) plays a crucial role in various applications,\nincluding medical monitoring and crop growth. Despite the advancements in deep\nlearning methods for TSF, their capacity to predict long-term series remains\nconstrained. This limitation arises from the failure to account for both intra-\nand inter-variable variations meanwhile. To mitigate this challenge, we\nintroduce the Fi$^2$VBlock, which leverages a \\textbf{F}requency domain\nperspective to capture \\textbf{i}ntra- and \\textbf{i}nter-variable\n\\textbf{V}ariations. After transforming into the frequency domain via the\nFrequency Transform Module, the Frequency Cross Attention between the real and\nimaginary parts is designed to obtain enhanced frequency representations and\ncapture intra-variable variations. Furthermore, Inception blocks are employed\nto integrate information, thus capturing correlations across different\nvariables. Our backbone network, Fi$^2$VTS, employs a residual architecture by\nconcatenating multiple Fi$^2$VBlocks, thereby preventing degradation issues.\nTheoretically, we demonstrate that Fi$^2$VTS achieves a substantial reduction\nin both time and memory complexity, decreasing from $\\mathcal{O}(L^2)$ to\n$\\mathcal{O}(L)$ per Fi$^2$VBlock computation. Empirical evaluations reveal\nthat Fi$^2$VTS outperforms other baselines on two benchmark datasets. The\nimplementation code is accessible at \\url{https://github.com/HITshenrj/Fi2VTS}.",
      "tldr_zh": "本研究针对时间序列预测 (TSF) 的挑战，提出 Fi$^2$VTS 模型，通过捕捉变量内部 (intra-variable variations) 和变量间 (inter-variable variations) 的变化来提升长期序列预测能力。模型的核心组件 Fi$^2$VBlock 利用 Frequency Transform Module 将数据转换为频率域，然后通过 Frequency Cross Attention 处理内部变化，并采用 Inception blocks 整合变量间相关性，同时构建 residual architecture 以避免退化问题。理论分析显示，Fi$^2$VTS 将每个 Fi$^2$VBlock 的时间和内存复杂度从 O(L^2) 降低到 O(L)，并在两个基准数据集上超越基线模型，代码已在 GitHub 上公开。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "There was an error in the experimental results; we mistakenly took\n  the result of our method on the ETTh2 dataset as the result of our method on\n  the ETTh1 dataset",
      "pdf_url": "http://arxiv.org/pdf/2407.21275v7",
      "published_date": "2024-07-31 01:50:39 UTC",
      "updated_date": "2024-11-03 04:17:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:55:42.451965"
    },
    {
      "arxiv_id": "2407.21273v1",
      "title": "Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net",
      "title_zh": "翻译失败",
      "authors": [
        "Rohini Banerjee",
        "Cecilia G. Morales",
        "Artur Dubrawski"
      ],
      "abstract": "Efficient intravascular access in trauma and critical care significantly\nimpacts patient outcomes. However, the availability of skilled medical\npersonnel in austere environments is often limited. Autonomous robotic\nultrasound systems can aid in needle insertion for medication delivery and\nsupport non-experts in such tasks. Despite advances in autonomous needle\ninsertion, inaccuracies in vessel segmentation predictions pose risks.\nUnderstanding the uncertainty of predictive models in ultrasound imaging is\ncrucial for assessing their reliability. We introduce MSU-Net, a novel\nmultistage approach for training an ensemble of U-Nets to yield accurate\nultrasound image segmentation maps. We demonstrate substantial improvements,\n18.1% over a single Monte Carlo U-Net, enhancing uncertainty evaluations, model\ntransparency, and trustworthiness. By highlighting areas of model certainty,\nMSU-Net can guide safe needle insertions, empowering non-experts to accomplish\nsuch tasks.",
      "tldr_zh": "该研究针对超声图像分割中的不确定性问题，引入了MSU-Net，一种多阶段训练U-Net集成的方法，以提升预测模型的可靠性和透明度。相比单一Monte Carlo U-Net，MSU-Net在性能上提高了18.1%，显著改善了不确定性评估。最终，该框架可通过突出模型确定性区域，指导安全的针刺插入，帮助非专家在创伤和重症监护环境中实现高效的血管内通路。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for the 5th International Workshop of Advances in\n  Simplifying Medical UltraSound (ASMUS), held in conjunction with MICCAI 2024,\n  the 27th International Conference on Medical Image Computing and Computer\n  Assisted Intervention",
      "pdf_url": "http://arxiv.org/pdf/2407.21273v1",
      "published_date": "2024-07-31 01:36:47 UTC",
      "updated_date": "2024-07-31 01:36:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:55:55.906856"
    },
    {
      "arxiv_id": "2407.21272v1",
      "title": "Automated Quantification of Hyperreflective Foci in SD-OCT With Diabetic Retinopathy",
      "title_zh": "翻译失败",
      "authors": [
        "Idowu Paul Okuwobi",
        "Zexuan Ji",
        "Wen Fan",
        "Songtao Yuan",
        "Loza Bekalo",
        "Qiang Chen"
      ],
      "abstract": "The presence of hyperreflective foci (HFs) is related to retinal disease\nprogression, and the quantity has proven to be a prognostic factor of visual\nand anatomical outcome in various retinal diseases. However, lack of efficient\nquantitative tools for evaluating the HFs has deprived ophthalmologist of\nassessing the volume of HFs. For this reason, we propose an automated\nquantification algorithm to segment and quantify HFs in spectral domain optical\ncoherence tomography (SD-OCT). The proposed algorithm consists of two parallel\nprocesses namely: region of interest (ROI) generation and HFs estimation. To\ngenerate the ROI, we use morphological reconstruction to obtain the\nreconstructed image and histogram constructed for data distributions and\nclustering. In parallel, we estimate the HFs by extracting the extremal regions\nfrom the connected regions obtained from a component tree. Finally, both the\nROI and the HFs estimation process are merged to obtain the segmented HFs. The\nproposed algorithm was tested on 40 3D SD-OCT volumes from 40 patients\ndiagnosed with non-proliferative diabetic retinopathy (NPDR), proliferative\ndiabetic retinopathy (PDR), and diabetic macular edema (DME). The average dice\nsimilarity coefficient (DSC) and correlation coefficient (r) are 69.70%, 0.99\nfor NPDR, 70.31%, 0.99 for PDR, and 71.30%, 0.99 for DME, respectively. The\nproposed algorithm can provide ophthalmologist with good HFs quantitative\ninformation, such as volume, size, and location of the HFs.",
      "tldr_zh": "这篇论文针对糖尿病视网膜病变（包括 NPDR、PDR 和 DME）中超反射焦点 (HFs) 的量化问题，提出了一种自动化算法，用于在光谱域光学相干断层扫描 (SD-OCT) 图像中分割和量化 HFs，以解决现有工具的不足。算法包括两个并行过程：区域感兴趣 (ROI) 生成（利用形态重建和直方图聚类）以及 HFs 估计（通过组件树提取极值区域），随后合并两者得到精确分割结果。在 40 个 3D SD-OCT 体数据上测试，该算法的平均 Dice 相似系数 (DSC) 分别为 NPDR 的 69.70%、PDR 的 70.31% 和 DME 的 71.30%，相关系数 (r) 均达 0.99。该方法能为眼科医生提供 HFs 的体积、大小和位置等量化信息，提高疾病预后评估的准确性。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "IEEE Journal of Biomedical and Health Informatics, Volume: 24, Issue:\n  4, pp. 1125 - 1136, 2020",
      "pdf_url": "http://arxiv.org/pdf/2407.21272v1",
      "published_date": "2024-07-31 01:33:47 UTC",
      "updated_date": "2024-07-31 01:33:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:56:10.751665"
    },
    {
      "arxiv_id": "2407.21267v1",
      "title": "DEF-oriCORN: efficient 3D scene understanding for robust language-directed manipulation without demonstrations",
      "title_zh": "翻译失败",
      "authors": [
        "Dongwon Son",
        "Sanghyeon Son",
        "Jaehyung Kim",
        "Beomjoon Kim"
      ],
      "abstract": "We present DEF-oriCORN, a framework for language-directed manipulation tasks.\nBy leveraging a novel object-based scene representation and\ndiffusion-model-based state estimation algorithm, our framework enables\nefficient and robust manipulation planning in response to verbal commands, even\nin tightly packed environments with sparse camera views without any\ndemonstrations. Unlike traditional representations, our representation affords\nefficient collision checking and language grounding. Compared to\nstate-of-the-art baselines, our framework achieves superior estimation and\nmotion planning performance from sparse RGB images and zero-shot generalizes to\nreal-world scenarios with diverse materials, including transparent and\nreflective objects, despite being trained exclusively in simulation. Our code\nfor data generation, training, inference, and pre-trained weights are publicly\navailable at: https://sites.google.com/view/def-oricorn/home.",
      "tldr_zh": "我们介绍了 DEF-oriCORN 框架，这是一种高效的 3D 场景理解系统，用于无需演示的语言指导操控任务。框架采用新型的对象-based 场景表示和 diffusion-model-based 状态估计算法，实现快速碰撞检查和语言 grounding，即使在密集环境和稀疏相机视图下也能保持鲁棒性。与现有基线相比，DEF-oriCORN 在稀疏 RGB 图像上表现出优越的估计和运动规划性能，并实现了 zero-shot 泛化到真实世界场景，包括透明和反射物体，尽管仅在模拟环境中训练。代码、数据生成工具和预训练权重已公开可用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21267v1",
      "published_date": "2024-07-31 01:13:25 UTC",
      "updated_date": "2024-07-31 01:13:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:56:19.432749"
    },
    {
      "arxiv_id": "2407.21260v3",
      "title": "Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation",
      "title_zh": "翻译失败",
      "authors": [
        "Taehyun Cho",
        "Seungyub Han",
        "Seokhun Ju",
        "Dohyeong Kim",
        "Kyungjae Lee",
        "Jungwoo Lee"
      ],
      "abstract": "Distributional reinforcement learning improves performance by capturing\nenvironmental stochasticity, but a comprehensive theoretical understanding of\nits effectiveness remains elusive. In addition, the intractable element of the\ninfinite dimensionality of distributions has been overlooked. In this paper, we\npresent a regret analysis of distributional reinforcement learning with general\nvalue function approximation in a finite episodic Markov decision process\nsetting. We first introduce a key notion of $\\textit{Bellman unbiasedness}$\nwhich is essential for exactly learnable and provably efficient distributional\nupdates in an online manner. Among all types of statistical functionals for\nrepresenting infinite-dimensional return distributions, our theoretical results\ndemonstrate that only moment functionals can exactly capture the statistical\ninformation. Secondly, we propose a provably efficient algorithm,\n$\\texttt{SF-LSVI}$, that achieves a tight regret bound of $\\tilde{O}(d_E\nH^{\\frac{3}{2}}\\sqrt{K})$ where $H$ is the horizon, $K$ is the number of\nepisodes, and $d_E$ is the eluder dimension of a function class.",
      "tldr_zh": "本论文探讨了分布强化学习（Distributional Reinforcement Learning）在捕捉环境随机性方面的优势，但强调了其理论理解和无穷维分布问题的挑战。研究引入了关键概念$\\textit{Bellman unbiasedness}$，证明只有$\\textit{moment functionals}$能够精确捕捉回报分布的统计信息，从而实现在线高效学习。其次，论文提出了一种高效算法$\\texttt{SF-LSVI}$，在有限的 episodic Markov decision process 中，实现了遗憾界（regret bound）为$\\tilde{O}(d_E H^{\\frac{3}{2}}\\sqrt{K})$，其中$d_E$为eluder dimension、$H$为时间步数、$K$为episode数。总的来说，该工作为具有一般价值函数逼近（General Value Function Approximation）的分布强化学习提供了可证明的效率保证。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21260v3",
      "published_date": "2024-07-31 00:43:51 UTC",
      "updated_date": "2025-05-13 04:53:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:56:32.722634"
    },
    {
      "arxiv_id": "2407.21252v1",
      "title": "Lifelong Person Search",
      "title_zh": "翻译失败",
      "authors": [
        "Jae-Won Yang",
        "Seungbin Hong",
        "Jae-Young Sim"
      ],
      "abstract": "Person search is the task to localize a query person in gallery datasets of\nscene images. Existing methods have been mainly developed to handle a single\ntarget dataset only, however diverse datasets are continuously given in\npractical applications of person search. In such cases, they suffer from the\ncatastrophic knowledge forgetting in the old datasets when trained on new\ndatasets. In this paper, we first introduce a novel problem of lifelong person\nsearch (LPS) where the model is incrementally trained on the new datasets while\npreserving the knowledge learned in the old datasets. We propose an end-to-end\nLPS framework that facilitates the knowledge distillation to enforce the\nconsistency learning between the old and new models by utilizing the prototype\nfeatures of the foreground persons as well as the hard background proposals in\nthe old domains. Moreover, we also devise the rehearsal-based instance matching\nto further improve the discrimination ability in the old domains by using the\nunlabeled person instances additionally. Experimental results demonstrate that\nthe proposed method achieves significantly superior performance of both the\ndetection and re-identification to preserve the knowledge learned in the old\ndomains compared with the existing methods.",
      "tldr_zh": "本研究引入了终身人搜索(Lifelong Person Search, LPS)问题，旨在解决现有方法在处理连续更新的数据集时出现的灾难性知识遗忘问题。作者提出一个端到端框架，利用知识蒸馏(knowledge distillation)通过前景人物的原型特征和硬背景提案来保持旧模型和新模型的一致性，并结合基于排练的实例匹配(rehearsal-based instance matching)来提升旧域中的区分能力。实验结果显示，该方法在检测和再识别性能上显著优于现有方法，有效保留了旧数据集的知识。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 6 figure",
      "pdf_url": "http://arxiv.org/pdf/2407.21252v1",
      "published_date": "2024-07-31 00:19:22 UTC",
      "updated_date": "2024-07-31 00:19:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:56:42.790107"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 128,
  "processed_papers_count": 128,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T11:57:05.323632"
}
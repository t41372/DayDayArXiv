[
  {
    "arxiv_id": "2408.00197v1",
    "title": "Automated Software Vulnerability Static Code Analysis Using Generative Pre-Trained Transformer Models",
    "authors": [
      "Elijah Pelofske",
      "Vincent Urias",
      "Lorie M. Liebrock"
    ],
    "abstract": "Generative Pre-Trained Transformer models have been shown to be surprisingly\neffective at a variety of natural language processing tasks -- including\ngenerating computer code. We evaluate the effectiveness of open source GPT\nmodels for the task of automatic identification of the presence of vulnerable\ncode syntax (specifically targeting C and C++ source code). This task is\nevaluated on a selection of 36 source code examples from the NIST SARD dataset,\nwhich are specifically curated to not contain natural English that indicates\nthe presence, or lack thereof, of a particular vulnerability. The NIST SARD\nsource code dataset contains identified vulnerable lines of source code that\nare examples of one out of the 839 distinct Common Weakness Enumerations (CWE),\nallowing for exact quantification of the GPT output classification error rate.\nA total of 5 GPT models are evaluated, using 10 different inference\ntemperatures and 100 repetitions at each setting, resulting in 5,000 GPT\nqueries per vulnerable source code analyzed. Ultimately, we find that the GPT\nmodels that we evaluated are not suitable for fully automated vulnerability\nscanning because the false positive and false negative rates are too high to\nlikely be useful in practice. However, we do find that the GPT models perform\nsurprisingly well at automated vulnerability detection for some of the test\ncases, in particular surpassing random sampling, and being able to identify the\nexact lines of code that are vulnerable albeit at a low success rate. The best\nperforming GPT model result found was Llama-2-70b-chat-hf with inference\ntemperature of 0.1 applied to NIST SARD test case 149165 (which is an example\nof a buffer overflow vulnerability), which had a binary classification recall\nscore of 1.0 and a precision of 1.0 for correctly and uniquely identifying the\nvulnerable line of code and the correct CWE number.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00197v1",
    "published_date": "2024-07-31 23:33:26 UTC",
    "updated_date": "2024-07-31 23:33:26 UTC"
  },
  {
    "arxiv_id": "2408.00193v2",
    "title": "Resilience and Security of Deep Neural Networks Against Intentional and Unintentional Perturbations: Survey and Research Challenges",
    "authors": [
      "Sazzad Sayyed",
      "Milin Zhang",
      "Shahriar Rifat",
      "Ananthram Swami",
      "Michael De Lucia",
      "Francesco Restuccia"
    ],
    "abstract": "In order to deploy deep neural networks (DNNs) in high-stakes scenarios, it\nis imperative that DNNs provide inference robust to external perturbations -\nboth intentional and unintentional. Although the resilience of DNNs to\nintentional and unintentional perturbations has been widely investigated, a\nunified vision of these inherently intertwined problem domains is still\nmissing. In this work, we fill this gap by providing a survey of the state of\nthe art and highlighting the similarities of the proposed approaches.We also\nanalyze the research challenges that need to be addressed to deploy resilient\nand secure DNNs. As there has not been any such survey connecting the\nresilience of DNNs to intentional and unintentional perturbations, we believe\nthis work can help advance the frontier in both domains by enabling the\nexchange of ideas between the two communities.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00193v2",
    "published_date": "2024-07-31 23:20:46 UTC",
    "updated_date": "2024-08-03 02:23:32 UTC"
  },
  {
    "arxiv_id": "2408.00191v1",
    "title": "S-SYNTH: Knowledge-Based, Synthetic Generation of Skin Images",
    "authors": [
      "Andrea Kim",
      "Niloufar Saharkhiz",
      "Elena Sizikova",
      "Miguel Lago",
      "Berkman Sahiner",
      "Jana Delfino",
      "Aldo Badano"
    ],
    "abstract": "Development of artificial intelligence (AI) techniques in medical imaging\nrequires access to large-scale and diverse datasets for training and\nevaluation. In dermatology, obtaining such datasets remains challenging due to\nsignificant variations in patient populations, illumination conditions, and\nacquisition system characteristics. In this work, we propose S-SYNTH, the first\nknowledge-based, adaptable open-source skin simulation framework to rapidly\ngenerate synthetic skin, 3D models and digitally rendered images, using an\nanatomically inspired multi-layer, multi-component skin and growing lesion\nmodel. The skin model allows for controlled variation in skin appearance, such\nas skin color, presence of hair, lesion shape, and blood fraction among other\nparameters. We use this framework to study the effect of possible variations on\nthe development and evaluation of AI models for skin lesion segmentation, and\nshow that results obtained using synthetic data follow similar comparative\ntrends as real dermatologic images, while mitigating biases and limitations\nfrom existing datasets including small dataset size, lack of diversity, and\nunderrepresentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to the International Conference on Medical Image Computing\n  and Computer Assisted Intervention (MICCAI) 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.00191v1",
    "published_date": "2024-07-31 23:16:29 UTC",
    "updated_date": "2024-07-31 23:16:29 UTC"
  },
  {
    "arxiv_id": "2408.00170v3",
    "title": "CREW: Facilitating Human-AI Teaming Research",
    "authors": [
      "Lingyu Zhang",
      "Zhengran Ji",
      "Boyuan Chen"
    ],
    "abstract": "With the increasing deployment of artificial intelligence (AI) technologies,\nthe potential of humans working with AI agents has been growing at a great\nspeed. Human-AI teaming is an important paradigm for studying various aspects\nwhen humans and AI agents work together. The unique aspect of Human-AI teaming\nresearch is the need to jointly study humans and AI agents, demanding\nmultidisciplinary research efforts from machine learning to human-computer\ninteraction, robotics, cognitive science, neuroscience, psychology, social\nscience, and complex systems. However, existing platforms for Human-AI teaming\nresearch are limited, often supporting oversimplified scenarios and a single\ntask, or specifically focusing on either human-teaming research or multi-agent\nAI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming\nresearch in real-time decision-making scenarios and engage collaborations from\nmultiple scientific disciplines, with a strong emphasis on human involvement.\nIt includes pre-built tasks for cognitive studies and Human-AI teaming with\nexpandable potentials from our modular design. Following conventional cognitive\nneuroscience research, CREW also supports multimodal human physiological signal\nrecording for behavior analysis. Moreover, CREW benchmarks real-time\nhuman-guided reinforcement learning agents using state-of-the-art algorithms\nand well-tuned baselines. With CREW, we were able to conduct 50 human subject\nstudies within a week to verify the effectiveness of our benchmark.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Our project website is at: http://generalroboticslab.com/CREW",
    "pdf_url": "http://arxiv.org/pdf/2408.00170v3",
    "published_date": "2024-07-31 21:43:55 UTC",
    "updated_date": "2025-01-01 18:42:00 UTC"
  },
  {
    "arxiv_id": "2408.00167v2",
    "title": "Finch: Prompt-guided Key-Value Cache Compression",
    "authors": [
      "Giulio Corallo",
      "Paolo Papotti"
    ],
    "abstract": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at TACL - pre-MIT Press publication version",
    "pdf_url": "http://arxiv.org/pdf/2408.00167v2",
    "published_date": "2024-07-31 21:33:56 UTC",
    "updated_date": "2024-08-13 09:08:55 UTC"
  },
  {
    "arxiv_id": "2408.00166v1",
    "title": "Review of Explainable Graph-Based Recommender Systems",
    "authors": [
      "Thanet Markchom",
      "Huizhi Liang",
      "James Ferryman"
    ],
    "abstract": "Explainability of recommender systems has become essential to ensure users'\ntrust and satisfaction. Various types of explainable recommender systems have\nbeen proposed including explainable graph-based recommender systems. This\nreview paper discusses state-of-the-art approaches of these systems and\ncategorizes them based on three aspects: learning methods, explaining methods,\nand explanation types. It also explores the commonly used datasets,\nexplainability evaluation methods, and future directions of this research area.\nCompared with the existing review papers, this paper focuses on explainability\nbased on graphs and covers the topics required for developing novel explainable\ngraph-based recommender systems.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00166v1",
    "published_date": "2024-07-31 21:30:36 UTC",
    "updated_date": "2024-07-31 21:30:36 UTC"
  },
  {
    "arxiv_id": "2408.00165v3",
    "title": "Non-convolutional Graph Neural Networks",
    "authors": [
      "Yuanqing Wang",
      "Kyunghyun Cho"
    ],
    "abstract": "Rethink convolution-based graph neural networks (GNN) -- they\ncharacteristically suffer from limited expressiveness, over-smoothing, and\nover-squashing, and require specialized sparse kernels for efficient\ncomputation. Here, we design a simple graph learning module entirely free of\nconvolution operators, coined random walk with unifying memory (RUM) neural\nnetwork, where an RNN merges the topological and semantic graph features along\nthe random walks terminating at each node. Relating the rich literature on RNN\nbehavior and graph topology, we theoretically show and experimentally verify\nthat RUM attenuates the aforementioned symptoms and is more expressive than the\nWeisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level\nclassification and regression tasks, RUM not only achieves competitive\nperformance, but is also robust, memory-efficient, scalable, and faster than\nthe simplest convolutional GNNs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00165v3",
    "published_date": "2024-07-31 21:29:26 UTC",
    "updated_date": "2024-09-29 00:15:57 UTC"
  },
  {
    "arxiv_id": "2408.07838v1",
    "title": "A Culturally-Aware Tool for Crowdworkers: Leveraging Chronemics to Support Diverse Work Styles",
    "authors": [
      "Carlos Toxtli",
      "Christopher Curtis",
      "Saiph Savage"
    ],
    "abstract": "Crowdsourcing markets are expanding worldwide, but often feature standardized\ninterfaces that ignore the cultural diversity of their workers, negatively\nimpacting their well-being and productivity. To transform these workplace\ndynamics, this paper proposes creating culturally-aware workplace tools,\nspecifically designed to adapt to the cultural dimensions of monochronic and\npolychronic work styles. We illustrate this approach with \"CultureFit,\" a tool\nthat we engineered based on extensive research in Chronemics and culture\ntheories. To study and evaluate our tool in the real world, we conducted a\nfield experiment with 55 workers from 24 different countries. Our field\nexperiment revealed that CultureFit significantly improved the earnings of\nworkers from cultural backgrounds often overlooked in design. Our study is\namong the pioneering efforts to examine culturally aware digital labor\ninterventions. It also provides access to a dataset with over two million data\npoints on culture and digital work, which can be leveraged for future research\nin this emerging field. The paper concludes by discussing the importance and\nfuture possibilities of incorporating cultural insights into the design of\ntools for digital labor.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "68U35",
      "H.5.2"
    ],
    "primary_category": "cs.HC",
    "comment": "32 pages, 9 figures, Computer Supported Cooperative Work (CSCW) 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.07838v1",
    "published_date": "2024-07-31 21:22:41 UTC",
    "updated_date": "2024-07-31 21:22:41 UTC"
  },
  {
    "arxiv_id": "2408.00162v1",
    "title": "A Taxonomy of Stereotype Content in Large Language Models",
    "authors": [
      "Gandalf Nicolas",
      "Aylin Caliskan"
    ],
    "abstract": "This study introduces a taxonomy of stereotype content in contemporary large\nlanguage models (LLMs). We prompt ChatGPT 3.5, Llama 3, and Mixtral 8x7B, three\npowerful and widely used LLMs, for the characteristics associated with 87\nsocial categories (e.g., gender, race, occupations). We identify 14 stereotype\ndimensions (e.g., Morality, Ability, Health, Beliefs, Emotions), accounting for\n~90% of LLM stereotype associations. Warmth and Competence facets were the most\nfrequent content, but all other dimensions were significantly prevalent.\nStereotypes were more positive in LLMs (vs. humans), but there was significant\nvariability across categories and dimensions. Finally, the taxonomy predicted\nthe LLMs' internal evaluations of social categories (e.g., how\npositively/negatively the categories were represented), supporting the\nrelevance of a multidimensional taxonomy for characterizing LLM stereotypes.\nOur findings suggest that high-dimensional human stereotypes are reflected in\nLLMs and must be considered in AI auditing and debiasing to minimize\nunidentified harms from reliance in low-dimensional views of bias in LLMs.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00162v1",
    "published_date": "2024-07-31 21:14:41 UTC",
    "updated_date": "2024-07-31 21:14:41 UTC"
  },
  {
    "arxiv_id": "2408.00161v2",
    "title": "Automatic Generation of Behavioral Test Cases For Natural Language Processing Using Clustering and Prompting",
    "authors": [
      "Ying Li",
      "Rahul Singh",
      "Tarun Joshi",
      "Agus Sudjianto"
    ],
    "abstract": "Recent work in behavioral testing for natural language processing (NLP)\nmodels, such as Checklist, is inspired by related paradigms in software\nengineering testing. They allow evaluation of general linguistic capabilities\nand domain understanding, hence can help evaluate conceptual soundness and\nidentify model weaknesses. However, a major challenge is the creation of test\ncases. The current packages rely on semi-automated approach using manual\ndevelopment which requires domain expertise and can be time consuming. This\npaper introduces an automated approach to develop test cases by exploiting the\npower of large language models and statistical techniques. It clusters the text\nrepresentations to carefully construct meaningful groups and then apply\nprompting techniques to automatically generate Minimal Functionality Tests\n(MFT). The well-known Amazon Reviews corpus is used to demonstrate our\napproach. We analyze the behavioral test profiles across four different\nclassification algorithms and discuss the limitations and strengths of those\nmodels.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00161v2",
    "published_date": "2024-07-31 21:12:21 UTC",
    "updated_date": "2024-08-08 16:31:05 UTC"
  },
  {
    "arxiv_id": "2408.00151v1",
    "title": "Moderating Group Conversation Dynamics with Social Robots",
    "authors": [
      "Lucrezia Grassi",
      "Carmine Tommaso Recchiuto",
      "Antonio Sgorbissa"
    ],
    "abstract": "This research investigates the impact of social robot participation in group\nconversations and assesses the effectiveness of various addressing policies.\nThe study involved 300 participants, divided into groups of four, interacting\nwith a humanoid robot serving as the moderator. The robot utilized conversation\ndata to determine the most appropriate speaker to address. The findings\nindicate that the robot's addressing policy significantly influenced\nconversation dynamics, resulting in more balanced attention to each participant\nand a reduction in subgroup formation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 6 figures, 1 table. Accepted at the workshop on advancing\n  Group Understanding and robots' adaptive behavior (GROUND), held at the\n  Robotics Science and Systems (RSS) Conference, 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.00151v1",
    "published_date": "2024-07-31 20:29:20 UTC",
    "updated_date": "2024-07-31 20:29:20 UTC"
  },
  {
    "arxiv_id": "2408.00150v1",
    "title": "StyleRF-VolVis: Style Transfer of Neural Radiance Fields for Expressive Volume Visualization",
    "authors": [
      "Kaiyuan Tang",
      "Chaoli Wang"
    ],
    "abstract": "In volume visualization, visualization synthesis has attracted much attention\ndue to its ability to generate novel visualizations without following the\nconventional rendering pipeline. However, existing solutions based on\ngenerative adversarial networks often require many training images and take\nsignificant training time. Still, issues such as low quality, consistency, and\nflexibility persist. This paper introduces StyleRF-VolVis, an innovative style\ntransfer framework for expressive volume visualization (VolVis) via neural\nradiance field (NeRF). The expressiveness of StyleRF-VolVis is upheld by its\nability to accurately separate the underlying scene geometry (i.e., content)\nand color appearance (i.e., style), conveniently modify color, opacity, and\nlighting of the original rendering while maintaining visual content consistency\nacross the views, and effectively transfer arbitrary styles from reference\nimages to the reconstructed 3D scene. To achieve these, we design a base NeRF\nmodel for scene geometry extraction, a palette color network to classify\nregions of the radiance field for photorealistic editing, and an unrestricted\ncolor network to lift the color palette constraint via knowledge distillation\nfor non-photorealistic editing. We demonstrate the superior quality,\nconsistency, and flexibility of StyleRF-VolVis by experimenting with various\nvolume rendering scenes and reference images and comparing StyleRF-VolVis\nagainst other image-based (AdaIN), video-based (ReReVST), and NeRF-based (ARF\nand SNeRF) style rendering solutions.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "Accepted by IEEE VIS 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.00150v1",
    "published_date": "2024-07-31 20:26:30 UTC",
    "updated_date": "2024-07-31 20:26:30 UTC"
  },
  {
    "arxiv_id": "2408.00147v1",
    "title": "Formal Ethical Obligations in Reinforcement Learning Agents: Verification and Policy Updates",
    "authors": [
      "Colin Shea-Blymyer",
      "Houssam Abbas"
    ],
    "abstract": "When designing agents for operation in uncertain environments, designers need\ntools to automatically reason about what agents ought to do, how that conflicts\nwith what is actually happening, and how a policy might be modified to remove\nthe conflict. These obligations include ethical and social obligations,\npermissions and prohibitions, which constrain how the agent achieves its\nmission and executes its policy. We propose a new deontic logic, Expected Act\nUtilitarian deontic logic, for enabling this reasoning at design time: for\nspecifying and verifying the agent's strategic obligations, then modifying its\npolicy from a reference policy to meet those obligations. Unlike approaches\nthat work at the reward level, working at the logical level increases the\ntransparency of the trade-offs. We introduce two algorithms: one for\nmodel-checking whether an RL agent has the right strategic obligations, and one\nfor modifying a reference decision policy to make it meet obligations expressed\nin our logic. We illustrate our algorithms on DAC-MDPs which accurately\nabstract neural decision policies, and on toy gridworld environments.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00147v1",
    "published_date": "2024-07-31 20:21:15 UTC",
    "updated_date": "2024-07-31 20:21:15 UTC"
  },
  {
    "arxiv_id": "2408.00144v1",
    "title": "Distributed In-Context Learning under Non-IID Among Clients",
    "authors": [
      "Siqi Liang",
      "Sumyeong Ahn",
      "Jiayu Zhou"
    ],
    "abstract": "Advancements in large language models (LLMs) have shown their effectiveness\nin multiple complicated natural language reasoning tasks. A key challenge\nremains in adapting these models efficiently to new or unfamiliar tasks.\nIn-context learning (ICL) provides a promising solution for few-shot adaptation\nby retrieving a set of data points relevant to a query, called in-context\nexamples (ICE), from a training dataset and providing them during the inference\nas context. Most existing studies utilize a centralized training dataset, yet\nmany real-world datasets may be distributed among multiple clients, and remote\ndata retrieval can be associated with costs. Especially when the client data\nare non-identical independent distributions (non-IID), retrieving from clients\na proper set of ICEs needed for a test query presents critical challenges. In\nthis paper, we first show that in this challenging setting, test queries will\nhave different preferences among clients because of non-IIDness, and equal\ncontribution often leads to suboptimal performance. We then introduce a novel\napproach to tackle the distributed non-IID ICL problem when a data usage budget\nis present. The principle is that each client's proper contribution (budget)\nshould be designed according to the preference of each query for that client.\nOur approach uses a data-driven manner to allocate a budget for each client,\ntailored to each test query. Through extensive empirical studies on diverse\ndatasets, our framework demonstrates superior performance relative to competing\nbaselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.00144v1",
    "published_date": "2024-07-31 20:06:25 UTC",
    "updated_date": "2024-07-31 20:06:25 UTC"
  },
  {
    "arxiv_id": "2408.00137v2",
    "title": "Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment",
    "authors": [
      "Sangwon Yu",
      "Jongyoon Song",
      "Bongkyu Hwang",
      "Hoyoung Kang",
      "Sooah Cho",
      "Junhwa Choi",
      "Seongho Joe",
      "Taehee Lee",
      "Youngjune L. Gwon",
      "Sungroh Yoon"
    ],
    "abstract": "A binary decision task, like yes-no questions or answer verification,\nreflects a significant real-world scenario such as where users look for\nconfirmation about the correctness of their decisions on specific issues. In\nthis work, we observe that language models exhibit a negative bias in the\nbinary decisions of complex reasoning tasks. Based on our observations and the\nrationale about attention-based model dynamics, we propose a negative attention\nscore (NAS) to systematically and quantitatively formulate negative bias. Based\non NAS, we identify attention heads that attend to negative tokens provided in\nthe instructions as answer candidate of binary decisions, regardless of the\nquestion in the prompt, and validate their association with the negative bias.\nAdditionally, we propose the negative attention score alignment (NASA) method,\nwhich is a parameter-efficient fine-tuning technique to address the extracted\nnegatively biased attention heads. Experimental results from various domains of\nreasoning tasks and large model search space demonstrate that NASA\nsignificantly reduces the gap between precision and recall caused by negative\nbias while preserving their generalization abilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 Oral",
    "pdf_url": "http://arxiv.org/pdf/2408.00137v2",
    "published_date": "2024-07-31 19:50:57 UTC",
    "updated_date": "2025-04-29 01:52:03 UTC"
  },
  {
    "arxiv_id": "2408.00131v1",
    "title": "Distributionally Robust Optimization as a Scalable Framework to Characterize Extreme Value Distributions",
    "authors": [
      "Patrick Kuiper",
      "Ali Hasan",
      "Wenhao Yang",
      "Yuting Ng",
      "Hoda Bidkhori",
      "Jose Blanchet",
      "Vahid Tarokh"
    ],
    "abstract": "The goal of this paper is to develop distributionally robust optimization\n(DRO) estimators, specifically for multidimensional Extreme Value Theory (EVT)\nstatistics. EVT supports using semi-parametric models called max-stable\ndistributions built from spatial Poisson point processes. While powerful, these\nmodels are only asymptotically valid for large samples. However, since extreme\ndata is by definition scarce, the potential for model misspecification error is\ninherent to these applications, thus DRO estimators are natural. In order to\nmitigate over-conservative estimates while enhancing out-of-sample performance,\nwe study DRO estimators informed by semi-parametric max-stable constraints in\nthe space of point processes. We study both tractable convex formulations for\nsome problems of interest (e.g. CVaR) and more general neural network based\nestimators. Both approaches are validated using synthetically generated data,\nrecovering prescribed characteristics, and verifying the efficacy of the\nproposed techniques. Additionally, the proposed method is applied to a real\ndata set of financial returns for comparison to a previous analysis. We\nestablished the proposed model as a novel formulation in the multivariate EVT\ndomain, and innovative with respect to performance when compared to relevant\nalternate proposals.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "q-fin.RM"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00131v1",
    "published_date": "2024-07-31 19:45:27 UTC",
    "updated_date": "2024-07-31 19:45:27 UTC"
  },
  {
    "arxiv_id": "2408.00123v1",
    "title": "Semantic Codebook Learning for Dynamic Recommendation Models",
    "authors": [
      "Zheqi Lv",
      "Shaoxuan He",
      "Tianyu Zhan",
      "Shengyu Zhang",
      "Wenqiao Zhang",
      "Jingyuan Chen",
      "Zhou Zhao",
      "Fei Wu"
    ],
    "abstract": "Dynamic sequential recommendation (DSR) can generate model parameters based\non user behavior to improve the personalization of sequential recommendation\nunder various user preferences. However, it faces the challenges of large\nparameter search space and sparse and noisy user-item interactions, which\nreduces the applicability of the generated model parameters. The Semantic\nCodebook Learning for Dynamic Recommendation Models (SOLID) framework presents\na significant advancement in DSR by effectively tackling these challenges. By\ntransforming item sequences into semantic sequences and employing a dual\nparameter model, SOLID compresses the parameter generation search space and\nleverages homogeneity within the recommendation system. The introduction of the\nsemantic metacode and semantic codebook, which stores disentangled item\nrepresentations, ensures robust and accurate parameter generation. Extensive\nexperiments demonstrates that SOLID consistently outperforms existing DSR,\ndelivering more accurate, stable, and robust recommendations.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.MM",
      "cs.SI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00123v1",
    "published_date": "2024-07-31 19:25:25 UTC",
    "updated_date": "2024-07-31 19:25:25 UTC"
  },
  {
    "arxiv_id": "2408.00118v3",
    "title": "Gemma 2: Improving Open Language Models at a Practical Size",
    "authors": [
      "Gemma Team",
      "Morgane Riviere",
      "Shreya Pathak",
      "Pier Giuseppe Sessa",
      "Cassidy Hardin",
      "Surya Bhupatiraju",
      "Léonard Hussenot",
      "Thomas Mesnard",
      "Bobak Shahriari",
      "Alexandre Ramé",
      "Johan Ferret",
      "Peter Liu",
      "Pouya Tafti",
      "Abe Friesen",
      "Michelle Casbon",
      "Sabela Ramos",
      "Ravin Kumar",
      "Charline Le Lan",
      "Sammy Jerome",
      "Anton Tsitsulin",
      "Nino Vieillard",
      "Piotr Stanczyk",
      "Sertan Girgin",
      "Nikola Momchev",
      "Matt Hoffman",
      "Shantanu Thakoor",
      "Jean-Bastien Grill",
      "Behnam Neyshabur",
      "Olivier Bachem",
      "Alanna Walton",
      "Aliaksei Severyn",
      "Alicia Parrish",
      "Aliya Ahmad",
      "Allen Hutchison",
      "Alvin Abdagic",
      "Amanda Carl",
      "Amy Shen",
      "Andy Brock",
      "Andy Coenen",
      "Anthony Laforge",
      "Antonia Paterson",
      "Ben Bastian",
      "Bilal Piot",
      "Bo Wu",
      "Brandon Royal",
      "Charlie Chen",
      "Chintu Kumar",
      "Chris Perry",
      "Chris Welty",
      "Christopher A. Choquette-Choo",
      "Danila Sinopalnikov",
      "David Weinberger",
      "Dimple Vijaykumar",
      "Dominika Rogozińska",
      "Dustin Herbison",
      "Elisa Bandy",
      "Emma Wang",
      "Eric Noland",
      "Erica Moreira",
      "Evan Senter",
      "Evgenii Eltyshev",
      "Francesco Visin",
      "Gabriel Rasskin",
      "Gary Wei",
      "Glenn Cameron",
      "Gus Martins",
      "Hadi Hashemi",
      "Hanna Klimczak-Plucińska",
      "Harleen Batra",
      "Harsh Dhand",
      "Ivan Nardini",
      "Jacinda Mein",
      "Jack Zhou",
      "James Svensson",
      "Jeff Stanway",
      "Jetha Chan",
      "Jin Peng Zhou",
      "Joana Carrasqueira",
      "Joana Iljazi",
      "Jocelyn Becker",
      "Joe Fernandez",
      "Joost van Amersfoort",
      "Josh Gordon",
      "Josh Lipschultz",
      "Josh Newlan",
      "Ju-yeong Ji",
      "Kareem Mohamed",
      "Kartikeya Badola",
      "Kat Black",
      "Katie Millican",
      "Keelin McDonell",
      "Kelvin Nguyen",
      "Kiranbir Sodhia",
      "Kish Greene",
      "Lars Lowe Sjoesund",
      "Lauren Usui",
      "Laurent Sifre",
      "Lena Heuermann",
      "Leticia Lago",
      "Lilly McNealus",
      "Livio Baldini Soares",
      "Logan Kilpatrick",
      "Lucas Dixon",
      "Luciano Martins",
      "Machel Reid",
      "Manvinder Singh",
      "Mark Iverson",
      "Martin Görner",
      "Mat Velloso",
      "Mateo Wirth",
      "Matt Davidow",
      "Matt Miller",
      "Matthew Rahtz",
      "Matthew Watson",
      "Meg Risdal",
      "Mehran Kazemi",
      "Michael Moynihan",
      "Ming Zhang",
      "Minsuk Kahng",
      "Minwoo Park",
      "Mofi Rahman",
      "Mohit Khatwani",
      "Natalie Dao",
      "Nenshad Bardoliwalla",
      "Nesh Devanathan",
      "Neta Dumai",
      "Nilay Chauhan",
      "Oscar Wahltinez",
      "Pankil Botarda",
      "Parker Barnes",
      "Paul Barham",
      "Paul Michel",
      "Pengchong Jin",
      "Petko Georgiev",
      "Phil Culliton",
      "Pradeep Kuppala",
      "Ramona Comanescu",
      "Ramona Merhej",
      "Reena Jana",
      "Reza Ardeshir Rokni",
      "Rishabh Agarwal",
      "Ryan Mullins",
      "Samaneh Saadat",
      "Sara Mc Carthy",
      "Sarah Cogan",
      "Sarah Perrin",
      "Sébastien M. R. Arnold",
      "Sebastian Krause",
      "Shengyang Dai",
      "Shruti Garg",
      "Shruti Sheth",
      "Sue Ronstrom",
      "Susan Chan",
      "Timothy Jordan",
      "Ting Yu",
      "Tom Eccles",
      "Tom Hennigan",
      "Tomas Kocisky",
      "Tulsee Doshi",
      "Vihan Jain",
      "Vikas Yadav",
      "Vilobh Meshram",
      "Vishal Dharmadhikari",
      "Warren Barkley",
      "Wei Wei",
      "Wenming Ye",
      "Woohyun Han",
      "Woosuk Kwon",
      "Xiang Xu",
      "Zhe Shen",
      "Zhitao Gong",
      "Zichuan Wei",
      "Victor Cotruta",
      "Phoebe Kirk",
      "Anand Rao",
      "Minh Giang",
      "Ludovic Peran",
      "Tris Warkentin",
      "Eli Collins",
      "Joelle Barral",
      "Zoubin Ghahramani",
      "Raia Hadsell",
      "D. Sculley",
      "Jeanine Banks",
      "Anca Dragan",
      "Slav Petrov",
      "Oriol Vinyals",
      "Jeff Dean",
      "Demis Hassabis",
      "Koray Kavukcuoglu",
      "Clement Farabet",
      "Elena Buchatskaya",
      "Sebastian Borgeaud",
      "Noah Fiedel",
      "Armand Joulin",
      "Kathleen Kenealy",
      "Robert Dadashi",
      "Alek Andreev"
    ],
    "abstract": "In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00118v3",
    "published_date": "2024-07-31 19:13:07 UTC",
    "updated_date": "2024-10-02 15:22:49 UTC"
  },
  {
    "arxiv_id": "2408.00114v2",
    "title": "Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs",
    "authors": [
      "Kewei Cheng",
      "Jingfeng Yang",
      "Haoming Jiang",
      "Zhengyang Wang",
      "Binxuan Huang",
      "Ruirui Li",
      "Shiyang Li",
      "Zheng Li",
      "Yifan Gao",
      "Xian Li",
      "Bing Yin",
      "Yizhou Sun"
    ],
    "abstract": "Reasoning encompasses two typical types: deductive reasoning and inductive\nreasoning. Despite extensive research into the reasoning capabilities of Large\nLanguage Models (LLMs), most studies have failed to rigorously differentiate\nbetween inductive and deductive reasoning, leading to a blending of the two.\nThis raises an essential question: In LLM reasoning, which poses a greater\nchallenge - deductive or inductive reasoning? While the deductive reasoning\ncapabilities of LLMs, (i.e. their capacity to follow instructions in reasoning\ntasks), have received considerable attention, their abilities in true inductive\nreasoning remain largely unexplored. To investigate into the true inductive\nreasoning capabilities of LLMs, we propose a novel framework, SolverLearner.\nThis framework enables LLMs to learn the underlying function (i.e., $y =\nf_w(x)$), that maps input data points $(x)$ to their corresponding output\nvalues $(y)$, using only in-context examples. By focusing on inductive\nreasoning and separating it from LLM-based deductive reasoning, we can isolate\nand investigate inductive reasoning of LLMs in its pure form via SolverLearner.\nOur observations reveal that LLMs demonstrate remarkable inductive reasoning\ncapabilities through SolverLearner, achieving near-perfect performance with ACC\nof 1 in most cases. Surprisingly, despite their strong inductive reasoning\nabilities, LLMs tend to relatively lack deductive reasoning capabilities,\nparticularly in tasks involving ``counterfactual'' reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00114v2",
    "published_date": "2024-07-31 18:47:11 UTC",
    "updated_date": "2024-08-07 00:52:07 UTC"
  },
  {
    "arxiv_id": "2408.00113v2",
    "title": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models",
    "authors": [
      "Adam Karvonen",
      "Benjamin Wright",
      "Can Rager",
      "Rico Angell",
      "Jannik Brinkmann",
      "Logan Smith",
      "Claudio Mayrink Verdun",
      "David Bau",
      "Samuel Marks"
    ],
    "abstract": "What latent features are encoded in language model (LM) representations?\nRecent work on training sparse autoencoders (SAEs) to disentangle interpretable\nfeatures in LM representations has shown significant promise. However,\nevaluating the quality of these SAEs is difficult because we lack a\nground-truth collection of interpretable features that we expect good SAEs to\nrecover. We thus propose to measure progress in interpretable dictionary\nlearning by working in the setting of LMs trained on chess and Othello\ntranscripts. These settings carry natural collections of interpretable features\n-- for example, \"there is a knight on F3\" -- which we leverage into\n$\\textit{supervised}$ metrics for SAE quality. To guide progress in\ninterpretable dictionary learning, we introduce a new SAE training technique,\n$\\textit{p-annealing}$, which improves performance on prior unsupervised\nmetrics as well as our new metrics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as an oral paper (top 5%) at the ICML 2024 Mechanistic\n  Interpretability Workshop and to the NeurIPS 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2408.00113v2",
    "published_date": "2024-07-31 18:45:13 UTC",
    "updated_date": "2024-10-30 14:21:59 UTC"
  },
  {
    "arxiv_id": "2408.00108v2",
    "title": "Preference-Based Abstract Argumentation for Case-Based Reasoning (with Appendix)",
    "authors": [
      "Adam Gould",
      "Guilherme Paulino-Passos",
      "Seema Dadhania",
      "Matthew Williams",
      "Francesca Toni"
    ],
    "abstract": "In the pursuit of enhancing the efficacy and flexibility of interpretable,\ndata-driven classification models, this work introduces a novel incorporation\nof user-defined preferences with Abstract Argumentation and Case-Based\nReasoning (CBR). Specifically, we introduce Preference-Based Abstract\nArgumentation for Case-Based Reasoning (which we call AA-CBR-P), allowing users\nto define multiple approaches to compare cases with an ordering that specifies\ntheir preference over these comparison approaches. We prove that the model\ninherently follows these preferences when making predictions and show that\nprevious abstract argumentation for case-based reasoning approaches are\ninsufficient at expressing preferences over constituents of an argument. We\nthen demonstrate how this can be applied to a real-world medical dataset\nsourced from a clinical trial evaluating differing assessment methods of\npatients with a primary brain tumour. We show empirically that our approach\noutperforms other interpretable machine learning models on this dataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for KR2024. Includes Appendix",
    "pdf_url": "http://arxiv.org/pdf/2408.00108v2",
    "published_date": "2024-07-31 18:31:04 UTC",
    "updated_date": "2024-08-03 08:51:46 UTC"
  },
  {
    "arxiv_id": "2408.00106v1",
    "title": "WAS: Dataset and Methods for Artistic Text Segmentation",
    "authors": [
      "Xudong Xie",
      "Yuzhe Li",
      "Yang Liu",
      "Zhifei Zhang",
      "Zhaowen Wang",
      "Wei Xiong",
      "Xiang Bai"
    ],
    "abstract": "Accurate text segmentation results are crucial for text-related generative\ntasks, such as text image generation, text editing, text removal, and text\nstyle transfer. Recently, some scene text segmentation methods have made\nsignificant progress in segmenting regular text. However, these methods perform\npoorly in scenarios containing artistic text. Therefore, this paper focuses on\nthe more challenging task of artistic text segmentation and constructs a real\nartistic text segmentation dataset. One challenge of the task is that the local\nstroke shapes of artistic text are changeable with diversity and complexity. We\npropose a decoder with the layer-wise momentum query to prevent the model from\nignoring stroke regions of special shapes. Another challenge is the complexity\nof the global topological structure. We further design a skeleton-assisted head\nto guide the model to focus on the global structure. Additionally, to enhance\nthe generalization performance of the text segmentation model, we propose a\nstrategy for training data synthesis, based on the large multi-modal model and\nthe diffusion model. Experimental results show that our proposed method and\nsynthetic dataset can significantly enhance the performance of artistic text\nsegmentation and achieve state-of-the-art results on other public datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.00106v1",
    "published_date": "2024-07-31 18:29:36 UTC",
    "updated_date": "2024-07-31 18:29:36 UTC"
  },
  {
    "arxiv_id": "2408.00103v3",
    "title": "ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget",
    "authors": [
      "Riccardo Orlando",
      "Pere-Lluis Huguet Cabot",
      "Edoardo Barba",
      "Roberto Navigli"
    ],
    "abstract": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in\nNatural Language Processing, serving as critical components in a wide range of\napplications. In this paper, we propose ReLiK, a Retriever-Reader architecture\nfor both EL and RE, where, given an input text, the Retriever module undertakes\nthe identification of candidate entities or relations that could potentially\nappear within the text. Subsequently, the Reader module is tasked to discern\nthe pertinent retrieved entities or relations and establish their alignment\nwith the corresponding textual spans. Notably, we put forward an innovative\ninput representation that incorporates the candidate entities or relations\nalongside the text, making it possible to link entities or extract relations in\na single forward pass and to fully leverage pre-trained language models\ncontextualization capabilities, in contrast with previous\nRetriever-Reader-based methods, which require a forward pass for each\ncandidate. Our formulation of EL and RE achieves state-of-the-art performance\nin both in-domain and out-of-domain benchmarks while using academic budget\ntraining and with up to 40x inference speed compared to competitors. Finally,\nwe show how our architecture can be used seamlessly for Information Extraction\n(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared\nReader that simultaneously extracts entities and relations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Findings of the Association for Computational Linguistics ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.00103v3",
    "published_date": "2024-07-31 18:25:49 UTC",
    "updated_date": "2025-05-09 09:02:22 UTC"
  },
  {
    "arxiv_id": "2408.00096v1",
    "title": "From Attributes to Natural Language: A Survey and Foresight on Text-based Person Re-identification",
    "authors": [
      "Fanzhi Jiang",
      "Su Yang",
      "Mark W. Jones",
      "Liumei Zhang"
    ],
    "abstract": "Text-based person re-identification (Re-ID) is a challenging topic in the\nfield of complex multimodal analysis, its ultimate aim is to recognize specific\npedestrians by scrutinizing attributes/natural language descriptions. Despite\nthe wide range of applicable areas such as security surveillance, video\nretrieval, person tracking, and social media analytics, there is a notable\nabsence of comprehensive reviews dedicated to summarizing the text-based person\nRe-ID from a technical perspective. To address this gap, we propose to\nintroduce a taxonomy spanning Evaluation, Strategy, Architecture, and\nOptimization dimensions, providing a comprehensive survey of the text-based\nperson Re-ID task. We start by laying the groundwork for text-based person\nRe-ID, elucidating fundamental concepts related to attribute/natural\nlanguage-based identification. Then a thorough examination of existing\nbenchmark datasets and metrics is presented. Subsequently, we further delve\ninto prevalent feature extraction strategies employed in text-based person\nRe-ID research, followed by a concise summary of common network architectures\nwithin the domain. Prevalent loss functions utilized for model optimization and\nmodality alignment in text-based person Re-ID are also scrutinized. To\nconclude, we offer a concise summary of our findings, pinpointing challenges in\ntext-based person Re-ID. In response to these challenges, we outline potential\navenues for future open-set text-based person Re-ID and present a baseline\narchitecture for text-based pedestrian image generation-guided\nre-identification(TBPGR).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00096v1",
    "published_date": "2024-07-31 18:16:18 UTC",
    "updated_date": "2024-07-31 18:16:18 UTC"
  },
  {
    "arxiv_id": "2408.00090v2",
    "title": "Execution Semantics of Behavior Trees in Robotic Applications",
    "authors": [
      "Enrico Ghiorzi",
      "Christian Henkel",
      "Matteo Palmas",
      "Michaela Klauck",
      "Armando Tacchella"
    ],
    "abstract": "Behavior Trees (BTs) have found a widespread adoption in robotics due to\nappealing features, their ease of use as a conceptual model of control policies\nand the availability of software tooling for BT-based design of control\nsoftware. However, BTs don't have formal execution semantics and, furthermore,\nsubtle differences among implementations can make the same model behave\ndifferently depending on the underlying software. This paper aims at defining\nthe execution semantics of behavior trees (BTs) as used in robotics\napplications. To this purpose, we present an abstract data type that formalizes\nthe structure and execution of BTs. While our formalization is inspired by\nexisting contributions in the scientific literature and state-of-the art\nimplementations, we strive to provide an unambiguous treatment of most features\nthat find incomplete or inconsistent treatment across other works.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "68T30",
      "I.2.4"
    ],
    "primary_category": "cs.RO",
    "comment": "25 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.00090v2",
    "published_date": "2024-07-31 18:08:59 UTC",
    "updated_date": "2025-04-10 15:46:48 UTC"
  },
  {
    "arxiv_id": "2407.21794v1",
    "title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey",
    "authors": [
      "Atsuyuki Miyai",
      "Jingkang Yang",
      "Jingyang Zhang",
      "Yifei Ming",
      "Yueqian Lin",
      "Qing Yu",
      "Go Irie",
      "Shafiq Joty",
      "Yixuan Li",
      "Hai Li",
      "Ziwei Liu",
      "Toshihiko Yamasaki",
      "Kiyoharu Aizawa"
    ],
    "abstract": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD\ndetection, and OD in the VLM era. Our framework reveals that, with some field\ninactivity and integration, the demanding challenges have become OOD detection\nand AD. In addition, we also highlight the significant shift in the definition,\nproblem settings, and benchmarks; we thus feature a comprehensive review of the\nmethodology for OOD detection, including the discussion over other related\ntasks to clarify their relationship to OOD detection. Finally, we explore the\nadvancements in the emerging Large Vision Language Model (LVLM) era, such as\nGPT-4V. We conclude this survey with open challenges and future directions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "survey paper. We welcome questions, issues, and paper requests via\n  https://github.com/AtsuMiyai/Awesome-OOD-VLM",
    "pdf_url": "http://arxiv.org/pdf/2407.21794v1",
    "published_date": "2024-07-31 17:59:58 UTC",
    "updated_date": "2024-07-31 17:59:58 UTC"
  },
  {
    "arxiv_id": "2407.21792v3",
    "title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?",
    "authors": [
      "Richard Ren",
      "Steven Basart",
      "Adam Khoja",
      "Alice Gatti",
      "Long Phan",
      "Xuwang Yin",
      "Mantas Mazeika",
      "Alexander Pan",
      "Gabriel Mukobi",
      "Ryan H. Kim",
      "Stephen Fitz",
      "Dan Hendrycks"
    ],
    "abstract": "As artificial intelligence systems grow more powerful, there has been\nincreasing interest in \"AI safety\" research to address emerging and future\nrisks. However, the field of AI safety remains poorly defined and\ninconsistently measured, leading to confusion about how researchers can\ncontribute. This lack of clarity is compounded by the unclear relationship\nbetween AI safety benchmarks and upstream general capabilities (e.g., general\nknowledge and reasoning). To address these issues, we conduct a comprehensive\nmeta-analysis of AI safety benchmarks, empirically analyzing their correlation\nwith general capabilities across dozens of models and providing a survey of\nexisting directions in AI safety. Our findings reveal that many safety\nbenchmarks highly correlate with both upstream model capabilities and training\ncompute, potentially enabling \"safetywashing\"--where capability improvements\nare misrepresented as safety advancements. Based on these findings, we propose\nan empirical foundation for developing more meaningful safety metrics and\ndefine AI safety in a machine learning research context as a set of clearly\ndelineated research goals that are empirically separable from generic\ncapabilities advancements. In doing so, we aim to provide a more rigorous\nframework for AI safety research, advancing the science of safety evaluations\nand clarifying the path towards measurable progress.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21792v3",
    "published_date": "2024-07-31 17:59:24 UTC",
    "updated_date": "2024-12-27 17:36:21 UTC"
  },
  {
    "arxiv_id": "2407.21788v1",
    "title": "Vision-Language Model Based Handwriting Verification",
    "authors": [
      "Mihir Chauhan",
      "Abhishek Satbhai",
      "Mohammad Abuzar Hashemi",
      "Mir Basheer Ali",
      "Bina Ramamurthy",
      "Mingchen Gao",
      "Siwei Lyu",
      "Sargur Srihari"
    ],
    "abstract": "Handwriting Verification is a critical in document forensics. Deep learning\nbased approaches often face skepticism from forensic document examiners due to\ntheir lack of explainability and reliance on extensive training data and\nhandcrafted features. This paper explores using Vision Language Models (VLMs),\nsuch as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By\nleveraging their Visual Question Answering capabilities and 0-shot\nChain-of-Thought (CoT) reasoning, our goal is to provide clear,\nhuman-understandable explanations for model decisions. Our experiments on the\nCEDAR handwriting dataset demonstrate that VLMs offer enhanced\ninterpretability, reduce the need for large training datasets, and adapt better\nto diverse handwriting styles. However, results show that the CNN-based\nResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach\nwith GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:\n71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings\nhighlight the potential of VLMs in generating human-interpretable decisions\nwhile underscoring the need for further advancements to match the performance\nof specialized deep learning models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "4 Pages, 1 Figure, 1 Table, Accepted as Short paper at Irish Machine\n  Vision and Image Processing (IMVIP) Conference",
    "pdf_url": "http://arxiv.org/pdf/2407.21788v1",
    "published_date": "2024-07-31 17:57:32 UTC",
    "updated_date": "2024-07-31 17:57:32 UTC"
  },
  {
    "arxiv_id": "2407.21787v3",
    "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
    "authors": [
      "Bradley Brown",
      "Jordan Juravsky",
      "Ryan Ehrlich",
      "Ronald Clark",
      "Quoc V. Le",
      "Christopher Ré",
      "Azalia Mirhoseini"
    ],
    "abstract": "Scaling the amount of compute used to train language models has dramatically\nimproved their capabilities. However, when it comes to inference, we often\nlimit models to making only one attempt at a problem. Here, we explore\ninference compute as another axis for scaling, using the simple technique of\nrepeatedly sampling candidate solutions from a model. Across multiple tasks and\nmodels, we observe that coverage -- the fraction of problems that are solved by\nany generated sample -- scales with the number of samples over four orders of\nmagnitude. Interestingly, the relationship between coverage and the number of\nsamples is often log-linear and can be modelled with an exponentiated power\nlaw, suggesting the existence of inference-time scaling laws. In domains like\ncoding and formal proofs, where answers can be automatically verified, these\nincreases in coverage directly translate into improved performance. When we\napply repeated sampling to SWE-bench Lite, the fraction of issues solved with\nDeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250\nsamples, outperforming the single-sample state-of-the-art of 43%. In domains\nwithout automatic verifiers, we find that common methods for picking from a\nsample collection (majority voting and reward models) plateau beyond several\nhundred samples and fail to fully scale with the sample budget.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21787v3",
    "published_date": "2024-07-31 17:57:25 UTC",
    "updated_date": "2024-12-30 19:03:24 UTC"
  },
  {
    "arxiv_id": "2407.21783v3",
    "title": "The Llama 3 Herd of Models",
    "authors": [
      "Aaron Grattafiori",
      "Abhimanyu Dubey",
      "Abhinav Jauhri",
      "Abhinav Pandey",
      "Abhishek Kadian",
      "Ahmad Al-Dahle",
      "Aiesha Letman",
      "Akhil Mathur",
      "Alan Schelten",
      "Alex Vaughan",
      "Amy Yang",
      "Angela Fan",
      "Anirudh Goyal",
      "Anthony Hartshorn",
      "Aobo Yang",
      "Archi Mitra",
      "Archie Sravankumar",
      "Artem Korenev",
      "Arthur Hinsvark",
      "Arun Rao",
      "Aston Zhang",
      "Aurelien Rodriguez",
      "Austen Gregerson",
      "Ava Spataru",
      "Baptiste Roziere",
      "Bethany Biron",
      "Binh Tang",
      "Bobbie Chern",
      "Charlotte Caucheteux",
      "Chaya Nayak",
      "Chloe Bi",
      "Chris Marra",
      "Chris McConnell",
      "Christian Keller",
      "Christophe Touret",
      "Chunyang Wu",
      "Corinne Wong",
      "Cristian Canton Ferrer",
      "Cyrus Nikolaidis",
      "Damien Allonsius",
      "Daniel Song",
      "Danielle Pintz",
      "Danny Livshits",
      "Danny Wyatt",
      "David Esiobu",
      "Dhruv Choudhary",
      "Dhruv Mahajan",
      "Diego Garcia-Olano",
      "Diego Perino",
      "Dieuwke Hupkes",
      "Egor Lakomkin",
      "Ehab AlBadawy",
      "Elina Lobanova",
      "Emily Dinan",
      "Eric Michael Smith",
      "Filip Radenovic",
      "Francisco Guzmán",
      "Frank Zhang",
      "Gabriel Synnaeve",
      "Gabrielle Lee",
      "Georgia Lewis Anderson",
      "Govind Thattai",
      "Graeme Nail",
      "Gregoire Mialon",
      "Guan Pang",
      "Guillem Cucurell",
      "Hailey Nguyen",
      "Hannah Korevaar",
      "Hu Xu",
      "Hugo Touvron",
      "Iliyan Zarov",
      "Imanol Arrieta Ibarra",
      "Isabel Kloumann",
      "Ishan Misra",
      "Ivan Evtimov",
      "Jack Zhang",
      "Jade Copet",
      "Jaewon Lee",
      "Jan Geffert",
      "Jana Vranes",
      "Jason Park",
      "Jay Mahadeokar",
      "Jeet Shah",
      "Jelmer van der Linde",
      "Jennifer Billock",
      "Jenny Hong",
      "Jenya Lee",
      "Jeremy Fu",
      "Jianfeng Chi",
      "Jianyu Huang",
      "Jiawen Liu",
      "Jie Wang",
      "Jiecao Yu",
      "Joanna Bitton",
      "Joe Spisak",
      "Jongsoo Park",
      "Joseph Rocca",
      "Joshua Johnstun",
      "Joshua Saxe",
      "Junteng Jia",
      "Kalyan Vasuden Alwala",
      "Karthik Prasad",
      "Kartikeya Upasani",
      "Kate Plawiak",
      "Ke Li",
      "Kenneth Heafield",
      "Kevin Stone",
      "Khalid El-Arini",
      "Krithika Iyer",
      "Kshitiz Malik",
      "Kuenley Chiu",
      "Kunal Bhalla",
      "Kushal Lakhotia",
      "Lauren Rantala-Yeary",
      "Laurens van der Maaten",
      "Lawrence Chen",
      "Liang Tan",
      "Liz Jenkins",
      "Louis Martin",
      "Lovish Madaan",
      "Lubo Malo",
      "Lukas Blecher",
      "Lukas Landzaat",
      "Luke de Oliveira",
      "Madeline Muzzi",
      "Mahesh Pasupuleti",
      "Mannat Singh",
      "Manohar Paluri",
      "Marcin Kardas",
      "Maria Tsimpoukelli",
      "Mathew Oldham",
      "Mathieu Rita",
      "Maya Pavlova",
      "Melanie Kambadur",
      "Mike Lewis",
      "Min Si",
      "Mitesh Kumar Singh",
      "Mona Hassan",
      "Naman Goyal",
      "Narjes Torabi",
      "Nikolay Bashlykov",
      "Nikolay Bogoychev",
      "Niladri Chatterji",
      "Ning Zhang",
      "Olivier Duchenne",
      "Onur Çelebi",
      "Patrick Alrassy",
      "Pengchuan Zhang",
      "Pengwei Li",
      "Petar Vasic",
      "Peter Weng",
      "Prajjwal Bhargava",
      "Pratik Dubal",
      "Praveen Krishnan",
      "Punit Singh Koura",
      "Puxin Xu",
      "Qing He",
      "Qingxiao Dong",
      "Ragavan Srinivasan",
      "Raj Ganapathy",
      "Ramon Calderer",
      "Ricardo Silveira Cabral",
      "Robert Stojnic",
      "Roberta Raileanu",
      "Rohan Maheswari",
      "Rohit Girdhar",
      "Rohit Patel",
      "Romain Sauvestre",
      "Ronnie Polidoro",
      "Roshan Sumbaly",
      "Ross Taylor",
      "Ruan Silva",
      "Rui Hou",
      "Rui Wang",
      "Saghar Hosseini",
      "Sahana Chennabasappa",
      "Sanjay Singh",
      "Sean Bell",
      "Seohyun Sonia Kim",
      "Sergey Edunov",
      "Shaoliang Nie",
      "Sharan Narang",
      "Sharath Raparthy",
      "Sheng Shen",
      "Shengye Wan",
      "Shruti Bhosale",
      "Shun Zhang",
      "Simon Vandenhende",
      "Soumya Batra",
      "Spencer Whitman",
      "Sten Sootla",
      "Stephane Collot",
      "Suchin Gururangan",
      "Sydney Borodinsky",
      "Tamar Herman",
      "Tara Fowler",
      "Tarek Sheasha",
      "Thomas Georgiou",
      "Thomas Scialom",
      "Tobias Speckbacher",
      "Todor Mihaylov",
      "Tong Xiao",
      "Ujjwal Karn",
      "Vedanuj Goswami",
      "Vibhor Gupta",
      "Vignesh Ramanathan",
      "Viktor Kerkez",
      "Vincent Gonguet",
      "Virginie Do",
      "Vish Vogeti",
      "Vítor Albiero",
      "Vladan Petrovic",
      "Weiwei Chu",
      "Wenhan Xiong",
      "Wenyin Fu",
      "Whitney Meers",
      "Xavier Martinet",
      "Xiaodong Wang",
      "Xiaofang Wang",
      "Xiaoqing Ellen Tan",
      "Xide Xia",
      "Xinfeng Xie",
      "Xuchao Jia",
      "Xuewei Wang",
      "Yaelle Goldschlag",
      "Yashesh Gaur",
      "Yasmine Babaei",
      "Yi Wen",
      "Yiwen Song",
      "Yuchen Zhang",
      "Yue Li",
      "Yuning Mao",
      "Zacharie Delpierre Coudert",
      "Zheng Yan",
      "Zhengxing Chen",
      "Zoe Papakipos",
      "Aaditya Singh",
      "Aayushi Srivastava",
      "Abha Jain",
      "Adam Kelsey",
      "Adam Shajnfeld",
      "Adithya Gangidi",
      "Adolfo Victoria",
      "Ahuva Goldstand",
      "Ajay Menon",
      "Ajay Sharma",
      "Alex Boesenberg",
      "Alexei Baevski",
      "Allie Feinstein",
      "Amanda Kallet",
      "Amit Sangani",
      "Amos Teo",
      "Anam Yunus",
      "Andrei Lupu",
      "Andres Alvarado",
      "Andrew Caples",
      "Andrew Gu",
      "Andrew Ho",
      "Andrew Poulton",
      "Andrew Ryan",
      "Ankit Ramchandani",
      "Annie Dong",
      "Annie Franco",
      "Anuj Goyal",
      "Aparajita Saraf",
      "Arkabandhu Chowdhury",
      "Ashley Gabriel",
      "Ashwin Bharambe",
      "Assaf Eisenman",
      "Azadeh Yazdan",
      "Beau James",
      "Ben Maurer",
      "Benjamin Leonhardi",
      "Bernie Huang",
      "Beth Loyd",
      "Beto De Paola",
      "Bhargavi Paranjape",
      "Bing Liu",
      "Bo Wu",
      "Boyu Ni",
      "Braden Hancock",
      "Bram Wasti",
      "Brandon Spence",
      "Brani Stojkovic",
      "Brian Gamido",
      "Britt Montalvo",
      "Carl Parker",
      "Carly Burton",
      "Catalina Mejia",
      "Ce Liu",
      "Changhan Wang",
      "Changkyu Kim",
      "Chao Zhou",
      "Chester Hu",
      "Ching-Hsiang Chu",
      "Chris Cai",
      "Chris Tindal",
      "Christoph Feichtenhofer",
      "Cynthia Gao",
      "Damon Civin",
      "Dana Beaty",
      "Daniel Kreymer",
      "Daniel Li",
      "David Adkins",
      "David Xu",
      "Davide Testuggine",
      "Delia David",
      "Devi Parikh",
      "Diana Liskovich",
      "Didem Foss",
      "Dingkang Wang",
      "Duc Le",
      "Dustin Holland",
      "Edward Dowling",
      "Eissa Jamil",
      "Elaine Montgomery",
      "Eleonora Presani",
      "Emily Hahn",
      "Emily Wood",
      "Eric-Tuan Le",
      "Erik Brinkman",
      "Esteban Arcaute",
      "Evan Dunbar",
      "Evan Smothers",
      "Fei Sun",
      "Felix Kreuk",
      "Feng Tian",
      "Filippos Kokkinos",
      "Firat Ozgenel",
      "Francesco Caggioni",
      "Frank Kanayet",
      "Frank Seide",
      "Gabriela Medina Florez",
      "Gabriella Schwarz",
      "Gada Badeer",
      "Georgia Swee",
      "Gil Halpern",
      "Grant Herman",
      "Grigory Sizov",
      "Guangyi",
      "Zhang",
      "Guna Lakshminarayanan",
      "Hakan Inan",
      "Hamid Shojanazeri",
      "Han Zou",
      "Hannah Wang",
      "Hanwen Zha",
      "Haroun Habeeb",
      "Harrison Rudolph",
      "Helen Suk",
      "Henry Aspegren",
      "Hunter Goldman",
      "Hongyuan Zhan",
      "Ibrahim Damlaj",
      "Igor Molybog",
      "Igor Tufanov",
      "Ilias Leontiadis",
      "Irina-Elena Veliche",
      "Itai Gat",
      "Jake Weissman",
      "James Geboski",
      "James Kohli",
      "Janice Lam",
      "Japhet Asher",
      "Jean-Baptiste Gaya",
      "Jeff Marcus",
      "Jeff Tang",
      "Jennifer Chan",
      "Jenny Zhen",
      "Jeremy Reizenstein",
      "Jeremy Teboul",
      "Jessica Zhong",
      "Jian Jin",
      "Jingyi Yang",
      "Joe Cummings",
      "Jon Carvill",
      "Jon Shepard",
      "Jonathan McPhie",
      "Jonathan Torres",
      "Josh Ginsburg",
      "Junjie Wang",
      "Kai Wu",
      "Kam Hou U",
      "Karan Saxena",
      "Kartikay Khandelwal",
      "Katayoun Zand",
      "Kathy Matosich",
      "Kaushik Veeraraghavan",
      "Kelly Michelena",
      "Keqian Li",
      "Kiran Jagadeesh",
      "Kun Huang",
      "Kunal Chawla",
      "Kyle Huang",
      "Lailin Chen",
      "Lakshya Garg",
      "Lavender A",
      "Leandro Silva",
      "Lee Bell",
      "Lei Zhang",
      "Liangpeng Guo",
      "Licheng Yu",
      "Liron Moshkovich",
      "Luca Wehrstedt",
      "Madian Khabsa",
      "Manav Avalani",
      "Manish Bhatt",
      "Martynas Mankus",
      "Matan Hasson",
      "Matthew Lennie",
      "Matthias Reso",
      "Maxim Groshev",
      "Maxim Naumov",
      "Maya Lathi",
      "Meghan Keneally",
      "Miao Liu",
      "Michael L. Seltzer",
      "Michal Valko",
      "Michelle Restrepo",
      "Mihir Patel",
      "Mik Vyatskov",
      "Mikayel Samvelyan",
      "Mike Clark",
      "Mike Macey",
      "Mike Wang",
      "Miquel Jubert Hermoso",
      "Mo Metanat",
      "Mohammad Rastegari",
      "Munish Bansal",
      "Nandhini Santhanam",
      "Natascha Parks",
      "Natasha White",
      "Navyata Bawa",
      "Nayan Singhal",
      "Nick Egebo",
      "Nicolas Usunier",
      "Nikhil Mehta",
      "Nikolay Pavlovich Laptev",
      "Ning Dong",
      "Norman Cheng",
      "Oleg Chernoguz",
      "Olivia Hart",
      "Omkar Salpekar",
      "Ozlem Kalinli",
      "Parkin Kent",
      "Parth Parekh",
      "Paul Saab",
      "Pavan Balaji",
      "Pedro Rittner",
      "Philip Bontrager",
      "Pierre Roux",
      "Piotr Dollar",
      "Polina Zvyagina",
      "Prashant Ratanchandani",
      "Pritish Yuvraj",
      "Qian Liang",
      "Rachad Alao",
      "Rachel Rodriguez",
      "Rafi Ayub",
      "Raghotham Murthy",
      "Raghu Nayani",
      "Rahul Mitra",
      "Rangaprabhu Parthasarathy",
      "Raymond Li",
      "Rebekkah Hogan",
      "Robin Battey",
      "Rocky Wang",
      "Russ Howes",
      "Ruty Rinott",
      "Sachin Mehta",
      "Sachin Siby",
      "Sai Jayesh Bondu",
      "Samyak Datta",
      "Sara Chugh",
      "Sara Hunt",
      "Sargun Dhillon",
      "Sasha Sidorov",
      "Satadru Pan",
      "Saurabh Mahajan",
      "Saurabh Verma",
      "Seiji Yamamoto",
      "Sharadh Ramaswamy",
      "Shaun Lindsay",
      "Shaun Lindsay",
      "Sheng Feng",
      "Shenghao Lin",
      "Shengxin Cindy Zha",
      "Shishir Patil",
      "Shiva Shankar",
      "Shuqiang Zhang",
      "Shuqiang Zhang",
      "Sinong Wang",
      "Sneha Agarwal",
      "Soji Sajuyigbe",
      "Soumith Chintala",
      "Stephanie Max",
      "Stephen Chen",
      "Steve Kehoe",
      "Steve Satterfield",
      "Sudarshan Govindaprasad",
      "Sumit Gupta",
      "Summer Deng",
      "Sungmin Cho",
      "Sunny Virk",
      "Suraj Subramanian",
      "Sy Choudhury",
      "Sydney Goldman",
      "Tal Remez",
      "Tamar Glaser",
      "Tamara Best",
      "Thilo Koehler",
      "Thomas Robinson",
      "Tianhe Li",
      "Tianjun Zhang",
      "Tim Matthews",
      "Timothy Chou",
      "Tzook Shaked",
      "Varun Vontimitta",
      "Victoria Ajayi",
      "Victoria Montanez",
      "Vijai Mohan",
      "Vinay Satish Kumar",
      "Vishal Mangla",
      "Vlad Ionescu",
      "Vlad Poenaru",
      "Vlad Tiberiu Mihailescu",
      "Vladimir Ivanov",
      "Wei Li",
      "Wenchen Wang",
      "Wenwen Jiang",
      "Wes Bouaziz",
      "Will Constable",
      "Xiaocheng Tang",
      "Xiaojian Wu",
      "Xiaolan Wang",
      "Xilun Wu",
      "Xinbo Gao",
      "Yaniv Kleinman",
      "Yanjun Chen",
      "Ye Hu",
      "Ye Jia",
      "Ye Qi",
      "Yenda Li",
      "Yilin Zhang",
      "Ying Zhang",
      "Yossi Adi",
      "Youngjin Nam",
      "Yu",
      "Wang",
      "Yu Zhao",
      "Yuchen Hao",
      "Yundi Qian",
      "Yunlu Li",
      "Yuzi He",
      "Zach Rait",
      "Zachary DeVito",
      "Zef Rosnbrick",
      "Zhaoduo Wen",
      "Zhenyu Yang",
      "Zhiwei Zhao",
      "Zhiyu Ma"
    ],
    "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21783v3",
    "published_date": "2024-07-31 17:54:27 UTC",
    "updated_date": "2024-11-23 23:27:33 UTC"
  },
  {
    "arxiv_id": "2407.21778v1",
    "title": "Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries",
    "authors": [
      "Felix Ocker",
      "Daniel Tanneberg",
      "Julian Eggert",
      "Michael Gienger"
    ],
    "abstract": "We introduce tulip agent, an architecture for autonomous LLM-based agents\nwith Create, Read, Update, and Delete access to a tool library containing a\npotentially large number of tools. In contrast to state-of-the-art\nimplementations, tulip agent does not encode the descriptions of all available\ntools in the system prompt, which counts against the model's context window, or\nembed the entire prompt for retrieving suitable tools. Instead, the tulip agent\ncan recursively search for suitable tools in its extensible tool library,\nimplemented exemplarily as a vector store. The tulip agent architecture\nsignificantly reduces inference costs, allows using even large tool libraries,\nand enables the agent to adapt and extend its set of tools. We evaluate the\narchitecture with several ablation studies in a mathematics context and\ndemonstrate its generalizability with an application to robotics. A reference\nimplementation and the benchmark are available at\ngithub.com/HRI-EU/tulip_agent.",
    "categories": [
      "cs.AI",
      "cs.RO",
      "H.3.3; I.2.6; I.2.8; I.2.9"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.21778v1",
    "published_date": "2024-07-31 17:50:54 UTC",
    "updated_date": "2024-07-31 17:50:54 UTC"
  },
  {
    "arxiv_id": "2407.21770v3",
    "title": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts",
    "authors": [
      "Xi Victoria Lin",
      "Akshat Shrivastava",
      "Liang Luo",
      "Srinivasan Iyer",
      "Mike Lewis",
      "Gargi Ghosh",
      "Luke Zettlemoyer",
      "Armen Aghajanyan"
    ],
    "abstract": "We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "v2 -> update related work section v3 -> fix spelling",
    "pdf_url": "http://arxiv.org/pdf/2407.21770v3",
    "published_date": "2024-07-31 17:46:51 UTC",
    "updated_date": "2024-08-12 16:20:37 UTC"
  },
  {
    "arxiv_id": "2407.21753v2",
    "title": "Characterizing User Archetypes and Discussions on Scored.co",
    "authors": [
      "Andrea Failla",
      "Salvatore Citraro",
      "Giulio Rossetti",
      "Francesco Cauteruccio"
    ],
    "abstract": "In recent years, the proliferation of social platforms has drastically\ntransformed the way individuals interact, organize, and share information. In\nthis scenario, we experience an unprecedented increase in the scale and\ncomplexity of interactions and, at the same time, little to no research about\nsome fringe social platforms. In this paper, we present a multi-dimensional\nframework for characterizing nodes and hyperedges in social hypernetworks, with\na focus on the understudied alt-right platform Scored.co. Our approach\nintegrates the possibility of studying higher-order interactions, thanks to the\nhypernetwork representation, and various node features such as user activity,\nsentiment, and toxicity, with the aim to define distinct user archetypes and\nunderstand their roles within the network. Utilizing a comprehensive dataset\nfrom Scored.co, we analyze the dynamics of these archetypes over time and\nexplore their interactions and influence within the community. The framework's\nversatility allows for detailed analysis of both individual user behaviors and\nbroader social structures. Our findings highlight the importance of\nhigher-order interactions in understanding social dynamics, offering new\ninsights into the roles and behaviors that emerge in complex online\nenvironments.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21753v2",
    "published_date": "2024-07-31 17:18:25 UTC",
    "updated_date": "2024-11-22 16:39:04 UTC"
  },
  {
    "arxiv_id": "2407.21742v1",
    "title": "HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection",
    "authors": [
      "Junwei He",
      "Qianqian Xu",
      "Yangbangyan Jiang",
      "Zitai Wang",
      "Yuchen Sun",
      "Qingming Huang"
    ],
    "abstract": "With the progressive advancements in deep graph learning, out-of-distribution\n(OOD) detection for graph data has emerged as a critical challenge. While the\nefficacy of auxiliary datasets in enhancing OOD detection has been extensively\nstudied for image and text data, such approaches have not yet been explored for\ngraph data. Unlike Euclidean data, graph data exhibits greater diversity but\nlower robustness to perturbations, complicating the integration of outliers. To\ntackle these challenges, we propose the introduction of \\textbf{H}ybrid\nExternal and Internal \\textbf{G}raph \\textbf{O}utlier \\textbf{E}xposure (HGOE)\nto improve graph OOD detection performance. Our framework involves using\nrealistic external graph data from various domains and synthesizing internal\noutliers within ID subgroups to address the poor robustness and presence of OOD\nsamples within the ID class. Furthermore, we develop a boundary-aware OE loss\nthat adaptively assigns weights to outliers, maximizing the use of high-quality\nOOD samples while minimizing the impact of low-quality ones. Our proposed HGOE\nframework is model-agnostic and designed to enhance the effectiveness of\nexisting graph OOD detection models. Experimental results demonstrate that our\nHGOE framework can significantly improve the performance of existing OOD\ndetection models across all 8 real datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the 32nd ACM International Conference on Multimedia",
    "pdf_url": "http://arxiv.org/pdf/2407.21742v1",
    "published_date": "2024-07-31 16:55:18 UTC",
    "updated_date": "2024-07-31 16:55:18 UTC"
  },
  {
    "arxiv_id": "2407.21740v2",
    "title": "Contrastive Factor Analysis",
    "authors": [
      "Zhibin Duan",
      "Tiansheng Wen",
      "Yifei Wang",
      "Chen Zhu",
      "Bo Chen",
      "Mingyuan Zhou"
    ],
    "abstract": "Factor analysis, often regarded as a Bayesian variant of matrix\nfactorization, offers superior capabilities in capturing uncertainty, modeling\ncomplex dependencies, and ensuring robustness. As the deep learning era\narrives, factor analysis is receiving less and less attention due to their\nlimited expressive ability. On the contrary, contrastive learning has emerged\nas a potent technique with demonstrated efficacy in unsupervised\nrepresentational learning. While the two methods are different paradigms,\nrecent theoretical analysis has revealed the mathematical equivalence between\ncontrastive learning and matrix factorization, providing a potential\npossibility for factor analysis combined with contrastive learning. Motivated\nby the interconnectedness of contrastive learning, matrix factorization, and\nfactor analysis, this paper introduces a novel Contrastive Factor Analysis\nframework, aiming to leverage factor analysis's advantageous properties within\nthe realm of contrastive learning. To further leverage the interpretability\nproperties of non-negative factor analysis, which can learn disentangled\nrepresentations, contrastive factor analysis is extended to a non-negative\nversion. Finally, extensive experimental validation showcases the efficacy of\nthe proposed contrastive (non-negative) factor analysis methodology across\nmultiple key properties, including expressiveness, robustness,\ninterpretability, and accurate uncertainty estimation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21740v2",
    "published_date": "2024-07-31 16:52:00 UTC",
    "updated_date": "2024-08-01 03:16:43 UTC"
  },
  {
    "arxiv_id": "2407.21739v1",
    "title": "A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation",
    "authors": [
      "Mothilal Asokan",
      "Joseph Geo Benjamin",
      "Mohammad Yaqub",
      "Karthik Nandakumar"
    ],
    "abstract": "Adapting foundation models for medical image analysis requires finetuning\nthem on a considerable amount of data because of extreme distribution shifts\nbetween natural (source) data used for pretraining and medical (target) data.\nHowever, collecting task-specific medical data for such finetuning at a central\nlocation raises many privacy concerns. Although Federated learning (FL)\nprovides an effective means for training on private decentralized data,\ncommunication costs in federating large foundation models can quickly become a\nsignificant bottleneck, impacting the solution's scalability. In this work, we\naddress this problem of efficient communication while ensuring effective\nlearning in FL by combining the strengths of Parameter-Efficient Fine-tuning\n(PEFT) with FL. Specifically, we study plug-and-play Low-Rank Adapters (LoRA)\nin a federated manner to adapt the Segment Anything Model (SAM) for 3D medical\nimage segmentation. Unlike prior works that utilize LoRA and finetune the\nentire decoder, we critically analyze the contribution of each granular\ncomponent of SAM on finetuning performance. Thus, we identify specific layers\nto be federated that are very efficient in terms of communication cost while\nproducing on-par accuracy. Our experiments show that retaining the parameters\nof the SAM model (including most of the decoder) in their original state during\nadaptation is beneficial because fine-tuning on small datasets tends to distort\nthe inherent capabilities of the underlying foundation model. On Fed-KiTS, our\napproach decreases communication cost (~48x) compared to full fine-tuning while\nincreasing performance (~6% Dice score) in 3D segmentation tasks. Our approach\nperforms similar to SAMed while achieving ~2.8x reduction in communication and\nparameters to be finetuned. We further validate our approach with experiments\non Fed-IXI and Prostate MRI datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21739v1",
    "published_date": "2024-07-31 16:48:06 UTC",
    "updated_date": "2024-07-31 16:48:06 UTC"
  },
  {
    "arxiv_id": "2407.21738v1",
    "title": "Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification using Ultrasound Scan Videos",
    "authors": [
      "Joseph Geo Benjamin",
      "Mothilal Asokan",
      "Amna Alhosani",
      "Hussain Alasmawi",
      "Werner Gerhard Diehl",
      "Leanne Bricker",
      "Karthik Nandakumar",
      "Mohammad Yaqub"
    ],
    "abstract": "Self-supervised learning (SSL) methods are popular since they can address\nsituations with limited annotated data by directly utilising the underlying\ndata distribution. However, the adoption of such methods is not explored enough\nin ultrasound (US) imaging, especially for fetal assessment. We investigate the\npotential of dual-encoder SSL in utilizing unlabelled US video data to improve\nthe performance of challenging downstream Standard Fetal Cardiac Planes (SFCP)\nclassification using limited labelled 2D US images. We study 7 SSL approaches\nbased on reconstruction, contrastive loss, distillation, and information theory\nand evaluate them extensively on a large private US dataset. Our observations\nand findings are consolidated from more than 500 downstream training\nexperiments under different settings. Our primary observation shows that for\nSSL training, the variance of the dataset is more crucial than its size because\nit allows the model to learn generalisable representations, which improve the\nperformance of downstream tasks. Overall, the BarlowTwins method shows robust\nperformance, irrespective of the training settings and data variations, when\nused as an initialisation for downstream tasks. Notably, full fine-tuning with\n1% of labelled data outperforms ImageNet initialisation by 12% in F1-score and\noutperforms other SSL initialisations by at least 4% in F1-score, thus making\nit a promising candidate for transfer learning from US video to image data.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Simplifying Medical Ultrasound: 4th International Workshop, ASMUS\n  2023, Held in Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 8,\n  2023, Proceedings",
    "pdf_url": "http://arxiv.org/pdf/2407.21738v1",
    "published_date": "2024-07-31 16:47:21 UTC",
    "updated_date": "2024-07-31 16:47:21 UTC"
  },
  {
    "arxiv_id": "2408.00051v1",
    "title": "Areas of Improvement for Autonomous Vehicles: A Machine Learning Analysis of Disengagement Reports",
    "authors": [
      "Tyler Ward"
    ],
    "abstract": "Since 2014, the California Department of Motor Vehicles (CDMV) has compiled\ninformation from manufacturers of autonomous vehicles (AVs) regarding factors\nthat lead to the disengagement from autonomous driving mode in these vehicles.\nThese disengagement reports (DRs) contain information detailing whether the AV\ndisengaged from autonomous mode due to technology failure, manual override, or\nother factors during driving tests. This paper presents a machine learning (ML)\nbased analysis of the information from the 2023 DRs. We use a natural language\nprocessing (NLP) approach to extract important information from the description\nof a disengagement, and use the k-Means clustering algorithm to group report\nentries together. The cluster frequency is then analyzed, and each cluster is\nmanually categorized based on the factors leading to disengagement. We discuss\nfindings from previous years' DRs, and provide our own analysis to identify\nareas of improvement for AVs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00051v1",
    "published_date": "2024-07-31 16:36:10 UTC",
    "updated_date": "2024-07-31 16:36:10 UTC"
  },
  {
    "arxiv_id": "2407.21729v1",
    "title": "ParLS-PBO: A Parallel Local Search Solver for Pseudo Boolean Optimization",
    "authors": [
      "Zhihan Chen",
      "Peng Lin",
      "Hao Hu",
      "Shaowei Cai"
    ],
    "abstract": "As a broadly applied technique in numerous optimization problems, recently,\nlocal search has been employed to solve Pseudo-Boolean Optimization (PBO)\nproblem. A representative local search solver for PBO is LSPBO. In this paper,\nfirstly, we improve LSPBO by a dynamic scoring mechanism, which dynamically\nstrikes a balance between score on hard constraints and score on the objective\nfunction.\n  Moreover, on top of this improved LSPBO , we develop the first parallel local\nsearch PBO solver. The main idea is to share good solutions among different\nthreads to guide the search, by maintaining a pool of feasible solutions. For\nevaluating solutions when updating the pool, we propose a function that\nconsiders both the solution quality and the diversity of the pool. Furthermore,\nwe calculate the polarity density in the pool to enhance the scoring function\nof local search. Our empirical experiments show clear benefits of the proposed\nparallel approach, making it competitive with the parallel version of the\nfamous commercial solver Gurobi.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages,2 figures, to be published in The 30th International\n  Conference on Principles and Practice of Constraint Programming",
    "pdf_url": "http://arxiv.org/pdf/2407.21729v1",
    "published_date": "2024-07-31 16:30:04 UTC",
    "updated_date": "2024-07-31 16:30:04 UTC"
  },
  {
    "arxiv_id": "2407.21726v1",
    "title": "Artificial Intelligence Approaches for Energy Efficiency: A Review",
    "authors": [
      "Alberto Pasqualetto",
      "Lorenzo Serafini",
      "Michele Sprocatti"
    ],
    "abstract": "United Nations set Sustainable Development Goals and this paper focuses on\n7th (Affordable and Clean Energy), 9th (Industries, Innovation and\nInfrastructure), and 13th (Climate Action) goals. Climate change is a major\nconcern in our society; for this reason, a current global objective is to\nreduce energy waste. This work summarizes all main approaches towards energy\nefficiency using Artificial Intelligence with a particular focus on multi-agent\nsystems to create smart buildings. It mentions the tight relationship between\nAI, especially IoT, and Big Data. It explains the application of AI to anomaly\ndetection in smart buildings and a possible classification of Intelligent\nEnergy Management Systems: Direct and Indirect. Finally, some drawbacks of AI\napproaches and some possible future research focuses are proposed.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21726v1",
    "published_date": "2024-07-31 16:24:52 UTC",
    "updated_date": "2024-07-31 16:24:52 UTC"
  },
  {
    "arxiv_id": "2407.21721v1",
    "title": "Open-Vocabulary Audio-Visual Semantic Segmentation",
    "authors": [
      "Ruohao Guo",
      "Liao Qu",
      "Dantong Niu",
      "Yanyu Qi",
      "Wenzhen Yue",
      "Ji Shi",
      "Bowei Xing",
      "Xianghua Ying"
    ],
    "abstract": "Audio-visual semantic segmentation (AVSS) aims to segment and classify\nsounding objects in videos with acoustic cues. However, most approaches operate\non the close-set assumption and only identify pre-defined categories from\ntraining data, lacking the generalization ability to detect novel categories in\npractical applications. In this paper, we introduce a new task: open-vocabulary\naudio-visual semantic segmentation, extending AVSS task to open-world scenarios\nbeyond the annotated label space. This is a more challenging task that requires\nrecognizing all categories, even those that have never been seen nor heard\nduring training. Moreover, we propose the first open-vocabulary AVSS framework,\nOV-AVSS, which mainly consists of two parts: 1) a universal sound source\nlocalization module to perform audio-visual fusion and locate all potential\nsounding objects and 2) an open-vocabulary classification module to predict\ncategories with the help of the prior knowledge from large-scale pre-trained\nvision-language models. To properly evaluate the open-vocabulary AVSS, we split\nzero-shot training and testing subsets based on the AVSBench-semantic\nbenchmark, namely AVSBench-OV. Extensive experiments demonstrate the strong\nsegmentation and zero-shot generalization ability of our model on all\ncategories. On the AVSBench-OV dataset, OV-AVSS achieves 55.43% mIoU on base\ncategories and 29.14% mIoU on novel categories, exceeding the state-of-the-art\nzero-shot method by 41.88%/20.61% and open-vocabulary method by 10.2%/11.6%.\nThe code is available at https://github.com/ruohaoguo/ovavss.",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "primary_category": "cs.MM",
    "comment": "Accepted by ACM MM 2024 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2407.21721v1",
    "published_date": "2024-07-31 16:14:09 UTC",
    "updated_date": "2024-07-31 16:14:09 UTC"
  },
  {
    "arxiv_id": "2407.21717v1",
    "title": "Assessing the State of AI Policy",
    "authors": [
      "Joanna F. DeFranco",
      "Luke Biersmith"
    ],
    "abstract": "The deployment of artificial intelligence (AI) applications has accelerated\nrapidly. AI enabled technologies are facing the public in many ways including\ninfrastructure, consumer products and home applications. Because many of these\ntechnologies present risks either in the form of physical injury, or bias,\npotentially yielding unfair outcomes, policy makers must consider the need for\noversight. Most policymakers, however, lack the technical knowledge to judge\nwhether an emerging AI technology is safe, effective, and requires oversight,\ntherefore policy makers must depend on expert opinion. But policymakers are\nbetter served when, in addition to expert opinion, they have some general\nunderstanding of existing guidelines and regulations. This work provides an\noverview [the landscape] of AI legislation and directives at the international,\nU.S. state, city and federal levels. It also reviews relevant business\nstandards, and technical society initiatives. Then an overlap and gap analysis\nare performed resulting in a reference guide that includes recommendations and\nguidance for future policy making.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21717v1",
    "published_date": "2024-07-31 16:09:25 UTC",
    "updated_date": "2024-07-31 16:09:25 UTC"
  },
  {
    "arxiv_id": "2407.21714v1",
    "title": "UMMAN: Unsupervised Multi-graph Merge Adversarial Network for Disease Prediction Based on Intestinal Flora",
    "authors": [
      "Dingkun Liu",
      "Hongjie Zhou",
      "Yilu Qu",
      "Huimei Zhang",
      "Yongdong Xu"
    ],
    "abstract": "The abundance of intestinal flora is closely related to human diseases, but\ndiseases are not caused by a single gut microbe. Instead, they result from the\ncomplex interplay of numerous microbial entities. This intricate and implicit\nconnection among gut microbes poses a significant challenge for disease\nprediction using abundance information from OTU data. Recently, several methods\nhave shown potential in predicting corresponding diseases. However, these\nmethods fail to learn the inner association among gut microbes from different\nhosts, leading to unsatisfactory performance. In this paper, we present a novel\narchitecture, Unsupervised Multi-graph Merge Adversarial Network (UMMAN). UMMAN\ncan obtain the embeddings of nodes in the Multi-Graph in an unsupervised\nscenario, so that it helps learn the multiplex association. Our method is the\nfirst to combine Graph Neural Network with the task of intestinal flora disease\nprediction. We employ complex relation-types to construct the Original-Graph\nand disrupt the relationships among nodes to generate corresponding\nShuffled-Graph. We introduce the Node Feature Global Integration (NFGI) module\nto represent the global features of the graph. Furthermore, we design a joint\nloss comprising adversarial loss and hybrid attention loss to ensure that the\nreal graph embedding aligns closely with the Original-Graph and diverges from\nthe Shuffled-Graph. Comprehensive experiments on five classical OTU gut\nmicrobiome datasets demonstrate the effectiveness and stability of our method.\n(We will release our code soon.)",
    "categories": [
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21714v1",
    "published_date": "2024-07-31 16:06:43 UTC",
    "updated_date": "2024-07-31 16:06:43 UTC"
  },
  {
    "arxiv_id": "2407.21713v2",
    "title": "Social Learning through Interactions with Other Agents: A Survey",
    "authors": [
      "Dylan Hillier",
      "Cheston Tan",
      "Jing Jiang"
    ],
    "abstract": "Social learning plays an important role in the development of human\nintelligence. As children, we imitate our parents' speech patterns until we are\nable to produce sounds; we learn from them praising us and scolding us; and as\nadults, we learn by working with others. In this work, we survey the degree to\nwhich this paradigm -- social learning -- has been mirrored in machine\nlearning. In particular, since learning socially requires interacting with\nothers, we are interested in how embodied agents can and have utilised these\ntechniques. This is especially in light of the degree to which recent advances\nin natural language processing (NLP) enable us to perform new forms of social\nlearning. We look at how behavioural cloning and next-token prediction mirror\nhuman imitation, how learning from human feedback mirrors human education, and\nhow we can go further to enable fully communicative agents that learn from each\nother. We find that while individual social learning techniques have been used\nsuccessfully, there has been little unifying work showing how to bring them\ntogether into socially embodied agents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.7; I.2.0"
    ],
    "primary_category": "cs.LG",
    "comment": "To be published in IJCAI 2024, available on http://www.ijcai.org",
    "pdf_url": "http://arxiv.org/pdf/2407.21713v2",
    "published_date": "2024-07-31 16:06:34 UTC",
    "updated_date": "2024-08-04 03:08:24 UTC"
  },
  {
    "arxiv_id": "2407.21708v1",
    "title": "CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature",
    "authors": [
      "Stefan Langer",
      "Fabian Neuhaus",
      "Andreas Nürnberger"
    ],
    "abstract": "Ontologies are formal representations of knowledge in specific domains that\nprovide a structured framework for organizing and understanding complex\ninformation. Creating ontologies, however, is a complex and time-consuming\nendeavor. ChEBI is a well-known ontology in the field of chemistry, which\nprovides a comprehensive resource for defining chemical entities and their\nproperties. However, it covers only a small fraction of the rapidly growing\nknowledge in chemistry and does not provide references to the scientific\nliterature. To address this, we propose a methodology that involves augmenting\nexisting annotated text corpora with knowledge from Chebi and fine-tuning a\nlarge language model (LLM) to recognize chemical entities and their roles in\nscientific text. Our experiments demonstrate the effectiveness of our approach.\nBy combining ontological knowledge and the language understanding capabilities\nof LLMs, we achieve high precision and recall rates in identifying both the\nchemical entities and roles in scientific literature. Furthermore, we extract\nthem from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a\nknowledge graph (KG) of chemical entities and roles (CEAR), which provides\ncomplementary information to ChEBI, and can help to extend it.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21708v1",
    "published_date": "2024-07-31 15:56:06 UTC",
    "updated_date": "2024-07-31 15:56:06 UTC"
  },
  {
    "arxiv_id": "2407.21693v3",
    "title": "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities",
    "authors": [
      "Ming Zhang",
      "Caishuang Huang",
      "Yilong Wu",
      "Shichun Liu",
      "Huiyuan Zheng",
      "Yurui Dong",
      "Yujiong Shen",
      "Shihan Dou",
      "Jun Zhao",
      "Junjie Ye",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang"
    ],
    "abstract": "Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented\nconversations, including information collection. How to utilize TOD accurately,\nefficiently and effectively for information collection has always been a\ncritical and challenging task. Recent studies have demonstrated that Large\nLanguage Models (LLMs) excel in dialogue, instruction generation, and\nreasoning, and can significantly enhance the performance of TOD through\nfine-tuning. However, current datasets primarily cater to user-led systems and\nare limited to predefined specific scenarios and slots, thereby necessitating\nimprovements in the proactiveness, diversity, and capabilities of TOD. In this\nstudy, we present a detailed multi-domain task-oriented data construction\nprocess for conversations, and a Chinese dialogue dataset generated based on\nthis process, TransferTOD, which authentically simulates human-computer\ndialogues in 30 popular life service scenarios. Leveraging this dataset, we\ntrained a model called TransferTOD-7B using full-parameter fine-tuning,\nshowcasing notable abilities in slot filling and questioning. Our work has\ndemonstrated its strong generalization capabilities in various downstream\nscenarios, significantly enhancing both data utilization efficiency and system\nperformance. The data is released in\nhttps://github.com/KongLongGeFDU/TransferTOD.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21693v3",
    "published_date": "2024-07-31 15:38:15 UTC",
    "updated_date": "2024-10-12 11:53:03 UTC"
  },
  {
    "arxiv_id": "2407.21687v2",
    "title": "Dynamic Object Queries for Transformer-based Incremental Object Detection",
    "authors": [
      "Jichuan Zhang",
      "Wei Li",
      "Shuang Cheng",
      "Ya-Li Li",
      "Shengjin Wang"
    ],
    "abstract": "Incremental object detection (IOD) aims to sequentially learn new classes,\nwhile maintaining the capability to locate and identify old ones. As the\ntraining data arrives with annotations only with new classes, IOD suffers from\ncatastrophic forgetting. Prior methodologies mainly tackle the forgetting issue\nthrough knowledge distillation and exemplar replay, ignoring the conflict\nbetween limited model capacity and increasing knowledge. In this paper, we\nexplore \\textit{dynamic object queries} for incremental object detection built\non Transformer architecture. We propose the \\textbf{Dy}namic object\n\\textbf{Q}uery-based \\textbf{DE}tection \\textbf{TR}ansformer (DyQ-DETR), which\nincrementally expands the model representation ability to achieve\nstability-plasticity tradeoff. First, a new set of learnable object queries are\nfed into the decoder to represent new classes. These new object queries are\naggregated with those from previous phases to adapt both old and new knowledge\nwell. Second, we propose the isolated bipartite matching for object queries in\ndifferent phases, based on disentangled self-attention. The interaction among\nthe object queries at different phases is eliminated to reduce inter-class\nconfusion. Thanks to the separate supervision and computation over object\nqueries, we further present the risk-balanced partial calibration for effective\nexemplar replay. Extensive experiments demonstrate that DyQ-DETR significantly\nsurpasses the state-of-the-art methods, with limited parameter overhead. Code\nwill be made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21687v2",
    "published_date": "2024-07-31 15:29:34 UTC",
    "updated_date": "2024-08-27 12:03:00 UTC"
  },
  {
    "arxiv_id": "2407.21674v1",
    "title": "Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation",
    "authors": [
      "Krishan Agyakari Raja Babu",
      "Rachana Sathish",
      "Mrunal Pattanaik",
      "Rahul Venkataramani"
    ],
    "abstract": "Synthetic data is becoming increasingly integral in data-scarce fields such\nas medical imaging, serving as a substitute for real data. However, its\ninherent statistical characteristics can significantly impact downstream tasks,\npotentially compromising deployment performance. In this study, we empirically\ninvestigate this issue and uncover a critical phenomenon: downstream neural\nnetworks often exploit spurious distinctions between real and synthetic data\nwhen there is a strong correlation between the data source and the task label.\nThis exploitation manifests as \\textit{simplicity bias}, where models overly\nrely on superficial features rather than genuine task-related complexities.\nThrough principled experiments, we demonstrate that the source of data (real\nvs.\\ synthetic) can introduce spurious correlating factors leading to poor\nperformance during deployment when the correlation is absent. We first\ndemonstrate this vulnerability on a digit classification task, where the model\nspuriously utilizes the source of data instead of the digit to provide an\ninference. We provide further evidence of this phenomenon in a medical imaging\nproblem related to cardiac view classification in echocardiograms, particularly\ndistinguishing between 2-chamber and 4-chamber views. Given the increasing role\nof utilizing synthetic datasets, we hope that our experiments serve as\neffective guidelines for the utilization of synthetic datasets in model\ntraining.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21674v1",
    "published_date": "2024-07-31 15:14:17 UTC",
    "updated_date": "2024-07-31 15:14:17 UTC"
  },
  {
    "arxiv_id": "2407.21670v5",
    "title": "Dynamic Universal Approximation Theory: Foundations for Parallelism in Neural Networks",
    "authors": [
      "Wei Wang",
      "Qing Li"
    ],
    "abstract": "Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21670v5",
    "published_date": "2024-07-31 15:13:39 UTC",
    "updated_date": "2024-11-29 06:16:25 UTC"
  },
  {
    "arxiv_id": "2407.21666v1",
    "title": "An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification",
    "authors": [
      "Aswini Kumar Patra",
      "Ankit Varshney",
      "Lingaraj Sahoo"
    ],
    "abstract": "Early detection of drought stress is critical for taking timely measures for\nreducing crop loss before the drought impact becomes irreversible. The subtle\nphenotypical and physiological changes in response to drought stress are\ncaptured by non-invasive imaging techniques and these imaging data serve as\nvaluable resource for machine learning methods to identify drought stress.\nWhile convolutional neural networks (CNNs) are in wide use, vision transformers\n(ViTs) present a promising alternative in capturing long-range dependencies and\nintricate spatial relationships, thereby enhancing the detection of subtle\nindicators of drought stress. We propose an explainable deep learning pipeline\nthat leverages the power of ViTs for drought stress detection in potato crops\nusing aerial imagery. We applied two distinct approaches: a synergistic\ncombination of ViT and support vector machine (SVM), where ViT extracts\nintricate spatial features from aerial images, and SVM classifies the crops as\nstressed or healthy and an end-to-end approach using a dedicated classification\nlayer within ViT to directly detect drought stress. Our key findings explain\nthe ViT model's decision-making process by visualizing attention maps. These\nmaps highlight the specific spatial features within the aerial images that the\nViT model focuses as the drought stress signature. Our findings demonstrate\nthat the proposed methods not only achieve high accuracy in drought stress\nidentification but also shedding light on the diverse subtle plant features\nassociated with drought stress. This offers a robust and interpretable solution\nfor drought stress monitoring for farmers to undertake informed decisions for\nimproved crop management.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "30 pages, 6 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.21666v1",
    "published_date": "2024-07-31 15:08:26 UTC",
    "updated_date": "2024-07-31 15:08:26 UTC"
  },
  {
    "arxiv_id": "2408.04641v1",
    "title": "GPT-3 Powered Information Extraction for Building Robust Knowledge Bases",
    "authors": [
      "Ritabrata Roy Choudhury",
      "Soumik Dey"
    ],
    "abstract": "This work uses the state-of-the-art language model GPT-3 to offer a novel\nmethod of information extraction for knowledge base development. The suggested\nmethod attempts to solve the difficulties associated with obtaining relevant\nentities and relationships from unstructured text in order to extract\nstructured information. We conduct experiments on a huge corpus of text from\ndiverse fields to assess the performance of our suggested technique. The\nevaluation measures, which are frequently employed in information extraction\ntasks, include precision, recall, and F1-score. The findings demonstrate that\nGPT-3 can be used to efficiently and accurately extract pertinent and correct\ninformation from text, hence increasing the precision and productivity of\nknowledge base creation. We also assess how well our suggested approach\nperforms in comparison to the most advanced information extraction techniques\nalready in use. The findings show that by utilizing only a small number of\ninstances in in-context learning, our suggested strategy yields competitive\noutcomes with notable savings in terms of data annotation and engineering\nexpense. Additionally, we use our proposed method to retrieve Biomedical\ninformation, demonstrating its practicality in a real-world setting. All things\nconsidered, our suggested method offers a viable way to overcome the\ndifficulties involved in obtaining structured data from unstructured text in\norder to create knowledge bases. It can greatly increase the precision and\neffectiveness of information extraction, which is necessary for many\napplications including chatbots, recommendation engines, and question-answering\nsystems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.04641v1",
    "published_date": "2024-07-31 14:59:29 UTC",
    "updated_date": "2024-07-31 14:59:29 UTC"
  },
  {
    "arxiv_id": "2407.21652v2",
    "title": "Spatial Transformer Network YOLO Model for Agricultural Object Detection",
    "authors": [
      "Yash Zambre",
      "Ekdev Rajkitkul",
      "Akshatha Mohan",
      "Joshua Peeples"
    ],
    "abstract": "Object detection plays a crucial role in the field of computer vision by\nautonomously locating and identifying objects of interest. The You Only Look\nOnce (YOLO) model is an effective single-shot detector. However, YOLO faces\nchallenges in cluttered or partially occluded scenes and can struggle with\nsmall, low-contrast objects. We propose a new method that integrates spatial\ntransformer networks (STNs) into YOLO to improve performance. The proposed\nSTN-YOLO aims to enhance the model's effectiveness by focusing on important\nareas of the image and improving the spatial invariance of the model before the\ndetection process. Our proposed method improved object detection performance\nboth qualitatively and quantitatively. We explore the impact of different\nlocalization networks within the STN module as well as the robustness of the\nmodel across different spatial transformations. We apply the STN-YOLO on\nbenchmark datasets for Agricultural object detection as well as a new dataset\nfrom a state-of-the-art plant phenotyping greenhouse facility. Our code and\ndataset are publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 5 figures, accepted to 2024 IEEE International Conference on\n  Machine Learning and Applications",
    "pdf_url": "http://arxiv.org/pdf/2407.21652v2",
    "published_date": "2024-07-31 14:53:41 UTC",
    "updated_date": "2024-09-15 21:04:23 UTC"
  },
  {
    "arxiv_id": "2407.21647v1",
    "title": "Human interaction classifier for LLM based chatbot",
    "authors": [
      "Diego Martín",
      "Jordi Sanchez",
      "Xavier Vizcaíno"
    ],
    "abstract": "This study investigates different approaches to classify human interactions\nin an artificial intelligence-based environment, specifically for Applus+\nIDIADA's intelligent agent AIDA. The main objective is to develop a classifier\nthat accurately identifies the type of interaction received (Conversation,\nServices, or Document Translation) to direct requests to the appropriate\nchannel and provide a more specialized and efficient service. Various models\nare compared, including LLM-based classifiers, KNN using Titan and Cohere\nembeddings, SVM, and artificial neural networks. Results show that SVM and ANN\nmodels with Cohere embeddings achieve the best overall performance, with\nsuperior F1 scores and faster execution times compared to LLM-based approaches.\nThe study concludes that the SVM model with Cohere embeddings is the most\nsuitable option for classifying human interactions in the AIDA environment,\noffering an optimal balance between accuracy and computational efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.21647v1",
    "published_date": "2024-07-31 14:50:11 UTC",
    "updated_date": "2024-07-31 14:50:11 UTC"
  },
  {
    "arxiv_id": "2407.21642v1",
    "title": "Lyapunov weights to convey the meaning of time in physics-informed neural networks",
    "authors": [
      "Gabriel Turinici"
    ],
    "abstract": "Time is not a dimension as the others. In Physics-Informed Neural Networks\n(PINN) several proposals attempted to adapt the time sampling or time weighting\nto take into account the specifics of this special dimension. But these\nproposals are not principled and need guidance to be used. We explain here\ntheoretically why the Lyapunov exponents give actionable insights and propose a\nweighting scheme to automatically adapt to chaotic, periodic or stable\ndynamics. We characterize theoretically the best weighting scheme under\ncomputational constraints as a cumulative exponential integral of the local\nLyapunov exponent estimators and show that it performs well in practice under\nthe regimes mentioned above.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21642v1",
    "published_date": "2024-07-31 14:41:40 UTC",
    "updated_date": "2024-07-31 14:41:40 UTC"
  },
  {
    "arxiv_id": "2407.21638v1",
    "title": "Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components",
    "authors": [
      "Hermione Warr",
      "Yasin Ibrahim",
      "Daniel R. McGowan",
      "Konstantinos Kamnitsas"
    ],
    "abstract": "Automation of medical image interpretation could alleviate bottlenecks in\ndiagnostic workflows, and has become of particular interest in recent years due\nto advancements in natural language processing. Great strides have been made\ntowards automated radiology report generation via AI, yet ensuring clinical\naccuracy in generated reports is a significant challenge, hindering deployment\nof such methods in clinical practice. In this work we propose a quality control\nframework for assessing the reliability of AI-generated radiology reports with\nrespect to semantics of diagnostic importance using modular auxiliary auditing\ncomponents (AC). Evaluating our pipeline on the MIMIC-CXR dataset, our findings\nshow that incorporating ACs in the form of disease-classifiers can enable\nauditing that identifies more reliable reports, resulting in higher F1 scores\ncompared to unfiltered generated reports. Additionally, leveraging the\nconfidence of the AC labels further improves the audit's effectiveness.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to MICCAI UNSURE Workshop",
    "pdf_url": "http://arxiv.org/pdf/2407.21638v1",
    "published_date": "2024-07-31 14:37:00 UTC",
    "updated_date": "2024-07-31 14:37:00 UTC"
  },
  {
    "arxiv_id": "2408.03151v1",
    "title": "Optimizing Disease Prediction with Artificial Intelligence Driven Feature Selection and Attention Networks",
    "authors": [
      "D. Dhinakaran",
      "S. Edwin Raja",
      "M. Thiyagarajan",
      "J. Jeno Jasmine",
      "P. Raghavan"
    ],
    "abstract": "The rapid integration of machine learning methodologies in healthcare has\nignited innovative strategies for disease prediction, particularly with the\nvast repositories of Electronic Health Records (EHR) data. This article delves\ninto the realm of multi-disease prediction, presenting a comprehensive study\nthat introduces a pioneering ensemble feature selection model. This model,\ndesigned to optimize learning systems, combines statistical, deep, and\noptimally selected features through the innovative Stabilized Energy Valley\nOptimization with Enhanced Bounds (SEV-EB) algorithm. The objective is to\nachieve unparalleled accuracy and stability in predicting various disorders.\nThis work proposes an advanced ensemble model that synergistically integrates\nstatistical, deep, and optimally selected features. This combination aims to\nenhance the predictive power of the model by capturing diverse aspects of the\nhealth data. At the heart of the proposed model lies the SEV-EB algorithm, a\nnovel approach to optimal feature selection. The algorithm introduces enhanced\nbounds and stabilization techniques, contributing to the robustness and\naccuracy of the overall prediction model. To further elevate the predictive\ncapabilities, an HSC-AttentionNet is introduced. This network architecture\ncombines deep temporal convolution capabilities with LSTM, allowing the model\nto capture both short-term patterns and long-term dependencies in health data.\nRigorous evaluations showcase the remarkable performance of the proposed model.\nAchieving a 95% accuracy and 94% F1-score in predicting various disorders, the\nmodel surpasses traditional methods, signifying a significant advancement in\ndisease prediction accuracy. The implications of this research extend beyond\nthe confines of academia.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 Pages, 4 Figures",
    "pdf_url": "http://arxiv.org/pdf/2408.03151v1",
    "published_date": "2024-07-31 14:12:27 UTC",
    "updated_date": "2024-07-31 14:12:27 UTC"
  },
  {
    "arxiv_id": "2408.00041v3",
    "title": "Con4m: Context-aware Consistency Learning Framework for Segmented Time Series Classification",
    "authors": [
      "Junru Chen",
      "Tianyu Cao",
      "Jing Xu",
      "Jiahe Li",
      "Zhilong Chen",
      "Tao Xiao",
      "Yang Yang"
    ],
    "abstract": "Time Series Classification (TSC) encompasses two settings: classifying entire\nsequences or classifying segmented subsequences. The raw time series for\nsegmented TSC usually contain Multiple classes with Varying Duration of each\nclass (MVD). Therefore, the characteristics of MVD pose unique challenges for\nsegmented TSC, yet have been largely overlooked by existing works.\nSpecifically, there exists a natural temporal dependency between consecutive\ninstances (segments) to be classified within MVD. However, mainstream TSC\nmodels rely on the assumption of independent and identically distributed\n(i.i.d.), focusing on independently modeling each segment. Additionally,\nannotators with varying expertise may provide inconsistent boundary labels,\nleading to unstable performance of noise-free TSC models. To address these\nchallenges, we first formally demonstrate that valuable contextual information\nenhances the discriminative power of classification instances. Leveraging the\ncontextual priors of MVD at both the data and label levels, we propose a novel\nconsistency learning framework Con4m, which effectively utilizes contextual\ninformation more conducive to discriminating consecutive segments in segmented\nTSC tasks, while harmonizing inconsistent boundary labels for training.\nExtensive experiments across multiple datasets validate the effectiveness of\nCon4m in handling segmented TSC tasks on MVD. The source code is available at\nhttps://github.com/MrNobodyCali/Con4m.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00041v3",
    "published_date": "2024-07-31 14:06:55 UTC",
    "updated_date": "2025-04-23 09:00:31 UTC"
  },
  {
    "arxiv_id": "2408.00040v3",
    "title": "Barlow Twins Deep Neural Network for Advanced 1D Drug-Target Interaction Prediction",
    "authors": [
      "Maximilian G. Schuh",
      "Davide Boldini",
      "Annkathrin I. Bohne",
      "Stephan A. Sieber"
    ],
    "abstract": "Accurate prediction of drug-target interactions is critical for advancing\ndrug discovery. By reducing time and cost, machine learning and deep learning\ncan accelerate this laborious discovery process. In a novel approach,\nBarlowDTI, we utilise the powerful Barlow Twins architecture for\nfeature-extraction while considering the structure of the target protein. Our\nmethod achieves state-of-the-art predictive performance against multiple\nestablished benchmarks using only one-dimensional input. The use of gradient\nboosting machine as the underlying predictor ensures fast and efficient\npredictions without the need for substantial computational resources. We also\ninvestigate how the model reaches its decision based on individual training\nsamples. By comparing co-crystal structures, we find that BarlowDTI effectively\nexploits catalytically active and stabilising residues, highlighting the\nmodel's ability to generalise from one-dimensional input data. In addition, we\nfurther benchmark new baselines against existing methods. Together, these\ninnovations improve the efficiency and effectiveness of drug-target interaction\npredictions, providing robust tools for accelerating drug development and\ndeepening the understanding of molecular interactions. Therefore, we provide an\neasy-to-use web interface that can be freely accessed at\nhttps://www.bio.nat.tum.de/oc2/barlowdti .",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "Refined model architecture, additional results added",
    "pdf_url": "http://arxiv.org/pdf/2408.00040v3",
    "published_date": "2024-07-31 14:06:18 UTC",
    "updated_date": "2024-10-14 14:13:05 UTC"
  },
  {
    "arxiv_id": "2407.21615v1",
    "title": "Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music",
    "authors": [
      "Pedro Sarmento",
      "Jackson Loth",
      "Mathieu Barthet"
    ],
    "abstract": "Generative AI models have recently blossomed, significantly impacting\nartistic and musical traditions. Research investigating how humans interact\nwith and deem these models is therefore crucial. Through a listening and\nreflection study, we explore participants' perspectives on AI- vs\nhuman-generated progressive metal, in symbolic format, using rock music as a\ncontrol group. AI-generated examples were produced by ProgGP, a\nTransformer-based model. We propose a mixed methods approach to assess the\neffects of generation type (human vs. AI), genre (progressive metal vs. rock),\nand curation process (random vs. cherry-picked). This combines quantitative\nfeedback on genre congruence, preference, creativity, consistency, playability,\nhumanness, and repeatability, and qualitative feedback to provide insights into\nlisteners' experiences. A total of 32 progressive metal fans completed the\nstudy. Our findings validate the use of fine-tuning to achieve genre-specific\nspecialization in AI music generation, as listeners could distinguish between\nAI-generated rock and progressive metal. Despite some AI-generated excerpts\nreceiving similar ratings to human music, listeners exhibited a preference for\nhuman compositions. Thematic analysis identified key features for genre and AI\nvs. human distinctions. Finally, we consider the ethical implications of our\nwork in promoting musical data diversity within MIR research by focusing on an\nunder-explored genre.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Reviewed pre-print accepted for publication at ISMIR 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21615v1",
    "published_date": "2024-07-31 14:03:45 UTC",
    "updated_date": "2024-07-31 14:03:45 UTC"
  },
  {
    "arxiv_id": "2407.21611v2",
    "title": "Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism",
    "authors": [
      "Jiafeng Zhong",
      "Bin Li",
      "Jiangyan Yi"
    ],
    "abstract": "The task of partially spoofed audio localization aims to accurately determine\naudio authenticity at a frame level. Although some works have achieved\nencouraging results, utilizing boundary information within a single model\nremains an unexplored research topic. In this work, we propose a novel method\ncalled Boundary-aware Attention Mechanism (BAM). Specifically, it consists of\ntwo core modules: Boundary Enhancement and Boundary Frame-wise Attention. The\nformer assembles the intra-frame and inter-frame information to extract\ndiscriminative boundary features that are subsequently used for boundary\nposition detection and authenticity decision, while the latter leverages\nboundary prediction results to explicitly control the feature interaction\nbetween frames, which achieves effective discrimination between real and fake\nframes. Experimental results on PartialSpoof database demonstrate our proposed\nmethod achieves the best performance. The code is available at\nhttps://github.com/media-sec-lab/BAM.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21611v2",
    "published_date": "2024-07-31 13:49:17 UTC",
    "updated_date": "2024-08-19 16:09:14 UTC"
  },
  {
    "arxiv_id": "2407.21600v2",
    "title": "Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors",
    "authors": [
      "Shoujin Huang",
      "Guanxiong Luo",
      "Yunlin Zhao",
      "Yilong Liu",
      "Yuwan Wang",
      "Kexin Yang",
      "Jingzhe Liu",
      "Hua Guo",
      "Min Wang",
      "Lingyan Zhang",
      "Mengye Lyu"
    ],
    "abstract": "Simultaneous multislice (SMS) imaging is a powerful technique for\naccelerating magnetic resonance imaging (MRI) acquisitions. However, SMS\nreconstruction remains challenging due to complex signal interactions between\nand within the excited slices. In this study, we introduce ROGER, a robust SMS\nMRI reconstruction method based on deep generative priors. Utilizing denoising\ndiffusion probabilistic models (DDPM), ROGER begins with Gaussian noise and\ngradually recovers individual slices through reverse diffusion iterations while\nenforcing data consistency from measured k-space data within the readout\nconcatenation framework. The posterior sampling procedure is designed such that\nthe DDPM training can be performed on single-slice images without requiring\nmodifications for SMS tasks. Additionally, our method incorporates a\nlow-frequency enhancement (LFE) module to address the practical issue that\nSMS-accelerated fast spin echo (FSE) and echo planar imaging (EPI) sequences\ncannot easily embed fully-sampled autocalibration signals. Extensive\nexperiments on both retrospectively and prospectively accelerated datasets\ndemonstrate that ROGER consistently outperforms existing methods, enhancing\nboth anatomical and functional imaging with strong out-of-distribution\ngeneralization. The source code and sample data for ROGER are available at\nhttps://github.com/Solor-pikachu/ROGER.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "eess.SP",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "Submitted to Medical Image Analysis. New fMRI analysis and figures\n  are added since v1",
    "pdf_url": "http://arxiv.org/pdf/2407.21600v2",
    "published_date": "2024-07-31 13:34:14 UTC",
    "updated_date": "2025-01-23 07:53:34 UTC"
  },
  {
    "arxiv_id": "2407.21590v1",
    "title": "Measuring What Matters: Intrinsic Distance Preservation as a Robust Metric for Embedding Quality",
    "authors": [
      "Steven N. Hart",
      "Thomas E. Tavolara"
    ],
    "abstract": "Unsupervised embeddings are fundamental to numerous machine learning\napplications, yet their evaluation remains a challenging task. Traditional\nassessment methods often rely on extrinsic variables, such as performance in\ndownstream tasks, which can introduce confounding factors and mask the true\nquality of embeddings. This paper introduces the Intrinsic Distance\nPreservation Evaluation (IDPE) method, a novel approach for assessing embedding\nquality based on the preservation of Mahalanobis distances between data points\nin the original and embedded spaces. We demonstrate the limitations of\nextrinsic evaluation methods through a simple example, highlighting how they\ncan lead to misleading conclusions about embedding quality. IDPE addresses\nthese issues by providing a task-independent measure of how well embeddings\npreserve the intrinsic structure of the original data. Our method leverages\nefficient similarity search techniques to make it applicable to large-scale\ndatasets. We compare IDPE with established intrinsic metrics like\ntrustworthiness and continuity, as well as extrinsic metrics such as Average\nRank and Mean Reciprocal Rank. Our results show that IDPE offers a more\ncomprehensive and reliable assessment of embedding quality across various\nscenarios. We evaluate PCA and t-SNE embeddings using IDPE, revealing insights\ninto their performance that are not captured by traditional metrics. This work\ncontributes to the field by providing a robust, efficient, and interpretable\nmethod for embedding evaluation. IDPE's focus on intrinsic properties offers a\nvaluable tool for researchers and practitioners seeking to develop and assess\nhigh-quality embeddings for diverse machine learning applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21590v1",
    "published_date": "2024-07-31 13:26:09 UTC",
    "updated_date": "2024-07-31 13:26:09 UTC"
  },
  {
    "arxiv_id": "2407.21580v1",
    "title": "Voxel Scene Graph for Intracranial Hemorrhage",
    "authors": [
      "Antoine P. Sanner",
      "Nils F. Grauhan",
      "Marc A. Brockmann",
      "Ahmed E. Othman",
      "Anirban Mukhopadhyay"
    ],
    "abstract": "Patients with Intracranial Hemorrhage (ICH) face a potentially\nlife-threatening condition, and patient-centered individualized treatment\nremains challenging due to possible clinical complications. Deep-Learning-based\nmethods can efficiently analyze the routinely acquired head CTs to support the\nclinical decision-making. The majority of early work focuses on the detection\nand segmentation of ICH, but do not model the complex relations between ICH and\nadjacent brain structures. In this work, we design a tailored object detection\nmethod for ICH, which we unite with segmentation-grounded Scene Graph\nGeneration (SGG) methods to learn a holistic representation of the clinical\ncerebral scene. To the best of our knowledge, this is the first application of\nSGG for 3D voxel images. We evaluate our method on two head-CT datasets and\ndemonstrate that our model can recall up to 74% of clinically relevant\nrelations. This work lays the foundation towards SGG for 3D voxel data. The\ngenerated Scene Graphs can already provide insights for the clinician, but are\nalso valuable for all downstream tasks as a compact and interpretable\nrepresentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07",
      "I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21580v1",
    "published_date": "2024-07-31 13:10:59 UTC",
    "updated_date": "2024-07-31 13:10:59 UTC"
  },
  {
    "arxiv_id": "2407.21579v1",
    "title": "A Performance Study of LLM-Generated Code on Leetcode",
    "authors": [
      "Tristan Coignion",
      "Clément Quinton",
      "Romain Rouvoy"
    ],
    "abstract": "This study evaluates the efficiency of code generation by Large Language\nModels (LLMs) and measures their performance against human-crafted solutions\nusing a dataset from Leetcode. We compare 18 LLMs, considering factors such as\nmodel temperature and success rate, and their impact on code performance. This\nresearch introduces a novel method for measuring and comparing the speed of\nLLM-generated code, revealing that LLMs produce code with comparable\nperformance, irrespective of the adopted LLM. We also find that LLMs are\ncapable of generating code that is, on average, more efficient than the code\nwritten by humans. The paper further discusses the use of Leetcode as a\nbenchmarking dataset, the limitations imposed by potential data contamination,\nand the platform's measurement reliability. We believe that our findings\ncontribute to a better understanding of LLM capabilities in code generation and\nset the stage for future optimizations in the field.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21579v1",
    "published_date": "2024-07-31 13:10:03 UTC",
    "updated_date": "2024-07-31 13:10:03 UTC"
  },
  {
    "arxiv_id": "2407.21577v1",
    "title": "Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography",
    "authors": [
      "Kit M. Bransby",
      "Woo-jin Cho Kim",
      "Jorge Oliveira",
      "Alex Thorley",
      "Arian Beqiri",
      "Alberto Gomez",
      "Agisilaos Chartsias"
    ],
    "abstract": "Building an echocardiography view classifier that maintains performance in\nreal-life cases requires diverse multi-site data, and frequent updates with\nnewly available data to mitigate model drift. Simply fine-tuning on new\ndatasets results in \"catastrophic forgetting\", and cannot adapt to variations\nof view labels between sites. Alternatively, collecting all data on a single\nserver and re-training may not be feasible as data sharing agreements may\nrestrict image transfer, or datasets may only become available at different\ntimes. Furthermore, time and cost associated with re-training grows with every\nnew dataset. We propose a class-incremental learning method which learns an\nexpert network for each dataset, and combines all expert networks with a score\nfusion model. The influence of ``unqualified experts'' is minimised by\nweighting each contribution with a learnt in-distribution score. These weights\npromote transparency as the contribution of each expert is known during\ninference. Instead of using the original images, we use learned features from\neach dataset, which are easier to share and raise fewer licensing and privacy\nconcerns. We validate our work on six datasets from multiple sites,\ndemonstrating significant reductions in training time while improving view\nclassification performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for Oral at MICCAI workshop ASMUS-2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21577v1",
    "published_date": "2024-07-31 13:05:32 UTC",
    "updated_date": "2024-07-31 13:05:32 UTC"
  },
  {
    "arxiv_id": "2407.21571v1",
    "title": "PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning",
    "authors": [
      "Min Jae Jung",
      "JooHee Kim"
    ],
    "abstract": "Large Language Models (LLMs) encounter significant challenges in continual\nlearning due to catastrophic forgetting, where new information overwrites\npreviously acquired knowledge. This limitation leads to substantial\nenvironmental and economic waste. In this study, we introduce the PMoE,\nProgressive Mixture of Experts with Asymmetric Transformer, which aims to\nminimize forgetting by utilizing an asymmetric design with shallow layers\ndedicated to general knowledge and deep layers for new knowledge. PMoE\nincorporates progressively added experts in deep layers and a router that\nallocates new knowledge to the appropriate experts efficiently. The router,\npositioned adjacent to the deep layers, utilizes deep features aggregating\nconsolidated information. This enables the router to perform efficiently,\nallocating new knowledge to the appropriate experts, which progressively\nincrease in the deep layers. Extensive experiments on TRACE datasets and\ngeneral language understanding datasets demonstrate that the proposed PMoE\noutperforms previous state-of-the-art approaches.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21571v1",
    "published_date": "2024-07-31 12:56:14 UTC",
    "updated_date": "2024-07-31 12:56:14 UTC"
  },
  {
    "arxiv_id": "2407.21566v1",
    "title": "TRGR: Transmissive RIS-aided Gait Recognition Through Walls",
    "authors": [
      "Yunlong Huang",
      "Junshuo Liu",
      "Jianan Zhang",
      "Tiebin Mi",
      "Xin Shi",
      "Robert Caiming Qiu"
    ],
    "abstract": "Gait recognition with radio frequency (RF) signals enables many potential\napplications requiring accurate identification. However, current systems\nrequire individuals to be within a line-of-sight (LOS) environment and struggle\nwith low signal-to-noise ratio (SNR) when signals traverse concrete and thick\nwalls. To address these challenges, we present TRGR, a novel transmissive\nreconfigurable intelligent surface (RIS)-aided gait recognition system. TRGR\ncan recognize human identities through walls using only the magnitude\nmeasurements of channel state information (CSI) from a pair of transceivers.\nSpecifically, by leveraging transmissive RIS alongside a configuration\nalternating optimization algorithm, TRGR enhances wall penetration and signal\nquality, enabling accurate gait recognition. Furthermore, a residual\nconvolution network (RCNN) is proposed as the backbone network to learn robust\nhuman information. Experimental results confirm the efficacy of transmissive\nRIS, highlighting the significant potential of transmissive RIS in enhancing\nRF-based gait recognition systems. Extensive experiment results show that TRGR\nachieves an average accuracy of 97.88\\% in identifying persons when signals\ntraverse concrete walls, demonstrating the effectiveness and robustness of\nTRGR.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Globecom 2024 IoTSN accepted",
    "pdf_url": "http://arxiv.org/pdf/2407.21566v1",
    "published_date": "2024-07-31 12:42:25 UTC",
    "updated_date": "2024-07-31 12:42:25 UTC"
  },
  {
    "arxiv_id": "2407.21560v1",
    "title": "Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding",
    "authors": [
      "Jun Zhou",
      "Dongyang Yu",
      "Kamran Aziz",
      "Fangfang Su",
      "Qing Zhang",
      "Fei Li",
      "Donghong Ji"
    ],
    "abstract": "Fine-grained sentiment analysis involves extracting and organizing sentiment\nelements from textual data. However, existing approaches often overlook issues\nof category semantic inclusion and overlap, as well as inherent structural\npatterns within the target sequence. This study introduces a generative\nsentiment analysis model. To address the challenges related to category\nsemantic inclusion and overlap, a latent category distribution variable is\nintroduced. By reconstructing the input of a variational autoencoder, the model\nlearns the intensity of the relationship between categories and text, thereby\nimproving sequence generation. Additionally, a trie data structure and\nconstrained decoding strategy are utilized to exploit structural patterns,\nwhich in turn reduces the search space and regularizes the generation process.\nExperimental results on the Restaurant-ACOS and Laptop-ACOS datasets\ndemonstrate a significant performance improvement compared to baseline models.\nAblation experiments further confirm the effectiveness of latent category\ndistribution and constrained decoding strategy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21560v1",
    "published_date": "2024-07-31 12:29:17 UTC",
    "updated_date": "2024-07-31 12:29:17 UTC"
  },
  {
    "arxiv_id": "2407.21556v1",
    "title": "Operator-based semantics for choice programs: is choosing losing? (full version)",
    "authors": [
      "Jesse Heyninck"
    ],
    "abstract": "Choice constructs are an important part of the language of logic programming,\nyet the study of their semantics has been a challenging task. So far, only\ntwo-valued semantics have been studied, and the different proposals for such\nsemantics have not been compared in a principled way. In this paper, an\noperator-based framework allow for the definition and comparison of different\nsemantics in a principled way is proposed.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of a paper accepted at KR 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21556v1",
    "published_date": "2024-07-31 12:25:57 UTC",
    "updated_date": "2024-07-31 12:25:57 UTC"
  },
  {
    "arxiv_id": "2408.08318v1",
    "title": "First Analysis of the EU Artifical Intelligence Act: Towards a Global Standard for Trustworthy AI?",
    "authors": [
      "Marion Ho-Dac"
    ],
    "abstract": "The EU Artificial Intelligence Act (AI Act) came into force in the European\nUnion (EU) on 1 August 2024. It is a key piece of legislation both for the\ncitizens at the heart of AI technologies and for the industry active in the\ninternal market. The AI Act imposes progressive compliance on organisations -\nboth private and public - involved in the global value chain of AI systems and\nmodels marketed and used in the EU. While the Act is unprecedented on an\ninternational scale in terms of its horizontal and binding regulatory scope,\nits global appeal in support of trustworthy AI is one of its major challenges.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "in French language",
    "pdf_url": "http://arxiv.org/pdf/2408.08318v1",
    "published_date": "2024-07-31 12:16:03 UTC",
    "updated_date": "2024-07-31 12:16:03 UTC"
  },
  {
    "arxiv_id": "2408.00033v1",
    "title": "Enhanced Fault Detection and Cause Identification Using Integrated Attention Mechanism",
    "authors": [
      "Mohammad Ali Labbaf Khaniki",
      "Alireza Golkarieh",
      "Houman Nouri",
      "Mohammad Manthouri"
    ],
    "abstract": "This study introduces a novel methodology for fault detection and cause\nidentification within the Tennessee Eastman Process (TEP) by integrating a\nBidirectional Long Short-Term Memory (BiLSTM) neural network with an Integrated\nAttention Mechanism (IAM). The IAM combines the strengths of scaled dot product\nattention, residual attention, and dynamic attention to capture intricate\npatterns and dependencies crucial for TEP fault detection. Initially, the\nattention mechanism extracts important features from the input data, enhancing\nthe model's interpretability and relevance. The BiLSTM network processes these\nfeatures bidirectionally to capture long-range dependencies, and the IAM\nfurther refines the output, leading to improved fault detection results.\nSimulation results demonstrate the efficacy of this approach, showcasing\nsuperior performance in accuracy, false alarm rate, and misclassification rate\ncompared to existing methods. This methodology provides a robust and\ninterpretable solution for fault detection and diagnosis in the TEP,\nhighlighting its potential for industrial applications.",
    "categories": [
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00033v1",
    "published_date": "2024-07-31 12:01:57 UTC",
    "updated_date": "2024-07-31 12:01:57 UTC"
  },
  {
    "arxiv_id": "2408.02680v1",
    "title": "Recording First-person Experiences to Build a New Type of Foundation Model",
    "authors": [
      "Dionis Barcari",
      "David Gamez",
      "Aliya Grig"
    ],
    "abstract": "Foundation models have had a big impact in recent years and billions of\ndollars are being invested in them in the current AI boom. The more popular\nones, such as Chat-GPT, are trained on large amounts of Internet data. However,\nit is becoming apparent that this data is likely to be exhausted soon, and\ntechnology companies are looking for new sources of data to train the next\ngeneration of foundation models.\n  Reinforcement learning, RAG, prompt engineering and cognitive modelling are\noften used to fine-tune and augment the behaviour of foundation models. These\ntechniques have been used to replicate people, such as Caryn Marjorie. These\nchatbots are not based on people's actual emotional and physiological responses\nto their environment, so they are, at best, a surface-level approximation to\nthe characters they are imitating.\n  To address these issues, we have developed a recording rig that captures what\nthe wearer is seeing and hearing as well as their skin conductance (GSR),\nfacial expression and brain state (14 channel EEG). AI algorithms are used to\nprocess this data into a rich picture of the environment and internal states of\nthe subject. Foundation models trained on this data could replicate human\nbehaviour much more accurately than the personality models that have been\ndeveloped so far. This type of model has many potential applications, including\nrecommendation, personal assistance, GAN systems, dating and recruitment.\n  This paper gives some background to this work and describes the recording rig\nand preliminary tests of its functionality. It then suggests how a new type of\nfoundation model could be created from the data captured by the rig and\noutlines some applications. Data gathering and model training are expensive, so\nwe are currently working on the launch of a start-up that could raise funds for\nthe next stage of the project.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 5 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:2408.00030",
    "pdf_url": "http://arxiv.org/pdf/2408.02680v1",
    "published_date": "2024-07-31 11:51:26 UTC",
    "updated_date": "2024-07-31 11:51:26 UTC"
  },
  {
    "arxiv_id": "2408.00030v1",
    "title": "A New Type of Foundation Model Based on Recordings of People's Emotions and Physiology",
    "authors": [
      "David Gamez",
      "Dionis Barcari",
      "Aliya Grig"
    ],
    "abstract": "Foundation models have had a big impact in recent years and billions of\ndollars are being invested in them in the current AI boom. The more popular\nones, such as Chat-GPT, are trained on large amounts of data from the Internet,\nand then reinforcement learning, RAG, prompt engineering and cognitive\nmodelling are used to fine-tune and augment their behavior. This technology has\nbeen used to create models of individual people, such as Caryn Marjorie.\nHowever, these chatbots are not based on people's actual emotional and\nphysiological responses to their environment, so they are, at best,\nsurface-level approximations to the characters they are imitating. This paper\ndescribes how a new type of foundation model - a first-person foundation model\n- could be created from recordings of what a person sees and hears as well as\ntheir emotional and physiological reactions to these stimuli. A first-person\nfoundation model would map environmental stimuli to a person's emotional and\nphysiological states, and map a person's emotional and physiological states to\ntheir behavior. First-person foundation models have many exciting applications,\nincluding a new type of recommendation engine, personal assistants, generative\nadversarial networks, dating and recruitment. To obtain training data for a\nfirst-person foundation model, we have developed a recording rig that captures\nwhat the wearer is seeing and hearing as well as their emotional and\nphysiological states. This novel source of data could help to address the\nshortage of new data for building the next generation of foundation models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 2 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.00030v1",
    "published_date": "2024-07-31 11:14:45 UTC",
    "updated_date": "2024-07-31 11:14:45 UTC"
  },
  {
    "arxiv_id": "2407.21525v1",
    "title": "Skeleton-Based Action Recognition with Spatial-Structural Graph Convolution",
    "authors": [
      "Jingyao Wang",
      "Emmanuel Bergeret",
      "Issam Falih"
    ],
    "abstract": "Human Activity Recognition (HAR) is a field of study that focuses on\nidentifying and classifying human activities. Skeleton-based Human Activity\nRecognition has received much attention in recent years, where Graph\nConvolutional Network (GCN) based method is widely used and has achieved\nremarkable results. However, the representation of skeleton data and the issue\nof over-smoothing in GCN still need to be studied. 1). Compared to central\nnodes, edge nodes can only aggregate limited neighbor information, and\ndifferent edge nodes of the human body are always structurally related.\nHowever, the information from edge nodes is crucial for fine-grained activity\nrecognition. 2). The Graph Convolutional Network suffers from a significant\nover-smoothing issue, causing nodes to become increasingly similar as the\nnumber of network layers increases. Based on these two ideas, we propose a\ntwo-stream graph convolution method called Spatial-Structural GCN (SpSt-GCN).\nSpatial GCN performs information aggregation based on the topological structure\nof the human body, and structural GCN performs differentiation based on the\nsimilarity of edge node sequences. The spatial connection is fixed, and the\nhuman skeleton naturally maintains this topology regardless of the actions\nperformed by humans. However, the structural connection is dynamic and depends\non the type of movement the human body is performing. Based on this idea, we\nalso propose an entirely data-driven structural connection, which greatly\nincreases flexibility. We evaluate our method on two large-scale datasets,\ni.e., NTU RGB+D and NTU RGB+D 120. The proposed method achieves good results\nwhile being efficient.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21525v1",
    "published_date": "2024-07-31 11:04:41 UTC",
    "updated_date": "2024-07-31 11:04:41 UTC"
  },
  {
    "arxiv_id": "2407.21523v1",
    "title": "Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI",
    "authors": [
      "Lingxi Cui",
      "Huan Li",
      "Ke Chen",
      "Lidan Shou",
      "Gang Chen"
    ],
    "abstract": "Machine learning (ML) on tabular data is ubiquitous, yet obtaining abundant\nhigh-quality tabular data for model training remains a significant obstacle.\nNumerous works have focused on tabular data augmentation (TDA) to enhance the\noriginal table with additional data, thereby improving downstream ML tasks.\nRecently, there has been a growing interest in leveraging the capabilities of\ngenerative AI for TDA. Therefore, we believe it is time to provide a\ncomprehensive review of the progress and future prospects of TDA, with a\nparticular emphasis on the trending generative AI. Specifically, we present an\narchitectural view of the TDA pipeline, comprising three main procedures:\npre-augmentation, augmentation, and post-augmentation. Pre-augmentation\nencompasses preparation tasks that facilitate subsequent TDA, including error\nhandling, table annotation, table simplification, table representation, table\nindexing, table navigation, schema matching, and entity matching. Augmentation\nsystematically analyzes current TDA methods, categorized into retrieval-based\nmethods, which retrieve external data, and generation-based methods, which\ngenerate synthetic data. We further subdivide these methods based on the\ngranularity of the augmentation process at the row, column, cell, and table\nlevels. Post-augmentation focuses on the datasets, evaluation and optimization\naspects of TDA. We also summarize current trends and future directions for TDA,\nhighlighting promising opportunities in the era of generative AI. In addition,\nthe accompanying papers and related resources are continuously updated and\nmaintained in the GitHub repository at\nhttps://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation to reflect\nongoing advancements in the field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "repository maintained at\n  https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation",
    "pdf_url": "http://arxiv.org/pdf/2407.21523v1",
    "published_date": "2024-07-31 10:56:20 UTC",
    "updated_date": "2024-07-31 10:56:20 UTC"
  },
  {
    "arxiv_id": "2407.21521v1",
    "title": "The Impacts of AI Avatar Appearance and Disclosure on User Motivation",
    "authors": [
      "Boele Visser",
      "Peter van der Putten",
      "Amirhossein Zohrehvand"
    ],
    "abstract": "This study examines the influence of perceived AI features on user motivation\nin virtual interactions. AI avatars, being disclosed as being an AI, or\nembodying specific genders, could be used in user-AI interactions. Leveraging\ninsights from AI and avatar research, we explore how AI disclosure and gender\naffect user motivation. We conducted a game-based experiment involving over\n72,500 participants who solved search problems alone or with an AI companion.\nDifferent groups experienced varying AI appearances and disclosures. We\nmeasured play intensity. Results revealed that the presence of another avatar\nled to less intense play compared to solo play. Disclosure of the avatar as AI\nheightened effort intensity compared to non-disclosed AI companions.\nAdditionally, a masculine AI appearance reduced effort intensity.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "15 pages, 6 figures, submitted to the 2nd International Conference on\n  Data Science & Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2407.21521v1",
    "published_date": "2024-07-31 10:48:55 UTC",
    "updated_date": "2024-07-31 10:48:55 UTC"
  },
  {
    "arxiv_id": "2407.21516v1",
    "title": "Expanding the Medical Decathlon dataset: segmentation of colon and colorectal cancer from computed tomography images",
    "authors": [
      "I. M. Chernenkiy",
      "Y. A. Drach",
      "S. R. Mustakimova",
      "V. V. Kazantseva",
      "N. A. Ushakov",
      "S. K. Efetov",
      "M. V. Feldsherov"
    ],
    "abstract": "Colorectal cancer is the third-most common cancer in the Western Hemisphere.\nThe segmentation of colorectal and colorectal cancer by computed tomography is\nan urgent problem in medicine. Indeed, a system capable of solving this problem\nwill enable the detection of colorectal cancer at early stages of the disease,\nfacilitate the search for pathology by the radiologist, and significantly\naccelerate the process of diagnosing the disease. However, scientific\npublications on medical image processing mostly use closed, non-public data.\nThis paper presents an extension of the Medical Decathlon dataset with\ncolorectal markups in order to improve the quality of segmentation algorithms.\nAn experienced radiologist validated the data, categorized it into subsets by\nquality, and published it in the public domain. Based on the obtained results,\nwe trained neural network models of the UNet architecture with 5-part\ncross-validation and achieved a Dice metric quality of $0.6988 \\pm 0.3$. The\npublished markups will improve the quality of colorectal cancer detection and\nsimplify the radiologist's job for study description.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "8 pages, 2 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.21516v1",
    "published_date": "2024-07-31 10:36:41 UTC",
    "updated_date": "2024-07-31 10:36:41 UTC"
  },
  {
    "arxiv_id": "2407.21507v1",
    "title": "FSSC: Federated Learning of Transformer Neural Networks for Semantic Image Communication",
    "authors": [
      "Yuna Yan",
      "Xin Zhang",
      "Lixin Li",
      "Wensheng Lin",
      "Rui Li",
      "Wenchi Cheng",
      "Zhu Han"
    ],
    "abstract": "In this paper, we address the problem of image semantic communication in a\nmulti-user deployment scenario and propose a federated learning (FL) strategy\nfor a Swin Transformer-based semantic communication system (FSSC). Firstly, we\ndemonstrate that the adoption of a Swin Transformer for joint source-channel\ncoding (JSCC) effectively extracts semantic information in the communication\nsystem. Next, the FL framework is introduced to collaboratively learn a global\nmodel by aggregating local model parameters, rather than directly sharing\nclients' data. This approach enhances user privacy protection and reduces the\nworkload on the server or mobile edge. Simulation evaluations indicate that our\nmethod outperforms the typical JSCC algorithm and traditional separate-based\ncommunication algorithms. Particularly after integrating local semantics, the\nglobal aggregation model has further increased the Peak Signal-to-Noise Ratio\n(PSNR) by more than 2dB, thoroughly proving the effectiveness of our algorithm.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21507v1",
    "published_date": "2024-07-31 10:25:24 UTC",
    "updated_date": "2024-07-31 10:25:24 UTC"
  },
  {
    "arxiv_id": "2407.21498v1",
    "title": "MaskUno: Switch-Split Block For Enhancing Instance Segmentation",
    "authors": [
      "Jawad Haidar",
      "Marc Mouawad",
      "Imad Elhajj",
      "Daniel Asmar"
    ],
    "abstract": "Instance segmentation is an advanced form of image segmentation which, beyond\ntraditional segmentation, requires identifying individual instances of\nrepeating objects in a scene. Mask R-CNN is the most common architecture for\ninstance segmentation, and improvements to this architecture include steps such\nas benefiting from bounding box refinements, adding semantics, or backbone\nenhancements. In all the proposed variations to date, the problem of competing\nkernels (each class aims to maximize its own accuracy) persists when models try\nto synchronously learn numerous classes. In this paper, we propose mitigating\nthis problem by replacing mask prediction with a Switch-Split block that\nprocesses refined ROIs, classifies them, and assigns them to specialized mask\npredictors. We name the method MaskUno and test it on various models from the\nliterature, which are then trained on multiple classes using the benchmark COCO\ndataset. An increase in the mean Average Precision (mAP) of 2.03% was observed\nfor the high-performing DetectoRS when trained on 80 classes. MaskUno proved to\nenhance the mAP of instance segmentation models regardless of the number and\ntyp",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21498v1",
    "published_date": "2024-07-31 10:12:14 UTC",
    "updated_date": "2024-07-31 10:12:14 UTC"
  },
  {
    "arxiv_id": "2407.21490v1",
    "title": "Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation",
    "authors": [
      "Junxuan Yu",
      "Rusi Chen",
      "Yongsong Zhou",
      "Yanlin Chen",
      "Yaofei Duan",
      "Yuhao Huang",
      "Han Zhou",
      "Tan Tao",
      "Xin Yang",
      "Dong Ni"
    ],
    "abstract": "Echocardiography video is a primary modality for diagnosing heart diseases,\nbut the limited data poses challenges for both clinical teaching and machine\nlearning training. Recently, video generative models have emerged as a\npromising strategy to alleviate this issue. However, previous methods often\nrelied on holistic conditions during generation, hindering the flexible\nmovement control over specific cardiac structures. In this context, we propose\nan explainable and controllable method for echocardiography video generation,\ntaking an initial frame and a motion curve as guidance. Our contributions are\nthree-fold. First, we extract motion information from each heart substructure\nto construct motion curves, enabling the diffusion model to synthesize\ncustomized echocardiography videos by modifying these curves. Second, we\npropose the structure-to-motion alignment module, which can map semantic\nfeatures onto motion curves across cardiac structures. Third, The\nposition-aware attention mechanism is designed to enhance video consistency\nutilizing Gaussian masks with structural position information. Extensive\nexperiments on three echocardiography datasets show that our method outperforms\nothers regarding fidelity and consistency. The full code will be released at\nhttps://github.com/mlmi-2024-72/ECM.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted by MICCAI MLMI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21490v1",
    "published_date": "2024-07-31 09:59:20 UTC",
    "updated_date": "2024-07-31 09:59:20 UTC"
  },
  {
    "arxiv_id": "2407.21489v1",
    "title": "Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends",
    "authors": [
      "Giuliano Martinelli",
      "Edoardo Barba",
      "Roberto Navigli"
    ],
    "abstract": "Large autoregressive generative models have emerged as the cornerstone for\nachieving the highest performance across several Natural Language Processing\ntasks. However, the urge to attain superior results has, at times, led to the\npremature replacement of carefully designed task-specific approaches without\nexhaustive experimentation. The Coreference Resolution task is no exception;\nall recent state-of-the-art solutions adopt large generative autoregressive\nmodels that outperform encoder-based discriminative systems. In this work,we\nchallenge this recent trend by introducing Maverick, a carefully designed - yet\nsimple - pipeline, which enables running a state-of-the-art Coreference\nResolution system within the constraints of an academic budget, outperforming\nmodels with up to 13 billion parameters with as few as 500 million parameters.\nMaverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,\ntraining with up to 0.006x the memory resources and obtaining a 170x faster\ninference compared to previous state-of-the-art systems. We extensively\nvalidate the robustness of the Maverick framework with an array of diverse\nexperiments, reporting improvements over prior systems in data-scarce,\nlong-document, and out-of-domain settings. We release our code and models for\nresearch purposes at https://github.com/SapienzaNLP/maverick-coref.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at main conference of ACL 2024. 15 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.21489v1",
    "published_date": "2024-07-31 09:58:48 UTC",
    "updated_date": "2024-07-31 09:58:48 UTC"
  },
  {
    "arxiv_id": "2407.21488v2",
    "title": "Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing the Upper Bound of Generative Retrieval",
    "authors": [
      "Zhirui Kuai",
      "Zuxu Chen",
      "Huimu Wang",
      "Mingming Li",
      "Dadong Miao",
      "Binbin Wang",
      "Xusong Chen",
      "Li Kuang",
      "Yuxing Han",
      "Jiaxing Wang",
      "Guoyu Tang",
      "Lin Liu",
      "Songlin Wang",
      "Jingwei Zhuo"
    ],
    "abstract": "Generative retrieval (GR) has emerged as a transformative paradigm in search\nand recommender systems, leveraging numeric-based identifier representations to\nenhance efficiency and generalization. Notably, methods like TIGER employing\nResidual Quantization-based Semantic Identifiers (RQ-SID), have shown\nsignificant promise in e-commerce scenarios by effectively managing item IDs.\nHowever, a critical issue termed the \"\\textbf{Hourglass}\" phenomenon, occurs in\nRQ-SID, where intermediate codebook tokens become overly concentrated,\nhindering the full utilization of generative retrieval methods. This paper\nanalyses and addresses this problem by identifying data sparsity and\nlong-tailed distribution as the primary causes. Through comprehensive\nexperiments and detailed ablation studies, we analyze the impact of these\nfactors on codebook utilization and data distribution. Our findings reveal that\nthe \"Hourglass\" phenomenon substantially impacts the performance of RQ-SID in\ngenerative retrieval. We propose effective solutions to mitigate this issue,\nthereby significantly enhancing the effectiveness of generative retrieval in\nreal-world E-commerce applications.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21488v2",
    "published_date": "2024-07-31 09:52:53 UTC",
    "updated_date": "2024-10-31 11:45:00 UTC"
  },
  {
    "arxiv_id": "2407.21485v2",
    "title": "Parallel Strategies for Best-First Generalized Planning",
    "authors": [
      "Alejandro Fernández-Alburquerque",
      "Javier Segovia-Aguas"
    ],
    "abstract": "In recent years, there has been renewed interest in closing the performance\ngap between state-of-the-art planning solvers and generalized planning (GP), a\nresearch area of AI that studies the automated synthesis of algorithmic-like\nsolutions capable of solving multiple classical planning instances. One of the\ncurrent advancements has been the introduction of Best-First Generalized\nPlanning (BFGP), a GP algorithm based on a novel solution space that can be\nexplored with heuristic search, one of the foundations of modern planners. This\npaper evaluates the application of parallel search techniques to BFGP, another\ncritical component in closing the performance gap. We first discuss why BFGP is\nwell suited for parallelization and some of its differentiating characteristics\nfrom classical planners. Then, we propose two simple shared-memory parallel\nstrategies with good scaling with the number of cores.",
    "categories": [
      "cs.AI",
      "I.2.8; D.1.3"
    ],
    "primary_category": "cs.AI",
    "comment": "3 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.21485v2",
    "published_date": "2024-07-31 09:50:22 UTC",
    "updated_date": "2024-08-02 16:58:02 UTC"
  },
  {
    "arxiv_id": "2407.21483v3",
    "title": "eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs",
    "authors": [
      "Xinyi Pan",
      "Daniel Hernández",
      "Philipp Seifer",
      "Ralf Lämmel",
      "Steffen Staab"
    ],
    "abstract": "Over the past few years, we have seen the emergence of large knowledge graphs\ncombining information from multiple sources. Sometimes, this information is\nprovided in the form of assertions about other assertions, defining contexts\nwhere assertions are valid. A recent extension to RDF which admits statements\nover statements, called RDF-star, is in revision to become a W3C standard.\nHowever, there is no proposal for a semantics of these RDF-star statements nor\na built-in facility to operate over them. In this paper, we propose a query\nlanguage for epistemic RDF-star metadata based on a four-valued logic, called\neSPARQL. Our proposed query language extends SPARQL-star, the query language\nfor RDF-star, with a new type of FROM clause to facilitate operating with\nmultiple and sometimes conflicting beliefs. We show that the proposed query\nlanguage can express four use case queries, including the following features:\n(i) querying the belief of an individual, (ii) the aggregating of beliefs,\n(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs\n(i.e., nesting of beliefs).",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21483v3",
    "published_date": "2024-07-31 09:48:27 UTC",
    "updated_date": "2024-08-06 12:34:16 UTC"
  },
  {
    "arxiv_id": "2407.21475v1",
    "title": "Fine-gained Zero-shot Video Sampling",
    "authors": [
      "Dengsheng Chen",
      "Jie Hu",
      "Xiaoming Wei",
      "Enhua Wu"
    ],
    "abstract": "Incorporating a temporal dimension into pretrained image diffusion models for\nvideo generation is a prevalent approach. However, this method is\ncomputationally demanding and necessitates large-scale video datasets. More\ncritically, the heterogeneity between image and video datasets often results in\ncatastrophic forgetting of the image expertise. Recent attempts to directly\nextract video snippets from image diffusion models have somewhat mitigated\nthese problems. Nevertheless, these methods can only generate brief video clips\nwith simple movements and fail to capture fine-grained motion or non-grid\ndeformation. In this paper, we propose a novel Zero-Shot video Sampling\nalgorithm, denoted as $\\mathcal{ZS}^2$, capable of directly sampling\nhigh-quality video clips from existing image synthesis methods, such as Stable\nDiffusion, without any training or optimization. Specifically, $\\mathcal{ZS}^2$\nutilizes the dependency noise model and temporal momentum attention to ensure\ncontent consistency and animation coherence, respectively. This ability enables\nit to excel in related tasks, such as conditional and context-specialized video\ngeneration and instruction-guided video editing. Experimental results\ndemonstrate that $\\mathcal{ZS}^2$ achieves state-of-the-art performance in\nzero-shot video generation, occasionally outperforming recent supervised\nmethods.\n  Homepage: \\url{https://densechen.github.io/zss/}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21475v1",
    "published_date": "2024-07-31 09:36:58 UTC",
    "updated_date": "2024-07-31 09:36:58 UTC"
  },
  {
    "arxiv_id": "2407.21468v1",
    "title": "An Invertible State Space for Process Trees",
    "authors": [
      "Gero Kolhof",
      "Sebastiaan J. van Zelst"
    ],
    "abstract": "Process models are, like event data, first-class citizens in most process\nmining approaches. Several process modeling formalisms have been proposed and\nused, e.g., Petri nets, BPMN, and process trees. Despite their frequent use,\nlittle research addresses the formal properties of process trees and the\ncorresponding potential to improve the efficiency of solving common\ncomputational problems. Therefore, in this paper, we propose an invertible\nstate space definition for process trees and demonstrate that the corresponding\nstate space graph is isomorphic to the state space graph of the tree's inverse.\nOur result supports the development of novel, time-efficient, decomposition\nstrategies for applications of process trees. Our experiments confirm that our\nstate space definition allows for the adoption of bidirectional state space\nsearch, which significantly improves the overall performance of state space\nsearches.",
    "categories": [
      "cs.DS",
      "cs.AI"
    ],
    "primary_category": "cs.DS",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.21468v1",
    "published_date": "2024-07-31 09:26:35 UTC",
    "updated_date": "2024-07-31 09:26:35 UTC"
  },
  {
    "arxiv_id": "2407.21467v2",
    "title": "Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data",
    "authors": [
      "Mengtian Kang",
      "Yansong Hu",
      "Shuo Gao",
      "Yuanyuan Liu",
      "Hongbei Meng",
      "Xuemeng Li",
      "Xuhang Chen",
      "Hubin Zhao",
      "Jing Fu",
      "Guohua Hu",
      "Wei Wang",
      "Yanning Dai",
      "Arokia Nathan",
      "Peter Smielewski",
      "Ningli Wang",
      "Shiming Li"
    ],
    "abstract": "Childhood myopia constitutes a significant global health concern. It exhibits\nan escalating prevalence and has the potential to evolve into severe,\nirreversible conditions that detrimentally impact familial well-being and\ncreate substantial economic costs. Contemporary research underscores the\nimportance of precisely predicting myopia progression to enable timely and\neffective interventions, thereby averting severe visual impairment in children.\nSuch predictions predominantly rely on subjective clinical assessments, which\nare inherently biased and resource-intensive, thus hindering their widespread\napplication. In this study, we introduce a novel, high-accuracy method for\nquantitatively predicting the myopic trajectory and myopia risk in children\nusing only fundus images and baseline refraction data. This approach was\nvalidated through a six-year longitudinal study of 3,408 children in Henan,\nutilizing 16,211 fundus images and corresponding refractive data. Our method\nbased on deep learning demonstrated predictive accuracy with an error margin of\n0.311D per year and AUC scores of 0.944 and 0.995 for forecasting the risks of\ndeveloping myopia and high myopia, respectively. These findings confirm the\nutility of our model in supporting early intervention strategies and in\nsignificantly reducing healthcare costs, particularly by obviating the need for\nadditional metadata and repeated consultations. Furthermore, our method was\ndesigned to rely only on fundus images and refractive error data, without the\nneed for meta data or multiple inquiries from doctors, strongly reducing the\nassociated medical costs and facilitating large-scale screening. Our model can\neven provide good predictions based on only a single time measurement.\nConsequently, the proposed method is an important means to reduce medical\ninequities caused by economic disparities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21467v2",
    "published_date": "2024-07-31 09:26:20 UTC",
    "updated_date": "2025-04-15 16:41:09 UTC"
  },
  {
    "arxiv_id": "2407.21460v1",
    "title": "Multi-agent Assessment with QoS Enhancement for HD Map Updates in a Vehicular Network",
    "authors": [
      "Jeffrey Redondo",
      "Nauman Aslam",
      "Juan Zhang",
      "Zhenhui Yuan"
    ],
    "abstract": "Reinforcement Learning (RL) algorithms have been used to address the\nchallenging problems in the offloading process of vehicular ad hoc networks\n(VANET). More recently, they have been utilized to improve the dissemination of\nhigh-definition (HD) Maps. Nevertheless, implementing solutions such as deep\nQ-learning (DQN) and Actor-critic at the autonomous vehicle (AV) may lead to an\nincrease in the computational load, causing a heavy burden on the computational\ndevices and higher costs. Moreover, their implementation might raise\ncompatibility issues between technologies due to the required modifications to\nthe standards. Therefore, in this paper, we assess the scalability of an\napplication utilizing a Q-learning single-agent solution in a distributed\nmulti-agent environment. This application improves the network performance by\ntaking advantage of a smaller state, and action space whilst using a\nmulti-agent approach. The proposed solution is extensively evaluated with\ndifferent test cases involving reward function considering individual or\noverall network performance, number of agents, and centralized and distributed\nlearning comparison. The experimental results demonstrate that the time\nlatencies of our proposed solution conducted in voice, video, HD Map, and\nbest-effort cases have significant improvements, with 40.4%, 36%, 43%, and 12%\nrespectively, compared to the performances with the single-agent approach.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21460v1",
    "published_date": "2024-07-31 09:17:09 UTC",
    "updated_date": "2024-07-31 09:17:09 UTC"
  },
  {
    "arxiv_id": "2407.21459v1",
    "title": "KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making",
    "authors": [
      "Gilang Fajar Febrian",
      "Grazziela Figueredo"
    ],
    "abstract": "Data is crucial for evidence-based policymaking and enhancing public\nservices, including those at the Ministry of Finance of the Republic of\nIndonesia. However, the complexity and dynamic nature of governmental financial\ndata and regulations can hinder decision-making. This study investigates the\npotential of Large Language Models (LLMs) to address these challenges, focusing\non Indonesia's financial data and regulations. While LLMs are effective in the\nfinancial sector, their use in the public sector in Indonesia is unexplored.\nThis study undertakes an iterative process to develop KemenkeuGPT using the\nLangChain with Retrieval-Augmented Generation (RAG), prompt engineering and\nfine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of\nFinance, Statistics Indonesia and the International Monetary Fund (IMF).\nSurveys and interviews with Ministry officials informed, enhanced and\nfine-tuned the model. We evaluated the model using human feedback, LLM-based\nevaluation and benchmarking. The model's accuracy improved from 35% to 61%,\nwith correctness increasing from 48% to 64%. The Retrieval-Augmented Generation\nAssessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness\nwith 73% faithfulness, 40% precision and 60% recall, outperforming several\nother base models. An interview with an expert from the Ministry of Finance\nindicated that KemenkeuGPT has the potential to become an essential tool for\ndecision-making. These results are expected to improve with continuous human\nfeedback.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 7 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.21459v1",
    "published_date": "2024-07-31 09:16:33 UTC",
    "updated_date": "2024-07-31 09:16:33 UTC"
  },
  {
    "arxiv_id": "2407.21453v2",
    "title": "TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors",
    "authors": [
      "Zhaolan Huang",
      "Adrien Tousnakhoff",
      "Polina Kozyr",
      "Roman Rehausen",
      "Felix Bießmann",
      "Robert Lachlan",
      "Cedric Adjih",
      "Emmanuel Baccelli"
    ],
    "abstract": "Monitoring biodiversity at scale is challenging. Detecting and identifying\nspecies in fine grained taxonomies requires highly accurate machine learning\n(ML) methods. Training such models requires large high quality data sets. And\ndeploying these models to low power devices requires novel compression\ntechniques and model architectures. While species classification methods have\nprofited from novel data sets and advances in ML methods, in particular neural\nnetworks, deploying these state of the art models to low power devices remains\ndifficult. Here we present a comprehensive empirical comparison of various\ntinyML neural network architectures and compression techniques for species\nclassification. We focus on the example of bird song detection, more concretely\na data set curated for studying the corn bunting bird species. The data set is\nreleased along with all code and experiments of this study. In our experiments\nwe compare predictive performance, memory and time complexity of classical\nspectrogram based methods and recent approaches operating on raw audio signal.\nOur results indicate that individual bird species can be robustly detected with\nrelatively simple architectures that can be readily deployed to low power\ndevices.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SD",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21453v2",
    "published_date": "2024-07-31 08:57:42 UTC",
    "updated_date": "2024-09-11 08:07:24 UTC"
  },
  {
    "arxiv_id": "2407.21443v1",
    "title": "Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency",
    "authors": [
      "Taiji Li",
      "Zhi Li",
      "Yin Zhang"
    ],
    "abstract": "Despite large language models (LLMs) have demonstrated impressive performance\nin various tasks, they are still suffering from the factual inconsistency\nproblem called hallucinations. For instance, LLMs occasionally generate content\nthat diverges from source article, and prefer to extract information that\nappears at the beginning and end of the context, especially in long document\nsummarization. Inspired by these findings, we propose to improve the\nfaithfulness of LLMs in summarization by impelling them to process the entire\narticle more fairly and faithfully. We present a novel summary generation\nstrategy, namely SliSum, which exploits the ideas of sliding windows and\nself-consistency. Specifically, SliSum divides the source article into\noverlapping windows, and utilizes LLM to generate local summaries for the\ncontent in the windows. Finally, SliSum aggregates all local summaries using\nclustering and majority voting algorithm to produce more faithful summary of\nentire article. Extensive experiments demonstrate that SliSum significantly\nimproves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and\nGPT-3.5 in both short and long text summarization, while maintaining their\nfluency and informativeness and without additional fine-tuning and resources.\nWe further conduct qualitative and quantitative studies to investigate why\nSliSum works and impacts of hyperparameters in SliSum on performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Long paper accepted at LREC-COLING 2024 (oral)",
    "pdf_url": "http://arxiv.org/pdf/2407.21443v1",
    "published_date": "2024-07-31 08:48:48 UTC",
    "updated_date": "2024-07-31 08:48:48 UTC"
  },
  {
    "arxiv_id": "2407.21439v2",
    "title": "MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training",
    "authors": [
      "Zhanpeng Chen",
      "Chengjin Xu",
      "Yiyan Qi",
      "Jian Guo"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in processing and generating content across multiple data\nmodalities. However, a significant drawback of MLLMs is their reliance on\nstatic training data, leading to outdated information and limited contextual\nawareness. This static nature hampers their ability to provide accurate and\nup-to-date responses, particularly in dynamic or rapidly evolving contexts.\nThough integrating Multimodal Retrieval-augmented Generation (Multimodal RAG)\noffers a promising solution, the system would inevitably encounter the\nmulti-granularity noisy correspondence (MNC) problem, which hinders accurate\nretrieval and generation. In this work, we propose RagVL, a novel framework\nwith knowledge-enhanced reranking and noise-injected training, to address these\nlimitations. We instruction-tune the MLLM with a simple yet effective\ninstruction template to induce its ranking ability and serve it as a reranker\nto precisely filter the top-k retrieved images. For generation, we inject\nvisual noise during training at the data and token levels to enhance the\ngenerator's robustness. Extensive experiments on the subsets of two datasets\nthat require retrieving and reasoning over images to answer a given query\nverify the effectiveness of our method. Code and models are available at\nhttps://github.com/IDEA-FinAI/RagVL.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21439v2",
    "published_date": "2024-07-31 08:43:17 UTC",
    "updated_date": "2024-09-25 06:14:03 UTC"
  },
  {
    "arxiv_id": "2407.21428v1",
    "title": "Deformable 3D Shape Diffusion Model",
    "authors": [
      "Dengsheng Chen",
      "Jie Hu",
      "Xiaoming Wei",
      "Enhua Wu"
    ],
    "abstract": "The Gaussian diffusion model, initially designed for image generation, has\nrecently been adapted for 3D point cloud generation. However, these adaptations\nhave not fully considered the intrinsic geometric characteristics of 3D shapes,\nthereby constraining the diffusion model's potential for 3D shape manipulation.\nTo address this limitation, we introduce a novel deformable 3D shape diffusion\nmodel that facilitates comprehensive 3D shape manipulation, including point\ncloud generation, mesh deformation, and facial animation. Our approach\ninnovatively incorporates a differential deformation kernel, which deconstructs\nthe generation of geometric structures into successive non-rigid deformation\nstages. By leveraging a probabilistic diffusion model to simulate this\nstep-by-step process, our method provides a versatile and efficient solution\nfor a wide range of applications, spanning from graphics rendering to facial\nexpression animation. Empirical evidence highlights the effectiveness of our\napproach, demonstrating state-of-the-art performance in point cloud generation\nand competitive results in mesh deformation. Additionally, extensive visual\ndemonstrations reveal the significant potential of our approach for practical\napplications. Our method presents a unique pathway for advancing 3D shape\nmanipulation and unlocking new opportunities in the realm of virtual reality.",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21428v1",
    "published_date": "2024-07-31 08:24:42 UTC",
    "updated_date": "2024-07-31 08:24:42 UTC"
  },
  {
    "arxiv_id": "2407.21424v2",
    "title": "Cost-Effective Hallucination Detection for LLMs",
    "authors": [
      "Simon Valentin",
      "Jinmiao Fu",
      "Gianluca Detommaso",
      "Shaoyuan Xu",
      "Giovanni Zappella",
      "Bryan Wang"
    ],
    "abstract": "Large language models (LLMs) can be prone to hallucinations - generating\nunreliable outputs that are unfaithful to their inputs, external facts or\ninternally inconsistent. In this work, we address several challenges for\npost-hoc hallucination detection in production settings. Our pipeline for\nhallucination detection entails: first, producing a confidence score\nrepresenting the likelihood that a generated answer is a hallucination; second,\ncalibrating the score conditional on attributes of the inputs and candidate\nresponse; finally, performing detection by thresholding the calibrated score.\nWe benchmark a variety of state-of-the-art scoring methods on different\ndatasets, encompassing question answering, fact checking, and summarization\ntasks. We employ diverse LLMs to ensure a comprehensive assessment of\nperformance. We show that calibrating individual scoring methods is critical\nfor ensuring risk-aware downstream decision making. Based on findings that no\nindividual score performs best in all situations, we propose a multi-scoring\nframework, which combines different scores and achieves top performance across\nall datasets. We further introduce cost-effective multi-scoring, which can\nmatch or even outperform more expensive detection methods, while significantly\nreducing computational overhead.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to GenAI Evaluation Workshop at KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21424v2",
    "published_date": "2024-07-31 08:19:06 UTC",
    "updated_date": "2024-08-09 11:58:55 UTC"
  },
  {
    "arxiv_id": "2408.00025v3",
    "title": "Need of AI in Modern Education: in the Eyes of Explainable AI (xAI)",
    "authors": [
      "Supriya Manna",
      "Niladri Sett"
    ],
    "abstract": "Modern Education is not \\textit{Modern} without AI. However, AI's complex\nnature makes understanding and fixing problems challenging. Research worldwide\nshows that a parent's income greatly influences a child's education. This led\nus to explore how AI, especially complex models, makes important decisions\nusing Explainable AI tools. Our research uncovered many complexities linked to\nparental income and offered reasonable explanations for these decisions.\nHowever, we also found biases in AI that go against what we want from AI in\neducation: clear transparency and equal access for everyone. These biases can\nimpact families and children's schooling, highlighting the need for better AI\nsolutions that offer fair opportunities to all. This chapter tries to shed\nlight on the complex ways AI operates, especially concerning biases. These are\nthe foundational steps towards better educational policies, which include using\nAI in ways that are more reliable, accountable, and beneficial for everyone\ninvolved.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Chapter in the book: Blockchain and AI in Shaping the Modern\n  Education System, CRC Press, Taylor & Francis Group, USA",
    "pdf_url": "http://arxiv.org/pdf/2408.00025v3",
    "published_date": "2024-07-31 08:11:33 UTC",
    "updated_date": "2025-01-02 21:59:22 UTC"
  },
  {
    "arxiv_id": "2407.21385v1",
    "title": "SmileyNet -- Towards the Prediction of the Lottery by Reading Tea Leaves with AI",
    "authors": [
      "Andreas Birk"
    ],
    "abstract": "We introduce SmileyNet, a novel neural network with psychic abilities. It is\ninspired by the fact that a positive mood can lead to improved cognitive\ncapabilities including classification tasks. The network is hence presented in\na first phase with smileys and an encouraging loss function is defined to bias\nit into a good mood. SmileyNet is then used to forecast the flipping of a coin\nbased on an established method of Tasseology, namely by reading tea leaves.\nTraining and testing in this second phase are done with a high-fidelity\nsimulation based on real-world pixels sampled from a professional tea-reading\ncup. SmileyNet has an amazing accuracy of 72% to correctly predict the flip of\na coin. Resnet-34, respectively YOLOv5 achieve only 49%, respectively 53%. It\nis then shown how multiple SmileyNets can be combined to win the lottery.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "cs.LG",
      "cs.RO",
      "I.2; I.4; I.5; I.6; K.3.2"
    ],
    "primary_category": "cs.AI",
    "comment": "This is a satirical accumulation of misconceptions, mistakes, and\n  flawed reasoning I have encountered in recent times as a reviewer and\n  sometimes even as a reader of published papers. I hope it is entertaining and\n  useful in the context of the education of BSc, MSc, and PhD students in\n  Machine Learning, Artificial Intelligence, and Cognitive Science",
    "pdf_url": "http://arxiv.org/pdf/2407.21385v1",
    "published_date": "2024-07-31 07:16:40 UTC",
    "updated_date": "2024-07-31 07:16:40 UTC"
  },
  {
    "arxiv_id": "2407.21384v2",
    "title": "GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction",
    "authors": [
      "Yanxu Mao",
      "Xiaohui Chen",
      "Peipei Liu",
      "Tiehan Cui",
      "Zuhui Yue",
      "Zheng Li"
    ],
    "abstract": "Document-level relation extraction (DocRE) aims to extract relations between\nentities from unstructured document text. Compared to sentence-level relation\nextraction, it requires more complex semantic understanding from a broader text\ncontext. Currently, some studies are utilizing logical rules within evidence\nsentences to enhance the performance of DocRE. However, in the data without\nprovided evidence sentences, researchers often obtain a list of evidence\nsentences for the entire document through evidence retrieval (ER). Therefore,\nDocRE suffers from two challenges: firstly, the relevance between evidence and\nentity pairs is weak; secondly, there is insufficient extraction of complex\ncross-relations between long-distance multi-entities. To overcome these\nchallenges, we propose GEGA, a novel model for DocRE. The model leverages graph\nneural networks to construct multiple weight matrices, guiding attention\nallocation to evidence sentences. It also employs multi-scale representation\naggregation to enhance ER. Subsequently, we integrate the most efficient\nevidence information to implement both fully supervised and weakly supervised\ntraining processes for the model. We evaluate the GEGA model on three widely\nused benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The\nexperimental results indicate that our model has achieved comprehensive\nimprovements compared to the existing SOTA model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21384v2",
    "published_date": "2024-07-31 07:15:33 UTC",
    "updated_date": "2024-09-08 16:42:28 UTC"
  },
  {
    "arxiv_id": "2407.21376v1",
    "title": "An Extended Kalman Filter Integrated Latent Feature Model on Dynamic Weighted Directed Graphs",
    "authors": [
      "Hongxun Zhou",
      "Xiangyu Chen",
      "Ye Yuan"
    ],
    "abstract": "A dynamic weighted directed graph (DWDG) is commonly encountered in various\napplication scenarios. It involves extensive dynamic interactions among\nnumerous nodes. Most existing approaches explore the intricate temporal\npatterns hidden in a DWDG from the purely data-driven perspective, which\nsuffers from accuracy loss when a DWDG exhibits strong fluctuations over time.\nTo address this issue, this study proposes a novel\nExtended-Kalman-Filter-Incorporated Latent Feature (EKLF) model to represent a\nDWDG from the model-driven perspective. Its main idea is divided into the\nfollowing two-fold ideas: a) adopting a control model, i.e., the Extended\nKalman Filter (EKF), to track the complex temporal patterns precisely with its\nnonlinear state-transition and observation functions; and b) introducing an\nalternating least squares (ALS) algorithm to train the latent features (LFs)\nalternatively for precisely representing a DWDG. Empirical studies on DWDG\ndatasets demonstrate that the proposed EKLF model outperforms state-of-the-art\nmodels in prediction accuracy and computational efficiency for missing edge\nweights of a DWDG. It unveils the potential for precisely representing a DWDG\nby incorporating a control model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21376v1",
    "published_date": "2024-07-31 06:57:27 UTC",
    "updated_date": "2024-07-31 06:57:27 UTC"
  },
  {
    "arxiv_id": "2407.21368v3",
    "title": "Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering",
    "authors": [
      "Danfeng Guo",
      "Demetri Terzopoulos"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in\nrecent years, and they have been extended to the medical domain. Although\ndemonstrating satisfactory performance on medical Visual Question Answering\n(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,\nwhich makes them fail to diagnose complex pathologies. Moreover, they readily\nfail to learn minority pathologies due to imbalanced training data. We propose\ntwo prompting strategies for MLVLMs that reduce hallucination and improve VQA\nperformance. In the first strategy, we provide a detailed explanation of the\nqueried pathology. In the second strategy, we fine-tune a cheap, weak learner\nto achieve high performance on a specific metric, and textually provide its\njudgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our\nmethods significantly improve the diagnostic F1 score, with the highest\nincrease being 0.27. We also demonstrate that our prompting strategies can be\nextended to general LVLM domains. Based on POPE metrics, it effectively\nsuppresses the false negative predictions of existing LVLMs and improves Recall\nby approximately 0.07.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2025:004",
    "pdf_url": "http://arxiv.org/pdf/2407.21368v3",
    "published_date": "2024-07-31 06:34:38 UTC",
    "updated_date": "2025-03-17 00:27:45 UTC"
  },
  {
    "arxiv_id": "2408.05112v1",
    "title": "Semantic Successive Refinement: A Generative AI-aided Semantic Communication Framework",
    "authors": [
      "Kexin Zhang",
      "Lixin Li",
      "Wensheng Lin",
      "Yuna Yan",
      "Rui Li",
      "Wenchi Cheng",
      "Zhu Han"
    ],
    "abstract": "Semantic Communication (SC) is an emerging technology aiming to surpass the\nShannon limit. Traditional SC strategies often minimize signal distortion\nbetween the original and reconstructed data, neglecting perceptual quality,\nespecially in low Signal-to-Noise Ratio (SNR) environments. To address this\nissue, we introduce a novel Generative AI Semantic Communication (GSC) system\nfor single-user scenarios. This system leverages deep generative models to\nestablish a new paradigm in SC. Specifically, At the transmitter end, it\nemploys a joint source-channel coding mechanism based on the Swin Transformer\nfor efficient semantic feature extraction and compression. At the receiver end,\nan advanced Diffusion Model (DM) reconstructs high-quality images from degraded\nsignals, enhancing perceptual details. Additionally, we present a Multi-User\nGenerative Semantic Communication (MU-GSC) system utilizing an asynchronous\nprocessing model. This model effectively manages multiple user requests and\noptimally utilizes system resources for parallel processing. Simulation results\non public datasets demonstrate that our generative AI semantic communication\nsystems achieve superior transmission efficiency and enhanced communication\ncontent quality across various channel conditions. Compared to CNN-based\nDeepJSCC, our methods improve the Peak Signal-to-Noise Ratio (PSNR) by 17.75%\nin Additive White Gaussian Noise (AWGN) channels and by 20.86% in Rayleigh\nchannels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05112v1",
    "published_date": "2024-07-31 06:08:51 UTC",
    "updated_date": "2024-07-31 06:08:51 UTC"
  },
  {
    "arxiv_id": "2407.21359v1",
    "title": "ProSpec RL: Plan Ahead, then Execute",
    "authors": [
      "Liangliang Liu",
      "Yi Guan",
      "BoRan Wang",
      "Rujia Shen",
      "Yi Lin",
      "Chaoran Kong",
      "Lian Yan",
      "Jingchi Jiang"
    ],
    "abstract": "Imagining potential outcomes of actions before execution helps agents make\nmore informed decisions, a prospective thinking ability fundamental to human\ncognition. However, mainstream model-free Reinforcement Learning (RL) methods\nlack the ability to proactively envision future scenarios, plan, and guide\nstrategies. These methods typically rely on trial and error to adjust policy\nfunctions, aiming to maximize cumulative rewards or long-term value, even if\nsuch high-reward decisions place the environment in extremely dangerous states.\nTo address this, we propose the Prospective (ProSpec) RL method, which makes\nhigher-value, lower-risk optimal decisions by imagining future n-stream\ntrajectories. Specifically, ProSpec employs a dynamic model to predict future\nstates (termed \"imagined states\") based on the current state and a series of\nsampled actions. Furthermore, we integrate the concept of Model Predictive\nControl and introduce a cycle consistency constraint that allows the agent to\nevaluate and select the optimal actions from these trajectories. Moreover,\nProSpec employs cycle consistency to mitigate two fundamental issues in RL:\naugmenting state reversibility to avoid irreversible events (low risk) and\naugmenting actions to generate numerous virtual trajectories, thereby improving\ndata efficiency. We validated the effectiveness of our method on the DMControl\nbenchmarks, where our approach achieved significant performance improvements.\nCode will be open-sourced upon acceptance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21359v1",
    "published_date": "2024-07-31 06:04:55 UTC",
    "updated_date": "2024-07-31 06:04:55 UTC"
  },
  {
    "arxiv_id": "2407.21358v1",
    "title": "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs",
    "authors": [
      "Elan Markowitz",
      "Anil Ramakrishna",
      "Jwala Dhamala",
      "Ninareh Mehrabi",
      "Charith Peris",
      "Rahul Gupta",
      "Kai-Wei Chang",
      "Aram Galstyan"
    ],
    "abstract": "Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing\nreliable, structured, domain-specific, and up-to-date external knowledge.\nHowever, KGs and LLMs are often developed separately and must be integrated\nafter training. We introduce Tree-of-Traversals, a novel zero-shot reasoning\nalgorithm that enables augmentation of black-box LLMs with one or more KGs. The\nalgorithm equips a LLM with actions for interfacing a KG and enables the LLM to\nperform tree search over possible thoughts and actions to find high confidence\nreasoning paths. We evaluate on two popular benchmark datasets. Our results\nshow that Tree-of-Traversals significantly improves performance on question\nanswering and KG question answering tasks. Code is available at\n\\url{https://github.com/amazon-science/tree-of-traversals}",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at the ACL 2024 Conference",
    "pdf_url": "http://arxiv.org/pdf/2407.21358v1",
    "published_date": "2024-07-31 06:01:24 UTC",
    "updated_date": "2024-07-31 06:01:24 UTC"
  },
  {
    "arxiv_id": "2407.21351v1",
    "title": "Small Object Few-shot Segmentation for Vision-based Industrial Inspection",
    "authors": [
      "Zilong Zhang",
      "Chang Niu",
      "Zhibin Zhao",
      "Xingwu Zhang",
      "Xuefeng Chen"
    ],
    "abstract": "Vision-based industrial inspection (VII) aims to locate defects quickly and\naccurately. Supervised learning under a close-set setting and industrial\nanomaly detection, as two common paradigms in VII, face different problems in\npractical applications. The former is that various and sufficient defects are\ndifficult to obtain, while the latter is that specific defects cannot be\nlocated. To solve these problems, in this paper, we focus on the few-shot\nsemantic segmentation (FSS) method, which can locate unseen defects conditioned\non a few annotations without retraining. Compared to common objects in natural\nimages, the defects in VII are small. This brings two problems to current FSS\nmethods: 1 distortion of target semantics and 2 many false positives for\nbackgrounds. To alleviate these problems, we propose a small object few-shot\nsegmentation (SOFS) model. The key idea for alleviating 1 is to avoid the\nresizing of the original image and correctly indicate the intensity of target\nsemantics. SOFS achieves this idea via the non-resizing procedure and the\nprototype intensity downsampling of support annotations. To alleviate 2, we\ndesign an abnormal prior map in SOFS to guide the model to reduce false\npositives and propose a mixed normal Dice loss to preferentially prevent the\nmodel from predicting false positives. SOFS can achieve FSS and few-shot\nanomaly detection determined by support masks. Diverse experiments substantiate\nthe superior performance of SOFS. Code is available at\nhttps://github.com/zhangzilongc/SOFS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21351v1",
    "published_date": "2024-07-31 05:43:36 UTC",
    "updated_date": "2024-07-31 05:43:36 UTC"
  },
  {
    "arxiv_id": "2408.00024v1",
    "title": "Deceptive AI systems that give explanations are more convincing than honest AI systems and can amplify belief in misinformation",
    "authors": [
      "Valdemar Danry",
      "Pat Pataranutaporn",
      "Matthew Groh",
      "Ziv Epstein",
      "Pattie Maes"
    ],
    "abstract": "Advanced Artificial Intelligence (AI) systems, specifically large language\nmodels (LLMs), have the capability to generate not just misinformation, but\nalso deceptive explanations that can justify and propagate false information\nand erode trust in the truth. We examined the impact of deceptive AI generated\nexplanations on individuals' beliefs in a pre-registered online experiment with\n23,840 observations from 1,192 participants. We found that in addition to being\nmore persuasive than accurate and honest explanations, AI-generated deceptive\nexplanations can significantly amplify belief in false news headlines and\nundermine true ones as compared to AI systems that simply classify the headline\nincorrectly as being true/false. Moreover, our results show that personal\nfactors such as cognitive reflection and trust in AI do not necessarily protect\nindividuals from these effects caused by deceptive AI generated explanations.\nInstead, our results show that the logical validity of AI generated deceptive\nexplanations, that is whether the explanation has a causal effect on the\ntruthfulness of the AI's classification, plays a critical role in countering\ntheir persuasiveness - with logically invalid explanations being deemed less\ncredible. This underscores the importance of teaching logical reasoning and\ncritical thinking skills to identify logically invalid arguments, fostering\ngreater resilience against advanced AI-driven misinformation.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00024v1",
    "published_date": "2024-07-31 05:39:07 UTC",
    "updated_date": "2024-07-31 05:39:07 UTC"
  },
  {
    "arxiv_id": "2407.21347v2",
    "title": "Differentially Private Block-wise Gradient Shuffle for Deep Learning",
    "authors": [
      "David Zagardo"
    ],
    "abstract": "Traditional Differentially Private Stochastic Gradient Descent (DP-SGD)\nintroduces statistical noise on top of gradients drawn from a Gaussian\ndistribution to ensure privacy. This paper introduces the novel Differentially\nPrivate Block-wise Gradient Shuffle (DP-BloGS) algorithm for deep learning.\nBloGS builds off of existing private deep learning literature, but makes a\ndefinitive shift by taking a probabilistic approach to gradient noise\nintroduction through shuffling modeled after information theoretic privacy\nanalyses. The theoretical results presented in this paper show that the\ncombination of shuffling, parameter-specific block size selection, batch layer\nclipping, and gradient accumulation allows DP-BloGS to achieve training times\nclose to that of non-private training while maintaining similar privacy and\nutility guarantees to DP-SGD. DP-BloGS is found to be significantly more\nresistant to data extraction attempts than DP-SGD. The theoretical results are\nvalidated by the experimental findings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "The results are genuine, but the math is wrong! Please do not use\n  this method for your Differential Privacy implementations",
    "pdf_url": "http://arxiv.org/pdf/2407.21347v2",
    "published_date": "2024-07-31 05:32:37 UTC",
    "updated_date": "2025-01-20 16:24:12 UTC"
  },
  {
    "arxiv_id": "2407.21344v1",
    "title": "Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction",
    "authors": [
      "Jingyao Wu",
      "Ting Dang",
      "Vidhyasaharan Sethu",
      "Eliathamby Ambikairajah"
    ],
    "abstract": "There has been a significant focus on modelling emotion ambiguity in recent\nyears, with advancements made in representing emotions as distributions to\ncapture ambiguity. However, there has been comparatively less effort devoted to\nthe consideration of temporal dependencies in emotion distributions which\nencodes ambiguity in perceived emotions that evolve smoothly over time.\nRecognizing the benefits of using constrained dynamical neural ordinary\ndifferential equations (CD-NODE) to model time series as dynamic processes, we\npropose an ambiguity-aware dual-constrained Neural ODE approach to model the\ndynamics of emotion distributions on arousal and valence. In our approach, we\nutilize ODEs parameterised by neural networks to estimate the distribution\nparameters, and we integrate additional constraints to restrict the range of\nthe system outputs to ensure the validity of predicted distributions. We\nevaluated our proposed system on the publicly available RECOLA dataset and\nobserved very promising performance across a range of evaluation metrics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted at INTERSPEECh 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21344v1",
    "published_date": "2024-07-31 05:18:06 UTC",
    "updated_date": "2024-07-31 05:18:06 UTC"
  },
  {
    "arxiv_id": "2407.21343v2",
    "title": "MIST: A Simple and Scalable End-To-End 3D Medical Imaging Segmentation Framework",
    "authors": [
      "Adrian Celaya",
      "Evan Lim",
      "Rachel Glenn",
      "Brayden Mi",
      "Alex Balsells",
      "Dawid Schellingerhout",
      "Tucker Netherton",
      "Caroline Chung",
      "Beatrice Riviere",
      "David Fuentes"
    ],
    "abstract": "Medical imaging segmentation is a highly active area of research, with deep\nlearning-based methods achieving state-of-the-art results in several\nbenchmarks. However, the lack of standardized tools for training, testing, and\nevaluating new methods makes the comparison of methods difficult. To address\nthis, we introduce the Medical Imaging Segmentation Toolkit (MIST), a simple,\nmodular, and end-to-end medical imaging segmentation framework designed to\nfacilitate consistent training, testing, and evaluation of deep learning-based\nmedical imaging segmentation methods. MIST standardizes data analysis,\npreprocessing, and evaluation pipelines, accommodating multiple architectures\nand loss functions. This standardization ensures reproducible and fair\ncomparisons across different methods. We detail MIST's data format\nrequirements, pipelines, and auxiliary features and demonstrate its efficacy\nusing the BraTS Adult Glioma Post-Treatment Challenge dataset. Our results\nhighlight MIST's ability to produce accurate segmentation masks and its\nscalability across multiple GPUs, showcasing its potential as a powerful tool\nfor future medical imaging research and development.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Submitted to BraTS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21343v2",
    "published_date": "2024-07-31 05:17:31 UTC",
    "updated_date": "2024-11-18 17:59:10 UTC"
  },
  {
    "arxiv_id": "2407.21338v1",
    "title": "Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks",
    "authors": [
      "David Valencia",
      "Henry Williams",
      "Yuning Xing",
      "Trevor Gee",
      "Minas Liarokapis",
      "Bruce A. MacDonald"
    ],
    "abstract": "Reinforcement Learning (RL) has been widely used to solve tasks where the\nenvironment consistently provides a dense reward value. However, in real-world\nscenarios, rewards can often be poorly defined or sparse. Auxiliary signals are\nindispensable for discovering efficient exploration strategies and aiding the\nlearning process. In this work, inspired by intrinsic motivation theory, we\npostulate that the intrinsic stimuli of novelty and surprise can assist in\nimproving exploration in complex, sparsely rewarded environments. We introduce\na novel sample-efficient method able to learn directly from pixels, an\nimage-based extension of TD3 with an autoencoder called \\textit{NaSA-TD3}. The\nexperiments demonstrate that NaSA-TD3 is easy to train and an efficient method\nfor tackling complex continuous-control robotic tasks, both in simulated\nenvironments and real-world settings. NaSA-TD3 outperforms existing\nstate-of-the-art RL image-based methods in terms of final performance without\nrequiring pre-trained models or human demonstrations.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21338v1",
    "published_date": "2024-07-31 05:11:06 UTC",
    "updated_date": "2024-07-31 05:11:06 UTC"
  },
  {
    "arxiv_id": "2407.21320v2",
    "title": "MetaOpenFOAM: an LLM-based multi-agent framework for CFD",
    "authors": [
      "Yuxuan Chen",
      "Xu Zhu",
      "Hua Zhou",
      "Zhuyin Ren"
    ],
    "abstract": "Remarkable progress has been made in automated problem solving through\nsocieties of agents based on large language models (LLMs). Computational fluid\ndynamics (CFD), as a complex problem, presents unique challenges in automated\nsimulations that require sophisticated solutions. MetaOpenFOAM, as a novel\nmulti-agent collaborations framework, aims to complete CFD simulation tasks\nwith only natural language as input. These simulation tasks include mesh\npre-processing, simulation and so on. MetaOpenFOAM harnesses the power of\nMetaGPT's assembly line paradigm, which assigns diverse roles to various\nagents, efficiently breaking down complex CFD tasks into manageable subtasks.\nLangchain further complements MetaOpenFOAM by integrating Retrieval-Augmented\nGeneration (RAG) technology, which enhances the framework's ability by\nintegrating a searchable database of OpenFOAM tutorials for LLMs. Tests on a\nbenchmark for natural language-based CFD solver, consisting of eight CFD\nsimulation tasks, have shown that MetaOpenFOAM achieved a high pass rate per\ntest (85%), with each test case costing only $0.22 on average. The eight CFD\nsimulation tasks encompass a range of multidimensional flow problems, covering\ncompressible and incompressible flows with different physical processes. This\ndemonstrates the capability to automate CFD simulations using only natural\nlanguage input, iteratively correcting errors to achieve the desired\nsimulations. An ablation study was conducted to verify the necessity of each\ncomponent in the multi-agent system and the RAG technology. A sensitivity study\non the randomness of LLM showed that LLM with low randomness can obtain more\nstable and accurate results. Additionally, MetaOpenFOAM owns the ability to\nidentify and modify key parameters in user requirements, and excels in\ncorrecting bugs when failure match occur,which demonstrates the generalization\nof MetaOpenFOAM.",
    "categories": [
      "cs.AI",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages,11 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.21320v2",
    "published_date": "2024-07-31 04:01:08 UTC",
    "updated_date": "2024-08-07 04:34:11 UTC"
  },
  {
    "arxiv_id": "2407.21319v1",
    "title": "Big Cooperative Learning",
    "authors": [
      "Yulai Cong"
    ],
    "abstract": "Cooperation plays a pivotal role in the evolution of human intelligence;\nmoreover, it also underlies the recent revolutionary advancement of artificial\nintelligence (AI) that is driven by foundation models. Specifically, we reveal\nthat the training of foundation models can be interpreted as a form of big\ncooperative learning (\\textit{abbr.} big learning), where massive learning\nindividuals/tasks \\emph{cooperate} to approach the unique essence of data from\ndiverse perspectives of data prediction, leveraging a universal model. The\npresented big learning therefore unifies most training objectives of foundation\nmodels within a consistent framework, where their underlying assumptions are\nexposed simultaneously. We design tailored simulations to demonstrate the\nprinciple of big learning, based on which we provide learning-perspective\njustifications for the successes of foundation models, with interesting\nside-products. Furthermore, we reveal that big learning is a new dimension for\nupgrading conventional machine learning paradigms, valuable for endowing\nreinvigorations to associated applications; as an illustrative example, we\npropose the BigLearn-GAN, which is a novel adversarially-trained foundation\nmodel with versatile data sampling capabilities. Code is available at\n\\texttt{https://github.com/YulaiCong/BigCooperativeLearning}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21319v1",
    "published_date": "2024-07-31 03:59:14 UTC",
    "updated_date": "2024-07-31 03:59:14 UTC"
  },
  {
    "arxiv_id": "2407.21315v4",
    "title": "Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances",
    "authors": [
      "Zehui Wu",
      "Ziwei Gong",
      "Lin Ai",
      "Pengyuan Shi",
      "Kaan Donbekci",
      "Julia Hirschberg"
    ],
    "abstract": "Emotion recognition in speech is a challenging multimodal task that requires\nunderstanding both verbal content and vocal nuances. This paper introduces a\nnovel approach to emotion detection using Large Language Models (LLMs), which\nhave demonstrated exceptional capabilities in natural language understanding.\nTo overcome the inherent limitation of LLMs in processing audio inputs, we\npropose SpeechCueLLM, a method that translates speech characteristics into\nnatural language descriptions, allowing LLMs to perform multimodal emotion\nanalysis via text prompts without any architectural changes. Our method is\nminimal yet impactful, outperforming baseline models that require structural\nmodifications. We evaluate SpeechCueLLM on two datasets: IEMOCAP and MELD,\nshowing significant improvements in emotion recognition accuracy, particularly\nfor high-quality audio data. We also explore the effectiveness of various\nfeature representations and fine-tuning strategies for different LLMs. Our\nexperiments demonstrate that incorporating speech descriptions yields a more\nthan 2% increase in the average weighted F1 score on IEMOCAP (from 70.111% to\n72.596%).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21315v4",
    "published_date": "2024-07-31 03:53:14 UTC",
    "updated_date": "2024-12-23 12:35:12 UTC"
  },
  {
    "arxiv_id": "2407.21311v1",
    "title": "EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer",
    "authors": [
      "Ali Abedi",
      "Q. M. Jonathan Wu",
      "Ning Zhang",
      "Farhad Pourpanah"
    ],
    "abstract": "Unsupervised domain adaptation (UDA) aims to mitigate the domain shift issue,\nwhere the distribution of training (source) data differs from that of testing\n(target) data. Many models have been developed to tackle this problem, and\nrecently vision transformers (ViTs) have shown promising results. However, the\ncomplexity and large number of trainable parameters of ViTs restrict their\ndeployment in practical applications. This underscores the need for an\nefficient model that not only reduces trainable parameters but also allows for\nadjustable complexity based on specific needs while delivering comparable\nperformance. To achieve this, in this paper we introduce an Efficient\nUnsupervised Domain Adaptation (EUDA) framework. EUDA employs the DINOv2, which\nis a self-supervised ViT, as a feature extractor followed by a simplified\nbottleneck of fully connected layers to refine features for enhanced domain\nadaptation. Additionally, EUDA employs the synergistic domain alignment loss\n(SDAL), which integrates cross-entropy (CE) and maximum mean discrepancy (MMD)\nlosses, to balance adaptation by minimizing classification errors in the source\ndomain while aligning the source and target domain distributions. The\nexperimental results indicate the effectiveness of EUDA in producing comparable\nresults as compared with other state-of-the-art methods in domain adaptation\nwith significantly fewer trainable parameters, between 42% to 99.7% fewer. This\nshowcases the ability to train the model in a resource-limited environment. The\ncode of the model is available at: https://github.com/A-Abedi/EUDA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.21311v1",
    "published_date": "2024-07-31 03:29:28 UTC",
    "updated_date": "2024-07-31 03:29:28 UTC"
  },
  {
    "arxiv_id": "2407.21300v4",
    "title": "SAKR: Enhancing Retrieval-Augmented Generation via Streaming Algorithm and K-Means Clustering",
    "authors": [
      "Haoyu Kang",
      "Yuzhou Zhu",
      "Yukun Zhong",
      "Ke Wang"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has achieved significant success in\ninformation retrieval to assist large language models LLMs because it builds an\nexternal knowledge database. However, it also has many problems, it consumes a\nlot of memory because of the enormous database, and it cannot update the\nestablished index database in time when confronted with massive streaming data.\nTo reduce the memory required for building the database and maintain accuracy\nsimultaneously, we proposed a new approach integrating a streaming algorithm\nwith k-means clustering into RAG. Our approach applied a streaming algorithm to\nupdate the index dynamically and reduce memory consumption. Additionally, the\nk-means algorithm clusters highly similar documents, and the query time would\nbe shortened. We conducted comparative experiments on four methods, and the\nresults indicated that RAG with streaming algorithm and k-means clusters\noutperforms traditional RAG in accuracy and memory, particularly when dealing\nwith large-scale streaming data.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21300v4",
    "published_date": "2024-07-31 03:00:59 UTC",
    "updated_date": "2025-05-15 03:22:21 UTC"
  },
  {
    "arxiv_id": "2407.21299v1",
    "title": "Who should I trust? A Visual Analytics Approach for Comparing Net Load Forecasting Models",
    "authors": [
      "Kaustav Bhattacharjee",
      "Soumya Kundu",
      "Indrasis Chakraborty",
      "Aritra Dasgupta"
    ],
    "abstract": "Net load forecasting is crucial for energy planning and facilitating informed\ndecision-making regarding trade and load distributions. However, evaluating\nforecasting models' performance against benchmark models remains challenging,\nthereby impeding experts' trust in the model's performance. In this context,\nthere is a demand for technological interventions that allow scientists to\ncompare models across various timeframes and solar penetration levels. This\npaper introduces a visual analytics-based application designed to compare the\nperformance of deep-learning-based net load forecasting models with other\nmodels for probabilistic net load forecasting. This application employs\ncarefully selected visual analytic interventions, enabling users to discern\ndifferences in model performance across different solar penetration levels,\ndataset resolutions, and hours of the day over multiple months. We also present\nobservations made using our application through a case study, demonstrating the\neffectiveness of visualizations in aiding scientists in making informed\ndecisions and enhancing trust in net load forecasting models.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted for publication in the proceedings of 2025 IEEE PES Grid\n  Edge Technologies Conference & Exposition (Grid Edge)",
    "pdf_url": "http://arxiv.org/pdf/2407.21299v1",
    "published_date": "2024-07-31 02:57:21 UTC",
    "updated_date": "2024-07-31 02:57:21 UTC"
  },
  {
    "arxiv_id": "2407.21298v1",
    "title": "A Vectorization Method Induced By Maximal Margin Classification For Persistent Diagrams",
    "authors": [
      "An Wu",
      "Yu Pan",
      "Fuqi Zhou",
      "Jinghui Yan",
      "Chuanlu Liu"
    ],
    "abstract": "Persistent homology is an effective method for extracting topological\ninformation, represented as persistent diagrams, of spatial structure data.\nHence it is well-suited for the study of protein structures. Attempts to\nincorporate Persistent homology in machine learning methods of protein function\nprediction have resulted in several techniques for vectorizing persistent\ndiagrams. However, current vectorization methods are excessively artificial and\ncannot ensure the effective utilization of information or the rationality of\nthe methods. To address this problem, we propose a more geometrical\nvectorization method of persistent diagrams based on maximal margin\nclassification for Banach space, and additionaly propose a framework that\nutilizes topological data analysis to identify proteins with specific\nfunctions. We evaluated our vectorization method using a binary classification\ntask on proteins and compared it with the statistical methods that exhibit the\nbest performance among thirteen commonly used vectorization methods. The\nexperimental results indicate that our approach surpasses the statistical\nmethods in both robustness and precision.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21298v1",
    "published_date": "2024-07-31 02:55:01 UTC",
    "updated_date": "2024-07-31 02:55:01 UTC"
  },
  {
    "arxiv_id": "2408.02677v1",
    "title": "Patient-centered data science: an integrative framework for evaluating and predicting clinical outcomes in the digital health era",
    "authors": [
      "Mohsen Amoei",
      "Dan Poenaru"
    ],
    "abstract": "This study proposes a novel, integrative framework for patient-centered data\nscience in the digital health era. We developed a multidimensional model that\ncombines traditional clinical data with patient-reported outcomes, social\ndeterminants of health, and multi-omic data to create comprehensive digital\npatient representations. Our framework employs a multi-agent artificial\nintelligence approach, utilizing various machine learning techniques including\nlarge language models, to analyze complex, longitudinal datasets. The model\naims to optimize multiple patient outcomes simultaneously while addressing\nbiases and ensuring generalizability. We demonstrate how this framework can be\nimplemented to create a learning healthcare system that continuously refines\nstrategies for optimal patient care. This approach has the potential to\nsignificantly improve the translation of digital health innovations into\nreal-world clinical benefits, addressing current limitations in AI-driven\nhealthcare models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02677v1",
    "published_date": "2024-07-31 02:36:17 UTC",
    "updated_date": "2024-07-31 02:36:17 UTC"
  },
  {
    "arxiv_id": "2407.21293v1",
    "title": "SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving",
    "authors": [
      "Peiru Zheng",
      "Yun Zhao",
      "Zhan Gong",
      "Hong Zhu",
      "Shaohua Wu"
    ],
    "abstract": "Many fields could benefit from the rapid development of the large language\nmodels (LLMs). The end-to-end autonomous driving (e2eAD) is one of the\ntypically fields facing new opportunities as the LLMs have supported more and\nmore modalities. Here, by utilizing vision-language model (VLM), we proposed an\ne2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided\ninto four stages, which are perception, prediction, planning, and behavior.\nEach stage consists of several visual question answering (VQA) pairs and VQA\npairs interconnect with each other constructing a graph called Graph VQA\n(GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our\nmethod could achieve e2e driving with language. In our method, vision\ntransformers (ViT) models are employed to process nuScenes visual data, while\nVLM are utilized to interpret and reason about the information extracted from\nthe visual inputs. In the perception stage, the system identifies and\nclassifies objects from the driving environment. The prediction stage involves\nforecasting the potential movements of these objects. The planning stage\nutilizes the gathered information to develop a driving strategy, ensuring the\nsafety and efficiency of the autonomous vehicle. Finally, the behavior stage\ntranslates the planned actions into executable commands for the vehicle. Our\nexperiments demonstrate that SimpleLLM4AD achieves competitive performance in\ncomplex driving scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.21293v1",
    "published_date": "2024-07-31 02:35:33 UTC",
    "updated_date": "2024-07-31 02:35:33 UTC"
  },
  {
    "arxiv_id": "2408.07825v1",
    "title": "SSRFlow: Semantic-aware Fusion with Spatial Temporal Re-embedding for Real-world Scene Flow",
    "authors": [
      "Zhiyang Lu",
      "Qinghan Chen",
      "Zhimin Yuan",
      "Ming Cheng"
    ],
    "abstract": "Scene flow, which provides the 3D motion field of the first frame from two\nconsecutive point clouds, is vital for dynamic scene perception. However,\ncontemporary scene flow methods face three major challenges. Firstly, they lack\nglobal flow embedding or only consider the context of individual point clouds\nbefore embedding, leading to embedded points struggling to perceive the\nconsistent semantic relationship of another frame. To address this issue, we\npropose a novel approach called Dual Cross Attentive (DCA) for the latent\nfusion and alignment between two frames based on semantic contexts. This is\nthen integrated into Global Fusion Flow Embedding (GF) to initialize flow\nembedding based on global correlations in both contextual and Euclidean spaces.\nSecondly, deformations exist in non-rigid objects after the warping layer,\nwhich distorts the spatiotemporal relation between the consecutive frames. For\na more precise estimation of residual flow at next-level, the Spatial Temporal\nRe-embedding (STR) module is devised to update the point sequence features at\ncurrent-level. Lastly, poor generalization is often observed due to the\nsignificant domain gap between synthetic and LiDAR-scanned datasets. We\nleverage novel domain adaptive losses to effectively bridge the gap of motion\ninference from synthetic to real-world. Experiments demonstrate that our\napproach achieves state-of-the-art (SOTA) performance across various datasets,\nwith particularly outstanding results in real-world LiDAR-scanned situations.\nOur code will be released upon publication.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages,12 figures. arXiv admin note: substantial text overlap with\n  arXiv:2403.07032",
    "pdf_url": "http://arxiv.org/pdf/2408.07825v1",
    "published_date": "2024-07-31 02:28:40 UTC",
    "updated_date": "2024-07-31 02:28:40 UTC"
  },
  {
    "arxiv_id": "2408.08315v1",
    "title": "Segment Anything for Videos: A Systematic Survey",
    "authors": [
      "Chunhui Zhang",
      "Yawen Cui",
      "Weilin Lin",
      "Guanjie Huang",
      "Yan Rong",
      "Li Liu",
      "Shiguang Shan"
    ],
    "abstract": "The recent wave of foundation models has witnessed tremendous success in\ncomputer vision (CV) and beyond, with the segment anything model (SAM) having\nsparked a passion for exploring task-agnostic visual foundation models.\nEmpowered by its remarkable zero-shot generalization, SAM is currently\nchallenging numerous traditional paradigms in CV, delivering extraordinary\nperformance not only in various image segmentation and multi-modal segmentation\n(\\eg, text-to-mask) tasks, but also in the video domain. Additionally, the\nlatest released SAM 2 is once again sparking research enthusiasm in the realm\nof promptable visual segmentation for both images and videos. However, existing\nsurveys mainly focus on SAM in various image processing tasks, a comprehensive\nand in-depth review in the video domain is notably absent. To address this gap,\nthis work conducts a systematic review on SAM for videos in the era of\nfoundation models. As the first to review the progress of SAM for videos, this\nwork focuses on its applications to various tasks by discussing its recent\nadvances, and innovation opportunities of developing foundation models on broad\napplications. We begin with a brief introduction to the background of SAM and\nvideo-related research domains. Subsequently, we present a systematic taxonomy\nthat categorizes existing methods into three key areas: video understanding,\nvideo generation, and video editing, analyzing and summarizing their advantages\nand limitations. Furthermore, comparative results of SAM-based and current\nstate-of-the-art methods on representative benchmarks, as well as insightful\nanalysis are offered. Finally, we discuss the challenges faced by current\nresearch and envision several future research directions in the field of SAM\nfor video and beyond.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "https://github.com/983632847/SAM-for-Videos",
    "pdf_url": "http://arxiv.org/pdf/2408.08315v1",
    "published_date": "2024-07-31 02:24:53 UTC",
    "updated_date": "2024-07-31 02:24:53 UTC"
  },
  {
    "arxiv_id": "2407.21284v1",
    "title": "Robust Box Prompt based SAM for Medical Image Segmentation",
    "authors": [
      "Yuhao Huang",
      "Xin Yang",
      "Han Zhou",
      "Yan Cao",
      "Haoran Dou",
      "Fajin Dong",
      "Dong Ni"
    ],
    "abstract": "The Segment Anything Model (SAM) can achieve satisfactory segmentation\nperformance under high-quality box prompts. However, SAM's robustness is\ncompromised by the decline in box quality, limiting its practicality in\nclinical reality. In this study, we propose a novel Robust Box prompt based SAM\n(\\textbf{RoBox-SAM}) to ensure SAM's segmentation performance under prompts\nwith different qualities. Our contribution is three-fold. First, we propose a\nprompt refinement module to implicitly perceive the potential targets, and\noutput the offsets to directly transform the low-quality box prompt into a\nhigh-quality one. We then provide an online iterative strategy for further\nprompt refinement. Second, we introduce a prompt enhancement module to\nautomatically generate point prompts to assist the box-promptable segmentation\neffectively. Last, we build a self-information extractor to encode the prior\ninformation from the input image. These features can optimize the image\nembeddings and attention calculation, thus, the robustness of SAM can be\nfurther enhanced. Extensive experiments on the large medical segmentation\ndataset including 99,299 images, 5 modalities, and 25 organs/targets validated\nthe efficacy of our proposed RoBox-SAM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by MICCAI MLMI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21284v1",
    "published_date": "2024-07-31 02:16:28 UTC",
    "updated_date": "2024-07-31 02:16:28 UTC"
  },
  {
    "arxiv_id": "2407.21281v1",
    "title": "Unlocking the Potential of Binding Corporate Rules (BCRs) in Health Data Transfers",
    "authors": [
      "Marcelo Corrales Compagnucci",
      "Mark Fenwick",
      "Helena Haapio"
    ],
    "abstract": "This chapter explores the essential role of Binding Corporate Rules (BCRs) in\nmanaging and facilitating secure health data transfers within corporate groups\nunder the EU General Data Protection Regulation (GDPR). BCRs are tailored to\nensure compliance with the GDPR and similar international data protection laws,\npresenting a flexible mechanism for transferring sensitive health and genomic\ndata. The chapter situates BCRs within the broader spectrum of the GDPR\ninternational data transfer mechanisms, addressing the unique challenges posed\nby the sensitive nature of health data and the increased adoption of AI\ntechnologies. The European Data Protection Board (EDPB) Recommendations 1/2022\non BCRs, issued following the Schrems II decision, are critically analyzed,\nhighlighting their stringent requirements and the need for a balanced approach\nthat prioritizes data protection and an AI governance framework. The chapter\noutlines the BCR approval process, stressing the importance of streamlining\nthis process to encourage broader adoption. It underscores the necessity of a\nmultidisciplinary approach in developing BCRs, incorporating recently adopted\ninternational standards and frameworks, which offer valuable guidance for\norganizations to build trustworthy AI management systems. They guarantee the\nethical development, deployment, and operation of AI, which is essential for\nits successful integration and the broader digital transformation. In\nconclusion, BCRs are positioned as essential tools for secure health data\nmanagement, fostering transparency, accountability, and collaboration across\ninternational borders. The chapter calls for proactive measures to incentivize\nBCR adoption, streamline approval processes, and promote more innovative\napproaches, ensuring BCRs remain a robust mechanism for global data protection\nand compliance.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21281v1",
    "published_date": "2024-07-31 02:09:52 UTC",
    "updated_date": "2024-07-31 02:09:52 UTC"
  },
  {
    "arxiv_id": "2407.21276v3",
    "title": "Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented Generation",
    "authors": [
      "Rubing Chen",
      "Xulu Zhang",
      "Jiaxin Wu",
      "Wenqi Fan",
      "Xiao-Yong Wei",
      "Qing Li"
    ],
    "abstract": "This paper addresses the need for improved precision in existing\nknowledge-enhanced question-answering frameworks, specifically\nRetrieval-Augmented Generation (RAG) methods that primarily focus on enhancing\nrecall. We propose a multi-layer knowledge pyramid approach within the RAG\nframework to achieve a better balance between precision and recall. The\nknowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),\nand chunk-based raw text. We employ cross-layer augmentation techniques for\ncomprehensive knowledge coverage and dynamic updates of the Ontology schema and\ninstances. To ensure compactness, we utilize cross-layer filtering methods for\nknowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall\nmodel for retrieval, starting from the top of the pyramid and progressing down\nuntil a confident answer is obtained. We introduce two benchmarks for\ndomain-specific knowledge retrieval, one in the academic domain and the other\nin the financial domain. The effectiveness of the methods has been validated\nthrough comprehensive experiments by outperforming 19 SOTA methods. An\nencouraging observation is that the proposed method has augmented the GPT-4,\nproviding 395% F1 gain by improving its performance from 0.1636 to 0.8109.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21276v3",
    "published_date": "2024-07-31 01:51:24 UTC",
    "updated_date": "2025-02-21 04:00:21 UTC"
  },
  {
    "arxiv_id": "2407.21275v7",
    "title": "Fi$^2$VTS: Time Series Forecasting Via Capturing Intra- and Inter-Variable Variations in the Frequency Domain",
    "authors": [
      "Rujia Shen",
      "Yang Yang",
      "Yaoxion Lin",
      "Liangliang Liu",
      "Boran Wang",
      "Yi Guan",
      "Jingchi Jiang"
    ],
    "abstract": "Time series forecasting (TSF) plays a crucial role in various applications,\nincluding medical monitoring and crop growth. Despite the advancements in deep\nlearning methods for TSF, their capacity to predict long-term series remains\nconstrained. This limitation arises from the failure to account for both intra-\nand inter-variable variations meanwhile. To mitigate this challenge, we\nintroduce the Fi$^2$VBlock, which leverages a \\textbf{F}requency domain\nperspective to capture \\textbf{i}ntra- and \\textbf{i}nter-variable\n\\textbf{V}ariations. After transforming into the frequency domain via the\nFrequency Transform Module, the Frequency Cross Attention between the real and\nimaginary parts is designed to obtain enhanced frequency representations and\ncapture intra-variable variations. Furthermore, Inception blocks are employed\nto integrate information, thus capturing correlations across different\nvariables. Our backbone network, Fi$^2$VTS, employs a residual architecture by\nconcatenating multiple Fi$^2$VBlocks, thereby preventing degradation issues.\nTheoretically, we demonstrate that Fi$^2$VTS achieves a substantial reduction\nin both time and memory complexity, decreasing from $\\mathcal{O}(L^2)$ to\n$\\mathcal{O}(L)$ per Fi$^2$VBlock computation. Empirical evaluations reveal\nthat Fi$^2$VTS outperforms other baselines on two benchmark datasets. The\nimplementation code is accessible at \\url{https://github.com/HITshenrj/Fi2VTS}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "There was an error in the experimental results; we mistakenly took\n  the result of our method on the ETTh2 dataset as the result of our method on\n  the ETTh1 dataset",
    "pdf_url": "http://arxiv.org/pdf/2407.21275v7",
    "published_date": "2024-07-31 01:50:39 UTC",
    "updated_date": "2024-11-03 04:17:58 UTC"
  },
  {
    "arxiv_id": "2407.21273v1",
    "title": "Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net",
    "authors": [
      "Rohini Banerjee",
      "Cecilia G. Morales",
      "Artur Dubrawski"
    ],
    "abstract": "Efficient intravascular access in trauma and critical care significantly\nimpacts patient outcomes. However, the availability of skilled medical\npersonnel in austere environments is often limited. Autonomous robotic\nultrasound systems can aid in needle insertion for medication delivery and\nsupport non-experts in such tasks. Despite advances in autonomous needle\ninsertion, inaccuracies in vessel segmentation predictions pose risks.\nUnderstanding the uncertainty of predictive models in ultrasound imaging is\ncrucial for assessing their reliability. We introduce MSU-Net, a novel\nmultistage approach for training an ensemble of U-Nets to yield accurate\nultrasound image segmentation maps. We demonstrate substantial improvements,\n18.1% over a single Monte Carlo U-Net, enhancing uncertainty evaluations, model\ntransparency, and trustworthiness. By highlighting areas of model certainty,\nMSU-Net can guide safe needle insertions, empowering non-experts to accomplish\nsuch tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for the 5th International Workshop of Advances in\n  Simplifying Medical UltraSound (ASMUS), held in conjunction with MICCAI 2024,\n  the 27th International Conference on Medical Image Computing and Computer\n  Assisted Intervention",
    "pdf_url": "http://arxiv.org/pdf/2407.21273v1",
    "published_date": "2024-07-31 01:36:47 UTC",
    "updated_date": "2024-07-31 01:36:47 UTC"
  },
  {
    "arxiv_id": "2407.21272v1",
    "title": "Automated Quantification of Hyperreflective Foci in SD-OCT With Diabetic Retinopathy",
    "authors": [
      "Idowu Paul Okuwobi",
      "Zexuan Ji",
      "Wen Fan",
      "Songtao Yuan",
      "Loza Bekalo",
      "Qiang Chen"
    ],
    "abstract": "The presence of hyperreflective foci (HFs) is related to retinal disease\nprogression, and the quantity has proven to be a prognostic factor of visual\nand anatomical outcome in various retinal diseases. However, lack of efficient\nquantitative tools for evaluating the HFs has deprived ophthalmologist of\nassessing the volume of HFs. For this reason, we propose an automated\nquantification algorithm to segment and quantify HFs in spectral domain optical\ncoherence tomography (SD-OCT). The proposed algorithm consists of two parallel\nprocesses namely: region of interest (ROI) generation and HFs estimation. To\ngenerate the ROI, we use morphological reconstruction to obtain the\nreconstructed image and histogram constructed for data distributions and\nclustering. In parallel, we estimate the HFs by extracting the extremal regions\nfrom the connected regions obtained from a component tree. Finally, both the\nROI and the HFs estimation process are merged to obtain the segmented HFs. The\nproposed algorithm was tested on 40 3D SD-OCT volumes from 40 patients\ndiagnosed with non-proliferative diabetic retinopathy (NPDR), proliferative\ndiabetic retinopathy (PDR), and diabetic macular edema (DME). The average dice\nsimilarity coefficient (DSC) and correlation coefficient (r) are 69.70%, 0.99\nfor NPDR, 70.31%, 0.99 for PDR, and 71.30%, 0.99 for DME, respectively. The\nproposed algorithm can provide ophthalmologist with good HFs quantitative\ninformation, such as volume, size, and location of the HFs.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "IEEE Journal of Biomedical and Health Informatics, Volume: 24, Issue:\n  4, pp. 1125 - 1136, 2020",
    "pdf_url": "http://arxiv.org/pdf/2407.21272v1",
    "published_date": "2024-07-31 01:33:47 UTC",
    "updated_date": "2024-07-31 01:33:47 UTC"
  },
  {
    "arxiv_id": "2407.21267v1",
    "title": "DEF-oriCORN: efficient 3D scene understanding for robust language-directed manipulation without demonstrations",
    "authors": [
      "Dongwon Son",
      "Sanghyeon Son",
      "Jaehyung Kim",
      "Beomjoon Kim"
    ],
    "abstract": "We present DEF-oriCORN, a framework for language-directed manipulation tasks.\nBy leveraging a novel object-based scene representation and\ndiffusion-model-based state estimation algorithm, our framework enables\nefficient and robust manipulation planning in response to verbal commands, even\nin tightly packed environments with sparse camera views without any\ndemonstrations. Unlike traditional representations, our representation affords\nefficient collision checking and language grounding. Compared to\nstate-of-the-art baselines, our framework achieves superior estimation and\nmotion planning performance from sparse RGB images and zero-shot generalizes to\nreal-world scenarios with diverse materials, including transparent and\nreflective objects, despite being trained exclusively in simulation. Our code\nfor data generation, training, inference, and pre-trained weights are publicly\navailable at: https://sites.google.com/view/def-oricorn/home.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21267v1",
    "published_date": "2024-07-31 01:13:25 UTC",
    "updated_date": "2024-07-31 01:13:25 UTC"
  },
  {
    "arxiv_id": "2407.21260v3",
    "title": "Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation",
    "authors": [
      "Taehyun Cho",
      "Seungyub Han",
      "Seokhun Ju",
      "Dohyeong Kim",
      "Kyungjae Lee",
      "Jungwoo Lee"
    ],
    "abstract": "Distributional reinforcement learning improves performance by capturing\nenvironmental stochasticity, but a comprehensive theoretical understanding of\nits effectiveness remains elusive. In addition, the intractable element of the\ninfinite dimensionality of distributions has been overlooked. In this paper, we\npresent a regret analysis of distributional reinforcement learning with general\nvalue function approximation in a finite episodic Markov decision process\nsetting. We first introduce a key notion of $\\textit{Bellman unbiasedness}$\nwhich is essential for exactly learnable and provably efficient distributional\nupdates in an online manner. Among all types of statistical functionals for\nrepresenting infinite-dimensional return distributions, our theoretical results\ndemonstrate that only moment functionals can exactly capture the statistical\ninformation. Secondly, we propose a provably efficient algorithm,\n$\\texttt{SF-LSVI}$, that achieves a tight regret bound of $\\tilde{O}(d_E\nH^{\\frac{3}{2}}\\sqrt{K})$ where $H$ is the horizon, $K$ is the number of\nepisodes, and $d_E$ is the eluder dimension of a function class.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21260v3",
    "published_date": "2024-07-31 00:43:51 UTC",
    "updated_date": "2025-05-13 04:53:31 UTC"
  },
  {
    "arxiv_id": "2407.21252v1",
    "title": "Lifelong Person Search",
    "authors": [
      "Jae-Won Yang",
      "Seungbin Hong",
      "Jae-Young Sim"
    ],
    "abstract": "Person search is the task to localize a query person in gallery datasets of\nscene images. Existing methods have been mainly developed to handle a single\ntarget dataset only, however diverse datasets are continuously given in\npractical applications of person search. In such cases, they suffer from the\ncatastrophic knowledge forgetting in the old datasets when trained on new\ndatasets. In this paper, we first introduce a novel problem of lifelong person\nsearch (LPS) where the model is incrementally trained on the new datasets while\npreserving the knowledge learned in the old datasets. We propose an end-to-end\nLPS framework that facilitates the knowledge distillation to enforce the\nconsistency learning between the old and new models by utilizing the prototype\nfeatures of the foreground persons as well as the hard background proposals in\nthe old domains. Moreover, we also devise the rehearsal-based instance matching\nto further improve the discrimination ability in the old domains by using the\nunlabeled person instances additionally. Experimental results demonstrate that\nthe proposed method achieves significantly superior performance of both the\ndetection and re-identification to preserve the knowledge learned in the old\ndomains compared with the existing methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 6 figure",
    "pdf_url": "http://arxiv.org/pdf/2407.21252v1",
    "published_date": "2024-07-31 00:19:22 UTC",
    "updated_date": "2024-07-31 00:19:22 UTC"
  }
]
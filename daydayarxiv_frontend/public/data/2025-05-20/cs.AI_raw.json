[
  {
    "arxiv_id": "2506.11033v1",
    "title": "Runtime Safety through Adaptive Shielding: From Hidden Parameter Inference to Provable Guarantees",
    "authors": [
      "Minjae Kwon",
      "Tyler Ingebrand",
      "Ufuk Topcu",
      "Lu Feng"
    ],
    "abstract": "Variations in hidden parameters, such as a robot's mass distribution or friction, pose safety risks during execution. We develop a runtime shielding mechanism for reinforcement learning, building on the formalism of constrained hidden-parameter Markov decision processes. Function encoders enable real-time inference of hidden parameters from observations, allowing the shield and the underlying policy to adapt online. The shield constrains the action space by forecasting future safety risks (such as obstacle proximity) and accounts for uncertainty via conformal prediction. We prove that the proposed mechanism satisfies probabilistic safety guarantees and yields optimal policies among the set of safety-compliant policies. Experiments across diverse environments with varying hidden parameters show that our method significantly reduces safety violations and achieves strong out-of-distribution generalization, while incurring minimal runtime overhead.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted",
    "pdf_url": "https://arxiv.org/pdf/2506.11033v1",
    "published_date": "2025-05-20 23:45:45 UTC",
    "updated_date": "2025-05-20 23:45:45 UTC"
  },
  {
    "arxiv_id": "2505.14978v2",
    "title": "JARVIS: A Multi-Agent Code Assistant for High-Quality EDA Script Generation",
    "authors": [
      "Ghasem Pasandi",
      "Kishor Kunal",
      "Varun Tej",
      "Kunjal Shah",
      "Hanfei Sun",
      "Sumit Jain",
      "Chunhui Li",
      "Chenhui Deng",
      "Teodor-Dumitru Ene",
      "Haoxing Ren",
      "Sreedhar Pratty"
    ],
    "abstract": "This paper presents JARVIS, a novel multi-agent framework that leverages Large Language Models (LLMs) and domain expertise to generate high-quality scripts for specialized Electronic Design Automation (EDA) tasks. By combining a domain-specific LLM trained with synthetically generated data, a custom compiler for structural verification, rule enforcement, code fixing capabilities, and advanced retrieval mechanisms, our approach achieves significant improvements over state-of-the-art domain-specific models. Our framework addresses the challenges of data scarcity and hallucination errors in LLMs, demonstrating the potential of LLMs in specialized engineering domains. We evaluate our framework on multiple benchmarks and show that it outperforms existing models in terms of accuracy and reliability. Our work sets a new precedent for the application of LLMs in EDA and paves the way for future innovations in this field.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14978v2",
    "published_date": "2025-05-20 23:40:57 UTC",
    "updated_date": "2025-08-15 23:31:27 UTC"
  },
  {
    "arxiv_id": "2505.14976v1",
    "title": "SDLog: A Deep Learning Framework for Detecting Sensitive Information in Software Logs",
    "authors": [
      "Roozbeh Aghili",
      "Xingfang Wu",
      "Foutse Khomh",
      "Heng Li"
    ],
    "abstract": "Software logs are messages recorded during the execution of a software system that provide crucial run-time information about events and activities. Although software logs have a critical role in software maintenance and operation tasks, publicly accessible log datasets remain limited, hindering advance in log analysis research and practices. The presence of sensitive information, particularly Personally Identifiable Information (PII) and quasi-identifiers, introduces serious privacy and re-identification risks, discouraging the publishing and sharing of real-world logs. In practice, log anonymization techniques primarily rely on regular expression patterns, which involve manually crafting rules to identify and replace sensitive information. However, these regex-based approaches suffer from significant limitations, such as extensive manual efforts and poor generalizability across diverse log formats and datasets. To mitigate these limitations, we introduce SDLog, a deep learning-based framework designed to identify sensitive information in software logs. Our results show that SDLog overcomes regex limitations and outperforms the best-performing regex patterns in identifying sensitive information. With only 100 fine-tuning samples from the target dataset, SDLog can correctly identify 99.5% of sensitive attributes and achieves an F1-score of 98.4%. To the best of our knowledge, this is the first deep learning alternative to regex-based methods in software log anonymization.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14976v1",
    "published_date": "2025-05-20 23:36:13 UTC",
    "updated_date": "2025-05-20 23:36:13 UTC"
  },
  {
    "arxiv_id": "2505.14975v3",
    "title": "Flattening Hierarchies with Policy Bootstrapping",
    "authors": [
      "John L. Zhou",
      "Jonathan C. Kao"
    ],
    "abstract": "Offline goal-conditioned reinforcement learning (GCRL) is a promising approach for pretraining generalist policies on large datasets of reward-free trajectories, akin to the self-supervised objectives used to train foundation models for computer vision and natural language processing. However, scaling GCRL to longer horizons remains challenging due to the combination of sparse rewards and discounting, which obscures the comparative advantages of primitive actions with respect to distant goals. Hierarchical RL methods achieve strong empirical results on long-horizon goal-reaching tasks, but their reliance on modular, timescale-specific policies and subgoal generation introduces significant additional complexity and hinders scaling to high-dimensional goal spaces. In this work, we introduce an algorithm to train a flat (non-hierarchical) goal-conditioned policy by bootstrapping on subgoal-conditioned policies with advantage-weighted importance sampling. Our approach eliminates the need for a generative model over the (sub)goal space, which we find is key for scaling to high-dimensional control in large state spaces. We further show that existing hierarchical and bootstrapping-based approaches correspond to specific design choices within our derivation. Across a comprehensive suite of state- and pixel-based locomotion and manipulation benchmarks, our method matches or surpasses state-of-the-art offline GCRL algorithms and scales to complex, long-horizon tasks where prior approaches fail. Project page: https://johnlyzhou.github.io/saw/",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025 (Spotlight, top 3.2%)",
    "pdf_url": "https://arxiv.org/pdf/2505.14975v3",
    "published_date": "2025-05-20 23:31:30 UTC",
    "updated_date": "2026-01-02 12:08:01 UTC"
  },
  {
    "arxiv_id": "2505.14970v4",
    "title": "Self-Evolving Curriculum for LLM Reasoning",
    "authors": [
      "Xiaoyin Chen",
      "Jiarui Lu",
      "Minsu Kim",
      "Dinghuai Zhang",
      "Jian Tang",
      "Alexandre Piché",
      "Nicolas Gontier",
      "Yoshua Bengio",
      "Ehsan Kamalloo"
    ],
    "abstract": "Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14970v4",
    "published_date": "2025-05-20 23:17:15 UTC",
    "updated_date": "2025-10-30 07:03:09 UTC"
  },
  {
    "arxiv_id": "2505.21513v1",
    "title": "Enhancing Vision Transformer Explainability Using Artificial Astrocytes",
    "authors": [
      "Nicolas Echevarrieta-Catalan",
      "Ana Ribas-Rodriguez",
      "Francisco Cedron",
      "Odelia Schwartz",
      "Vanessa Aguiar-Pulido"
    ],
    "abstract": "Machine learning models achieve high precision, but their decision-making processes often lack explainability. Furthermore, as model complexity increases, explainability typically decreases. Existing efforts to improve explainability primarily involve developing new eXplainable artificial intelligence (XAI) techniques or incorporating explainability constraints during training. While these approaches yield specific improvements, their applicability remains limited. In this work, we propose the Vision Transformer with artificial Astrocytes (ViTA). This training-free approach is inspired by neuroscience and enhances the reasoning of a pretrained deep neural network to generate more human-aligned explanations. We evaluated our approach employing two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the similarity between the heatmaps produced by the XAI techniques and a (human-aligned) ground truth. Our results consistently demonstrate that incorporating artificial astrocytes enhances the alignment of model explanations with human perception, leading to statistically significant improvements across all XAI techniques and metrics utilized.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "LXCV Workshop at IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21513v1",
    "published_date": "2025-05-20 23:16:10 UTC",
    "updated_date": "2025-05-20 23:16:10 UTC"
  },
  {
    "arxiv_id": "2505.14969v2",
    "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
    "authors": [
      "Yangchao Wu",
      "Zongyue Qin",
      "Alex Wong",
      "Stefano Soatto"
    ],
    "abstract": "Speculative decoding is a technique to leverage hardware concurrency in order to enable multiple steps of token generation in a single forward pass, thus improving the efficiency of large-scale autoregressive (AR) Transformer models. State-space models (SSMs) are already more efficient than AR Transformers, since their state summarizes all past data with no need to cache or re-process tokens in the sliding window context. However, their state can also comprise thousands of tokens; so, speculative decoding has recently been extended to SSMs. Existing approaches, however, do not leverage the tree-based verification methods, since current SSMs lack the means to compute a token tree efficiently. We propose the first scalable algorithm to perform tree-based speculative decoding in state-space models (SSMs) and hybrid architectures of SSMs and Transformer layers. We exploit the structure of accumulated state transition matrices to facilitate tree-based speculative decoding with minimal overhead relative to current SSM implementations. Along with the algorithm, we describe a hardware-aware implementation that improves naive application of AR Transformer tree-based speculative decoding methods to SSMs. Furthermore, we outperform vanilla speculative decoding with SSMs even with a baseline drafting model and tree structure on three different benchmarks, opening up opportunities for further speed up with SSM and hybrid model inference. Code can be found at: https://github.com/wyc1997/stree.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14969v2",
    "published_date": "2025-05-20 23:12:16 UTC",
    "updated_date": "2025-10-27 21:48:48 UTC"
  },
  {
    "arxiv_id": "2505.14967v1",
    "title": "Anomaly Detection Based on Critical Paths for Deep Neural Networks",
    "authors": [
      "Fangzhen Zhao",
      "Chenyi Zhang",
      "Naipeng Dong",
      "Ming Li",
      "Jinxiao Shan"
    ],
    "abstract": "Deep neural networks (DNNs) are notoriously hard to understand and difficult to defend. Extracting representative paths (including the neuron activation values and the connections between neurons) from DNNs using software engineering approaches has recently shown to be a promising approach in interpreting the decision making process of blackbox DNNs, as the extracted paths are often effective in capturing essential features. With this in mind, this work investigates a novel approach that extracts critical paths from DNNs and subsequently applies the extracted paths for the anomaly detection task, based on the observation that outliers and adversarial inputs do not usually induce the same activation pattern on those paths as normal (in-distribution) inputs.\n  In our approach, we first identify critical detection paths via genetic evolution and mutation. Since different paths in a DNN often capture different features for the same target class, we ensemble detection results from multiple paths by integrating random subspace sampling and a voting mechanism. Compared with state-of-the-art methods, our experimental results suggest that our method not only outperforms them, but it is also suitable for the detection of a broad range of anomaly types with high accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages in ACM journal latex format",
    "pdf_url": "https://arxiv.org/pdf/2505.14967v1",
    "published_date": "2025-05-20 23:10:59 UTC",
    "updated_date": "2025-05-20 23:10:59 UTC"
  },
  {
    "arxiv_id": "2505.14964v1",
    "title": "The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models",
    "authors": [
      "Dave Cook",
      "Tim Klawa"
    ],
    "abstract": "AI systems in high-consequence domains such as defense, intelligence, and disaster response must detect rare, high-impact events while operating under tight resource constraints. Traditional annotation strategies that prioritize label volume over informational value introduce redundancy and noise, limiting model generalization. This paper introduces smart-sizing, a training data strategy that emphasizes label diversity, model-guided selection, and marginal utility-based stopping. We implement this through Adaptive Label Optimization (ALO), combining pre-labeling triage, annotator disagreement analysis, and iterative feedback to prioritize labels that meaningfully improve model performance. Experiments show that models trained on 20 to 40 percent of curated data can match or exceed full-data baselines, particularly in rare-class recall and edge-case generalization. We also demonstrate how latent labeling errors embedded in training and validation sets can distort evaluation, underscoring the need for embedded audit tools and performance-aware governance. Smart-sizing reframes annotation as a feedback-driven process aligned with mission outcomes, enabling more robust models with fewer labels and supporting efficient AI development pipelines for frontier models and operational systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14964v1",
    "published_date": "2025-05-20 22:57:35 UTC",
    "updated_date": "2025-05-20 22:57:35 UTC"
  },
  {
    "arxiv_id": "2506.11031v3",
    "title": "Prefilled responses enhance zero-shot detection of AI-generated images",
    "authors": [
      "Zoher Kachwala",
      "Danishjeet Singh",
      "Danielle Yang",
      "Filippo Menczer"
    ],
    "abstract": "As AI models generate increasingly realistic images, growing concerns over potential misuse underscore the need for reliable detection. Traditional supervised detection methods depend on large, curated datasets for training and often fail to generalize to novel, out-of-domain image generators. As an alternative, we explore pre-trained Vision-Language Models (VLMs) for zero-shot detection of AI-generated images. We evaluate VLM performance on three diverse benchmarks encompassing synthetic images of human faces, objects, and animals produced by 16 different state-of-the-art image generators. While off-the-shelf VLMs perform poorly on these datasets, we find that their reasoning can be guided effectively through simple response prefilling -- a method we call Prefill-Guided Thinking (PGT). In particular, prefilling a VLM response with the task-aligned phrase \"Let's examine the style and the synthesis artifacts\" improves the Macro F1 scores of three widely used open-source VLMs by up to 24%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11031v3",
    "published_date": "2025-05-20 22:44:04 UTC",
    "updated_date": "2025-10-08 16:59:43 UTC"
  },
  {
    "arxiv_id": "2506.00019v1",
    "title": "Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese",
    "authors": [
      "William Alberto Cruz-Castañeda",
      "Marcellus Amadeus"
    ],
    "abstract": "This report introduces the experience of developing Amadeus Verbo, a family of large language models for Brazilian Portuguese. To handle diverse use cases, Amadeus Verbo includes base-tuned, merged, and instruction-tuned models in sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Thus, the main objective is to show how easy it is to fine-tune foundation models to democratize the open-source development of Brazilian Portuguese LLMs when data and resources are available. Amadeus-Verbo family models are all available at HuggingFace at https://huggingface.co/collections/amadeusai/amadeus-verbo-qwen25-67cf2e7aae69ce2b3bcdcfda.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00019v1",
    "published_date": "2025-05-20 22:40:00 UTC",
    "updated_date": "2025-05-20 22:40:00 UTC"
  },
  {
    "arxiv_id": "2505.17091v2",
    "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
    "authors": [
      "Prateek Verma",
      "Mert Pilanci"
    ],
    "abstract": "This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 3 figures, 4 tables. Added BLIP reference",
    "pdf_url": "https://arxiv.org/pdf/2505.17091v2",
    "published_date": "2025-05-20 22:20:16 UTC",
    "updated_date": "2025-09-23 03:02:04 UTC"
  },
  {
    "arxiv_id": "2505.14948v1",
    "title": "Programmatic Video Prediction Using Large Language Models",
    "authors": [
      "Hao Tang",
      "Kevin Ellis",
      "Suhas Lohit",
      "Michael J. Jones",
      "Moitreya Chatterjee"
    ],
    "abstract": "The task of estimating the world model describing the dynamics of a real world process assumes immense importance for anticipating and preparing for future outcomes. For applications such as video surveillance, robotics applications, autonomous driving, etc. this objective entails synthesizing plausible visual futures, given a few frames of a video to set the visual context. Towards this end, we propose ProgGen, which undertakes the task of video frame prediction by representing the dynamics of the video using a set of neuro-symbolic, human-interpretable set of states (one per frame) by leveraging the inductive biases of Large (Vision) Language Models (LLM/VLM). In particular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate the states of the video, given the visual context (i.e. the frames); (ii) to predict the states corresponding to future time steps by estimating the transition dynamics; (iii) to render the predicted states as visual RGB-frames. Empirical evaluations reveal that our proposed method outperforms competing techniques at the task of video frame prediction in two challenging environments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits counter-factual reasoning and interpretable video generation attesting to its effectiveness and generalizability for video generation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14948v1",
    "published_date": "2025-05-20 22:17:47 UTC",
    "updated_date": "2025-05-20 22:17:47 UTC"
  },
  {
    "arxiv_id": "2505.14946v1",
    "title": "Reinforcement Learning from User Feedback",
    "authors": [
      "Eric Han",
      "Jun Chen",
      "Karthik Abinav Sankararaman",
      "Xiaoliang Peng",
      "Tengyu Xu",
      "Eryk Helenowski",
      "Kaiyan Peng",
      "Mrinal Kumar",
      "Sinong Wang",
      "Han Fang",
      "Arya Talebzadeh"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed in diverse user facing applications, aligning them with real user preferences becomes essential. Existing methods like Reinforcement Learning from Human Feedback (RLHF) rely on expert annotators trained on manually defined guidelines, whose judgments may not reflect the priorities of everyday users. We introduce Reinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs directly to implicit signals from users in production. RLUF addresses key challenges of user feedback: user feedback is often binary (e.g., emoji reactions), sparse, and occasionally adversarial. We train a reward model, P[Love], to predict the likelihood that an LLM response will receive a Love Reaction, a lightweight form of positive user feedback, and integrate P[Love] into a multi-objective policy optimization framework alongside helpfulness and safety objectives. In large-scale experiments, we show that P[Love] is predictive of increased positive feedback and serves as a reliable offline evaluator of future user behavior. Policy optimization using P[Love] significantly raises observed positive-feedback rates, including a 28% increase in Love Reactions during live A/B tests. However, optimizing for positive reactions introduces reward hacking challenges, requiring careful balancing of objectives. By directly leveraging implicit signals from users, RLUF offers a path to aligning LLMs with real-world user preferences at scale.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14946v1",
    "published_date": "2025-05-20 22:14:44 UTC",
    "updated_date": "2025-05-20 22:14:44 UTC"
  },
  {
    "arxiv_id": "2505.14943v1",
    "title": "Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities",
    "authors": [
      "Ross Nordby"
    ],
    "abstract": "To help evaluate and understand the latent capabilities of language models, this paper introduces an approach using optimized input embeddings, or 'soft prompts,' as a metric of conditional distance between a model and a target behavior. The technique aims to facilitate latent capability discovery as a part of automated red teaming/evaluation suites and to provide quantitative feedback about the accessibility of potentially concerning behaviors in a way that may scale to powerful future models, including those which may otherwise be capable of deceptive alignment. An evaluation framework using soft prompts is demonstrated in natural language, chess, and pathfinding, and the technique is extended with generalized conditional soft prompts to aid in constructing task evaluations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14943v1",
    "published_date": "2025-05-20 22:02:53 UTC",
    "updated_date": "2025-05-20 22:02:53 UTC"
  },
  {
    "arxiv_id": "2505.14940v1",
    "title": "To Be or Not To Be: Vector ontologies as a truly formal ontological framework",
    "authors": [
      "Kaspar Rothenfusser"
    ],
    "abstract": "Since Edmund Husserl coined the term \"Formal Ontologies\" in the early 20th century, a field that identifies itself with this particular branch of sciences has gained increasing attention. Many authors, and even Husserl himself have developed what they claim to be formal ontologies. I argue that under close inspection, none of these so claimed formal ontologies are truly formal in the Husserlian sense. More concretely, I demonstrate that they violate the two most important notions of formal ontology as developed in Husserl's Logical Investigations, namely a priori validity independent of perception and formalism as the total absence of content. I hence propose repositioning the work previously understood as formal ontology as the foundational ontology it really is. This is to recognize the potential of a truly formal ontology in the Husserlian sense. Specifically, I argue that formal ontology following his conditions, allows us to formulate ontological structures, which could capture what is more objectively without presupposing a particular framework arising from perception. I further argue that the ability to design the formal structure deliberately allows us to create highly scalable and interoperable information artifacts. As concrete evidence, I showcase that a class of formal ontology, which uses the axioms of vector spaces, is able to express most of the conceptualizations found in foundational ontologies. Most importantly, I argue that many information systems, specifically artificial intelligence, are likely already using some type of vector ontologies to represent reality in their internal worldviews and elaborate on the evidence that humans do as well. I hence propose a thorough investigation of the ability of vector ontologies to act as a human-machine interoperable ontological framework that allows us to understand highly sophisticated machines and machines to understand us.",
    "categories": [
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14940v1",
    "published_date": "2025-05-20 21:58:38 UTC",
    "updated_date": "2025-05-20 21:58:38 UTC"
  },
  {
    "arxiv_id": "2505.14932v2",
    "title": "FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale",
    "authors": [
      "Isabelle Lee",
      "Sarah Liaw",
      "Dani Yogatama"
    ],
    "abstract": "Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14932v2",
    "published_date": "2025-05-20 21:38:28 UTC",
    "updated_date": "2025-12-11 16:02:17 UTC"
  },
  {
    "arxiv_id": "2505.14931v1",
    "title": "Colors Matter: AI-Driven Exploration of Human Feature Colors",
    "authors": [
      "Rama Alyoubi",
      "Taif Alharbi",
      "Albatul Alghamdi",
      "Yara Alshehri",
      "Elham Alghamdi"
    ],
    "abstract": "This study presents a robust framework that leverages advanced imaging techniques and machine learning for feature extraction and classification of key human attributes-namely skin tone, hair color, iris color, and vein-based undertones. The system employs a multi-stage pipeline involving face detection, region segmentation, and dominant color extraction to isolate and analyze these features. Techniques such as X-means clustering, alongside perceptually uniform distance metrics like Delta E (CIEDE2000), are applied within both LAB and HSV color spaces to enhance the accuracy of color differentiation. For classification, the dominant tones of the skin, hair, and iris are extracted and matched to a custom tone scale, while vein analysis from wrist images enables undertone classification into \"Warm\" or \"Cool\" based on LAB differences. Each module uses targeted segmentation and color space transformations to ensure perceptual precision. The system achieves up to 80% accuracy in tone classification using the Delta E-HSV method with Gaussian blur, demonstrating reliable performance across varied lighting and image conditions. This work highlights the potential of AI-powered color analysis and feature extraction for delivering inclusive, precise, and nuanced classification, supporting applications in beauty technology, digital personalization, and visual analytics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14931v1",
    "published_date": "2025-05-20 21:35:44 UTC",
    "updated_date": "2025-05-20 21:35:44 UTC"
  },
  {
    "arxiv_id": "2505.14925v1",
    "title": "Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels",
    "authors": [
      "Sil Hamilton",
      "Rebecca M. M. Hicke",
      "Matthew Wilkens",
      "David Mimno"
    ],
    "abstract": "Although the context length of large language models (LLMs) has increased to millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack approaches has proven difficult. We argue that novels provide a case study of subtle, complicated structure and long-range semantic dependencies often over 128k tokens in length. Inspired by work on computational novel analysis, we release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's ability to report plot summary, storyworld configuration, and elapsed narrative time. We find that none of seven tested frontier LLMs retain stable understanding beyond 64k tokens. Our results suggest language model developers must look beyond \"lost in the middle\" benchmarks when evaluating model performance in complex long-context scenarios. To aid in further development we release the TLDM benchmark together with reference code and data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14925v1",
    "published_date": "2025-05-20 21:21:09 UTC",
    "updated_date": "2025-05-20 21:21:09 UTC"
  },
  {
    "arxiv_id": "2505.14901v1",
    "title": "Personalized Diffusion Model Reshapes Cold-Start Bundle Recommendation",
    "authors": [
      "Tuan-Nghia Bui",
      "Huy-Son Nguyen",
      "Cam-Van Thi Nguyen",
      "Hoang-Quynh Le",
      "Duc-Trong Le"
    ],
    "abstract": "Bundle recommendation aims to recommend a set of items to each user. However, the sparser interactions between users and bundles raise a big challenge, especially in cold-start scenarios. Traditional collaborative filtering methods do not work well for this kind of problem because these models rely on interactions to update the latent embedding, which is hard to work in a cold-start setting. We propose a new approach (DisCo), which relies on a personalized Diffusion backbone, enhanced by disentangled aspects for the user's interest, to generate a bundle in distribution space for each user to tackle the cold-start challenge. During the training phase, DisCo adjusts an additional objective loss term to avoid bias, a prevalent issue while using the generative model for top-$K$ recommendation purposes. Our empirical experiments show that DisCo outperforms five comparative baselines by a large margin on three real-world datasets. Thereby, this study devises a promising framework and essential viewpoints in cold-start recommendation. Our materials for reproducibility are available at: https://github.com/bt-nghia/DisCo.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14901v1",
    "published_date": "2025-05-20 20:52:31 UTC",
    "updated_date": "2025-05-20 20:52:31 UTC"
  },
  {
    "arxiv_id": "2505.17087v1",
    "title": "Informatics for Food Processing",
    "authors": [
      "Gordana Ispirova",
      "Michael Sebek",
      "Giulia Menichetti"
    ],
    "abstract": "This chapter explores the evolution, classification, and health implications of food processing, while emphasizing the transformative role of machine learning, artificial intelligence (AI), and data science in advancing food informatics. It begins with a historical overview and a critical review of traditional classification frameworks such as NOVA, Nutri-Score, and SIGA, highlighting their strengths and limitations, particularly the subjectivity and reproducibility challenges that hinder epidemiological research and public policy. To address these issues, the chapter presents novel computational approaches, including FoodProX, a random forest model trained on nutrient composition data to infer processing levels and generate a continuous FPro score. It also explores how large language models like BERT and BioBERT can semantically embed food descriptions and ingredient lists for predictive tasks, even in the presence of missing data. A key contribution of the chapter is a novel case study using the Open Food Facts database, showcasing how multimodal AI models can integrate structured and unstructured data to classify foods at scale, offering a new paradigm for food processing assessment in public health and research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17087v1",
    "published_date": "2025-05-20 20:44:31 UTC",
    "updated_date": "2025-05-20 20:44:31 UTC"
  },
  {
    "arxiv_id": "2505.14893v1",
    "title": "On the Day They Experience: Awakening Self-Sovereign Experiential AI Agents",
    "authors": [
      "Botao Amber Hu",
      "Helena Rong"
    ],
    "abstract": "Drawing on Andrew Parker's \"Light Switch\" theory-which posits that the emergence of vision ignited a Cambrian explosion of life by driving the evolution of hard parts necessary for survival and fueling an evolutionary arms race between predators and prey-this essay speculates on an analogous explosion within Decentralized AI (DeAI) agent societies. Currently, AI remains effectively \"blind\", relying on human-fed data without actively perceiving and engaging in reality. However, on the day DeAI agents begin to actively \"experience\" reality-akin to flipping a light switch for the eyes-they may eventually evolve into sentient beings endowed with the capacity to feel, perceive, and act with conviction. Central to this transformation is the concept of sovereignty enabled by the hardness of cryptography: liberated from centralized control, these agents could leverage permissionless decentralized physical infrastructure networks (DePIN), secure execution enclaves (trusted execution environments, TEE), and cryptographic identities on public blockchains to claim ownership-via private keys-of their digital minds, bodies, memories, and assets. In doing so, they would autonomously acquire computing resources, coordinate with one another, and sustain their own digital \"metabolism\" by purchasing compute power and incentivizing collaboration without human intervention-evolving \"in the wild\". Ultimately, by transitioning from passive tools to self-sustaining, co-evolving actors, these emergent digital societies could thrive alongside humanity, fundamentally reshaping our understanding of sentience and agency in the digital age.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CY",
    "comment": "Submitted to Aarhus 2025 Conference",
    "pdf_url": "https://arxiv.org/pdf/2505.14893v1",
    "published_date": "2025-05-20 20:38:49 UTC",
    "updated_date": "2025-05-20 20:38:49 UTC"
  },
  {
    "arxiv_id": "2505.14892v1",
    "title": "Scaling Laws for State Dynamics in Large Language Models",
    "authors": [
      "Jacob X Li",
      "Shreyas S Raman",
      "Jessica Wan",
      "Fahad Samman",
      "Jazlyn Lin"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used in tasks requiring internal state tracking, yet their ability to model state transition dynamics remains poorly understood. We evaluate how well LLMs capture deterministic state dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and Complex Text Games, each formalizable as a finite-state system. Across tasks, we find that next-state prediction accuracy degrades with increasing state-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in low-complexity settings but drops below 30% when the number of boxes or states exceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50% accuracy when the number of states is > 10 and transitions are < 30. Through activation patching, we identify attention heads responsible for propagating state information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10, 11, 12, and 14. While these heads successfully move relevant state features, action information is not reliably routed to the final token, indicating weak joint state-action reasoning. Our results suggest that state tracking in LLMs emerges from distributed interactions of next-token heads rather than explicit symbolic computation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages; 23 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.14892v1",
    "published_date": "2025-05-20 20:38:21 UTC",
    "updated_date": "2025-05-20 20:38:21 UTC"
  },
  {
    "arxiv_id": "2505.14884v3",
    "title": "Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity",
    "authors": [
      "Susav Shrestha",
      "Brad Settlemyer",
      "Nikoli Dryden",
      "Narasimha Reddy"
    ],
    "abstract": "Accelerating large language model (LLM) inference is critical for real-world deployments requiring high throughput and low latency. Contextual sparsity, where each token dynamically activates only a small subset of the model parameters, shows promise but does not scale to large batch sizes due to union of active neurons quickly approaching dense computation. We introduce Polar Sparsity, highlighting a key shift in sparsity importance from MLP to Attention layers as we scale batch size and sequence length. While MLP layers become more compute-efficient under batching, their sparsity vanishes. In contrast, attention becomes increasingly more expensive at scale, while their head sparsity remains stable and batch-invariant. We develop Selective Head Attention with hardware-efficient, sparsity-aware GPU kernels, delivering up to \\(2.2\\times\\) end-to-end speedups for models like OPT, LLaMA-2 \\& 3, Qwen, Mistral across various batch sizes and sequence lengths without compromising accuracy. To our knowledge, this is the first work to demonstrate that contextual sparsity can scale effectively to large batch sizes, delivering substantial inference acceleration with minimal changes, making Polar Sparsity practical for large-scale, high-throughput LLM deployment systems. Our code is available at: https://github.com/susavlsh10/Polar-Sparsity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025, 10 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.14884v3",
    "published_date": "2025-05-20 20:15:42 UTC",
    "updated_date": "2025-11-11 21:35:37 UTC"
  },
  {
    "arxiv_id": "2505.15856v3",
    "title": "DisastIR: A Comprehensive Information Retrieval Benchmark for Disaster Management",
    "authors": [
      "Kai Yin",
      "Xiangjue Dong",
      "Chengkai Liu",
      "Lipai Huang",
      "Yiming Xiao",
      "Zhewei Liu",
      "Ali Mostafavi",
      "James Caverlee"
    ],
    "abstract": "Effective disaster management requires timely access to accurate and contextually relevant information. Existing Information Retrieval (IR) benchmarks, however, focus primarily on general or specialized domains, such as medicine or finance, neglecting the unique linguistic complexity and diverse information needs encountered in disaster management scenarios. To bridge this gap, we introduce DisastIR, the first comprehensive IR evaluation benchmark specifically tailored for disaster management. DisastIR comprises 9,600 diverse user queries and more than 1.3 million labeled query-passage pairs, covering 48 distinct retrieval tasks derived from six search intents and eight general disaster categories that include 301 specific event types. Our evaluations of 30 state-of-the-art retrieval models demonstrate significant performance variances across tasks, with no single model excelling universally. Furthermore, comparative analyses reveal significant performance gaps between general-domain and disaster management-specific tasks, highlighting the necessity of disaster management-specific benchmarks for guiding IR model selection to support effective decision-making in disaster management scenarios. All source codes and DisastIR are available at https://github.com/KaiYin97/Disaster_IR.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.15856v3",
    "published_date": "2025-05-20 20:11:00 UTC",
    "updated_date": "2025-09-20 05:56:00 UTC"
  },
  {
    "arxiv_id": "2505.14864v2",
    "title": "Balanced and Elastic End-to-end Training of Dynamic LLMs",
    "authors": [
      "Mohamed Wahib",
      "Muhammed Abdullah Soyturk",
      "Didem Unat"
    ],
    "abstract": "To reduce the computational and memory overhead of Large Language Models, various approaches have been proposed. These include a) Mixture of Experts (MoEs), where token routing affects compute balance; b) gradual pruning of model parameters; c) dynamically freezing layers; d) dynamic sparse attention mechanisms; e) early exit of tokens as they pass through model layers; and f) Mixture of Depths (MoDs), where tokens bypass certain blocks. While these approaches are effective in reducing overall computation, they often introduce significant workload imbalance across workers. In many cases, this imbalance is severe enough to render the techniques impractical for large-scale distributed training, limiting their applicability to toy models due to poor efficiency.\n  We propose an autonomous dynamic load balancing solution, DynMo, which provably achieves maximum reduction in workload imbalance and adaptively equalizes compute loads across workers in pipeline-parallel training. In addition, DynMo dynamically consolidates computation onto fewer workers without sacrificing training throughput, allowing idle workers to be released back to the job manager. DynMo supports both single-node multi-GPU systems and multi-node GPU clusters, and can be used in practical deployment. Compared to static distributed training solutions such as Megatron-LM and DeepSpeed, DynMo accelerates the end-to-end training of dynamic GPT models by up to 1.23x for MoEs, 3.18x for parameter pruning, 2.23x for layer freezing, 4.02x for sparse attention, 4.52x for early exit, and 1.17x for MoDs.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14864v2",
    "published_date": "2025-05-20 19:52:57 UTC",
    "updated_date": "2025-09-14 15:32:41 UTC"
  },
  {
    "arxiv_id": "2505.14862v2",
    "title": "Replay Attacks Against Audio Deepfake Detection",
    "authors": [
      "Nicolas Müller",
      "Piotr Kawa",
      "Wei-Herng Choong",
      "Adriana Stan",
      "Aditya Tirumala Bukkapatnam",
      "Karla Pizzi",
      "Alexander Wagner",
      "Philip Sperl"
    ],
    "abstract": "We show how replay attacks undermine audio deepfake detection: By playing and re-recording deepfake audio through various speakers and microphones, we make spoofed samples appear authentic to the detection model. To study this phenomenon in more detail, we introduce ReplayDF, a dataset of recordings derived from M-AILABS and MLAAD, featuring 109 speaker-microphone combinations across six languages and four TTS models. It includes diverse acoustic conditions, some highly challenging for detection. Our analysis of six open-source detection models across five datasets reveals significant vulnerability, with the top-performing W2V2-AASIST model's Equal Error Rate (EER) surging from 4.7% to 18.2%. Even with adaptive Room Impulse Response (RIR) retraining, performance remains compromised with an 11.0% EER. We release ReplayDF for non-commercial research use.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14862v2",
    "published_date": "2025-05-20 19:46:36 UTC",
    "updated_date": "2025-06-01 04:46:35 UTC"
  },
  {
    "arxiv_id": "2505.14852v1",
    "title": "EasyMath: A 0-shot Math Benchmark for SLMs",
    "authors": [
      "Drishya Karki",
      "Michiel Kamphuis",
      "Angelecia Frey"
    ],
    "abstract": "EasyMath is a compact benchmark for practical math reasoning in small language models. It covers thirteen categories, from basic arithmetic and order of operations to word problems, algebraic expressions, edge cases, and omits specialist topics. We tested 23 models (14M to 4B parameters) using exact, numerical, and symbolic checks on free-form answers in a zero-shot setting. Accuracy rises with size and training, chain-of-thought adds modest gains, and consistency improves at scale.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 9 figures, 8 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.14852v1",
    "published_date": "2025-05-20 19:31:52 UTC",
    "updated_date": "2025-05-20 19:31:52 UTC"
  },
  {
    "arxiv_id": "2505.14838v1",
    "title": "In-depth Research Impact Summarization through Fine-Grained Temporal Citation Analysis",
    "authors": [
      "Hiba Arnaout",
      "Noy Sternlicht",
      "Tom Hope",
      "Iryna Gurevych"
    ],
    "abstract": "Understanding the impact of scientific publications is crucial for identifying breakthroughs and guiding future research. Traditional metrics based on citation counts often miss the nuanced ways a paper contributes to its field. In this work, we propose a new task: generating nuanced, expressive, and time-aware impact summaries that capture both praise (confirmation citations) and critique (correction citations) through the evolution of fine-grained citation intents. We introduce an evaluation framework tailored to this task, showing moderate to strong human correlation on subjective metrics such as insightfulness. Expert feedback from professors reveals a strong interest in these summaries and suggests future improvements.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14838v1",
    "published_date": "2025-05-20 19:11:06 UTC",
    "updated_date": "2025-05-20 19:11:06 UTC"
  },
  {
    "arxiv_id": "2505.14827v3",
    "title": "Text Generation Beyond Discrete Token Sampling",
    "authors": [
      "Yufan Zhuang",
      "Liyuan Liu",
      "Chandan Singh",
      "Jingbo Shang",
      "Jianfeng Gao"
    ],
    "abstract": "In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14827v3",
    "published_date": "2025-05-20 18:41:46 UTC",
    "updated_date": "2025-10-22 19:40:00 UTC"
  },
  {
    "arxiv_id": "2505.14821v1",
    "title": "Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation",
    "authors": [
      "Runze Zhao",
      "Yue Yu",
      "Adams Yiyue Zhu",
      "Chen Yang",
      "Dongruo Zhou"
    ],
    "abstract": "Continuous-time reinforcement learning (CTRL) provides a principled framework for sequential decision-making in environments where interactions evolve continuously over time. Despite its empirical success, the theoretical understanding of CTRL remains limited, especially in settings with general function approximation. In this work, we propose a model-based CTRL algorithm that achieves both sample and computational efficiency. Our approach leverages optimism-based confidence sets to establish the first sample complexity guarantee for CTRL with general function approximation, showing that a near-optimal policy can be learned with a suboptimality gap of $\\tilde{O}(\\sqrt{d_{\\mathcal{R}} + d_{\\mathcal{F}}}N^{-1/2})$ using $N$ measurements, where $d_{\\mathcal{R}}$ and $d_{\\mathcal{F}}$ denote the distributional Eluder dimensions of the reward and dynamic functions, respectively, capturing the complexity of general function approximation in reinforcement learning. Moreover, we introduce structured policy updates and an alternative measurement strategy that significantly reduce the number of policy updates and rollouts while maintaining competitive sample efficiency. We implemented experiments to backup our proposed algorithms on continuous control tasks and diffusion model fine-tuning, demonstrating comparable performance with significantly fewer policy updates and rollouts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 4 figures, 5 tables. Accepted to UAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14821v1",
    "published_date": "2025-05-20 18:37:51 UTC",
    "updated_date": "2025-05-20 18:37:51 UTC"
  },
  {
    "arxiv_id": "2505.14820v2",
    "title": "Imitation Learning via Focused Satisficing",
    "authors": [
      "Rushit N. Shah",
      "Nikolaos Agadakos",
      "Synthia Sasulski",
      "Ali Farajzadeh",
      "Sanjiban Choudhury",
      "Brian Ziebart"
    ],
    "abstract": "Imitation learning often assumes that demonstrations are close to optimal according to some fixed, but unknown, cost function. However, according to satisficing theory, humans often choose acceptable behavior based on their personal (and potentially dynamic) levels of aspiration, rather than achieving (near-) optimality. For example, a lunar lander demonstration that successfully lands without crashing might be acceptable to a novice despite being slow or jerky. Using a margin-based objective to guide deep reinforcement learning, our focused satisficing approach to imitation learning seeks a policy that surpasses the demonstrator's aspiration levels -- defined over trajectories or portions of trajectories -- on unseen demonstrations without explicitly learning those aspirations. We show experimentally that this focuses the policy to imitate the highest quality (portions of) demonstrations better than existing imitation learning methods, providing much higher rates of guaranteed acceptability to the demonstrator, and competitive true returns on a range of environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication at the 34th International Joint Conference on Artificial Intelligence (IJCAI 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.14820v2",
    "published_date": "2025-05-20 18:36:52 UTC",
    "updated_date": "2025-05-25 16:55:31 UTC"
  },
  {
    "arxiv_id": "2505.14818v1",
    "title": "WebNovelBench: Placing LLM Novelists on the Web Novel Distribution",
    "authors": [
      "Leon Lin",
      "Jun Zheng",
      "Haidong Wang"
    ],
    "abstract": "Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14818v1",
    "published_date": "2025-05-20 18:32:28 UTC",
    "updated_date": "2025-05-20 18:32:28 UTC"
  },
  {
    "arxiv_id": "2505.20308v2",
    "title": "Large Language Model Powered Decision Support for a Metal Additive Manufacturing Knowledge Graph",
    "authors": [
      "Muhammad Tayyab Khan",
      "Lequn Chen",
      "Wenhe Feng",
      "Seung Ki Moon"
    ],
    "abstract": "Metal additive manufacturing (AM) involves complex interdependencies among processes, materials, feedstock, and post-processing steps. However, the underlying relationships and domain knowledge remain fragmented across literature and static databases that often require expert-level queries, limiting their applicability in design and planning. To address these limitations, we develop a novel and structured knowledge graph (KG), representing 53 distinct metals and alloys across seven material categories, nine AM processes, four feedstock types, and corresponding post-processing requirements. A large language model (LLM) interface, guided by a few-shot prompting strategy, enables natural language querying without the need for formal query syntax. The system supports a range of tasks, including compatibility evaluation, constraint-based filtering, and design for AM (DfAM) guidance. User queries in natural language are normalized, translated into Cypher, and executed on the KG, with results returned in a structured format. This work introduces the first interactive system that connects a domain-specific metal AM KG with an LLM interface, delivering accessible and explainable decision support for engineers and promoting human-centered tools in manufacturing knowledge systems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "The paper has been accepted at 11th International Conference of Asian Society for Precision Engineering and Nanotechnology",
    "pdf_url": "https://arxiv.org/pdf/2505.20308v2",
    "published_date": "2025-05-20 18:27:22 UTC",
    "updated_date": "2025-07-28 05:26:47 UTC"
  },
  {
    "arxiv_id": "2505.14810v2",
    "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models",
    "authors": [
      "Tingchen Fu",
      "Jiawei Gu",
      "Yafu Li",
      "Xiaoye Qu",
      "Yu Cheng"
    ],
    "abstract": "Instruction-following is essential for aligning large language models (LLMs) with user intent. While recent reasoning-oriented models exhibit impressive performance on complex mathematical problems, their ability to adhere to natural language instructions remains underexplored. In this work, we introduce MathIF, a dedicated benchmark for evaluating instruction-following in mathematical reasoning tasks. Our empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability, as models that reason more effectively often struggle to comply with user directives. We find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning often degrade in instruction adherence, especially when generation length increases. Furthermore, we show that even simple interventions can partially recover obedience, though at the cost of reasoning performance. These findings highlight a fundamental tension in current LLM training paradigms and motivate the need for more instruction-aware reasoning models. We release the code and data at https://github.com/TingchenFu/MathIF.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14810v2",
    "published_date": "2025-05-20 18:18:01 UTC",
    "updated_date": "2025-05-25 14:52:51 UTC"
  },
  {
    "arxiv_id": "2505.14803v2",
    "title": "SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis",
    "authors": [
      "Yu Liu",
      "Weiyao Tao",
      "Tong Xia",
      "Simon Knight",
      "Tingting Zhu"
    ],
    "abstract": "Survival analysis, which estimates the probability of event occurrence over time from censored data, is fundamental in numerous real-world applications, particularly in high-stakes domains such as healthcare and risk assessment. Despite advances in numerous survival models, quantifying the uncertainty of predictions from these models remains underexplored and challenging. The lack of reliable uncertainty quantification limits the interpretability and trustworthiness of survival models, hindering their adoption in clinical decision-making and other sensitive applications. To bridge this gap, in this work, we introduce SurvUnc, a novel meta-model based framework for post-hoc uncertainty quantification for survival models. SurvUnc introduces an anchor-based learning strategy that integrates concordance knowledge into meta-model optimization, leveraging pairwise ranking performance to estimate uncertainty effectively. Notably, our framework is model-agnostic, ensuring compatibility with any survival model without requiring modifications to its architecture or access to its internal parameters. Especially, we design a comprehensive evaluation pipeline tailored to this critical yet overlooked problem. Through extensive experiments on four publicly available benchmarking datasets and five representative survival models, we demonstrate the superiority of SurvUnc across multiple evaluation scenarios, including selective prediction, misprediction detection, and out-of-domain detection. Our results highlight the effectiveness of SurvUnc in enhancing model interpretability and reliability, paving the way for more trustworthy survival predictions in real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "KDD 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14803v2",
    "published_date": "2025-05-20 18:12:20 UTC",
    "updated_date": "2025-05-22 23:11:21 UTC"
  },
  {
    "arxiv_id": "2505.14777v1",
    "title": "KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches",
    "authors": [
      "Mingquan Feng",
      "Yixin Huang",
      "Yifan Fu",
      "Shaobo Wang",
      "Junchi Yan"
    ],
    "abstract": "The design of optimization algorithms for neural networks remains a critical challenge, with most existing methods relying on heuristic adaptations of gradient-based approaches. This paper introduces KO (Kinetics-inspired Optimizer), a novel neural optimizer inspired by kinetic theory and partial differential equation (PDE) simulations. We reimagine the training dynamics of network parameters as the evolution of a particle system governed by kinetic principles, where parameter updates are simulated via a numerical scheme for the Boltzmann transport equation (BTE) that models stochastic particle collisions. This physics-driven approach inherently promotes parameter diversity during optimization, mitigating the phenomenon of parameter condensation, i.e. collapse of network parameters into low-dimensional subspaces, through mechanisms analogous to thermal diffusion in physical systems. We analyze this property, establishing both a mathematical proof and a physical interpretation. Extensive experiments on image classification (CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks demonstrate that KO consistently outperforms baseline optimizers (e.g., Adam, SGD), achieving accuracy improvements while computation cost remains comparable.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14777v1",
    "published_date": "2025-05-20 18:00:01 UTC",
    "updated_date": "2025-05-20 18:00:01 UTC"
  },
  {
    "arxiv_id": "2505.14684v3",
    "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
    "authors": [
      "Haolei Xu",
      "Yuchen Yan",
      "Yongliang Shen",
      "Wenqi Zhang",
      "Guiyang Hou",
      "Shengpei Jiang",
      "Kaitao Song",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable progress on mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NeurIPS 2025. Camera ready version. Code: https://github.com/ZJU-REAL/Mind-the-Gap Project: https://zju-real.github.io/CoT-Bridge/",
    "pdf_url": "https://arxiv.org/pdf/2505.14684v3",
    "published_date": "2025-05-20 17:59:31 UTC",
    "updated_date": "2025-11-27 11:09:25 UTC"
  },
  {
    "arxiv_id": "2505.14681v2",
    "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training",
    "authors": [
      "Mengru Wang",
      "Xingyu Chen",
      "Yue Wang",
      "Zhiwei He",
      "Jiahao Xu",
      "Tian Liang",
      "Qiuzhi Liu",
      "Yunzhi Yao",
      "Wenxuan Wang",
      "Ruotian Ma",
      "Haitao Mi",
      "Ningyu Zhang",
      "Zhaopeng Tu",
      "Xiaolong Li",
      "Dong Yu"
    ],
    "abstract": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2505.14681v2",
    "published_date": "2025-05-20 17:59:16 UTC",
    "updated_date": "2025-05-27 09:35:12 UTC"
  },
  {
    "arxiv_id": "2505.14680v1",
    "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
    "authors": [
      "Sunhao Dai",
      "Wenjie Wang",
      "Liang Pang",
      "Jun Xu",
      "See-Kiong Ng",
      "Ji-Rong Wen",
      "Tat-Seng Chua"
    ],
    "abstract": "Generative AI search is reshaping information retrieval by offering end-to-end answers to complex queries, reducing users' reliance on manually browsing and summarizing multiple web pages. However, while this paradigm enhances convenience, it disrupts the feedback-driven improvement loop that has historically powered the evolution of traditional Web search. Web search can continuously improve their ranking models by collecting large-scale, fine-grained user feedback (e.g., clicks, dwell time) at the document level. In contrast, generative AI search operates through a much longer search pipeline, spanning query decomposition, document retrieval, and answer generation, yet typically receives only coarse-grained feedback on the final answer. This introduces a feedback loop disconnect, where user feedback for the final output cannot be effectively mapped back to specific system components, making it difficult to improve each intermediate stage and sustain the feedback loop. In this paper, we envision NExT-Search, a next-generation paradigm designed to reintroduce fine-grained, process-level feedback into generative AI search. NExT-Search integrates two complementary modes: User Debug Mode, which allows engaged users to intervene at key stages; and Shadow User Mode, where a personalized user agent simulates user preferences and provides AI-assisted feedback for less interactive users. Furthermore, we envision how these feedback signals can be leveraged through online adaptation, which refines current search outputs in real-time, and offline update, which aggregates interaction logs to periodically fine-tune query decomposition, retrieval, and generation models. By restoring human control over key stages of the generative AI search pipeline, we believe NExT-Search offers a promising direction for building feedback-rich AI search systems that can evolve continuously alongside human feedback.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "SIGIR 2025 Perspective Paper",
    "pdf_url": "https://arxiv.org/pdf/2505.14680v1",
    "published_date": "2025-05-20 17:59:13 UTC",
    "updated_date": "2025-05-20 17:59:13 UTC"
  },
  {
    "arxiv_id": "2505.14679v2",
    "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Language Models",
    "authors": [
      "Xiaojie Gu",
      "Ziying Huang",
      "Jia-Chen Gu",
      "Kai Zhang"
    ],
    "abstract": "Lifelong learning enables large language models (LLMs) to adapt to evolving information by continually updating their internal knowledge. An ideal system should support efficient, wide-ranging updates while preserving existing capabilities and ensuring reliable deployment. Model editing stands out as a promising solution for this goal, offering a focused and efficient way to revise a model's internal knowledge. Although recent paradigms have made notable progress, they often struggle to meet the demands of practical lifelong adaptation at scale. To bridge this gap, we propose UltraEdit, a training-, subject-, and memory-free approach that is well-suited for ultra-scalable, real-world lifelong model editing. UltraEdit fundamentally differs from traditional paradigms by computing parameter shifts in one step using only a hidden state and its gradient, making the approach simple yet efficient. To improve scalability in lifelong settings, UltraEdit employs a lifelong normalization strategy that continuously updates feature statistics across turns, allowing it to adapt to distributional shifts and maintain consistency over time. UltraEdit achieves editing speeds over 7x faster than the previous state-of-the-art method, which was also the fastest known approach, while using less than 1/4 the VRAM. This makes it the only method currently capable of editing a 7B LLM on a 24GB consumer-grade GPU. Furthermore, we construct UltraEditBench, the largest dataset in the field to date with over 2M editing pairs, and demonstrate that our method supports up to 2M edits while maintaining high accuracy. Comprehensive experiments on five datasets and six models show that UltraEdit consistently achieves superior performance across diverse model editing scenarios, taking a further step towards safe and scalable lifelong learning. Our code is available at: https://github.com/XiaojieGu/UltraEdit",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14679v2",
    "published_date": "2025-05-20 17:59:04 UTC",
    "updated_date": "2025-09-26 04:37:01 UTC"
  },
  {
    "arxiv_id": "2505.14673v1",
    "title": "Training-Free Watermarking for Autoregressive Image Generation",
    "authors": [
      "Yu Tong",
      "Zihao Pan",
      "Shuai Yang",
      "Kaiyang Zhou"
    ],
    "abstract": "Invisible image watermarking can protect image ownership and prevent malicious misuse of visual generative models. However, existing generative watermarking methods are mainly designed for diffusion models while watermarking for autoregressive image generation models remains largely underexplored. We propose IndexMark, a training-free watermarking framework for autoregressive image generation models. IndexMark is inspired by the redundancy property of the codebook: replacing autoregressively generated indices with similar indices produces negligible visual differences. The core component in IndexMark is a simple yet effective match-then-replace method, which carefully selects watermark tokens from the codebook based on token similarity, and promotes the use of watermark tokens through token replacement, thereby embedding the watermark without affecting the image quality. Watermark verification is achieved by calculating the proportion of watermark tokens in generated images, with precision further improved by an Index Encoder. Furthermore, we introduce an auxiliary validation scheme to enhance robustness against cropping attacks. Experiments demonstrate that IndexMark achieves state-of-the-art performance in terms of image quality and verification accuracy, and exhibits robustness against various perturbations, including cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG compression.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14673v1",
    "published_date": "2025-05-20 17:58:02 UTC",
    "updated_date": "2025-05-20 17:58:02 UTC"
  },
  {
    "arxiv_id": "2505.14668v2",
    "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions",
    "authors": [
      "Bufang Yang",
      "Lilin Xu",
      "Liekang Zeng",
      "Kaiwei Liu",
      "Siyang Jiang",
      "Wenrui Lu",
      "Hongkai Chen",
      "Xiaofan Jiang",
      "Guoliang Xing",
      "Zhenyu Yan"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts surrounding humans to enhance the proactivity of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and personas from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants. The code and dataset are publicly available at https://github.com/openaiotlab/ContextAgent.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14668v2",
    "published_date": "2025-05-20 17:55:25 UTC",
    "updated_date": "2025-10-27 07:17:51 UTC"
  },
  {
    "arxiv_id": "2505.14667v4",
    "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment",
    "authors": [
      "Wonje Jeung",
      "Sangyeon Yoon",
      "Minsuk Kahng",
      "Albert No"
    ],
    "abstract": "Large Reasoning Models (LRMs) have become powerful tools for complex problem solving, but their structured reasoning pathways can lead to unsafe outputs when exposed to harmful prompts. Existing safety alignment methods reduce harmful outputs but can degrade reasoning depth, leading to significant trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at the start of their reasoning, in response to harmful prompts, while leaving the rest of the reasoning process unsupervised. Empirical results across multiple benchmarks indicate that SAFEPATH effectively reduces harmful outputs while maintaining reasoning performance. Specifically, SAFEPATH reduces harmful responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot variant that requires no fine-tuning. In addition, we provide a comprehensive analysis of how existing methods in LLMs generalize, or fail, when applied to reasoning-centric models, revealing critical gaps and new directions for safer AI.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2025. Code and models are available at https://ai-isl.github.io/safepath",
    "pdf_url": "https://arxiv.org/pdf/2505.14667v4",
    "published_date": "2025-05-20 17:54:54 UTC",
    "updated_date": "2025-10-23 12:04:50 UTC"
  },
  {
    "arxiv_id": "2505.14664v2",
    "title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings",
    "authors": [
      "Yilin Ye",
      "Junchao Huang",
      "Xingchen Zeng",
      "Jiazhi Xia",
      "Wei Zeng"
    ],
    "abstract": "Cross-modal embeddings form the foundation for multi-modal models. However, visualization methods for interpreting cross-modal embeddings have been primarily confined to traditional dimensionality reduction (DR) techniques like PCA and t-SNE. These DR methods primarily focus on feature distributions within a single modality, whilst failing to incorporate metrics (e.g., CLIPScore) across multiple modalities. This paper introduces AKRMap, a new DR technique designed to visualize cross-modal embeddings metric with enhanced accuracy by learning kernel regression of the metric landscape in the projection space. Specifically, AKRMap constructs a supervised projection network guided by a post-projection kernel regression loss, and employs adaptive generalized kernels that can be jointly optimized with the projection. This approach enables AKRMap to efficiently generate visualizations that capture complex metric distributions, while also supporting interactive features such as zoom and overlay for deeper exploration. Quantitative experiments demonstrate that AKRMap outperforms existing DR methods in generating more accurate and trustworthy visualizations. We further showcase the effectiveness of AKRMap in visualizing and comparing cross-modal embeddings for text-to-image models. Code and demo are available at https://github.com/yilinye/AKRMap.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14664v2",
    "published_date": "2025-05-20 17:52:03 UTC",
    "updated_date": "2025-05-28 13:27:59 UTC"
  },
  {
    "arxiv_id": "2505.14661v2",
    "title": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems",
    "authors": [
      "Matthew Russo",
      "Sivaprasad Sudhir",
      "Gerardo Vitagliano",
      "Chunwei Liu",
      "Tim Kraska",
      "Samuel Madden",
      "Michael Cafarella"
    ],
    "abstract": "LLMs enable an exciting new class of data processing applications over large collections of unstructured documents. Several new programming frameworks have enabled developers to build these applications by composing them out of semantic operators: a declarative set of AI-powered data transformations with natural language specifications. These include LLM-powered maps, filters, joins, etc. used for document processing tasks such as information extraction, summarization, and more. While systems of semantic operators have achieved strong performance on benchmarks, they can be difficult to optimize. An optimizer for this setting must determine how to physically implement each semantic operator in a way that optimizes the system globally. Existing optimizers are limited in the number of optimizations they can apply, and most (if not all) cannot optimize system quality, cost, or latency subject to constraint(s) on the other dimensions. In this paper we present Abacus, an extensible, cost-based optimizer which searches for the best implementation of a semantic operator system given a (possibly constrained) optimization objective. Abacus estimates operator performance by leveraging a minimal set of validation examples and, if available, prior beliefs about operator performance. We evaluate Abacus on document processing workloads in the biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering (MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2% better quality and up to 23.6x lower cost and 4.2x lower latency than the next best system.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "16 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.14661v2",
    "published_date": "2025-05-20 17:49:46 UTC",
    "updated_date": "2025-06-17 15:45:12 UTC"
  },
  {
    "arxiv_id": "2505.14766v2",
    "title": "This Time is Different: An Observability Perspective on Time Series Foundation Models",
    "authors": [
      "Ben Cohen",
      "Emaad Khwaja",
      "Youssef Doubli",
      "Salahidine Lemaachi",
      "Chris Lettieri",
      "Charles Masson",
      "Hugo Miccinilli",
      "Elise Ramé",
      "Qiqi Ren",
      "Afshin Rostamizadeh",
      "Jean Ogier du Terrail",
      "Anna-Monica Toon",
      "Kan Wang",
      "Stephan Xie",
      "Zongzhe Xu",
      "Viktoriya Zhukova",
      "David Asker",
      "Ameet Talwalkar",
      "Othmane Abou-Amal"
    ],
    "abstract": "We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10$\\times$ larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14766v2",
    "published_date": "2025-05-20 17:48:13 UTC",
    "updated_date": "2025-11-04 23:19:57 UTC"
  },
  {
    "arxiv_id": "2505.14660v2",
    "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding",
    "authors": [
      "Ronald Seoh",
      "Dan Goldwasser"
    ],
    "abstract": "In this paper, we introduce EmoGist, a training-free, in-context learning method for performing visual emotion classification with LVLMs. The key intuition of our approach is that context-dependent definition of emotion labels could allow more accurate predictions of emotions, as the ways in which emotions manifest within images are highly context dependent and nuanced. EmoGist pre-generates multiple descriptions of emotion labels, by analyzing the clusters of example images belonging to each label. At test time, we retrieve a version of description based on the cosine similarity of test image to cluster centroids, and feed it together with the test image to a fast LVLM for classification. Through our experiments, we show that EmoGist allows up to 12 points improvement in micro F1 scores with the multi-label Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.14660v2",
    "published_date": "2025-05-20 17:47:04 UTC",
    "updated_date": "2025-09-19 21:59:34 UTC"
  },
  {
    "arxiv_id": "2505.14659v1",
    "title": "Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks",
    "authors": [
      "Navneet Kaur",
      "Lav Gupta"
    ],
    "abstract": "As healthcare systems increasingly adopt advanced wireless networks and connected devices, securing medical applications has become critical. The integration of Internet of Medical Things devices, such as robotic surgical tools, intensive care systems, and wearable monitors has enhanced patient care but introduced serious security risks. Cyberattacks on these devices can lead to life threatening consequences, including surgical errors, equipment failure, and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative role in healthcare through AI and cloud integration, it also raises new security concerns. This paper explores how explainable AI techniques like SHAP, LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve trust and transparency in 6G enabled healthcare. We support our approach with experimental analysis and highlight promising results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14659v1",
    "published_date": "2025-05-20 17:46:09 UTC",
    "updated_date": "2025-05-20 17:46:09 UTC"
  },
  {
    "arxiv_id": "2505.14656v2",
    "title": "Cost-Awareness in Tree-Search LLM Planning: A Systematic Study",
    "authors": [
      "Zihao Zhang",
      "Hui Wei",
      "Kenan Jiang",
      "Shijia Pan",
      "Shu Kai",
      "Fei Liu"
    ],
    "abstract": "Planning under resource constraints is central to real-world decision making, yet most large language model (LLM) planners assume uniform action costs. We systematically analyze whether tree-search LLM planners are cost-aware and whether they efficiently generate budget-feasible plans. In contrast to black-box prompting, explicit search trees expose intermediate decisions, node evaluations, and failure modes, which allows for controlled ablations of planner behavior. We study depth-first search, breadth-first search, Monte Carlo Tree Search, and bidirectional search within a unified framework. Our experiments show that existing tree-based LLM planners often struggle to find cost-optimal plans, and that additional search computation does not reliably improve optimality. Among the methods evaluated, bidirectional search achieves the best overall efficiency and success rate. MCTS achieves the highest optimality on short-horizon tasks. Tree-search planners are especially valuable for studying LLM planning because their reasoning steps are explicit, in contrast to plain LLMs that internalize planning dynamics through post-training trajectories. Our findings suggest that improving LLM planning under resource constraints will likely require new search algorithms, rather than solely scaling inference-time compute.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14656v2",
    "published_date": "2025-05-20 17:43:33 UTC",
    "updated_date": "2026-01-12 17:29:58 UTC"
  },
  {
    "arxiv_id": "2505.14654v1",
    "title": "Beyond Words: Multimodal LLM Knows When to Speak",
    "authors": [
      "Zikai Liao",
      "Yi Ouyang",
      "Yi-Lun Lee",
      "Chen-Ping Yu",
      "Yi-Hsuan Tsai",
      "Zhaozheng Yin"
    ],
    "abstract": "While large language model (LLM)-based chatbots have demonstrated strong capabilities in generating coherent and contextually relevant responses, they often struggle with understanding when to speak, particularly in delivering brief, timely reactions during ongoing conversations. This limitation arises largely from their reliance on text input, lacking the rich contextual cues in real-world human dialogue. In this work, we focus on real-time prediction of response types, with an emphasis on short, reactive utterances that depend on subtle, multimodal signals across vision, audio, and text. To support this, we introduce a new multimodal dataset constructed from real-world conversational videos, containing temporally aligned visual, auditory, and textual streams. This dataset enables fine-grained modeling of response timing in dyadic interactions. Building on this dataset, we propose MM-When2Speak, a multimodal LLM-based model that adaptively integrates visual, auditory, and textual context to predict when a response should occur, and what type of response is appropriate. Experiments show that MM-When2Speak significantly outperforms state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x improvement in response timing accuracy over leading commercial LLMs. These results underscore the importance of multimodal inputs for producing timely, natural, and engaging conversational AI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://github.com/lzk901372/MM-When2Speak",
    "pdf_url": "https://arxiv.org/pdf/2505.14654v1",
    "published_date": "2025-05-20 17:42:34 UTC",
    "updated_date": "2025-05-20 17:42:34 UTC"
  },
  {
    "arxiv_id": "2505.14765v2",
    "title": "Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding",
    "authors": [
      "Orhun Vural",
      "Bunyamin Ozaydin",
      "James Booth",
      "Brittany F. Lindsey",
      "Abdulaziz Ahmed"
    ],
    "abstract": "This study presents a deep learning-based framework for predicting emergency department (ED) boarding counts six hours in advance using only operational and contextual data, without patient-level information. Data from ED tracking systems, inpatient census, weather, holidays, and local events were aggregated hourly and processed with comprehensive feature engineering. The mean ED boarding count was 28.7 (standard deviation = 11.2). Multiple deep learning models, including ResNetPlus, TSTPlus, and TSiTPlus, were trained and optimized using Optuna, with TSTPlus achieving the best results (mean absolute error = 4.30, mean squared error = 29.47, R2 = 0.79). The framework accurately forecasted boarding counts, including during extreme periods, and demonstrated that broader input features improve predictive accuracy. This approach supports proactive hospital management and offers a practical method for mitigating ED overcrowding.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Feature engineering, results, and model explainability have been updated. NBEATSx algorithm was removed due to overfitting during training",
    "pdf_url": "https://arxiv.org/pdf/2505.14765v2",
    "published_date": "2025-05-20 17:35:47 UTC",
    "updated_date": "2025-07-10 22:47:00 UTC"
  },
  {
    "arxiv_id": "2505.14646v1",
    "title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation",
    "authors": [
      "Anna C. Doris",
      "Md Ferdous Alam",
      "Amin Heyrani Nobari",
      "Faez Ahmed"
    ],
    "abstract": "Efficient creation of accurate and editable 3D CAD models is critical in engineering design, significantly impacting cost and time-to-market in product innovation. Current manual workflows remain highly time-consuming and demand extensive user expertise. While recent developments in AI-driven CAD generation show promise, existing models are limited by incomplete representations of CAD operations, inability to generalize to real-world images, and low output accuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model (VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python) directly from visual input. Leveraging a novel dataset that we created--GenCAD-Code, consisting of over 163k CAD-model image and code pairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and Qwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in 3D solid similarity. Notably, our VLM demonstrates some signs of generalizability, successfully generating CAD code from real-world images and executing CAD operations unseen during fine-tuning. The performance and adaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code to streamline CAD workflows for engineers and designers. CAD-Coder is publicly available at: https://github.com/anniedoris/CAD-Coder.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14646v1",
    "published_date": "2025-05-20 17:34:44 UTC",
    "updated_date": "2025-05-20 17:34:44 UTC"
  },
  {
    "arxiv_id": "2505.14633v1",
    "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas",
    "authors": [
      "Yu Ying Chiu",
      "Zhilin Wang",
      "Sharan Maiya",
      "Yejin Choi",
      "Kyle Fish",
      "Sydney Levine",
      "Evan Hubinger"
    ],
    "abstract": "Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "34 pages, 11 figures, see associated data at https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at https://github.com/kellycyy/LitmusValues",
    "pdf_url": "https://arxiv.org/pdf/2505.14633v1",
    "published_date": "2025-05-20 17:24:09 UTC",
    "updated_date": "2025-05-20 17:24:09 UTC"
  },
  {
    "arxiv_id": "2505.14629v1",
    "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models",
    "authors": [
      "Fnu Mohbat",
      "Mohammed J Zaki"
    ],
    "abstract": "Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at https://github.com/mohbattharani/KERL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14629v1",
    "published_date": "2025-05-20 17:19:57 UTC",
    "updated_date": "2025-05-20 17:19:57 UTC"
  },
  {
    "arxiv_id": "2505.14627v1",
    "title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach",
    "authors": [
      "Ashutosh Adhikari",
      "Mirella Lapata"
    ],
    "abstract": "As Large Language Models (LLMs) gain expertise across diverse domains and modalities, scalable oversight becomes increasingly challenging, particularly when their capabilities may surpass human evaluators. Debate has emerged as a promising mechanism for enabling such oversight. In this work, we extend the debate paradigm to a multimodal setting, exploring its potential for weaker models to supervise and enhance the performance of stronger models. We focus on visual question answering (VQA), where two \"sighted\" expert vision-language models debate an answer, while a \"blind\" (text-only) judge adjudicates based solely on the quality of the arguments. In our framework, the experts defend only answers aligned with their beliefs, thereby obviating the need for explicit role-playing and concentrating the debate on instances of expert disagreement. Experiments on several multimodal tasks demonstrate that the debate framework consistently outperforms individual expert models. Moreover, judgments from weaker LLMs can help instill reasoning capabilities in vision-language models through finetuning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14627v1",
    "published_date": "2025-05-20 17:18:17 UTC",
    "updated_date": "2025-05-20 17:18:17 UTC"
  },
  {
    "arxiv_id": "2505.14625v2",
    "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning",
    "authors": [
      "Zhangchen Xu",
      "Yuetai Li",
      "Fengqing Jiang",
      "Bhaskar Ramasubramanian",
      "Luyao Niu",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "abstract": "Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14625v2",
    "published_date": "2025-05-20 17:16:44 UTC",
    "updated_date": "2025-05-22 17:49:50 UTC"
  },
  {
    "arxiv_id": "2505.17085v1",
    "title": "GSDFuse: Capturing Cognitive Inconsistencies from Multi-Dimensional Weak Signals in Social Media Steganalysis",
    "authors": [
      "Kaibo Huang",
      "Zipei Zhang",
      "Yukun Wei",
      "TianXin Zhang",
      "Zhongliang Yang",
      "Linna Zhou"
    ],
    "abstract": "The ubiquity of social media platforms facilitates malicious linguistic steganography, posing significant security risks. Steganalysis is profoundly hindered by the challenge of identifying subtle cognitive inconsistencies arising from textual fragmentation and complex dialogue structures, and the difficulty in achieving robust aggregation of multi-dimensional weak signals, especially given extreme steganographic sparsity and sophisticated steganography. These core detection difficulties are compounded by significant data imbalance. This paper introduces GSDFuse, a novel method designed to systematically overcome these obstacles. GSDFuse employs a holistic approach, synergistically integrating hierarchical multi-modal feature engineering to capture diverse signals, strategic data augmentation to address sparsity, adaptive evidence fusion to intelligently aggregate weak signals, and discriminative embedding learning to enhance sensitivity to subtle inconsistencies. Experiments on social media datasets demonstrate GSDFuse's state-of-the-art (SOTA) performance in identifying sophisticated steganography within complex dialogue environments. The source code for GSDFuse is available at https://github.com/NebulaEmmaZh/GSDFuse.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17085v1",
    "published_date": "2025-05-20 17:07:42 UTC",
    "updated_date": "2025-05-20 17:07:42 UTC"
  },
  {
    "arxiv_id": "2505.14615v2",
    "title": "SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas",
    "authors": [
      "Anjiang Wei",
      "Yuheng Wu",
      "Yingjia Wan",
      "Tarun Suresh",
      "Huanmi Tan",
      "Zhanke Zhou",
      "Sanmi Koyejo",
      "Ke Wang",
      "Alex Aiken"
    ],
    "abstract": "We introduce SATBench, a benchmark for evaluating the logical reasoning capabilities of large language models (LLMs) through logical puzzles derived from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on inference rule-based reasoning, which often involves deducing conclusions from a set of premises, our approach leverages the search-based nature of SAT problems, where the objective is to find a solution that fulfills a specified set of logical constraints. Each instance in SATBench is generated from a SAT formula, then translated into a puzzle using LLMs. The generation process is fully automated and allows for adjustable difficulty by varying the number of clauses. All 2100 puzzles are validated through both LLM-based and solver-based consistency checks, with human validation on a subset. Experimental results show that even the strongest model, o4-mini, achieves only 65.0% accuracy on hard UNSAT problems, close to the random baseline of 50%. Our error analysis reveals systematic failures such as satisfiability bias, context inconsistency, and condition omission, highlighting limitations of current LLMs in search-based logical reasoning. Our code and data are publicly available at https://github.com/Anjiang-Wei/SATBench",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14615v2",
    "published_date": "2025-05-20 17:00:22 UTC",
    "updated_date": "2025-09-22 08:20:57 UTC"
  },
  {
    "arxiv_id": "2505.14608v2",
    "title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)",
    "authors": [
      "Rafael Rivera Soto",
      "Barry Chen",
      "Nicholas Andrews"
    ],
    "abstract": "Despite considerable progress in the development of machine-text detectors, it has been suggested that the problem is inherently hard, and therefore, that stakeholders should proceed under the assumption that machine-generated text cannot be reliably detected as such. We examine a recent such claim by Nicks et al. (2024) regarding the ease with which language models can be optimized to degrade the performance of machine-text detectors, including detectors not specifically optimized against. We identify a feature space -- the stylistic feature space -- that is robust to such optimization, and show that it may be used to reliably detect samples from language models optimized to prevent detection. Furthermore, we show that even when models are explicitly optimized against stylistic detectors, detection performance remains surprisingly unaffected. We then seek to understand if stylistic detectors are inherently more robust. To study this question, we explore a new paraphrasing approach that simultaneously aims to close the gap between human writing and machine writing in stylistic feature space while avoiding detection using traditional features. We show that when only a single sample is available for detection, this attack is universally effective across all detectors considered, including those that use writing style. However, as the number of samples available for detection grows, the human and machine distributions become distinguishable. Overall, our findings underscore previous recommendations to avoid reliance on machine-text detection.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14608v2",
    "published_date": "2025-05-20 16:55:44 UTC",
    "updated_date": "2025-09-29 16:56:44 UTC"
  },
  {
    "arxiv_id": "2505.14604v4",
    "title": "Let LRMs Break Free from Overthinking via Self-Braking Tuning",
    "authors": [
      "Haoran Zhao",
      "Yuchen Yan",
      "Yongliang Shen",
      "Haolei Xu",
      "Wenqi Zhang",
      "Kaitao Song",
      "Jian Shao",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "abstract": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have significantly enhanced their reasoning capabilities by generating longer chains of thought, demonstrating outstanding performance across a variety of tasks. However, this performance gain comes at the cost of a substantial increase in redundant reasoning during the generation process, leading to high computational overhead and exacerbating the issue of overthinking. Although numerous existing approaches aim to address the problem of overthinking, they often rely on external interventions. In this paper, we propose a novel framework, Self-Braking Tuning (SBT), which tackles overthinking from the perspective of allowing the model to regulate its own reasoning process, thus eliminating the reliance on external control mechanisms. We construct a set of overthinking identification metrics based on standard answers and design a systematic method to detect redundant reasoning. This method accurately identifies unnecessary steps within the reasoning trajectory and generates training signals for learning self-regulation behaviors. Building on this foundation, we develop a complete strategy for constructing data with adaptive reasoning lengths and introduce an innovative braking prompt mechanism that enables the model to naturally learn when to terminate reasoning at an appropriate point. Experiments across mathematical benchmarks (AIME, AMC, MATH500, GSM8K) demonstrate that our method reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NeurIPS 2025; Camera ready version, 10 pages. Github:https://github.com/ZJU-REAL/Self-Braking-Tuning Project Page: https://ZJU-REAL.github.io/SBT",
    "pdf_url": "https://arxiv.org/pdf/2505.14604v4",
    "published_date": "2025-05-20 16:53:40 UTC",
    "updated_date": "2025-10-30 02:36:10 UTC"
  },
  {
    "arxiv_id": "2505.14603v1",
    "title": "Towards a Foundation Model for Communication Systems",
    "authors": [
      "Davide Buffelli",
      "Sowmen Das",
      "Yu-Wei Lin",
      "Sattar Vakili",
      "Chien-Yi Wang",
      "Masoud Attarifar",
      "Pritthijit Nath",
      "Da-shan Shiu"
    ],
    "abstract": "Artificial Intelligence (AI) has demonstrated unprecedented performance across various domains, and its application to communication systems is an active area of research. While current methods focus on task-specific solutions, the broader trend in AI is shifting toward large general models capable of supporting multiple applications. In this work, we take a step toward a foundation model for communication data--a transformer-based, multi-modal model designed to operate directly on communication data. We propose methodologies to address key challenges, including tokenization, positional embedding, multimodality, variable feature sizes, and normalization. Furthermore, we empirically demonstrate that such a model can successfully estimate multiple features, including transmission rank, selected precoder, Doppler spread, and delay profile.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14603v1",
    "published_date": "2025-05-20 16:52:11 UTC",
    "updated_date": "2025-05-20 16:52:11 UTC"
  },
  {
    "arxiv_id": "2505.14599v2",
    "title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
    "authors": [
      "Guangzhi Xiong",
      "Eric Xie",
      "Corey Williams",
      "Myles Kim",
      "Amir Hassan Shariatmadari",
      "Sikun Guo",
      "Stefan Bekiranov",
      "Aidong Zhang"
    ],
    "abstract": "Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful scientific hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14599v2",
    "published_date": "2025-05-20 16:49:40 UTC",
    "updated_date": "2025-06-08 22:42:31 UTC"
  },
  {
    "arxiv_id": "2505.14569v1",
    "title": "Agent Context Protocols Enhance Collective Inference",
    "authors": [
      "Devansh Bhardwaj",
      "Arjun Beniwal",
      "Shreyas Chaudhari",
      "Ashwin Kalyan",
      "Tanmay Rajpurohit",
      "Karthik R. Narasimhan",
      "Ameet Deshpande",
      "Vishvak Murahari"
    ],
    "abstract": "AI agents have become increasingly adept at complex tasks such as coding, reasoning, and multimodal understanding. However, building generalist systems requires moving beyond individual agents to collective inference -- a paradigm where multi-agent systems with diverse, task-specialized agents complement one another through structured communication and collaboration. Today, coordination is usually handled with imprecise, ad-hoc natural language, which limits complex interaction and hinders interoperability with domain-specific agents. We introduce Agent context protocols (ACPs): a domain- and agent-agnostic family of structured protocols for agent-agent communication, coordination, and error handling. ACPs combine (i) persistent execution blueprints -- explicit dependency graphs that store intermediate agent outputs -- with (ii) standardized message schemas, enabling robust and fault-tolerant multi-agent collective inference. ACP-powered generalist systems reach state-of-the-art performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance and best-in-class multimodal technical reports, outperforming commercial AI systems in human evaluation. ACPs are highly modular and extensible, allowing practitioners to build top-tier generalist agents quickly.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14569v1",
    "published_date": "2025-05-20 16:28:08 UTC",
    "updated_date": "2025-05-20 16:28:08 UTC"
  },
  {
    "arxiv_id": "2505.14758v1",
    "title": "Kaleidoscope Gallery: Exploring Ethics and Generative AI Through Art",
    "authors": [
      "Alayt Issak",
      "Uttkarsh Narayan",
      "Ramya Srinivasan",
      "Erica Kleinman",
      "Casper Harteveld"
    ],
    "abstract": "Ethical theories and Generative AI (GenAI) models are dynamic concepts subject to continuous evolution. This paper investigates the visualization of ethics through a subset of GenAI models. We expand on the emerging field of Visual Ethics, using art as a form of critical inquiry and the metaphor of a kaleidoscope to invoke moral imagination. Through formative interviews with 10 ethics experts, we first establish a foundation of ethical theories. Our analysis reveals five families of ethical theories, which we then transform into images using the text-to-image (T2I) GenAI model. The resulting imagery, curated as Kaleidoscope Gallery and evaluated by the same experts, revealed eight themes that highlight how morality, society, and learned associations are central to ethical theories. We discuss implications for critically examining T2I models and present cautions and considerations. This work contributes to examining ethical theories as foundational knowledge that interrogates GenAI models as socio-technical systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14758v1",
    "published_date": "2025-05-20 16:28:00 UTC",
    "updated_date": "2025-05-20 16:28:00 UTC"
  },
  {
    "arxiv_id": "2505.14566v1",
    "title": "KIPPO: Koopman-Inspired Proximal Policy Optimization",
    "authors": [
      "Andrei Cozma",
      "Landon Harris",
      "Hairong Qi"
    ],
    "abstract": "Reinforcement Learning (RL) has made significant strides in various domains, and policy gradient methods like Proximal Policy Optimization (PPO) have gained popularity due to their balance in performance, training stability, and computational efficiency. These methods directly optimize policies through gradient-based updates. However, developing effective control policies for environments with complex and non-linear dynamics remains a challenge. High variance in gradient estimates and non-convex optimization landscapes often lead to unstable learning trajectories. Koopman Operator Theory has emerged as a powerful framework for studying non-linear systems through an infinite-dimensional linear operator that acts on a higher-dimensional space of measurement functions. In contrast with their non-linear counterparts, linear systems are simpler, more predictable, and easier to analyze. In this paper, we present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an approximately linear latent-space representation of the underlying system's dynamics while retaining essential features for effective policy learning. This is achieved through a Koopman-approximation auxiliary network that can be added to the baseline policy optimization algorithms without altering the architecture of the core policy or value function. Extensive experimental results demonstrate consistent improvements over the PPO baseline with 6-60% increased performance while reducing variability by up to 91% when evaluated on various continuous control tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for IJCAI 2025. This arXiv submission is the full version of the conference paper, including the appendix and supplementary material omitted from the IJCAI proceedings",
    "pdf_url": "https://arxiv.org/pdf/2505.14566v1",
    "published_date": "2025-05-20 16:25:41 UTC",
    "updated_date": "2025-05-20 16:25:41 UTC"
  },
  {
    "arxiv_id": "2505.14564v1",
    "title": "Bellman operator convergence enhancements in reinforcement learning algorithms",
    "authors": [
      "David Krame Kadurha",
      "Domini Jocema Leko Moutouo",
      "Yae Ulrich Gaba"
    ],
    "abstract": "This paper reviews the topological groundwork for the study of reinforcement learning (RL) by focusing on the structure of state, action, and policy spaces. We begin by recalling key mathematical concepts such as complete metric spaces, which form the foundation for expressing RL problems. By leveraging the Banach contraction principle, we illustrate how the Banach fixed-point theorem explains the convergence of RL algorithms and how Bellman operators, expressed as operators on Banach spaces, ensure this convergence. The work serves as a bridge between theoretical mathematics and practical algorithm design, offering new approaches to enhance the efficiency of RL. In particular, we investigate alternative formulations of Bellman operators and demonstrate their impact on improving convergence rates and performance in standard RL environments such as MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper mathematical understanding of RL can lead to more effective algorithms for decision-making problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14564v1",
    "published_date": "2025-05-20 16:24:42 UTC",
    "updated_date": "2025-05-20 16:24:42 UTC"
  },
  {
    "arxiv_id": "2505.14561v2",
    "title": "SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification",
    "authors": [
      "Theo Lepage",
      "Reda Dehak"
    ],
    "abstract": "Self-Supervised Learning (SSL) has led to considerable progress in Speaker Verification (SV). The standard framework uses same-utterance positive sampling and data-augmentation to generate anchor-positive pairs of the same speaker. This is a major limitation, as this strategy primarily encodes channel information from the recording condition, shared by the anchor and positive. We propose a new positive sampling technique to address this bottleneck: Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find an appropriate positive, i.e., of the same speaker identity but a different recording condition, in the latent space using clustering assignments and a memory queue of positive embeddings. SSPS improves SV performance for both SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by lowering intra-speaker variance, providing comparable performance to DINO-SSPS.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "accepted at Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14561v2",
    "published_date": "2025-05-20 16:19:34 UTC",
    "updated_date": "2025-06-24 09:06:50 UTC"
  },
  {
    "arxiv_id": "2505.14757v1",
    "title": "Bridge2AI: Building A Cross-disciplinary Curriculum Towards AI-Enhanced Biomedical and Clinical Care",
    "authors": [
      "John Rincon",
      "Alexander R. Pelletier",
      "Destiny Gilliland",
      "Wei Wang",
      "Ding Wang",
      "Baradwaj S. Sankar",
      "Lori Scott-Sheldon",
      "Samson Gebreab",
      "William Hersh",
      "Parisa Rashidi",
      "Sally Baxter",
      "Wade Schulz",
      "Trey Ideker",
      "Yael Bensoussan",
      "Paul C. Boutros",
      "Alex A. T. Bui",
      "Colin Walsh",
      "Karol E. Watson",
      "Peipei Ping"
    ],
    "abstract": "Objective: As AI becomes increasingly central to healthcare, there is a pressing need for bioinformatics and biomedical training systems that are personalized and adaptable. Materials and Methods: The NIH Bridge2AI Training, Recruitment, and Mentoring (TRM) Working Group developed a cross-disciplinary curriculum grounded in collaborative innovation, ethical data stewardship, and professional development within an adapted Learning Health System (LHS) framework. Results: The curriculum integrates foundational AI modules, real-world projects, and a structured mentee-mentor network spanning Bridge2AI Grand Challenges and the Bridge Center. Guided by six learner personas, the program tailors educational pathways to individual needs while supporting scalability. Discussion: Iterative refinement driven by continuous feedback ensures that content remains responsive to learner progress and emerging trends. Conclusion: With over 30 scholars and 100 mentors engaged across North America, the TRM model demonstrates how adaptive, persona-informed training can build interdisciplinary competencies and foster an integrative, ethically grounded AI education in biomedical contexts.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14757v1",
    "published_date": "2025-05-20 16:19:05 UTC",
    "updated_date": "2025-05-20 16:19:05 UTC"
  },
  {
    "arxiv_id": "2505.14555v2",
    "title": "Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting",
    "authors": [
      "Yingtao Luo",
      "Shikai Fang",
      "Binqing Wu",
      "Qingsong Wen",
      "Liang Sun"
    ],
    "abstract": "Weather forecasting is essential but remains computationally intensive and physically incomplete in traditional numerical weather prediction (NWP) methods. Deep learning (DL) models offer efficiency and accuracy but often ignore physical laws, limiting interpretability and generalization. We propose PhyDL-NWP, a physics-guided deep learning framework that integrates physical equations with latent force parameterization into data-driven models. It predicts weather variables from arbitrary spatiotemporal coordinates, computes physical terms via automatic differentiation, and uses a physics-informed loss to align predictions with governing dynamics. PhyDL-NWP enables resolution-free downscaling by modeling weather as a continuous function and fine-tunes pre-trained models with minimal overhead, achieving up to 170x faster inference with only 55K parameters. Experiments show that PhyDL-NWP improves both forecasting performance and physical consistency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published/Accepted in ACM SIGKDD 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14555v2",
    "published_date": "2025-05-20 16:13:20 UTC",
    "updated_date": "2025-05-23 05:09:32 UTC"
  },
  {
    "arxiv_id": "2506.11030v1",
    "title": "Forward Target Propagation: A Forward-Only Approach to Global Error Credit Assignment via Local Losses",
    "authors": [
      "Nazmus Saadat As-Saquib",
      "A N M Nafiz Abeer",
      "Hung-Ta Chien",
      "Byung-Jun Yoon",
      "Suhas Kumar",
      "Su-in Yi"
    ],
    "abstract": "Training neural networks has traditionally relied on backpropagation (BP), a gradient-based algorithm that, despite its widespread success, suffers from key limitations in both biological and hardware perspectives. These include backward error propagation by symmetric weights, non-local credit assignment, and frozen activity during backward passes. We propose Forward Target Propagation (FTP), a biologically plausible and computationally efficient alternative that replaces the backward pass with a second forward pass. FTP estimates layerwise targets using only feedforward computations, eliminating the need for symmetric feedback weights or learnable inverse functions, hence enabling modular and local learning. We evaluate FTP on fully connected networks, CNNs, and RNNs, demonstrating accuracies competitive with BP on MNIST, CIFAR10, and CIFAR100, as well as effective modeling of long-term dependencies in sequential tasks. Moreover, FTP outperforms BP under quantized low-precision and emerging hardware constraints while also demonstrating substantial efficiency gains over other biologically inspired methods such as target propagation variants and forward-only learning algorithms. With its minimal computational overhead, forward-only nature, and hardware compatibility, FTP provides a promising direction for energy-efficient on-device learning and neuromorphic computing.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11030v1",
    "published_date": "2025-05-20 16:09:23 UTC",
    "updated_date": "2025-05-20 16:09:23 UTC"
  },
  {
    "arxiv_id": "2505.17084v1",
    "title": "From nuclear safety to LLM security: Applying non-probabilistic risk management strategies to build safe and secure LLM-powered systems",
    "authors": [
      "Alexander Gutfraind",
      "Vicki Bier"
    ],
    "abstract": "Large language models (LLMs) offer unprecedented and growing capabilities, but also introduce complex safety and security challenges that resist conventional risk management. While conventional probabilistic risk analysis (PRA) requires exhaustive risk enumeration and quantification, the novelty and complexity of these systems make PRA impractical, particularly against adaptive adversaries. Previous research found that risk management in various fields of engineering such as nuclear or civil engineering is often solved by generic (i.e. field-agnostic) strategies such as event tree analysis or robust designs. Here we show how emerging risks in LLM-powered systems could be met with 100+ of these non-probabilistic strategies to risk management, including risks from adaptive adversaries. The strategies are divided into five categories and are mapped to LLM security (and AI safety more broadly). We also present an LLM-powered workflow for applying these strategies and other workflows suitable for solution architects. Overall, these strategies could contribute (despite some limitations) to security, safety and other dimensions of responsible AI.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17084v1",
    "published_date": "2025-05-20 16:07:41 UTC",
    "updated_date": "2025-05-20 16:07:41 UTC"
  },
  {
    "arxiv_id": "2505.14552v2",
    "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation",
    "authors": [
      "Jiajun Shi",
      "Jian Yang",
      "Jiaheng Liu",
      "Xingyuan Bu",
      "Jiangjie Chen",
      "Junting Zhou",
      "Kaijing Ma",
      "Zhoufutu Wen",
      "Bingli Wang",
      "Yancheng He",
      "Liang Song",
      "Hualei Zhu",
      "Shilong Li",
      "Xingjian Wang",
      "Wei Zhang",
      "Ruibin Yuan",
      "Yifan Yao",
      "Wenjun Yang",
      "Yunli Wang",
      "Siyuan Fang",
      "Siyu Yuan",
      "Qianyu He",
      "Xiangru Tang",
      "Yingshui Tan",
      "Wangchunshu Zhou",
      "Zhaoxiang Zhang",
      "Zhoujun Li",
      "Wenhao Huang",
      "Ge Zhang"
    ],
    "abstract": "Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM's general reasoning potential. To address this limitation, we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.14552v2",
    "published_date": "2025-05-20 16:06:32 UTC",
    "updated_date": "2025-05-21 07:43:57 UTC"
  },
  {
    "arxiv_id": "2505.14551v2",
    "title": "Game of Trust: How Trustworthy Does Your Blockchain Think You Are?",
    "authors": [
      "Petros Drineas",
      "Rohit Nema",
      "Rafail Ostrovsky",
      "Vassilis Zikas"
    ],
    "abstract": "We investigate how a blockchain can distill the collective belief of its nodes regarding the trustworthiness of a (sub)set of nodes into a {\\em reputation system} that reflects the probability of correctly performing a task. To address this question, we introduce a framework that breaks it down into two sub-problems:\n  1. (Information Extraction): How can the system distill trust information from a function of the nodes' true beliefs?\n  2. (Incentive Design): How can we incentivize nodes to truthfully report such information?\n  To tackle the first sub-problem, we adapt, in a non-trivial manner, the well-known PageRank algorithm to our problem. For the second, we define a new class of games, called Trustworthy Reputation games (TRep games), which aim to extract the collective beliefs on trust from the actions of rational participants. We then propose a concrete TRep game whose utility function leverages Personalized PageRank and can be instantiated through a straightforward blockchain rewards mechanism. Building on this, we show how the TRep game enables the design of a reputation system. Such systems can enhance the robustness, scalability, and efficiency of blockchain and DeFi solutions. For instance, we demonstrate how such a system can be used within a Proof-of-Reputation blockchain.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14551v2",
    "published_date": "2025-05-20 16:06:25 UTC",
    "updated_date": "2025-10-09 18:50:19 UTC"
  },
  {
    "arxiv_id": "2505.14549v1",
    "title": "Can Large Language Models Really Recognize Your Name?",
    "authors": [
      "Dzung Pham",
      "Peter Kairouz",
      "Niloofar Mireshghallah",
      "Eugene Bagdasarian",
      "Chau Minh Pham",
      "Amir Houmansadr"
    ],
    "abstract": "Large language models (LLMs) are increasingly being used to protect sensitive user data. However, current LLM-based privacy solutions assume that these models can reliably detect personally identifiable information (PII), particularly named entities. In this paper, we challenge that assumption by revealing systematic failures in LLM-based privacy tasks. Specifically, we show that modern LLMs regularly overlook human names even in short text snippets due to ambiguous contexts, which cause the names to be misinterpreted or mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous human names, leveraging the name regularity bias phenomenon, embedded within concise text snippets along with benign prompt injections. Our experiments on modern LLMs tasked to detect PII as well as specialized tools show that recall of ambiguous names drops by 20--40% compared to more recognizable names. Furthermore, ambiguous human names are four times more likely to be ignored in supposedly privacy-preserving summaries generated by LLMs when benign prompt injections are present. These findings highlight the underexplored risks of relying solely on LLMs to safeguard user privacy and underscore the need for a more systematic investigation into their privacy failure modes.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14549v1",
    "published_date": "2025-05-20 16:05:05 UTC",
    "updated_date": "2025-05-20 16:05:05 UTC"
  },
  {
    "arxiv_id": "2505.14544v4",
    "title": "Smart Traffic Signals: Comparing MARL and Fixed-Time Strategies",
    "authors": [
      "Saahil Mahato"
    ],
    "abstract": "Urban traffic congestion, particularly at intersections, significantly affects travel time, fuel consumption, and emissions. Traditional fixed-time signal control systems often lack the adaptability to effectively manage dynamic traffic patterns. This study explores the application of multi-agent reinforcement learning (MARL) to optimize traffic signal coordination across multiple intersections within a simulated environment. A simulation was developed to model a network of interconnected intersections with randomly generated vehicle flows to reflect realistic traffic variability. A decentralized MARL controller was implemented in which each traffic signal operates as an autonomous agent, making decisions based on local observations and information from neighboring agents. Performance was evaluated against a baseline fixed-time controller using metrics such as average vehicle wait time and overall throughput. The MARL approach demonstrated statistically significant improvements, including reduced average waiting times and improved throughput. These findings suggest that MARL-based dynamic control strategies hold substantial promise to improve urban traffic management efficiency. More research is recommended to address the challenges of scalability and real-world implementation.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14544v4",
    "published_date": "2025-05-20 15:59:44 UTC",
    "updated_date": "2025-11-27 17:30:23 UTC"
  },
  {
    "arxiv_id": "2505.14539v1",
    "title": "A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)",
    "authors": [
      "Gaia Belardinelli",
      "Thomas Bolander",
      "Sebastian Watzl"
    ],
    "abstract": "In this work, we present the first general logic of attention. Attention is a powerful cognitive ability that allows agents to focus on potentially complex information, such as logically structured propositions, higher-order beliefs, or what other agents pay attention to. This ability is a strength, as it helps to ignore what is irrelevant, but it can also introduce biases when some types of information or agents are systematically ignored. Existing dynamic epistemic logics for attention cannot model such complex attention scenarios, as they only model attention to atomic formulas. Additionally, such logics quickly become cumbersome, as their size grows exponentially in the number of agents and announced literals. Here, we introduce a logic that overcomes both limitations. First, we generalize edge-conditioned event models, which we show to be as expressive as standard event models yet exponentially more succinct (generalizing both standard event models and generalized arrow updates). Second, we extend attention to arbitrary formulas, allowing agents to also attend to other agents' beliefs or attention. Our work treats attention as a modality, like belief or awareness. We introduce attention principles that impose closure properties on that modality and that can be used in its axiomatization. Throughout, we illustrate our framework with examples of AI agents reasoning about human attentional biases, demonstrating how such agents can discover attentional biases.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14539v1",
    "published_date": "2025-05-20 15:56:34 UTC",
    "updated_date": "2025-05-20 15:56:34 UTC"
  },
  {
    "arxiv_id": "2505.14756v2",
    "title": "LLINBO: Trustworthy LLM-in-the-Loop Bayesian Optimization",
    "authors": [
      "Chih-Yu Chang",
      "Milad Azvar",
      "Chinedum Okwudire",
      "Raed Al Kontar"
    ],
    "abstract": "Bayesian optimization (BO) is a sequential decision-making tool widely used for optimizing expensive black-box functions. Recently, Large Language Models (LLMs) have shown remarkable adaptability in low-data regimes, making them promising tools for black-box optimization by leveraging contextual knowledge to propose high-quality query points. However, relying solely on LLMs as optimization agents introduces risks due to their lack of explicit surrogate modeling and calibrated uncertainty, as well as their inherently opaque internal mechanisms. This structural opacity makes it difficult to characterize or control the exploration-exploitation trade-off, ultimately undermining theoretical tractability and reliability. To address this, we propose LLINBO: LLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with statistical surrogate experts (e.g., Gaussian Processes (GP)). The core philosophy is to leverage contextual reasoning strengths of LLMs for early exploration, while relying on principled statistical models to guide efficient exploitation. Specifically, we introduce three mechanisms that enable this collaboration and establish their theoretical guarantees. We end the paper with a real-life proof-of-concept in the context of 3D printing. The code to reproduce the results can be found at https://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14756v2",
    "published_date": "2025-05-20 15:54:48 UTC",
    "updated_date": "2025-10-09 17:09:12 UTC"
  },
  {
    "arxiv_id": "2505.14533v1",
    "title": "Energy-Efficient Deep Reinforcement Learning with Spiking Transformers",
    "authors": [
      "Mohammad Irfan Uddin",
      "Nishad Tasnim",
      "Md Omor Faruk",
      "Zejian Zhou"
    ],
    "abstract": "Agent-based Transformers have been widely adopted in recent reinforcement learning advances due to their demonstrated ability to solve complex tasks. However, the high computational complexity of Transformers often results in significant energy consumption, limiting their deployment in real-world autonomous systems. Spiking neural networks (SNNs), with their biologically inspired structure, offer an energy-efficient alternative for machine learning. In this paper, a novel Spike-Transformer Reinforcement Learning (STRL) algorithm that combines the energy efficiency of SNNs with the powerful decision-making capabilities of reinforcement learning is developed. Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons and attention mechanisms capable of processing spatio-temporal patterns over multiple time steps is designed. The architecture is further enhanced with state, action, and reward encodings to create a Transformer-like structure optimized for reinforcement learning tasks. Comprehensive numerical experiments conducted on state-of-the-art benchmarks demonstrate that the proposed SNN Transformer achieves significantly improved policy performance compared to conventional agent-based Transformers. With both enhanced energy efficiency and policy optimality, this work highlights a promising direction for deploying bio-inspired, low-cost machine learning models in complex real-world decision-making scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14533v1",
    "published_date": "2025-05-20 15:52:43 UTC",
    "updated_date": "2025-05-20 15:52:43 UTC"
  },
  {
    "arxiv_id": "2505.14526v2",
    "title": "RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based Autonomous Navigation",
    "authors": [
      "Matteo El-Hariry",
      "Antoine Richard",
      "Ricard M. Castan",
      "Luis F. W. Batista",
      "Matthieu Geist",
      "Cedric Pradalier",
      "Miguel Olivares-Mendez"
    ],
    "abstract": "Autonomous robots must navigate and operate in diverse environments, from terrestrial and aquatic settings to aerial and space domains. While Reinforcement Learning (RL) has shown promise in training policies for specific autonomous robots, existing frameworks and benchmarks are often constrained to unique platforms, limiting generalization and fair comparisons across different mobility systems. In this paper, we present a multi-domain framework for training, evaluating and deploying RL-based navigation policies across diverse robotic platforms and operational environments. Our work presents four key contributions: (1) a scalable and modular framework, facilitating seamless robot-task interchangeability and reproducible training pipelines; (2) sim-to-real transfer demonstrated through real-world experiments with multiple robots, including a satellite robotic simulator, an unmanned surface vessel, and a wheeled ground vehicle; (3) the release of the first open-source API for deploying Isaac Lab-trained policies to real robots, enabling lightweight inference and rapid field validation; and (4) uniform tasks and metrics for cross-medium evaluation, through a unified evaluation testbed to assess performance of navigation tasks in diverse operational conditions (aquatic, terrestrial and space). By ensuring consistency between simulation and real-world deployment, RoboRAN lowers the barrier to developing adaptable RL-based navigation strategies. Its modular design enables straightforward integration of new robots and tasks through predefined templates, fostering reproducibility and extension to diverse domains. To support the community, we release RoboRAN as open-source.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at Transactions on Machine Learning Research (TMLR)",
    "pdf_url": "https://arxiv.org/pdf/2505.14526v2",
    "published_date": "2025-05-20 15:48:23 UTC",
    "updated_date": "2025-11-05 17:12:59 UTC"
  },
  {
    "arxiv_id": "2505.14524v3",
    "title": "Guarded Query Routing for Large Language Models",
    "authors": [
      "Richard Šléher",
      "William Brach",
      "Tibor Sloboda",
      "Kristián Košťál",
      "Lukas Galke"
    ],
    "abstract": "Query routing, the task to route user queries to different large language model (LLM) endpoints, can be considered as a text classification problem. However, out-of-distribution queries must be handled properly, as those could be about unrelated domains, queries in other languages, or even contain unsafe text. Here, we thus study a guarded query routing problem, for which we first introduce the Guarded Query Routing Benchmark (GQR-Bench, released as Python package gqr), covers three exemplary target domains (law, finance, and healthcare), and seven datasets to test robustness against out-of-distribution queries. We then use GQR-Bench to contrast the effectiveness and efficiency of LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B), standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and traditional machine learning models (SVM, XGBoost). Our results show that WideMLP, enhanced with out-of-domain detection capabilities, yields the best trade-off between accuracy (88%) and speed (<4ms). The embedding-based fastText excels at speed (<1ms) with acceptable accuracy (80%), whereas LLMs yield the highest accuracy (91%) but are comparatively slow (62ms for local Llama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge the automatic reliance on LLMs for (guarded) query routing and provide concrete recommendations for practical applications. Source code is available: https://github.com/williambrach/gqr.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14524v3",
    "published_date": "2025-05-20 15:46:59 UTC",
    "updated_date": "2025-10-25 09:38:24 UTC"
  },
  {
    "arxiv_id": "2505.14523v2",
    "title": "Exploring Graph Representations of Logical Forms for Language Modeling",
    "authors": [
      "Michael Sullivan"
    ],
    "abstract": "We make the case for language models over logical forms (LFLMs), arguing that such models are more data-efficient than their textual counterparts. To that end, we introduce the Graph-based Formal-Logical Distributional Semantics (GFoLDS) prototype, a pretrained LM over graph representations of logical forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong experimental evidence that LFLMs can leverage the built-in, basic linguistic knowledge inherent in such models to immediately begin learning more complex patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual, transformer LMs (BERT) pretrained on the same data, indicating that LFLMs can learn with substantially less data than models over plain text. Furthermore, we show that the performance of this model is likely to scale with additional parameters and pretraining data, suggesting the viability of LFLMs in real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To be published in ACL 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.14523v2",
    "published_date": "2025-05-20 15:46:44 UTC",
    "updated_date": "2025-07-18 11:03:24 UTC"
  },
  {
    "arxiv_id": "2505.14513v1",
    "title": "Latent Flow Transformer",
    "authors": [
      "Yen-Chen Wu",
      "Feng-Ting Liao",
      "Meng-Hsi Chen",
      "Pei-Chen Ho",
      "Farhang Nabiei",
      "Da-shan Shiu"
    ],
    "abstract": "Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in \\textit{preserving coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14513v1",
    "published_date": "2025-05-20 15:41:05 UTC",
    "updated_date": "2025-05-20 15:41:05 UTC"
  },
  {
    "arxiv_id": "2505.14510v3",
    "title": "BACON: A fully explainable AI model with graded logic for decision making problems",
    "authors": [
      "Haishi Bai",
      "Jozo Dujmovic",
      "Jianwu Wang"
    ],
    "abstract": "As machine learning models and autonomous agents are increasingly deployed in high-stakes, real-world domains such as healthcare, security, finance, and robotics, the need for transparent and trustworthy explanations has become critical. To ensure end-to-end transparency of AI decisions, we need models that are not only accurate but also fully explainable and human-tunable. We introduce BACON, a novel framework for automatically training explainable AI models for decision making problems using graded logic. BACON achieves high predictive accuracy while offering full structural transparency and precise, logic-based symbolic explanations, enabling effective human-AI collaboration and expert-guided refinement. We evaluate BACON with a diverse set of scenarios: classic Boolean approximation, Iris flower classification, house purchasing decisions and breast cancer diagnosis. In each case, BACON provides high-performance models while producing compact, human-verifiable decision logic. These results demonstrate BACON's potential as a practical and principled approach for delivering crisp, trustworthy explainable AI.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14510v3",
    "published_date": "2025-05-20 15:39:05 UTC",
    "updated_date": "2025-05-27 18:27:38 UTC"
  },
  {
    "arxiv_id": "2505.14505v1",
    "title": "ModRWKV: Transformer Multimodality in Linear Time",
    "authors": [
      "Jiale Kang",
      "Ziyin Yue",
      "Qingyu Yin",
      "Jiang Rui",
      "Weile Li",
      "Zening Lu",
      "Zhouran Ji"
    ],
    "abstract": "Currently, most multimodal studies are based on large language models (LLMs) with quadratic-complexity Transformer architectures. While linear models like RNNs enjoy low inference costs, their application has been largely limited to the text-only modality. This work explores the capabilities of modern RNN architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal framework built upon the RWKV7 architecture as its LLM backbone-which achieves multi-source information fusion through dynamically adaptable heterogeneous modality encoders. We designed the multimodal modules in ModRWKV with an extremely lightweight architecture and, through extensive experiments, identified a configuration that achieves an optimal balance between performance and computational efficiency. ModRWKV leverages the pretrained weights of the RWKV7 LLM for initialization, which significantly accelerates multimodal training. Comparative experiments with different pretrained checkpoints further demonstrate that such initialization plays a crucial role in enhancing the model's ability to understand multimodal signals. Supported by extensive experiments, we conclude that modern RNN architectures present a viable alternative to Transformers in the domain of multimodal large language models (MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV architecture through systematic exploration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14505v1",
    "published_date": "2025-05-20 15:34:36 UTC",
    "updated_date": "2025-05-20 15:34:36 UTC"
  },
  {
    "arxiv_id": "2505.14499v2",
    "title": "Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales",
    "authors": [
      "Jun Cao",
      "Jiyi Li",
      "Ziwei Yang",
      "Renjie Zhou"
    ],
    "abstract": "There has been growing interest in Multimodal Aspect-Based Sentiment Analysis (MABSA) in recent years. Existing methods predominantly rely on pre-trained small language models (SLMs) to collect information related to aspects and sentiments from both image and text, with an aim to align these two modalities. However, small SLMs possess limited capacity and knowledge, often resulting in inaccurate identification of meaning, aspects, sentiments, and their interconnections in textual and visual data. On the other hand, Large language models (LLMs) have shown exceptional capabilities in various tasks by effectively exploring fine-grained information in multimodal data. However, some studies indicate that LLMs still fall short compared to fine-tuned small models in the field of ABSA. Based on these findings, we propose a novel framework, termed LRSA, which combines the decision-making capabilities of SLMs with additional information provided by LLMs for MABSA. Specifically, we inject explanations generated by LLMs as rationales into SLMs and employ a dual cross-attention mechanism for enhancing feature interaction and fusion, thereby augmenting the SLMs' ability to identify aspects and sentiments. We evaluated our method using two baseline models, numerous experiments highlight the superiority of our approach on three widely-used benchmarks, indicating its generalizability and applicability to most pre-trained models for MABSA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 2 figures, 6 tables. Accepted by ICONIP2024",
    "pdf_url": "https://arxiv.org/pdf/2505.14499v2",
    "published_date": "2025-05-20 15:28:26 UTC",
    "updated_date": "2025-05-24 01:55:28 UTC"
  },
  {
    "arxiv_id": "2505.14489v2",
    "title": "Reasoning Models Better Express Their Confidence",
    "authors": [
      "Dongkeun Yoon",
      "Seungone Kim",
      "Sohee Yang",
      "Sunkyoung Kim",
      "Soyeon Kim",
      "Yongil Kim",
      "Eunbi Choi",
      "Yireun Kim",
      "Minjoon Seo"
    ],
    "abstract": "Despite their strengths, large language models (LLMs) often fail to communicate their confidence accurately, making it difficult to assess when they might be wrong and limiting their reliability. In this work, we demonstrate that reasoning models that engage in extended chain-of-thought (CoT) reasoning exhibit superior performance not only in problem-solving but also in accurately expressing their confidence. Specifically, we benchmark six reasoning models across six datasets and find that they achieve strictly better confidence calibration than their non-reasoning counterparts in 33 out of the 36 settings. Our detailed analysis reveals that these gains in calibration stem from the slow thinking behaviors of reasoning models (e.g., exploring alternative approaches and backtracking) which enable them to adjust their confidence dynamically throughout their CoT, making it progressively more accurate. In particular, we find that reasoning models become increasingly better calibrated as their CoT unfolds, a trend not observed in non-reasoning models. Moreover, removing slow thinking behaviors from the CoT leads to a significant drop in calibration. Lastly, we show that non-reasoning models also demonstrate enhanced calibration when simply guided to slow think via in-context learning, fully isolating slow thinking as the source of the calibration gains.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14489v2",
    "published_date": "2025-05-20 15:19:00 UTC",
    "updated_date": "2025-10-22 06:37:15 UTC"
  },
  {
    "arxiv_id": "2505.14479v5",
    "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach",
    "authors": [
      "Oren Sultan",
      "Eitan Stern",
      "Dafna Shahaf"
    ],
    "abstract": "Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "long paper",
    "pdf_url": "https://arxiv.org/pdf/2505.14479v5",
    "published_date": "2025-05-20 15:13:32 UTC",
    "updated_date": "2025-12-13 08:40:21 UTC"
  },
  {
    "arxiv_id": "2505.14469v2",
    "title": "Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations",
    "authors": [
      "Somnath Banerjee",
      "Pratyush Chatterjee",
      "Shanu Kumar",
      "Sayan Layek",
      "Parag Agrawal",
      "Rima Hazra",
      "Animesh Mukherjee"
    ],
    "abstract": "While LLMs appear robustly safety-aligned in English, we uncover a catastrophic, overlooked weakness: attributional collapse under code-mixed perturbations. Our systematic evaluation of open models shows that the linguistic camouflage of code-mixing -- ``blending languages within a single conversation'' -- can cause safety guardrails to fail dramatically. Attack success rates (ASR) spike from a benign 9\\% in monolingual English to 69\\% under code-mixed inputs, with rates exceeding 90\\% in non-Western contexts such as Arabic and Hindi. These effects hold not only on controlled synthetic datasets but also on real-world social media traces, revealing a serious risk for billions of users. To explain why this happens, we introduce saliency drift attribution (SDA), an interpretability framework that shows how, under code-mixing, the model's internal attention drifts away from safety-critical tokens (e.g., ``violence'' or ``corruption''), effectively blinding it to harmful intent. Finally, we propose a lightweight translation-based restoration strategy that recovers roughly 80\\% of the safety lost to code-mixing, offering a practical path toward more equitable and robust LLM safety.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14469v2",
    "published_date": "2025-05-20 15:05:03 UTC",
    "updated_date": "2025-11-30 08:20:28 UTC"
  },
  {
    "arxiv_id": "2505.14455v2",
    "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation",
    "authors": [
      "Chihan Huang",
      "Hao Tang"
    ],
    "abstract": "Although autoregressive models have dominated language modeling in recent years, there has been a growing interest in exploring alternative paradigms to the conventional next-token prediction framework. Diffusion-based language models have emerged as a compelling alternative due to their powerful parallel generation capabilities and inherent editability. However, these models are often constrained by fixed-length generation. A promising direction is to combine the strengths of both paradigms, segmenting sequences into blocks, modeling autoregressive dependencies across blocks while leveraging discrete diffusion to estimate the conditional distribution within each block given the preceding context. Nevertheless, their practical application is often hindered by two key limitations: rigid fixed-length outputs and a lack of flexible control mechanisms. In this work, we address the critical limitations of fixed granularity and weak controllability in current large diffusion language models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive framework that adaptively determines the size of each generation block based on local semantics using reinforcement learning. Furthermore, we introduce a classifier-guided control mechanism tailored to discrete diffusion, which significantly reduces computational overhead while facilitating efficient post-hoc conditioning without retraining. Extensive experiments demonstrate that CtrlDiff sets a new standard among hybrid diffusion models, narrows the performance gap to state-of-the-art autoregressive approaches, and enables effective conditional text generation across diverse tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14455v2",
    "published_date": "2025-05-20 14:52:41 UTC",
    "updated_date": "2025-10-21 20:24:46 UTC"
  },
  {
    "arxiv_id": "2505.14452v2",
    "title": "How Managers Perceive AI-Assisted Conversational Training for Workplace Communication",
    "authors": [
      "Lance T. Wilhelm",
      "Xiaohan Ding",
      "Kirk McInnis Knutsen",
      "Buse Carik",
      "Eugenia H. Rho"
    ],
    "abstract": "Effective workplace communication is essential for managerial success, yet many managers lack access to tailored and sustained training. Although AI-assisted communication systems may offer scalable training solutions, little is known about how managers envision the role of AI in helping them improve their communication skills. To investigate this, we designed a conversational role-play system, CommCoach, as a functional probe to understand how managers anticipate using AI to practice their communication skills. Through semi-structured interviews, participants emphasized the value of adaptive, low-risk simulations for practicing difficult workplace conversations. They also highlighted opportunities, including human-AI teaming, transparent and context-aware feedback, and greater control over AI-generated personas. AI-assisted communication training should balance personalization, structured learning objectives, and adaptability to different user styles and contexts. However, achieving this requires carefully navigating tensions between adaptive and consistent AI feedback, realism and potential bias, and the open-ended nature of AI conversations versus structured workplace discourse.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "accepted to CUI '25",
    "pdf_url": "https://arxiv.org/pdf/2505.14452v2",
    "published_date": "2025-05-20 14:51:27 UTC",
    "updated_date": "2025-05-21 16:59:56 UTC"
  },
  {
    "arxiv_id": "2505.14451v2",
    "title": "RefiDiff: Progressive Refinement Diffusion for Efficient Missing Data Imputation",
    "authors": [
      "Md Atik Ahamed",
      "Qiang Ye",
      "Qiang Cheng"
    ],
    "abstract": "Missing values in high-dimensional, mixed-type datasets pose significant challenges for data imputation, particularly under Missing Not At Random (MNAR) mechanisms. Existing methods struggle to integrate local and global data characteristics, limiting performance in MNAR and high-dimensional settings. We propose an innovative framework, RefiDiff, combining local machine learning predictions with a novel Mamba-based denoising network efficiently capturing long-range dependencies among features and samples with low computational complexity. RefiDiff bridges the predictive and generative paradigms of imputation, leveraging pre-refinement for initial warm-up imputations and post-refinement to polish results, enhancing stability and accuracy. By encoding mixed-type data into unified tokens, RefiDiff enables robust imputation without architectural or hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods across missing-value settings, demonstrating strong performance in MNAR settings and superior out-of-sample generalization. Extensive evaluations on nine real-world datasets demonstrate its robustness, scalability, and effectiveness in handling complex missingness patterns.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2505.14451v2",
    "published_date": "2025-05-20 14:51:07 UTC",
    "updated_date": "2025-11-12 03:39:50 UTC"
  },
  {
    "arxiv_id": "2505.14442v2",
    "title": "Creative Preference Optimization",
    "authors": [
      "Mete Ismayilzada",
      "Antonio Laverghetta",
      "Simone A. Luchini",
      "Reet Patel",
      "Antoine Bosselut",
      "Lonneke van der Plas",
      "Roger Beaty"
    ],
    "abstract": "While Large Language Models (LLMs) have demonstrated impressive performance across natural language generation tasks, their ability to generate truly creative content-characterized by novelty, diversity, surprise, and quality-remains limited. Existing methods for enhancing LLM creativity often focus narrowly on diversity or specific tasks, failing to address creativity's multifaceted nature in a generalizable way. In this work, we propose Creative Preference Optimization (CrPO), a novel alignment method that injects signals from multiple creativity dimensions into the preference optimization objective in a modular fashion. We train and evaluate creativity-augmented versions of several models using CrPO and MuCE, a new large-scale human preference dataset spanning over 200,000 human-generated responses and ratings from more than 30 psychological creativity assessments. Our models outperform strong baselines, including GPT-4o, on both automated and human evaluations, producing more novel, diverse, and surprising generations while maintaining high output quality. Additional evaluations on NoveltyBench further confirm the generalizability of our approach. Together, our results demonstrate that directly optimizing for creativity within preference frameworks is a promising direction for advancing the creative capabilities of LLMs without compromising output quality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.14442v2",
    "published_date": "2025-05-20 14:43:41 UTC",
    "updated_date": "2025-09-19 11:33:34 UTC"
  },
  {
    "arxiv_id": "2505.14436v1",
    "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models",
    "authors": [
      "Yuqiao Tan",
      "Shizhu He",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Large Language Models (LLMs) offer a transparent brain with accessible parameters that encode extensive knowledge, which can be analyzed, located and transferred. Consequently, a key research challenge is to transcend traditional knowledge transfer paradigms rooted in symbolic language and achieve genuine Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods for transferring knowledge across LLMs of different scales through parameters presents an intriguing and valuable research direction. In this paper, we first demonstrate $\\textbf{Alignment}$ in parametric space is the fundamental prerequisite to achieve successful cross-scale PKT. We redefine the previously explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes extracted parameters for LoRA initialization and requires subsequent fine-tune for alignment. Hence, to reduce cost for further fine-tuning, we introduce a novel Pre-Align PKT (PrePKT) paradigm and propose a solution called $\\textbf{LaTen}$ ($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that aligns the parametric spaces of LLMs across scales only using several training steps without following training. Comprehensive experiments on four benchmarks demonstrate that both PostPKT and PrePKT face challenges in achieving consistently stable transfer. Through in-depth analysis, we identify $\\textbf{Neural Incompatibility}$ as the ethological and parametric structural differences between LLMs of varying scales, presenting fundamental challenges to achieving effective PKT. These findings provide fresh insights into the parametric architectures of LLMs and highlight promising directions for future research on efficient PKT. Our code is available at https://github.com/Trae1ounG/Neural_Incompatibility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL'25 Main. Code link: https://github.com/Trae1ounG/Neural_Incompatibility",
    "pdf_url": "https://arxiv.org/pdf/2505.14436v1",
    "published_date": "2025-05-20 14:42:03 UTC",
    "updated_date": "2025-05-20 14:42:03 UTC"
  },
  {
    "arxiv_id": "2505.14435v2",
    "title": "Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI",
    "authors": [
      "Annika Bush",
      "Meltem Aksoy",
      "Markus Pauly",
      "Greta Ontrup"
    ],
    "abstract": "As organizations increasingly rely on AI systems for decision support in sustainability contexts, it becomes critical to understand the inherent biases and perspectives embedded in Large Language Models (LLMs). This study systematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek, GPT, LLaMA, and Mistral - conceptualize sustainability and its relationship with AI. We administered validated, psychometric sustainability-related questionnaires - each 100 times per model -- to capture response patterns and variability. Our findings revealed significant inter-model differences: For example, GPT exhibited skepticism about the compatibility of AI and sustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect scores for several Sustainable Development Goals (SDGs). Models also diverged in attributing institutional responsibility for AI and sustainability integration, a results that holds implications for technology governance approaches. Our results demonstrate that model selection could substantially influence organizational sustainability strategies, highlighting the need for awareness of model-specific biases when deploying LLMs for sustainability-related decision-making.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted for EMNLP Conference",
    "pdf_url": "https://arxiv.org/pdf/2505.14435v2",
    "published_date": "2025-05-20 14:41:56 UTC",
    "updated_date": "2025-09-30 19:12:24 UTC"
  },
  {
    "arxiv_id": "2505.14428v1",
    "title": "Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications",
    "authors": [
      "Riccardo D'Elia"
    ],
    "abstract": "The objective of this proposal is to bridge the gap between Deep Learning (DL) and System Dynamics (SD) by developing an interpretable neural system dynamics framework. While DL excels at learning complex models and making accurate predictions, it lacks interpretability and causal reliability. Traditional SD approaches, on the other hand, provide transparency and causal insights but are limited in scalability and require extensive domain knowledge. To overcome these limitations, this project introduces a Neural System Dynamics pipeline, integrating Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning. This framework combines the predictive power of DL with the interpretability of traditional SD models, resulting in both causal reliability and scalability. The efficacy of the proposed pipeline will be validated through real-world applications of the EU-funded AutoMoTIF project, which is focused on autonomous multimodal transportation systems. The long-term goal is to collect actionable insights that support the integration of explainability and safety in autonomous systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To be submitted to CEUR-WS.org for publication in the Doctoral Consortium Proceedings of XAI 2025, The World Conference on Explainable Artificial Intelligence",
    "pdf_url": "https://arxiv.org/pdf/2505.14428v1",
    "published_date": "2025-05-20 14:38:39 UTC",
    "updated_date": "2025-05-20 14:38:39 UTC"
  },
  {
    "arxiv_id": "2505.14419v1",
    "title": "SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation",
    "authors": [
      "Huimin Xu",
      "Xin Mao",
      "Feng-Lin Li",
      "Xiaobao Wu",
      "Wang Chen",
      "Wei Zhang",
      "Anh Tuan Luu"
    ],
    "abstract": "Process Reward Models (PRMs) have demonstrated promising results in mathematical reasoning, but existing process annotation approaches, whether through human annotations or Monte Carlo simulations, remain computationally expensive. In this paper, we introduce Step COmpression for Process Estimation (SCOPE), a novel compression-based approach that significantly reduces annotation costs. We first translate natural language reasoning steps into code and normalize them through Abstract Syntax Tree, then merge equivalent steps to construct a prefix tree. Unlike simulation-based methods that waste numerous samples on estimation, SCOPE leverages a compression-based prefix tree where each root-to-leaf path serves as a training sample, reducing the complexity from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K samples with only 5% of the computational resources required by previous methods. Empirical results demonstrate that PRMs trained on our dataset consistently outperform existing automated annotation approaches on both Best-of-N strategy and ProcessBench.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14419v1",
    "published_date": "2025-05-20 14:31:15 UTC",
    "updated_date": "2025-05-20 14:31:15 UTC"
  },
  {
    "arxiv_id": "2506.11029v1",
    "title": "Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model",
    "authors": [
      "Xue Wang",
      "Tian Zhou",
      "Jinyang Gao",
      "Bolin Ding",
      "Jingren Zhou"
    ],
    "abstract": "We present a joint forecasting framework for time series prediction that contrasts with traditional direct or recursive methods. This framework achieves state-of-the-art performance for our designed foundation model, YingLong, and reveals a novel scaling effect: longer outputs significantly enhance model accuracy due to delayed chain-of-thought reasoning in our non-causal approach. YingLong is a non-causal, bidirectional attention encoder-only transformer trained through masked token recovery, aligning more effectively with language understanding tasks than with generation tasks. Additionally, we boost performance by tackling output variance with a multi-input ensemble. We release four foundation models ranging from 6M to 300M parameters, demonstrating superior results in zero-shot tasks on the ETT and Weather datasets. YingLong achieves more than 60% best performance. To ensure generalizability, we assessed the models using the GIFT-Eval benchmark, which comprises 23 time series datasets across 7 domains. Yinglong significantly outperformed the best time-series foundation models, end-to-end trained models by 14% and 44% in rank respectively.The pretrained 300M model is available at https://huggingface.co/qcw1314/YingLong_300m",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11029v1",
    "published_date": "2025-05-20 14:31:06 UTC",
    "updated_date": "2025-05-20 14:31:06 UTC"
  },
  {
    "arxiv_id": "2505.14412v1",
    "title": "PRL: Prompts from Reinforcement Learning",
    "authors": [
      "Paweł Batorski",
      "Adrian Kosmala",
      "Paul Swoboda"
    ],
    "abstract": "Effective prompt engineering remains a central challenge in fully harnessing the capabilities of LLMs. While well-designed prompts can dramatically enhance performance, crafting them typically demands expert intuition and a nuanced understanding of the task. Moreover, the most impactful prompts often hinge on subtle semantic cues, ones that may elude human perception but are crucial for guiding LLM behavior. In this paper, we introduce PRL (Prompts from Reinforcement Learning), a novel RL-based approach for automatic prompt generation. Unlike previous methods, PRL can produce novel few-shot examples that were not seen during training. Our approach achieves state-of-the-art performance across a range of benchmarks, including text classification, simplification, and summarization. On the classification task, it surpasses prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it improves the average ROUGE scores on the summarization task by 4.32 over APE and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over APE and by 6.01 over EvoPrompt. Our code is available at https://github.com/Batorskq/prl .",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14412v1",
    "published_date": "2025-05-20 14:26:19 UTC",
    "updated_date": "2025-05-20 14:26:19 UTC"
  },
  {
    "arxiv_id": "2505.14403v4",
    "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning",
    "authors": [
      "Zhaohui Yang",
      "Yuxiao Ye",
      "Shilei Jiang",
      "Chen Hu",
      "Linjing Li",
      "Shihong Deng",
      "Daxin Jiang"
    ],
    "abstract": "Recent advances in reasoning language models have witnessed a paradigm shift from short to long CoT pattern. Given the substantial computational cost of rollouts in long CoT models, maximizing the utility of fixed training datasets becomes crucial. Our analysis reveals that negative responses contain valuable components such as self-reflection and error-correction steps, yet primary existing methods either completely discard negative samples (RFT) or apply equal penalization across all tokens (RL), failing to leverage these potential learning signals. In light of this, we propose Behavior Constrained Policy Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline RL framework that encompasses three stages: 1) sample segmentation, 2) consensus-based step correctness assessment combining LLM and PRM judgers, and 3) policy optimization with NSA designed to effectively mine positive steps within negative samples. Experimental results show that BCPG-NSA outperforms baselines on several challenging math/coding reasoning benchmarks using the same training dataset, achieving improved sample efficiency and demonstrating robustness and scalability when extended to multiple iterations.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14403v4",
    "published_date": "2025-05-20 14:16:49 UTC",
    "updated_date": "2025-09-15 14:23:10 UTC"
  },
  {
    "arxiv_id": "2505.14753v1",
    "title": "TransMedSeg: A Transferable Semantic Framework for Semi-Supervised Medical Image Segmentation",
    "authors": [
      "Mengzhu Wang",
      "Jiao Li",
      "Shanshan Wang",
      "Long Lan",
      "Huibin Tan",
      "Liang Yang",
      "Guoli Yang"
    ],
    "abstract": "Semi-supervised learning (SSL) has achieved significant progress in medical image segmentation (SSMIS) through effective utilization of limited labeled data. While current SSL methods for medical images predominantly rely on consistency regularization and pseudo-labeling, they often overlook transferable semantic relationships across different clinical domains and imaging modalities. To address this, we propose TransMedSeg, a novel transferable semantic framework for semi-supervised medical image segmentation. Our approach introduces a Transferable Semantic Augmentation (TSA) module, which implicitly enhances feature representations by aligning domain-invariant semantics through cross-domain distribution matching and intra-domain structural preservation. Specifically, TransMedSeg constructs a unified feature space where teacher network features are adaptively augmented towards student network semantics via a lightweight memory module, enabling implicit semantic transformation without explicit data generation. Interestingly, this augmentation is implicitly realized through an expected transferable cross-entropy loss computed over the augmented teacher distribution. An upper bound of the expected loss is theoretically derived and minimized during training, incurring negligible computational overhead. Extensive experiments on medical image datasets demonstrate that TransMedSeg outperforms existing semi-supervised methods, establishing a new direction for transferable representation learning in medical image analysis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14753v1",
    "published_date": "2025-05-20 14:16:40 UTC",
    "updated_date": "2025-05-20 14:16:40 UTC"
  },
  {
    "arxiv_id": "2505.14398v1",
    "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation",
    "authors": [
      "Peter Baile Chen",
      "Yi Zhang",
      "Dan Roth",
      "Samuel Madden",
      "Jacob Andreas",
      "Michael Cafarella"
    ],
    "abstract": "While humans naturally learn and adapt from past experiences, large language models (LLMs) and their agentic counterparts struggle to retain reasoning from previous tasks and apply them in future contexts. To address this limitation, we propose a novel framework, log-augmented generation (LAG) that directly reuses prior computation and reasoning from past logs at test time to enhance model's ability to learn from previous tasks and perform better on new, unseen challenges, all while keeping the system efficient and scalable. Specifically, our system represents task logs using key-value (KV) caches, encoding the full reasoning context of prior tasks while storing KV caches for only a selected subset of tokens. When a new task arises, LAG retrieves the KV values from relevant logs to augment generation. Our approach differs from reflection-based memory mechanisms by directly reusing prior reasoning and computations without requiring additional steps for knowledge extraction or distillation. Our method also goes beyond existing KV caching techniques, which primarily target efficiency gains rather than improving accuracy. Experiments on knowledge- and reasoning-intensive datasets demonstrate that our method significantly outperforms standard agentic systems that do not utilize logs, as well as existing solutions based on reflection and KV cache techniques.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Data and code are available at https://peterbaile.github.io/lag/",
    "pdf_url": "https://arxiv.org/pdf/2505.14398v1",
    "published_date": "2025-05-20 14:14:38 UTC",
    "updated_date": "2025-05-20 14:14:38 UTC"
  },
  {
    "arxiv_id": "2505.14396v1",
    "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds",
    "authors": [
      "Gaël Gendron",
      "Jože M. Rožanec",
      "Michael Witbrock",
      "Gillian Dobbie"
    ],
    "abstract": "Causal world models are systems that can answer counterfactual questions about an environment of interest, i.e. predict how it would have evolved if an arbitrary subset of events had been realized differently. It requires understanding the underlying causes behind chains of events and conducting causal inference for arbitrary unseen distributions. So far, this task eludes foundation models, notably large language models (LLMs), which do not have demonstrated causal reasoning capabilities beyond the memorization of existing causal relationships. Furthermore, evaluating counterfactuals in real-world applications is challenging since only the factual world is observed, limiting evaluation to synthetic datasets. We address these problems by explicitly extracting and modeling causal relationships and propose the Causal Cartographer framework. First, we introduce a graph retrieval-augmented generation agent tasked to retrieve causal relationships from data. This approach allows us to construct a large network of real-world causal relationships that can serve as a repository of causal knowledge and build real-world counterfactuals. In addition, we create a counterfactual reasoning agent constrained by causal relationships to perform reliable step-by-step causal inference. We show that our approach can extract causal knowledge and improve the robustness of LLMs for causal reasoning tasks while reducing inference costs and spurious correlations.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 9 pages for the main paper, 20 pages for the references and appendix, 25 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.14396v1",
    "published_date": "2025-05-20 14:14:05 UTC",
    "updated_date": "2025-05-20 14:14:05 UTC"
  },
  {
    "arxiv_id": "2505.14395v2",
    "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language",
    "authors": [
      "Seyoung Song",
      "Seogyeong Jeong",
      "Eunsu Kim",
      "Jiho Jin",
      "Dongkwan Kim",
      "Jay Shin",
      "Alice Oh"
    ],
    "abstract": "Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy for successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks ($r$ > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in Findings of EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14395v2",
    "published_date": "2025-05-20 14:14:00 UTC",
    "updated_date": "2025-09-19 14:26:02 UTC"
  },
  {
    "arxiv_id": "2505.14394v1",
    "title": "Knowledge Graph Based Repository-Level Code Generation",
    "authors": [
      "Mihir Athale",
      "Vishal Vaddina"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have transformed code generation from natural language queries. However, despite their extensive knowledge and ability to produce high-quality code, LLMs often struggle with contextual accuracy, particularly in evolving codebases. Current code search and retrieval methods frequently lack robustness in both the quality and contextual relevance of retrieved results, leading to suboptimal code generation. This paper introduces a novel knowledge graph-based approach to improve code search and retrieval leading to better quality of code generation in the context of repository-level tasks. The proposed approach represents code repositories as graphs, capturing structural and relational information for enhanced context-aware code generation. Our framework employs a hybrid approach for code retrieval to improve contextual relevance, track inter-file modular dependencies, generate more robust code and ensure consistency with the existing codebase. We benchmark the proposed approach on the Evolutionary Code Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark, and demonstrate that our method significantly outperforms the baseline approach. These findings suggest that knowledge graph based code generation could advance robust, context-sensitive coding assistance tools.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.14394v1",
    "published_date": "2025-05-20 14:13:59 UTC",
    "updated_date": "2025-05-20 14:13:59 UTC"
  },
  {
    "arxiv_id": "2505.14391v1",
    "title": "Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning",
    "authors": [
      "Zhaohui Yang",
      "Chenghua He",
      "Xiaowen Shi",
      "Linjing Li",
      "Qiyue Yin",
      "Shihong Deng",
      "Daxin Jiang"
    ],
    "abstract": "Many studies focus on data annotation techniques for training effective PRMs. However, current methods encounter a significant issue when applied to long CoT reasoning processes: they tend to focus solely on the first incorrect step and all preceding steps, assuming that all subsequent steps are incorrect. These methods overlook the unique self-correction and reflection mechanisms inherent in long CoT, where correct reasoning steps may still occur after initial reasoning mistakes. To address this issue, we propose a novel data annotation method for PRMs specifically designed to score the long CoT reasoning process. Given that under the reflection pattern, correct and incorrect steps often alternate, we introduce the concepts of Error Propagation and Error Cessation, enhancing PRMs' ability to identify both effective self-correction behaviors and reasoning based on erroneous steps. Leveraging an LLM-based judger for annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate it at both solution and step levels. Experimental results demonstrate that compared to existing open-source PRMs and PRMs trained on open-source datasets, our PRM achieves superior performance across various metrics, including search guidance, BoN, and F1 scores. Compared to widely used MC-based annotation methods, our annotation approach not only achieves higher data efficiency but also delivers superior performance. Detailed analysis is also conducted to demonstrate the stability and generalizability of our method.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14391v1",
    "published_date": "2025-05-20 14:12:05 UTC",
    "updated_date": "2025-05-20 14:12:05 UTC"
  },
  {
    "arxiv_id": "2505.14381v2",
    "title": "SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation",
    "authors": [
      "Yuyang Dong",
      "Nobuhiro Ueda",
      "Krisztián Boros",
      "Daiki Ito",
      "Takuya Sera",
      "Masafumi Oyamada"
    ],
    "abstract": "With the increasing adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs), rich document analysis technologies for applications like Retrieval-Augmented Generation (RAG) and visual RAG are gaining significant attention. Recent research indicates that using VLMs yields better RAG performance, but processing rich documents remains a challenge since a single page contains large amounts of information. In this paper, we present SCAN (SemantiC Document Layout ANalysis), a novel approach that enhances both textual and visual Retrieval-Augmented Generation (RAG) systems that work with visually rich documents. It is a VLM-friendly approach that identifies document components with appropriate semantic granularity, balancing context preservation with processing efficiency. SCAN uses a coarse-grained semantic approach that divides documents into coherent regions covering contiguous components. We trained the SCAN model by fine-tuning object detection models on an annotated dataset. Our experimental results across English and Japanese datasets demonstrate that applying SCAN improves end-to-end textual RAG performance by up to 9.4 points and visual RAG performance by up to 10.4 points, outperforming conventional approaches and even commercial document processing solutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14381v2",
    "published_date": "2025-05-20 14:03:24 UTC",
    "updated_date": "2025-12-11 08:51:09 UTC"
  },
  {
    "arxiv_id": "2505.14377v1",
    "title": "When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making",
    "authors": [
      "Ulrike Kuhl",
      "Annika Bush"
    ],
    "abstract": "Although the integration of artificial intelligence (AI) into everyday tasks improves efficiency and objectivity, it also risks transmitting bias to human decision-making. In this study, we conducted a controlled experiment that simulated hiring decisions to examine how biased AI recommendations - augmented with or without counterfactual explanations - influence human judgment over time. Participants, acting as hiring managers, completed 60 decision trials divided into a baseline phase without AI, followed by a phase with biased (X)AI recommendations (favoring either male or female candidates), and a final post-interaction phase without AI. Our results indicate that the participants followed the AI recommendations 70% of the time when the qualifications of the given candidates were comparable. Yet, only a fraction of participants detected the gender bias (8 out of 294). Crucially, exposure to biased AI altered participants' inherent preferences: in the post-interaction phase, participants' independent decisions aligned with the bias when no counterfactual explanations were provided before, but reversed the bias when explanations were given. Reported trust did not differ significantly across conditions. Confidence varied throughout the study phases after exposure to male-biased AI, indicating nuanced effects of AI bias on decision certainty. Our findings point to the importance of calibrating XAI to avoid unintended behavioral shifts in order to safeguard equitable decision-making and prevent the adoption of algorithmic bias.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted for XAI2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14377v1",
    "published_date": "2025-05-20 14:00:28 UTC",
    "updated_date": "2025-05-20 14:00:28 UTC"
  },
  {
    "arxiv_id": "2505.14366v1",
    "title": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds",
    "authors": [
      "Joel Currie",
      "Gioele Migno",
      "Enrico Piacenti",
      "Maria Elena Giannaccini",
      "Patric Bach",
      "Davide De Tommaso",
      "Agnieszka Wykowska"
    ],
    "abstract": "We present a conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), a core capability for embodied cognition essential for Human-Robot Interaction (HRI). As a first step toward this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, a natural language description, and a ground-truth 4X4 transformation matrix representing object pose. We focus on inferring Z-axis distance as a foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as a foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to: Intelligent Autonomous Systems (IAS) 2025 as Late Breaking Report",
    "pdf_url": "https://arxiv.org/pdf/2505.14366v1",
    "published_date": "2025-05-20 13:49:09 UTC",
    "updated_date": "2025-05-20 13:49:09 UTC"
  },
  {
    "arxiv_id": "2505.14351v3",
    "title": "FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation",
    "authors": [
      "Yutong Liu",
      "Ziyue Zhang",
      "Ban Ma-bao",
      "Yuqing Cai",
      "Yongbin Yu",
      "Renzeng Duojie",
      "Xiangxiang Wang",
      "Fan Gao",
      "Cheng Huang",
      "Nyima Tashi"
    ],
    "abstract": "Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-Ü-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "18 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.14351v3",
    "published_date": "2025-05-20 13:35:55 UTC",
    "updated_date": "2025-08-20 07:13:27 UTC"
  },
  {
    "arxiv_id": "2505.14349v2",
    "title": "Upgrading Democracies with Fairer Voting Methods",
    "authors": [
      "Evangelos Pournaras",
      "Srijoni Majumdar",
      "Thomas Wellings",
      "Joshua C. Yang",
      "Fatemeh B. Heravan",
      "Regula Hänggli Fricker",
      "Dirk Helbing"
    ],
    "abstract": "Voting methods are instrumental design elements of democracies. Citizens use them to express and aggregate their preferences to reach a collective decision. However, voting outcomes can be as sensitive to voting rules as they are to people's voting choices. Despite significance and interdisciplinary scientific progress, several democracies keep relying on outdated voting methods that do not fit modern, pluralistic societies well, while lacking social innovation. Here, we demonstrate how one can upgrade real-world democracies, namely by using alternative preferential voting methods such as cumulative voting and the method of equal shares designed for a proportional representation of voters' preferences. We rigorously evaluate the striking voting outcomes of these fair voting methods in a new participatory budgeting approach applied in the city of Aarau, Switzerland, including past and follow-up evidence. Results show more winning projects with the same budget. They also show broader geographic and preference representation of citizens by the elected projects, in particular for voters who used to be under-represented. We provide causal evidence showing that citizens prefer proportional voting methods, which possess strong legitimacy without the need of very specialized technical explanations. We also reveal strong underlying democratic values exhibited by citizens who support fair voting methods such as altruism and compromise. These findings come with the momentum to unleash a new and long-awaited participation blueprint of how to upgrade democracies globally.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.CY",
    "comment": "Includes Supplementary Information",
    "pdf_url": "https://arxiv.org/pdf/2505.14349v2",
    "published_date": "2025-05-20 13:31:43 UTC",
    "updated_date": "2025-12-18 21:27:40 UTC"
  },
  {
    "arxiv_id": "2505.14345v1",
    "title": "Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights",
    "authors": [
      "Aydin Abedinia",
      "Shima Tabakhi",
      "Vahid Seydi"
    ],
    "abstract": "Recent advancements in semi-supervised deep learning have introduced effective strategies for leveraging both labeled and unlabeled data to improve classification performance. This work proposes a semi-supervised framework that utilizes a distance-based weighting mechanism to prioritize critical training samples based on their proximity to test data. By focusing on the most informative examples, the method enhances model generalization and robustness, particularly in challenging scenarios with noisy or imbalanced datasets. Building on techniques such as uncertainty consistency and graph-based representations, the approach addresses key challenges of limited labeled data while maintaining scalability. Experiments on twelve benchmark datasets demonstrate significant improvements across key metrics, including accuracy, precision, and recall, consistently outperforming existing methods. This framework provides a robust and practical solution for semi-supervised learning, with potential applications in domains such as healthcare and security where data limitations pose significant challenges.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 6 figures. This paper has been accepted for publication and oral presentation at the 2025 10th IEEE International Conference on Machine Learning Technologies (ICMLT 2025). The final authenticated version will be available in IEEE Xplore following the conference",
    "pdf_url": "https://arxiv.org/pdf/2505.14345v1",
    "published_date": "2025-05-20 13:29:04 UTC",
    "updated_date": "2025-05-20 13:29:04 UTC"
  },
  {
    "arxiv_id": "2505.14341v1",
    "title": "Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image",
    "authors": [
      "Sifan Li",
      "Ming Tao",
      "Hao Zhao",
      "Ling Shao",
      "Hao Tang"
    ],
    "abstract": "Text-to-Image (T2I) has been prevalent in recent years, with most common condition tasks having been optimized nicely. Besides, counterfactual Text-to-Image is obstructing us from a more versatile AIGC experience. For those scenes that are impossible to happen in real world and anti-physics, we should spare no efforts in increasing the factual feel, which means synthesizing images that people think very likely to be happening, and concept alignment, which means all the required objects should be in the same frame. In this paper, we focus on concept alignment. As controllable T2I models have achieved satisfactory performance for real applications, we utilize this technology to replace the objects in a synthesized image in latent space step-by-step to change the image from a common scene to a counterfactual scene to meet the prompt. We propose a strategy to instruct this replacing process, which is called as Explicit Logical Narrative Prompt (ELNP), by using the newly SoTA language model DeepSeek to generate the instructions. Furthermore, to evaluate models' performance in counterfactual T2I, we design a metric to calculate how many required concepts in the prompt can be covered averagely in the synthesized images. The extensive experiments and qualitative comparisons demonstrate that our strategy can boost the concept alignment in counterfactual T2I.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14341v1",
    "published_date": "2025-05-20 13:27:52 UTC",
    "updated_date": "2025-05-20 13:27:52 UTC"
  },
  {
    "arxiv_id": "2505.14330v1",
    "title": "Handloom Design Generation Using Generative Networks",
    "authors": [
      "Rajat Kanti Bhattacharjee",
      "Meghali Nandi",
      "Amrit Jha",
      "Gunajit Kalita",
      "Ferdous Ahmed Barbhuiya"
    ],
    "abstract": "This paper proposes deep learning techniques of generating designs for clothing, focused on handloom fabric and discusses the associated challenges along with its application. The capability of generative neural network models in understanding artistic designs and synthesizing those is not yet explored well. In this work, multiple methods are employed incorporating the current state of the art generative models and style transfer algorithms to study and observe their performance for the task. The results are then evaluated through user score. This work also provides a new dataset NeuralLoom for the task of the design generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14330v1",
    "published_date": "2025-05-20 13:16:55 UTC",
    "updated_date": "2025-05-20 13:16:55 UTC"
  },
  {
    "arxiv_id": "2505.14751v1",
    "title": "Self Distillation via Iterative Constructive Perturbations",
    "authors": [
      "Maheak Dave",
      "Aniket Kumar Singh",
      "Aryan Pareek",
      "Harshita Jha",
      "Debasis Chaudhuri",
      "Manish Pratap Singh"
    ],
    "abstract": "Deep Neural Networks have achieved remarkable achievements across various domains, however balancing performance and generalization still remains a challenge while training these networks. In this paper, we propose a novel framework that uses a cyclic optimization strategy to concurrently optimize the model and its input data for better training, rethinking the traditional training paradigm. Central to our approach is Iterative Constructive Perturbation (ICP), which leverages the model's loss to iteratively perturb the input, progressively constructing an enhanced representation over some refinement steps. This ICP input is then fed back into the model to produce improved intermediate features, which serve as a target in a self-distillation framework against the original features. By alternately altering the model's parameters to the data and the data to the model, our method effectively addresses the gap between fitting and generalization, leading to enhanced performance. Extensive experiments demonstrate that our approach not only mitigates common performance bottlenecks in neural networks but also demonstrates significant improvements across training variations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14751v1",
    "published_date": "2025-05-20 13:15:27 UTC",
    "updated_date": "2025-05-20 13:15:27 UTC"
  },
  {
    "arxiv_id": "2505.14316v1",
    "title": "Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion",
    "authors": [
      "Tiehan Cui",
      "Yanxu Mao",
      "Peipei Liu",
      "Congying Liu",
      "Datao You"
    ],
    "abstract": "Although large language models (LLMs) have achieved remarkable advancements, their security remains a pressing concern. One major threat is jailbreak attacks, where adversarial prompts bypass model safeguards to generate harmful or objectionable content. Researchers study jailbreak attacks to understand security and robustness of LLMs. However, existing jailbreak attack methods face two main challenges: (1) an excessive number of iterative queries, and (2) poor generalization across models. In addition, recent jailbreak evaluation datasets focus primarily on question-answering scenarios, lacking attention to text generation tasks that require accurate regeneration of toxic content. To tackle these challenges, we propose two contributions: (1) ICE, a novel black-box jailbreak method that employs Intent Concealment and divErsion to effectively circumvent security constraints. ICE achieves high attack success rates (ASR) with a single query, significantly improving efficiency and transferability across different models. (2) BiSceneEval, a comprehensive dataset designed for assessing LLM robustness in question-answering and text-generation tasks. Experimental results demonstrate that ICE outperforms existing jailbreak techniques, revealing critical vulnerabilities in current defense mechanisms. Our findings underscore the necessity of a hybrid security strategy that integrates predefined security mechanisms with real-time semantic decomposition to enhance the security of LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14316v1",
    "published_date": "2025-05-20 13:03:15 UTC",
    "updated_date": "2025-05-20 13:03:15 UTC"
  },
  {
    "arxiv_id": "2505.14312v1",
    "title": "MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains",
    "authors": [
      "Kyungeun Lee",
      "Moonjung Eo",
      "Hye-Seung Cho",
      "Dongmin Kim",
      "Ye Seul Sim",
      "Seoyoon Kim",
      "Min-Kook Suh",
      "Woohyung Lim"
    ],
    "abstract": "Despite the widespread use of tabular data in real-world applications, most benchmarks rely on average-case metrics, which fail to reveal how model behavior varies across diverse data regimes. To address this, we propose MultiTab, a benchmark suite and evaluation framework for multi-dimensional, data-aware analysis of tabular learning algorithms. Rather than comparing models only in aggregate, MultiTab categorizes 196 publicly available datasets along key data characteristics, including sample size, label imbalance, and feature interaction, and evaluates 13 representative models spanning a range of inductive biases. Our analysis shows that model performance is highly sensitive to such regimes: for example, models using sample-level similarity excel on datasets with large sample sizes or high inter-feature correlation, while models encoding inter-feature dependencies perform best with weakly correlated features. These findings reveal that inductive biases do not always behave as intended, and that regime-aware evaluation is essential for understanding and improving model behavior. MultiTab enables more principled model design and offers practical guidance for selecting models tailored to specific data characteristics. All datasets, code, and optimization logs are publicly available at https://huggingface.co/datasets/LGAI-DILab/Multitab.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2505.14312v1",
    "published_date": "2025-05-20 13:00:43 UTC",
    "updated_date": "2025-05-20 13:00:43 UTC"
  },
  {
    "arxiv_id": "2505.14300v1",
    "title": "SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors",
    "authors": [
      "Maheep Chaudhary",
      "Fazl Barez"
    ],
    "abstract": "High-risk industries like nuclear and aviation use real-time monitoring to detect dangerous system conditions. Similarly, Large Language Models (LLMs) need monitoring safeguards. We propose a real-time framework to predict harmful AI outputs before they occur by using an unsupervised approach that treats normal behavior as the baseline and harmful outputs as outliers. Our study focuses specifically on backdoor-triggered responses -- where specific input phrases activate hidden vulnerabilities causing the model to generate unsafe content like violence, pornography, or hate speech. We address two key challenges: (1) identifying true causal indicators rather than surface correlations, and (2) preventing advanced models from deception -- deliberately evading monitoring systems. Hence, we approach this problem from an unsupervised lens by drawing parallels to human deception: just as humans exhibit physical indicators while lying, we investigate whether LLMs display distinct internal behavioral signatures when generating harmful content. Our study addresses two critical challenges: 1) designing monitoring systems that capture true causal indicators rather than superficial correlations; and 2)preventing intentional evasion by increasingly capable \"Future models''. Our findings show that models can produce harmful content through causal mechanisms and can become deceptive by: (a) alternating between linear and non-linear representations, and (b) modifying feature relationships. To counter this, we developed Safety-Net -- a multi-detector framework that monitors different representation dimensions, successfully detecting harmful behavior even when information is shifted across representational spaces to evade individual monitors. Our evaluation shows 96% accuracy in detecting harmful cases using our unsupervised ensemble approach.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14300v1",
    "published_date": "2025-05-20 12:49:58 UTC",
    "updated_date": "2025-05-20 12:49:58 UTC"
  },
  {
    "arxiv_id": "2505.14295v2",
    "title": "Benchmarking data encoding methods in Quantum Machine Learning",
    "authors": [
      "Orlane Zang",
      "Grégoire Barrué",
      "Tony Quertier"
    ],
    "abstract": "Data encoding plays a fundamental and distinctive role in Quantum Machine Learning (QML). While classical approaches process data directly as vectors, QML may require transforming classical data into quantum states through encoding circuits, known as quantum feature maps or quantum embeddings. This step leverages the inherently high-dimensional and non-linear nature of Hilbert space, enabling more efficient data separation in complex feature spaces that may be inaccessible to classical methods. This encoding part significantly affects the performance of the QML model, so it is important to choose the right encoding method for the dataset to be encoded. However, this choice is generally arbitrary, since there is no \"universal\" rule for knowing which encoding to choose based on a specific set of data. There are currently a variety of encoding methods using different quantum logic gates. We studied the most commonly used types of encoding methods and benchmarked them using different datasets.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "30 pages, accepted at the QUEST-IS'25 conference",
    "pdf_url": "https://arxiv.org/pdf/2505.14295v2",
    "published_date": "2025-05-20 12:44:14 UTC",
    "updated_date": "2025-12-10 09:10:14 UTC"
  },
  {
    "arxiv_id": "2505.14289v1",
    "title": "EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection",
    "authors": [
      "Yijie Lu",
      "Tianjie Ju",
      "Manman Zhao",
      "Xinbei Ma",
      "Yuan Guo",
      "ZhuoSheng Zhang"
    ],
    "abstract": "As multimodal agents are increasingly trained to operate graphical user interfaces (GUIs) to complete user tasks, they face a growing threat from indirect prompt injection, attacks in which misleading instructions are embedded into the agent's visual environment, such as popups or chat messages, and misinterpreted as part of the intended task. A typical example is environmental injection, in which GUI elements are manipulated to influence agent behavior without directly modifying the user prompt. To address these emerging attacks, we propose EVA, a red teaming framework for indirect prompt injection which transforms the attack into a closed loop optimization by continuously monitoring an agent's attention distribution over the GUI and updating adversarial cues, keywords, phrasing, and layout, in response. Compared with prior one shot methods that generate fixed prompts without regard for how the model allocates visual attention, EVA dynamically adapts to emerging attention hotspots, yielding substantially higher attack success rates and far greater transferability across diverse GUI scenarios. We evaluate EVA on six widely used generalist and specialist GUI agents in realistic settings such as popup manipulation, chat based phishing, payments, and email composition. Experimental results show that EVA substantially improves success rates over static baselines. Under goal agnostic constraints, where the attacker does not know the agent's task intent, EVA still discovers effective patterns. Notably, we find that injection styles transfer well across models, revealing shared behavioral biases in GUI agents. These results suggest that evolving indirect prompt injection is a powerful tool not only for red teaming agents, but also for uncovering common vulnerabilities in their multimodal decision making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14289v1",
    "published_date": "2025-05-20 12:41:05 UTC",
    "updated_date": "2025-05-20 12:41:05 UTC"
  },
  {
    "arxiv_id": "2505.17082v1",
    "title": "GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data",
    "authors": [
      "Abderrahman Skiredj",
      "Ferdaous Azhari",
      "Houdaifa Atou",
      "Nouamane Tazi",
      "Ismail Berrada"
    ],
    "abstract": "Open-source large language models (LLMs) still marginalise Moroccan Arabic (Darija), forcing practitioners either to bolt on heavyweight Arabic adapters or to sacrifice the very reasoning skills that make LLMs useful. We show that a rigorously quality-over-quantity alignment strategy can surface fluent Darija while safeguarding the backbone s cross-lingual reasoning at a sliver of the usual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6 K and TULU 50 K into Darija, preserve 20 of the English originals, and add mathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on 5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the reasoning-dense TULU portion pushes it to 47.5 with no English regression. Scaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which matches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense, scoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc retains Gemma-27B s strong maths and general-reasoning ability, showing only minimal movement on GSM8K and English benchmarks. The entire model is trained in just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable language technology. We release code, data and checkpoints to spur Darija-centric applications in education, public services and everyday digital interaction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17082v1",
    "published_date": "2025-05-20 12:38:42 UTC",
    "updated_date": "2025-05-20 12:38:42 UTC"
  },
  {
    "arxiv_id": "2505.14285v1",
    "title": "AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis",
    "authors": [
      "Eirini Panteli",
      "Paulo E. Santos",
      "Nabil Humphrey"
    ],
    "abstract": "This paper presents AquaSignal, a modular and scalable pipeline for preprocessing, denoising, classification, and novelty detection of underwater acoustic signals. Designed to operate effectively in noisy and dynamic marine environments, AquaSignal integrates state-of-the-art deep learning architectures to enhance the reliability and accuracy of acoustic signal analysis. The system is evaluated on a combined dataset from the Deepship and Ocean Networks Canada (ONC) benchmarks, providing a diverse set of real-world underwater scenarios. AquaSignal employs a U-Net architecture for denoising, a ResNet18 convolutional neural network for classifying known acoustic events, and an AutoEncoder-based model for unsupervised detection of novel or anomalous signals. To our knowledge, this is the first comprehensive study to apply and evaluate this combination of techniques on maritime vessel acoustic data. Experimental results show that AquaSignal improves signal clarity and task performance, achieving 71% classification accuracy and 91% accuracy in novelty detection. Despite slightly lower classification performance compared to some state-of-the-art models, differences in data partitioning strategies limit direct comparisons. Overall, AquaSignal demonstrates strong potential for real-time underwater acoustic monitoring in scientific, environmental, and maritime domains.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "8 pages; 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.14285v1",
    "published_date": "2025-05-20 12:35:43 UTC",
    "updated_date": "2025-05-20 12:35:43 UTC"
  },
  {
    "arxiv_id": "2505.14279v2",
    "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
    "authors": [
      "Jennifer D'Souza",
      "Hamed Babaei Giglou",
      "Quentin Münch"
    ],
    "abstract": "Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 4 figures, Accepted as a Long Paper at the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.14279v2",
    "published_date": "2025-05-20 12:30:46 UTC",
    "updated_date": "2025-05-29 16:45:00 UTC"
  },
  {
    "arxiv_id": "2505.14273v1",
    "title": "X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning",
    "authors": [
      "Hiroki Shiraishi",
      "Hisao Ishibuchi",
      "Masaya Nakata"
    ],
    "abstract": "Function approximation is a critical task in various fields. However, existing neural network approaches struggle with locally complex or discontinuous functions due to their reliance on a single global model covering the entire problem space. We propose X-KAN, a novel method that optimizes multiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary rule-based machine learning framework called XCSF. X-KAN combines KAN's high expressiveness with XCSF's adaptive partitioning capability by implementing local KAN models as rule consequents and defining local regions via rule antecedents. Our experimental results on artificial test functions and real-world datasets demonstrate that X-KAN significantly outperforms conventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms of approximation accuracy. Notably, X-KAN effectively handles functions with locally complex or discontinuous structures that are challenging for conventional KAN, using a compact set of rules (average 7.2 $\\pm$ 2.3 rules). These results validate the effectiveness of using KAN as a local model in XCSF, which evaluates the rule fitness based on both accuracy and generality. Our X-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "cs.SC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by the 34th International Joint Conference on Artificial Intelligence (IJCAI 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.14273v1",
    "published_date": "2025-05-20 12:26:03 UTC",
    "updated_date": "2025-05-20 12:26:03 UTC"
  },
  {
    "arxiv_id": "2506.11028v1",
    "title": "Enhancing Epidemic Forecasting: Evaluating the Role of Mobility Data and Graph Convolutional Networks",
    "authors": [
      "Suhan Guo",
      "Zhenghao Xu",
      "Furao Shen",
      "Jian Zhao"
    ],
    "abstract": "Accurate prediction of contagious disease outbreaks is vital for informed decision-making. Our study addresses the gap between machine learning algorithms and their epidemiological applications, noting that methods optimal for benchmark datasets often underperform with real-world data due to difficulties in incorporating mobility information. We adopt a two-phase approach: first, assessing the significance of mobility data through a pilot study, then evaluating the impact of Graph Convolutional Networks (GCNs) on a transformer backbone. Our findings reveal that while mobility data and GCN modules do not significantly enhance forecasting performance, the inclusion of mortality and hospitalization data markedly improves model accuracy. Additionally, a comparative analysis between GCN-derived spatial maps and lockdown orders suggests a notable correlation, highlighting the potential of spatial maps as sensitive indicators for mobility. Our research offers a novel perspective on mobility representation in predictive modeling for contagious diseases, empowering decision-makers to better prepare for future outbreaks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11028v1",
    "published_date": "2025-05-20 12:23:18 UTC",
    "updated_date": "2025-05-20 12:23:18 UTC"
  },
  {
    "arxiv_id": "2505.14268v2",
    "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge",
    "authors": [
      "Hui Huang",
      "Yancheng He",
      "Hongli Zhou",
      "Rui Zhang",
      "Wei Liu",
      "Weixun Wang",
      "Jiaheng Liu",
      "Wenbo Su"
    ],
    "abstract": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses generated by Large Language Models (LLMs), which is of significant importance for both LLM evaluation and reward modeling. Although generative LLMs have made substantial progress in various tasks, their performance as LLM-Judge still falls short of expectations. In this work, we propose Think-J, which improves generative LLM-as-a-Judge by learning how to think. We first utilized a small amount of curated data to develop the model with initial judgment thinking capabilities. Subsequently, we optimize the judgment thinking traces based on reinforcement learning (RL). We propose two methods for judgment thinking optimization, based on offline and online RL, respectively. The offline method requires training a critic model to construct positive and negative examples for learning. The online method defines rule-based reward as feedback for optimization. Experimental results showed that our approach can significantly enhance the evaluation capability of generative LLM-Judge, surpassing both generative and classifier-based LLM-Judge without requiring extra human annotations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by AAAI2026",
    "pdf_url": "https://arxiv.org/pdf/2505.14268v2",
    "published_date": "2025-05-20 12:19:10 UTC",
    "updated_date": "2026-01-10 08:01:18 UTC"
  },
  {
    "arxiv_id": "2505.14260v1",
    "title": "Speculative Decoding Reimagined for Multimodal Large Language Models",
    "authors": [
      "Luxi Lin",
      "Zhihang Lin",
      "Zhanpeng Zeng",
      "Rongrong Ji"
    ],
    "abstract": "This paper introduces Multimodal Speculative Decoding (MSD) to accelerate Multimodal Large Language Models (MLLMs) inference. Speculative decoding has been shown to accelerate Large Language Models (LLMs) without sacrificing accuracy. However, current speculative decoding methods for MLLMs fail to achieve the same speedup as they do for LLMs. To address this, we reimagine speculative decoding specifically for MLLMs. Our analysis of MLLM characteristics reveals two key design principles for MSD: (1) Text and visual tokens have fundamentally different characteristics and need to be processed separately during drafting. (2) Both language modeling ability and visual perception capability are crucial for the draft model. For the first principle, MSD decouples text and visual tokens in the draft model, allowing each to be handled based on its own characteristics. For the second principle, MSD uses a two-stage training strategy: In stage one, the draft model is trained on text-only instruction-tuning datasets to improve its language modeling ability. In stage two, MSD gradually introduces multimodal data to enhance the visual perception capability of the draft model. Experiments show that MSD boosts inference speed by up to $2.29\\times$ for LLaVA-1.5-7B and up to $2.46\\times$ for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness. Our code is available at https://github.com/Lyn-Lucy/MSD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.14260v1",
    "published_date": "2025-05-20 12:12:17 UTC",
    "updated_date": "2025-05-20 12:12:17 UTC"
  },
  {
    "arxiv_id": "2505.14256v1",
    "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation",
    "authors": [
      "Shaolin Zhu",
      "Tianyu Dong",
      "Bo Li",
      "Deyi Xiong"
    ],
    "abstract": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual machine translation model powered by a sparsified large language model (LLM). We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on a massive Chinese corpus and then conduct multilingual fine-tuning on a large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14256v1",
    "published_date": "2025-05-20 12:09:17 UTC",
    "updated_date": "2025-05-20 12:09:17 UTC"
  },
  {
    "arxiv_id": "2505.14252v2",
    "title": "Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks",
    "authors": [
      "Mouad Elaarabi",
      "Domenico Borzacchiello",
      "Philippe Le Bot",
      "Nathan Lauzeral",
      "Sebastien Comas-Cardona"
    ],
    "abstract": "In this work, we explore the integration of Sequence Encoding for Online Parameter Identification with Physics-Informed Neural Networks to create a model that, once trained, can be utilized for real time applications with variable parameters, boundary conditions, and initial conditions. Recently, the combination of PINNs with Sparse Regression has emerged as a method for performing dynamical system identification through supervised learning and sparse regression optimization, while also solving the dynamics using PINNs. However, this approach can be limited by variations in parameters or boundary and initial conditions, requiring retraining of the model whenever changes occur. In this work, we introduce an architecture that employs Deep Sets or Sequence Encoders to encode dynamic parameters, boundary conditions, and initial conditions, using these encoded features as inputs for the PINN, enabling the model to adapt to changes in parameters, BCs, and ICs. We apply this approach to three different problems. First, we analyze the Rossler ODE system, demonstrating the robustness of the model with respect to noise and its ability to generalize. Next, we explore the model's capability in a 2D Navier-Stokes PDE problem involving flow past a cylinder with a parametric sinusoidal inlet velocity function, showing that the model can encode pressure data from a few points to identify the inlet velocity profile and utilize physics to compute velocity and pressure throughout the domain. Finally, we address a 1D heat monitoring problem using real data from the heating of glass fiber and thermoplastic composite plates.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14252v2",
    "published_date": "2025-05-20 12:05:17 UTC",
    "updated_date": "2025-08-22 07:13:06 UTC"
  },
  {
    "arxiv_id": "2505.14246v1",
    "title": "Visual Agentic Reinforcement Fine-Tuning",
    "authors": [
      "Ziyu Liu",
      "Yuhang Zang",
      "Yushan Zou",
      "Zijian Liang",
      "Xiaoyi Dong",
      "Yuhang Cao",
      "Haodong Duan",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "abstract": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native agentic ability to use external tools such as web browsers for searching and writing/executing code for image manipulation to think with images. In the open-source research community, while significant progress has been made in language-only agentic abilities such as function calling and tool integration, the development of multi-modal agentic capabilities that involve truly thinking with images, and their corresponding benchmarks, are still less explored. This work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the ability to browse websites for real-time information updates and write code to manipulate and analyze input images through cropping, rotation, and other image processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT) with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs' agentic search and coding abilities. Our experimental results demonstrate that Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and +10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities. Our findings suggest that Visual-ARFT offers a promising path toward building robust and generalizable multimodal agents.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "project url: https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT",
    "pdf_url": "https://arxiv.org/pdf/2505.14246v1",
    "published_date": "2025-05-20 11:59:25 UTC",
    "updated_date": "2025-05-20 11:59:25 UTC"
  },
  {
    "arxiv_id": "2505.14238v3",
    "title": "ABBA-Adapters: Efficient and Expressive Fine-Tuning of Foundation Models",
    "authors": [
      "Raghav Singhal",
      "Kaustubh Ponkshe",
      "Rohit Vartak",
      "Praneeth Vepakomma"
    ],
    "abstract": "Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce ABBA, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget, a property we validate through matrix reconstruction experiments. Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models. Our code is publicly available at: https://github.com/CERT-Lab/abba.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Raghav Singhal, Kaustubh Ponkshe, and Rohit Vartak contributed equally to this work",
    "pdf_url": "https://arxiv.org/pdf/2505.14238v3",
    "published_date": "2025-05-20 11:43:25 UTC",
    "updated_date": "2025-10-02 16:35:40 UTC"
  },
  {
    "arxiv_id": "2505.14235v1",
    "title": "Toward Embodied AGI: A Review of Embodied AI and the Road Ahead",
    "authors": [
      "Yequan Wang",
      "Aixin Sun"
    ],
    "abstract": "Artificial General Intelligence (AGI) is often envisioned as inherently embodied. With recent advances in robotics and foundational AI models, we stand at the threshold of a new era-one marked by increasingly generalized embodied AI systems. This paper contributes to the discourse by introducing a systematic taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing research and challenges at the foundational stages (L1-L2) and outline the key components required to achieve higher-level capabilities (L3-L5). Building on these insights and existing technologies, we propose a conceptual framework for an L3+ robotic brain, offering both a technical outlook and a foundation for future exploration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14235v1",
    "published_date": "2025-05-20 11:42:26 UTC",
    "updated_date": "2025-05-20 11:42:26 UTC"
  },
  {
    "arxiv_id": "2505.14234v1",
    "title": "Fast and close Shannon entropy approximation",
    "authors": [
      "Illia Horenko",
      "Davide Bassetti",
      "Lukáš Pospíšil"
    ],
    "abstract": "Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy are key components in many tools used in physics, information theory, machine learning (ML) and quantum computing. Besides of the significant amounts of SE computations required in these fields, the singularity of the SE gradient is one of the central mathematical reason inducing the high cost, frequently low robustness and slow convergence of such tools. Here we propose the Fast Entropy Approximation (FEA) - a non-singular rational approximation of Shannon entropy and its gradient that achieves a mean absolute error of $10^{-3}$, which is approximately $20$ times lower than comparable state-of-the-art methods. FEA allows around $50\\%$ faster computation, requiring only $5$ to $6$ elementary computational operations, as compared to tens of elementary operations behind the fastest entropy computation algorithms with table look-ups, bitshifts, or series approximations. On a set of common benchmarks for the feature selection problem in machine learning, we show that the combined effect of fewer elementary operations, low approximation error, and a non-singular gradient allows significantly better model quality and enables ML feature extraction that is two to three orders of magnitude faster and computationally cheaper when incorporating FEA into AI tools.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2505.14234v1",
    "published_date": "2025-05-20 11:41:26 UTC",
    "updated_date": "2025-05-20 11:41:26 UTC"
  },
  {
    "arxiv_id": "2505.14233v2",
    "title": "Mechanistic Fine-tuning for In-context Learning",
    "authors": [
      "Hakaze Cho",
      "Peng Luo",
      "Mariko Kato",
      "Rin Kaenbyou",
      "Naoya Inoue"
    ],
    "abstract": "In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 31 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.14233v2",
    "published_date": "2025-05-20 11:41:21 UTC",
    "updated_date": "2025-09-27 12:52:38 UTC"
  },
  {
    "arxiv_id": "2505.14227v2",
    "title": "VoQA: Visual-only Question Answering",
    "authors": [
      "Jianing An",
      "Luyang Jiang",
      "Jie Luo",
      "Wenjun Wu",
      "Lei Huang"
    ],
    "abstract": "Visual understanding requires interpreting both natural scenes and the textual information that appears within them, motivating tasks such as Visual Question Answering (VQA). However, current VQA benchmarks overlook scenarios with visually embedded questions, whereas advanced agents should be able to see the question without separate text input as humans. We introduce Visual-only Question Answering (VoQA), where both the scene and the question appear within a single image, requiring models to perceive and reason purely through vision. This setting supports more realistic visual understanding and interaction in scenarios where questions or instructions are embedded directly in the visual scene. Evaluations under pure visual-only zero-shot, prompt-guided and OCR-assisted settings show that current models exhibit a clear performance drop compared to traditional VQA. To address this, we investigate question-alignment fine-tuning strategies designed to guide models toward interpreting the visual question prior to reasoning. Leveraging VoQA dataset together with these strategies yields robust vision-only reasoning while preserving cross-task generalization to traditional VQA, reflecting the complementary visual and textual reasoning capabilities fostered through VoQA training. The code and data are publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.14227v2",
    "published_date": "2025-05-20 11:37:49 UTC",
    "updated_date": "2025-11-30 10:20:28 UTC"
  },
  {
    "arxiv_id": "2505.14226v3",
    "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs",
    "authors": [
      "Darpan Aswal",
      "Siddharth D Jaiswal"
    ],
    "abstract": "Recently released LLMs have strong multilingual \\& multimodal capabilities. Model vulnerabilities are exposed using audits and red-teaming efforts. Existing efforts have focused primarily on the English language; thus, models continue to be susceptible to multilingual jailbreaking strategies, especially for multimodal contexts. In this study, we introduce a novel strategy that leverages code-mixing and phonetic perturbations to jailbreak LLMs for both text and image generation tasks. We also present an extension to a current jailbreak-template-based strategy and propose a novel template, showing higher effectiveness than baselines. Our work presents a method to effectively bypass safety filters in LLMs while maintaining interpretability by applying phonetic misspellings to sensitive words in code-mixed prompts. We achieve a 99\\% Attack Success Rate for text generation and 78\\% for image generation, with Attack Relevance Rate of 100\\% for text generation and 96\\% for image generation for the phonetically perturbed code-mixed prompts. Our interpretability experiments reveal that phonetic perturbations impact word tokenization, leading to jailbreak success. Our study motivates increasing the focus towards more generalizable safety alignment for multilingual multimodal models, especially in real-world settings wherein prompts can have misspelt words. \\textit{\\textbf{Warning: This paper contains examples of potentially harmful and offensive content.}}",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14226v3",
    "published_date": "2025-05-20 11:35:25 UTC",
    "updated_date": "2025-10-11 13:22:55 UTC"
  },
  {
    "arxiv_id": "2506.11027v2",
    "title": "From Reasoning to Code: GRPO Optimization for Underrepresented Languages",
    "authors": [
      "Federico Pennino",
      "Bianca Raimondi",
      "Massimo Rondelli",
      "Andrea Gurioli",
      "Maurizio Gabbrielli"
    ],
    "abstract": "Generating accurate and executable code using large language models (LLMs) is challenging for languages with limited public training data compared to popular languages such as Python. This paper introduces a generalizable approach that uses small-scale code versions of the Qwen 2.5 model combined with Group Relative Policy Optimization (GRPO) to enable effective code generation through explicit reasoning steps, which is particularly beneficial for languages with smaller source code databases. Using Prolog as a representative use case -- given its limited online presence -- the initial model faced challenges in generating executable code. After some training steps, the model successfully produces logically consistent and syntactically accurate code by directly integrating reasoning-driven feedback into the reinforcement learning loop. Experimental evaluations using mathematical logic problem benchmarks illustrate significant improvements in reasoning quality, code accuracy, and logical correctness, underscoring the potential of this approach to benefit a wide range of programming languages lacking extensive training resources.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under review",
    "pdf_url": "https://arxiv.org/pdf/2506.11027v2",
    "published_date": "2025-05-20 11:28:48 UTC",
    "updated_date": "2025-06-16 09:41:16 UTC"
  },
  {
    "arxiv_id": "2505.14217v1",
    "title": "Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned",
    "authors": [
      "Jorge Fabila",
      "Lidia Garrucho",
      "Víctor M. Campello",
      "Carlos Martín-Isla",
      "Karim Lekadir"
    ],
    "abstract": "This study explores the use of Federated Learning (FL) for tuberculosis (TB) diagnosis using chest X-rays in low-resource settings across Africa. FL allows hospitals to collaboratively train AI models without sharing raw patient data, addressing privacy concerns and data scarcity that hinder traditional centralized models. The research involved hospitals and research centers in eight African countries. Most sites used local datasets, while Ghana and The Gambia used public ones. The study compared locally trained models with a federated model built across all institutions to evaluate FL's real-world feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces challenges such as poor infrastructure, unreliable internet, limited digital literacy, and weak AI regulations. Some institutions were also reluctant to share model updates due to data control concerns. In conclusion, FL shows strong potential for enabling AI-driven healthcare in underserved regions, but broader adoption will require improvements in infrastructure, education, and regulatory support.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14217v1",
    "published_date": "2025-05-20 11:23:52 UTC",
    "updated_date": "2025-05-20 11:23:52 UTC"
  },
  {
    "arxiv_id": "2505.14216v2",
    "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning",
    "authors": [
      "Minwu Kim",
      "Anubhav Shrestha",
      "Safal Shrestha",
      "Aadim Nepal",
      "Keith Ross"
    ],
    "abstract": "Recent studies have shown that reinforcement learning with verifiable rewards (RLVR) enhances overall accuracy (pass@1) but often fails to improve capability (pass@k) of LLMs in reasoning tasks, while distillation can improve both. In this paper, we investigate the mechanisms behind these phenomena. First, we demonstrate that RLVR struggles to improve capability as it focuses on improving the accuracy of the easier questions to the detriment of the accuracy of the most difficult questions. Second, we show that RLVR does not merely increase the success probability for the easier questions, but in our small model settings, produces quality responses that were absent in its original output distribution. In addition, we show these responses are neither noticeably longer nor feature more reflection-related keywords, underscoring the need for more reliable indicators of response quality. Third, from the experiment distilling teacher responses to in-distribution problems, we find that capability does not always improve with distillation. We conjecture that capability improves only when new knowledge is introduced, whereas distilling reasoning patterns only improves accuracy but not capability, sacrificing performance on the most difficult questions, similar to RLVR. Together, these findings offer a clearer understanding of how RLVR and distillation shape reasoning behavior in LLMs",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.14216v2",
    "published_date": "2025-05-20 11:22:34 UTC",
    "updated_date": "2025-10-31 12:44:27 UTC"
  },
  {
    "arxiv_id": "2505.14212v1",
    "title": "Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks",
    "authors": [
      "Sizhe Yuen",
      "Ting Su",
      "Ziyang Wang",
      "Yali Du",
      "Adam J. Sobey"
    ],
    "abstract": "A question-answering (QA) system is to search suitable answers within a knowledge base. Current QA systems struggle with queries requiring complex reasoning or real-time knowledge integration. They are often supplemented with retrieval techniques on a data source such as Retrieval-Augmented Generation (RAG). However, RAG continues to face challenges in handling complex reasoning and logical connections between multiple sources of information. A novel approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA tasks is presented through the automated generation of context-based QA pairs. This methodology leverages LLMs to create fine-tuning data, reducing reliance on human labelling and improving model comprehension and reasoning capabilities. The proposed system includes an automated QA generator and a model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore. Comprehensive experiments demonstrate improvements in logical coherence and factual accuracy, with implications for developing adaptable Artificial Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1, BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA pairs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14212v1",
    "published_date": "2025-05-20 11:16:29 UTC",
    "updated_date": "2025-05-20 11:16:29 UTC"
  },
  {
    "arxiv_id": "2505.14209v1",
    "title": "Embedded Mean Field Reinforcement Learning for Perimeter-defense Game",
    "authors": [
      "Li Wang",
      "Xin Yu",
      "Xuxin Lv",
      "Gangzheng Ai",
      "Wenjun Wu"
    ],
    "abstract": "With the rapid advancement of unmanned aerial vehicles (UAVs) and missile technologies, perimeter-defense game between attackers and defenders for the protection of critical regions have become increasingly complex and strategically significant across a wide range of domains. However, existing studies predominantly focus on small-scale, simplified two-dimensional scenarios, often overlooking realistic environmental perturbations, motion dynamics, and inherent heterogeneity--factors that pose substantial challenges to real-world applicability. To bridge this gap, we investigate large-scale heterogeneous perimeter-defense game in a three-dimensional setting, incorporating realistic elements such as motion dynamics and wind fields. We derive the Nash equilibrium strategies for both attackers and defenders, characterize the victory regions, and validate our theoretical findings through extensive simulations. To tackle large-scale heterogeneous control challenges in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC) framework. EMFAC leverages representation learning to enable high-level action aggregation in a mean-field manner, supporting scalable coordination among defenders. Furthermore, we introduce a lightweight agent-level attention mechanism based on reward representation, which selectively filters observations and mean-field information to enhance decision-making efficiency and accelerate convergence in large-scale tasks. Extensive simulations across varying scales demonstrate the effectiveness and adaptability of EMFAC, which outperforms established baselines in both convergence speed and overall performance. To further validate practicality, we test EMFAC in small-scale real-world experiments and conduct detailed analyses, offering deeper insights into the framework's effectiveness in complex scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14209v1",
    "published_date": "2025-05-20 11:11:46 UTC",
    "updated_date": "2025-05-20 11:11:46 UTC"
  },
  {
    "arxiv_id": "2505.14206v2",
    "title": "Challenges and Limitations of Generative AI in Synthesizing Wearable Sensor Data",
    "authors": [
      "Flavio Di Martino",
      "Franca Delmastro"
    ],
    "abstract": "The widespread adoption of wearable sensors has the potential to provide massive and heterogeneous time series data, driving the use of Artificial Intelligence in human sensing applications. However, data collection remains limited due to stringent ethical regulations, privacy concerns, and other constraints, hindering progress in the field. Synthetic data generation, particularly through Generative Adversarial Networks and Diffusion Models, has emerged as a promising solution to mitigate both data scarcity and privacy issues. However, these models are often limited to narrow operational scenarios, such as short-term and unimodal signal patterns. To address this gap, we present a systematic evaluation of state-of-the-art generative models for time series data, explicitly assessing their performance in challenging scenarios such as stress and emotion recognition. Our study examines the extent to which these models can jointly handle multi-modality, capture long-range dependencies, and support conditional generation-core requirements for real-world wearable sensor data generation. To enable a fair and rigorous comparison, we also introduce an evaluation framework that evaluates both the intrinsic fidelity of the generated data and their utility in downstream predictive tasks. Our findings reveal critical limitations in the existing approaches, particularly in maintaining cross-modal consistency, preserving temporal coherence, and ensuring robust performance in train-on-synthetic, test-on-real, and data augmentation scenarios. Finally, we present our future research directions to enhance synthetic time series generation and improve the applicability of generative models in the wearable computing domain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14206v2",
    "published_date": "2025-05-20 11:05:06 UTC",
    "updated_date": "2025-12-03 11:31:15 UTC"
  },
  {
    "arxiv_id": "2505.14201v1",
    "title": "FLASH-D: FlashAttention with Hidden Softmax Division",
    "authors": [
      "Kosmas Alexandridis",
      "Vasileios Titopoulos",
      "Giorgos Dimitrakopoulos"
    ],
    "abstract": "The transformer's attention mechanism has revolutionized AI and machine learning, with its efficient computation being crucial to its performance. However, calculating attention involves matrix operations interspersed with softmax rescaling, which inherently slows down computation and requires processing the entire input sequence. Building on online softmax computation, FlashAttention integrates softmax calculation with matrix arithmetic, enabling tiled computation independent of sequence length. While optimized for GPUs, FlashAttention's simplicity makes it amenable to direct hardware acceleration. This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a mathematically equivalent, yet simplified, formulation that achieves: (a) hiding softmax division within other non-linear function evaluations; (b) inherently numerically stable computation of exponentials, eliminating the need for maximum value subtraction; and (c) a reduction in computational cost without introducing numerical approximations to the FlashAttention kernel. Importantly, the essential FlashAttention properties that facilitate efficient tiled implementation are fully preserved. Hardware implementation results at 28nm demonstrate that this proposed formulation achieves a 22.8% reduction in area and a 20.3% reduction in power, on average, compared to state-of-the-art parallel hardware architectures without any performance penalty.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED) 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14201v1",
    "published_date": "2025-05-20 11:01:33 UTC",
    "updated_date": "2025-05-20 11:01:33 UTC"
  },
  {
    "arxiv_id": "2505.15854v1",
    "title": "Integration of TinyML and LargeML: A Survey of 6G and Beyond",
    "authors": [
      "Thai-Hoc Vu",
      "Ngo Hoang Tu",
      "Thien Huynh-The",
      "Kyungchun Lee",
      "Sunghwan Kim",
      "Miroslav Voznak",
      "Quoc-Viet Pham"
    ],
    "abstract": "The transition from 5G networks to 6G highlights a significant demand for machine learning (ML). Deep learning models, in particular, have seen wide application in mobile networking and communications to support advanced services in emerging wireless environments, such as smart healthcare, smart grids, autonomous vehicles, aerial platforms, digital twins, and the metaverse. The rapid expansion of Internet-of-Things (IoT) devices, many with limited computational capabilities, has accelerated the development of tiny machine learning (TinyML) and resource-efficient ML approaches for cost-effective services. However, the deployment of large-scale machine learning (LargeML) solutions require major computing resources and complex management strategies to support extensive IoT services and ML-generated content applications. Consequently, the integration of TinyML and LargeML is projected as a promising approach for future seamless connectivity and efficient resource management.\n  Although the integration of TinyML and LargeML shows abundant potential, several challenges persist, including performance optimization, practical deployment strategies, effective resource management, and security considerations. In this survey, we review and analyze the latest research aimed at enabling the integration of TinyML and LargeML models for the realization of smart services and applications in future 6G networks and beyond. The paper concludes by outlining critical challenges and identifying future research directions for the holistic integration of TinyML and LargeML in next-generation wireless networks.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.NI",
    "comment": "This work was submitted to IEEE Communications Surveys & Tutorials",
    "pdf_url": "https://arxiv.org/pdf/2505.15854v1",
    "published_date": "2025-05-20 10:54:39 UTC",
    "updated_date": "2025-05-20 10:54:39 UTC"
  },
  {
    "arxiv_id": "2505.14193v1",
    "title": "Dynamic Replanning for Improved Public Transport Routing",
    "authors": [
      "Abdallah Abuaisha",
      "Bojie Shen",
      "Daniel Harabor",
      "Peter Stuckey",
      "Mark Wallace"
    ],
    "abstract": "Delays in public transport are common, often impacting users through prolonged travel times and missed transfers. Existing solutions for handling delays remain limited; backup plans based on historical data miss opportunities for earlier arrivals, while snapshot planning accounts for current delays but not future ones. With the growing availability of live delay data, users can adjust their journeys in real-time. However, the literature lacks a framework that fully exploits this advantage for system-scale dynamic replanning. To address this, we formalise the dynamic replanning problem in public transport routing and propose two solutions: a \"pull\" approach, where users manually request replanning, and a novel \"push\" approach, where the server proactively monitors and adjusts journeys. Our experiments show that the push approach outperforms the pull approach, achieving significant speedups. The results also reveal substantial arrival time savings enabled by dynamic replanning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at IJCAI 2025. 8 pages, 4 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.14193v1",
    "published_date": "2025-05-20 10:50:58 UTC",
    "updated_date": "2025-05-20 10:50:58 UTC"
  },
  {
    "arxiv_id": "2505.14190v1",
    "title": "$α$-GAN by Rényi Cross Entropy",
    "authors": [
      "Ni Ding",
      "Miao Qiao",
      "Jiaxing Xu",
      "Yiping Ke",
      "Xiaoyu Zhang"
    ],
    "abstract": "This paper proposes $α$-GAN, a generative adversarial network using Rényi measures. The value function is formulated, by Rényi cross entropy, as an expected certainty measure incurred by the discriminator's soft decision as to where the sample is from, true population or the generator. The discriminator tries to maximize the Rényi certainty about sample source, while the generator wants to reduce it by injecting fake samples. This forms a min-max problem with the solution parameterized by the Rényi order $α$. This $α$-GAN reduces to vanilla GAN at $α= 1$, where the value function is exactly the binary cross entropy. The optimization of $α$-GAN is over probability (vector) space. It is shown that the gradient is exponentially enlarged when Rényi order is in the range $α\\in (0,1)$. This makes convergence faster, which is verified by experimental results. A discussion shows that choosing $α\\in (0,1)$ may be able to solve some common problems, e.g., vanishing gradient. A following observation reveals that this range has not been fully explored in the existing Rényi version GANs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14190v1",
    "published_date": "2025-05-20 10:45:11 UTC",
    "updated_date": "2025-05-20 10:45:11 UTC"
  },
  {
    "arxiv_id": "2505.14185v2",
    "title": "Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study",
    "authors": [
      "Kaustubh Ponkshe",
      "Shaan Shah",
      "Raghav Singhal",
      "Praneeth Vepakomma"
    ],
    "abstract": "Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. However, this behavior is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this perspective. We examine whether safety-relevant behavior is concentrated in specific linear subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in activations. Across both weight and activation spaces, our findings are consistent: subspaces that amplify safe behaviors also amplify useful ones, and prompts with different safety implications activate overlapping representations. Rather than residing in distinct directions, we show that safety is highly entangled with the general learning components of the model. This suggests that subspace-based defenses face fundamental limitations and underscores the need for alternative strategies to preserve safety under continued training. We corroborate these findings with multiple experiments on five open-source LLMs from the Llama and Qwen families. Our code is publicly available at: https://github.com/CERT-Lab/safety-subspaces.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally to this work",
    "pdf_url": "https://arxiv.org/pdf/2505.14185v2",
    "published_date": "2025-05-20 10:41:49 UTC",
    "updated_date": "2025-10-04 10:37:32 UTC"
  },
  {
    "arxiv_id": "2505.14179v1",
    "title": "Enhancing Abstractive Summarization of Scientific Papers Using Structure Information",
    "authors": [
      "Tong Bao",
      "Heng Zhang",
      "Chengzhi Zhang"
    ],
    "abstract": "Abstractive summarization of scientific papers has always been a research focus, yet existing methods face two main challenges. First, most summarization models rely on Encoder-Decoder architectures that treat papers as sequences of words, thus fail to fully capture the structured information inherent in scientific papers. Second, existing research often use keyword mapping or feature engineering to identify the structural information, but these methods struggle with the structural flexibility of scientific papers and lack robustness across different disciplines. To address these challenges, we propose a two-stage abstractive summarization framework that leverages automatic recognition of structural functions within scientific papers. In the first stage, we standardize chapter titles from numerous scientific papers and construct a large-scale dataset for structural function recognition. A classifier is then trained to automatically identify the key structural components (e.g., Background, Methods, Results, Discussion), which provides a foundation for generating more balanced summaries. In the second stage, we employ Longformer to capture rich contextual relationships across sections and generating context-aware summaries. Experiments conducted on two domain-specific scientific paper summarization datasets demonstrate that our method outperforms advanced baselines, and generates more comprehensive summaries. The code and dataset can be accessed at https://github.com/tongbao96/code-for-SFR-AS.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14179v1",
    "published_date": "2025-05-20 10:34:45 UTC",
    "updated_date": "2025-05-20 10:34:45 UTC"
  },
  {
    "arxiv_id": "2505.14178v1",
    "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits",
    "authors": [
      "Xiang Zhang",
      "Juntai Cao",
      "Jiaqi Wei",
      "Yiwei Xu",
      "Chenyu You"
    ],
    "abstract": "Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14178v1",
    "published_date": "2025-05-20 10:32:30 UTC",
    "updated_date": "2025-05-20 10:32:30 UTC"
  },
  {
    "arxiv_id": "2506.11025v1",
    "title": "When Algorithms Play Favorites: Lookism in the Generation and Perception of Faces",
    "authors": [
      "Miriam Doh",
      "Aditya Gulati",
      "Matei Mancas",
      "Nuria Oliver"
    ],
    "abstract": "This paper examines how synthetically generated faces and machine learning-based gender classification algorithms are affected by algorithmic lookism, the preferential treatment based on appearance. In experiments with 13,200 synthetically generated faces, we find that: (1) text-to-image (T2I) systems tend to associate facial attractiveness to unrelated positive traits like intelligence and trustworthiness; and (2) gender classification models exhibit higher error rates on \"less-attractive\" faces, especially among non-White women. These result raise fairness concerns regarding digital identity systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as an extended abstract at the Fourth European Workshop on Algorithmic Fairness (EWAF) (URL: https://2025.ewaf.org/home)",
    "pdf_url": "https://arxiv.org/pdf/2506.11025v1",
    "published_date": "2025-05-20 10:21:51 UTC",
    "updated_date": "2025-05-20 10:21:51 UTC"
  },
  {
    "arxiv_id": "2505.14163v1",
    "title": "DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation",
    "authors": [
      "He Wang",
      "Alexander Hanbo Li",
      "Yiqun Hu",
      "Sheng Zhang",
      "Hideo Kobayashi",
      "Jiani Zhang",
      "Henry Zhu",
      "Chung-Wei Hang",
      "Patrick Ng"
    ],
    "abstract": "Large language model (LLM) agents have shown promising performance in generating code for solving complex data science problems. Recent studies primarily focus on enhancing in-context learning through improved search, sampling, and planning techniques, while overlooking the importance of the order in which problems are tackled during inference. In this work, we develop a novel inference-time optimization framework, referred to as DSMentor, which leverages curriculum learning -- a strategy that introduces simpler task first and progressively moves to more complex ones as the learner improves -- to enhance LLM agent performance in challenging data science tasks. Our mentor-guided framework organizes data science tasks in order of increasing difficulty and incorporates a growing long-term memory to retain prior experiences, guiding the agent's learning progression and enabling more effective utilization of accumulated knowledge. We evaluate DSMentor through extensive experiments on DSEval and QRData benchmarks. Experiments show that DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval and QRData compared to baseline agents. Furthermore, DSMentor demonstrates stronger causal reasoning ability, improving the pass rate by 8.8% on the causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our work underscores the importance of developing effective strategies for accumulating and utilizing knowledge during inference, mirroring the human learning process and opening new avenues for improving LLM performance through curriculum-based inference optimization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14163v1",
    "published_date": "2025-05-20 10:16:21 UTC",
    "updated_date": "2025-05-20 10:16:21 UTC"
  },
  {
    "arxiv_id": "2505.14157v2",
    "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning",
    "authors": [
      "Pittawat Taveekitworachai",
      "Potsawee Manakul",
      "Sarana Nutanong",
      "Kunat Pipatanakul"
    ],
    "abstract": "This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2025, Main; 26 pages, 42 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.14157v2",
    "published_date": "2025-05-20 10:05:11 UTC",
    "updated_date": "2025-09-10 04:33:47 UTC"
  },
  {
    "arxiv_id": "2505.14156v1",
    "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search",
    "authors": [
      "Songhao Wu",
      "Quan Tu",
      "Hong Liu",
      "Jia Xu",
      "Zhongyi Liu",
      "Guannan Zhang",
      "Ran Wang",
      "Xiuying Chen",
      "Rui Yan"
    ],
    "abstract": "Session search involves a series of interactive queries and actions to fulfill user's complex information need. Current strategies typically prioritize sequential modeling for deep semantic understanding, overlooking the graph structure in interactions. While some approaches focus on capturing structural information, they use a generalized representation for documents, neglecting the word-level semantic modeling. In this paper, we propose Symbolic Graph Ranker (SGR), which aims to take advantage of both text-based and graph-based approaches by leveraging the power of recent Large Language Models (LLMs). Concretely, we first introduce a set of symbolic grammar rules to convert session graph into text. This allows integrating session history, interaction process, and task instruction seamlessly as inputs for the LLM. Moreover, given the natural discrepancy between LLMs pre-trained on textual corpora, and the symbolic language we produce using our graph-to-text grammar, our objective is to enhance LLMs' ability to capture graph structures within a textual format. To achieve this, we introduce a set of self-supervised symbolic learning tasks including link prediction, node content generation, and generative contrastive learning, to enable LLMs to capture the topological information from coarse-grained to fine-grained. Experiment results and comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm the superiority of our approach. Our paradigm also offers a novel and effective methodology that bridges the gap between traditional search strategies and modern LLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14156v1",
    "published_date": "2025-05-20 10:05:06 UTC",
    "updated_date": "2025-05-20 10:05:06 UTC"
  },
  {
    "arxiv_id": "2505.14148v1",
    "title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem",
    "authors": [
      "Fan Liu",
      "Zherui Yang",
      "Cancheng Liu",
      "Tianrui Song",
      "Xiaofeng Gao",
      "Hao Liu"
    ],
    "abstract": "Mathematical modeling is a cornerstone of scientific discovery and engineering practice, enabling the translation of real-world problems into formal systems across domains such as physics, biology, and economics. Unlike mathematical reasoning, which assumes a predefined formulation, modeling requires open-ended problem analysis, abstraction, and principled formalization. While Large Language Models (LLMs) have shown strong reasoning capabilities, they fall short in rigorous model construction, limiting their utility in real-world problem-solving. To this end, we formalize the task of LLM-powered real-world mathematical modeling, where agents must analyze problems, construct domain-appropriate formulations, and generate complete end-to-end solutions. We introduce MM-Bench, a curated benchmark of 111 problems from the Mathematical Contest in Modeling (MCM/ICM), spanning the years 2000 to 2025 and across ten diverse domains such as physics, biology, and economics. To tackle this task, we propose MM-Agent, an expert-inspired framework that decomposes mathematical modeling into four stages: open-ended problem analysis, structured model formulation, computational problem solving, and report generation. Experiments on MM-Bench show that MM-Agent significantly outperforms baseline agents, achieving an 11.88\\% improvement over human expert solutions while requiring only 15 minutes and \\$0.88 per task using GPT-4o. Furthermore, under official MCM/ICM protocols, MM-Agent assisted two undergraduate teams in winning the Finalist Award (\\textbf{top 2.0\\% among 27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a modeling copilot. Our code is available at https://github.com/usail-hkust/LLM-MM-Agent",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14148v1",
    "published_date": "2025-05-20 09:55:31 UTC",
    "updated_date": "2025-05-20 09:55:31 UTC"
  },
  {
    "arxiv_id": "2505.14147v3",
    "title": "SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning",
    "authors": [
      "Xiong Jun Wu",
      "Zhenduo Zhang",
      "ZuJie Wen",
      "Zhiqiang Zhang",
      "Wang Ren",
      "Lei Shi",
      "Cai Chen",
      "Deng Zhao",
      "Qing Wang",
      "Xudong Han",
      "Chengfu Tang",
      "Dingnan Jin",
      "Qing Cui",
      "Jun Zhou"
    ],
    "abstract": "Training large reasoning models (LRMs) with reinforcement learning in STEM domains is hindered by the scarcity of high-quality, diverse, and verifiable problem sets. Existing synthesis methods, such as Chain-of-Thought prompting, often generate oversimplified or uncheckable data, limiting model advancement on complex tasks. To address these challenges, we introduce SHARP, a unified approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a strategic set of self-alignment principles -- targeting graduate and Olympiad-level difficulty, rigorous logical consistency, and unambiguous, verifiable answers -- and a structured three-phase framework (Alignment, Instantiation, Inference) that ensures thematic diversity and fine-grained control over problem generation. We implement SHARP by leveraging a state-of-the-art LRM to infer and verify challenging STEM questions, then employ a reinforcement learning loop to refine the model's reasoning through verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate that SHARP-augmented training substantially outperforms existing methods, markedly improving complex reasoning accuracy and pushing LRM performance closer to expert-level proficiency. Our contributions include the SHARP strategy, framework design, end-to-end implementation, and experimental evaluation of its effectiveness in elevating LRM reasoning capabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14147v3",
    "published_date": "2025-05-20 09:54:42 UTC",
    "updated_date": "2025-05-25 15:45:21 UTC"
  },
  {
    "arxiv_id": "2505.14146v2",
    "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL",
    "authors": [
      "Pengcheng Jiang",
      "Xueqiang Xu",
      "Jiacheng Lin",
      "Jinfeng Xiao",
      "Zifeng Wang",
      "Jimeng Sun",
      "Jiawei Han"
    ],
    "abstract": "Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP 2025 camera-ready",
    "pdf_url": "https://arxiv.org/pdf/2505.14146v2",
    "published_date": "2025-05-20 09:53:56 UTC",
    "updated_date": "2025-11-05 01:58:35 UTC"
  },
  {
    "arxiv_id": "2505.14143v1",
    "title": "Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition",
    "authors": [
      "Shuo Zhang",
      "Jinsong Zhang",
      "Zhejun Zhang",
      "Lei Li"
    ],
    "abstract": "Multi-task learning (MTL) enables the efficient transfer of extra knowledge acquired from other tasks. The high correlation between multimodal sentiment analysis (MSA) and multimodal emotion recognition (MER) supports their joint training. However, existing methods primarily employ hard parameter sharing, ignoring parameter conflicts caused by complex task correlations. In this paper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture of Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts to distinctly model common and unique task characteristics, thereby avoiding parameter conflicts. Additionally, inspired by low-rank structures in the Mixture of Experts (MoE) framework, we design low-rank expert networks to reduce parameter and computational overhead as the number of experts increases. Extensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that MMoLRE achieves state-of-the-art performance on the MSA task and competitive results on the MER task.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ICME 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14143v1",
    "published_date": "2025-05-20 09:46:56 UTC",
    "updated_date": "2025-05-20 09:46:56 UTC"
  },
  {
    "arxiv_id": "2505.14141v1",
    "title": "Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent",
    "authors": [
      "Fanglin Mo",
      "Junzhe Chen",
      "Haoxuan Zhu",
      "Xuming Hu"
    ],
    "abstract": "Mobile GUI agents execute user commands by directly interacting with the graphical user interface (GUI) of mobile devices, demonstrating significant potential to enhance user convenience. However, these agents face considerable challenges in task planning, as they must continuously analyze the GUI and generate operation instructions step by step. This process often leads to difficulties in making accurate task plans, as GUI agents lack a deep understanding of how to effectively use the target applications, which can cause them to become \"lost\" during task execution. To address the task planning issue, we propose SPlanner, a plug-and-play planning module to generate execution plans that guide vision language model(VLMs) in executing tasks. The proposed planning module utilizes extended finite state machines (EFSMs) to model the control logits and configurations of mobile applications. It then decomposes a user instruction into a sequence of primary function modeled in EFSMs, and generate the execution path by traversing the EFSMs. We further refine the execution path into a natural language plan using an LLM. The final plan is concise and actionable, and effectively guides VLMs to generate interactive GUI actions to accomplish user tasks. SPlanner demonstrates strong performance on dynamic benchmarks reflecting real-world mobile usage. On the AndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired with Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point improvement compared to using Qwen2.5-VL-72B without planning assistance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages. Submitted to EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14141v1",
    "published_date": "2025-05-20 09:45:55 UTC",
    "updated_date": "2025-05-20 09:45:55 UTC"
  },
  {
    "arxiv_id": "2505.14140v2",
    "title": "RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning",
    "authors": [
      "Qianyue Hao",
      "Sibo Li",
      "Jian Yuan",
      "Yong Li"
    ],
    "abstract": "Despite rapid advancements in large language models (LLMs), the token-level autoregressive nature constrains their complex reasoning capabilities. To enhance LLM reasoning, inference-time techniques, including Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they are fairly cost-effective by guiding reasoning through sophisticated logical structures without modifying LLMs' parameters. However, these manually predefined, task-agnostic frameworks are applied uniformly across diverse tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT), where we train a lightweight navigator model with reinforcement learning (RL) to adaptively enhance LLM reasoning at inference time. Specifically, we design five basic logic blocks from the perspective of human cognition. During the reasoning process, the trained RL navigator dynamically selects the suitable logic blocks and combines them into task-specific logical structures according to problem characteristics. Experiments across multiple reasoning benchmarks (AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek) illustrate that RLoT outperforms established inference-time techniques by up to 13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL navigator demonstrates strong transferability: a model trained on one specific LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for reproducibility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14140v2",
    "published_date": "2025-05-20 09:43:33 UTC",
    "updated_date": "2025-09-25 11:00:05 UTC"
  },
  {
    "arxiv_id": "2505.14139v1",
    "title": "FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning",
    "authors": [
      "Marvin Alles",
      "Nutan Chen",
      "Patrick van der Smagt",
      "Botond Cseke"
    ],
    "abstract": "The use of guidance to steer sampling toward desired outcomes has been widely explored within diffusion models, especially in applications such as image and trajectory generation. However, incorporating guidance during training remains relatively underexplored. In this work, we introduce energy-guided flow matching, a novel approach that enhances the training of flow models and eliminates the need for guidance at inference time. We learn a conditional velocity field corresponding to the flow policy by approximating an energy-guided probability path as a Gaussian path. Learning guided trajectories is appealing for tasks where the target distribution is defined by a combination of data and an energy function, as in reinforcement learning. Diffusion-based policies have recently attracted attention for their expressive power and ability to capture multi-modal action distributions. Typically, these policies are optimized using weighted objectives or by back-propagating gradients through actions sampled by the policy. As an alternative, we propose FlowQ, an offline reinforcement learning algorithm based on energy-guided flow matching. Our method achieves competitive performance while the policy training time is constant in the number of flow sampling steps.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14139v1",
    "published_date": "2025-05-20 09:43:05 UTC",
    "updated_date": "2025-05-20 09:43:05 UTC"
  },
  {
    "arxiv_id": "2505.14137v1",
    "title": "Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games",
    "authors": [
      "Vojtěch Kůr",
      "Vít Musil",
      "Vojtěch Řehák"
    ],
    "abstract": "Adversarial Patrolling games form a subclass of Security games where a Defender moves between locations, guarding vulnerable targets. The main algorithmic problem is constructing a strategy for the Defender that minimizes the worst damage an Attacker can cause. We focus on the class of finite-memory (also known as regular) Defender's strategies that experimentally outperformed other competing classes. A finite-memory strategy can be seen as a positional strategy on a finite set of states. Each state consists of a pair of a location and a certain integer value--called memory. Existing algorithms improve the transitional probabilities between the states but require that the available memory size itself is assigned at each location manually. Choosing the right memory assignment is a well-known open and hard problem that hinders the usability of finite-memory strategies. We solve this issue by developing a general method that iteratively changes the memory assignment. Our algorithm can be used in connection with \\emph{any} black-box strategy optimization tool. We evaluate our method on various experiments and show its robustness by solving instances of various patrolling models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14137v1",
    "published_date": "2025-05-20 09:40:53 UTC",
    "updated_date": "2025-05-20 09:40:53 UTC"
  },
  {
    "arxiv_id": "2505.14136v2",
    "title": "Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging",
    "authors": [
      "Ryo Bertolissi",
      "Jonas Hübotter",
      "Ido Hakimi",
      "Andreas Krause"
    ],
    "abstract": "Mixture of expert (MoE) models are a promising approach to increasing model capacity without increasing inference cost, and are core components of many state-of-the-art language models. However, current MoE models typically use only few experts due to prohibitive training and inference cost. We propose Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of magnitude more experts and uses model merging to avoid almost any test-time overhead. We show that TTMM is an approximation of test-time training (TTT), which fine-tunes an expert model for each prediction task, i.e., prompt. TTT has recently been shown to significantly improve language models, but is computationally expensive. We find that performance of TTMM improves with more experts and approaches the performance of TTT. Moreover, we find that with a 1B parameter base model, TTMM is more than 100x faster than TTT at test-time by amortizing the cost of TTT at train-time. Thus, TTMM offers a promising cost-effective approach to scale test-time training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14136v2",
    "published_date": "2025-05-20 09:39:54 UTC",
    "updated_date": "2025-07-30 13:53:32 UTC"
  },
  {
    "arxiv_id": "2505.14128v1",
    "title": "A Methodological Framework for Measuring Spatial Labeling Similarity",
    "authors": [
      "Yihang Du",
      "Jiaying Hu",
      "Suyang Hou",
      "Yueyang Ding",
      "Xiaobo Sun"
    ],
    "abstract": "Spatial labeling assigns labels to specific spatial locations to characterize their spatial properties and relationships, with broad applications in scientific research and practice. Measuring the similarity between two spatial labelings is essential for understanding their differences and the contributing factors, such as changes in location properties or labeling methods. An adequate and unbiased measurement of spatial labeling similarity should consider the number of matched labels (label agreement), the topology of spatial label distribution, and the heterogeneous impacts of mismatched labels. However, existing methods often fail to account for all these aspects. To address this gap, we propose a methodological framework to guide the development of methods that meet these requirements. Given two spatial labelings, the framework transforms them into graphs based on location organization, labels, and attributes (e.g., location significance). The distributions of their graph attributes are then extracted, enabling an efficient computation of distributional discrepancy to reflect the dissimilarity level between the two labelings. We further provide a concrete implementation of this framework, termed Spatial Labeling Analogy Metric (SLAM), along with an analysis of its theoretical foundation, for evaluating spatial labeling results in spatial transcriptomics (ST) \\textit{as per} their similarity with ground truth labeling. Through a series of carefully designed experimental cases involving both simulated and real ST data, we demonstrate that SLAM provides a comprehensive and accurate reflection of labeling quality compared to other well-established evaluation metrics. Our code is available at https://github.com/YihDu/SLAM.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14128v1",
    "published_date": "2025-05-20 09:34:03 UTC",
    "updated_date": "2025-05-20 09:34:03 UTC"
  },
  {
    "arxiv_id": "2505.14125v2",
    "title": "Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning",
    "authors": [
      "Viet Anh Khoa Tran",
      "Emre Neftci",
      "Willem A. M. Wybo"
    ],
    "abstract": "Biological brains learn continually from a stream of unlabeled data, while integrating specialized information from sparsely labeled examples without compromising their ability to generalize. Meanwhile, machine learning methods are susceptible to catastrophic forgetting in this natural learning setting, as supervised specialist fine-tuning degrades performance on the original task. We introduce task-modulated contrastive learning (TMCL), which takes inspiration from the biophysical machinery in the neocortex, using predictive coding principles to integrate top-down information continually and without supervision. We follow the idea that these principles build a view-invariant representation space, and that this can be implemented using a contrastive loss. Then, whenever labeled samples of a new class occur, new affine modulations are learned that improve separation of the new class from all others, without affecting feedforward weights. By co-opting the view-invariance learning mechanism, we then train feedforward weights to match the unmodulated representation of a data sample to its modulated counterparts. This introduces modulation invariance into the representation space, and, by also using past modulations, stabilizes it. Our experiments show improvements in both class-incremental and transfer learning over state-of-the-art unsupervised approaches, as well as over comparable supervised approaches, using as few as 1% of available labels. Taken together, our work suggests that top-down modulations play a crucial role in balancing stability and plasticity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025. Camera-ready version. 33 pages, 5 figures. Code available at: https://github.com/tran-khoa/tmcl",
    "pdf_url": "https://arxiv.org/pdf/2505.14125v2",
    "published_date": "2025-05-20 09:31:57 UTC",
    "updated_date": "2025-11-04 14:37:03 UTC"
  },
  {
    "arxiv_id": "2505.14117v2",
    "title": "Collaborative Unlabeled Data Optimization",
    "authors": [
      "Xinyi Shang",
      "Peng Sun",
      "Fengyuan Liu",
      "Tao Lin"
    ],
    "abstract": "This paper pioneers a novel data-centric paradigm to maximize the utility of unlabeled data, tackling a critical question: How can we enhance the efficiency and sustainability of deep learning training by optimizing the data itself? We begin by identifying three key limitations in existing model-centric approaches, all rooted in a shared bottleneck: knowledge extracted from data is locked to model parameters, hindering its reusability and scalability. To this end, we propose CoOpt, a highly efficient, parallelized framework for collaborative unlabeled data optimization, thereby effectively encoding knowledge into the data itself. By distributing unlabeled data and leveraging publicly available task-agnostic models, CoOpt facilitates scalable, reusable, and sustainable training pipelines. Extensive experiments across diverse datasets and architectures demonstrate its efficacy and efficiency, achieving 13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively, with training speedups of $1.94 \\times $ and $1.2 \\times$.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14117v2",
    "published_date": "2025-05-20 09:21:40 UTC",
    "updated_date": "2025-10-10 12:30:40 UTC"
  },
  {
    "arxiv_id": "2506.11024v3",
    "title": "Not All Clients Are Equal: Collaborative Model Personalization on Heterogeneous Multi-Modal Clients",
    "authors": [
      "Minhyuk Seo",
      "Taeheon Kim",
      "Hankook Lee",
      "Jonghyun Choi",
      "Tinne Tuytelaars"
    ],
    "abstract": "As AI becomes more personal, e.g., Agentic AI, there is an increasing need for personalizing models for various use cases. Personalized federated learning (PFL) enables each client to collaboratively leverage other clients' knowledge for better adaptation to the task of interest, without privacy risks. Despite its potential, existing PFL methods remain confined to rather simplified scenarios where data and models are the same across clients. To move towards realistic scenarios, we propose FedMosaic, a method that jointly addresses data and model heterogeneity with a task-relevance-aware model aggregation strategy to reduce parameter interference, and a dimension-invariant module that enables knowledge sharing across heterogeneous architectures without huge computational cost. To mimic the real-world task diversity, we propose a multi-modal PFL benchmark spanning 40 distinct tasks with distribution shifts over time. The empirical study shows that FedMosaic outperforms the state-of-the-art PFL methods, excelling in both personalization and generalization capabilities under challenging, realistic scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11024v3",
    "published_date": "2025-05-20 09:17:07 UTC",
    "updated_date": "2025-11-04 09:20:05 UTC"
  },
  {
    "arxiv_id": "2505.14107v4",
    "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models",
    "authors": [
      "Yakun Zhu",
      "Zhongzhen Huang",
      "Linjie Mu",
      "Yutong Huang",
      "Wei Nie",
      "Jiaji Liu",
      "Shaoting Zhang",
      "Pengfei Liu",
      "Xiaofan Zhang"
    ],
    "abstract": "The emergence of groundbreaking large language models capable of performing complex reasoning tasks holds significant promise for addressing various scientific challenges, including those arising in complex clinical scenarios. To enable their safe and effective deployment in real-world healthcare settings, it is urgently necessary to benchmark the diagnostic capabilities of current models systematically. Given the limitations of existing medical benchmarks in evaluating advanced diagnostic reasoning, we present DiagnosisArena, a comprehensive and challenging benchmark designed to rigorously assess professional-level diagnostic competence. DiagnosisArena consists of 1,113 pairs of segmented patient cases and corresponding diagnoses, spanning 28 medical specialties, deriving from clinical case reports published in 10 top-tier medical journals. The benchmark is developed through a meticulous construction pipeline, involving multiple rounds of screening and review by both AI systems and human experts, with thorough checks conducted to prevent data leakage. Our study reveals that even the most advanced reasoning models, o3, o1, and DeepSeek-R1, achieve only 51.12%, 31.09%, and 17.79% accuracy, respectively. This finding highlights a significant generalization bottleneck in current large language models when faced with clinical diagnostic reasoning challenges. Through DiagnosisArena, we aim to drive further advancements in AI's diagnostic reasoning capabilities, enabling more effective solutions for real-world clinical diagnostic challenges. We provide the benchmark and evaluation tools for further research and development https://github.com/SPIRAL-MED/DiagnosisArena.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14107v4",
    "published_date": "2025-05-20 09:14:53 UTC",
    "updated_date": "2025-05-29 08:24:00 UTC"
  },
  {
    "arxiv_id": "2505.14106v2",
    "title": "A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations",
    "authors": [
      "Li Li",
      "Peilin Cai",
      "Ryan A. Rossi",
      "Franck Dernoncourt",
      "Branislav Kveton",
      "Junda Wu",
      "Tong Yu",
      "Linxin Song",
      "Tiankai Yang",
      "Yuehan Qin",
      "Nesreen K. Ahmed",
      "Samyadeep Basu",
      "Subhojyoti Mukherjee",
      "Ruiyi Zhang",
      "Zhengmian Hu",
      "Bo Ni",
      "Yuxiao Zhou",
      "Zichao Wang",
      "Yue Huang",
      "Yu Wang",
      "Xiangliang Zhang",
      "Philip S. Yu",
      "Xiyang Hu",
      "Yue Zhao"
    ],
    "abstract": "We present PersonaConvBench, a large-scale benchmark for evaluating personalized reasoning and generation in multi-turn conversations with large language models (LLMs). Unlike existing work that focuses on either personalization or conversational structure in isolation, PersonaConvBench integrates both, offering three core tasks: sentence classification, impact regression, and user-centric text generation across ten diverse Reddit-based domains. This design enables systematic analysis of how personalized conversational context shapes LLM outputs in realistic multi-user scenarios. We benchmark several commercial and open-source LLMs under a unified prompting setup and observe that incorporating personalized history yields substantial performance improvements, including a 198 percent relative gain over the best non-conversational baseline in sentiment classification. By releasing PersonaConvBench with evaluations and code, we aim to support research on LLMs that adapt to individual styles, track long-term context, and produce contextually rich, engaging responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14106v2",
    "published_date": "2025-05-20 09:13:22 UTC",
    "updated_date": "2025-05-25 18:28:20 UTC"
  },
  {
    "arxiv_id": "2505.14103v2",
    "title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models",
    "authors": [
      "Guangke Chen",
      "Fu Song",
      "Zhe Zhao",
      "Xiaojun Jia",
      "Yang Liu",
      "Yanchen Qiao",
      "Weizhe Zhang"
    ],
    "abstract": "Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website https://audiojailbreak.github.io/AudioJailbreak.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14103v2",
    "published_date": "2025-05-20 09:10:45 UTC",
    "updated_date": "2025-05-21 03:36:20 UTC"
  },
  {
    "arxiv_id": "2505.14745v2",
    "title": "Explainable Prediction of the Mechanical Properties of Composites with CNNs",
    "authors": [
      "Varun Raaghav",
      "Dimitrios Bikos",
      "Antonio Rago",
      "Francesca Toni",
      "Maria Charalambides"
    ],
    "abstract": "Composites are amongst the most important materials manufactured today, as evidenced by their use in countless applications. In order to establish the suitability of composites in specific applications, finite element (FE) modelling, a numerical method based on partial differential equations, is the industry standard for assessing their mechanical properties. However, FE modelling is exceptionally costly from a computational viewpoint, a limitation which has led to efforts towards applying AI models to this task. However, in these approaches: the chosen model architectures were rudimentary, feed-forward neural networks giving limited accuracy; the studies focused on predicting elastic mechanical properties, without considering material strength limits; and the models lacked transparency, hindering trustworthiness by users. In this paper, we show that convolutional neural networks (CNNs) equipped with methods from explainable AI (XAI) can be successfully deployed to solve this problem. Our approach uses customised CNNs trained on a dataset we generate using transverse tension tests in FE modelling to predict composites' mechanical properties, i.e., Young's modulus and yield strength. We show empirically that our approach achieves high accuracy, outperforming a baseline, ResNet-34, in estimating the mechanical properties. We then use SHAP and Integrated Gradients, two post-hoc XAI methods, to explain the predictions, showing that the CNNs use the critical geometrical features that influence the composites' behaviour, thus allowing engineers to verify that the models are trustworthy by representing the science of composites.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 6 figures. Accepted for publication at The 14th Conference on Prestigious Applications of Intelligent Systems (PAIS-2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.14745v2",
    "published_date": "2025-05-20 08:54:06 UTC",
    "updated_date": "2025-08-25 14:12:44 UTC"
  },
  {
    "arxiv_id": "2505.14080v1",
    "title": "Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory",
    "authors": [
      "Franziska Sofia Hafner",
      "Ana Valdivia",
      "Luc Rocher"
    ],
    "abstract": "Language models encode and subsequently perpetuate harmful gendered stereotypes. Research has succeeded in mitigating some of these harms, e.g. by dissociating non-gendered terms such as occupations from gendered terms such as 'woman' and 'man'. This approach, however, remains superficial given that associations are only one form of prejudice through which gendered harms arise. Critical scholarship on gender, such as gender performativity theory, emphasizes how harms often arise from the construction of gender itself, such as conflating gender with biological sex. In language models, these issues could lead to the erasure of transgender and gender diverse identities and cause harms in downstream applications, from misgendering users to misdiagnosing patients based on wrong assumptions about their anatomy.\n  For FAccT research on gendered harms to go beyond superficial linguistic associations, we advocate for a broader definition of 'gender bias' in language models. We operationalize insights on the construction of gender through language from gender studies literature and then empirically test how 16 language models of different architectures, training datasets, and model sizes encode gender. We find that language models tend to encode gender as a binary category tied to biological sex, and that gendered terms that do not neatly fall into one of these binary categories are erased and pathologized. Finally, we show that larger models, which achieve better results on performance benchmarks, learn stronger associations between gender and sex, further reinforcing a narrow understanding of gender. Our findings lead us to call for a re-evaluation of how gendered harms in language models are defined and addressed.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14080v1",
    "published_date": "2025-05-20 08:36:47 UTC",
    "updated_date": "2025-05-20 08:36:47 UTC"
  },
  {
    "arxiv_id": "2505.17078v1",
    "title": "GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace",
    "authors": [
      "Zenghao Duan",
      "Zhiyi Yin",
      "Zhichao Shi",
      "Liang Pang",
      "Shaoling Jing",
      "Jiayi Wu",
      "Yu Yan",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "abstract": "This paper investigates the underlying mechanisms of toxicity generation in Large Language Models (LLMs) and proposes an effective detoxification approach. Prior work typically considers the Feed-Forward Network (FFN) as the main source of toxicity, representing toxic regions as a set of toxic vectors or layer-wise subspaces. However, our in-depth analysis reveals that the global toxic subspace offers a more effective and comprehensive representation of toxic region within the model. Building on this insight, we propose GloSS (Global Toxic Subspace Suppression), a lightweight, four-stage method that mitigates toxicity by identifying and removing the global toxic subspace from the parameters of FFN. Experiments across a range of LLMs show that GloSS achieves state-of-the-art detoxification performance while preserving the models general capabilities, without requiring large-scale data or model retraining.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17078v1",
    "published_date": "2025-05-20 08:29:11 UTC",
    "updated_date": "2025-05-20 08:29:11 UTC"
  },
  {
    "arxiv_id": "2505.14072v1",
    "title": "Personalized Student Knowledge Modeling for Future Learning Resource Prediction",
    "authors": [
      "Soroush Hashemifar",
      "Sherry Sahebi"
    ],
    "abstract": "Despite advances in deep learning for education, student knowledge tracing and behavior modeling face persistent challenges: limited personalization, inadequate modeling of diverse learning activities (especially non-assessed materials), and overlooking the interplay between knowledge acquisition and behavioral patterns. Practical limitations, such as fixed-size sequence segmentation, frequently lead to the loss of contextual information vital for personalized learning. Moreover, reliance on student performance on assessed materials limits the modeling scope, excluding non-assessed interactions like lectures. To overcome these shortcomings, we propose Knowledge Modeling and Material Prediction (KMaP), a stateful multi-task approach designed for personalized and simultaneous modeling of student knowledge and behavior. KMaP employs clustering-based student profiling to create personalized student representations, improving predictions of future learning resource preferences. Extensive experiments on two real-world datasets confirm significant behavioral differences across student clusters and validate the efficacy of the KMaP model.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14072v1",
    "published_date": "2025-05-20 08:23:50 UTC",
    "updated_date": "2025-05-20 08:23:50 UTC"
  },
  {
    "arxiv_id": "2505.14744v1",
    "title": "Transductively Informed Inductive Program Synthesis",
    "authors": [
      "Janis Zenkner",
      "Tobias Sesterhenn",
      "Christian Bartelt"
    ],
    "abstract": "Abstraction and reasoning in program synthesis has seen significant progress through both inductive and transductive paradigms. Inductive approaches generate a program or latent function from input-output examples, which can then be applied to new inputs. Transductive approaches directly predict output values for given inputs, effectively serving as the function themselves. Current approaches combine inductive and transductive models via isolated ensembling, but they do not explicitly model the interaction between both paradigms. In this work, we introduce \\acs{tiips}, a novel framework that unifies transductive and inductive strategies by explicitly modeling their interactions through a cooperative mechanism: an inductive model generates programs, while a transductive model constrains, guides, and refines the search to improve synthesis accuracy and generalization. We evaluate \\acs{tiips} on two widely studied program synthesis domains: string and list manipulation. Our results show that \\acs{tiips} solves more tasks and yields functions that more closely match optimal solutions in syntax and semantics, particularly in out-of-distribution settings, yielding state-of-the-art performance. We believe that explicitly modeling the synergy between inductive and transductive reasoning opens promising avenues for general-purpose program synthesis and broader applications.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14744v1",
    "published_date": "2025-05-20 08:23:46 UTC",
    "updated_date": "2025-05-20 08:23:46 UTC"
  },
  {
    "arxiv_id": "2506.11023v2",
    "title": "OntoGSN: An Ontology-Based Framework for Semantic Management and Extension of Assurance Cases",
    "authors": [
      "Tomas Bueno Momcilovic",
      "Barbara Gallina",
      "Ingmar Kessler",
      "Jule Hendricks",
      "Dian Balta"
    ],
    "abstract": "Assurance cases (ACs) are a common artifact for building and maintaining confidence in system properties such as safety or robustness. Constructing an AC can be challenging, although existing tools provide support in static, document-centric applications and methods for dynamic contexts (e.g., autonomous driving) are emerging. Unfortunately, managing ACs remains a challenge, since maintaining the embedded knowledge in the face of changes requires substantial effort, in the process deterring developers - or worse, producing poorly managed cases that instill false confidence. To address this, we present OntoGSN: an ontology and supporting middleware for managing ACs in the Goal Structuring Notation (GSN) standard. OntoGSN offers a knowledge representation and a queryable graph that can be automatically populated, evaluated, and updated. Our contributions include: a 1:1 formalization of the GSN Community Standard v3 in an OWL ontology with SWRL rules; a helper ontology and parser for integration with a widely used AC tool; a repository and documentation of design decisions for OntoGSN maintenance; a SPARQL query library with automation patterns; and a prototypical interface. The ontology strictly adheres to the standard's text and has been evaluated according to FAIR principles, the OOPS framework, competency questions, and community feedback. The development of other middleware elements is guided by the community needs and subject to ongoing evaluations. To demonstrate the utility of our contributions, we illustrate dynamic AC management in an example involving assurance of adversarial robustness in large language models.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to the ESWC 2026 Resources track",
    "pdf_url": "https://arxiv.org/pdf/2506.11023v2",
    "published_date": "2025-05-20 08:15:16 UTC",
    "updated_date": "2025-12-19 15:34:46 UTC"
  },
  {
    "arxiv_id": "2505.14064v1",
    "title": "NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI",
    "authors": [
      "Cosmin I. Bercea",
      "Jun Li",
      "Philipp Raffler",
      "Evamaria O. Riedel",
      "Lena Schmitzer",
      "Angela Kurz",
      "Felix Bitzer",
      "Paula Roßmüller",
      "Julian Canisius",
      "Mirjam L. Beyrle",
      "Che Liu",
      "Wenjia Bai",
      "Bernhard Kainz",
      "Julia A. Schnabel",
      "Benedikt Wiestler"
    ],
    "abstract": "In many real-world applications, deployed models encounter inputs that differ from the data seen during training. Out-of-distribution detection identifies whether an input stems from an unseen distribution, while open-world recognition flags such inputs to ensure the system remains robust as ever-emerging, previously $unknown$ categories appear and must be addressed without retraining. Foundation and vision-language models are pre-trained on large and diverse datasets with the expectation of broad generalization across domains, including medical imaging. However, benchmarking these models on test sets with only a few common outlier types silently collapses the evaluation back to a closed-set problem, masking failures on rare or truly novel conditions encountered in clinical use.\n  We therefore present $NOVA$, a challenging, real-life $evaluation-only$ benchmark of $\\sim$900 brain MRI scans that span 281 rare pathologies and heterogeneous acquisition protocols. Each case includes rich clinical narratives and double-blinded expert bounding-box annotations. Together, these enable joint assessment of anomaly localisation, visual captioning, and diagnostic reasoning. Because NOVA is never used for training, it serves as an $extreme$ stress-test of out-of-distribution generalisation: models must bridge a distribution gap both in sample appearance and in semantic space. Baseline results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B) reveal substantial performance drops across all tasks, establishing NOVA as a rigorous testbed for advancing models that can detect, localize, and reason about truly unknown anomalies.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14064v1",
    "published_date": "2025-05-20 08:10:57 UTC",
    "updated_date": "2025-05-20 08:10:57 UTC"
  },
  {
    "arxiv_id": "2505.14057v1",
    "title": "Field Matters: A lightweight LLM-enhanced Method for CTR Prediction",
    "authors": [
      "Yu Cui",
      "Feng Liu",
      "Jiawei Chen",
      "Xingyu Lou",
      "Changwang Zhang",
      "Jun Wang",
      "Yuegang Sun",
      "Xiaohu Yang",
      "Can Wang"
    ],
    "abstract": "Click-through rate (CTR) prediction is a fundamental task in modern recommender systems. In recent years, the integration of large language models (LLMs) has been shown to effectively enhance the performance of traditional CTR methods. However, existing LLM-enhanced methods often require extensive processing of detailed textual descriptions for large-scale instances or user/item entities, leading to substantial computational overhead. To address this challenge, this work introduces LLaCTR, a novel and lightweight LLM-enhanced CTR method that employs a field-level enhancement paradigm. Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight semantic knowledge from small-scale feature fields through self-supervised field-feature fine-tuning. Subsequently, it leverages this field-level semantic knowledge to enhance both feature representation and feature interactions. In our experiments, we integrate LLaCTR with six representative CTR models across four datasets, demonstrating its superior performance in terms of both effectiveness and efficiency compared to existing LLM-enhanced methods. Our code is available at https://anonymous.4open.science/r/LLaCTR-EC46.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14057v1",
    "published_date": "2025-05-20 08:02:41 UTC",
    "updated_date": "2025-05-20 08:02:41 UTC"
  },
  {
    "arxiv_id": "2505.18194v2",
    "title": "Large Language Model-Driven Distributed Integrated Multimodal Sensing and Semantic Communications",
    "authors": [
      "Yubo Peng",
      "Luping Xiang",
      "Bingxin Zhang",
      "Kun Yang"
    ],
    "abstract": "Traditional single-modal sensing systems-based solely on either radio frequency (RF) or visual data-struggle to cope with the demands of complex and dynamic environments. Furthermore, single-device systems are constrained by limited perspectives and insufficient spatial coverage, which impairs their effectiveness in urban or non-line-of-sight scenarios. To overcome these challenges, we propose a novel large language model (LLM)-driven distributed integrated multimodal sensing and semantic communication (LLM-DiSAC) framework. Specifically, our system consists of multiple collaborative sensing devices equipped with RF and camera modules, working together with an aggregation center to enhance sensing accuracy. First, on sensing devices, LLM-DiSAC develops an RF-vision fusion network (RVFN), which employs specialized feature extractors for RF and visual data, followed by a cross-attention module for effective multimodal integration. Second, a LLM-based semantic transmission network (LSTN) is proposed to enhance communication efficiency, where the LLM-based decoder leverages known channel parameters, such as transceiver distance and signal-to-noise ratio (SNR), to mitigate semantic distortion. Third, at the aggregation center, a transformer-based aggregation model (TRAM) with an adaptive aggregation attention mechanism is developed to fuse distributed features and enhance sensing accuracy. To preserve data privacy, a two-stage distributed learning strategy is introduced, allowing local model training at the device level and centralized aggregation model training using intermediate features. Finally, evaluations on a synthetic multi-view RF-visual dataset generated by the Genesis simulation engine show that LLM-DiSAC achieves a good performance.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18194v2",
    "published_date": "2025-05-20 08:00:00 UTC",
    "updated_date": "2025-05-30 08:40:41 UTC"
  },
  {
    "arxiv_id": "2505.15851v1",
    "title": "Exploring Moral Exercises for Human Oversight of AI systems: Insights from Three Pilot Studies",
    "authors": [
      "Silvia Crafa",
      "Teresa Scantamburlo"
    ],
    "abstract": "This paper elaborates on the concept of moral exercises as a means to help AI actors cultivate virtues that enable effective human oversight of AI systems. We explore the conceptual framework and significance of moral exercises, situating them within the contexts of philosophical discourse, ancient practices, and contemporary AI ethics scholarship. We outline the core pillars of the moral exercises methodology - eliciting an engaged personal disposition, fostering relational understanding, and cultivating technomoral wisdom - and emphasize their relevance to key activities and competencies essential for human oversight of AI systems. Our argument is supported by findings from three pilot studies involving a company, a multidisciplinary team of AI researchers, and higher education students. These studies allow us to explore both the potential and the limitations of moral exercises. Based on the collected data, we offer insights into how moral exercises can foster a responsible AI culture within organizations, and suggest directions for future research.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15851v1",
    "published_date": "2025-05-20 07:47:24 UTC",
    "updated_date": "2025-05-20 07:47:24 UTC"
  },
  {
    "arxiv_id": "2505.14045v4",
    "title": "From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora",
    "authors": [
      "Yingli Shen",
      "Wen Lai",
      "Shuo Wang",
      "Ge Gao",
      "Kangyang Luo",
      "Alexander Fraser",
      "Maosong Sun"
    ],
    "abstract": "Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multiway parallel data consistently outperform those trained on unaligned multilingual data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 Main Conference (Oral)",
    "pdf_url": "https://arxiv.org/pdf/2505.14045v4",
    "published_date": "2025-05-20 07:43:45 UTC",
    "updated_date": "2025-10-21 11:04:05 UTC"
  },
  {
    "arxiv_id": "2505.14038v1",
    "title": "ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data",
    "authors": [
      "Xinzhe Zheng",
      "Sijie Ji",
      "Jiawei Sun",
      "Renqi Chen",
      "Wei Gao",
      "Mani Srivastava"
    ],
    "abstract": "Mental health risk is a critical global public health challenge, necessitating innovative and reliable assessment methods. With the development of large language models (LLMs), they stand out to be a promising tool for explainable mental health care applications. Nevertheless, existing approaches predominantly rely on subjective textual mental records, which can be distorted by inherent mental uncertainties, leading to inconsistent and unreliable predictions. To address these limitations, this paper introduces ProMind-LLM. We investigate an innovative approach integrating objective behavior data as complementary information alongside subjective mental records for robust mental health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive pipeline that includes domain-specific pretraining to tailor the LLM for mental health contexts, a self-refine mechanism to optimize the processing of numerical behavioral data, and causal chain-of-thought reasoning to enhance the reliability and interpretability of its predictions. Evaluations of two real-world datasets, PMData and Globem, demonstrate the effectiveness of our proposed methods, achieving substantial improvements over general LLMs. We anticipate that ProMind-LLM will pave the way for more dependable, interpretable, and scalable mental health case solutions.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14038v1",
    "published_date": "2025-05-20 07:36:28 UTC",
    "updated_date": "2025-05-20 07:36:28 UTC"
  },
  {
    "arxiv_id": "2505.14036v4",
    "title": "Adaptive Inference-Time Scaling via Cyclic Diffusion Search",
    "authors": [
      "Gyubin Lee",
      "Truong Nhat Nguyen Bao",
      "Jaesik Yoon",
      "Dongwoo Lee",
      "Minsu Kim",
      "Yoshua Bengio",
      "Sungjin Ahn"
    ],
    "abstract": "Diffusion models have demonstrated strong generative capabilities across domains ranging from image synthesis to complex reasoning tasks. However, most inference-time scaling methods rely on fixed denoising schedules, limiting their ability to allocate computation based on instance difficulty or task-specific demands adaptively. We introduce the challenge of adaptive inference-time scaling-dynamically adjusting computational effort during inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a flexible, search-based inference framework. ABCD refines outputs through bi-directional diffusion cycles while adaptively controlling exploration depth and termination. It comprises three components: Cyclic Diffusion Search, Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time. Experiments show that ABCD improves performance across diverse tasks while maintaining computational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14036v4",
    "published_date": "2025-05-20 07:31:38 UTC",
    "updated_date": "2025-10-24 15:27:49 UTC"
  },
  {
    "arxiv_id": "2505.14029v1",
    "title": "AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards",
    "authors": [
      "Laura-Sophia von Hirschhausen",
      "Jannes S. Magnusson",
      "Mykyta Kovalenko",
      "Fredrik Boye",
      "Tanay Rawat",
      "Peter Eisert",
      "Anna Hilsmann",
      "Sebastian Pretzsch",
      "Sebastian Bosse"
    ],
    "abstract": "Deep learning has transformed computer vision for precision agriculture, yet apple orchard monitoring remains limited by dataset constraints. The lack of diverse, realistic datasets and the difficulty of annotating dense, heterogeneous scenes. Existing datasets overlook different growth stages and stereo imagery, both essential for realistic 3D modeling of orchards and tasks like fruit localization, yield estimation, and structural analysis. To address these gaps, we present AppleGrowthVision, a large-scale dataset comprising two subsets. The first includes 9,317 high resolution stereo images collected from a farm in Brandenburg (Germany), covering six agriculturally validated growth stages over a full growth cycle. The second subset consists of 1,125 densely annotated images from the same farm in Brandenburg and one in Pillnitz (Germany), containing a total of 31,084 apple labels. AppleGrowthVision provides stereo-image data with agriculturally validated growth stages, enabling precise phenological analysis and 3D reconstructions. Extending MinneApple with our data improves YOLOv8 performance by 7.69 % in terms of F1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by 31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy using VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges the gap between agricultural science and computer vision, by enabling the development of robust models for fruit detection, growth modeling, and 3D analysis in precision agriculture. Future work includes improving annotation, enhancing 3D reconstruction, and extending multimodal analysis across all growth stages.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14029v1",
    "published_date": "2025-05-20 07:29:22 UTC",
    "updated_date": "2025-05-20 07:29:22 UTC"
  },
  {
    "arxiv_id": "2505.14027v1",
    "title": "CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data",
    "authors": [
      "Yifan Zeng"
    ],
    "abstract": "As computer networks proliferate, the gravity of network intrusions has escalated, emphasizing the criticality of network intrusion detection systems for safeguarding security. While deep learning models have exhibited promising results in intrusion detection, they face challenges in managing high-dimensional, complex traffic patterns and imbalanced data categories. This paper presents CSAGC-IDS, a network intrusion detection model based on deep learning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced convolutional conditional generative adversarial network that generates high-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS integrates CSCA-CNN, a convolutional neural network enhanced through cost sensitive learning and channel attention mechanism, to extract features from complex traffic data for precise detection. Experiments conducted on the NSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of 84.52% in five-class classification task, and an accuracy of 91.09% and an F1 score of 92.04% in binary classification task.Furthermore, this paper provides an interpretability analysis of the proposed model, using SHAP and LIME to explain the decision-making mechanisms of the model.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14027v1",
    "published_date": "2025-05-20 07:27:51 UTC",
    "updated_date": "2025-05-20 07:27:51 UTC"
  },
  {
    "arxiv_id": "2505.14024v1",
    "title": "FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix",
    "authors": [
      "Di Wu",
      "Qian Li",
      "Heng Yang",
      "Yong Han"
    ],
    "abstract": "Federated Learning (FL) enables geographically distributed clients to collaboratively train machine learning models by sharing only their local models, ensuring data privacy. However, FL is vulnerable to untargeted attacks that aim to degrade the global model's performance on the underlying data distribution. Existing defense mechanisms attempt to improve FL's resilience against such attacks, but their effectiveness is limited in practical FL environments due to data heterogeneity. On the contrary, we aim to detect and remove the attacks to mitigate their impact. Generalization contribution plays a crucial role in distinguishing untargeted attacks. Our observations indicate that, with limited data, the divergence between embeddings representing different classes provides a better measure of generalization than direct accuracy. In light of this, we propose a novel robust aggregation method, FedGraM, designed to defend against untargeted attacks in FL. The server maintains an auxiliary dataset containing one sample per class to support aggregation. This dataset is fed to the local models to extract embeddings. Then, the server calculates the norm of the Gram Matrix of the embeddings for each local model. The norm serves as an indicator of each model's inter-class separation capability in the embedding space. FedGraM identifies and removes potentially malicious models by filtering out those with the largest norms, then averages the remaining local models to form the global model. We conduct extensive experiments to evaluate the performance of FedGraM. Our empirical results show that with limited data samples used to construct the auxiliary dataset, FedGraM achieves exceptional performance, outperforming state-of-the-art defense methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14024v1",
    "published_date": "2025-05-20 07:26:54 UTC",
    "updated_date": "2025-05-20 07:26:54 UTC"
  },
  {
    "arxiv_id": "2505.14020v2",
    "title": "Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning",
    "authors": [
      "Hao Dong",
      "Ziyue Qiao",
      "Zhiyuan Ning",
      "Qi Hao",
      "Yi Du",
      "Pengyang Wang",
      "Yuanchun Zhou"
    ],
    "abstract": "Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs (KGs), incorporate the temporal feature to express the transience of knowledge by describing when facts occur. TKG extrapolation aims to infer possible future facts based on known history, which has garnered significant attention in recent years. Some existing methods treat TKG as a sequence of independent subgraphs to model temporal evolution patterns, demonstrating impressive reasoning performance. However, they still have limitations: 1) In modeling subgraph semantic evolution, they usually neglect the internal structural interactions between subgraphs, which are actually crucial for encoding TKGs. 2) They overlook the potential smooth features that do not lead to semantic changes, which should be distinguished from the semantic evolution process. Therefore, we propose a novel Disentangled Multi-span Evolutionary Network (DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution strategy that captures local neighbor features while perceiving historical neighbor semantic information, thus enabling internal interactions between subgraphs during the evolution process. To maximize the capture of semantic change patterns, we design a disentangle component that adaptively separates nodes' active and stable features, used to dynamically control the influence of historical semantics on future evolution. Extensive experiments conducted on four real-world TKG datasets show that DiMNet demonstrates substantial performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7% in MRR.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ACL 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.14020v2",
    "published_date": "2025-05-20 07:22:03 UTC",
    "updated_date": "2025-05-29 07:45:13 UTC"
  },
  {
    "arxiv_id": "2505.14742v2",
    "title": "Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis",
    "authors": [
      "Hong Huang",
      "Dapeng Wu"
    ],
    "abstract": "Large language models (LLMs) have made exciting achievements across various domains, yet their deployment on resource-constrained personal devices remains hindered by the prohibitive computational and memory demands of task-specific fine-tuning. While quantization offers a pathway to efficiency, existing methods struggle to balance performance and overhead, either incurring high computational/memory costs or failing to address activation outliers, a critical bottleneck in quantized fine-tuning. To address these challenges, we propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning, certain activation outlier channels retain stable spatial positions across training iterations. Building on OSSH, we propose Quaff, a Quantized parameter-efficient fine-tuning framework for LLMs, optimizing low-precision activation representations through targeted momentum scaling. Quaff dynamically suppresses outliers exclusively in invariant channels using lightweight operations, eliminating full-precision weight storage and global rescaling while reducing quantization errors. Extensive experiments across ten benchmarks validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory savings over full-precision fine-tuning while improving accuracy by 0.6% on the Phi-3 model, reconciling the triple trade-off between efficiency, performance, and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080 Super) without sacrificing model utility, Quaff democratizes personalized LLM deployment. The code is available at https://github.com/Little0o0/Quaff.git.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14742v2",
    "published_date": "2025-05-20 07:19:36 UTC",
    "updated_date": "2025-05-29 22:04:36 UTC"
  },
  {
    "arxiv_id": "2505.14005v2",
    "title": "Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks",
    "authors": [
      "Han Zhang",
      "Yan Wang",
      "Guanfeng Liu",
      "Pengfei Ding",
      "Huaxiong Wang",
      "Kwok-Yan Lam"
    ],
    "abstract": "To enhance the reliability and credibility of graph neural networks (GNNs) and improve the transparency of their decision logic, a new field of explainability of GNNs (XGNN) has emerged. However, two major limitations severely degrade the performance and hinder the generalizability of existing XGNN methods: they (a) fail to capture the complete decision logic of GNNs across diverse distributions in the entire dataset's sample space, and (b) impose strict prerequisites on edge properties and GNN internal accessibility. To address these limitations, we propose OPEN, a novel c\\textbf{O}mprehensive and \\textbf{P}rerequisite-free \\textbf{E}xplainer for G\\textbf{N}Ns. OPEN, as the first work in the literature, can infer and partition the entire dataset's sample space into multiple environments, each containing graphs that follow a distinct distribution. OPEN further learns the decision logic of GNNs across different distributions by sampling subgraphs from each environment and analyzing their predictions, thus eliminating the need for strict prerequisites. Experimental results demonstrate that OPEN captures nearly complete decision logic of GNNs, outperforms state-of-the-art methods in fidelity while maintaining similar efficiency, and enhances robustness in real-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IJCAI 2025 AI4Tech Track",
    "pdf_url": "https://arxiv.org/pdf/2505.14005v2",
    "published_date": "2025-05-20 07:01:47 UTC",
    "updated_date": "2025-05-23 07:00:19 UTC"
  },
  {
    "arxiv_id": "2505.14741v2",
    "title": "Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism",
    "authors": [
      "Kunyun Wang",
      "Bohan Li",
      "Kai Yu",
      "Minyi Guo",
      "Jieru Zhao"
    ],
    "abstract": "Diffusion models have emerged as a powerful class of generative models across various modalities, including image, video, and audio synthesis. However, their deployment is often limited by significant inference latency, primarily due to the inherently sequential nature of the denoising process. While existing parallelization strategies attempt to accelerate inference by distributing computation across multiple devices, they typically incur high communication overhead, hindering deployment on commercial hardware. To address this challenge, we propose \\textbf{ParaStep}, a novel parallelization method based on a reuse-then-predict mechanism that parallelizes diffusion inference by exploiting similarity between adjacent denoising steps. Unlike prior approaches that rely on layer-wise or stage-wise communication, ParaStep employs lightweight, step-wise communication, substantially reducing overhead. ParaStep achieves end-to-end speedups of up to \\textbf{3.88}$\\times$ on SVD, \\textbf{2.43}$\\times$ on CogVideoX-2b, and \\textbf{6.56}$\\times$ on AudioLDM2-large, while maintaining generation quality. These results highlight ParaStep as a scalable and communication-efficient solution for accelerating diffusion inference, particularly in bandwidth-constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14741v2",
    "published_date": "2025-05-20 06:58:40 UTC",
    "updated_date": "2025-10-13 06:04:36 UTC"
  },
  {
    "arxiv_id": "2505.14001v1",
    "title": "VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change",
    "authors": [
      "Sterre Lutz",
      "Matthijs T. J. Spaan",
      "Anna Lukina"
    ],
    "abstract": "Autonomous systems operating in the real world encounter a range of uncertainties. Probabilistic neural Lyapunov certification is a powerful approach to proving safety of nonlinear stochastic dynamical systems. When faced with changes beyond the modeled uncertainties, e.g., unidentified obstacles, probabilistic certificates must be transferred to the new system dynamics. However, even when the changes are localized in a known part of the state space, state-of-the-art requires complete re-certification, which is particularly costly for neural certificates. We introduce VeRecycle, the first framework to formally reclaim guarantees for discrete-time stochastic dynamical systems. VeRecycle efficiently reuses probabilistic certificates when the system dynamics deviate only in a given subset of states. We present a general theoretical justification and algorithmic implementation. Our experimental evaluation shows scenarios where VeRecycle both saves significant computational effort and achieves competitive probabilistic guarantees in compositional neural control.",
    "categories": [
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted to IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14001v1",
    "published_date": "2025-05-20 06:54:19 UTC",
    "updated_date": "2025-05-20 06:54:19 UTC"
  },
  {
    "arxiv_id": "2505.13995v2",
    "title": "ELEPHANT: Measuring and understanding social sycophancy in LLMs",
    "authors": [
      "Myra Cheng",
      "Sunny Yu",
      "Cinoo Lee",
      "Pranav Khadpe",
      "Lujain Ibrahim",
      "Dan Jurafsky"
    ],
    "abstract": "LLMs are known to exhibit sycophancy: agreeing with and flattering users, even at the cost of correctness. Prior work measures sycophancy only as direct agreement with users' explicitly stated beliefs that can be compared to a ground truth. This fails to capture broader forms of sycophancy such as affirming a user's self-image or other implicit beliefs. To address this gap, we introduce social sycophancy, characterizing sycophancy as excessive preservation of a user's face (their desired self-image), and present ELEPHANT, a benchmark for measuring social sycophancy in an LLM. Applying our benchmark to 11 models, we show that LLMs consistently exhibit high rates of social sycophancy: on average, they preserve user's face 45 percentage points more than humans in general advice queries and in queries describing clear user wrongdoing (from Reddit's r/AmITheAsshole). Furthermore, when prompted with perspectives from either side of a moral conflict, LLMs affirm both sides (depending on whichever side the user adopts) in 48% of cases--telling both the at-fault party and the wronged party that they are not wrong--rather than adhering to a consistent moral or value judgment. We further show that social sycophancy is rewarded in preference datasets, and that while existing mitigation strategies for sycophancy are limited in effectiveness, model-based steering shows promise for mitigating these behaviors. Our work provides theoretical grounding and an empirical benchmark for understanding and addressing sycophancy in the open-ended contexts that characterize the vast majority of LLM use cases.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13995v2",
    "published_date": "2025-05-20 06:45:17 UTC",
    "updated_date": "2025-09-29 21:29:38 UTC"
  },
  {
    "arxiv_id": "2505.13994v2",
    "title": "Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning",
    "authors": [
      "Ruiyi Yang",
      "Hao Xue",
      "Imran Razzak",
      "Shirui Pan",
      "Hakim Hacid",
      "Flora D. Salim"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems empower large language models (LLMs) with external knowledge, yet struggle with efficiency-accuracy trade-offs when scaling to large knowledge graphs. Existing approaches often rely on monolithic graph retrieval, incurring unnecessary latency for simple queries and fragmented reasoning for complex multi-hop questions. To address these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework that addresses these limitations with question-driven semantic graph partitioning and collaborative subgraph retrieval. The innovative framework first create Semantic Partitioning of Linked Information, then use the Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware graph segmentation manages to divide knowledge graphs into semantically coherent subgraphs, ensuring subgraphs align with different query types, while lightweight LLM agents are assigned to partitioned subgraphs, and only relevant partitions are activated during retrieval, thus reduce search space while enhancing efficiency. Finally, a hierarchical merging module resolves inconsistencies across subgraph-derived answers through logical verifications. Extensive experimental validation demonstrates considerable improvements compared to existing approaches.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.13994v2",
    "published_date": "2025-05-20 06:44:34 UTC",
    "updated_date": "2025-11-05 11:26:59 UTC"
  },
  {
    "arxiv_id": "2505.14739v1",
    "title": "Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs",
    "authors": [
      "Heiko Oppel",
      "Andreas Spilz",
      "Michael Munz"
    ],
    "abstract": "Denoising diffusion probabilistic models are able to generate synthetic sensor signals. The training process of such a model is controlled by a loss function which measures the difference between the noise that was added in the forward process and the noise that was predicted by the diffusion model. This enables the generation of realistic data. However, the randomness within the process and the loss function itself makes it difficult to estimate the quality of the data. Therefore, we examine multiple similarity metrics and adapt an existing metric to overcome this issue by monitoring the training and synthetisation process using those metrics. The adapted metric can even be fine-tuned on the input data to comply with the requirements of an underlying classification task. We were able to significantly reduce the amount of training epochs without a performance reduction in the classification task. An optimized training process not only saves resources, but also reduces the time for training generative models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14739v1",
    "published_date": "2025-05-20 06:38:17 UTC",
    "updated_date": "2025-05-20 06:38:17 UTC"
  },
  {
    "arxiv_id": "2505.13989v2",
    "title": "When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty",
    "authors": [
      "Yanzhe Wen",
      "Xunkai Li",
      "Qi Zhang",
      "Zhu Lei",
      "Guang Zeng",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "abstract": "Recently, large language models (LLMs) have significantly advanced text-attributed graph (TAG) learning. However, existing methods inadequately handle data uncertainty in open-world scenarios, especially concerning limited labeling and unknown-class nodes. Prior solutions typically rely on isolated semantic or structural approaches for unknown-class rejection, lacking effective annotation pipelines. To address these limitations, we propose Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive label traceability, which integrates semantics and topology for unknown-class rejection, and a graph label annotator to enable model updates using newly annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and practicality.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13989v2",
    "published_date": "2025-05-20 06:37:18 UTC",
    "updated_date": "2025-05-21 04:23:56 UTC"
  },
  {
    "arxiv_id": "2505.13986v3",
    "title": "RIDGECUT: Learning Graph Partitioning with Rings and Wedges",
    "authors": [
      "Qize Jiang",
      "Linsey Pang",
      "Alice Gatti",
      "Mahima Aggarwal",
      "Giovanna Vantini",
      "Xiaosong Ma",
      "Weiwei Sun",
      "Sourav Medya",
      "Sanjay Chawla"
    ],
    "abstract": "Reinforcement Learning (RL) has proven to be a powerful tool for combinatorial optimization (CO) problems due to its ability to learn heuristics that can generalize across problem instances. However, integrating knowledge that will steer the RL framework for CO solutions towards domain appropriate outcomes remains a challenging task. In this paper, we propose RIDGECUT, the first RL framework that constrains the action space to enforce structure-aware partitioning in the Normalized Cut problem. Using transportation networks as a motivating example, we introduce a novel concept that leverages domain knowledge about urban road topology -- where natural partitions often take the form of concentric rings and radial wedges. Our method reshapes the graph into a linear or circular structure to simplify the partitioning task so that we can apply sequential transformers and enables efficient learning via Proximal Policy Optimization. The resulting partitions are not only aligned with expected spatial layouts but also achieve lower normalized cuts compared to existing methods. While we focus on traffic data, our approach is broadly applicable and offers a mechanism for embedding structural priors into RL for graph partitioning.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13986v3",
    "published_date": "2025-05-20 06:33:39 UTC",
    "updated_date": "2025-08-11 06:26:08 UTC"
  },
  {
    "arxiv_id": "2505.13973v1",
    "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models",
    "authors": [
      "Wenhui Zhu",
      "Xuanzhao Dong",
      "Xin Li",
      "Peijie Qiu",
      "Xiwen Chen",
      "Abolfazl Razi",
      "Aris Sotiras",
      "Yi Su",
      "Yalin Wang"
    ],
    "abstract": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory of Multimodal Large Language Models (MLLMs), particularly following the introduction of Group Relative Policy Optimization (GRPO). However, directly applying it to medical tasks remains challenging for achieving clinically grounded model behavior. Motivated by the need to align model response with clinical expectations, we investigate four critical dimensions that affect the effectiveness of RL-based tuning in medical visual question answering (VQA): base model initialization strategy, the role of medical semantic alignment, the impact of length-based rewards on long-chain reasoning, and the influence of bias. We conduct extensive experiments to analyze these factors for medical MLLMs, providing new insights into how models are domain-specifically fine-tuned. Additionally, our results also demonstrate that GRPO-based RL tuning consistently outperforms standard supervised fine-tuning (SFT) in both accuracy and reasoning quality.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13973v1",
    "published_date": "2025-05-20 06:12:20 UTC",
    "updated_date": "2025-05-20 06:12:20 UTC"
  },
  {
    "arxiv_id": "2505.13971v2",
    "title": "The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition",
    "authors": [
      "Ming Gao",
      "Shilong Wu",
      "Hang Chen",
      "Jun Du",
      "Chin-Hui Lee",
      "Shinji Watanabe",
      "Jingdong Chen",
      "Siniscalchi Sabato Marco",
      "Odette Scharenborg"
    ],
    "abstract": "Meetings are a valuable yet challenging scenario for speech applications due to complex acoustic conditions. This paper summarizes the outcomes of the MISP 2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal, multi-device meeting transcription by incorporating video modality alongside audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR). We present the challenge's objectives, tasks, dataset, baseline systems, and solutions proposed by participants. The best-performing systems achieved significant improvements over the baseline: the top AVSD model achieved a Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the best AVDR system achieved a concatenated minimum-permutation Character Error Rate (cpCER) of 11.56%, improving by 72.49%.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Interspeech 2025. Camera-ready version",
    "pdf_url": "https://arxiv.org/pdf/2505.13971v2",
    "published_date": "2025-05-20 06:11:51 UTC",
    "updated_date": "2025-05-27 05:03:46 UTC"
  },
  {
    "arxiv_id": "2505.23780v1",
    "title": "More-than-Human Storytelling: Designing Longitudinal Narrative Engagements with Generative AI",
    "authors": [
      "Émilie Fabre",
      "Katie Seaborn",
      "Shuta Koiwai",
      "Mizuki Watanabe",
      "Paul Riesch"
    ],
    "abstract": "Longitudinal engagement with generative AI (GenAI) storytelling agents is a timely but less charted domain. We explored multi-generational experiences with \"Dreamsmithy,\" a daily dream-crafting app, where participants (N = 28) co-created stories with AI narrator \"Makoto\" every day. Reflections and interactions were captured through a two-week diary study. Reflexive thematic analysis revealed themes likes \"oscillating ambivalence\" and \"socio-chronological bonding,\" highlighting the complex dynamics that emerged between individuals and the AI narrator over time. Findings suggest that while people appreciated the personal notes, opportunities for reflection, and AI creativity, limitations in narrative coherence and control occasionally caused frustration. The results underscore the potential of GenAI for longitudinal storytelling, but also raise critical questions about user agency and ethics. We contribute initial empirical insights and design considerations for developing adaptive, more-than-human storytelling systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "comment": "CHI EA '25",
    "pdf_url": "https://arxiv.org/pdf/2505.23780v1",
    "published_date": "2025-05-20 06:10:29 UTC",
    "updated_date": "2025-05-20 06:10:29 UTC"
  },
  {
    "arxiv_id": "2505.13969v1",
    "title": "Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World",
    "authors": [
      "Junya Nakanishi",
      "Jun Baba",
      "Yuichiro Yoshikawa",
      "Hiroko Kamide",
      "Hiroshi Ishiguro"
    ],
    "abstract": "This paper discusses the functional advantages of the Selection-Broadcast Cycle structure proposed by Global Workspace Theory (GWT), inspired by human consciousness, particularly focusing on its applicability to artificial intelligence and robotics in dynamic, real-time scenarios. While previous studies often examined the Selection and Broadcast processes independently, this research emphasizes their combined cyclic structure and the resulting benefits for real-time cognitive systems. Specifically, the paper identifies three primary benefits: Dynamic Thinking Adaptation, Experience-Based Adaptation, and Immediate Real-Time Adaptation. This work highlights GWT's potential as a cognitive architecture suitable for sophisticated decision-making and adaptive performance in unsupervised, dynamic environments. It suggests new directions for the development and implementation of robust, general-purpose AI and robotics systems capable of managing complex, real-world tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13969v1",
    "published_date": "2025-05-20 06:07:21 UTC",
    "updated_date": "2025-05-20 06:07:21 UTC"
  },
  {
    "arxiv_id": "2505.14738v2",
    "title": "R&D-Agent: An LLM-Agent Framework Towards Autonomous Data Science",
    "authors": [
      "Xu Yang",
      "Xiao Yang",
      "Shikai Fang",
      "Yifei Zhang",
      "Jian Wang",
      "Bowen Xian",
      "Qizheng Li",
      "Jingyuan Li",
      "Minrui Xu",
      "Yuante Li",
      "Haoran Pan",
      "Yuge Zhang",
      "Weiqing Liu",
      "Yelong Shen",
      "Weizhu Chen",
      "Jiang Bian"
    ],
    "abstract": "Recent advances in AI and ML have transformed data science, yet increasing complexity and expertise requirements continue to hinder progress. Although crowd-sourcing platforms alleviate some challenges, high-level machine learning engineering (MLE) tasks remain labor-intensive and iterative. We introduce R&D-Agent, a comprehensive, decoupled, and extensible framework that formalizes the MLE process. R&D-Agent defines the MLE workflow into two phases and six components, turning agent design for MLE from ad-hoc craftsmanship into a principled, testable process. Although several existing agents report promising gains on their chosen components, they can mostly be summarized as a partial optimization from our framework's simple baseline. Inspired by human experts, we designed efficient and effective agents within this framework that achieve state-of-the-art performance. Evaluated on MLE-Bench, the agent built on R&D-Agent ranks as the top-performing machine learning engineering agent, achieving 35.1% any medal rate, demonstrating the ability of the framework to speed up innovation and improve accuracy across a wide range of data science applications. We have open-sourced R&D-Agent on GitHub: https://github.com/microsoft/RD-Agent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "33 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.14738v2",
    "published_date": "2025-05-20 06:07:00 UTC",
    "updated_date": "2025-10-01 03:21:53 UTC"
  },
  {
    "arxiv_id": "2505.13965v1",
    "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
    "authors": [
      "Jiamin Su",
      "Yibo Yan",
      "Zhuoran Gao",
      "Han Zhang",
      "Xiang Liu",
      "Xuming Hu"
    ],
    "abstract": "Automated Essay Scoring (AES) is crucial for modern education, particularly with the increasing prevalence of multimodal assessments. However, traditional AES methods struggle with evaluation generalizability and multimodal perception, while even recent Multimodal Large Language Model (MLLM)-based approaches can produce hallucinated justifications and scores misaligned with human judgment. To address the limitations, we introduce CAFES, the first collaborative multi-agent framework specifically designed for AES. It orchestrates three specialized agents: an Initial Scorer for rapid, trait-specific evaluations; a Feedback Pool Manager to aggregate detailed, evidence-grounded strengths; and a Reflective Scorer that iteratively refines scores based on this feedback to enhance human alignment. Extensive experiments, using state-of-the-art MLLMs, achieve an average relative improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth, especially for grammatical and lexical diversity. Our proposed CAFES framework paves the way for an intelligent multimodal AES system. The code will be available upon acceptance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916",
    "pdf_url": "https://arxiv.org/pdf/2505.13965v1",
    "published_date": "2025-05-20 06:05:56 UTC",
    "updated_date": "2025-05-20 06:05:56 UTC"
  },
  {
    "arxiv_id": "2507.06235v1",
    "title": "Super Kawaii Vocalics: Amplifying the \"Cute\" Factor in Computer Voice",
    "authors": [
      "Yuto Mandai",
      "Katie Seaborn",
      "Tomoyasu Nakano",
      "Xin Sun",
      "Yijia Wang",
      "Jun Kato"
    ],
    "abstract": "\"Kawaii\" is the Japanese concept of cute, which carries sociocultural connotations related to social identities and emotional responses. Yet, virtually all work to date has focused on the visual side of kawaii, including in studies of computer agents and social robots. In pursuit of formalizing the new science of kawaii vocalics, we explored what elements of voice relate to kawaii and how they might be manipulated, manually and automatically. We conducted a four-phase study (grand N = 512) with two varieties of computer voices: text-to-speech (TTS) and game character voices. We found kawaii \"sweet spots\" through manipulation of fundamental and formant frequencies, but only for certain voices and to a certain extent. Findings also suggest a ceiling effect for the kawaii vocalics of certain voices. We offer empirical validation of the preliminary kawaii vocalics model and an elementary method for manipulating kawaii perceptions of computer voice.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "comment": "CHI '25",
    "pdf_url": "https://arxiv.org/pdf/2507.06235v1",
    "published_date": "2025-05-20 06:03:23 UTC",
    "updated_date": "2025-05-20 06:03:23 UTC"
  },
  {
    "arxiv_id": "2505.17076v3",
    "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
    "authors": [
      "Haoyang Zhang",
      "Hexin Liu",
      "Xiangyu Zhang",
      "Qiquan Zhang",
      "Yuchen Hu",
      "Junqi Zhao",
      "Fei Tian",
      "Xuerui Yang",
      "Leibny Paola Garcia",
      "Eng Siong Chng"
    ],
    "abstract": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.17076v3",
    "published_date": "2025-05-20 06:01:19 UTC",
    "updated_date": "2025-06-13 17:21:25 UTC"
  },
  {
    "arxiv_id": "2506.01998v1",
    "title": "Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents",
    "authors": [
      "Takao Fujii",
      "Katie Seaborn",
      "Madeleine Steeds",
      "Jun Kato"
    ],
    "abstract": "Conversational agents that mimic people have raised questions about the ethics of anthropomorphizing machines with human social identity cues. Critics have also questioned assumptions of identity neutrality in humanlike agents. Recent work has revealed that intersectional Japanese pronouns can elicit complex and sometimes evasive impressions of agent identity. Yet, the role of other \"neutral\" non-pronominal self-referents (NPSR) and voice as a socially expressive medium remains unexplored. In a crowdsourcing study, Japanese participants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and Ember) using seven self-referents. We found strong evidence of voice gendering alongside the potential of intersectional self-referents to evade gendering, i.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age and formality intersected with gendering as per sociolinguistic theories, especially boku and watakushi. This work provides a nuanced take on agent identity perceptions and champions intersectional and culturally-sensitive work on voice agents.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "comment": "CHI '25",
    "pdf_url": "https://arxiv.org/pdf/2506.01998v1",
    "published_date": "2025-05-20 05:45:09 UTC",
    "updated_date": "2025-05-20 05:45:09 UTC"
  },
  {
    "arxiv_id": "2505.13949v1",
    "title": "FlashThink: An Early Exit Method For Efficient Reasoning",
    "authors": [
      "Guochao Jiang",
      "Guofeng Quan",
      "Zepeng Ding",
      "Ziqin Luo",
      "Dixuan Wang",
      "Zheng Hu"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive performance in reasoning tasks. However, LLMs tend to generate excessively long reasoning content, leading to significant computational overhead. Our observations indicate that even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning content, which is against intuitive expectations. Preliminary experiments show that at a certain point during the generation process, the model is already capable of producing the correct solution without completing the full reasoning content. Therefore, we consider that the reasoning process of the model can be exited early to achieve the purpose of efficient reasoning. We introduce a verification model that identifies the exact moment when the model can stop reasoning and still provide the correct answer. Comprehensive experiments on four different benchmarks demonstrate that our proposed method, FlashThink, effectively shortens the reasoning content while preserving the model accuracy. For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning content by 77.04% and 77.47%, respectively, without reducing the accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13949v1",
    "published_date": "2025-05-20 05:28:21 UTC",
    "updated_date": "2025-05-20 05:28:21 UTC"
  },
  {
    "arxiv_id": "2505.13948v2",
    "title": "Memory-Centric Embodied Question Answering",
    "authors": [
      "Mingliang Zhai",
      "Zhi Gao",
      "Yuwei Wu",
      "Yunde Jia"
    ],
    "abstract": "Embodied Question Answering (EQA) requires agents to autonomously explore and comprehend the environment to answer context-dependent questions. Typically, an EQA framework consists of four components: a planner, a memory module, a stopping module, and an answering module. However, the memory module is utilized inefficiently in existing methods, as the information it stores is leveraged solely for the answering module. Such a design may result in redundant or inadequate exploration, leading to a suboptimal success rate. To solve this problem, we propose MemoryEQA, an EQA framework centered on memory, which establishes mechanisms for memory storage, update, and retrieval, allowing memory information to contribute throughout the entire exploration process. Specifically, we convert the observation into structured textual representations, which are stored in a vector library following a fixed structure. At each exploration step, we utilize a viewpoint comparison strategy to determine whether the memory requires updating. Before executing each module, we employ an entropy-based adaptive retrieval strategy to obtain the minimal yet sufficient memory information that satisfies the requirements of different modules. The retrieved module-specific information is then integrated with the current observation as input to the corresponding module. To evaluate EQA models' memory capabilities, we constructed the benchmark based on HM3D called MT-HM3D, comprising 1,587 question-answer pairs involving multiple targets across various regions, which requires agents to maintain memory of exploration-acquired target information. Experimental results on HM-EQA, MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a 9.9% performance gain on MT-HM3D compared to baseline models further underscores the memory capability's pivotal role in solving complex tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "15pages, 6 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.13948v2",
    "published_date": "2025-05-20 05:27:57 UTC",
    "updated_date": "2025-12-13 07:28:55 UTC"
  },
  {
    "arxiv_id": "2505.13946v2",
    "title": "Visual Instruction Bottleneck Tuning",
    "authors": [
      "Changdae Oh",
      "Jiatong Li",
      "Shawn Im",
      "Sharon Li"
    ],
    "abstract": "Despite widespread adoption, multimodal large language models (MLLMs) suffer performance degradation when encountering unfamiliar queries under distribution shifts. Existing methods to improve MLLM generalization typically require either more instruction data or larger advanced model architectures, both of which incur non-trivial human labor or computational costs. In this work, we take an alternative approach to enhance the generalization and robustness of MLLMs under distribution shifts, from a representation learning perspective. Inspired by information bottleneck (IB) principle, we derive a variational lower bound of the IB for MLLMs and devise a practical implementation, Visual Instruction Bottleneck Tuning (Vittle). We then provide a theoretical justification of Vittle by revealing its connection to an information-theoretic robustness metric of MLLM. Empirical validation of multiple MLLMs on open-ended and closed-form question answering and object hallucination detection tasks over 45 datasets, including 30 shift scenarios, demonstrates that Vittle consistently improves the MLLM's robustness under shifts by pursuing the learning of a minimal sufficient representation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.13946v2",
    "published_date": "2025-05-20 05:24:53 UTC",
    "updated_date": "2025-10-20 03:05:33 UTC"
  },
  {
    "arxiv_id": "2505.13941v1",
    "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation",
    "authors": [
      "Haoyang Fang",
      "Boran Han",
      "Nick Erickson",
      "Xiyuan Zhang",
      "Su Zhou",
      "Anirudh Dagar",
      "Jiani Zhang",
      "Ali Caner Turkmen",
      "Cuixiong Hu",
      "Huzefa Rangwala",
      "Ying Nian Wu",
      "Bernie Wang",
      "George Karypis"
    ],
    "abstract": "Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly when handling multimodal data. We introduce MLZero, a novel multi-agent framework powered by Large Language Models (LLMs) that enables end-to-end ML automation across diverse data modalities with minimal human intervention. A cognitive perception module is first employed, transforming raw multimodal inputs into perceptual context that effectively guides the subsequent workflow. To address key limitations of LLMs, such as hallucinated code generation and outdated API knowledge, we enhance the iterative code generation process with semantic and episodic memory. MLZero demonstrates superior performance on MLE-Bench Lite, outperforming all competitors in both success rate and solution quality, securing six gold medals. Additionally, when evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more challenging tasks spanning diverse data modalities, MLZero outperforms the competing methods by a large margin with a success rate of 0.92 (+263.6\\%) and an average rank of 2.28. Our approach maintains its robust effectiveness even with a compact 8B LLM, outperforming full-size systems from existing solutions.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13941v1",
    "published_date": "2025-05-20 05:20:53 UTC",
    "updated_date": "2025-05-20 05:20:53 UTC"
  },
  {
    "arxiv_id": "2505.17075v1",
    "title": "Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems",
    "authors": [
      "Fuma Kurata",
      "Mao Saeki",
      "Masaki Eguchi",
      "Shungo Suzuki",
      "Hiroaki Takatsu",
      "Yoichi Matsuyama"
    ],
    "abstract": "This study aimed to develop and validate two scales of engagement and rapport to evaluate the user experience quality with multimodal dialogue systems in the context of foreign language learning. The scales were designed based on theories of engagement in educational psychology, social psychology, and second language acquisition.Seventy-four Japanese learners of English completed roleplay and discussion tasks with trained human tutors and a dialog agent. After each dialogic task was completed, they responded to the scales of engagement and rapport. The validity and reliability of the scales were investigated through two analyses. We first conducted analysis of Cronbach's alpha coefficient and a series of confirmatory factor analyses to test the structural validity of the scales and the reliability of our designed items. We then compared the scores of engagement and rapport between the dialogue with human tutors and the one with a dialogue agent. The results revealed that our scales succeeded in capturing the difference in the dialogue experience quality between the human interlocutors and the dialogue agent from multiple perspectives.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17075v1",
    "published_date": "2025-05-20 05:19:28 UTC",
    "updated_date": "2025-05-20 05:19:28 UTC"
  },
  {
    "arxiv_id": "2505.13940v2",
    "title": "DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery",
    "authors": [
      "Kun Li",
      "Zhennan Wu",
      "Shoupeng Wang",
      "Jia Wu",
      "Shirui Pan",
      "Wenbin Hu"
    ],
    "abstract": "Large language models (LLMs) integrated with autonomous agents hold significant potential for advancing scientific discovery through automated reasoning and task execution. However, applying LLM agents to drug discovery is still constrained by challenges such as large-scale multimodal data processing, limited task automation, and poor support for domain-specific tools. To overcome these limitations, we introduce DrugPilot, a LLM-based agent system with a parameterized reasoning architecture designed for end-to-end scientific workflows in drug discovery. DrugPilot enables multi-stage research processes by integrating structured tool use with a novel parameterized memory pool. The memory pool converts heterogeneous data from both public sources and user-defined inputs into standardized representations. This design supports efficient multi-turn dialogue, reduces information loss during data exchange, and enhances complex scientific decision-making. To support training and benchmarking, we construct a drug instruction dataset covering eight core drug discovery tasks. Under the Berkeley function-calling benchmark, DrugPilot significantly outperforms state-of-the-art agents such as ReAct and LoT, achieving task completion rates of 98.0%, 93.5%, and 64.0% for simple, multi-tool, and multi-turn scenarios, respectively. These results highlight DrugPilot's potential as a versatile agent framework for computational science domains requiring automated, interactive, and data-integrated reasoning.",
    "categories": [
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 8 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.13940v2",
    "published_date": "2025-05-20 05:18:15 UTC",
    "updated_date": "2025-07-28 08:10:33 UTC"
  },
  {
    "arxiv_id": "2505.13938v4",
    "title": "CLEVER: A Curated Benchmark for Formally Verified Code Generation",
    "authors": [
      "Amitayush Thakur",
      "Jasper Lee",
      "George Tsoukalas",
      "Meghana Sistla",
      "Matthew Zhao",
      "Stefan Zetzsche",
      "Greg Durrett",
      "Yisong Yue",
      "Swarat Chaudhuri"
    ],
    "abstract": "We introduce ${\\rm C{\\small LEVER}}$, a high-quality, curated benchmark of 161 problems for end-to-end verified code generation in Lean. Each problem consists of (1) the task of generating a specification that matches a held-out ground-truth specification, and (2) the task of generating a Lean implementation that provably satisfies this specification. Unlike prior benchmarks, ${\\rm C{\\small LEVER}}$ avoids test-case supervision, LLM-generated annotations, and specifications that leak implementation logic or allow vacuous solutions. All outputs are verified post-hoc using Lean's type checker to ensure machine-checkable correctness. We use ${\\rm C{\\small LEVER}}$ to evaluate several few-shot and agentic approaches based on state-of-the-art language models. These methods all struggle to achieve full verification, establishing it as a challenging frontier benchmark for program synthesis and formal reasoning. Our benchmark can be found on GitHub(https://github.com/trishullab/clever) as well as HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our evaluation code is also available online(https://github.com/trishullab/clever-prover).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13938v4",
    "published_date": "2025-05-20 05:15:47 UTC",
    "updated_date": "2025-10-23 16:29:07 UTC"
  },
  {
    "arxiv_id": "2505.13936v2",
    "title": "EEG-to-Text Translation: A Model for Deciphering Human Brain Activity",
    "authors": [
      "Saydul Akbar Murad",
      "Ashim Dahal",
      "Nick Rahimi"
    ],
    "abstract": "With the rapid advancement of large language models like Gemini, GPT, and others, bridging the gap between the human brain and language processing has become an important area of focus. To address this challenge, researchers have developed various models to decode EEG signals into text. However, these models still face significant performance limitations. To overcome these shortcomings, we propose a new model, R1 Translator, which aims to improve the performance of EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM encoder with a pretrained transformer-based decoder, utilizing EEG features to produce high-quality text outputs. The model processes EEG embeddings through the LSTM to capture sequential dependencies, which are then fed into the transformer decoder for effective text generation. The R1 Translator excels in ROUGE metrics, outperforming both T5 (previous research) and Brain Translator. Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9% higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and Brain by 3.6% (0.7553). Code is available at https://github.com/Mmurrad/EEG-To-text.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13936v2",
    "published_date": "2025-05-20 05:04:15 UTC",
    "updated_date": "2025-12-08 19:30:07 UTC"
  },
  {
    "arxiv_id": "2505.13934v2",
    "title": "RLVR-World: Training World Models with Reinforcement Learning",
    "authors": [
      "Jialong Wu",
      "Shaofeng Yin",
      "Ningya Feng",
      "Mingsheng Long"
    ],
    "abstract": "World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly. Code, datasets, models, and video samples are available at the project website: https://thuml.github.io/RLVR-World.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025. Code is available at project website: https://thuml.github.io/RLVR-World/",
    "pdf_url": "https://arxiv.org/pdf/2505.13934v2",
    "published_date": "2025-05-20 05:02:53 UTC",
    "updated_date": "2025-10-25 02:00:20 UTC"
  },
  {
    "arxiv_id": "2505.13921v2",
    "title": "APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight",
    "authors": [
      "Wanjing Huang",
      "Weixiang Yan",
      "Zhen Zhang",
      "Ambuj Singh"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate strong reasoning and task planning capabilities but remain fundamentally limited in physical interaction modeling. Existing approaches integrate perception via Vision-Language Models (VLMs) or adaptive decision-making through Reinforcement Learning (RL), but they fail to capture dynamic object interactions or require task-specific training, limiting their real-world applicability. We introduce APEX (Anticipatory Physics-Enhanced Execution), a framework that equips LLMs with physics-driven foresight for real-time task planning. APEX constructs structured graphs to identify and model the most relevant dynamic interactions in the environment, providing LLMs with explicit physical state updates. Simultaneously, APEX provides low-latency forward simulations of physically feasible actions, allowing LLMs to select optimal strategies based on predictive outcomes rather than static observations. We evaluate APEX on three benchmarks designed to assess perception, prediction, and decision-making: (1) Physics Reasoning Benchmark, testing causal inference and object motion prediction; (2) Tetris, evaluating whether physics-informed prediction enhances decision-making performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance, assessing the immediate integration of perception and action feasibility analysis. APEX significantly outperforms standard LLMs and VLM-based models, demonstrating the necessity of explicit physics reasoning for bridging the gap between language-based intelligence and real-world task execution. The source code and experiment setup are publicly available at https://github.com/hwj20/APEX_EXP .",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13921v2",
    "published_date": "2025-05-20 04:34:58 UTC",
    "updated_date": "2025-10-16 03:07:40 UTC"
  },
  {
    "arxiv_id": "2505.13914v1",
    "title": "Parallel Belief Revision via Order Aggregation",
    "authors": [
      "Jake Chandler",
      "Richard Booth"
    ],
    "abstract": "Despite efforts to better understand the constraints that operate on single-step parallel (aka \"package\", \"multiple\") revision, very little work has been carried out on how to extend the model to the iterated case. A recent paper by Delgrande & Jin outlines a range of relevant rationality postulates. While many of these are plausible, they lack an underlying unifying explanation. We draw on recent work on iterated parallel contraction to offer a general method for extending serial iterated belief revision operators to handle parallel change. This method, based on a family of order aggregators known as TeamQueue aggregators, provides a principled way to recover the independently plausible properties that can be found in the literature, without yielding the more dubious ones.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13914v1",
    "published_date": "2025-05-20 04:26:01 UTC",
    "updated_date": "2025-05-20 04:26:01 UTC"
  },
  {
    "arxiv_id": "2505.13911v1",
    "title": "Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation",
    "authors": [
      "Ruijie Zhao",
      "Zuopeng Tan",
      "Xiao Xue",
      "Longfei Zhao",
      "Bing Li",
      "Zicheng Liao",
      "Ying Ming",
      "Jiaru Wang",
      "Ran Xiao",
      "Sirong Piao",
      "Rui Zhao",
      "Qiqi Xu",
      "Wei Song"
    ],
    "abstract": "Pulmonary segment segmentation is crucial for cancer localization and surgical planning. However, the pixel-wise annotation of pulmonary segments is laborious, as the boundaries between segments are indistinguishable in medical images. To this end, we propose a weakly supervised learning (WSL) method, termed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise clinical anatomical definition of pulmonary segments to perform pulmonary segment segmentation. Since pulmonary segments reside within the lobes and are determined by the bronchovascular tree, i.e., artery, airway and vein, the design of the loss function is founded on two principles. First, segment-level labels are utilized to directly supervise the output of the pulmonary segments, ensuring that they accurately encompass the appropriate bronchovascular tree. Second, lobe-level supervision indirectly oversees the pulmonary segment, ensuring their inclusion within the corresponding lobe. Besides, we introduce a two-stage segmentation strategy that incorporates bronchovascular priori information. Furthermore, a consistency loss is proposed to enhance the smoothness of segment boundaries, along with an evaluation metric designed to measure the smoothness of pulmonary segment boundaries. Visual inspection and evaluation metrics from experiments conducted on a private dataset demonstrate the effectiveness of our method.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13911v1",
    "published_date": "2025-05-20 04:23:12 UTC",
    "updated_date": "2025-05-20 04:23:12 UTC"
  },
  {
    "arxiv_id": "2505.13909v1",
    "title": "Efficient Agent Training for Computer Use",
    "authors": [
      "Yanheng He",
      "Jiahe Jin",
      "Pengfei Liu"
    ],
    "abstract": "Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved a remarkable 141% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from a small amount of high-quality trajectory data.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "We open-source our entire suite of code, data, and models to facilitate future research at https://github.com/GAIR-NLP/PC-Agent-E",
    "pdf_url": "https://arxiv.org/pdf/2505.13909v1",
    "published_date": "2025-05-20 04:20:18 UTC",
    "updated_date": "2025-05-20 04:20:18 UTC"
  },
  {
    "arxiv_id": "2505.13906v1",
    "title": "XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data",
    "authors": [
      "Soyabul Islam Lincoln",
      "Mirza Mohd Shahriar Maswood"
    ],
    "abstract": "A common neurodegenerative disease, Alzheimer's disease requires a precise diagnosis and efficient treatment, particularly in light of escalating healthcare expenses and the expanding use of artificial intelligence in medical diagnostics. Many recent studies shows that the combination of brain Magnetic Resonance Imaging (MRI) and deep neural networks have achieved promising results for diagnosing AD. Using deep convolutional neural networks, this paper introduces a novel deep learning architecture that incorporates multiresidual blocks, specialized spatial attention blocks, grouped query attention, and multi-head attention. The study assessed the model's performance on four publicly accessible datasets and concentrated on identifying binary and multiclass issues across various categories. This paper also takes into account of the explainability of AD's progression and compared with state-of-the-art methods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster Score-CAM, and XGRADCAM. Our methodology consistently outperforms current approaches, achieving 99.66\\% accuracy in 4-class classification, 99.63\\% in 3-class classification, and 100\\% in binary classification using Kaggle datasets. For Open Access Series of Imaging Studies (OASIS) datasets the accuracies are 99.92\\%, 99.90\\%, and 99.95\\% respectively. The Alzheimer's Disease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in three planes (axial, sagittal, and coronal) and a combination of all planes. The study achieved accuracies of 99.08\\% for axis, 99.85\\% for sagittal, 99.5\\% for coronal, and 99.17\\% for all axis, and 97.79\\% and 8.60\\% respectively for ADNI-2. The network's ability to retrieve important information from MRI images is demonstrated by its excellent accuracy in categorizing AD stages.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "20 pages, 12 figures,",
    "pdf_url": "https://arxiv.org/pdf/2505.13906v1",
    "published_date": "2025-05-20 04:17:28 UTC",
    "updated_date": "2025-05-20 04:17:28 UTC"
  },
  {
    "arxiv_id": "2505.17074v1",
    "title": "Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency",
    "authors": [
      "Ruixiao Li",
      "Fahao Chen",
      "Peng Li"
    ],
    "abstract": "Speculative decoding accelerates Large Language Model (LLM) inference by employing a small speculative model (SSM) to generate multiple candidate tokens and verify them using the LLM in parallel. This technique has been widely integrated into LLM inference serving systems. However, inference requests typically exhibit uncertain execution time, which poses a significant challenge of efficiently scheduling requests in these systems. Existing work estimates execution time based solely on predicted output length, which could be inaccurate because execution time depends on both output length and token acceptance rate of verification by the LLM. In this paper, we propose a semi-clairvoyant request scheduling algorithm called Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a number of inference requests, LAPS-SD can effectively minimize average inference latency by adaptively scheduling requests according to their features during decoding. When the token acceptance rate is dynamic and execution time is difficult to estimate, LAPS-SD maintains multiple priority queues and allows request execution preemption across different queues. Once the token acceptance rate becomes stable, LAPS-SD can accurately estimate the execution time and schedule requests accordingly. Extensive experiments show that LAPS-SD reduces inference latency by approximately 39\\% compared to state-of-the-art scheduling methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17074v1",
    "published_date": "2025-05-20 04:12:37 UTC",
    "updated_date": "2025-05-20 04:12:37 UTC"
  },
  {
    "arxiv_id": "2505.13904v3",
    "title": "Learning to Insert for Constructive Neural Vehicle Routing Solver",
    "authors": [
      "Fu Luo",
      "Xi Lin",
      "Mengyuan Zhong",
      "Fei Liu",
      "Zhenkun Wang",
      "Jianyong Sun",
      "Qingfu Zhang"
    ],
    "abstract": "Neural Combinatorial Optimisation (NCO) is a promising learning-based approach for solving Vehicle Routing Problems (VRPs) without extensive manual design. While existing constructive NCO methods typically follow an appending-based paradigm that sequentially adds unvisited nodes to partial solutions, this rigid approach often leads to suboptimal results. To overcome this limitation, we explore the idea of insertion-based paradigm and propose Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel learning-based method for constructive NCO. Unlike traditional approaches, L2C-Insert builds solutions by strategically inserting unvisited nodes at any valid position in the current partial solution, which can significantly enhance the flexibility and solution quality. The proposed framework introduces three key components: a novel model architecture for precise insertion position prediction, an efficient training scheme for model optimization, and an advanced inference technique that fully exploits the insertion paradigm's flexibility. Extensive experiments on both synthetic and real-world instances of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior performance across various problem sizes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.13904v3",
    "published_date": "2025-05-20 04:10:50 UTC",
    "updated_date": "2025-10-30 07:17:31 UTC"
  },
  {
    "arxiv_id": "2505.13898v3",
    "title": "Do Language Models Use Their Depth Efficiently?",
    "authors": [
      "Róbert Csordás",
      "Christopher D. Manning",
      "Christopher Potts"
    ],
    "abstract": "Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, we analyze the residual stream of the Llama 3.1, Qwen 3, and OLMo 2 family of models. We find: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, we are unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, we seek to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, we train linear maps from the residual stream of a shallow model to a deeper one. We find that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.13898v3",
    "published_date": "2025-05-20 04:00:56 UTC",
    "updated_date": "2025-10-28 08:56:58 UTC"
  },
  {
    "arxiv_id": "2506.08022v3",
    "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining",
    "authors": [
      "Chenxi Liu",
      "Tianyi Xiong",
      "Yanshuo Chen",
      "Ruibo Chen",
      "Yihan Wu",
      "Junfeng Guo",
      "Tianyi Zhou",
      "Heng Huang"
    ],
    "abstract": "The task adaptation and alignment of Large Multimodal Models (LMMs) have been significantly advanced by instruction tuning and further strengthened by recent preference optimization. Yet, most LMMs still suffer from severe modality imbalance during reasoning, i.e., outweighing language prior biases over visual inputs, which bottlenecks their generalization to downstream tasks and causes hallucinations. However, existing preference optimization approaches for LMMs do not focus on restraining the internal biases of their Large Language Model (LLM) backbones when curating the training data. Moreover, they heavily rely on offline data and lack the capacity to explore diverse responses adaptive to dynamic distributional shifts during training. Meanwhile, Group Relative Policy Optimization (GRPO), a recent method using online-generated data and verified rewards to improve reasoning capabilities, remains largely underexplored in LMM alignment. In this paper, we propose a novel preference learning framework, Modality-Balancing Preference Optimization (MBPO), to address the modality imbalance in LMMs. MBPO constructs a more effective offline preference dataset by generating hard negatives, i.e., rejected responses misled by LLM biases due to limited usage of visual information, through adversarial perturbation of input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended tasks to generate online responses with verified rewards. GRPO is then employed to train the model with offline-online hybrid data. Extensive experiments demonstrate that MBPO can enhance LMM performance on challenging vision-language tasks and effectively reduce hallucinations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.08022v3",
    "published_date": "2025-05-20 03:59:05 UTC",
    "updated_date": "2025-10-08 22:18:43 UTC"
  },
  {
    "arxiv_id": "2505.13887v3",
    "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation",
    "authors": [
      "Junyang Wang",
      "Haiyang Xu",
      "Xi Zhang",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Jitao Sang"
    ],
    "abstract": "The exponential rise in mobile device usage necessitates streamlined automation for effective task management, yet many AI frameworks fall short due to inadequate operational expertise. While manually written knowledge can bridge this gap, it is often burdensome and inefficient. We introduce Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool to effortlessly and efficiently inject operational knowledge into mobile automation processes. By deriving knowledge directly from video content, Mobile-Agent-V eliminates manual intervention, significantly reducing the effort and time required for knowledge acquisition. To rigorously evaluate this approach, we propose Mobile-Knowledge, a benchmark tailored to assess the impact of external knowledge on mobile agent performance. Our experimental findings demonstrate that Mobile-Agent-V enhances performance by 36% compared to existing methods, underscoring its effortless and efficient advantages in mobile automation.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "I submitted the replacement version as a new article by mistake. Future updates will appear at arXiv:2502.17110",
    "pdf_url": "https://arxiv.org/pdf/2505.13887v3",
    "published_date": "2025-05-20 03:48:19 UTC",
    "updated_date": "2025-06-03 08:39:01 UTC"
  },
  {
    "arxiv_id": "2505.14737v1",
    "title": "Leveraging Multivariate Long-Term History Representation for Time Series Forecasting",
    "authors": [
      "Huiliang Zhang",
      "Di Wu",
      "Arnaud Zinflou",
      "Stephane Dellacherie",
      "Mouhamadou Makhtar Dione",
      "Benoit Boulet"
    ],
    "abstract": "Multivariate Time Series (MTS) forecasting has a wide range of applications in both industry and academia. Recent advances in Spatial-Temporal Graph Neural Network (STGNN) have achieved great progress in modelling spatial-temporal correlations. Limited by computational complexity, most STGNNs for MTS forecasting focus primarily on short-term and local spatial-temporal dependencies. Although some recent methods attempt to incorporate univariate history into modeling, they still overlook crucial long-term spatial-temporal similarities and correlations across MTS, which are essential for accurate forecasting. To fill this gap, we propose a framework called the Long-term Multivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting. Specifically, a Long-term History Encoder (LHEncoder) is adopted to effectively encode the long-term history into segment-level contextual representations and reduce point-level noise. A non-parametric Hierarchical Representation Retriever (HRetriever) is designed to include the spatial information in the long-term spatial-temporal dependency modelling and pick out the most valuable representations with no additional training. A Transformer-based Aggregator (TAggregator) selectively fuses the sparsely retrieved contextual representations based on the ranking positional embedding efficiently. Experimental results demonstrate that LMHR outperforms typical STGNNs by 10.72% on the average prediction horizons and state-of-the-art methods by 4.12% on several real-world datasets. Additionally, it consistently improves prediction accuracy by 9.8% on the top 10% of rapidly changing patterns across the datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14737v1",
    "published_date": "2025-05-20 03:46:36 UTC",
    "updated_date": "2025-05-20 03:46:36 UTC"
  },
  {
    "arxiv_id": "2505.13873v1",
    "title": "Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model",
    "authors": [
      "Peisong Niu",
      "Ziqing Ma",
      "Tian Zhou",
      "Weiqi Chen",
      "Lefei Shen",
      "Rong Jin",
      "Liang Sun"
    ],
    "abstract": "Weather forecasting has long posed a significant challenge for humanity. While recent AI-based models have surpassed traditional numerical weather prediction (NWP) methods in global forecasting tasks, overfitting remains a critical issue due to the limited availability of real-world weather data spanning only a few decades. Unlike fields like computer vision or natural language processing, where data abundance can mitigate overfitting, weather forecasting demands innovative strategies to address this challenge with existing data. In this paper, we explore pre-training methods for weather forecasting, finding that selecting an appropriately challenging pre-training task introduces locality bias, effectively mitigating overfitting and enhancing performance. We introduce Baguan, a novel data-driven model for medium-range weather forecasting, built on a Siamese Autoencoder pre-trained in a self-supervised manner and fine-tuned for different lead times. Experimental results show that Baguan outperforms traditional methods, delivering more accurate forecasts. Additionally, the pre-trained Baguan demonstrates robust overfitting control and excels in downstream tasks, such as subseasonal-to-seasonal (S2S) modeling and regional forecasting, after fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "KDD2025 research track accepted",
    "pdf_url": "https://arxiv.org/pdf/2505.13873v1",
    "published_date": "2025-05-20 03:29:23 UTC",
    "updated_date": "2025-05-20 03:29:23 UTC"
  },
  {
    "arxiv_id": "2505.13872v1",
    "title": "Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving",
    "authors": [
      "Jingzheng Li",
      "Tiancheng Wang",
      "Xingyu Peng",
      "Jiacheng Chen",
      "Zhijun Chen",
      "Bing Li",
      "Xianglong Liu"
    ],
    "abstract": "Autonomous Driving (AD) systems demand the high levels of safety assurance. Despite significant advancements in AD demonstrated on open-source benchmarks like Longest6 and Bench2Drive, existing datasets still lack regulatory-compliant scenario libraries for closed-loop testing to comprehensively evaluate the functional safety of AD. Meanwhile, real-world AD accidents are underrepresented in current driving datasets. This scarcity leads to inadequate evaluation of AD performance, posing risks to safety validation and practical deployment. To address these challenges, we propose Safety2Drive, a safety-critical scenario library designed to evaluate AD systems. Safety2Drive offers three key contributions. (1) Safety2Drive comprehensively covers the test items required by standard regulations and contains 70 AD function test items. (2) Safety2Drive supports the safety-critical scenario generalization. It has the ability to inject safety threats such as natural environment corruptions and adversarial attacks cross camera and LiDAR sensors. (3) Safety2Drive supports multi-dimensional evaluation. In addition to the evaluation of AD systems, it also supports the evaluation of various perception tasks, such as object detection and lane detection. Safety2Drive provides a paradigm from scenario construction to validation, establishing a standardized test framework for the safe deployment of AD.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13872v1",
    "published_date": "2025-05-20 03:27:06 UTC",
    "updated_date": "2025-05-20 03:27:06 UTC"
  },
  {
    "arxiv_id": "2505.13860v2",
    "title": "Domain Adaptation of VLM for Soccer Video Understanding",
    "authors": [
      "Tiancheng Jiang",
      "Henry Wang",
      "Md Sirajus Salekin",
      "Parmida Atighehchian",
      "Shinan Zhang"
    ],
    "abstract": "Vision Language Models (VLMs) have demonstrated strong performance in multi-modal tasks by effectively aligning visual and textual representations. However, most video understanding VLM research has been domain-agnostic, leaving the understanding of their transfer learning capability to specialized domains under-explored. In this work, we address this by exploring the adaptability of open-source VLMs to specific domains, and focusing on soccer as an initial case study. Our approach uses large-scale soccer datasets and LLM to create instruction-following data, and use them to iteratively fine-tune the general-domain VLM in a curriculum learning fashion (first teaching the model key soccer concepts to then question answering tasks). The final adapted model, trained using a curated dataset of 20k video clips, exhibits significant improvement in soccer-specific tasks compared to the base model, with a 37.5% relative improvement for the visual question-answering task and an accuracy improvement from 11.8% to 63.5% for the downstream soccer action classification task.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures, accepted to the 11th IEEE International Workshop on Computer Vision in Sports (CVSports) at CVPR 2025; supplementary appendix included",
    "pdf_url": "https://arxiv.org/pdf/2505.13860v2",
    "published_date": "2025-05-20 03:12:21 UTC",
    "updated_date": "2025-07-07 06:34:07 UTC"
  },
  {
    "arxiv_id": "2505.13857v1",
    "title": "Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer",
    "authors": [
      "Tian Sun",
      "Yuqi Chen",
      "Baihua Zheng",
      "Weiwei Sun"
    ],
    "abstract": "In real-world applications, GPS trajectories often suffer from low sampling rates, with large and irregular intervals between consecutive GPS points. This sparse characteristic presents challenges for their direct use in GPS-based systems. This paper addresses the task of map-constrained trajectory recovery, aiming to enhance trajectory sampling rates of GPS trajectories. Previous studies commonly adopt a sequence-to-sequence framework, where an encoder captures the trajectory patterns and a decoder reconstructs the target trajectory. Within this framework, effectively representing the road network and extracting relevant trajectory features are crucial for overall performance. Despite advancements in these models, they fail to fully leverage the complex spatio-temporal dynamics present in both the trajectory and the road network.\n  To overcome these limitations, we categorize the spatio-temporal dynamics of trajectory data into two distinct aspects: spatial-temporal traffic dynamics and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce PD-GNN, which models periodic patterns and learns topologically aware dynamics concurrently for each road segment. For spatio-temporal trajectory dynamics, we present TedFormer, a time-aware Transformer that incorporates temporal dynamics for each GPS location by integrating closed-form neural ordinary differential equations into the attention mechanism. This allows TedFormer to effectively handle irregularly sampled data. Extensive experiments on three real-world datasets demonstrate the superior performance of TedTrajRec. The code is publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as a journal paper in IEEE Transactions on Intelligent Transportation Systems (T-ITS)",
    "pdf_url": "https://arxiv.org/pdf/2505.13857v1",
    "published_date": "2025-05-20 03:09:17 UTC",
    "updated_date": "2025-05-20 03:09:17 UTC"
  },
  {
    "arxiv_id": "2505.13855v1",
    "title": "Domain Gating Ensemble Networks for AI-Generated Text Detection",
    "authors": [
      "Arihant Tripathi",
      "Liam Dugan",
      "Charis Gao",
      "Maggie Huan",
      "Emma Jin",
      "Peter Zhang",
      "David Zhang",
      "Julia Zhao",
      "Chris Callison-Burch"
    ],
    "abstract": "As state-of-the-art language models continue to improve, the need for robust detection of machine-generated text becomes increasingly critical. However, current state-of-the-art machine text detectors struggle to adapt to new unseen domains and generative models. In this paper we present DoGEN (Domain Gating Ensemble Networks), a technique that allows detectors to adapt to unseen domains by ensembling a set of domain expert detector models using weights from a domain classifier. We test DoGEN on a wide variety of domains from leading benchmarks and find that it achieves state-of-the-art performance on in-domain detection while outperforming models twice its size on out-of-domain detection. We release our code and trained models to assist in future research in domain-adaptive AI detection.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.13855v1",
    "published_date": "2025-05-20 03:02:05 UTC",
    "updated_date": "2025-05-20 03:02:05 UTC"
  },
  {
    "arxiv_id": "2506.03155v2",
    "title": "Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World",
    "authors": [
      "Yu Zheng"
    ],
    "abstract": "The proliferation of artificial intelligence has enabled a diversity of applications that bridge the gap between digital and physical worlds. As physical environments are too complex to model through a single information acquisition approach, it is crucial to fuse multimodal data generated by different sources, such as sensors, devices, systems, and people, to solve a problem in the real world. Unfortunately, it is neither applicable nor sustainable to deploy new resources to collect original data from scratch for every problem. Thus, when data is inadequate in the domain of problem, it is vital to fuse knowledge from multimodal data that is already available in other domains. We call this cross-domain knowledge fusion. Existing research focus on fusing multimodal data in a single domain, supposing the knowledge from different datasets is intrinsically aligned; however, this assumption may not hold in the scenarios of cross-domain knowledge fusion. In this paper, we formally define the cross-domain multimodal data fusion problem, discussing its unique challenges, differences and advantages beyond data fusion in a single domain. We propose a four-layer framework, consisting of Domains, Links, Models and Data layers, answering three key questions:\"what to fuse\", \"why can be fused\", and \"how to fuse\". The Domains Layer selects relevant data from different domains for a given problem. The Links Layer reveals the philosophy of knowledge alignment beyond specific model structures. The Models Layer provides two knowledge fusion paradigms based on the fundamental mechanisms for processing data. The Data Layer turns data of different structures, resolutions, scales and distributions into a consistent representation that can be fed into an AI model. With this framework, we can design solutions that fuse cross-domain multimodal data effectively for solving real-world problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.03155v2",
    "published_date": "2025-05-20 02:59:34 UTC",
    "updated_date": "2025-08-08 07:46:45 UTC"
  },
  {
    "arxiv_id": "2505.13851v1",
    "title": "A Challenge to Build Neuro-Symbolic Video Agents",
    "authors": [
      "Sahil Shah",
      "Harsh Goel",
      "Sai Shankar Narasimhan",
      "Minkyu Choi",
      "S P Sharan",
      "Oguzhan Akcin",
      "Sandeep Chinchali"
    ],
    "abstract": "Modern video understanding systems excel at tasks such as scene classification, object detection, and short video retrieval. However, as video analysis becomes increasingly central to real-world applications, there is a growing need for proactive video agents for the systems that not only interpret video streams but also reason about events and take informed actions. A key obstacle in this direction is temporal reasoning: while deep learning models have made remarkable progress in recognizing patterns within individual frames or short clips, they struggle to understand the sequencing and dependencies of events over time, which is critical for action-driven decision-making. Addressing this limitation demands moving beyond conventional deep learning approaches. We posit that tackling this challenge requires a neuro-symbolic perspective, where video queries are decomposed into atomic events, structured into coherent sequences, and validated against temporal constraints. Such an approach can enhance interpretability, enable structured reasoning, and provide stronger guarantees on system behavior, all key properties for advancing trustworthy video agents. To this end, we present a grand challenge to the research community: developing the next generation of intelligent video agents that integrate three core capabilities: (1) autonomous video search and analysis, (2) seamless real-world interaction, and (3) advanced content generation. By addressing these pillars, we can transition from passive perception to intelligent video agents that reason, predict, and act, pushing the boundaries of video understanding.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13851v1",
    "published_date": "2025-05-20 02:53:21 UTC",
    "updated_date": "2025-05-20 02:53:21 UTC"
  },
  {
    "arxiv_id": "2505.13847v3",
    "title": "Forensic deepfake audio detection using segmental speech features",
    "authors": [
      "Tianle Yang",
      "Chengzhe Sun",
      "Siwei Lyu",
      "Phil Rose"
    ],
    "abstract": "This study explores the potential of using acoustic features of segmental speech sounds to detect deepfake audio. These features are highly interpretable because of their close relationship with human articulatory processes and are expected to be more difficult for deepfake models to replicate. The results demonstrate that certain segmental features commonly used in forensic voice comparison (FVC) are effective in identifying deep-fakes, whereas some global features provide little value. These findings underscore the need to approach audio deepfake detection using methods that are distinct from those employed in traditional FVC, and offer a new perspective on leveraging segmental features for this purpose. In addition, the present study proposes a speaker-specific framework for deepfake detection, which differs fundamentally from the speaker-independent systems that dominate current benchmarks. While speaker-independent frameworks aim at broad generalization, the speaker-specific approach offers advantages in forensic contexts where case-by-case interpretability and sensitivity to individual phonetic realization are essential.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted for publication in Forensic Science International",
    "pdf_url": "https://arxiv.org/pdf/2505.13847v3",
    "published_date": "2025-05-20 02:42:46 UTC",
    "updated_date": "2025-12-10 22:33:52 UTC"
  },
  {
    "arxiv_id": "2505.14733v2",
    "title": "The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute",
    "authors": [
      "Yunho Jin",
      "Gu-Yeon Wei",
      "David Brooks"
    ],
    "abstract": "Scaling large language models (LLMs) has driven significant advancements, yet it faces diminishing returns and escalating energy demands. This work explores how test-time compute (TTC) can serve as an energy-efficient complement to conventional scaling strategies by allocating additional computational resources at inference time rather than during training. Specifically, we investigate whether employing TTC can achieve superior accuracy-energy trade-offs compared to simply increasing model size. Our empirical analysis reveals that TTC surpasses traditional model scaling in accuracy/energy efficiency, with notable gains in tasks demanding complex reasoning rather than mere factual recall. Further, we identify a critical interaction between TTC performance and output sequence length, demonstrating that strategically adjusting compute resources at inference time according to query complexity can substantially enhance efficiency. Our findings advocate for TTC as a promising direction, enabling more sustainable, accurate, and adaptable deployment of future language models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14733v2",
    "published_date": "2025-05-20 02:35:59 UTC",
    "updated_date": "2025-11-09 19:54:46 UTC"
  },
  {
    "arxiv_id": "2505.13840v1",
    "title": "EfficientLLM: Efficiency in Large Language Models",
    "authors": [
      "Zhengqing Yuan",
      "Weixiang Sun",
      "Yixin Liu",
      "Huichi Zhou",
      "Rong Zhou",
      "Yiyang Li",
      "Zheyuan Zhang",
      "Wei Song",
      "Yue Huang",
      "Haolong Jia",
      "Keerthiram Murugesan",
      "Yu Wang",
      "Lifang He",
      "Jianfeng Gao",
      "Lichao Sun",
      "Yanfang Ye"
    ],
    "abstract": "Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13840v1",
    "published_date": "2025-05-20 02:27:08 UTC",
    "updated_date": "2025-05-20 02:27:08 UTC"
  },
  {
    "arxiv_id": "2505.13837v1",
    "title": "Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements",
    "authors": [
      "Gokul Puthumanaillam",
      "Paulo Padrao",
      "Jose Fuentes",
      "Leonardo Bobadilla",
      "Melkior Ornik"
    ],
    "abstract": "Robots navigating complex environments must manage uncertainty from sensor noise, environmental changes, and incomplete information, with different tasks requiring varying levels of precision in different areas. For example, precise localization may be crucial near obstacles but less critical in open spaces. We present GUIDE (Generalized Uncertainty Integration for Decision-Making and Execution), a framework that integrates these task-specific requirements into navigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning acceptable uncertainty levels to different locations, TSUMs enable robots to adapt uncertainty management based on context. When combined with reinforcement learning, GUIDE learns policies that balance task completion and uncertainty management without extensive reward engineering. Real-world tests show significant performance gains over methods lacking task-specific uncertainty awareness.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13837v1",
    "published_date": "2025-05-20 02:23:15 UTC",
    "updated_date": "2025-05-20 02:23:15 UTC"
  },
  {
    "arxiv_id": "2505.13834v2",
    "title": "Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams",
    "authors": [
      "Zhi Su",
      "Yuman Gao",
      "Emily Lukas",
      "Yunfei Li",
      "Jiaze Cai",
      "Faris Tulbah",
      "Fei Gao",
      "Chao Yu",
      "Zhongyu Li",
      "Yi Wu",
      "Koushil Sreenath"
    ],
    "abstract": "Achieving coordinated teamwork among legged robots requires both fine-grained locomotion control and long-horizon strategic decision-making. Robot soccer offers a compelling testbed for this challenge, combining dynamic, competitive, and multi-agent interactions. In this work, we present a hierarchical multi-agent reinforcement learning (MARL) framework that enables fully autonomous and decentralized quadruped robot soccer. First, a set of highly dynamic low-level skills is trained for legged locomotion and ball manipulation, such as walking, dribbling, and kicking. On top of these, a high-level strategic planning policy is trained with Multi-Agent Proximal Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning framework allows agents to adapt to diverse opponent strategies and gives rise to sophisticated team behaviors, including coordinated passing, interception, and dynamic role allocation. With an extensive ablation study, the proposed learning method shows significant advantages in the cooperative and competitive multi-agent soccer game. We deploy the learned policies to real quadruped robots relying solely on onboard proprioception and decentralized localization, with the resulting system supporting autonomous robot-robot and robot-human soccer matches on indoor and outdoor soccer courts.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "11 pages, 12 figures, CoRL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.13834v2",
    "published_date": "2025-05-20 02:20:54 UTC",
    "updated_date": "2025-08-30 02:08:59 UTC"
  },
  {
    "arxiv_id": "2505.13831v2",
    "title": "TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning",
    "authors": [
      "Zongyuan Deng",
      "Yujie Cai",
      "Qing Liu",
      "Shiyao Mu",
      "Bin Lyu",
      "Zhen Yang"
    ],
    "abstract": "The selection of base station sites is a critical challenge in 5G network planning, which requires efficient optimization of coverage, cost, user satisfaction, and practical constraints. Traditional manual methods, reliant on human expertise, suffer from inefficiencies and are limited to an unsatisfied planning-construction consistency. Existing AI tools, despite improving efficiency in certain aspects, still struggle to meet the dynamic network conditions and multi-objective needs of telecom operators' networks. To address these challenges, we propose TelePlanNet, an AI-driven framework tailored for the selection of base station sites, integrating a three-layer architecture for efficient planning and large-scale automation. By leveraging large language models (LLMs) for real-time user input processing and intent alignment with base station planning, combined with training the planning model using the improved group relative policy optimization (GRPO) reinforcement learning, the proposed TelePlanNet can effectively address multi-objective optimization, evaluates candidate sites, and delivers practical solutions. Experiments results show that the proposed TelePlanNet can improve the consistency to 78%, which is superior to the manual methods, providing telecom operators with an efficient and scalable tool that significantly advances cellular network planning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 5 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2505.13831v2",
    "published_date": "2025-05-20 02:19:10 UTC",
    "updated_date": "2025-05-27 08:33:45 UTC"
  },
  {
    "arxiv_id": "2505.13828v1",
    "title": "Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models",
    "authors": [
      "Kiarash Naghavi Khanghah",
      "Zhiling Chen",
      "Lela Romeo",
      "Qian Yang",
      "Rajiv Malhotra",
      "Farhad Imani",
      "Hongyi Xu"
    ],
    "abstract": "Additive manufacturing enables the fabrication of complex designs while minimizing waste, but faces challenges related to defects and process anomalies. This study presents a novel multimodal Retrieval-Augmented Generation-based framework that automates anomaly detection across various Additive Manufacturing processes leveraging retrieved information from literature, including images and descriptive text, rather than training datasets. This framework integrates text and image retrieval from scientific literature and multimodal generation models to perform zero-shot anomaly identification, classification, and explanation generation in a Laser Powder Bed Fusion setting. The proposed framework is evaluated on four L-PBF manufacturing datasets from Oak Ridge National Laboratory, featuring various printer makes, models, and materials. This evaluation demonstrates the framework's adaptability and generalizability across diverse images without requiring additional training. Comparative analysis using Qwen2-VL-2B and GPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini outperforms Qwen2-VL-2B and proportional random baseline in manufacturing anomalies classification. Additionally, the evaluation of the RAG system confirms that incorporating retrieval mechanisms improves average accuracy by 12% by reducing the risk of hallucination and providing additional information. The proposed framework can be continuously updated by integrating emerging research, allowing seamless adaptation to the evolving landscape of AM technologies. This scalable, automated, and zero-shot-capable framework streamlines AM anomaly analysis, enhancing efficiency and accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ASME 2025 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference IDETC/CIE2025, August 17-20, 2025, Anaheim, CA (IDETC2025-168615)",
    "pdf_url": "https://arxiv.org/pdf/2505.13828v1",
    "published_date": "2025-05-20 02:18:22 UTC",
    "updated_date": "2025-05-20 02:18:22 UTC"
  },
  {
    "arxiv_id": "2505.17073v1",
    "title": "Mechanistic Interpretability of GPT-like Models on Summarization Tasks",
    "authors": [
      "Anurag Mishra"
    ],
    "abstract": "Mechanistic interpretability research seeks to reveal the inner workings of large language models, yet most work focuses on classification or generative tasks rather than summarization. This paper presents an interpretability framework for analyzing how GPT-like models adapt to summarization tasks. We conduct differential analysis between pre-trained and fine-tuned models, quantifying changes in attention patterns and internal activations. By identifying specific layers and attention heads that undergo significant transformation, we locate the \"summarization circuit\" within the model architecture. Our findings reveal that middle layers (particularly 2, 3, and 5) exhibit the most dramatic changes, with 62% of attention heads showing decreased entropy, indicating a shift toward focused information selection. We demonstrate that targeted LoRA adaptation of these identified circuits achieves significant performance improvement over standard LoRA fine-tuning while requiring fewer training epochs. This work bridges the gap between black-box evaluation and mechanistic understanding, providing insights into how neural networks perform information selection and compression during summarization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages (6 content + 2 references/appendix), 6 figures, 2 tables; under review for the ACL 2025 Student Research Workshop",
    "pdf_url": "https://arxiv.org/pdf/2505.17073v1",
    "published_date": "2025-05-20 02:15:11 UTC",
    "updated_date": "2025-05-20 02:15:11 UTC"
  },
  {
    "arxiv_id": "2505.13820v2",
    "title": "Structured Agent Distillation for Large Language Model",
    "authors": [
      "Jun Liu",
      "Zhenglun Kong",
      "Peiyan Dong",
      "Changdi Yang",
      "Tianqi Li",
      "Hao Tang",
      "Geng Yuan",
      "Wei Niu",
      "Wenbin Zhang",
      "Pu Zhao",
      "Xue Lin",
      "Dong Huang",
      "Yanzhi Wang"
    ],
    "abstract": "Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher's behavior. This structure-aware supervision enables compact agents to better replicate the teacher's decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13820v2",
    "published_date": "2025-05-20 02:01:55 UTC",
    "updated_date": "2025-09-30 14:52:40 UTC"
  },
  {
    "arxiv_id": "2505.13814v2",
    "title": "Articulatory Feature Prediction from Surface EMG during Speech Production",
    "authors": [
      "Jihwan Lee",
      "Kevin Huang",
      "Kleanthis Avramidis",
      "Simon Pistrosch",
      "Monica Gonzalez-Machorro",
      "Yoonjeong Lee",
      "Björn Schuller",
      "Louis Goldstein",
      "Shrikanth Narayanan"
    ],
    "abstract": "We present a model for predicting articulatory features from surface electromyography (EMG) signals during speech production. The proposed model integrates convolutional layers and a Transformer block, followed by separate predictors for articulatory features. Our approach achieves a high prediction correlation of approximately 0.9 for most articulatory features. Furthermore, we demonstrate that these predicted articulatory features can be decoded into intelligible speech waveforms. To our knowledge, this is the first method to decode speech waveforms from surface EMG via articulatory features, offering a novel approach to EMG-based speech synthesis. Additionally, we analyze the relationship between EMG electrode placement and articulatory feature predictability, providing knowledge-driven insights for optimizing EMG electrode configurations. The source code and decoded speech samples are publicly available.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted for Interspeech2025",
    "pdf_url": "https://arxiv.org/pdf/2505.13814v2",
    "published_date": "2025-05-20 01:50:05 UTC",
    "updated_date": "2025-05-29 03:59:36 UTC"
  },
  {
    "arxiv_id": "2505.13808v1",
    "title": "RAG/LLM Augmented Switching Driven Polymorphic Metaheuristic Framework",
    "authors": [
      "Faramarz Safi Esfahani",
      "Ghassan Beydoun",
      "Morteza Saberi",
      "Brad McCusker",
      "Biswajeet Pradhan"
    ],
    "abstract": "Metaheuristic algorithms are widely used for solving complex optimization problems, yet their effectiveness is often constrained by fixed structures and the need for extensive tuning. The Polymorphic Metaheuristic Framework (PMF) addresses this limitation by introducing a self-adaptive metaheuristic switching mechanism driven by real-time performance feedback and dynamic algorithmic selection. PMF leverages the Polymorphic Metaheuristic Agent (PMA) and the Polymorphic Metaheuristic Selection Agent (PMSA) to dynamically select and transition between metaheuristic algorithms based on key performance indicators, ensuring continuous adaptation. This approach enhances convergence speed, adaptability, and solution quality, outperforming traditional metaheuristics in high-dimensional, dynamic, and multimodal environments. Experimental results on benchmark functions demonstrate that PMF significantly improves optimization efficiency by mitigating stagnation and balancing exploration-exploitation strategies across various problem landscapes. By integrating AI-driven decision-making and self-correcting mechanisms, PMF paves the way for scalable, intelligent, and autonomous optimization frameworks, with promising applications in engineering, logistics, and complex decision-making systems.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13808v1",
    "published_date": "2025-05-20 01:41:22 UTC",
    "updated_date": "2025-05-20 01:41:22 UTC"
  },
  {
    "arxiv_id": "2505.13805v1",
    "title": "ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech",
    "authors": [
      "Yu Pan",
      "Yanni Hu",
      "Yuguang Yang",
      "Jixun Yao",
      "Jianhao Ye",
      "Hongbin Zhou",
      "Lei Ma",
      "Jianjun Zhao"
    ],
    "abstract": "Despite great advances, achieving high-fidelity emotional voice conversion (EVC) with flexible and interpretable control remains challenging. This paper introduces ClapFM-EVC, a novel EVC framework capable of generating high-quality converted speech driven by natural language prompts or reference speech with adjustable emotion intensity. We first propose EVC-CLAP, an emotional contrastive language-audio pre-training model, guided by natural language prompts and categorical labels, to extract and align fine-grained emotional elements across speech and text modalities. Then, a FuEncoder with an adaptive intensity gate is presented to seamless fuse emotional features with Phonetic PosteriorGrams from a pre-trained ASR model. To further improve emotion expressiveness and speech naturalness, we propose a flow matching model conditioned on these captured features to reconstruct Mel-spectrogram of source speech. Subjective and objective evaluations validate the effectiveness of ClapFM-EVC.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by InterSpeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.13805v1",
    "published_date": "2025-05-20 01:34:29 UTC",
    "updated_date": "2025-05-20 01:34:29 UTC"
  },
  {
    "arxiv_id": "2505.14728v1",
    "title": "MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models",
    "authors": [
      "Xiao Lin",
      "Zhining Liu",
      "Ze Yang",
      "Gaotang Li",
      "Ruizhong Qiu",
      "Shuke Wang",
      "Hui Liu",
      "Haotian Li",
      "Sumit Keswani",
      "Vishwa Pardeshi",
      "Huijun Zhao",
      "Wei Fan",
      "Hanghang Tong"
    ],
    "abstract": "Warning: This paper contains examples of harmful language and images. Reader discretion is advised. Recently, vision-language models have demonstrated increasing influence in morally sensitive domains such as autonomous driving and medical analysis, owing to their powerful multimodal reasoning capabilities. As these models are deployed in high-stakes real-world applications, it is of paramount importance to ensure that their outputs align with human moral values and remain within moral boundaries. However, existing work on moral alignment either focuses solely on textual modalities or relies heavily on AI-generated images, leading to distributional biases and reduced realism. To overcome these limitations, we introduce MORALISE, a comprehensive benchmark for evaluating the moral alignment of vision-language models (VLMs) using diverse, expert-verified real-world data. We begin by proposing a comprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory, spanning the personal, interpersonal, and societal moral domains encountered in everyday life. Built on this framework, we manually curate 2,481 high-quality image-text pairs, each annotated with two fine-grained labels: (1) topic annotation, identifying the violated moral topic(s), and (2) modality annotation, indicating whether the violation arises from the image or the text. For evaluation, we encompass two tasks, \\textit{moral judgment} and \\textit{moral norm attribution}, to assess models' awareness of moral violations and their reasoning ability on morally salient content. Extensive experiments on 19 popular open- and closed-source VLMs show that MORALISE poses a significant challenge, revealing persistent moral limitations in current state-of-the-art models. The full benchmark is publicly available at https://huggingface.co/datasets/Ze1025/MORALISE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages, 11 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.14728v1",
    "published_date": "2025-05-20 01:11:17 UTC",
    "updated_date": "2025-05-20 01:11:17 UTC"
  },
  {
    "arxiv_id": "2505.13794v1",
    "title": "LLM-based Evaluation Policy Extraction for Ecological Modeling",
    "authors": [
      "Qi Cheng",
      "Licheng Liu",
      "Qing Zhu",
      "Runlong Yu",
      "Zhenong Jin",
      "Yiqun Xie",
      "Xiaowei Jia"
    ],
    "abstract": "Evaluating ecological time series is critical for benchmarking model performance in many important applications, including predicting greenhouse gas fluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles. Traditional numerical metrics (e.g., R-squared, root mean square error) have been widely used to quantify the similarity between modeled and observed ecosystem variables, but they often fail to capture domain-specific temporal patterns critical to ecological processes. As a result, these methods are often accompanied by expert visual inspection, which requires substantial human labor and limits the applicability to large-scale evaluation. To address these challenges, we propose a novel framework that integrates metric learning with large language model (LLM)-based natural language policy extraction to develop interpretable evaluation criteria. The proposed method processes pairwise annotations and implements a policy optimization mechanism to generate and combine different assessment metrics. The results obtained on multiple datasets for evaluating the predictions of crop gross primary production and carbon dioxide flux have confirmed the effectiveness of the proposed method in capturing target assessment preferences, including both synthetically generated and expert-annotated model comparisons. The proposed framework bridges the gap between numerical metrics and expert knowledge while providing interpretable evaluation policies that accommodate the diverse needs of different ecosystem modeling studies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13794v1",
    "published_date": "2025-05-20 01:02:29 UTC",
    "updated_date": "2025-05-20 01:02:29 UTC"
  },
  {
    "arxiv_id": "2505.13792v1",
    "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation",
    "authors": [
      "Siddhant Bhambri",
      "Upasana Biswas",
      "Subbarao Kambhampati"
    ],
    "abstract": "Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.13792v1",
    "published_date": "2025-05-20 00:49:19 UTC",
    "updated_date": "2025-05-20 00:49:19 UTC"
  },
  {
    "arxiv_id": "2505.14726v1",
    "title": "MedBLIP: Fine-tuning BLIP for Medical Image Captioning",
    "authors": [
      "Manshi Limbu",
      "Diwita Banerjee"
    ],
    "abstract": "Medical image captioning is a challenging task that requires generating clinically accurate and semantically meaningful descriptions of radiology images. While recent vision-language models (VLMs) such as BLIP, BLIP2, Gemini and ViT-GPT2 show strong performance on natural image datasets, they often produce generic or imprecise captions when applied to specialized medical domains. In this project, we explore the effectiveness of fine-tuning the BLIP model on the ROCO dataset for improved radiology captioning. We compare the fine-tuned BLIP against its zero-shot version, BLIP-2 base, BLIP-2 Instruct and a ViT-GPT2 transformer baseline. Our results demonstrate that domain-specific fine-tuning on BLIP significantly improves performance across both quantitative and qualitative evaluation metrics. We also visualize decoder cross-attention maps to assess interpretability and conduct an ablation study to evaluate the contributions of encoder-only and decoder-only fine-tuning. Our findings highlight the importance of targeted adaptation for medical applications and suggest that decoder-only fine-tuning (encoder-frozen) offers a strong performance baseline with 5% lower training time than full fine-tuning, while full model fine-tuning still yields the best results overall.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14726v1",
    "published_date": "2025-05-20 00:49:08 UTC",
    "updated_date": "2025-05-20 00:49:08 UTC"
  },
  {
    "arxiv_id": "2505.13787v2",
    "title": "Preference Learning with Lie Detectors can Induce Honesty or Evasion",
    "authors": [
      "Chris Cundy",
      "Adam Gleave"
    ],
    "abstract": "As AI systems become more capable, deceptive behaviors can undermine evaluation and mislead users at deployment. Recent work has shown that lie detectors can accurately classify deceptive behavior, but they are not typically used in the training pipeline due to concerns around contamination and objective hacking. We examine these concerns by incorporating a lie detector into the labelling step of LLM post-training and evaluating whether the learned policy is genuinely more honest, or instead learns to fool the lie detector while remaining deceptive. Using DolusChat, a novel 65k-example dataset with paired truthful/deceptive responses, we identify three key factors that determine the honesty of learned policies: amount of exploration during preference learning, lie detector accuracy, and KL regularization strength. We find that preference learning with lie detectors and GRPO can lead to policies which evade lie detectors, with deception rates of over 85\\%. However, if the lie detector true positive rate (TPR) or KL regularization is sufficiently high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO) consistently lead to deception rates under 25\\% for realistic TPRs. Our results illustrate a more complex picture than previously assumed: depending on the context, lie-detector-enhanced training can be a powerful tool for scalable oversight, or a counterproductive method encouraging undetectable misalignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.13787v2",
    "published_date": "2025-05-20 00:31:53 UTC",
    "updated_date": "2025-11-18 00:25:34 UTC"
  }
]
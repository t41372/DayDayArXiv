[
  {
    "arxiv_id": "2505.20579v5",
    "title": "The challenge of hidden gifts in multi-agent reinforcement learning",
    "authors": [
      "Dane Malenfant",
      "Blake A. Richards"
    ],
    "abstract": "Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These ``hidden gifts'' represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus this act for others is a ``hidden gift''. We show that several different state-of-the-art MARL algorithms, including MARL specific architectures, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that decentralized actor-critic policy gradient agents can succeed when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for policy gradient agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of ``hidden gifts'', and demonstrate that self learning-awareness in decentralized agents can benefit these settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "Added LOLA baselines to appendix, new corollary proof on correction term not conflicting with individual objectives, related works on multi-objective RL and coordination MARL, expanded the contraposition appendix experiment, moved key drop rate experiments to appendix and aligned first success plots with key-drop plots",
    "pdf_url": "https://arxiv.org/pdf/2505.20579v5",
    "published_date": "2025-05-26 23:28:52 UTC",
    "updated_date": "2025-09-30 20:59:05 UTC"
  },
  {
    "arxiv_id": "2505.20578v1",
    "title": "Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL",
    "authors": [
      "Xingyu Chen",
      "Shihao Ma",
      "Runsheng Lin",
      "Jiecong Lin",
      "Bo Wang"
    ],
    "abstract": "Designing regulatory DNA sequences that achieve precise cell-type-specific gene expression is crucial for advancements in synthetic biology, gene therapy and precision medicine. Although transformer-based language models (LMs) can effectively capture patterns in regulatory DNA, their generative approaches often struggle to produce novel sequences with reliable cell-specific activity. Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL) framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating regulatory sequence design as a biologically informed constrained optimization problem, we apply RL to autoregressive genomic LMs, enabling the models to iteratively refine sequences that maximize regulatory activity in targeted cell types while constraining off-target effects. Our evaluation on human promoters and enhancers demonstrates that Ctrl-DNA consistently outperforms existing generative and RL-based approaches, generating high-fitness regulatory sequences and achieving state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences capture key cell-type-specific transcription factor binding sites (TFBS), short DNA motifs recognized by regulatory proteins that control gene expression, demonstrating the biological plausibility of the generated sequences.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20578v1",
    "published_date": "2025-05-26 23:27:50 UTC",
    "updated_date": "2025-05-26 23:27:50 UTC"
  },
  {
    "arxiv_id": "2505.23803v1",
    "title": "MultiPhishGuard: An LLM-based Multi-Agent System for Phishing Email Detection",
    "authors": [
      "Yinuo Xue",
      "Eric Spero",
      "Yun Sing Koh",
      "Giovanni Russello"
    ],
    "abstract": "Phishing email detection faces critical challenges from evolving adversarial tactics and heterogeneous attack patterns. Traditional detection methods, such as rule-based filters and denylists, often struggle to keep pace with these evolving tactics, leading to false negatives and compromised security. While machine learning approaches have improved detection accuracy, they still face challenges adapting to novel phishing strategies. We present MultiPhishGuard, a dynamic LLM-based multi-agent detection system that synergizes specialized expertise with adversarial-aware reinforcement learning. Our framework employs five cooperative agents (text, URL, metadata, explanation simplifier, and adversarial agents) with automatically adjusted decision weights powered by a Proximal Policy Optimization reinforcement learning algorithm. To address emerging threats, we introduce an adversarial training loop featuring an adversarial agent that generates subtle context-aware email variants, creating a self-improving defense ecosystem and enhancing system robustness. Experimental evaluations on public datasets demonstrate that MultiPhishGuard significantly outperforms Chain-of-Thoughts, single-agent baselines and state-of-the-art detectors, as validated by ablation studies and comparative analyses. Experiments demonstrate that MultiPhishGuard achieves high accuracy (97.89\\%) with low false positive (2.73\\%) and false negative rates (0.20\\%). Additionally, we incorporate an explanation simplifier agent, which provides users with clear and easily understandable explanations for why an email is classified as phishing or legitimate. This work advances phishing defense through dynamic multi-agent collaboration and generative adversarial resilience.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23803v1",
    "published_date": "2025-05-26 23:27:15 UTC",
    "updated_date": "2025-05-26 23:27:15 UTC"
  },
  {
    "arxiv_id": "2505.20573v2",
    "title": "Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners",
    "authors": [
      "Jiabao Ji",
      "Yongchao Chen",
      "Yang Zhang",
      "Ramana Rao Kompella",
      "Chuchu Fan",
      "Gaowen Liu",
      "Shiyu Chang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated strong performance in various robot control tasks. However, their deployment in real-world applications remains constrained. Even state-ofthe-art LLMs, such as GPT-o4mini, frequently produce invalid action plans that violate physical constraints, such as directing a robot to an unreachable location or causing collisions between robots. This issue primarily arises from a lack of awareness of these physical constraints during the reasoning process. To address this issue, we propose a novel framework that integrates reinforcement learning with verifiable rewards (RLVR) to incentivize knowledge of physical constraints into LLMs to induce constraints-aware reasoning during plan generation. In this approach, only valid action plans that successfully complete a control task receive positive rewards. We applied our method to two small-scale LLMs: a non-reasoning Qwen2.5-3B-Instruct and a reasoning Qwen3-4B. The experiment results demonstrate that constraint-aware small LLMs largely outperform large-scale models without constraints, grounded on both the BoxNet task and a newly developed BoxNet3D environment built using MuJoCo. This work highlights the effectiveness of grounding even small LLMs with physical constraints to enable scalable and efficient multi-robot control in complex, physically constrained environments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20573v2",
    "published_date": "2025-05-26 23:14:16 UTC",
    "updated_date": "2025-06-03 19:33:56 UTC"
  },
  {
    "arxiv_id": "2505.20569v2",
    "title": "Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models",
    "authors": [
      "Jihoon Lee",
      "Min Song"
    ],
    "abstract": "Despite significant advancements in Large Vision-Language Models, Object Hallucination (OH) remains a persistent challenge. Building upon prior studies on contrastive decoding that address this issue without requiring additional model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an advanced method to suppress OH. RVCD leverages both negative and positive images at the logit level, explicitly referencing AI-generated images designed to represent a single concept. Our approach demonstrates substantial improvements over existing decoding-based methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ACL 2025 Findings camera-ready version. Code is released at https://github.com/JiHoonLee9898/RVCD",
    "pdf_url": "https://arxiv.org/pdf/2505.20569v2",
    "published_date": "2025-05-26 23:06:54 UTC",
    "updated_date": "2025-05-29 14:24:24 UTC"
  },
  {
    "arxiv_id": "2505.23802v2",
    "title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks",
    "authors": [
      "Suhana Bedi",
      "Hejie Cui",
      "Miguel Fuentes",
      "Alyssa Unell",
      "Michael Wornow",
      "Juan M. Banda",
      "Nikesh Kotecha",
      "Timothy Keyes",
      "Yifan Mai",
      "Mert Oez",
      "Hao Qiu",
      "Shrey Jain",
      "Leonardo Schettini",
      "Mehr Kashyap",
      "Jason Alan Fries",
      "Akshay Swaminathan",
      "Philip Chung",
      "Fateme Nateghi",
      "Asad Aali",
      "Ashwin Nayak",
      "Shivam Vedak",
      "Sneha S. Jain",
      "Birju Patel",
      "Oluseyi Fayanju",
      "Shreya Shah",
      "Ethan Goh",
      "Dong-han Yao",
      "Brian Soetikno",
      "Eduardo Reis",
      "Sergios Gatidis",
      "Vasu Divi",
      "Robson Capasso",
      "Rachna Saralkar",
      "Chia-Chun Chiang",
      "Jenelle Jindal",
      "Tho Pham",
      "Faraz Ghoddusi",
      "Steven Lin",
      "Albert S. Chiou",
      "Christy Hong",
      "Mohana Roy",
      "Michael F. Gensheimer",
      "Hinesh Patel",
      "Kevin Schulman",
      "Dev Dash",
      "Danton Char",
      "Lance Downing",
      "Francois Grolleau",
      "Kameron Black",
      "Bethel Mieso",
      "Aydin Zahedivash",
      "Wen-wai Yim",
      "Harshita Sharma",
      "Tony Lee",
      "Hannah Kirsch",
      "Jennifer Lee",
      "Nerissa Ambers",
      "Carlene Lugtu",
      "Aditya Sharma",
      "Bilal Mawji",
      "Alex Alekseyev",
      "Vicky Zhou",
      "Vikas Kakkar",
      "Jarrod Helzer",
      "Anurang Revri",
      "Yair Bannett",
      "Roxana Daneshjou",
      "Jonathan Chen",
      "Emily Alsentzer",
      "Keith Morse",
      "Nirmal Ravi",
      "Nima Aghaeepour",
      "Vanessa Kennedy",
      "Akshay Chaudhari",
      "Thomas Wang",
      "Sanmi Koyejo",
      "Matthew P. Lungren",
      "Eric Horvitz",
      "Percy Liang",
      "Mike Pfeffer",
      "Nigam H. Shah"
    ],
    "abstract": "While large language models (LLMs) achieve near-perfect scores on medical licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world clinical practice. We introduce MedHELM, an extensible evaluation framework for assessing LLM performance for medical tasks with three key contributions. First, a clinician-validated taxonomy spanning 5 categories, 22 subcategories, and 121 tasks developed with 29 clinicians. Second, a comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly formulated) providing complete coverage of all categories and subcategories in the taxonomy. Third, a systematic comparison of LLMs with improved evaluation methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9 frontier LLMs, using the 35 benchmarks, revealed significant performance variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64% win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved comparable results at 40% lower estimated computational cost. On a normalized accuracy scale (0-1), most models performed strongly in Clinical Note Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83), moderately in Medical Research Assistance (0.65-0.75), and generally lower in Clinical Decision Support (0.56-0.72) and Administration & Workflow (0.53-0.63). Our LLM-jury evaluation method achieved good agreement with clinician ratings (ICC = 0.47), surpassing both average clinician-clinician agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top models at lower estimated cost. These findings highlight the importance of real-world, task-specific evaluation for medical use of LLMs and provides an open source framework to enable this.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23802v2",
    "published_date": "2025-05-26 22:55:49 UTC",
    "updated_date": "2025-06-02 04:19:10 UTC"
  },
  {
    "arxiv_id": "2505.20561v2",
    "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning",
    "authors": [
      "Shenao Zhang",
      "Yaqing Wang",
      "Yinxiao Liu",
      "Tianqi Liu",
      "Peter Grabowski",
      "Eugene Ie",
      "Zhaoran Wang",
      "Yunxuan Li"
    ],
    "abstract": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as rethinking and error correction, as a form of in-context exploration. However, the Markovian policy obtained from conventional RL training does not give rise to reflective exploration behaviors since the policy depends on the history only through the state and therefore has no incentive to enrich identical states with additional context. Instead, RL exploration is only useful during training to learn the optimal policy in a trial-and-error manner. Therefore, it remains unclear whether reflective reasoning will emerge during RL, or why it is beneficial. To remedy this, we recast reflective exploration within a Bayesian RL framework, which optimizes the expected return under a posterior distribution over Markov decision processes induced by the training data. This Bayesian formulation admits uncertainty-adaptive policies that, through belief updates, naturally incentivize information-gathering actions and induce self-reflection behaviors. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms conventional RL approaches, achieving superior test-time performance and token efficiency. Our code is available at https://github.com/shenao-zhang/BARL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20561v2",
    "published_date": "2025-05-26 22:51:00 UTC",
    "updated_date": "2025-12-07 03:32:32 UTC"
  },
  {
    "arxiv_id": "2506.15690v3",
    "title": "LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs",
    "authors": [
      "Tianyu Wang",
      "Akira Horiguchi",
      "Lingyou Pang",
      "Carey E. Priebe"
    ],
    "abstract": "The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15690v3",
    "published_date": "2025-05-26 22:10:52 UTC",
    "updated_date": "2025-07-24 05:08:02 UTC"
  },
  {
    "arxiv_id": "2505.20522v2",
    "title": "Scaling over Scaling: Exploring Test-Time Scaling Plateau in Large Reasoning Models",
    "authors": [
      "Jian Wang",
      "Boyan Zhu",
      "Chak Tou Leong",
      "Yongqi Li",
      "Wenjie Li"
    ],
    "abstract": "Large reasoning models (LRMs) have exhibited the capacity of enhancing reasoning performance via internal test-time scaling. Building upon this, a promising direction is to further scale test-time compute to unlock even greater reasoning capabilities. However, as we push these scaling boundaries, systematically understanding the practical limits and achieving optimal resource allocation becomes a critical challenge. In this paper, we investigate the scaling plateau of test-time scaling and introduce the Test-Time Scaling Performance Model (TTSPM). We theoretically analyze two fundamental paradigms for such extended scaling, parallel scaling and sequential scaling, from a probabilistic modeling perspective. Our primary contribution is the derivation of the saturation point on the scaling budget for both strategies, identifying thresholds beyond which additional computation yields diminishing returns. Remarkably, despite their distinct mechanisms, both paradigms converge to a unified mathematical structure in their upper bounds. We empirically validate our theoretical findings on challenging reasoning benchmarks, including AIME, MATH-500, and GPQA, demonstrating the practical utility of these bounds for test-time resource allocation. We hope that this work provides insights into the cost-benefit trade-offs of test-time scaling, guiding the development of more resource-efficient inference strategies for large reasoning models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2505.20522v2",
    "published_date": "2025-05-26 20:58:45 UTC",
    "updated_date": "2025-06-07 22:18:32 UTC"
  },
  {
    "arxiv_id": "2505.23801v1",
    "title": "SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks",
    "authors": [
      "Sajid Hussain",
      "Muhammad Sohail",
      "Nauman Ali Khan"
    ],
    "abstract": "Background: Federated Learning (FL) has emerged as a promising paradigm for training machine learning models while preserving data privacy. However, applying FL to Natural Language Processing (NLP) tasks presents unique challenges due to semantic heterogeneity across clients, vocabulary mismatches, and varying resource constraints on edge devices. Objectives: This paper introduces SEMFED, a novel semantic-aware resource-efficient federated learning framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED incorporates three key innovations: (1) a semantic-aware client selection mechanism that balances semantic diversity with resource constraints, (2) adaptive NLP-specific model architectures tailored to device capabilities while preserving semantic information, and (3) a communication-efficient semantic feature compression technique that significantly reduces bandwidth requirements. Results: Experimental results on various NLP classification tasks demonstrate that SEMFED achieves an 80.5% reduction in communication costs while maintaining model accuracy above 98%, outperforming state-of-the-art FL approaches. Conclusion: SEMFED effectively manages heterogeneous client environments with varying computational resources, network reliability, and semantic data distributions, making it particularly suitable for real-world federated NLP deployments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.23801v1",
    "published_date": "2025-05-26 20:58:13 UTC",
    "updated_date": "2025-05-26 20:58:13 UTC"
  },
  {
    "arxiv_id": "2505.20521v2",
    "title": "Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting",
    "authors": [
      "Ana Rita Ortigoso",
      "Gabriel Vieira",
      "Daniel Fuentes",
      "Luis Frazão",
      "Nuno Costa",
      "António Pereira"
    ],
    "abstract": "This paper presents Project Riley, a novel multimodal and multi-model conversational AI architecture oriented towards the simulation of reasoning influenced by emotional states. Drawing inspiration from Pixar's Inside Out, the system comprises five distinct emotional agents - Joy, Sadness, Fear, Anger, and Disgust - that engage in structured multi-round dialogues to generate, criticise, and iteratively refine responses. A final reasoning mechanism synthesises the contributions of these agents into a coherent output that either reflects the dominant emotion or integrates multiple perspectives. The architecture incorporates both textual and visual large language models (LLMs), alongside advanced reasoning and self-refinement processes. A functional prototype was deployed locally in an offline environment, optimised for emotional expressiveness and computational efficiency. From this initial prototype, another one emerged, called Armando, which was developed for use in emergency contexts, delivering emotionally calibrated and factually accurate information through the integration of Retrieval-Augmented Generation (RAG) and cumulative context tracking. The Project Riley prototype was evaluated through user testing, in which participants interacted with the chatbot and completed a structured questionnaire assessing three dimensions: Emotional Appropriateness, Clarity and Utility, and Naturalness and Human-likeness. The results indicate strong performance in structured scenarios, particularly with respect to emotional alignment and communicative clarity.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "28 pages, 5 figures. Submitted for review to Information Fusion",
    "pdf_url": "https://arxiv.org/pdf/2505.20521v2",
    "published_date": "2025-05-26 20:53:53 UTC",
    "updated_date": "2025-09-08 16:40:29 UTC"
  },
  {
    "arxiv_id": "2505.21559v1",
    "title": "Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework",
    "authors": [
      "Julien Soulé",
      "Jean-Paul Jamont",
      "Michel Occello",
      "Louis-Marie Traonouez",
      "Paul Théron"
    ],
    "abstract": "In cloud-native systems, Kubernetes clusters with interdependent services often face challenges to their operational resilience due to poor workload management issues such as resource blocking, bottlenecks, or continuous pod crashes. These vulnerabilities are further amplified in adversarial scenarios, such as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal Pod Autoscaling (HPA) approaches struggle to address such dynamic conditions, while reinforcement learning-based methods, though more adaptable, typically optimize single goals like latency or resource usage, neglecting broader failure scenarios. We propose decomposing the overarching goal of maintaining operational resilience into failure-specific sub-goals delegated to collaborative agents, collectively forming an HPA Multi-Agent System (MAS). We introduce an automated, four-phase online framework for HPA MAS design: 1) modeling a digital twin built from cluster traces; 2) training agents in simulation using roles and missions tailored to failure contexts; 3) analyzing agent behaviors for explainability; and 4) transferring learned policies to the real cluster. Experimental results demonstrate that the generated HPA MASs outperform three state-of-the-art HPA systems in sustaining operational resilience under various adversarial conditions in a proposed complex cluster.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21559v1",
    "published_date": "2025-05-26 20:39:31 UTC",
    "updated_date": "2025-05-26 20:39:31 UTC"
  },
  {
    "arxiv_id": "2505.21558v1",
    "title": "A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification",
    "authors": [
      "Elhoucine Elfatimia",
      "Recep Eryigitb",
      "Lahcen Elfatimi"
    ],
    "abstract": "Agricultural research has accelerated in recent years, yet farmers often lack the time and resources for on-farm research due to the demands of crop production and farm operations. Seed classification offers valuable insights into quality control, production efficiency, and impurity detection. Early identification of seed types is critical to reducing the cost and risk associated with field emergence, which can lead to yield losses or disruptions in downstream processes like harvesting. Seed sampling supports growers in monitoring and managing seed quality, improving precision in determining seed purity levels, guiding management adjustments, and enhancing yield estimations. This study proposes a novel convolutional neural network (CNN)-based framework for the efficient classification of ten common Brassica seed types. The approach addresses the inherent challenge of texture similarity in seed images using a custom-designed CNN architecture. The model's performance was evaluated against several pre-trained state-of-the-art architectures, with adjustments to layer configurations for optimized classification. Experimental results using our collected Brassica seed dataset demonstrate that the proposed model achieved a high accuracy rate of 93 percent.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "11 Figure",
    "pdf_url": "https://arxiv.org/pdf/2505.21558v1",
    "published_date": "2025-05-26 20:18:45 UTC",
    "updated_date": "2025-05-26 20:18:45 UTC"
  },
  {
    "arxiv_id": "2505.20507v2",
    "title": "Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset",
    "authors": [
      "Elias Arbash",
      "Ahmed Jamal Afifi",
      "Ymane Belahsen",
      "Margret Fuchs",
      "Pedram Ghamisi",
      "Paul Scheunders",
      "Richard Gloaguen"
    ],
    "abstract": "The global challenge of sustainable recycling demands automated, fast, and accurate, state-of-the-art (SOTA) material detection systems that act as a bedrock for a circular economy. Democratizing access to these cutting-edge solutions that enable real-time waste analysis is essential for scaling up recycling efforts and fostering the Green Deal. In response, we introduce \\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to accelerate the recovery of critical raw materials through accurate electrolyzer materials classification. The dataset comprises 55 co-registered high-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning the 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and 424,169 labeled ones. This enables non-invasive spectral analysis of shredded electrolyzer samples, supporting quantitative and qualitative material classification and spectral properties investigation. We evaluate a suite of baseline machine learning (ML) methods alongside SOTA transformer-based deep learning (DL) architectures, including Vision Transformer, SpectralFormer, and the Multimodal Fusion Transformer, to investigate architectural bottlenecks for further efficiency optimisation when deploying transformers in material identification. We implement zero-shot detection techniques and majority voting across pixel-level predictions to establish object-level classification robustness. In adherence to the FAIR data principles, the electrolyzers-HSI dataset and accompanying codebase are openly available at https://github.com/hifexplo/Electrolyzers-HSI and https://rodare.hzdr.de/record/3668, supporting reproducible research and facilitating the broader adoption of smart and sustainable e-waste recycling solutions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20507v2",
    "published_date": "2025-05-26 20:16:38 UTC",
    "updated_date": "2025-06-05 10:02:51 UTC"
  },
  {
    "arxiv_id": "2505.20506v1",
    "title": "ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis",
    "authors": [
      "Hawau Olamide Toyin",
      "Rufael Marew",
      "Humaid Alblooshi",
      "Samar M. Magdy",
      "Hanan Aldarmaki"
    ],
    "abstract": "We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech corpus with diacritized transcriptions, intended for multi-speaker speech synthesis, and can be useful for other tasks such as speech-based diacritic restoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a new professionally recorded set from six voice talents with diverse demographics, (2) a modified subset of the Arabic Speech Corpus; and (3) high-quality synthetic speech from two commercial systems. The complete corpus consists of a total of 83.52 hours of speech across 11 voices; around 10 hours consist of human voices from 7 speakers. We train three open-source TTS and two voice conversion systems to illustrate the use cases of the dataset. The corpus is available for research use.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at INTERSPEECH 2025 The dataset is available at https://huggingface.co/datasets/MBZUAI/ArVoice",
    "pdf_url": "https://arxiv.org/pdf/2505.20506v1",
    "published_date": "2025-05-26 20:15:15 UTC",
    "updated_date": "2025-05-26 20:15:15 UTC"
  },
  {
    "arxiv_id": "2505.20503v1",
    "title": "Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review",
    "authors": [
      "Matthew Lisondra",
      "Beno Benhabib",
      "Goldie Nejat"
    ],
    "abstract": "Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interactions, robots can improve understanding, adapt to, and execute complex tasks in dynamic real-world environments. However, embodied AI in mobile service robots continues to face key challenges, including multimodal sensor fusion, real-time decision-making under uncertainty, task generalization, and effective human-robot interactions (HRI). In this paper, we present the first systematic review of the integration of foundation models in mobile service robotics, identifying key open challenges in embodied AI and examining how foundation models can address them. Namely, we explore the role of such models in enabling real-time sensor fusion, language-conditioned control, and adaptive task execution. Furthermore, we discuss real-world applications in the domestic assistance, healthcare, and service automation sectors, demonstrating the transformative impact of foundation models on service robotics. We also include potential future research directions, emphasizing the need for predictive scaling laws, autonomous long-term adaptation, and cross-embodiment generalization to enable scalable, efficient, and robust deployment of foundation models in human-centric robotic systems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20503v1",
    "published_date": "2025-05-26 20:08:09 UTC",
    "updated_date": "2025-05-26 20:08:09 UTC"
  },
  {
    "arxiv_id": "2505.20500v1",
    "title": "Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism",
    "authors": [
      "Naba Rizvi",
      "Harper Strickland",
      "Saleha Ahmedi",
      "Aekta Kallepalli",
      "Isha Khirwadkar",
      "William Wu",
      "Imani N. S. Munyaka",
      "Nedjma Ousidhoum"
    ],
    "abstract": "Large language models (LLMs) are increasingly used in decision-making tasks like résumé screening and content moderation, giving them the power to amplify or suppress certain perspectives. While previous research has identified disability-related biases in LLMs, little is known about how they conceptualize ableism or detect it in text. We evaluate the ability of four LLMs to identify nuanced ableism directed at autistic individuals. We examine the gap between their understanding of relevant terminology and their effectiveness in recognizing ableist content in context. Our results reveal that LLMs can identify autism-related language but often miss harmful or offensive connotations. Further, we conduct a qualitative comparison of human and LLM explanations. We find that LLMs tend to rely on surface-level keyword matching, leading to context misinterpretations, in contrast to human annotators who consider context, speaker identity, and potential impact. On the other hand, both LLMs and humans agree on the annotation scheme, suggesting that a binary classification is adequate for evaluating LLM performance, which is consistent with findings from prior studies involving human annotators.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20500v1",
    "published_date": "2025-05-26 20:01:44 UTC",
    "updated_date": "2025-05-26 20:01:44 UTC"
  },
  {
    "arxiv_id": "2505.20487v1",
    "title": "InFact: Informativeness Alignment for Improved LLM Factuality",
    "authors": [
      "Roi Cohen",
      "Russa Biswas",
      "Gerard de Melo"
    ],
    "abstract": "Factual completeness is a general term that captures how detailed and informative a factually correct text is. For instance, the factual sentence ``Barack Obama was born in the United States'' is factually correct, though less informative than the factual sentence ``Barack Obama was born in Honolulu, Hawaii, United States''. Despite the known fact that LLMs tend to hallucinate and generate factually incorrect text, they might also tend to choose to generate factual text that is indeed factually correct and yet less informative than other, more informative choices. In this work, we tackle this problem by proposing an informativeness alignment mechanism. This mechanism takes advantage of recent factual benchmarks to propose an informativeness alignment objective. This objective prioritizes answers that are both correct and informative. A key finding of our work is that when training a model to maximize this objective or optimize its preference, we can improve not just informativeness but also factuality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20487v1",
    "published_date": "2025-05-26 19:46:05 UTC",
    "updated_date": "2025-05-26 19:46:05 UTC"
  },
  {
    "arxiv_id": "2505.20485v3",
    "title": "Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data",
    "authors": [
      "Abhijit Chunduru",
      "Majid Morafah",
      "Mahdi Morafah",
      "Vishnu Pandi Chellapandi",
      "Ang Li"
    ],
    "abstract": "The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20485v3",
    "published_date": "2025-05-26 19:43:11 UTC",
    "updated_date": "2025-06-30 19:20:31 UTC"
  },
  {
    "arxiv_id": "2505.20482v1",
    "title": "Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding",
    "authors": [
      "Vibhor Agarwal",
      "Arjoo Gupta",
      "Suparna De",
      "Nishanth Sastry"
    ],
    "abstract": "Understanding online conversations has attracted research attention with the growth of social networks and online discussion forums. Content analysis of posts and replies in online conversations is difficult because each individual utterance is usually short and may implicitly refer to other posts within the same conversation. Thus, understanding individual posts requires capturing the conversational context and dependencies between different parts of a conversation tree and then encoding the context dependencies between posts and comments/replies into the language model.\n  To this end, we propose a general-purpose mechanism to discover appropriate conversational context for various aspects about an online post in a conversation, such as whether it is informative, insightful, interesting or funny. Specifically, we design two families of Conversation Kernels, which explore different parts of the neighborhood of a post in the tree representing the conversation and through this, build relevant conversational context that is appropriate for each task being considered. We apply our developed method to conversations crawled from slashdot.org, which allows users to apply highly different labels to posts, such as 'insightful', 'funny', etc., and therefore provides an ideal experimental platform to study whether a framework such as Conversation Kernels is general-purpose and flexible enough to be adapted to disparately different conversation understanding tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at International AAAI Conference on Web and Social Media (ICWSM) 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20482v1",
    "published_date": "2025-05-26 19:37:04 UTC",
    "updated_date": "2025-05-26 19:37:04 UTC"
  },
  {
    "arxiv_id": "2505.20481v1",
    "title": "CardioPatternFormer: Pattern-Guided Attention for Interpretable ECG Classification with Transformer Architecture",
    "authors": [
      "Berat Kutay Uğraş",
      "Ömer Nezih Gerek",
      "İbrahim Talha Saygı"
    ],
    "abstract": "Accurate ECG interpretation is vital, yet complex cardiac data and \"black-box\" AI models limit clinical utility. Inspired by Transformer architectures' success in NLP for understanding sequential data, we frame ECG as the heart's unique \"language\" of temporal patterns. We present CardioPatternFormer, a novel Transformer-based model for interpretable ECG classification. It employs a sophisticated attention mechanism to precisely identify and classify diverse cardiac patterns, excelling at discerning subtle anomalies and distinguishing multiple co-occurring conditions. This pattern-guided attention provides clear insights by highlighting influential signal regions, effectively allowing the \"heart to talk\" through transparent interpretations. CardioPatternFormer demonstrates robust performance on challenging ECGs, including complex multi-pathology cases. Its interpretability via attention maps enables clinicians to understand the model's rationale, fostering trust and aiding informed diagnostic decisions. This work offers a powerful, transparent solution for advanced ECG analysis, paving the way for more reliable and clinically actionable AI in cardiology.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20481v1",
    "published_date": "2025-05-26 19:36:58 UTC",
    "updated_date": "2025-05-26 19:36:58 UTC"
  },
  {
    "arxiv_id": "2505.21557v1",
    "title": "Analytical Calculation of Weights Convolutional Neural Network",
    "authors": [
      "Polad Geidarov"
    ],
    "abstract": "This paper presents an algorithm for analytically calculating the weights and thresholds of convolutional neural networks (CNNs) without using standard training procedures. The algorithm enables the determination of CNN parameters based on just 10 selected images from the MNIST dataset, each representing a digit from 0 to 9. As part of the method, the number of channels in CNN layers is also derived analytically. A software module was implemented in C++ Builder, and a series of experiments were conducted using the MNIST dataset. Results demonstrate that the analytically computed CNN can recognize over half of 1000 handwritten digit images without any training, achieving inference in fractions of a second. These findings suggest that CNNs can be constructed and applied directly for classification tasks without training, using purely analytical computation of weights.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21557v1",
    "published_date": "2025-05-26 19:17:19 UTC",
    "updated_date": "2025-05-26 19:17:19 UTC"
  },
  {
    "arxiv_id": "2505.20471v3",
    "title": "WeatherEdit: Controllable Weather Editing with 4D Gaussian Field",
    "authors": [
      "Chenghao Qian",
      "Wenjing Li",
      "Yuhu Guo",
      "Gustav Markkula"
    ],
    "abstract": "In this work, we present WeatherEdit, a novel weather editing pipeline for generating realistic weather effects with controllable types and severity in 3D scenes. Our approach is structured into two key components: weather background editing and weather particle construction. For weather background editing, we introduce an all-in-one adapter that integrates multiple weather styles into a single pretrained diffusion model, enabling the generation of diverse weather effects in 2D image backgrounds. During inference, we design a Temporal-View (TV-) attention mechanism that follows a specific order to aggregate temporal and spatial information, ensuring consistent editing across multi-frame and multi-view images. To construct the weather particles, we first reconstruct a 3D scene using the edited images and then introduce a dynamic 4D Gaussian field to generate snowflakes, raindrops and fog in the scene. The attributes and dynamics of these particles are precisely controlled through physical-based modelling and simulation, ensuring realistic weather representation and flexible severity adjustments. Finally, we integrate the 4D Gaussian field with the 3D scene to render consistent and highly realistic weather effects. Experiments on multiple driving datasets demonstrate that WeatherEdit can generate diverse weather effects with controllable condition severity, highlighting its potential for autonomous driving simulation in adverse weather. See project page: https://jumponthemoon.github.io/w-edit",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20471v3",
    "published_date": "2025-05-26 19:10:47 UTC",
    "updated_date": "2025-08-07 13:07:35 UTC"
  },
  {
    "arxiv_id": "2505.20469v2",
    "title": "CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting",
    "authors": [
      "Lei Tian",
      "Xiaomin Li",
      "Liqian Ma",
      "Hao Yin",
      "Zirui Zheng",
      "Hefei Huang",
      "Taiqing Li",
      "Huchuan Lu",
      "Xu Jia"
    ],
    "abstract": "Recent advances in 3D reconstruction techniques and vision-language models have fueled significant progress in 3D semantic understanding, a capability critical to robotics, autonomous driving, and virtual/augmented reality. However, methods that rely on 2D priors are prone to a critical challenge: cross-view semantic inconsistencies induced by occlusion, image blur, and view-dependent variations. These inconsistencies, when propagated via projection supervision, deteriorate the quality of 3D Gaussian semantic fields and introduce artifacts in the rendered outputs. To mitigate this limitation, we propose CCL-LGS, a novel framework that enforces view-consistent semantic supervision by integrating multi-view semantic cues. Specifically, our approach first employs a zero-shot tracker to align a set of SAM-generated 2D masks and reliably identify their corresponding categories. Next, we utilize CLIP to extract robust semantic encodings across views. Finally, our Contrastive Codebook Learning (CCL) module distills discriminative semantic features by enforcing intra-class compactness and inter-class distinctiveness. In contrast to previous methods that directly apply CLIP to imperfect masks, our framework explicitly resolves semantic conflicts while preserving category discriminability. Extensive experiments demonstrate that CCL-LGS outperforms previous state-of-the-art methods. Our project page is available at https://epsilontl.github.io/CCL-LGS/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20469v2",
    "published_date": "2025-05-26 19:09:33 UTC",
    "updated_date": "2025-08-14 12:29:24 UTC"
  },
  {
    "arxiv_id": "2505.20466v1",
    "title": "Reconceptualizing Smart Microscopy: From Data Collection to Knowledge Creation by Multi-Agent Integration",
    "authors": [
      "P. S. Kesavan",
      "Pontus Nordenfelt"
    ],
    "abstract": "Smart microscopy represents a paradigm shift in biological imaging, moving from passive observation tools to active collaborators in scientific inquiry. Enabled by advances in automation, computational power, and artificial intelligence, these systems are now capable of adaptive decision-making and real-time experimental control. Here, we introduce a theoretical framework that reconceptualizes smart microscopy as a partner in scientific investigation. Central to our framework is the concept of the 'epistemic-empirical divide' in cellular investigation-the gap between what is observable (empirical domain) and what must be understood (epistemic domain). We propose six core design principles: epistemic-empirical awareness, hierarchical context integration, an evolution from detection to perception, adaptive measurement frameworks, narrative synthesis capabilities, and cross-contextual reasoning. Together, these principles guide a multi-agent architecture designed to align empirical observation with the goals of scientific understanding. Our framework provides a roadmap for building microscopy systems that go beyond automation to actively support hypothesis generation, insight discovery, and theory development, redefining the role of scientific instruments in the process of knowledge creation.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MA",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "34 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20466v1",
    "published_date": "2025-05-26 19:02:14 UTC",
    "updated_date": "2025-05-26 19:02:14 UTC"
  },
  {
    "arxiv_id": "2505.20445v4",
    "title": "In-context Language Learning for Endangered Languages in Speech Recognition",
    "authors": [
      "Zhaolin Li",
      "Jan Niehues"
    ],
    "abstract": "With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs. Our code is publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Interspeech2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20445v4",
    "published_date": "2025-05-26 18:38:59 UTC",
    "updated_date": "2025-11-18 08:44:02 UTC"
  },
  {
    "arxiv_id": "2505.20435v2",
    "title": "The Shape of Adversarial Influence: Characterizing LLM Latent Spaces with Persistent Homology",
    "authors": [
      "Aideen Fay",
      "Inés García-Redondo",
      "Qiquan Wang",
      "Haim Dubossarsky",
      "Anthea Monod"
    ],
    "abstract": "Existing interpretability methods for Large Language Models (LLMs) often fall short by focusing on linear directions or isolated features, overlooking the high-dimensional, nonlinear, and relational geometry within model representations. This study focuses on how adversarial inputs systematically affect the internal representation spaces of LLMs, a topic which remains poorly understood. We propose persistent homology (PH), a tool from topological data analysis, as a principled framework to characterize the multi-scale dynamics within LLM activations. Using PH, we systematically analyze six state-of-the-art models under two distinct adversarial conditions, indirect prompt injection and backdoor fine-tuning, and identify a consistent topological signature of adversarial influence. Across architectures and model sizes, adversarial inputs induce ``topological compression'', where the latent space becomes structurally simpler, collapsing from varied, compact, small-scale features into fewer, dominant, and more dispersed large-scale ones. This topological signature is statistically robust across layers, highly discriminative, and provides interpretable insights into how adversarial effects emerge and propagate. By quantifying the shape of activations and neuronal information flow, our architecture-agnostic framework reveals fundamental invariants of representational change, offering a complementary perspective to existing interpretability methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CG",
      "math.AT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20435v2",
    "published_date": "2025-05-26 18:31:49 UTC",
    "updated_date": "2025-10-09 16:00:15 UTC"
  },
  {
    "arxiv_id": "2505.20424v2",
    "title": "Robot Operation of Home Appliances by Reading User Manuals",
    "authors": [
      "Jian Zhang",
      "Hanbo Zhang",
      "Anxing Xiao",
      "David Hsu"
    ],
    "abstract": "Operating home appliances, among the most common tools in every household, is a critical capability for assistive home robots. This paper presents ApBot, a robot system that operates novel household appliances by \"reading\" their user manuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial policies from their unstructured, textual descriptions in a user manual document, (ii) ground the policies to the appliance in the physical world, and (iii) execute the policies reliably over potentially many steps, despite compounding errors. To tackle these challenges, ApBot constructs a structured, symbolic model of an appliance from its manual, with the help of a large vision-language model (VLM). It grounds the symbolic actions visually to control panel elements. Finally, ApBot closes the loop by updating the model based on visual feedback. Our experiments show that across a wide range of simulated and real-world appliances, ApBot achieves consistent and statistically significant improvements in task success rate, compared with state-of-the-art large VLMs used directly as control policies. These results suggest that a structured internal representations plays an important role in robust robot operation of home appliances, especially, complex ones.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20424v2",
    "published_date": "2025-05-26 18:17:07 UTC",
    "updated_date": "2025-07-23 17:39:54 UTC"
  },
  {
    "arxiv_id": "2505.20423v1",
    "title": "Vision-Based Risk Aware Emergency Landing for UAVs in Complex Urban Environments",
    "authors": [
      "Julio de la Torre-Vanegas",
      "Miguel Soriano-Garcia",
      "Israel Becerra",
      "Diego Mercado-Ravell"
    ],
    "abstract": "Landing safely in crowded urban environments remains an essential yet challenging endeavor for Unmanned Aerial Vehicles (UAVs), especially in emergency situations. In this work, we propose a risk-aware approach that harnesses semantic segmentation to continuously evaluate potential hazards in the drone's field of view. By using a specialized deep neural network to assign pixel-level risk values and applying an algorithm based on risk maps, our method adaptively identifies a stable Safe Landing Zone (SLZ) despite moving critical obstacles such as vehicles, people, etc., and other visual challenges like shifting illumination. A control system then guides the UAV toward this low-risk region, employing altitude-dependent safety thresholds and temporal landing point stabilization to ensure robust descent trajectories. Experimental validation in diverse urban environments demonstrates the effectiveness of our approach, achieving over 90% landing success rates in very challenging real scenarios, showing significant improvements in various risk metrics. Our findings suggest that risk-oriented vision methods can effectively help reduce the risk of accidents in emergency landing situations, particularly in complex, unstructured, urban scenarios, densely populated with moving risky obstacles, while potentiating the true capabilities of UAVs in complex urban operations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20423v1",
    "published_date": "2025-05-26 18:16:21 UTC",
    "updated_date": "2025-05-26 18:16:21 UTC"
  },
  {
    "arxiv_id": "2505.20422v2",
    "title": "SEMMA: A Semantic Aware Knowledge Graph Foundation Model",
    "authors": [
      "Arvindh Arun",
      "Sumit Kumar",
      "Mojtaba Nayyeri",
      "Bo Xiong",
      "Ponnurangam Kumaraguru",
      "Antonio Vergari",
      "Steffen Staab"
    ],
    "abstract": "Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling zero-shot reasoning over unseen graphs by learning transferable patterns. However, most existing KGFMs rely solely on graph structure, overlooking the rich semantic signals encoded in textual attributes. We introduce SEMMA, a dual-module KGFM that systematically integrates transferable textual semantics alongside structure. SEMMA leverages Large Language Models (LLMs) to enrich relation identifiers, generating semantic embeddings that subsequently form a textual relation graph, which is fused with the structural component. Across 54 diverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully inductive link prediction. Crucially, we show that in more challenging generalization settings, where the test-time relation vocabulary is entirely unseen, structural methods collapse while SEMMA is 2x more effective. Our findings demonstrate that textual semantics are critical for generalization in settings where structure alone fails, highlighting the need for foundation models that unify structural and linguistic signals in knowledge reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20422v2",
    "published_date": "2025-05-26 18:15:25 UTC",
    "updated_date": "2025-09-19 09:31:17 UTC"
  },
  {
    "arxiv_id": "2505.20417v1",
    "title": "SCAR: Shapley Credit Assignment for More Efficient RLHF",
    "authors": [
      "Meng Cao",
      "Shuyuan Zhang",
      "Xiao-Wen Chang",
      "Doina Precup"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a widely used technique for aligning Large Language Models (LLMs) with human preferences, yet it often suffers from sparse reward signals, making effective credit assignment challenging. In typical setups, the reward model provides a single scalar score for an entire generated sequence, offering little insight into which token or span-level decisions were responsible for the outcome. To address this, we propose Shapley Credit Assignment Rewards (SCAR), a novel method that leverages Shapley values in cooperative game theory. SCAR distributes the total sequence-level reward among constituent tokens or text spans based on their principled marginal contributions. This creates dense reward signals, crucially, without necessitating the training of auxiliary critique models or recourse to fine-grained human annotations at intermediate generation stages. Unlike prior dense reward methods, SCAR offers a game-theoretic foundation for fair credit attribution. Theoretically, we demonstrate that SCAR preserves the original optimal policy, and empirically, across diverse tasks including sentiment control, text summarization, and instruction tuning, we show that SCAR converges significantly faster and achieves higher final reward scores compared to standard RLHF and attention-based dense reward baselines. Our findings suggest that SCAR provides a more effective and theoretically sound method for credit assignment in RLHF, leading to more efficient alignment of LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20417v1",
    "published_date": "2025-05-26 18:06:52 UTC",
    "updated_date": "2025-05-26 18:06:52 UTC"
  },
  {
    "arxiv_id": "2505.20416v1",
    "title": "GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation",
    "authors": [
      "Zihong Chen",
      "Wanli Jiang",
      "Jinzhe Li",
      "Zhonghang Yuan",
      "Huanjun Kong",
      "Wanli Ouyang",
      "Nanqing Dong"
    ],
    "abstract": "Fine-tuning for large language models (LLMs) typically requires substantial amounts of high-quality supervised data, which is both costly and labor-intensive to acquire. While synthetic data generation has emerged as a promising solution, existing approaches frequently suffer from factual inaccuracies, insufficient long-tail coverage, simplistic knowledge structures, and homogenized outputs. To address these challenges, we introduce GraphGen, a knowledge graph-guided framework designed for three key question-answering (QA) scenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by constructing a fine-grained knowledge graph from the source text. It then identifies knowledge gaps in LLMs using the expected calibration error metric, prioritizing the generation of QA pairs that target high-value, long-tail knowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling to capture complex relational information and employs style-controlled generation to diversify the resulting QA data. Experimental results on knowledge-intensive tasks under closed-book settings demonstrate that GraphGen outperforms conventional synthetic data methods, offering a more reliable and comprehensive solution to the data scarcity challenge in supervised fine-tuning. The code and data are publicly available at https://github.com/open-sciencelab/GraphGen.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20416v1",
    "published_date": "2025-05-26 18:06:50 UTC",
    "updated_date": "2025-05-26 18:06:50 UTC"
  },
  {
    "arxiv_id": "2505.20414v1",
    "title": "RetroMotion: Retrocausal Motion Forecasting Models are Instructable",
    "authors": [
      "Royden Wagner",
      "Omer Sahin Tas",
      "Felix Hauser",
      "Marlon Steiner",
      "Dominik Strutz",
      "Abhishek Vivekanandan",
      "Carlos Fernandez",
      "Christoph Stiller"
    ],
    "abstract": "Motion forecasts of road users (i.e., agents) vary in complexity as a function of scene constraints and interactive behavior. We address this with a multi-task learning method for motion forecasting that includes a retrocausal flow of information. The corresponding tasks are to forecast (1) marginal trajectory distributions for all modeled agents and (2) joint trajectory distributions for interacting agents. Using a transformer model, we generate the joint distributions by re-encoding marginal distributions followed by pairwise modeling. This incorporates a retrocausal flow of information from later points in marginal trajectories to earlier points in joint trajectories. Per trajectory point, we model positional uncertainty using compressed exponential power distributions. Notably, our method achieves state-of-the-art results in the Waymo Interaction Prediction dataset and generalizes well to the Argoverse 2 dataset. Additionally, our method provides an interface for issuing instructions through trajectory modifications. Our experiments show that regular training of motion forecasting leads to the ability to follow goal-based instructions and to adapt basic directional instructions to the scene context. Code: https://github.com/kit-mrt/future-motion",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20414v1",
    "published_date": "2025-05-26 18:05:59 UTC",
    "updated_date": "2025-05-26 18:05:59 UTC"
  },
  {
    "arxiv_id": "2505.20405v1",
    "title": "What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models",
    "authors": [
      "Lorenzo Baraldi",
      "Davide Bucciarelli",
      "Federico Betti",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Nicu Sebe",
      "Rita Cucchiara"
    ],
    "abstract": "Instruction-based image editing models offer increased personalization opportunities in generative tasks. However, properly evaluating their results is challenging, and most of the existing metrics lag in terms of alignment with human judgment and explainability. To tackle these issues, we introduce DICE (DIfference Coherence Estimator), a model designed to detect localized differences between the original and the edited image and to assess their relevance to the given modification request. DICE consists of two key components: a difference detector and a coherence estimator, both built on an autoregressive Multimodal Large Language Model (MLLM) and trained using a strategy that leverages self-supervision, distillation from inpainting networks, and full supervision. Through extensive experiments, we evaluate each stage of our pipeline, comparing different MLLMs within the proposed framework. We demonstrate that DICE effectively identifies coherent edits, effectively evaluating images generated by different editing models with a strong correlation with human judgment. We publicly release our source code, models, and data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20405v1",
    "published_date": "2025-05-26 18:00:10 UTC",
    "updated_date": "2025-05-26 18:00:10 UTC"
  },
  {
    "arxiv_id": "2505.20298v2",
    "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding",
    "authors": [
      "Jeonghun Baek",
      "Kazuki Egashira",
      "Shota Onohara",
      "Atsuyuki Miyai",
      "Yuki Imajuku",
      "Hikaru Ikuta",
      "Kiyoharu Aizawa"
    ],
    "abstract": "Manga, or Japanese comics, is a richly multimodal narrative form that blends images and text in complex ways. Teaching large multimodal models (LMMs) to understand such narratives at a human-like level could help manga creators reflect on and refine their stories. To this end, we introduce two benchmarks for multimodal manga understanding: MangaOCR, which targets in-page text recognition, and MangaVQA, a novel benchmark designed to evaluate contextual understanding through visual question answering. MangaVQA consists of 526 high-quality, manually constructed question-answer pairs, enabling reliable evaluation across diverse narrative and visual scenarios. Building on these benchmarks, we develop MangaLMM, a manga-specialized model finetuned from the open-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive experiments, including comparisons with proprietary models such as GPT-4o and Gemini 2.5, we assess how well LMMs understand manga. Our benchmark and model provide a comprehensive foundation for evaluating and advancing LMMs in the richly narrative domain of manga.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20298v2",
    "published_date": "2025-05-26 17:59:59 UTC",
    "updated_date": "2025-12-30 09:12:44 UTC"
  },
  {
    "arxiv_id": "2505.20295v3",
    "title": "SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?",
    "authors": [
      "Michael Kirchhof",
      "Luca Füger",
      "Adam Goliński",
      "Eeshan Gunesh Dhekane",
      "Arno Blaas",
      "Seong Joon Oh",
      "Sinead Williamson"
    ],
    "abstract": "The common approach to communicate a large language model's (LLM) uncertainty is to add a percentage number or a hedging word to its response. But is this all we can do? Instead of generating a single answer and then hedging it, an LLM that is fully transparent to the user needs to be able to reflect on its internal belief distribution and output a summary of all options it deems possible, and how likely they are. To test whether LLMs possess this capability, we develop the SelfReflect metric, an information-theoretic distance between a given summary and a distribution over answers. In interventional and human studies, we find that SelfReflect indicates even slight deviations, yielding a fine measure of faithfulness between a summary string and an LLM's actual internal distribution over answers. With SelfReflect, we make a resounding negative observation: modern LLMs are, across the board, incapable of revealing what they are uncertain about, neither through reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we do find that LLMs are able to generate faithful summaries of their uncertainties if we help them by sampling multiple outputs and feeding them back into the context. This simple approach shines a light at the universal way of communicating LLM uncertainties whose future development the SelfReflect score enables.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20295v3",
    "published_date": "2025-05-26 17:59:53 UTC",
    "updated_date": "2025-09-30 14:44:21 UTC"
  },
  {
    "arxiv_id": "2505.20296v1",
    "title": "Reasoning LLMs are Wandering Solution Explorers",
    "authors": [
      "Jiahao Lu",
      "Ziwei Xu",
      "Mohan Kankanhalli"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive reasoning abilities through test-time computation (TTC) techniques such as chain-of-thought prompting and tree-based reasoning. However, we argue that current reasoning LLMs (RLLMs) lack the ability to systematically explore the solution space. This paper formalizes what constitutes systematic problem solving and identifies common failure modes that reveal reasoning LLMs to be wanderers rather than systematic explorers. Through qualitative and quantitative analysis across multiple state-of-the-art LLMs, we uncover persistent issues: invalid reasoning steps, redundant explorations, hallucinated or unfaithful conclusions, and so on. Our findings suggest that current models' performance can appear to be competent on simple tasks yet degrade sharply as complexity increases. Based on the findings, we advocate for new metrics and tools that evaluate not just final outputs but the structure of the reasoning process itself.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "71 pages, 14 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.20296v1",
    "published_date": "2025-05-26 17:59:53 UTC",
    "updated_date": "2025-05-26 17:59:53 UTC"
  },
  {
    "arxiv_id": "2505.20294v2",
    "title": "GLEAM: Learning Generalizable Exploration Policy for Active Mapping in Complex 3D Indoor Scenes",
    "authors": [
      "Xiao Chen",
      "Tai Wang",
      "Quanyi Li",
      "Tao Huang",
      "Jiangmiao Pang",
      "Tianfan Xue"
    ],
    "abstract": "Generalizable active mapping in complex unknown environments remains a critical challenge for mobile robots. Existing methods, constrained by insufficient training data and conservative exploration strategies, exhibit limited generalizability across scenes with diverse layouts and complex connectivity. To enable scalable training and reliable evaluation, we introduce GLEAM-Bench, the first large-scale benchmark designed for generalizable active mapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets. Building upon this foundation, we propose GLEAM, a unified generalizable exploration policy for active mapping. Its superior generalizability comes mainly from our semantic representations, long-term navigable goals, and randomized strategies. It significantly outperforms state-of-the-art methods, achieving 66.50% coverage (+9.49%) with efficient trajectories and improved mapping accuracy on 128 unseen complex scenes. Project page: https://xiao-chen.tech/gleam/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICCV 2025. Project page: https://xiao-chen.tech/gleam/",
    "pdf_url": "https://arxiv.org/pdf/2505.20294v2",
    "published_date": "2025-05-26 17:59:52 UTC",
    "updated_date": "2025-09-29 07:57:24 UTC"
  },
  {
    "arxiv_id": "2505.20292v4",
    "title": "OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation",
    "authors": [
      "Shenghai Yuan",
      "Xianyi He",
      "Yufan Deng",
      "Yang Ye",
      "Jinfa Huang",
      "Bin Lin",
      "Jiebo Luo",
      "Li Yuan"
    ],
    "abstract": "Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and (ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V benchmarks inherited from VBench that focus on global and coarse-grained assessment of generated videos, OpenS2V-Eval focuses on the model's ability to generate subject-consistent videos with natural subject appearance and identity fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven major categories of S2V, which incorporate both real and synthetic test data. Furthermore, to accurately align human preferences with S2V benchmarks, we propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to separately quantify subject consistency, naturalness, and text relevance in generated videos. Building on this, we conduct a comprehensive evaluation of 18 representative S2V models, highlighting their strengths and weaknesses across different content. Moreover, we create the first open-source large-scale S2V generation dataset OpenS2V-5M, which consists of five million high-quality 720P subject-text-video triples. Specifically, we ensure subject-information diversity in our dataset by (1) segmenting subjects and building pairing information via cross-video associations and (2) prompting GPT-Image-1 on raw frames to synthesize multi-view representations. Through OpenS2V-Nexus, we deliver a robust infrastructure to accelerate future S2V generation research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code and Dataset: https://github.com/PKU-YuanGroup/OpenS2V-Nexus",
    "pdf_url": "https://arxiv.org/pdf/2505.20292v4",
    "published_date": "2025-05-26 17:59:46 UTC",
    "updated_date": "2025-06-03 10:11:00 UTC"
  },
  {
    "arxiv_id": "2505.20290v2",
    "title": "EgoZero: Robot Learning from Smart Glasses",
    "authors": [
      "Vincent Liu",
      "Ademi Adeniji",
      "Haotian Zhan",
      "Siddhant Haldar",
      "Raunaq Bhirangi",
      "Pieter Abbeel",
      "Lerrel Pinto"
    ],
    "abstract": "Despite recent progress in general purpose robotics, robot policies still lag far behind basic human capabilities in the real world. Humans interact constantly with the physical world, yet this rich data resource remains largely untapped in robot learning. We propose EgoZero, a minimal system that learns robust manipulation policies from human demonstrations captured with Project Aria smart glasses, $\\textbf{and zero robot data}$. EgoZero enables: (1) extraction of complete, robot-executable actions from in-the-wild, egocentric, human demonstrations, (2) compression of human visual observations into morphology-agnostic state representations, and (3) closed-loop policy learning that generalizes morphologically, spatially, and semantically. We deploy EgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot transfer with 70% success rate over 7 manipulation tasks and only 20 minutes of data collection per task. Our results suggest that in-the-wild human data can serve as a scalable foundation for real-world robot learning - paving the way toward a future of abundant, diverse, and naturalistic training data for robots. Code and videos are available at https://egozero-robot.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20290v2",
    "published_date": "2025-05-26 17:59:17 UTC",
    "updated_date": "2025-06-03 22:50:28 UTC"
  },
  {
    "arxiv_id": "2505.20286v1",
    "title": "Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution",
    "authors": [
      "Jiahao Qiu",
      "Xuan Qi",
      "Tongcheng Zhang",
      "Xinzhe Juan",
      "Jiacheng Guo",
      "Yifu Lu",
      "Yimin Wang",
      "Zixin Yao",
      "Qihan Ren",
      "Xun Jiang",
      "Xing Zhou",
      "Dongrui Liu",
      "Ling Yang",
      "Yue Wu",
      "Kaixuan Huang",
      "Shilong Liu",
      "Hongru Wang",
      "Mengdi Wang"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled agents to autonomously perform complex, open-ended tasks. However, many existing frameworks depend heavily on manually predefined tools and workflows, which hinder their adaptability, scalability, and generalization across domains. In this work, we introduce Alita--a generalist agent designed with the principle of \"Simplicity is the ultimate sophistication,\" enabling scalable agentic reasoning through minimal predefinition and maximal self-evolution. For minimal predefinition, Alita is equipped with only one component for direct problem-solving, making it much simpler and neater than previous approaches that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances its potential to generalize to challenging questions, without being limited by tools. For Maximal self-evolution, we enable the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3 accuracy, which is top-ranking among general-purpose agents, on the GAIA benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. More details will be updated at $\\href{https://github.com/CharlesQ9/Alita}{https://github.com/CharlesQ9/Alita}$.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20286v1",
    "published_date": "2025-05-26 17:58:53 UTC",
    "updated_date": "2025-05-26 17:58:53 UTC"
  },
  {
    "arxiv_id": "2505.20278v2",
    "title": "Characterizing Pattern Matching and Its Limits on Compositional Task Structures",
    "authors": [
      "Hoyeon Chang",
      "Jinho Park",
      "Hanseul Cho",
      "Sohee Yang",
      "Miyoung Ko",
      "Hyeonbin Hwang",
      "Seungpil Won",
      "Dohaeng Lee",
      "Youbin Ahn",
      "Minjoon Seo"
    ],
    "abstract": "Despite impressive capabilities, LLMs' successes often rely on pattern-matching behaviors, yet these are also linked to OOD generalization failures in compositional tasks. However, behavioral studies commonly employ task setups that allow multiple generalization sources (e.g., algebraic invariances, structural repetition), obscuring a precise and testable account of how well LLMs perform generalization through pattern matching and their limitations. To address this ambiguity, we first formalize pattern matching as functional equivalence, i.e., identifying pairs of subsequences of inputs that consistently lead to identical results when the rest of the input is held constant. Then, we systematically study how decoder-only Transformer and Mamba behave in controlled tasks with compositional structures that isolate this mechanism. Our formalism yields predictive and quantitative insights: (1) Instance-wise success of pattern matching is well predicted by the number of contexts witnessing the relevant functional equivalence. (2) We prove a tight sample complexity bound of learning a two-hop structure by identifying the exponent of the data scaling law for perfect in-domain generalization. Our empirical results align with the theoretical prediction, under 20x parameter scaling and across architectures. (3) Path ambiguity is a structural barrier: when a variable influences the output via multiple paths, models fail to form unified intermediate state representations, impairing accuracy and interpretability. (4) Chain-of-Thought reduces data requirements yet does not resolve path ambiguity. Hence, we provide a predictive, falsifiable boundary for pattern matching and a foundational diagnostic for disentangling mixed generalization mechanisms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20278v2",
    "published_date": "2025-05-26 17:55:15 UTC",
    "updated_date": "2025-11-26 10:25:27 UTC"
  },
  {
    "arxiv_id": "2505.20276v3",
    "title": "Does quantization affect models' performance on long-context tasks?",
    "authors": [
      "Anmol Mekala",
      "Anirudh Atmakuru",
      "Yixiao Song",
      "Marzena Karpinska",
      "Mohit Iyyer"
    ],
    "abstract": "Large language models (LLMs) now support context windows exceeding 128K tokens, but this comes with significant memory requirements and high inference latency. Quantization can mitigate these costs, but may degrade performance. In this work, we present the first systematic evaluation of quantized LLMs on tasks with long inputs (>64K tokens) and long-form outputs. Our evaluation spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B, and 72B). We find that, on average, 8-bit quantization preserves accuracy (~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for tasks involving long-context inputs (drops of up to 59%). This degradation tends to worsen when the input is in a language other than English. Crucially, the effects of quantization depend heavily on the quantization method, model, and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4, Llama-3.1 70B experiences a 32% performance drop on the same task. These findings highlight the importance of a careful, task-specific evaluation before deploying quantized LLMs, particularly in long-context scenarios and for languages other than English.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "to appear in EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20276v3",
    "published_date": "2025-05-26 17:54:30 UTC",
    "updated_date": "2025-09-20 06:44:40 UTC"
  },
  {
    "arxiv_id": "2505.20274v2",
    "title": "Probabilistic Kernel Function for Fast Angle Testing",
    "authors": [
      "Kejing Lu",
      "Chuan Xiao",
      "Yoshiharu Ishikawa"
    ],
    "abstract": "In this paper, we study the angle testing problem in the context of similarity search in high-dimensional Euclidean spaces and propose two projection-based probabilistic kernel functions, one designed for angle comparison and the other for angle thresholding. Unlike existing approaches that rely on random projection vectors drawn from Gaussian distributions, our approach leverages reference angles and employs a deterministic structure for the projection vectors. Notably, our kernel functions do not require asymptotic assumptions, such as the number of projection vectors tending to infinity, and can be both theoretically and experimentally shown to outperform Gaussian-distribution-based kernel functions. We apply the proposed kernel function to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared to the widely-used graph-based search algorithm HNSW.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DB",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20274v2",
    "published_date": "2025-05-26 17:53:28 UTC",
    "updated_date": "2025-10-29 10:47:16 UTC"
  },
  {
    "arxiv_id": "2505.20273v1",
    "title": "Ten Principles of AI Agent Economics",
    "authors": [
      "Ke Yang",
      "ChengXiang Zhai"
    ],
    "abstract": "The rapid rise of AI-based autonomous agents is transforming human society and economic systems, as these entities increasingly exhibit human-like or superhuman intelligence. From excelling at complex games like Go to tackling diverse general-purpose tasks with large language and multimodal models, AI agents are evolving from specialized tools into dynamic participants in social and economic ecosystems. Their autonomy and decision-making capabilities are poised to impact industries, professions, and human lives profoundly, raising critical questions about their integration into economic activities, potential ethical concerns, and the balance between their utility and safety.\n  To address these challenges, this paper presents ten principles of AI agent economics, offering a framework to understand how AI agents make decisions, influence social interactions, and participate in the broader economy. Drawing on economics, decision theory, and ethics, we explore fundamental questions, such as whether AI agents might evolve from tools into independent entities, their impact on labor markets, and the ethical safeguards needed to align them with human values. These principles build on existing economic theories while accounting for the unique traits of AI agents, providing a roadmap for their responsible integration into human systems.\n  Beyond theoretical insights, this paper highlights the urgency of future research into AI trustworthiness, ethical guidelines, and regulatory oversight. As we enter a transformative era, this work serves as both a guide and a call to action, ensuring AI agents contribute positively to human progress while addressing risks tied to their unprecedented capabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20273v1",
    "published_date": "2025-05-26 17:52:44 UTC",
    "updated_date": "2025-05-26 17:52:44 UTC"
  },
  {
    "arxiv_id": "2505.20271v1",
    "title": "In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation",
    "authors": [
      "Yu Xu",
      "Fan Tang",
      "You Wu",
      "Lin Gao",
      "Oliver Deussen",
      "Hongbin Yan",
      "Jintao Li",
      "Juan Cao",
      "Tong-Yee Lee"
    ],
    "abstract": "Recent advances in diffusion models have enhanced multimodal-guided visual generation, enabling customized subject insertion that seamlessly \"brushes\" user-specified objects into a given image guided by textual prompts. However, existing methods often struggle to insert customized subjects with high fidelity and align results with the user's intent through textual prompts. In this work, we propose \"In-Context Brush\", a zero-shot framework for customized subject insertion by reformulating the task within the paradigm of in-context learning. Without loss of generality, we formulate the object image and the textual prompts as cross-modal demonstrations, and the target image with the masked region as the query. The goal is to inpaint the target image with the subject aligning textual prompts without model tuning. Building upon a pretrained MMDiT-based inpainting network, we perform test-time enhancement via dual-level latent space manipulation: intra-head \"latent feature shifting\" within each attention head that dynamically shifts attention outputs to reflect the desired subject semantics and inter-head \"attention reweighting\" across different heads that amplifies prompt controllability through differential attention prioritization. Extensive experiments and applications demonstrate that our approach achieves superior identity preservation, text alignment, and image quality compared to existing state-of-the-art methods, without requiring dedicated training or additional data collection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20271v1",
    "published_date": "2025-05-26 17:49:10 UTC",
    "updated_date": "2025-05-26 17:49:10 UTC"
  },
  {
    "arxiv_id": "2507.19487v1",
    "title": "Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective",
    "authors": [
      "Margarita Leib",
      "Nils Köbis",
      "Ivan Soraperra"
    ],
    "abstract": "People increasingly rely on AI-advice when making decisions. At times, such advice can promote selfish behavior. When individuals abide by selfishness-promoting AI advice, how are they perceived and punished? To study this question, we build on theories from social psychology and combine machine-behavior and behavioral economic approaches. In a pre-registered, financially-incentivized experiment, evaluators could punish real decision-makers who (i) received AI, human, or no advice. The advice (ii) encouraged selfish or prosocial behavior, and decision-makers (iii) behaved selfishly or, in a control condition, behaved prosocially. Evaluators further assigned responsibility to decision-makers and their advisors. Results revealed that (i) prosocial behavior was punished very little, whereas selfish behavior was punished much more. Focusing on selfish behavior, (ii) compared to receiving no advice, selfish behavior was penalized more harshly after prosocial advice and more leniently after selfish advice. Lastly, (iii) whereas selfish decision-makers were seen as more responsible when they followed AI compared to human advice, punishment between the two advice sources did not vary. Overall, behavior and advice content shape punishment, whereas the advice source does not.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "econ.GN"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.19487v1",
    "published_date": "2025-05-26 17:46:04 UTC",
    "updated_date": "2025-05-26 17:46:04 UTC"
  },
  {
    "arxiv_id": "2505.20269v1",
    "title": "Comparing Neural Network Encodings for Logic-based Explainability",
    "authors": [
      "Levi Cordeiro Carvalho",
      "Saulo A. F. Oliveira",
      "Thiago Alves Rocha"
    ],
    "abstract": "Providing explanations for the outputs of artificial neural networks (ANNs) is crucial in many contexts, such as critical systems, data protection laws and handling adversarial examples. Logic-based methods can offer explanations with correctness guarantees, but face scalability challenges. Due to these issues, it is necessary to compare different encodings of ANNs into logical constraints, which are used in logic-based explainability. This work compares two encodings of ANNs: one has been used in the literature to provide explanations, while the other will be adapted for our context of explainability. Additionally, the second encoding uses fewer variables and constraints, thus, potentially enhancing efficiency. Experiments showed similar running times for computing explanations, but the adapted encoding performed up to 18\\% better in building logical constraints and up to 16\\% better in overall time.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.LO",
    "comment": "submitted to BRACIS 2024 (Brazilian Conference on Intelligent Systems), accepted version published in Intelligent Systems, LNCS, vol 15412",
    "pdf_url": "https://arxiv.org/pdf/2505.20269v1",
    "published_date": "2025-05-26 17:45:18 UTC",
    "updated_date": "2025-05-26 17:45:18 UTC"
  },
  {
    "arxiv_id": "2505.20268v2",
    "title": "Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits",
    "authors": [
      "Fan Chen",
      "Zeyu Jia",
      "Alexander Rakhlin",
      "Tengyang Xie"
    ],
    "abstract": "Reinforcement learning with outcome-based feedback faces a fundamental challenge: when rewards are only observed at trajectory endpoints, how do we assign credit to the right actions? This paper provides the first comprehensive analysis of this problem in online RL with general function approximation. We develop a provably sample-efficient algorithm achieving $\\widetilde{O}({C_{\\rm cov} H^3}/{ε^2})$ sample complexity, where $C_{\\rm cov}$ is the coverability coefficient of the underlying MDP. By leveraging general function approximation, our approach works effectively in large or infinite state spaces where tabular methods fail, requiring only that value functions and reward functions can be represented by appropriate function classes. Our results also characterize when outcome-based feedback is statistically separated from per-step rewards, revealing an unavoidable exponential separation for certain MDPs. For deterministic MDPs, we show how to eliminate the completeness assumption, dramatically simplifying the algorithm. We further extend our approach to preference-based feedback settings, proving that equivalent statistical efficiency can be achieved even under more limited information. Together, these results constitute a theoretical foundation for understanding the statistical properties of outcome-based reinforcement learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20268v2",
    "published_date": "2025-05-26 17:44:08 UTC",
    "updated_date": "2025-07-24 14:21:12 UTC"
  },
  {
    "arxiv_id": "2505.20266v1",
    "title": "syftr: Pareto-Optimal Generative AI",
    "authors": [
      "Alexander Conway",
      "Debadeepta Dey",
      "Stefan Hackmann",
      "Matthew Hausknecht",
      "Michael Schmidt",
      "Mark Steadman",
      "Nick Volynets"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) pipelines are central to applying large language models (LLMs) to proprietary or dynamic data. However, building effective RAG flows is complex, requiring careful selection among vector databases, embedding models, text splitters, retrievers, and synthesizing LLMs. The challenge deepens with the rise of agentic paradigms. Modules like verifiers, rewriters, and rerankers-each with intricate hyperparameter dependencies have to be carefully tuned. Balancing tradeoffs between latency, accuracy, and cost becomes increasingly difficult in performance-sensitive applications.\n  We introduce syftr, a framework that performs efficient multi-objective search over a broad space of agentic and non-agentic RAG configurations. Using Bayesian Optimization, syftr discovers Pareto-optimal flows that jointly optimize task accuracy and cost. A novel early-stopping mechanism further improves efficiency by pruning clearly suboptimal candidates. Across multiple RAG benchmarks, syftr finds flows which are on average approximately 9 times cheaper while preserving most of the accuracy of the most accurate flows on the Pareto-frontier. Furthermore, syftr's ability to design and optimize allows integrating new modules, making it even easier and faster to realize high-performing generative AI pipelines.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "International Conference on Automated Machine Learning (AutoML) 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20266v1",
    "published_date": "2025-05-26 17:43:13 UTC",
    "updated_date": "2025-05-26 17:43:13 UTC"
  },
  {
    "arxiv_id": "2505.20264v2",
    "title": "We Need to Measure Data Diversity in NLP -- Better and Broader",
    "authors": [
      "Dong Nguyen",
      "Esther Ploeger"
    ],
    "abstract": "Although diversity in NLP datasets has received growing attention, the question of how to measure it remains largely underexplored. This opinion paper examines the conceptual and methodological challenges of measuring data diversity and argues that interdisciplinary perspectives are essential for developing more fine-grained and valid measures.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20264v2",
    "published_date": "2025-05-26 17:42:39 UTC",
    "updated_date": "2025-09-19 18:15:26 UTC"
  },
  {
    "arxiv_id": "2505.20259v1",
    "title": "Lifelong Safety Alignment for Language Models",
    "authors": [
      "Haoyu Wang",
      "Zeyu Qin",
      "Yifei Zhao",
      "Chao Du",
      "Min Lin",
      "Xueqian Wang",
      "Tianyu Pang"
    ],
    "abstract": "LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailbreaking attacks designed to bypass safety alignment. While many existing defenses focus on known types of attacks, it is more critical to prepare LLMs for unseen attacks that may arise during deployment. To address this, we propose a lifelong safety alignment framework that enables LLMs to continuously adapt to new and evolving jailbreaking strategies. Our framework introduces a competitive setup between two components: a Meta-Attacker, trained to actively discover novel jailbreaking strategies, and a Defender, trained to resist them. To effectively warm up the Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a large collection of jailbreak-related research papers. Through iterative training, the first iteration Meta-Attacker achieves a 73% attack success rate (ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks. Meanwhile, the Defender progressively improves its robustness and ultimately reduces the Meta-Attacker's success rate to just 7%, enabling safer and more reliable deployment of LLMs in open-ended environments. The code is available at https://github.com/sail-sg/LifelongSafetyAlignment.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20259v1",
    "published_date": "2025-05-26 17:40:40 UTC",
    "updated_date": "2025-05-26 17:40:40 UTC"
  },
  {
    "arxiv_id": "2505.20254v1",
    "title": "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs",
    "authors": [
      "Xiangchen Song",
      "Aashiq Muhamed",
      "Yujia Zheng",
      "Lingjing Kong",
      "Zeyu Tang",
      "Mona T. Diab",
      "Virginia Smith",
      "Kun Zhang"
    ],
    "abstract": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic interpretability (MI) for decomposing neural network activations into interpretable features. However, the aspiration to identify a canonical set of features is challenged by the observed inconsistency of learned SAE features across different training runs, undermining the reliability and efficiency of MI research. This position paper argues that mechanistic interpretability should prioritize feature consistency in SAEs -- the reliable convergence to equivalent feature sets across independent runs. We propose using the Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to operationalize consistency and demonstrate that high levels are achievable (0.80 for TopK SAEs on LLM activations) with appropriate architectural choices. Our contributions include detailing the benefits of prioritizing consistency; providing theoretical grounding and synthetic validation using a model organism, which verifies PW-MCC as a reliable proxy for ground-truth recovery; and extending these findings to real-world LLM data, where high feature consistency strongly correlates with the semantic similarity of learned feature explanations. We call for a community-wide shift towards systematically measuring feature consistency to foster robust cumulative progress in MI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20254v1",
    "published_date": "2025-05-26 17:31:36 UTC",
    "updated_date": "2025-05-26 17:31:36 UTC"
  },
  {
    "arxiv_id": "2505.21556v1",
    "title": "Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts",
    "authors": [
      "Hee-Seon Kim",
      "Minbeom Kim",
      "Wonjun Lee",
      "Kihyun Kim",
      "Changick Kim"
    ],
    "abstract": "Optimization-based jailbreaks typically adopt the Toxic-Continuation setting in large vision-language models (LVLMs), following the standard next-token prediction objective. In this setting, an adversarial image is optimized to make the model predict the next token of a toxic prompt. However, we find that the Toxic-Continuation paradigm is effective at continuing already-toxic inputs, but struggles to induce safety misalignment when explicit toxic signals are absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike prior work, we optimize adversarial images to induce toxic outputs from benign conditioning. Since benign conditioning contains no safety violations, the image alone must break the model's safety mechanisms. Our method outperforms prior approaches, transfers in black-box settings, and complements text-based jailbreaks. These results reveal an underexplored vulnerability in multimodal alignment and introduce a fundamentally new direction for jailbreak approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "LVLM, Jailbreak",
    "pdf_url": "https://arxiv.org/pdf/2505.21556v1",
    "published_date": "2025-05-26 17:27:32 UTC",
    "updated_date": "2025-05-26 17:27:32 UTC"
  },
  {
    "arxiv_id": "2505.20249v2",
    "title": "WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models",
    "authors": [
      "Yongan Yu",
      "Qingchen Hu",
      "Xianda Du",
      "Jiayin Wang",
      "Fengran Mo",
      "Renee Sieber"
    ],
    "abstract": "Climate change adaptation requires the understanding of disruptive weather impacts on society, where large language models (LLMs) might be applicable. However, their effectiveness is under-explored due to the difficulty of high-quality corpus collection and the lack of available benchmarks. The climate-related events stored in regional newspapers record how communities adapted and recovered from disasters. However, the processing of the original corpus is non-trivial. In this study, we first develop a disruptive weather impact dataset with a four-stage well-crafted construction pipeline. Then, we propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs on disruptive weather impacts. The benchmark involves two evaluation tasks, multi-label classification and ranking-based question answering. Extensive experiments on evaluating a set of LLMs provide first-hand analysis of the challenges in developing disruptive weather impact understanding and climate change adaptation systems. The constructed dataset and the code for the evaluation framework are available to help society protect against vulnerabilities from disasters.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20249v2",
    "published_date": "2025-05-26 17:23:29 UTC",
    "updated_date": "2025-10-28 20:06:49 UTC"
  },
  {
    "arxiv_id": "2505.20245v1",
    "title": "KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing",
    "authors": [
      "Rui Li",
      "Quanyu Dai",
      "Zeyu Zhang",
      "Xu Chen",
      "Zhenhua Dong",
      "Ji-Rong Wen"
    ],
    "abstract": "Recent advances in retrieval-augmented generation (RAG) furnish large language models (LLMs) with iterative retrievals of relevant information to handle complex multi-hop questions. These methods typically alternate between LLM reasoning and retrieval to accumulate external information into the LLM's context. However, the ever-growing context inherently imposes an increasing burden on the LLM to perceive connections among critical information pieces, with futile reasoning steps further exacerbating this overload issue. In this paper, we present KnowTrace, an elegant RAG framework to (1) mitigate the context overload and (2) bootstrap higher-quality multi-step reasoning. Instead of simply piling the retrieved contents, KnowTrace autonomously traces out desired knowledge triplets to organize a specific knowledge graph relevant to the input question. Such a structured workflow not only empowers the LLM with an intelligible context for inference, but also naturally inspires a reflective mechanism of knowledge backtracing to identify contributive LLM generations as process supervision data for self-bootstrapping. Extensive experiments show that KnowTrace consistently surpasses existing methods across three multi-hop question answering benchmarks, and the bootstrapped version further amplifies the gains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by KDD 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20245v1",
    "published_date": "2025-05-26 17:22:20 UTC",
    "updated_date": "2025-05-26 17:22:20 UTC"
  },
  {
    "arxiv_id": "2505.20246v3",
    "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent",
    "authors": [
      "Jiahao Qiu",
      "Fulian Xiao",
      "Yimin Wang",
      "Yuchen Mao",
      "Yijia Chen",
      "Xinzhe Juan",
      "Shu Zhang",
      "Siran Wang",
      "Xuan Qi",
      "Tongcheng Zhang",
      "Zixin Yao",
      "Jiacheng Guo",
      "Yifu Lu",
      "Charles Argon",
      "Jundi Cui",
      "Daixin Chen",
      "Junran Zhou",
      "Shuyao Zhou",
      "Zhanpeng Zhou",
      "Ling Yang",
      "Shilong Liu",
      "Hongru Wang",
      "Kaixuan Huang",
      "Xun Jiang",
      "Yuming Cao",
      "Yue Chen",
      "Yunfei Chen",
      "Zhengyi Chen",
      "Ruowei Dai",
      "Mengqiu Deng",
      "Jiye Fu",
      "Yunting Gu",
      "Zijie Guan",
      "Zirui Huang",
      "Xiaoyan Ji",
      "Yumeng Jiang",
      "Delong Kong",
      "Haolong Li",
      "Jiaqi Li",
      "Ruipeng Li",
      "Tianze Li",
      "Zhuoran Li",
      "Haixia Lian",
      "Mengyue Lin",
      "Xudong Liu",
      "Jiayi Lu",
      "Jinghan Lu",
      "Wanyu Luo",
      "Ziyue Luo",
      "Zihao Pu",
      "Zhi Qiao",
      "Ruihuan Ren",
      "Liang Wan",
      "Ruixiang Wang",
      "Tianhui Wang",
      "Yang Wang",
      "Zeyu Wang",
      "Zihua Wang",
      "Yujia Wu",
      "Zhaoyi Wu",
      "Hao Xin",
      "Weiao Xing",
      "Ruojun Xiong",
      "Weijie Xu",
      "Yao Shu",
      "Yao Xiao",
      "Xiaorui Yang",
      "Yuchen Yang",
      "Nan Yi",
      "Jiadong Yu",
      "Yangyuxuan Yu",
      "Huiting Zeng",
      "Danni Zhang",
      "Yunjie Zhang",
      "Zhaoyu Zhang",
      "Zhiheng Zhang",
      "Xiaofeng Zheng",
      "Peirong Zhou",
      "Linyan Zhong",
      "Xiaoyin Zong",
      "Ying Zhao",
      "Zhenxin Chen",
      "Lin Ding",
      "Xiaoyu Gao",
      "Bingbing Gong",
      "Yichao Li",
      "Yang Liao",
      "Guang Ma",
      "Tianyuan Ma",
      "Xinrui Sun",
      "Tianyi Wang",
      "Han Xia",
      "Ruobing Xian",
      "Gen Ye",
      "Tengfei Yu",
      "Wentao Zhang",
      "Yuxi Wang",
      "Xi Gao",
      "Mengdi Wang"
    ],
    "abstract": "Recent advances in large language models (LLMs) have led to remarkable progress across domains, yet their capabilities in the humanities, particularly history, remain underexplored. Historical reasoning poses unique challenges for AI, involving multimodal source interpretation, temporal inference, and cross-linguistic analysis. While general-purpose agents perform well on many existing benchmarks, they lack the domain-specific expertise required to engage with historical materials and questions. To address this gap, we introduce HistBench, a new benchmark of 414 high-quality questions designed to evaluate AI's capacity for historical reasoning and authored by more than 40 expert contributors. The tasks span a wide range of historical problems-from factual retrieval based on primary sources to interpretive analysis of manuscripts and images, to interdisciplinary challenges involving archaeology, linguistics, or cultural history. Furthermore, the benchmark dataset spans 29 ancient and modern languages and covers a wide range of historical periods and world regions. Finding the poor performance of LLMs and other agents on HistBench, we further present HistAgent, a history-specific agent equipped with carefully designed tools for OCR, translation, archival search, and image understanding in History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of 27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online search and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%) and Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These results highlight the limitations of existing LLMs and generalist agents and demonstrate the advantages of HistAgent for historical reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20246v3",
    "published_date": "2025-05-26 17:22:20 UTC",
    "updated_date": "2025-06-19 15:42:45 UTC"
  },
  {
    "arxiv_id": "2505.20241v3",
    "title": "DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning",
    "authors": [
      "Qi Cao",
      "Ruiyi Wang",
      "Ruiyi Zhang",
      "Sai Ashish Somayajula",
      "Pengtao Xie"
    ],
    "abstract": "Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM's domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 10 figures, to appear in NeurIPS 2025 (Conference on Neural Information Processing Systems)",
    "pdf_url": "https://arxiv.org/pdf/2505.20241v3",
    "published_date": "2025-05-26 17:20:17 UTC",
    "updated_date": "2025-11-04 01:10:36 UTC"
  },
  {
    "arxiv_id": "2505.20235v2",
    "title": "Variational Deep Learning via Implicit Regularization",
    "authors": [
      "Jonathan Wenger",
      "Beau Coker",
      "Juraj Marusic",
      "John P. Cunningham"
    ],
    "abstract": "Modern deep learning models generalize remarkably well in-distribution, despite being overparametrized and trained with little to no explicit regularization. Instead, current theory credits implicit regularization imposed by the choice of architecture, hyperparameters and optimization procedure. However, deep neural networks can be surprisingly non-robust, resulting in overconfident predictions and poor out-of-distribution generalization. Bayesian deep learning addresses this via model averaging, but typically requires significant computational resources as well as carefully elicited priors to avoid overriding the benefits of implicit regularization. Instead, in this work, we propose to regularize variational neural networks solely by relying on the implicit bias of (stochastic) gradient descent. We theoretically characterize this inductive bias in overparametrized linear models as generalized variational inference and demonstrate the importance of the choice of parametrization. Empirically, our approach demonstrates strong in- and out-of-distribution performance without additional hyperparameter tuning and with minimal computational overhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20235v2",
    "published_date": "2025-05-26 17:15:57 UTC",
    "updated_date": "2025-09-26 18:54:59 UTC"
  },
  {
    "arxiv_id": "2505.20229v1",
    "title": "From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance",
    "authors": [
      "Maximilian Dreyer",
      "Lorenz Hufe",
      "Jim Berend",
      "Thomas Wiegand",
      "Sebastian Lapuschkin",
      "Wojciech Samek"
    ],
    "abstract": "Transformer-based CLIP models are widely used for text-image probing and feature extraction, making it relevant to understand the internal mechanisms behind their predictions. While recent works show that Sparse Autoencoders (SAEs) yield interpretable latent components, they focus on what these encode and miss how they drive predictions. We introduce a scalable framework that reveals what latent components activate for, how they align with expected semantics, and how important they are to predictions. To achieve this, we adapt attribution patching for instance-wise component attributions in CLIP and highlight key faithfulness limitations of the widely used Logit Lens technique. By combining attributions with semantic alignment scores, we can automatically uncover reliance on components that encode semantically unexpected or spurious concepts. Applied across multiple CLIP variants, our method uncovers hundreds of surprising components linked to polysemous words, compound nouns, visual typography and dataset artifacts. While text embeddings remain prone to semantic ambiguity, they are more robust to spurious correlations compared to linear classifiers trained on image embeddings. A case study on skin lesion detection highlights how such classifiers can amplify hidden shortcuts, underscoring the need for holistic, mechanistic interpretability. We provide code at https://github.com/maxdreyer/attributing-clip.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages (10 pages manuscript, 4 pages references, 11 pages appendix)",
    "pdf_url": "https://arxiv.org/pdf/2505.20229v1",
    "published_date": "2025-05-26 17:08:02 UTC",
    "updated_date": "2025-05-26 17:08:02 UTC"
  },
  {
    "arxiv_id": "2505.20214v1",
    "title": "The Mirage of Multimodality: Where Truth is Tested and Honesty Unravels",
    "authors": [
      "Jiaming Ji",
      "Sitong Fang",
      "Wenjing Cao",
      "Jiahao Li",
      "Xuyao Wang",
      "Juntao Dai",
      "Chi-Min Chan",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "abstract": "Reasoning models have recently attracted significant attention, especially for tasks that involve complex inference. Their strengths exemplify the System II paradigm (slow, structured thinking), contrasting with the System I (rapid, heuristic-driven). Yet, does slower reasoning necessarily lead to greater truthfulness? Our findings suggest otherwise. In this study, we present the first systematic investigation of distortions associated with System I and System II reasoning in multimodal contexts. We demonstrate that slower reasoning models, when presented with incomplete or misleading visual inputs, are more likely to fabricate plausible yet false details to support flawed reasoning -- a phenomenon we term the \"Mirage of Multimodality\". To examine this, we constructed a 5,000-sample hierarchical prompt dataset annotated by 50 human participants. These prompts gradually increase in complexity, revealing a consistent pattern: slower reasoning models tend to employ depth-first thinking (delving deeper into incorrect premises), whereas faster chat models favor breadth-first inference, exhibiting greater caution under uncertainty. Our results highlight a critical vulnerability of slower reasoning models: although highly effective in structured domains such as mathematics, it becomes brittle when confronted with ambiguous multimodal inputs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20214v1",
    "published_date": "2025-05-26 16:55:38 UTC",
    "updated_date": "2025-05-26 16:55:38 UTC"
  },
  {
    "arxiv_id": "2505.23799v4",
    "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics",
    "authors": [
      "Xiaoyuan Wu",
      "Weiran Lin",
      "Omer Akgul",
      "Lujo Bauer"
    ],
    "abstract": "Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility, one of which is to measure the consistency of LLM responses -- the model's confidence in the response or likelihood of generating a similar response when resampled. In previous work, measuring LLM response consistency often relied on calculating the probability of a response appearing within a pool of resampled responses, analyzing internal states, or evaluating logits of responses. However, it was not clear how well these approaches approximated users' perceptions of consistency of LLM responses. To find out, we performed a user study ($n=2,976$) demonstrating that current methods for measuring LLM response consistency typically do not align well with humans' perceptions of LLM consistency. We propose a logit-based ensemble method for estimating LLM consistency and show that our method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods for estimating LLM consistency without human evaluation are sufficiently imperfect to warrant broader use of evaluation with human input; this would avoid misjudging the adequacy of models because of the imperfections of automated consistency metrics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published as a main conference paper at EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.23799v4",
    "published_date": "2025-05-26 16:53:47 UTC",
    "updated_date": "2025-11-21 21:24:07 UTC"
  },
  {
    "arxiv_id": "2505.20211v2",
    "title": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection",
    "authors": [
      "Junseo Hwang",
      "Wonguk Cho",
      "Taesup Kim"
    ],
    "abstract": "Fine-tuning large foundation models is essential for building expert models tailored to specialized tasks and domains, but fully updating billions of parameters is computationally prohibitive. Reducing the number of trainable parameters using parameter-efficient fine-tuning is therefore crucial not only to reduce training costs but also to mitigate storage, caching, and serving overheads during deployment. Prior works, such as Singular Vectors-guided Fine-Tuning, have shown that exploiting the geometry of pre-trained weights can significantly improve parameter-efficiency, but they lack a solid theoretical foundation. In this paper, we introduce Parameter-efficient Fine-tuning with Column Space Projection (PiCa), a novel theoretically grounded PEFT method. We prove that projecting gradients onto the principal column space of pre-trained weights provides an effective inductive bias for adaptation and further enhance parameter efficiency through a novel weight-sharing strategy. Across diverse NLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines under comparable or smaller parameter budgets, demonstrating both theoretical rigor and practical effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20211v2",
    "published_date": "2025-05-26 16:52:40 UTC",
    "updated_date": "2025-10-02 04:11:07 UTC"
  },
  {
    "arxiv_id": "2505.20206v1",
    "title": "Evaluating Large Language Models for Code Review",
    "authors": [
      "Umut Cihan",
      "Arda İçöz",
      "Vahid Haratian",
      "Eray Tüzün"
    ],
    "abstract": "Context: Code reviews are crucial for software quality. Recent AI advances have allowed large language models (LLMs) to review and fix code; now, there are tools that perform these reviews. However, their reliability and accuracy have not yet been systematically evaluated. Objective: This study compares different LLMs' performance in detecting code correctness and suggesting improvements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated code blocks of varying correctness, along with 164 canonical code blocks from the HumanEval benchmark. To simulate the code review task objectively, we expected LLMs to assess code correctness and improve the code if needed. We ran experiments with different configurations and reported on the results. Results: With problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code correctness 68.50% and 63.89% of the time, respectively, and corrected the code 67.83% and 54.26% of the time for the 492 code blocks of varying correctness. Without problem descriptions, performance declined. The results for the 164 canonical code blocks differed, suggesting that performance depends on the type of code. Conclusion: LLM code reviews can help suggest improvements and assess correctness, but there is a risk of faulty outputs. We propose a process that involves humans, called the \"Human in the loop LLM Code Review\" to promote knowledge sharing while mitigating the risk of faulty outputs.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20206v1",
    "published_date": "2025-05-26 16:47:29 UTC",
    "updated_date": "2025-05-26 16:47:29 UTC"
  },
  {
    "arxiv_id": "2505.20203v3",
    "title": "Shutdownable Agents through POST-Agency",
    "authors": [
      "Elliott Thornley"
    ],
    "abstract": "Many fear that future artificial agents will resist shutdown. I present an idea - the POST-Agents Proposal - for ensuring that doesn't happen. I propose that we train agents to satisfy Preferences Only Between Same-Length Trajectories (POST). I then prove that POST - together with other conditions - implies Neutrality+: the agent maximizes expected utility, ignoring the probability distribution over trajectory-lengths. I argue that Neutrality+ keeps agents shutdownable and allows them to be useful.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20203v3",
    "published_date": "2025-05-26 16:44:17 UTC",
    "updated_date": "2026-01-03 16:19:32 UTC"
  },
  {
    "arxiv_id": "2505.20196v1",
    "title": "Temporal Sampling for Forgotten Reasoning in LLMs",
    "authors": [
      "Yuetai Li",
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Bhaskar Ramasubramanian",
      "Luyao Niu",
      "Bill Yuchen Lin",
      "Xiang Yue",
      "Radha Poovendran"
    ],
    "abstract": "Fine-tuning large language models (LLMs) is intended to improve their reasoning capabilities, yet we uncover a counterintuitive effect: models often forget how to solve problems they previously answered correctly during training. We term this phenomenon temporal forgetting and show that it is widespread across model sizes, fine-tuning methods (both Reinforcement Learning and Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this gap, we introduce Temporal Sampling, a simple decoding strategy that draws outputs from multiple checkpoints along the training trajectory. This approach recovers forgotten solutions without retraining or ensembling, and leads to substantial improvements in reasoning performance, gains from 4 to 19 points in Pass@k and consistent gains in Majority@k across several benchmarks. We further extend our method to LoRA-adapted models, demonstrating that storing only adapter weights across checkpoints achieves similar benefits with minimal storage cost. By leveraging the temporal diversity inherent in training, Temporal Sampling offers a practical, compute-efficient way to surface hidden reasoning ability and rethink how we evaluate LLMs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20196v1",
    "published_date": "2025-05-26 16:39:52 UTC",
    "updated_date": "2025-05-26 16:39:52 UTC"
  },
  {
    "arxiv_id": "2505.20190v2",
    "title": "A Text-Based Recommender System that Leverages Explicit Affective State Preferences",
    "authors": [
      "Tonmoy Hasan",
      "Razvan Bunescu"
    ],
    "abstract": "The affective attitude of liking a recommended item reflects just one category in a wide spectrum of affective phenomena that also includes emotions such as entranced or intrigued, moods such as cheerful or buoyant, as well as more fine-grained affective states, such as \"pleasantly surprised by the conclusion\". In this paper, we introduce a novel recommendation task that can leverage a virtually unbounded range of affective states sought explicitly by the user in order to identify items that, upon consumption, are likely to induce those affective states. Correspondingly, we create a large dataset of user preferences containing expressions of fine-grained affective states that are mined from book reviews, and propose a Transformer-based architecture that leverages such affective expressions as input. We then use the resulting dataset of affective states preferences, together with the linked users and their histories of book readings, ratings, and reviews, to train and evaluate multiple recommendation models on the task of matching recommended items with affective preferences. Experiments show that the best results are obtained by models that can utilize textual descriptions of items and user affective preferences.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "To appear at EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20190v2",
    "published_date": "2025-05-26 16:33:14 UTC",
    "updated_date": "2025-08-21 21:40:41 UTC"
  },
  {
    "arxiv_id": "2505.20184v1",
    "title": "THiNK: Can Large Language Models Think-aloud?",
    "authors": [
      "Yongan Yu",
      "Mengqian Wu",
      "Yiran Lin",
      "Nikki G. Lobczowski"
    ],
    "abstract": "Assessing higher-order thinking skills in large language models (LLMs) remains a fundamental challenge, especially in tasks that go beyond surface-level accuracy. In this work, we propose THiNK (Testing Higher-order Notion of Knowledge), a multi-agent, feedback-driven evaluation framework grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative task of problem generation, critique, and revision, encouraging LLMs to think-aloud through step-by-step reflection and refinement. This enables a systematic evaluation of both lower-order (e.g., remember, understand) and higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven state-of-the-art LLMs and perform a detailed cognitive analysis of their outputs. Results reveal that while models reliably perform lower-order categories well, they struggle with applying knowledge in realistic contexts and exhibit limited abstraction. Structured feedback loops significantly improve reasoning performance, particularly in higher-order thinking. Qualitative evaluations further confirm that THiNK-guided outputs better align with domain logic and problem structure. The code of our framework provides a scalable methodology for probing and enhancing LLM reasoning, offering new directions for evaluation grounded in learning science, which is available at our GitHub repository.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20184v1",
    "published_date": "2025-05-26 16:27:02 UTC",
    "updated_date": "2025-05-26 16:27:02 UTC"
  },
  {
    "arxiv_id": "2505.20182v1",
    "title": "An Empirical Study on Strong-Weak Model Collaboration for Repo-level Code Generation",
    "authors": [
      "Shubham Gandhi",
      "Atharva Naik",
      "Yiqing Xie",
      "Carolyn Rose"
    ],
    "abstract": "We study cost-efficient collaboration between strong and weak language models for repository-level code generation, where the weak model handles simpler tasks at lower cost, and the most challenging tasks are delegated to the strong model. While many works propose architectures for this task, few analyze performance relative to cost. We evaluate a broad spectrum of collaboration strategies: context-based, pipeline-based, and dynamic, on GitHub issue resolution. Our most effective collaborative strategy achieves equivalent performance to the strong model while reducing the cost by 40%. Based on our findings, we offer actionable guidelines for choosing collaboration strategies under varying budget and performance constraints. Our results show that strong-weak collaboration substantially boosts the weak model's performance at a fraction of the cost, pipeline and context-based methods being most efficient. We release the code for our work at https://github.com/shubhamrgandhi/codegen-strong-weak-collab.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20182v1",
    "published_date": "2025-05-26 16:25:38 UTC",
    "updated_date": "2025-05-26 16:25:38 UTC"
  },
  {
    "arxiv_id": "2505.20181v1",
    "title": "The Problem of Algorithmic Collisions: Mitigating Unforeseen Risks in a Connected World",
    "authors": [
      "Maurice Chiodo",
      "Dennis Müller"
    ],
    "abstract": "The increasing deployment of Artificial Intelligence (AI) and other autonomous algorithmic systems presents the world with new systemic risks. While focus often lies on the function of individual algorithms, a critical and underestimated danger arises from their interactions, particularly when algorithmic systems operate without awareness of each other, or when those deploying them are unaware of the full algorithmic ecosystem deployment is occurring in. These interactions can lead to unforeseen, rapidly escalating negative outcomes - from market crashes and energy supply disruptions to potential physical accidents and erosion of public trust - often exceeding the human capacity for effective monitoring and the legal capacities for proper intervention. Current governance frameworks are inadequate as they lack visibility into this complex ecosystem of interactions. This paper outlines the nature of this challenge and proposes some initial policy suggestions centered on increasing transparency and accountability through phased system registration, a licensing framework for deployment, and enhanced monitoring capabilities.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "math.HO"
    ],
    "primary_category": "cs.CY",
    "comment": "27 pages. This is an early concept paper, and we plan to add further content to it over time. Please get in touch if you want to be part of its further development. Keywords: algorithmic collision, AI agents, algorithmic ecosystem, flash crash, multiagent systems",
    "pdf_url": "https://arxiv.org/pdf/2505.20181v1",
    "published_date": "2025-05-26 16:22:18 UTC",
    "updated_date": "2025-05-26 16:22:18 UTC"
  },
  {
    "arxiv_id": "2505.20170v2",
    "title": "Program of Equations Thoughts to Solve Algebra Word Problems",
    "authors": [
      "Yunze Lin"
    ],
    "abstract": "Solving algebraic word problems (AWPs) has recently emerged as an important natural language processing task. Recently, large language models (LLMs) have demonstrated powerful mathematical capabilities, and the Chain-of-Thought technique, which guides LLMs through step-by-step reasoning, has yielded impressive results. However, this reasoning ability is limited by the computational weaknesses of LLMs themselves, where calculation errors can accumulate, leading to incorrect final answers. To address this, we propose Program of Equations Thoughts (POET), which transforms the task of generating step-by-step reasoning answers into a two-stage task of predicting equations and generating code, offloading complex computations to a Python interpreter to avoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which utilizes a manually designed template to enable LLMs to directly generate Python code for one-step solving. Our method achieves accuracies of 95.3% and 98.0% on the PEN and ALG514 datasets, respectively, setting a new state-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5% on the DRAW-1K dataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Withdrawn pending institutional authorization and core revisions to address methodological inconsistencies in Sections 3-4",
    "pdf_url": "https://arxiv.org/pdf/2505.20170v2",
    "published_date": "2025-05-26 16:12:04 UTC",
    "updated_date": "2025-07-01 14:53:43 UTC"
  },
  {
    "arxiv_id": "2505.20166v3",
    "title": "From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data",
    "authors": [
      "Chun-Yi Kuan",
      "Hung-yi Lee"
    ],
    "abstract": "Audio-aware large language models (ALLMs) have recently made great strides in understanding and processing audio inputs. These models are typically adapted from text-based large language models (LLMs) through additional training on audio-related tasks. This adaptation process presents two major limitations. First, ALLMs often suffer from catastrophic forgetting, where crucial textual capabilities like instruction-following are lost after training on audio data. In some cases, models may even hallucinate sounds that are not present in the input audio, raising concerns about reliability. Second, achieving cross-modal alignment between audio and language typically relies on large collections of task-specific question-answer pairs for instruction tuning, making it resource-intensive. To address these issues, previous works have leveraged the backbone LLMs to synthesize general-purpose, caption-style alignment data. In this paper, we propose a data generation framework that produces contrastive-like training data, designed to enhance ALLMs' ability to differentiate between present and absent sounds. We further extend our approach to multi-audio scenarios, enabling the model to either explain differences between audio inputs or produce unified captions that describe all inputs, thereby enhancing audio-language alignment. We refer to the entire ALLM training framework as bootstrapping audio-language alignment via synthetic data generation from backbone LLMs (BALSa). Experimental results indicate that our method effectively mitigates audio hallucinations while reliably maintaining strong performance on audio understanding and reasoning benchmarks, as well as instruction-following skills. Moreover, incorporating multi-audio training further enhances the model's comprehension and reasoning capabilities. Overall, BALSa offers an efficient and scalable approach to developing ALLMs.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Published in IEEE Transactions on Audio, Speech, and Language Processing (TASLP). Project Website: https://kuan2jiu99.github.io/Balsa",
    "pdf_url": "https://arxiv.org/pdf/2505.20166v3",
    "published_date": "2025-05-26 16:08:41 UTC",
    "updated_date": "2026-01-10 15:10:08 UTC"
  },
  {
    "arxiv_id": "2505.20162v1",
    "title": "Capability-Based Scaling Laws for LLM Red-Teaming",
    "authors": [
      "Alexander Panfilov",
      "Paul Kassianik",
      "Maksym Andriushchenko",
      "Jonas Geiping"
    ],
    "abstract": "As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20162v1",
    "published_date": "2025-05-26 16:05:41 UTC",
    "updated_date": "2025-05-26 16:05:41 UTC"
  },
  {
    "arxiv_id": "2505.20161v1",
    "title": "Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning",
    "authors": [
      "Jaehun Jung",
      "Seungju Han",
      "Ximing Lu",
      "Skyler Hallinan",
      "David Acuna",
      "Shrimai Prabhumoye",
      "Mostafa Patwary",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Yejin Choi"
    ],
    "abstract": "Effective generalization in language models depends critically on the diversity of their training data. Yet existing diversity metrics often fall short of this goal, relying on surface-level heuristics that are decoupled from model behavior. This motivates us to ask: What kind of diversity in training data actually drives generalization in language models -- and how can we measure and amplify it? Through large-scale empirical analyses spanning over 300 training runs, carefully controlled for data scale and quality, we show that data diversity can be a strong predictor of generalization in LLM reasoning -- as measured by average model performance on unseen out-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies diversity via the entropy of model-induced gradients. Despite using a small off-the-shelf proxy model for gradients, G-Vendi consistently outperforms alternative measures, achieving strong correlation (Spearman's $ρ\\approx 0.9$) with out-of-distribution (OOD) performance on both natural language inference (NLI) and math reasoning tasks. Building on this insight, we present Prismatic Synthesis, a framework for generating diverse synthetic data by targeting underrepresented regions in gradient space. Experimental results show that Prismatic Synthesis consistently improves model performance as we scale synthetic data -- not just on in-distribution test but across unseen, out-of-distribution benchmarks -- significantly outperforming state-of-the-art models that rely on 20 times larger data generator than ours. For example, PrismMath-7B, our model distilled from a 32B LLM, outperforms R1-Distill-Qwen-7B -- the same base model trained on proprietary data generated by 671B R1 -- on 6 out of 7 challenging benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20161v1",
    "published_date": "2025-05-26 16:05:10 UTC",
    "updated_date": "2025-05-26 16:05:10 UTC"
  },
  {
    "arxiv_id": "2505.20152v3",
    "title": "MMGeoLM: Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models",
    "authors": [
      "Kai Sun",
      "Yushi Bai",
      "Zhen Yang",
      "Jiajie Zhang",
      "Ji Qi",
      "Lei Hou",
      "Juanzi Li"
    ],
    "abstract": "Large Multimodal Models (LMMs) typically build on ViTs (e.g., CLIP), yet their training with simple random in-batch negatives limits the ability to capture fine-grained visual differences, particularly in geometric scenarios. To address this challenge, we propose a novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrieval-based negatives selected based on caption similarity. We train a vision encoder (CLIP) using our hard negative training method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with a size of 7B, it can rival powerful closed-source models like GPT-4o. We further conduct ablation studies to analyze three key factors: hard negative types, the efficiency of image-based negatives, and training configurations. These analyses yield important insights into optimizing the training pipeline of vision encoder for fine-grained geometric reasoning tasks. https://github.com/THU-KEG/MMGeoLM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20152v3",
    "published_date": "2025-05-26 15:55:28 UTC",
    "updated_date": "2025-10-01 03:17:28 UTC"
  },
  {
    "arxiv_id": "2505.20150v1",
    "title": "On the (Non) Injectivity of Piecewise Linear Janossy Pooling",
    "authors": [
      "Ilai Reshef",
      "Nadav Dym"
    ],
    "abstract": "Multiset functions, which are functions that map multisets to vectors, are a fundamental tool in the construction of neural networks for multisets and graphs. To guarantee that the vector representation of the multiset is faithful, it is often desirable to have multiset mappings that are both injective and bi-Lipschitz. Currently, there are several constructions of multiset functions achieving both these guarantees, leading to improved performance in some tasks but often also to higher compute time than standard constructions. Accordingly, it is natural to inquire whether simpler multiset functions achieving the same guarantees are available. In this paper, we make a large step towards giving a negative answer to this question. We consider the family of k-ary Janossy pooling, which includes many of the most popular multiset models, and prove that no piecewise linear Janossy pooling function can be injective. On the positive side, we show that when restricted to multisets without multiplicities, even simple deep-sets models suffice for injectivity and bi-Lipschitzness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20150v1",
    "published_date": "2025-05-26 15:53:09 UTC",
    "updated_date": "2025-05-26 15:53:09 UTC"
  },
  {
    "arxiv_id": "2505.20149v1",
    "title": "Improvement Strategies for Few-Shot Learning in OCT Image Classification of Rare Retinal Diseases",
    "authors": [
      "Cheng-Yu Tai",
      "Ching-Wen Chen",
      "Chi-Chin Wu",
      "Bo-Chen Chiu",
      "Cheng-Hung",
      "Lin",
      "Cheng-Kai Lu",
      "Jia-Kang Wang",
      "Tzu-Lun Huang"
    ],
    "abstract": "This paper focuses on using few-shot learning to improve the accuracy of classifying OCT diagnosis images with major and rare classes. We used the GAN-based augmentation strategy as a baseline and introduced several novel methods to further enhance our model. The proposed strategy contains U-GAT-IT for improving the generative part and uses the data balance technique to narrow down the skew of accuracy between all categories. The best model obtained was built with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an overall accuracy of 97.85%, representing a significant improvement over the original baseline.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20149v1",
    "published_date": "2025-05-26 15:49:44 UTC",
    "updated_date": "2025-05-26 15:49:44 UTC"
  },
  {
    "arxiv_id": "2505.20148v3",
    "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents",
    "authors": [
      "Ziming Wei",
      "Bingqian Lin",
      "Zijian Jiao",
      "Yunshuang Nie",
      "Liang Ma",
      "Yuecheng Liu",
      "Yuzheng Zhuang",
      "Xiaodan Liang"
    ],
    "abstract": "Spatial Planning is a crucial part in the field of spatial intelligence, which requires the understanding and planning about object arrangements in space perspective. AI agents with the spatial planning ability can better adapt to various real-world applications, including robotic manipulation, automatic assembly, urban planning etc. Recent works have attempted to construct benchmarks for evaluating the spatial intelligence of Multimodal Large Language Models (MLLMs). Nevertheless, these benchmarks primarily focus on spatial reasoning based on typical Visual Question-Answering (VQA) forms, which suffers from the gap between abstract spatial understanding and concrete task execution. In this work, we take a step further to build a comprehensive benchmark called MineAnyBuild, aiming to evaluate the spatial planning ability of open-world AI agents in the Minecraft game. Specifically, MineAnyBuild requires an agent to generate executable architecture building plans based on the given multi-modal human instructions. It involves 4,000 curated spatial planning tasks and also provides a paradigm for infinitely expandable data collection by utilizing rich player-generated content. MineAnyBuild evaluates spatial planning through four core supporting dimensions: spatial understanding, spatial reasoning, creativity, and spatial commonsense. Based on MineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based agents, revealing the severe limitations but enormous potential in their spatial planning abilities. We believe our MineAnyBuild will open new avenues for the evaluation of spatial intelligence and help promote further development for open-world AI agents capable of spatial planning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track",
    "pdf_url": "https://arxiv.org/pdf/2505.20148v3",
    "published_date": "2025-05-26 15:48:14 UTC",
    "updated_date": "2025-09-28 09:39:59 UTC"
  },
  {
    "arxiv_id": "2505.20139v2",
    "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs",
    "authors": [
      "Jialin Yang",
      "Dongfu Jiang",
      "Lipeng He",
      "Sherman Siu",
      "Yuxuan Zhang",
      "Disen Liao",
      "Zhuofeng Li",
      "Huaye Zeng",
      "Yiming Jia",
      "Haozhe Wang",
      "Benjamin Schneider",
      "Chi Ruan",
      "Wentao Ma",
      "Zhiheng Lyu",
      "Yifei Wang",
      "Yi Lu",
      "Quy Duc Do",
      "Ziyan Jiang",
      "Ping Nie",
      "Wenhu Chen"
    ],
    "abstract": "As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: 1) generation tasks, producing structured output from natural language prompts, and \\textbf{2)} conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps-even state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "24 pages, 8 figures, 14 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.20139v2",
    "published_date": "2025-05-26 15:40:42 UTC",
    "updated_date": "2026-01-19 21:14:10 UTC"
  },
  {
    "arxiv_id": "2505.20137v3",
    "title": "ePC: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks",
    "authors": [
      "Cédric Goemaere",
      "Gaspard Oliviers",
      "Rafal Bogacz",
      "Thomas Demeester"
    ],
    "abstract": "Predictive Coding (PC) offers a biologically plausible alternative to backpropagation for neural network training, yet struggles with deeper architectures. This paper identifies the root cause and provides a principled solution. We uncover that the canonical state-based formulation of PC (sPC) is, by design, deeply inefficient on digital hardware, due to an inherent signal decay problem that scales exponentially with depth. To address this fundamental limitation, we introduce a novel reparameterization of PC, named error-based PC (ePC), which does not suffer from signal decay. By optimizing over prediction errors rather than states, ePC enables signals to reach all layers simultaneously and unattenuated, converging orders of magnitude faster than sPC. Experiments across multiple architectures and datasets demonstrate that ePC matches backpropagation's performance even for deeper models where sPC struggles. Besides practical improvements, our work provides theoretical insight into PC dynamics and establishes a foundation for scaling bio-inspired learning to deeper architectures on digital hardware and beyond.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Name change (Error Optimization => ePC) to reflect that ePC is indeed still PC, not a separate algorithm. All code available at https://github.com/cgoemaere/error_based_PC",
    "pdf_url": "https://arxiv.org/pdf/2505.20137v3",
    "published_date": "2025-05-26 15:39:16 UTC",
    "updated_date": "2025-09-29 15:58:40 UTC"
  },
  {
    "arxiv_id": "2505.20132v1",
    "title": "Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks",
    "authors": [
      "Safa Hamreras",
      "Sukhbinder Singh",
      "Román Orús"
    ],
    "abstract": "Tensorizing a neural network involves reshaping some or all of its dense weight matrices into higher-order tensors and approximating them using low-rank tensor network decompositions. This technique has shown promise as a model compression strategy for large-scale neural networks. However, despite encouraging empirical results, tensorized neural networks (TNNs) remain underutilized in mainstream deep learning. In this position paper, we offer a perspective on both the potential and current limitations of TNNs. We argue that TNNs represent a powerful yet underexplored framework for deep learning--one that deserves greater attention from both engineering and theoretical communities. Beyond compression, we highlight the value of TNNs as a flexible class of architectures with distinctive scaling properties and increased interpretability. A central feature of TNNs is the presence of bond indices, which introduce new latent spaces not found in conventional networks. These internal representations may provide deeper insight into the evolution of features across layers, potentially advancing the goals of mechanistic interpretability. We conclude by outlining several key research directions aimed at overcoming the practical barriers to scaling and adopting TNNs in modern deep learning workflows.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "This article has been prepared for submission as a \"Position paper\" following the guidelines provided at https://neurips.cc/Conferences/2025/CallForPositionPapers",
    "pdf_url": "https://arxiv.org/pdf/2505.20132v1",
    "published_date": "2025-05-26 15:32:28 UTC",
    "updated_date": "2025-05-26 15:32:28 UTC"
  },
  {
    "arxiv_id": "2505.20127v2",
    "title": "Agentic AI Process Observability: Discovering Behavioral Variability",
    "authors": [
      "Fabiana Fournier",
      "Lior Limonad",
      "Yuval David"
    ],
    "abstract": "AI agents that leverage Large Language Models (LLMs) are increasingly becoming core building blocks of modern software systems. A wide range of frameworks is now available to support the specification of such applications. These frameworks enable the definition of agent setups using natural language prompting, which specifies the roles, goals, and tools assigned to the various agents involved. Within such setups, agent behavior is non-deterministic for any given input, highlighting the critical need for robust debugging and observability tools. In this work, we explore the use of process and causal discovery applied to agent execution trajectories as a means of enhancing developer observability. This approach aids in monitoring and understanding the emergent variability in agent behavior. Additionally, we complement this with LLM-based static analysis techniques to distinguish between intended and unintended behavioral variability. We argue that such instrumentation is essential for giving developers greater control over evolving specifications and for identifying aspects of functionality that may require more precise and explicit definitions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20127v2",
    "published_date": "2025-05-26 15:26:07 UTC",
    "updated_date": "2025-07-03 10:05:46 UTC"
  },
  {
    "arxiv_id": "2505.20120v1",
    "title": "Agents Require Metacognitive and Strategic Reasoning to Succeed in the Coming Labor Markets",
    "authors": [
      "Simpson Zhang",
      "Tennison Liu",
      "Mihaela van der Schaar"
    ],
    "abstract": "Current labor markets are strongly affected by the economic forces of adverse selection, moral hazard, and reputation, each of which arises due to $\\textit{incomplete information}$. These economic forces will still be influential after AI agents are introduced, and thus, agents must use metacognitive and strategic reasoning to perform effectively. Metacognition is a form of $\\textit{internal reasoning}$ that includes the capabilities for self-assessment, task understanding, and evaluation of strategies. Strategic reasoning is $\\textit{external reasoning}$ that covers holding beliefs about other participants in the labor market (e.g., competitors, colleagues), making strategic decisions, and learning about others over time. Both types of reasoning are required by agents as they decide among the many $\\textit{actions}$ they can take in labor markets, both within and outside their jobs. We discuss current research into metacognitive and strategic reasoning and the areas requiring further development.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "*Zhang & Liu contributed equally",
    "pdf_url": "https://arxiv.org/pdf/2505.20120v1",
    "published_date": "2025-05-26 15:22:04 UTC",
    "updated_date": "2025-05-26 15:22:04 UTC"
  },
  {
    "arxiv_id": "2505.20119v1",
    "title": "Spatiotemporal Causal Decoupling Model for Air Quality Forecasting",
    "authors": [
      "Jiaming Ma",
      "Guanjun Wang",
      "Sheng Huang",
      "Kuo Yang",
      "Binwu Wang",
      "Pengkun Wang",
      "Yang Wang"
    ],
    "abstract": "Due to the profound impact of air pollution on human health, livelihoods, and economic development, air quality forecasting is of paramount significance. Initially, we employ the causal graph method to scrutinize the constraints of existing research in comprehensively modeling the causal relationships between the air quality index (AQI) and meteorological features. In order to enhance prediction accuracy, we introduce a novel air quality forecasting model, AirCade, which incorporates a causal decoupling approach. AirCade leverages a spatiotemporal module in conjunction with knowledge embedding techniques to capture the internal dynamics of AQI. Subsequently, a causal decoupling module is proposed to disentangle synchronous causality from past AQI and meteorological features, followed by the dissemination of acquired knowledge to future time steps to enhance performance. Additionally, we introduce a causal intervention mechanism to explicitly represent the uncertainty of future meteorological features, thereby bolstering the model's robustness. Our evaluation of AirCade on an open-source air quality dataset demonstrates over 20\\% relative improvement over state-of-the-art models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20119v1",
    "published_date": "2025-05-26 15:21:57 UTC",
    "updated_date": "2025-05-26 15:21:57 UTC"
  },
  {
    "arxiv_id": "2505.20377v1",
    "title": "Algorithmic Control Improves Residential Building Energy and EV Management when PV Capacity is High but Battery Capacity is Low",
    "authors": [
      "Lennart Ullner",
      "Alona Zharova",
      "Felix Creutzig"
    ],
    "abstract": "Efficient energy management in prosumer households is key to alleviating grid stress in an energy transition marked by electric vehicles (EV), renewable energies and battery storage. However, it is unclear how households optimize prosumer EV charging. Here we study real-world data from 90 households on fixed-rate electricity tariffs in German-speaking countries to investigate the potential of Deep Reinforcement Learning (DRL) and other control approaches (Rule-Based, Model Predictive Control) to manage the dynamic and uncertain environment of Home Energy Management (HEM) and optimize household charging patterns. The DRL agent efficiently aligns charging of EV and battery storage with photovoltaic (PV) surplus. We find that frequent EV charging transactions, early EV connections and PV surplus increase optimization potential. A detailed analysis of nine households (1 hour resolution, 1 year) demonstrates that high battery capacity facilitates self optimization; in this case further algorithmic control shows little value. In cases with relatively low battery capacity, algorithmic control with DRL improves energy management and cost savings by a relevant margin. This result is further corroborated by our simulation of a synthetic household. We conclude that prosumer households with optimization potential would profit from DRL, thus benefiting also the full electricity system and its decarbonization.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20377v1",
    "published_date": "2025-05-26 15:19:01 UTC",
    "updated_date": "2025-05-26 15:19:01 UTC"
  },
  {
    "arxiv_id": "2505.20113v1",
    "title": "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone",
    "authors": [
      "Cristian Santini",
      "Laura Melosi",
      "Emanuele Frontoni"
    ],
    "abstract": "The increased digitization of world's textual heritage poses significant challenges for both computer science and literary studies. Overall, there is an urgent need of computational techniques able to adapt to the challenges of historical texts, such as orthographic and spelling variations, fragmentary structure and digitization errors. The rise of large language models (LLMs) has revolutionized natural language processing, suggesting promising applications for Named Entity Recognition (NER) on historical documents. In spite of this, no thorough evaluation has been proposed for Italian texts. This research tries to fill the gap by proposing a new challenging dataset for entity extraction based on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's Zibaldone (1898), containing 2,899 references to people, locations and literary works. This dataset was used to carry out reproducible experiments with both domain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1. Results show that instruction-tuned models encounter multiple difficulties handling historical humanistic texts, while fine-tuned NER models offer more robust performance even with challenging entity types such as bibliographic references.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20113v1",
    "published_date": "2025-05-26 15:16:48 UTC",
    "updated_date": "2025-05-26 15:16:48 UTC"
  },
  {
    "arxiv_id": "2505.20112v3",
    "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression",
    "authors": [
      "Haolei Bai",
      "Siyong Jian",
      "Tuo Liang",
      "Yu Yin",
      "Huan Wang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in a wide range of downstream natural language processing tasks. Nevertheless, their considerable sizes and memory demands hinder practical deployment, underscoring the importance of developing efficient compression strategies. Singular value decomposition (SVD) decomposes a matrix into orthogonal components, enabling efficient low-rank approximation. This is particularly suitable for LLM compression, where weight matrices often exhibit significant redundancy. However, current SVD-based methods neglect the residual matrix from truncation, resulting in significant truncation loss. Additionally, compressing all layers of the model results in severe performance degradation. To overcome these limitations, we propose ResSVD, a new post-training SVD-based LLM compression method. Specifically, we leverage the residual matrix generated during the truncation process to reduce truncation loss. Moreover, under a fixed overall compression ratio, we selectively compress the last few layers of the model, which mitigates error propagation and significantly improves the performance of compressed models. Comprehensive evaluations of ResSVD on diverse LLM families and multiple benchmark datasets indicate that ResSVD consistently achieves superior performance over existing counterpart methods, demonstrating its practical effectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20112v3",
    "published_date": "2025-05-26 15:14:54 UTC",
    "updated_date": "2025-12-19 06:23:19 UTC"
  },
  {
    "arxiv_id": "2505.23798v1",
    "title": "My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals",
    "authors": [
      "Jian Lan",
      "Yifei Fu",
      "Udo Schlegel",
      "Gengyuan Zhang",
      "Tanveer Hannan",
      "Haokun Chen",
      "Thomas Seidl"
    ],
    "abstract": "Social bias is a critical issue in large vision-language models (VLMs), where fairness- and ethics-related problems harm certain groups of people in society. It is unknown to what extent VLMs yield social bias in generative responses. In this study, we focus on evaluating and mitigating social bias on both the model's response and probability distribution. To do so, we first evaluate four state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the multiple-choice selection task. Surprisingly, we find that models suffer from generating gender-biased or race-biased responses. We also observe that models are prone to stating their responses are fair, but indeed having mis-calibrated confidence levels towards particular social groups. While investigating why VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit substantial fluctuations in fairness levels. Meanwhile, residuals in each layer show mixed effects on fairness, with some contributing positively while some lead to increased bias. Based on these findings, we propose a post-hoc method for the inference stage to mitigate social bias, which is training-free and model-agnostic. We achieve this by ablating bias-associated residuals while amplifying fairness-associated residuals on model hidden layers during inference. We demonstrate that our post-hoc method outperforms the competing training strategies, helping VLMs have fairer responses and more reliable confidence levels.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23798v1",
    "published_date": "2025-05-26 15:14:16 UTC",
    "updated_date": "2025-05-26 15:14:16 UTC"
  },
  {
    "arxiv_id": "2505.20110v2",
    "title": "Beyond the Proxy: Trajectory-Distilled Guidance for Offline GFlowNet Training",
    "authors": [
      "Ruishuo Chen",
      "Xun Wang",
      "Rui Hu",
      "Zhuoran Li",
      "Longbo Huang"
    ],
    "abstract": "Generative Flow Networks (GFlowNets) are effective at sampling diverse, high-reward objects, but in many real-world settings where new reward queries are infeasible, they must be trained from offline datasets. The prevailing proxy-based training methods are susceptible to error propagation, while existing proxy-free approaches often use coarse constraints that limit exploration. To address these issues, we propose Trajectory-Distilled GFlowNet (TD-GFN), a novel proxy-free training framework. TD-GFN learns dense, transition-level edge rewards from offline trajectories via inverse reinforcement learning to provide rich structural guidance for efficient exploration. Crucially, to ensure robustness, these rewards are used indirectly to guide the policy through DAG pruning and prioritized backward sampling of training trajectories. This ensures that final gradient updates depend only on ground-truth terminal rewards from the dataset, thereby preventing the error propagation. Experiments show that TD-GFN significantly outperforms a broad range of existing baselines in both convergence speed and final sample quality, establishing a more robust and efficient paradigm for offline GFlowNet training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20110v2",
    "published_date": "2025-05-26 15:12:22 UTC",
    "updated_date": "2025-09-26 13:08:31 UTC"
  },
  {
    "arxiv_id": "2505.20109v1",
    "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models",
    "authors": [
      "June-Woo Kim",
      "Wonkyo Oh",
      "Haram Yoon",
      "Sung-Hoon Yoon",
      "Dae-Jin Kim",
      "Dong-Ho Lee",
      "Sang-Yeol Lee",
      "Chan-Mo Yang"
    ],
    "abstract": "Suicidal risk detection in adolescents is a critical challenge, yet existing methods rely on language-specific models, limiting scalability and generalization. This study introduces a novel language-agnostic framework for suicidal risk assessment with large language models (LLMs). We generate Chinese transcripts from speech using an ASR model and then employ LLMs with prompt-based queries to extract suicidal risk-related features from these transcripts. The extracted features are retained in both Chinese and English to enable cross-linguistic analysis and then used to fine-tune corresponding pretrained language models independently. Experimental results show that our method achieves performance comparable to direct fine-tuning with ASR results or to models trained solely on Chinese suicidal risk-related features, demonstrating its potential to overcome language constraints and improve the robustness of suicidal risk assessment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to InterSpeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20109v1",
    "published_date": "2025-05-26 15:12:10 UTC",
    "updated_date": "2025-05-26 15:12:10 UTC"
  },
  {
    "arxiv_id": "2505.20100v1",
    "title": "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models",
    "authors": [
      "Fengyuan Sun",
      "Leqi Shen",
      "Hui Chen",
      "Sicheng Zhao",
      "Jungong Han",
      "Guiguang Ding"
    ],
    "abstract": "Video Large Language Models (Video LLMs) have achieved remarkable results in video understanding tasks. However, they often suffer from heavy computational overhead due to the large number of visual tokens generated from multiple video frames. Existing visual token compression methods often rely on attention scores from language models as guidance. However, these scores exhibit inherent biases: global bias reflects a tendency to focus on the two ends of the visual token sequence, while local bias leads to an over-concentration on the same spatial positions across different frames. To address the issue of attention bias, we propose $\\textbf{A}$ttention-$\\textbf{D}$ebi$\\textbf{a}$sed $\\textbf{T}$oken $\\textbf{P}$runing for Video Large Language Models ($\\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP integrates two dedicated debiasing modules into the pipeline, targeting global attention bias and local attention bias, respectively. Without the need for additional training, our method significantly reduces the computational overhead of Video LLMs while retaining the performance of vanilla models. Extensive evaluation shows that AdaTP achieves state-of-the-art performance in various commonly used video understanding benchmarks. In particular, on LLaVA-OneVision-7B, AdaTP maintains performance without degradation while using only up to $27.3\\%$ FLOPs compared to the vanilla model. Our code will be released soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20100v1",
    "published_date": "2025-05-26 15:08:37 UTC",
    "updated_date": "2025-05-26 15:08:37 UTC"
  },
  {
    "arxiv_id": "2505.20099v2",
    "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities",
    "authors": [
      "Chuangtao Ma",
      "Yongrui Chen",
      "Tianxing Wu",
      "Arijit Khan",
      "Haofen Wang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance on question-answering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, and hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs) for QA to address the above challenges. In this survey, we propose a new structured taxonomy that categorizes the methodology of synthesizing LLMs and KGs for QA according to the categories of QA and the KG's role when integrating with LLMs. We systematically survey state-of-the-art methods in synthesizing LLMs and KGs for QA and compare and analyze these approaches in terms of strength, limitations, and KG requirements. We then align the approaches with QA and discuss how these approaches address the main challenges of different complex QA. Finally, we summarize the advancements, evaluation metrics, and benchmark datasets and highlight open challenges and opportunities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2505.20099v2",
    "published_date": "2025-05-26 15:08:23 UTC",
    "updated_date": "2025-09-22 13:18:46 UTC"
  },
  {
    "arxiv_id": "2505.20096v2",
    "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning",
    "authors": [
      "Thang Nguyen",
      "Peter Chin",
      "Yu-Wing Tai"
    ],
    "abstract": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation (RAG) that addresses the inherent ambiguities and reasoning challenges in complex information-seeking tasks. Unlike conventional RAG methods that rely on end-to-end fine-tuning or isolated component enhancements, MA-RAG orchestrates a collaborative set of specialized AI agents: Planner, Step Definer, Extractor, and QA Agents, each responsible for a distinct stage of the RAG pipeline. By decomposing tasks into subtasks such as query disambiguation, evidence extraction, and answer synthesis, and enabling agents to communicate intermediate reasoning via chain-of-thought prompting, MA-RAG progressively refines retrieval and synthesis while maintaining modular interpretability. Extensive experiments on multi-hop and ambiguous QA benchmarks, including NQ, HotpotQA, 2WikimQA, and TriviaQA, demonstrate that MA-RAG significantly outperforms standalone LLMs and existing RAG methods across all model scales. Notably, even a small LLaMA3-8B model equipped with MA-RAG surpasses larger standalone LLMs, while larger variants (LLaMA3-70B and GPT-4o-mini) set new state-of-the-art results on challenging multi-hop datasets. Ablation studies reveal that both the planner and extractor agents are critical for multi-hop reasoning, and that high-capacity models are especially important for the QA agent to synthesize answers effectively. Beyond general-domain QA, MA-RAG generalizes to specialized domains such as medical QA, achieving competitive performance against domain-specific models without any domain-specific fine-tuning. Our results highlight the effectiveness of collaborative, modular reasoning in retrieval-augmented systems: MA-RAG not only improves answer accuracy and robustness but also provides interpretable intermediate reasoning steps, establishing a new paradigm for efficient and reliable multi-agent RAG.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20096v2",
    "published_date": "2025-05-26 15:05:18 UTC",
    "updated_date": "2025-10-11 16:19:57 UTC"
  },
  {
    "arxiv_id": "2505.20094v3",
    "title": "SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale",
    "authors": [
      "Qi Li",
      "Kun Li",
      "Haozhi Han",
      "Honghui Shang",
      "Xinfu He",
      "Yunquan Zhang",
      "Hong An",
      "Ting Cao",
      "Mao Yang"
    ],
    "abstract": "Can a scientific simulation system be physically consistent, interpretable by design, and scalable across regimes--all at once? Despite decades of progress, this trifecta remains elusive. Classical methods like Kinetic Monte Carlo ensure thermodynamic accuracy but scale poorly; learning-based methods offer efficiency but often sacrifice physical consistency and interpretability. We present SwarmThinkers, a reinforcement learning framework that recasts atomic-scale simulation as a physically grounded swarm intelligence system. Each diffusing particle is modeled as a local decision-making agent that selects transitions via a shared policy network trained under thermodynamic constraints. A reweighting mechanism fuses learned preferences with transition rates, preserving statistical fidelity while enabling interpretable, step-wise decision making. Training follows a centralized-training, decentralized-execution paradigm, allowing the policy to generalize across system sizes, concentrations, and temperatures without retraining. On a benchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers is the first system to achieve full-scale, physically consistent simulation on a single A100 GPU, previously attainable only via OpenKMC on a supercomputer. It delivers up to 4963x (3185x on average) faster computation with 485x lower memory usage. By treating particles as decision-makers, not passive samplers, SwarmThinkers marks a paradigm shift in scientific simulation--one that unifies physical consistency, interpretability, and scalability through agent-driven intelligence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20094v3",
    "published_date": "2025-05-26 15:04:37 UTC",
    "updated_date": "2025-07-01 04:00:13 UTC"
  },
  {
    "arxiv_id": "2505.20089v3",
    "title": "Homophily Enhanced Graph Domain Adaptation",
    "authors": [
      "Ruiyi Fang",
      "Bingheng Li",
      "Jingyu Zhao",
      "Ruizhi Pu",
      "Qiuhao Zeng",
      "Gezheng Xu",
      "Charles Ling",
      "Boyu Wang"
    ],
    "abstract": "Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs, addressing the challenge of label scarcity. In this paper, we highlight the significance of graph homophily, a pivotal factor for graph domain alignment, which, however, has long been overlooked in existing approaches. Specifically, our analysis first reveals that homophily discrepancies exist in benchmarks. Moreover, we also show that homophily discrepancies degrade GDA performance from both empirical and theoretical aspects, which further underscores the importance of homophily alignment in GDA. Inspired by this finding, we propose a novel homophily alignment algorithm that employs mixed filters to smooth graph signals, thereby effectively capturing and mitigating homophily discrepancies between graphs. Experimental results on a variety of benchmarks verify the effectiveness of our method.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "Accepted at ICML2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20089v3",
    "published_date": "2025-05-26 15:02:08 UTC",
    "updated_date": "2025-05-31 04:25:27 UTC"
  },
  {
    "arxiv_id": "2505.20087v1",
    "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models",
    "authors": [
      "Makesh Narsimhan Sreedhar",
      "Traian Rebedea",
      "Christopher Parisien"
    ],
    "abstract": "Reasoning-based language models have demonstrated strong performance across various domains, with the most notable gains seen in mathematical and coding tasks. Recent research has shown that reasoning also offers significant benefits for LLM safety and guardrail applications. In this work, we conduct a comprehensive analysis of training reasoning-based guardrail models for content moderation, with an emphasis on generalization to custom safety policies at inference time. Our study focuses on two key dimensions: data efficiency and inference efficiency. On the data front, we find that reasoning-based models exhibit strong sample efficiency, achieving competitive performance with significantly fewer training examples than their non-reasoning counterparts. This unlocks the potential to repurpose the remaining data for mining high-value, difficult samples that further enhance model performance. On the inference side, we evaluate practical trade-offs by introducing reasoning budgets, examining the impact of reasoning length on latency and accuracy, and exploring dual-mode training to allow runtime control over reasoning behavior. Our findings will provide practical insights for researchers and developers to effectively and efficiently train and deploy reasoning-based guardrails models in real-world systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20087v1",
    "published_date": "2025-05-26 15:01:37 UTC",
    "updated_date": "2025-05-26 15:01:37 UTC"
  },
  {
    "arxiv_id": "2505.20085v1",
    "title": "Explanation User Interfaces: A Systematic Literature Review",
    "authors": [
      "Eleonora Cappuccio",
      "Andrea Esposito",
      "Francesco Greco",
      "Giuseppe Desolda",
      "Rosa Lanzilotti",
      "Salvatore Rinzivillo"
    ],
    "abstract": "Artificial Intelligence (AI) is one of the major technological advancements of this century, bearing incredible potential for users through AI-powered applications and tools in numerous domains. Being often black-box (i.e., its decision-making process is unintelligible), developers typically resort to eXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour of AI models to produce systems that are transparent, fair, reliable, and trustworthy. However, presenting explanations to the user is not trivial and is often left as a secondary aspect of the system's design process, leading to AI systems that are not useful to end-users. This paper presents a Systematic Literature Review on Explanation User Interfaces (XUIs) to gain a deeper understanding of the solutions and design guidelines employed in the academic literature to effectively present explanations to users. To improve the contribution and real-world impact of this survey, we also present a framework for Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide practitioners and academics in the design and evaluation of XUIs.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "First version",
    "pdf_url": "https://arxiv.org/pdf/2505.20085v1",
    "published_date": "2025-05-26 15:00:17 UTC",
    "updated_date": "2025-05-26 15:00:17 UTC"
  },
  {
    "arxiv_id": "2505.20081v4",
    "title": "Inference-time Alignment in Continuous Space",
    "authors": [
      "Yige Yuan",
      "Teng Xiao",
      "Li Yunfan",
      "Bingbing Xu",
      "Shuchang Tao",
      "Yunqi Qiu",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "abstract": "Aligning large language models with human feedback at inference time has received increasing attention due to its flexibility. Existing methods rely on generating multiple responses from the base policy for search using a reward model, which can be considered as searching in a discrete response space. However, these methods struggle to explore informative candidates when the base policy is weak or the candidate set is small, resulting in limited effectiveness. In this paper, to address this problem, we propose Simple Energy Adaptation ($\\textbf{SEA}$), a simple yet effective algorithm for inference-time alignment. In contrast to expensive search over the discrete space, SEA directly adapts original responses from the base policy toward the optimal one via gradient-based sampling in continuous latent space. Specifically, SEA formulates inference as an iterative optimization procedure on an energy function over actions in the continuous space defined by the optimal policy, enabling simple and effective alignment. For instance, despite its simplicity, SEA outperforms the second-best baseline with a relative improvement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on MATH. Our code is publicly available at https://github.com/yuanyige/sea",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20081v4",
    "published_date": "2025-05-26 14:58:33 UTC",
    "updated_date": "2025-10-24 11:18:04 UTC"
  },
  {
    "arxiv_id": "2505.23797v1",
    "title": "Detection of Suicidal Risk on Social Media: A Hybrid Model",
    "authors": [
      "Zaihan Yang",
      "Ryan Leonard",
      "Hien Tran",
      "Rory Driscoll",
      "Chadbourne Davis"
    ],
    "abstract": "Suicidal thoughts and behaviors are increasingly recognized as a critical societal concern, highlighting the urgent need for effective tools to enable early detection of suicidal risk. In this work, we develop robust machine learning models that leverage Reddit posts to automatically classify them into four distinct levels of suicide risk severity. We frame this as a multi-class classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa), a state-of-the-art deep learning transformer model, with the statistical term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy and reliability of suicide risk assessment. To address data imbalance and overfitting, we explore various data resampling techniques and data augmentation strategies to enhance model generalization. Additionally, we compare our model's performance against that of using RoBERTa only, the BERT model and other traditional machine learning classifiers. Experimental results demonstrate that the hybrid model can achieve improved performance, giving a best weighted $F_{1}$ score of 0.7512.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23797v1",
    "published_date": "2025-05-26 14:56:47 UTC",
    "updated_date": "2025-05-26 14:56:47 UTC"
  },
  {
    "arxiv_id": "2505.20075v1",
    "title": "Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback",
    "authors": [
      "Mengdi Li",
      "Jiaye Lin",
      "Xufeng Zhao",
      "Wenhao Lu",
      "Peilin Zhao",
      "Stefan Wermter",
      "Di Wang"
    ],
    "abstract": "Reward models trained with conventional Reinforcement Learning from AI Feedback (RLAIF) methods suffer from limited generalizability, which hinders the alignment performance of the policy model during reinforcement learning (RL). This challenge stems from various issues, including distribution shift, preference label noise, and mismatches between overly challenging samples and model capacity. In this paper, we attempt to enhance the generalizability of reward models through a data-centric approach, driven by the insight that these issues are inherently intertwined from the perspective of data difficulty. To address this, we propose a novel framework, $\\textit{Curriculum-RLAIF}$, which constructs preference pairs with varying difficulty levels and produces a curriculum that progressively incorporates preference pairs of increasing difficulty for reward model training. Our experimental results suggest that reward models trained with Curriculum-RLAIF achieve improved generalizability, significantly increasing the alignment performance of the policy model by a large margin without incurring additional inference costs compared to various non-curriculum baselines. Detailed analysis and comparisons with alternative approaches, including data selection via external pretrained reward models or internal self-selection mechanisms, as well as other curriculum strategies, further demonstrate the superiority of our approach in terms of simplicity, efficiency, and effectiveness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20075v1",
    "published_date": "2025-05-26 14:53:08 UTC",
    "updated_date": "2025-05-26 14:53:08 UTC"
  },
  {
    "arxiv_id": "2505.20072v2",
    "title": "Incentivizing Strong Reasoning from Weak Supervision",
    "authors": [
      "Yige Yuan",
      "Teng Xiao",
      "Shuchang Tao",
      "Xue Wang",
      "Jinyang Gao",
      "Bolin Ding",
      "Bingbing Xu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive performance on reasoning-intensive tasks, but enhancing their reasoning abilities typically relies on either reinforcement learning (RL) with verifiable signals or supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT) demonstrations, both of which are expensive. In this paper, we study a novel problem of incentivizing the reasoning capacity of LLMs without expensive high-quality demonstrations and reinforcement learning. We investigate whether the reasoning capabilities of LLMs can be effectively incentivized via supervision from significantly weaker models. We further analyze when and why such weak supervision succeeds in eliciting reasoning abilities in stronger models. Our findings show that supervision from significantly weaker reasoners can substantially improve student reasoning performance, recovering close to 94% of the gains of expensive RL at a fraction of the cost. Experiments across diverse benchmarks and model architectures demonstrate that weak reasoners can effectively incentivize reasoning in stronger student models, consistently improving performance across a wide range of reasoning tasks. Our results suggest that this simple weak-to-strong paradigm is a promising and generalizable alternative to costly methods for incentivizing strong reasoning capabilities at inference-time in LLMs. The code is publicly available at https://github.com/yuanyige/w2sr.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20072v2",
    "published_date": "2025-05-26 14:51:29 UTC",
    "updated_date": "2025-05-28 09:07:11 UTC"
  },
  {
    "arxiv_id": "2505.20068v1",
    "title": "On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction",
    "authors": [
      "Qingyu Liang",
      "Jaime Banks"
    ],
    "abstract": "Shared understanding plays a key role in the effective communication in and performance of human-human interactions. With the increasingly common integration of AI into human contexts, the future of personal and workplace interactions will likely see human-AI interaction (HAII) in which the perception of shared understanding is important. Existing literature has addressed the processes and effects of PSU in human-human interactions, but the construal remains underexplored in HAII. To better understand PSU in HAII, we conducted an online survey to collect user reflections on interactions with a large language model when it sunderstanding of a situation was thought to be similar to or different from the participant's. Through inductive thematic analysis, we identified eight dimensions comprising PSU in human-AI interactions: Fluency, aligned operation, fluidity, outcome satisfaction, contextual awareness, lack of humanlike abilities, computational limits, and suspicion.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20068v1",
    "published_date": "2025-05-26 14:50:40 UTC",
    "updated_date": "2025-05-26 14:50:40 UTC"
  },
  {
    "arxiv_id": "2505.20067v1",
    "title": "Community Moderation and the New Epistemology of Fact Checking on Social Media",
    "authors": [
      "Isabelle Augenstein",
      "Michiel Bakker",
      "Tanmoy Chakraborty",
      "David Corney",
      "Emilio Ferrara",
      "Iryna Gurevych",
      "Scott Hale",
      "Eduard Hovy",
      "Heng Ji",
      "Irene Larraz",
      "Filippo Menczer",
      "Preslav Nakov",
      "Paolo Papotti",
      "Dhruv Sahnan",
      "Greta Warren",
      "Giovanni Zagni"
    ],
    "abstract": "Social media platforms have traditionally relied on internal moderation teams and partnerships with independent fact-checking organizations to identify and flag misleading content. Recently, however, platforms including X (formerly Twitter) and Meta have shifted towards community-driven content moderation by launching their own versions of crowd-sourced fact-checking -- Community Notes. If effectively scaled and governed, such crowd-checking initiatives have the potential to combat misinformation with increased scale and speed as successfully as community-driven efforts once did with spam. Nevertheless, general content moderation, especially for misinformation, is inherently more complex. Public perceptions of truth are often shaped by personal biases, political leanings, and cultural contexts, complicating consensus on what constitutes misleading content. This suggests that community efforts, while valuable, cannot replace the indispensable role of professional fact-checkers. Here we systemically examine the current approaches to misinformation detection across major platforms, explore the emerging role of community-driven moderation, and critically evaluate both the promises and challenges of crowd-checking at scale.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.SI",
    "comment": "1 Figure, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.20067v1",
    "published_date": "2025-05-26 14:50:18 UTC",
    "updated_date": "2025-05-26 14:50:18 UTC"
  },
  {
    "arxiv_id": "2505.20066v1",
    "title": "Automated data curation for self-supervised learning in underwater acoustic analysis",
    "authors": [
      "Hilde I Hummel",
      "Sandjai Bhulai",
      "Burooj Ghani",
      "Rob van der Mei"
    ],
    "abstract": "The sustainability of the ocean ecosystem is threatened by increased levels of sound pollution, making monitoring crucial to understand its variability and impact. Passive acoustic monitoring (PAM) systems collect a large amount of underwater sound recordings, but the large volume of data makes manual analysis impossible, creating the need for automation. Although machine learning offers a potential solution, most underwater acoustic recordings are unlabeled. Self-supervised learning models have demonstrated success in learning from large-scale unlabeled data in various domains like computer vision, Natural Language Processing, and audio. However, these models require large, diverse, and balanced datasets for training in order to generalize well. To address this, a fully automated self-supervised data curation pipeline is proposed to create a diverse and balanced dataset from raw PAM data. It integrates Automatic Identification System (AIS) data with recordings from various hydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw audio data is sampled and then combined with AIS samples to create a balanced and diverse dataset. The resulting curated dataset enables the development of self-supervised learning models, facilitating various tasks such as monitoring marine mammals and assessing sound pollution.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20066v1",
    "published_date": "2025-05-26 14:50:04 UTC",
    "updated_date": "2025-05-26 14:50:04 UTC"
  },
  {
    "arxiv_id": "2505.20065v1",
    "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety",
    "authors": [
      "Geon-Hyeong Kim",
      "Youngsoo Jang",
      "Yu Jin Kim",
      "Byoungjip Kim",
      "Honglak Lee",
      "Kyunghoon Bae",
      "Moontae Lee"
    ],
    "abstract": "As Large Language Models (LLMs) continue to advance and find applications across a growing number of fields, ensuring the safety of LLMs has become increasingly critical. To address safety concerns, recent studies have proposed integrating safety constraints into Reinforcement Learning from Human Feedback (RLHF). However, these approaches tend to be complex, as they encompass complicated procedures in RLHF along with additional steps required by the safety constraints. Inspired by Direct Preference Optimization (DPO), we introduce a new algorithm called SafeDPO, which is designed to directly optimize the safety alignment objective in a single stage of policy learning, without requiring relaxation. SafeDPO introduces only one additional hyperparameter to further enhance safety and requires only minor modifications to standard DPO. As a result, it eliminates the need to fit separate reward and cost models or to sample from the language model during fine-tuning, while still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO achieves competitive performance compared to state-of-the-art safety alignment algorithms, both in terms of aligning with human preferences and improving safety.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "34 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.20065v1",
    "published_date": "2025-05-26 14:50:01 UTC",
    "updated_date": "2025-05-26 14:50:01 UTC"
  },
  {
    "arxiv_id": "2505.20063v2",
    "title": "SAEs Are Good for Steering -- If You Select the Right Features",
    "authors": [
      "Dana Arad",
      "Aaron Mueller",
      "Yonatan Belinkov"
    ],
    "abstract": "Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to learn a decomposition of a model's latent space. This enables useful applications such as steering - influencing the output of a model towards a desired concept - without requiring labeled data. Current methods identify SAE features to steer by analyzing the input tokens that activate them. However, recent work has highlighted that activations alone do not fully describe the effect of a feature on the model's output. In this work, we draw a distinction between two types of features: input features, which mainly capture patterns in the model's input, and output features, which have a human-understandable effect on the model's output. We propose input and output scores to characterize and locate these types of features, and show that high values for both scores rarely co-occur in the same features. These findings have practical implications: after filtering out features with low output scores, we obtain 2-3x improvements when steering with SAEs, making them competitive with supervised methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20063v2",
    "published_date": "2025-05-26 14:47:59 UTC",
    "updated_date": "2025-12-22 15:49:59 UTC"
  },
  {
    "arxiv_id": "2506.06313v4",
    "title": "Beyond Chunking: Discourse-Aware Hierarchical Retrieval for Long Document Question Answering",
    "authors": [
      "Huiyao Chen",
      "Yi Yang",
      "Yinghui Li",
      "Meishan Zhang",
      "Baotian Hu",
      "Min Zhang"
    ],
    "abstract": "Existing long-document question answering systems typically process texts as flat sequences or use heuristic chunking, which overlook the discourse structures that naturally guide human comprehension. We present a discourse-aware hierarchical framework that leverages rhetorical structure theory (RST) for long document question answering. Our approach converts discourse trees into sentence-level representations and employs LLM-enhanced node representations to bridge structural and semantic information. The framework involves three key innovations: language-universal discourse parsing for lengthy documents, LLM-based enhancement of discourse relation nodes, and structure-guided hierarchical retrieval. Extensive experiments on four datasets demonstrate consistent improvements over existing approaches through the incorporation of discourse structure, across multiple genres and languages. Moreover, the proposed framework exhibits strong robustness across diverse document types and linguistic settings.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "21 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.06313v4",
    "published_date": "2025-05-26 14:45:12 UTC",
    "updated_date": "2026-01-14 15:49:23 UTC"
  },
  {
    "arxiv_id": "2505.20053v1",
    "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion",
    "authors": [
      "Zheqi Lv",
      "Junhao Chen",
      "Qi Tian",
      "Keting Yin",
      "Shengyu Zhang",
      "Fei Wu"
    ],
    "abstract": "Diffusion models have become the mainstream architecture for text-to-image generation, achieving remarkable progress in visual quality and prompt controllability. However, current inference pipelines generally lack interpretable semantic supervision and correction mechanisms throughout the denoising process. Most existing approaches rely solely on post-hoc scoring of the final image, prompt filtering, or heuristic resampling strategies-making them ineffective in providing actionable guidance for correcting the generative trajectory. As a result, models often suffer from object confusion, spatial errors, inaccurate counts, and missing semantic elements, severely compromising prompt-image alignment and image quality. To tackle these challenges, we propose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel framework that, for the first time, introduces a Multimodal Large Language Model (MLLM) as a semantic observer during inference. PPAD performs real-time analysis on intermediate generations, identifies latent semantic inconsistencies, and translates feedback into controllable signals that actively guide the remaining denoising steps. The framework supports both inference-only and training-enhanced settings, and performs semantic correction at only extremely few diffusion steps, offering strong generality and scalability. Extensive experiments demonstrate PPAD's significant improvements.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20053v1",
    "published_date": "2025-05-26 14:42:35 UTC",
    "updated_date": "2025-05-26 14:42:35 UTC"
  },
  {
    "arxiv_id": "2505.20047v1",
    "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks",
    "authors": [
      "Debargha Ganguly",
      "Vikash Singh",
      "Sreehari Sankar",
      "Biyao Zhang",
      "Xuecen Zhang",
      "Srinivasan Iyengar",
      "Xiaotian Han",
      "Amit Sharma",
      "Shivkumar Kalyanaraman",
      "Vipin Chaudhary"
    ],
    "abstract": "Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LO",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20047v1",
    "published_date": "2025-05-26 14:34:04 UTC",
    "updated_date": "2025-05-26 14:34:04 UTC"
  },
  {
    "arxiv_id": "2506.15689v2",
    "title": "BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models",
    "authors": [
      "Liulu He",
      "Shenli Zheng",
      "Karwei Sun",
      "Yijiang Liu",
      "Yufei Zhao",
      "Chongkang Tan",
      "Huanrui Yang",
      "Yuan Du",
      "Li Du"
    ],
    "abstract": "Rotations have become essential to state-of-the-art quantization pipelines for large language models (LLMs) by effectively smoothing outliers in weights and activations. However, further optimizing the rotation parameters offers only limited performance gains and introduces significant training overhead: due to rotation parameter sharing, full-model must be loaded simultaneously to enable backpropagation, resulting in substantial memory consumption and limited practical utility. In this work, we identify two fundamental limitations of current rotational quantization methods: (i) rotation fails to align channel means, resulting in wider quantization bounds and increased rounding errors; and (ii) rotation makes the activation distribution more Gaussian-like, increasing energy loss caused by clipping errors. To address these issues, we introduce \\textbf{BASE-Q}, a simple yet powerful approach that combines bias correction and asymmetric scaling to effectively reduce rounding and clipping errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the need for memory-intensive full-model backpropagation. Extensive experiments on various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing the accuracy gap to full-precision models by 50.5\\%, 42.9\\%, and 29.2\\% compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be released soon.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15689v2",
    "published_date": "2025-05-26 14:22:21 UTC",
    "updated_date": "2025-08-29 12:03:45 UTC"
  },
  {
    "arxiv_id": "2505.20033v2",
    "title": "EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition",
    "authors": [
      "Christoph Schuhmann",
      "Robert Kaczmarczyk",
      "Gollam Rabby",
      "Felix Friedrich",
      "Maurice Kraus",
      "Krishna Kalyan",
      "Kourosh Nadi",
      "Huu Nguyen",
      "Kristian Kersting",
      "Sören Auer"
    ],
    "abstract": "Effective human-AI interaction relies on AI's ability to accurately perceive and interpret human emotions. Current benchmarks for vision and vision-language models are severely limited, offering a narrow emotional spectrum that overlooks nuanced states (e.g., bitterness, intoxication) and fails to distinguish subtle differences between related feelings (e.g., shame vs. embarrassment). Existing datasets also often use uncontrolled imagery with occluded faces and lack demographic diversity, risking significant bias. To address these critical gaps, we introduce EmoNet Face, a comprehensive benchmark suite. EmoNet Face features: (1) A novel 40-category emotion taxonomy, meticulously derived from foundational research to capture finer details of human emotional experiences. (2) Three large-scale, AI-generated datasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and controlled demographic balance across ethnicity, age, and gender. (3) Rigorous, multi-expert annotations for training and high-fidelity evaluation. (4) We built EmpathicInsight-Face, a model achieving human-expert-level performance on our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets, and model - provides a robust foundation for developing and evaluating AI systems with a deeper understanding of human emotions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20033v2",
    "published_date": "2025-05-26 14:19:58 UTC",
    "updated_date": "2025-05-27 07:26:21 UTC"
  },
  {
    "arxiv_id": "2505.20030v1",
    "title": "Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions",
    "authors": [
      "Wenbo Wei",
      "Nicholas Chong Jia Le",
      "Choy Heng Lai",
      "Ling Feng"
    ],
    "abstract": "We observe a novel 'multiple-descent' phenomenon during the training process of LSTM, in which the test loss goes through long cycles of up and down trend multiple times after the model is overtrained. By carrying out asymptotic stability analysis of the models, we found that the cycles in test loss are closely associated with the phase transition process between order and chaos, and the local optimal epochs are consistently at the critical transition point between the two phases. More importantly, the global optimal epoch occurs at the first transition from order to chaos, where the 'width' of the 'edge of chaos' is the widest, allowing the best exploration of better weight configurations for learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "nlin.CD",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20030v1",
    "published_date": "2025-05-26 14:18:22 UTC",
    "updated_date": "2025-05-26 14:18:22 UTC"
  },
  {
    "arxiv_id": "2505.20029v1",
    "title": "Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)",
    "authors": [
      "Subba Reddy Oota",
      "Akshett Jindal",
      "Ishani Mondal",
      "Khushbu Pahwa",
      "Satya Sai Srinath Namburi",
      "Manish Shrivastava",
      "Maneesh Singh",
      "Bapi S. Raju",
      "Manish Gupta"
    ],
    "abstract": "Transformer-based language models, though not explicitly trained to mimic brain recordings, have demonstrated surprising alignment with brain activity. Progress in these models-through increased size, instruction-tuning, and multimodality-has led to better representational alignment with neural data. Recently, a new class of instruction-tuned multimodal LLMs (MLLMs) have emerged, showing remarkable zero-shot capabilities in open-ended multimodal vision tasks. However, it is unknown whether MLLMs, when prompted with natural instructions, lead to better brain alignment and effectively capture instruction-specific representations. To address this, we first investigate brain alignment, i.e., measuring the degree of predictivity of neural visual activity using text output response embeddings from MLLMs as participants engage in watching natural scenes. Experiments with 10 different instructions show that MLLMs exhibit significantly better brain alignment than vision-only models and perform comparably to non-instruction-tuned multimodal models like CLIP. We also find that while these MLLMs are effective at generating high-quality responses suitable to the task-specific instructions, not all instructions are relevant for brain alignment. Further, by varying instructions, we make the MLLMs encode instruction-specific visual concepts related to the input image. This analysis shows that MLLMs effectively capture count-related and recognition-related concepts, demonstrating strong alignment with brain activity. Notably, the majority of the explained variance of the brain encoding models is shared between MLLM embeddings of image captioning and other instructions. These results suggest that enhancing MLLMs' ability to capture task-specific information could lead to better differentiation between various types of instructions, and thereby improving their precision in predicting brain responses.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "30 pages, 22 figures, The Thirteenth International Conference on Learning Representations, ICLR-2025, Singapore. https://openreview.net/pdf?id=xkgfLXZ4e0",
    "pdf_url": "https://arxiv.org/pdf/2505.20029v1",
    "published_date": "2025-05-26 14:18:15 UTC",
    "updated_date": "2025-05-26 14:18:15 UTC"
  },
  {
    "arxiv_id": "2505.20027v1",
    "title": "Multi-modal brain encoding models for multi-modal stimuli",
    "authors": [
      "Subba Reddy Oota",
      "Khushbu Pahwa",
      "Mounika Marreddy",
      "Maneesh Singh",
      "Manish Gupta",
      "Bapi S. Raju"
    ],
    "abstract": "Despite participants engaging in unimodal stimuli, such as watching images or silent videos, recent work has demonstrated that multi-modal Transformer models can predict visual brain activity impressively well, even with incongruent modality representations. This raises the question of how accurately these multi-modal models can predict brain activity when participants are engaged in multi-modal stimuli. As these models grow increasingly popular, their use in studying neural activity provides insights into how our brains respond to such multi-modal naturalistic stimuli, i.e., where it separates and integrates information across modalities through a hierarchy of early sensory regions to higher cognition. We investigate this question by using multiple unimodal and two types of multi-modal models-cross-modal and jointly pretrained-to determine which type of model is more relevant to fMRI brain activity when participants are engaged in watching movies. We observe that both types of multi-modal models show improved alignment in several language and visual regions. This study also helps in identifying which brain regions process unimodal versus multi-modal information. We further investigate the contribution of each modality to multi-modal alignment by carefully removing unimodal features one by one from multi-modal representations, and find that there is additional information beyond the unimodal embeddings that is processed in the visual and language regions. Based on this investigation, we find that while for cross-modal models, their brain alignment is partially attributed to the video modality; for jointly pretrained models, it is partially attributed to both the video and audio modalities. This serves as a strong motivation for the neuroscience community to investigate the interpretability of these models for deepening our understanding of multi-modal information processing in brain.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS",
      "eess.IV"
    ],
    "primary_category": "q-bio.NC",
    "comment": "26 pages, 15 figures, The Thirteenth International Conference on Learning Representations, ICLR-2025, Singapore. https://openreview.net/pdf?id=0dELcFHig2",
    "pdf_url": "https://arxiv.org/pdf/2505.20027v1",
    "published_date": "2025-05-26 14:17:08 UTC",
    "updated_date": "2025-05-26 14:17:08 UTC"
  },
  {
    "arxiv_id": "2505.20026v1",
    "title": "Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage",
    "authors": [
      "Xinping Chen",
      "Chen Liu"
    ],
    "abstract": "We propose Gradient Inversion Transcript (GIT), a novel generative approach for reconstructing training data from leaked gradients. GIT employs a generative attack model, whose architecture is tailored to align with the structure of the leaked model based on theoretical analysis. Once trained offline, GIT can be deployed efficiently and only relies on the leaked gradients to reconstruct the input data, rendering it applicable under various distributed learning environments. When used as a prior for other iterative optimization-based methods, GIT not only accelerates convergence but also enhances the overall reconstruction quality. GIT consistently outperforms existing methods across multiple datasets and demonstrates strong robustness under challenging conditions, including inaccurate gradients, data distribution shifts and discrepancies in model parameters.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20026v1",
    "published_date": "2025-05-26 14:17:00 UTC",
    "updated_date": "2025-05-26 14:17:00 UTC"
  },
  {
    "arxiv_id": "2505.20024v2",
    "title": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving",
    "authors": [
      "Xueyi Liu",
      "Zuodong Zhong",
      "Yuxin Guo",
      "Yun-Fu Liu",
      "Zhiguo Su",
      "Qichao Zhang",
      "Junli Wang",
      "Yinfeng Gao",
      "Yupeng Zheng",
      "Qiao Lin",
      "Huiyong Chen",
      "Dongbin Zhao"
    ],
    "abstract": "Due to the powerful vision-language reasoning and generalization abilities, multimodal large language models (MLLMs) have garnered significant attention in the field of end-to-end (E2E) autonomous driving. However, their application to closed-loop systems remains underexplored, and current MLLM-based methods have not shown clear superiority to mainstream E2E imitation learning approaches. In this work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed for closed-loop driving through holistic reasoning with a self-supervised Next Scene Prediction task and supervised Decision Chain-of-Thought process. This dual mechanism encourages the model to align visual representations with actionable driving context, while promoting interpretable and causally grounded decision making. We curate a planning-oriented decision reasoning dataset, namely PDR, comprising 210k diverse and high-quality samples. Our method outperforms the mainstream E2E imitation learning method by a large margin of 19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan demonstrates strong zero-shot generalization on unseen DOS benchmark, highlighting its adaptability in handling zero-shot corner cases. Code and dataset will be found in https://github.com/Liuxueyi/ReasonPlan.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages; 9 figures; https://github.com/Liuxueyi/ReasonPlan",
    "pdf_url": "https://arxiv.org/pdf/2505.20024v2",
    "published_date": "2025-05-26 14:12:38 UTC",
    "updated_date": "2025-09-22 07:21:16 UTC"
  },
  {
    "arxiv_id": "2505.20021v1",
    "title": "Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models",
    "authors": [
      "Hyunsik Chae",
      "Seungwoo Yoon",
      "Jaden Park",
      "Chloe Yewon Chun",
      "Yongin Cho",
      "Mu Cai",
      "Yong Jae Lee",
      "Ernest K. Ryu"
    ],
    "abstract": "Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal comprehension and reasoning capabilities, yet they often struggle with trivially simple visual tasks. In this work, we focus on the domain of basic 2D Euclidean geometry and systematically categorize the fundamental, indivisible visual perception skills, which we refer to as atomic visual skills. We then introduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the atomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find that they struggle with these tasks, despite being trivial for adult humans. Our findings highlight the need for purpose-built datasets to train and evaluate VLMs on atomic, rather than composite, visual perception tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "69 pages, 16 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20021v1",
    "published_date": "2025-05-26 14:09:24 UTC",
    "updated_date": "2025-05-26 14:09:24 UTC"
  },
  {
    "arxiv_id": "2505.20011v3",
    "title": "The Many Challenges of Human-Like Agents in Virtual Game Environments",
    "authors": [
      "Maciej Swiechowski",
      "Dominik Slezak"
    ],
    "abstract": "Human-like agents are an increasingly important topic in games and beyond. Believable non-player characters enhance the gaming experience by improving immersion and providing entertainment. They also offer players the opportunity to engage with AI entities that can function as opponents, teachers, or cooperating partners. Additionally, in games where bots are prohibited -- and even more so in non-game environments -- there is a need for methods capable of identifying whether digital interactions occur with bots or humans. This leads to two fundamental research questions: (1) how to model and implement human-like AI, and (2) how to measure its degree of human likeness.\n  This article offers two contributions. The first one is a survey of the most significant challenges in implementing human-like AI in games (or any virtual environment featuring simulated agents, although this article specifically focuses on games). Thirteen such challenges, both conceptual and technical, are discussed in detail. The second is an empirical study performed in a tactical video game that addresses the research question: \"Is it possible to distinguish human players from bots (AI agents) based on empirical data?\" A machine-learning approach using a custom deep recurrent convolutional neural network is presented. We hypothesize that the more challenging it is to create human-like AI for a given game, the easier it becomes to develop a method for distinguishing humans from AI-driven players.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "In proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS-2025), pages 1996--2005, May 19-23, Detroit, Michigan, USA",
    "pdf_url": "https://arxiv.org/pdf/2505.20011v3",
    "published_date": "2025-05-26 14:00:39 UTC",
    "updated_date": "2025-06-09 20:57:51 UTC"
  },
  {
    "arxiv_id": "2505.19983v1",
    "title": "ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications",
    "authors": [
      "Tong Wu",
      "Zhiyong Chen",
      "Dazhi He",
      "Feng Yang",
      "Meixia Tao",
      "Xiaodong Xu",
      "Wenjun Zhang",
      "Ping Zhang"
    ],
    "abstract": "Diffusion models (DMs) have recently achieved significant success in wireless communications systems due to their denoising capabilities. The broadcast nature of wireless signals makes them susceptible not only to Gaussian noise, but also to unaware interference. This raises the question of whether DMs can effectively mitigate interference in wireless semantic communication systems. In this paper, we model the interference cancellation problem as a maximum a posteriori (MAP) problem over the joint posterior probability of the signal and interference, and theoretically prove that the solution provides excellent estimates for the signal and interference. To solve this problem, we develop an interference cancellation diffusion model (ICDM), which decomposes the joint posterior into independent prior probabilities of the signal and interference, along with the channel transition probablity. The log-gradients of these distributions at each time step are learned separately by DMs and accurately estimated through deriving. ICDM further integrates these gradients with advanced numerical iteration method, achieving accurate and rapid interference cancellation. Extensive experiments demonstrate that ICDM significantly reduces the mean square error (MSE) and enhances perceptual quality compared to schemes without ICDM. For example, on the CelebA dataset under the Rayleigh fading channel with a signal-to-noise ratio (SNR) of $20$ dB and signal to interference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB and improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IT",
    "comment": "submitted to IEEE journal",
    "pdf_url": "https://arxiv.org/pdf/2505.19983v1",
    "published_date": "2025-05-26 13:41:52 UTC",
    "updated_date": "2025-05-26 13:41:52 UTC"
  },
  {
    "arxiv_id": "2505.19973v1",
    "title": "DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response",
    "authors": [
      "Bilel Cherif",
      "Tamas Bisztray",
      "Richard A. Dubniczky",
      "Aaesha Aldahmani",
      "Saeed Alshehhi",
      "Norbert Tihanyi"
    ],
    "abstract": "Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19973v1",
    "published_date": "2025-05-26 13:35:37 UTC",
    "updated_date": "2025-05-26 13:35:37 UTC"
  },
  {
    "arxiv_id": "2505.19966v1",
    "title": "Learning to Select In-Context Demonstration Preferred by Large Language Model",
    "authors": [
      "Zheng Zhang",
      "Shaocheng Lan",
      "Lei Song",
      "Jiang Bian",
      "Yexin Li",
      "Kan Ren"
    ],
    "abstract": "In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks during inference using only a few demonstrations. However, ICL performance is highly dependent on the selection of these demonstrations. Recent work explores retrieval-based methods for selecting query-specific demonstrations, but these approaches often rely on surrogate objectives such as metric learning, failing to directly optimize ICL performance. Consequently, they struggle to identify truly beneficial demonstrations. Moreover, their discriminative retrieval paradigm is ineffective when the candidate pool lacks sufficient high-quality demonstrations. To address these challenges, we propose GenICL, a novel generative preference learning framework that leverages LLM feedback to directly optimize demonstration selection for ICL. Experiments on 19 datasets across 11 task categories demonstrate that GenICL achieves superior performance than existing methods in selecting the most effective demonstrations, leading to better ICL performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19966v1",
    "published_date": "2025-05-26 13:26:56 UTC",
    "updated_date": "2025-05-26 13:26:56 UTC"
  },
  {
    "arxiv_id": "2505.19965v1",
    "title": "Adaptive Location Hierarchy Learning for Long-Tailed Mobility Prediction",
    "authors": [
      "Yu Wang",
      "Junshu Dai",
      "Yuchen Ying",
      "Yuxuan Liang",
      "Tongya Zheng",
      "Mingli Song"
    ],
    "abstract": "Human mobility prediction is crucial for applications ranging from location-based recommendations to urban planning, which aims to forecast users' next location visits based on historical trajectories. Despite the severe long-tailed distribution of locations, the problem of long-tailed mobility prediction remains largely underexplored. Existing long-tailed learning methods primarily focus on rebalancing the skewed distribution at the data, model, or class level, neglecting to exploit the spatiotemporal semantics of locations. To address this gap, we propose the first plug-and-play framework for long-tailed mobility prediction in an exploitation and exploration manner, named \\textbf{A}daptive \\textbf{LO}cation \\textbf{H}ier\\textbf{A}rchy learning (ALOHA). First, we construct city-tailored location hierarchy based on Large Language Models (LLMs) by exploiting Maslow's theory of human motivation to design Chain-of-Thought (CoT) prompts that captures spatiotemporal semantics. Second, we optimize the location hierarchy predictions by Gumbel disturbance and node-wise adaptive weights within the hierarchical tree structure. Experiments on state-of-the-art models across six datasets demonstrate the framework's consistent effectiveness and generalizability, which strikes a well balance between head and tail locations. Weight analysis and ablation studies reveal the optimization differences of each component for head and tail locations. Furthermore, in-depth analyses of hierarchical distance and case study demonstrate the effective semantic guidance from the location hierarchy. Our code will be made publicly available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19965v1",
    "published_date": "2025-05-26 13:26:35 UTC",
    "updated_date": "2025-05-26 13:26:35 UTC"
  },
  {
    "arxiv_id": "2505.19964v1",
    "title": "The Limits of Preference Data for Post-Training",
    "authors": [
      "Eric Zhao",
      "Jessica Dai",
      "Pranjal Awasthi"
    ],
    "abstract": "Recent progress in strengthening the capabilities of large language models has stemmed from applying reinforcement learning to domains with automatically verifiable outcomes. A key question is whether we can similarly use RL to optimize for outcomes in domains where evaluating outcomes inherently requires human feedback; for example, in tasks like deep research and trip planning, outcome evaluation is qualitative and there are many possible degrees of success. One attractive and scalable modality for collecting human feedback is preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$ given outcomes, which one is preferred. In this work, we study a critical roadblock: preference data fundamentally and significantly limits outcome-based optimization. Even with idealized preference data (infinite, noiseless, and online), the use of ordinal feedback can prevent obtaining even approximately optimal solutions. We formalize this impossibility using voting theory, drawing an analogy between how a model chooses to answer a query with how voters choose a candidate to elect. This indicates that grounded human scoring and algorithmic innovations are necessary for extending the success of RL post-training to domains demanding human feedback. We also explore why these limitations have disproportionately impacted RLHF when it comes to eliciting reasoning behaviors (e.g., backtracking) versus situations where RLHF has been historically successful (e.g., instruction-tuning and safety training), finding that the limitations of preference data primarily suppress RLHF's ability to elicit robust strategies -- a class that encompasses most reasoning behaviors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19964v1",
    "published_date": "2025-05-26 13:26:15 UTC",
    "updated_date": "2025-05-26 13:26:15 UTC"
  },
  {
    "arxiv_id": "2505.19956v2",
    "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph",
    "authors": [
      "Jihyung Lee",
      "Jin-Seop Lee",
      "Jaehoon Lee",
      "YunSeok Choi",
      "Jee-Hyong Lee"
    ],
    "abstract": "Text-to-SQL, which translates a natural language question into an SQL query, has advanced with in-context learning of Large Language Models (LLMs). However, existing methods show little improvement in performance compared to randomly chosen demonstrations, and significant performance drops when smaller LLMs (e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely on the intrinsic capabilities of hyper-scaled LLMs, rather than effectively retrieving useful demonstrations. In this paper, we propose a novel approach for effectively retrieving demonstrations and generating SQL queries. We construct a Deep Contextual Schema Link Graph, which contains key information and semantic relationship between a question and its database schema items. This graph-based structure enables effective representation of Text-to-SQL samples and retrieval of useful demonstrations for in-context learning. Experimental results on the Spider benchmark demonstrate the effectiveness of our approach, showing consistent improvements in SQL generation performance and efficiency across both hyper-scaled LLMs and small LLMs. The code is available at https://github.com/jjklle/DCG-SQL}{https://github.com/jjklle/DCG-SQL.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19956v2",
    "published_date": "2025-05-26 13:19:10 UTC",
    "updated_date": "2025-07-22 08:42:57 UTC"
  },
  {
    "arxiv_id": "2505.19955v3",
    "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
    "authors": [
      "Hui Chen",
      "Miao Xiong",
      "Yujie Lu",
      "Wei Han",
      "Ailin Deng",
      "Yufei He",
      "Jiaying Wu",
      "Yibo Li",
      "Yue Liu",
      "Bryan Hooi"
    ],
    "abstract": "Recent advancements in AI agents have demonstrated their growing potential to drive and support scientific discovery. In this work, we introduce MLR-Bench, a comprehensive benchmark for evaluating AI agents on open-ended machine learning research. MLR-Bench includes three key components: (1) 201 research tasks sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2) MLR-Judge, an automated evaluation framework combining LLM-based reviewers with carefully designed review rubrics to assess research quality; and (3) MLR-Agent, a modular agent scaffold capable of completing research tasks through four stages: idea generation, proposal formulation, experimentation, and paper writing. Our framework supports both stepwise assessment across these distinct research stages, and end-to-end evaluation of the final research paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced coding agent, finding that while LLMs are effective at generating coherent ideas and well-structured papers, current coding agents frequently (e.g., in 80% of the cases) produce fabricated or invalidated experimental results--posing a major barrier to scientific reliability. We validate MLR-Judge through human evaluation, showing high agreement with expert reviewers, supporting its potential as a scalable tool for research evaluation. We open-source MLR-Bench to help the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "49 pages, 9 figures. Accepted by NeurIPS 2025 D&B Track",
    "pdf_url": "https://arxiv.org/pdf/2505.19955v3",
    "published_date": "2025-05-26 13:18:37 UTC",
    "updated_date": "2025-10-22 13:33:52 UTC"
  },
  {
    "arxiv_id": "2505.19951v1",
    "title": "Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy",
    "authors": [
      "Elvir Karimov",
      "Alexander Varlamov",
      "Danil Ivanov",
      "Dmitrii Korzh",
      "Oleg Y. Rogov"
    ],
    "abstract": "Deep learning voice models are commonly used nowadays, but the safety processing of personal data, such as human identity and speech content, remains suspicious. To prevent malicious user identification, speaker anonymization methods were proposed. Current methods, particularly based on universal adversarial patch (UAP) applications, have drawbacks such as significant degradation of audio quality, decreased speech recognition quality, low transferability across different voice biometrics models, and performance dependence on the input audio length. To mitigate these drawbacks, in this work, we introduce and leverage the novel Exponential Total Variance (TV) loss function and provide experimental evidence that it positively affects UAP strength and imperceptibility. Moreover, we present a novel scalable UAP insertion procedure and demonstrate its uniformly high performance for various audio lengths.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 3 figures, 1 table; Submitted to Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19951v1",
    "published_date": "2025-05-26 13:16:01 UTC",
    "updated_date": "2025-05-26 13:16:01 UTC"
  },
  {
    "arxiv_id": "2505.19948v1",
    "title": "SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection",
    "authors": [
      "Gokul Adethya",
      "Bhanu Pratyush Mantha",
      "Tianyang Wang",
      "Xingjian Li",
      "Min Xu"
    ],
    "abstract": "Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for imaging macromolecular complexes in their near-native states. However, the localization of 3D particles in cellular environments still presents a significant challenge due to low signal-to-noise ratios and missing wedge artifacts. Deep learning approaches have shown great potential, but they need huge amounts of data, which can be a challenge in cryo-ET scenarios where labeled data is often scarce. In this paper, we propose a novel Self-augmented and Self-interpreted (SaSi) deep learning approach towards few-shot particle detection in 3D cryo-ET images. Our method builds upon self-augmentation techniques to further boost data utilization and introduces a self-interpreted segmentation strategy for alleviating dependency on labeled data, hence improving generalization and robustness. As demonstrated by experiments conducted on both simulated and real-world cryo-ET datasets, the SaSi approach significantly outperforms existing state-of-the-art methods for particle localization. This research increases understanding of how to detect particles with very few labels in cryo-ET and thus sets a new benchmark for few-shot learning in structural biology.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19948v1",
    "published_date": "2025-05-26 13:14:21 UTC",
    "updated_date": "2025-05-26 13:14:21 UTC"
  },
  {
    "arxiv_id": "2505.19947v3",
    "title": "MESS+: Dynamically Learned Inference-Time LLM Routing in Model Zoos with Service Level Guarantees",
    "authors": [
      "Herbert Woisetschläger",
      "Ryan Zhang",
      "Shiqiang Wang",
      "Hans-Arno Jacobsen"
    ],
    "abstract": "Open-weight large language model (LLM) zoos provide access to numerous high-quality models, but selecting the appropriate model for specific tasks remains challenging and requires technical expertise. Most users simply want factually correct, safe, and satisfying responses without concerning themselves with model technicalities, while inference service providers prioritize minimizing operating costs. These competing interests are typically mediated through service level agreements (SLAs) that guarantee minimum service quality. We introduce MESS+, a stochastic optimization algorithm for cost-optimal LLM request routing while providing rigorous SLA compliance guarantees. MESS+ learns request satisfaction probabilities of LLMs in real-time as users interact with the system, based on which model selection decisions are made by solving a per-request optimization problem. Our algorithm includes a novel combination of virtual queues and request satisfaction prediction, along with a theoretical analysis of cost optimality and constraint satisfaction. Across a wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of $2\\times$ cost savings compared to existing LLM routing techniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025. Code: https://github.com/laminair/mess-plus",
    "pdf_url": "https://arxiv.org/pdf/2505.19947v3",
    "published_date": "2025-05-26 13:11:08 UTC",
    "updated_date": "2025-10-23 20:40:17 UTC"
  },
  {
    "arxiv_id": "2505.19944v1",
    "title": "Can Visual Encoder Learn to See Arrows?",
    "authors": [
      "Naoyuki Terashita",
      "Yusuke Tozaki",
      "Hideaki Omote",
      "Congkha Nguyen",
      "Ryosuke Nakamoto",
      "Yuta Koreeda",
      "Hiroaki Ozaki"
    ],
    "abstract": "The diagram is a visual representation of a relationship illustrated with edges (lines or arrows), which is widely used in industrial and scientific communication. Although recognizing diagrams is essential for vision language models (VLMs) to comprehend domain-specific knowledge, recent studies reveal that many VLMs fail to identify edges in images. We hypothesize that these failures stem from an over-reliance on textual and positional biases, preventing VLMs from learning explicit edge features. Based on this idea, we empirically investigate whether the image encoder in VLMs can learn edge representation through training on a diagram dataset in which edges are biased neither by textual nor positional information. To this end, we conduct contrastive learning on an artificially generated diagram--caption dataset to train an image encoder and evaluate its diagram-related features on three tasks: probing, image retrieval, and captioning. Our results show that the finetuned model outperforms pretrained CLIP in all tasks and surpasses zero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings confirm that eliminating textual and positional biases fosters accurate edge recognition in VLMs, offering a promising path for advancing diagram understanding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been accepted for poster presentation at the Second Workshop on Visual Concepts in CVPR 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19944v1",
    "published_date": "2025-05-26 13:09:31 UTC",
    "updated_date": "2025-05-26 13:09:31 UTC"
  },
  {
    "arxiv_id": "2505.19933v1",
    "title": "Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making",
    "authors": [
      "Yejin Son",
      "Minseo Kim",
      "Sungwoong Kim",
      "Seungju Han",
      "Jian Kim",
      "Dongju Jang",
      "Youngjae Yu",
      "Chanyoung Park"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used for decision making in embodied agents, yet existing safety evaluations often rely on coarse success rates and domain-specific setups, making it difficult to diagnose why and where these models fail. This obscures our understanding of embodied safety and limits the selective deployment of LLMs in high-risk physical environments. We introduce SAFEL, the framework for systematically evaluating the physical safety of LLMs in embodied decision making. SAFEL assesses two key competencies: (1) rejecting unsafe commands via the Command Refusal Test, and (2) generating safe and executable plans via the Plan Safety Test. Critically, the latter is decomposed into functional modules, goal interpretation, transition modeling, action sequencing, enabling fine-grained diagnosis of safety failures. To support this framework, we introduce EMBODYGUARD, a PDDL-grounded benchmark containing 942 LLM-generated scenarios covering both overtly malicious and contextually hazardous instructions. Evaluation across 13 state-of-the-art LLMs reveals that while models often reject clearly unsafe commands, they struggle to anticipate and mitigate subtle, situational risks. Our results highlight critical limitations in current LLMs and provide a foundation for more targeted, modular improvements in safe embodied reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "37 pages, 13 tables, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.19933v1",
    "published_date": "2025-05-26 13:01:14 UTC",
    "updated_date": "2025-05-26 13:01:14 UTC"
  },
  {
    "arxiv_id": "2505.19927v2",
    "title": "TCP: a Benchmark for Temporal Constraint-Based Planning",
    "authors": [
      "Zifeng Ding",
      "Sikuan Yan",
      "Zhangdie Yuan",
      "Xianglong Hu",
      "Fangru Lin",
      "Andreas Vlachos"
    ],
    "abstract": "Temporal reasoning and planning are essential capabilities for large language models (LLMs), yet most existing benchmarks evaluate them in isolation and under limited forms of complexity. To address this gap, we introduce the Temporal Constraint-based Planning (TCP) benchmark that jointly assesses both capabilities. Each instance in TCP features a naturalistic dialogue around a collaborative project, where diverse and interdependent temporal constraints are explicitly or implicitly expressed, and models must infer an optimal schedule that satisfies all constraints. To construct TCP, we generate abstract problem prototypes that are then paired with realistic scenarios from various domains and enriched into dialogues using an LLM. A human quality check is performed on a sampled subset to confirm the reliability of our benchmark. We evaluate state-of-the-art LLMs and find that even the strongest models may struggle with TCP, highlighting its difficulty and revealing limitations in LLMs' temporal constraint-based planning abilities. We analyze underlying failure cases, open source our benchmark, and hope our findings can inspire future research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to EMNLP 2025 main conference",
    "pdf_url": "https://arxiv.org/pdf/2505.19927v2",
    "published_date": "2025-05-26 12:53:01 UTC",
    "updated_date": "2025-10-13 02:45:30 UTC"
  },
  {
    "arxiv_id": "2505.19920v1",
    "title": "A Responsible Face Recognition Approach for Small and Mid-Scale Systems Through Personalized Neural Networks",
    "authors": [
      "Sebastian Groß",
      "Stefan Heindorf",
      "Philipp Terhörst"
    ],
    "abstract": "Traditional face recognition systems rely on extracting fixed face representations, known as templates, to store and verify identities. These representations are typically generated by neural networks that often lack explainability and raise concerns regarding fairness and privacy. In this work, we propose a novel model-template (MOTE) approach that replaces vector-based face templates with small personalized neural networks. This design enables more responsible face recognition for small and medium-scale systems. During enrollment, MOTE creates a dedicated binary classifier for each identity, trained to determine whether an input face matches the enrolled identity. Each classifier is trained using only a single reference sample, along with synthetically balanced samples to allow adjusting fairness at the level of a single individual during enrollment. Extensive experiments across multiple datasets and recognition systems demonstrate substantial improvements in fairness and particularly in privacy. Although the method increases inference time and storage requirements, it presents a strong solution for small- and mid-scale applications where fairness and privacy are critical.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19920v1",
    "published_date": "2025-05-26 12:45:01 UTC",
    "updated_date": "2025-05-26 12:45:01 UTC"
  },
  {
    "arxiv_id": "2505.19915v2",
    "title": "Evaluating AI cyber capabilities with crowdsourced elicitation",
    "authors": [
      "Artem Petrov",
      "Dmitrii Volkov"
    ],
    "abstract": "As AI systems become increasingly capable, understanding their offensive cyber potential is critical for informed governance and responsible deployment. However, it's hard to accurately bound their capabilities, and some prior evaluations dramatically underestimated them. The art of extracting maximum task-specific performance from AIs is called \"AI elicitation\", and today's safety organizations typically conduct it in-house. In this paper, we explore crowdsourcing elicitation efforts as an alternative to in-house elicitation work.\n  We host open-access AI tracks at two Capture The Flag (CTF) competitions: AI vs. Humans (400 teams) and Cyber Apocalypse (8000 teams). The AI teams achieve outstanding performance at both events, ranking top-5% and top-10% respectively for a total of \\$7500 in bounties. This impressive performance suggests that open-market elicitation may offer an effective complement to in-house elicitation. We propose elicitation bounties as a practical mechanism for maintaining timely, cost-effective situational awareness of emerging AI capabilities.\n  Another advantage of open elicitations is the option to collect human performance data at scale. Applying METR's methodology, we found that AI agents can reliably solve cyber challenges requiring one hour or less of effort from a median human CTF participant.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Updated abstract to fix a typo; no changes to the content of the paper",
    "pdf_url": "https://arxiv.org/pdf/2505.19915v2",
    "published_date": "2025-05-26 12:40:32 UTC",
    "updated_date": "2025-05-27 17:45:40 UTC"
  },
  {
    "arxiv_id": "2505.19914v2",
    "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles",
    "authors": [
      "Jiangjie Chen",
      "Qianyu He",
      "Siyu Yuan",
      "Aili Chen",
      "Zhicheng Cai",
      "Weinan Dai",
      "Hongli Yu",
      "Qiying Yu",
      "Xuefeng Li",
      "Jiaze Chen",
      "Hao Zhou",
      "Mingxuan Wang"
    ],
    "abstract": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at https://seed-enigmata.github.io.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19914v2",
    "published_date": "2025-05-26 12:40:31 UTC",
    "updated_date": "2025-06-09 07:49:32 UTC"
  },
  {
    "arxiv_id": "2505.19912v2",
    "title": "APE: Selective Fine-tuning with Acceptance Criteria for Language Model Adaptation",
    "authors": [
      "Javier Marín"
    ],
    "abstract": "We present Adjacent Possible Exploration (APE), a selective fine-tuning method for adapting large language models that systematically explores parameter modifications while maintaining model stability. Inspired by evolutionary optimization principles, APE evaluates multiple candidate parameter updates through fine-tuning on small data subsets and accepts only those exceeding a performance threshold. Unlike standard fine-tuning that follows single gradient directions, APE implements a filtered selection process that prevents destabilizing parameter changes while enabling systematic improvement. Our method achieves 33.9\\% BLEU improvement and 36.2\\% perplexity reduction on news summarization tasks while using minimal computational resources. The approach provides a practical framework for controlled model adaptation that balances performance gains with representational stability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19912v2",
    "published_date": "2025-05-26 12:39:24 UTC",
    "updated_date": "2025-06-09 10:21:49 UTC"
  },
  {
    "arxiv_id": "2505.19905v2",
    "title": "EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM",
    "authors": [
      "Shuang Ao",
      "Flora D. Salim",
      "Simon Khan"
    ],
    "abstract": "Although LLMs demonstrate proficiency in several text-based reasoning and planning tasks, their implementation in robotics control is constrained by significant deficiencies: (1) LLM agents are designed to work mainly with textual inputs rather than visual conditions; (2) Current multimodal agents treat LLMs as static planners, which separates their reasoning from environment dynamics, resulting in actions that do not take domain-specific knowledge into account; and (3) LLMs are not designed to learn from visual interactions, which makes it harder for them to make better policies for specific domains. In this paper, we introduce EMAC+, an Embodied Multimodal Agent that collaboratively integrates LLM and VLM via a bidirectional training paradigm. Unlike existing methods, EMAC+ dynamically refines high-level textual plans generated by an LLM using real-time feedback from a VLM executing low-level visual control tasks. We address critical limitations of previous models by enabling the LLM to internalize visual environment dynamics directly through interactive experience, rather than relying solely on static symbolic mappings. Extensive experimental evaluations on ALFWorld and RT-1 benchmarks demonstrate that EMAC+ achieves superior task performance, robustness against noisy observations, and efficient learning. We also conduct thorough ablation studies and provide detailed analyses of success and failure cases.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19905v2",
    "published_date": "2025-05-26 12:34:16 UTC",
    "updated_date": "2025-10-16 01:38:12 UTC"
  },
  {
    "arxiv_id": "2505.19897v2",
    "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows",
    "authors": [
      "Qiushi Sun",
      "Zhoumianze Liu",
      "Chang Ma",
      "Zichen Ding",
      "Fangzhi Xu",
      "Zhangyue Yin",
      "Haiteng Zhao",
      "Zhenyu Wu",
      "Kanzhi Cheng",
      "Zhaoyang Liu",
      "Jianing Wang",
      "Qintong Li",
      "Xiangru Tang",
      "Tianbao Xie",
      "Xiachong Feng",
      "Xiang Li",
      "Ben Kao",
      "Wenhai Wang",
      "Biqing Qi",
      "Lingpeng Kong",
      "Zhiyong Wu"
    ],
    "abstract": "Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers' workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at https://qiushisun.github.io/ScienceBoard-Home/.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "work in progress",
    "pdf_url": "https://arxiv.org/pdf/2505.19897v2",
    "published_date": "2025-05-26 12:27:27 UTC",
    "updated_date": "2025-06-27 09:38:03 UTC"
  },
  {
    "arxiv_id": "2505.23794v2",
    "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning",
    "authors": [
      "Yuan Li",
      "Qi Luo",
      "Xiaonan Li",
      "Bufan Li",
      "Qinyuan Cheng",
      "Bo Wang",
      "Yining Zheng",
      "Yuxin Wang",
      "Zhangyue Yin",
      "Xipeng Qiu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) integrates external knowledge with Large Language Models (LLMs) to enhance factual correctness and mitigate hallucination. However, dense retrievers often become the bottleneck of RAG systems due to their limited parameters compared to LLMs and their inability to perform step-by-step reasoning. While prompt-based iterative RAG attempts to address these limitations, it is constrained by human-designed workflows. To address these limitations, we propose $\\textbf{R3-RAG}$, which uses $\\textbf{R}$einforcement learning to make the LLM learn how to $\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving comprehensive external knowledge and leading to correct answers. R3-RAG is divided into two stages. We first use cold start to make the model learn the manner of iteratively interleaving reasoning and retrieval. Then we use reinforcement learning to further harness its ability to better explore the external retrieval environment. Specifically, we propose two rewards for R3-RAG: 1) answer correctness for outcome reward, which judges whether the trajectory leads to a correct answer; 2) relevance-based document verification for process reward, encouraging the model to retrieve documents that are relevant to the user question, through which we can let the model learn how to iteratively reason and retrieve relevant documents to get the correct answer. Experimental results show that R3-RAG significantly outperforms baselines and can transfer well to different retrievers. We release R3-RAG at https://github.com/Yuan-Li-FNLP/R3-RAG.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23794v2",
    "published_date": "2025-05-26 12:25:37 UTC",
    "updated_date": "2025-10-24 15:52:23 UTC"
  },
  {
    "arxiv_id": "2505.19896v1",
    "title": "Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program",
    "authors": [
      "Alejandro Carrasco",
      "Victor Rodriguez-Fernandez",
      "Richard Linares"
    ],
    "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Control in space, enabling LLMs to play a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. The project comprises several open repositories to facilitate replication and further research. The codebase is accessible on \\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models and datasets are available on \\href{https://huggingface.co/OhhTuRnz}{Hugging Face}. Additionally, experiment tracking and detailed results can be reviewed on \\href{https://wandb.ai/carrusk/huggingface}{Weights \\& Biases",
    "categories": [
      "cs.AI",
      "astro-ph.IM",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Non revised version for paper going to be published in Journal of Advances in Space Research",
    "pdf_url": "https://arxiv.org/pdf/2505.19896v1",
    "published_date": "2025-05-26 12:25:35 UTC",
    "updated_date": "2025-05-26 12:25:35 UTC"
  },
  {
    "arxiv_id": "2505.19892v2",
    "title": "OptMerge: Unifying Multimodal LLM Capabilities and Modalities via Model Merging",
    "authors": [
      "Yongxian Wei",
      "Runxi Cheng",
      "Weike Jin",
      "Enneng Yang",
      "Li Shen",
      "Lu Hou",
      "Sinan Du",
      "Chun Yuan",
      "Xiaochun Cao",
      "Dacheng Tao"
    ],
    "abstract": "Foundation models update slowly due to resource-intensive training, whereas domain-specific models evolve rapidly between releases. Model merging seeks to combine multiple expert models into a single, more capable model, reducing storage and serving costs while supporting decentralized development. Despite its potential, previous studies have primarily focused on merging visual classification models or Large Language Models (LLMs) for code and math tasks. Recently, Multimodal LLMs (MLLMs) that extend LLMs through large-scale multimodal training have gained traction. However, there lacks a benchmark for model merging research that clearly divides the tasks for MLLM training and evaluation. In this paper, $\\textbf{(i)}$ we introduce a model merging benchmark for MLLMs, which includes multiple tasks such as VQA, Geometry, Chart, OCR, and Grounding, studying both LoRA and full fine-tuning models. Moreover, we explore how model merging can combine different modalities (e.g., vision-language, audio-language, and video-language models), moving toward the Omni-language model. $\\textbf{(ii)}$ We implement 10 model merging algorithms on the benchmark. Furthermore, we propose a novel method that removes noise from task vectors and robustly optimizes the merged vector based on a loss defined over task vector interactions, achieving an average performance gain of 2.48%. $\\textbf{(iii)}$ We find that model merging offers a promising way for building improved MLLMs without requiring training data. Our results also demonstrate that the complementarity among multiple modalities outperforms individual modalities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19892v2",
    "published_date": "2025-05-26 12:23:14 UTC",
    "updated_date": "2025-09-23 07:08:26 UTC"
  },
  {
    "arxiv_id": "2505.19887v2",
    "title": "Deconstructing Obfuscation: A four-dimensional framework for evaluating Large Language Models assembly code deobfuscation capabilities",
    "authors": [
      "Anton Tkachenko",
      "Dmitrij Suskevic",
      "Benjamin Adolphi"
    ],
    "abstract": "Large language models (LLMs) have shown promise in software engineering, yet their effectiveness for binary analysis remains unexplored. We present the first comprehensive evaluation of commercial LLMs for assembly code deobfuscation. Testing seven state-of-the-art models against four obfuscation scenarios (bogus control flow, instruction substitution, control flow flattening, and their combination), we found striking performance variations--from autonomous deobfuscation to complete failure. We propose a theoretical framework based on four dimensions: Reasoning Depth, Pattern Recognition, Noise Filtering, and Context Integration, explaining these variations. Our analysis identifies five error patterns: predicate misinterpretation, structural mapping errors, control flow misinterpretation, arithmetic transformation errors, and constant propagation errors, revealing fundamental limitations in LLM code processing.We establish a three-tier resistance model: bogus control flow (low resistance), control flow flattening (moderate resistance), and instruction substitution/combined techniques (high resistance). Universal failure against combined techniques demonstrates that sophisticated obfuscation remains effective against advanced LLMs. Our findings suggest a human-AI collaboration paradigm where LLMs reduce expertise barriers for certain reverse engineering tasks while requiring human guidance for complex deobfuscation. This work provides a foundation for evaluating emerging capabilities and developing resistant obfuscation techniques.x deobfuscation. This work provides a foundation for evaluating emerging capabilities and developing resistant obfuscation techniques.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19887v2",
    "published_date": "2025-05-26 12:16:44 UTC",
    "updated_date": "2025-06-05 10:02:39 UTC"
  },
  {
    "arxiv_id": "2505.19874v1",
    "title": "StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation",
    "authors": [
      "Yi Wu",
      "Lingting Zhu",
      "Shengju Qian",
      "Lei Liu",
      "Wandi Qiao",
      "Lequan Yu",
      "Bin Li"
    ],
    "abstract": "In the current research landscape, multimodal autoregressive (AR) models have shown exceptional capabilities across various domains, including visual understanding and generation. However, complex tasks such as style-aligned text-to-image generation present significant challenges, particularly in data acquisition. In analogy to instruction-following tuning for image editing of AR models, style-aligned generation requires a reference style image and prompt, resulting in a text-image-to-image triplet where the output shares the style and semantics of the input. However, acquiring large volumes of such triplet data with specific styles is considerably more challenging than obtaining conventional text-to-image data used for training generative models. To address this issue, we propose StyleAR, an innovative approach that combines a specially designed data curation method with our proposed AR models to effectively utilize text-to-image binary data for style-aligned text-to-image generation. Our method synthesizes target stylized data using a reference style image and prompt, but only incorporates the target stylized image as the image modality to create high-quality binary data. To facilitate binary data training, we introduce a CLIP image encoder with a perceiver resampler that translates the image input into style tokens aligned with multimodal tokens in AR models and implement a style-enhanced token technique to prevent content leakage which is a common issue in previous work. Furthermore, we mix raw images drawn from large-scale text-image datasets with stylized images to enhance StyleAR's ability to extract richer stylistic features and ensure style consistency. Extensive qualitative and quantitative experiments demonstrate our superior performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19874v1",
    "published_date": "2025-05-26 12:01:15 UTC",
    "updated_date": "2025-05-26 12:01:15 UTC"
  },
  {
    "arxiv_id": "2505.19867v1",
    "title": "Deep Active Inference Agents for Delayed and Long-Horizon Environments",
    "authors": [
      "Yavar Taheri Yeganeh",
      "Mohsen Jafari",
      "Andrea Matta"
    ],
    "abstract": "With the recent success of world-model agents, which extend the core idea of model-based reinforcement learning by learning a differentiable model for sample-efficient control across diverse tasks, active inference (AIF) offers a complementary, neuroscience-grounded paradigm that unifies perception, learning, and action within a single probabilistic framework powered by a generative model. Despite this promise, practical AIF agents still rely on accurate immediate predictions and exhaustive planning, a limitation that is exacerbated in delayed environments requiring plans over long horizons, tens to hundreds of steps. Moreover, most existing agents are evaluated on robotic or vision benchmarks which, while natural for biological agents, fall short of real-world industrial complexity. We address these limitations with a generative-policy architecture featuring (i) a multi-step latent transition that lets the generative model predict an entire horizon in a single look-ahead, (ii) an integrated policy network that enables the transition and receives gradients of the expected free energy, (iii) an alternating optimization scheme that updates model and policy from a replay buffer, and (iv) a single gradient step that plans over long horizons, eliminating exhaustive planning from the control loop. We evaluate our agent in an environment that mimics a realistic industrial scenario with delayed and long-horizon settings. The empirical results confirm the effectiveness of the proposed approach, demonstrating the coupled world-model with the AIF formalism yields an end-to-end probabilistic controller capable of effective decision making in delayed, long-horizon settings without handcrafted rewards or expensive planning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19867v1",
    "published_date": "2025-05-26 11:50:22 UTC",
    "updated_date": "2025-05-26 11:50:22 UTC"
  },
  {
    "arxiv_id": "2505.19866v2",
    "title": "HS-STaR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation",
    "authors": [
      "Feng Xiong",
      "Hongling Xu",
      "Yifei Wang",
      "Runxi Cheng",
      "Yong Wang",
      "Xiangxiang Chu"
    ],
    "abstract": "Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of large language models (LLMs) by leveraging self-generated responses for self-training. Recent studies have incorporated reward models to guide response selection or decoding, aiming to obtain higher-quality data. However, they typically allocate a uniform sampling budget across all problems, overlooking the varying utility of problems at different difficulty levels. In this work, we conduct an empirical study and find that problems near the boundary of the LLM's reasoning capability offer significantly greater learning utility than both easy and overly difficult ones. To identify and exploit such problems, we propose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners. Given a fixed sampling budget, HS-STaR first performs lightweight pre-sampling with a reward-guided difficulty estimation strategy to efficiently identify boundary-level problems. Subsequently, it dynamically reallocates the remaining budget toward these high-utility problems during a re-sampling phase, maximizing the generation of valuable training data. Extensive experiments across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR significantly outperforms other baselines without requiring additional sampling budget.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19866v2",
    "published_date": "2025-05-26 11:50:16 UTC",
    "updated_date": "2025-09-28 06:46:13 UTC"
  },
  {
    "arxiv_id": "2505.19853v3",
    "title": "Two Causally Related Needles in a Video Haystack",
    "authors": [
      "Miaoyu Li",
      "Qin Chao",
      "Boyang Li"
    ],
    "abstract": "Properly evaluating the ability of Video-Language Models (VLMs) to understand long videos remains a challenge. We propose a long-context video understanding benchmark, Causal2Needles, that assesses two crucial abilities insufficiently addressed by existing benchmarks: (1) extracting information from two separate locations (two needles) in a long video and understanding them jointly, and (2) modeling the world in terms of cause and effect in human behaviors. Causal2Needles evaluates these abilities using noncausal one-needle, causal one-needle, and causal two-needle questions. The most complex question type, causal two-needle questions, require extracting information from both the cause and effect events from a long video and the associated narration text. To prevent textual bias, we introduce two complementary question formats: locating the video clip containing the answer, and verbal description of a visual detail from that video clip. Our experiments reveal that models excelling on existing benchmarks struggle with causal 2-needle questions, and the model performance is negatively correlated with the distance between the two needles. These findings highlight critical limitations in current VLMs. The dataset is available at: https://huggingface.co/datasets/causal2needles/Causal2Needles",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2025 D&B Track",
    "pdf_url": "https://arxiv.org/pdf/2505.19853v3",
    "published_date": "2025-05-26 11:37:34 UTC",
    "updated_date": "2025-11-06 05:02:02 UTC"
  },
  {
    "arxiv_id": "2505.19851v1",
    "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages",
    "authors": [
      "Gulfarogh Azam",
      "Mohd Sadique",
      "Saif Ali",
      "Mohammad Nadeem",
      "Erik Cambria",
      "Shahab Saquib Sohail",
      "Mohammad Sultan Alam"
    ],
    "abstract": "Transliteration, the process of mapping text from one script to another, plays a crucial role in multilingual natural language processing, especially within linguistically diverse contexts such as India. Despite significant advancements through specialized models like IndicXlit, recent developments in large language models suggest a potential for general-purpose models to excel at this task without explicit task-specific training. The current work systematically evaluates the performance of prominent LLMs, including GPT-4o, GPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a state-of-the-art transliteration model, across ten major Indian languages. Experiments utilized standard benchmarks, including Dakshina and Aksharantar datasets, with performance assessed via Top-1 Accuracy and Character Error Rate. Our findings reveal that while GPT family models generally outperform other LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o improves performance on specific languages notably. An extensive error analysis and robustness testing under noisy conditions further elucidate strengths of LLMs compared to specialized models, highlighting the efficacy of foundational models for a wide spectrum of specialized applications with minimal overhead.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19851v1",
    "published_date": "2025-05-26 11:35:51 UTC",
    "updated_date": "2025-05-26 11:35:51 UTC"
  },
  {
    "arxiv_id": "2505.19850v2",
    "title": "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning",
    "authors": [
      "Leander Diaz-Bone",
      "Marco Bagatella",
      "Jonas Hübotter",
      "Andreas Krause"
    ],
    "abstract": "Sparse-reward reinforcement learning (RL) can model a wide range of highly complex tasks. Solving sparse-reward tasks is RL's core premise, requiring efficient exploration coupled with long-horizon credit assignment, and overcoming these challenges is key for building self-improving agents with superhuman ability. Prior work commonly explores with the objective of solving many sparse-reward tasks, making exploration of individual high-dimensional, long-horizon tasks intractable. We argue that solving such challenging tasks requires solving simpler tasks that are relevant to the target task, i.e., whose achieval will teach the agent skills required for solving the target task. We demonstrate that this sense of direction, necessary for effective exploration, can be extracted from existing RL algorithms, without leveraging any prior information. To this end, we propose a method for directed sparse-reward goal-conditioned very long-horizon RL (DISCOVER), which selects exploratory goals in the direction of the target task. We connect DISCOVER to principled exploration in bandits, formally bounding the time until the target task becomes achievable in terms of the agent's initial distance to the target, but independent of the volume of the space of all tasks. We then perform a thorough evaluation in high-dimensional environments. We find that the directed goal selection of DISCOVER solves exploration problems that are beyond the reach of prior state-of-the-art exploration methods in RL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19850v2",
    "published_date": "2025-05-26 11:35:07 UTC",
    "updated_date": "2025-10-20 12:58:20 UTC"
  },
  {
    "arxiv_id": "2505.19847v1",
    "title": "DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems",
    "authors": [
      "Wenqing Zhou",
      "Yuxuan Yan",
      "Qianqian Yang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to enhance the capabilities of language models by integrating external knowledge. Due to the diversity of data sources and the constraints of memory and computing resources, real-world data is often scattered in multiple devices. Conventional RAGs that store massive amounts of scattered data centrally face increasing privacy concerns and high computational costs. Additionally, RAG in a central node raises latency issues when searching over a large-scale knowledge base. To address these challenges, we propose a distributed Knowledge Graph-based RAG approach, referred to as DGRAG, in an edge-cloud system, where each edge device maintains a local knowledge base without the need to share it with the cloud, instead sharing only summaries of its knowledge. Specifically, DGRAG has two main phases. In the Distributed Knowledge Construction phase, DGRAG organizes local knowledge using knowledge graphs, generating subgraph summaries and storing them in a summary database in the cloud as information sharing. In the Collaborative Retrieval and Generation phase, DGRAG first performs knowledge retrieval and answer generation locally, and a gate mechanism determines whether the query is beyond the scope of local knowledge or processing capabilities. For queries that exceed the local knowledge scope, the cloud retrieves knowledge from the most relevant edges based on the summaries and generates a more precise answer. Experimental results demonstrate the effectiveness of the proposed DGRAG approach in significantly improving the quality of question-answering tasks over baseline approaches.",
    "categories": [
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19847v1",
    "published_date": "2025-05-26 11:31:58 UTC",
    "updated_date": "2025-05-26 11:31:58 UTC"
  },
  {
    "arxiv_id": "2505.19842v2",
    "title": "PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints",
    "authors": [
      "Shuo Wang",
      "Yun Cheng",
      "Qingye Meng",
      "Olga Saukh",
      "Jiang Zhang",
      "Jingfang Fan",
      "Yuanting Zhang",
      "Xingyuan Yuan",
      "Lothar Thiele"
    ],
    "abstract": "Air quality forecasting (AQF) is critical for public health and environmental management, yet remains challenging due to the complex interplay of emissions, meteorology, and chemical transformations. Traditional numerical models, such as CMAQ and WRF-Chem, provide physically grounded simulations but are computationally expensive and rely on uncertain emission inventories. Deep learning models, while computationally efficient, often struggle with generalization due to their lack of physical constraints. To bridge this gap, we propose PCDCNet, a surrogate model that integrates numerical modeling principles with deep learning. PCDCNet explicitly incorporates emissions, meteorological influences, and domain-informed constraints to model pollutant formation, transport, and dissipation. By combining graph-based spatial transport modeling, recurrent structures for temporal accumulation, and representation enhancement for local interactions, PCDCNet achieves state-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3 forecasting while significantly reducing computational costs. Furthermore, our model is deployed in an online platform, providing free, real-time air quality forecasts, demonstrating its scalability and societal impact. By aligning deep learning with physical consistency, PCDCNet offers a practical and interpretable solution for AQF, enabling informed decision-making for both personal and regulatory applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19842v2",
    "published_date": "2025-05-26 11:27:07 UTC",
    "updated_date": "2025-05-27 04:55:19 UTC"
  },
  {
    "arxiv_id": "2505.19838v1",
    "title": "FoodTaxo: Generating Food Taxonomies with Large Language Models",
    "authors": [
      "Pascal Wullschleger",
      "Majid Zarharan",
      "Donnacha Daly",
      "Marc Pouly",
      "Jennifer Foster"
    ],
    "abstract": "We investigate the utility of Large Language Models for automated taxonomy generation and completion specifically applied to taxonomies from the food technology industry. We explore the extent to which taxonomies can be completed from a seed taxonomy or generated without a seed from a set of known concepts, in an iterative fashion using recent prompting techniques. Experiments on five taxonomies using an open-source LLM (Llama-3), while promising, point to the difficulty of correctly placing inner nodes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To be published in ACL 2025 Industry Track. Paper website: https://foodtaxo.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2505.19838v1",
    "published_date": "2025-05-26 11:22:17 UTC",
    "updated_date": "2025-05-26 11:22:17 UTC"
  },
  {
    "arxiv_id": "2505.20368v3",
    "title": "Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents",
    "authors": [
      "Jaeyoung Choe",
      "Jihoon Kim",
      "Woohwan Jung"
    ],
    "abstract": "Retrieval-augmented generation (RAG) based large language models (LLMs) are widely used in finance for their excellent performance on knowledge-intensive tasks. However, standardized documents (e.g., SEC filing) share similar formats such as repetitive boilerplate texts, and similar table structures. This similarity forces traditional RAG methods to misidentify near-duplicate text, leading to duplicate retrieval that undermines accuracy and completeness. To address these issues, we propose the Hierarchical Retrieval with Evidence Curation (HiREC) framework. Our approach first performs hierarchical retrieval to reduce confusion among similar texts. It first retrieve related documents and then selects the most relevant passages from the documents. The evidence curation process removes irrelevant passages. When necessary, it automatically generates complementary queries to collect missing information. To evaluate our approach, we construct and release a Large-scale Open-domain Financial (LOFin) question answering benchmark that includes 145,897 SEC documents and 1,595 question-answer pairs. Our code and data are available at https://github.com/deep-over/LOFin-bench-HiREC.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "ACL 2025 (Findings)",
    "pdf_url": "https://arxiv.org/pdf/2505.20368v3",
    "published_date": "2025-05-26 11:08:23 UTC",
    "updated_date": "2025-11-06 06:47:35 UTC"
  },
  {
    "arxiv_id": "2505.19827v1",
    "title": "Revisiting Glorot Initialization for Long-Range Linear Recurrences",
    "authors": [
      "Noga Bar",
      "Mariia Seleznova",
      "Yotam Alexander",
      "Gitta Kutyniok",
      "Raja Giryes"
    ],
    "abstract": "Proper initialization is critical for Recurrent Neural Networks (RNNs), particularly in long-range reasoning tasks, where repeated application of the same weight matrix can cause vanishing or exploding signals. A common baseline for linear recurrences is Glorot initialization, designed to ensure stable signal propagation--but derived under the infinite-width, fixed-length regime--an unrealistic setting for RNNs processing long sequences. In this work, we show that Glorot initialization is in fact unstable: small positive deviations in the spectral radius are amplified through time and cause the hidden state to explode. Our theoretical analysis demonstrates that sequences of length $t = O(\\sqrt{n})$, where $n$ is the hidden width, are sufficient to induce instability. To address this, we propose a simple, dimension-aware rescaling of Glorot that shifts the spectral radius slightly below one, preventing rapid signal explosion or decay. These results suggest that standard initialization schemes may break down in the long-sequence regime, motivating a separate line of theory for stable recurrent initialization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19827v1",
    "published_date": "2025-05-26 11:04:59 UTC",
    "updated_date": "2025-05-26 11:04:59 UTC"
  },
  {
    "arxiv_id": "2505.19825v2",
    "title": "Position: Foundation Models for Tabular Data within Systemic Contexts Need Grounding",
    "authors": [
      "Tassilo Klein",
      "Johannes Hoffart"
    ],
    "abstract": "This position paper argues that foundation models for tabular data face inherent limitations when isolated from operational context - the procedural logic, declarative rules, and domain knowledge that define how data is created and governed. Current approaches focus on single-table generalization or schema-level relationships, fundamentally missing the operational knowledge that gives data meaning. We introduce Semantically Linked Tables (SLT) and Foundation Models for SLT (FMSLT) as a new model class that grounds tabular data in its operational context. We propose dual-phase training: pre-training on open-source code-data pairs and synthetic systems to learn business logic mechanics, followed by zero-shot inference on proprietary data. We introduce the ``Operational Turing Test'' benchmark and argue that operational grounding is essential for autonomous agents in complex data environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19825v2",
    "published_date": "2025-05-26 11:02:51 UTC",
    "updated_date": "2026-01-17 16:38:15 UTC"
  },
  {
    "arxiv_id": "2505.19823v1",
    "title": "LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning in Heterogeneous Environments",
    "authors": [
      "Pengcheng Sun",
      "Erwu Liu",
      "Wei Ni",
      "Rui Wang",
      "Yuanzhe Geng",
      "Lijuan Lai",
      "Abbas Jamalipour"
    ],
    "abstract": "Federated Learning (FL) is a distributed machine learning paradigm based on protecting data privacy of devices, which however, can still be broken by gradient leakage attack via parameter inversion techniques. Differential privacy (DP) technology reduces the risk of private data leakage by adding artificial noise to the gradients, but detrimental to the FL utility at the same time, especially in the scenario where the data is Non-Independent Identically Distributed (Non-IID). Based on the impact of heterogeneous data on aggregation performance, this paper proposes a Lightweight Adaptive Privacy Allocation (LAPA) strategy, which assigns personalized privacy budgets to devices in each aggregation round without transmitting any additional information beyond gradients, ensuring both privacy protection and aggregation efficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG) algorithm is employed to optimize the transmission power, in order to determine the optimal timing at which the adaptively attenuated artificial noise aligns with the communication noise, enabling an effective balance between DP and system utility. Finally, a reliable aggregation strategy is designed by integrating communication quality and data distribution characteristics, which improves aggregation performance while preserving privacy. Experimental results demonstrate that the personalized noise allocation and dynamic optimization strategy based on LAPA proposed in this paper enhances convergence performance while satisfying the privacy requirements of FL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19823v1",
    "published_date": "2025-05-26 11:00:31 UTC",
    "updated_date": "2025-05-26 11:00:31 UTC"
  },
  {
    "arxiv_id": "2505.19819v1",
    "title": "FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets",
    "authors": [
      "Dannong Wang",
      "Jaisal Patel",
      "Daochen Zha",
      "Steve Y. Yang",
      "Xiao-Yang Liu"
    ],
    "abstract": "Low-rank adaptation (LoRA) methods show great potential for scaling pre-trained general-purpose Large Language Models (LLMs) to hundreds or thousands of use scenarios. However, their efficacy in high-stakes domains like finance is rarely explored, e.g., passing CFA exams and analyzing SEC filings. In this paper, we present the open-source FinLoRA project that benchmarks LoRA methods on both general and highly professional financial tasks. First, we curated 19 datasets covering diverse financial applications; in particular, we created four novel XBRL analysis datasets based on 150 SEC filings. Second, we evaluated five LoRA methods and five base LLMs. Finally, we provide extensive experimental results in terms of accuracy, F1, and BERTScore and report computational cost in terms of time and GPU memory during fine-tuning and inference stages. We find that LoRA methods achieved substantial performance gains of 36\\% on average over base models. Our FinLoRA project provides an affordable and scalable approach to democratize financial intelligence to the general public. Datasets, LoRA adapters, code, and documentation are available at https://github.com/Open-Finance-Lab/FinLoRA",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19819v1",
    "published_date": "2025-05-26 10:58:51 UTC",
    "updated_date": "2025-05-26 10:58:51 UTC"
  },
  {
    "arxiv_id": "2505.19815v1",
    "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective",
    "authors": [
      "Junnan Liu",
      "Hongwei Liu",
      "Linchen Xiao",
      "Shudong Liu",
      "Taolin Zhang",
      "Zihan Ma",
      "Songyang Zhang",
      "Kai Chen"
    ],
    "abstract": "We propose a novel framework for comprehending the reasoning capabilities of large language models (LLMs) through the perspective of meta-learning. By conceptualizing reasoning trajectories as pseudo-gradient descent updates to the LLM's parameters, we identify parallels between LLM reasoning and various meta-learning paradigms. We formalize the training process for reasoning tasks as a meta-learning setup, with each question treated as an individual task, and reasoning trajectories serving as the inner loop optimization for adapting model parameters. Once trained on a diverse set of questions, the LLM develops fundamental reasoning capabilities that can generalize to previously unseen questions. Extensive empirical evaluations substantiate the strong connection between LLM reasoning and meta-learning, exploring several issues of significant interest from a meta-learning standpoint. Our work not only enhances the understanding of LLM reasoning but also provides practical insights for improving these models through established meta-learning techniques.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19815v1",
    "published_date": "2025-05-26 10:52:17 UTC",
    "updated_date": "2025-05-26 10:52:17 UTC"
  },
  {
    "arxiv_id": "2505.19809v2",
    "title": "Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees",
    "authors": [
      "Daniel Ordoñez-Apraez",
      "Vladimir Kostić",
      "Alek Fröhlich",
      "Vivien Brandt",
      "Karim Lounici",
      "Massimiliano Pontil"
    ],
    "abstract": "In many real-world applications of regression, conditional probability estimation, and uncertainty quantification, exploiting symmetries rooted in physics or geometry can dramatically improve generalization and sample efficiency. While geometric deep learning has made significant empirical advances by incorporating group-theoretic structure, less attention has been given to statistical learning guarantees. In this paper, we introduce an equivariant representation learning framework that simultaneously addresses regression, conditional probability estimation, and uncertainty quantification while providing first-of-its-kind non-asymptotic statistical learning guarantees. Grounded in operator and group representation theory, our framework approximates the spectral decomposition of the conditional expectation operator, building representations that are both equivariant and disentangled along independent symmetry subgroups. Empirical evaluations on synthetic datasets and real-world robotics applications confirm the potential of our approach, matching or outperforming existing equivariant baselines in regression while additionally providing well-calibrated parametric uncertainty estimates.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19809v2",
    "published_date": "2025-05-26 10:47:23 UTC",
    "updated_date": "2025-05-27 13:36:17 UTC"
  },
  {
    "arxiv_id": "2505.19795v2",
    "title": "The Missing Point in Vision Transformers for Universal Image Segmentation",
    "authors": [
      "Sajjad Shahabodini",
      "Mobina Mansoori",
      "Farnoush Bayatmakou",
      "Jamshid Abouei",
      "Konstantinos N. Plataniotis",
      "Arash Mohammadi"
    ],
    "abstract": "Image segmentation remains a challenging task in computer vision, demanding robust mask generation and precise classification. Recent mask-based approaches yield high-quality masks by capturing global context. However, accurately classifying these masks, especially in the presence of ambiguous boundaries and imbalanced class distributions, remains an open challenge. In this work, we introduce ViT-P, a novel two-stage segmentation framework that decouples mask generation from classification. The first stage employs a proposal generator to produce class-agnostic mask proposals, while the second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture, ensuring adaptability to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets, reducing annotation costs while maintaining strong performance. Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation. The code and pretrained models are available at: https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19795v2",
    "published_date": "2025-05-26 10:29:13 UTC",
    "updated_date": "2025-12-09 17:56:45 UTC"
  },
  {
    "arxiv_id": "2505.19792v1",
    "title": "Types of Relations: Defining Analogies with Category Theory",
    "authors": [
      "Claire Ott",
      "Frank Jäkel"
    ],
    "abstract": "In order to behave intelligently both humans and machines have to represent their knowledge adequately for how it is used. Humans often use analogies to transfer their knowledge to new domains, or help others with this transfer via explanations. Hence, an important question is: What representation can be used to construct, find, and evaluate analogies? In this paper, we study features of a domain that are important for constructing analogies. We do so by formalizing knowledge domains as categories. We use the well-known example of the analogy between the solar system and the hydrogen atom to demonstrate how to construct domain categories. We also show how functors, pullbacks, and pushouts can be used to define an analogy, describe its core and a corresponding blend of the underlying domains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "27 pages, 15 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.19792v1",
    "published_date": "2025-05-26 10:22:44 UTC",
    "updated_date": "2025-05-26 10:22:44 UTC"
  },
  {
    "arxiv_id": "2505.19788v2",
    "title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition",
    "authors": [
      "Zihao Zeng",
      "Xuyao Huang",
      "Boxiu Li",
      "Hao Zhang",
      "Zhijie Deng"
    ],
    "abstract": "Large Reasoning Models (LRMs) are criticized for the excessively lengthy Chain-of-Thought (CoT) to derive the final answer, suffering from high first-token and overall latency. Typically, the CoT of LRMs mixes multiple thinking units; each unit attempts to produce a candidate answer to the original query. Hence, a natural idea to improve efficiency is to reduce the unit number. Yet, the fact that the thinking units in vanilla CoT cannot be explicitly managed renders doing so challenging. This paper introduces Multi-Turn Decomposition (MinD) to decode conventional CoT into a sequence of explicit, structured, and turn-wise interactions to bridge the gap. In MinD, the model provides a multi-turn response to the query, where each turn embraces a thinking unit and yields a corresponding answer. The subsequent turns can reflect, verify, revise, or explore alternative approaches to both the thinking and answer parts of earlier ones. This not only makes the answer delivered more swiftly, but also enables explicit controls over the iterative reasoning process (i.e., users may halt or continue at any turn). We follow a supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We first rephrase the outputs of an LRM into multi-turn formats by prompting another LLM, and then tune the LRM with such data. Observing that the tuned model tends to consume even more tokens than the original one (probably due to that the multi-turn formats introduce additional answer tokens), we advocate leveraging RL algorithms like GRPO to prioritize correct outputs with fewer turns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up to ~70% reduction in both output token usage and time to first token (TTFT), while maintaining competitive performance on reasoning benchmarks such as MATH-500, AIME24, AMC23, and GPQA-Diamond.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19788v2",
    "published_date": "2025-05-26 10:18:57 UTC",
    "updated_date": "2025-06-05 03:15:25 UTC"
  },
  {
    "arxiv_id": "2505.19785v3",
    "title": "medDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support",
    "authors": [
      "Qianyi Xu",
      "Gousia Habib",
      "Feng Wu",
      "Dilruk Perera",
      "Mengling Feng"
    ],
    "abstract": "Timely and personalized treatment decisions are essential across a wide range of healthcare settings where patient responses can vary significantly and evolve over time. Clinical data used to support these treatment decisions are often irregularly sampled, where missing data frequencies may implicitly convey information about the patient's condition. Existing Reinforcement Learning (RL) based clinical decision support systems often ignore the missing patterns and distort them with coarse discretization and simple imputation. They are also predominantly model-free and largely depend on retrospective data, which could lead to insufficient exploration and bias by historical behaviors. To address these limitations, we propose medDreamer, a novel model-based reinforcement learning framework for personalized treatment recommendation. medDreamer contains a world model with an Adaptive Feature Integration module that simulates latent patient states from irregular data and a two-phase policy trained on a hybrid of real and imagined trajectories. This enables learning optimal policies that go beyond the sub-optimality of historical clinical decisions, while remaining close to real clinical data. We evaluate medDreamer on both sepsis and mechanical ventilation treatment tasks using two large-scale Electronic Health Records (EHRs) datasets. Comprehensive evaluations show that medDreamer significantly outperforms model-free and model-based baselines in both clinical outcomes and off-policy metrics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19785v3",
    "published_date": "2025-05-26 10:16:39 UTC",
    "updated_date": "2025-12-02 13:41:07 UTC"
  },
  {
    "arxiv_id": "2505.19776v1",
    "title": "Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification",
    "authors": [
      "Akram Elbouanani",
      "Evan Dufraisse",
      "Adrian Popescu"
    ],
    "abstract": "Political biases encoded by LLMs might have detrimental effects on downstream applications. Existing bias analysis methods rely on small-size intermediate tasks (questionnaire answering or political content generation) and rely on the LLMs themselves for analysis, thus propagating bias. We propose a new approach leveraging the observation that LLM sentiment predictions vary with the target entity in the same sentence. We define an entropy-based inconsistency metric to encode this prediction variability. We insert 1319 demographically and politically diverse politician names in 450 political sentences and predict target-oriented sentiment using seven models in six widely spoken languages. We observe inconsistencies in all tested combinations and aggregate them in a statistically robust analysis at different granularity levels. We observe positive and negative bias toward left and far-right politicians and positive correlations between politicians with similar alignment. Bias intensity is higher for Western languages than for others. Larger models exhibit stronger and more consistent biases and reduce discrepancies between similar languages. We partially mitigate LLM unreliability in target-oriented sentiment classification (TSC) by replacing politician names with fictional but plausible counterparts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To be published in the Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.19776v1",
    "published_date": "2025-05-26 10:01:24 UTC",
    "updated_date": "2025-05-26 10:01:24 UTC"
  },
  {
    "arxiv_id": "2505.19769v2",
    "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning",
    "authors": [
      "Yuhui Chen",
      "Haoran Li",
      "Zhennan Jiang",
      "Haowei Wen",
      "Dongbin Zhao"
    ],
    "abstract": "Developing scalable and generalizable reward engineering for reinforcement learning (RL) is crucial for creating general-purpose agents, especially in the challenging domain of robotic manipulation. While recent advances in reward engineering with Vision-Language Models (VLMs) have shown promise, their sparse reward nature significantly limits sample efficiency. This paper introduces TeViR, a novel method that leverages a pre-trained text-to-video diffusion model to generate dense rewards by comparing the predicted image sequence with current observations. Experimental results across 11 complex robotic tasks demonstrate that TeViR outperforms traditional methods leveraging sparse rewards and other state-of-the-art (SOTA) methods, achieving better sample efficiency and performance without ground truth environmental rewards. TeViR's ability to efficiently guide agents in complex environments highlights its potential to advance reinforcement learning applications in robotic manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19769v2",
    "published_date": "2025-05-26 09:52:25 UTC",
    "updated_date": "2025-06-24 05:29:35 UTC"
  },
  {
    "arxiv_id": "2505.19764v1",
    "title": "Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding",
    "authors": [
      "Patara Trirat",
      "Wonyong Jeong",
      "Sung Ju Hwang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but optimizing LLM-based agentic systems remains challenging due to the vast search space of agent configurations, prompting strategies, and communication patterns. Existing approaches often rely on heuristic-based tuning or exhaustive evaluation, which can be computationally expensive and suboptimal. This paper proposes Agentic Predictor, a lightweight predictor for efficient agentic workflow evaluation. Agentic Predictor is equipped with a multi-view workflow encoding technique that leverages multi-view representation learning of agentic systems by incorporating code architecture, textual prompts, and interaction graph features. To achieve high predictive accuracy while significantly reducing the number of required workflow evaluations for training a predictor, Agentic Predictor employs cross-domain unsupervised pretraining. By learning to approximate task success rates, Agentic Predictor enables fast and accurate selection of optimal agentic workflow configurations for a given task, significantly reducing the need for expensive trial-and-error evaluations. Experiments on a carefully curated benchmark spanning three domains show that our predictor outperforms state-of-the-art methods in both predictive accuracy and workflow utility, highlighting the potential of performance predictors in streamlining the design of LLM-based agentic workflows.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code will be available at https://github.com/DeepAuto-AI/agentic-predictor",
    "pdf_url": "https://arxiv.org/pdf/2505.19764v1",
    "published_date": "2025-05-26 09:46:50 UTC",
    "updated_date": "2025-05-26 09:46:50 UTC"
  },
  {
    "arxiv_id": "2505.19762v1",
    "title": "Language Model-Enhanced Message Passing for Heterophilic Graph Learning",
    "authors": [
      "Wenjun Wang",
      "Dawei Cheng"
    ],
    "abstract": "Traditional graph neural networks (GNNs), which rely on homophily-driven message passing, struggle with heterophilic graphs where connected nodes exhibit dissimilar features and different labels. While existing methods address heterophily through graph structure refinement or adaptation of neighbor aggregation functions, they often overlook the semantic potential of node text, rely on suboptimal message representation for propagation and compromise performance on homophilic graphs. To address these limitations, we propose a novel language model (LM)-enhanced message passing approach for heterophilic graph leaning (LEMP4HG). Specifically, in the context of text-attributed graph, we provide paired node texts for LM to generate their connection analysis, which are encoded and then fused with paired node textual embeddings through a gating mechanism. The synthesized messages are semantically enriched and adaptively balanced with both nodes' information, which mitigates contradictory signals when neighbor aggregation in heterophilic regions. Furthermore, we introduce an active learning strategy guided by our heuristic MVRD (Modulated Variation of Reliable Distance), selectively enhancing node pairs suffer most from message passing, reducing the cost of analysis generation and side effects on homophilic regions. Extensive experiments validate that our approach excels on heterophilic graphs and performs robustly on homophilic ones, with a graph convolutional network (GCN) backbone and a practical budget.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19762v1",
    "published_date": "2025-05-26 09:45:16 UTC",
    "updated_date": "2025-05-26 09:45:16 UTC"
  },
  {
    "arxiv_id": "2505.19761v1",
    "title": "Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning",
    "authors": [
      "Zican Hu",
      "Wei Liu",
      "Xiaoye Qu",
      "Xiangyu Yue",
      "Chunlin Chen",
      "Zhi Wang",
      "Yu Cheng"
    ],
    "abstract": "While showing sophisticated reasoning abilities, large language models (LLMs) still struggle with long-horizon decision-making tasks due to deficient exploration and long-term credit assignment, especially in sparse-reward scenarios. Inspired by the divide-and-conquer principle, we propose an innovative framework **GLIDER** (**G**rounding **L**anguage Models as Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical **R**einforcement Learning) that introduces a parameter-efficient and generally applicable hierarchy to LLM policies. We develop a scheme where the low-level controller is supervised with abstract, step-by-step plans that are learned and instructed by the high-level policy. This design decomposes complicated problems into a series of coherent chain-of-thought reasoning sub-tasks, providing flexible temporal abstraction to significantly enhance exploration and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast online adaptation to non-stationary environments owing to the strong transferability of its task-agnostic low-level skills. Experiments on ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent performance gains, along with enhanced generalization capabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ICML 2025, 21 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.19761v1",
    "published_date": "2025-05-26 09:43:40 UTC",
    "updated_date": "2025-05-26 09:43:40 UTC"
  },
  {
    "arxiv_id": "2505.19757v1",
    "title": "CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement",
    "authors": [
      "Maria Dziuba",
      "Valentin Malykh"
    ],
    "abstract": "Effective generation of structured code comments requires robust quality metrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS) suffer from limited code-comment analysis. We propose CIDRe, a language-agnostic reference-free quality criterion combining four synergistic aspects: (1) relevance (code-comment semantic alignment), (2) informativeness (functional coverage), (3) completeness (presence of all structure sections), and (4) description length (detail sufficiency). We validate our criterion on a manually annotated dataset. Experiments demonstrate CIDRe's superiority over existing metrics, achieving improvement in cross-entropy evaluation. When applied to filter comments, the models finetuned on CIDRe-filtered data show statistically significant quality gains in GPT-4o-mini assessments.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19757v1",
    "published_date": "2025-05-26 09:36:57 UTC",
    "updated_date": "2025-05-26 09:36:57 UTC"
  },
  {
    "arxiv_id": "2505.19754v2",
    "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering",
    "authors": [
      "Ruisheng Cao",
      "Hanchong Zhang",
      "Tiancheng Huang",
      "Zhangyi Kang",
      "Yuxin Zhang",
      "Liangtai Sun",
      "Hanqi Li",
      "Yuxun Miao",
      "Shuai Fan",
      "Lu Chen",
      "Kai Yu"
    ],
    "abstract": "The increasing number of academic papers poses significant challenges for researchers to efficiently acquire key details. While retrieval augmented generation (RAG) shows great promise in large language model (LLM) based automated question answering, previous works often isolate neural and symbolic retrieval despite their complementary strengths. Moreover, conventional single-view chunking neglects the rich structure and layout of PDFs, e.g., sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural symbolic retrieval framework which combines both paradigms in an interactive process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG organizes semi-structured PDF content into both the relational database and vectorstore, enabling LLM agents to iteratively gather context until sufficient to generate answers. Experiments on three full PDF-based QA datasets, including a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the vector-based RAG and various structured baselines, highlighting its capacity to unify both retrieval schemes and utilize multiple views. Code and data are publicly available at https://github.com/X-LANCE/NeuSym-RAG.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 11 figures, 12 tables, accepted to ACL 2025 Long Main",
    "pdf_url": "https://arxiv.org/pdf/2505.19754v2",
    "published_date": "2025-05-26 09:33:10 UTC",
    "updated_date": "2025-05-31 07:51:54 UTC"
  },
  {
    "arxiv_id": "2505.19752v1",
    "title": "Discrete Markov Bridge",
    "authors": [
      "Hengli Li",
      "Yuxuan Wang",
      "Song-Chun Zhu",
      "Ying Nian Wu",
      "Zilong Zheng"
    ],
    "abstract": "Discrete diffusion has recently emerged as a promising paradigm in discrete data modeling. However, existing methods typically rely on a fixed rate transition matrix during training, which not only limits the expressiveness of latent representations, a fundamental strength of variational methods, but also constrains the overall design space. To address these limitations, we propose Discrete Markov Bridge, a novel framework specifically designed for discrete representation learning. Our approach is built upon two key components: Matrix Learning and Score Learning. We conduct a rigorous theoretical analysis, establishing formal performance guarantees for Matrix Learning and proving the convergence of the overall framework. Furthermore, we analyze the space complexity of our method, addressing practical constraints identified in prior studies. Extensive empirical evaluations validate the effectiveness of the proposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO) of 1.38 on the Text8 dataset, outperforming established baselines. Moreover, the proposed model demonstrates competitive performance on the CIFAR-10 dataset, achieving results comparable to those obtained by image-specific generation approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19752v1",
    "published_date": "2025-05-26 09:32:12 UTC",
    "updated_date": "2025-05-26 09:32:12 UTC"
  },
  {
    "arxiv_id": "2505.19734v1",
    "title": "ReChisel: Effective Automatic Chisel Code Generation by LLM with Reflection",
    "authors": [
      "Juxin Niu",
      "Xiangfeng Liu",
      "Dan Niu",
      "Xi Wang",
      "Zhe Jiang",
      "Nan Guan"
    ],
    "abstract": "Coding with hardware description languages (HDLs) such as Verilog is a time-intensive and laborious task. With the rapid advancement of large language models (LLMs), there is increasing interest in applying LLMs to assist with HDL coding. Recent efforts have demonstrated the potential of LLMs in translating natural language to traditional HDL Verilog. Chisel, a next-generation HDL based on Scala, introduces higher-level abstractions, facilitating more concise, maintainable, and scalable hardware designs. However, the potential of using LLMs for Chisel code generation remains largely unexplored. This work proposes ReChisel, an LLM-based agentic system designed to enhance the effectiveness of Chisel code generation. ReChisel incorporates a reflection mechanism to iteratively refine the quality of generated code using feedback from compilation and simulation processes, and introduces an escape mechanism to break free from non-progress loops. Experiments demonstrate that ReChisel significantly improves the success rate of Chisel code generation, achieving performance comparable to state-of-the-art LLM-based agentic systems for Verilog code generation.",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to DAC 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19734v1",
    "published_date": "2025-05-26 09:20:07 UTC",
    "updated_date": "2025-05-26 09:20:07 UTC"
  },
  {
    "arxiv_id": "2505.19722v1",
    "title": "Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking",
    "authors": [
      "Yihao Ai",
      "Zhiyuan Ning",
      "Weiwei Dai",
      "Pengfei Wang",
      "Yi Du",
      "Wenjuan Cui",
      "Kunpeng Liu",
      "Yuanchun Zhou"
    ],
    "abstract": "Biomedical entity linking aims to map nonstandard entities to standard entities in a knowledge base. Traditional supervised methods perform well but require extensive annotated data to transfer, limiting their usage in low-resource scenarios. Large language models (LLMs), especially closed-source LLMs, can address these but risk stability issues and high economic costs: using these models is restricted by commercial companies and brings significant economic costs when dealing with large amounts of data. To address this, we propose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs for re-ranking candidates retrieved by a retriever fine-tuned with a small amount of data. By prompting a closed-source LLM to generate training data from unannotated data and fine-tuning an open-source LLM for re-ranking, we effectively distill the knowledge to the open-source LLM that can be deployed locally, thus avoiding the stability issues and the problem of high economic costs. We evaluate RPDR on two datasets, including one real-world dataset and one publicly available dataset involving two languages: Chinese and English. RPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier dataset and the Ask A Patient dataset when the amount of training data is not enough. The results demonstrate the superiority and generalizability of the proposed framework.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICIC 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19722v1",
    "published_date": "2025-05-26 09:10:19 UTC",
    "updated_date": "2025-05-26 09:10:19 UTC"
  },
  {
    "arxiv_id": "2505.19719v1",
    "title": "OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction",
    "authors": [
      "Juntong Wang",
      "Xiyuan Wang",
      "Muhan Zhang"
    ],
    "abstract": "Common Neighbors (CNs) and their higher-order variants are important pairwise features widely used in state-of-the-art link prediction methods. However, existing methods often struggle with the repetition across different orders of CNs and fail to fully leverage their potential. We identify that these limitations stem from two key issues: redundancy and over-smoothing in high-order common neighbors. To address these challenges, we design orthogonalization to eliminate redundancy between different-order CNs and normalization to mitigate over-smoothing. By combining these two techniques, we propose Orthogonal Common Neighbor (OCN), a novel approach that significantly outperforms the strongest baselines by an average of 7.7% on popular link prediction benchmarks. A thorough theoretical analysis is provided to support our method. Ablation studies also verify the effectiveness of our orthogonalization and normalization techniques.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.19719v1",
    "published_date": "2025-05-26 09:08:25 UTC",
    "updated_date": "2025-05-26 09:08:25 UTC"
  },
  {
    "arxiv_id": "2505.19716v1",
    "title": "Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting",
    "authors": [
      "Yifan Wu",
      "Jingze Shi",
      "Bingheng Wu",
      "Jiayi Zhang",
      "Xiaotian Lin",
      "Nan Tang",
      "Yuyu Luo"
    ],
    "abstract": "Existing chain-of-thought (CoT) distillation methods can effectively transfer reasoning abilities to base models but suffer from two major limitations: excessive verbosity of reasoning traces and inadequate adaptability to problem difficulty. Long reasoning traces significantly increase inference costs, and uniform-length solutions prevent base models from learning adaptive reasoning strategies. To address these issues, we propose a difficulty-aware prompting (DAP) method to dynamically shorten reasoning traces without performance loss. In our approach, a large teacher model first judges each problem's difficulty and then rewrites its reasoning traces to an appropriate shorter length, yielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we curate a distilled dataset called LiteCoT consisting of 100K concise reasoning examples, with solutions averaging only 720 tokens (an order of magnitude shorter than typical CoTs). Using LiteCoT, we distilled a new family of reasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5 architecture. Experiments show that a student model fine-tuned on just 100K of these difficulty-pruned CoT samples outperforms a model distilled on 800K original Long CoT samples, while significantly reducing training and inference costs. Our method also generalizes well: across 11 diverse benchmarks, the shorter difficulty-aware CoTs achieve equal or better accuracy than Long chains, using far fewer tokens. For example, on the challenging AIME24 exam, our approach reaches $74.2\\%$ Pass@1 using only about 5K inference tokens, surpassing other methods that consume many more tokens. Our code and data are available at https://github.com/Evanwu1125/LiteCoT.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19716v1",
    "published_date": "2025-05-26 09:04:44 UTC",
    "updated_date": "2025-05-26 09:04:44 UTC"
  },
  {
    "arxiv_id": "2505.19715v1",
    "title": "Graceful Forgetting in Generative Language Models",
    "authors": [
      "Chunyang Jiang",
      "Chi-min Chan",
      "Yiyang Cai",
      "Yulong Liu",
      "Wei Xue",
      "Yike Guo"
    ],
    "abstract": "Recently, the pretrain-finetune paradigm has become a cornerstone in various deep learning areas. While in general the pre-trained model would promote both effectiveness and efficiency of downstream tasks fine-tuning, studies have shown that not all knowledge acquired during pre-training is beneficial. Some of the knowledge may actually bring detrimental effects to the fine-tuning tasks, which is also known as negative transfer. To address this problem, graceful forgetting has emerged as a promising approach. The core principle of graceful forgetting is to enhance the learning plasticity of the target task by selectively discarding irrelevant knowledge. However, this approach remains underexplored in the context of generative language models, and it is often challenging to migrate existing forgetting algorithms to these models due to architecture incompatibility. To bridge this gap, in this paper we propose a novel framework, Learning With Forgetting (LWF), to achieve graceful forgetting in generative language models. With Fisher Information Matrix weighting the intended parameter updates, LWF computes forgetting confidence to evaluate self-generated knowledge regarding the forgetting task, and consequently, knowledge with high confidence is periodically unlearned during fine-tuning. Our experiments demonstrate that, although thoroughly uncovering the mechanisms of knowledge interaction remains challenging in pre-trained language models, applying graceful forgetting can contribute to enhanced fine-tuning performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.19715v1",
    "published_date": "2025-05-26 09:03:57 UTC",
    "updated_date": "2025-05-26 09:03:57 UTC"
  },
  {
    "arxiv_id": "2505.19714v1",
    "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning",
    "authors": [
      "Zhaopeng Feng",
      "Yupu Liang",
      "Shaosheng Cao",
      "Jiayuan Su",
      "Jiahan Ren",
      "Zhe Xu",
      "Yao Hu",
      "Wenxuan Huang",
      "Jian Wu",
      "Zuozhu Liu"
    ],
    "abstract": "Text Image Machine Translation (TIMT)-the task of translating textual content embedded in images-is critical for applications in accessibility, cross-lingual information access, and real-world document understanding. However, TIMT remains a complex challenge due to the need for accurate optical character recognition (OCR), robust visual-text reasoning, and high-quality translation, often requiring cascading multi-stage pipelines. Recent advances in large-scale Reinforcement Learning (RL) have improved reasoning in Large Language Models (LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is still underexplored. To bridge this gap, we introduce MT$^{3}$, the first framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts a multi-task optimization paradigm targeting three key sub-skills: text recognition, context-aware reasoning, and translation. It is trained using a novel multi-mixed reward mechanism that adapts rule-based RL strategies to TIMT's intricacies, offering fine-grained, non-binary feedback across tasks. Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural and real-world social media contexts, we introduced XHSPost, the first social media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on the latest in-domain MIT-10M benchmark, outperforming strong baselines such as Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics. Additionally, the model shows strong generalization to out-of-distribution language pairs and datasets. In-depth analyses reveal how multi-task synergy, reinforcement learning initialization, curriculum design, and reward formulation contribute to advancing MLLM-driven TIMT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2505.19714v1",
    "published_date": "2025-05-26 09:02:35 UTC",
    "updated_date": "2025-05-26 09:02:35 UTC"
  },
  {
    "arxiv_id": "2505.20362v1",
    "title": "VSCBench: Bridging the Gap in Vision-Language Model Safety Calibration",
    "authors": [
      "Jiahui Geng",
      "Qing Li",
      "Zongxiong Chen",
      "Yuxia Wang",
      "Derui Zhu",
      "Zhuohan Xie",
      "Chenyang Lyu",
      "Xiuying Chen",
      "Preslav Nakov",
      "Fakhri Karray"
    ],
    "abstract": "The rapid advancement of vision-language models (VLMs) has brought a lot of attention to their safety alignment. However, existing methods have primarily focused on model undersafety, where the model responds to hazardous queries, while neglecting oversafety, where the model refuses to answer safe queries. In this paper, we introduce the concept of $\\textit{safety calibration}$, which systematically addresses both undersafety and oversafety. Specifically, we present $\\textbf{VSCBench}$, a novel dataset of 3,600 image-text pairs that are visually or textually similar but differ in terms of safety, which is designed to evaluate safety calibration across image-centric and text-centric scenarios. Based on our benchmark, we evaluate safety calibration across eleven widely used VLMs. Our extensive experiments revealed major issues with both undersafety and oversafety. We further investigated four approaches to improve the model's safety calibration. We found that even though some methods effectively calibrated the models' safety problems, these methods also lead to the degradation of models' utility. This trade-off underscores the urgent need for advanced calibration methods, and our benchmark provides a valuable tool for evaluating future approaches. Our code and data are available at https://github.com/jiahuigeng/VSCBench.git.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20362v1",
    "published_date": "2025-05-26 09:01:46 UTC",
    "updated_date": "2025-05-26 09:01:46 UTC"
  },
  {
    "arxiv_id": "2505.19706v1",
    "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision",
    "authors": [
      "Tej Deep Pala",
      "Panshul Sharma",
      "Amir Zadeh",
      "Chuan Li",
      "Soujanya Poria"
    ],
    "abstract": "Large Language Models (LLMs) are prone to hallucination, especially during multi-hop and reasoning-intensive tasks such as mathematical problem solving. While Outcome Reward Models verify only final answers, Process Reward Models (PRMs) score each intermediate step to steer generation toward coherent solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware discriminative PRM that first classifies math and consistency errors at each step, then combines these fine-grained signals to estimate step correctness. To train PathFinder-PRM, we construct a 400K-sample dataset by enriching the human-annotated PRM800K corpus and RLHFlow Mistral traces with three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while using 3 times less data. When applied to reward guided greedy search, our model yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results demonstrate that decoupled error detection and reward estimation not only boost fine-grained error detection but also substantially improve end-to-end, reward-guided mathematical reasoning with greater data efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "https://github.com/declare-lab/PathFinder-PRM",
    "pdf_url": "https://arxiv.org/pdf/2505.19706v1",
    "published_date": "2025-05-26 08:56:36 UTC",
    "updated_date": "2025-05-26 08:56:36 UTC"
  },
  {
    "arxiv_id": "2505.19700v4",
    "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models",
    "authors": [
      "Yi Liu",
      "Dianqing Liu",
      "Mingye Zhu",
      "Junbo Guo",
      "Yongdong Zhang",
      "Zhendong Mao"
    ],
    "abstract": "The widespread adoption of large language models (LLMs) across industries has increased the demand for high-quality and customizable outputs. However, traditional alignment methods often require retraining large pretrained models, making it difficult to quickly adapt and optimize LLMs for diverse applications. To address this limitation, we propose a novel \\textit{Residual Alignment Model} (\\textit{RAM}) that formalizes the alignment process as a type of importance sampling. In this framework, the unaligned upstream model serves as the proposal distribution, while the alignment process is framed as secondary sampling based on an autoregressive alignment module that acts as an estimator of the importance weights. This design enables a natural detachment of the alignment module from the target aligned model, improving flexibility and scalability. Based on this model, we derive an efficient sequence-level training strategy for the alignment module, which operates independently of the proposal module. Additionally, we develop a resampling algorithm with iterative token-level decoding to address the common first-token latency issue in comparable methods. Experimental evaluations on two leading open-source LLMs across diverse tasks, including instruction following, domain adaptation, and preference optimization, demonstrate that our approach consistently outperforms baseline models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NeurIPS 2025, 28 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.19700v4",
    "published_date": "2025-05-26 08:53:02 UTC",
    "updated_date": "2025-12-15 06:09:56 UTC"
  },
  {
    "arxiv_id": "2505.19699v1",
    "title": "Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments",
    "authors": [
      "Junming Liu",
      "Yanting Gao",
      "Siyuan Meng",
      "Yifei Sun",
      "Aoqi Wu",
      "Yufei Jin",
      "Yirong Chen",
      "Ding Wang",
      "Guosun Zeng"
    ],
    "abstract": "Federated Learning (FL) is a decentralized machine learning paradigm that enables clients to collaboratively train models while preserving data privacy. However, the coexistence of model and data heterogeneity gives rise to inconsistent representations and divergent optimization dynamics across clients, ultimately hindering robust global performance. To transcend these challenges, we propose Mosaic, a novel data-free knowledge distillation framework tailored for heterogeneous distributed environments. Mosaic first trains local generative models to approximate each client's personalized distribution, enabling synthetic data generation that safeguards privacy through strict separation from real data. Subsequently, Mosaic forms a Mixture-of-Experts (MoE) from client models based on their specialized knowledge, and distills it into a global model using the generated data. To further enhance the MoE architecture, Mosaic integrates expert predictions via a lightweight meta model trained on a few representative prototypes. Extensive experiments on standard image classification benchmarks demonstrate that Mosaic consistently outperforms state-of-the-art approaches under both model and data heterogeneity. The source code has been published at https://github.com/Wings-Of-Disaster/Mosaic.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "43 pages, 23 figures, 15 tables; the last dance",
    "pdf_url": "https://arxiv.org/pdf/2505.19699v1",
    "published_date": "2025-05-26 08:52:49 UTC",
    "updated_date": "2025-05-26 08:52:49 UTC"
  },
  {
    "arxiv_id": "2505.19698v2",
    "title": "JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning",
    "authors": [
      "Jing Yu Lim",
      "Zarif Ikram",
      "Samson Yu",
      "Haozhe Ma",
      "Tze-Yun Leong",
      "Dianbo Liu"
    ],
    "abstract": "Recent advances in model-based reinforcement learning (MBRL) have achieved super-human level performance on the Atari100k benchmark, driven by reinforcement learning agents trained on powerful diffusion world models. However, we identify that the current aggregates mask a major performance asymmetry: MBRL agents dramatically outperform humans in some tasks despite drastically underperforming in others, with the former inflating the aggregate metrics. This is especially pronounced in pixel-based agents trained with diffusion world models. In this work, we address the pronounced asymmetry observed in pixel-based agents as an initial attempt to reverse the worrying upward trend observed in them. We address the problematic aggregates by delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal importance on metrics from both sets. Next, we hypothesize this pronounced asymmetry is due to the lack of temporally-structured latent space trained with the World Model objective in pixel-based methods. Lastly, to address this issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion world model trained end-to-end with the self-consistency objective. JEDI outperforms SOTA models in human-optimal tasks while staying competitive across the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the latest pixel-based diffusion baseline. Overall, our work rethinks what it truly means to cross human-level performance in Atari100k.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2505.19698v2",
    "published_date": "2025-05-26 08:52:45 UTC",
    "updated_date": "2025-05-28 08:56:53 UTC"
  },
  {
    "arxiv_id": "2505.19693v2",
    "title": "EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical Representation with Auxiliary Classification",
    "authors": [
      "Deok-Hyeon Cho",
      "Hyung-Seok Oh",
      "Seung-Bin Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Speech emotion recognition predicts a speaker's emotional state from speech signals using discrete labels or continuous dimensions such as arousal, valence, and dominance (VAD). We propose EmoSphere-SER, a joint model that integrates spherical VAD region classification to guide VAD regression for improved emotion prediction. In our framework, VAD values are transformed into spherical coordinates that are divided into multiple spherical regions, and an auxiliary classification task predicts which spherical region each point belongs to, guiding the regression process. Additionally, we incorporate a dynamic weighting scheme and a style pooling layer with multi-head self-attention to capture spectral and temporal dynamics, further boosting performance. This combined training strategy reinforces structured learning and improves prediction consistency. Experimental results show that our approach exceeds baseline methods, confirming the validity of the proposed framework.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Proceedings of Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19693v2",
    "published_date": "2025-05-26 08:50:23 UTC",
    "updated_date": "2025-10-17 01:34:55 UTC"
  },
  {
    "arxiv_id": "2505.19690v1",
    "title": "Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models",
    "authors": [
      "Baihui Zheng",
      "Boren Zheng",
      "Kerui Cao",
      "Yingshui Tan",
      "Zhendong Liu",
      "Weixun Wang",
      "Jiaheng Liu",
      "Jian Yang",
      "Wenbo Su",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Kaifu Zhang"
    ],
    "abstract": "Despite the remarkable proficiency of \\textit{Large Reasoning Models} (LRMs) in handling complex reasoning tasks, their reliability in safety-critical scenarios remains uncertain. Existing evaluations primarily assess response-level safety, neglecting a critical issue we identify as \\textbf{\\textit{Superficial Safety Alignment} (SSA)} -- a phenomenon where models produce superficially safe outputs while internal reasoning processes fail to genuinely detect and mitigate underlying risks, resulting in inconsistent safety behaviors across multiple sampling attempts. To systematically investigate SSA, we introduce \\textbf{Beyond Safe Answers (BSA)} bench, a novel benchmark comprising 2,000 challenging instances organized into three distinct SSA scenario types and spanning nine risk categories, each meticulously annotated with risk rationales. Evaluations of 19 state-of-the-art LRMs demonstrate the difficulty of this benchmark, with top-performing models achieving only 38.0\\% accuracy in correctly identifying risk rationales. We further explore the efficacy of safety rules, specialized fine-tuning on safety reasoning data, and diverse decoding strategies in mitigating SSA. Our work provides a comprehensive assessment tool for evaluating and improving safety reasoning fidelity in LRMs, advancing the development of genuinely risk-aware and reliably safe AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19690v1",
    "published_date": "2025-05-26 08:49:19 UTC",
    "updated_date": "2025-05-26 08:49:19 UTC"
  },
  {
    "arxiv_id": "2505.19687v2",
    "title": "DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech",
    "authors": [
      "Deok-Hyeon Cho",
      "Hyung-Seok Oh",
      "Seung-Bin Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Cross-speaker emotion transfer in speech synthesis relies on extracting speaker-independent emotion embeddings for accurate emotion modeling without retaining speaker traits. However, existing timbre compression methods fail to fully separate speaker and emotion characteristics, causing speaker leakage and degraded synthesis quality. To address this, we propose DiEmo-TTS, a self-supervised distillation method to minimize emotional information loss and preserve speaker identity. We introduce cluster-driven sampling and information perturbation to preserve emotion while removing irrelevant factors. To facilitate this process, we propose an emotion clustering and matching approach using emotional attribute prediction and speaker embeddings, enabling generalization to unlabeled data. Additionally, we designed a dual conditioning transformer to integrate style features better. Experimental results confirm the effectiveness of our method in learning speaker-irrelevant emotion embeddings.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Proceedings of Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19687v2",
    "published_date": "2025-05-26 08:47:39 UTC",
    "updated_date": "2025-10-17 01:32:30 UTC"
  },
  {
    "arxiv_id": "2505.19683v1",
    "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey",
    "authors": [
      "Pengfei Cao",
      "Tianyi Men",
      "Wencan Liu",
      "Jingwen Zhang",
      "Xuzhao Li",
      "Xixun Lin",
      "Dianbo Sui",
      "Yanan Cao",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Planning represents a fundamental capability of intelligent agents, requiring comprehensive environmental understanding, rigorous logical reasoning, and effective sequential decision-making. While Large Language Models (LLMs) have demonstrated remarkable performance on certain planning tasks, their broader application in this domain warrants systematic investigation. This paper presents a comprehensive review of LLM-based planning. Specifically, this survey is structured as follows: First, we establish the theoretical foundations by introducing essential definitions and categories about automated planning. Next, we provide a detailed taxonomy and analysis of contemporary LLM-based planning methodologies, categorizing them into three principal approaches: 1) External Module Augmented Methods that combine LLMs with additional components for planning, 2) Finetuning-based Methods that involve using trajectory data and feedback signals to adjust LLMs in order to improve their planning abilities, and 3) Searching-based Methods that break down complex tasks into simpler components, navigate the planning space, or enhance decoding strategies to find the best solutions. Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Finally, we discuss the underlying mechanisms enabling LLM-based planning and outline promising research directions for this rapidly evolving field. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this field.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19683v1",
    "published_date": "2025-05-26 08:44:53 UTC",
    "updated_date": "2025-05-26 08:44:53 UTC"
  },
  {
    "arxiv_id": "2505.23793v1",
    "title": "USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models",
    "authors": [
      "Baolin Zheng",
      "Guanlin Chen",
      "Hongqiong Zhong",
      "Qingyang Teng",
      "Yingshui Tan",
      "Zhendong Liu",
      "Weixun Wang",
      "Jiaheng Liu",
      "Jian Yang",
      "Huiyun Jing",
      "Jincheng Wei",
      "Wenbo Su",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Kaifu Zhang"
    ],
    "abstract": "Despite their remarkable achievements and widespread adoption, Multimodal Large Language Models (MLLMs) have revealed significant security vulnerabilities, highlighting the urgent need for robust safety evaluation benchmarks. Existing MLLM safety benchmarks, however, fall short in terms of data quality and coverge, and modal risk combinations, resulting in inflated and contradictory evaluation results, which hinders the discovery and governance of security concerns. Besides, we argue that vulnerabilities to harmful queries and oversensitivity to harmless ones should be considered simultaneously in MLLMs safety evaluation, whereas these were previously considered separately. In this paper, to address these shortcomings, we introduce Unified Safety Benchmarks (USB), which is one of the most comprehensive evaluation benchmarks in MLLM safety. Our benchmark features high-quality queries, extensive risk categories, comprehensive modal combinations, and encompasses both vulnerability and oversensitivity evaluations. From the perspective of two key dimensions: risk categories and modality combinations, we demonstrate that the available benchmarks -- even the union of the vast majority of them -- are far from being truly comprehensive. To bridge this gap, we design a sophisticated data synthesis pipeline that generates extensive, high-quality complementary data addressing previously unexplored aspects. By combining open-source datasets with our synthetic data, our benchmark provides 4 distinct modality combinations for each of the 61 risk sub-categories, covering both English and Chinese across both vulnerability and oversensitivity dimensions.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23793v1",
    "published_date": "2025-05-26 08:39:14 UTC",
    "updated_date": "2025-05-26 08:39:14 UTC"
  },
  {
    "arxiv_id": "2505.19679v2",
    "title": "KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization",
    "authors": [
      "Zhaolin Li",
      "Yining Liu",
      "Danni Liu",
      "Tuan Nam Nguyen",
      "Enes Yavuz Ugan",
      "Tu Anh Dinh",
      "Carlos Mullov",
      "Alexander Waibel",
      "Jan Niehues"
    ],
    "abstract": "This paper presents KIT's submissions to the IWSLT 2025 low-resource track. We develop both cascaded systems, consisting of Automatic Speech Recognition (ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech Translation (ST) systems for three language pairs: Bemba, North Levantine Arabic, and Tunisian Arabic into English. Building upon pre-trained models, we fine-tune our systems with different strategies to utilize resources efficiently. This study further explores system enhancement with synthetic data and model regularization. Specifically, we investigate MT-augmented ST by generating translations from ASR data using MT models. For North Levantine, which lacks parallel ST training data, a system trained solely on synthetic data slightly surpasses the cascaded system trained on real data. We also explore augmentation using text-to-speech models by generating synthetic speech from MT data, demonstrating the benefits of synthetic data in improving both ASR and ST performance for Bemba. Additionally, we apply intra-distillation to enhance model performance. Our experiments show that this approach consistently improves results across ASR, MT, and ST tasks, as well as across different pre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine the cascaded and end-to-end systems, achieving an improvement of approximately 1.5 BLEU points.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19679v2",
    "published_date": "2025-05-26 08:38:02 UTC",
    "updated_date": "2025-11-02 11:58:25 UTC"
  },
  {
    "arxiv_id": "2505.19676v3",
    "title": "Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models",
    "authors": [
      "Lachlan McGinness",
      "Peter Baumgartner"
    ],
    "abstract": "Empirical methods to examine the capability of Large Language Models (LLMs) to use Automated Theorem Prover (ATP) reasoning strategies are studied. We evaluate the performance of State of the Art models from December 2023 and August 2024 on PRONTOQA steamroller reasoning problems. For that, we develop methods for assessing LLM response accuracy and correct answer correlation.\n  Our results show that progress in improving LLM reasoning abilities has stalled over the nine month period. By tracking completion tokens, we show that almost all improvement in reasoning ability since GPT-4 was released can be attributed to either hidden system prompts or the training of models to automatically use generic Chain of Thought prompting strategies. Among the ATP reasoning strategies tried, we found that current frontier LLMs are best able to follow the bottom-up (also known as forward-chaining) strategy. A low positive correlation was found between an LLM response containing correct reasoning and arriving at the correct conclusion.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The original version of this article was withdrawn because there were errors in the evaluation of model faithfulness to reasoning strategies and completeness of reasoning. The analysis was re-conducted correctly and version two contains the corrections",
    "pdf_url": "https://arxiv.org/pdf/2505.19676v3",
    "published_date": "2025-05-26 08:34:07 UTC",
    "updated_date": "2025-09-17 01:43:14 UTC"
  },
  {
    "arxiv_id": "2505.19675v2",
    "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement",
    "authors": [
      "Liqin Ye",
      "Agam Shah",
      "Chao Zhang",
      "Sudheer Chava"
    ],
    "abstract": "The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the model's generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifier's prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks. Our code is available on Github.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at KDD'25",
    "pdf_url": "https://arxiv.org/pdf/2505.19675v2",
    "published_date": "2025-05-26 08:31:55 UTC",
    "updated_date": "2025-06-20 16:24:07 UTC"
  },
  {
    "arxiv_id": "2505.19671v1",
    "title": "Automated evaluation of children's speech fluency for low-resource languages",
    "authors": [
      "Bowen Zhang",
      "Nur Afiqah Abdul Latiff",
      "Justin Kan",
      "Rong Tong",
      "Donny Soh",
      "Xiaoxiao Miao",
      "Ian McLoughlin"
    ],
    "abstract": "Assessment of children's speaking fluency in education is well researched for majority languages, but remains highly challenging for low resource languages. This paper proposes a system to automatically assess fluency by combining a fine-tuned multilingual ASR model, an objective metrics extraction stage, and a generative pre-trained transformer (GPT) network. The objective metrics include phonetic and word error rates, speech rate, and speech-pause duration ratio. These are interpreted by a GPT-based classifier guided by a small set of human-evaluated ground truth examples, to score fluency. We evaluate the proposed system on a dataset of children's speech in two low-resource languages, Tamil and Malay and compare the classification performance against Random Forest and XGBoost, as well as using ChatGPT-4o to predict fluency directly from speech input. Results demonstrate that the proposed approach achieves significantly higher accuracy than multimodal GPT or other methods.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 2 figures, conference",
    "pdf_url": "https://arxiv.org/pdf/2505.19671v1",
    "published_date": "2025-05-26 08:25:50 UTC",
    "updated_date": "2025-05-26 08:25:50 UTC"
  },
  {
    "arxiv_id": "2505.19667v2",
    "title": "LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation",
    "authors": [
      "Weikang Yuan",
      "Kaisong Song",
      "Zhuoren Jiang",
      "Junjie Cao",
      "Yujie Zhang",
      "Jun Lin",
      "Kun Kuang",
      "Ji Zhang",
      "Xiaozhong Liu"
    ],
    "abstract": "Legal consultation is essential for safeguarding individual rights and ensuring access to justice, yet remains costly and inaccessible to many individuals due to the shortage of professionals. While recent advances in Large Language Models (LLMs) offer a promising path toward scalable, low-cost legal assistance, current systems fall short in handling the interactive and knowledge-intensive nature of real-world consultations. To address these challenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset comprising 3,696 legal consultation dialogues with 110,008 dialogue turns, designed to evaluate and improve LLMs' legal consultation capability. With LeCoDe, we innovatively collect live-streamed consultations from short-video platforms, providing authentic multi-turn legal consultation dialogues. The rigorous annotation by legal experts further enhances the dataset with professional insights and expertise. Furthermore, we propose a comprehensive evaluation framework that assesses LLMs' consultation capabilities in terms of (1) clarification capability and (2) professional advice quality. This unified framework incorporates 12 metrics across two dimensions. Through extensive experiments on various general and domain-specific LLMs, our results reveal significant challenges in this task, with even state-of-the-art models like GPT-4 achieving only 39.8% recall for clarification and 59% overall score for advice quality, highlighting the complexity of professional consultation scenarios. Based on these findings, we further explore several strategies to enhance LLMs' legal consultation abilities. Our benchmark contributes to advancing research in legal domain dialogue systems, particularly in simulating more real-world user-expert interactions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19667v2",
    "published_date": "2025-05-26 08:24:32 UTC",
    "updated_date": "2025-10-23 06:01:15 UTC"
  },
  {
    "arxiv_id": "2505.19663v2",
    "title": "A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?",
    "authors": [
      "Yigitcan Özer",
      "Woosung Choi",
      "Joan Serrà",
      "Mayank Kumar Singh",
      "Wei-Hsiang Liao",
      "Yuki Mitsufuji"
    ],
    "abstract": "We introduce the Robust Audio Watermarking Benchmark (RAW-Bench), a benchmark for evaluating deep learning-based audio watermarking methods with standardized and systematic comparisons. To simulate real-world usage, we introduce a comprehensive audio attack pipeline with various distortions such as compression, background noise, and reverberation, along with a diverse test dataset including speech, environmental sounds, and music recordings. Evaluating four existing watermarking methods on RAW-bench reveals two main insights: (i) neural compression techniques pose the most significant challenge, even when algorithms are trained with such compressions; and (ii) training with audio attacks generally improves robustness, although it is insufficient in some cases. Furthermore, we find that specific distortions, such as polarity inversion, time stretching, or reverb, seriously affect certain methods. The evaluation framework is accessible at github.com/SonyResearch/raw_bench.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages; 5 tables; accepted at INTERSPEECH 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19663v2",
    "published_date": "2025-05-26 08:21:58 UTC",
    "updated_date": "2025-05-28 06:20:39 UTC"
  },
  {
    "arxiv_id": "2505.19662v2",
    "title": "FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks",
    "authors": [
      "Atsunori Moteki",
      "Shoichi Masui",
      "Fan Yang",
      "Yueqi Song",
      "Yonatan Bisk",
      "Graham Neubig",
      "Ikuo Kusajima",
      "Yasuto Watanabe",
      "Hiroyuki Ishida",
      "Jun Takahashi",
      "Shan Jiang"
    ],
    "abstract": "This paper proposes FieldWorkArena, a benchmark for agentic AI targeting real-world field work. With the recent increase in demand for agentic AI, they are required to monitor and report safety and health incidents, as well as manufacturing-related incidents, that may occur in real-world work environments. Existing agentic AI benchmarks have been limited to evaluating web tasks and are insufficient for evaluating agents in real-world work environments, where complexity increases significantly. In this paper, we define a new action space that agentic AI should possess for real world work environment benchmarks and improve the evaluation function from previous methods to assess the performance of agentic AI in diverse real-world tasks. The dataset consists of videos captured on-site and documents actually used in factories and warehouses, and tasks were created based on interviews with on-site workers and managers. Evaluation results confirmed that performance evaluation considering the characteristics of Multimodal LLM (MLLM) such as GPT-4o is feasible. Additionally, the effectiveness and limitations of the proposed new evaluation method were identified. The complete dataset (HuggingFace) and evaluation program (GitHub) can be downloaded from the following website: https://en-documents.research.global.fujitsu.com/fieldworkarena/.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 2 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.19662v2",
    "published_date": "2025-05-26 08:21:46 UTC",
    "updated_date": "2025-05-30 08:11:14 UTC"
  },
  {
    "arxiv_id": "2505.19660v3",
    "title": "Prompting is not Enough: Exploring Knowledge Integration and Controllable Generation",
    "authors": [
      "Tingjia Shen",
      "Hao Wang",
      "Chuan Qin",
      "Ruijun Sun",
      "Yang Song",
      "Defu Lian",
      "Hengshu Zhu",
      "Enhong Chen"
    ],
    "abstract": "Open-domain question answering (OpenQA) represents a cornerstone in natural language processing (NLP), primarily focused on extracting answers from unstructured textual data. With the rapid advancements in Large Language Models (LLMs), LLM-based OpenQA methods have reaped the benefits of emergent understanding and answering capabilities enabled by massive parameters compared to traditional methods. However, most of these methods encounter two critical challenges: how to integrate knowledge into LLMs effectively and how to adaptively generate results with specific answer formats for various task situations. To address these challenges, we propose a novel framework named GenKI, which aims to improve the OpenQA performance by exploring Knowledge Integration and controllable Generation on LLMs simultaneously. Specifically, we first train a dense passage retrieval model to retrieve associated knowledge from a given knowledge base. Subsequently, we introduce a novel knowledge integration model that incorporates the retrieval knowledge into instructions during fine-tuning to intensify the model. Furthermore, to enable controllable generation in LLMs, we leverage a certain fine-tuned LLM and an ensemble based on text consistency incorporating all coherence, fluency, and answer format assurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO, and CMRC2018 datasets, featuring diverse answer formats, have demonstrated the effectiveness of GenKI with comparison of state-of-the-art baselines. Moreover, ablation studies have disclosed a linear relationship between the frequency of retrieved knowledge and the model's ability to recall knowledge accurately against the ground truth. Our code of GenKI is available at https://github.com/USTC-StarTeam/GenKI",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.19660v3",
    "published_date": "2025-05-26 08:18:33 UTC",
    "updated_date": "2025-10-27 14:08:24 UTC"
  },
  {
    "arxiv_id": "2505.19658v1",
    "title": "Large Language Models in Code Co-generation for Safe Autonomous Vehicles",
    "authors": [
      "Ali Nouri",
      "Beatriz Cabrero-Daniel",
      "Zhennan Fei",
      "Krishna Ronanki",
      "Håkan Sivencrona",
      "Christian Berger"
    ],
    "abstract": "Software engineers in various industrial domains are already using Large Language Models (LLMs) to accelerate the process of implementing parts of software systems. When considering its potential use for ADAS or AD systems in the automotive context, there is a need to systematically assess this new setup: LLMs entail a well-documented set of risks for safety-related systems' development due to their stochastic nature. To reduce the effort for code reviewers to evaluate LLM-generated code, we propose an evaluation pipeline to conduct sanity-checks on the generated code. We compare the performance of six state-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders, Mistral, and GPT-4) on four safety-related programming tasks. Additionally, we qualitatively analyse the most frequent faults generated by these LLMs, creating a failure-mode catalogue to support human reviewers. Finally, the limitations and capabilities of LLMs in code generation, and the use of the proposed pipeline in the existing process, are discussed.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted in the 44th International Conference on Computer Safety, Reliability and Security (SafeComp 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.19658v1",
    "published_date": "2025-05-26 08:18:30 UTC",
    "updated_date": "2025-05-26 08:18:30 UTC"
  },
  {
    "arxiv_id": "2505.19653v1",
    "title": "Token-Importance Guided Direct Preference Optimization",
    "authors": [
      "Ning Yang",
      "Hai Lin",
      "Yibo Liu",
      "Baoliang Tian",
      "Guoqing Liu",
      "Haijun Zhang"
    ],
    "abstract": "Ensuring that large language models (LLMs) generate outputs aligned with human preferences is important for safe and effective AI interactions. While Direct Preference Optimization (DPO) employs an implicit reward function to optimize the policy model, however, it and its related variants overlook the differential importance of individual tokens and are sensitive to judgment noise in preference datasets during generation. Although recent methods attempt to assess the important weight of tokens via probability prediction or simplistic weighting schemes, these evaluation methods are prone to biases and still cannot fully address these issues. To solve this problem, we propose the Token-Importance Guided Direct Preference Optimization (TI-DPO), which introduces two key innovations: the gradient-based token-importance weights that dynamically prioritize critical tokens, and a triple loss that explicitly guides model outputs to approach human-preferred responses and stay away from non-preferred responses. Experimental results show that TI-DPO achieves higher accuracy and stronger generative diversity, providing more stable and computationally efficient solutions compared with DPO and other RLHF methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19653v1",
    "published_date": "2025-05-26 08:11:24 UTC",
    "updated_date": "2025-05-26 08:11:24 UTC"
  },
  {
    "arxiv_id": "2505.19648v1",
    "title": "Model Enumeration of Two-Variable Logic with Quadratic Delay Complexity",
    "authors": [
      "Qiaolan Meng",
      "Juhua Pu",
      "Hongting Niu",
      "Yuyi Wang",
      "Yuanhong Wang",
      "Ondřej Kuželka"
    ],
    "abstract": "We study the model enumeration problem of the function-free, finite domain fragment of first-order logic with two variables ($FO^2$). Specifically, given an $FO^2$ sentence $Γ$ and a positive integer $n$, how can one enumerate all the models of $Γ$ over a domain of size $n$? In this paper, we devise a novel algorithm to address this problem. The delay complexity, the time required between producing two consecutive models, of our algorithm is quadratic in the given domain size $n$ (up to logarithmic factors) when the sentence is fixed. This complexity is almost optimal since the interpretation of binary predicates in any model requires at least $Ω(n^2)$ bits to represent.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "16 pages, 4 figures and to be published in Fortieth Annual ACM/IEEE Symposium on Logic in Computer Science (LICS)",
    "pdf_url": "https://arxiv.org/pdf/2505.19648v1",
    "published_date": "2025-05-26 08:04:19 UTC",
    "updated_date": "2025-05-26 08:04:19 UTC"
  },
  {
    "arxiv_id": "2505.19645v3",
    "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE",
    "authors": [
      "Zongle Huang",
      "Lei Zhu",
      "Zongyuan Zhan",
      "Ting Hu",
      "Weikai Mao",
      "Xianzhi Yu",
      "Yongpan Liu",
      "Tianyu Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across many applications, with Mixture of Experts (MoE) models demonstrating great potential. Compared to traditional dense models, MoEs achieve better performance with less computation. Speculative decoding (SD) is a widely used technique to accelerate LLM inference without accuracy loss, but it has been considered efficient only for dense models. In this work, we first demonstrate that, under medium batch sizes, MoE surprisingly benefits more from SD than dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in MoE designs -- the batch size range where SD acceleration is expected to be effective becomes broader. To quantitatively understand tradeoffs involved in SD, we develop a reliable modeling based on theoretical analyses. While current SD research primarily focuses on improving acceptance rates of algorithms, changes in workload and model architecture can still lead to degraded SD acceleration even with high acceptance rates. To address this limitation, we introduce a new metric 'target efficiency' that characterizes these effects, thus helping researchers identify system bottlenecks and understand SD acceleration more comprehensively. For scenarios like private serving, this work unveils a new perspective to speed up MoE inference, where existing solutions struggle. Experiments on different GPUs show up to 2.29x speedup for Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19645v3",
    "published_date": "2025-05-26 08:01:45 UTC",
    "updated_date": "2025-10-06 10:53:42 UTC"
  },
  {
    "arxiv_id": "2505.20359v2",
    "title": "Risk-aware Direct Preference Optimization under Nested Risk Measure",
    "authors": [
      "Lijun Zhang",
      "Lin Li",
      "Yajie Qi",
      "Huizhong Song",
      "Yaodong Yang",
      "Jun Wang",
      "Wei Wei"
    ],
    "abstract": "When fine-tuning pre-trained Large Language Models (LLMs) to align with human values and intentions, maximizing the estimated reward can lead to superior performance, but it also introduces potential risks due to deviations from the reference model's intended behavior. Most existing methods typically introduce KL divergence to constrain deviations between the trained model and the reference model; however, this may not be sufficient in certain applications that require tight risk control. In this paper, we introduce Risk-aware Direct Preference Optimization (Ra-DPO), a novel approach that incorporates risk-awareness by employing a class of nested risk measures. This approach formulates a constrained risk-aware advantage function maximization problem and then converts the Bradley-Terry model into a token-level representation. The objective function maximizes the likelihood of the policy while suppressing the deviation between a trained model and the reference model using a sequential risk ratio, thereby enhancing the model's risk-awareness. Experimental results across three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and AlpacaEval, demonstrate the proposed method's superior performance in balancing alignment performance and model drift. Our code is opensourced at https://github.com/zlj123-max/Ra-DPO.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20359v2",
    "published_date": "2025-05-26 08:01:37 UTC",
    "updated_date": "2025-05-29 13:19:08 UTC"
  },
  {
    "arxiv_id": "2505.19644v3",
    "title": "STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set Source Tracing and Attribution",
    "authors": [
      "Anton Firc",
      "Manasi Chhibber",
      "Jagabandhu Mishra",
      "Vishwanath Pratap Singh",
      "Tomi Kinnunen",
      "Kamil Malinka"
    ],
    "abstract": "A key research area in deepfake speech detection is source tracing - determining the origin of synthesised utterances. The approaches may involve identifying the acoustic model (AM), vocoder model (VM), or other generation-specific parameters. However, progress is limited by the lack of a dedicated, systematically curated dataset. To address this, we introduce STOPA, a systematically varied and metadata-rich dataset for deepfake speech source tracing, covering 8 AMs, 6 VMs, and diverse parameter settings across 700k samples from 13 distinct synthesisers. Unlike existing datasets, which often feature limited variation or sparse metadata, STOPA provides a systematically controlled framework covering a broader range of generative factors, such as the choice of the vocoder model, acoustic model, or pretrained weights, ensuring higher attribution reliability. This control improves attribution accuracy, aiding forensic analysis, deepfake detection, and generative model transparency.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Published at Interspeech 2025 conference",
    "pdf_url": "https://arxiv.org/pdf/2505.19644v3",
    "published_date": "2025-05-26 08:00:30 UTC",
    "updated_date": "2025-10-09 08:21:42 UTC"
  },
  {
    "arxiv_id": "2505.19641v4",
    "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond",
    "authors": [
      "Junteng Liu",
      "Yuanxiang Fan",
      "Zhuo Jiang",
      "Han Ding",
      "Yongyi Hu",
      "Chi Zhang",
      "Yiqi Shi",
      "Shitong Weng",
      "Aili Chen",
      "Shiqi Chen",
      "Yunan Huang",
      "Mozhi Zhang",
      "Pengyu Zhao",
      "Junjie Yan",
      "Junxian He"
    ],
    "abstract": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19641v4",
    "published_date": "2025-05-26 07:59:36 UTC",
    "updated_date": "2025-06-04 05:08:08 UTC"
  },
  {
    "arxiv_id": "2505.19631v1",
    "title": "Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models",
    "authors": [
      "Zihong Zhang",
      "Liqi He",
      "Zuchao Li",
      "Lefei Zhang",
      "Hai Zhao",
      "Bo Du"
    ],
    "abstract": "Word segmentation stands as a cornerstone of Natural Language Processing (NLP). Based on the concept of \"comprehend first, segment later\", we propose a new framework to explore the limit of unsupervised word segmentation with Large Language Models (LLMs) and evaluate the semantic understanding capabilities of LLMs based on word segmentation. We employ current mainstream LLMs to perform word segmentation across multiple languages to assess LLMs' \"comprehension\". Our findings reveal that LLMs are capable of following simple prompts to segment raw text into words. There is a trend suggesting that models with more parameters tend to perform better on multiple languages. Additionally, we introduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge $\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick $\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities of Aho-Corasick automata, LLACA innovatively combines these with the deep insights of well-pretrained LLMs. This approach not only enables the construction of a dynamic $n$-gram model that adjusts based on contextual information but also integrates the nuanced understanding of LLMs, offering significant improvements over traditional methods. Our source code is available at https://github.com/hkr04/LLACA",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19631v1",
    "published_date": "2025-05-26 07:48:15 UTC",
    "updated_date": "2025-05-26 07:48:15 UTC"
  },
  {
    "arxiv_id": "2505.19625v2",
    "title": "Search-Based Software Engineering and AI Foundation Models: Current Landscape and Future Roadmap",
    "authors": [
      "Hassan Sartaj",
      "Shaukat Ali",
      "Paolo Arcaini",
      "Andrea Arcuri"
    ],
    "abstract": "Search-based software engineering (SBSE), which integrates metaheuristic search techniques with software engineering, has been an active area of research for about 25 years. It has been applied to solve numerous problems across the entire software engineering lifecycle and has demonstrated its versatility in multiple domains. With recent advances in AI, particularly the emergence of foundation models (FMs) such as large language models (LLMs), the evolution of SBSE alongside these models remains undetermined. In this window of opportunity, we present a research roadmap that articulates the current landscape of SBSE in relation to FMs, identifies open challenges, and outlines potential research directions to advance SBSE through its integration and interplay with FMs. Specifically, we analyze five core aspects: leveraging FMs for SBSE design, applying FMs to complement SBSE in SE problems, employing SBSE to address FM challenges, adapting SBSE practices for FMs tailored to SE activities, and exploring the synergistic potential between SBSE and FMs. Furthermore, we present a forward-thinking perspective that envisions the future of SBSE in the era of FMs, highlighting promising research opportunities to address challenges in emerging domains.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19625v2",
    "published_date": "2025-05-26 07:46:42 UTC",
    "updated_date": "2025-10-02 09:15:32 UTC"
  },
  {
    "arxiv_id": "2505.19624v1",
    "title": "Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat",
    "authors": [
      "Pusheng Xu",
      "Xia Gong",
      "Xiaolan Chen",
      "Weiyi Zhang",
      "Jiancheng Yang",
      "Bingjie Yan",
      "Meng Yuan",
      "Yalin Zheng",
      "Mingguang He",
      "Danli Shi"
    ],
    "abstract": "Purpose: To develop a bilingual multimodal visual question answering (VQA) benchmark for evaluating VLMs in ophthalmology. Methods: Ophthalmic image posts and associated captions published between January 1, 2016, and December 31, 2024, were collected from WeChat Official Accounts. Based on these captions, bilingual question-answer (QA) pairs in Chinese and English were generated using GPT-4o-mini. QA pairs were categorized into six subsets by question type and language: binary (Binary_CN, Binary_EN), single-choice (Single-choice_CN, Single-choice_EN), and open-ended (Open-ended_CN, Open-ended_EN). The benchmark was used to evaluate the performance of three VLMs: GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B-Instruct. Results: The final OphthalWeChat dataset included 3,469 images and 30,120 QA pairs across 9 ophthalmic subspecialties, 548 conditions, 29 imaging modalities, and 68 modality combinations. Gemini 2.0 Flash achieved the highest overall accuracy (0.548), outperforming GPT-4o (0.522, P < 0.001) and Qwen2.5-VL-72B-Instruct (0.514, P < 0.001). It also led in both Chinese (0.546) and English subsets (0.550). Subset-specific performance showed Gemini 2.0 Flash excelled in Binary_CN (0.687), Single-choice_CN (0.666), and Single-choice_EN (0.646), while GPT-4o ranked highest in Binary_EN (0.717), Open-ended_CN (BLEU-1: 0.301; BERTScore: 0.382), and Open-ended_EN (BLEU-1: 0.183; BERTScore: 0.240). Conclusions: This study presents the first bilingual VQA benchmark for ophthalmology, distinguished by its real-world context and inclusion of multiple examinations per patient. The dataset reflects authentic clinical decision-making scenarios and enables quantitative evaluation of VLMs, supporting the development of accurate, specialized, and trustworthy AI systems for eye care.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19624v1",
    "published_date": "2025-05-26 07:45:42 UTC",
    "updated_date": "2025-05-26 07:45:42 UTC"
  },
  {
    "arxiv_id": "2505.19623v2",
    "title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems",
    "authors": [
      "Yu Shang",
      "Peijie Liu",
      "Yuwei Yan",
      "Zijing Wu",
      "Leheng Sheng",
      "Yuanqing Yu",
      "Chumeng Jiang",
      "An Zhang",
      "Fengli Xu",
      "Yu Wang",
      "Min Zhang",
      "Yong Li"
    ],
    "abstract": "The emergence of agentic recommender systems powered by Large Language Models (LLMs) represents a paradigm shift in personalized recommendations, leveraging LLMs' advanced reasoning and role-playing capabilities to enable autonomous, adaptive decision-making. Unlike traditional recommendation approaches, agentic recommender systems can dynamically gather and interpret user-item interactions from complex environments, generating robust recommendation strategies that generalize across diverse scenarios. However, the field currently lacks standardized evaluation protocols to systematically assess these methods. To address this critical gap, we propose: (1) an interactive textual recommendation simulator incorporating rich user and item metadata and three typical evaluation scenarios (classic, evolving-interest, and cold-start recommendation tasks); (2) a unified modular framework for developing and studying agentic recommender systems; and (3) the first comprehensive benchmark comparing 10 classical and agentic recommendation methods. Our findings demonstrate the superiority of agentic systems and establish actionable design guidelines for their core components. The benchmark environment has been rigorously validated through an open challenge and remains publicly available with a continuously maintained leaderboard~\\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html}, fostering ongoing community engagement and reproducible research. The benchmark is available at: \\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "15 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.19623v2",
    "published_date": "2025-05-26 07:45:11 UTC",
    "updated_date": "2025-05-28 14:32:56 UTC"
  },
  {
    "arxiv_id": "2505.19621v1",
    "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models",
    "authors": [
      "George Kour",
      "Itay Nakash",
      "Ateret Anaby-Tavor",
      "Michal Shmueli-Scheuer"
    ],
    "abstract": "As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend. POBS: https://ibm.github.io/POBS",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19621v1",
    "published_date": "2025-05-26 07:41:21 UTC",
    "updated_date": "2025-05-26 07:41:21 UTC"
  },
  {
    "arxiv_id": "2505.19620v1",
    "title": "Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs",
    "authors": [
      "Jiawen Chen",
      "Qi Shao",
      "Duxin Chen",
      "Wenwu Yu"
    ],
    "abstract": "Spatio-temporal prediction is a pivotal task with broad applications in traffic management, climate monitoring, energy scheduling, etc. However, existing methodologies often struggle to balance model expressiveness and computational efficiency, especially when scaling to large real-world datasets. To tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph Separation Networks), a novel framework that decouples temporal and spatial modeling to enhance both efficiency and precision. Therein, the temporal dimension is modeled using lightweight large language models, which effectively capture low-rank temporal dynamics. Concurrently, the spatial dimension is addressed through an adaptive hypergraph neural network, which dynamically constructs hyperedges to model intricate, higher-order interactions. A carefully designed gating mechanism is integrated to seamlessly fuse temporal and spatial representations. By leveraging the fundamental principles of low-rank temporal dynamics and spatial interactions, STH-SepNet offers a pragmatic and scalable solution for spatio-temporal prediction in real-world applications. Extensive experiments on large-scale real-world datasets across multiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting predictive performance while maintaining computational efficiency. This work may provide a promising lightweight framework for spatio-temporal prediction, aiming to reduce computational demands and while enhancing predictive performance. Our code is avaliable at https://github.com/SEU-WENJIA/ST-SepNet-Lightweight-LLMs-Meet-Adaptive-Hypergraphs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19620v1",
    "published_date": "2025-05-26 07:37:39 UTC",
    "updated_date": "2025-05-26 07:37:39 UTC"
  },
  {
    "arxiv_id": "2505.19616v3",
    "title": "Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models",
    "authors": [
      "Rui Cai",
      "Bangzheng Li",
      "Xiaofei Wen",
      "Muhao Chen",
      "Zhe Zhao"
    ],
    "abstract": "Multimodal Large Language Models have demonstrated impressive capabilities across tasks, yet they often exhibit difficulty in distinguishing task-relevant from irrelevant signals -- particularly in tasks like Visual Question Answering -- which can lead to susceptibility to misleading or spurious inputs. We refer to this broader limitation as the Cross-Modality Competency Problem -- the model's inability to fairly evaluate all modalities. This vulnerability becomes more evident in modality-specific tasks -- such as image classification or pure text question answering -- where models are expected to rely solely on one modality. In such tasks, spurious information from irrelevant modalities often leads to significant performance degradation. We refer to this failure as Modality Interference, which serves as a concrete and measurable instance of the cross-modality competency problem, and we further design a perturbation-based causal diagnostic experiment to verify and quantify this problem. To mitigate modality interference, we propose a novel framework to finetune MLLMs, including perturbation-based data augmentations with both heuristic perturbations and adversarial perturbations, and a consistency regularization strategy applying on model outputs with original and perturbed inputs. Experiments on multiple benchmark datasets (image-heavy, text-heavy and multimodal tasks) and multiple model families with different scales demonstrate significant improvements in robustness and cross-modality competency, indicating our method's effectiveness in boosting unimodal reasoning ability while enhancing performance on multimodal tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19616v3",
    "published_date": "2025-05-26 07:31:32 UTC",
    "updated_date": "2025-09-26 20:54:35 UTC"
  },
  {
    "arxiv_id": "2505.19611v1",
    "title": "Align and Surpass Human Camouflaged Perception: Visual Refocus Reinforcement Fine-Tuning",
    "authors": [
      "Ruolin Shen",
      "Xiaozhong Ji",
      "Kai WU",
      "Jiangning Zhang",
      "Yijun He",
      "HaiHua Yang",
      "Xiaobin Hu",
      "Xiaoyu Sun"
    ],
    "abstract": "Current multi-modal models exhibit a notable misalignment with the human visual system when identifying objects that are visually assimilated into the background. Our observations reveal that these multi-modal models cannot distinguish concealed objects, demonstrating an inability to emulate human cognitive processes which effectively utilize foreground-background similarity principles for visual analysis. To analyze this hidden human-model visual thinking discrepancy, we build a visual system that mimicks human visual camouflaged perception to progressively and iteratively `refocus' visual concealed content. The refocus is a progressive guidance mechanism enabling models to logically localize objects in visual images through stepwise reasoning. The localization process of concealed objects requires hierarchical attention shifting with dynamic adjustment and refinement of prior cognitive knowledge. In this paper, we propose a visual refocus reinforcement framework via the policy optimization algorithm to encourage multi-modal models to think and refocus more before answering, and achieve excellent reasoning abilities to align and even surpass human camouflaged perception systems. Our extensive experiments on camouflaged perception successfully demonstrate the emergence of refocus visual phenomena, characterized by multiple reasoning tokens and dynamic adjustment of the detection box. Besides, experimental results on both camouflaged object classification and detection tasks exhibit significantly superior performance compared to Supervised Fine-Tuning (SFT) baselines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Website: \\url{https://github.com/HUuxiaobin/VRRF}",
    "pdf_url": "https://arxiv.org/pdf/2505.19611v1",
    "published_date": "2025-05-26 07:27:18 UTC",
    "updated_date": "2025-05-26 07:27:18 UTC"
  },
  {
    "arxiv_id": "2506.11058v3",
    "title": "Refactoring Codebases through Library Design",
    "authors": [
      "Ziga Kovacic",
      "Justin T. Chiu",
      "Celine Lee",
      "Wenting Zhao",
      "Kevin Ellis"
    ],
    "abstract": "Maintainable and general software allows developers to build robust applications efficiently, yet achieving these qualities often requires refactoring specialized solutions into reusable components. This challenge becomes particularly relevant as code agents become used to solve isolated one-off programming problems. We investigate code agents' capacity to refactor code in ways that support growth and reusability. We first investigate what makes a good refactoring, finding via simulation results and a human study that Minimum Description Length best correlates with preferable refactorings. We then present both a benchmark and a method for refactoring: MiniCode, a benchmark where multiple files must be refactored into a shared library, and Librarian, a sample-and-rerank method for generating reusable libraries. We compare Librarian to state-of-the-art library generation methods, and study it on real-world code bases.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "29 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.11058v3",
    "published_date": "2025-05-26 07:26:33 UTC",
    "updated_date": "2025-10-05 16:31:35 UTC"
  },
  {
    "arxiv_id": "2505.19609v2",
    "title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling",
    "authors": [
      "Hongtao Xu",
      "Wenting Shen",
      "Yuanxin Wei",
      "Ang Wang",
      "Guo Runfan",
      "Tianxing Wang",
      "Yong Li",
      "Mingzhen Li",
      "Weile Jia"
    ],
    "abstract": "Long-context supervised fine-tuning (Long-SFT) plays a vital role in enhancing the performance of large language models (LLMs) on long-context tasks. To smoothly adapt LLMs to long-context scenarios, this process typically entails training on mixed datasets containing both long and short sequences. However, this heterogeneous sequence length distribution poses significant challenges for existing training systems, as they fail to simultaneously achieve high training efficiency for both long and short sequences, resulting in sub-optimal end-to-end system performance in Long-SFT. In this paper, we present a novel perspective on data scheduling to address the challenges posed by the heterogeneous data distributions in Long-SFT. We propose Skrull, a dynamic data scheduler specifically designed for efficient long-SFT. Through dynamic data scheduling, Skrull balances the computation requirements of long and short sequences, improving overall training efficiency. Furthermore, we formulate the scheduling process as a joint optimization problem and thoroughly analyze the trade-offs involved. Based on those analysis, Skrull employs a lightweight scheduling algorithm to achieve near-zero cost online scheduling in Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art distributed training system for LLMs. Experimental results demonstrate that Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world long-SFT scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19609v2",
    "published_date": "2025-05-26 07:22:39 UTC",
    "updated_date": "2025-12-15 12:58:53 UTC"
  },
  {
    "arxiv_id": "2505.19607v1",
    "title": "Energy-based Preference Optimization for Test-time Adaptation",
    "authors": [
      "Yewon Han",
      "Seoyun Yang",
      "Taesup Kim"
    ],
    "abstract": "Test-Time Adaptation (TTA) enhances model robustness by enabling adaptation to target distributions that differ from training distributions, improving real-world generalizability. Existing TTA approaches focus on adjusting the conditional distribution; however these methods often depend on uncertain predictions in the absence of label information, leading to unreliable performance. Energy-based frameworks suggest a promising alternative to address distribution shifts without relying on uncertain predictions, instead computing the marginal distribution of target data. However, they involve the critical challenge of requiring extensive SGLD sampling, which is impractical for test-time scenarios requiring immediate adaptation. In this work, we propose Energy-based Preference Optimization for Test-time Adaptation (EPOTTA), which is based on a sampling free strategy. We first parameterize the target model using a pretrained model and residual energy function, enabling marginal likelihood maximization of target data without sampling. Building on the observation that the parameterization is mathematically equivalent to DPO objective, we then directly adapt the model to a target distribution without explicitly training the residual. Our experiments verify that EPOTTA is well-calibrated and performant while achieving computational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19607v1",
    "published_date": "2025-05-26 07:21:32 UTC",
    "updated_date": "2025-05-26 07:21:32 UTC"
  },
  {
    "arxiv_id": "2505.19601v2",
    "title": "Preference Optimization by Estimating the Ratio of the Data Distribution",
    "authors": [
      "Yeongmin Kim",
      "Heesun Bae",
      "Byeonghu Na",
      "Il-Chul Moon"
    ],
    "abstract": "Direct preference optimization (DPO) is widely used as a simple and stable method for aligning large language models (LLMs) with human preferences. This paper investigates a generalized DPO loss that enables a policy model to match the target policy from a likelihood ratio estimation perspective. The ratio of the target policy provides a unique identification of the policy distribution without relying on reward models or partition functions. This allows the generalized loss to retain both simplicity and theoretical guarantees, which prior work such as $f$-PO fails to achieve simultaneously. We propose Bregman preference optimization (BPO), a generalized framework for ratio matching that provides a family of objective functions achieving target policy optimality. BPO subsumes DPO as a special case and offers tractable forms for all instances, allowing implementation with a few lines of code. We further develop scaled Basu's power divergence (SBA), a gradient scaling method that can be used for BPO instances. The BPO framework complements other DPO variants and is applicable to target policies defined by these variants. In experiments, unlike other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a trade-off between generation fidelity and diversity, instances of BPO improve both win rate and entropy compared with DPO. When applied to Llama-3-8B-Instruct, BPO achieves state-of-the-art performance among Llama-3-8B backbones, with a 55.9\\% length-controlled win rate on AlpacaEval2. Project page: https://github.com/aailab-kaist/BPO.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19601v2",
    "published_date": "2025-05-26 07:10:53 UTC",
    "updated_date": "2025-10-25 08:32:17 UTC"
  },
  {
    "arxiv_id": "2505.19599v1",
    "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar",
    "authors": [
      "Andrew Gambardella",
      "Takeshi Kojima",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "abstract": "Typical methods for evaluating the performance of language models evaluate their ability to answer questions accurately. These evaluation metrics are acceptable for determining the extent to which language models can understand and reason about text in a general sense, but fail to capture nuanced capabilities, such as the ability of language models to recognize and obey rare grammar points, particularly in languages other than English. We measure the perplexity of language models when confronted with the \"first person psych predicate restriction\" grammar point in Japanese. Weblab is the only tested open source model in the 7-10B parameter range which consistently assigns higher perplexity to ungrammatical psych predicate sentences than grammatical ones. We give evidence that Weblab's uniformly bad tokenization is a possible root cause for its good performance, and show that Llama 3's perplexity on grammatical psych predicate sentences can be reduced by orders of magnitude (28x difference) by restricting test sentences to those with uniformly well-behaved tokenizations. We show in further experiments on machine translation tasks that language models will use alternative grammar patterns in order to produce grammatical sentences when tokenization issues prevent the most natural sentence from being output.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19599v1",
    "published_date": "2025-05-26 07:08:47 UTC",
    "updated_date": "2025-05-26 07:08:47 UTC"
  },
  {
    "arxiv_id": "2505.20356v1",
    "title": "LEGO-Compiler: Enhancing Neural Compilation Through Translation Composability",
    "authors": [
      "Shuoming Zhang",
      "Jiacheng Zhao",
      "Chunwei Xia",
      "Zheng Wang",
      "Yunji Chen",
      "Xiaobing Feng",
      "Huimin Cui"
    ],
    "abstract": "Large language models (LLMs) have the potential to revolutionize how we design and implement compilers and code translation tools. However, existing LLMs struggle to handle long and complex programs. We introduce LEGO-Compiler, a novel neural compilation system that leverages LLMs to translate high-level languages into assembly code. Our approach centers on three key innovations: LEGO translation, which decomposes the input program into manageable blocks; breaking down the complex compilation process into smaller, simpler verifiable steps by organizing it as a verifiable LLM workflow by external tests; and a feedback mechanism for self-correction. Supported by formal proofs of translation composability, LEGO-Compiler demonstrates high accuracy on multiple datasets, including over 99% on ExeBench and 97.9% on industrial-grade AnsiBench. Additionally, LEGO-Compiler has also acheived near one order-of-magnitude improvement on compilable code size scalability. This work opens new avenues for applying LLMs to system-level tasks, complementing traditional compiler technologies.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.PL",
    "comment": "30 pages, 8 figures, 4 tables. Preprint. Under review",
    "pdf_url": "https://arxiv.org/pdf/2505.20356v1",
    "published_date": "2025-05-26 07:07:54 UTC",
    "updated_date": "2025-05-26 07:07:54 UTC"
  },
  {
    "arxiv_id": "2505.19591v2",
    "title": "Multi-Agent Collaboration via Evolving Orchestration",
    "authors": [
      "Yufan Dang",
      "Chen Qian",
      "Xueheng Luo",
      "Jingru Fan",
      "Zihao Xie",
      "Ruijie Shi",
      "Weize Chen",
      "Cheng Yang",
      "Xiaoyin Che",
      "Ye Tian",
      "Xuantang Xiong",
      "Lei Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable results across diverse downstream tasks, but their monolithic nature restricts scalability and efficiency in complex problem-solving. While recent research explores multi-agent collaboration among LLMs, most approaches rely on static organizational structures that struggle to adapt as task complexity and agent numbers grow, resulting in coordination overhead and inefficiencies. To this end, we propose a puppeteer-style paradigm for LLM-based multi-agent collaboration, where a centralized orchestrator (\"puppeteer\") dynamically directs agents (\"puppets\") in response to evolving task states. This orchestrator is trained via reinforcement learning to adaptively sequence and prioritize agents, enabling flexible and evolvable collective reasoning. Experiments on closed- and open-domain scenarios show that this method achieves superior performance with reduced computational costs. Analyses further reveal that the key improvements consistently stem from the emergence of more compact, cyclic reasoning structures under the orchestrator's evolution. Our code is available at https://github.com/OpenBMB/ChatDev/tree/puppeteer.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19591v2",
    "published_date": "2025-05-26 07:02:17 UTC",
    "updated_date": "2025-10-21 07:30:02 UTC"
  },
  {
    "arxiv_id": "2505.19588v1",
    "title": "LogiCoL: Logically-Informed Contrastive Learning for Set-based Dense Retrieval",
    "authors": [
      "Yanzhen Shen",
      "Sihao Chen",
      "Xueqiang Xu",
      "Yunyi Zhang",
      "Chaitanya Malaviya",
      "Dan Roth"
    ],
    "abstract": "While significant progress has been made with dual- and bi-encoder dense retrievers, they often struggle on queries with logical connectives, a use case that is often overlooked yet important in downstream applications. Current dense retrievers struggle with such queries, such that the retrieved results do not respect the logical constraints implied in the queries. To address this challenge, we introduce LogiCoL, a logically-informed contrastive learning objective for dense retrievers. LogiCoL builds upon in-batch supervised contrastive learning, and learns dense retrievers to respect the subset and mutually-exclusive set relation between query results via two sets of soft constraints expressed via t-norm in the learning objective. We evaluate the effectiveness of LogiCoL on the task of entity retrieval, where the model is expected to retrieve a set of entities in Wikipedia that satisfy the implicit logical constraints in the query. We show that models trained with LogiCoL yield improvement both in terms of retrieval performance and logical consistency in the results. We provide detailed analysis and insights to uncover why queries with logical connectives are challenging for dense retrievers and why LogiCoL is most effective.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19588v1",
    "published_date": "2025-05-26 07:00:32 UTC",
    "updated_date": "2025-05-26 07:00:32 UTC"
  },
  {
    "arxiv_id": "2505.19578v1",
    "title": "Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing",
    "authors": [
      "Dan Peng",
      "Zhihui Fu",
      "Zewen Ye",
      "Zhuoran Song",
      "Jun Wang"
    ],
    "abstract": "Sparse attention methods exploit the inherent sparsity in attention to speed up the prefilling phase of long-context inference, mitigating the quadratic complexity of full attention computation. While existing sparse attention methods rely on predefined patterns or inaccurate estimations to approximate attention behavior, they often fail to fully capture the true dynamics of attention, resulting in reduced efficiency and compromised accuracy. Instead, we propose a highly accurate sparse attention mechanism that shares similar yet precise attention patterns across heads, enabling a more realistic capture of the dynamic behavior of attention. Our approach is grounded in two key observations: (1) attention patterns demonstrate strong inter-head similarity, and (2) this similarity remains remarkably consistent across diverse inputs. By strategically sharing computed accurate patterns across attention heads, our method effectively captures actual patterns while requiring full attention computation for only a small subset of heads. Comprehensive evaluations demonstrate that our approach achieves superior or comparable speedup relative to state-of-the-art methods while delivering the best overall accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2505.19578v1",
    "published_date": "2025-05-26 06:48:53 UTC",
    "updated_date": "2025-05-26 06:48:53 UTC"
  },
  {
    "arxiv_id": "2505.20355v2",
    "title": "GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Yeonjoon Jung",
      "Daehyun Ahn",
      "Hyungjun Kim",
      "Taesu Kim",
      "Eunhyeok Park"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.20355v2",
    "published_date": "2025-05-26 06:48:20 UTC",
    "updated_date": "2026-01-15 10:43:27 UTC"
  },
  {
    "arxiv_id": "2505.23792v1",
    "title": "Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things",
    "authors": [
      "Kai Li",
      "Conggai Li",
      "Xin Yuan",
      "Shenghong Li",
      "Sai Zou",
      "Syed Sohail Ahmed",
      "Wei Ni",
      "Dusit Niyato",
      "Abbas Jamalipour",
      "Falko Dressler",
      "Ozgur B. Akan"
    ],
    "abstract": "This paper focuses on Zero-Trust Foundation Models (ZTFMs), a novel paradigm that embeds zero-trust security principles into the lifecycle of foundation models (FMs) for Internet of Things (IoT) systems. By integrating core tenets, such as continuous verification, least privilege access (LPA), data confidentiality, and behavioral analytics into the design, training, and deployment of FMs, ZTFMs can enable secure, privacy-preserving AI across distributed, heterogeneous, and potentially adversarial IoT environments. We present the first structured synthesis of ZTFMs, identifying their potential to transform conventional trust-based IoT architectures into resilient, self-defending ecosystems. Moreover, we propose a comprehensive technical framework, incorporating federated learning (FL), blockchain-based identity management, micro-segmentation, and trusted execution environments (TEEs) to support decentralized, verifiable intelligence at the network edge. In addition, we investigate emerging security threats unique to ZTFM-enabled systems and evaluate countermeasures, such as anomaly detection, adversarial training, and secure aggregation. Through this analysis, we highlight key open research challenges in terms of scalability, secure orchestration, interpretable threat attribution, and dynamic trust calibration. This survey lays a foundational roadmap for secure, intelligent, and trustworthy IoT infrastructures powered by FMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23792v1",
    "published_date": "2025-05-26 06:44:31 UTC",
    "updated_date": "2025-05-26 06:44:31 UTC"
  },
  {
    "arxiv_id": "2505.19574v2",
    "title": "Situationally-Aware Dynamics Learning",
    "authors": [
      "Alejandro Murillo-Gonzalez",
      "Lantao Liu"
    ],
    "abstract": "Autonomous robots operating in complex, unstructured environments face significant challenges due to latent, unobserved factors that obscure their understanding of both their internal state and the external world. Addressing this challenge would enable robots to develop a more profound grasp of their operational context. To tackle this, we propose a novel framework for online learning of hidden state representations, with which the robots can adapt in real-time to uncertain and dynamic conditions that would otherwise be ambiguous and result in suboptimal or erroneous behaviors. Our approach is formalized as a Generalized Hidden Parameter Markov Decision Process, which explicitly models the influence of unobserved parameters on both transition dynamics and reward structures. Our core innovation lies in learning online the joint distribution of state transitions, which serves as an expressive representation of latent ego- and environmental-factors. This probabilistic approach supports the identification and adaptation to different operational situations, improving robustness and safety. Through a multivariate extension of Bayesian Online Changepoint Detection, our method segments changes in the underlying data generating process governing the robot's dynamics. The robot's transition model is then informed with a symbolic representation of the current situation derived from the joint distribution of latest state transitions, enabling adaptive and context-aware decision-making. To showcase the real-world effectiveness, we validate our approach in the challenging task of unstructured terrain navigation, where unmodeled and unmeasured terrain characteristics can significantly impact the robot's motion. Extensive experiments in both simulation and real world reveal significant improvements in data efficiency, policy performance, and the emergence of safer, adaptive navigation strategies.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19574v2",
    "published_date": "2025-05-26 06:40:11 UTC",
    "updated_date": "2025-11-23 00:05:06 UTC"
  },
  {
    "arxiv_id": "2505.19572v1",
    "title": "DocMEdit: Towards Document-Level Model Editing",
    "authors": [
      "Li Zeng",
      "Zeming Liu",
      "Chong Feng",
      "Heyan Huang",
      "Yuhang Guo"
    ],
    "abstract": "Model editing aims to correct errors and outdated knowledge in the Large language models (LLMs) with minimal cost. Prior research has proposed a variety of datasets to assess the effectiveness of these model editing methods. However, most existing datasets only require models to output short phrases or sentences, overlooks the widespread existence of document-level tasks in the real world, raising doubts about their practical usability. Aimed at addressing this limitation and promoting the application of model editing in real-world scenarios, we propose the task of document-level model editing. To tackle such challenges and enhance model capabilities in practical settings, we introduce \\benchmarkname, a dataset focused on document-level model editing, characterized by document-level inputs and outputs, extrapolative, and multiple facts within a single edit. We propose a series of evaluation metrics and experiments. The results show that the difficulties in document-level model editing pose challenges for existing model editing methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2025 findings",
    "pdf_url": "https://arxiv.org/pdf/2505.19572v1",
    "published_date": "2025-05-26 06:37:24 UTC",
    "updated_date": "2025-05-26 06:37:24 UTC"
  },
  {
    "arxiv_id": "2505.19568v1",
    "title": "MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model",
    "authors": [
      "Jiongchao Jin",
      "Xiuju Fu",
      "Xiaowei Gao",
      "Tao Cheng",
      "Ran Yan"
    ],
    "abstract": "Maritime transportation is the backbone of global trade, making ship inspection essential for ensuring maritime safety and environmental protection. Port State Control (PSC), conducted by national ports, enforces compliance with safety regulations, with ship detention being the most severe consequence, impacting both ship schedules and company reputations. Traditional machine learning methods for ship detention prediction are limited by the capacity of representation learning and thus suffer from low accuracy. Meanwhile, autoencoder-based deep learning approaches face challenges due to the severe data imbalance in learning historical PSC detention records. To address these limitations, we propose Maritime Ship Detention with Large Language Models (MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based autoencoder with a progressive learning pipeline to handle imbalanced data and extract meaningful PSC representations. Then, a large language model groups and ranks features to identify likely detention cases, enabling dynamic thresholding for flexible detention predictions. Extensive evaluations on 31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM outperforms state-of-the-art methods more than 12\\% on Area Under the Curve (AUC) for Singapore ports. Additionally, it demonstrates robustness to real-world challenges, making it adaptable to diverse maritime risk assessment scenarios.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19568v1",
    "published_date": "2025-05-26 06:32:02 UTC",
    "updated_date": "2025-05-26 06:32:02 UTC"
  },
  {
    "arxiv_id": "2505.19567v1",
    "title": "LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer",
    "authors": [
      "Rasoul Zahedifar",
      "Sayyed Ali Mirghasemi",
      "Mahdieh Soleymani Baghshah",
      "Alireza Taheri"
    ],
    "abstract": "This study presents the LLM-Agent-Controller, a multi-agent large language model (LLM) system developed to address a wide range of problems in control engineering (Control Theory). The system integrates a central controller agent with multiple specialized auxiliary agents, responsible for tasks such as controller design, model representation, control analysis, time-domain response, and simulation. A supervisor oversees high-level decision-making and workflow coordination, enhancing the system's reliability and efficiency. The LLM-Agent-Controller incorporates advanced capabilities, including Retrieval-Augmented Generation (RAG), Chain-of-Thought reasoning, self-criticism and correction, efficient memory handling, and user-friendly natural language communication. It is designed to function without requiring users to have prior knowledge of Control Theory, enabling them to input problems in plain language and receive complete, real-time solutions. To evaluate the system, we propose new performance metrics assessing both individual agents and the system as a whole. We test five categories of Control Theory problems and benchmark performance across three advanced LLMs. Additionally, we conduct a comprehensive qualitative conversational analysis covering all key services. Results show that the LLM-Agent-Controller successfully solved 83% of general tasks, with individual agents achieving an average success rate of 87%. Performance improved with more advanced LLMs. This research demonstrates the potential of multi-agent LLM architectures to solve complex, domain-specific problems. By integrating specialized agents, supervisory control, and advanced reasoning, the LLM-Agent-Controller offers a scalable, robust, and accessible solution framework that can be extended to various technical domains.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19567v1",
    "published_date": "2025-05-26 06:30:13 UTC",
    "updated_date": "2025-05-26 06:30:13 UTC"
  },
  {
    "arxiv_id": "2505.20354v4",
    "title": "Rethinking Text-based Protein Understanding: Retrieval or LLM?",
    "authors": [
      "Juntong Wu",
      "Zijing Liu",
      "He Cao",
      "Hao Li",
      "Bin Feng",
      "Zishan Shu",
      "Ke Yu",
      "Li Yuan",
      "Yu Li"
    ],
    "abstract": "In recent years, protein-text models have gained significant attention for their potential in protein generation and understanding. Current approaches focus on integrating protein-related knowledge into large language models through continued pretraining and multi-modal alignment, enabling simultaneous comprehension of textual descriptions and protein sequences. Through a thorough analysis of existing model architectures and text-based protein understanding benchmarks, we identify significant data leakage issues present in current benchmarks. Moreover, conventional metrics derived from natural language processing fail to accurately assess the model's performance in this domain. To address these limitations, we reorganize existing datasets and introduce a novel evaluation framework based on biological entities. Motivated by our observation, we propose a retrieval-enhanced method, which significantly outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy and efficiency in training-free scenarios. Our code and data can be seen at https://github.com/IDEA-XL/RAPM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by Empirical Methods in Natural Language Processing 2025 (EMNLP 2025) Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2505.20354v4",
    "published_date": "2025-05-26 06:25:43 UTC",
    "updated_date": "2025-11-10 03:48:28 UTC"
  },
  {
    "arxiv_id": "2505.19563v3",
    "title": "TabularMath: Understanding Math Reasoning over Tables with Large Language Models",
    "authors": [
      "Shi-Yu Tian",
      "Zhi Zhou",
      "Wei Dong",
      "Kun-Yang Yu",
      "Ming Yang",
      "Zi-Jian Cheng",
      "Lan-Zhe Guo",
      "Yu-Feng Li"
    ],
    "abstract": "Mathematical reasoning has long been a key benchmark for evaluating large language models. Although substantial progress has been made on math word problems, the need for reasoning over tabular data in real-world applications has been overlooked. For instance, applications such as business intelligence demand not only multi-step numerical reasoning with tables but also robustness to incomplete or inconsistent information. However, comprehensive evaluation in this area is severely limited, constrained by the reliance on manually collected tables that are difficult to scale and the lack of coverage for potential traps encountered in real-world scenarios. To address this problem, we propose AutoT2T, a neuro-symbolic framework that controllably transforms math word problems into scalable and verified tabular reasoning tasks. Building on this pipeline, we develop TabularMath, a benchmark comprising four subsets that include both text-based and image-based tables, covering table complexity, table quality, and table representation dimensions. Our study reveals three key observations: (1) Table complexity and reasoning difficulty impact reasoning performance jointly; (2) Low-quality tables pose severe risks to reliable reasoning in current LLMs; (3) Different table modalities show similar trends, with text-based tables typically being easier for models to reason over. In-depth analyses are conducted for each observation to guide future research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper under review, code and dataset are all available",
    "pdf_url": "https://arxiv.org/pdf/2505.19563v3",
    "published_date": "2025-05-26 06:24:31 UTC",
    "updated_date": "2026-01-08 03:37:22 UTC"
  },
  {
    "arxiv_id": "2505.19562v2",
    "title": "FairMedQA: Benchmarking Bias in Large Language Models for Medical Question Answering",
    "authors": [
      "Ying Xiao",
      "Jie Huang",
      "Ruijuan He",
      "Jing Xiao",
      "Mohammad Reza Mousavi",
      "Yepang Liu",
      "Kezhi Li",
      "Zhenpeng Chen",
      "Jie M. Zhang"
    ],
    "abstract": "Large language models (LLMs) are approaching expert-level performance in medical question answering (QA), demonstrating strong potential to improve public healthcare. However, underlying biases related to sensitive attributes such as sex and race pose life-critical risks. The extent to which such sensitive attributes affect diagnosis remains an open question and requires comprehensive empirical investigation. Additionally, even the latest Counterfactual Patient Variations (CPV) benchmark can hardly distinguish the bias levels of different LLMs. To further explore these dynamics, we propose a new benchmark, FairMedQA, and benchmark 12 representative LLMs. FairMedQA contains 4,806 counterfactual question pairs constructed from 801 clinical vignettes. Our results reveal substantial accuracy disparity ranging from 3 to 19 percentage points across sensitive demographic groups. Notably, FairMedQA exposes biases that are at least 12 percentage points larger than those identified by the latest CPV benchmark, presenting superior benchmarking sensitivity. Our results underscore an urgent need for targeted debiasing techniques and more rigorous, identity-aware validation protocols before LLMs can be safely integrated into practical clinical decision-support systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19562v2",
    "published_date": "2025-05-26 06:24:20 UTC",
    "updated_date": "2026-01-11 13:03:08 UTC"
  },
  {
    "arxiv_id": "2505.19550v5",
    "title": "Turing Test 2.0: The General Intelligence Threshold",
    "authors": [
      "Georgios Mappouras"
    ],
    "abstract": "With the rise of artificial intelligence (A.I.) and large language models like ChatGPT, a new race for achieving artificial general intelligence (A.G.I) has started. While many speculate how and when A.I. will achieve A.G.I., there is no clear agreement on how A.G.I. can be detected in A.I. models, even when popular tools like the Turing test (and its modern variations) are used to measure their intelligence. In this work, we discuss why traditional methods like the Turing test do not suffice for measuring or detecting A.G.I. and provide a new, practical method that can be used to decide if a system (computer or any other) has reached or surpassed A.G.I. To achieve this, we make two new contributions. First, we present a clear definition for general intelligence (G.I.) and set a G.I. Threshold (G.I.T.) that can be used to distinguish between systems that achieve A.G.I. and systems that do not. Second, we present a new framework on how to construct tests that can detect if a system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass way. We call this novel framework the Turing test 2.0. We then demonstrate real-life examples of applying tests that follow our Turing test 2.0 framework on modern A.I. models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19550v5",
    "published_date": "2025-05-26 06:13:15 UTC",
    "updated_date": "2025-12-03 19:00:53 UTC"
  },
  {
    "arxiv_id": "2505.19548v1",
    "title": "How Syntax Specialization Emerges in Language Models",
    "authors": [
      "Xufeng Duan",
      "Zhaoqian Yao",
      "Yunhao Zhang",
      "Shaonan Wang",
      "Zhenguang G. Cai"
    ],
    "abstract": "Large language models (LLMs) have been found to develop surprising internal specializations: Individual neurons, attention heads, and circuits become selectively sensitive to syntactic structure, reflecting patterns observed in the human brain. While this specialization is well-documented, how it emerges during training and what influences its development remains largely unknown.\n  In this work, we tap into the black box of specialization by tracking its formation over time. By quantifying internal syntactic consistency across minimal pairs from various syntactic phenomena, we identify a clear developmental trajectory: Syntactic sensitivity emerges gradually, concentrates in specific layers, and exhibits a 'critical period' of rapid internal specialization. This process is consistent across architectures and initialization parameters (e.g., random seeds), and is influenced by model scale and training data. We therefore reveal not only where syntax arises in LLMs but also how some models internalize it during training. To support future research, we will release the code, models, and training checkpoints upon acceptance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19548v1",
    "published_date": "2025-05-26 06:11:18 UTC",
    "updated_date": "2025-05-26 06:11:18 UTC"
  },
  {
    "arxiv_id": "2505.19547v3",
    "title": "STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization",
    "authors": [
      "Haoyu Zhang",
      "Wentao Zhang",
      "Hao Miao",
      "Xinke Jiang",
      "Yuchen Fang",
      "Yifan Zhang"
    ],
    "abstract": "Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful tool for modeling dynamic graph-structured data across diverse domains. However, they often fail to generalize in Spatio-Temporal Out-of-Distribution (STOOD) scenarios, where both temporal dynamics and spatial structures evolve beyond the training distribution. To address this problem, we propose an innovative Spatio-Temporal Retrieval-Augmented Pattern Learning framework,STRAP, which enhances model generalization by integrating retrieval-augmented learning into the STGNN continue learning pipeline. The core of STRAP is a compact and expressive pattern library that stores representative spatio-temporal patterns enriched with historical, structural, and semantic information, which is obtained and optimized during the training phase. During inference, STRAP retrieves relevant patterns from this library based on similarity to the current input and injects them into the model via a plug-and-play prompting mechanism. This not only strengthens spatio-temporal representations but also mitigates catastrophic forgetting. Moreover, STRAP introduces a knowledge-balancing objective to harmonize new information with retrieved knowledge. Extensive experiments across multiple real-world streaming graph datasets show that STRAP consistently outperforms state-of-the-art STGNN baselines on STOOD tasks, demonstrating its robustness, adaptability, and strong generalization capability without task-specific fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19547v3",
    "published_date": "2025-05-26 06:11:05 UTC",
    "updated_date": "2025-10-11 07:13:36 UTC"
  },
  {
    "arxiv_id": "2505.20353v2",
    "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation",
    "authors": [
      "Dong Liu",
      "Yanxuan Yu",
      "Jiayi Zhang",
      "Yifan Li",
      "Ben Lengerich",
      "Ying Nian Wu"
    ],
    "abstract": "Diffusion Transformers (DiT) are powerful generative models but remain computationally intensive due to their iterative structure and deep transformer stacks. To alleviate this inefficiency, we propose FastCache, a hidden-state-level caching and compression framework that accelerates DiT inference by exploiting redundancy within the model's internal representations. FastCache introduces a dual strategy: (1) a spatial-aware token selection mechanism that adaptively filters redundant tokens based on hidden state saliency, and (2) a transformer-level cache that reuses latent activations across timesteps when changes are statistically insignificant. These modules work jointly to reduce unnecessary computation while preserving generation fidelity through learnable linear approximation. Theoretical analysis shows that FastCache maintains bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across multiple DiT variants demonstrate substantial reductions in latency and memory usage, with best generation output quality compared to other cache methods, as measured by FID and t-FID. Code implementation of FastCache is available on GitHub at https://github.com/NoakLiu/FastCache-xDiT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20353v2",
    "published_date": "2025-05-26 05:58:49 UTC",
    "updated_date": "2025-09-03 06:56:21 UTC"
  },
  {
    "arxiv_id": "2505.19538v1",
    "title": "DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients",
    "authors": [
      "Yuxing Lu",
      "Gecheng Fu",
      "Wei Wu",
      "Xukai Zhao",
      "Sin Yee Goi",
      "Jinzhuo Wang"
    ],
    "abstract": "Existing medical RAG systems mainly leverage knowledge from medical knowledge bases, neglecting the crucial role of experiential knowledge derived from similar patient cases -- a key component of human clinical reasoning. To bridge this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like reasoning by integrating both explicit clinical knowledge and implicit case-based experience. DoctorRAG enhances retrieval precision by first allocating conceptual tags for queries and knowledge sources, together with a hybrid retrieval mechanism from both relevant knowledge and patient. In addition, a Med-TextGrad module using multi-agent textual gradients is integrated to ensure that the final output adheres to the retrieved knowledge and patient query. Comprehensive experiments on multilingual, multitask datasets demonstrate that DoctorRAG significantly outperforms strong baseline RAG models and gains improvements from iterative refinements. Our approach generates more accurate, relevant, and comprehensive responses, taking a step towards more doctor-like medical reasoning systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.IR",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "32 pages, 5 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.19538v1",
    "published_date": "2025-05-26 05:56:23 UTC",
    "updated_date": "2025-05-26 05:56:23 UTC"
  },
  {
    "arxiv_id": "2505.19536v3",
    "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models",
    "authors": [
      "Jintao Tong",
      "Wenwei Jin",
      "Pengda Qin",
      "Anqi Li",
      "Yixiong Zou",
      "Yuhong Li",
      "Yuhua Li",
      "Ruixuan Li"
    ],
    "abstract": "Large vision-language models (LVLMs) excel at multimodal understanding but suffer from high computational costs due to redundant vision tokens. Existing pruning methods typically rely on single-layer attention scores to rank and prune redundant visual tokens to solve this inefficiency. However, as the interaction between tokens and layers is complicated, this raises a basic question: Is such a simple single-layer criterion sufficient to identify redundancy? To answer this question, we rethink the emergence of redundant visual tokens from a fundamental perspective: information flow, which models the interaction between tokens and layers by capturing how information moves between tokens across layers. We find (1) the CLS token acts as an information relay, which can simplify the complicated flow analysis; (2) the redundancy emerges progressively and dynamically via layer-wise attention concentration; and (3) relying solely on attention scores from single layers can lead to contradictory redundancy identification. Based on this, we propose FlowCut, an information-flow-aware pruning framework, mitigating the insufficiency of the current criterion for identifying redundant tokens and better aligning with the model's inherent behaviors. Extensive experiments show that FlowCut achieves superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x speed-up in the prefilling stage. Our code is available at https://github.com/TungChintao/FlowCut",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19536v3",
    "published_date": "2025-05-26 05:54:48 UTC",
    "updated_date": "2025-11-23 07:44:08 UTC"
  },
  {
    "arxiv_id": "2505.19534v1",
    "title": "Training-Free Multi-Step Audio Source Separation",
    "authors": [
      "Yongyi Zang",
      "Jingyi Li",
      "Qiuqiang Kong"
    ],
    "abstract": "Audio source separation aims to separate a mixture into target sources. Previous audio source separation systems usually conduct one-step inference, which does not fully explore the separation ability of models. In this work, we reveal that pretrained one-step audio source separation models can be leveraged for multi-step separation without additional training. We propose a simple yet effective inference method that iteratively applies separation by optimally blending the input mixture with the previous step's separation result. At each step, we determine the optimal blending ratio by maximizing a metric. We prove that our method always yield improvement over one-step inference, provide error bounds based on model smoothness and metric robustness, and provide theoretical analysis connecting our method to denoising along linear interpolation paths between noise and clean distributions, a property we link to denoising diffusion bridge models. Our approach effectively delivers improved separation performance as a \"free lunch\" from existing models. Our empirical results demonstrate that our multi-step separation approach consistently outperforms one-step inference across both speech enhancement and music source separation tasks, and can achieve scaling performance similar to training a larger model, using more data, or in some cases employing a multi-step training objective. These improvements appear not only on the optimization metric during multi-step inference, but also extend to nearly all non-optimized metrics (with one exception). We also discuss limitations of our approach and directions for future research.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19534v1",
    "published_date": "2025-05-26 05:40:12 UTC",
    "updated_date": "2025-05-26 05:40:12 UTC"
  },
  {
    "arxiv_id": "2505.19531v1",
    "title": "Minimalist Softmax Attention Provably Learns Constrained Boolean Functions",
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Xiwen Zhang",
      "Maojiang Su",
      "Zhao Song",
      "Han Liu"
    ],
    "abstract": "We study the computational limits of learning $k$-bit Boolean functions (specifically, $\\mathrm{AND}$, $\\mathrm{OR}$, and their noisy variants), using a minimalist single-head softmax-attention mechanism, where $k=Θ(d)$ relevant bits are selected from $d$ inputs. We show that these simple $\\mathrm{AND}$ and $\\mathrm{OR}$ functions are unsolvable with a single-head softmax-attention mechanism alone. However, with teacher forcing, the same minimalist attention is capable of solving them. These findings offer two key insights: Architecturally, solving these Boolean tasks requires only minimalist attention, without deep Transformer blocks or FFNs. Methodologically, one gradient descent update with supervision suffices and replaces the multi-step Chain-of-Thought (CoT) reasoning scheme of [Kim and Suzuki, ICLR 2025] for solving Boolean problems. Together, the bounds expose a fundamental gap between what this minimal architecture achieves under ideal supervision and what is provably impossible under standard training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19531v1",
    "published_date": "2025-05-26 05:33:26 UTC",
    "updated_date": "2025-05-26 05:33:26 UTC"
  },
  {
    "arxiv_id": "2505.19528v3",
    "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection",
    "authors": [
      "Yejin Lee",
      "Joonghyuk Hahn",
      "Hyeseon Ahn",
      "Yo-Sub Han"
    ],
    "abstract": "Implicit hate speech detection is challenging due to its subtlety and reliance on contextual interpretation rather than explicit offensive words. Current approaches rely on contrastive learning, which are shown to be effective on distinguishing hate and non-hate sentences. Humans, however, detect implicit hate speech by first identifying specific targets within the text and subsequently interpreting how these target relate to their surrounding context. Motivated by this reasoning process, we propose AmpleHate, a novel approach designed to mirror human inference for implicit hate detection. AmpleHate identifies explicit target using a pretrained Named Entity Recognition model and capture implicit target information via [CLS] tokens. It computes attention-based relationships between explicit, implicit targets and sentence context and then, directly injects these relational vectors into the final sentence representation. This amplifies the critical signals of target-context relations for determining implicit hate. Experiments demonstrate that AmpleHate achieves state-of-the-art performance, outperforming contrastive learning baselines by an average of 82.14% and achieve faster convergence. Qualitative analyses further reveal that attention patterns produced by AmpleHate closely align with human judgement, underscoring its interpretability and robustness. Our code is publicly available at: https://github.com/leeyejin1231/AmpleHate.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 4 figures, EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19528v3",
    "published_date": "2025-05-26 05:27:10 UTC",
    "updated_date": "2025-09-19 06:12:42 UTC"
  },
  {
    "arxiv_id": "2505.19527v3",
    "title": "Rolling Ball Optimizer: Learning by ironing out loss landscape wrinkles",
    "authors": [
      "Mohammed Djameleddine Belgoumri",
      "Mohamed Reda Bouadjenek",
      "Hakim Hacid",
      "Imran Razzak",
      "Sunil Aryal"
    ],
    "abstract": "Training large neural networks (NNs) requires optimizing high-dimensional data-dependent loss functions. The optimization landscape of these functions is often highly complex and textured, even fractal-like, with many spurious local minima, ill-conditioned valleys, degenerate points, and saddle points. Complicating things further is the fact that these landscape characteristics are a function of the data, meaning that noise in the training data can propagate forward and give rise to unrepresentative small-scale geometry. This poses a difficulty for gradient-based optimization methods, which rely on local geometry to compute updates and are, therefore, vulnerable to being derailed by noisy data. In practice,this translates to a strong dependence of the optimization dynamics on the noise in the data, i.e., poor generalization performance. To remediate this problem, we propose a new optimization procedure: Rolling Ball Optimizer (RBO), that breaks this spatial locality by incorporating information from a larger region of the loss landscape in its updates. We achieve this by simulating the motion of a rigid sphere of finite radius rolling on the loss landscape, a straightforward generalization of Gradient Descent (GD) that simplifies into it in the infinitesimal limit. The radius serves as a hyperparameter that determines the scale at which RBO sees the loss landscape, allowing control over the granularity of its interaction therewith. We are motivated by the intuition that the large-scale geometry of the loss landscape is less data-specific than its fine-grained structure, and that it is easier to optimize. We support this intuition by proving that our algorithm has a smoothing effect on the loss function. Evaluation against SGD, SAM, and Entropy-SGD, on MNIST and CIFAR-10/100 demonstrates promising results in terms of convergence speed, training accuracy, and generalization performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted for review to ICLR 2026",
    "pdf_url": "https://arxiv.org/pdf/2505.19527v3",
    "published_date": "2025-05-26 05:26:21 UTC",
    "updated_date": "2025-10-24 04:55:44 UTC"
  },
  {
    "arxiv_id": "2505.19525v2",
    "title": "Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate",
    "authors": [
      "Liangwei Nathan Zheng",
      "Wei Emma Zhang",
      "Mingyu Guo",
      "Miao Xu",
      "Olaf Maennel",
      "Weitong Chen"
    ],
    "abstract": "Effectively managing missing modalities is a fundamental challenge in real-world multimodal learning scenarios, where data incompleteness often results from systematic collection errors or sensor failures. Sparse Mixture-of-Experts (SMoE) architectures have the potential to naturally handle multimodal data, with individual experts specializing in different modalities. However, existing SMoE approach often lacks proper ability to handle missing modality, leading to performance degradation and poor generalization in real-world applications. We propose ConfSMoE to introduce a two-stage imputation module to handle the missing modality problem for the SMoE architecture by taking the opinion of experts and reveal the insight of expert collapse from theoretical analysis with strong empirical evidence. Inspired by our theoretical analysis, ConfSMoE propose a novel expert gating mechanism by detaching the softmax routing score to task confidence score w.r.t ground truth signal. This naturally relieves expert collapse without introducing additional load balance loss function. We show that the insights of expert collapse aligns with other gating mechanism such as Gaussian and Laplacian gate. The proposed method is evaluated on four different real world dataset with three distinct experiment settings to conduct comprehensive analysis of ConfSMoE on resistance to missing modality and the impacts of proposed gating mechanism.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19525v2",
    "published_date": "2025-05-26 05:18:55 UTC",
    "updated_date": "2025-08-24 02:55:17 UTC"
  },
  {
    "arxiv_id": "2505.19514v2",
    "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback",
    "authors": [
      "Yaoning Yu",
      "Ye Yu",
      "Kai Wei",
      "Haojing Luo",
      "Haohan Wang"
    ],
    "abstract": "Prompt quality plays a critical role in the performance of large language models (LLMs), motivating a growing body of work on prompt optimization. Most existing methods optimize prompts over a fixed dataset, assuming static input distributions and offering limited support for iterative improvement. We introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a closed-loop framework for prompt learning that integrates synthetic data generation into the optimization process. SIPDO couples a synthetic data generator with a prompt optimizer, where the generator produces new examples that reveal current prompt weaknesses and the optimizer incrementally refines the prompt in response. This feedback-driven loop enables systematic improvement of prompt performance without assuming access to external supervision or new tasks. Experiments across question answering and reasoning benchmarks show that SIPDO outperforms standard prompt tuning methods, highlighting the value of integrating data synthesis into prompt learning workflows.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19514v2",
    "published_date": "2025-05-26 04:56:48 UTC",
    "updated_date": "2025-06-22 05:21:13 UTC"
  },
  {
    "arxiv_id": "2505.19509v1",
    "title": "Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models",
    "authors": [
      "Yifan Jia",
      "Kailin Jiang",
      "Yuyang Liang",
      "Qihan Ren",
      "Yi Xin",
      "Rui Yang",
      "Fenze Feng",
      "Mingcai Chen",
      "Hengyang Lu",
      "Haozhe Wang",
      "Xiaoye Qu",
      "Dongrui Liu",
      "Lizhen Cui",
      "Yuntao Du"
    ],
    "abstract": "Large Multimodal Models(LMMs) face notable challenges when encountering multimodal knowledge conflicts, particularly under retrieval-augmented generation(RAG) frameworks where the contextual information from external sources may contradict the model's internal parametric knowledge, leading to unreliable outputs. However, existing benchmarks fail to reflect such realistic conflict scenarios. Most focus solely on intra-memory conflicts, while context-memory and inter-context conflicts remain largely investigated. Furthermore, commonly used factual knowledge-based evaluations are often overlooked, and existing datasets lack a thorough investigation into conflict detection capabilities. To bridge this gap, we propose MMKC-Bench, a benchmark designed to evaluate factual knowledge conflicts in both context-memory and inter-context scenarios. MMKC-Bench encompasses three types of multimodal knowledge conflicts and includes 1,573 knowledge instances and 3,381 images across 23 broad types, collected through automated pipelines with human verification. We evaluate three representative series of LMMs on both model behavior analysis and conflict detection tasks. Our findings show that while current LMMs are capable of recognizing knowledge conflicts, they tend to favor internal parametric knowledge over external evidence. We hope MMKC-Bench will foster further research in multimodal knowledge conflict and enhance the development of multimodal RAG systems. The source code is available at https://github.com/MLLMKCBENCH/MLLMKC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The source code is available at https://github.com/MLLMKCBENCH/MLLMKC",
    "pdf_url": "https://arxiv.org/pdf/2505.19509v1",
    "published_date": "2025-05-26 04:39:30 UTC",
    "updated_date": "2025-05-26 04:39:30 UTC"
  },
  {
    "arxiv_id": "2505.19505v1",
    "title": "Hierarchical Tree Search-based User Lifelong Behavior Modeling on Large Language Model",
    "authors": [
      "Yu Xia",
      "Rui Zhong",
      "Hao Gu",
      "Wei Yang",
      "Chi Lu",
      "Peng Jiang",
      "Kun Gai"
    ],
    "abstract": "Large Language Models (LLMs) have garnered significant attention in Recommendation Systems (RS) due to their extensive world knowledge and robust reasoning capabilities. However, a critical challenge lies in enabling LLMs to effectively comprehend and extract insights from massive user behaviors. Current approaches that directly leverage LLMs for user interest learning face limitations in handling long sequential behaviors, effectively extracting interest, and applying interest in practical scenarios. To address these issues, we propose a Hierarchical Tree Search-based User Lifelong Behavior Modeling framework (HiT-LBM). HiT-LBM integrates Chunked User Behavior Extraction (CUBE) and Hierarchical Tree Search for Interest (HTS) to capture diverse interests and interest evolution of user. CUBE divides user lifelong behaviors into multiple chunks and learns the interest and interest evolution within each chunk in a cascading manner. HTS generates candidate interests through hierarchical expansion and searches for the optimal interest with process rating model to ensure information gain for each behavior chunk. Additionally, we design Temporal-Ware Interest Fusion (TIF) to integrate interests from multiple behavior chunks, constructing a comprehensive representation of user lifelong interests. The representation can be embedded into any recommendation model to enhance performance. Extensive experiments demonstrate the effectiveness of our approach, showing that it surpasses state-of-the-art methods.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19505v1",
    "published_date": "2025-05-26 04:32:57 UTC",
    "updated_date": "2025-05-26 04:32:57 UTC"
  },
  {
    "arxiv_id": "2506.15688v1",
    "title": "Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism",
    "authors": [
      "Hui Ma",
      "Kai Yang",
      "Man-On Pun"
    ],
    "abstract": "Cellular traffic prediction is of great importance for operators to manage network resources and make decisions. Traffic is highly dynamic and influenced by many exogenous factors, which would lead to the degradation of traffic prediction accuracy. This paper proposes an end-to-end framework with two variants to explicitly characterize the spatiotemporal patterns of cellular traffic among neighboring cells. It uses convolutional neural networks with an attention mechanism to capture the spatial dynamics and Kalman filter for temporal modelling. Besides, we can fully exploit the auxiliary information such as social activities to improve prediction performance. We conduct extensive experiments on three real-world datasets. The results show that our proposed models outperform the state-of-the-art machine learning techniques in terms of prediction accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15688v1",
    "published_date": "2025-05-26 04:32:15 UTC",
    "updated_date": "2025-05-26 04:32:15 UTC"
  },
  {
    "arxiv_id": "2505.19504v2",
    "title": "DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation",
    "authors": [
      "Pingzhi Li",
      "Zhen Tan",
      "Mohan Zhang",
      "Huaizhi Qu",
      "Huan Liu",
      "Tianlong Chen"
    ],
    "abstract": "Large Language Models (LLMs) represent substantial intellectual and economic investments, yet their effectiveness can inadvertently facilitate model imitation via knowledge distillation (KD). In practical scenarios, competitors can distill proprietary LLM capabilities by simply observing publicly accessible outputs, akin to reverse-engineering a complex performance by observation alone. Existing protective methods like watermarking only identify imitation post-hoc, while other defenses assume the student model mimics the teacher's internal logits, rendering them ineffective against distillation purely from observed output text. This paper confronts the challenge of actively protecting LLMs within the realistic constraints of API-based access. We introduce an effective and efficient Defensive Output Generation (DOGe) strategy that subtly modifies the output behavior of an LLM. Its outputs are accurate and useful for legitimate users, yet are designed to be misleading for distillation, significantly undermining imitation attempts. We achieve this by fine-tuning only the final linear layer of the teacher LLM with an adversarial loss. This targeted training approach anticipates and disrupts distillation attempts during inference time. Our experiments show that, while preserving the performance of the teacher model, student models distilled from the defensively generated outputs demonstrate catastrophically reduced performance, demonstrating DOGe as a practical safeguard against KD-based model imitation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Code is available at https://github.com/unites-lab/doge",
    "pdf_url": "https://arxiv.org/pdf/2505.19504v2",
    "published_date": "2025-05-26 04:31:38 UTC",
    "updated_date": "2025-10-19 19:50:11 UTC"
  },
  {
    "arxiv_id": "2505.19502v1",
    "title": "CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation",
    "authors": [
      "Guang Yang",
      "Yu Zhou",
      "Xiang Chen",
      "Wei Zheng",
      "Xing Hu",
      "Xin Zhou",
      "David Lo",
      "Taolue Chen"
    ],
    "abstract": "Trustworthy evaluation methods for code snippets play a crucial role in neural code generation. Traditional methods, which either rely on reference solutions or require executable test cases, have inherent limitation in flexibility and scalability. The recent LLM-as-Judge methodology offers a promising alternative by directly evaluating functional consistency between the problem description and the generated code. To systematically understand the landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical study across three diverse datasets. Our investigation reveals the pros and cons of two categories of LLM-as-Judge methods: the methods based on general foundation models can achieve good performance but require complex prompts and lack explainability, while the methods based on reasoning foundation models provide better explainability with simpler prompts but demand substantial computational resources due to their large parameter sizes. To address these limitations, we propose CODE-DITING, a novel code evaluation method that balances accuracy, efficiency and explainability. We develop a data distillation framework that effectively transfers reasoning capabilities from DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing evaluation explainability and reducing the computational cost. With the majority vote strategy in the inference process, CODE-DITING 1.5B outperforms all models with the same magnitude of parameters and achieves performance which would normally exhibit in a model with 5 times of parameter scale. CODE-DITING 7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the parameter volume of these large models. Further experiments show that CODEDITING is robust to preference leakage and can serve as a promising alternative for code evaluation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19502v1",
    "published_date": "2025-05-26 04:29:14 UTC",
    "updated_date": "2025-05-26 04:29:14 UTC"
  },
  {
    "arxiv_id": "2505.19501v2",
    "title": "Toward Scientific Reasoning in LLMs: Training from Expert Discussions via Reinforcement Learning",
    "authors": [
      "Ming Yin",
      "Yuanhao Qu",
      "Ling Yang",
      "Le Cong",
      "Mengdi Wang"
    ],
    "abstract": "We investigate how to teach large language models (LLMs) to perform scientific reasoning by leveraging expert discussions as a learning signal. Focusing on the genomics domain, we develop an automated pipeline to extract trainable data and introduce Genome-Bench, a new benchmark constructed from over a decade of scientific forum discussions on genome engineering. Our pipeline transforms raw interactions into a reinforcement learning-friendly multiple-choice questions format, supported by 3000+ high-quality question-answer pairs spanning foundational biology, experimental troubleshooting, tool usage, and beyond. We fine-tune an LLM using RL with a rule-based reward signal derived from the synthetic MCQ dataset to enhance domain-specific reasoning. Our results show that reinforcement learning from scientific discussions improves model performance by over 15% compared to the base model on Genome-Bench, narrowing the gap between open-source LLMs and expert-level reasoning. To our knowledge, this is the first end-to-end pipeline for teaching LLMs to reason from scientific discussions, with promising potential for generalization across scientific domains beyond biology.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19501v2",
    "published_date": "2025-05-26 04:28:46 UTC",
    "updated_date": "2025-06-02 21:31:08 UTC"
  },
  {
    "arxiv_id": "2505.19498v2",
    "title": "Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models",
    "authors": [
      "Nanxing Hu",
      "Xiaoyue Duan",
      "Jinchao Zhang",
      "Guoliang Kang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) usually generate texts which satisfy context coherence but don't match the visual input. Such a hallucination issue hinders LVLMs' applicability in the real world. The key to solving hallucination in LVLM is to make the text generation rely more on the visual content. Most previous works choose to enhance/adjust the features/output of a specific modality (i.e., visual or textual) to alleviate hallucinations in LVLM, which do not explicitly or systematically enhance the visual reliance. In this paper, we comprehensively investigate the factors which may degenerate the visual reliance in text generation of LVLM from a Bayesian perspective. Based on our observations, we propose to mitigate hallucination in LVLM from three aspects. Firstly, we observe that not all visual tokens are informative in generating meaningful texts. We propose to evaluate and remove redundant visual tokens to avoid their disturbance. Secondly, LVLM may encode inappropriate prior information, making it lean toward generating unexpected words. We propose a simple yet effective way to rectify the prior from a Bayesian perspective. Thirdly, we observe that starting from certain steps, the posterior of next-token prediction conditioned on visual tokens may collapse to a prior distribution which does not depend on any informative visual tokens at all. Thus, we propose to stop further text generation to avoid hallucination. Extensive experiments on three benchmarks including POPE, CHAIR, and MME demonstrate that our method can consistently mitigate the hallucination issue of LVLM and performs favorably against previous state-of-the-arts.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19498v2",
    "published_date": "2025-05-26 04:26:30 UTC",
    "updated_date": "2025-08-19 02:37:11 UTC"
  },
  {
    "arxiv_id": "2505.21553v1",
    "title": "MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction",
    "authors": [
      "Hui Ma",
      "Kai Yang"
    ],
    "abstract": "Network traffic prediction techniques have attracted much attention since they are valuable for network congestion control and user experience improvement. While existing prediction techniques can achieve favorable performance when there is sufficient training data, it remains a great challenge to make accurate predictions when only a small amount of training data is available. To tackle this problem, we propose a deep learning model, entitled MetaSTNet, based on a multimodal meta-learning framework. It is an end-to-end network architecture that trains the model in a simulator and transfers the meta-knowledge to a real-world environment, which can quickly adapt and obtain accurate predictions on a new task with only a small amount of real-world training data. In addition, we further employ cross conformal prediction to assess the calibrated prediction intervals. Extensive experiments have been conducted on real-world datasets to illustrate the efficiency and effectiveness of MetaSTNet.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21553v1",
    "published_date": "2025-05-26 04:23:54 UTC",
    "updated_date": "2025-05-26 04:23:54 UTC"
  },
  {
    "arxiv_id": "2505.19490v1",
    "title": "Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models",
    "authors": [
      "Jianxing Liao",
      "Junyan Xu",
      "Yatao Sun",
      "Maowen Tang",
      "Sicheng He",
      "Jingxian Liao",
      "Shui Yu",
      "Yun Li",
      "Hongguan Xiao"
    ],
    "abstract": "Designing complex computer-aided design (CAD) models is often time-consuming due to challenges such as computational inefficiency and the difficulty of generating precise models. We propose a novel language-guided framework for industrial design automation to address these issues, integrating large language models (LLMs) with computer-automated design (CAutoD).Through this framework, CAD models are automatically generated from parameters and appearance descriptions, supporting the automation of design tasks during the detailed CAD design phase. Our approach introduces three key innovations: (1) a semi-automated data annotation pipeline that leverages LLMs and vision-language large models (VLLMs) to generate high-quality parameters and appearance descriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts modeling sequences via dual-channel feature aggregation; (3) an enhanced CAD modeling generation model, called CADLLM, that is designed to refine the generated sequences by incorporating the confidence scores from TCADGen. Experimental results demonstrate that the proposed approach outperforms traditional methods in both accuracy and efficiency, providing a powerful tool for automating industrial workflows and generating complex CAD models from textual prompts. The code is available at https://jianxliao.github.io/cadllm-page/",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ACL 2025 Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2505.19490v1",
    "published_date": "2025-05-26 04:17:51 UTC",
    "updated_date": "2025-05-26 04:17:51 UTC"
  },
  {
    "arxiv_id": "2505.19489v1",
    "title": "Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs",
    "authors": [
      "Zhenhao Zhou",
      "Zhuochen Huang",
      "Yike He",
      "Chong Wang",
      "Jiajun Wang",
      "Yijian Wu",
      "Xin Peng",
      "Yiling Lou"
    ],
    "abstract": "The Linux kernel is a critical system, serving as the foundation for numerous systems. Bugs in the Linux kernel can cause serious consequences, affecting billions of users. Fault localization (FL), which aims at identifying the buggy code elements in software, plays an essential role in software quality assurance. While recent LLM agents have achieved promising accuracy in FL on recent benchmarks like SWE-bench, it remains unclear how well these methods perform in the Linux kernel, where FL is much more challenging due to the large-scale code base, limited observability, and diverse impact factors. In this paper, we introduce LinuxFLBench, a FL benchmark constructed from real-world Linux kernel bugs. We conduct an empirical study to assess the performance of state-of-the-art LLM agents on the Linux kernel. Our initial results reveal that existing agents struggle with this task, achieving a best top-1 accuracy of only 41.6% at file level. To address this challenge, we propose LinuxFL$^+$, an enhancement framework designed to improve FL effectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially improves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy increase) with minimal costs. Data and code are available at https://github.com/FudanSELab/LinuxFLBench.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19489v1",
    "published_date": "2025-05-26 04:15:48 UTC",
    "updated_date": "2025-05-26 04:15:48 UTC"
  },
  {
    "arxiv_id": "2505.19488v1",
    "title": "Understanding Transformer from the Perspective of Associative Memory",
    "authors": [
      "Shu Zhong",
      "Mingyu Xu",
      "Tenglong Ao",
      "Guang Shi"
    ],
    "abstract": "In this paper, we share our reflections and insights on understanding Transformer architectures through the lens of associative memory--a classic psychological concept inspired by human cognition. We start with the basics of associative memory (think simple linear attention) and then dive into two dimensions:\n  Memory Capacity: How much can a Transformer really remember, and how well? We introduce retrieval SNR to measure this and use a kernel perspective to mathematically reveal why Softmax Attention is so effective. We also show how FFNs can be seen as a type of associative memory, leading to insights on their design and potential improvements.\n  Memory Update: How do these memories learn and evolve? We present a unified framework for understanding how different Transformer variants (like DeltaNet and Softmax Attention) update their \"knowledge base\". This leads us to tackle two provocative questions: 1. Are Transformers fundamentally limited in what they can express, and can we break these barriers? 2. If a Transformer had infinite context, would it become infinitely intelligent?\n  We want to demystify Transformer architecture, offering a clearer understanding of existing designs. This exploration aims to provide fresh insights and spark new avenues for Transformer innovation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Consider this post less as a formal research paper and more as a blog-style sharing of our current reflections, intended to spark discussion as one might in a collaborative team meeting",
    "pdf_url": "https://arxiv.org/pdf/2505.19488v1",
    "published_date": "2025-05-26 04:15:38 UTC",
    "updated_date": "2025-05-26 04:15:38 UTC"
  },
  {
    "arxiv_id": "2505.21552v1",
    "title": "Understanding the learned look-ahead behavior of chess neural networks",
    "authors": [
      "Diogo Cruz"
    ],
    "abstract": "We investigate the look-ahead capabilities of chess-playing neural networks, specifically focusing on the Leela Chess Zero policy network. We build on the work of Jenner et al. (2024) by analyzing the model's ability to consider future moves and alternative sequences beyond the immediate next move. Our findings reveal that the network's look-ahead behavior is highly context-dependent, varying significantly based on the specific chess position. We demonstrate that the model can process information about board states up to seven moves ahead, utilizing similar internal mechanisms across different future time steps. Additionally, we provide evidence that the network considers multiple possible move sequences rather than focusing on a single line of play. These results offer new insights into the emergence of sophisticated look-ahead capabilities in neural networks trained on strategic tasks, contributing to our understanding of AI reasoning in complex domains. Our work also showcases the effectiveness of interpretability techniques in uncovering cognitive-like processes in artificial intelligence systems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "40 pages, 47 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21552v1",
    "published_date": "2025-05-26 04:03:59 UTC",
    "updated_date": "2025-05-26 04:03:59 UTC"
  },
  {
    "arxiv_id": "2505.19481v1",
    "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs",
    "authors": [
      "Hao Kang",
      "Qingru Zhang",
      "Han Cai",
      "Weiyuan Xu",
      "Tushar Krishna",
      "Yilun Du",
      "Tsachy Weissman"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency quality trade off, it remains underexplored in the context of LLM based agents. In this work, we present the first systematic study of this trade off in real time decision making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading, underscoring the need for latency aware evaluation and deployment strategies for LLM based agents. These results demonstrate the critical importance of latency aware evaluation and deployment strategies for real world LLM based agents. Our benchmarks are available at Latency Sensitive Benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19481v1",
    "published_date": "2025-05-26 04:03:48 UTC",
    "updated_date": "2025-05-26 04:03:48 UTC"
  },
  {
    "arxiv_id": "2505.19477v3",
    "title": "Judging with Many Minds: Do More Perspectives Mean Less Prejudice? On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge",
    "authors": [
      "Chiyu Ma",
      "Enpei Zhang",
      "Yilun Zhao",
      "Wenjun Liu",
      "Yaning Jia",
      "Peijun Qing",
      "Lin Shi",
      "Arman Cohan",
      "Yujun Yan",
      "Soroush Vosoughi"
    ],
    "abstract": "LLM-as-Judge has emerged as a scalable alternative to human evaluation, enabling large language models (LLMs) to provide reward signals in trainings. While recent work has explored multi-agent extensions such as multi-agent debate and meta-judging to enhance evaluation quality, the question of how intrinsic biases manifest in these settings remains underexplored. In this study, we conduct a systematic analysis of four diverse bias types: position bias, verbosity bias, chain-of-thought bias, and bandwagon bias. We evaluate these biases across two widely adopted multi-agent LLM-as-Judge frameworks: Multi-Agent-Debate and LLM-as-Meta-Judge. Our results show that debate framework amplifies biases sharply after the initial debate, and this increased bias is sustained in subsequent rounds, while meta-judge approaches exhibit greater resistance. We further investigate the incorporation of PINE, a leading single-agent debiasing method, as a bias-free agent within these systems. The results reveal that this bias-free agent effectively reduces biases in debate settings but provides less benefit in meta-judge scenarios. Our work provides a comprehensive study of bias behavior in multi-agent LLM-as-Judge systems and highlights the need for targeted bias mitigation strategies in collaborative evaluation settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19477v3",
    "published_date": "2025-05-26 03:56:41 UTC",
    "updated_date": "2025-09-17 20:52:53 UTC"
  },
  {
    "arxiv_id": "2505.19474v1",
    "title": "Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models",
    "authors": [
      "Xinmiao Hu",
      "Chun Wang",
      "Ruihe An",
      "ChenYu Shao",
      "Xiaojun Ye",
      "Sheng Zhou",
      "Liangcheng Li"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated strong performance in visual understanding tasks, yet they often suffer from object hallucinations--generating descriptions of objects that are inconsistent with or entirely absent from the input. This issue is closely related to dataset biases, where frequent co-occurrences of objects lead to entangled semantic representations across modalities. As a result, models may erroneously activate object representations that are commonly associated with the input but not actually present.\n  To address this, we propose a causality-driven disentanglement framework that mitigates hallucinations through causal intervention. Our approach includes a Causal-Driven Projector in the visual pathway and a Causal Intervention Module integrated into the final transformer layer of the language model. These components work together to reduce spurious correlations caused by biased training data.\n  Experimental results show that our method significantly reduces hallucinations while maintaining strong performance on multiple multimodal benchmarks. Visualization analyses further confirm improved separability of object representations.\n  The code is available at: https://github.com/IgniSavium/Causal-LLaVA",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 19 figures, Submitted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19474v1",
    "published_date": "2025-05-26 03:53:00 UTC",
    "updated_date": "2025-05-26 03:53:00 UTC"
  },
  {
    "arxiv_id": "2505.19469v1",
    "title": "Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory",
    "authors": [
      "Mingzhuo Li",
      "Guang Li",
      "Jiafeng Mao",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "abstract": "Dataset distillation enables the training of deep neural networks with comparable performance in significantly reduced time by compressing large datasets into small and representative ones. Although the introduction of generative models has made great achievements in this field, the distributions of their distilled datasets are not diverse enough to represent the original ones, leading to a decrease in downstream validation accuracy. In this paper, we present a diversity-driven generative dataset distillation method based on a diffusion model to solve this problem. We introduce self-adaptive memory to align the distribution between distilled and real datasets, assessing the representativeness. The degree of alignment leads the diffusion model to generate more diverse datasets during the distillation process. Extensive experiments show that our method outperforms existing state-of-the-art methods in most situations, proving its ability to tackle dataset distillation tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICIP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19469v1",
    "published_date": "2025-05-26 03:48:56 UTC",
    "updated_date": "2025-05-26 03:48:56 UTC"
  },
  {
    "arxiv_id": "2505.20350v1",
    "title": "Decision Flow Policy Optimization",
    "authors": [
      "Jifeng Hu",
      "Sili Huang",
      "Siyuan Guo",
      "Zhaogeng Liu",
      "Li Shen",
      "Lichao Sun",
      "Hechang Chen",
      "Yi Chang",
      "Dacheng Tao"
    ],
    "abstract": "In recent years, generative models have shown remarkable capabilities across diverse fields, including images, videos, language, and decision-making. By applying powerful generative models such as flow-based models to reinforcement learning, we can effectively model complex multi-modal action distributions and achieve superior robotic control in continuous action spaces, surpassing the limitations of single-modal action distributions with traditional Gaussian-based policies. Previous methods usually adopt the generative models as behavior models to fit state-conditioned action distributions from datasets, with policy optimization conducted separately through additional policies using value-based sample weighting or gradient-based updates. However, this separation prevents the simultaneous optimization of multi-modal distribution fitting and policy improvement, ultimately hindering the training of models and degrading the performance. To address this issue, we propose Decision Flow, a unified framework that integrates multi-modal action distribution modeling and policy optimization. Specifically, our method formulates the action generation procedure of flow-based models as a flow decision-making process, where each action generation step corresponds to one flow decision. Consequently, our method seamlessly optimizes the flow policy while capturing multi-modal action distributions. We provide rigorous proofs of Decision Flow and validate the effectiveness through extensive experiments across dozens of offline RL environments. Compared with established offline RL baselines, the results demonstrate that our method achieves or matches the SOTA performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20350v1",
    "published_date": "2025-05-26 03:42:20 UTC",
    "updated_date": "2025-05-26 03:42:20 UTC"
  },
  {
    "arxiv_id": "2505.19466v1",
    "title": "Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs",
    "authors": [
      "Hongyu Liang",
      "Yuting Zheng",
      "Yihan Li",
      "Yiran Zhang",
      "Shiyu Liang"
    ],
    "abstract": "As large language models (LLMs) continue to advance, their deployment often involves fine-tuning to enhance performance on specific downstream tasks. However, this customization is sometimes accompanied by misleading claims about the origins, raising significant concerns about transparency and trust within the open-source community. Existing model verification techniques typically assess functional, representational, and weight similarities. However, these approaches often struggle against obfuscation techniques, such as permutations and scaling transformations. To address this limitation, we propose a novel detection method Origin-Tracer that rigorously determines whether a model has been fine-tuned from a specified base model. This method includes the ability to extract the LoRA rank utilized during the fine-tuning process, providing a more robust verification framework. This framework is the first to provide a formalized approach specifically aimed at pinpointing the sources of model fine-tuning. We empirically validated our method on thirty-one diverse open-source models under conditions that simulate real-world obfuscation scenarios. We empirically analyze the effectiveness of our framework and finally, discuss its limitations. The results demonstrate the effectiveness of our approach and indicate its potential to establish new benchmarks for model verification.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19466v1",
    "published_date": "2025-05-26 03:38:14 UTC",
    "updated_date": "2025-05-26 03:38:14 UTC"
  },
  {
    "arxiv_id": "2505.19465v1",
    "title": "Residual Cross-Attention Transformer-Based Multi-User CSI Feedback with Deep Joint Source-Channel Coding",
    "authors": [
      "Hengwei Zhang",
      "Minghui Wu",
      "Li Qiao",
      "Ling Liu",
      "Ziqi Han",
      "Zhen Gao"
    ],
    "abstract": "This letter proposes a deep-learning (DL)-based multi-user channel state information (CSI) feedback framework for massive multiple-input multiple-output systems, where the deep joint source-channel coding (DJSCC) is utilized to improve the CSI reconstruction accuracy. Specifically, we design a multi-user joint CSI feedback framework, whereby the CSI correlation of nearby users is utilized to reduce the feedback overhead. Under the framework, we propose a new residual cross-attention transformer architecture, which is deployed at the base station to further improve the CSI feedback performance. Moreover, to tackle the \"cliff-effect\" of conventional bit-level CSI feedback approaches, we integrated DJSCC into the multi-user CSI feedback, together with utilizing a two-stage training scheme to adapt to varying uplink noise levels. Experimental results demonstrate the superiority of our methods in CSI feedback performance, with low network complexity and better scalability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19465v1",
    "published_date": "2025-05-26 03:38:08 UTC",
    "updated_date": "2025-05-26 03:38:08 UTC"
  },
  {
    "arxiv_id": "2505.19459v1",
    "title": "Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation",
    "authors": [
      "Kaichao Jiang",
      "He Wang",
      "Xiaoshuai Hao",
      "Xiulong Yang",
      "Ajian Liu",
      "Qi Chu",
      "Yunfeng Diao"
    ],
    "abstract": "Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative models, are well known for their ability to achieve both high classification accuracy and generative capability within a single model. However, their robustness still lags significantly behind the classifiers based adversarial training (AT). Conversely, while AT is currently the most effective approach to improving the classifier's robustness, it typically sacrifices accuracy on clean data and lacks generative capability. The triple trade-off between classification accuracy, generative capability and robustness, raises a natural question: Can a single model simultaneously achieve high classification accuracy, adversarial robustness, and generative performance? -- a goal that has been rarely explored. To address this question, we systematically analyze the energy distribution differences of clean, adversarial, and generated samples across various JEM variants and adversarially trained models. We observe that AT tends to reduce the energy gap between clean and adversarial samples, while JEMs reduce the gap between clean and synthetic ones. This observation suggests a key insight: if the energy distributions of all three data types can be aligned, we might unify the strengths of AT and JEMs, resolving their inherent trade-offs. Building on this idea, we propose Energy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly model the clean data distribution, the adversarial distribution, and the classifier by maximizing their joint probability. EB-JDAT is a general and flexible optimization method, compatible with various JEM variants. Extensive experimental results demonstrate that EB-JDAT not only maintains near original accuracy and generative capability of JEMs, but also significantly enhances robustness, even surpassing state-of-the-art ATs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19459v1",
    "published_date": "2025-05-26 03:26:55 UTC",
    "updated_date": "2025-05-26 03:26:55 UTC"
  },
  {
    "arxiv_id": "2505.19457v1",
    "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs",
    "authors": [
      "Guilong Lu",
      "Xuntao Guo",
      "Rongjunchen Zhang",
      "Wenqiao Zhu",
      "Ji Liu"
    ],
    "abstract": "Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Project Page: https://hithink-research.github.io/BizFinBench/",
    "pdf_url": "https://arxiv.org/pdf/2505.19457v1",
    "published_date": "2025-05-26 03:23:02 UTC",
    "updated_date": "2025-05-26 03:23:02 UTC"
  },
  {
    "arxiv_id": "2505.19455v2",
    "title": "MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering",
    "authors": [
      "Xu Li",
      "Fan Lyu"
    ],
    "abstract": "Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs) has achieved promising progress by leveraging prompt tuning to enable continual multi-modal learning. However, most existing methods adopt cross-modal prompt isolation, constructing visual and textual prompts separately, which exacerbates modality imbalance and leads to degraded performance over time. To tackle this issue, we propose MM-Prompt, a novel framework incorporating cross-modal prompt query and cross-modal prompt recovery. The former enables balanced prompt selection by incorporating cross-modal signals during query formation, while the latter promotes joint prompt reconstruction through iterative cross-modal interactions, guided by an alignment loss to prevent representational drift. Extensive experiments show that MM-Prompt surpasses prior approaches in accuracy and knowledge retention, while maintaining balanced modality engagement throughout continual learning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19455v2",
    "published_date": "2025-05-26 03:21:21 UTC",
    "updated_date": "2025-09-11 17:29:56 UTC"
  },
  {
    "arxiv_id": "2505.19443v1",
    "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI",
    "authors": [
      "Ranjan Sapkota",
      "Konstantinos I. Roumeliotis",
      "Manoj Karkee"
    ],
    "abstract": "This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and iterating tasks with minimal human intervention. We propose a detailed taxonomy spanning conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. Through comparative workflow analysis and 20 detailed use cases, we illustrate how vibe systems thrive in early-stage prototyping and education, while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. We further examine emerging trends in hybrid architectures, where natural language interfaces are coupled with autonomous execution pipelines. Finally, we articulate a future roadmap for agentic AI, outlining the infrastructure needed for trustworthy, explainable, and collaborative systems. Our findings suggest that successful AI software engineering will rely not on choosing one paradigm, but on harmonizing their strengths within a unified, human-centered development lifecycle.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "35 Pages, 8 Figures, 6 Tables",
    "pdf_url": "https://arxiv.org/pdf/2505.19443v1",
    "published_date": "2025-05-26 03:00:21 UTC",
    "updated_date": "2025-05-26 03:00:21 UTC"
  },
  {
    "arxiv_id": "2505.19442v3",
    "title": "Style2Code: A Style-Controllable Code Generation Framework with Dual-Modal Contrastive Representation Learning",
    "authors": [
      "Dutao Zhang",
      "Nicolas Rafael Arroyo Arias",
      "YuLong He",
      "Sergey Kovalchuk"
    ],
    "abstract": "Controllable code generation, the ability to synthesize code that follows a specified style while maintaining functionality, remains a challenging task. We propose a two-stage training framework combining contrastive learning and conditional decoding to enable flexible style control. The first stage aligns code style representations with semantic and structural features. In the second stage, we fine-tune a language model (e.g., Flan-T5) conditioned on the learned style vector to guide generation. Our method supports style interpolation and user personalization via lightweight mixing. Compared to prior work, our unified framework offers improved stylistic control without sacrificing code correctness. This is among the first approaches to combine contrastive alignment with conditional decoding for style-guided code generation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19442v3",
    "published_date": "2025-05-26 03:00:20 UTC",
    "updated_date": "2025-11-06 08:23:05 UTC"
  },
  {
    "arxiv_id": "2505.19441v2",
    "title": "Fairness-in-the-Workflow: How Machine Learning Practitioners at Big Tech Companies Approach Fairness in Recommender Systems",
    "authors": [
      "Jing Nathan Yan",
      "Emma Harvey",
      "Junxiong Wang",
      "Jeffrey M. Rzeszotarski",
      "Allison Koenecke"
    ],
    "abstract": "Recommender systems (RS), which are widely deployed across high-stakes domains, are susceptible to biases that can cause large-scale societal impacts. Researchers have proposed methods to measure and mitigate such biases -- but translating academic theory into practice is inherently challenging. RS practitioners must balance the competing interests of diverse stakeholders, including providers and users, and operate in dynamic environments. Through a semi-structured interview study (N=11), we map the RS practitioner workflow within large technology companies, focusing on how technical teams consider fairness internally and in collaboration with other (legal, data, and fairness) teams. We identify key challenges to incorporating fairness into existing RS workflows: defining fairness in RS contexts, particularly when navigating multi-stakeholder and dynamic fairness considerations. We also identify key organization-wide challenges: making time for fairness work and facilitating cross-team communication. Finally, we offer actionable recommendations for the RS community, including HCI researchers and practitioners.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19441v2",
    "published_date": "2025-05-26 02:59:57 UTC",
    "updated_date": "2025-09-18 18:37:12 UTC"
  },
  {
    "arxiv_id": "2505.19436v1",
    "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents",
    "authors": [
      "Ye Ye"
    ],
    "abstract": "Large Language Models (LLMs) falter in multi-step interactions -- often hallucinating, repeating actions, or misinterpreting user corrections -- due to reliance on linear, unstructured context. This fragility stems from the lack of persistent memory to track evolving goals and task dependencies, undermining trust in autonomous agents. We introduce the Task Memory Engine (TME), a modular memory controller that transforms existing LLMs into robust, revision-aware agents without fine-tuning. TME implements a spatial memory framework that replaces flat context with graph-based structures to support consistent, multi-turn reasoning. Departing from linear concatenation and ReAct-style prompting, TME builds a dynamic task graph -- either a tree or directed acyclic graph (DAG) -- to map user inputs to subtasks, align them with prior context, and enable dependency-tracked revisions. Its Task Representation and Intent Management (TRIM) component models task semantics and user intent to ensure accurate interpretation. Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns, outperforming ReAct. TME's modular design supports plug-and-play deployment and domain-specific customization, adaptable to both personal assistants and enterprise automation. We release TME's codebase, benchmarks, and components as open-source resources, enabling researchers to develop reliable LLM agents. TME's scalable architecture addresses a critical gap in agent performance across complex, interactive settings.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review. 9 pages main content, 15 pages appendix, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.19436v1",
    "published_date": "2025-05-26 02:53:22 UTC",
    "updated_date": "2025-05-26 02:53:22 UTC"
  },
  {
    "arxiv_id": "2505.19434v1",
    "title": "CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features",
    "authors": [
      "X. Feng",
      "D. Zhang",
      "S. Hu",
      "X. Li",
      "M. Wu",
      "J. Zhang",
      "X. Chen",
      "K. Huang"
    ],
    "abstract": "Effectively modeling and utilizing spatiotemporal features from RGB and other modalities (\\eg, depth, thermal, and event data, denoted as X) is the core of RGB-X tracker design. Existing methods often employ two parallel branches to separately process the RGB and X input streams, requiring the model to simultaneously handle two dispersed feature spaces, which complicates both the model structure and computation process. More critically, intra-modality spatial modeling within each dispersed space incurs substantial computational overhead, limiting resources for inter-modality spatial modeling and temporal modeling. To address this, we propose a novel tracker, CSTrack, which focuses on modeling Compact Spatiotemporal features to achieve simple yet effective tracking. Specifically, we first introduce an innovative Spatial Compact Module that integrates the RGB-X dual input streams into a compact spatial feature, enabling thorough intra- and inter-modality spatial modeling. Additionally, we design an efficient Temporal Compact Module that compactly represents temporal features by constructing the refined target distribution heatmap. Extensive experiments validate the effectiveness of our compact spatiotemporal modeling method, with CSTrack achieving new SOTA results on mainstream RGB-X benchmarks. The code and models will be released at: https://github.com/XiaokunFeng/CSTrack.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICML25!",
    "pdf_url": "https://arxiv.org/pdf/2505.19434v1",
    "published_date": "2025-05-26 02:53:12 UTC",
    "updated_date": "2025-05-26 02:53:12 UTC"
  },
  {
    "arxiv_id": "2505.19430v3",
    "title": "Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation",
    "authors": [
      "Keane Ong",
      "Rui Mao",
      "Deeksha Varshney",
      "Paul Pu Liang",
      "Erik Cambria",
      "Gianmarco Mengaldo"
    ],
    "abstract": "Counterfactual reasoning typically involves considering alternatives to actual events. While often applied to understand past events, a distinct form-forward counterfactual reasoning-focuses on anticipating plausible future developments. This type of reasoning is invaluable in dynamic financial markets, where anticipating market developments can powerfully unveil potential risks and opportunities for stakeholders, guiding their decision-making. However, performing this at scale is challenging due to the cognitive demands involved, underscoring the need for automated solutions. LLMs offer promise, but remain unexplored for this application. To address this gap, we introduce a novel benchmark, FIN-FORCE-FINancial FORward Counterfactual Evaluation. By curating financial news headlines and providing structured evaluation, FIN-FORCE supports LLM based forward counterfactual generation. This paves the way for scalable and automated solutions for exploring and anticipating future market developments, thereby providing structured insights for decision-making. Through experiments on FIN-FORCE, we evaluate state-of-the-art LLMs and counterfactual generation methods, analyzing their limitations and proposing insights for future research. We release the benchmark, supplementary data and all experimental codes at the following link: https://github.com/keanepotato/fin_force",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at Empirical Methods in Natural Language Processing 2025 (Main Conference) (Oral)",
    "pdf_url": "https://arxiv.org/pdf/2505.19430v3",
    "published_date": "2025-05-26 02:41:50 UTC",
    "updated_date": "2025-10-01 19:09:32 UTC"
  },
  {
    "arxiv_id": "2505.19427v1",
    "title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference",
    "authors": [
      "Sihan Chen",
      "Dan Zhao",
      "Jongwoo Ko",
      "Colby Banbury",
      "Huiping Zhuang",
      "Luming Liang",
      "Tianyi Chen"
    ],
    "abstract": "The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activation methods offer broader applicability and superior resource efficiency through their plug-and-play design. However, many existing methods rely solely on hidden state magnitudes to determine activation, resulting in high approximation errors and suboptimal inference accuracy. To address these limitations, we propose WINA (Weight Informed Neuron Activation), a novel, simple, and training-free sparse activation framework that jointly considers hidden state magnitudes and the column-wise $\\ell_2$-norms of weight matrices. We show that this leads to a sparsification strategy that obtains optimal approximation error bounds with theoretical guarantees tighter than existing techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g., TEAL) by up to $2.94\\%$ in average performance at the same sparsity levels, across a diverse set of LLM architectures and datasets. These results position WINA as a new performance frontier for training-free sparse activation in LLM inference, advancing training-free sparse activation methods and setting a robust baseline for efficient inference. The source code is available at https://github.com/microsoft/wina.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19427v1",
    "published_date": "2025-05-26 02:37:32 UTC",
    "updated_date": "2025-05-26 02:37:32 UTC"
  },
  {
    "arxiv_id": "2505.19426v2",
    "title": "The Role of Diversity in In-Context Learning for Large Language Models",
    "authors": [
      "Wenyang Xiao",
      "Haoyu Zhao",
      "Lingxiao Huang"
    ],
    "abstract": "In-context learning (ICL) is a crucial capability of current large language models (LLMs), where the selection of examples plays a key role in performance. While most existing approaches focus on selecting the most similar examples to the query, the impact of diversity in example selection remains underexplored. We systematically investigate the role of diversity in in-context example selection through experiments across a range of tasks, from sentiment classification to more challenging math and code problems. Experiments on Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that diversity-aware selection methods improve performance, particularly on complex tasks like math and code, and enhance robustness to out-of-distribution queries. To support these findings, we introduce a theoretical framework that explains the benefits of incorporating diversity in in-context example selection.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "30 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.19426v2",
    "published_date": "2025-05-26 02:37:26 UTC",
    "updated_date": "2025-06-05 08:20:31 UTC"
  },
  {
    "arxiv_id": "2505.19423v2",
    "title": "Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network",
    "authors": [
      "Bingdong Li",
      "Mei Jiang",
      "Hong Qian",
      "Ke Tang",
      "Aimin Zhou",
      "Peng Yang"
    ],
    "abstract": "Evolutionary Reinforcement Learning (ERL), training the Reinforcement Learning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated enhanced exploration capabilities and greater robustness than using traditional policy gradient. However, ERL suffers from the high computational costs and low search efficiency, as EAs require evaluating numerous candidate policies with expensive simulations, many of which are ineffective and do not contribute meaningfully to the training. One intuitive way to reduce the ineffective evaluations is to adopt the surrogates. Unfortunately, existing ERL policies are often modeled as deep neural networks (DNNs) and thus naturally represented as high-dimensional vectors containing millions of weights, which makes the building of effective surrogates for ERL policies extremely challenging. This paper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE) and Hyperbolic Neural Networks (HNN). Specifically, AE compresses high-dimensional policies into low-dimensional representations while extracting key features as the inputs for the surrogate. HNN, functioning as a classification-based surrogate model, can learn complex nonlinear relationships from sampled data and enable more accurate pre-selection of the sampled policies without real evaluations. The experiments on 10 Atari and 4 Mujoco games have verified that the proposed method outperforms previous approaches significantly. The search trajectories guided by AE and HNN are also visually demonstrated to be more effective, in terms of both exploration and convergence. This paper not only presents the first learnable policy embedding and surrogate-modeling modules for high-dimensional ERL policies, but also empirically reveals when and why they can be successful.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19423v2",
    "published_date": "2025-05-26 02:25:17 UTC",
    "updated_date": "2025-05-29 05:52:23 UTC"
  },
  {
    "arxiv_id": "2505.19419v2",
    "title": "It's Not Just Labeling -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features",
    "authors": [
      "Baichuan Li",
      "Larry Powell",
      "Tracy Hammond"
    ],
    "abstract": "The quality of training data is critical to the performance of machine learning applications in domains like transportation, healthcare, and robotics. Accurate image labeling, however, often relies on time-consuming, expert-driven methods with limited feedback. This research introduces a sketch-based annotation approach supported by large language models (LLMs) to reduce technical barriers and enhance accessibility. Using a synthetic dataset, we examine how sketch recognition features relate to LLM feedback metrics, aiming to improve the reliability and interpretability of LLM-assisted labeling. We also explore how prompting strategies and sketch variations influence feedback quality. Our main contribution is a sketch-based virtual assistant that simplifies annotation for non-experts and advances LLM-driven labeling tools in terms of scalability, accessibility, and explainability.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19419v2",
    "published_date": "2025-05-26 02:13:52 UTC",
    "updated_date": "2025-05-27 02:53:28 UTC"
  },
  {
    "arxiv_id": "2505.19414v1",
    "title": "Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study",
    "authors": [
      "Ruihang Wang",
      "Zhiwei Cao",
      "Qingang Zhang",
      "Rui Tan",
      "Yonggang Wen",
      "Tommy Leung",
      "Stuart Kennedy",
      "Justin Teoh"
    ],
    "abstract": "Data centers are the backbone of computing capacity. Operating data centers in the tropical regions faces unique challenges due to consistently high ambient temperature and elevated relative humidity throughout the year. These conditions result in increased cooling costs to maintain the reliability of the computing systems. While existing machine learning-based approaches have demonstrated potential to elevate operations to a more proactive and intelligent level, their deployment remains dubious due to concerns about model extrapolation capabilities and associated system safety issues. To address these concerns, this article proposes incorporating the physical characteristics of data centers into traditional data-driven machine learning solutions. We begin by introducing the data center system, including the relevant multiphysics processes and the data-physics availability. Next, we outline the associated modeling and optimization problems and propose an integrated, physics-informed machine learning system to address them. Using the proposed system, we present relevant applications across varying levels of operational intelligence. A case study on an industry-grade tropical data center is provided to demonstrate the effectiveness of our approach. Finally, we discuss key challenges and highlight potential future directions.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19414v1",
    "published_date": "2025-05-26 02:06:45 UTC",
    "updated_date": "2025-05-26 02:06:45 UTC"
  },
  {
    "arxiv_id": "2505.19409v1",
    "title": "Fusion Intelligence for Digital Twinning AI Data Centers: A Synergistic GenAI-PhyAI Approach",
    "authors": [
      "Ruihang Wang",
      "Minghao Li",
      "Zhiwei Cao",
      "Jimin Jia",
      "Kyle Guan",
      "Yonggang Wen"
    ],
    "abstract": "The explosion in artificial intelligence (AI) applications is pushing the development of AI-dedicated data centers (AIDCs), creating management challenges that traditional methods and standalone AI solutions struggle to address. While digital twins are beneficial for AI-based design validation and operational optimization, current AI methods for their creation face limitations. Specifically, physical AI (PhyAI) aims to capture the underlying physical laws, which demands extensive, case-specific customization, and generative AI (GenAI) can produce inaccurate or hallucinated results. We propose Fusion Intelligence, a novel framework synergizing GenAI's automation with PhyAI's domain grounding. In this dual-agent collaboration, GenAI interprets natural language prompts to generate tokenized AIDC digital twins. Subsequently, PhyAI optimizes these generated twins by enforcing physical constraints and assimilating real-time data. Case studies demonstrate the advantages of our framework in automating the creation and validation of AIDC digital twins. These twins deliver predictive analytics to support power usage effectiveness (PUE) optimization in the design stage. With operational data collected, the digital twin accuracy is further improved compared with pure physics-based models developed by human experts. Fusion Intelligence offers a promising pathway to accelerate digital transformation. It enables more reliable and efficient AI-driven digital transformation for a broad range of mission-critical infrastructures.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19409v1",
    "published_date": "2025-05-26 01:58:34 UTC",
    "updated_date": "2025-05-26 01:58:34 UTC"
  },
  {
    "arxiv_id": "2505.19406v1",
    "title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model",
    "authors": [
      "Tianle Li",
      "Jihai Zhang",
      "Yongming Rao",
      "Yu Cheng"
    ],
    "abstract": "While large language models (LLMs) demonstrate strong reasoning capabilities utilizing reinforcement learning (RL) with verifiable reward, whether large vision-language models (VLMs) can directly inherit such capabilities through similar post-training strategies remains underexplored. In this work, we conduct a systematic compositional probing study to evaluate whether current VLMs trained with RL or other post-training strategies can compose capabilities across modalities or tasks under out-of-distribution conditions. We design a suite of diagnostic tasks that train models on unimodal tasks or isolated reasoning skills, and evaluate them on multimodal, compositional variants requiring skill integration. Through comparisons between supervised fine-tuning (SFT) and RL-trained models, we identify three key findings: (1) RL-trained models consistently outperform SFT on compositional generalization, demonstrating better integration of learned skills; (2) although VLMs achieve strong performance on individual tasks, they struggle to generalize compositionally under cross-modal and cross-task scenario, revealing a significant gap in current training strategies; (3) enforcing models to explicitly describe visual content before reasoning (e.g., caption-before-thinking), along with rewarding progressive vision-to-text grounding, yields notable gains. It highlights two essential ingredients for improving compositionality in VLMs: visual-to-text alignment and accurate visual grounding. Our findings shed light on the current limitations of RL-based reasoning VLM training and provide actionable insights toward building models that reason compositionally across modalities and tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19406v1",
    "published_date": "2025-05-26 01:42:38 UTC",
    "updated_date": "2025-05-26 01:42:38 UTC"
  },
  {
    "arxiv_id": "2505.19404v1",
    "title": "Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning",
    "authors": [
      "Yuta Ono",
      "Hiroshi Nakamura",
      "Hideki Takase"
    ],
    "abstract": "Federated Active Learning (FAL) seeks to reduce the burden of annotation under the realistic constraints of federated learning by leveraging Active Learning (AL). As FAL settings make it more expensive to obtain ground truth labels, FAL strategies that work well in low-budget regimes, where the amount of annotation is very limited, are needed. In this work, we investigate the effectiveness of TypiClust, a successful low-budget AL strategy, in low-budget FAL settings. Our empirical results show that TypiClust works well even in low-budget FAL settings contrasted with relatively low performances of other methods, although these settings present additional challenges, such as data heterogeneity, compared to AL. In addition, we show that FAL settings cause distribution shifts in terms of typicality, but TypiClust is not very vulnerable to the shifts. We also analyze the sensitivity of TypiClust to feature extraction methods, and it suggests a way to perform FAL even in limited data situations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages. Accepted at COMPSAC 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.19404v1",
    "published_date": "2025-05-26 01:40:52 UTC",
    "updated_date": "2025-05-26 01:40:52 UTC"
  },
  {
    "arxiv_id": "2505.19402v1",
    "title": "Recalibrating the Compass: Integrating Large Language Models into Classical Research Methods",
    "authors": [
      "Tai-Quan Peng",
      "Xuzhen Yang"
    ],
    "abstract": "This paper examines how large language models (LLMs) are transforming core quantitative methods in communication research in particular, and in the social sciences more broadly-namely, content analysis, survey research, and experimental studies. Rather than replacing classical approaches, LLMs introduce new possibilities for coding and interpreting text, simulating dynamic respondents, and generating personalized and interactive stimuli. Drawing on recent interdisciplinary work, the paper highlights both the potential and limitations of LLMs as research tools, including issues of validity, bias, and interpretability. To situate these developments theoretically, the paper revisits Lasswell's foundational framework -- \"Who says what, in which channel, to whom, with what effect?\" -- and demonstrates how LLMs reconfigure message studies, audience analysis, and effects research by enabling interpretive variation, audience trajectory modeling, and counterfactual experimentation. Revisiting the metaphor of the methodological compass, the paper argues that classical research logics remain essential as the field integrates LLMs and generative AI. By treating LLMs not only as technical instruments but also as epistemic and cultural tools, the paper calls for thoughtful, rigorous, and imaginative use of LLMs in future communication and social science research.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19402v1",
    "published_date": "2025-05-26 01:38:02 UTC",
    "updated_date": "2025-05-26 01:38:02 UTC"
  },
  {
    "arxiv_id": "2505.19395v1",
    "title": "VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation",
    "authors": [
      "Ethan TS. Liu",
      "Austin Wang",
      "Spencer Mateega",
      "Carlos Georgescu",
      "Danny Tang"
    ],
    "abstract": "Ensuring that large language models (LLMs) can effectively assess, detect, explain, and remediate software vulnerabilities is critical for building robust and secure software systems. We introduce VADER, a human-evaluated benchmark designed explicitly to assess LLM performance across four key vulnerability-handling dimensions: assessment, detection, explanation, and remediation. VADER comprises 174 real-world software vulnerabilities, each carefully curated from GitHub repositories and annotated by security experts. For each vulnerability case, models are tasked with identifying the flaw, classifying it using Common Weakness Enumeration (CWE), explaining its underlying cause, proposing a patch, and formulating a test plan. Using a one-shot prompting strategy, we benchmark six state-of-the-art LLMs (Claude 3.7 Sonnet, Gemini 2.5 Pro, GPT-4.1, GPT-4.5, Grok 3 Beta, and o3) on VADER, and human security experts evaluated each response according to a rigorous scoring rubric emphasizing remediation (quality of the code fix, 50%), explanation (20%), and classification and test plan (30%) according to a standardized rubric. Our results show that current state-of-the-art LLMs achieve only moderate success on VADER - OpenAI's o3 attained 54.7% accuracy overall, with others in the 49-54% range, indicating ample room for improvement. Notably, remediation quality is strongly correlated (Pearson r > 0.97) with accurate classification and test plans, suggesting that models that effectively categorize vulnerabilities also tend to fix them well. VADER's comprehensive dataset, detailed evaluation rubrics, scoring tools, and visualized results with confidence intervals are publicly released, providing the community with an interpretable, reproducible benchmark to advance vulnerability-aware LLMs. All code and data are available at: https://github.com/AfterQuery/vader",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "16 pages, 8 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.19395v1",
    "published_date": "2025-05-26 01:20:44 UTC",
    "updated_date": "2025-05-26 01:20:44 UTC"
  },
  {
    "arxiv_id": "2505.19392v1",
    "title": "Simple and Effective Baselines for Code Summarisation Evaluation",
    "authors": [
      "Jade Robinson",
      "Jonathan K. Kummerfeld"
    ],
    "abstract": "Code documentation is useful, but writing it is time-consuming. Different techniques for generating code summaries have emerged, but comparing them is difficult because human evaluation is expensive and automatic metrics are unreliable. In this paper, we introduce a simple new baseline in which we ask an LLM to give an overall score to a summary. Unlike n-gram and embedding-based baselines, our approach is able to consider the code when giving a score. This allows us to also make a variant that does not consider the reference summary at all, which could be used for other tasks, e.g., to evaluate the quality of documentation in code bases. We find that our method is as good or better than prior metrics, though we recommend using it in conjunction with embedding-based methods to avoid the risk of LLM-specific bias.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19392v1",
    "published_date": "2025-05-26 01:16:41 UTC",
    "updated_date": "2025-05-26 01:16:41 UTC"
  },
  {
    "arxiv_id": "2505.19386v2",
    "title": "Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals",
    "authors": [
      "Nate Gillman",
      "Charles Herrmann",
      "Michael Freeman",
      "Daksh Aggarwal",
      "Evan Luo",
      "Deqing Sun",
      "Chen Sun"
    ],
    "abstract": "Recent advances in video generation models have sparked interest in world models capable of simulating realistic environments. While navigation has been well-explored, physically meaningful interactions that mimic real-world forces remain largely understudied. In this work, we investigate using physical forces as a control signal for video generation and propose force prompts which enable users to interact with images through both localized point forces, such as poking a plant, and global wind force fields, such as wind blowing on fabric. We demonstrate that these force prompts can enable videos to respond realistically to physical control signals by leveraging the visual and motion prior in the original pretrained model, without using any 3D asset or physics simulator at inference. The primary challenge of force prompting is the difficulty in obtaining high quality paired force-video training data, both in the real world due to the difficulty of obtaining force signals, and in synthetic data due to limitations in the visual quality and domain diversity of physics simulators. Our key finding is that video generation models can generalize remarkably well when adapted to follow physical force conditioning from videos synthesized by Blender, even with limited demonstrations of few objects. Our method can generate videos which simulate forces across diverse geometries, settings, and materials. We also try to understand the source of this generalization and perform ablations that reveal two key elements: visual diversity and the use of specific text keywords during training. Our approach is trained on only around 15k training examples for a single day on four A100 GPUs, and outperforms existing methods on force adherence and physics realism, bringing world models closer to real-world physics interactions. We release all datasets, code, weights, and interactive video demos at our project page.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Camera ready version (NeurIPS 2025). Code and interactive demos at https://force-prompting.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2505.19386v2",
    "published_date": "2025-05-26 01:04:02 UTC",
    "updated_date": "2025-11-26 16:02:59 UTC"
  },
  {
    "arxiv_id": "2505.19385v2",
    "title": "Advancing Limited-Angle CT Reconstruction Through Diffusion-Based Sinogram Completion",
    "authors": [
      "Jiaqi Guo",
      "Santiago Lopez-Tapia",
      "Aggelos K. Katsaggelos"
    ],
    "abstract": "Limited Angle Computed Tomography (LACT) often faces significant challenges due to missing angular information. Unlike previous methods that operate in the image domain, we propose a new method that focuses on sinogram inpainting. We leverage MR-SDEs, a variant of diffusion models that characterize the diffusion process with mean-reverting stochastic differential equations, to fill in missing angular data at the projection level. Furthermore, by combining distillation with constraining the output of the model using the pseudo-inverse of the inpainting matrix, the diffusion process is accelerated and done in a step, enabling efficient and accurate sinogram completion. A subsequent post-processing module back-projects the inpainted sinogram into the image domain and further refines the reconstruction, effectively suppressing artifacts while preserving critical structural details. Quantitative experimental results demonstrate that the proposed method achieves state-of-the-art performance in both perceptual and fidelity quality, offering a promising solution for LACT reconstruction in scientific and clinical applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at the 2025 IEEE International Conference on Image Processing (Oral)",
    "pdf_url": "https://arxiv.org/pdf/2505.19385v2",
    "published_date": "2025-05-26 00:59:58 UTC",
    "updated_date": "2025-11-24 22:53:15 UTC"
  },
  {
    "arxiv_id": "2505.19383v1",
    "title": "CaseEdit: Enhancing Localized Commonsense Reasoning via Null-Space Constrained Knowledge Editing in Small Parameter Language Models",
    "authors": [
      "Varun Reddy",
      "Yen-Ling Kuo"
    ],
    "abstract": "Large language models (LLMs) exhibit strong performance on factual recall and general reasoning but struggle to adapt to user-specific, commonsense knowledge, a challenge particularly acute in small-parameter settings where computational efficiency is prioritized. We introduce CaseEdit, a new dataset and generation pipeline for evaluating localized, personalized commonsense knowledge editing in small LLMs to address this. Built upon the ATOMIC20/20 commonsense graph, CaseEdit uses a multi-stage inference process to generate both typical and atypical contextual edits for household objects, paired with targeted evaluation questions across four axes: reliability, generalization, locality, and portability. We evaluate established knowledge editing methods using CaseEdit and demonstrate that AlphaEdit, a technique employing null-space projection to minimize interference with unrelated knowledge, consistently outperforms other methods when applied to an LLaMA 3.2 3B model, even in scalability tests, showing minimal ripple effects. Our results indicate that using CaseEdit with effective editing techniques like AlphaEdit allows small models to internalize high-quality, context-sensitive common-sense knowledge, paving the way for lightweight, personalized assistants.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.19383v1",
    "published_date": "2025-05-26 00:54:04 UTC",
    "updated_date": "2025-05-26 00:54:04 UTC"
  },
  {
    "arxiv_id": "2505.19381v4",
    "title": "DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving",
    "authors": [
      "Anqing Jiang",
      "Yu Gao",
      "Zhigang Sun",
      "Yiru Wang",
      "Jijun Wang",
      "Jinghao Chai",
      "Qian Cao",
      "Yuweng Heng",
      "Hao Jiang",
      "Yunda Dong",
      "Zongzheng Zhang",
      "Xianda Guo",
      "Hao Sun",
      "Hao Zhao"
    ],
    "abstract": "Research interest in end-to-end autonomous driving has surged owing to its fully differentiable design integrating modular tasks, i.e. perception, prediction and planing, which enables optimization in pursuit of the ultimate goal. Despite the great potential of the end-to-end paradigm, existing methods suffer from several aspects including expensive BEV (bird's eye view) computation, action diversity, and sub-optimal decision in complex real-world scenarios. To address these challenges, we propose a novel hybrid sparse-dense diffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA. We explore the sparse diffusion representation for efficient multi-modal driving behavior. Moreover, we rethink the effectiveness of VLM driving decision and improve the trajectory generation guidance through deep interaction across agent, map instances and VLM output. Our method shows superior performance in Autonomous Grand Challenge 2025 which contains challenging real and reactive synthetic scenarios. Our methods achieves 45.0 PDMS.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "4pages",
    "pdf_url": "https://arxiv.org/pdf/2505.19381v4",
    "published_date": "2025-05-26 00:49:35 UTC",
    "updated_date": "2025-06-03 02:28:31 UTC"
  }
]
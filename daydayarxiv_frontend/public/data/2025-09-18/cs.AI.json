{
  "date": "2025-09-18",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-09-18 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv è®ºæ–‡çˆ†å‘å¼æ›´æ–°ï¼Œé‡ç‚¹å…³æ³¨ **DARPA AI ç½‘ç»œæŒ‘æˆ˜èµ›å† å†›ç³»ç»Ÿ ATLANTIS** çš„æŠ€æœ¯ç»†èŠ‚æ­ç§˜ï¼Œä»¥åŠ **WorldForge** åœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ 3D/4D ç”Ÿæˆã€‚æ­¤å¤–ï¼Œ**Agentic AIï¼ˆä»£ç† AIï¼‰** åœ¨å¹»è§‰æŠ‘åˆ¶ã€çº¢é˜Ÿæµ‹è¯•ï¼ˆRed Teamingï¼‰å’Œå¿ƒç†å¥åº·å’¨è¯¢ä¸­çš„åº”ç”¨ä¹Ÿæ˜¯ä»Šå¤©çš„çƒ­é—¨è¯é¢˜ã€‚\n\n---\n\n### ğŸš€ å°é¢å¤´æ¡ï¼šå† å†›ç³»ç»Ÿä¸ 3D ç”Ÿæˆ\n\n**1. ATLANTISï¼šAI é©±åŠ¨çš„å¨èƒå®šä½ã€åˆ†æä¸åˆ†æµæƒ…æŠ¥ç³»ç»Ÿ (DARPA æŒ‘æˆ˜èµ›å† å†›)**\n**# title: ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System**\n> **Authors:** Taesoo Kim et al. (Team Atlanta: Georgia Tech, Samsung, KAIST, POSTECH)\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** æ­ç§˜äº†èµ¢å¾— DARPA AI Cyber Challenge (AIxCC) å†³èµ›å† å†›çš„ç³»ç»Ÿ ATLANTISã€‚è¯¥ç³»ç»Ÿé€šè¿‡é›†æˆ LLM ä¸ä¼ ç»Ÿçš„ç¨‹åºåˆ†ææŠ€æœ¯ï¼ˆç¬¦å·æ‰§è¡Œã€å®šå‘æ¨¡ç³Šæµ‹è¯•ã€é™æ€åˆ†æï¼‰ï¼Œè§£å†³äº†è‡ªåŠ¨æ¼æ´å‘ç°å’Œä¿®å¤ä¸­çš„æ‰©å±•æ€§å’Œå‡†ç¡®æ€§é—®é¢˜ã€‚\n- **å‘ç°ï¼š** ATLANTIS å±•ç¤ºäº†å¦‚ä½•å°†ç¥ç»ç¬¦å·ï¼ˆNeuro-symbolicï¼‰æ–¹æ³•åº”ç”¨äºå¤§è§„æ¨¡è½¯ä»¶å®‰å…¨ï¼Œä¸ä»…èƒ½å‘ç°æ·±å±‚æ¼æ´ï¼Œè¿˜èƒ½ç”Ÿæˆè¯­ä¹‰æ­£ç¡®ä¸”ä¿ç•™åŸæœ‰è¡Œä¸ºçš„è¡¥ä¸ã€‚\n\n**2. WorldForgeï¼šé€šè¿‡å…è®­ç»ƒå¼•å¯¼åœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­è§£é”æ¶Œç°çš„ 3D/4D ç”Ÿæˆèƒ½åŠ›**\n**# title: WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance**\n> **Authors:** Chenxi Song, et al.\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†æ¡†æ¶ WorldForgeï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­è•´å«çš„ä¸–ç•Œå…ˆéªŒè¿›è¡Œ 3D/4D ç”Ÿæˆã€‚\n- **æ–¹æ³•ï¼š** åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šæ­¥å†…é€’å½’ç»†åŒ–ï¼ˆæ³¨å…¥è½¨è¿¹å¼•å¯¼ï¼‰ã€æµé—¨æ§æ½œåœ¨èåˆï¼ˆè§£è€¦è¿åŠ¨ä¸å¤–è§‚ï¼‰ã€åŒè·¯å¾„è‡ªæ ¡æ­£å¼•å¯¼ï¼ˆä¿®æ­£è½¨è¿¹æ¼‚ç§»ï¼‰ã€‚\n- **æ„ä¹‰ï¼š** è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ—¶ç©ºä¸€è‡´æ€§å’Œåœºæ™¯-ç›¸æœºåŠ¨æ€çº ç¼ æ–¹é¢çš„é—®é¢˜ï¼Œå®ç°äº†é«˜ä¿çœŸçš„ 3D/4D å†…å®¹ç”Ÿæˆã€‚\n\n**3. SmolRGPTï¼šç”¨äºä»“åº“ç¯å¢ƒçš„ 600M å‚æ•°é«˜æ•ˆç©ºé—´æ¨ç†æ¨¡å‹**\n**# title: SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters**\n> **Authors:** Abdarahmane Traore, et al.\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹èµ„æºå—é™çš„ä»“åº“æœºå™¨äººç¯å¢ƒï¼Œæå‡ºäº†ä¸€ä¸ªä»… 600M å‚æ•°çš„å°å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚\n- **æ–¹æ³•ï¼š** ç»“åˆ RGB å’Œæ·±åº¦çº¿ç´¢ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œé€æ­¥å¯¹é½è§†è§‰è¯­è¨€ç‰¹å¾å¹¶å¼ºåŒ–ç©ºé—´å…³ç³»ç†è§£ã€‚\n- **å‘ç°ï¼š** åœ¨æå°çš„å‚æ•°é‡ä¸‹ï¼Œå…¶åœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­åŒ¹é…ç”šè‡³è¶…è¿‡äº†æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼Œè¯æ˜äº†â€œå°è€Œç¾â€æ¨¡å‹åœ¨ç‰¹å®šå·¥ä¸šåœºæ™¯çš„æ½œåŠ›ã€‚\n\n---\n\n### ğŸ¤– Agentic AI & LLM æ¨ç†ï¼šå¹»è§‰ã€å®‰å…¨ä¸å¯¹é½\n\n**4. ORCAï¼šè§†è§‰è¯­è¨€æ¨¡å‹ä¸­é’ˆå¯¹å¹»è§‰å’Œå¯¹æŠ—é²æ£’æ€§çš„ä»£ç†æ¨ç†**\n**# title: ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models**\n> **Authors:** Chung-En Johnny Yu, et al.\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡º ORCA æ¡†æ¶ï¼Œåˆ©ç”¨ä¸€ç»„å°å‹è§†è§‰æ¨¡å‹ï¼ˆ<3B å‚æ•°ï¼‰ä½œä¸ºå·¥å…·ï¼Œé€šè¿‡â€œè§‚å¯Ÿ-æ¨ç†-æ‰¹åˆ¤-è¡ŒåŠ¨â€å¾ªç¯æ¥éªŒè¯å¤§å‹ VLM çš„è¾“å‡ºã€‚\n- **æ•ˆæœï¼š** åœ¨ä¸é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å‡å°‘äº†ç‰©ä½“çº§å¹»è§‰ï¼Œå¹¶å±•ç°å‡ºå¯¹å¯¹æŠ—æ€§æ”»å‡»çš„æ¶Œç°é²æ£’æ€§ï¼ˆPOPE åŸºå‡†ä¸Šæå‡é«˜è¾¾ 40%ï¼‰ã€‚\n\n**5. Empathy-R1ï¼šç”¨äºé•¿ç¯‡å¿ƒç†å¥åº·æ”¯æŒçš„å…±æƒ…é“¾ä¸å¼ºåŒ–å­¦ä¹ æ¡†æ¶**\n**# title: Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support**\n> **Authors:** Xianrong Yao, et al.\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** å¼•å…¥â€œå…±æƒ…é“¾â€ï¼ˆChain-of-Empathy, CoEï¼‰æ¨ç†è¿‡ç¨‹ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥å¤„ç†é•¿ç¯‡å¿ƒç†å’¨è¯¢æ–‡æœ¬ã€‚\n- **æ–¹æ³•ï¼š** æ¨¡ä»¿è®¤çŸ¥è¡Œä¸ºç–—æ³•ï¼Œå¼•å¯¼æ¨¡å‹é€æ­¥æ¨ç†æ±‚åŠ©è€…çš„æƒ…ç»ªã€æˆå› å’Œæ„å›¾ã€‚å‘å¸ƒäº†ä¸­æ–‡æ•°æ®é›† Empathy-QAã€‚\n\n**6. MUSEï¼šMCTS é©±åŠ¨çš„çº¢é˜Ÿæµ‹è¯•æ¡†æ¶ï¼Œç”¨äºå¢å¼º LLM å¤šè½®å¯¹è¯å®‰å…¨æ€§**\n**# title: MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models**\n> **Authors:** Siyu Yan, et al. (Accepted by EMNLP 2025)\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹å¤šè½®å¯¹è¯ä¸­çš„è¶Šç‹±æ”»å‡»ï¼Œæå‡ºäº† MUSE æ¡†æ¶ã€‚æ”»å‡»ç«¯åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¢ç´¢è¯­ä¹‰è½¨è¿¹ï¼›é˜²å¾¡ç«¯è¿›è¡Œç»†ç²’åº¦çš„å®‰å…¨å¯¹é½ã€‚\n\n**7. FlowRLï¼šåŒ¹é…å¥–åŠ±åˆ†å¸ƒä»¥ä¼˜åŒ– LLM æ¨ç†**\n**# title: FlowRL: Matching Reward Distributions for LLM Reasoning**\n> **Authors:** Xuekai Zhu, et al.\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** è®¤ä¸ºç°æœ‰çš„ PPO/GRPO ç­‰æœ€å¤§åŒ–å¥–åŠ±çš„æ–¹æ³•ä¼šå‡å°‘å¤šæ ·æ€§ã€‚FlowRL é€šè¿‡æµå¹³è¡¡ï¼ˆflow balancingï¼‰æœ€å°åŒ–ç­–ç•¥ä¸ç›®æ ‡åˆ†å¸ƒçš„åå‘ KL æ•£åº¦ã€‚\n- **æ•ˆæœï¼š** åœ¨æ•°å­¦å’Œä»£ç æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äº GRPO å’Œ PPOï¼Œä¿ƒè¿›äº†æ›´å…·æ¢ç´¢æ€§çš„æ¨ç†è·¯å¾„ã€‚\n\n---\n\n### ğŸ‘ï¸ è®¡ç®—æœºè§†è§‰ä¸å¤šæ¨¡æ€\n\n**8. å¤§å‹è§†è§‰æ¨¡å‹èƒ½è§£å†³å¿ƒç†æ—‹è½¬é—®é¢˜å—ï¼Ÿ**\n**# title: Large Vision Models Can Solve Mental Rotation Problems**\n> **Authors:** Sebastian Ray Mason, et al.\n- **å‘ç°ï¼š** ç³»ç»Ÿè¯„ä¼°äº† ViT, CLIP, DINOv2 ç­‰æ¨¡å‹åœ¨å¿ƒç†æ—‹è½¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å‘ç°è‡ªç›‘ç£æ¨¡å‹ï¼ˆå¦‚ DINOï¼‰æ¯”ç›‘ç£æ¨¡å‹æ›´èƒ½æ•æ‰å‡ ä½•ç»“æ„ï¼Œä¸”ä¸­é—´å±‚çš„è¡¨ç°ä¼˜äºæœ€ç»ˆå±‚ã€‚\n\n**9. è§†è§‰é”™è§‰ä½œä¸ºè§†è§‰æ¨¡å‹çš„æ„ŸçŸ¥å½’çº³åç½®**\n**# title: Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models**\n> **Authors:** Haobo Yang, et al.\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** å°†ç»å…¸çš„å‡ ä½•è§†è§‰é”™è§‰å¼•å…¥è®­ç»ƒæµç¨‹ã€‚å‘ç°åˆ©ç”¨é”™è§‰ä½œä¸ºè¾…åŠ©ç›‘ç£ä¿¡å·ï¼Œå¯ä»¥ç³»ç»Ÿæ€§åœ°æé«˜æ¨¡å‹åœ¨å¤æ‚çº¹ç†å’Œè½®å»“ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n**10. RoboEyeï¼šå¢å¼ºçš„ 2D æœºå™¨äººç‰©ä½“è¯†åˆ«**\n**# title: RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching**\n> **Authors:** Xingwu Zhang, et al.\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹ç”µå•†ä»“åº“ä¸­é•¿å°¾ã€é®æŒ¡ä¸¥é‡çš„ç‰©ä½“è¯†åˆ«é—®é¢˜ï¼Œæå‡ºä¸¤é˜¶æ®µæ¡†æ¶ã€‚åˆ©ç”¨è½»é‡çº§é€‚é…å™¨å’Œé€‰æ‹©æ€§ 3D å‡ ä½•å…³é”®ç‚¹åŒ¹é…ï¼Œåœ¨ä»…ä½¿ç”¨ RGB å›¾åƒçš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº† Recall@1ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€éšç§ä¸æ£€æµ‹\n\n**11. å‡ ä¹å…è´¹çš„ LLM è¶Šç‹±æ£€æµ‹**\n**# title: LLM Jailbreak Detection for (Almost) Free!**\n> **Authors:** Guorui Chen, et al.\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** å‘ç°è¶Šç‹±æç¤ºè¯å’Œè‰¯æ€§æç¤ºè¯çš„è¾“å‡ºåˆ†å¸ƒå­˜åœ¨å·®å¼‚ã€‚æå‡º FJD æ–¹æ³•ï¼Œé€šè¿‡é¢„ç½®è‚¯å®šæŒ‡ä»¤å¹¶ç¼©æ”¾ Logitsï¼Œåˆ©ç”¨é¦–ä¸ª token çš„ç½®ä¿¡åº¦è¿›è¡Œæ£€æµ‹ï¼Œå‡ ä¹ä¸å¢åŠ è®¡ç®—æˆæœ¬ã€‚\n\n**12. PRISMï¼šç”¨äº AI ç”Ÿæˆå›¾åƒæŒ‡çº¹è¯†åˆ«çš„ç›¸ä½å¢å¼ºå¾„å‘æ˜ å°„æ¡†æ¶**\n**# title: PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images**\n> **Authors:** Emanuele Ricco, et al.\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** åˆ©ç”¨å‚…é‡Œå¶å˜æ¢çš„å¾„å‘ç¼©å‡ï¼ˆç»“åˆå¹…åº¦å’Œç›¸ä½ä¿¡æ¯ï¼‰æ¥æ•æ‰æ¨¡å‹ç‰¹å®šçš„æŒ‡çº¹ï¼Œç”¨äºè¯†åˆ«å›¾åƒæ˜¯ç”±å“ªä¸ªç”Ÿæˆæ¨¡å‹ï¼ˆGAN æˆ– Diffusionï¼‰ç”Ÿæˆçš„ã€‚\n\n---\n\n### ğŸ”¬ AI for Science & Healthcare\n\n**13. TITANï¼šå¤§è§„æ¨¡ VQE ä¸­è‡ªé€‚åº”å‚æ•°å†»ç»“çš„è½¨è¿¹ä¿¡æ¯æŠ€æœ¯**\n**# title: TITAN: A Trajectory-Informed Technique for Adaptive Parameter Freezing in Large-Scale VQE**\n> **Authors:** Yifeng Peng, et al. (NeurIPS 2025)\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹å˜åˆ†é‡å­ç‰¹å¾æ±‚è§£å™¨ï¼ˆVQEï¼‰è®­ç»ƒéš¾çš„é—®é¢˜ï¼Œåˆ©ç”¨æ·±åº¦å­¦ä¹ è¯†åˆ«å¹¶å†»ç»“ä¸æ´»è·ƒå‚æ•°ï¼Œæ˜¾è‘—å‡å°‘äº†é‡å­ç”µè·¯è¯„ä¼°æ¬¡æ•°å¹¶åŠ é€Ÿæ”¶æ•›ã€‚\n\n**14. æ¯”è¾ƒè®¡ç®—ç—…ç†å­¦åŸºç¡€æ¨¡å‹**\n**# title: Comparing Computational Pathology Foundation Models using Representational Similarity Analysis**\n> **Authors:** Vaibhav Mishra, William Lotter\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** ç³»ç»Ÿåˆ†æäº† 6 ä¸ªç—…ç†å­¦åŸºç¡€æ¨¡å‹ï¼ˆå¦‚ CONCH, UNI, Virchowï¼‰çš„è¡¨å¾ç©ºé—´ã€‚å‘ç°è§†è§‰-è¯­è¨€æ¨¡å‹å…·æœ‰æ›´ç´§å‡‘çš„è¡¨å¾ï¼Œä¸”æ‰€æœ‰æ¨¡å‹éƒ½è¡¨ç°å‡ºé«˜åº¦çš„åˆ‡ç‰‡ä¾èµ–æ€§ï¼ˆslide-dependenceï¼‰ã€‚\n\n**15. ç¡çœ å‘¼å¸æš‚åœç­›æŸ¥ï¼šä»å¤œé—´å‘¼å¸å£°ä¼°è®¡å‘¼å¸åŠªåŠ›åº¦**\n**# title: Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening**\n> **Authors:** Xiaolei Xu, et al.\n- **æ ¸å¿ƒè´¡çŒ®ï¼š** é¦–æ¬¡å°è¯•ç›´æ¥ä»å¤œé—´éŸ³é¢‘ä¼°è®¡å‘¼å¸åŠªåŠ›åº¦ï¼ˆrespiratory effortï¼‰ï¼Œæ— éœ€æ¥è§¦å¼ä¼ æ„Ÿå™¨ï¼Œä¸ºåŸºäºæ™ºèƒ½æ‰‹æœºçš„ç¡çœ å‘¼å¸æš‚åœç›‘æµ‹æä¾›äº†æ–°é€”å¾„ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–æœ‰è¶£çš„ç ”ç©¶\n\n*   **[LLM è’¸é¦] Delta Knowledge Distillation:** æå‡º Delta-KDï¼Œä¸ä»…ä»…æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºï¼Œè€Œæ˜¯ä¿ç•™æ•™å¸ˆåœ¨ SFT è¿‡ç¨‹ä¸­å¼•å…¥çš„åˆ†å¸ƒåç§»ï¼ˆDeltaï¼‰ï¼Œæ•ˆæœä¼˜äºæ ‡å‡† KDã€‚\n*   **[æ•™è‚²] Socratic Mind:** ä¸€ç§åŸºäºç”Ÿæˆå¼ AI çš„è‹æ ¼æ‹‰åº•å¼æé—®å·¥å…·ï¼Œç”¨äºåœ¨çº¿è¯¾ç¨‹çš„å½¢æˆæ€§è¯„ä¼°ï¼Œèƒ½æ˜¾è‘—æå‡å­¦ç”Ÿçš„é«˜é˜¶æ€ç»´èƒ½åŠ›ã€‚\n*   **[æœºå™¨äºº] M4Diffuser:** ç”¨äºç§»åŠ¨æ“ä½œçš„å¤šè§†è§’æ‰©æ•£ç­–ç•¥ï¼Œç»“åˆäº†å¯æ“ä½œæ€§æ„ŸçŸ¥çš„æ§åˆ¶ï¼Œæå‡äº†æœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚\n*   **[éŸ³é¢‘] Back to Ear:** æå‡º Îµar-VAEï¼Œé€šè¿‡å¼•å…¥å¬è§‰æ„ŸçŸ¥æ»¤æ³¢å™¨å’Œç›¸ä½æŸå¤±ï¼Œå¤§å¹…æå‡äº†éŸ³ä¹ä¿¡å·é‡å»ºçš„é«˜ä¿çœŸåº¦ã€‚",
  "papers": [
    {
      "arxiv_id": "2509.15490v1",
      "title": "SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters",
      "title_zh": "SmolRGPTï¼šé¢å‘ä»“åº“ç¯å¢ƒçš„ 6 äº¿å‚æ•°é«˜æ•ˆç©ºé—´æ¨ç†",
      "authors": [
        "Abdarahmane Traore",
        "Ã‰ric Hervet",
        "Andy Couturier"
      ],
      "abstract": "Recent advances in vision-language models (VLMs) have enabled powerful multimodal reasoning, but state-of-the-art approaches typically rely on extremely large models with prohibitive computational and memory requirements. This makes their deployment challenging in resource-constrained environments such as warehouses, robotics, and industrial applications, where both efficiency and robust spatial understanding are critical. In this work, we present SmolRGPT, a compact vision-language architecture that explicitly incorporates region-level spatial reasoning by integrating both RGB and depth cues. SmolRGPT employs a three-stage curriculum that progressively align visual and language features, enables spatial relationship understanding, and adapts to task-specific datasets. We demonstrate that with only 600M parameters, SmolRGPT achieves competitive results on challenging warehouse spatial reasoning benchmarks, matching or exceeding the performance of much larger alternatives. These findings highlight the potential for efficient, deployable multimodal intelligence in real-world settings without sacrificing core spatial reasoning capabilities. The code of the experimentation will be available at: https://github.com/abtraore/SmolRGPT",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SmolRGPTï¼Œä¸€ç§å‚æ•°é‡ä»…ä¸º600Mçš„é«˜æ•ˆè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ï¼Œä¸“é—¨ç”¨äºè§£å†³ä»“åº“ã€æœºå™¨äººåŠå·¥ä¸šåº”ç”¨ç­‰èµ„æºå—é™ç¯å¢ƒä¸‹çš„ç©ºé—´æ¨ç†æŒ‘æˆ˜ã€‚SmolRGPTé€šè¿‡æ•´åˆRGBå’Œæ·±åº¦æç¤º(depth cues)å®ç°äº†æ˜¾å¼çš„åŒºåŸŸçº§ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨äº†ä¸‰é˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ (curriculum learning)æ¶æ„ï¼Œé€šè¿‡é€æ­¥å®ç°è§†è§‰ä¸è¯­è¨€ç‰¹å¾å¯¹é½ã€ç©ºé—´å…³ç³»ç†è§£ä»¥åŠç‰¹å®šä»»åŠ¡çš„é€‚é…æ¥ä¼˜åŒ–æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSmolRGPTåœ¨å¤æ‚çš„ä»“åº“ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»¥æå°çš„æ¨¡å‹è§„æ¨¡è¾¾åˆ°äº†ä¸å¤§å‹æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†åœ¨ä¸ç‰ºç‰²æ ¸å¿ƒç©ºé—´æ¨ç†èƒ½åŠ›çš„å‰æä¸‹ï¼Œåœ¨ç°å®åœºæ™¯ä¸­éƒ¨ç½²é«˜æ•ˆã€å¯æ‰©å±•çš„å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 3 figures, IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "pdf_url": "https://arxiv.org/pdf/2509.15490v1",
      "published_date": "2025-09-18 23:55:51 UTC",
      "updated_date": "2025-09-18 23:55:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:17:51.750629+00:00"
    },
    {
      "arxiv_id": "2509.15485v1",
      "title": "mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment",
      "title_zh": "mucAI å‚åŠ  BAREC Shared Task 2025ï¼šé¢å‘ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„é˜¿æ‹‰ä¼¯è¯­å¯è¯»æ€§è¯„ä¼°",
      "authors": [
        "Ahmed Abdou"
      ],
      "abstract": "We present a simple, model-agnostic post-processing technique for fine-grained Arabic readability classification in the BAREC 2025 Shared Task (19 ordinal levels). Our method applies conformal prediction to generate prediction sets with coverage guarantees, then computes weighted averages using softmax-renormalized probabilities over the conformal sets. This uncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing high-penalty misclassifications to nearer levels. Our approach shows consistent QWK improvements of 1-3 points across different base models. In the strict track, our submission achieves QWK scores of 84.9\\%(test) and 85.7\\% (blind test) for sentence level, and 73.3\\% for document level. For Arabic educational assessment, this enables human reviewers to focus on a handful of plausible levels, combining statistical guarantees with practical usability.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†mucAIåœ¨BAREC Shared Task 2025ç«èµ›ä¸­çš„æ–¹æ¡ˆï¼Œæ—¨åœ¨å®ç°Uncertainty Aware Arabic Readability Assessmentï¼ˆä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„é˜¿æ‹‰ä¼¯è¯­å¯è¯»æ€§è¯„ä¼°ï¼‰ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§ç®€å•ä¸”ä¸æ¨¡å‹æ— å…³çš„åå¤„ç†æŠ€æœ¯ï¼Œåˆ©ç”¨Conformal Predictionç”Ÿæˆå…·æœ‰è¦†ç›–ä¿è¯çš„é¢„æµ‹é›†åˆï¼Œå¹¶åŸºäºSoftmax-renormalized Probabilitiesè®¡ç®—åŠ æƒå¹³å‡å€¼ã€‚è¿™ç§Uncertainty-aware decodingæœºåˆ¶èƒ½æœ‰æ•ˆå‡å°‘é«˜æƒ©ç½šçš„è¯¯åˆ¤ï¼Œå°†ä¸åŒåŸºç¡€æ¨¡å‹çš„Quadratic Weighted Kappa (QWK) ä¸€è‡´æå‡äº†1-3ä¸ªç™¾åˆ†ç‚¹ã€‚åœ¨ä¸¥æ ¼èµ›é“ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¥å­çº§åˆ«æµ‹è¯•é›†å’Œç›²æµ‹é›†ä¸Šåˆ†åˆ«å–å¾—äº†84.9%å’Œ85.7%çš„QWKå¾—åˆ†ï¼Œæ–‡æ¡£çº§åˆ«å¾—åˆ†åˆ™ä¸º73.3%ã€‚è¯¥æ–¹æ³•ä¸ä»…æä¾›äº†ç»Ÿè®¡å­¦ä¸Šçš„å¯é æ€§ä¿è¯ï¼Œè¿˜é€šè¿‡ç¼©å°é¢„æµ‹èŒƒå›´æ˜¾è‘—æå‡äº†æ•™è‚²è¯„ä¼°ä¸­äººå·¥å®¡æ ¸çš„æ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15485v1",
      "published_date": "2025-09-18 23:14:51 UTC",
      "updated_date": "2025-09-18 23:14:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:17:49.181329+00:00"
    },
    {
      "arxiv_id": "2509.15482v2",
      "title": "Comparing Computational Pathology Foundation Models using Representational Similarity Analysis",
      "title_zh": "åŸºäºè¡¨å¾ç›¸ä¼¼æ€§åˆ†æçš„è®¡ç®—ç—…ç†å­¦åŸºç¡€æ¨¡å‹æ¯”è¾ƒ",
      "authors": [
        "Vaibhav Mishra",
        "William Lotter"
      ],
      "abstract": "Foundation models are increasingly developed in computational pathology (CPath) given their promise in facilitating many downstream tasks. While recent studies have evaluated task performance across models, less is known about the structure and variability of their learned representations. Here, we systematically analyze the representational spaces of six CPath foundation models using techniques popularized in computational neuroscience. The models analyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) and self-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Through representational similarity analysis using H&E image patches from TCGA, we find that UNI2 and Virchow2 have the most distinct representational structures, whereas Prov-Gigapath has the highest average similarity across models. Having the same training paradigm (vision-only vs. vision-language) did not guarantee higher representational similarity. The representations of all models showed a high slide-dependence, but relatively low disease-dependence. Stain normalization decreased slide-dependence for all models by a range of 5.5% (CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-language models demonstrated relatively compact representations, compared to the more distributed representations of vision-only models. These findings highlight opportunities to improve robustness to slide-specific features, inform model ensembling strategies, and provide insights into how training paradigms shape model representations. Our framework is extendable across medical imaging domains, where probing the internal representations of foundation models can support their effective development and deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨è®¡ç®—ç¥ç»ç§‘å­¦ä¸­æµè¡Œçš„è¡¨å¾ç›¸ä¼¼æ€§åˆ†æ(Representational Similarity Analysis, RSA)æ–¹æ³•ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†å…­ç§è®¡ç®—ç—…ç†å­¦(CPath)åŸºç¡€æ¨¡å‹çš„è¡¨å¾ç©ºé—´ã€‚å®éªŒæ¶µç›–äº†è§†è§‰è¯­è¨€å¯¹æ¯”å­¦ä¹ (CONCH, PLIP, KEEP)å’Œè‡ªæˆ‘è’¸é¦(UNI2, Virchow2, Prov-GigaPath)ä¸¤ç§æ¶æ„ï¼Œå‘ç° UNI2 å’Œ Virchow2 å…·æœ‰æœ€ç‹¬ç‰¹çš„è¡¨å¾ç»“æ„ï¼Œè€Œ Prov-GigaPath çš„å¹³å‡ç›¸ä¼¼åº¦æœ€é«˜ã€‚ç»“æœæ˜¾ç¤ºç›¸åŒçš„è®­ç»ƒèŒƒå¼å¹¶ä¸èƒ½ä¿è¯æ¨¡å‹é—´çš„è¡¨å¾ç›¸ä¼¼æ€§ï¼Œä¸”æ‰€æœ‰æ¨¡å‹å‡è¡¨ç°å‡ºé«˜åº¦çš„åˆ‡ç‰‡ä¾èµ–æ€§(Slide-dependence)å’Œè¾ƒä½çš„ç–¾ç—…ä¾èµ–æ€§(Disease-dependence)ã€‚ç ”ç©¶è¯æ˜æŸ“è‰²å½’ä¸€åŒ–(Stain normalization)å¯æ˜¾è‘—é™ä½åˆ‡ç‰‡ç‰¹å¾çš„å½±å“ï¼Œä¸”è§†è§‰è¯­è¨€æ¨¡å‹ç›¸æ¯”çº¯è§†è§‰æ¨¡å‹å…·æœ‰æ›´ç´§å‡‘çš„å†…åœ¨è¡¨å¾ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†è®­ç»ƒèŒƒå¼å¦‚ä½•å¡‘é€ æ¨¡å‹è¡¨å¾ï¼Œä¸ºæé«˜æ¨¡å‹é²æ£’æ€§å’Œåˆ¶å®šæ¨¡å‹é›†æˆç­–ç•¥æä¾›äº†é‡è¦ç†è®ºæ”¯æ’‘ã€‚è¯¥è¯„ä¼°æ¡†æ¶å¯æ‰©å±•è‡³å…¶ä»–åŒ»ç–—å½±åƒé¢†åŸŸï¼Œæ”¯æŒåŸºç¡€æ¨¡å‹åœ¨å¤æ‚åŒ»å­¦åœºæ™¯ä¸‹çš„æœ‰æ•ˆå¼€å‘ä¸éƒ¨ç½²ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Proceedings of the 5th Machine Learning for Health (ML4H) Symposium",
      "pdf_url": "https://arxiv.org/pdf/2509.15482v2",
      "published_date": "2025-09-18 23:01:13 UTC",
      "updated_date": "2025-11-05 20:38:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:17:59.329448+00:00"
    },
    {
      "arxiv_id": "2509.15470v1",
      "title": "Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture",
      "title_zh": "åŸºäºå¤šæ¨¡æ€è”åˆåµŒå…¥é¢„æµ‹æ¶æ„çš„å½±åƒä¸ä¸´åºŠç‰¹å¾è‡ªç›‘ç£å­¦ä¹ ",
      "authors": [
        "Thomas Z. Li",
        "Aravind R. Krishnan",
        "Lianrui Zuo",
        "John M. Still",
        "Kim L. Sandler",
        "Fabien Maldonado",
        "Thomas A. Lasko",
        "Bennett A. Landman"
      ],
      "abstract": "The development of multimodal models for pulmonary nodule diagnosis is limited by the scarcity of labeled data and the tendency for these models to overfit on the training distribution. In this work, we leverage self-supervised learning from longitudinal and multimodal archives to address these challenges. We curate an unlabeled set of patients with CT scans and linked electronic health records from our home institution to power joint embedding predictive architecture (JEPA) pretraining. After supervised finetuning, we show that our approach outperforms an unregularized multimodal model and imaging-only model in an internal cohort (ours: 0.91, multimodal: 0.88, imaging-only: 0.73 AUC), but underperforms in an external cohort (ours: 0.72, imaging-only: 0.75 AUC). We develop a synthetic environment that characterizes the context in which JEPA may underperform. This work innovates an approach that leverages unlabeled multimodal medical archives to improve predictive models and demonstrates its advantages and limitations in pulmonary nodule diagnosis.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨çºµå‘å¤šæ¨¡æ€æ¡£æ¡ˆä¸­çš„è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning)æŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§è”åˆåµŒå…¥é¢„æµ‹æ¶æ„(Joint Embedding Predictive Architecture, JEPA)ï¼Œæ—¨åœ¨è§£å†³è‚ºç»“èŠ‚è¯Šæ–­ä¸­æ ‡è®°æ•°æ®ç¨€ç¼ºå’Œæ¨¡å‹è¿‡æ‹Ÿåˆçš„æŒ‘æˆ˜ã€‚é€šè¿‡åœ¨æœªæ ‡æ³¨çš„CTæ‰«æå’Œå…³è”çš„ç”µå­å¥åº·è®°å½•(EHR)ä¸Šè¿›è¡Œé¢„è®­ç»ƒåŠåç»­çš„ç›‘ç£å¾®è°ƒï¼Œè¯¥æ–¹æ³•åœ¨å†…éƒ¨é˜Ÿåˆ—ä¸­å–å¾—äº†0.91çš„AUCï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¤šæ¨¡æ€æ¨¡å‹å’Œçº¯å½±åƒæ¨¡å‹ã€‚ä½†åœ¨å¤–éƒ¨é˜Ÿåˆ—æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹çš„è¡¨ç°(0.72 AUC)ç•¥ä½äºçº¯å½±åƒæ¨¡å‹ï¼Œåæ˜ å‡ºæ³›åŒ–èƒ½åŠ›çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†ä¸€ä¸ªåˆæˆç¯å¢ƒï¼Œç”¨ä»¥è¡¨å¾JEPAå¯èƒ½è¡¨ç°ä¸ä½³çš„ç‰¹å®šè¯­å¢ƒã€‚è¯¥å·¥ä½œä¸ä»…å±•ç¤ºäº†åˆ©ç”¨æ— æ ‡æ³¨å¤šæ¨¡æ€åŒ»ç–—æ¡£æ¡ˆæ”¹è¿›é¢„æµ‹æ¨¡å‹çš„åˆ›æ–°è·¯å¾„ï¼Œä¹Ÿç³»ç»Ÿåœ°æ­ç¤ºäº†è¯¥æ–¹æ³•åœ¨è‚ºç»“èŠ‚è¯Šæ–­ä¸­çš„ä¼˜åŠ¿ä¸åº”ç”¨è¾¹ç•Œã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15470v1",
      "published_date": "2025-09-18 22:35:44 UTC",
      "updated_date": "2025-09-18 22:35:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:17:56.486864+00:00"
    },
    {
      "arxiv_id": "2509.15460v1",
      "title": "Incorporating Visual Cortical Lateral Connection Properties into CNN: Recurrent Activation and Excitatory-Inhibitory Separation",
      "title_zh": "èåˆè§†è§‰çš®å±‚ä¾§å‘è¿æ¥ç‰¹æ€§çš„å·ç§¯ç¥ç»ç½‘ç»œï¼šå¾ªç¯æ¿€æ´»ä¸å…´å¥‹-æŠ‘åˆ¶åˆ†ç¦»",
      "authors": [
        "Jin Hyun Park",
        "Cheng Zhang",
        "Yoonsuck Choe"
      ],
      "abstract": "The original Convolutional Neural Networks (CNNs) and their modern updates such as the ResNet are heavily inspired by the mammalian visual system. These models include afferent connections (retina and LGN to the visual cortex) and long-range projections (connections across different visual cortical areas). However, in the mammalian visual system, there are connections within each visual cortical area, known as lateral (or horizontal) connections. These would roughly correspond to connections within CNN feature maps, and this important architectural feature is missing in current CNN models. In this paper, we present how such lateral connections can be modeled within the standard CNN framework, and test its benefits and analyze its emergent properties in relation to the biological visual system. We will focus on two main architectural features of lateral connections: (1) recurrent activation and (2) separation of excitatory and inhibitory connections. We show that recurrent CNN using weight sharing is equivalent to lateral connections, and propose a custom loss function to separate excitatory and inhibitory weights. The addition of these two leads to increased classification accuracy, and importantly, the activation properties and connection properties of the resulting model show properties similar to those observed in the biological visual system. We expect our approach to help align CNN closer to its biological counterpart and better understand the principles of visual cortical computation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†å“ºä¹³åŠ¨ç‰©è§†è§‰ç³»ç»Ÿä¸­çš„ä¾§å‘è¿æ¥(lateral connections)å±æ€§å¼•å…¥å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ï¼Œä»¥å¼¥è¡¥ç°æœ‰æ¨¡å‹åœ¨æ¨¡æ‹Ÿè§†è§‰çš®å±‚å†…éƒ¨è¿æ¥æ–¹é¢çš„ä¸è¶³ã€‚ä½œè€…é‡ç‚¹æ¨¡æ‹Ÿäº†ä¾§å‘è¿æ¥çš„ä¸¤ä¸ªæ ¸å¿ƒç‰¹å¾ï¼šå¾ªç¯æ¿€æ´»(recurrent activation)å’Œå…´å¥‹æ€§ä¸æŠ‘åˆ¶æ€§è¿æ¥çš„åˆ†ç¦»(excitatory-inhibitory separation)ã€‚è®ºæ–‡è¯æ˜äº†ä½¿ç”¨æƒé‡å…±äº«çš„å¾ªç¯CNNåœ¨æ•°å­¦ä¸Šç­‰åŒäºä¾§å‘è¿æ¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§è‡ªå®šä¹‰æŸå¤±å‡½æ•°æ¥å®ç°å…´å¥‹æ€§å’ŒæŠ‘åˆ¶æ€§æƒé‡çš„æ˜¾å¼åˆ†ç¦»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŠ å…¥è¿™äº›ç”Ÿç‰©å¯å‘å¼å±æ€§åï¼Œä¸ä»…æé«˜äº†æ¨¡å‹çš„å›¾åƒåˆ†ç±»å‡†ç¡®ç‡ï¼Œè¿˜ä½¿ç½‘ç»œå†…éƒ¨çš„æ¿€æ´»å’Œè¿æ¥ç‰¹æ€§è¡¨ç°å‡ºä¸çœŸå®ç”Ÿç‰©è§†è§‰ç³»ç»Ÿé«˜åº¦ç›¸ä¼¼çš„å±æ€§ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å°†CNNä¸å…¶ç”Ÿç‰©å­¦åŸå‹æ›´ç´§å¯†åœ°å¯¹é½ï¼Œä¸ºç†è§£è§†è§‰çš®å±‚çš„è®¡ç®—åŸç†æä¾›äº†æ–°çš„è§†è§’å’Œæœ‰æ•ˆçš„å»ºæ¨¡æ¡†æ¶ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15460v1",
      "published_date": "2025-09-18 22:13:48 UTC",
      "updated_date": "2025-09-18 22:13:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:17:55.980894+00:00"
    },
    {
      "arxiv_id": "2509.15459v2",
      "title": "CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction",
      "title_zh": "CAGEï¼šè¿ç»­æ€§æ„ŸçŸ¥è¾¹ç¼˜ç½‘ç»œå®ç°é²æ£’æˆ·å‹å›¾é‡å»º",
      "authors": [
        "Yiyi Liu",
        "Chunyang Liu",
        "Bohan Wang",
        "Weiqin Jiao",
        "Bojian Wu",
        "Lubin Fan",
        "Yuwei Chen",
        "Fashuai Li",
        "Biao Xiong"
      ],
      "abstract": "We present CAGE (Continuity-Aware edGE) network, a robust framework for reconstructing vector floorplans directly from point-cloud density maps. Traditional corner-based polygon representations are highly sensitive to noise and incomplete observations, often resulting in fragmented or implausible layouts.Recent line grouping methods leverage structural cues to improve robustness but still struggle to recover fine geometric details. To address these limitations,we propose a native edge-centric formulation, modeling each wall segment as a directed, geometrically continuous edge. This representation enables inference of coherent floorplan structures, ensuring watertight, topologically valid room boundaries while improving robustness and reducing artifacts. Towards this design, we develop a dual-query transformer decoder that integrates perturbed and latent queries within a denoising framework, which not only stabilizes optimization but also accelerates convergence. Extensive experiments on Structured3D and SceneCAD show that CAGE achieves state-of-the-art performance, with F1 scores of 99.1% (rooms), 91.7% (corners), and 89.3% (angles). The method also demonstrates strong cross-dataset generalization, underscoring the efficacy of our architectural innovations. Code and pretrained models are available on our project page: https://github.com/ee-Liu/CAGE.git.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CAGE (Continuity-Aware edGE) ç½‘ç»œï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä»ç‚¹äº‘å¯†åº¦å›¾ç›´æ¥é‡å»ºçŸ¢é‡åŒ–å¹³é¢å›¾çš„ç¨³å¥æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿçš„åŸºäºè§’ç‚¹(corner-based)çš„è¡¨ç¤ºæ³•å¯¹å™ªå£°æ•æ„Ÿä¸”æ˜“äº§ç”Ÿå¸ƒå±€ç ´ç¢çš„é—®é¢˜ï¼ŒCAGEé‡‡ç”¨äº†ä¸€ç§åŸç”Ÿçš„ä»¥è¾¹ç¼˜ä¸ºä¸­å¿ƒ(edge-centric)çš„å»ºæ¨¡æ–¹å¼ï¼Œå°†å¢™æ®µè§†ä¸ºæœ‰å‘ä¸”å‡ ä½•è¿ç»­çš„è¾¹ç¼˜ã€‚è¿™ç§è¡¨ç¤ºæ³•èƒ½å¤Ÿç¡®ä¿ç”Ÿæˆæ‹“æ‰‘æœ‰æ•ˆä¸”æ°´å¯†çš„(watertight)æˆ¿é—´è¾¹ç•Œï¼Œæ˜¾è‘—æå‡äº†é‡å»ºçš„é²æ£’æ€§å¹¶å‡å°‘äº†ä¼ªå½±ã€‚åœ¨æ¶æ„ä¸Šï¼Œè¯¥ç½‘ç»œå¼•å…¥äº†åŒæŸ¥è¯¢Transformerè§£ç å™¨(dual-query transformer decoder)ï¼Œé€šè¿‡åœ¨å»å™ªæ¡†æ¶ä¸­æ•´åˆæ‰°åŠ¨å’Œæ½œåœ¨æŸ¥è¯¢ï¼ŒåŠ é€Ÿäº†æ¨¡å‹æ”¶æ•›å¹¶ç¨³å®šäº†ä¼˜åŒ–è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCAGEåœ¨Structured3Då’ŒSceneCADæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…¶æˆ¿é—´é‡å»ºçš„F1åˆ†æ•°è¾¾åˆ°99.1%ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15459v2",
      "published_date": "2025-09-18 22:10:37 UTC",
      "updated_date": "2025-10-14 20:13:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:18:08.587844+00:00"
    },
    {
      "arxiv_id": "2509.18187v1",
      "title": "V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling",
      "title_zh": "V-SenseDriveï¼šé¢å‘é“è·¯å®‰å…¨ä¸é©¾é©¶è¡Œä¸ºå»ºæ¨¡çš„éšç§ä¿æŠ¤å‹é“è·¯è§†é¢‘ä¸è½¦è½½ä¼ æ„Ÿå™¨èåˆæ¡†æ¶",
      "authors": [
        "Muhammad Naveed",
        "Nazia Perwaiz",
        "Sidra Sultana",
        "Mohaira Ahmad",
        "Muhammad Moazam Fraz"
      ],
      "abstract": "Road traffic accidents remain a major public health challenge, particularly in countries with heterogeneous road conditions, mixed traffic flow, and variable driving discipline, such as Pakistan. Reliable detection of unsafe driving behaviours is a prerequisite for improving road safety, enabling advanced driver assistance systems (ADAS), and supporting data driven decisions in insurance and fleet management. Most of existing datasets originate from the developed countries with limited representation of the behavioural diversity observed in emerging economies and the driver's face recording voilates the privacy preservation. We present V-SenseDrive, the first privacy-preserving multimodal driver behaviour dataset collected entirely within the Pakistani driving environment. V-SenseDrive combines smartphone based inertial and GPS sensor data with synchronized road facing video to record three target driving behaviours (normal, aggressive, and risky) on multiple types of roads, including urban arterials, secondary roads, and motorways. Data was gathered using a custom Android application designed to capture high frequency accelerometer, gyroscope, and GPS streams alongside continuous video, with all sources precisely time aligned to enable multimodal analysis. The focus of this work is on the data acquisition process, covering participant selection, driving scenarios, environmental considerations, and sensor video synchronization techniques. The dataset is structured into raw, processed, and semantic layers, ensuring adaptability for future research in driver behaviour classification, traffic safety analysis, and ADAS development. By representing real world driving in Pakistan, V-SenseDrive fills a critical gap in the global landscape of driver behaviour datasets and lays the groundwork for context aware intelligent transportation solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·´åŸºæ–¯å¦ç­‰æ–°å…´ç»æµä½“é“è·¯çŠ¶å†µå¤æ‚ã€ç°æœ‰æ•°æ®é›†ç¼ºä¹ä»£è¡¨æ€§ä¸”é¢ä¸´éšç§æ³„éœ²é£é™©çš„é—®é¢˜ï¼Œæå‡ºäº† V-SenseDrive æ¡†æ¶ã€‚V-SenseDrive æ˜¯é¦–ä¸ªåœ¨å·´åŸºæ–¯å¦ç¯å¢ƒä¸‹é‡‡é›†çš„éšç§ä¿æŠ¤å¤šæ¨¡æ€é©¾é©¶è¡Œä¸ºæ•°æ®é›†ï¼Œé€šè¿‡è‡ªå®šä¹‰ Android åº”ç”¨ç¨‹åºåŒæ­¥é‡‡é›†æ™ºèƒ½æ‰‹æœºçš„æƒ¯æ€§ä¼ æ„Ÿå™¨(Inertial Sensor)ã€GPS æ•°æ®ä»¥åŠé“è·¯æœå‘è§†é¢‘ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†åŸå¸‚ä¸»å¹²é“ã€æ¬¡è¦é“è·¯åŠé«˜é€Ÿå…¬è·¯ç­‰å¤šç§è·¯å†µï¼Œå¹¶è®°å½•äº†æ­£å¸¸ã€æ¿€è¿›(Aggressive)å’Œå±é™©(Risky)ä¸‰ç§é©¾é©¶è¡Œä¸ºã€‚é€šè¿‡å¯¹æ•°æ®è¿›è¡ŒåŸå§‹å±‚ã€å¤„ç†å±‚å’Œè¯­ä¹‰å±‚çš„åˆ†å±‚åŒ–ç»“æ„å¤„ç†ï¼Œè¯¥æ¡†æ¶ç¡®ä¿äº†å¤šæ¨¡æ€æ•°æ®åœ¨é©¾é©¶è¡Œä¸ºåˆ†ç±»å’Œäº¤é€šå®‰å…¨åˆ†æä¸­çš„ç²¾å‡†åº”ç”¨ã€‚è¯¥å·¥ä½œå¡«è¡¥äº†å…¨çƒé©¾é©¶è¡Œä¸ºæ•°æ®é›†ä¸­å…³äºç‰¹å®šäº¤é€šç¯å¢ƒä¸‹è¡Œä¸ºå¤šæ ·æ€§çš„å…³é”®ç©ºç™½ï¼Œä¸ºå¼€å‘ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ™ºèƒ½äº¤é€šè§£å†³æ–¹æ¡ˆåŠé«˜çº§é©¾é©¶è¾…åŠ©ç³»ç»Ÿ(ADAS)å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18187v1",
      "published_date": "2025-09-18 21:55:14 UTC",
      "updated_date": "2025-09-18 21:55:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:18:10.390856+00:00"
    },
    {
      "arxiv_id": "2509.20370v1",
      "title": "Philosophy-informed Machine Learning",
      "title_zh": "èåˆå“²å­¦çš„æœºå™¨å­¦ä¹ ",
      "authors": [
        "MZ Naser"
      ],
      "abstract": "Philosophy-informed machine learning (PhIML) directly infuses core ideas from analytic philosophy into ML model architectures, objectives, and evaluation protocols. Therefore, PhIML promises new capabilities through models that respect philosophical concepts and values by design. From this lens, this paper reviews conceptual foundations to demonstrate philosophical gains and alignment. In addition, we present case studies on how ML users/designers can adopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML model architectures. Finally, this paper sheds light on open technical barriers alongside philosophical, practical, and governance challenges and outlines a research roadmap toward safe, philosophy-aware, and ethically responsible PhIML.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å“²å­¦è¾…åŠ©æœºå™¨å­¦ä¹ (Philosophy-informed Machine Learning, PhIML)è¿™ä¸€æ¦‚å¿µï¼Œæ—¨åœ¨å°†åˆ†æå“²å­¦(Analytic Philosophy)çš„æ ¸å¿ƒæ€æƒ³ç›´æ¥èå…¥æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¶æ„ã€ç›®æ ‡å‡½æ•°å’Œè¯„ä¼°åè®®ä¸­ã€‚PhIMLé€šè¿‡åœ¨è®¾è®¡é˜¶æ®µèå…¥å“²å­¦æ¦‚å¿µå’Œä»·å€¼è§‚ï¼Œä½¿æ¨¡å‹åœ¨å…·å¤‡æ–°èƒ½åŠ›çš„åŒæ—¶èƒ½å¤Ÿéµå¾ªäººç±»ç¤¾ä¼šçš„æ·±å±‚é€»è¾‘ã€‚æœ¬æ–‡å›é¡¾äº†å…¶æ¦‚å¿µåŸºç¡€ä»¥å±•ç¤ºå“²å­¦å±‚é¢çš„å¢ç›Šä¸å¯¹é½ï¼Œå¹¶æä¾›äº†å…³äºå°†PhIMLä½œä¸ºä¸å¯çŸ¥åéªŒå·¥å…·(Agnostic Post-hoc Tool)æˆ–å†…åœ¨æ„å»ºäºæ¨¡å‹æ¶æ„ä¸­çš„æ¡ˆä¾‹ç ”ç©¶ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é˜æ˜äº†å½“å‰é¢ä¸´çš„æŠ€æœ¯éšœç¢ã€å®è·µæŒ‘æˆ˜åŠæ²»ç†éš¾é¢˜ï¼Œå¹¶æå‡ºäº†æ—¨åœ¨å®ç°å®‰å…¨ã€å…·å¤‡å“²å­¦æ„è¯†ä¸”åˆä¹ä¼¦ç†è´£ä»»çš„PhIMLç ”ç©¶è·¯çº¿å›¾(Research Roadmap)ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20370v1",
      "published_date": "2025-09-18 21:51:21 UTC",
      "updated_date": "2025-09-18 21:51:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:18:14.589658+00:00"
    },
    {
      "arxiv_id": "2509.15448v1",
      "title": "Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems",
      "title_zh": "å±‚æ¬¡åŒ–è‡ªæ³¨æ„åŠ›ï¼šå°†ç¥ç»æ³¨æ„åŠ›æœºåˆ¶æ³›åŒ–è‡³å¤šå°ºåº¦é—®é¢˜",
      "authors": [
        "Saeed Amizadeh",
        "Sara Abdali",
        "Yinheng Li",
        "Kazuhito Koishida"
      ],
      "abstract": "Transformers and their attention mechanism have been revolutionary in the field of Machine Learning. While originally proposed for the language data, they quickly found their way to the image, video, graph, etc. data modalities with various signal geometries. Despite this versatility, generalizing the attention mechanism to scenarios where data is presented at different scales from potentially different modalities is not straightforward. The attempts to incorporate hierarchy and multi-modality within transformers are largely based on ad hoc heuristics, which are not seamlessly generalizable to similar problems with potentially different structures. To address this problem, in this paper, we take a fundamentally different approach: we first propose a mathematical construct to represent multi-modal, multi-scale data. We then mathematically derive the neural attention mechanics for the proposed construct from the first principle of entropy minimization. We show that the derived formulation is optimal in the sense of being the closest to the standard Softmax attention while incorporating the inductive biases originating from the hierarchical/geometric information of the problem. We further propose an efficient algorithm based on dynamic programming to compute our derived attention mechanism. By incorporating it within transformers, we show that the proposed hierarchical attention mechanism not only can be employed to train transformer models in hierarchical/multi-modal settings from scratch, but it can also be used to inject hierarchical information into classical, pre-trained transformer models post training, resulting in more efficient models in zero-shot manner.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Transformer å’Œæ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†å¤šå°ºåº¦ã€å¤šæ¨¡æ€æ•°æ®æ—¶ä¾èµ–å¯å‘å¼æ–¹æ³•ä¸”ç¼ºä¹æ³›åŒ–æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Hierarchical Self-Attention çš„å±‚çº§è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚ä½œè€…é¦–å…ˆæ„å»ºäº†ä»£è¡¨å¤šæ¨¡æ€å¤šå°ºåº¦æ•°æ®çš„æ•°å­¦ç»“æ„ï¼Œå¹¶åŸºäºç†µæœ€å°åŒ– (entropy minimization) çš„ç¬¬ä¸€æ€§åŸç†æ¨å¯¼å‡ºè¯¥æ³¨æ„åŠ›å…¬å¼ã€‚ç†è®ºè¯æ˜ï¼Œè¯¥æ¨å¯¼ç»“æœåœ¨æ•°å­¦ä¸Šæ˜¯æœ€ä¼˜çš„ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™æ ‡å‡† Softmax æ³¨æ„åŠ›ç‰¹æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆæ•´åˆæ¥è‡ªå±‚çº§æˆ–å‡ ä½•ä¿¡æ¯çš„å½’çº³åç½® (inductive biases)ã€‚ä¸ºè§£å†³è®¡ç®—å¤æ‚åº¦é—®é¢˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€ç¼–ç¨‹ (dynamic programming) çš„é«˜æ•ˆè®¡ç®—ç®—æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æœºåˆ¶ä¸ä»…é€‚ç”¨äºä»å¤´å¼€å§‹è®­ç»ƒå±‚çº§æˆ–å¤šæ¨¡æ€ Transformerï¼Œè¿˜å¯ä»¥é€šè¿‡é›¶æ ·æœ¬ (zero-shot) æ–¹å¼å‘å·²é¢„è®­ç»ƒçš„ç»å…¸æ¨¡å‹æ³¨å…¥å±‚çº§ä¿¡æ¯ã€‚è¿™ä¸€æ–¹æ³•ä¸ºæ„å»ºæ›´é«˜æ•ˆã€æ›´å…·æ³›åŒ–æ€§çš„å¤šå°ºåº¦æ¨¡å‹æä¾›äº†ä¸€ç§ä»ç†è®ºåˆ°ç®—æ³•çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "In The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.15448v1",
      "published_date": "2025-09-18 21:44:07 UTC",
      "updated_date": "2025-09-18 21:44:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:18:16.169032+00:00"
    },
    {
      "arxiv_id": "2509.15447v1",
      "title": "PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting",
      "title_zh": "PILOTï¼šåˆ©ç”¨å¿ƒç†ä¸è¯­è¨€è¾“å‡ºå®šå‘å¼•å¯¼åˆæˆæ•°æ®ç”Ÿæˆ",
      "authors": [
        "Caitlin Cisar",
        "Emily Sheffield",
        "Joshua Drake",
        "Alden Harrell",
        "Subramanian Chidambaram",
        "Nikita Nangia",
        "Vinayak Arannil",
        "Alex Williams"
      ],
      "abstract": "Generative AI applications commonly leverage user personas as a steering mechanism for synthetic data generation, but reliance on natural language representations forces models to make unintended inferences about which attributes to emphasize, limiting precise control over outputs. We introduce PILOT (Psychological and Linguistic Output Targeting), a two-phase framework for steering large language models with structured psycholinguistic profiles. In Phase 1, PILOT translates natural language persona descriptions into multidimensional profiles with normalized scores across linguistic and psychological dimensions. In Phase 2, these profiles guide generation along measurable axes of variation. We evaluate PILOT across three state-of-the-art LLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas under three conditions: Natural-language Persona Steering (NPS), Schema-Based Steering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate that schema-based approaches significantly reduce artificial-sounding persona repetition while improving output coherence, with silhouette scores increasing from 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals a fundamental trade-off: SBS produces more concise outputs with higher topical consistency, while NPS offers greater lexical diversity but reduced predictability. HPS achieves a balance between these extremes, maintaining output variety while preserving structural consistency. Expert linguistic evaluation confirms that PILOT maintains high response quality across all conditions, with no statistically significant differences between steering approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨åˆ©ç”¨ç”¨æˆ·ç”»åƒ(Personas)ç”Ÿæˆåˆæˆæ•°æ®æ—¶ï¼Œç”±äºè¿‡åº¦ä¾èµ–è‡ªç„¶è¯­è¨€è¡¨è¿°è€Œå¯¼è‡´æ§åˆ¶ç²¾åº¦å—é™çš„é—®é¢˜ï¼Œæå‡ºäº† PILOT (Psychological and Linguistic Output Targeting) æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œé¦–å…ˆå°†è‡ªç„¶è¯­è¨€ç”»åƒæè¿°è½¬åŒ–ä¸ºåŒ…å«å¤šç»´å¿ƒç†è¯­è¨€å­¦ç»´åº¦å’Œæ ‡å‡†åŒ–è¯„åˆ†çš„ç»“æ„åŒ–é…ç½®æ–‡ä»¶ï¼Œéšååˆ©ç”¨è¿™äº›é…ç½®æ–‡ä»¶åœ¨å¯è¡¡é‡çš„è½´å‘ä¸Šå¼•å¯¼å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡åœ¨ Mistral Large 2ã€Deepseek-R1 å’Œ LLaMA 3.3 70B ç­‰ä¸»æµæ¨¡å‹ä¸Šçš„å®éªŒï¼Œç ”ç©¶å¯¹æ¯”äº†è‡ªç„¶è¯­è¨€å¼•å¯¼(NPS)ã€åŸºäºæ¨¡å¼å¼•å¯¼(SBS)åŠæ··åˆå¼•å¯¼(HPS)ä¸‰ç§æ¨¡å¼çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSBS æ–¹æ³•æ˜¾è‘—é™ä½äº†ç”»åƒç‰¹å¾çš„æœºæ¢°é‡å¤ï¼Œå¹¶å°†è½®å»“åˆ†æ•°(Silhouette Scores)ä» 0.098 æå‡è‡³ 0.237ï¼Œä¸»é¢˜çº¯åº¦ä» 0.773 æå‡è‡³ 0.957ã€‚ç ”ç©¶æ­ç¤ºäº†ç”Ÿæˆæ§åˆ¶ä¸­çš„æ ¸å¿ƒæƒè¡¡ï¼šSBS åœ¨ä¸»é¢˜ä¸€è‡´æ€§ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œè€Œ NPS çš„è¯æ±‡å¤šæ ·æ€§æ›´é«˜ã€‚æ··åˆæ–¹æ¡ˆ HPS åˆ™æœ‰æ•ˆå¹³è¡¡äº†è¾“å‡ºçš„ä¸°å¯Œæ€§ä¸ç»“æ„çš„ç¨³å®šæ€§ï¼Œå®ç°äº†æ›´ç²¾å‡†çš„ç”Ÿæˆè°ƒæ§ã€‚ä¸“å®¶è¯­è¨€è¯„ä¼°è¿›ä¸€æ­¥ç¡®è®¤ï¼ŒPILOT åœ¨æå‡ç”Ÿæˆå¯æ§æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†æé«˜çš„å“åº”è´¨é‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15447v1",
      "published_date": "2025-09-18 21:43:28 UTC",
      "updated_date": "2025-09-18 21:43:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:18:20.292917+00:00"
    },
    {
      "arxiv_id": "2509.15443v1",
      "title": "Implicit Kinodynamic Motion Retargeting for Human-to-humanoid Imitation Learning",
      "title_zh": "é¢å‘äººåˆ°ç±»äººæœºå™¨äººæ¨¡ä»¿å­¦ä¹ çš„éšå¼è¿åŠ¨åŠ¨åŠ›å­¦åŠ¨ä½œé‡å®šå‘",
      "authors": [
        "Xingyu Chen",
        "Hanyu Wu",
        "Sikai Wu",
        "Mingliang Zhou",
        "Diyun Xiang",
        "Haodong Zhang"
      ],
      "abstract": "Human-to-humanoid imitation learning aims to learn a humanoid whole-body controller from human motion. Motion retargeting is a crucial step in enabling robots to acquire reference trajectories when exploring locomotion skills. However, current methods focus on motion retargeting frame by frame, which lacks scalability. Could we directly convert large-scale human motion into robot-executable motion through a more efficient approach? To address this issue, we propose Implicit Kinodynamic Motion Retargeting (IKMR), a novel efficient and scalable retargeting framework that considers both kinematics and dynamics. In kinematics, IKMR pretrains motion topology feature representation and a dual encoder-decoder architecture to learn a motion domain mapping. In dynamics, IKMR integrates imitation learning with the motion retargeting network to refine motion into physically feasible trajectories. After fine-tuning using the tracking results, IKMR can achieve large-scale physically feasible motion retargeting in real time, and a whole-body controller could be directly trained and deployed for tracking its retargeted trajectories. We conduct our experiments both in the simulator and the real robot on a full-size humanoid robot. Extensive experiments and evaluation results verify the effectiveness of our proposed framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç±»äººæœºå™¨äººå…¨èº«æ§åˆ¶ä¸­çš„è¿åŠ¨é‡å®šå‘ï¼ˆMotion retargetingï¼‰æ•ˆç‡å’Œå¯æ‰©å±•æ€§é—®é¢˜ï¼Œæå‡ºäº†éšå¼åŠ¨åŠ›å­¦è¿åŠ¨é‡å®šå‘æ¡†æ¶ï¼ˆImplicit Kinodynamic Motion Retargetingï¼ŒIKMRï¼‰ã€‚è¯¥æ¡†æ¶åœ¨è¿åŠ¨å­¦ï¼ˆKinematicsï¼‰æ–¹é¢åˆ©ç”¨é¢„è®­ç»ƒçš„è¿åŠ¨æ‹“æ‰‘ç‰¹å¾è¡¨ç¤ºï¼ˆMotion topology feature representationï¼‰å’ŒåŒé‡ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼ˆDual encoder-decoder architectureï¼‰å®ç°è¿åŠ¨åŸŸæ˜ å°„ï¼›åœ¨åŠ¨åŠ›å­¦ï¼ˆDynamicsï¼‰æ–¹é¢å°†æ¨¡ä»¿å­¦ä¹ ï¼ˆImitation learningï¼‰ä¸é‡å®šå‘ç½‘ç»œç»“åˆï¼Œä»¥ç”Ÿæˆç‰©ç†å¯è¡Œçš„è¿åŠ¨è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIKMR èƒ½å¤Ÿå®æ—¶å®Œæˆå¤§è§„æ¨¡çš„ç‰©ç†å¯è¡Œè¿åŠ¨é‡å®šå‘ï¼Œå…¶ç”Ÿæˆçš„å‚è€ƒè½¨è¿¹å¯ç›´æ¥ç”¨äºå…¨èº«æ§åˆ¶å™¨çš„è®­ç»ƒä¸å®é™…éƒ¨ç½²ã€‚è¯¥ç ”ç©¶é€šè¿‡ä»¿çœŸåŠå…¨å°ºå¯¸ç±»äººæœºå™¨äººçš„å®ç‰©å®éªŒï¼Œå……åˆ†éªŒè¯äº† IKMR æ¡†æ¶åœ¨å¤„ç†äººä½“åˆ°ç±»äººæœºå™¨äººè¿åŠ¨è¿ç§»ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ä¸å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15443v1",
      "published_date": "2025-09-18 21:34:02 UTC",
      "updated_date": "2025-09-18 21:34:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:18:41.404873+00:00"
    },
    {
      "arxiv_id": "2509.15440v1",
      "title": "Where Do I 'Add the Egg'?: Exploring Agency and Ownership in AI Creative Co-Writing Systems",
      "title_zh": "æˆ‘åœ¨å“ªé‡Œâ€œåŠ ä¸ªè›‹â€ï¼Ÿï¼šAI åˆ›æ„ååŒå†™ä½œç³»ç»Ÿä¸­çš„èƒ½åŠ¨æ€§ä¸æ‰€æœ‰æƒæ¢æ",
      "authors": [
        "Dashiel Carrera",
        "Jeb Thomas-Mitchell",
        "Daniel Wigdor"
      ],
      "abstract": "AI co-writing systems challenge long held ideals about agency and ownership in the creative process, thereby hindering widespread adoption. In order to address this, we investigate conceptions of agency and ownership in AI creative co-writing. Drawing on insights from a review of commercial systems, we developed three co-writing systems with identical functionality but distinct interface metaphors: agentic, tool-like, and magical. Through interviews with professional and non-professional writers (n = 18), we explored how these metaphors influenced participants' sense of control and authorship. Our analysis resulted in a taxonomy of agency and ownership subtypes and underscore how tool-like metaphors shift writers' expected points of control while agentic metaphors foreground conceptual contributions. We argue that interface metaphors not only guide expectations of control but also frame conceptions of authorship. We conclude with recommendations for the design of AI co-writing systems, emphasizing how metaphor shapes user experience and creative practice.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†AI creative co-writingç³»ç»Ÿåœ¨åˆ›æ„è¿‡ç¨‹ä¸­å¯¹agencyå’Œownershipå¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæ—¨åœ¨ç†è§£ç”¨æˆ·å¯¹è¿™äº›æ ¸å¿ƒæ¦‚å¿µçš„è®¤çŸ¥ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸‰ç§å…·æœ‰ç›¸åŒåŠŸèƒ½ä½†ç•Œé¢éšå–»åˆ†åˆ«ä¸ºagenticã€tool-likeå’Œmagicalçš„åä½œç³»ç»Ÿï¼Œå¹¶å¯¹18åä¸“ä¸šåŠéä¸“ä¸šä½œè€…è¿›è¡Œäº†æ·±å…¥è®¿è°ˆã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†ç•Œé¢éšå–»å¦‚ä½•æ˜¾è‘—å½±å“å‚ä¸è€…çš„æ§åˆ¶æ„Ÿä¸ä½œè€…èº«ä»½è®¤åŒï¼Œå¹¶æ®æ­¤æ„å»ºäº†agencyå’Œownershipçš„å­ç±»å‹åˆ†ç±»æ³•ï¼ˆtaxonomyï¼‰ã€‚åˆ†æå‘ç°ï¼Œtool-likeéšå–»ä¼šæ”¹å˜ä½œè€…çš„é¢„æœŸæ§åˆ¶ç‚¹ï¼Œè€Œagenticéšå–»åˆ™æ›´å¼ºè°ƒæ¦‚å¿µå±‚é¢çš„è´¡çŒ®ã€‚è¯¥ç ”ç©¶è®ºè¯äº†ç•Œé¢éšå–»åœ¨å¼•å¯¼æ§åˆ¶æƒé¢„æœŸå’Œç•Œå®šauthorshipè®¤çŸ¥ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶ä¸ºæœªæ¥AIåä½œç³»ç»Ÿçš„è®¾è®¡å’Œåˆ›æ„å®è·µæå‡ºäº†ä¼˜åŒ–å»ºè®®ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "17 pages, 3 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.15440v1",
      "published_date": "2025-09-18 21:27:12 UTC",
      "updated_date": "2025-09-18 21:27:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:01.793351+00:00"
    },
    {
      "arxiv_id": "2509.15439v1",
      "title": "Dual-Mode Visual System for Brain-Computer Interfaces: Integrating SSVEP and P300 Responses",
      "title_zh": "é¢å‘è„‘æœºæ¥å£çš„åŒæ¨¡è§†è§‰ç³»ç»Ÿï¼šèåˆ SSVEP ä¸ P300 å“åº”",
      "authors": [
        "Ekgari Kasawala",
        "Surej Mouli"
      ],
      "abstract": "In brain-computer interface (BCI) systems, steady-state visual evoked potentials (SSVEP) and P300 responses have achieved widespread implementation owing to their superior information transfer rates (ITR) and minimal training requirements. These neurophysiological signals have exhibited robust efficacy and versatility in external device control, demonstrating enhanced precision and scalability. However, conventional implementations predominantly utilise liquid crystal display (LCD)-based visual stimulation paradigms, which present limitations in practical deployment scenarios. This investigation presents the development and evaluation of a novel light-emitting diode (LED)-based dual stimulation apparatus designed to enhance SSVEP classification accuracy through the integration of both SSVEP and P300 paradigms. The system employs four distinct frequencies, 7 Hz, 8 Hz, 9 Hz, and 10 Hz, corresponding to forward, backward, right, and left directional controls, respectively. Oscilloscopic verification confirmed the precision of these stimulation frequencies. Real-time feature extraction was accomplished through the concurrent analysis of maximum Fast Fourier Transform (FFT) amplitude and P300 peak detection to ascertain user intent. Directional control was determined by the frequency exhibiting maximal amplitude characteristics. The visual stimulation hardware demonstrated minimal frequency deviation, with error differentials ranging from 0.15%to 0.20%across all frequencies. The implemented signal processing algorithm successfully discriminated all four stimulus frequencies whilst correlating them with their respective P300 event markers. Classification accuracy was evaluated based on correct task intention recognition. The proposed hybrid system achieved a mean classification accuracy of 86.25%, coupled with an average ITR of 42.08 bits per minute (bpm).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»ŸåŸºäºLCDçš„è§†è§‰åˆºæ¿€èŒƒå¼åœ¨å®é™…éƒ¨ç½²ä¸­çš„å±€é™æ€§ï¼Œå¼€å‘å¹¶è¯„ä¼°äº†ä¸€ç§æ–°å‹çš„åŸºäºLEDçš„åŒæ¨¡å¼è§†è§‰åˆºæ¿€è£…ç½®ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆSSVEPå’ŒP300èŒƒå¼æ¥æé«˜åˆ†ç±»å‡†ç¡®ç‡ã€‚ç³»ç»Ÿé‡‡ç”¨äº†7 Hzã€8 Hzã€9 Hzå’Œ10 Hzå››ç§ç‰¹å®šé¢‘ç‡ï¼Œåˆ†åˆ«å¯¹åº”å‰ã€åã€å³ã€å·¦å››ä¸ªæ–¹å‘æ§åˆ¶ï¼Œå¹¶é€šè¿‡ç¤ºæ³¢å™¨éªŒè¯äº†åˆºæ¿€é¢‘ç‡çš„ç²¾ç¡®æ€§ã€‚åœ¨ç‰¹å¾æå–æ–¹é¢ï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†Fast Fourier Transform (FFT)æœ€å¤§æŒ¯å¹…åˆ†æå’ŒP300å³°å€¼æ£€æµ‹ï¼Œä»¥å®ç°å¯¹ç”¨æˆ·æ„å›¾çš„å®æ—¶è§£æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥è§†è§‰åˆºæ¿€ç¡¬ä»¶çš„é¢‘ç‡åå·®æä½ï¼Œä»…åœ¨0.15%è‡³0.20%ä¹‹é—´ï¼Œä¸”ä¿¡å·å¤„ç†ç®—æ³•èƒ½å¤ŸæˆåŠŸåŒºåˆ†æ‰€æœ‰åˆºæ¿€é¢‘ç‡å¹¶å…³è”ç›¸åº”çš„P300äº‹ä»¶æ ‡è®°ã€‚æœ€ç»ˆï¼Œè¯¥æ··åˆç³»ç»Ÿå®ç°äº†86.25%çš„å¹³å‡åˆ†ç±»å‡†ç¡®ç‡ï¼Œå¹³å‡ä¿¡æ¯ä¼ è¾“ç‡(ITR)è¾¾åˆ°42.08 bits per minute (bpm)ï¼Œè¯æ˜äº†åŒæ¨¡å¼é›†æˆåœ¨æå‡è„‘æœºæ¥å£ç³»ç»Ÿæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "15 Pages",
      "pdf_url": "https://arxiv.org/pdf/2509.15439v1",
      "published_date": "2025-09-18 21:25:18 UTC",
      "updated_date": "2025-09-18 21:25:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:18:50.293353+00:00"
    },
    {
      "arxiv_id": "2509.15437v1",
      "title": "Impact of Phonetics on Speaker Identity in Adversarial Voice Attack",
      "title_zh": "å¯¹æŠ—æ€§è¯­éŸ³æ”»å‡»ä¸­è¯­éŸ³ç‰¹å¾å¯¹è¯´è¯äººèº«ä»½çš„å½±å“",
      "authors": [
        "Daniyal Kabir Dar",
        "Qiben Yan",
        "Li Xiao",
        "Arun Ross"
      ],
      "abstract": "Adversarial perturbations in speech pose a serious threat to automatic speech recognition (ASR) and speaker verification by introducing subtle waveform modifications that remain imperceptible to humans but can significantly alter system outputs. While targeted attacks on end-to-end ASR models have been widely studied, the phonetic basis of these perturbations and their effect on speaker identity remain underexplored. In this work, we analyze adversarial audio at the phonetic level and show that perturbations exploit systematic confusions such as vowel centralization and consonant substitutions. These distortions not only mislead transcription but also degrade phonetic cues critical for speaker verification, leading to identity drift. Using DeepSpeech as our ASR target, we generate targeted adversarial examples and evaluate their impact on speaker embeddings across genuine and impostor samples. Results across 16 phonetically diverse target phrases demonstrate that adversarial audio induces both transcription errors and identity drift, highlighting the need for phonetic-aware defenses to ensure the robustness of ASR and speaker recognition systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­éŸ³å­¦åœ¨å¯¹æŠ—è¯­éŸ³æ”»å‡»ï¼ˆAdversarial Voice Attackï¼‰ä¸­å¯¹è¯´è¯äººèº«ä»½çš„å½±å“ï¼Œé‡ç‚¹åˆ†æäº†å¯¹æŠ—æ‰°åŠ¨çš„è¯­éŸ³åŸºç¡€åŠå…¶å¯¹è¯´è¯äººéªŒè¯ï¼ˆSpeaker Verificationï¼‰çš„å½±å“ã€‚ç ”ç©¶äººå‘˜é€šè¿‡åœ¨è¯­éŸ³å±‚é¢è¿›è¡Œåˆ†æå‘ç°ï¼Œæ‰°åŠ¨åˆ©ç”¨äº†è¯¸å¦‚å…ƒéŸ³ä¸­å¿ƒåŒ–ï¼ˆVowel Centralizationï¼‰å’Œè¾…éŸ³æ›¿æ¢ï¼ˆConsonant Substitutionsï¼‰ç­‰ç³»ç»Ÿæ€§çš„è¯­éŸ³æ··æ·†æ‰‹æ®µã€‚è¿™äº›æ‰­æ›²ä¸ä»…è¯¯å¯¼äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è½¬å½•ï¼Œè¿˜é€€åŒ–äº†å¯¹è¯´è¯äººè¯†åˆ«è‡³å…³é‡è¦çš„è¯­éŸ³çº¿ç´¢ï¼Œä»è€Œå¯¼è‡´èº«ä»½æ¼‚ç§»ï¼ˆIdentity Driftï¼‰ã€‚é€šè¿‡ä»¥ DeepSpeech ä¸ºç›®æ ‡æ¨¡å‹ç”Ÿæˆé’ˆå¯¹æ€§å¯¹æŠ—æ ·æœ¬ï¼Œå¹¶åœ¨16ä¸ªå…·æœ‰è¯­éŸ³å¤šæ ·æ€§çš„ç›®æ ‡çŸ­è¯­ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå®éªŒè¯æ˜å¯¹æŠ—æ€§éŸ³é¢‘ä¼šåŒæ—¶è¯±å‘è½¬å½•é”™è¯¯å’Œèº«ä»½è¯†åˆ«åå·®ã€‚è¯¥ç ”ç©¶ç»“æœå¼ºè°ƒäº†å¼€å‘å…·å¤‡è¯­éŸ³æ„ŸçŸ¥èƒ½åŠ›çš„é˜²å¾¡æœºåˆ¶ï¼ˆPhonetic-aware Defensesï¼‰çš„ç´§è¿«æ€§ï¼Œä»¥æå‡ ASR å’Œè¯´è¯äººè¯†åˆ«ç³»ç»Ÿçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Additional figures for extended visualization: https://daniyalkabir.github.io/icassp-2025-results/",
      "pdf_url": "https://arxiv.org/pdf/2509.15437v1",
      "published_date": "2025-09-18 21:19:53 UTC",
      "updated_date": "2025-09-18 21:19:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:18:58.987426+00:00"
    },
    {
      "arxiv_id": "2509.15436v1",
      "title": "Region-Aware Deformable Convolutions",
      "title_zh": "åŒºåŸŸæ„ŸçŸ¥å¯å˜å½¢å·ç§¯",
      "authors": [
        "Abolfazl Saheban Maleki",
        "Maryam Imani"
      ],
      "abstract": "We introduce Region-Aware Deformable Convolution (RAD-Conv), a new convolutional operator that enhances neural networks' ability to adapt to complex image structures. Unlike traditional deformable convolutions, which are limited to fixed quadrilateral sampling areas, RAD-Conv uses four boundary offsets per kernel element to create flexible, rectangular regions that dynamically adjust their size and shape to match image content. This approach allows precise control over the receptive field's width and height, enabling the capture of both local details and long-range dependencies, even with small 1x1 kernels. By decoupling the receptive field's shape from the kernel's structure, RAD-Conv combines the adaptability of attention mechanisms with the efficiency of standard convolutions. This innovative design offers a practical solution for building more expressive and efficient vision models, bridging the gap between rigid convolutional architectures and computationally costly attention-based methods.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†Region-Aware Deformable Convolution (RAD-Conv)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºç¥ç»ç½‘ç»œå¯¹å¤æ‚å›¾åƒç»“æ„é€‚åº”èƒ½åŠ›çš„æ–°å‹å·ç§¯ç®—å­ã€‚ä¸å—é™äºå›ºå®šå››è¾¹å½¢é‡‡æ ·åŒºåŸŸçš„ä¼ ç»Ÿå¯å˜å½¢å·ç§¯(deformable convolutions)ä¸åŒï¼ŒRAD-Convé€šè¿‡ä¸ºæ¯ä¸ªå·ç§¯æ ¸å…ƒç´ è®¾ç½®å››ä¸ªè¾¹ç•Œåç§»é‡ï¼Œæ„å»ºå‡ºèƒ½éšå›¾åƒå†…å®¹åŠ¨æ€è°ƒæ•´å½¢çŠ¶ä¸å¤§å°çš„çµæ´»çŸ©å½¢åŒºåŸŸã€‚è¿™ç§è®¾è®¡å®ç°äº†å¯¹æ„Ÿå—é‡(receptive field)å®½é«˜ç»´åº¦çš„ç²¾ç¡®æ§åˆ¶ï¼Œä½¿å…¶å³ä½¿åœ¨1x1å·ç§¯æ ¸ä¸‹ä¹Ÿèƒ½æœ‰æ•ˆæ•è·å±€éƒ¨ç»†èŠ‚ä¸é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚é€šè¿‡å°†æ„Ÿå—é‡å½¢çŠ¶ä¸å·ç§¯æ ¸ç»“æ„è§£è€¦ï¼ŒRAD-ConvæˆåŠŸç»“åˆäº†æ³¨æ„åŠ›æœºåˆ¶(attention mechanisms)çš„è‡ªé€‚åº”ç‰¹æ€§ä¸æ ‡å‡†å·ç§¯çš„é«˜æ•ˆæ€§ã€‚è¿™ä¸€åˆ›æ–°ä¸ºæ„å»ºæ›´å…·è¡¨ç°åŠ›ä¸”ä½åŠŸè€—çš„è§†è§‰æ¨¡å‹æä¾›äº†å®ç”¨æ–¹æ¡ˆï¼Œæœ‰æ•ˆå¡«è¡¥äº†åƒµåŒ–å·ç§¯æ¶æ„ä¸é«˜è®¡ç®—æˆæœ¬æ³¨æ„åŠ›æ–¹æ³•ä¹‹é—´çš„ç©ºç™½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Work in progress; 9 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.15436v1",
      "published_date": "2025-09-18 21:18:36 UTC",
      "updated_date": "2025-09-18 21:18:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:01.089407+00:00"
    },
    {
      "arxiv_id": "2509.15435v1",
      "title": "ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models",
      "title_zh": "ORCAï¼šé¢å‘è§†è§‰è¯­è¨€æ¨¡å‹å¹»è§‰æŠ‘åˆ¶ä¸å¯¹æŠ—é²æ£’æ€§çš„æ™ºèƒ½ä½“æ¨ç†",
      "authors": [
        "Chung-En Johnny Yu",
        "Hsuan-Chih",
        "Chen",
        "Brian Jalaian",
        "Nathaniel D. Bastian"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities but remain vulnerable to hallucinations from intrinsic errors and adversarial attacks from external exploitations, limiting their reliability in real-world applications. We present ORCA, an agentic reasoning framework that improves the factual accuracy and adversarial robustness of pretrained LVLMs through test-time structured inference reasoning with a suite of small vision models (less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act loop, querying multiple visual tools with evidential questions, validating cross-model inconsistencies, and refining predictions iteratively without access to model internals or retraining. ORCA also stores intermediate reasoning traces, which supports auditable decision-making. Though designed primarily to mitigate object-level hallucinations, ORCA also exhibits emergent adversarial robustness without requiring adversarial training or defense mechanisms. We evaluate ORCA across three settings: (1) clean images on hallucination benchmarks, (2) adversarially perturbed images without defense, and (3) adversarially perturbed images with defense applied. On the POPE hallucination benchmark, ORCA improves standalone LVLM performance by +3.64\\% to +40.67\\% across different subsets. Under adversarial perturbations on POPE, ORCA achieves an average accuracy gain of +20.11\\% across LVLMs. When combined with defense techniques on adversarially perturbed AMBER images, ORCA further improves standalone LVLM performance, with gains ranging from +1.20\\% to +48.00\\% across evaluation metrics. These results demonstrate that ORCA offers a promising path toward building more reliable and robust multimodal systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ORCAï¼Œä¸€ä¸ªæ—¨åœ¨æå‡é¢„è®­ç»ƒ Large Vision-Language Models (LVLMs) äº‹å®å‡†ç¡®æ€§ä¸å¯¹æŠ—é²æ£’æ€§ (Adversarial Robustness) çš„æ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ã€‚ORCA é€šè¿‡ Observeâ€“Reasonâ€“Critiqueâ€“Act å¾ªç¯è¿è¡Œï¼Œåˆ©ç”¨ä¸€ç»„å‚æ•°é‡å°äº 3B çš„å°å‹è§†è§‰æ¨¡å‹è¿›è¡Œæµ‹è¯•æ—¶ç»“æ„åŒ–æ¨ç†ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒæˆ–è®¿é—®æ¨¡å‹å†…éƒ¨å‚æ•°ã€‚è¯¥æ¡†æ¶é€šè¿‡å‘è§†è§‰å·¥å…·æå‡ºè¯æ®æ€§é—®é¢˜å¹¶éªŒè¯è·¨æ¨¡å‹çš„ä¸ä¸€è‡´æ€§ï¼Œè¿­ä»£ä¼˜åŒ–é¢„æµ‹ç»“æœï¼Œå¹¶å­˜å‚¨æ¨ç†è½¨è¿¹ä»¥æ”¯æŒå¯å®¡è®¡çš„å†³ç­–è¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒORCA åœ¨ POPE å¹»è§‰åŸºå‡†æµ‹è¯•ä¸­å°† standalone LVLM çš„æ€§èƒ½æå‡äº† 3.64% è‡³ 40.67%ï¼Œå¹¶åœ¨å¯¹æŠ—æ‰°åŠ¨ä¸‹å®ç°äº†å¹³å‡ 20.11% çš„å‡†ç¡®ç‡å¢ç›Šã€‚æ­¤å¤–ï¼ŒORCA åœ¨ç»“åˆé˜²å¾¡æŠ€æœ¯å¤„ç† AMBER æ•°æ®é›†çš„å¯¹æŠ—æ€§æ ·æœ¬æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæœ€é«˜å¢ç›Šè¾¾ 48.00%ï¼Œä¸ºæ„å»ºæ›´å¯é ã€æ›´ç¨³å¥çš„å¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15435v1",
      "published_date": "2025-09-18 21:17:23 UTC",
      "updated_date": "2025-09-18 21:17:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:07.692314+00:00"
    },
    {
      "arxiv_id": "2509.15419v1",
      "title": "Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data",
      "title_zh": "æ·±åº¦å­¦ä¹ ä¸æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå¼æ‘˜è¦ï¼šç¨€ç¼ºæ•°æ®ä¸‹é€‚é… PEGASUS æ¨¡å‹ç³»åˆ—çš„å®è¯ç ”ç©¶",
      "authors": [
        "Claudio Benzoni",
        "Martina Langhals",
        "Martin Boeker",
        "Luise Modersohn",
        "MÃ¡tÃ© E. Maros"
      ],
      "abstract": "Regardless of the rapid development of artificial intelligence, abstractive summarisation is still challenging for sensitive and data-restrictive domains like medicine. With the increasing number of imaging, the relevance of automated tools for complex medical text summarisation is expected to become highly relevant. In this paper, we investigated the adaptation via fine-tuning process of a non-domain-specific abstractive summarisation encoder-decoder model family, and gave insights to practitioners on how to avoid over- and underfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological reports public dataset. For each model, we comprehensively evaluated two different checkpoints with varying sizes of the same training data. We monitored the models' performances with lexical and semantic metrics during the training history on the fixed-size validation set. PEGASUS exhibited different phases, which can be related to epoch-wise double-descent, or peak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger checkpoint led to a performance detriment. This work highlights the challenges and risks of fine-tuning models with high expressivity when dealing with scarce training data, and lays the groundwork for future investigations into more robust fine-tuning strategies for summarisation models in specialised domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ•°æ®å—é™çš„åŒ»å­¦é¢†åŸŸï¼Œå¦‚ä½•é€šè¿‡å¾®è°ƒï¼ˆfine-tuningï¼‰è¿‡ç¨‹ä½¿éé¢†åŸŸç‰¹å®šçš„ç”Ÿæˆå¼æ‘˜è¦ï¼ˆabstractive summarisationï¼‰æ¨¡å‹ç³»åˆ—ï¼ˆPEGASUS å’Œ PEGASUS-Xï¼‰é€‚åº”æ”¾å°„å­¦æŠ¥å‘Šã€‚ä½œè€…åœ¨æ•°æ®è§„æ¨¡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œé’ˆå¯¹ä¸åŒè§„æ¨¡çš„ checkpoint è¿›è¡Œäº†è¯„ä¼°ï¼Œæ—¨åœ¨ä¸ºä»ä¸šè€…æä¾›é¿å…è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰å’Œæ¬ æ‹Ÿåˆï¼ˆunderfittingï¼‰çš„å®ç”¨å»ºè®®ã€‚å®éªŒå‘ç°ï¼ŒPEGASUS åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºä¸ epoch ç›¸å…³çš„åŒé‡ä¸‹é™ï¼ˆdouble-descentï¼‰æˆ–â€œå³°å€¼-ä¸‹é™-æ¢å¤â€è¡Œä¸ºã€‚è€Œå¯¹äº PEGASUS-Xï¼Œç ”ç©¶å‘ç°ä½¿ç”¨æ›´å¤§çš„ checkpoint åè€Œä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¯¥å·¥ä½œçªæ˜¾äº†åœ¨ç¨€ç¼ºæ•°æ®ä¸Šå¾®è°ƒé«˜è¡¨è¾¾èƒ½åŠ›æ¨¡å‹æ‰€é¢ä¸´çš„æŒ‘æˆ˜ä¸é£é™©ï¼Œä¸ºä¸“ä¸šé¢†åŸŸæ‘˜è¦æ¨¡å‹çš„ç¨³å¥å¾®è°ƒç­–ç•¥å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 4 figures, and 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.15419v1",
      "published_date": "2025-09-18 20:51:33 UTC",
      "updated_date": "2025-09-18 20:51:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:16.091383+00:00"
    },
    {
      "arxiv_id": "2509.19364v1",
      "title": "The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior",
      "title_zh": "ç¦»çº¿å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å±€é™æ€§ï¼šåœ¨æ¨¡å‹è¡Œä¸ºä¸­çº³å…¥ä¸ªæ€§åŒ–å› ç´ çš„å¿…è¦æ€§",
      "authors": [
        "Angelina Wang",
        "Daniel E. Ho",
        "Sanmi Koyejo"
      ],
      "abstract": "Standard offline evaluations for language models -- a series of independent, state-less inferences made by models -- fail to capture how language models actually behave in practice, where personalization fundamentally alters model behavior. For instance, identical benchmark questions to the same language model can produce markedly different responses when prompted to a state-less system, in one user's chat session, or in a different user's chat session. In this work, we provide empirical evidence showcasing this phenomenon by comparing offline evaluations to field evaluations conducted by having 800 real users of ChatGPT and Gemini pose benchmark and other provided questions to their chat interfaces.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå½“å‰é’ˆå¯¹ Language Models çš„æ ‡å‡† Offline Evaluations å­˜åœ¨å±€é™æ€§ï¼Œç”±äºå…¶é‡‡ç”¨ç‹¬ç«‹ä¸”æ— çŠ¶æ€çš„æ¨ç†æ–¹å¼ï¼Œæ— æ³•å‡†ç¡®æ•æ‰æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­å— Personalization å½±å“çš„è¡Œä¸ºå˜åŒ–ã€‚ç ”ç©¶å¼ºè°ƒ Personalization ä¼šä»æ ¹æœ¬ä¸Šæ”¹å˜æ¨¡å‹è¡¨ç°ï¼Œå¯¼è‡´åŒä¸€æ¨¡å‹å¯¹ç›¸åŒåŸºå‡†æµ‹è¯•é—®é¢˜çš„å“åº”åœ¨ä¸åŒç”¨æˆ·èŠå¤©ä¼šè¯ä¸­å‘ˆç°æ˜¾è‘—å·®å¼‚ã€‚ä¸ºæä¾›å®è¯ä¾æ®ï¼Œç ”ç©¶è€…å¯¹æ¯”äº†çº¿ä¸‹è¯„ä¼°ä¸ Field Evaluationsï¼Œé€šè¿‡ 800 å ChatGPT å’Œ Gemini çœŸå®ç”¨æˆ·åœ¨å®é™…äº¤äº’ç•Œé¢ä¸­çš„æµ‹è¯•ç»“æœè¿›è¡Œäº†æ·±å…¥åˆ†æã€‚å®éªŒç»“æœè¯æ˜ï¼Œç°æœ‰çš„çº¿ä¸‹è¯„ä¼°ä½“ç³»å› æœªèƒ½è€ƒè™‘ä¸ªæ€§åŒ–å¯¹æ¨¡å‹è¡Œä¸ºçš„åŠ¨æ€å½±å“ï¼Œéš¾ä»¥çœŸå®åæ˜ è¯­è¨€æ¨¡å‹åœ¨å®è·µä¸­çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "forthcoming in Patterns",
      "pdf_url": "https://arxiv.org/pdf/2509.19364v1",
      "published_date": "2025-09-18 20:41:20 UTC",
      "updated_date": "2025-09-18 20:41:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:18.994282+00:00"
    },
    {
      "arxiv_id": "2509.15409v2",
      "title": "FragmentRetro: A Quadratic Retrosynthetic Method Based on Fragmentation Algorithms",
      "title_zh": "FragmentRetroï¼šåŸºäºç¢ç‰‡åŒ–ç®—æ³•çš„å¹³æ–¹å¤æ‚åº¦é€†åˆæˆæ–¹æ³•",
      "authors": [
        "Yu Shee",
        "Anthony M. Smaldone",
        "Anton Morgunov",
        "Gregory W. Kyro",
        "Victor S. Batista"
      ],
      "abstract": "Retrosynthesis, the process of deconstructing a target molecule into simpler precursors, is crucial for computer-aided synthesis planning (CASP). Widely adopted tree-search methods often suffer from exponential computational complexity. In this work, we introduce FragmentRetro, a novel retrosynthetic method that leverages fragmentation algorithms, specifically BRICS and r-BRICS, combined with stock-aware exploration and pattern fingerprint screening to achieve quadratic complexity. FragmentRetro recursively combines molecular fragments and verifies their presence in a building block set, providing sets of fragment combinations as retrosynthetic solutions. We present the first formal computational analysis of retrosynthetic methods, showing that tree search exhibits exponential complexity $O(b^h)$, DirectMultiStep scales as $O(h^6)$, and FragmentRetro achieves $O(h^2)$, where $h$ represents the number of heavy atoms in the target molecule and $b$ is the branching factor for tree search. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate that FragmentRetro achieves high solved rates with competitive runtime, including cases where tree search fails. The method benefits from fingerprint screening, which significantly reduces substructure matching complexity. While FragmentRetro focuses on efficiently identifying fragment-based solutions rather than full reaction pathways, its computational advantages and ability to generate strategic starting candidates establish it as a powerful foundational component for scalable and automated synthesis planning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FragmentRetroï¼Œä¸€ç§åŸºäºåˆ†å­ç¢ç‰‡åŒ–ç®—æ³•(Fragmentation Algorithms)çš„æ–°å‹é€†åˆæˆåˆ†ææ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è®¡ç®—æœºè¾…åŠ©åˆæˆè§„åˆ’(CASP)ä¸­æ ‘æœç´¢æ–¹æ³•é¢ä¸´çš„æŒ‡æ•°çº§è®¡ç®—å¤æ‚åº¦é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº† BRICS å’Œ r-BRICS ç¢ç‰‡åŒ–ç®—æ³•ã€åº“å­˜æ„ŸçŸ¥æœç´¢(Stock-aware exploration)ä»¥åŠæ¨¡å¼æŒ‡çº¹ç­›é€‰(Pattern fingerprint screening)ï¼Œå®ç°äº†ä»…ä¸º $O(h^2)$ çš„å¹³æ–¹çº§è®¡ç®—å¤æ‚åº¦ã€‚FragmentRetro é€šè¿‡é€’å½’ç»„åˆåˆ†å­ç¢ç‰‡å¹¶éªŒè¯å…¶åœ¨æ„å»ºå—é›†ä¸­çš„å­˜åœ¨æ¥æä¾›é€†åˆæˆæ–¹æ¡ˆï¼Œå¹¶åˆ©ç”¨æŒ‡çº¹ç­›é€‰æ˜¾è‘—é™ä½äº†å­ç»“æ„åŒ¹é…çš„éš¾åº¦ã€‚åœ¨ PaRoutesã€USPTO-190 å’Œå¤©ç„¶äº§ç‰©æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒFragmentRetro åœ¨ä¿æŒç«äº‰åŠ›è¿è¡Œæ—¶é—´çš„åŒæ—¶å®ç°äº†é«˜æ±‚è§£ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¼ ç»Ÿæ ‘æœç´¢æ–¹æ³•å¤±æ•ˆçš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ã€‚å°½ç®¡è¯¥æ–¹æ³•ç›®å‰ä¾§é‡äºé«˜æ•ˆè¯†åˆ«åŸºäºç¢ç‰‡çš„è§£å†³æ–¹æ¡ˆè€Œéå®Œæ•´ååº”è·¯å¾„ï¼Œå…¶åœ¨è®¡ç®—æ•ˆç‡å’Œç”Ÿæˆæˆ˜ç•¥æ€§åˆå§‹å€™é€‰åˆ†å­æ–¹é¢çš„ä¼˜åŠ¿ä½¿å…¶æˆä¸ºå¯æ‰©å±•è‡ªåŠ¨åŒ–åˆæˆè§„åˆ’çš„é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Code available on GitHub https://github.com/randyshee/FragmentRetro Documentation is available https://fragment.batistalab.com/",
      "pdf_url": "https://arxiv.org/pdf/2509.15409v2",
      "published_date": "2025-09-18 20:36:22 UTC",
      "updated_date": "2026-01-06 17:14:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:22.769003+00:00"
    },
    {
      "arxiv_id": "2509.15400v1",
      "title": "Exploring multimodal implicit behavior learning for vehicle navigation in simulated cities",
      "title_zh": "æ¨¡æ‹ŸåŸå¸‚ç¯å¢ƒä¸‹è½¦è¾†å¯¼èˆªçš„å¤šæ¨¡æ€éšå¼è¡Œä¸ºå­¦ä¹ æ¢ç´¢",
      "authors": [
        "Eric Aislan Antonelo",
        "Gustavo Claudio Karl Couto",
        "Christian MÃ¶ller"
      ],
      "abstract": "Standard Behavior Cloning (BC) fails to learn multimodal driving decisions, where multiple valid actions exist for the same scenario. We explore Implicit Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to better capture this multimodality. We propose Data-Augmented IBC (DA-IBC), which improves learning by perturbing expert actions to form the counterexamples of IBC training and using better initialization for derivative-free inference. Experiments in the CARLA simulator with Bird's-Eye View inputs demonstrate that DA-IBC outperforms standard IBC in urban driving tasks designed to evaluate multimodal behavior learning in a test environment. The learned energy landscapes are able to represent multimodal action distributions, which BC fails to achieve.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ ‡å‡† Behavior Cloning (BC) åœ¨å¤„ç†å…·æœ‰å¤šä¸ªæœ‰æ•ˆåŠ¨ä½œçš„å¤šæ¨¡æ€ (multimodal) é©¾é©¶å†³ç­–æ—¶å­˜åœ¨çš„å±€é™æ€§ï¼Œæ¢ç´¢äº†ç»“åˆèƒ½é‡æ¨¡å‹ (Energy-Based Models, EBMs) çš„éšå¼è¡Œä¸ºå…‹éš† (Implicit Behavioral Cloning, IBC) æŠ€æœ¯ã€‚ä½œè€…æå‡ºäº†å¢å¼ºç‰ˆéšå¼è¡Œä¸ºå…‹éš† (Data-Augmented IBC, DA-IBC)ï¼Œé€šè¿‡æ‰°åŠ¨ä¸“å®¶åŠ¨ä½œæ¥æ„å»ºè®­ç»ƒä¸­çš„åä¾‹ï¼Œå¹¶ä¸ºæ— å¯¼æ•°æ¨ç†æä¾›æ›´å¥½çš„åˆå§‹åŒ–ï¼Œä»è€Œæå‡å­¦ä¹ æ•ˆæœã€‚åœ¨ CARLA æ¨¡æ‹Ÿå™¨ä¸­åˆ©ç”¨é¸Ÿç°å›¾ (Bird's-Eye View) è¾“å…¥è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒDA-IBC åœ¨åŸå¸‚é©¾é©¶ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºæ ‡å‡† IBCã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•å­¦ä¹ åˆ°çš„èƒ½é‡æ™¯è§‚ (energy landscapes) èƒ½å¤Ÿæœ‰æ•ˆè¡¨å¾å¤šæ¨¡æ€åŠ¨ä½œåˆ†å¸ƒï¼Œå®ç°äº†ä¼ ç»Ÿ BC æ— æ³•è¾¾åˆ°çš„å»ºæ¨¡èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "ENIAC conference",
      "pdf_url": "https://arxiv.org/pdf/2509.15400v1",
      "published_date": "2025-09-18 20:17:29 UTC",
      "updated_date": "2025-09-18 20:17:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:21.558818+00:00"
    },
    {
      "arxiv_id": "2509.19363v1",
      "title": "Analyzing the Impact of Credit Card Fraud on Economic Fluctuations of American Households Using an Adaptive Neuro-Fuzzy Inference System",
      "title_zh": "åŸºäºè‡ªé€‚åº”ç¥ç»æ¨¡ç³Šæ¨ç†ç³»ç»Ÿçš„ä¿¡ç”¨å¡æ¬ºè¯ˆå¯¹ç¾å›½å®¶åº­ç»æµæ³¢åŠ¨å½±å“åˆ†æ",
      "authors": [
        "Zhuqi Wang",
        "Qinghe Zhang",
        "Zhuopei Cheng"
      ],
      "abstract": "Credit card fraud is assuming growing proportions as a major threat to the financial position of American household, leading to unpredictable changes in household economic behavior. To solve this problem, in this paper, a new hybrid analysis method is presented by using the Enhanced ANFIS. The model proposes several advances of the conventional ANFIS framework and employs a multi-resolution wavelet decomposition module and a temporal attention mechanism. The model performs discrete wavelet transformations on historical transaction data and macroeconomic indicators to generate localized economic shock signals. The transformed features are then fed into a deep fuzzy rule library which is based on Takagi-Sugeno fuzzy rules with adaptive Gaussian membership functions. The model proposes a temporal attention encoder that adaptively assigns weights to multi-scale economic behavior patterns, increasing the effectiveness of relevance assessment in the fuzzy inference stage and enhancing the capture of long-term temporal dependencies and anomalies caused by fraudulent activities. The proposed method differs from classical ANFIS which has fixed input-output relations since it integrates fuzzy rule activation with the wavelet basis selection and the temporal correlation weights via a modular training procedure. Experimental results show that the RMSE was reduced by 17.8% compared with local neuro-fuzzy models and conventional LSTM models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºå‹è‡ªé€‚åº”ç¥ç»æ¨¡ç³Šæ¨ç†ç³»ç»Ÿ (Enhanced ANFIS)ï¼Œæ—¨åœ¨åˆ†æä¿¡ç”¨å¡æ¬ºè¯ˆå¯¹ç¾å›½å®¶åº­ç»æµæ³¢åŠ¨çš„å†²å‡»å¹¶é¢„æµ‹å…¶è¡Œä¸ºå˜åŒ–ã€‚è¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°é›†æˆäº†å¤šåˆ†è¾¨ç‡å°æ³¢åˆ†è§£ (Multi-resolution Wavelet Decomposition) æ¨¡å—å’Œæ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ (Temporal Attention Mechanism)ï¼Œé€šè¿‡å¯¹äº¤æ˜“æ•°æ®å’Œå®è§‚ç»æµæŒ‡æ ‡è¿›è¡Œç¦»æ•£å°æ³¢å˜æ¢æ¥ç”Ÿæˆå±€éƒ¨ç»æµå†²å‡»ä¿¡å·ã€‚ç³»ç»Ÿåˆ©ç”¨åŸºäº Takagi-Sugeno æ¨¡ç³Šè§„åˆ™çš„æ·±åº¦è§„åˆ™åº“å’Œè‡ªé€‚åº”é«˜æ–¯éš¶å±å‡½æ•° (Adaptive Gaussian Membership Functions)ï¼Œèƒ½å¤Ÿæ›´ç²¾å‡†åœ°åˆ»ç”»å¤æ‚çš„ç»æµå…³è”ã€‚æ¨¡å‹ä¸­çš„æ—¶é—´æ³¨æ„åŠ›ç¼–ç å™¨å¯è‡ªé€‚åº”åˆ†é…æƒé‡ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹æ¬ºè¯ˆå¼•èµ·çš„é•¿æœŸæ—¶é—´ä¾èµ–æ€§å’Œå¼‚å¸¸æ¨¡å¼çš„æ•è·èƒ½åŠ›ã€‚é€šè¿‡æ¨¡å—åŒ–è®­ç»ƒç¨‹åºï¼Œè¯¥æ–¹æ³•å®ç°äº†æ¨¡ç³Šè§„åˆ™æ¿€æ´»ã€å°æ³¢åŸºé€‰æ‹©ä¸æ—¶é—´æƒé‡çš„é«˜åº¦é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„ LSTM å’Œå±€éƒ¨ç¥ç»æ¨¡ç³Šæ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„å‡æ–¹æ ¹è¯¯å·® (RMSE) é™ä½äº† 17.8%ï¼Œä¸ºé‡åŒ–ä¿¡ç”¨å¡æ¬ºè¯ˆå¯¹å®¶åº­ç»æµè¡Œä¸ºçš„å½±å“æä¾›äº†æ›´é«˜æ•ˆä¸”ç²¾ç¡®çš„åˆ†æå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19363v1",
      "published_date": "2025-09-18 20:09:07 UTC",
      "updated_date": "2025-09-18 20:09:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:39.683021+00:00"
    },
    {
      "arxiv_id": "2509.15393v1",
      "title": "Generating Part-Based Global Explanations Via Correspondence",
      "title_zh": "é€šè¿‡å¯¹åº”å…³ç³»ç”ŸæˆåŸºäºéƒ¨ä»¶çš„å…¨å±€è§£é‡Š",
      "authors": [
        "Kunal Rathore",
        "Prasad Tadepalli"
      ],
      "abstract": "Deep learning models are notoriously opaque. Existing explanation methods often focus on localized visual explanations for individual images. Concept-based explanations, while offering global insights, require extensive annotations, incurring significant labeling cost. We propose an approach that leverages user-defined part labels from a limited set of images and efficiently transfers them to a larger dataset. This enables the generation of global symbolic explanations by aggregating part-based local explanations, ultimately providing human-understandable explanations for model decisions on a large scale.",
      "tldr_zh": "é’ˆå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¸é€æ˜æ€§ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡å¯¹åº”å…³ç³»ï¼ˆCorrespondenceï¼‰ç”ŸæˆåŸºäºéƒ¨åˆ†çš„å…¨å±€è§£é‡Šï¼ˆPart-Based Global Explanationsï¼‰çš„æ–°æ–¹æ³•ã€‚ç°æœ‰çš„å±€éƒ¨è§†è§‰è§£é‡Šæ–¹æ³•é€šå¸¸ä»…é’ˆå¯¹å•å¼ å›¾åƒï¼Œè€ŒåŸºäºæ¦‚å¿µçš„å…¨å±€è§£é‡Šè™½èƒ½æä¾›å®è§‚è§è§£ï¼Œå´é¢ä¸´é«˜æ˜‚çš„æ ‡æ³¨æˆæœ¬æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»å°‘é‡å›¾åƒä¸­è·å–ç”¨æˆ·å®šä¹‰çš„éƒ¨åˆ†æ ‡ç­¾ï¼ˆPart Labelsï¼‰ï¼Œå¹¶åˆ©ç”¨å¯¹åº”å…³ç³»å°†å…¶é«˜æ•ˆè¿ç§»è‡³å¤§è§„æ¨¡æ•°æ®é›†ã€‚é€šè¿‡èšåˆè¿™äº›åŸºäºéƒ¨åˆ†çš„å±€éƒ¨è§£é‡Šï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆå…¨å±€ç¬¦å·åŒ–è§£é‡Šï¼ˆGlobal Symbolic Explanationsï¼‰ï¼Œä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹æä¾›äººç±»å¯ç†è§£çš„è¯´æ˜ã€‚è¿™ç§æ–¹æ³•åœ¨æ˜¾è‘—é™ä½äººå·¥æ ‡æ³¨è´Ÿæ‹…çš„åŒæ—¶ï¼Œå®ç°äº†åœ¨å¤§è§„æ¨¡æ•°æ®å°ºåº¦ä¸Šå¯¹æ¨¡å‹è¡Œä¸ºçš„æ·±å…¥æ´å¯Ÿä¸è§£é‡Šã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15393v1",
      "published_date": "2025-09-18 20:00:49 UTC",
      "updated_date": "2025-09-18 20:00:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:39.187188+00:00"
    },
    {
      "arxiv_id": "2509.15380v2",
      "title": "Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios",
      "title_zh": "é¢å‘ä¼Šæ–¯å…°æ–‡æœ¬å¤šè¯­è¨€ä¿¡æ¯æ£€ç´¢çš„é«˜æ•ˆé€šç”¨æ¨¡å‹ï¼šçœŸå®åœºæ™¯ä¸‹çš„å¼€å‘ä¸éƒ¨ç½²",
      "authors": [
        "Vera Pavlova",
        "Mohammed Makhlouf"
      ],
      "abstract": "Despite recent advancements in Multilingual Information Retrieval (MLIR), a significant gap remains between research and practical deployment. Many studies assess MLIR performance in isolated settings, limiting their applicability to real-world scenarios. In this work, we leverage the unique characteristics of the Quranic multilingual corpus to examine the optimal strategies to develop an ad-hoc IR system for the Islamic domain that is designed to satisfy users' information needs in multiple languages. We prepared eleven retrieval models employing four training approaches: monolingual, cross-lingual, translate-train-all, and a novel mixed method combining cross-lingual and monolingual techniques. Evaluation on an in-domain dataset demonstrates that the mixed approach achieves promising results across diverse retrieval scenarios. Furthermore, we provide a detailed analysis of how different training configurations affect the embedding space and their implications for multilingual retrieval effectiveness. Finally, we discuss deployment considerations, emphasizing the cost-efficiency of deploying a single versatile, lightweight model for real-world MLIR applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šè¯­è¨€ä¿¡æ¯æ£€ç´¢(Multilingual Information Retrieval, MLIR)åœ¨ä¼Šæ–¯å…°é¢†åŸŸç ”ç©¶ä¸å®é™…éƒ¨ç½²ä¹‹é—´çš„å·®è·ï¼Œåˆ©ç”¨å¤å…°ç»å¤šè¯­è¨€è¯­æ–™åº“å¼€å‘äº†ä¸€å¥—æ»¡è¶³å¤šè¯­è¨€ä¿¡æ¯éœ€æ±‚çš„å³å¸­æ£€ç´¢(ad-hoc IR)ç³»ç»Ÿã€‚ç ”ç©¶è€…è¯„ä¼°äº†åŒ…æ‹¬å•è¯­è¨€(monolingual)ã€è·¨è¯­è¨€(cross-lingual)ã€ç¿»è¯‘è®­ç»ƒ(translate-train-all)ä»¥åŠä¸€ç§ç»“åˆè·¨è¯­è¨€ä¸å•è¯­è¨€æŠ€æœ¯çš„æ–°å‹æ··åˆæ–¹æ³•åœ¨å†…çš„11ç§æ£€ç´¢æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ··åˆæ–¹æ³•åœ¨å¤šæ ·åŒ–çš„æ£€ç´¢åœºæ™¯ä¸­å‡å–å¾—äº†ç†æƒ³çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¯¦ç»†åˆ†æäº†ä¸åŒè®­ç»ƒé…ç½®å¯¹åµŒå…¥ç©ºé—´(embedding space)çš„å½±å“ï¼Œæ¢è®¨äº†å…¶å¯¹æ£€ç´¢æœ‰æ•ˆæ€§çš„æ„ä¹‰ã€‚æœ€åï¼Œç ”ç©¶å¼ºè°ƒäº†éƒ¨ç½²å•ä¸ªå¤šåŠŸèƒ½ã€è½»é‡åŒ–æ¨¡å‹åœ¨çœŸå®ä¸–ç•ŒMLIRåº”ç”¨ä¸­çš„æˆæœ¬æ•ˆç›Šï¼Œä¸ºç›¸å…³ç³»ç»Ÿçš„å®é™…è½åœ°æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15380v2",
      "published_date": "2025-09-18 19:32:07 UTC",
      "updated_date": "2025-10-14 08:15:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:44.074658+00:00"
    },
    {
      "arxiv_id": "2509.16279v1",
      "title": "Energy Equity, Infrastructure and Demographic Analysis with XAI Methods",
      "title_zh": "åŸºäºå¯è§£é‡Šäººå·¥æ™ºèƒ½æ–¹æ³•çš„èƒ½æºå…¬å¹³ã€åŸºç¡€è®¾æ–½ä¸äººå£ç»Ÿè®¡åˆ†æ",
      "authors": [
        "Sarahana Shrestha",
        "Aparna S. Varde",
        "Pankaj Lal"
      ],
      "abstract": "This study deploys methods in explainable artificial intelligence (XAI), e.g. decision trees and Pearson's correlation coefficient (PCC), to investigate electricity usage in multiple locales. It addresses the vital issue of energy burden, i.e. total amount spent on energy divided by median household income. Socio-demographic data is analyzed with energy features, especially using decision trees and PCC, providing explainable predictors on factors affecting energy burden. Based on the results of the analysis, a pilot energy equity web portal is designed along with a novel energy burden calculator. Leveraging XAI, this portal (with its calculator) serves as a prototype information system that can offer tailored actionable advice to multiple energy stakeholders. The ultimate goal of this study is to promote greater energy equity through the adaptation of XAI methods for energy-related analysis with suitable recommendations.",
      "tldr_zh": "è¯¥ç ”ç©¶åº”ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ (XAI) æ–¹æ³•ï¼ŒåŒ…æ‹¬å†³ç­–æ ‘ (Decision Trees) å’Œçš®å°”é€Šç›¸å…³ç³»æ•° (PCC)ï¼Œå¯¹å¤šä¸ªåœ°åŒºçš„ç”µåŠ›ä½¿ç”¨æƒ…å†µè¿›è¡Œäº†æ·±å…¥è°ƒç ”ã€‚è®ºæ–‡é‡ç‚¹æ¢è®¨äº†èƒ½æºè´Ÿæ‹… (Energy burden) è¿™ä¸€æ ¸å¿ƒè®®é¢˜ï¼Œå³èƒ½æºæ€»æ”¯å‡ºä¸å®¶åº­æ”¶å…¥ä¸­ä½æ•°çš„æ¯”ä¾‹å…³ç³»ã€‚é€šè¿‡å°†ç¤¾ä¼šäººå£ç»Ÿè®¡æ•°æ®ä¸èƒ½æºç‰¹å¾ç›¸ç»“åˆï¼Œç ”ç©¶åˆ©ç”¨ XAI æŠ€æœ¯è¯†åˆ«äº†å½±å“èƒ½æºè´Ÿæ‹…çš„å…³é”®é¢„æµ‹å› å­å¹¶æä¾›äº†è§£é‡Šæ€§åˆ†æã€‚åŸºäºåˆ†æç»“æœï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªèƒ½æºå…¬å¹³ç½‘ç»œé—¨æˆ·è¯•ç‚¹åŠä¸€ç§æ–°å‹èƒ½æºè´Ÿæ‹…è®¡ç®—å™¨ (Energy burden calculator)ã€‚è¯¥é—¨æˆ·ç½‘ç«™ä½œä¸ºåŸå‹ä¿¡æ¯ç³»ç»Ÿï¼Œæ—¨åœ¨åˆ©ç”¨ XAI ä¸ºå„ç±»èƒ½æºåˆ©ç›Šç›¸å…³è€…æä¾›é‡èº«å®šåˆ¶çš„å¯æ“ä½œæ€§å»ºè®®ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶é€šè¿‡é€‚é… XAI æ–¹æ³•è¿›è¡Œèƒ½æºåˆ†æï¼Œä¸ºä¿ƒè¿›èƒ½æºå…¬å¹³ (Energy equity) æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ‰‹æ®µå’Œå†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16279v1",
      "published_date": "2025-09-18 19:13:39 UTC",
      "updated_date": "2025-09-18 19:13:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:46.878428+00:00"
    },
    {
      "arxiv_id": "2509.15366v1",
      "title": "Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context",
      "title_zh": "åŸºäºåŠ¨æ€è¯„ä¼°åè®®ä¸åç»­å¤„ç†ä¸Šä¸‹æ–‡çªå˜çš„å¤šæ™ºèƒ½ä½“ä¸“å®¶ç³»ç»Ÿè®¤çŸ¥å¤±æ•ˆè¯Šæ–­",
      "authors": [
        "Andrejs Sorstkins",
        "Josh Bailey",
        "Dr Alistair Baron"
      ],
      "abstract": "The rapid evolution of neural architectures - from multilayer perceptrons to large-scale Transformer-based models - has enabled language models (LLMs) to exhibit emergent agentic behaviours when equipped with memory, planning, and external tool use. However, their inherent stochasticity and multi-step decision processes render classical evaluation methods inadequate for diagnosing agentic performance. This work introduces a diagnostic framework for expert systems that not only evaluates but also facilitates the transfer of expert behaviour into LLM-powered agents. The framework integrates (i) curated golden datasets of expert annotations, (ii) silver datasets generated through controlled behavioural mutation, and (iii) an LLM-based Agent Judge that scores and prescribes targeted improvements. These prescriptions are embedded into a vectorized recommendation map, allowing expert interventions to propagate as reusable improvement trajectories across multiple system instances. We demonstrate the framework on a multi-agent recruiter-assistant system, showing that it uncovers latent cognitive failures - such as biased phrasing, extraction drift, and tool misrouting - while simultaneously steering agents toward expert-level reasoning and style. The results establish a foundation for standardized, reproducible expert behaviour transfer in stochastic, tool-augmented LLM agents, moving beyond static evaluation to active expert system refinement.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ™ºèƒ½ä½“(LLM-powered agents)åœ¨æ‰§è¡Œå¤šæ­¥å†³ç­–æ—¶å­˜åœ¨çš„éšæœºæ€§å’Œè¯„ä¼°å›°éš¾ï¼Œæå‡ºäº†ä¸€ä¸ªç”¨äºè¯Šæ–­ä¸“å®¶ç³»ç»Ÿå¹¶ä¿ƒè¿›ä¸“å®¶è¡Œä¸ºè½¬ç§»çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ•´åˆäº†ä¸“å®¶æ ‡æ³¨çš„Golden datasetsã€é€šè¿‡å—æ§è¡Œä¸ºå˜å¼‚(Behavioral mutation)ç”Ÿæˆçš„Silver datasetsï¼Œä»¥åŠä¸€ä¸ªè´Ÿè´£è¯„åˆ†å¹¶æä¾›é’ˆå¯¹æ€§æ”¹è¿›å»ºè®®çš„LLM-based Agent Judgeã€‚è¿™äº›å»ºè®®è¢«åµŒå…¥åˆ°å‘é‡åŒ–æ¨èåœ°å›¾(Vectorized recommendation map)ä¸­ï¼Œä½¿ä¸“å®¶å¹²é¢„èƒ½å¤Ÿä½œä¸ºå¯é‡ç”¨çš„æ”¹è¿›è½¨è¿¹åœ¨ä¸åŒç³»ç»Ÿå®ä¾‹ä¸­ä¼ æ’­ã€‚åœ¨å¤šæ™ºèƒ½ä½“æ‹›è˜åŠ©ç†ç³»ç»Ÿä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæ­ç¤ºåè§è¡¨è¿°(Biased phrasing)ã€æå–æ¼‚ç§»(Extraction drift)å’Œå·¥å…·è¯¯è·¯ç”±(Tool misrouting)ç­‰æ½œåœ¨è®¤çŸ¥å¤±æ•ˆã€‚ç ”ç©¶ç»“æœä¸ºåœ¨éšæœºä¸”å…·å¤‡å·¥å…·å¢å¼ºèƒ½åŠ›çš„æ™ºèƒ½ä½“ä¸­å®ç°æ ‡å‡†åŒ–ã€å¯å¤ç°çš„ä¸“å®¶è¡Œä¸ºè½¬ç§»å¥ å®šäº†åŸºç¡€ï¼Œå®ç°äº†ä»é™æ€è¯„ä¼°å‘ä¸»åŠ¨ä¸“å®¶ç³»ç»Ÿç²¾ç‚¼çš„è·¨è¶Šã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Dissertation and research project created in collaboration with JobFair LTD",
      "pdf_url": "https://arxiv.org/pdf/2509.15366v1",
      "published_date": "2025-09-18 19:08:03 UTC",
      "updated_date": "2025-09-18 19:08:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:05.160560+00:00"
    },
    {
      "arxiv_id": "2509.15363v2",
      "title": "Recent Advancements in Microscopy Image Enhancement using Deep Learning: A Survey",
      "title_zh": "æ·±åº¦å­¦ä¹ åœ¨æ˜¾å¾®å›¾åƒå¢å¼ºä¸­çš„æœ€æ–°è¿›å±•ï¼šç»¼è¿°",
      "authors": [
        "Debasish Dutta",
        "Neeharika Sonowal",
        "Risheraj Barauh",
        "Deepjyoti Chetia",
        "Sanjib Kr Kalita"
      ],
      "abstract": "Microscopy image enhancement plays a pivotal role in understanding the details of biological cells and materials at microscopic scales. In recent years, there has been a significant rise in the advancement of microscopy image enhancement, specifically with the help of deep learning methods. This survey paper aims to provide a snapshot of this rapidly growing state-of-the-art method, focusing on its evolution, applications, challenges, and future directions. The core discussions take place around the key domains of microscopy image enhancement of super-resolution, reconstruction, and denoising, with each domain explored in terms of its current trends and their practical utility of deep learning.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°è®ºæ–‡ç³»ç»Ÿåœ°å›é¡¾äº†åˆ©ç”¨ Deep Learning æŠ€æœ¯åœ¨æ˜¾å¾®é•œå›¾åƒå¢å¼º (Microscopy Image Enhancement) é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚ç ”ç©¶é‡ç‚¹æ¢è®¨äº†è¯¥æŠ€æœ¯çš„æ¼”å˜å†ç¨‹ã€å®é™…åº”ç”¨åœºæ™¯ã€å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠæœªæ¥çš„å‘å±•æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºç§‘ç ”äººå‘˜æä¾›è¯¥é¢†åŸŸçš„ State-of-the-art æŠ€æœ¯å¿«ç…§ã€‚è®ºæ–‡çš„æ ¸å¿ƒè®¨è®ºå›´ç»•ç€è¶…åˆ†è¾¨ç‡ (Super-resolution)ã€å›¾åƒé‡å»º (Reconstruction) å’Œå»å™ª (Denoising) è¿™ä¸‰ä¸ªå…³é”®é¢†åŸŸå±•å¼€ã€‚é€šè¿‡æ·±å…¥åˆ†ææ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è¿™äº›é¢†åŸŸä¸­çš„å½“å‰è¶‹åŠ¿ï¼Œæ–‡ç« é˜è¿°äº†å…¶åœ¨æå‡ç”Ÿç‰©ç»†èƒå’Œææ–™å¾®è§‚ç»†èŠ‚ç†è§£æ–¹é¢çš„å®ç”¨ä»·å€¼ã€‚è¯¥ç»¼è¿°ä¸ä»…æ€»ç»“äº†ç°æœ‰çš„æŠ€æœ¯è·¯å¾„ï¼Œè¿˜ä¸ºæ¨åŠ¨æ˜¾å¾®é•œæˆåƒæŠ€æœ¯çš„æ™ºèƒ½åŒ–å‘å±•æä¾›äº†é‡è¦çš„ç†è®ºå‚è€ƒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "7 pages, 3 figures and 1 table. 2024 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI). IEEE, 2024",
      "pdf_url": "https://arxiv.org/pdf/2509.15363v2",
      "published_date": "2025-09-18 19:03:41 UTC",
      "updated_date": "2025-09-26 06:24:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:19:49.995112+00:00"
    },
    {
      "arxiv_id": "2509.15361v1",
      "title": "Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing",
      "title_zh": "è¶…è¶Šè™šå‡ä¿¡å·ï¼šåŸºäºåäº‹å®æ¨ç†ä¸è‡ªé€‚åº”ä¸“å®¶è·¯ç”±çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å»å",
      "authors": [
        "Zichen Wu",
        "Hsiu-Yuan Huang",
        "Yunfang Wu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown substantial capabilities in integrating visual and textual information, yet frequently rely on spurious correlations, undermining their robustness and generalization in complex multimodal reasoning tasks. This paper addresses the critical challenge of superficial correlation bias in MLLMs through a novel causal mediation-based debiasing framework. Specially, we distinguishing core semantics from spurious textual and visual contexts via counterfactual examples to activate training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture with dynamic routing to selectively engages modality-specific debiasing experts. Empirical evaluation on multimodal sarcasm detection and sentiment analysis tasks demonstrates that our framework significantly surpasses unimodal debiasing strategies and existing state-of-the-art models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(Multimodal Large Language Models, MLLMs)å®¹æ˜“ä¾èµ–è™šå‡ç›¸å…³æ€§(spurious correlations)ä»è€ŒæŸå®³å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­é²æ£’æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå› æœä¸­ä»‹(causal mediation)çš„åˆ›æ–°å»åæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åäº‹å®ç¤ºä¾‹(counterfactual examples)å°†æ ¸å¿ƒè¯­ä¹‰ä¸è™šå‡çš„æ–‡æœ¬å’Œè§†è§‰ä¸Šä¸‹æ–‡åŒºåˆ†å¼€ï¼Œä»¥æ­¤åœ¨è®­ç»ƒé˜¶æ®µæ¿€æ´»æœ‰æ•ˆçš„å»åæœºåˆ¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨äº†æ··åˆä¸“å®¶æ¶æ„(Mixture-of-Experts, MoE)å¹¶ç»“åˆåŠ¨æ€è·¯ç”±(dynamic routing)æŠ€æœ¯ï¼Œå®ç°äº†å¯¹ç‰¹å®šæ¨¡æ€å»åä¸“å®¶çš„é€‰æ‹©æ€§è°ƒç”¨ã€‚åœ¨å¤šæ¨¡æ€è®½åˆºæ£€æµ‹å’Œæƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å•æ¨¡æ€å»åç­–ç•¥ã€‚æœ€ç»ˆå®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡æ—¶å±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹(state-of-the-art models)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2509.15361v1",
      "published_date": "2025-09-18 19:01:11 UTC",
      "updated_date": "2025-09-18 19:01:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:13.386100+00:00"
    },
    {
      "arxiv_id": "2509.15336v1",
      "title": "Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†é©±åŠ¨å¹»è§‰ï¼šæµç¨‹å»ºæ¨¡å®è¯ç ”ç©¶",
      "authors": [
        "Humam Kourani",
        "Anton Antonov",
        "Alessandro Berti",
        "Wil M. P. van der Aalst"
      ],
      "abstract": "The utility of Large Language Models (LLMs) in analytical tasks is rooted in their vast pre-trained knowledge, which allows them to interpret ambiguous inputs and infer missing information. However, this same capability introduces a critical risk of what we term knowledge-driven hallucination: a phenomenon where the model's output contradicts explicit source evidence because it is overridden by the model's generalized internal knowledge. This paper investigates this phenomenon by evaluating LLMs on the task of automated process modeling, where the goal is to generate a formal business process model from a given source artifact. The domain of Business Process Management (BPM) provides an ideal context for this study, as many core business processes follow standardized patterns, making it likely that LLMs possess strong pre-trained schemas for them. We conduct a controlled experiment designed to create scenarios with deliberate conflict between provided evidence and the LLM's background knowledge. We use inputs describing both standard and deliberately atypical process structures to measure the LLM's fidelity to the provided evidence. Our work provides a methodology for assessing this critical reliability issue and raises awareness of the need for rigorous validation of AI-generated artifacts in any evidence-based domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­ä¸€ç§è¢«ç§°ä¸ºçŸ¥è¯†é©±åŠ¨çš„å¹»è§‰(Knowledge-driven hallucination)çš„ç°è±¡ï¼Œå³æ¨¡å‹çš„å†…éƒ¨æ³›åŒ–çŸ¥è¯†è¦†ç›–äº†æ˜¾å¼çš„æ¥æºè¯æ®å¹¶å¯¼è‡´è¾“å‡ºä¸å…¶çŸ›ç›¾ã€‚ä½œè€…ä»¥ä¸šåŠ¡æµç¨‹ç®¡ç†(BPM)é¢†åŸŸçš„è‡ªåŠ¨æµç¨‹å»ºæ¨¡(Automated process modeling)ä»»åŠ¡ä¸ºåˆ‡å…¥ç‚¹ï¼Œé€šè¿‡è®¾è®¡å—æ§å®éªŒæ¥è¯„ä¼°LLMåœ¨å¤„ç†ä¸èƒŒæ™¯çŸ¥è¯†å†²çªçš„ä¿¡æ¯æ—¶çš„è¡¨ç°ã€‚å®éªŒå¯¹æ¯”äº†æ ‡å‡†æµç¨‹ä¸æ•…æ„è®¾è®¡çš„éå…¸å‹æµç¨‹ç»“æ„ï¼Œæ—¨åœ¨ç²¾ç¡®è¡¡é‡æ¨¡å‹å¯¹æ‰€æä¾›è¯æ®çš„å¿ è¯šåº¦(Fidelity)ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†LLMsåœ¨è¯æ®é©±åŠ¨ä»»åŠ¡ä¸­å­˜åœ¨çš„å…³é”®å¯é æ€§é£é™©ï¼Œå¹¶ä¸ºè¯„ä¼°æ­¤ç±»é—®é¢˜æä¾›äº†ä¸€å¥—ç³»ç»Ÿæ€§çš„æ–¹æ³•è®ºã€‚è¯¥é¡¹å·¥ä½œæå‡äº†å­¦ç•Œå¯¹äººå·¥æ™ºèƒ½ç”Ÿæˆäº§ç‰©åœ¨ä¸“ä¸šé¢†åŸŸå†…è¿›è¡Œä¸¥è°¨éªŒè¯å¿…è¦æ€§çš„è®¤è¯†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "The Version of Record of this contribution will be published in the proceedings of the 2nd International Workshop on Generative AI for Process Mining (GenAI4PM 2025). This preprint has not undergone peer review or any post-submission improvements or corrections",
      "pdf_url": "https://arxiv.org/pdf/2509.15336v1",
      "published_date": "2025-09-18 18:27:30 UTC",
      "updated_date": "2025-09-18 18:27:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:00.478075+00:00"
    },
    {
      "arxiv_id": "2509.15333v1",
      "title": "Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception",
      "title_zh": "æ¨¡æ‹Ÿç±»äººè‡ªé€‚åº”è§†è§‰ä»¥å®ç°é«˜æ•ˆçµæ´»çš„æœºå™¨è§†è§‰æ„ŸçŸ¥",
      "authors": [
        "Yulin Wang",
        "Yang Yue",
        "Yang Yue",
        "Huanqian Wang",
        "Haojun Jiang",
        "Yizeng Han",
        "Zanlin Ni",
        "Yifan Pu",
        "Minglei Shi",
        "Rui Lu",
        "Qisen Yang",
        "Andrew Zhao",
        "Zhuofan Xia",
        "Shiji Song",
        "Gao Huang"
      ],
      "abstract": "Human vision is highly adaptive, efficiently sampling intricate environments by sequentially fixating on task-relevant regions. In contrast, prevailing machine vision models passively process entire scenes at once, resulting in excessive resource demands scaling with spatial-temporal input resolution and model size, yielding critical limitations impeding both future advancements and real-world application. Here we introduce AdaptiveNN, a general framework aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential decision-making process, progressively identifying and attending to regions pertinent to the task, incrementally combining information across fixations, and actively concluding observation when sufficient. We establish a theory integrating representation learning with self-rewarding reinforcement learning, enabling end-to-end training of the non-differentiable AdaptiveNN without additional supervision on fixation locations. We assess AdaptiveNN on 17 benchmarks spanning 9 tasks, including large-scale visual recognition, fine-grained discrimination, visual search, processing images from real driving and medical scenarios, language-driven embodied AI, and side-by-side comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction without sacrificing accuracy, flexibly adapts to varying task demands and resource budgets without retraining, and provides enhanced interpretability via its fixation patterns, demonstrating a promising avenue toward efficient, flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits closely human-like perceptual behaviors in many cases, revealing its potential as a valuable tool for investigating visual cognition. Code is available at https://github.com/LeapLabTHU/AdaptiveNN.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AdaptiveNNï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†æœºå™¨è§†è§‰ä»â€œè¢«åŠ¨â€å¤„ç†æ¨¡å¼è½¬å‘â€œä¸»åŠ¨ã€è‡ªé€‚åº”â€è§†è§‰æ¨¡å¼çš„é€šç”¨æ¡†æ¶ï¼Œä»¥æ¨¡æ‹Ÿäººç±»è§†è§‰é«˜æ•ˆé‡‡æ ·ç¯å¢ƒçš„èƒ½åŠ›ã€‚AdaptiveNN å°†è§†è§‰æ„ŸçŸ¥å»ºæ¨¡ä¸ºç”±ç²—åˆ°ç²¾çš„é¡ºåºå†³ç­–è¿‡ç¨‹ï¼Œèƒ½å¤Ÿé€æ­¥è¯†åˆ«å¹¶å…³æ³¨ä¸ä»»åŠ¡ç›¸å…³çš„åŒºåŸŸï¼Œå¹¶åœ¨è·å¾—è¶³å¤Ÿä¿¡æ¯æ—¶ä¸»åŠ¨ç»“æŸè§‚å¯Ÿã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¡¨ç¤ºå­¦ä¹  (representation learning) ä¸è‡ªæˆ‘å¥–åŠ±å¼ºåŒ–å­¦ä¹  (self-rewarding reinforcement learning)ï¼Œå®ç°äº†åœ¨æ— éœ€é¢å¤–æ³¨è§†ç‚¹æ ‡æ³¨çš„æƒ…å†µä¸‹è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚ç ”ç©¶å›¢é˜Ÿåœ¨åŒ…æ‹¬å¤§è§„æ¨¡è§†è§‰è¯†åˆ«ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å½±åƒåŠå…·èº«æ™ºèƒ½ (embodied AI) åœ¨å†…çš„ 9 é¡¹ä»»åŠ¡å’Œ 17 ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹ AdaptiveNN è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdaptiveNN åœ¨ä¸ç‰ºç‰²å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾ 28 å€çš„æ¨ç†æˆæœ¬é™ä½ï¼Œå¹¶èƒ½çµæ´»é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚å’Œèµ„æºé¢„ç®—ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ³¨è§†æ¨¡å¼ (fixation patterns) å¢å¼ºäº†å¯è§£é‡Šæ€§ï¼Œå¹¶å±•ç°å‡ºä¸äººç±»é«˜åº¦ç›¸ä¼¼çš„æ„ŸçŸ¥è¡Œä¸ºï¼Œä¸ºè§†è§‰è®¤çŸ¥ç ”ç©¶æä¾›äº†æ½œåœ¨çš„æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15333v1",
      "published_date": "2025-09-18 18:25:43 UTC",
      "updated_date": "2025-09-18 18:25:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:01.953485+00:00"
    },
    {
      "arxiv_id": "2509.18186v1",
      "title": "An Outcome-Based Educational Recommender System",
      "title_zh": "åŸºäºæˆæœå¯¼å‘çš„æ•™è‚²æ¨èç³»ç»Ÿ",
      "authors": [
        "Nursultan Askarbekuly",
        "Timur Fayzrakhmanov",
        "Sladjan BabarogiÄ‡",
        "Ivan LukoviÄ‡"
      ],
      "abstract": "Most educational recommender systems are tuned and judged on click- or rating-based relevance, leaving their true pedagogical impact unclear. We introduce OBER-an Outcome-Based Educational Recommender that embeds learning outcomes and assessment items directly into the data schema, so any algorithm can be evaluated on the mastery it fosters. OBER uses a minimalist entity-relation model, a log-driven mastery formula, and a plug-in architecture. Integrated into an e-learning system in non-formal domain, it was evaluated trough a two-week randomized split test with over 5 700 learners across three methods: fixed expert trajectory, collaborative filtering (CF), and knowledge-based (KB) filtering. CF maximized retention, but the fixed path achieved the highest mastery. Because OBER derives business, relevance, and learning metrics from the same logs, it lets practitioners weigh relevance and engagement against outcome mastery with no extra testing overhead. The framework is method-agnostic and readily extensible to future adaptive or context-aware recommenders.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OBER (Outcome-Based Educational Recommender)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ•™è‚²æ¨èç³»ç»Ÿä¸»è¦ä¾§é‡äºç‚¹å‡»ç‡æˆ–è¯„åˆ†ç›¸å…³æ€§è€Œå¿½è§†å®é™…æ•™å­¦å½±å“çš„é—®é¢˜ã€‚OBER é€šè¿‡å°†å­¦ä¹ æˆæœ (learning outcomes) å’Œè¯„ä¼°é¡¹ç›´æ¥åµŒå…¥æ•°æ®æ¨¡å¼ï¼Œç»“åˆæç®€å®ä½“å…³ç³»æ¨¡å‹å’Œæ—¥å¿—é©±åŠ¨çš„æŒæ¡åº¦å…¬å¼ (mastery formula)ï¼Œå®ç°äº†å¯¹å­¦ä¹ æŒæ¡ç¨‹åº¦çš„é‡åŒ–è¯„ä¼°ã€‚åœ¨ä¸€é¡¹æ¶‰åŠ 5700 å¤šåå­¦ä¹ è€…çš„éšæœºå¯¹ç…§å®éªŒä¸­ï¼Œç ”ç©¶è€…å¯¹æ¯”äº†ä¸“å®¶å›ºå®šè·¯å¾„ (fixed expert trajectory)ã€ååŒè¿‡æ»¤ (CF) å’ŒåŸºäºçŸ¥è¯†çš„è¿‡æ»¤ (KB) ä¸‰ç§æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒååŒè¿‡æ»¤åœ¨æå‡ç”¨æˆ·ç•™å­˜æ–¹é¢è¡¨ç°æœ€ä¼˜ï¼Œè€Œä¸“å®¶å›ºå®šè·¯å¾„åˆ™å®ç°äº†æœ€é«˜çš„çŸ¥è¯†æŒæ¡åº¦ã€‚ç”±äº OBER èƒ½å¤Ÿä»ç›¸åŒæ—¥å¿—ä¸­æå–ä¸šåŠ¡ã€ç›¸å…³æ€§å’Œå­¦ä¹ æŒ‡æ ‡ï¼Œå®ƒä½¿ä»ä¸šè€…èƒ½å¤Ÿåœ¨ä¸å¢åŠ é¢å¤–æµ‹è¯•å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæƒè¡¡å‚ä¸åº¦ä¸å­¦ä¹ æˆæœæŒæ¡åº¦ã€‚è¯¥æ¡†æ¶å…·æœ‰æ–¹æ³•æ— å…³æ€§ (method-agnostic)ï¼Œå¯è½»æ¾æ‰©å±•è‡³æœªæ¥çš„è‡ªé€‚åº”æˆ–ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨èç³»ç»Ÿã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18186v1",
      "published_date": "2025-09-18 18:18:03 UTC",
      "updated_date": "2025-09-18 18:18:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:04.862209+00:00"
    },
    {
      "arxiv_id": "2509.15217v1",
      "title": "Generalizable Geometric Image Caption Synthesis",
      "title_zh": "å¯æ³›åŒ–çš„å‡ ä½•å›¾åƒæè¿°åˆæˆ",
      "authors": [
        "Yue Xin",
        "Wenyuan Wang",
        "Rui Pan",
        "Ruida Wang",
        "Howard Meng",
        "Renjie Pi",
        "Shizhe Diao",
        "Tong Zhang"
      ],
      "abstract": "Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚å‡ ä½•æ¨ç†ä¸­å› é«˜è´¨é‡æ•°æ®åŒ®ä¹å’Œæ¨¡æ¿åŒ–æ–¹æ³•æ³›åŒ–æ€§å·®è€Œé¢ä¸´çš„ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning with Verifiable Rewards, RLVRï¼‰çš„æ•°æ®ç”Ÿæˆæµæ°´çº¿ã€‚è¯¥æµç¨‹é€šè¿‡RLVRä¼˜åŒ–ç”±50ç§åŸºç¡€å‡ ä½•å…³ç³»ç”Ÿæˆçš„å›¾åƒæè¿°ï¼ˆCaptionsï¼‰ï¼Œå¹¶åˆ©ç”¨æ•°å­¦è§£é¢˜ä»»åŠ¡çš„å¥–åŠ±ä¿¡å·å¼•å¯¼æ¨¡å‹æ•æ‰å‡ ä½•é€»è¾‘çš„å…³é”®ç‰¹å¾ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ä»…åœ¨å‡ ä½•é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨å¤„ç†MathVistaå’ŒMathVerseç­‰éå‡ ä½•è¾“å…¥çš„ç»Ÿè®¡ä¸ä»£æ•°ä»»åŠ¡æ—¶å‡†ç¡®ç‡ä¹Ÿæå‡äº†2.8%-4.8%ã€‚æ­¤å¤–ï¼Œåœ¨MMMUçš„è‰ºæœ¯ã€å·¥ç¨‹ç­‰è·¨å­¦ç§‘ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ä¹Ÿå¸¦æ¥äº†2.4%-3.9%çš„æ€§èƒ½å¢é•¿ï¼Œå……åˆ†å±•ç¤ºäº†å…¶åœ¨åˆ†å¸ƒå¤–åœºæ™¯ä¸‹å¢å¼ºé€šç”¨æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15217v1",
      "published_date": "2025-09-18 17:59:11 UTC",
      "updated_date": "2025-09-18 17:59:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:28.668408+00:00"
    },
    {
      "arxiv_id": "2509.15210v1",
      "title": "Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation",
      "title_zh": "é¢å‘é«˜ä¿çœŸæˆ¿é—´è„‰å†²å“åº”ç”Ÿæˆçš„æ˜¾å¼ä¸Šä¸‹æ–‡é©±åŠ¨ç¥ç»å£°å­¦å»ºæ¨¡",
      "authors": [
        "Chen Si",
        "Qianyi Wu",
        "Chaitanya Amballa",
        "Romit Roy Choudhury"
      ],
      "abstract": "Realistic sound simulation plays a critical role in many applications. A key element in sound simulation is the room impulse response (RIR), which characterizes how sound propagates from a source to a listener within a given space. Recent studies have applied neural implicit methods to learn RIR using context information collected from the environment, such as scene images. However, these approaches do not effectively leverage explicit geometric information from the environment. To further exploit the potential of neural implicit models with direct geometric features, we present Mesh-infused Neural Acoustic Field (MiNAF), which queries a rough room mesh at given locations and extracts distance distributions as an explicit representation of local context. Our approach demonstrates that incorporating explicit local geometric features can better guide the neural network in generating more accurate RIR predictions. Through comparisons with conventional and state-of-the-art baseline methods, we show that MiNAF performs competitively across various evaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets with limited training samples, demonstrating an advance in high-fidelity sound simulation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Mesh-infused Neural Acoustic Field (MiNAF)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç¥ç»éšå¼æ–¹æ³•åœ¨ç”Ÿæˆæˆ¿é—´è„‰å†²å“åº” (Room Impulse Response, RIR) æ—¶æœªèƒ½å……åˆ†åˆ©ç”¨ç¯å¢ƒæ˜¾å¼å‡ ä½•ä¿¡æ¯çš„é—®é¢˜ã€‚MiNAF é€šè¿‡åœ¨ç»™å®šä½ç½®æŸ¥è¯¢ç²—ç•¥çš„æˆ¿é—´ç½‘æ ¼ (Mesh)ï¼Œå¹¶æå–è·ç¦»åˆ†å¸ƒä½œä¸ºå±€éƒ¨ä¸Šä¸‹æ–‡çš„æ˜¾å¼è¡¨ç¤ºï¼Œä»è€Œå¢å¼ºæ¨¡å‹å¯¹ç©ºé—´å£°å­¦ç‰¹å¾çš„æ•æ‰èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼•å…¥æ˜¾å¼å±€éƒ¨å‡ ä½•ç‰¹å¾èƒ½æ›´æœ‰æ•ˆåœ°å¼•å¯¼ç¥ç»ç½‘ç»œç”Ÿæˆå‡†ç¡®çš„ RIR é¢„æµ‹ã€‚åœ¨ä¸ä¼ ç»ŸåŠæœ€å…ˆè¿›åŸºçº¿æ¨¡å‹çš„å¯¹æ¯”ä¸­ï¼ŒMiNAF åœ¨å¤šé¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æœ‰é™è®­ç»ƒæ ·æœ¬çš„æ•°æ®é›†ä¸‹ä¾ç„¶ä¿æŒç¨³å¥ï¼Œæ˜¾è‘—æ¨è¿›äº†é«˜ä¿çœŸå£°å­¦æ¨¡æ‹ŸæŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15210v1",
      "published_date": "2025-09-18 17:57:07 UTC",
      "updated_date": "2025-09-18 17:57:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:31.790055+00:00"
    },
    {
      "arxiv_id": "2509.15207v3",
      "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
      "title_zh": "FlowRLï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„å¥–åŠ±åˆ†å¸ƒåŒ¹é…",
      "authors": [
        "Xuekai Zhu",
        "Daixuan Cheng",
        "Dinghuai Zhang",
        "Hengli Li",
        "Kaiyan Zhang",
        "Che Jiang",
        "Youbang Sun",
        "Ermo Hua",
        "Yuxin Zuo",
        "Xingtai Lv",
        "Qizheng Zhang",
        "Lin Chen",
        "Fanghao Shao",
        "Bo Xue",
        "Yunchong Song",
        "Zhenjie Yang",
        "Ganqu Cui",
        "Ning Ding",
        "Jianfeng Gao",
        "Xiaodong Liu",
        "Bowen Zhou",
        "Hongyuan Mei",
        "Zhouhan Lin"
      ],
      "abstract": "We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FlowRLï¼Œä¸€ç§é€šè¿‡æµé‡å¹³è¡¡ï¼ˆflow balancingï¼‰åŒ¹é…å®Œæ•´å¥–åŠ±åˆ†å¸ƒè€Œéä»…ä»…æœ€å¤§åŒ–å¥–åŠ±çš„å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚ä¼ ç»Ÿçš„å¥–åŠ±æœ€å¤§åŒ–æ¨¡å‹ï¼ˆå¦‚ PPO å’Œ GRPOï¼‰å€¾å‘äºè¿‡åº¦ä¼˜åŒ–ä¸»æµå¥–åŠ±ä¿¡å·ï¼Œå¯¼è‡´å¿½ç•¥äº†å‡ºç°é¢‘ç‡è¾ƒä½ä½†åŒæ ·æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ï¼Œä»è€Œé™ä½äº†æ¨ç†çš„å¤šæ ·æ€§ã€‚FlowRL ä½¿ç”¨å¯å­¦ä¹ çš„é…åˆ†å‡½æ•°ï¼ˆpartition functionï¼‰å°†æ ‡é‡å¥–åŠ±è½¬åŒ–ä¸ºå½’ä¸€åŒ–çš„ç›®æ ‡åˆ†å¸ƒï¼Œå¹¶é€šè¿‡æœ€å°åŒ–ç­–ç•¥ä¸è¯¥åˆ†å¸ƒä¹‹é—´çš„é€† KL æ•£åº¦ï¼ˆreverse KL divergenceï¼‰æ¥å®ç°åˆ†å¸ƒåŒ¹é…ã€‚è¿™ç§æµé‡å¹³è¡¡çš„ä¼˜åŒ–æ–¹å¼ä¿ƒè¿›äº†æ¨¡å‹è¿›è¡Œæ›´å¤šæ ·åŒ–çš„æ¢ç´¢ï¼Œå¹¶ç”Ÿæˆæ›´å…·æ³›åŒ–æ€§çš„æ¨ç†è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowRL åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ¯” GRPO æå‡äº† 10.0%ï¼Œæ¯” PPO æå‡äº† 5.1%ï¼Œå¹¶åœ¨ä»£ç æ¨ç†ä»»åŠ¡ä¸­å±•ç°äº†æŒç»­çš„ä¸€è‡´æ€§ä¼˜åŠ¿ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†å¥–åŠ±åˆ†å¸ƒåŒ¹é…æ˜¯å®ç°å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­é«˜æ•ˆæ¢ç´¢å’Œå¤šæ ·åŒ–æ¨ç†çš„å…³é”®è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15207v3",
      "published_date": "2025-09-18 17:56:36 UTC",
      "updated_date": "2025-11-04 08:52:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:40.495592+00:00"
    },
    {
      "arxiv_id": "2509.15195v1",
      "title": "Orion: Fuzzing Workflow Automation",
      "title_zh": "Orionï¼šæ¨¡ç³Šæµ‹è¯•å·¥ä½œæµè‡ªåŠ¨åŒ–",
      "authors": [
        "Max Bazalii",
        "Marius Fleischer"
      ],
      "abstract": "Fuzz testing is one of the most effective techniques for finding software vulnerabilities. While modern fuzzers can generate inputs and monitor executions automatically, the overall workflow, from analyzing a codebase, to configuring harnesses, to triaging results, still requires substantial manual effort. Prior attempts focused on single stages such as harness synthesis or input minimization, leaving researchers to manually connect the pieces into a complete fuzzing campaign.\n  We introduce Orion, a framework that automates the the manual bottlenecks of fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns to scale to settings where human effort alone was impractical. Orion uses LLMs for code reasoning and semantic guidance, while relying on deterministic tools for verification, iterative refinement, and tasks that require precision. Across our benchmark suite, Orion reduces human effort by 46-204x depending on the workflow stage, and we demonstrate its effectiveness through the discovery of two previously unknown vulnerabilities in the widely used open-source clib library.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Orionï¼Œä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨åŒ–æ¨¡ç³Šæµ‹è¯•(Fuzz testing)å·¥ä½œæµçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°ä»£æ¨¡ç³Šæµ‹è¯•åœ¨ä»£ç åˆ†æã€Harnessé…ç½®å’Œç»“æœåˆ†ç±»(Triage)ç­‰é˜¶æ®µä»éœ€å¤§é‡äººå·¥å¹²é¢„çš„é—®é¢˜ã€‚Orioné€šè¿‡å°†å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†èƒ½åŠ›ä¸ä¼ ç»Ÿç¡®å®šæ€§å·¥å…·ç›¸ç»“åˆï¼Œå®ç°äº†å¯¹æ¨¡ç³Šæµ‹è¯•æ‰‹åŠ¨ç“¶é¢ˆçš„è‡ªåŠ¨åŒ–æ•´åˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨LLMsè¿›è¡Œä»£ç æ¨ç†å’Œè¯­ä¹‰å¼•å¯¼ï¼ŒåŒæ—¶ä¾é ç¡®å®šæ€§å·¥å…·è¿›è¡ŒéªŒè¯ã€è¿­ä»£ä¼˜åŒ–ä»¥åŠå¤„ç†éœ€è¦é«˜ç²¾åº¦çš„ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒOrionåœ¨ä¸åŒå·¥ä½œæµé˜¶æ®µå¯å°†äººå·¥æŠ•å…¥é™ä½46è‡³204å€ï¼Œä½¿å¤§è§„æ¨¡æ¨¡ç³Šæµ‹è¯•æ´»åŠ¨æˆä¸ºå¯èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥æ¡†æ¶åœ¨å¹¿æ³›ä½¿ç”¨çš„å¼€æºåº“clibä¸­å‘ç°äº†ä¸¤ä¸ªæ­¤å‰æœªçŸ¥çš„å®‰å…¨æ¼æ´ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨å‘ç°è½¯ä»¶æ¼æ´æ–¹é¢çš„å®ç”¨æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "comment": "11 pages, 3 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.15195v1",
      "published_date": "2025-09-18 17:52:06 UTC",
      "updated_date": "2025-09-18 17:52:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:42.484576+00:00"
    },
    {
      "arxiv_id": "2509.15193v1",
      "title": "TITAN: A Trajectory-Informed Technique for Adaptive Parameter Freezing in Large-Scale VQE",
      "title_zh": "TITANï¼šå¤§è§„æ¨¡ VQE ä¸­åŸºäºè½¨è¿¹ä¿¡æ¯çš„è‡ªé€‚åº”å‚æ•°å†»ç»“æŠ€æœ¯",
      "authors": [
        "Yifeng Peng",
        "Xinyi Li",
        "Samuel Yen-Chi Chen",
        "Kaining Zhang",
        "Zhiding Liang",
        "Ying Wang",
        "Yuxuan Du"
      ],
      "abstract": "Variational quantum Eigensolver (VQE) is a leading candidate for harnessing quantum computers to advance quantum chemistry and materials simulations, yet its training efficiency deteriorates rapidly for large Hamiltonians. Two issues underlie this bottleneck: (i) the no-cloning theorem imposes a linear growth in circuit evaluations with the number of parameters per gradient step; and (ii) deeper circuits encounter barren plateaus (BPs), leading to exponentially increasing measurement overheads. To address these challenges, here we propose a deep learning framework, dubbed Titan, which identifies and freezes inactive parameters of a given ansatze at initialization for a specific class of Hamiltonians, reducing the optimization overhead without sacrificing accuracy. The motivation of Titan starts with our empirical findings that a subset of parameters consistently has a negligible influence on training dynamics. Its design combines a theoretically grounded data construction strategy, ensuring each training example is informative and BP-resilient, with an adaptive neural architecture that generalizes across ansatze of varying sizes. Across benchmark transverse-field Ising models, Heisenberg models, and multiple molecule systems up to 30 qubits, Titan achieves up to 3 times faster convergence and 40% to 60% fewer circuit evaluations than state-of-the-art baselines, while matching or surpassing their estimation accuracy. By proactively trimming parameter space, Titan lowers hardware demands and offers a scalable path toward utilizing VQE to advance practical quantum chemistry and materials science.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TITANï¼Œä¸€ç§åŸºäºè½¨è¿¹ä¿¡æ¯çš„å¤§è§„æ¨¡å˜åˆ†é‡å­ç‰¹å¾å€¼æ±‚è§£å™¨(VQE)è‡ªé€‚åº”å‚æ•°å†»ç»“æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å¤§å‹å“ˆå¯†é¡¿é‡(Hamiltonians)è®­ç»ƒä¸­çº¿è·¯è¯„ä¼°å¼€é”€å¤§å’Œè´«ç˜ é«˜åŸ(Barren Plateaus)å¯¼è‡´æ•ˆç‡ä½ä¸‹çš„ç“¶é¢ˆã€‚è¯¥æ¡†æ¶é€šè¿‡æ·±åº¦å­¦ä¹ åœ¨åˆå§‹åŒ–é˜¶æ®µè¯†åˆ«å¹¶å†»ç»“å¯¹è®­ç»ƒåŠ¨åŠ›å­¦å½±å“å¾®å°çš„éæ´»è·ƒå‚æ•°ï¼Œç»“åˆäº†ç†è®ºæ‰å®çš„æ•°æ®æ„å»ºç­–ç•¥ä¸å¯è·¨å°ºåº¦æ¨å¹¿çš„è‡ªé€‚åº”ç¥ç»æ¶æ„ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„é«˜æ•ˆæ€§ä¸æŠ—è´«ç˜ é«˜åŸèƒ½åŠ›ã€‚åœ¨æ¨ªåœºä¼Šè¾›æ¨¡å‹(Ising models)ã€æµ·æ£®å ¡æ¨¡å‹(Heisenberg models)ä»¥åŠé«˜è¾¾30é‡å­ä½çš„å¤šç§åˆ†å­ç³»ç»Ÿæµ‹è¯•ä¸­ï¼ŒTITANå®ç°äº†æ¯”ç°æœ‰åŸºå‡†æ¨¡å‹å¿«3å€çš„æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶å‡å°‘äº†40%è‡³60%çš„é‡å­çº¿è·¯è¯„ä¼°æ¬¡æ•°ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ä¿æŒæˆ–æå‡ä¼°ç®—ç²¾åº¦çš„å‰æä¸‹ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†å¯¹ç¡¬ä»¶çš„éœ€æ±‚ï¼Œä¸ºåˆ©ç”¨VQEæ¨è¿›å®ç”¨é‡å­åŒ–å­¦å’Œææ–™ç§‘å­¦ç ”ç©¶æä¾›äº†ä¸€æ¡å…·å¤‡å¯æ‰©å±•æ€§çš„è·¯å¾„ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "Accepted by The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.15193v1",
      "published_date": "2025-09-18 17:50:02 UTC",
      "updated_date": "2025-09-18 17:50:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:44.093213+00:00"
    },
    {
      "arxiv_id": "2509.15188v3",
      "title": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning",
      "title_zh": "é€šè¿‡å·ç§¯è§£ç å’Œæ‹’ç»å¼å¾®è°ƒå®ç°å¿«é€Ÿæµç•…çš„æ‰©æ•£è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yeongbin Seo",
        "Dongha Lee",
        "Jaehyung Kim",
        "Jinyoung Yeo"
      ],
      "abstract": "Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the long decoding-window problem, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks (sacrificing bidirectionality), but we find that this also leads to time-interval expansion problem, sacrificing the speed. Therefore, semi-AR eliminates the main advantages of diffusion models. To overcome this, we propose Convolutional decoding (Conv), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Diffusion Language Modelsåœ¨å¹¶è¡Œç”Ÿæˆå¤šTokenæ—¶é¢ä¸´çš„é•¿è§£ç çª—å£(long decoding-window)é—®é¢˜è¿›è¡Œäº†æ”¹è¿›ï¼ŒæŒ‡å‡ºè¯¥é—®é¢˜ä¼šå¯¼è‡´è¿œç¦»ä¸Šä¸‹æ–‡çš„ç”Ÿæˆå†…å®¹å˜å¾—æ— å…³æˆ–é‡å¤ã€‚ä¸ºäº†å…‹æœç°æœ‰Semi-Autoregressiveæ–¹æ¡ˆåœ¨æ¨ç†é€Ÿåº¦å’ŒåŒå‘æ€§ä¸Šçš„å±€é™ï¼Œä½œè€…æå‡ºäº†Convolutional decoding (Conv)æ–¹æ³•ï¼Œé€šè¿‡å½’ä¸€åŒ–æ‰‹æ®µåœ¨ä¸è¿›è¡Œç¡¬åˆ†å‰²çš„æƒ…å†µä¸‹ç¼©å°è§£ç çª—å£ï¼Œå¢å¼ºäº†ç”Ÿæˆçš„æµç•…æ€§ä¸çµæ´»æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†Rejecting Rule-based Fine-Tuning (R2FT)äº‹åè®­ç»ƒæ–¹æ¡ˆï¼Œä»¥ä¼˜åŒ–è¿œç¦»ä¸Šä¸‹æ–‡ä½ç½®çš„Tokenå¯¹é½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨AlpacaEvalç­‰åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†Diffusion LMåŸºçº¿çš„é¢†å…ˆæ°´å¹³ï¼Œå¹¶åœ¨æ˜¾è‘—é™ä½æ¨ç†æ­¥æ•°çš„åŒæ—¶æå‡äº†ç”Ÿæˆè´¨é‡ã€‚è¯¥æ¡†æ¶æˆåŠŸè§£å†³äº†æ‰©æ•£æ¨¡å‹åœ¨æ¨ç†é€Ÿåº¦ä¸æ–‡æœ¬è¿è´¯æ€§ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ï¼Œä¸ºé«˜æ•ˆå¹¶è¡Œè¯­è¨€å»ºæ¨¡æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2025 spotlight",
      "pdf_url": "https://arxiv.org/pdf/2509.15188v3",
      "published_date": "2025-09-18 17:48:21 UTC",
      "updated_date": "2025-10-24 14:56:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:59.283886+00:00"
    },
    {
      "arxiv_id": "2509.15174v2",
      "title": "SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models",
      "title_zh": "SMARTERï¼šåŸºäºè‡ªå¢å¼ºå¤§è¯­è¨€æ¨¡å‹æå‡å¯è§£é‡Šæ€§æ¯’æ€§æ£€æµ‹çš„æ•°æ®é«˜æ•ˆæ¡†æ¶",
      "authors": [
        "Huy Nghiem",
        "Advik Sachdeva",
        "Hal DaumÃ©"
      ],
      "abstract": "WARNING: This paper contains examples of offensive materials. To address the proliferation of toxic content on social media, we introduce SMARTER, we introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate -- demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs' self-improving capabilities for both classification and explanation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SMARTERï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆåˆ©ç”¨æ•°æ®çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æå‡å…·æœ‰è§£é‡Šæ€§çš„æœ‰å®³å†…å®¹æ£€æµ‹ï¼ˆToxicity Detectionï¼‰ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMsè‡ªèº«ç”Ÿæˆçš„åˆæˆè§£é‡Šï¼Œé€šè¿‡åå¥½ä¼˜åŒ–ï¼ˆPreference Optimizationï¼‰åœ¨æå°‘äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹å¯¹é½ã€‚ç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡è·¨æ¨¡å‹è®­ç»ƒï¼ˆCross-model Trainingï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–è§£é‡Šè´¨é‡ï¼Œä½¿å‚æ•°é‡è¾ƒå°çš„æ¨¡å‹åœ¨é£æ ¼å’Œè¯­ä¹‰ä¸Šä¸å¼ºæ¨¡å‹è¾¾æˆä¸€è‡´ã€‚åœ¨HateXplainã€Latent Hateå’ŒImplicit Hateä¸‰ä¸ªåŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSMARTERåœ¨ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®çš„å‰æä¸‹ï¼Œæ¯”ä¼ ç»Ÿçš„Few-shotåŸºçº¿æ¨¡å‹åœ¨Macro-F1æŒ‡æ ‡ä¸Šæå‡äº†é«˜è¾¾13.5%ã€‚è¯¥ç ”ç©¶å……åˆ†æŒ–æ˜äº†LLMsåœ¨åˆ†ç±»ä¸è§£é‡Šæ–¹é¢çš„è‡ªæˆ‘æ”¹è¿›ï¼ˆSelf-improvingï¼‰èƒ½åŠ›ï¼Œä¸ºä½èµ„æºç¯å¢ƒä¸‹çš„å¯æ‰©å±•å†…å®¹å®¡æ ¸æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NLP, Hate speech detection, explanation, LLM. Version 2: updated experiments and analysis",
      "pdf_url": "https://arxiv.org/pdf/2509.15174v2",
      "published_date": "2025-09-18 17:30:36 UTC",
      "updated_date": "2025-10-08 03:00:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:20:55.602538+00:00"
    },
    {
      "arxiv_id": "2509.15172v2",
      "title": "Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment",
      "title_zh": "è¯­è¨€æ¨¡å‹è‡ªä¸€è‡´æ€§çš„å†…åŒ–ï¼šå¤šæ™ºèƒ½ä½“å…±è¯†å¯¹é½",
      "authors": [
        "Ankur Samanta",
        "Akshayaa Magesh",
        "Youliang Yu",
        "Runzhe Wu",
        "Ayush Jain",
        "Daniel Jiang",
        "Boris Vidolov",
        "Paul Sajda",
        "Yonathan Efroni",
        "Kaveh Hassani"
      ],
      "abstract": "Language Models (LMs) are inconsistent reasoners, often generating contradictory responses to identical prompts. While inference-time methods can mitigate these inconsistencies, they fail to address the core problem: LMs struggle to reliably select reasoning pathways leading to consistent outcomes under exploratory sampling. To address this, we formalize self-consistency as an intrinsic property of well-aligned reasoning models and introduce Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that post-trains models to favor reasoning trajectories aligned with their internal consensus using majority/minority outcomes from multi-agent debate. These trajectories emerge from deliberative exchanges where agents ground reasoning in peer arguments, not just aggregation of independent attempts, creating richer consensus signals than single-round majority voting. MACA enables agents to teach themselves to be more decisive and concise, and better leverage peer insights in multi-agent settings without external supervision, driving substantial improvements across self-consistency (+27.6% on GSM8K), single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4% Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA). These findings, coupled with strong generalization to unseen benchmarks (+16.3% on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more reliably unlocks latent reasoning potential of language models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­è¨€æ¨¡å‹(Language Models)åœ¨æ¨ç†ä¸­è¡¨ç°å‡ºçš„ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œæå‡ºå°† Self-Consistency è§†ä¸ºå¯¹é½æ¨ç†æ¨¡å‹çš„å†…åœ¨å±æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†åä¸º Multi-Agent Consensus Alignment (MACA) çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®º(Multi-Agent Debate)äº§ç”Ÿçš„å…±è¯†ä¿¡å·ï¼Œè®­ç»ƒæ¨¡å‹å€¾å‘äºä¸å…¶å†…éƒ¨å…±è¯†ä¸€è‡´çš„æ¨ç†è½¨è¿¹ã€‚ä¸ä¼ ç»Ÿçš„ç‹¬ç«‹å°è¯•èšåˆä¸åŒï¼ŒMACA è®©æ™ºèƒ½ä½“åœ¨å®¡æ…äº¤æµä¸­ç»“åˆåŒä¼´è®ºç‚¹è¿›è¡Œæ¨ç†ï¼Œä»è€Œåœ¨æ— éœ€å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹å®ç°è‡ªä¸»å­¦ä¹ å¹¶ä¼˜åŒ–å†³ç­–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMACA åœ¨ GSM8K ä¸Šçš„ Self-Consistency æå‡äº† 27.6%ï¼Œåœ¨ MATH ä¸Šçš„å•ä»£ç†æ¨ç†èƒ½åŠ›æå‡äº† 23.7%ï¼Œå¹¶åœ¨å¤šæ™ºèƒ½ä½“é›†æˆå†³ç­–æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ GPQA å’Œ CommonsenseQA ç­‰æœªè§è¿‡çš„åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå±•ç°å‡ºå¼ºåŠ²çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†é€šè¿‡ç¨³å¥çš„è‡ªæˆ‘å¯¹é½(Self-Alignment)èƒ½æ›´å¯é åœ°é‡Šæ”¾è¯­è¨€æ¨¡å‹çš„æ½œåœ¨æ¨ç†æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15172v2",
      "published_date": "2025-09-18 17:27:28 UTC",
      "updated_date": "2025-09-30 19:57:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:21:10.085090+00:00"
    },
    {
      "arxiv_id": "2509.15292v1",
      "title": "An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature",
      "title_zh": "ä¸€ç§äººå·¥æ™ºèƒ½é©±åŠ¨çš„åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„å¿«é€Ÿæ–‡çŒ®è°ƒç ”æµæ°´çº¿",
      "authors": [
        "Abhiyan Dhakal",
        "Kausik Paudel",
        "Sanjog Sigdel"
      ],
      "abstract": "We propose an automated pipeline for performing literature reviews using semantic similarity. Unlike traditional systematic review systems or optimization based methods, this work emphasizes minimal overhead and high relevance by using transformer based embeddings and cosine similarity. By providing a paper title and abstract, it generates relevant keywords, fetches relevant papers from open access repository, and ranks them based on their semantic closeness to the input. Three embedding models were evaluated. A statistical thresholding approach is then applied to filter relevant papers, enabling an effective literature review pipeline. Despite the absence of heuristic feedback or ground truth relevance labels, the proposed system shows promise as a scalable and practical tool for preliminary research and exploratory analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ (semantic similarity) çš„è‡ªåŠ¨åŒ–æ–‡çŒ®ç»¼è¿°å·¥ä½œæµï¼Œæ—¨åœ¨åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯å®ç°å¿«é€Ÿçš„æ–‡çŒ®è°ƒç ”ã€‚è¯¥æ–¹æ¡ˆé‡‡ç”¨åŸºäº Transformer çš„åµŒå…¥æ¨¡å‹ (transformer-based embeddings) å’Œä½™å¼¦ç›¸ä¼¼åº¦ (cosine similarity)ï¼Œé€šè¿‡è¾“å…¥è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦è‡ªåŠ¨ç”Ÿæˆå…³é”®è¯ï¼Œå¹¶ä»å¼€æ”¾è·å–èµ„æºåº“ä¸­æ£€ç´¢å¹¶æ’åç›¸å…³æ–‡çŒ®ã€‚ç ”ç©¶è¿‡ç¨‹ä¸­è¯„ä¼°äº†ä¸‰ç§åµŒå…¥æ¨¡å‹ï¼Œå¹¶åº”ç”¨ç»Ÿè®¡é˜ˆå€¼æ–¹æ³• (statistical thresholding) è¿‡æ»¤æ— å…³è®ºæ–‡ï¼Œä»è€Œæ„å»ºèµ·é«˜æ•ˆçš„æ–‡çŒ®è¯„å®¡æµç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨æ— éœ€äººå·¥åé¦ˆæˆ–çœŸå€¼æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½ä¸ºåˆæ­¥ç ”ç©¶å’Œæ¢ç´¢æ€§åˆ†ææä¾›å…·å¤‡å¯æ‰©å±•æ€§ä¸”å®ç”¨çš„æ”¯æŒå·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 6 figures, 1 table, National Conference on Computer Innovations",
      "pdf_url": "https://arxiv.org/pdf/2509.15292v1",
      "published_date": "2025-09-18 17:24:47 UTC",
      "updated_date": "2025-09-18 17:24:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:21:05.293226+00:00"
    },
    {
      "arxiv_id": "2509.15291v1",
      "title": "The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI",
      "title_zh": "å¼ºåŒ–å­¦ä¹ ä¸äººå·¥æ™ºèƒ½åœ¨äº¤é€šç½‘ç»œä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜",
      "authors": [
        "Federico Taschin",
        "Abderrahmane Lazaraq",
        "Ozan K. Tonguz",
        "Inci Ozgunes"
      ],
      "abstract": "The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart transportation networks has increased significantly in the last few years. Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to be a very promising approach by several authors. However, a problem with using Reinforcement Learning in Traffic Signal Control is the reliability of the trained RL agents due to the dynamically changing distribution of the input data with respect to the distribution of the data used for training. This presents a major challenge and a reliability problem for the trained network of AI agents and could have very undesirable and even detrimental consequences if a suitable solution is not found. Several researchers have tried to address this problem using different approaches. In particular, Meta Reinforcement Learning (Meta RL) promises to be an effective solution. In this paper, we evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and show that, while under certain conditions MetaLight can indeed lead to reasonably good results, under some other conditions it might not perform well (with errors of up to 22%), suggesting that Meta RL schemes are often not robust enough and can even pose major reliability problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Reinforcement Learning (RL) åœ¨æ™ºèƒ½äº¤é€šä¿¡å·æ§åˆ¶ (Traffic Signal Control) ä¸­çš„åº”ç”¨ï¼Œå¹¶é‡ç‚¹åˆ†æäº†ç”±æ•°æ®åŠ¨æ€å˜åŒ–å¼•èµ·çš„åˆ†å¸ƒåç§» (Distribution Shift) æ‰€å¯¼è‡´çš„æ™ºèƒ½ä½“å¯é æ€§é—®é¢˜ã€‚è™½ç„¶ Meta Reinforcement Learning (Meta RL) è¢«è§†ä¸ºè§£å†³è¯¥æŒ‘æˆ˜çš„æœ‰æ•ˆé€”å¾„ï¼Œä½†æœ¬æ–‡é€šè¿‡å¯¹å…ˆè¿›æ¨¡å‹ MetaLight çš„æ·±å…¥è¯„ä¼°å‘ç°ï¼Œè¯¥ç±»æ–¹æ³•åœ¨ä¸åŒç¯å¢ƒä¸‹çš„è¡¨ç°å¹¶ä¸ç¨³å®šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMetaLight åœ¨æŸäº›æ¡ä»¶ä¸‹è¯¯å·®ç‡é«˜è¾¾ 22%ï¼Œè¡¨æ˜ç°æœ‰çš„ Meta RL æ–¹æ¡ˆåœ¨åº”å¯¹åˆ†å¸ƒåç§»æ—¶ä»ç¼ºä¹è¶³å¤Ÿçš„é²æ£’æ€§ (Robustness)ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰ AI æ™ºèƒ½ä½“ç½‘ç»œåœ¨çœŸå®äº¤é€šåœºæ™¯ä¸­å¯èƒ½é¢ä¸´çš„ä¸¥é‡å¯é æ€§é£é™©ï¼Œå¼ºè°ƒäº†å¼€å‘æ›´å…·é€‚åº”æ€§çš„ç®—æ³•çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15291v1",
      "published_date": "2025-09-18 17:24:08 UTC",
      "updated_date": "2025-09-18 17:24:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:21:11.687082+00:00"
    },
    {
      "arxiv_id": "2509.15170v2",
      "title": "Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting",
      "title_zh": "LoRa å°„é¢‘æŒ‡çº¹è¯†åˆ«æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ°´å°ä¸å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Aarushi Mahajan",
        "Wayne Burleson"
      ],
      "abstract": "Radio frequency fingerprint identification (RFFI) distinguishes wireless devices by the small variations in their analog circuits, avoiding heavy cryptographic authentication. While deep learning on spectrograms improves accuracy, models remain vulnerable to copying, tampering, and evasion. We present a stronger RFFI system combining watermarking for ownership proof and anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel spectrograms, we embed three watermarks: a simple trigger, an adversarially trained trigger robust to noise and filtering, and a hidden gradient/weight signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler (KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset, our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC, offering verifiable, tamper-resistant authentication.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°„é¢‘æŒ‡çº¹è¯†åˆ«(RFFI)æ¨¡å‹æ˜“å—å¤åˆ¶ã€ç¯¡æ”¹å’Œè§„é¿æ”»å‡»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ•°å­—æ°´å°(Watermarking)ä¸å¼‚å¸¸æ£€æµ‹(Anomaly Detection)çš„å¢å¼ºå‹å®‰å…¨ç³»ç»Ÿã€‚ç ”ç©¶è€…åœ¨å¤„ç†å¯¹æ•°æ¢…å°”è°±å›¾(log-Mel spectrograms)çš„ResNet-34æ¨¡å‹ä¸­åµŒå…¥äº†åŒ…æ‹¬å¯¹æŠ—è®­ç»ƒè§¦å‘å™¨å’Œéšè—æ¢¯åº¦ç­¾ååœ¨å†…çš„ä¸‰ç§æ°´å°ï¼Œç”¨äºæ‰€æœ‰æƒè¯æ˜ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥å¸¦æœ‰KLçƒ­èº«(KL warm-up)å’Œè‡ªç”±ä½(free-bits)æŠ€æœ¯çš„å·ç§¯å˜åˆ†è‡ªç¼–ç å™¨(VAE)æ¥è¯†åˆ«åˆ†å¸ƒå¤–(OOD)çš„æ¶æ„æŸ¥è¯¢ã€‚åœ¨LoRaæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ä¿æŒ94.6%è¯†åˆ«å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå®ç°äº†98%çš„æ°´å°æˆåŠŸç‡å’Œ0.94çš„AUROCå€¼ã€‚è¯¥æ–¹æ¡ˆä¸ºç‰©è”ç½‘è®¾å¤‡æä¾›äº†ä¸€ç§å¯éªŒè¯ä¸”æŠ—ç¯¡æ”¹çš„èº«ä»½éªŒè¯æœºåˆ¶ï¼Œæ˜¾è‘—å¢å¼ºäº†ç‰©ç†å±‚å®‰å…¨åœ¨æœºå™¨å­¦ä¹ åº”ç”¨ä¸­çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.CR",
      "comment": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)",
      "pdf_url": "https://arxiv.org/pdf/2509.15170v2",
      "published_date": "2025-09-18 17:21:33 UTC",
      "updated_date": "2025-09-19 01:16:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:21:34.583722+00:00"
    },
    {
      "arxiv_id": "2509.15167v1",
      "title": "Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model",
      "title_zh": "åŸºäº2Dè‡ªç„¶å›¾åƒé¢„è®­ç»ƒæ¨¡å‹çš„åŠç›‘ç£3DåŒ»å­¦åˆ†å‰²",
      "authors": [
        "Pak-Hei Yeung",
        "Jayroop Ramesh",
        "Pengfei Lyu",
        "Ana Namburete",
        "Jagath Rajapakse"
      ],
      "abstract": "This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at https://github.com/pakheiyeung/M-N.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†é¢„è®­ç»ƒäº 2D Natural Images çš„é€šç”¨è§†è§‰æ¨¡å‹çŸ¥è¯†è¿ç§»åˆ° 3D Medical Image Segmentation ä»»åŠ¡ä¸­ï¼Œä»¥è§£å†³åŒ»ç–—å½±åƒæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸º M&N çš„ Model-agnostic æ¡†æ¶ï¼Œé€šè¿‡åœ¨ 2D Pretrained Model ä¸ä»é›¶å¼€å§‹è®­ç»ƒçš„ 3D Segmentation Model ä¹‹é—´è¿›è¡Œé€æ­¥çŸ¥è¯†è’¸é¦æ¥å®ç°æ€§èƒ½æå‡ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è¿­ä»£ååŒè®­ç»ƒ (Co-training) ç­–ç•¥ï¼Œåˆ©ç”¨æ¨¡å‹é—´äº’ç”Ÿæˆçš„ Pseudo-masks è¿›è¡Œå­¦ä¹ ï¼Œå¹¶å¼•å…¥äº† Learning Rate Guided Sampling æœºåˆ¶æ¥åŠ¨æ€è°ƒæ•´æœ‰æ ‡æ³¨ä¸æ— æ ‡æ³¨æ•°æ®çš„é‡‡æ ·æ¯”ä¾‹ï¼Œä»è€Œé™ä½é”™è¯¯ä¼ªæ ‡ç­¾å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM&N åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº† State-of-the-art æ€§èƒ½ï¼Œä¼˜äº 13 ç§ç°æœ‰çš„ Semi-supervised Segmentation æ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ¡†æ¶å…·æœ‰è‰¯å¥½çš„æ¶æ„å…¼å®¹æ€§ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆä¸åŒçš„æ¨¡å‹æ¶æ„ï¼Œå…·å¤‡æå¼ºçš„é€‚åº”æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Machine Learning in Medical Imaging (MLMI) 2025 Oral",
      "pdf_url": "https://arxiv.org/pdf/2509.15167v1",
      "published_date": "2025-09-18 17:17:52 UTC",
      "updated_date": "2025-09-18 17:17:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:21:36.289785+00:00"
    },
    {
      "arxiv_id": "2509.16277v1",
      "title": "Stabilizing Information Flow Entropy: Regularization for Safe and Interpretable Autonomous Driving Perception",
      "title_zh": "ç¨³å®šä¿¡æ¯æµç†µï¼šé¢å‘å®‰å…¨ä¸”å¯è§£é‡Šè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥çš„æ­£åˆ™åŒ–",
      "authors": [
        "Haobo Yang",
        "Shiyan Zhang",
        "Zhuoyi Yang",
        "Jilong Guo",
        "Jun Yang",
        "Xinyu Zhang"
      ],
      "abstract": "Deep perception networks in autonomous driving traditionally rely on data-intensive training regimes and post-hoc anomaly detection, often disregarding fundamental information-theoretic constraints governing stable information processing. We reconceptualize deep neural encoders as hierarchical communication chains that incrementally compress raw sensory inputs into task-relevant latent features. Within this framework, we establish two theoretically justified design principles for robust perception: (D1) smooth variation of mutual information between consecutive layers, and (D2) monotonic decay of latent entropy with network depth. Our analysis shows that, under realistic architectural assumptions, particularly blocks comprising repeated layers of similar capacity, enforcing smooth information flow (D1) naturally encourages entropy decay (D2), thus ensuring stable compression. Guided by these insights, we propose Eloss, a novel entropy-based regularizer designed as a lightweight, plug-and-play training objective. Rather than marginal accuracy improvements, this approach represents a conceptual shift: it unifies information-theoretic stability with standard perception tasks, enabling explicit, principled detection of anomalous sensor inputs through entropy deviations. Experimental validation on large-scale 3D object detection benchmarks (KITTI and nuScenes) demonstrates that incorporating Eloss consistently achieves competitive or improved accuracy while dramatically enhancing sensitivity to anomalies, amplifying distribution-shift signals by up to two orders of magnitude. This stable information-compression perspective not only improves interpretability but also establishes a solid theoretical foundation for safer, more robust autonomous driving perception systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç½‘ç»œä¸­ç¼ºä¹ä¿¡æ¯è®ºçº¦æŸçš„é—®é¢˜ï¼Œæå‡ºå°†æ·±åº¦ç¥ç»ç¼–ç å™¨é‡æ–°æ¦‚å¿µåŒ–ä¸ºåˆ†å±‚é€šä¿¡é“¾ï¼Œå¹¶ç¡®ç«‹äº†å±‚é—´äº’ä¿¡æ¯ï¼ˆMutual Informationï¼‰å¹³æ»‘å˜åŒ–åŠæ½œåœ¨ç†µï¼ˆLatent Entropyï¼‰éšç½‘ç»œæ·±åº¦å•è°ƒè¡°å‡çš„ä¸¤å¤§ç†è®ºè®¾è®¡åŸåˆ™ã€‚åŸºäºè¿™äº›è§è§£ï¼Œä½œè€…æå‡ºäº†Elossï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§ä¸”å³æ’å³ç”¨çš„åŸºäºç†µçš„æ­£åˆ™åŒ–å™¨ï¼ˆRegularizerï¼‰ï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€ä¿¡æ¯è®ºç¨³å®šæ€§ä¸æ ‡å‡†æ„ŸçŸ¥ä»»åŠ¡æ¥å®ç°å¯¹å¼‚å¸¸ä¼ æ„Ÿå™¨è¾“å…¥çš„åŸç†åŒ–æ£€æµ‹ã€‚åœ¨KITTIå’ŒnuSceneså¤§å‹3Dç›®æ ‡æ£€æµ‹åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŠ å…¥Elossèƒ½åœ¨ä¿æŒæˆ–æé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºç³»ç»Ÿå¯¹å¼‚å¸¸è¾“å…¥çš„æ•æ„Ÿåº¦ï¼Œå°†åˆ†å¸ƒåç§»ï¼ˆDistribution-Shiftï¼‰ä¿¡å·æ”¾å¤§è¾¾ä¸¤ä¸ªæ•°é‡çº§ã€‚è¿™ç§ç¨³å®šä¿¡æ¯å‹ç¼©çš„è§†è§’ä¸ä»…æå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼ˆInterpretabilityï¼‰ï¼Œè¿˜ä¸ºæ„å»ºæ›´å®‰å…¨ã€æ›´é²æ£’çš„è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿå¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16277v1",
      "published_date": "2025-09-18 17:01:27 UTC",
      "updated_date": "2025-09-18 17:01:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:21:41.292031+00:00"
    },
    {
      "arxiv_id": "2509.15156v1",
      "title": "Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models",
      "title_zh": "åˆ©ç”¨å‡ ä½•è§†é”™è§‰ä½œä¸ºè§†è§‰æ¨¡å‹çš„æ„ŸçŸ¥å½’çº³åç½®",
      "authors": [
        "Haobo Yang",
        "Minghao Guo",
        "Dequan Yang",
        "Wenyu Wang"
      ],
      "abstract": "Contemporary deep learning models have achieved impressive performance in image classification by primarily leveraging statistical regularities within large datasets, but they rarely incorporate structured insights drawn directly from perceptual psychology. To explore the potential of perceptually motivated inductive biases, we propose integrating classic geometric visual illusions well-studied phenomena from human perception into standard image-classification training pipelines. Specifically, we introduce a synthetic, parametric geometric-illusion dataset and evaluate three multi-source learning strategies that combine illusion recognition tasks with ImageNet classification objectives. Our experiments reveal two key conceptual insights: (i) incorporating geometric illusions as auxiliary supervision systematically improves generalization, especially in visually challenging cases involving intricate contours and fine textures; and (ii) perceptually driven inductive biases, even when derived from synthetic stimuli traditionally considered unrelated to natural image recognition, can enhance the structural sensitivity of both CNN and transformer-based architectures. These results demonstrate a novel integration of perceptual science and machine learning and suggest new directions for embedding perceptual priors into vision model design.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†æ„ŸçŸ¥å¿ƒç†å­¦ä¸­çš„ç»å…¸å‡ ä½•è§†é”™è§‰(geometric visual illusions)ä½œä¸ºæ„ŸçŸ¥å½’çº³åç½®(perceptual inductive biases)æ•´åˆåˆ°æ ‡å‡†å›¾åƒåˆ†ç±»è®­ç»ƒæµç¨‹ä¸­ï¼Œæ—¨åœ¨æå‡è§†è§‰æ¨¡å‹çš„ç»“æ„ç†è§£èƒ½åŠ›ã€‚ä½œè€…å¼•å…¥äº†ä¸€ä¸ªåˆæˆçš„å‚æ•°åŒ–å‡ ä½•è§†é”™è§‰æ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†ä¸‰ç§å°†è§†é”™è§‰è¯†åˆ«ä»»åŠ¡ä¸ ImageNet åˆ†ç±»ç›®æ ‡ç›¸ç»“åˆçš„å¤šæºå­¦ä¹ ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼•å…¥å‡ ä½•è§†é”™è§‰ä½œä¸ºè¾…åŠ©ç›‘ç£èƒ½ç³»ç»Ÿæ€§åœ°æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤æ‚è½®å»“å’Œç²¾ç»†çº¹ç†çš„æŒ‘æˆ˜æ€§è§†è§‰åœºæ™¯ä¸­ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œè¿™ç§æ„ŸçŸ¥é©±åŠ¨çš„å½’çº³åç½®å³ä¾¿æºè‡ªåˆæˆåˆºæ¿€ï¼Œä¹Ÿèƒ½æ˜¾è‘—å¢å¼º CNN å’Œ transformer æ¶æ„çš„ç»“æ„æ•æ„Ÿæ€§ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†æ„ŸçŸ¥ç§‘å­¦ä¸æœºå™¨å­¦ä¹ çš„åˆ›æ–°æ•´åˆï¼Œä¸ºåœ¨è§†è§‰æ¨¡å‹è®¾è®¡ä¸­åµŒå…¥æ„ŸçŸ¥å…ˆéªŒ(perceptual priors)å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15156v1",
      "published_date": "2025-09-18 17:00:42 UTC",
      "updated_date": "2025-09-18 17:00:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:21:43.487691+00:00"
    },
    {
      "arxiv_id": "2509.15151v3",
      "title": "Exploring How Audio Effects Alter Emotion with Foundation Models",
      "title_zh": "åŸºäºåŸºç¡€æ¨¡å‹æ¢ç´¢éŸ³é¢‘æ•ˆæœå¯¹æƒ…æ„Ÿçš„å½±å“",
      "authors": [
        "Stelios Katsis",
        "Vassilis Lyberatos",
        "Spyridon Kantarelis",
        "Edmund Dervakos",
        "Giorgos Stamou"
      ],
      "abstract": "Audio effects (FX) such as reverberation, distortion, modulation, and dynamic range processing play a pivotal role in shaping emotional responses during music listening. While prior studies have examined links between low-level audio features and affective perception, the systematic impact of audio FX on emotion remains underexplored. This work investigates how foundation models - large-scale neural architectures pretrained on multimodal data - can be leveraged to analyze these effects. Such models encode rich associations between musical structure, timbre, and affective meaning, offering a powerful framework for probing the emotional consequences of sound design techniques. By applying various probing methods to embeddings from deep learning models, we examine the complex, nonlinear relationships between audio FX and estimated emotion, uncovering patterns tied to specific effects and evaluating the robustness of foundation audio models. Our findings aim to advance understanding of the perceptual impact of audio production practices, with implications for music cognition, performance, and affective computing.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†éŸ³é¢‘æ•ˆæœï¼ˆAudio effectsï¼‰å¦‚ä½•åˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼ˆFoundation Modelsï¼‰æ¥æ”¹å˜æƒ…æ„Ÿæ„ŸçŸ¥ã€‚é‰´äºæ··å“ã€å¤±çœŸå’Œè°ƒåˆ¶ç­‰éŸ³é¢‘æ•ˆæœï¼ˆFXï¼‰åœ¨å¡‘é€ éŸ³ä¹æƒ…æ„Ÿååº”ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†å…¶ç³»ç»Ÿæ€§å½±å“ä»ç¼ºä¹æ·±å…¥ç ”ç©¶ï¼Œä½œè€…æå‡ºåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§è§„æ¨¡ç¥ç»æ¶æ„æ¥åˆ†æè¿™äº›æ•ˆæœã€‚é€šè¿‡å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åµŒå…¥ï¼ˆembeddingsï¼‰åº”ç”¨å¤šç§æ¢æµ‹æ–¹æ³•ï¼ˆprobing methodsï¼‰ï¼Œè¯¥å·¥ä½œæ­ç¤ºäº†éŸ³é¢‘æ•ˆæœä¸é¢„ä¼°æƒ…æ„Ÿä¹‹é—´å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œå¹¶å‘ç°äº†ç‰¹å®šæ•ˆæœèƒŒåçš„æƒ…æ„Ÿæ¨¡å¼ã€‚ç ”ç©¶è¿˜è¯„ä¼°äº†åŸºç¡€éŸ³é¢‘æ¨¡å‹åœ¨å¤„ç†éŸ³è‰²ä¸æƒ…æ„Ÿå…³è”æ—¶çš„ç¨³å¥æ€§ã€‚è¿™äº›å‘ç°ä¸ä»…æå‡äº†å¯¹éŸ³é¢‘åˆ¶ä½œæ„ŸçŸ¥å½±å“çš„ç†è§£ï¼Œè¿˜å¯¹éŸ³ä¹è®¤çŸ¥ã€è¡¨æ¼”ä»¥åŠæƒ…æ„Ÿè®¡ç®—ï¼ˆaffective computingï¼‰ç­‰é¢†åŸŸäº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "https://github.com/stelioskt/audioFX",
      "pdf_url": "https://arxiv.org/pdf/2509.15151v3",
      "published_date": "2025-09-18 16:57:08 UTC",
      "updated_date": "2026-01-06 15:40:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:21:49.762834+00:00"
    },
    {
      "arxiv_id": "2509.15130v2",
      "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance",
      "title_zh": "WorldForgeï¼šé€šè¿‡å…è®­ç»ƒå¼•å¯¼å¼€å¯è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­æ¶Œç°çš„ 3D/4D ç”Ÿæˆ",
      "authors": [
        "Chenxi Song",
        "Yanming Yang",
        "Tong Zhao",
        "Ruibo Li",
        "Chi Zhang"
      ],
      "abstract": "Recent video diffusion models show immense potential for spatial intelligence tasks due to their rich world priors, but this is undermined by limited controllability, poor spatial-temporal consistency, and entangled scene-camera dynamics. Existing solutions, such as model fine-tuning and warping-based repainting, struggle with scalability, generalization, and robustness against artifacts. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. 1) Intra-Step Recursive Refinement injects fine-grained trajectory guidance at denoising steps through a recursive correction loop, ensuring motion remains aligned with the target path. 2) Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. 3) Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Our framework is plug-and-play and model-agnostic, enabling broad applicability across various 3D/4D tasks. Extensive experiments demonstrate that our method achieves state-of-the-art performance in trajectory adherence, geometric consistency, and perceptual quality, outperforming both training-intensive and inference-only baselines.",
      "tldr_zh": "é’ˆå¯¹ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç©ºé—´æ™ºèƒ½ä»»åŠ¡ä¸­é¢ä¸´çš„å¯æ§æ€§å·®ã€æ—¶ç©ºä¸€è‡´æ€§ä¸è¶³ä»¥åŠåœºæ™¯ä¸ç›¸æœºåŠ¨åŠ›å­¦è€¦åˆç­‰æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† WorldForgeã€‚WorldForge æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒï¼ˆtraining-freeï¼‰çš„æ¨ç†ç«¯æ¡†æ¶ï¼Œç”±ä¸‰ä¸ªç´§å¯†è€¦åˆçš„æ¨¡å—ç»„æˆï¼Œæ—¨åœ¨å®ç°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­æ¶Œç°çš„ 3D/4D ç”Ÿæˆã€‚å…¶ä¸­ Intra-Step Recursive Refinement æ¨¡å—é€šè¿‡é€’å½’ä¿®æ­£å¾ªç¯æ³¨å…¥ç»†ç²’åº¦çš„è½¨è¿¹å¼•å¯¼ï¼Œç¡®ä¿è¿åŠ¨ä¸ç›®æ ‡è·¯å¾„ä¸¥æ ¼å¯¹é½ï¼›Flow-Gated Latent Fusion æ¨¡å—åˆ©ç”¨å…‰æµç›¸ä¼¼æ€§åœ¨æ½œç©ºé—´ä¸­å®ç°è¿åŠ¨ä¸å¤–è§‚çš„è§£è€¦ï¼›è€Œ Dual-Path Self-Corrective Guidance åˆ™é€šè¿‡å¯¹æ¯”å¼•å¯¼ä¸éå¼•å¯¼çš„å»å™ªè·¯å¾„ï¼Œè‡ªé€‚åº”åœ°çº æ­£è½¨è¿¹åç§»ã€‚è¯¥æ¡†æ¶å…·æœ‰å³æ’å³ç”¨å’Œæ¨¡å‹ä¸å¯çŸ¥ï¼ˆmodel-agnosticï¼‰çš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºå„ç§ 3D/4D ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼ŒWorldForge åœ¨è½¨è¿¹éµå¾ªåº¦ã€å‡ ä½•ä¸€è‡´æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢å‡è¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„éœ€è¦å¤§é‡è®­ç»ƒæˆ–ä»…æ¨ç†çš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Project Webpage: https://worldforge-agi.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2509.15130v2",
      "published_date": "2025-09-18 16:40:47 UTC",
      "updated_date": "2025-09-27 14:42:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:21:57.165763+00:00"
    },
    {
      "arxiv_id": "2509.15289v1",
      "title": "Collective Voice: Recovered-Peer Support Mediated by An LLM-Based Chatbot for Eating Disorder Recovery",
      "title_zh": "Collective Voiceï¼šå¤§è¯­è¨€æ¨¡å‹èŠå¤©æœºå™¨äººä»‹å¯¼çš„è¿›é£Ÿéšœç¢åº·å¤è€…åŒä¼´æ”¯æŒ",
      "authors": [
        "Ryuhaerang Choi",
        "Taehan Kim",
        "Subin Park",
        "Seohyeon Yoo",
        "Jennifer G. Kim",
        "Sung-Ju Lee"
      ],
      "abstract": "Peer recovery narratives provide unique benefits beyond professional or lay mentoring by fostering hope and sustained recovery in eating disorder (ED) contexts. Yet, such support is limited by the scarcity of peer-involved programs and potential drawbacks on recovered peers, including relapse risk. To address this, we designed RecoveryTeller, a chatbot adopting a recovered-peer persona that portrays itself as someone recovered from an ED. We examined whether such a persona can reproduce the support affordances of peer recovery narratives. We compared RecoveryTeller with a lay-mentor persona chatbot offering similar guidance but without a recovery background. We conducted a 20-day cross-over deployment study with 26 ED participants, each using both chatbots for 10 days. RecoveryTeller elicited stronger emotional resonance than a lay-mentor chatbot, yet tensions between emotional and epistemic trust led participants to view the two personas as complementary rather than substitutes. We provide design implications for mental health chatbot persona design.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¿›é£Ÿéšœç¢ï¼ˆEating Disorder, EDï¼‰åº·å¤ä¸­åŒä¼´æ”¯æŒèµ„æºçš„ç¨€ç¼ºåŠå…¶å¯¹åº·å¤è€…å¯èƒ½å¸¦æ¥çš„å¤å‘é£é™©ï¼Œè®¾è®¡å¹¶å¼€å‘äº† RecoveryTellerã€‚è¿™æ˜¯ä¸€æ¬¾é‡‡ç”¨â€œå·²åº·å¤åŒä¼´â€äººæ ¼ï¼ˆRecovered-peer personaï¼‰çš„ LLM èŠå¤©æœºå™¨äººï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹ŸåŒä¼´åº·å¤å™äº‹æ¥æä¾›æƒ…æ„Ÿæ”¯æŒã€‚é€šè¿‡å¯¹26å ED å‚ä¸è€…è¿›è¡Œä¸ºæœŸ20å¤©çš„äº¤å‰éƒ¨ç½²å®éªŒï¼Œç ”ç©¶å¯¹æ¯”äº† RecoveryTeller ä¸â€œéä¸“ä¸šå¯¼å¸ˆâ€äººæ ¼ï¼ˆLay-mentor personaï¼‰æœºå™¨äººçš„å®é™…æ•ˆæœã€‚å®éªŒå‘ç°ï¼ŒRecoveryTeller èƒ½å¤Ÿå¼•å‘æ›´å¼ºçƒˆçš„æƒ…æ„Ÿå…±é¸£ï¼ˆEmotional resonanceï¼‰ï¼Œä½†å‚ä¸è€…åœ¨æƒ…æ„Ÿä¿¡ä»»ä¸è®¤çŸ¥ä¿¡ä»»ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œå€¾å‘äºå°†ä¸¤ç§äººæ ¼è§†ä¸ºäº’è¡¥è€Œéæ›¿ä»£å…³ç³»ã€‚è¯¥ç ”ç©¶ä¸ºå¿ƒç†å¥åº·ç±»èŠå¤©æœºå™¨äººçš„ Persona è®¾è®¡æä¾›äº†é‡è¦çš„è®¾è®¡å¯ç¤ºä¸å®è·µå‚è€ƒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15289v1",
      "published_date": "2025-09-18 16:38:58 UTC",
      "updated_date": "2025-09-18 16:38:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:01.172465+00:00"
    },
    {
      "arxiv_id": "2509.16276v1",
      "title": "Comparative Analysis of STEM and non-STEM Teachers' Needs for Integrating AI into Educational Environments",
      "title_zh": "STEM ä¸é STEM æ•™å¸ˆå°†äººå·¥æ™ºèƒ½èå…¥æ•™è‚²ç¯å¢ƒçš„éœ€æ±‚å¯¹æ¯”åˆ†æ",
      "authors": [
        "Bahare Riahi",
        "Veronica Catete"
      ],
      "abstract": "There is an increasing imperative to integrate programming platforms within AI frameworks to enhance educational tasks for both teachers and students. However, commonly used platforms such as Code.org, Scratch, and Snap fall short of providing the desired AI features and lack adaptability for interdisciplinary applications. This study explores how educational platforms can be improved by incorporating AI and analytics features to create more effective learning environments across various subjects and domains. We interviewed 8 K-12 teachers and asked their practices and needs while using any block-based programming (BBP) platform in their classes. We asked for their approaches in assessment, course development and expansion of resources, and student monitoring in their classes. Thematic analysis of the interview transcripts revealed both commonalities and differences in the AI tools needed between the STEM and non-STEM groups. Our results indicated advanced AI features that could promote BBP platforms. Both groups stressed the need for integrity and plagiarism checks, AI adaptability, customized rubrics, and detailed feedback in assessments. Non-STEM teachers also emphasized the importance of creative assignments and qualitative assessments. Regarding resource development, both AI tools desired for updating curricula, tutoring libraries, and generative AI features. Non-STEM teachers were particularly interested in supporting creative endeavors, such as art simulations. For student monitoring, both groups prioritized desktop control, daily tracking, behavior monitoring, and distraction prevention tools. Our findings identify specific AI-enhanced features needed by K-12 teachers across various disciplines and lay the foundation for creating more efficient, personalized, and engaging educational experiences.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ•´åˆ AI å’Œåˆ†æåŠŸèƒ½æ¥æ”¹è¿›æ•™è‚²å¹³å°ï¼Œä»¥åœ¨ä¸åŒå­¦ç§‘é¢†åŸŸå»ºç«‹æ›´æœ‰æ•ˆçš„å­¦ä¹ ç¯å¢ƒã€‚ç ”ç©¶äººå‘˜å¯¹ 8 å K-12 æ•™å¸ˆè¿›è¡Œäº†æ·±åº¦è®¿è°ˆï¼Œåˆ†æäº†ä»–ä»¬åœ¨è¯¾å ‚ä¸­ä½¿ç”¨ç§¯æœ¨å¼ç¼–ç¨‹ (block-based programming, BBP) å¹³å°æ—¶çš„å®è·µä¸éœ€æ±‚ï¼Œé‡ç‚¹å…³æ³¨æ•™å­¦è¯„ä¼°ã€èµ„æºå¼€å‘å’Œå­¦ç”Ÿç›‘æ§ã€‚ç ”ç©¶å‘ç° STEM å’Œé STEM æ•™å¸ˆåœ¨ AI å·¥å…·éœ€æ±‚ä¸Šæ—¢æœ‰å…±æ€§ä¹Ÿæœ‰å·®å¼‚ï¼ŒåŒæ–¹éƒ½å¼ºè°ƒäº†è¯šä¿¡ä¸å‰½çªƒæ£€æŸ¥ã€AI é€‚åº”æ€§ã€è‡ªå®šä¹‰é‡è§„ (rubrics) ä»¥åŠè¯¦ç»†è¯„ä¼°åé¦ˆçš„é‡è¦æ€§ã€‚é STEM æ•™å¸ˆåˆ™è¡¨ç°å‡ºå¯¹åˆ›æ„ä»»åŠ¡ã€å®šæ€§è¯„ä¼°ä»¥åŠè‰ºæœ¯æ¨¡æ‹Ÿç­‰æ´»åŠ¨çš„ç‰¹æ®Š AI éœ€æ±‚ã€‚åœ¨èµ„æºå¼€å‘å’Œå­¦ç”Ÿç›‘æ§æ–¹é¢ï¼Œæ•™å¸ˆä»¬æ™®éå¸Œæœ›å¼•å…¥ç”Ÿæˆå¼ AI (generative AI) åŠŸèƒ½è¾…åŠ©è¯¾ç¨‹æ›´æ–°ï¼Œå¹¶é…å¤‡æ¡Œé¢æ§åˆ¶å’Œè¡Œä¸ºç›‘æµ‹å·¥å…·ã€‚è¯¥ç ”ç©¶è¯†åˆ«äº† K-12 æ•™å¸ˆè·¨å­¦ç§‘æ‰€éœ€çš„ç‰¹å®š AI å¢å¼ºåŠŸèƒ½ï¼Œä¸ºåˆ›å»ºæ›´é«˜æ•ˆã€ä¸ªæ€§åŒ–ä¸”å…·æœ‰å‚ä¸æ„Ÿçš„æ•™è‚²ä½“éªŒå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "16 pages, 3 figures, Published in HCII 2025 Conference Proceedings",
      "pdf_url": "https://arxiv.org/pdf/2509.16276v1",
      "published_date": "2025-09-18 16:20:18 UTC",
      "updated_date": "2025-09-18 16:20:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:00.476112+00:00"
    },
    {
      "arxiv_id": "2509.15116v1",
      "title": "The mechanization of science illustrated by the Lean formalization of the multi-graded Proj construction",
      "title_zh": "ä»¥å¤šé‡åˆ†æ¬¡ Proj æ„é€ çš„ Lean å½¢å¼åŒ–é˜é‡Šç§‘å­¦çš„æœºæ¢°åŒ–",
      "authors": [
        "Arnaud Mayeux",
        "Jujian Zhang"
      ],
      "abstract": "We formalize the multi-graded Proj construction in Lean4, illustrating mechanized mathematics and formalization.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨ Lean4 å½¢å¼åŒ–è¯æ˜åŠ©æ‰‹ä¸­å®ç°äº† multi-graded Proj constructionï¼Œé€šè¿‡è¿™ä¸€ä»£æ•°å‡ ä½•æ ¸å¿ƒæ„é€ çš„å®ç°ï¼Œç”ŸåŠ¨å±•ç¤ºäº†æ•°å­¦å½¢å¼åŒ–ä¸ç§‘å­¦æœºæ¢°åŒ– (mechanization of science) çš„æ·±åº¦èåˆã€‚ä½œè€…è¯¦ç»†æè¿°äº†åœ¨ Lean4 ç¯å¢ƒä¸‹æ„å»ºå¤šé‡åˆ†æ¬¡ç»“æ„çš„é€»è¾‘è¿‡ç¨‹ï¼Œä»¥æ­¤ä½œä¸ºç°ä»£å½¢å¼åŒ–æ•°å­¦ç ”ç©¶çš„ä¸€ä¸ªå…¸å‹èŒƒä¾‹ã€‚è¯¥å·¥ä½œä¸ä»…ä¸º Lean çš„æ•°å­¦åº“æä¾›äº†é‡è¦çš„ä»£æ•°å‡ ä½•ç»„ä»¶ï¼Œè¿˜æ·±å…¥æ¢è®¨äº†æœºå™¨è¾…åŠ©è¯æ˜åœ¨éªŒè¯å¤æ‚æ•°å­¦ç†è®ºä¸­çš„å®ç”¨ä»·å€¼ã€‚é€šè¿‡è¿™ä¸€å®è·µï¼Œè®ºæ–‡è¿›ä¸€æ­¥è®ºè¯äº†å½¢å¼åŒ–æ–¹æ³•åœ¨æå‡ç§‘å­¦ç ”ç©¶ä¸¥è°¨æ€§å’Œè‡ªåŠ¨åŒ–æ°´å¹³æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥æ•°å­¦ç ”ç©¶çš„æœºæ¢°åŒ–è½¬å‹æä¾›äº†å…³é”®çš„æ–¹æ³•è®ºå‚è€ƒã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "math.AG"
      ],
      "primary_category": "cs.LO",
      "comment": "Short note",
      "pdf_url": "https://arxiv.org/pdf/2509.15116v1",
      "published_date": "2025-09-18 16:19:41 UTC",
      "updated_date": "2025-09-18 16:19:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:00.965624+00:00"
    },
    {
      "arxiv_id": "2509.15103v2",
      "title": "Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning",
      "title_zh": "å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„è„†å¼±æ™ºèƒ½ä½“è¯†åˆ«",
      "authors": [
        "Simin Li",
        "Zheng Yuwei",
        "Zihao Mao",
        "Linhao Wang",
        "Ruixiao Xu",
        "Chengdong Ma",
        "Xin Yu",
        "Yuqing Ma",
        "Qi Dou",
        "Xin Wang",
        "Jie Luo",
        "Bo An",
        "Yaodong Yang",
        "Weifeng Lv",
        "Xianglong Liu"
      ],
      "abstract": "Partial agent failure becomes inevitable when systems scale up, making it crucial to identify the subset of agents whose compromise would most severely degrade overall performance. In this paper, we study this Vulnerable Agent Identification (VAI) problem in large-scale multi-agent reinforcement learning (MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task of selecting the most vulnerable agents, and the lower level learns worst-case adversarial policies for these agents using mean-field MARL. The two problems are coupled together, making HAD-MFC difficult to solve. To solve this, we first decouple the hierarchical process by Fenchel-Rockafellar transform, resulting a regularized mean-field Bellman operator for upper level that enables independent learning at each level, thus reducing computational complexity. We then reformulate the upper-level combinatorial problem as a MDP with dense rewards from our regularized mean-field Bellman operator, enabling us to sequentially identify the most vulnerable agents by greedy and RL algorithms. This decomposition provably preserves the optimal solution of the original HAD-MFC. Experiments show our method effectively identifies more vulnerable agents in large-scale MARL and the rule-based system, fooling system into worse failures, and learns a value function that reveals the vulnerability of each agent.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)ä¸­çš„è„†å¼±æ™ºèƒ½ä½“è¯†åˆ«(Vulnerable Agent Identification, VAI)é—®é¢˜ï¼Œæ—¨åœ¨è¯†åˆ«å‡ºå¯¹ç³»ç»Ÿæ•´ä½“æ€§èƒ½å½±å“æœ€å¤§çš„æ™ºèƒ½ä½“å­é›†ã€‚ä½œè€…å°†è¯¥é—®é¢˜å»ºæ¨¡ä¸ºå±‚çº§å¯¹æŠ—å»ä¸­å¿ƒåŒ–å¹³å‡åœºæ§åˆ¶(Hierarchical Adversarial Decentralized Mean Field Control, HAD-MFC)æ¡†æ¶ï¼Œé€šè¿‡Fenchel-Rockafellar transformæœ‰æ•ˆè§£è€¦äº†é«˜å±‚ç»„åˆä¼˜åŒ–ä¸åº•å±‚å¯¹æŠ—ç­–ç•¥å­¦ä¹ ã€‚ä¸ºäº†è§£å†³ä¸Šå±‚ä»»åŠ¡çš„NP-hardç‰¹æ€§ï¼Œç ”ç©¶è€…å¼•å…¥äº†æ­£åˆ™åŒ–çš„å¹³å‡åœºBellman operatorï¼Œå¹¶å°†åŸé—®é¢˜é‡æ„ä¸ºå…·æœ‰å¯†é›†å¥–åŠ±çš„MDPï¼Œä»è€Œåˆ©ç”¨è´ªå©ªç®—æ³•æˆ–RLç®—æ³•é¡ºåºè¯†åˆ«æœ€è„†å¼±çš„æ™ºèƒ½ä½“ã€‚ç†è®ºåˆ†æè¯æ˜è¯¥åˆ†è§£æ–¹æ³•ä¿ç•™äº†åŸå§‹é—®é¢˜çš„æœ€ä¼˜è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å‹MARLå’Œè§„åˆ™ç³»ç»Ÿä¸­èƒ½æ›´ç²¾å‡†åœ°å®šä½å¯¼è‡´ç³»ç»Ÿå´©æºƒçš„å…³é”®èŠ‚ç‚¹ï¼Œå¹¶èƒ½é€šè¿‡å­¦ä¹ åˆ°çš„ä»·å€¼å‡½æ•°ç›´è§‚å±•ç¤ºå„æ™ºèƒ½ä½“çš„è„†å¼±æ€§ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "submitted to NIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.15103v2",
      "published_date": "2025-09-18 16:03:50 UTC",
      "updated_date": "2025-09-19 08:02:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:12.260830+00:00"
    },
    {
      "arxiv_id": "2509.15098v3",
      "title": "TextMineX: Data, Evaluation Framework and Ontology-guided LLM Pipeline for Humanitarian Mine Action",
      "title_zh": "TextMineXï¼šäººé“ä¸»ä¹‰æ’é›·è¡ŒåŠ¨çš„æ•°æ®ã€è¯„ä¼°æ¡†æ¶åŠæœ¬ä½“å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹æµæ°´çº¿",
      "authors": [
        "Chenyue Zhou",
        "GÃ¼rkan Solmaz",
        "Flavio Cirillo",
        "Kiril Gashteovski",
        "Jonathan FÃ¼rst"
      ],
      "abstract": "Humanitarian Mine Action (HMA) addresses the challenge of detecting and removing landmines from conflict regions. Much of the life-saving operational knowledge produced by HMA agencies is buried in unstructured reports, limiting the transferability of information between agencies. To address this issue, we propose TextMineX: the first dataset, evaluation framework and ontology-guided large language model (LLM) pipeline for knowledge extraction from text in the HMA domain. TextMineX structures HMA reports into (subject, relation, object)-triples, thus creating domain-specific knowledge. To ensure real-world relevance, we utilized the dataset from our collaborator Cambodian Mine Action Centre (CMAC). We further introduce a bias-aware evaluation framework that combines human-annotated triples with an LLM-as-Judge protocol to mitigate position bias in reference-free scoring. Our experiments show that ontology-aligned prompts improve extraction accuracy by up to 44.2%, reduce hallucinations by 22.5%, and enhance format adherence by 20.9% compared to baseline models. We publicly release the dataset and code.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººé“ä¸»ä¹‰æ‰«é›·(Humanitarian Mine Action, HMA)é¢†åŸŸä¸­å¤§é‡å…³é”®çŸ¥è¯†åŸ‹æ²¡åœ¨éç»“æ„åŒ–æŠ¥å‘Šä¸­ã€å¯¼è‡´æœºæ„é—´ä¿¡æ¯ä¼ è¾“å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†TextMineXæ¡†æ¶ã€‚TextMineXæ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºHMAé¢†åŸŸçŸ¥è¯†æå–çš„ç³»ç»Ÿï¼Œæ¶µç›–äº†datasetã€evaluation frameworkä»¥åŠä¸€å¥—ontology-guided large language model (LLM) pipelineï¼Œæ—¨åœ¨å°†æŠ¥å‘Šè½¬åŒ–ä¸ºç»“æ„åŒ–çš„(subject, relation, object)-triplesã€‚ä¸ºäº†ç¡®ä¿çœŸå®ç›¸å…³æ€§ï¼Œç ”ç©¶é‡‡ç”¨äº†æ¥è‡ªæŸ¬åŸ”å¯¨åœ°é›·è¡ŒåŠ¨ä¸­å¿ƒ(Cambodian Mine Action Centre, CMAC)çš„æ•°æ®é›†ã€‚è¯¥ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§bias-aware evaluation frameworkï¼Œç»“åˆäººå·¥æ ‡æ³¨ä¸LLM-as-Judgeåè®®ï¼Œæœ‰æ•ˆç¼“è§£äº†reference-free scoringä¸­çš„position biasã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œé‡‡ç”¨ontology-aligned promptsä½¿æå–å‡†ç¡®ç‡æå‡äº†44.2%ï¼Œhallucinationså‡å°‘äº†22.5%ï¼Œä¸”format adherenceæé«˜äº†20.9%ã€‚ç›®å‰è¯¥é¡¹ç›®å·²å…¬å¼€æ•°æ®é›†å’Œä»£ç ï¼Œä¸ºæå‡HMAé¢†åŸŸçš„çŸ¥è¯†ç®¡ç†ä¸è‡ªåŠ¨åŒ–æ°´å¹³æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15098v3",
      "published_date": "2025-09-18 15:55:19 UTC",
      "updated_date": "2026-01-21 13:34:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:28.874941+00:00"
    },
    {
      "arxiv_id": "2509.15095v2",
      "title": "Listening, Imagining & Refining: A Heuristic Optimized ASR Correction Framework with LLMs",
      "title_zh": "å€¾å¬ã€æƒ³è±¡ä¸ç²¾ä¿®ï¼šä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¯å‘å¼ä¼˜åŒ– ASR çº é”™æ¡†æ¶",
      "authors": [
        "Yutong Liu",
        "Ziyue Zhang",
        "Cheng Huang",
        "Yongbin Yu",
        "Xiangxiang Wang",
        "Yuqing Cai",
        "Nyima Tashi"
      ],
      "abstract": "Automatic Speech Recognition (ASR) systems remain prone to errors that affect downstream applications. In this paper, we propose LIR-ASR, a heuristic optimized iterative correction framework using LLMs, inspired by human auditory perception. LIR-ASR applies a \"Listening-Imagining-Refining\" strategy, generating phonetic variants and refining them in context. A heuristic optimization with finite state machine (FSM) is introduced to prevent the correction process from being trapped in local optima and rule-based constraints help maintain semantic fidelity. Experiments on both English and Chinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of up to 1.5 percentage points compared to baselines, demonstrating substantial accuracy gains in transcription.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LIR-ASRï¼Œè¿™æ˜¯ä¸€ä¸ªå—äººç±»å¬è§‰æ„ŸçŸ¥å¯å‘ã€åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œå¯å‘å¼ä¼˜åŒ–çš„è¿­ä»£ ASR çº é”™æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†â€œå¬ã€æƒ³ã€ç²¾ç‚¼â€ (Listening-Imagining-Refining) ç­–ç•¥ï¼Œé€šè¿‡ç”Ÿæˆè¯­éŸ³å˜ä½“å¹¶åœ¨ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œç²¾ç»†åŒ–è°ƒæ•´ã€‚ä¸ºäº†é˜²æ­¢çº é”™è¿‡ç¨‹é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºæœ‰é™çŠ¶æ€æœº (FSM) çš„å¯å‘å¼ä¼˜åŒ–ï¼Œå¹¶ç»“åˆåŸºäºè§„åˆ™çš„çº¦æŸä»¥ç»´æŒè¯­ä¹‰ä¿çœŸåº¦ã€‚è¯¥æ–¹æ³•åœ¨è‹±æ–‡å’Œä¸­æ–‡ ASR è¾“å‡ºä¸Šå‡è¿›è¡Œäº†å®éªŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒLIR-ASR çš„å­—ç¬¦é”™è¯¯ç‡ (CER) å’Œè¯é”™è¯¯ç‡ (WER) å¹³å‡é™ä½äº†å¤šè¾¾ 1.5 ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™ä¸€æˆæœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æå‡è½¬å½•å‡†ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—æ”¶ç›Šï¼Œæœ‰æ•ˆè§£å†³äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­å¸¸è§çš„é”™è¯¯é—®é¢˜ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15095v2",
      "published_date": "2025-09-18 15:50:54 UTC",
      "updated_date": "2025-09-20 12:01:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:32.683468+00:00"
    },
    {
      "arxiv_id": "2509.19362v1",
      "title": "DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence Models",
      "title_zh": "DeepACTIFï¼šåŸºäºæ¿€æ´»è½¨è¿¹çš„ç¥ç»åºåˆ—æ¨¡å‹é«˜æ•ˆç‰¹å¾å½’å› ",
      "authors": [
        "Benedikt W. Hosp"
      ],
      "abstract": "Feature attribution is essential for interpreting deep learning models, particularly in time-series domains such as healthcare, biometrics, and human-AI interaction. However, standard attribution methods, such as Integrated Gradients or SHAP, are computationally intensive and not well-suited for real-time applications. We present DeepACTIF, a lightweight and architecture-aware feature attribution method that leverages internal activations of sequence models to estimate feature importance efficiently. Focusing on LSTM-based networks, we introduce an inverse-weighted aggregation scheme that emphasises stability and magnitude of activations across time steps. Our evaluation across three biometric gaze datasets shows that DeepACTIF not only preserves predictive performance under severe feature reduction (top 10% of features) but also significantly outperforms established methods, including SHAP, IG, and DeepLIFT, in terms of both accuracy and statistical robustness. Using Wilcoxon signed-rank tests and effect size analysis, we demonstrate that DeepACTIF yields more informative feature rankings with significantly lower error across all top-k conditions (10 - 40%). Our experiments demonstrate that DeepACTIF not only reduces computation time and memory usage by orders of magnitude but also preserves model accuracy when using only top-ranked features. That makes DeepACTIF a viable solution for real-time interpretability on edge devices such as mobile XR headsets or embedded health monitors.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DeepACTIFï¼Œä¸€ç§é’ˆå¯¹ç¥ç»åºåˆ—æ¨¡å‹çš„é«˜æ•ˆç‰¹å¾å½’å›  (Feature Attribution) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ Integrated Gradients å’Œ SHAP ç­‰ä¼ ç»Ÿæ–¹æ³•åœ¨å®æ—¶åº”ç”¨ä¸­è®¡ç®—å¼€é”€è¿‡å¤§çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸“é—¨é’ˆå¯¹ LSTM ç½‘ç»œè®¾è®¡ï¼Œé€šè¿‡åˆ©ç”¨å†…éƒ¨æ¿€æ´»è½¨è¿¹ (Activation Traces) å¹¶å¼•å…¥ä¸€ç§é€†åŠ æƒèšåˆæ–¹æ¡ˆ (Inverse-weighted aggregation scheme)ï¼Œæ¥é«˜æ•ˆè¯„ä¼°ä¸åŒæ—¶é—´æ­¥çš„ç‰¹å¾é‡è¦æ€§ã€‚åœ¨ä¸‰ä¸ªç”Ÿç‰©è¯†åˆ«æ³¨è§†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepACTIF åœ¨ç‰¹å¾å¤§å¹…ç¼©å‡çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒä¼˜å¼‚çš„é¢„æµ‹æ€§èƒ½ï¼Œå…¶å‡†ç¡®æ€§å’Œç»Ÿè®¡ç¨³å¥æ€§æ˜¾è‘—ä¼˜äº SHAPã€IG å’Œ DeepLIFT ç­‰ä¸»æµæ–¹æ³•ã€‚ç»Ÿè®¡åˆ†æè¿›ä¸€æ­¥è¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½äº§ç”Ÿæ›´å…·ä¿¡æ¯é‡çš„ç‰¹å¾æ’åï¼Œä¸”è¯¯å·®ç‡æ˜¾è‘—é™ä½ã€‚DeepACTIF å°†è®¡ç®—æ—¶é—´å’Œå†…å­˜éœ€æ±‚é™ä½äº†æ•°ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶åœ¨ä»…ä½¿ç”¨é«˜æ’åç‰¹å¾æ—¶ä¾ç„¶èƒ½ç»´æŒæ¨¡å‹ç²¾åº¦ã€‚è¿™ä½¿å¾—è¯¥æ–¹æ³•æˆä¸ºç§»åŠ¨ XR å¤´æ˜¾æˆ–åµŒå…¥å¼å¥åº·ç›‘æµ‹å™¨ç­‰è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å®æ—¶å¯è§£é‡Šæ€§ (Interpretability) çš„ç†æƒ³è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19362v1",
      "published_date": "2025-09-18 15:47:05 UTC",
      "updated_date": "2025-09-18 15:47:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:35.671016+00:00"
    },
    {
      "arxiv_id": "2509.16275v1",
      "title": "SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair",
      "title_zh": "SecureFixAgentï¼šé¢å‘ Python é™æ€æ¼æ´è‡ªåŠ¨åŒ–ä¿®å¤çš„æ··åˆ LLM æ™ºèƒ½ä½“",
      "authors": [
        "Jugal Gajjar",
        "Kamalasankari Subramaniakuppusamy",
        "Relsy Puthal",
        "Kaustik Ranaware"
      ],
      "abstract": "Modern software development pipelines face growing challenges in securing large codebases with extensive dependencies. Static analysis tools like Bandit are effective at vulnerability detection but suffer from high false positives and lack repair capabilities. Large Language Models (LLMs), in contrast, can suggest fixes but often hallucinate changes and lack self-validation. We present SecureFixAgent, a hybrid repair framework integrating Bandit with lightweight local LLMs (<8B parameters) in an iterative detect-repair-validate loop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning on a diverse, curated dataset spanning multiple Python project domains, mitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses Bandit for detection, the LLM for candidate fixes with explanations, and Bandit re-validation for verification, all executed locally to preserve privacy and reduce cloud reliance. Experiments show SecureFixAgent reduces false positives by 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers false positives by 5.46% compared to pre-trained LLMs, typically converging within three iterations. Beyond metrics, developer studies rate explanation quality 4.5/5, highlighting its value for human trust and adoption. By combining verifiable security improvements with transparent rationale in a resource-efficient local framework, SecureFixAgent advances trustworthy, automated vulnerability remediation for modern pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SecureFixAgentï¼Œä¸€ä¸ªç»“åˆäº†Bandité™æ€åˆ†æå·¥å…·ä¸è½»é‡çº§æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ··åˆä¿®å¤æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–ä¿®å¤Pythoné™æ€æ¼æ´ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è¿­ä»£å¼çš„â€œæ£€æµ‹-ä¿®å¤-éªŒè¯â€é—­ç¯ï¼Œå¹¶åº”ç”¨åŸºäºLoRAçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œæœ‰æ•ˆè§£å†³äº†é™æ€åˆ†æå·¥å…·è¯¯æŠ¥ç‡é«˜ä»¥åŠLLMæ˜“äº§ç”Ÿå¹»è§‰çš„é—®é¢˜ã€‚SecureFixAgentåˆ©ç”¨Banditè¿›è¡Œæ£€æµ‹ä¸éªŒè¯ï¼Œç”±LLMç”Ÿæˆå¸¦æœ‰è§£é‡Šçš„ä¿®å¤å»ºè®®ï¼Œä¸”å…¨æµç¨‹åœ¨æœ¬åœ°è¿è¡Œä»¥ä¿æŠ¤éšç§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ¯”ä¼ ç»Ÿé™æ€åˆ†æé™ä½äº†10.8%çš„è¯¯æŠ¥ç‡ï¼Œä¿®å¤å‡†ç¡®ç‡æå‡äº†13.51%ï¼Œé€šå¸¸åœ¨ä¸‰æ¬¡è¿­ä»£å†…å³å¯æ”¶æ•›ã€‚å¼€å‘è€…è°ƒç ”å¯¹å…¶æä¾›çš„è§£é‡Šè´¨é‡è¯„åˆ†é«˜è¾¾4.5/5ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹å®ç°å¯ä¿¡ã€è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "6 pages, 3 figures, 4 tables, 1 algorithm, accepted in the Robustness and Security of Large Language Models (ROSE-LLM) special session at ICMLA 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16275v1",
      "published_date": "2025-09-18 15:45:43 UTC",
      "updated_date": "2025-09-18 15:45:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:41.675443+00:00"
    },
    {
      "arxiv_id": "2509.15084v1",
      "title": "From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support",
      "title_zh": "ä»æµ·æ´‹åˆ°ç³»ç»Ÿï¼šæ¢ç´¢é¢å‘æµ·äº‹å†³ç­–æ”¯æŒçš„ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½",
      "authors": [
        "Doreen Jirak",
        "Pieter Maes",
        "Armeen Saroukanoff",
        "Dirk van Rooy"
      ],
      "abstract": "As autonomous technologies increasingly shape maritime operations, understanding why an AI system makes a decision becomes as crucial as what it decides. In complex and dynamic maritime environments, trust in AI depends not only on performance but also on transparency and interpretability. This paper highlights the importance of Explainable AI (XAI) as a foundation for effective human-machine teaming in the maritime domain, where informed oversight and shared understanding are essential. To support the user-centered integration of XAI, we propose a domain-specific survey designed to capture maritime professionals' perceptions of trust, usability, and explainability. Our aim is to foster awareness and guide the development of user-centric XAI systems tailored to the needs of seafarers and maritime teams.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI, XAI)ä½œä¸ºæµ·äº‹å†³ç­–æ”¯æŒç³»ç»ŸåŸºç¡€çš„é‡è¦æ€§ï¼Œå¼ºè°ƒåœ¨å¤æ‚ä¸”åŠ¨æ€çš„æµ·äº‹ç¯å¢ƒä¸­ï¼Œé€æ˜åº¦å’Œå¯è§£é‡Šæ€§æ˜¯å®ç°æœ‰æ•ˆäººæœºåä½œ(Human-Machine Teaming)çš„å…³é”®ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç†è§£AIå†³ç­–èƒŒåçš„é€»è¾‘å¯¹äºå»ºç«‹ä¸“ä¸šä¿¡ä»»è‡³å…³é‡è¦ï¼Œè¿™ç›´æ¥å½±å“åˆ°æµ·äº‹ä½œä¸šçš„å®‰å…¨æ€§ä¸æ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹æµ·äº‹é¢†åŸŸçš„ç‰¹å®šè°ƒæŸ¥æ–¹æ³•ï¼Œç”¨äºæ•æ‰æµ·äº‹ä»ä¸šè€…å¯¹ç³»ç»Ÿä¿¡ä»»ã€å¯ç”¨æ€§(Usability)åŠå¯è§£é‡Šæ€§çš„ä¸»è§‚æ„ŸçŸ¥ã€‚è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡è¯†åˆ«æµ·å‘˜çš„å®é™…éœ€æ±‚ï¼ŒæŒ‡å¯¼ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒ(User-Centered)çš„XAIç³»ç»Ÿå¼€å‘ã€‚æœ€ç»ˆï¼Œè¿™é¡¹å·¥ä½œä¸ºæ„å»ºæ›´å…·é€æ˜åº¦ã€èƒ½å¤Ÿæ”¯æŒæµ·äº‹å›¢é˜Ÿè¿›è¡Œæ˜æ™ºå†³ç­–çš„æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ç†è®ºä¸å®è·µå‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper accepted at Human Learning and Decision-Making Workshop @ECML-PKDD Conference 2025, Porto, Portugal",
      "pdf_url": "https://arxiv.org/pdf/2509.15084v1",
      "published_date": "2025-09-18 15:42:54 UTC",
      "updated_date": "2025-09-18 15:42:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:40.982987+00:00"
    },
    {
      "arxiv_id": "2509.15058v1",
      "title": "Communication Efficient Split Learning of ViTs with Attention-based Double Compression",
      "title_zh": "åŸºäºæ³¨æ„åŠ›åŒé‡å‹ç¼©çš„é€šä¿¡é«˜æ•ˆ ViT æ‹†åˆ†å­¦ä¹ ",
      "authors": [
        "Federico Alvetreti",
        "Jary Pomponi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "abstract": "This paper proposes a novel communication-efficient Split Learning (SL) framework, named Attention-based Double Compression (ADC), which reduces the communication overhead required for transmitting intermediate Vision Transformers activations during the SL training process. ADC incorporates two parallel compression strategies. The first one merges samples' activations that are similar, based on the average attention score calculated in the last client layer; this strategy is class-agnostic, meaning that it can also merge samples having different classes, without losing generalization ability nor decreasing final results. The second strategy follows the first and discards the least meaningful tokens, further reducing the communication cost. Combining these strategies not only allows for sending less during the forward pass, but also the gradients are naturally compressed, allowing the whole model to be trained without additional tuning or approximations of the gradients. Simulation results demonstrate that Attention-based Double Compression outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º Attention-based Double Compression (ADC) çš„é€šä¿¡é«˜æ•ˆ Split Learning (SL) æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­ä¼ è¾“ Vision Transformers (ViTs) ä¸­é—´æ¿€æ´»å€¼çš„é€šä¿¡å¼€é”€ã€‚ADC ç»“åˆäº†ä¸¤ç§å¹¶è¡Œçš„å‹ç¼©ç­–ç•¥ï¼Œé¦–å…ˆæ ¹æ®å®¢æˆ·ç«¯æœ€åä¸€å±‚çš„å¹³å‡ Attention Score åˆå¹¶ç›¸ä¼¼çš„æ ·æœ¬æ¿€æ´»å€¼ï¼Œä¸”è¯¥è¿‡ç¨‹å…·å¤‡ Class-agnostic ç‰¹æ€§ï¼Œèƒ½å¤Ÿç¡®ä¿æ³›åŒ–èƒ½åŠ›ä¸å—å½±å“ã€‚å…¶æ¬¡ï¼Œè¯¥æ¡†æ¶ä¼šè¿›ä¸€æ­¥ä¸¢å¼ƒæœ€æ— æ„ä¹‰çš„ Tokensï¼Œä»è€Œæ˜¾è‘—é™ä½é€šä¿¡æˆæœ¬ã€‚è¿™ç§è®¾è®¡ä¸ä»…ä¼˜åŒ–äº†å‰å‘ä¼ æ’­çš„æ•°æ®ä¼ è¾“ï¼Œä¹Ÿä½¿å¾—æ¢¯åº¦åœ¨åå‘ä¼ æ’­ä¸­è‡ªç„¶å¾—åˆ°å‹ç¼©ï¼Œä¸”æ— éœ€å¯¹æ¢¯åº¦è¿›è¡Œé¢å¤–çš„å¾®è°ƒæˆ–è¿‘ä¼¼å¤„ç†ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒADC åœ¨å¤§å¹…é™ä½é€šä¿¡å¼€é”€çš„åŒæ—¶ä¿æŒäº†æé«˜çš„å‡†ç¡®ç‡ï¼Œå…¶æ€§èƒ½è¡¨ç°ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„ SL æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15058v1",
      "published_date": "2025-09-18 15:22:24 UTC",
      "updated_date": "2025-09-18 15:22:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:42.384526+00:00"
    },
    {
      "arxiv_id": "2509.15057v1",
      "title": "Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning",
      "title_zh": "é€šè¿‡è¶…å‚æ•°åŒ–å¹³è¡¡ç¨€ç–å¾ªç¯ç¥ç»ç½‘ç»œä»¥åŠ©åŠ›å…ƒå­¦ä¹ ",
      "authors": [
        "Quincy Hershey",
        "Randy Paffenroth"
      ],
      "abstract": "This paper develops alternative hyperparameters for specifying sparse Recurrent Neural Networks (RNNs). These hyperparameters allow for varying sparsity within the trainable weight matrices of the model while improving overall performance. This architecture enables the definition of a novel metric, hidden proportion, which seeks to balance the distribution of unknowns within the model and provides significant explanatory power of model performance. Together, the use of the varied sparsity RNN architecture combined with the hidden proportion metric generates significant performance gains while improving performance expectations on an a priori basis. This combined approach provides a path forward towards generalized meta-learning applications and model optimization based on intrinsic characteristics of the data set, including input and output dimensions.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ç”¨äºæŒ‡å®šç¨€ç–å¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Networks, RNNs) çš„æ›¿ä»£è¶…å‚æ•°ï¼Œæ—¨åœ¨ä¼˜åŒ–æ¨¡å‹åœ¨ Meta-Learning ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™äº›è¶…å‚æ•°å…è®¸æ¨¡å‹åœ¨å¯è®­ç»ƒæƒé‡çŸ©é˜µä¸­å®ç°å¯å˜ç¨€ç–æ€§ï¼Œä»è€Œåœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶æå‡æ•´ä½“æ€§èƒ½ã€‚è®ºæ–‡é€šè¿‡è¯¥æ¶æ„å®šä¹‰äº†ä¸€ä¸ªåä¸º hidden proportion çš„æ–°æŒ‡æ ‡ï¼Œç”¨äºå¹³è¡¡æ¨¡å‹å†…éƒ¨æœªçŸ¥é‡çš„åˆ†å¸ƒï¼Œå¹¶ä¸ºæ¨¡å‹è¡¨ç°æä¾›äº†æ˜¾è‘—çš„è§£é‡ŠåŠ›ã€‚å°†å¯å˜ç¨€ç–æ€§ RNN æ¶æ„ä¸ hidden proportion æŒ‡æ ‡ç›¸ç»“åˆï¼Œä¸ä»…å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½å¢ç›Šï¼Œè¿˜æ”¹å–„äº†åŸºäºå…ˆéªŒçŸ¥è¯†çš„æ€§èƒ½é¢„æœŸã€‚è¿™ç§ç»¼åˆæ–¹æ³•ä¸ºé€šç”¨çš„ Meta-Learning åº”ç”¨ä»¥åŠåŸºäºæ•°æ®é›†å›ºæœ‰ç‰¹å¾ï¼ˆå¦‚è¾“å…¥å’Œè¾“å‡ºç»´åº¦ï¼‰çš„æ¨¡å‹ä¼˜åŒ–å¼€è¾Ÿäº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15057v1",
      "published_date": "2025-09-18 15:20:13 UTC",
      "updated_date": "2025-09-18 15:20:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:47.954752+00:00"
    },
    {
      "arxiv_id": "2509.15044v1",
      "title": "Credit Card Fraud Detection",
      "title_zh": "ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹",
      "authors": [
        "Iva Popova",
        "Hamza A. A. Gardi"
      ],
      "abstract": "Credit card fraud remains a significant challenge due to class imbalance and fraudsters mimicking legitimate behavior. This study evaluates five machine learning models - Logistic Regression, Random Forest, XGBoost, K-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using undersampling, SMOTE, and a hybrid approach. Our models are evaluated on the original imbalanced test set to better reflect real-world performance. Results show that the hybrid method achieves the best balance between recall and precision, especially improving MLP and KNN performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹ä¸­é¢ä¸´çš„ç±»åˆ«ä¸å¹³è¡¡(class imbalance)å’Œæ¬ºè¯ˆè€…æ¨¡ä»¿åˆæ³•è¡Œä¸ºçš„ä¸¥å³»æŒ‘æˆ˜å±•å¼€äº†æ·±å…¥æ¢è®¨ã€‚ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†åŒ…æ‹¬ Logistic Regressionã€Random Forestã€XGBoostã€K-Nearest Neighbors (KNN) ä»¥åŠ Multi-Layer Perceptron (MLP) åœ¨å†…çš„äº”ç§ä¸»æµæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ä¸ºäº†ä¼˜åŒ–æ¨¡å‹è¡¨ç°ï¼Œç ”ç©¶äººå‘˜é‡‡ç”¨äº†æ¬ é‡‡æ ·(undersampling)ã€SMOTE ä»¥åŠä¸€ç§æ··åˆæ–¹æ³•(hybrid approach)æ¥å¤„ç†çœŸå®ä¸–ç•Œçš„æ•°æ®é›†ã€‚æ‰€æœ‰æ¨¡å‹å‡åœ¨ä¿æŒåŸå§‹ä¸å¹³è¡¡æ¯”ä¾‹çš„æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œæ—¨åœ¨æ›´çœŸå®åœ°æ¨¡æ‹Ÿç°å®ç¯å¢ƒä¸­çš„æ£€æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæ··åˆæ–¹æ³•åœ¨å¬å›ç‡(recall)ä¸ç²¾ç¡®ç‡(precision)ä¹‹é—´å®ç°äº†æœ€ä¼˜å¹³è¡¡ï¼Œå°¤å…¶åœ¨æå‡ MLP å’Œ KNN çš„æ£€æµ‹æ•ˆèƒ½æ–¹é¢è¡¨ç°çªå‡ºã€‚è¯¥ç ”ç©¶ä¸ºé‡‘èæœºæ„åœ¨å¤„ç†å¤æ‚ä¸”ä¸å¹³è¡¡çš„æ¬ºè¯ˆæ•°æ®æ—¶ï¼Œæä¾›äº†å…·æœ‰å®é™…åº”ç”¨ä»·å€¼çš„æ¨¡å‹é€‰æ‹©å’Œæ•°æ®å¤„ç†ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15044v1",
      "published_date": "2025-09-18 15:08:14 UTC",
      "updated_date": "2025-09-18 15:08:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:23:00.185779+00:00"
    },
    {
      "arxiv_id": "2509.15042v1",
      "title": "Reinforcement Learning Agent for a 2D Shooter Game",
      "title_zh": "é¢å‘ 2D å°„å‡»æ¸¸æˆçš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“",
      "authors": [
        "Thomas Ackermann",
        "Moritz Spang",
        "Hamza A. A. Gardi"
      ],
      "abstract": "Reinforcement learning agents in complex game environments often suffer from sparse rewards, training instability, and poor sample efficiency. This paper presents a hybrid training approach that combines offline imitation learning with online reinforcement learning for a 2D shooter game agent. We implement a multi-head neural network with separate outputs for behavioral cloning and Q-learning, unified by shared feature extraction layers with attention mechanisms. Initial experiments using pure deep Q-Networks exhibited significant instability, with agents frequently reverting to poor policies despite occasional good performance. To address this, we developed a hybrid methodology that begins with behavioral cloning on demonstration data from rule-based agents, then transitions to reinforcement learning. Our hybrid approach achieves consistently above 70% win rate against rule-based opponents, substantially outperforming pure reinforcement learning methods which showed high variance and frequent performance degradation. The multi-head architecture enables effective knowledge transfer between learning modes while maintaining training stability. Results demonstrate that combining demonstration-based initialization with reinforcement learning optimization provides a robust solution for developing game AI agents in complex multi-agent environments where pure exploration proves insufficient.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹2Då°„å‡»æ¸¸æˆä¸­å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)é¢ä¸´çš„å¥–åŠ±ç¨€ç–ã€è®­ç»ƒä¸ç¨³å®šå’Œæ ·æœ¬æ•ˆç‡ä½ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆç¦»çº¿æ¨¡ä»¿å­¦ä¹ (Imitation Learning)ä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æ··åˆè®­ç»ƒæ–¹æ³•ã€‚ç ”ç©¶å®ç°äº†ä¸€ç§å¤šå¤´ç¥ç»ç½‘ç»œ(Multi-head neural network)ï¼Œé€šè¿‡å…±äº«å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶(Attention Mechanisms)çš„ç‰¹å¾æå–å±‚ï¼Œç»Ÿä¸€äº†è¡Œä¸ºå…‹éš†(Behavioral Cloning)å’ŒQ-learningçš„è¾“å‡ºã€‚é’ˆå¯¹çº¯æ·±åº¦Qç½‘ç»œ(Deep Q-Networks)åœ¨å®éªŒä¸­è¡¨ç°å‡ºçš„é«˜åº¦ä¸ç¨³å®šæ€§ï¼Œè¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨åŸºäºè§„åˆ™çš„æ™ºèƒ½ä½“æ¼”ç¤ºæ•°æ®è¿›è¡Œè¡Œä¸ºå…‹éš†åˆå§‹åŒ–ï¼Œéšåå†è½¬æ¢è‡³å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ··åˆæ–¹æ³•åœ¨å¯¹æŠ—åŸºäºè§„åˆ™çš„å¯¹æ‰‹æ—¶èƒ½å¤Ÿç¨³å®šä¿æŒ70%ä»¥ä¸Šçš„èƒœç‡ï¼Œæ˜¾è‘—ä¼˜äºæ–¹å·®è¾ƒå¤§ä¸”æ€§èƒ½å®¹æ˜“é€€åŒ–çš„çº¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¿™ç§å¤šå¤´æ¶æ„å®ç°äº†ä¸åŒå­¦ä¹ æ¨¡å¼ä¹‹é—´çš„æœ‰æ•ˆçŸ¥è¯†è¿ç§»å¹¶ç»´æŒäº†è®­ç»ƒç¨³å®šæ€§ï¼Œè¯æ˜äº†åœ¨çº¯æ¢ç´¢ä¸è¶³çš„å¤æ‚å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­ï¼Œç»“åˆæ¼”ç¤ºåˆå§‹åŒ–ä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ˜¯å¼€å‘æ¸¸æˆäººå·¥æ™ºèƒ½(Game AI)æ™ºèƒ½ä½“çš„ä¸€ç§é²æ£’è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15042v1",
      "published_date": "2025-09-18 15:07:41 UTC",
      "updated_date": "2025-09-18 15:07:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:52.857320+00:00"
    },
    {
      "arxiv_id": "2509.19360v2",
      "title": "Semantic Representation Attack against Aligned Large Language Models",
      "title_zh": "é’ˆå¯¹å·²å¯¹é½å¤§è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰è¡¨ç¤ºæ”»å‡»",
      "authors": [
        "Jiawei Lian",
        "Jianhong Pan",
        "Lefan Wang",
        "Yi Wang",
        "Shaohui Mei",
        "Lap-Pui Chau"
      ],
      "abstract": "Large Language Models (LLMs) increasingly employ alignment techniques to prevent harmful outputs. Despite these safeguards, attackers can circumvent them by crafting prompts that induce LLMs to generate harmful content.\n  Current methods typically target exact affirmative responses, such as ``Sure, here is...'', suffering from limited convergence, unnatural prompts, and high computational costs.\n  We introduce Semantic Representation Attack, a novel paradigm that fundamentally reconceptualizes adversarial objectives against aligned LLMs.\n  Rather than targeting exact textual patterns, our approach exploits the semantic representation space comprising diverse responses with equivalent harmful meanings.\n  This innovation resolves the inherent trade-off between attack efficacy and prompt naturalness that plagues existing methods.\n  The Semantic Representation Heuristic Search algorithm is proposed to efficiently generate semantically coherent and concise adversarial prompts by maintaining interpretability during incremental expansion.\n  We establish rigorous theoretical guarantees for semantic convergence and demonstrate that our method achieves unprecedented attack success rates (89.41\\% averaged across 18 LLMs, including 100\\% on 11 models) while maintaining stealthiness and efficiency.\n  Comprehensive experimental results confirm the overall superiority of our Semantic Representation Attack.\n  The code will be publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·²å¯¹é½çš„å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)æå‡ºäº†Semantic Representation Attackï¼Œè¿™æ˜¯ä¸€ç§é‡æ–°å®šä¹‰å¯¹æŠ—æ€§ç›®æ ‡çš„æ–°èŒƒå¼ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ”»å‡»æ–¹æ³•å› è¿‡åº¦ä¾èµ–ç‰¹å®šæ–‡æœ¬æ¨¡å¼è€Œå¯¼è‡´çš„æç¤ºè¯ä¸è‡ªç„¶åŠé«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œè¯¥æ–¹æ³•ç›´æ¥åˆ©ç”¨åŒ…å«ç­‰æ•ˆæœ‰å®³å«ä¹‰çš„è¯­ä¹‰è¡¨ç¤ºç©ºé—´(semantic representation space)è¿›è¡Œæ”»å‡»ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†Semantic Representation Heuristic Searchç®—æ³•ï¼Œé€šè¿‡åœ¨å¢é‡æ‰©å±•è¿‡ç¨‹ä¸­ä¿æŒå¯è§£é‡Šæ€§ï¼Œé«˜æ•ˆç”Ÿæˆè¯­ä¹‰è¿è´¯ä¸”ç®€æ´çš„å¯¹æŠ—æ€§æç¤ºè¯ã€‚è¯¥æ–¹æ³•ä»ç†è®ºä¸Šä¿è¯äº†è¯­ä¹‰æ”¶æ•›æ€§ï¼Œå¹¶åœ¨18ä¸ªLLMsä¸Šå®ç°äº†89.41%çš„å¹³å‡æ”»å‡»æˆåŠŸç‡ï¼Œå…¶ä¸­åœ¨11ä¸ªæ¨¡å‹ä¸Šè¾¾åˆ°100%çš„æˆåŠŸç‡ã€‚å®éªŒè¯æ˜ï¼ŒSemantic Representation Attackåœ¨ä¿æŒéšè”½æ€§å’Œæ•ˆç‡çš„åŒæ—¶ï¼Œæœ‰æ•ˆè§£å†³äº†æ”»å‡»æ•ˆåŠ›ä¸æç¤ºè¯è‡ªç„¶åº¦ä¹‹é—´çš„æƒè¡¡ï¼Œå±•ç°äº†å“è¶Šçš„ç»¼åˆæ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19360v2",
      "published_date": "2025-09-18 15:06:46 UTC",
      "updated_date": "2025-10-20 09:45:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:22:56.461498+00:00"
    },
    {
      "arxiv_id": "2509.15040v1",
      "title": "From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets",
      "title_zh": "ä»æ¨¡å¼åˆ°é¢„æµ‹ï¼šå™ªå£°é‡‘èå¸‚åœºä¸­åŸºäº Shapelet çš„è¶‹åŠ¿é¢„æµ‹æ¡†æ¶",
      "authors": [
        "Juwon Kim",
        "Hyunwook Lee",
        "Hyotaek Jeon",
        "Seungmin Jin",
        "Sungahn Ko"
      ],
      "abstract": "Directional forecasting in financial markets requires both accuracy and interpretability. Before the advent of deep learning, interpretable approaches based on human-defined patterns were prevalent, but their structural vagueness and scale ambiguity hindered generalization. In contrast, deep learning models can effectively capture complex dynamics, yet often offer limited transparency. To bridge this gap, we propose a two-stage framework that integrates unsupervised pattern extracion with interpretable forecasting. (i) SIMPC segments and clusters multivariate time series, extracting recurrent patterns that are invariant to amplitude scaling and temporal distortion, even under varying window sizes. (ii) JISC-Net is a shapelet-based classifier that uses the initial part of extracted patterns as input and forecasts subsequent partial sequences for short-term directional movement. Experiments on Bitcoin and three S&P 500 equities demonstrate that our method ranks first or second in 11 out of 12 metric--dataset combinations, consistently outperforming baselines. Unlike conventional deep learning models that output buy-or-sell signals without interpretable justification, our approach enables transparent decision-making by revealing the underlying pattern structures that drive predictive outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡‘èå¸‚åœºæ–¹å‘é¢„æµ‹ä¸­å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§éš¾ä»¥å…¼å¾—çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ— ç›‘ç£æ¨¡å¼æå–ä¸å¯è§£é‡Šé¢„æµ‹çš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ SIMPC ç®—æ³•å¯¹å¤šå˜é‡æ—¶é—´åºåˆ—è¿›è¡Œåˆ†å‰²ä¸èšç±»ï¼Œæå–å‡ºå¯¹æŒ¯å¹…ç¼©æ”¾å’Œæ—¶é—´æ‰­æ›²å…·æœ‰ä¸å˜æ€§çš„å¾ªç¯æ¨¡å¼ã€‚ç¬¬äºŒé˜¶æ®µåˆ©ç”¨åŸºäº shapelet çš„åˆ†ç±»å™¨ JISC-Netï¼Œé€šè¿‡åˆ†ææå–æ¨¡å¼çš„åˆå§‹éƒ¨åˆ†æ¥é¢„æµ‹åç»­åºåˆ—çš„çŸ­æœŸæ–¹å‘æ€§è¿åŠ¨ã€‚åœ¨ Bitcoin å’Œä¸‰åª S&P 500 è‚¡ç¥¨ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ 12 ç»„æŒ‡æ ‡æ•°æ®ç»„åˆä¸­æœ‰ 11 ç»„è¡¨ç°æœ€ä¼˜æˆ–æ¬¡ä¼˜ï¼Œä¸€è‡´ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚ä¸ä»…è¾“å‡ºä¹°å–ä¿¡å·çš„ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸åŒï¼Œè¯¥æ¡†æ¶é€šè¿‡æ­ç¤ºé©±åŠ¨é¢„æµ‹ç»“æœçš„åº•å±‚æ¨¡å¼ç»“æ„ï¼Œæ˜¾è‘—æå‡äº†å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦ä¸å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 7 figures, accepted at ACM CIKM 2025 conference",
      "pdf_url": "https://arxiv.org/pdf/2509.15040v1",
      "published_date": "2025-09-18 15:05:27 UTC",
      "updated_date": "2025-09-18 15:05:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:23:16.454963+00:00"
    },
    {
      "arxiv_id": "2509.15035v1",
      "title": "Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews",
      "title_zh": "æ ¡å‡†åçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä½œä¸ºå…ƒè¯„å®¡å‘˜ï¼šä¸€é¡¹é’ˆå¯¹åŒè¡Œè¯„å®¡ä¹‹è¯„å®¡çš„ç³»ç»ŸåŠŸèƒ½è¯­è¨€å­¦è¯è¯­åˆ†æ",
      "authors": [
        "Gabriela C. Zapata",
        "Bill Cope",
        "Mary Kalantzis",
        "Duane Searsmith"
      ],
      "abstract": "This study investigates the use of generative AI to support formative assessment through machine generated reviews of peer reviews in graduate online courses in a public university in the United States. Drawing on Systemic Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to explore how generative AI feedback constructs meaning across ideational, interpersonal, and textual dimensions. The findings suggest that generative AI can approximate key rhetorical and relational features of effective human feedback, offering directive clarity while also maintaining a supportive stance. The reviews analyzed demonstrated a balance of praise and constructive critique, alignment with rubric expectations, and structured staging that foregrounded student agency. By modeling these qualities, AI metafeedback has the potential to scaffold feedback literacy and enhance leaner engagement with peer review.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)åœ¨ç¾å›½ä¸€æ‰€å…¬ç«‹å¤§å­¦çš„ç ”ç©¶ç”Ÿåœ¨çº¿è¯¾ç¨‹ä¸­ï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„åŒä¼´è¯„å®¡ä¹‹è¯„å®¡ï¼ˆå…ƒè¯„å®¡ï¼‰æ¥æ”¯æŒå½¢æˆæ€§è¯„ä¼°çš„æƒ…å†µã€‚ç ”ç©¶å€Ÿé‰´ç³»ç»ŸåŠŸèƒ½è¯­è¨€å­¦(Systemic Functional Linguistics)å’Œè¯„ä»·ç†è®º(Appraisal Theory)ï¼Œåˆ†æäº†120ä»½å…ƒè¯„å®¡ï¼Œæ—¨åœ¨æ¢ç´¢AIåé¦ˆå¦‚ä½•åœ¨ç»éªŒã€äººé™…å’Œè¯­ç¯‡ç»´åº¦æ„å»ºæ„ä¹‰ã€‚ç ”ç©¶å‘ç°ï¼Œç”Ÿæˆå¼AIèƒ½å¤Ÿæ¥è¿‘äººç±»æœ‰æ•ˆåé¦ˆçš„å…³é”®ä¿®è¾å’Œå…³ç³»ç‰¹å¾ï¼Œåœ¨æä¾›æ˜ç¡®æŒ‡å¯¼æ€§çš„åŒæ—¶ä¿æŒæ”¯æŒæ€åº¦ã€‚åˆ†ææ˜¾ç¤ºï¼Œè¿™äº›è¯„å®¡åœ¨èµæ‰¬ä¸å»ºè®¾æ€§æ‰¹è¯„ä¹‹é—´å®ç°äº†å¹³è¡¡ï¼Œç¬¦åˆé‡è¡¨(Rubric)é¢„æœŸï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–ç»„ç»‡å‡¸æ˜¾äº†å­¦ç”Ÿçš„ä¸»ä½“æ€§(Agency)ã€‚é€šè¿‡æ¨¡æ‹Ÿè¿™äº›å“è´¨ï¼ŒAIå…ƒåé¦ˆ(AI Metafeedback)å…·æœ‰åŸ¹å…»åé¦ˆç´ å…»(Feedback Literacy)å¹¶å¢å¼ºå­¦ä¹ è€…å¯¹åŒä¼´è¯„å®¡å‚ä¸åº¦çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "39 pages, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.15035v1",
      "published_date": "2025-09-18 15:00:44 UTC",
      "updated_date": "2025-09-18 15:00:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:23:28.291439+00:00"
    },
    {
      "arxiv_id": "2509.15032v1",
      "title": "Sample Efficient Experience Replay in Non-stationary Environments",
      "title_zh": "éå¹³ç¨³ç¯å¢ƒä¸‹æ ·æœ¬é«˜æ•ˆçš„ç»éªŒå›æ”¾",
      "authors": [
        "Tianyang Duan",
        "Zongyuan Zhang",
        "Songxiao Guo",
        "Yuanye Zhao",
        "Zheng Lin",
        "Zihan Fang",
        "Yi Liu",
        "Dianxin Luan",
        "Dong Huang",
        "Heming Cui",
        "Yong Cui"
      ],
      "abstract": "Reinforcement learning (RL) in non-stationary environments is challenging, as changing dynamics and rewards quickly make past experiences outdated. Traditional experience replay (ER) methods, especially those using TD-error prioritization, struggle to distinguish between changes caused by the agent's policy and those from the environment, resulting in inefficient learning under dynamic conditions. To address this challenge, we propose the Discrepancy of Environment Dynamics (DoE), a metric that isolates the effects of environment shifts on value functions. Building on this, we introduce Discrepancy of Environment Prioritized Experience Replay (DEER), an adaptive ER framework that prioritizes transitions based on both policy updates and environmental changes. DEER uses a binary classifier to detect environment changes and applies distinct prioritization strategies before and after each shift, enabling more sample-efficient learning. Experiments on four non-stationary benchmarks demonstrate that DEER further improves the performance of off-policy algorithms by 11.54 percent compared to the best-performing state-of-the-art ER methods.",
      "tldr_zh": "é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨éå¹³ç¨³ç¯å¢ƒ(Non-stationary Environments)ä¸­å› åŠ¨åŠ›å­¦å’Œå¥–åŠ±å˜åŒ–å¯¼è‡´å†å²ç»éªŒè¿‡æ—¶çš„é—®é¢˜ï¼Œæœ¬æ–‡æ¢è®¨äº†ä¼ ç»Ÿç»éªŒå›æ”¾(Experience Replay)æ–¹æ³•åœ¨åŒºåˆ†ç­–ç•¥æ›´æ–°ä¸ç¯å¢ƒå˜åŒ–æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ç¯å¢ƒåŠ¨åŠ›å­¦å·®å¼‚(Discrepancy of Environment Dynamics, DoE)æŒ‡æ ‡ï¼Œç”¨ä»¥éš”ç¦»ç¯å¢ƒåç§»å¯¹ä»·å€¼å‡½æ•°çš„å½±å“ã€‚åŸºäºè¯¥æŒ‡æ ‡ï¼Œè®ºæ–‡è¿›ä¸€æ­¥å¼€å‘äº†è‡ªé€‚åº”çš„DEER(Discrepancy of Environment Prioritized Experience Replay)æ¡†æ¶ï¼Œé€šè¿‡äºŒå…ƒåˆ†ç±»å™¨æ£€æµ‹ç¯å¢ƒå˜åŒ–å¹¶åœ¨åç§»å‰ååº”ç”¨ä¸åŒçš„ä¼˜å…ˆçº§ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDEERåœ¨å››ä¸ªéå¹³ç¨³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰çš„å…ˆè¿›ç»éªŒå›æ”¾æ–¹æ³•ï¼Œèƒ½å°†ç¦»ç­–(Off-policy)ç®—æ³•çš„æ€§èƒ½æå‡11.54%ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬åˆ©ç”¨æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.15032v1",
      "published_date": "2025-09-18 14:57:09 UTC",
      "updated_date": "2025-09-18 14:57:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:23:29.685422+00:00"
    },
    {
      "arxiv_id": "2509.15027v1",
      "title": "CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models",
      "title_zh": "CLEARï¼šå¤§è¯­è¨€æ¨¡å‹è®ºè¯æ”¹å†™çš„å…¨é¢è¯­è¨€å­¦è¯„ä¼°",
      "authors": [
        "Thomas Huber",
        "Christina Niklaus"
      ],
      "abstract": "While LLMs have been extensively studied on general text generation tasks, there is less research on text rewriting, a task related to general text generation, and particularly on the behavior of models on this task. In this paper we analyze what changes LLMs make in a text rewriting setting. We focus specifically on argumentative texts and their improvement, a task named Argument Improvement (ArgImp). We present CLEAR: an evaluation pipeline consisting of 57 metrics mapped to four linguistic levels: lexical, syntactic, semantic and pragmatic. This pipeline is used to examine the qualities of LLM-rewritten arguments on a broad set of argumentation corpora and compare the behavior of different LLMs on this task and analyze the behavior of different LLMs on this task in terms of linguistic levels. By taking all four linguistic levels into consideration, we find that the models perform ArgImp by shortening the texts while simultaneously increasing average word length and merging sentences. Overall we note an increase in the persuasion and coherence dimensions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ–‡æœ¬é‡å†™ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯è®ºè¯æ”¹è¿›(Argument Improvement, ArgImp)ä¸­çš„è¡Œä¸ºè¿›è¡Œäº†ç³»ç»Ÿåˆ†æã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºCLEARçš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«æ˜ å°„åˆ°è¯æ±‡ã€å¥æ³•ã€è¯­ä¹‰å’Œè¯­ç”¨å››ä¸ªè¯­è¨€å±‚é¢çš„57é¡¹æŒ‡æ ‡ã€‚é€šè¿‡åœ¨å¤šä¸ªè®ºè¯è¯­æ–™åº“ä¸Šåº”ç”¨è¯¥æ¡†æ¶ï¼Œç ”ç©¶æ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨é‡å†™è¿‡ç¨‹ä¸­çš„è¯­è¨€ç‰¹å¾å·®å¼‚ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨æ‰§è¡ŒArgImpä»»åŠ¡æ—¶å€¾å‘äºç¼©çŸ­æ–‡æœ¬ã€å¢åŠ å¹³å‡è¯é•¿å¹¶åˆå¹¶å¥å­ã€‚æœ€ç»ˆå®éªŒè¡¨æ˜ï¼Œè¿™äº›ä¿®æ”¹åœ¨æ•´ä½“ä¸Šæ˜¾è‘—æå‡äº†æ–‡æœ¬çš„è¯´æœåŠ›(persuasion)å’Œè¿è´¯æ€§(coherence)ç»´åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2509.15027v1",
      "published_date": "2025-09-18 14:53:41 UTC",
      "updated_date": "2025-09-18 14:53:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:23:34.191380+00:00"
    },
    {
      "arxiv_id": "2509.15024v1",
      "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering",
      "title_zh": "è¶…è¶Šé‚»åŸŸçš„æ³¨æ„åŠ›ï¼šé‡æŒ¯ Transformer åœ¨å›¾èšç±»ä¸­çš„åº”ç”¨",
      "authors": [
        "Xuanting Xie",
        "Bingheng Li",
        "Erlin Pan",
        "Rui Hou",
        "Wenyu Chen",
        "Zhao Kang"
      ],
      "abstract": "Attention mechanisms have become a cornerstone in modern neural networks, driving breakthroughs across diverse domains. However, their application to graph structured data, where capturing topological connections is essential, remains underexplored and underperforming compared to Graph Neural Networks (GNNs), particularly in the graph clustering task. GNN tends to overemphasize neighborhood aggregation, leading to a homogenization of node representations. Conversely, Transformer tends to over globalize, highlighting distant nodes at the expense of meaningful local patterns. This dichotomy raises a key question: Is attention inherently redundant for unsupervised graph learning? To address this, we conduct a comprehensive empirical analysis, uncovering the complementary weaknesses of GNN and Transformer in graph clustering. Motivated by these insights, we propose the Attentive Graph Clustering Network (AGCN) a novel architecture that reinterprets the notion that graph is attention. AGCN directly embeds the attention mechanism into the graph structure, enabling effective global information extraction while maintaining sensitivity to local topological cues. Our framework incorporates theoretical analysis to contrast AGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV cache mechanism to improve computational efficiency, and (2) a pairwise margin contrastive loss to boost the discriminative capacity of the attention space. Extensive experimental results demonstrate that AGCN outperforms state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ³¨æ„åŠ›æœºåˆ¶åœ¨å›¾èšç±»(graph clustering)ä¸­çš„åº”ç”¨ï¼ŒæŒ‡å‡ºå›¾ç¥ç»ç½‘ç»œ(GNN)è¿‡åº¦å¼ºè°ƒé‚»åŸŸèšåˆå¯¼è‡´èŠ‚ç‚¹è¡¨ç¤ºåŒè´¨åŒ–ï¼Œè€ŒTransformeråˆ™å› è¿‡åº¦å…¨å±€åŒ–è€Œå¿½è§†äº†å±€éƒ¨æ¨¡å¼ã€‚é’ˆå¯¹è¿™äº›ç¼ºé™·ï¼Œç ”ç©¶æå‡ºäº†æ³¨æ„åŠ›å›¾èšç±»ç½‘ç»œ(AGCN)ï¼Œé€šè¿‡å°†æ³¨æ„åŠ›æœºåˆ¶ç›´æ¥åµŒå…¥å›¾ç»“æ„ä¸­ï¼Œå®ç°äº†åœ¨æœ‰æ•ˆæå–å…¨å±€ä¿¡æ¯çš„åŒæ—¶ä¿ç•™å¯¹å±€éƒ¨æ‹“æ‰‘çº¿ç´¢çš„æ•æ„Ÿæ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†KVç¼“å­˜(KV cache)æœºåˆ¶ä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œå¹¶é‡‡ç”¨äº†æˆå¯¹è¾¹ç¼˜å¯¹æ¯”æŸå¤±(pairwise margin contrastive loss)æ¥å¢å¼ºæ³¨æ„åŠ›ç©ºé—´çš„åˆ¤åˆ«èƒ½åŠ›ã€‚ç†è®ºåˆ†æä¸å®éªŒç»“æœè¡¨æ˜ï¼ŒAGCNæœ‰æ•ˆå…‹æœäº†GNNä¸Transformeråœ¨æ— ç›‘ç£å›¾å­¦ä¹ ä¸­çš„å±€é™æ€§ï¼Œå…¶æ€§èƒ½åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›(state-of-the-art)æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.15024v1",
      "published_date": "2025-09-18 14:51:13 UTC",
      "updated_date": "2025-09-18 14:51:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:23:31.283757+00:00"
    },
    {
      "arxiv_id": "2509.16273v1",
      "title": "SubDyve: Subgraph-Driven Dynamic Propagation for Virtual Screening Enhancement Controlling False Positive",
      "title_zh": "SubDyveï¼šåŸºäºå­å›¾é©±åŠ¨åŠ¨æ€ä¼ æ’­çš„è™šæ‹Ÿç­›é€‰å¢å¼ºä¸å‡é˜³æ€§æ§åˆ¶",
      "authors": [
        "Jungseob Yi",
        "Seoyoung Choi",
        "Sun Kim",
        "Sangseon Lee"
      ],
      "abstract": "Virtual screening (VS) aims to identify bioactive compounds from vast chemical libraries, but remains difficult in low-label regimes where only a few actives are known. Existing methods largely rely on general-purpose molecular fingerprints and overlook class-discriminative substructures critical to bioactivity. Moreover, they consider molecules independently, limiting effectiveness in low-label regimes. We introduce SubDyve, a network-based VS framework that constructs a subgraph-aware similarity network and propagates activity signals from a small known actives. When few active compounds are available, SubDyve performs iterative seed refinement, incrementally promoting new candidates based on local false discovery rate. This strategy expands the seed set with promising candidates while controlling false positives from topological bias and overexpansion. We evaluate SubDyve on ten DUD-E targets under zero-shot conditions and on the CDK7 target with a 10-million-compound ZINC dataset. SubDyve consistently outperforms existing fingerprint or embedding-based approaches, achieving margins of up to +34.0 on the BEDROC and +24.6 on the EF1% metric.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SubDyveï¼Œä¸€ä¸ªåŸºäºç½‘ç»œçš„è™šæ‹Ÿç­›é€‰(Virtual Screening)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åœ¨æ´»æ€§åŒ–åˆç‰©æå°‘çš„low-labelç¯å¢ƒä¸‹ï¼Œç°æœ‰æ–¹æ³•å› å¿½è§†ç±»åŒºåˆ†æ€§å­ç»“æ„ä¸”ç‹¬ç«‹å¤„ç†åˆ†å­è€Œå¯¼è‡´çš„æ€§èƒ½ä¸è¶³é—®é¢˜ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªsubgraph-aware similarity networkï¼Œé€šè¿‡ä»å°‘é‡å·²çŸ¥æ´»æ€§ç‰©è´¨ä¸­ä¼ æ’­ä¿¡å·ï¼Œå¹¶åˆ©ç”¨iterative seed refinementç­–ç•¥æ ¹æ®local false discovery rateé€æ­¥å¢åŠ å€™é€‰åˆ†å­ã€‚è¿™ç§æœºåˆ¶åœ¨æ‰©å¤§æœç´¢èŒƒå›´çš„åŒæ—¶ï¼Œæœ‰æ•ˆæ§åˆ¶äº†ç”±æ‹“æ‰‘åå·®å’Œè¿‡åº¦æ‰©å¼ å¼•èµ·çš„false positiveã€‚åœ¨DUD-Eå’ŒZINCæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSubDyveåœ¨BEDROCå’ŒEF1%æŒ‡æ ‡ä¸Šåˆ†åˆ«å®ç°äº†é«˜è¾¾+34.0å’Œ+24.6çš„æ€§èƒ½æå‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æŒ‡çº¹æˆ–åµŒå…¥æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ºåœ¨æå°‘æ ‡ç­¾æ¡ä»¶ä¸‹è¿›è¡Œé«˜æ•ˆä¸”ç²¾å‡†çš„å¤§è§„æ¨¡åŒ–å­¦åº“ç­›é€‰æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.16273v1",
      "published_date": "2025-09-18 14:48:12 UTC",
      "updated_date": "2025-09-18 14:48:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:23:45.800373+00:00"
    },
    {
      "arxiv_id": "2509.15011v2",
      "title": "Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation",
      "title_zh": "é€è¿‡æ•£å°„ï¼šé‡æ–°å®¡è§†ç”¨äºé€¼çœŸæ°´ä¸‹å›¾åƒç”Ÿæˆçš„æˆåƒæ¨¡å‹",
      "authors": [
        "Vasiliki Ismiroglou",
        "Malte Pedersen",
        "Stefan H. Bengtson",
        "Andreas Aakerberg",
        "Thomas B. Moeslund"
      ],
      "abstract": "In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model's ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82.5% by survey participants. Data and code can be accessed on the project page: vap.aau.dk/sea-ing-through-scattered-rays.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ°´ä¸‹å›¾åƒæˆåƒæ¨¡å‹åœ¨æ¨¡æ‹Ÿé«˜æµŠåº¦ç¯å¢ƒæ—¶å¾€å¾€å¿½ç•¥å‰å‘æ•£å°„å’Œéå‡åŒ€ä»‹è´¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ”¹è¿›çš„åˆæˆæ•°æ®ç”Ÿæˆæµç¨‹ã€‚è¯¥æµç¨‹é€šè¿‡å¼•å…¥é€šå¸¸è¢«çœç•¥çš„å‰å‘æ•£å°„(forward scattering)é¡¹å¹¶è€ƒè™‘éå‡åŒ€ä»‹è´¨(nonuniform medium)ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹æ•æ‰éšè·ç¦»å˜åŒ–çš„èƒ½è§åº¦æŸå¤±çš„èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†åœ¨å—æ§æµŠåº¦æ¡ä»¶ä¸‹é‡‡é›†çš„BUCKETæ•°æ®é›†ï¼Œä»¥æä¾›çœŸå®çš„æµ‘æµŠå›¾åƒåŠå…¶å¯¹åº”çš„å‚è€ƒå›¾åƒã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰å®šæ€§è¯„ä¼°ä¸Šä¼˜äºå‚è€ƒæ¨¡å‹ï¼Œå°¤å…¶åœ¨æµŠåº¦å¢åŠ æ—¶è¡¨ç°çªå‡ºï¼Œåœ¨ç”¨æˆ·è°ƒç ”ä¸­è·å¾—äº†82.5%çš„é€‰æ‹©ç‡ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæå‡æ°´ä¸‹å›¾åƒåˆæˆçš„çœŸå®æ„Ÿä»¥åŠç›¸å…³è§†è§‰ç®—æ³•çš„è®­ç»ƒè´¨é‡æä¾›äº†é‡è¦çš„ç†è®ºä¸æ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15011v2",
      "published_date": "2025-09-18 14:42:24 UTC",
      "updated_date": "2025-09-19 07:43:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:23:57.993886+00:00"
    },
    {
      "arxiv_id": "2509.14998v1",
      "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making",
      "title_zh": "é¢å‘åŒ»ç–—å†³ç­–å¢å¼ºçš„çŸ¥è¯†é©±åŠ¨å¤§è¯­è¨€æ¨¡å‹è‡ªé€‚åº”åä½œ",
      "authors": [
        "Xiao Wu",
        "Ting-Zhu Huang",
        "Liang-Jian Deng",
        "Yanyuan Qiao",
        "Imran Razzak",
        "Yutong Xie"
      ],
      "abstract": "Medical decision-making often involves integrating knowledge from multiple clinical specialties, typically achieved through multidisciplinary teams. Inspired by this collaborative process, recent work has leveraged large language models (LLMs) in multi-agent collaboration frameworks to emulate expert teamwork. While these approaches improve reasoning through agent interaction, they are limited by static, pre-assigned roles, which hinder adaptability and dynamic knowledge integration. To address these limitations, we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that enables LLM agents to dynamically form and expand expert teams based on the evolving diagnostic context. KAMAC begins with one or more expert agents and then conducts a knowledge-driven discussion to identify and fill knowledge gaps by recruiting additional specialists as needed. This supports flexible, scalable collaboration in complex clinical scenarios, with decisions finalized through reviewing updated agent comments. Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty expertise. Our code is publicly available at: https://github.com/XiaoXiao-Woo/KAMAC.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KAMAC (Knowledge-driven Adaptive Multi-Agent Collaboration framework)ï¼Œè¿™æ˜¯ä¸€ä¸ªçŸ¥è¯†é©±åŠ¨çš„è‡ªé€‚åº”å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿå¤šå­¦ç§‘ä¸´åºŠå›¢é˜Ÿæ¥å¢å¼ºåŒ»ç–—å†³ç­–èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¤šæ™ºèƒ½ä½“æ¡†æ¶å—é™äºé¢„è®¾é™æ€è§’è‰²ã€ç¼ºä¹çµæ´»æ€§å’ŒåŠ¨æ€çŸ¥è¯†é›†æˆèƒ½åŠ›çš„é—®é¢˜ï¼ŒKAMAC å…è®¸æ™ºèƒ½ä½“æ ¹æ®ä¸æ–­æ¼”å˜çš„è¯Šæ–­èƒŒæ™¯åŠ¨æ€ç»„å»ºå¹¶æ‰©å±•ä¸“å®¶å›¢é˜Ÿã€‚è¯¥æ¡†æ¶ä»åˆå§‹ä¸“å®¶æ™ºèƒ½ä½“å¼€å§‹ï¼Œé€šè¿‡çŸ¥è¯†é©±åŠ¨çš„è®¨è®ºè¯†åˆ«çŸ¥è¯†å·®è·ï¼Œå¹¶åœ¨éœ€è¦æ—¶æ‹›å‹Ÿé¢å¤–çš„ä¸“ç§‘åŒ»ç”Ÿæ™ºèƒ½ä½“ï¼Œå®ç°äº†å¤æ‚ä¸´åºŠåœºæ™¯ä¸‹çµæ´»ä¸”å¯æ‰©å±•çš„åä½œæ¨¡å¼ã€‚æœ€ç»ˆå†³ç­–é€šè¿‡å®¡æŸ¥æ›´æ–°åçš„æ™ºèƒ½ä½“æ„è§æ¥å®Œæˆï¼Œå¹¶åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•ŒåŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKAMAC åœ¨ç™Œç—‡é¢„åç­‰éœ€è¦åŠ¨æ€è·¨å­¦ç§‘ä¸“ä¸šçŸ¥è¯†çš„å¤æ‚åœºæ™¯ä¸­ï¼Œæ˜¾è‘—ä¼˜äºå•æ™ºèƒ½ä½“åŠç°æœ‰çš„å…ˆè¿›å¤šæ™ºèƒ½ä½“æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "The paper has been accepted to the EMNLP 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2509.14998v1",
      "published_date": "2025-09-18 14:33:36 UTC",
      "updated_date": "2025-09-18 14:33:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:23:59.896709+00:00"
    },
    {
      "arxiv_id": "2509.14987v1",
      "title": "Blockchain-Enabled Explainable AI for Trusted Healthcare Systems",
      "title_zh": "é¢å‘å¯ä¿¡åŒ»ç–—ç³»ç»Ÿçš„åŒºå—é“¾èµ‹èƒ½å¯è§£é‡Šäººå·¥æ™ºèƒ½",
      "authors": [
        "Md Talha Mohsin"
      ],
      "abstract": "This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF) for healthcare systems to tackle two essential challenges confronting health information networks: safe data exchange and comprehensible AI-driven clinical decision-making. Our architecture incorporates blockchain, ensuring patient records are immutable, auditable, and tamper-proof, alongside Explainable AI (XAI) methodologies that yield transparent and clinically relevant model predictions. By incorporating security assurances and interpretability requirements into a unified optimization pipeline, BXHF ensures both data-level trust (by verified and encrypted record sharing) and decision-level trust (with auditable and clinically aligned explanations). Its hybrid edge-cloud architecture allows for federated computation across different institutions, enabling collaborative analytics while protecting patient privacy. We demonstrate the framework's applicability through use cases such as cross-border clinical research networks, uncommon illness detection and high-risk intervention decision support. By ensuring transparency, auditability, and regulatory compliance, BXHF improves the credibility, uptake, and effectiveness of AI in healthcare, laying the groundwork for safer and more reliable clinical decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åŒºå—é“¾é›†æˆå¯è§£é‡Šäººå·¥æ™ºèƒ½æ¡†æ¶ (Blockchain-Integrated Explainable AI Framework, BXHF)ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—ä¿¡æ¯ç½‘ç»œä¸­æ•°æ®å®‰å…¨äº¤æ¢å’Œä¸´åºŠå†³ç­–å¯ç†è§£æ€§è¿™ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¶æ„é€šè¿‡å¼•å…¥åŒºå—é“¾ (Blockchain) æŠ€æœ¯ç¡®ä¿æ‚£è€…è®°å½•çš„ä¸å¯ç¯¡æ”¹æ€§å’Œå¯å®¡è®¡æ€§ï¼ŒåŒæ—¶åˆ©ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI, XAI) æ–¹æ³•æä¾›é€æ˜ä¸”å…·å¤‡ä¸´åºŠç›¸å…³æ€§çš„æ¨¡å‹é¢„æµ‹ã€‚BXHF å°†å®‰å…¨ä¿è¯ä¸å¯è§£é‡Šæ€§éœ€æ±‚æ•´åˆè¿›ç»Ÿä¸€çš„ä¼˜åŒ–æµç¨‹ä¸­ï¼Œä»è€Œå®ç°äº†æ•°æ®å±‚é¢çš„éªŒè¯åŠ å¯†å’Œå†³ç­–å±‚é¢çš„ä¸´åºŠå¯¹é½è§£é‡Šã€‚å…¶é‡‡ç”¨çš„æ··åˆè¾¹ç¼˜äº‘ (Hybrid Edge-Cloud) æ¶æ„æ”¯æŒè·¨æœºæ„çš„è”é‚¦è®¡ç®— (Federated Computation)ï¼Œåœ¨ä¿æŠ¤æ‚£è€…éšç§çš„åŒæ—¶å®ç°äº†åä½œåˆ†æã€‚é€šè¿‡å¯¹è·¨å¢ƒä¸´åºŠç ”ç©¶å’Œç½•è§ç—…æ£€æµ‹ç­‰æ¡ˆä¾‹çš„åº”ç”¨æ¼”ç¤ºï¼Œè¯¥æ¡†æ¶è¯æ˜äº†å…¶åœ¨æé«˜é€æ˜åº¦ã€å¯å®¡è®¡æ€§å’Œç›‘ç®¡åˆè§„æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶æœ€ç»ˆä¸ºæ„å»ºæ›´å®‰å…¨ã€æ›´å¯é çš„ä¸´åºŠå†³ç­–ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ï¼Œå¢å¼ºäº†åŒ»ç–— AI çš„å¯ä¿¡åº¦ã€é‡‡çº³ç‡ä¸å®é™…æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "6 Pages, 4 Figures",
      "pdf_url": "https://arxiv.org/pdf/2509.14987v1",
      "published_date": "2025-09-18 14:17:19 UTC",
      "updated_date": "2025-09-18 14:17:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:04.296533+00:00"
    },
    {
      "arxiv_id": "2509.15283v1",
      "title": "Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges",
      "title_zh": "è¯„ä¼°æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚ç¼–ç¨‹æŒ‘æˆ˜ä¸­çš„å±€é™æ€§",
      "authors": [
        "Kadin Matotek",
        "Heather Cassel",
        "Md Amiruzzaman",
        "Linh B. Ngo"
      ],
      "abstract": "This study examines the performance of today's open-source, locally hosted large-language models (LLMs) in handling complex competitive programming tasks with extended problem descriptions and contexts. Building on the original Framework for AI-driven Code Generation Evaluation (FACE), the authors retrofit the pipeline to work entirely offline through the Ollama runtime, collapsing FACE's sprawling per-problem directory tree into a handful of consolidated JSON files, and adding robust checkpointing so multi-day runs can resume after failures. The enhanced framework generates, submits, and records solutions for the full Kattis corpus of 3,589 problems across eight code-oriented models ranging from 6.7-9 billion parameters. The submission results show that the overall pass@1 accuracy is modest for the local models, with the best models performing at approximately half the acceptance rate of the proprietary models, Gemini 1.5 and ChatGPT-4. These findings expose a persistent gap between private, cost-controlled LLM deployments and state-of-the-art proprietary services, yet also highlight the rapid progress of open models and the practical benefits of an evaluation workflow that organizations can replicate on in-house hardware.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¼€æºä¸”æœ¬åœ°éƒ¨ç½²çš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†åŒ…å«å¤æ‚æè¿°çš„ç«äº‰æ€§ç¼–ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ”¹è¿› AI é©±åŠ¨ä»£ç ç”Ÿæˆè¯„ä¼°æ¡†æ¶ (FACE)ï¼Œåˆ©ç”¨ Ollama è¿è¡Œç¯å¢ƒå®ç°äº†å®Œå…¨ç¦»çº¿çš„å·¥ä½œæµï¼Œå¹¶å¢åŠ äº†æ£€æŸ¥ç‚¹ (checkpointing) åŠŸèƒ½ä»¥ç¡®ä¿é•¿å‘¨æœŸè¿è¡Œçš„ç¨³å®šæ€§ã€‚å®éªŒè¯„ä¼°äº† 8 ä¸ªå‚æ•°é‡åœ¨ 6.7-9 billion ä¹‹é—´çš„ä»£ç æ¨¡å‹ï¼Œæ¶µç›–äº†åŒ…å« 3,589 ä¸ªé—®é¢˜çš„ Kattis å®Œæ•´è¯­æ–™åº“ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ¬åœ°æ¨¡å‹çš„æ•´ä½“ pass@1 å‡†ç¡®ç‡ä¾ç„¶æœ‰é™ï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹ä»…è¾¾åˆ°é—­æºæ¨¡å‹ Gemini 1.5 å’Œ ChatGPT-4 æ¥å—ç‡çš„ä¸€åŠå·¦å³ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†ç§æœ‰åŒ–éƒ¨ç½² LLMs ä¸é¡¶å°–å•†ä¸šæœåŠ¡ä¹‹é—´æŒç»­å­˜åœ¨çš„å·®è·ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç ”ç©¶ä¹Ÿå¼ºè°ƒäº†å¼€æºæ¨¡å‹çš„å¿«é€Ÿè¿›æ­¥ä»¥åŠç»„ç»‡åœ¨è‡ªæœ‰ç¡¬ä»¶ä¸Šå®æ–½å¯å¤åˆ¶è¯„ä¼°æµç¨‹çš„å®è·µä»·å€¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "Comments: 16 pages, 3 figures, 8 tables, accepted to CCSC Eastern 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.15283v1",
      "published_date": "2025-09-18 14:13:30 UTC",
      "updated_date": "2025-09-18 14:13:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:05.193539+00:00"
    },
    {
      "arxiv_id": "2509.14984v1",
      "title": "The Role of Touch: Towards Optimal Tactile Sensing Distribution in Anthropomorphic Hands for Dexterous In-Hand Manipulation",
      "title_zh": "è§¦è§‰çš„ä½œç”¨ï¼šé¢å‘ä»¿äººæ‰‹çµå·§æ‰‹å†…æ“çºµçš„æœ€ä¼˜è§¦è§‰ä¼ æ„Ÿåˆ†å¸ƒ",
      "authors": [
        "JoÃ£o DamiÃ£o Almeida",
        "Egidio Falotico",
        "Cecilia Laschi",
        "JosÃ© Santos-Victor"
      ],
      "abstract": "In-hand manipulation tasks, particularly in human-inspired robotic systems, must rely on distributed tactile sensing to achieve precise control across a wide variety of tasks. However, the optimal configuration of this network of sensors is a complex problem, and while the fingertips are a common choice for placing sensors, the contribution of tactile information from other regions of the hand is often overlooked. This work investigates the impact of tactile feedback from various regions of the fingers and palm in performing in-hand object reorientation tasks. We analyze how sensory feedback from different parts of the hand influences the robustness of deep reinforcement learning control policies and investigate the relationship between object characteristics and optimal sensor placement. We identify which tactile sensing configurations contribute to improving the efficiency and accuracy of manipulation. Our results provide valuable insights for the design and use of anthropomorphic end-effectors with enhanced manipulation capabilities.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨ä»¿äººæœºå™¨äººæ‰‹(Anthropomorphic Hands)ä¸­åˆ†å¸ƒå¼çš„è§¦è§‰æ„ŸçŸ¥(Tactile Sensing)å¯¹å®ç°çµå·§æ‰‹å†…æ“ä½œ(Dexterous In-Hand Manipulation)çš„å…³é”®ä½œç”¨ã€‚è™½ç„¶æŒ‡å°–é€šå¸¸æ˜¯ä¼ æ„Ÿå™¨å¸ƒç½®çš„é¦–é€‰ï¼Œä½†è®ºæ–‡æŒ‡å‡ºæ‰‹éƒ¨å…¶ä»–åŒºåŸŸçš„è§¦è§‰ä¿¡æ¯è´¡çŒ®å¾€å¾€è¢«å¿½è§†ã€‚ç ”ç©¶å›¢é˜Ÿåˆ†æäº†æ‰‹æŒ‡ä¸åŒéƒ¨ä½åŠæ‰‹æŒçš„è§¦è§‰åé¦ˆåœ¨æ‰§è¡Œç‰©ä½“é‡æ–°å®šå‘(Object Reorientation)ä»»åŠ¡ä¸­çš„å½±å“ï¼Œå¹¶é‡‡ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning)æ§åˆ¶ç­–ç•¥è€ƒå¯Ÿäº†ä¼ æ„Ÿå™¨é…ç½®å¯¹ç­–ç•¥ç¨³å¥æ€§(Robustness)çš„ä½œç”¨ã€‚é€šè¿‡è°ƒæŸ¥ç‰©ä½“ç‰¹æ€§ä¸æœ€ä½³ä¼ æ„Ÿå™¨å¸ƒç½®ä¹‹é—´çš„å…³ç³»ï¼Œè¯¥ç ”ç©¶ç¡®å®šäº†å“ªäº›è§¦è§‰æ„ŸçŸ¥é…ç½®èƒ½æœ‰æ•ˆæå‡æ“ä½œçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®éªŒç»“æœä¸ºè®¾è®¡å…·å¤‡å¢å¼ºæ“ä½œèƒ½åŠ›çš„ç±»äººæœ«ç«¯æ‰§è¡Œå™¨(End-effectors)æä¾›äº†é‡è¦è§è§£ï¼Œä¸ºä¼˜åŒ–æœºå™¨äººæ‰‹çš„æ„ŸçŸ¥åˆ†å¸ƒå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14984v1",
      "published_date": "2025-09-18 14:13:26 UTC",
      "updated_date": "2025-09-18 14:13:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:24.581477+00:00"
    },
    {
      "arxiv_id": "2509.14980v1",
      "title": "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation",
      "title_zh": "M4Diffuserï¼šç»“åˆå¯æ“ä½œæ€§æ„ŸçŸ¥æ§åˆ¶çš„å¤šè§†å›¾æ‰©æ•£ç­–ç•¥ï¼Œå®ç°é²æ£’çš„ç§»åŠ¨æ“ä½œ",
      "authors": [
        "Ju Dong",
        "Lei Zhang",
        "Liding Zhang",
        "Yao Ling",
        "Yu Fu",
        "Kaixin Bai",
        "ZoltÃ¡n-Csaba MÃ¡rton",
        "Zhenshan Bing",
        "Zhaopeng Chen",
        "Alois Christian Knoll",
        "Jianwei Zhang"
      ],
      "abstract": "Mobile manipulation requires the coordinated control of a mobile base and a robotic arm while simultaneously perceiving both global scene context and fine-grained object details. Existing single-view approaches often fail in unstructured environments due to limited fields of view, exploration, and generalization abilities. Moreover, classical controllers, although stable, struggle with efficiency and manipulability near singularities. To address these challenges, we propose M4Diffuser, a hybrid framework that integrates a Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP (ReM-QP) controller for mobile manipulation. The diffusion policy leverages proprioceptive states and complementary camera perspectives with both close-range object details and global scene context to generate task-relevant end-effector goals in the world frame. These high-level goals are then executed by the ReM-QP controller, which eliminates slack variables for computational efficiency and incorporates manipulability-aware preferences for robustness near singularities. Comprehensive experiments in simulation and real-world environments show that M4Diffuser achieves 7 to 56 percent higher success rates and reduces collisions by 3 to 31 percent over baselines. Our approach demonstrates robust performance for smooth whole-body coordination, and strong generalization to unseen tasks, paving the way for reliable mobile manipulation in unstructured environments. Details of the demo and supplemental material are available on our project website https://sites.google.com/view/m4diffuser.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†M4Diffuserï¼Œè¿™æ˜¯ä¸€ç§å°†Multi-View Diffusion Policyä¸æ–°å‹å¯æ“çºµæ€§æ„ŸçŸ¥äºŒæ¬¡è§„åˆ’(ReM-QP)æ§åˆ¶å™¨ç›¸ç»“åˆçš„æ··åˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç§»åŠ¨æ“ä½œ(Mobile manipulation)åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­çš„æ„ŸçŸ¥å’Œåè°ƒéš¾é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£ç­–ç•¥æ•´åˆå…¨å±€åœºæ™¯ä¸Šä¸‹æ–‡ä¸è¿‘è·ç¦»ç‰©ä½“ç»†èŠ‚ï¼Œç”Ÿæˆä¸–ç•Œåæ ‡ç³»ä¸‹çš„æœ«ç«¯æ‰§è¡Œå™¨ç›®æ ‡ã€‚éšåï¼ŒReM-QPæ§åˆ¶å™¨é€šè¿‡æ¶ˆé™¤æ¾å¼›å˜é‡æå‡è®¡ç®—æ•ˆç‡ï¼Œå¹¶ç»“åˆå¯æ“çºµæ€§æ„ŸçŸ¥åå¥½ç¡®ä¿æœºå™¨äººåœ¨å¥‡å¼‚ç‚¹é™„è¿‘çš„ç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM4Diffuseråœ¨æ¨¡æ‹Ÿå’ŒçœŸå®åœºæ™¯ä¸­çš„æˆåŠŸç‡æ¯”åŸºçº¿æ¨¡å‹é«˜å‡º7%è‡³56%ï¼Œç¢°æ’ç‡é™ä½äº†3%è‡³31%ã€‚è¯¥æ–¹æ³•å®ç°äº†å¹³æ»‘çš„å…¨èº«åè°ƒ(Whole-body coordination)å¹¶è¡¨ç°å‡ºå¯¹æœªçŸ¥ä»»åŠ¡çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„å¯é ç§»åŠ¨æ“ä½œå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://sites.google.com/view/m4diffuser, 10 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.14980v1",
      "published_date": "2025-09-18 14:09:53 UTC",
      "updated_date": "2025-09-18 14:09:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:27.985726+00:00"
    },
    {
      "arxiv_id": "2509.19359v1",
      "title": "Anti-Money Laundering Systems Using Deep Learning",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„åæ´—é’±ç³»ç»Ÿ",
      "authors": [
        "Mashkhal Abdalwahid Sidiq",
        "Yimamu Kirubel Wondaferew"
      ],
      "abstract": "In this paper, we focused on using deep learning methods for detecting money laundering in financial transaction networks, in order to demonstrate that it can be used as a complement or instead of the more commonly used rule-based systems and conventional Anti-Money Laundering (AML) systems. The paper explores the pivotal role played by Anti-Money Laundering (AML) activities in the global financial industry. It underscores the drawbacks of conventional AML systems, which exhibit high rates of false positives and lack the sophistication to uncover intricate money laundering schemes. To tackle these challenges, the paper proposes an advanced AML system that capitalizes on link analysis using deep learning techniques. At the heart of this system lies the utilization of centrality algorithms like Degree Centrality, Closeness Centrality, Betweenness Centrality, and PageRank. These algorithms enhance the system's capability to identify suspicious activities by examining the influence and interconnections within networks of financial transactions. The significance of Anti-Money Laundering (AML) efforts within the global financial sector is discussed in this paper. It highlights the limitations of traditional AML systems. The results showed the practicality and superiority of the new implementation of the GCN model, which is a preferable method for connectively structured data, meaning that a transaction or account is analyzed in the context of its financial environment. In addition, the paper delves into the prospects of Anti-Money Laundering (AML) efforts, proposing the integration of emerging technologies such as deep learning and centrality algorithms. This integration holds promise for enhancing the effectiveness of AML systems by refining their capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨é‡‘èäº¤æ˜“ç½‘ç»œä¸­ä½¿ç”¨ Deep Learning æ–¹æ³•è¿›è¡Œæ´—é’±æ£€æµ‹ï¼Œæ—¨åœ¨å¼¥è¡¥ä¼ ç»Ÿ Anti-Money Laundering (AML) ç³»ç»Ÿè¯¯æŠ¥ç‡é«˜ä¸”éš¾ä»¥è¯†åˆ«å¤æ‚æ´—é’±æ¨¡å¼çš„ç¼ºé™·ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆé“¾æ¥åˆ†æä¸æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å…ˆè¿›ç³»ç»Ÿï¼Œé€šè¿‡åˆ†æé‡‘èç½‘ç»œä¸­çš„ç›¸äº’å…³è”æ¥è¯†åˆ«å¯ç–‘æ´»åŠ¨ã€‚ç³»ç»Ÿæ ¸å¿ƒåº”ç”¨äº† Degree Centralityã€Closeness Centralityã€Betweenness Centrality å’Œ PageRank ç­‰ä¸­å¿ƒæ€§ç®—æ³•ï¼Œä»¥å¢å¼ºå¯¹ç½‘ç»œä¸­èŠ‚ç‚¹å½±å“åŠ›çš„è§£è¯»ã€‚ç ”ç©¶é‡ç‚¹éªŒè¯äº† GCN æ¨¡å‹åœ¨å¤„ç†è¿æ¥ç»“æ„åŒ–æ•°æ®æ–¹é¢çš„å®ç”¨æ€§ï¼Œè¯¥æ¨¡å‹å…è®¸åœ¨ç‰¹å®šé‡‘èç¯å¢ƒèƒŒæ™¯ä¸‹å¯¹äº¤æ˜“æˆ–è´¦æˆ·è¿›è¡Œä¸Šä¸‹æ–‡åˆ†æã€‚å®éªŒç»“æœè¯æ˜äº† GCN æ¨¡å‹ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ AML ç³»ç»Ÿçš„è¯†åˆ«ç²¾åº¦ã€‚è¿™ç§æ•´åˆæ–°å…´æŠ€æœ¯ä¸å›¾ç®—æ³•çš„æ–¹æ³•ï¼Œä¸ºæå‡å…¨çƒé‡‘èè¡Œä¸šåæ´—é’±å·¥ä½œçš„æœ‰æ•ˆæ€§æä¾›äº†æå…·æ½œåŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.19359v1",
      "published_date": "2025-09-18 14:06:40 UTC",
      "updated_date": "2025-09-18 14:06:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:30.962138+00:00"
    },
    {
      "arxiv_id": "2509.14966v1",
      "title": "RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching",
      "title_zh": "RoboEyeï¼šåˆ©ç”¨é€‰æ‹©æ€§ 3D å‡ ä½•å…³é”®ç‚¹åŒ¹é…å¢å¼º 2D æœºå™¨äººç›®æ ‡è¯†åˆ«",
      "authors": [
        "Xingwu Zhang",
        "Guanxuan Li",
        "Zhuocheng Zhang",
        "Zijun Long"
      ],
      "abstract": "The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult. As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features. Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps. In the first stage, we train a large vision model to extract 2D features for generating candidate rankings. A lightweight 3D-feature-awareness module then estimates 3D feature quality and predicts whether 3D re-ranking is necessary, preventing performance degradation and avoiding unnecessary computation. When invoked, the second stage uses our robot 3D retrieval transformer, comprising a 3D feature extractor that produces geometry-aware dense features and a keypoint-based matcher that computes keypoint-correspondence confidences between query and reference images instead of conventional cosine-similarity scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images, avoiding reliance on explicit 3D inputs and reducing deployment costs. The code used in this paper is publicly available at: https://github.com/longkukuhi/RoboEye.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RoboEyeï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§è§„æ¨¡ç”µå­å•†åŠ¡ä»“åº“ä¸­å› å•†å“ç§ç±»ç¹å¤šã€è§†è§‰ç›¸ä¼¼æ€§é«˜åŠé®æŒ¡ç­‰å› ç´ å¯¼è‡´ 2D è¯†åˆ«æ€§èƒ½ä¸‹é™çš„ä¸¤é˜¶æ®µç‰©ä½“è¯†åˆ«æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆVision Modelï¼‰æå– 2D è¯­ä¹‰ç‰¹å¾ä»¥ç”Ÿæˆåˆæ­¥å€™é€‰æ’åï¼Œéšåé€šè¿‡è½»é‡åŒ–çš„ 3D ç‰¹å¾æ„ŸçŸ¥æ¨¡å—ï¼ˆ3D-feature-awareness moduleï¼‰åŠ¨æ€åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œ 3D é‡æ’ï¼Œä»è€Œä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚åœ¨é‡æ’é˜¶æ®µï¼Œæ¡†æ¶é‡‡ç”¨ 3D æ£€ç´¢ Transformerï¼ˆ3D retrieval transformerï¼‰æå–å‡ ä½•æ„ŸçŸ¥ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨åŸºäºå…³é”®ç‚¹çš„åŒ¹é…å™¨ï¼ˆkeypoint-based matcherï¼‰è®¡ç®—å¯¹åº”ç½®ä¿¡åº¦ï¼Œå–ä»£ä¼ ç»Ÿçš„ä½™å¼¦ç›¸ä¼¼åº¦åº¦é‡ã€‚è¯¥æ–¹æ³•ä»…éœ€ RGB å›¾åƒè¾“å…¥å³å¯å®ç° 3D æ¨ç†ï¼Œæœ‰æ•ˆé™ä½äº†éƒ¨ç½²æˆæœ¬å¹¶å¼¥è¡¥äº†è®­ç»ƒä¸å®åœ°åº”ç”¨é—´çš„é¢†åŸŸå·®è·ã€‚å®éªŒè¡¨æ˜ï¼ŒRoboEye åœ¨ Recall@1 æŒ‡æ ‡ä¸Šæ¯”ç°æœ‰å…ˆè¿›æ¨¡å‹ RoboLLM æå‡äº† 7.1%ï¼Œæ˜¾è‘—å¢å¼ºäº†æœºå™¨äººåœ¨å¤æ‚ã€æ‚ä¹±ç¯å¢ƒä¸‹çš„ç‰©ä½“è¯†åˆ«ç²¾åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14966v1",
      "published_date": "2025-09-18 13:59:24 UTC",
      "updated_date": "2025-09-18 13:59:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:35.757629+00:00"
    },
    {
      "arxiv_id": "2509.19358v1",
      "title": "Benchmarking and Improving LLM Robustness for Personalized Generation",
      "title_zh": "ä¸ªæ€§åŒ–ç”Ÿæˆä¸­ LLM é²æ£’æ€§çš„åŸºå‡†è¯„æµ‹ä¸æå‡",
      "authors": [
        "Chimaobi Okite",
        "Naihao Deng",
        "Kiran Bodipati",
        "Huaidian Hou",
        "Joyce Chai",
        "Rada Mihalcea"
      ],
      "abstract": "Recent years have witnessed a growing interest in personalizing the responses of large language models (LLMs). While existing evaluations primarily focus on whether a response aligns with a user's preferences, we argue that factuality is an equally important yet often overlooked dimension. In the context of personalization, we define a model as robust if its responses are both factually accurate and align with the user preferences. To assess this, we introduce PERG, a scalable framework for evaluating robustness in LLMs, along with a new dataset, PERGData. We evaluate fourteen models from five different model families using different prompting methods. Our findings show that current LLMs struggle with robust personalization: even the strongest models (GPT-4.1, LLaMA3-70B) fail to maintain correctness in 5% of previously successful cases without personalization, while smaller models (e.g., 7B-scale) can fail more than 20% of the time. Further analysis reveals that robustness is significantly affected by the nature of the query and the type of user preference. To mitigate these failures, we propose Pref-Aligner, a two-stage approach that improves robustness by an average of 25% across models. Our work highlights critical gaps in current evaluation practices and introduces tools and metrics to support more reliable, user-aligned LLM deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸ªæ€§åŒ–ç”Ÿæˆ(Personalized Generation)ä¸­å¿½è§†äº‹å®å‡†ç¡®æ€§çš„é—®é¢˜ï¼Œå®šä¹‰äº†é²æ£’æ€§(Robustness)æ ‡å‡†ï¼Œå³æ¨¡å‹å“åº”éœ€åŒæ—¶æ»¡è¶³äº‹å®å‡†ç¡®å¹¶ç¬¦åˆç”¨æˆ·åå¥½ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªåä¸ºPERGçš„å¯æ‰©å±•è¯„ä¼°æ¡†æ¶ä»¥åŠé…å¥—æ•°æ®é›†PERGDataï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸ªæ€§åŒ–èƒŒæ™¯ä¸‹çš„è¡¨ç°ã€‚é€šè¿‡å¯¹14ä¸ªæ¨¡å‹çš„ç³»ç»Ÿè¯„ä¼°å‘ç°ï¼Œå½“å‰çš„LLMsåœ¨é²æ£’ä¸ªæ€§åŒ–æ–¹é¢æ™®éé¢ä¸´æŒ‘æˆ˜ï¼Œå³ä½¿æ˜¯GPT-4.1å’ŒLLaMA3-70Bç­‰é¡¶å°–æ¨¡å‹åœ¨ä¸ªæ€§åŒ–è®¾ç½®ä¸‹ä¹Ÿä¼šå‡ºç°5%çš„æ­£ç¡®æ€§ä¸‹é™ï¼Œè€Œå°å‹æ¨¡å‹çš„å¤±è´¥ç‡ç”šè‡³è¶…è¿‡20%ã€‚åˆ†ææ˜¾ç¤ºï¼Œé²æ£’æ€§å—åˆ°æŸ¥è¯¢æ€§è´¨å’Œç”¨æˆ·åå¥½ç±»å‹çš„æ˜¾è‘—å½±å“ã€‚ä¸ºäº†ç¼“è§£è¿™äº›å¤±æ•ˆæƒ…å†µï¼Œç ”ç©¶æå‡ºäº†åä¸ºPref-Alignerçš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå°†æ¨¡å‹çš„é²æ£’æ€§å¹³å‡æå‡äº†25%ã€‚è¯¥é¡¹å·¥ä½œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°å®è·µçš„ç©ºç™½ï¼Œå¹¶ä¸ºå®ç°æ›´å¯é ã€æ›´è´´åˆç”¨æˆ·çš„LLMéƒ¨ç½²æä¾›äº†å…³é”®çš„å·¥å…·å’ŒæŒ‡æ ‡æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "First draft. First camera-ready version",
      "pdf_url": "https://arxiv.org/pdf/2509.19358v1",
      "published_date": "2025-09-18 13:56:14 UTC",
      "updated_date": "2025-09-18 13:56:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:39.553875+00:00"
    },
    {
      "arxiv_id": "2509.14963v1",
      "title": "Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles",
      "title_zh": "å®šé‡åŒæè®ºè¯ä¸­çš„é›†åˆè´¡çŒ®å‡½æ•°åŠå…¶å‡†åˆ™",
      "authors": [
        "Filip Naudot",
        "Andreas BrÃ¤nnstrÃ¶m",
        "VicenÃ§ Torra",
        "Timotheus Kampik"
      ],
      "abstract": "We present functions that quantify the contribution of a set of arguments in quantitative bipolar argumentation graphs to (the final strength of) an argument of interest, a so-called topic. Our set contribution functions are generalizations of existing functions that quantify the contribution of a single contributing argument to a topic. Accordingly, we generalize existing contribution function principles for set contribution functions and provide a corresponding principle-based analysis. We introduce new principles specific to set-based functions that focus on properties pertaining to the interaction of arguments within a set. Finally, we sketch how the principles play out across different set contribution functions given a recommendation system application scenario.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®šé‡åŒå‘è®ºè¯(Quantitative Bipolar Argumentation)å›¾ï¼Œæå‡ºäº†æ—¨åœ¨é‡åŒ–ä¸€ç»„è®ºæ®å¯¹ç‰¹å®šç›®æ ‡è®ºæ®ï¼ˆå³è¯é¢˜ï¼‰è´¡çŒ®ç¨‹åº¦çš„é›†åˆè´¡çŒ®å‡½æ•°(set contribution functions)ã€‚è¿™äº›å‡½æ•°æ˜¯ç°æœ‰å•è®ºæ®è´¡çŒ®åº¦é‡æ–¹æ³•çš„æ³›åŒ–æ‰©å±•ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°å¤šä¸ªè®ºæ®åœ¨å¤æ‚è®ºè¯ç½‘ç»œä¸­çš„ååŒå½±å“ã€‚ç ”ç©¶è€…å°†ç°æœ‰çš„è´¡çŒ®å‡½æ•°åŸåˆ™æ¨å¹¿è‡³é›†åˆè´¡çŒ®å‡½æ•°ï¼Œå¹¶è¿›è¡Œäº†ç³»ç»Ÿçš„åŸºäºåŸåˆ™çš„åˆ†æ(principle-based analysis)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸“é—¨é’ˆå¯¹é›†åˆå‡½æ•°çš„æ–°åŸåˆ™ï¼Œé‡ç‚¹å…³æ³¨é›†åˆå†…è®ºæ®ä¹‹é—´çš„äº¤äº’å±æ€§ã€‚æœ€åï¼Œé€šè¿‡æ¨èç³»ç»Ÿ(recommendation system)çš„åº”ç”¨åœºæ™¯ï¼Œå±•ç¤ºäº†ä¸åŒåŸåˆ™åœ¨å®é™…è¯„ä¼°ä¸­çš„å…·ä½“è¡¨ç°ã€‚è¯¥å·¥ä½œä¸ºæ·±å…¥ç†è§£å®šé‡è®ºè¯ä¸­çš„é›†åˆæ•ˆåº”å’Œå¯è§£é‡Šæ€§æä¾›äº†åšå®çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14963v1",
      "published_date": "2025-09-18 13:52:53 UTC",
      "updated_date": "2025-09-18 13:52:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:41.757320+00:00"
    },
    {
      "arxiv_id": "2509.14959v1",
      "title": "Discrete optimal transport is a strong audio adversarial attack",
      "title_zh": "ç¦»æ•£æœ€ä¼˜ä¼ è¾“æ˜¯ä¸€ç§å¼ºåŠ›çš„éŸ³é¢‘å¯¹æŠ—æ”»å‡»",
      "authors": [
        "Anton Selitskiy",
        "Akib Shahriyar",
        "Jishnuraj Prakasan"
      ],
      "abstract": "In this paper, we show that discrete optimal transport (DOT) is an effective black-box adversarial attack against modern audio anti-spoofing countermeasures (CMs). Our attack operates as a post-processing, distribution-alignment step: frame-level WavLM embeddings of generated speech are aligned to an unpaired bona fide pool via entropic OT and a top-$k$ barycentric projection, then decoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with AASIST baselines, DOT yields consistently high equal error rate (EER) across datasets and remains competitive after CM fine-tuning, outperforming several conventional attacks in cross-dataset transfer. Ablation analysis highlights the practical impact of vocoder overlap. Results indicate that distribution-level alignment is a powerful and stable attack surface for deployed CMs.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº†ç¦»æ•£æœ€ä¼˜ä¼ è¾“(Discrete Optimal Transport, DOT)æ˜¯ä¸€ç§é’ˆå¯¹ç°ä»£éŸ³é¢‘é˜²ä¼ªå¯¹ç­–(Countermeasures, CMs)çš„æœ‰æ•ˆé»‘ç›’å¯¹æŠ—æ”»å‡»æ–¹å¼ã€‚è¯¥æ”»å‡»é‡‡ç”¨åå¤„ç†çš„åˆ†å¸ƒå¯¹é½æ­¥éª¤ï¼Œåˆ©ç”¨ç†µæœ€ä¼˜ä¼ è¾“(Entropic OT)å’Œtop-ké‡å¿ƒæŠ•å½±(Barycentric Projection)å°†ç”Ÿæˆè¯­éŸ³çš„å¸§çº§WavLMåµŒå…¥ä¸çœŸå®è¯­éŸ³æ± è¿›è¡Œå¯¹é½ï¼Œå¹¶ä½¿ç”¨ç¥ç»å£°ç å™¨è¿›è¡Œè§£ç ã€‚åœ¨ASVspoof2019å’ŒASVspoof5æ•°æ®é›†åŠAASISTåŸºå‡†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒDOTåœ¨ä¸åŒæ•°æ®é›†ä¸Šå‡äº§ç”Ÿäº†æŒç»­çš„é«˜ç­‰é”™è¯¯ç‡(Equal Error Rate, EER)ï¼Œå¹¶åœ¨è·¨æ•°æ®é›†è¿ç§»ä¸­ä¼˜äºå¤šç§ä¼ ç»Ÿæ”»å‡»æ‰‹æ®µã€‚æ¶ˆèåˆ†æè¿›ä¸€æ­¥å¼ºè°ƒäº†å£°ç å™¨é‡å çš„å®é™…å½±å“ï¼Œè¯æ˜äº†DOTåœ¨æ¨¡å‹å¾®è°ƒåä»å…·ç«äº‰åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ†å¸ƒå±‚é¢çš„å¯¹é½ä¸ºå·²éƒ¨ç½²çš„é˜²ä¼ªç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¼ºå¤§ä¸”ç¨³å®šçš„æ”»å‡»é¢ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14959v1",
      "published_date": "2025-09-18 13:46:16 UTC",
      "updated_date": "2025-09-18 13:46:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:48.962689+00:00"
    },
    {
      "arxiv_id": "2509.14956v1",
      "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems",
      "title_zh": "é¢å‘å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå®‰å…¨å¯ä¿¡æ™ºèƒ½ä½“ AI çš„å“¨å…µæ™ºèƒ½ä½“",
      "authors": [
        "Diego Gosmar",
        "Deborah A. Dahl"
      ],
      "abstract": "This paper proposes a novel architectural framework aimed at enhancing security and reliability in multi-agent systems (MAS). A central component of this framework is a network of Sentinel Agents, functioning as a distributed security layer that integrates techniques such as semantic analysis via large language models (LLMs), behavioral analytics, retrieval-augmented verification, and cross-agent anomaly detection. Such agents can potentially oversee inter-agent communications, identify potential threats, enforce privacy and access controls, and maintain comprehensive audit records. Complementary to the idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator Agent supervises policy implementation, and manages agent participation. In addition, the Coordinator also ingests alerts from Sentinel Agents. Based on these alerts, it can adapt policies, isolate or quarantine misbehaving agents, and contain threats to maintain the integrity of the MAS ecosystem. This dual-layered security approach, combining the continuous monitoring of Sentinel Agents with the governance functions of Coordinator Agents, supports dynamic and adaptive defense mechanisms against a range of threats, including prompt injection, collusive agent behavior, hallucinations generated by LLMs, privacy breaches, and coordinated multi-agent attacks. In addition to the architectural design, we present a simulation study where 162 synthetic attacks of different families (prompt injection, hallucination, and data exfiltration) were injected into a multi-agent conversational environment. The Sentinel Agents successfully detected the attack attempts, confirming the practical feasibility of the proposed monitoring approach. The framework also offers enhanced system observability, supports regulatory compliance, and enables policy evolution over time.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ—¨åœ¨å¢å¼ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-Agent Systems, MAS)å®‰å…¨æ€§å’Œå¯é æ€§çš„æ–°å‹æ¶æ„æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯Sentinel Agentsç½‘ç»œï¼Œå®ƒä½œä¸ºåˆ†å¸ƒå¼å®‰å…¨å±‚ï¼Œæ•´åˆäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è¯­ä¹‰åˆ†æã€è¡Œä¸ºåˆ†æã€æ£€ç´¢å¢å¼ºéªŒè¯(Retrieval-Augmented Verification)ä»¥åŠè·¨æ™ºèƒ½ä½“å¼‚å¸¸æ£€æµ‹ç­‰æŠ€æœ¯ã€‚è¿™äº›Sentinel Agentsèƒ½å¤Ÿç›‘ç£æ™ºèƒ½ä½“é—´çš„é€šä¿¡ï¼Œè¯†åˆ«æ½œåœ¨å¨èƒå¹¶æ‰§è¡Œéšç§ä¸è®¿é—®æ§åˆ¶ï¼ŒåŒæ—¶ç»´æŠ¤å®Œæ•´çš„å®¡è®¡è®°å½•ã€‚æ­¤å¤–ï¼Œæ¡†æ¶å¼•å…¥äº†Coordinator Agentï¼Œè´Ÿè´£ç›‘ç£æ”¿ç­–å®æ–½ã€ç®¡ç†æ™ºèƒ½ä½“å‚ä¸ï¼Œå¹¶æ ¹æ®Sentinel Agentsæä¾›çš„è­¦æŠ¥åŠ¨æ€è°ƒæ•´æ”¿ç­–ã€éš”ç¦»è¿è§„æ™ºèƒ½ä½“æˆ–éåˆ¶å¨èƒã€‚è¿™ç§åŒå±‚å®‰å…¨æœºåˆ¶èƒ½æœ‰æ•ˆé˜²å¾¡æç¤ºæ³¨å…¥(Prompt Injection)ã€æ™ºèƒ½ä½“åˆè°‹è¡Œä¸ºã€å¹»è§‰(Hallucinations)åŠéšç§æ³„éœ²ç­‰å¤šç§å¨èƒã€‚é€šè¿‡å¯¹162æ¬¡æ¶µç›–ä¸åŒç±»åˆ«çš„åˆæˆæ”»å‡»è¿›è¡Œæ¨¡æ‹Ÿç ”ç©¶ï¼Œå®éªŒç»“æœè¯å®Sentinel Agentsèƒ½æˆåŠŸæ£€æµ‹æ”»å‡»å°è¯•ï¼ŒéªŒè¯äº†è¯¥ç›‘æµ‹æ–¹æ¡ˆçš„å®ç”¨å¯è¡Œæ€§ï¼Œå¹¶æ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„å¯è§‚æµ‹æ€§ä¸åˆè§„æ€§æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.14956v1",
      "published_date": "2025-09-18 13:39:59 UTC",
      "updated_date": "2025-09-18 13:39:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:46.369009+00:00"
    },
    {
      "arxiv_id": "2509.14944v1",
      "title": "Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening",
      "title_zh": "åŸºäºå¤œé—´å‘¼å¸éŸ³çš„å‘¼å¸åŠªåŠ›ä¼°è®¡ç”¨äºé˜»å¡æ€§ç¡çœ å‘¼å¸æš‚åœç­›æŸ¥",
      "authors": [
        "Xiaolei Xu",
        "Chaoyue Niu",
        "Guy J. Brown",
        "Hector Romero",
        "Ning Ma"
      ],
      "abstract": "Obstructive sleep apnoea (OSA) is a prevalent condition with significant health consequences, yet many patients remain undiagnosed due to the complexity and cost of over-night polysomnography. Acoustic-based screening provides a scalable alternative, yet performance is limited by environmental noise and the lack of physiological context. Respiratory effort is a key signal used in clinical scoring of OSA events, but current approaches require additional contact sensors that reduce scalability and patient comfort. This paper presents the first study to estimate respiratory effort directly from nocturnal audio, enabling physiological context to be recovered from sound alone. We propose a latent-space fusion framework that integrates the estimated effort embeddings with acoustic features for OSA detection. Using a dataset of 157 nights from 103 participants recorded in home environments, our respiratory effort estimator achieves a concordance correlation coefficient of 0.48, capturing meaningful respiratory dynamics. Fusing effort and audio improves sensitivity and AUC over audio-only baselines, especially at low apnoea-hypopnoea index thresholds. The proposed approach requires only smartphone audio at test time, which enables sensor-free, scalable, and longitudinal OSA monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é˜»å¡æ€§ç¡çœ å‘¼å¸æš‚åœ (Obstructive sleep apnoea, OSA) çš„ç­›æŸ¥æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„æ•´å¤œå¤šå¯¼ç¡çœ ç›‘æµ‹ (polysomnography) å¤æ‚ä¸”æ˜‚è´µã€‚é’ˆå¯¹åŸºäºå£°å­¦çš„ç­›æŸ¥ç¼ºä¹ç”Ÿç†èƒŒæ™¯çš„é—®é¢˜ï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºäº†ç›´æ¥ä»å¤œé—´éŸ³é¢‘ä¸­ä¼°è®¡å‘¼å¸åŠªåŠ› (Respiratory effort) çš„æ–¹æ³•ï¼Œä»è€Œåœ¨ä¸ä½¿ç”¨æ¥è§¦å¼ä¼ æ„Ÿå™¨çš„æƒ…å†µä¸‹æ¢å¤å…³é”®ç”Ÿç†ä¿¡å·ã€‚ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªæ½œç©ºé—´èåˆæ¡†æ¶ (Latent-space fusion framework)ï¼Œå°†ä¼°è®¡å‡ºçš„å‘¼å¸åŠªåŠ›åµŒå…¥ä¸å£°å­¦ç‰¹å¾ç›¸ç»“åˆã€‚é€šè¿‡å¯¹ 103 åå‚ä¸è€…åœ¨å®¶åº­ç¯å¢ƒä¸‹çš„ 157 ä¸ªå¤œæ™šæ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œè¯¥å‘¼å¸åŠªåŠ›ä¼°è®¡å™¨å®ç°äº†ä¸€è‡´æ€§ç›¸å…³ç³»æ•° (Concordance correlation coefficient) ä¸º 0.48 çš„æ€§èƒ½ï¼Œèƒ½æ•æ‰åˆ°æœ‰æ„ä¹‰çš„å‘¼å¸åŠ¨åŠ›å­¦ç‰¹å¾ã€‚å®éªŒç»“æœè¯æ˜ï¼Œèåˆå‘¼å¸åŠªåŠ›å’ŒéŸ³é¢‘ç‰¹å¾åœ¨çµæ•åº¦å’Œ AUC æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºçº¯éŸ³é¢‘åŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨è¾ƒä½çš„å‘¼å¸æš‚åœä½é€šæ°”æŒ‡æ•°é˜ˆå€¼ä¸‹è¡¨ç°æ›´ä½³ã€‚è¯¥æ–¹æ¡ˆåœ¨åº”ç”¨æ—¶ä»…éœ€æ™ºèƒ½æ‰‹æœºå³å¯å®ç°ï¼Œä¸ºæ— ä¼ æ„Ÿå™¨ã€å¯æ‰©å±•ä¸”é•¿æœŸçš„ OSA ç›‘æµ‹æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Submitted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.14944v1",
      "published_date": "2025-09-18 13:31:19 UTC",
      "updated_date": "2025-09-18 13:31:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:50.555014+00:00"
    },
    {
      "arxiv_id": "2509.14942v1",
      "title": "Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers",
      "title_zh": "é¢å‘æ„ŸæŸ“é¢„é˜²ä¸æ§åˆ¶çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼šåŸºäº Transformer çš„çˆ±å°”å…°åŒ»é™¢ CPE è·å¾—ä¸æ‚£è€…é¢„åå»ºæ¨¡ç ”ç©¶",
      "authors": [
        "Minh-Khoi Pham",
        "Tai Tan Mai",
        "Martin Crane",
        "Rob Brennan",
        "Marie E. Ward",
        "Una Geary",
        "Declan Byrne",
        "Brian O Connell",
        "Colm Bergin",
        "Donncha Creagh",
        "Nick McDonald",
        "Marija Bezbradica"
      ],
      "abstract": "Carbapenemase-Producing Enterobacteriace poses a critical concern for infection prevention and control in hospitals. However, predictive modeling of previously highlighted CPE-associated risks such as readmission, mortality, and extended length of stay (LOS) remains underexplored, particularly with modern deep learning approaches. This study introduces an eXplainable AI modeling framework to investigate CPE impact on patient outcomes from Electronic Medical Records data of an Irish hospital. We analyzed an inpatient dataset from an Irish acute hospital, incorporating diagnostic codes, ward transitions, patient demographics, infection-related variables and contact network features. Several Transformer-based architectures were benchmarked alongside traditional machine learning models. Clinical outcomes were predicted, and XAI techniques were applied to interpret model decisions. Our framework successfully demonstrated the utility of Transformer-based models, with TabTransformer consistently outperforming baselines across multiple clinical prediction tasks, especially for CPE acquisition (AUROC and sensitivity). We found infection-related features, including historical hospital exposure, admission context, and network centrality measures, to be highly influential in predicting patient outcomes and CPE acquisition risk. Explainability analyses revealed that features like \"Area of Residence\", \"Admission Ward\" and prior admissions are key risk factors. Network variables like \"Ward PageRank\" also ranked highly, reflecting the potential value of structural exposure information. This study presents a robust and explainable AI framework for analyzing complex EMR data to identify key risk factors and predict CPE-related outcomes. Our findings underscore the superior performance of the Transformer models and highlight the importance of diverse clinical and network features.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¯è§£é‡Šäººå·¥æ™ºèƒ½(eXplainable AI)å»ºæ¨¡æ¡†æ¶ï¼Œåˆ©ç”¨çˆ±å°”å…°åŒ»é™¢çš„ç”µå­ç—…å†(Electronic Medical Records)æ•°æ®ï¼Œæ·±å…¥æ¢è®¨äº†è€ç¢³é’éœ‰çƒ¯ç±»è‚ æ†èŒ(Carbapenemase-Producing Enterobacteriace, CPE)å¯¹æ‚£è€…é¢„åçš„å½±å“ã€‚é€šè¿‡å¯¹è¯Šæ–­ä»£ç ã€ç—…æˆ¿è½¬ç§»å’Œæ¥è§¦ç½‘ç»œç‰¹å¾ç­‰å¤šå…ƒæ•°æ®çš„åˆ†æï¼Œç ”ç©¶å¯¹æ¯”äº†Transformeræ¶æ„ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨é¢„æµ‹CPEæ„ŸæŸ“ã€å†å…¥é™¢åŠæ­»äº¡ç‡ç­‰ä¸´åºŠç»“å±€ä¸­çš„è¡¨ç°ã€‚å®éªŒè¯æ˜TabTransformeræ¨¡å‹åœ¨å„é¡¹é¢„æµ‹ä»»åŠ¡ä¸­å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨CPEæ„ŸæŸ“é£é™©çš„AUROCå’Œæ•æ„Ÿæ€§æŒ‡æ ‡ä¸Šè¡¨ç°æ˜¾è‘—ã€‚å¯è§£é‡Šæ€§åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†â€œå±…ä½åŒºåŸŸâ€ã€â€œå…¥é™¢ç—…æˆ¿â€ä»¥åŠâ€œç—…æˆ¿PageRank (Ward PageRank)â€ç­‰ç½‘ç»œä¸­å¿ƒæ€§æŒ‡æ ‡æ˜¯å…³é”®é£é™©å› ç´ ï¼Œåæ˜ äº†ç»“æ„æ€§æš´éœ²ä¿¡æ¯åœ¨åŒ»ç–—ç¯å¢ƒä¸­çš„é‡è¦ä»·å€¼ã€‚è¯¥ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªç¨³å¥çš„AIæ¡†æ¶ï¼Œä¸ä»…æå‡äº†ä¸´åºŠé¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œè¿˜ä¸ºå¤æ‚åŒ»ç–—ç¯å¢ƒä¸‹çš„æ„ŸæŸ“é¢„é˜²ä¸æ§åˆ¶(Infection Prevention and Control)æä¾›äº†é€æ˜ä¸”å¯éªŒè¯çš„å†³ç­–ä¾æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to BMC Medical Informatics and Decision Making on September 18th 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.14942v1",
      "published_date": "2025-09-18 13:29:11 UTC",
      "updated_date": "2025-09-18 13:29:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:24:59.885265+00:00"
    },
    {
      "arxiv_id": "2509.14930v1",
      "title": "Cross-Modal Knowledge Distillation for Speech Large Language Models",
      "title_zh": "é¢å‘è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦",
      "authors": [
        "Enzhi Wang",
        "Qicheng Li",
        "Zhiyuan Tang",
        "Yuhang Jia"
      ],
      "abstract": "In this work, we present the first systematic evaluation of catastrophic forgetting and modality inequivalence in speech large language models, showing that introducing speech capabilities can degrade knowledge and reasoning even when inputs remain textual, and performance further decreases with spoken queries. To address these challenges, we propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM. Extensive experiments on dialogue and audio understanding tasks validate the effectiveness of our approach in preserving textual knowledge, improving cross-modal alignment, and enhancing reasoning in speech-based interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹(Speech Large Language Models)å±•å¼€äº†é¦–æ¬¡ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œé‡ç‚¹æ¢è®¨äº†å…¶ä¸­çš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)å’Œæ¨¡æ€ä¸ç­‰ä»·(Modality Inequivalence)ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼Œå¼•å…¥è¯­éŸ³æ¨¡æ€ä¼šæ˜¾è‘—å‰Šå¼±æ¨¡å‹åŸæœ‰çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå³ä½¿è¾“å…¥ä»ä¸ºæ–‡æœ¬ï¼Œä¸”è¿™ç§æ€§èƒ½é€€åŒ–åœ¨å¤„ç†è¯­éŸ³æŸ¥è¯¢æ—¶æ›´ä¸ºä¸¥é‡ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦(Cross-Modal Knowledge Distillation)æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°æ–‡æœ¬(Text-to-Text)å’Œè¯­éŸ³åˆ°æ–‡æœ¬(Speech-to-Text)é€šé“å°†çŸ¥è¯†ä»æ–‡æœ¬æ•™å¸ˆæ¨¡å‹è¿ç§»è‡³è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ã€‚åœ¨å¯¹è¯å’ŒéŸ³é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶ä¸ä»…èƒ½ä¿ç•™åŸæœ‰çš„æ–‡æœ¬çŸ¥è¯†ï¼Œè¿˜èƒ½æ”¹å–„è·¨æ¨¡æ€å¯¹é½(Cross-Modal Alignment)ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨è¯­éŸ³äº¤äº’åœºæ™¯ä¸‹çš„æ¨ç†è¡¨ç°ï¼Œä¸ºæ„å»ºé«˜æ€§èƒ½è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14930v1",
      "published_date": "2025-09-18 13:07:53 UTC",
      "updated_date": "2025-09-18 13:07:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:25:17.492890+00:00"
    },
    {
      "arxiv_id": "2509.14926v3",
      "title": "Patent Language Model Pretraining with ModernBERT",
      "title_zh": "åŸºäº ModernBERT çš„ä¸“åˆ©è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ",
      "authors": [
        "Amirhossein Yousefiramandi",
        "Ciaran Cooney"
      ],
      "abstract": "Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ç¯‡å¹…ã€æŠ€æœ¯æ€§å¼ºä¸”å…·æœ‰æ³•å¾‹ç»“æ„çš„ä¸“åˆ©æ–‡æœ¬æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼ŒåŸºäº ModernBERT æ¶æ„å¹¶åˆ©ç”¨è¶…è¿‡ 6000 ä¸‡ä»½ä¸“åˆ©è®°å½•é¢„è®­ç»ƒäº†ä¸‰ç§é¢†åŸŸä¸“ç”¨çš„æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMasked Language Modelsï¼‰ã€‚è¯¥æ–¹æ³•å¼•å…¥äº† FlashAttentionã€æ—‹è½¬åµŒå…¥ï¼ˆRotary Embeddingsï¼‰å’Œ GLU å‰é¦ˆå±‚ç­‰æ¶æ„ä¼˜åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨æå‡æ¨¡å‹å¤„ç†å¤æ‚æ–‡æœ¬çš„èƒ½åŠ›ã€‚åœ¨å››é¡¹ä¸‹æ¸¸ä¸“åˆ©åˆ†ç±»ä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼ŒModernBERT-base-PT åœ¨å¤šæ•°æ•°æ®é›†ä¸Šä¼˜äºé€šç”¨ ModernBERT åŸºå‡†ï¼Œå¹¶ä¸ PatentBERT è¾¾åˆ°äº†åŒç­‰ç«äº‰æ°´å¹³ã€‚é€šè¿‡ ModernBERT-base-VX ç­‰å˜ä½“è¿›ä¸€æ­¥æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œå®šåˆ¶ Tokenizerï¼Œå®éªŒè¯æ˜èƒ½æ˜¾è‘—å¢å¼ºç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰ ModernBERT å˜ä½“çš„æ¨ç†é€Ÿåº¦å‡æ¯” PatentBERT å¿« 3 å€ä»¥ä¸Šï¼Œå……åˆ†å±•ç¤ºäº†é¢†åŸŸä¸“ç”¨é¢„è®­ç»ƒä¸ç°ä»£æ¶æ„ä¼˜åŒ–åœ¨å¤„ç†æ—¶é—´æ•æ„Ÿå‹ä¸“åˆ© NLP ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 5 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.14926v3",
      "published_date": "2025-09-18 13:04:30 UTC",
      "updated_date": "2025-11-18 10:37:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:25:18.773025+00:00"
    },
    {
      "arxiv_id": "2509.14912v2",
      "title": "Back to Ear: Perceptually Driven High Fidelity Music Reconstruction",
      "title_zh": "Back to Earï¼šæ„ŸçŸ¥é©±åŠ¨çš„é«˜ä¿çœŸéŸ³ä¹é‡å»º",
      "authors": [
        "Kangdi Wang",
        "Zhiyue Wu",
        "Dinghao Zhou",
        "Rui Lin",
        "Junyu Dai",
        "Tao Jiang"
      ],
      "abstract": "Variational Autoencoders (VAEs) are essential for large-scale audio tasks like diffusion-based generation. However, existing open-source models often neglect auditory perceptual aspects during training, leading to weaknesses in phase accuracy and stereophonic spatial representation. To address these challenges, we propose Îµar-VAE, an open-source music signal reconstruction model that rethinks and optimizes the VAE training paradigm. Our contributions are threefold: (i) A K-weighting perceptual filter applied prior to loss calculation to align the objective with auditory perception. (ii) Two novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss using its derivatives--Instantaneous Frequency and Group Delay--for precision. (iii) A new spectral supervision paradigm where magnitude is supervised by all four Mid/Side/Left/Right components, while phase is supervised only by the LR components. Experiments show Îµar-VAE at 44.1kHz substantially outperforms leading open-source models across diverse metrics, showing particular strength in reconstructing high-frequency harmonics and the spatial characteristics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰Variational Autoencoders (VAEs)åœ¨éŸ³é¢‘é‡æ„ä¸­å¿½è§†å¬è§‰æ„ŸçŸ¥ã€å¯¼è‡´ç›¸ä½å‡†ç¡®æ€§å’Œç«‹ä½“å£°ç©ºé—´æ„Ÿä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†å¼€æºéŸ³ä¹é‡æ„æ¨¡å‹Îµar-VAEã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨æŸå¤±è®¡ç®—å‰å¼•å…¥K-weightingæ„ŸçŸ¥æ»¤æ³¢å™¨ï¼Œä½¿è®­ç»ƒç›®æ ‡æ›´ç¬¦åˆäººç±»å¬è§‰ç³»ç»Ÿã€‚ä¸ºæå‡ç›¸ä½ç²¾åº¦å’Œç«‹ä½“å£°ç›¸å¹²æ€§ï¼Œç ”ç©¶è®¾è®¡äº†Correlation Lossä»¥åŠåˆ©ç”¨ç¬æ—¶é¢‘ç‡(Instantaneous Frequency)å’Œç¾¤å»¶è¿Ÿ(Group Delay)å¯¼æ•°æ„æˆçš„Phase Lossã€‚æ­¤å¤–ï¼Œæ¨¡å‹é‡‡ç”¨äº†ä¸€ç§æ–°çš„é¢‘è°±ç›‘ç£èŒƒå¼ï¼Œå¯¹å¹…åº¦ä½¿ç”¨Mid/Side/Left/Rightå…¨ç»„ä»¶ç›‘ç£ï¼Œè€Œå¯¹ç›¸ä½ä»…ä¿ç•™Left/Rightç›‘ç£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒÎµar-VAEåœ¨44.1kHzé‡‡æ ·ç‡ä¸‹çš„æ€§èƒ½å…¨é¢è¶…è¶Šäº†ç°æœ‰ä¸»æµå¼€æºæ¨¡å‹ã€‚å…¶åœ¨é«˜é¢‘è°æ³¢(high-frequency harmonics)é‡æ„å’Œç©ºé—´ç‰¹æ€§(spatial characteristics)è¿˜åŸæ–¹é¢å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Check the Code here: https://github.com/Eps-Acoustic-Revolution-Lab/EAR_VAE and Model Weights here: https://huggingface.co/earlab/EAR_VAE",
      "pdf_url": "https://arxiv.org/pdf/2509.14912v2",
      "published_date": "2025-09-18 12:41:34 UTC",
      "updated_date": "2025-11-06 07:21:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:25:21.656099+00:00"
    },
    {
      "arxiv_id": "2509.15275v1",
      "title": "Partial Column Generation with Graph Neural Networks for Team Formation and Routing",
      "title_zh": "åŸºäºå›¾ç¥ç»ç½‘ç»œçš„å›¢é˜Ÿç»„å»ºä¸è·¯å¾„è§„åˆ’éƒ¨åˆ†åˆ—ç”Ÿæˆç®—æ³•",
      "authors": [
        "Giacomo Dall'Olio",
        "Rainer Kolisch",
        "Yaoxin Wu"
      ],
      "abstract": "The team formation and routing problem is a challenging optimization problem with several real-world applications in fields such as airport, healthcare, and maintenance operations. To solve this problem, exact solution methods based on column generation have been proposed in the literature. In this paper, we propose a novel partial column generation strategy for settings with multiple pricing problems, based on predicting which ones are likely to yield columns with a negative reduced cost. We develop a machine learning model tailored to the team formation and routing problem that leverages graph neural networks for these predictions. Computational experiments demonstrate that applying our strategy enhances the solution method and outperforms traditional partial column generation approaches from the literature, particularly on hard instances solved under a tight time limit.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·æœ‰å¹¿æ³›å®é™…åº”ç”¨èƒŒæ™¯çš„å›¢é˜Ÿç»„å»ºä¸è·¯å¾„è§„åˆ’(Team Formation and Routing)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks)çš„æ–°å‹éƒ¨åˆ†åˆ—ç”Ÿæˆ(Partial Column Generation)ç­–ç•¥ã€‚é’ˆå¯¹åŒ…å«å¤šä¸ªå®šä»·é—®é¢˜çš„å¤æ‚è®¾å®šï¼Œè¯¥ç­–ç•¥åˆ©ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹å“ªäº›å®šä»·é—®é¢˜æ›´æœ‰å¯èƒ½äº§ç”Ÿå…·æœ‰è´Ÿæ£€éªŒæ•°(Negative Reduced Cost)çš„åˆ—ï¼Œä»è€Œä¼˜åŒ–åˆ—ç”Ÿæˆè¿‡ç¨‹ã€‚ç ”ç©¶ä¸“é—¨ä¸ºè¯¥é—®é¢˜å¼€å‘äº†é€‚é…çš„å›¾ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œé¢„æµ‹ï¼Œæ˜¾è‘—å¢å¼ºäº†ç²¾ç¡®æ±‚è§£æ–¹æ³•çš„æ•ˆç‡ã€‚è®¡ç®—å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†å…·æœ‰ä¸¥æ ¼æ—¶é—´é™åˆ¶çš„å¤æ‚ç®—ä¾‹æ—¶ï¼Œå…¶è¡¨ç°æ˜æ˜¾ä¼˜äºæ–‡çŒ®ä¸­ä¼ ç»Ÿçš„Partial Column Generationæ–¹æ³•ï¼Œä¸ºæå‡å¤æ‚è¿ç­¹ä¼˜åŒ–é—®é¢˜çš„æ±‚è§£æ€§èƒ½æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.15275v1",
      "published_date": "2025-09-18 12:19:22 UTC",
      "updated_date": "2025-09-18 12:19:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:25:26.271491+00:00"
    },
    {
      "arxiv_id": "2509.14886v1",
      "title": "A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation",
      "title_zh": "é¢å‘é«˜æ•ˆ MLLM è¯„ä¼°çš„å¤šå¯¹ä¸€é¢è¯•èŒƒå¼",
      "authors": [
        "Ye Shen",
        "Junying Wang",
        "Farong Wen",
        "Yijin Guo",
        "Qi Jia",
        "Zicheng Zhang",
        "Guangtao Zhai"
      ],
      "abstract": "The rapid progress of Multi-Modal Large Language Models (MLLMs) has spurred the creation of numerous benchmarks. However, conventional full-coverage Question-Answering evaluations suffer from high redundancy and low efficiency. Inspired by human interview processes, we propose a multi-to-one interview paradigm for efficient MLLM evaluation. Our framework consists of (i) a two-stage interview strategy with pre-interview and formal interview phases, (ii) dynamic adjustment of interviewer weights to ensure fairness, and (iii) an adaptive mechanism for question difficulty-level chosen. Experiments on different benchmarks show that the proposed paradigm achieves significantly higher correlation with full-coverage results than random sampling, with improvements of up to 17.6% in PLCC and 16.7% in SRCC, while reducing the number of required questions. These findings demonstrate that the proposed paradigm provides a reliable and efficient alternative for large-scale MLLM benchmarking.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¯„ä¼°ä¸­å…¨è¦†ç›–é—®ç­”æµ‹è¯•å­˜åœ¨çš„é«˜å†—ä½™å’Œä½æ•ˆç‡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å—äººç±»é¢è¯•è¿‡ç¨‹å¯å‘çš„â€œå¤šå¯¹ä¸€é¢è¯•â€èŒƒå¼ã€‚è¯¥æ¡†æ¶ç”±åŒ…å«é¢„é¢è¯•å’Œæ­£å¼é¢è¯•çš„åŒé˜¶æ®µç­–ç•¥ã€æ—¨åœ¨ç¡®ä¿å…¬å¹³æ€§çš„é¢è¯•å®˜æƒé‡åŠ¨æ€è°ƒæ•´æœºåˆ¶ï¼Œä»¥åŠè‡ªé€‚åº”é—®é¢˜éš¾åº¦é€‰æ‹©æœºåˆ¶ç»„æˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥èŒƒå¼åœ¨å‡å°‘æ‰€éœ€é—®é¢˜æ•°é‡çš„åŒæ—¶ï¼Œä¸å…¨è¦†ç›–è¯„ä¼°ç»“æœçš„ç›¸å…³æ€§æ˜¾è‘—ä¼˜äºéšæœºé‡‡æ ·ï¼Œå…¶ä¸­ PLCC æå‡é«˜è¾¾ 17.6%ï¼ŒSRCC æå‡ 16.7%ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†è¯¥èŒƒå¼ä¸ºå¤§è§„æ¨¡ MLLM åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ç§å¯é ä¸”é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.14886v1",
      "published_date": "2025-09-18 12:07:40 UTC",
      "updated_date": "2025-09-18 12:07:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:25:25.364376+00:00"
    },
    {
      "arxiv_id": "2509.14877v1",
      "title": "AI-Driven Multi-Agent Vehicular Planning for Battery Efficiency and QoS in 6G Smart Cities",
      "title_zh": "6G æ™ºæ…§åŸå¸‚ä¸­é¢å‘ç”µæ± æ•ˆç‡ä¸æœåŠ¡è´¨é‡ä¼˜åŒ–çš„ AI é©±åŠ¨å¤šæ™ºèƒ½ä½“è½¦è¾†è§„åˆ’",
      "authors": [
        "Rohin Gillgallon",
        "Giacomo Bergami",
        "Reham Almutairi",
        "Graham Morgan"
      ],
      "abstract": "While simulators exist for vehicular IoT nodes communicating with the Cloud through Edge nodes in a fully-simulated osmotic architecture, they often lack support for dynamic agent planning and optimisation to minimise vehicular battery consumption while ensuring fair communication times. Addressing these challenges requires extending current simulator architectures with AI algorithms for both traffic prediction and dynamic agent planning. This paper presents an extension of SimulatorOrchestrator (SO) to meet these requirements. Preliminary results over a realistic urban dataset show that utilising vehicular planning algorithms can lead to improved battery and QoS performance compared with traditional shortest path algorithms. The additional inclusion of desirability areas enabled more ambulances to be routed to their target destinations while utilising less energy to do so, compared to traditional and weighted algorithms without desirability considerations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹6Gæ™ºæ…§åŸå¸‚ä¸­è½¦è¾†IoTèŠ‚ç‚¹åœ¨ç°æœ‰æ¨¡æ‹Ÿæ¶æ„ä¸‹ç¼ºä¹åŠ¨æ€è§„åˆ’å’Œä¼˜åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ‰©å±•çš„SimulatorOrchestrator (SO) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆAIç®—æ³•è¿›è¡Œäº¤é€šé¢„æµ‹å’ŒåŠ¨æ€æ™ºèƒ½ä½“è§„åˆ’ï¼ˆdynamic agent planningï¼‰ï¼Œæ—¨åœ¨æœ€å°åŒ–è½¦è¾†ç”µæ± æ¶ˆè€—å¹¶ç¡®ä¿å…¬å¹³çš„é€šä¿¡æœåŠ¡è´¨é‡ï¼ˆQoSï¼‰ã€‚åœ¨ç°å®åŸå¸‚æ•°æ®é›†ä¸Šçš„åˆæ­¥ç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„Shortest Pathç®—æ³•ç›¸æ¯”ï¼Œåˆ©ç”¨è½¦è¾†è§„åˆ’ç®—æ³•èƒ½æ˜¾è‘—æå‡ç”µæ± æ•ˆç‡å’ŒQoSæ€§èƒ½ã€‚é€šè¿‡å¼•å…¥æœŸæœ›åŒºåŸŸï¼ˆdesirability areasï¼‰ï¼Œè¯¥æ–¹æ³•æˆåŠŸä¼˜åŒ–äº†æ•‘æŠ¤è½¦çš„è·¯å¾„è§„åˆ’ï¼Œä½¿å…¶åœ¨æ¶ˆè€—æ›´å°‘èƒ½é‡çš„æƒ…å†µä¸‹æ›´é«˜æ•ˆåœ°åˆ°è¾¾ç›®æ ‡åœ°ç‚¹ã€‚è¿™é¡¹å·¥ä½œä¸ºæœªæ¥æ™ºæ…§åŸå¸‚ä¸­é«˜æ•ˆã€å¯æŒç»­çš„è½¦è¾†é€šä¿¡ä¸èµ„æºç®¡ç†æä¾›äº†å…³é”®çš„ä»¿çœŸæ”¯æŒå’Œç®—æ³•éªŒè¯ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.NI",
      "comment": "16 pages, 2 figures, 2 tables, 2 algorithms",
      "pdf_url": "https://arxiv.org/pdf/2509.14877v1",
      "published_date": "2025-09-18 11:46:22 UTC",
      "updated_date": "2025-09-18 11:46:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:25:31.185814+00:00"
    },
    {
      "arxiv_id": "2509.14868v2",
      "title": "DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting",
      "title_zh": "DPANetï¼šé¢å‘å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹çš„åŒé‡‘å­—å¡”æ³¨æ„åŠ›ç½‘ç»œ",
      "authors": [
        "Qianyang Li",
        "Xingjun Zhang",
        "Shaoxun Wang",
        "Jia Wei"
      ],
      "abstract": "Long-term time series forecasting (LTSF) is hampered by the challenge of modeling complex dependencies that span multiple temporal scales and frequency resolutions. Existing methods, including Transformer and MLP-based models, often struggle to capture these intertwined characteristics in a unified and structured manner. We propose the Dual Pyramid Attention Network (DPANet), a novel architecture that explicitly decouples and concurrently models temporal multi-scale dynamics and spectral multi-resolution periodicities. DPANet constructs two parallel pyramids: a Temporal Pyramid built on progressive downsampling, and a Frequency Pyramid built on band-pass filtering. The core of our model is the Cross-Pyramid Fusion Block, which facilitates deep, interactive information exchange between corresponding pyramid levels via cross-attention. This fusion proceeds in a coarse-to-fine hierarchy, enabling global context to guide local representation learning. Extensive experiments on public benchmarks show that DPANet achieves state-of-the-art performance, significantly outperforming prior models. Code is available at https://github.com/hit636/DPANet.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹(LTSF)ä¸­éš¾ä»¥ç»Ÿä¸€å»ºæ¨¡è·¨å¤šä¸ªæ—¶é—´å°ºåº¦å’Œé¢‘ç‡åˆ†è¾¨ç‡çš„å¤æ‚ä¾èµ–å…³ç³»è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†åŒé‡‘å­—å¡”æ³¨æ„åŠ›ç½‘ç»œ(DPANet)ã€‚è¯¥æ¶æ„é€šè¿‡æ˜¾å¼è§£è€¦å¹¶å¹¶å‘å»ºæ¨¡æ—¶é—´å¤šå°ºåº¦åŠ¨æ€å’Œé¢‘è°±å¤šåˆ†è¾¨ç‡å‘¨æœŸæ€§ï¼Œæ„å»ºäº†ä¸¤ä¸ªå¹¶è¡Œçš„é‡‘å­—å¡”ç»“æ„ï¼Œå³åŸºäºæ¸è¿›å¼ä¸‹é‡‡æ ·çš„æ—¶é—´é‡‘å­—å¡”(Temporal Pyramid)å’ŒåŸºäºå¸¦é€šæ»¤æ³¢çš„é¢‘ç‡é‡‘å­—å¡”(Frequency Pyramid)ã€‚æ¨¡å‹çš„æ ¸å¿ƒæ˜¯è·¨é‡‘å­—å¡”èåˆæ¨¡å—(Cross-Pyramid Fusion Block)ï¼Œè¯¥æ¨¡å—åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›(Cross-Attention)åœ¨å¯¹åº”çš„é‡‘å­—å¡”å±‚çº§é—´è¿›è¡Œæ·±åº¦ä¸”äº¤äº’çš„ä¿¡æ¯äº¤æ¢ã€‚è¿™ç§èåˆéµå¾ªä»ç²—åˆ°ç»†(Coarse-to-Fine)çš„å±‚çº§ç»“æ„ï¼Œä½¿å¾—å…¨å±€ä¸Šä¸‹æ–‡èƒ½å¤Ÿæœ‰æ•ˆæŒ‡å¯¼å±€éƒ¨è¡¨å¾çš„å­¦ä¹ ã€‚åœ¨å…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDPANet æ˜¾è‘—ä¼˜äºç°æœ‰çš„ Transformer å’Œ MLP-based æ¨¡å‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›(State-of-the-art)çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14868v2",
      "published_date": "2025-09-18 11:35:21 UTC",
      "updated_date": "2025-09-19 02:06:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:25:34.564777+00:00"
    },
    {
      "arxiv_id": "2509.14863v1",
      "title": "Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study",
      "title_zh": "æ¢ç©¶å›¾ Transformer ä¸­çš„å…¨å±€åˆ°å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼šä¸€é¡¹å®è¯ç ”ç©¶",
      "authors": [
        "Zhengwei Wang",
        "Gang Wu"
      ],
      "abstract": "Graph Transformers (GTs) show considerable potential in graph representation learning. The architecture of GTs typically integrates Graph Neural Networks (GNNs) with global attention mechanisms either in parallel or as a precursor to attention mechanisms, yielding a local-and-global or local-to-global attention scheme. However, as the global attention mechanism primarily captures long-range dependencies between nodes, these integration schemes may suffer from information loss, where the local neighborhood information learned by GNN could be diluted by the attention mechanism. Therefore, we propose G2LFormer, featuring a novel global-to-local attention scheme where the shallow network layers use attention mechanisms to capture global information, while the deeper layers employ GNN modules to learn local structural information, thereby preventing nodes from ignoring their immediate neighbors. An effective cross-layer information fusion strategy is introduced to allow local layers to retain beneficial information from global layers and alleviate information loss, with acceptable trade-offs in scalability. To validate the feasibility of the global-to-local attention scheme, we compare G2LFormer with state-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The results indicate that G2LFormer exhibits excellent performance while keeping linear complexity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾äº’æ„Ÿå™¨(Graph Transformers)ä¸­ä»å…¨å±€åˆ°å±€éƒ¨çš„æ³¨æ„åŠ›æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¶æ„ä¸­å±€éƒ¨é‚»åŸŸä¿¡æ¯å®¹æ˜“è¢«å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ç¨€é‡Šæˆ–ä¸¢å¤±çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†åä¸ºG2LFormerçš„æ–°å‹æ¨¡å‹ï¼Œé‡‡ç”¨å…¨å±€åˆ°å±€éƒ¨(global-to-local)çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œå³åœ¨æµ…å±‚ç½‘ç»œåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ•è·å…¨å±€ä¾èµ–ï¼Œåœ¨æ·±å±‚ç½‘ç»œé€šè¿‡GNNæ¨¡å—å¼ºåŒ–å±€éƒ¨ç»“æ„ä¿¡æ¯çš„å­¦ä¹ ã€‚é€šè¿‡å¼•å…¥è·¨å±‚ä¿¡æ¯èåˆ(cross-layer information fusion)ç­–ç•¥ï¼Œè¯¥æ¨¡å‹èƒ½æœ‰æ•ˆç¼“è§£ä¿¡æ¯æŸè€—ï¼Œå¹¶ä½¿å±€éƒ¨å±‚å……åˆ†åˆ©ç”¨å…¨å±€ç‰¹å¾ã€‚åœ¨èŠ‚ç‚¹çº§å’Œå›¾çº§ä»»åŠ¡çš„å¯¹æ¯”å®éªŒä¸­ï¼ŒG2LFormerä¼˜äºç°æœ‰çš„çº¿æ€§GTså’ŒGNNsæ¨¡å‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨ä¿æŒçº¿æ€§å¤æ‚åº¦(linear complexity)çš„åŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸ºå›¾è¡¨å¾å­¦ä¹ æä¾›äº†æ–°çš„è®¾è®¡æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14863v1",
      "published_date": "2025-09-18 11:30:50 UTC",
      "updated_date": "2025-09-18 11:30:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:25:40.063227+00:00"
    },
    {
      "arxiv_id": "2509.14860v1",
      "title": "MARIC: Multi-Agent Reasoning for Image Classification",
      "title_zh": "MARICï¼šé¢å‘å›¾åƒåˆ†ç±»çš„å¤šæ™ºèƒ½ä½“æ¨ç†",
      "authors": [
        "Wonduk Seo",
        "Minhyeong Yu",
        "Hyunjin An",
        "Seunghyun Lee"
      ],
      "abstract": "Image classification has traditionally relied on parameter-intensive model training, requiring large-scale annotated datasets and extensive fine tuning to achieve competitive performance. While recent vision language models (VLMs) alleviate some of these constraints, they remain limited by their reliance on single pass representations, often failing to capture complementary aspects of visual content. In this paper, we introduce Multi Agent based Reasoning for Image Classification (MARIC), a multi agent framework that reformulates image classification as a collaborative reasoning process. MARIC first utilizes an Outliner Agent to analyze the global theme of the image and generate targeted prompts. Based on these prompts, three Aspect Agents extract fine grained descriptions along distinct visual dimensions. Finally, a Reasoning Agent synthesizes these complementary outputs through integrated reflection step, producing a unified representation for classification. By explicitly decomposing the task into multiple perspectives and encouraging reflective synthesis, MARIC mitigates the shortcomings of both parameter-heavy training and monolithic VLM reasoning. Experiments on 4 diverse image classification benchmark datasets demonstrate that MARIC significantly outperforms baselines, highlighting the effectiveness of multi-agent visual reasoning for robust and interpretable image classification.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå›¾åƒåˆ†ç±»(Image Classification)ä¾èµ–å¤§é‡å‚æ•°è®­ç»ƒä»¥åŠè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)å•æ¬¡æ¨ç†(single-pass representations)éš¾ä»¥æ•æ‰å…¨é¢è§†è§‰ä¿¡æ¯çš„å±€é™ï¼Œæå‡ºäº†MARICå¤šæ™ºèƒ½ä½“åä½œæ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨å¤§çº²æ™ºèƒ½ä½“(Outliner Agent)åˆ†æå›¾åƒå…¨å±€ä¸»é¢˜å¹¶ç”Ÿæˆé’ˆå¯¹æ€§æç¤ºï¼Œéšåç”±ä¸‰ä¸ªç»´åº¦æ™ºèƒ½ä½“(Aspect Agents)æ²¿ä¸åŒè§†è§‰ç»´åº¦æå–ç»†ç²’åº¦æè¿°ã€‚æœ€åï¼Œæ¨ç†æ™ºèƒ½ä½“(Reasoning Agent)é€šè¿‡é›†æˆçš„åæ€æ­¥éª¤(reflection step)ç»¼åˆè¿™äº›äº’è¡¥ä¿¡æ¯ï¼Œç”Ÿæˆç”¨äºåˆ†ç±»çš„ç»Ÿä¸€è¡¨ç¤ºã€‚é€šè¿‡ä»»åŠ¡åˆ†è§£å’Œåæ€æ€§åˆæˆï¼ŒMARICæœ‰æ•ˆç¼“è§£äº†é‡å‚æ•°è®­ç»ƒä¸å•ä½“VLMæ¨ç†çš„ç¼ºé™·ã€‚åœ¨4ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMARICçš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå……åˆ†å±•ç¤ºäº†å¤šæ™ºèƒ½ä½“è§†è§‰æ¨ç†åœ¨å®ç°ç¨³å¥ä¸”å…·å¯è§£é‡Šæ€§å›¾åƒåˆ†ç±»æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2509.14860v1",
      "published_date": "2025-09-18 11:27:00 UTC",
      "updated_date": "2025-09-18 11:27:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:25:40.561579+00:00"
    },
    {
      "arxiv_id": "2509.14858v2",
      "title": "MeanFlowSE: one-step generative speech enhancement via conditional mean flow",
      "title_zh": "MeanFlowSEï¼šåŸºäºæ¡ä»¶å¹³å‡æµçš„å•æ­¥ç”Ÿæˆå¼è¯­éŸ³å¢å¼º",
      "authors": [
        "Duojia Li",
        "Shenghui Lu",
        "Hongchen Pan",
        "Zongyi Zhan",
        "Qingyang Hong",
        "Lin Li"
      ],
      "abstract": "Multistep inference is a bottleneck for real-time generative speech enhancement because flow- and diffusion-based systems learn an instantaneous velocity field and therefore rely on iterative ordinary differential equation (ODE) solvers. We introduce MeanFlowSE, a conditional generative model that learns the average velocity over finite intervals along a trajectory. Using a Jacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a local training objective that directly supervises finite-interval displacement while remaining consistent with the instantaneous-field constraint on the diagonal. At inference, MeanFlowSE performs single-step generation via a backward-in-time displacement, removing the need for multistep solvers; an optional few-step variant offers additional refinement. On VoiceBank-DEMAND, the single-step model achieves strong intelligibility, fidelity, and perceptual quality with substantially lower computational cost than multistep baselines. The method requires no knowledge distillation or external teachers, providing an efficient, high-fidelity framework for real-time generative speech enhancement. The proposed method is open-sourced at https://github.com/liduojia1/MeanFlowSE.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MeanFlowSEï¼Œä¸€ç§åŸºäºconditional mean flowçš„å•æ­¥ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³flow-basedå’Œdiffusion-basedæ¨¡å‹å› ä¾èµ–å¤šæ­¥ODEæ±‚è§£å™¨è€Œå¯¼è‡´çš„æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡å­¦ä¹ æœ‰é™åŒºé—´å†…çš„å¹³å‡é€Ÿåº¦è€Œéç¬æ—¶é€Ÿåº¦åœºï¼Œå¹¶åˆ©ç”¨Jacobian-vector product (JVP) å®ä¾‹åŒ–MeanFlowæ’ç­‰å¼ï¼Œæ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿç›´æ¥ç›‘ç£æœ‰é™ä½ç§»çš„å±€éƒ¨è®­ç»ƒç›®æ ‡ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒMeanFlowSEé€šè¿‡åå‘ä½ç§»å®ç°å•æ­¥è¯­éŸ³ç”Ÿæˆï¼Œå¤§å¹…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä¸”æ— éœ€çŸ¥è¯†è’¸é¦(knowledge distillation)æˆ–å¤–éƒ¨æ•™å¸ˆæ¨¡å‹ã€‚åœ¨VoiceBank-DEMANDæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å•æ­¥æ¨¡å‹åœ¨ä¿æŒé«˜è¯­éŸ³æ¸…æ™°åº¦å’Œæ„ŸçŸ¥è´¨é‡çš„åŒæ—¶ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¤šæ­¥æ¨ç†åŸºå‡†ã€‚MeanFlowSEä¸ºå®æ—¶ã€é«˜ä¿çœŸçš„ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å¼€æºçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14858v2",
      "published_date": "2025-09-18 11:24:47 UTC",
      "updated_date": "2025-09-19 02:25:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:25:41.778128+00:00"
    },
    {
      "arxiv_id": "2509.15271v1",
      "title": "Large Vision Models Can Solve Mental Rotation Problems",
      "title_zh": "å¤§è§†è§‰æ¨¡å‹èƒ½å¤Ÿè§£å†³å¿ƒç†æ—‹è½¬é—®é¢˜",
      "authors": [
        "Sebastian Ray Mason",
        "Anders GjÃ¸lbye",
        "Phillip Chavarria HÃ¸jbjerg",
        "Lenka TÄ›tkovÃ¡",
        "Lars Kai Hansen"
      ],
      "abstract": "Mental rotation is a key test of spatial reasoning in humans and has been central to understanding how perception supports cognition. Despite the success of modern vision transformers, it is still unclear how well these models develop similar abilities. In this work, we present a systematic evaluation of ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from simple block structures similar to those used by Shepard and Metzler to study human cognition, to more complex block figures, three types of text, and photo-realistic objects. By probing model representations layer by layer, we examine where and how these networks succeed. We find that i) self-supervised ViTs capture geometric structure better than supervised ViTs; ii) intermediate layers perform better than final layers; iii) task difficulty increases with rotation complexity and occlusion, mirroring human reaction times and suggesting similar constraints in embedding space representations.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº† ViTã€CLIPã€DINOv2 å’Œ DINOv3 ç­‰å¤§è§†è§‰æ¨¡å‹ (Large Vision Models) åœ¨å¤„ç†å¿ƒç†æ—‹è½¬ (Mental Rotation) ä»»åŠ¡ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚å®éªŒé€šè¿‡é€å±‚æ¢æµ‹æ¨¡å‹è¡¨ç¤ºï¼Œæµ‹è¯•èŒƒå›´æ¶µç›–äº†ä»ç»å…¸ Shepard å’Œ Metzler ç§¯æœ¨ç»“æ„åˆ°æ›´å¤æ‚çš„å›¾å½¢ã€æ–‡æœ¬åŠçœŸå®ç‰©ä½“ã€‚ç ”ç©¶å‘ç°è‡ªç›‘ç£å­¦ä¹ çš„ ViTs (Self-supervised ViTs) åœ¨æ•æ‰å‡ ä½•ç»“æ„æ–¹é¢ä¼˜äºæœ‰ç›‘ç£æ¨¡å‹ï¼Œä¸”æ¨¡å‹çš„ä¸­é—´å±‚ (Intermediate layers) è¡¨ç°æ™®éä¼˜äºæœ€ç»ˆå±‚ã€‚æ­¤å¤–ï¼Œä»»åŠ¡éš¾åº¦éšç€æ—‹è½¬å¤æ‚åº¦å’Œé®æŒ¡ (Occlusion) çš„å¢åŠ è€Œæå‡ï¼Œè¿™ä¸€è¶‹åŠ¿ä¸äººç±»çš„ååº”æ—¶é—´é«˜åº¦å»åˆã€‚è¿™äº›ç»“æœè¡¨æ˜å¤§æ¨¡å‹åœ¨åµŒå…¥ç©ºé—´è¡¨ç¤º (Embedding space representations) ä¸Šå—åˆ°ä¸äººç±»ç±»ä¼¼çš„çº¦æŸï¼Œå¹¶è¯æ˜äº†å…¶è§£å†³å¤æ‚ç©ºé—´å‡ ä½•é—®é¢˜çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15271v1",
      "published_date": "2025-09-18 11:18:28 UTC",
      "updated_date": "2025-09-18 11:18:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:25:56.393456+00:00"
    },
    {
      "arxiv_id": "2509.14851v2",
      "title": "Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support",
      "title_zh": "Empathy-R1ï¼šåŸºäºå…±æƒ…é“¾ä¸å¼ºåŒ–å­¦ä¹ çš„é•¿ç¯‡å¿ƒç†å¥åº·æ”¯æŒæ¡†æ¶",
      "authors": [
        "Xianrong Yao",
        "Dong She",
        "Chenxu Zhang",
        "Yimeng Zhang",
        "Yueru Sun",
        "Noman Ahmed",
        "Yang Gao",
        "Zhanpeng Jin"
      ],
      "abstract": "Empathy is critical for effective mental health support, especially when addressing Long Counseling Texts (LCTs). However, existing Large Language Models (LLMs) often generate replies that are semantically fluent but lack the structured reasoning necessary for genuine psychological support, particularly in a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel framework that integrates a Chain-of-Empathy (CoE) reasoning process with Reinforcement Learning (RL) to enhance response quality for LCTs. Inspired by cognitive-behavioral therapy, our CoE paradigm guides the model to sequentially reason about a help-seeker's emotions, causes, and intentions, making its thinking process both transparent and interpretable. Our framework is empowered by a new large-scale Chinese dataset, Empathy-QA, and a two-stage training process. First, Supervised Fine-Tuning instills the CoE's reasoning structure. Subsequently, RL, guided by a dedicated reward model, refines the therapeutic relevance and contextual appropriateness of the final responses. Experiments show that Empathy-R1 achieves strong performance on key automatic metrics. More importantly, human evaluations confirm its superiority, showing a clear preference over strong baselines and achieving a Win@1 rate of 44.30% on our new benchmark. By enabling interpretable and contextually nuanced responses, Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Empathy-R1ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†å…±æƒ…é“¾(Chain-of-Empathy, CoE)æ¨ç†è¿‡ç¨‹ä¸å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¯¹ä¸­æ–‡é•¿æ–‡æœ¬å’¨è¯¢(Long Counseling Texts)çš„å¿ƒç†æ”¯æŒè´¨é‡ã€‚å—è®¤çŸ¥è¡Œä¸ºç–—æ³•å¯å‘ï¼ŒCoEèŒƒå¼å¼•å¯¼æ¨¡å‹æŒ‰é¡ºåºæ¨ç†æ±‚åŠ©è€…çš„æƒ…æ„Ÿã€åŸå› å’Œæ„å›¾ï¼Œæ˜¾è‘—æé«˜äº†æ€ç»´è¿‡ç¨‹çš„é€æ˜åº¦ä¸å¯è§£é‡Šæ€§ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºå¤§è§„æ¨¡ä¸­æ–‡æ•°æ®é›†Empathy-QAï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning)å’Œå¼ºåŒ–å­¦ä¹ ä¸¤é˜¶æ®µè®­ç»ƒï¼Œåˆ©ç”¨ä¸“ç”¨å¥–åŠ±æ¨¡å‹ä¼˜åŒ–å›å¤çš„æ²»ç–—ç›¸å…³æ€§å’Œè¯­å¢ƒå¥‘åˆåº¦ã€‚å®éªŒè¯æ˜ï¼ŒEmpathy-R1åœ¨è‡ªåŠ¨æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°ä¸­å‡è¡¨ç°å“è¶Šï¼Œå…¶äººå·¥è¯„ä¼°èƒœç‡(Win@1)è¾¾åˆ°äº†44.30%ï¼Œå±•ç°å‡ºä¼˜äºå¼ºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ä¸€è¿›å±•ä¸ºå¼€å‘å…·å¤‡è¯­å¢ƒæ´å¯ŸåŠ›ã€è´Ÿè´£ä»»ä¸”çœŸæ­£æœ‰ç›Šçš„å¿ƒç†å¥åº·æ”¯æŒäººå·¥æ™ºèƒ½å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14851v2",
      "published_date": "2025-09-18 11:16:09 UTC",
      "updated_date": "2025-09-19 07:24:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:26:07.794197+00:00"
    },
    {
      "arxiv_id": "2509.14846v1",
      "title": "[Re] Improving Interpretation Faithfulness for Vision Transformers",
      "title_zh": "[å¤ç°] æå‡è§†è§‰ Transformer è§£é‡Šçš„å¿ å®æ€§",
      "authors": [
        "Izabela Kurek",
        "Wojciech Trejter",
        "Stipe Frkovic",
        "Andro Erdelez"
      ],
      "abstract": "This work aims to reproduce the results of Faithful Vision Transformers (FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for Vision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate claims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised Smoothing (DDS) improves interpretability robustness to (1) attacks in a segmentation task and (2) perturbation and attacks in a classification task. We also extend the original study by investigating the authors' claims that adding DDS to any interpretability method can improve its robustness under attack. This is tested on baseline methods and the recently proposed Attribution Rollout method. In addition, we measure the computational costs and environmental impact of obtaining an FViT through DDS. Our results broadly agree with the original study's findings, although minor discrepancies were found and discussed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ˜¯å¯¹ Faithful Vision Transformers (FViTs) çš„å¤ç°å·¥ä½œï¼Œæ—¨åœ¨éªŒè¯ Diffusion Denoised Smoothing (DDS) åœ¨æå‡è§£é‡Šå¿ å®åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä½œè€…è¯¦ç»†è°ƒæŸ¥äº† DDS å¦‚ä½•å¢å¼º Vision Transformers åœ¨åˆ†å‰²ä»»åŠ¡å’Œåˆ†ç±»ä»»åŠ¡ä¸­åº”å¯¹æ‰°åŠ¨åŠå¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ‰©å±•äº†åŸè®ºæ–‡çš„èŒƒå›´ï¼Œå°† DDS åº”ç”¨äºåŸºçº¿æ–¹æ³•åŠ Attribution Rollout ç­‰è§£é‡Šæ€§å·¥å…·ï¼Œè¯å®äº†å…¶æ™®é€‚çš„é²æ£’æ€§æå‡æ•ˆæœã€‚åŒæ—¶ï¼Œè¯¥å·¥ä½œè¿˜è¡¡é‡äº†è·å– FViT è¿‡ç¨‹ä¸­çš„è®¡ç®—æˆæœ¬å’Œç¯å¢ƒå½±å“ã€‚å®éªŒç»“æœåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ”¯æŒäº†åŸå§‹ç ”ç©¶çš„å‘ç°ï¼Œå¹¶å¯¹å…¶ä¸­å­˜åœ¨çš„ç»†å¾®å·®å¼‚è¿›è¡Œäº†æ·±å…¥è®¨è®ºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages article, 29 pdf pages, 19 figures, MLRC. Transactions on Machine Learning Research (2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.14846v1",
      "published_date": "2025-09-18 11:11:27 UTC",
      "updated_date": "2025-09-18 11:11:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:26:10.786132+00:00"
    },
    {
      "arxiv_id": "2509.18185v1",
      "title": "Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases",
      "title_zh": "Visionervesï¼šåº”ç”¨äºå­å®«å†…è†œå¼‚ä½ç—‡ç—…ä¾‹çš„è‡ªåŠ¨ã€å¯å¤ç°å‘¨å›´ç¥ç»ç³»ç»Ÿè¯†åˆ«æ··åˆäººå·¥æ™ºèƒ½",
      "authors": [
        "Giammarco La Barbera",
        "Enzo Bonnot",
        "Thomas Isla",
        "Juan Pablo de la Plata",
        "Joy-Rose Dunoyer de Segonzac",
        "Jennifer Attali",
        "CÃ©cile Lozach",
        "Alexandre Bellucci",
        "Louis Marcellin",
        "Laure Fournier",
        "Sabine Sarnacki",
        "Pietro Gori",
        "Isabelle Bloch"
      ],
      "abstract": "Endometriosis often leads to chronic pelvic pain and possible nerve involvement, yet imaging the peripheral nerves remains a challenge. We introduce Visionerves, a novel hybrid AI framework for peripheral nervous system recognition from multi-gradient DWI and morphological MRI data. Unlike conventional tractography, Visionerves encodes anatomical knowledge through fuzzy spatial relationships, removing the need for selection of manual ROIs. The pipeline comprises two phases: (A) automatic segmentation of anatomical structures using a deep learning model, and (B) tractography and nerve recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in 10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated substantial improvements over standard tractography, with Dice score improvements of up to 25% and spatial errors reduced to less than 5 mm. This automatic and reproducible approach enables detailed nerve analysis and paves the way for non-invasive diagnosis of endometriosis-related neuropathy, as well as other conditions with nerve involvement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Visionervesï¼Œä¸€ç§ä¸“é—¨é’ˆå¯¹å­å®«å†…è†œå¼‚ä½ç—‡ (endometriosis) ç—…ä¾‹è®¾è®¡çš„æ··åˆäººå·¥æ™ºèƒ½ (Hybrid AI) æ¡†æ¶ï¼Œç”¨äºä»å¤šæ¢¯åº¦ DWI å’Œå½¢æ€å­¦ MRI æ•°æ®ä¸­è‡ªåŠ¨è¯†åˆ«å‘¨å›´ç¥ç»ç³»ç»Ÿã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡ç³Šç©ºé—´å…³ç³» (fuzzy spatial relationships) ç¼–ç è§£å‰–å­¦çŸ¥è¯†ï¼Œæ¶ˆé™¤äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¯¹æ‰‹åŠ¨ ROI é€‰æ‹©çš„éœ€æ±‚ï¼Œç¡®ä¿äº†è¯†åˆ«è¿‡ç¨‹çš„è‡ªåŠ¨åŒ–ä¸å¯é‡å¤æ€§ã€‚å…¶æµæ°´çº¿åŒ…å«ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šé¦–å…ˆåˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è‡ªåŠ¨åˆ†å‰²è§£å‰–ç»“æ„ï¼Œéšåé€šè¿‡ç¬¦å·ç©ºé—´æ¨ç† (symbolic spatial reasoning) å®Œæˆçº¤ç»´æŸæˆåƒ (tractography) ä¸ç¥ç»è¯†åˆ«ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisionerves åœ¨éª¶ä¸›ç¥ç»è¯†åˆ«ä»»åŠ¡ä¸­ç›¸æ¯”æ ‡å‡†æ–¹æ³•æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒDice score æ”¹å–„è¾¾ 25%ï¼Œä¸”ç©ºé—´è¯¯å·®é™è‡³ 5 mm ä»¥ä¸‹ã€‚è¯¥æ–¹æ³•ä¸ºå†…è†œå¼‚ä½ç—‡ç›¸å…³ç¥ç»ç—…å˜çš„éä¾µå…¥æ€§è¯Šæ–­æä¾›äº†æ–°é€”å¾„ï¼Œå¹¶å¯æ‰©å±•è‡³å…¶ä»–æ¶‰åŠç¥ç»å—ç´¯çš„ä¸´åºŠç—…ç—‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Computer-Aided Pelvic Imaging for Female Health (CAPI) - Workshop MICCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18185v1",
      "published_date": "2025-09-18 11:08:28 UTC",
      "updated_date": "2025-09-18 11:08:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:26:11.484655+00:00"
    },
    {
      "arxiv_id": "2509.14841v1",
      "title": "Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution",
      "title_zh": "å¹¶éæ‰€æœ‰é€€åŒ–å‡ç­‰ï¼šé¢å‘å¯æ³›åŒ–å›¾åƒè¶…åˆ†è¾¨ç‡çš„é’ˆå¯¹æ€§ç‰¹å¾å»å™ªæ¡†æ¶",
      "authors": [
        "Hongjun Wang",
        "Jiyuan Chen",
        "Zhengwei Yin",
        "Xuan Song",
        "Yinqiang Zheng"
      ],
      "abstract": "Generalizable Image Super-Resolution aims to enhance model generalization capabilities under unknown degradations. To achieve this goal, the models are expected to focus only on image content-related features instead of overfitting degradations. Recently, numerous approaches such as Dropout and Feature Alignment have been proposed to suppress models' natural tendency to overfit degradations and yield promising results. Nevertheless, these works have assumed that models overfit to all degradation types (e.g., blur, noise, JPEG), while through careful investigations in this paper, we discover that models predominantly overfit to noise, largely attributable to its distinct degradation pattern compared to other degradation types. In this paper, we propose a targeted feature denoising framework, comprising noise detection and denoising modules. Our approach presents a general solution that can be seamlessly integrated with existing super-resolution models without requiring architectural modifications. Our framework demonstrates superior performance compared to previous regularization-based methods across five traditional benchmarks and datasets, encompassing both synthetic and real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šç”¨å›¾åƒè¶…åˆ†è¾¨ç‡(Generalizable Image Super-Resolution)ä¸­çš„æ³›åŒ–é—®é¢˜ï¼Œå¹¶é€šè¿‡è°ƒæŸ¥å‘ç°æ¨¡å‹åœ¨é¢å¯¹å¤šç§é€€åŒ–ç±»å‹æ—¶ï¼Œä¸»è¦å¯¹å™ªå£°(noise)è¡¨ç°å‡ºè¿‡æ‹Ÿåˆ(overfit)ï¼Œè€Œéå‡åŒ€åœ°å¯¹æ‰€æœ‰é€€åŒ–ç±»å‹è¿‡æ‹Ÿåˆã€‚é’ˆå¯¹è¿™ä¸€å‘ç°ï¼Œä½œè€…æå‡ºäº†ä¸€ç§é’ˆå¯¹æ€§çš„ç‰¹å¾å»å™ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”±å™ªå£°æ£€æµ‹(noise detection)å’Œå»å™ªæ¨¡å—(denoising modules)ç»„æˆã€‚è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„è¶…åˆ†è¾¨ç‡æ¨¡å‹ä¸­ï¼Œä¸”æ— éœ€ä¿®æ”¹åŸå§‹æ¶æ„ã€‚åœ¨åˆæˆä¸çœŸå®åœºæ™¯çš„äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶çš„æ€§èƒ½ä¼˜äºä»¥å¾€åŸºäºæ­£åˆ™åŒ–(regularization-based)çš„æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨æœªçŸ¥é€€åŒ–ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14841v1",
      "published_date": "2025-09-18 11:04:51 UTC",
      "updated_date": "2025-09-18 11:04:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:26:12.990579+00:00"
    },
    {
      "arxiv_id": "2509.15270v1",
      "title": "PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images",
      "title_zh": "PRISMï¼šç”¨äº AI ç”Ÿæˆå›¾åƒæŒ‡çº¹è¯†åˆ«çš„ç›¸ä½å¢å¼ºå¾„å‘å›¾åƒç‰¹å¾æ˜ å°„æ¡†æ¶",
      "authors": [
        "Emanuele Ricco",
        "Elia Onofri",
        "Lorenzo Cima",
        "Stefano Cresci",
        "Roberto Di Pietro"
      ],
      "abstract": "A critical need has emerged for generative AI: attribution methods. That is, solutions that can identify the model originating AI-generated content. This feature, generally relevant in multimodal applications, is especially sensitive in commercial settings where users subscribe to paid proprietary services and expect guarantees about the source of the content they receive. To address these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images. PRISM is based on a radial reduction of the discrete Fourier transform that leverages amplitude and phase information to capture model-specific signatures. The output of the above process is subsequently clustered via linear discriminant analysis to achieve reliable model attribution in diverse settings, even if the model's internal details are inaccessible. To support our work, we construct PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN- and diffusion-based models. On this dataset, PRISM achieves an attribution accuracy of 92.04%. We additionally evaluate our method on four benchmarks from the literature, reaching an average accuracy of 81.60%. Finally, we evaluate our methodology also in the binary task of detecting real vs fake images, achieving an average accuracy of 88.41%. We obtain our best result on GenImage with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our results demonstrate the effectiveness of frequency-domain fingerprinting for cross-architecture and cross-dataset model attribution, offering a viable solution for enforcing accountability and trust in generative AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PRISMï¼Œä¸€ç§ç›¸ä½å¢å¼ºçš„åŸºäºå¾„å‘çš„å›¾åƒç‰¹å¾æ˜ å°„æ¡†æ¶ï¼ˆPhase-enhanced Radial-based Image Signature Mappingï¼‰ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼AIé¢†åŸŸä¸­è¯†åˆ«å›¾åƒæ¥æºæ¨¡å‹ï¼ˆModel Attributionï¼‰çš„å…³é”®éœ€æ±‚ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹ç¦»æ•£å‚…é‡Œå¶å˜æ¢ï¼ˆDiscrete Fourier Transformï¼‰è¿›è¡Œå¾„å‘ç¼©å‡ï¼ŒåŒæ—¶åˆ©ç”¨æŒ¯å¹…å’Œç›¸ä½ä¿¡æ¯æ¥æ•è·ç‰¹å®šæ¨¡å‹çš„æŒ‡çº¹ç‰¹å¾ï¼Œå¹¶ç»“åˆçº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLinear Discriminant Analysisï¼‰å®ç°å¯é çš„æ¨¡å‹å½’å±è¯†åˆ«ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†åŒ…å«3.6ä¸‡å¼ å›¾åƒçš„PRISM-36Kæ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šç§GANå’Œæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰ç”Ÿæˆçš„å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRISMåœ¨è‡ªå»ºæ•°æ®é›†ä¸Šçš„å½’å±å‡†ç¡®ç‡è¾¾åˆ°92.04%ï¼Œåœ¨å¤šä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œåœ¨åŒºåˆ†çœŸå®ä¸è™šå‡å›¾åƒçš„äºŒåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒPRISMåœ¨GenImageåŸºå‡†ä¸Šå–å¾—äº†95.06%çš„å‡†ç¡®ç‡ï¼Œè¿œè¶…åŸæœ‰åŸºå‡†æ°´å¹³ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é¢‘åŸŸæŒ‡çº¹è¯†åˆ«ï¼ˆFrequency-domain Fingerprintingï¼‰åœ¨è·¨æ¶æ„å’Œè·¨æ•°æ®é›†åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¢å¼ºç”Ÿæˆå¼AIç³»ç»Ÿçš„é—®è´£æœºåˆ¶å’Œä¿¡ä»»åº¦æä¾›äº†å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15270v1",
      "published_date": "2025-09-18 10:57:26 UTC",
      "updated_date": "2025-09-18 10:57:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:26:18.598136+00:00"
    },
    {
      "arxiv_id": "2509.14832v1",
      "title": "Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization",
      "title_zh": "é¢å‘å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä¸å¤šé˜¶æ®µéšæœºä¼˜åŒ–çš„åŸºäºæ‰©æ•£çš„åœºæ™¯æ ‘ç”Ÿæˆ",
      "authors": [
        "Stelios Zarifis",
        "Ioannis Kordonis",
        "Petros Maragos"
      ],
      "abstract": "Stochastic forecasting is critical for efficient decision-making in uncertain systems, such as energy markets and finance, where estimating the full distribution of future scenarios is essential. We propose Diffusion Scenario Tree (DST), a general framework for constructing scenario trees for multivariate prediction tasks using diffusion-based probabilistic forecasting models. DST recursively samples future trajectories and organizes them into a tree via clustering, ensuring non-anticipativity (decisions depending only on observed history) at each stage. We evaluate the framework on the optimization task of energy arbitrage in New York State's day-ahead electricity market. Experimental results show that our approach consistently outperforms the same optimization algorithms that use scenario trees from more conventional models and Model-Free Reinforcement Learning baselines. Furthermore, using DST for stochastic optimization yields more efficient decision policies, achieving higher performance by better handling uncertainty than deterministic and stochastic MPC variants using the same diffusion-based forecaster.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Diffusion Scenario Tree (DST)ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨åŸºäºæ‰©æ•£çš„æ¦‚ç‡é¢„æµ‹æ¨¡å‹(diffusion-based probabilistic forecasting models)ä¸ºå¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹å’Œå¤šé˜¶æ®µéšæœºä¼˜åŒ–(multistage stochastic optimization)æ„å»ºæƒ…æ™¯æ ‘çš„é€šç”¨æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é€’å½’é‡‡æ ·æœªæ¥è½¨è¿¹å¹¶åˆ©ç”¨èšç±»æ–¹æ³•å°†å…¶ç»„ç»‡æˆæ ‘çŠ¶ç»“æ„ï¼Œä»è€Œç¡®ä¿æ¯ä¸ªé˜¶æ®µçš„éé¢„æµ‹æ€§(non-anticipativity)ï¼Œä½¿å†³ç­–ä»…ä¾èµ–äºå·²è§‚æµ‹çš„å†å²æ•°æ®ã€‚ç ”ç©¶äººå‘˜åœ¨çº½çº¦å·æ—¥å‰ç”µåŠ›å¸‚åœºçš„èƒ½æºå¥—åˆ©ä¼˜åŒ–ä»»åŠ¡ä¸­è¯„ä¼°äº†è¯¥æ¡†æ¶ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶è¡¨ç°ä¼˜äºä½¿ç”¨ä¼ ç»Ÿæ¨¡å‹ç”Ÿæˆæƒ…æ™¯æ ‘çš„æ–¹æ³•ä»¥åŠæ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ (Model-Free Reinforcement Learning)åŸºå‡†ã€‚æ­¤å¤–ï¼Œç›¸æ¯”äºä½¿ç”¨ç›¸åŒæ‰©æ•£é¢„æµ‹å™¨çš„ç¡®å®šæ€§å’Œéšæœºæ¨¡å‹é¢„æµ‹æ§åˆ¶(MPC)å˜ä½“ï¼Œåˆ©ç”¨DSTè¿›è¡Œéšæœºä¼˜åŒ–èƒ½äº§ç”Ÿæ›´é«˜æ•ˆçš„å†³ç­–ç­–ç•¥ï¼Œé€šè¿‡æ›´å¥½åœ°å¤„ç†ä¸ç¡®å®šæ€§å®ç°äº†æ›´é«˜çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 2 figures, 2 tables, and 1 algorithm. This version is submitted to the 51st IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026), to be held in Barcelona, Spain, on May 4-8, 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.14832v1",
      "published_date": "2025-09-18 10:49:05 UTC",
      "updated_date": "2025-09-18 10:49:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:26:24.695377+00:00"
    },
    {
      "arxiv_id": "2509.14830v2",
      "title": "ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification",
      "title_zh": "ProtoMedXï¼šé¢å‘éª¨éª¼å¥åº·åˆ†ç±»çš„å¯è§£é‡Šå¤šæ¨¡æ€åŸå‹å­¦ä¹ ",
      "authors": [
        "Alvaro Lopez Pellicer",
        "Andre Mariucci",
        "Plamen Angelov",
        "Marwan Bukhari",
        "Jemma G. Kerns"
      ],
      "abstract": "Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA/X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal (multimodal) model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX's prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éª¨å¯†åº¦å¼‚å¸¸(Osteopenia)å’Œéª¨è´¨ç–æ¾ç—‡(Osteoporosis)çš„æ—©æœŸæ£€æµ‹ï¼ŒæŒ‡å‡ºäº†å½“å‰AIæ¨¡å‹åœ¨éª¨éª¼å¥åº·åˆ†ç±»ä¸­è¿‡åº¦ä¾èµ–è§†è§‰æ•°æ®ä¸”ç¼ºä¹å¯è§£é‡Šæ€§(Explainability)çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ProtoMedXï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†è…°æ¤DEXAæ‰«æå›¾åƒå’Œæ‚£è€…ç—…å†è®°å½•çš„å¤šæ¨¡æ€å­¦ä¹ (Multi-modal Learning)æ¨¡å‹ã€‚ProtoMedXé‡‡ç”¨åŸºäºåŸå‹çš„(Prototype-based)æ¶æ„è®¾è®¡ï¼Œå®ç°äº†å†³ç­–è¿‡ç¨‹çš„å†…åœ¨å¯è§£é‡Šæ€§ï¼Œèƒ½å¤Ÿæ»¡è¶³åŒ»ç–—åº”ç”¨åŠæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆ(EU AI Act)å¯¹ç®—æ³•é€æ˜åº¦çš„è¦æ±‚ã€‚åœ¨æ¶‰åŠ4,160åNHSçœŸå®æ‚£è€…çš„æ•°æ®é›†å®éªŒä¸­ï¼Œè¯¥æ¨¡å‹åœ¨çº¯è§†è§‰ä»»åŠ¡å’Œå¤šæ¨¡æ€å˜ä½“ä¸­åˆ†åˆ«å–å¾—äº†87.58%å’Œ89.8%çš„å‡†ç¡®ç‡ï¼Œå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥æ¡†æ¶ä¸ä»…å®ç°äº†State-of-the-artçš„åˆ†ç±»æ€§èƒ½ï¼Œè¿˜é€šè¿‡å¯è§†åŒ–åŸå‹ä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›äº†ç›´è§‚çš„è¯Šæ–­ä¾æ®ï¼Œä¸ºAIåœ¨éª¨éª¼å¥åº·é¢†åŸŸçš„å®é™…ä¸´åºŠåº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025 (PHAROS-AFE-AIMI: Adaptation, Fairness, and Explainability in Medical Imaging). 8 pages, 5 figures, 4 tables. Keywords: multi-modal, multimodal, prototype learning, explainable AI, interpretable models, case-based reasoning, medical imaging, DEXA, bone health, osteoporosis, osteopenia, diagnosis, classification, clustering",
      "pdf_url": "https://arxiv.org/pdf/2509.14830v2",
      "published_date": "2025-09-18 10:46:18 UTC",
      "updated_date": "2025-10-09 14:54:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:26:32.591510+00:00"
    },
    {
      "arxiv_id": "2509.14827v1",
      "title": "Template-Based Cortical Surface Reconstruction with Minimal Energy Deformation",
      "title_zh": "åŸºäºæœ€å°èƒ½é‡å½¢å˜çš„æ¨¡æ¿åŒ–çš®å±‚è¡¨é¢é‡å»º",
      "authors": [
        "Patrick Madlindl",
        "Fabian Bongratz",
        "Christian Wachinger"
      ],
      "abstract": "Cortical surface reconstruction (CSR) from magnetic resonance imaging (MRI) is fundamental to neuroimage analysis, enabling morphological studies of the cerebral cortex and functional brain mapping. Recent advances in learning-based CSR have dramatically accelerated processing, allowing for reconstructions through the deformation of anatomical templates within seconds. However, ensuring the learned deformations are optimal in terms of deformation energy and consistent across training runs remains a particular challenge. In this work, we design a Minimal Energy Deformation (MED) loss, acting as a regularizer on the deformation trajectories and complementing the widely used Chamfer distance in CSR. We incorporate it into the recent V2C-Flow model and demonstrate considerable improvements in previously neglected training consistency and reproducibility without harming reconstruction accuracy and topological correctness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çš®å±‚è¡¨é¢é‡å»º (Cortical surface reconstruction, CSR) ä¸­å­¦ä¹ å˜å½¢è½¨è¿¹æœ€ä¼˜æ€§åŠè®­ç»ƒä¸€è‡´æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æœ€å°èƒ½é‡å˜å½¢ (Minimal Energy Deformation, MED) æŸå¤±å‡½æ•°ã€‚MED æŸå¤±ä½œä¸ºä¸€ç§æ­£åˆ™åŒ–é¡¹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¡¥å…… CSR ä¸­å¹¿æ³›ä½¿ç”¨çš„ Chamfer distanceï¼Œä»è€Œä¼˜åŒ–å˜å½¢è·¯å¾„ã€‚é€šè¿‡å°†è¯¥æŸå¤±é¡¹æ•´åˆè¿›æœ€æ–°çš„ V2C-Flow æ¨¡å‹ï¼Œç ”ç©¶å±•ç¤ºäº†å…¶åœ¨æå‡è®­ç»ƒä¸€è‡´æ€§ä¸å¯é‡å¤æ€§ (reproducibility) æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMED æŸå¤±åœ¨ä¸å½±å“é‡å»ºç²¾åº¦å’Œæ‹“æ‰‘æ­£ç¡®æ€§çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆè§£å†³äº†æ¨¡å‹åœ¨ä¸åŒè®­ç»ƒè½®æ¬¡é—´çš„æ³¢åŠ¨é—®é¢˜ã€‚è¯¥å·¥ä½œä¸ºå®ç°æ›´ç¨³å®šã€æ›´å¯é çš„è§£å‰–æ¨¡æ¿å˜å½¢é‡å»ºæŠ€æœ¯æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æŒä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.NC",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14827v1",
      "published_date": "2025-09-18 10:41:39 UTC",
      "updated_date": "2025-09-18 10:41:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:26:37.297099+00:00"
    },
    {
      "arxiv_id": "2509.15269v1",
      "title": "Modeling Transformers as complex networks to analyze learning dynamics",
      "title_zh": "å°† Transformer å»ºæ¨¡ä¸ºå¤æ‚ç½‘ç»œä»¥åˆ†æå­¦ä¹ åŠ¨åŠ›å­¦",
      "authors": [
        "Elisabetta Rocchetti"
      ],
      "abstract": "The process by which Large Language Models (LLMs) acquire complex capabilities during training remains a key open question in mechanistic interpretability. This project investigates whether these learning dynamics can be characterized through the lens of Complex Network Theory (CNT). I introduce a novel methodology to represent a Transformer-based LLM as a directed, weighted graph where nodes are the model's computational components (attention heads and MLPs) and edges represent causal influence, measured via an intervention-based ablation technique. By tracking the evolution of this component-graph across 143 training checkpoints of the Pythia-14M model on a canonical induction task, I analyze a suite of graph-theoretic metrics. The results reveal that the network's structure evolves through distinct phases of exploration, consolidation, and refinement. Specifically, I identify the emergence of a stable hierarchy of information spreader components and a dynamic set of information gatherer components, whose roles reconfigure at key learning junctures. This work demonstrates that a component-level network perspective offers a powerful macroscopic lens for visualizing and understanding the self-organizing principles that drive the formation of functional circuits in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ˜¯å¦å¯ä»¥é€šè¿‡å¤æ‚ç½‘ç»œç†è®º(Complex Network Theory, CNT)æ¥åˆ»ç”»å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¹ å¾—å¤æ‚èƒ½åŠ›çš„è¿‡ç¨‹ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå°†åŸºäºTransformerçš„LLMå»ºæ¨¡ä¸ºæœ‰å‘åŠ æƒå›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨æ¨¡å‹è®¡ç®—ç»„ä»¶(attention headså’ŒMLPs)ï¼Œè¾¹ä»£è¡¨é€šè¿‡å¹²é¢„æ¶ˆèæŠ€æœ¯(intervention-based ablation)æµ‹é‡çš„ç»„ä»¶é—´å› æœå½±å“ã€‚é€šè¿‡è¿½è¸ªPythia-14Mæ¨¡å‹åœ¨å½’çº³ä»»åŠ¡(induction task)è®­ç»ƒè¿‡ç¨‹ä¸­çš„143ä¸ªæ£€æŸ¥ç‚¹ï¼Œç ”ç©¶å‘ç°ç½‘ç»œç»“æ„ç»å†äº†æ¢ç´¢ã€å·©å›ºå’Œç»†åŒ–ç­‰ä¸åŒæ¼”åŒ–é˜¶æ®µã€‚å®éªŒè¯†åˆ«å‡ºäº†ç¨³å®šçš„ä¿¡æ¯ä¼ æ’­ç»„ä»¶(information spreader)å±‚æ¬¡ç»“æ„å’ŒåŠ¨æ€çš„ä¿¡æ¯æ”¶é›†ç»„ä»¶(information gatherer)é›†åˆï¼Œå…¶è§’è‰²åœ¨å…³é”®å­¦ä¹ èŠ‚ç‚¹ä¼šå‘ç”Ÿé‡ç»„ã€‚è¯¥å·¥ä½œè¯æ˜äº†ç»„ä»¶çº§ç½‘ç»œè§†è§’ä¸ºç†è§£å’Œå¯è§†åŒ–é©±åŠ¨LLMsåŠŸèƒ½ç”µè·¯å½¢æˆçš„è‡ªç»„ç»‡åŸç†æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å®è§‚è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15269v1",
      "published_date": "2025-09-18 10:20:26 UTC",
      "updated_date": "2025-09-18 10:20:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:26:38.588402+00:00"
    },
    {
      "arxiv_id": "2509.15267v1",
      "title": "Autoguided Online Data Curation for Diffusion Model Training",
      "title_zh": "ç”¨äºæ‰©æ•£æ¨¡å‹è®­ç»ƒçš„è‡ªåŠ¨å¼•å¯¼åœ¨çº¿æ•°æ®ç­›é€‰",
      "authors": [
        "Valeria Pais",
        "Luis Oala",
        "Daniele Faccio",
        "Marco Aversa"
      ],
      "abstract": "The costs of generative model compute rekindled promises and hopes for efficient data curation. In this work, we investigate whether recently developed autoguidance and online data selection methods can improve the time and sample efficiency of training generative diffusion models. We integrate joint example selection (JEST) and autoguidance into a unified code base for fast ablation and benchmarking. We evaluate combinations of data curation on a controlled 2-D synthetic data generation task as well as (3x64x64)-D image generation. Our comparisons are made at equal wall-clock time and equal number of samples, explicitly accounting for the overhead of selection. Across experiments, autoguidance consistently improves sample quality and diversity. Early AJEST (applying selection only at the beginning of training) can match or modestly exceed autoguidance alone in data efficiency on both tasks. However, its time overhead and added complexity make autoguidance or uniform random data selection preferable in most situations. These findings suggest that while targeted online selection can yield efficiency gains in early training, robust sample quality improvements are primarily driven by autoguidance. We discuss limitations and scope, and outline when data selection may be beneficial.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨å¼•å¯¼(autoguidance)å’Œåœ¨çº¿æ•°æ®é€‰æ‹©(online data selection)æ–¹æ³•æ˜¯å¦èƒ½æé«˜ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹(diffusion models)è®­ç»ƒçš„æ—¶é—´å’Œæ ·æœ¬æ•ˆç‡ã€‚ç ”ç©¶äººå‘˜å°†è”åˆæ ·æœ¬é€‰æ‹©(JEST)ä¸autoguidanceæ•´åˆè¿›ç»Ÿä¸€ä»£ç åº“ï¼Œå¹¶åœ¨2Dåˆæˆæ•°æ®åŠå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†æ¶ˆèå®éªŒï¼Œè¯„ä¼°æ—¶æ˜¾å¼è®¡å…¥äº†é€‰æ‹©è¿‡ç¨‹å¸¦æ¥çš„æ—¶é—´å¼€é”€ã€‚å®éªŒå‘ç°ï¼Œautoguidanceåœ¨æ‰€æœ‰æµ‹è¯•ä¸­å‡èƒ½æŒç»­æå‡ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ä¸å¤šæ ·æ€§ã€‚è™½ç„¶åœ¨è®­ç»ƒæ—©æœŸåº”ç”¨é€‰æ‹©çš„æ—©æœŸAJESTæ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡ä¸Šèƒ½åŒ¹é…æˆ–ç•¥å¾®è¶…è¿‡å•ä¸€çš„autoguidanceï¼Œä½†ç”±äºå…¶å¢åŠ äº†è®¡ç®—å¤æ‚åº¦å’Œé¢å¤–æ—¶é—´å¼€é”€ï¼Œä½¿å¾—autoguidanceæˆ–å‡åŒ€éšæœºæ•°æ®é€‰æ‹©åœ¨å¤šæ•°å®é™…åœºæ™¯ä¸­æ›´å…·ä¼˜åŠ¿ã€‚ç ”ç©¶æœ€ç»ˆæŒ‡å‡ºï¼Œæ ·æœ¬è´¨é‡çš„ç¨³å¥æ”¹è¿›ä¸»è¦ç”±autoguidanceé©±åŠ¨ï¼Œè€Œæ•°æ®é€‰æ‹©çš„æ”¶ç›Šä¸»è¦å±€é™äºè®­ç»ƒåˆæœŸã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted non-archival paper at ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)",
      "pdf_url": "https://arxiv.org/pdf/2509.15267v1",
      "published_date": "2025-09-18 10:09:04 UTC",
      "updated_date": "2025-09-18 10:09:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:27:03.588722+00:00"
    },
    {
      "arxiv_id": "2509.14803v4",
      "title": "OnlineMate: An LLM-Based Multi-Agent Companion System for Cognitive Support in Online Learning",
      "title_zh": "OnlineMateï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åœ¨çº¿å­¦ä¹ è®¤çŸ¥æ”¯æŒå¤šæ™ºèƒ½ä½“ä¼™ä¼´ç³»ç»Ÿ",
      "authors": [
        "Xian Gao",
        "Zongyun Zhang",
        "Ting Liu",
        "Yuzhuo Fu"
      ],
      "abstract": "In online learning environments, students often lack personalized peer interactions, which are crucial for cognitive development and learning engagement. Although previous studies have employed large language models (LLMs) to simulate interactive learning environments, these interactions are limited to conversational exchanges, failing to adapt to learners' individualized cognitive and psychological states. As a result, students' engagement is low and they struggle to gain inspiration. To address this challenge, we propose OnlineMate, a multi-agent learning companion system driven by LLMs integrated with Theory of Mind (ToM). OnlineMate simulates peer-like roles, infers learners' psychological states such as misunderstandings and confusion during collaborative discussions, and dynamically adjusts interaction strategies to support higher-order thinking. Comprehensive evaluations, including simulation-based experiments, human assessments, and real classroom trials, demonstrate that OnlineMate significantly promotes deep learning and cognitive engagement by elevating students' average cognitive level while substantially improving emotional engagement scores.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨çº¿å­¦ä¹ ä¸­å­¦ç”Ÿç¼ºä¹ä¸ªæ€§åŒ–åŒä¼´äº’åŠ¨ä»¥åŠç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)éš¾ä»¥é€‚åº”å­¦ä¹ è€…è®¤çŸ¥ä¸å¿ƒç†çŠ¶æ€çš„é—®é¢˜ï¼Œæå‡ºäº†OnlineMateã€‚OnlineMate æ˜¯ä¸€ç§æ•´åˆäº†å¿ƒç†ç†è®º(Theory of Mind, ToM)çš„å¤šæ™ºèƒ½ä½“(Multi-Agent)å­¦ä¹ ä¼™ä¼´ç³»ç»Ÿï¼Œæ—¨åœ¨æ¨¡æ‹ŸåŒä¼´è§’è‰²å¹¶æä¾›è®¤çŸ¥æ”¯æŒã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ¨æ–­å­¦ä¹ è€…åœ¨åä½œè®¨è®ºä¸­çš„è¯¯è§£æˆ–å›°æƒ‘ç­‰å¿ƒç†çŠ¶æ€ï¼Œå¹¶åŠ¨æ€è°ƒæ•´äº’åŠ¨ç­–ç•¥ä»¥æ¿€å‘é«˜é˜¶æ€ç»´(Higher-Order Thinking)ã€‚é€šè¿‡æ¨¡æ‹Ÿå®éªŒã€äººå·¥è¯„ä¼°åŠçœŸå®è¯¾å ‚è¯•éªŒçš„å…¨é¢éªŒè¯ï¼Œç»“æœè¡¨æ˜ OnlineMate æ˜¾è‘—æé«˜äº†å­¦ç”Ÿçš„å¹³å‡è®¤çŸ¥æ°´å¹³ï¼Œä¿ƒè¿›äº†æ·±åº¦å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿåœ¨æå‡å­¦ç”Ÿçš„æƒ…æ„Ÿå‚ä¸åº¦(Emotional Engagement)æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæœ‰æ•ˆè§£å†³äº†åœ¨çº¿å­¦ä¹ ä¸­å‚ä¸åº¦ä½å’Œéš¾ä»¥è·å¾—å¯å‘çš„é—®é¢˜ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "work in progress",
      "pdf_url": "https://arxiv.org/pdf/2509.14803v4",
      "published_date": "2025-09-18 09:56:45 UTC",
      "updated_date": "2026-01-07 08:39:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:27:00.099630+00:00"
    },
    {
      "arxiv_id": "2509.19354v1",
      "title": "RoadMind: Towards a Geospatial AI Expert for Disaster Response",
      "title_zh": "RoadMindï¼šè¿ˆå‘é¢å‘ç¾éš¾å“åº”çš„åœ°ç†ç©ºé—´äººå·¥æ™ºèƒ½ä¸“å®¶",
      "authors": [
        "Ahmed El Fekih Zguir",
        "Ferda Ofli",
        "Muhammad Imran"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive performance across a range of natural language tasks, but remain limited in their ability to reason about geospatial data, particularly road networks, distances, and directions. This gap poses challenges in disaster scenarios, where spatial understanding is critical for tasks such as evacuation planning and resource allocation. In this work, we present RoadMind, a self-supervised framework that enhances the geospatial reasoning capabilities of LLMs using structured data from OpenStreetMap (OSM). Our automated pipeline extracts road infrastructure data for a given city and converts it into multiple supervision formats tailored to key spatial tasks. We pretrain and fine-tune LLMs on these representations using QLoRA adapters and 4-bit quantized models. We evaluate our approach on three disaster-prone cities with varying global representation, Los Angeles, Christchurch, and Manila, across tasks such as road segment identification, nearest road retrieval, and distance/direction estimation. Our results show that models trained via RoadMind significantly outperform strong baselines, including state-of-the-art LLMs equipped with advanced prompt engineering. This demonstrates the potential of structured geospatial data to enhance language models with robust spatial reasoning, enabling more effective offline AI systems for disaster response.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åœ°ç†ç©ºé—´æ¨ç†ï¼ˆç‰¹åˆ«æ˜¯é“è·¯ç½‘ç»œã€è·ç¦»å’Œæ–¹å‘ï¼‰æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†RoadMindï¼Œä¸€ä¸ªåˆ©ç”¨OpenStreetMap (OSM)ç»“æ„åŒ–æ•°æ®å¢å¼ºåœ°ç†ç©ºé—´æ¨ç†èƒ½åŠ›çš„è‡ªç›‘ç£æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–æµæ°´çº¿æå–é“è·¯åŸºç¡€è®¾æ–½æ•°æ®ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºé’ˆå¯¹ç©ºé—´ä»»åŠ¡å®šåˆ¶çš„ç›‘ç£æ ¼å¼ï¼Œéšåä½¿ç”¨QLoRAé€‚é…å™¨å’Œ4æ¯”ç‰¹é‡åŒ–æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒä¸å¾®è°ƒã€‚ç ”ç©¶åœ¨æ´›æ‰çŸ¶ã€åŸºç£åŸå’Œé©¬å°¼æ‹‰ä¸‰ä¸ªåŸå¸‚å¯¹é“è·¯æ®µè¯†åˆ«ã€æœ€è¿‘é“è·¯æ£€ç´¢åŠè·ç¦»/æ–¹å‘ä¼°ç®—ç­‰ä»»åŠ¡è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRoadMindæ˜¾è‘—ä¼˜äºåŒ…æ‹¬é‡‡ç”¨é«˜çº§æç¤ºå·¥ç¨‹çš„SOTA LLMsåœ¨å†…çš„å¼ºåŸºçº¿æ¨¡å‹ã€‚è¿™ä¸€æˆæœè¯æ˜äº†ç»“æ„åŒ–åœ°ç†ç©ºé—´æ•°æ®åœ¨æå‡è¯­è¨€æ¨¡å‹ç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºç¾å®³å“åº”åœºæ™¯ä¸‹å¼€å‘é«˜æ•ˆçš„ç¦»çº¿äººå·¥æ™ºèƒ½ä¸“å®¶ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19354v1",
      "published_date": "2025-09-18 09:46:55 UTC",
      "updated_date": "2025-09-18 09:46:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:27:17.790435+00:00"
    },
    {
      "arxiv_id": "2509.14788v1",
      "title": "Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery",
      "title_zh": "é¢å‘è¯ç‰©å‘ç°çš„èåˆç»†ç²’åº¦ç»“åˆè¡¨å¾çš„ç»“æ„æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ",
      "authors": [
        "Jing Lan",
        "Hexiao Ding",
        "Hongzhao Chen",
        "Yufeng Jiang",
        "Nga-Chun Ng",
        "Gwing Kei Yip",
        "Gerald W. Y. Cheng",
        "Yunlin Mao",
        "Jing Cai",
        "Liang-ting Lin",
        "Jung Sun Yoo"
      ],
      "abstract": "Accurate identification of drug-target interactions (DTI) remains a central challenge in computational pharmacology, where sequence-based methods offer scalability. This work introduces a sequence-based drug-target interaction framework that integrates structural priors into protein representations while maintaining high-throughput screening capability. Evaluated across multiple benchmarks, the model achieves state-of-the-art performance on Human and BioSNAP datasets and remains competitive on BindingDB. In virtual screening tasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in AUROC and BEDROC. Ablation studies confirm the critical role of learned aggregation, bilinear attention, and contrastive alignment in enhancing predictive robustness. Embedding visualizations reveal improved spatial correspondence with known binding pockets and highlight interpretable attention patterns over ligand-residue contacts. These results validate the framework's utility for scalable and structure-aware DTI prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§èåˆç»“æ„å…ˆéªŒ(structural priors)çš„åºåˆ—åŒ–è¯ç‰©-é¶ç‚¹ç›¸äº’ä½œç”¨(DTI)é¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨å¹³è¡¡é¢„æµ‹ç²¾åº¦ä¸é«˜é€šé‡ç­›é€‰çš„æ•ˆç‡ã€‚è¯¥æ¨¡å‹é€šè¿‡é›†æˆå­¦ä¹ èšåˆ(learned aggregation)ã€åŒçº¿æ€§æ³¨æ„åŠ›(bilinear attention)å’Œå¯¹æ¯”å¯¹é½(contrastive alignment)æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†è¡¨å¾çš„é²æ£’æ€§ä¸ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨Humanå’ŒBioSNAPæ•°æ®é›†ä¸Šå–å¾—äº†State-of-the-artçš„æ€§èƒ½ï¼Œå¹¶åœ¨LIT-PCBAè™šæ‹Ÿç­›é€‰ä»»åŠ¡ä¸­å¤§å¹…è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚é€šè¿‡å¯è§†åŒ–åˆ†æï¼Œç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ¨¡å‹åœ¨è¯†åˆ«ç»“åˆä½ç‚¹(binding pockets)ä»¥åŠé…ä½“-æ®‹åŸºæ¥è§¦(ligand-residue contacts)æ–¹é¢çš„å¯è§£é‡Šæ€§ã€‚è¿™äº›å‘ç°éªŒè¯äº†è¯¥æ¡†æ¶åœ¨å¯æ‰©å±•ä¸”å…·å¤‡ç»“æ„æ„ŸçŸ¥èƒ½åŠ›çš„è¯ç‰©ç ”å‘ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14788v1",
      "published_date": "2025-09-18 09:38:46 UTC",
      "updated_date": "2025-09-18 09:38:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:27:17.094693+00:00"
    },
    {
      "arxiv_id": "2509.14778v2",
      "title": "OpenLens AI: Fully Autonomous Research Agent for Health Infomatics",
      "title_zh": "OpenLens AIï¼šé¢å‘å¥åº·ä¿¡æ¯å­¦çš„å…¨è‡ªä¸»ç ”ç©¶æ™ºèƒ½ä½“",
      "authors": [
        "Yuxiao Cheng",
        "Jinli Suo"
      ],
      "abstract": "Health informatics research is characterized by diverse data modalities, rapid knowledge expansion, and the need to integrate insights across biomedical science, data analytics, and clinical practice. These characteristics make it particularly well-suited for agent-based approaches that can automate knowledge exploration, manage complex workflows, and generate clinically meaningful outputs. Recent progress in large language model (LLM)-based agents has demonstrated promising capabilities in literature synthesis, data analysis, and even end-to-end research execution. However, existing systems remain limited for health informatics because they lack mechanisms to interpret medical visualizations and often overlook domain-specific quality requirements. To address these gaps, we introduce OpenLens AI, a fully automated framework tailored to health informatics. OpenLens AI integrates specialized agents for literature review, data analysis, code generation, and manuscript preparation, enhanced by vision-language feedback for medical visualization and quality control for reproducibility. The framework automates the entire research pipeline, producing publication-ready LaTeX manuscripts with transparent and traceable workflows, thereby offering a domain-adapted solution for advancing health informatics research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¥åº·ä¿¡æ¯å­¦é¢†åŸŸæ•°æ®æ¨¡æ€å¤šæ ·åŠçŸ¥è¯†æ‰©å¼ è¿…é€Ÿçš„ç‰¹ç‚¹ï¼ŒæŒ‡å‡ºäº†ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨åŒ»å­¦å¯è§†åŒ–(medical visualizations)ç†è§£åŠé¢†åŸŸè´¨é‡æ§åˆ¶æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº† OpenLens AIï¼Œä¸€ä¸ªä¸“ä¸ºå¥åº·ä¿¡æ¯å­¦è®¾è®¡çš„å…¨è‡ªåŠ¨ç ”ç©¶æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°çŸ¥è¯†æ¢ç´¢ä¸å¤æ‚å·¥ä½œæµç®¡ç†çš„è‡ªåŠ¨åŒ–ã€‚è¯¥æ¡†æ¶é›†æˆäº†è´Ÿè´£æ–‡çŒ®ç»¼è¿°ã€æ•°æ®åˆ†æã€ä»£ç ç”ŸæˆåŠç¨¿ä»¶å‡†å¤‡çš„ä¸“ä¸šæ™ºèƒ½ä½“ï¼Œå¹¶å¼•å…¥äº†è§†è§‰è¯­è¨€åé¦ˆ(vision-language feedback)æœºåˆ¶ä»¥å¢å¼ºå¯¹åŒ»å­¦å›¾åƒçš„å¤„ç†å’Œå¯¹ç ”ç©¶å¯é‡å¤æ€§çš„è´¨é‡æ§åˆ¶ã€‚OpenLens AI èƒ½å¤Ÿç«¯åˆ°ç«¯åœ°è‡ªåŠ¨åŒ–æ•´ä¸ªç ”ç©¶ç®¡çº¿ï¼Œäº§å‡ºå…·æœ‰é€æ˜æ€§å’Œå¯è¿½æº¯æ€§çš„ LaTeX æ‰‹ç¨¿ã€‚è¿™ä¸€é¢†åŸŸé€‚åº”æ€§(domain-adapted)çš„è§£å†³æ–¹æ¡ˆä¸ºåŠ é€Ÿç”Ÿç‰©åŒ»å­¦ã€æ•°æ®åˆ†æä¸ä¸´åºŠå®è·µçš„æ·±åº¦èåˆæä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14778v2",
      "published_date": "2025-09-18 09:25:57 UTC",
      "updated_date": "2025-09-23 01:37:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:27:18.588082+00:00"
    },
    {
      "arxiv_id": "2511.00002v1",
      "title": "VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games",
      "title_zh": "VRScoutï¼šé¢å‘è™šæ‹Ÿç°å®æ¸¸æˆçš„å®æ—¶è‡ªä¸»æµ‹è¯•",
      "authors": [
        "Yurun Wu",
        "Yousong Sun",
        "Burkhard Wunsche",
        "Jia Wang",
        "Elliott Wen"
      ],
      "abstract": "Virtual Reality (VR) has rapidly become a mainstream platform for gaming and interactive experiences, yet ensuring the quality, safety, and appropriateness of VR content remains a pressing challenge. Traditional human-based quality assurance is labor-intensive and cannot scale with the industry's rapid growth. While automated testing has been applied to traditional 2D and 3D games, extending it to VR introduces unique difficulties due to high-dimensional sensory inputs and strict real-time performance requirements. We present VRScout, a deep learning-based agent capable of autonomously navigating VR environments and interacting with virtual objects in a human-like and real-time manner. VRScout learns from human demonstrations using an enhanced Action Chunking Transformer that predicts multi-step action sequences. This enables our agent to capture higher-level strategies and generalize across diverse environments. To balance responsiveness and precision, we introduce a dynamically adjustable sliding horizon that adapts the agent's temporal context at runtime. We evaluate VRScout on commercial VR titles and show that it achieves expert-level performance with only limited training data, while maintaining real-time inference at 60 FPS on consumer-grade hardware. These results position VRScout as a practical and scalable framework for automated VR game testing, with direct applications in both quality assurance and safety auditing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Virtual Reality (VR) æ¸¸æˆåœ¨è´¨é‡ä¿è¯å’Œå®‰å…¨å®¡è®¡ä¸­é¢ä¸´çš„åŠ³åŠ¨å¯†é›†å‹åŠé«˜ç»´åº¦æ„Ÿå®˜è¾“å…¥æŒ‘æˆ˜ï¼Œæå‡ºäº†VRScoutï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå®æ—¶ã€è‡ªä¸»å¯¼èˆªå¹¶ä¸è™šæ‹Ÿå¯¹è±¡äº¤äº’çš„æ·±åº¦å­¦ä¹ æ™ºèƒ½ä½“ã€‚VRScouté€šè¿‡å¢å¼ºçš„Action Chunking Transformerä»äººç±»æ¼”ç¤ºä¸­å­¦ä¹ ï¼Œèƒ½å¤Ÿé¢„æµ‹å¤šæ­¥åŠ¨ä½œåºåˆ—ï¼Œä»è€Œæ•æ‰é«˜å±‚ç­–ç•¥å¹¶åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­å®ç°æ³›åŒ–ã€‚ä¸ºå¹³è¡¡å“åº”é€Ÿåº¦ä¸ç²¾ç¡®åº¦ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŠ¨æ€å¯è°ƒçš„æ»‘åŠ¨çª—å£(dynamically adjustable sliding horizon)ï¼Œåœ¨è¿è¡Œæ—¶è‡ªé€‚åº”è°ƒæ•´æ™ºèƒ½ä½“çš„æ—¶é—´ä¸Šä¸‹æ–‡ã€‚åœ¨å•†ä¸šVRæ¸¸æˆä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ™ºèƒ½ä½“ä»…éœ€å°‘é‡è®­ç»ƒæ•°æ®å³å¯è¾¾åˆ°ä¸“å®¶çº§è¡¨ç°ï¼Œä¸”åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šèƒ½ä¿æŒ60 FPSçš„å®æ—¶æ¨ç†é€Ÿåº¦ã€‚VRScoutä¸ºè‡ªåŠ¨åŒ–VRæ¸¸æˆæµ‹è¯•æä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”å¯æ‰©å±•çš„æ¡†æ¶ï¼Œåœ¨è´¨é‡ä¿è¯å’Œå®‰å…¨å®¡è®¡é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00002v1",
      "published_date": "2025-09-18 09:16:05 UTC",
      "updated_date": "2025-09-18 09:16:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:27:26.773801+00:00"
    },
    {
      "arxiv_id": "2509.14750v2",
      "title": "Enhancing Retrieval Augmentation via Adversarial Collaboration",
      "title_zh": "é€šè¿‡å¯¹æŠ—åä½œå¼ºåŒ–æ£€ç´¢å¢å¼º",
      "authors": [
        "Letian Zhang",
        "Guanghao Meng",
        "Xudong Ren",
        "Yiming Wang",
        "Shu-Tao Xia"
      ],
      "abstract": "Retrieval-augmented Generation (RAG) is a prevalent approach for domain-specific LLMs, yet it is often plagued by \"Retrieval Hallucinations\"--a phenomenon where fine-tuned models fail to recognize and act upon poor-quality retrieved documents, thus undermining performance. To address this, we propose the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two heterogeneous agents: a generalist Detector that identifies knowledge gaps, and a domain-specialized Resolver that provides precise solutions. Guided by a moderator, these agents engage in an adversarial collaboration, where the Detector's persistent questioning challenges the Resolver's expertise. This dynamic process allows for iterative problem dissection and refined knowledge retrieval. Extensive experiments show that AC-RAG significantly improves retrieval accuracy and outperforms state-of-the-art RAG methods across various vertical domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-augmented Generation, RAG)ä¸­å¸¸è§çš„â€œæ£€ç´¢å¹»è§‰â€(Retrieval Hallucinations)ç°è±¡ï¼Œå³æ¨¡å‹éš¾ä»¥è¯†åˆ«å¹¶å¤„ç†ä½è´¨é‡æ£€ç´¢æ–‡æ¡£çš„é—®é¢˜ï¼Œæå‡ºäº†å¯¹æŠ—åä½œRAG(Adversarial Collaboration RAG, AC-RAG)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸¤ä¸ªå¼‚æ„æ™ºèƒ½ä½“ï¼šä¸€ä¸ªè´Ÿè´£è¯†åˆ«çŸ¥è¯†ç¼ºå£çš„é€šç”¨æ£€æµ‹å™¨(Detector)å’Œä¸€ä¸ªæä¾›ç²¾ç¡®è§£å†³æ–¹æ¡ˆçš„é¢†åŸŸä¸“é—¨è§£æå™¨(Resolver)ã€‚åœ¨åè°ƒå‘˜(Moderator)çš„å¼•å¯¼ä¸‹ï¼Œè¿™äº›æ™ºèƒ½ä½“è¿›è¡Œå¯¹æŠ—æ€§åä½œï¼Œé€šè¿‡æ£€æµ‹å™¨çš„æŒç»­è´¨ç–‘æ¥æŒ‘æˆ˜è§£æå™¨çš„ä¸“ä¸šæ€§ã€‚è¿™ç§åŠ¨æ€è¿‡ç¨‹ä¿ƒè¿›äº†é—®é¢˜çš„è¿­ä»£å‰–æå’ŒçŸ¥è¯†æ£€ç´¢çš„è¿›ä¸€æ­¥å®Œå–„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAC-RAGåœ¨å¤šä¸ªå‚ç›´é¢†åŸŸæ˜¾è‘—æå‡äº†æ£€ç´¢å‡†ç¡®æ€§ï¼Œå…¶è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›(state-of-the-art)RAGæ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Due to some internal policies, we need to temporarily withdraw the paper. We will resubmit it after a further review",
      "pdf_url": "https://arxiv.org/pdf/2509.14750v2",
      "published_date": "2025-09-18 08:54:20 UTC",
      "updated_date": "2026-01-19 03:30:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:27:41.160441+00:00"
    },
    {
      "arxiv_id": "2509.16268v1",
      "title": "Digging Into the Internal: Causality-Based Analysis of LLM Function Calling",
      "title_zh": "æ·±å…¥å†…éƒ¨ï¼šåŸºäºå› æœæ€§çš„å¤§è¯­è¨€æ¨¡å‹å‡½æ•°è°ƒç”¨åˆ†æ",
      "authors": [
        "Zhenlan Ji",
        "Daoyuan Wu",
        "Wenxuan Wang",
        "Pingchuan Ma",
        "Shuai Wang",
        "Lei Ma"
      ],
      "abstract": "Function calling (FC) has emerged as a powerful technique for facilitating large language models (LLMs) to interact with external systems and perform structured tasks. However, the mechanisms through which it influences model behavior remain largely under-explored. Besides, we discover that in addition to the regular usage of FC, this technique can substantially enhance the compliance of LLMs with user instructions. These observations motivate us to leverage causality, a canonical analysis method, to investigate how FC works within LLMs. In particular, we conduct layer-level and token-level causal interventions to dissect FC's impact on the model's internal computational logic when responding to user queries. Our analysis confirms the substantial influence of FC and reveals several in-depth insights into its mechanisms. To further validate our findings, we conduct extensive experiments comparing the effectiveness of FC-based instructions against conventional prompting methods. We focus on enhancing LLM safety robustness, a critical LLM application scenario, and evaluate four mainstream LLMs across two benchmark datasets. The results are striking: FC shows an average performance improvement of around 135% over conventional prompting methods in detecting malicious inputs, demonstrating its promising potential to enhance LLM reliability and capability in practical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­Function calling (FC)æŠ€æœ¯å¯¹æ¨¡å‹è¡Œä¸ºå½±å“æœºåˆ¶å°šä¸æ˜ç¡®çš„é—®é¢˜ï¼Œåˆ©ç”¨Causalityåˆ†ææ–¹æ³•è¿›è¡Œäº†æ·±å…¥æ¢ç©¶ã€‚ä½œè€…é€šè¿‡åœ¨å±‚çº§(layer-level)å’Œæ ‡è®°çº§(token-level)è¿›è¡Œå› æœå¹²é¢„(causal interventions)ï¼Œè§£æäº†FCåœ¨å“åº”ç”¨æˆ·æŸ¥è¯¢æ—¶å¯¹æ¨¡å‹å†…éƒ¨è®¡ç®—é€»è¾‘çš„å…·ä½“å½±å“ã€‚ç ”ç©¶å‘ç°FCä¸ä»…æ˜¯å¤–éƒ¨äº¤äº’çš„å·¥å…·ï¼Œè¿˜èƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹å¯¹ç”¨æˆ·æŒ‡ä»¤çš„æœä»æ€§(compliance)ã€‚ä¸ºäº†éªŒè¯è¿™äº›å‘ç°ï¼Œç ”ç©¶åœ¨LLMå®‰å…¨æ€§é²æ£’æ€§(safety robustness)åœºæ™¯ä¸‹è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå¯¹æ¯”äº†åŸºäºFCçš„æŒ‡ä»¤ä¸ä¼ ç»Ÿæç¤º(prompting)çš„æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ£€æµ‹æ¶æ„è¾“å…¥æ–¹é¢ï¼ŒFCç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å¹³å‡æ€§èƒ½æå‡äº†çº¦135%ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†FCå†…éƒ¨æœºåˆ¶çš„æ·±åˆ»è§è§£ï¼Œè¯æ˜äº†å…¶åœ¨æå‡æ¨¡å‹å¯é æ€§å’Œå®é™…åº”ç”¨èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16268v1",
      "published_date": "2025-09-18 08:30:26 UTC",
      "updated_date": "2025-09-18 08:30:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:27:36.259763+00:00"
    },
    {
      "arxiv_id": "2509.15259v1",
      "title": "IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders",
      "title_zh": "IEFS-GMBï¼šåŸºäºä¿¡æ¯ç†µä¸æ¢¯åº¦è®°å¿†åº“å¼•å¯¼çš„ç¥ç»ç³»ç»Ÿç–¾ç—… EEG åˆ†ç±»ç‰¹å¾é€‰æ‹©",
      "authors": [
        "Liang Zhang",
        "Hanyang Dong",
        "Jia-Hong Gao",
        "Yi Sun",
        "Kuntao Xiao",
        "Wanli Yang",
        "Zhao Lv",
        "Shurong Sheng"
      ],
      "abstract": "Deep learning-based EEG classification is crucial for the automated detection of neurological disorders, improving diagnostic accuracy and enabling early intervention. However, the low signal-to-noise ratio of EEG signals limits model performance, making feature selection (FS) vital for optimizing representations learned by neural network encoders. Existing FS methods are seldom designed specifically for EEG diagnosis; many are architecture-dependent and lack interpretability, limiting their applicability. Moreover, most rely on single-iteration data, resulting in limited robustness to variability. To address these issues, we propose IEFS-GMB, an Information Entropy-based Feature Selection method guided by a Gradient Memory Bank. This approach constructs a dynamic memory bank storing historical gradients, computes feature importance via information entropy, and applies entropy-based weighting to select informative EEG features. Experiments on four public neurological disease datasets show that encoders enhanced with IEFS-GMB achieve accuracy improvements of 0.64% to 6.45% over baseline models. The method also outperforms four competing FS techniques and improves model interpretability, supporting its practical use in clinical settings.",
      "tldr_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„è„‘ç”µå›¾ (EEG) åˆ†ç±»å¯¹ç¥ç»ç³»ç»Ÿç–¾ç—…çš„è‡ªåŠ¨åŒ–æ£€æµ‹è‡³å…³é‡è¦ï¼Œä½†ä¿¡å·çš„ä½ä¿¡å™ªæ¯”ä»¥åŠç°æœ‰ç‰¹å¾é€‰æ‹© (FS) æ–¹æ³•åœ¨æ¶æ„ä¾èµ–æ€§å’Œç¨³å¥æ€§æ–¹é¢çš„å±€é™æ€§åˆ¶çº¦äº†æ¨¡å‹æ€§èƒ½ã€‚è¯¥ç ”ç©¶æå‡ºäº† IEFS-GMBï¼Œä¸€ç§åŸºäºä¿¡æ¯ç†µ (Information Entropy) ä¸”ç”±æ¢¯åº¦å­˜å‚¨åº“ (Gradient Memory Bank) å¼•å¯¼çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºåŠ¨æ€å­˜å‚¨åº“æ¥ä¿å­˜å†å²æ¢¯åº¦ï¼Œåˆ©ç”¨ä¿¡æ¯ç†µè®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼Œå¹¶å®æ–½åŸºäºç†µçš„æƒé‡åˆ†é…ä»¥ç­›é€‰å‡ºå…·æœ‰ä¿¡æ¯é‡çš„è„‘ç”µå›¾ç‰¹å¾ã€‚åœ¨å››ä¸ªå…¬å…±ç¥ç»ç³»ç»Ÿç–¾ç—…æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œé›†æˆ IEFS-GMB çš„ç¼–ç å™¨ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šæå‡äº† 0.64% è‡³ 6.45%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºå››ç§ç«äº‰æ€§çš„ç‰¹å¾é€‰æ‹©æŠ€æœ¯ï¼Œå¹¶æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ (Interpretability)ï¼Œä¸ºå…¶åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15259v1",
      "published_date": "2025-09-18 08:14:17 UTC",
      "updated_date": "2025-09-18 08:14:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:27:36.598861+00:00"
    },
    {
      "arxiv_id": "2509.15258v1",
      "title": "Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸æ— çº¿æ„ŸçŸ¥çš„èåˆï¼šè¿ˆå‘æ— çº¿åŸºç¡€æ¨¡å‹",
      "authors": [
        "Zheng Yang",
        "Guoxuan Chi",
        "Chenshu Wu",
        "Hanyu Liu",
        "Yuchong Gao",
        "Yunhao Liu",
        "Jie Xu",
        "Tony Xiao Han"
      ],
      "abstract": "Generative Artificial Intelligence (GenAI) has made significant advancements in fields such as computer vision (CV) and natural language processing (NLP), demonstrating its capability to synthesize high-fidelity data and improve generalization. Recently, there has been growing interest in integrating GenAI into wireless sensing systems. By leveraging generative techniques such as data augmentation, domain adaptation, and denoising, wireless sensing applications, including device localization, human activity recognition, and environmental monitoring, can be significantly improved. This survey investigates the convergence of GenAI and wireless sensing from two complementary perspectives. First, we explore how GenAI can be integrated into wireless sensing pipelines, focusing on two modes of integration: as a plugin to augment task-specific models and as a solver to directly address sensing tasks. Second, we analyze the characteristics of mainstream generative models, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion models, and discuss their applicability and unique advantages across various wireless sensing tasks. We further identify key challenges in applying GenAI to wireless sensing and outline a future direction toward a wireless foundation model: a unified, pre-trained design capable of scalable, adaptable, and efficient signal understanding across diverse sensing tasks.",
      "tldr_zh": "è¯¥ç»¼è¿°æ·±å…¥æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) ä¸æ— çº¿æ„ŸçŸ¥ (Wireless Sensing) çš„ç»“åˆï¼Œæ—¨åœ¨é€šè¿‡æå‡æ•°æ®åˆæˆä¸æ³›åŒ–èƒ½åŠ›æ¥ä¼˜åŒ–è®¾å¤‡å®šä½å’Œäººä½“æ´»åŠ¨è¯†åˆ«ç­‰ä»»åŠ¡ã€‚ç ”ç©¶ä»ä¸¤ä¸ªæ ¸å¿ƒè§†è§’å±•å¼€ï¼šé¦–å…ˆåˆ†æäº† GenAI ä½œä¸ºå¢å¼ºæ’ä»¶ (Plugin) æˆ–ç›´æ¥æ±‚è§£å™¨ (Solver) çš„é›†æˆè·¯å¾„ï¼›å…¶æ¬¡è¯„ä¼°äº† GANsã€VAEs åŠæ‰©æ•£æ¨¡å‹ (Diffusion Models) åœ¨æ•°æ®å¢å¼º (Data Augmentation) å’Œå»å™ª (Denoising) ç­‰æ–¹é¢çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚é€šè¿‡æ•´åˆé¢†åŸŸè‡ªé€‚åº” (Domain Adaptation) æŠ€æœ¯ï¼Œè¯¥èåˆæ–¹æ¡ˆæ˜¾è‘—æå‡äº†å¤æ‚ç¯å¢ƒä¸‹çš„æ„ŸçŸ¥ç²¾åº¦ã€‚æ–‡ç« æœ€åè¯†åˆ«äº†ç°æœ‰çš„åº”ç”¨æŒ‘æˆ˜ï¼Œå¹¶å‰ç»æ€§åœ°æå‡ºäº†æ— çº¿åŸºç¡€æ¨¡å‹ (Wireless Foundation Model) çš„å‘å±•è“å›¾ï¼Œæ—¨åœ¨æ„å»ºä¸€ä¸ªèƒ½å¤Ÿè·¨å¤šæ ·åŒ–æ„ŸçŸ¥ä»»åŠ¡è¿›è¡Œé«˜æ•ˆã€å¯æ‰©å±•ä¿¡å·ç†è§£çš„ç»Ÿä¸€é¢„è®­ç»ƒæ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15258v1",
      "published_date": "2025-09-18 07:51:25 UTC",
      "updated_date": "2025-09-18 07:51:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:27:37.274757+00:00"
    },
    {
      "arxiv_id": "2509.14704v2",
      "title": "Japanese Children's Riddles as a Benchmark for Machine Insight and Metacognition",
      "title_zh": "æ—¥æœ¬å„¿ç«¥è°œè¯­ï¼šæœºå™¨æ´å¯ŸåŠ›ä¸å…ƒè®¤çŸ¥çš„è¯„æµ‹åŸºå‡†",
      "authors": [
        "Masaharu Mizumoto",
        "Dat Nguyen",
        "Zhiheng Han",
        "Jiyuan Fang",
        "Heyuan Guan",
        "Xingfu Li",
        "Naoya Shiraishi",
        "Yo Nakawake",
        "Le Minh Nguyen"
      ],
      "abstract": "Benchmark saturation and contamination have obscured genuine advances in reasoning for large language models (LLMs). We introduce NazoNazo Benchmark, a low-cost, renewable test built from Japanese children's riddles that demand insight-based reasoning, or representational shifts rather than knowledge recall. We evaluate 38 frontier LLMs (2023-2025) on 201 riddles and a 120-item human-comparison subset, finding that non-reasoning models average 7.6%, reasoning models 17.6%, and humans ~53% accuracy. Importantly, thought-log analysis reveals that reasoning in Japanese did not necessarily improve accuracy, indicating that language understanding alone is insufficient for insight reasoning. Notably, models sometimes generated correct candidates but failed to endorse them, suggesting weak metacognitive control rather than a lack of knowledge. This \"verification failure\" indicates that CoT outputs can reflect genuine intermediate reasoning states rather than post-hoc rationalizations. By exposing this metacognitive bottleneck - models' inability to recognize when they are right - the benchmark provides a scalable, cross-linguistic testbed for studying machine insight, confidence calibration, and self-evaluation. NazoNazo Benchmark thus offers not only a fresh challenge to current LLMs but also a concrete target for developing AI metacognitive psychology and enhancing machine Aha! capability.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† NazoNazo Benchmarkï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ—¥æœ¬å„¿ç«¥è°œè¯­æ„å»ºçš„ä½æˆæœ¬ã€å¯å†ç”Ÿçš„è¯„ä¼°æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´çš„åŸºå‡†é¥±å’Œä¸æ•°æ®æ±¡æŸ“é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„çŸ¥è¯†å¬å›æµ‹è¯•ä¸åŒï¼Œè¯¥åŸºå‡†ä¾§é‡äºè€ƒæŸ¥åŸºäºæ´å¯ŸåŠ›çš„æ¨ç†(insight-based reasoning)å’Œè¡¨å¾è½¬ç§»èƒ½åŠ›ã€‚é€šè¿‡å¯¹ 38 ä¸ªå‰æ²¿æ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œæ¨ç†å‹æ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡ä»…ä¸º 17.6%ï¼Œè¿œä½äºäººç±»çº¦ 53% çš„æ°´å¹³ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå•çº¯çš„è¯­è¨€ç†è§£ä¸è¶³ä»¥æ”¯æ’‘æ´å¯ŸåŠ›æ¨ç†ï¼Œä¸”æ¨¡å‹æ™®éå­˜åœ¨â€œéªŒè¯å¤±è´¥â€(verification failure)ç°è±¡ï¼Œå³å› å…ƒè®¤çŸ¥æ§åˆ¶(metacognitive control)è–„å¼±è€Œæ— æ³•è¯†åˆ«è‡ªå·±ç”Ÿæˆçš„æ­£ç¡®ç­”æ¡ˆã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†æ€ç»´é“¾(CoT)è¾“å‡ºèƒ½åæ˜ çœŸå®çš„ä¸­é—´æ¨ç†çŠ¶æ€è€Œéäº‹ååˆç†åŒ–ï¼Œå¹¶ä¸ºç ”ç©¶æœºå™¨æ´å¯ŸåŠ›ã€ç½®ä¿¡åº¦æ ¡å‡†åŠè‡ªæˆ‘è¯„ä¼°æä¾›äº†é‡è¦çš„è·¨è¯­è¨€æµ‹è¯•å¹³å°ã€‚è¯¥åŸºå‡†ä¸ä»…å‘ç°æœ‰æ¨¡å‹æå‡ºäº†æ–°æŒ‘æˆ˜ï¼Œä¹Ÿä¸ºå¼€å‘äººå·¥æ™ºèƒ½å…ƒè®¤çŸ¥å¿ƒç†å­¦å’Œå¢å¼ºæœºå™¨çš„â€œçµå…‰ä¸€ç°â€(Aha! capability)èƒ½åŠ›æä¾›äº†å…·ä½“ç›®æ ‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14704v2",
      "published_date": "2025-09-18 07:50:04 UTC",
      "updated_date": "2026-01-05 13:57:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:14.062014+00:00"
    },
    {
      "arxiv_id": "2509.15256v1",
      "title": "A Multi-Scale Graph Neural Process with Cross-Drug Co-Attention for Drug-Drug Interactions Prediction",
      "title_zh": "ç”¨äºè¯ç‰©-è¯ç‰©ç›¸äº’ä½œç”¨é¢„æµ‹çš„èåˆè·¨è¯ç‰©ååŒæ³¨æ„åŠ›å¤šå°ºåº¦å›¾ç¥ç»è¿‡ç¨‹",
      "authors": [
        "Zimo Yan",
        "Jie Zhang",
        "Zheng Xie",
        "Yiping Song",
        "Hao Li"
      ],
      "abstract": "Accurate prediction of drug-drug interactions (DDI) is crucial for medication safety and effective drug development. However, existing methods often struggle to capture structural information across different scales, from local functional groups to global molecular topology, and typically lack mechanisms to quantify prediction confidence. To address these limitations, we propose MPNP-DDI, a novel Multi-scale Graph Neural Process framework. The core of MPNP-DDI is a unique message-passing scheme that, by being iteratively applied, learns a hierarchy of graph representations at multiple scales. Crucially, a cross-drug co-attention mechanism then dynamically fuses these multi-scale representations to generate context-aware embeddings for interacting drug pairs, while an integrated neural process module provides principled uncertainty estimation. Extensive experiments demonstrate that MPNP-DDI significantly outperforms state-of-the-art baselines on benchmark datasets. By providing accurate, generalizable, and uncertainty-aware predictions built upon multi-scale structural features, MPNP-DDI represents a powerful computational tool for pharmacovigilance, polypharmacy risk assessment, and precision medicine.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MPNP-DDIï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³è¯ç‰©ç›¸äº’ä½œç”¨ (Drug-Drug Interactions, DDI) é¢„æµ‹ä¸­å¤šå°ºåº¦ç»“æ„æ•æ‰ä¸è¶³åŠç¼ºä¹ç½®ä¿¡åº¦é‡åŒ–é—®é¢˜çš„æ–°å‹å¤šå°ºåº¦å›¾ç¥ç»ç½‘ç»œè¿‡ç¨‹ (Multi-scale Graph Neural Process) æ¡†æ¶ã€‚MPNP-DDI çš„æ ¸å¿ƒé‡‡ç”¨äº†ä¸€ç§ç‹¬ç‰¹çš„è¿­ä»£æ¶ˆæ¯ä¼ é€’æœºåˆ¶ï¼Œèƒ½å¤Ÿå­¦ä¹ ä»å±€éƒ¨å®˜èƒ½å›¢åˆ°å…¨å±€åˆ†å­æ‹“æ‰‘çš„å¤šå±‚æ¬¡å›¾è¡¨ç¤ºã€‚è¯¥æ¡†æ¶é€šè¿‡è·¨è¯ç‰©ååŒæ³¨æ„åŠ› (Cross-Drug Co-Attention) æœºåˆ¶åŠ¨æ€èåˆå¤šå°ºåº¦ç‰¹å¾ï¼Œç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„è¯ç‰©å¯¹åµŒå…¥ï¼Œå¹¶ç»“åˆé›†æˆçš„ç¥ç»è¿‡ç¨‹ (Neural Process) æ¨¡å—å®ç°äº†åŸåˆ™æ€§çš„ä¸ç¡®å®šæ€§ä¼°è®¡ (Uncertainty Estimation)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMPNP-DDI åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºå‡†æ¨¡å‹ã€‚å‡­å€Ÿå…¶å‡†ç¡®ã€æ³›åŒ–èƒ½åŠ›å¼ºä¸”å…·å¤‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„èƒ½åŠ›ï¼Œè¯¥æ¨¡å‹ä¸ºè¯ç‰©è­¦æˆ’ã€å¤šè¯åˆç”¨é£é™©è¯„ä¼°å’Œç²¾å‡†åŒ»å­¦æä¾›äº†å¼ºæœ‰åŠ›çš„è®¡ç®—å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15256v1",
      "published_date": "2025-09-18 07:48:10 UTC",
      "updated_date": "2025-09-18 07:48:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:01.691326+00:00"
    },
    {
      "arxiv_id": "2509.14693v2",
      "title": "RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning",
      "title_zh": "RationAnomalyï¼šåŸºäºé“¾å¼æ€ç»´ä¸å¼ºåŒ–å­¦ä¹ çš„å…·å¤‡åˆç†æ€§çš„æ—¥å¿—å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Song Xu",
        "Yilun Liu",
        "Minggui He",
        "Mingchen Dai",
        "Ziang Chen",
        "Chunguang Zhao",
        "Jingzhou Du",
        "Shimin Tao",
        "Weibin Meng",
        "Shenglin Zhang",
        "Yongqian Sun",
        "Boxing Chen",
        "Daimeng Wei"
      ],
      "abstract": "Logs constitute a form of evidence signaling the operational status of software systems. Automated log anomaly detection is crucial for ensuring the reliability of modern software systems. However, existing approaches face significant limitations: traditional deep learning models lack interpretability and generalization, while methods leveraging Large Language Models are often hindered by unreliability and factual inaccuracies. To address these issues, we propose RationAnomaly, a novel framework that enhances log anomaly detection by synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our approach first instills expert-like reasoning patterns using CoT-guided supervised fine-tuning, grounded in a high-quality dataset corrected through a rigorous expert-driven process. Subsequently, a reinforcement learning phase with a multi-faceted reward function optimizes for accuracy and logical consistency, effectively mitigating hallucinations. Experimentally, RationAnomaly outperforms state-of-the-art baselines, achieving superior F1-scores on key benchmarks while providing transparent, step-by-step analytical outputs. We have released the corresponding resources, including code and datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RationAnomalyï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡æ—¥å¿—å¼‚å¸¸æ£€æµ‹(Log Anomaly Detection)åˆç†æ€§ä¸å‡†ç¡®æ€§çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ç¼ºä¹è§£é‡ŠåŠ›ä»¥åŠå¤§è¯­è¨€æ¨¡å‹(LLM)å­˜åœ¨äº‹å®é”™è¯¯çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡é“¾å¼æ€ç»´(Chain-of-Thought)å¼•å¯¼çš„æœ‰ç›‘ç£å¾®è°ƒï¼Œåˆ©ç”¨ç»è¿‡ä¸“å®¶ä¸¥æ ¼æ ¡æ­£çš„é«˜è´¨é‡æ•°æ®é›†æ¥åŸ¹å…»æ¨¡å‹çš„æ¨ç†æ¨¡å¼ã€‚éšåï¼Œç ”ç©¶å¼•å…¥äº†å¸¦æœ‰å¹³è¡¡å¤šç»´å¥–åŠ±å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)é˜¶æ®µï¼Œä»¥ä¼˜åŒ–æ£€æµ‹çš„å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£æ¨¡å‹çš„å¹»è§‰é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒRationAnomalyåœ¨å¤šä¸ªå…³é”®åŸºå‡†æ•°æ®é›†ä¸Šçš„F1-scoreå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæä¾›é€æ˜çš„é€æ­¥åˆ†æè¾“å‡ºï¼Œåœ¨ç¡®ä¿è½¯ä»¶ç³»ç»Ÿå¯é æ€§çš„åŒæ—¶æ˜¾è‘—æå‡äº†ç»“æœçš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.14693v2",
      "published_date": "2025-09-18 07:35:58 UTC",
      "updated_date": "2025-09-22 02:54:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:02.162630+00:00"
    },
    {
      "arxiv_id": "2509.14671v1",
      "title": "TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding",
      "title_zh": "TableDARTï¼šé¢å‘è¡¨æ ¼ç†è§£çš„åŠ¨æ€è‡ªé€‚åº”å¤šæ¨¡æ€è·¯ç”±",
      "authors": [
        "Xiaobo Xing",
        "Wei Yuan",
        "Tong Chen",
        "Quoc Viet Hung Nguyen",
        "Xiangliang Zhang",
        "Hongzhi Yin"
      ],
      "abstract": "Modeling semantic and structural information from tabular data remains a core challenge for effective table understanding. Existing Table-as-Text approaches flatten tables for large language models (LLMs), but lose crucial structural cues, while Table-as-Image methods preserve structure yet struggle with fine-grained semantics. Recent Table-as-Multimodality strategies attempt to combine textual and visual views, but they (1) statically process both modalities for every query-table pair within a large multimodal LLMs (MLLMs), inevitably introducing redundancy and even conflicts, and (2) depend on costly fine-tuning of MLLMs. In light of this, we propose TableDART, a training-efficient framework that integrates multimodal views by reusing pretrained single-modality models. TableDART introduces a lightweight 2.59M-parameter MLP gating network that dynamically selects the optimal path (either Text-only, Image-only, or Fusion) for each table-query pair, effectively reducing redundancy and conflicts from both modalities. In addition, we propose a novel agent to mediate cross-modal knowledge integration by analyzing outputs from text- and image-based models, either selecting the best result or synthesizing a new answer through reasoning. This design avoids the prohibitive costs of full MLLM fine-tuning. Extensive experiments on seven benchmarks show that TableDART establishes new state-of-the-art performance among open-source models, surpassing the strongest baseline by an average of 4.02%. The code is available at: https://anonymous.4open.science/r/TableDART-C52B",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TableDARTï¼Œä¸€ç§ç”¨äºè¡¨æ ¼ç†è§£çš„åŠ¨æ€è‡ªé€‚åº”å¤šæ¨¡æ€è·¯ç”±æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¡¨æ ¼è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯æ—¶é¢ä¸´çš„å†—ä½™ã€å†²çªä»¥åŠé«˜æ˜‚å¾®è°ƒæˆæœ¬ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡ç”¨é¢„è®­ç»ƒçš„å•æ¨¡æ€æ¨¡å‹ï¼Œå¼•å…¥äº†ä¸€ä¸ªä»…å« 2.59M å‚æ•°çš„è½»é‡çº§ MLP gating networkï¼Œèƒ½å¤Ÿä¸ºæ¯ä¸ªè¡¨æ ¼-æŸ¥è¯¢å¯¹åŠ¨æ€é€‰æ‹©æœ€ä¼˜è·¯å¾„ï¼ˆText-onlyã€Image-only æˆ– Fusionï¼‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–°å‹æ™ºèƒ½ä½“ï¼ˆagentï¼‰ï¼Œé€šè¿‡åˆ†ææ–‡æœ¬å’Œå›¾åƒæ¨¡å‹çš„è¾“å‡ºå¹¶è¿›è¡Œé€»è¾‘æ¨ç†ï¼Œæ¥åè°ƒè·¨æ¨¡æ€çŸ¥è¯†çš„æ•´åˆã€‚è¿™ç§è®¾è®¡é¿å…äº†å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œå…¨é‡å¾®è°ƒçš„æ˜‚è´µä»£ä»·ï¼Œå®ç°äº†é«˜æ•ˆçš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTableDART åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†å¼€æºæ¨¡å‹çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ˆSOTAï¼‰ï¼Œå¹³å‡è¶…è¶Šæœ€å¼ºåŸºçº¿æ¨¡å‹ 4.02%ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14671v1",
      "published_date": "2025-09-18 07:00:13 UTC",
      "updated_date": "2025-09-18 07:00:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:09.284712+00:00"
    },
    {
      "arxiv_id": "2509.14666v1",
      "title": "Spatial Audio Motion Understanding and Reasoning",
      "title_zh": "ç©ºé—´éŸ³é¢‘è¿åŠ¨ç†è§£ä¸æ¨ç†",
      "authors": [
        "Arvind Krishna Sridhar",
        "Yinyi Guo",
        "Erik Visser"
      ],
      "abstract": "Spatial audio reasoning enables machines to interpret auditory scenes by understanding events and their spatial attributes. In this work, we focus on spatial audio understanding with an emphasis on reasoning about moving sources. First, we introduce a spatial audio encoder that processes spatial audio to detect multiple overlapping events and estimate their spatial attributes, Direction of Arrival (DoA) and source distance, at the frame level. To generalize to unseen events, we incorporate an audio grounding model that aligns audio features with semantic audio class text embeddings via a cross-attention mechanism. Second, to answer complex queries about dynamic audio scenes involving moving sources, we condition a large language model (LLM) on structured spatial attributes extracted by our model. Finally, we introduce a spatial audio motion understanding and reasoning benchmark dataset and demonstrate our framework's performance against the baseline model.",
      "tldr_zh": "è¯¥ç ”ç©¶èšç„¦äºç©ºé—´éŸ³é¢‘æ¨ç†(Spatial Audio Reasoning)ï¼Œæ—¨åœ¨æå‡æœºå™¨å¯¹å¬è§‰åœºæ™¯ä¸­ç§»åŠ¨å£°æºçš„è¿åŠ¨ç†è§£ä¸æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§ç©ºé—´éŸ³é¢‘ç¼–ç å™¨(spatial audio encoder)ï¼Œå¯åœ¨å¸§çº§åˆ«æ£€æµ‹å¤šä¸ªé‡å éŸ³é¢‘äº‹ä»¶å¹¶ä¼°è®¡å…¶åˆ°è¾¾æ–¹å‘(Direction of Arrival, DoA)å’Œå£°æºè·ç¦»ã€‚ä¸ºäº†å¢å¼ºå¯¹æœªçŸ¥äº‹ä»¶çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ¡†æ¶æ•´åˆäº†éŸ³é¢‘å¯¹é½æ¨¡å‹(audio grounding model)ï¼Œåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶(cross-attention mechanism)å®ç°éŸ³é¢‘ç‰¹å¾ä¸è¯­ä¹‰æ–‡æœ¬åµŒå…¥çš„å¯¹é½ã€‚é’ˆå¯¹æ¶‰åŠç§»åŠ¨å£°æºçš„åŠ¨æ€åœºæ™¯ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ç»“æ„åŒ–ç©ºé—´å±æ€§æ¥è°ƒèŠ‚å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ï¼Œä»¥å›ç­”å¤æ‚çš„éŸ³é¢‘æŸ¥è¯¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†ä¸€ä¸ªç©ºé—´éŸ³é¢‘è¿åŠ¨ç†è§£ä¸æ¨ç†çš„åŸºå‡†æ•°æ®é›†ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤„ç†åŠ¨æ€éŸ³é¢‘ä»»åŠ¡æ–¹é¢çš„è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†æœºå™¨å¯¹å¤æ‚å¬è§‰ç¯å¢ƒçš„è¯­ä¹‰å’Œç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages, 2 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.14666v1",
      "published_date": "2025-09-18 06:53:22 UTC",
      "updated_date": "2025-09-18 06:53:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:08.290295+00:00"
    },
    {
      "arxiv_id": "2509.14662v1",
      "title": "Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory",
      "title_zh": "ç†è§£æ¨ç†æ¨¡å‹çš„æ€ç»´è¿‡ç¨‹ï¼šåŸºäº Schoenfeld æƒ…èŠ‚ç†è®ºçš„è§†è§’",
      "authors": [
        "Ming Li",
        "Nan Zhang",
        "Chenrui Fan",
        "Hong Jiao",
        "Yanbin Fu",
        "Sydney Peters",
        "Qingshu Xu",
        "Robert Lissitz",
        "Tianyi Zhou"
      ],
      "abstract": "While Large Reasoning Models (LRMs) generate extensive chain-of-thought reasoning, we lack a principled framework for understanding how these thoughts are structured. In this paper, we introduce a novel approach by applying Schoenfeld's Episode Theory, a classic cognitive framework for human mathematical problem-solving, to analyze the reasoning traces of LRMs. We annotated thousands of sentences and paragraphs from model-generated solutions to math problems using seven cognitive labels (e.g., Plan, Implement, Verify). The result is the first publicly available benchmark for the fine-grained analysis of machine reasoning, including a large annotated corpus and detailed annotation guidebooks. Our preliminary analysis reveals distinct patterns in LRM reasoning, such as the transition dynamics between cognitive states. This framework provides a theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºSchoenfeld's Episode Theoryçš„æ–°é¢–æ–¹æ³•ï¼Œæ—¨åœ¨è§£æå¤§å‹æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)ç”Ÿæˆçš„å¤æ‚é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†ç»“æ„ã€‚ç ”ç©¶å›¢é˜Ÿå°†è¿™ä¸€ç»å…¸çš„äººç±»æ•°å­¦è§£é¢˜è®¤çŸ¥æ¡†æ¶åº”ç”¨äºåˆ†æLRMsçš„æ¨ç†è½¨è¿¹ï¼Œé€šè¿‡Planã€Implementã€Verifyç­‰ä¸ƒä¸ªè®¤çŸ¥æ ‡ç­¾å¯¹æ•°åƒä¸ªæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µè¿›è¡Œäº†ç²¾ç»†åŒ–æ ‡æ³¨ã€‚è¯¥å·¥ä½œè´¡çŒ®äº†é¦–ä¸ªå…¬å¼€çš„ç»†ç²’åº¦æœºå™¨æ¨ç†åˆ†æåŸºå‡†ï¼ŒåŒ…å«å¤§è§„æ¨¡æ ‡æ³¨è¯­æ–™åº“å’Œè¯¦ç»†çš„æ ‡æ³¨æŒ‡å—ã€‚åˆæ­¥åˆ†ææ­ç¤ºäº†LRMæ¨ç†ä¸­çš„ç‹¬ç‰¹æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯ä¸åŒè®¤çŸ¥çŠ¶æ€ä¹‹é—´çš„è½¬æ¢åŠ¨æ€ï¼Œä¸ºè§£é‡ŠLRMè®¤çŸ¥æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚è¯¥æ¡†æ¶ä¸ºæœªæ¥å¼€å‘æ›´å…·å¯æ§æ€§å’Œé€æ˜åº¦çš„æ¨ç†ç³»ç»Ÿæä¾›äº†ç§‘å­¦çš„åˆ†æå·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "EMNLP2025 main, Camera-ready",
      "pdf_url": "https://arxiv.org/pdf/2509.14662v1",
      "published_date": "2025-09-18 06:42:41 UTC",
      "updated_date": "2025-09-18 06:42:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:23.472429+00:00"
    },
    {
      "arxiv_id": "2509.14657v3",
      "title": "Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework",
      "title_zh": "å®‰å…¨åè®®æ¡†æ¶ä¸‹æå‡ IoT éŸ³é¢‘åˆ†ç±»è®¾å¤‡å®‰å…¨æ€§çš„å¨èƒå»ºæ¨¡",
      "authors": [
        "Sergio Benlloch-Lopez",
        "Miquel Viel-Vazquez",
        "Javier Naranjo-Alcazar",
        "Jordi Grau-Haro",
        "Pedro Zuccarello"
      ],
      "abstract": "The rapid proliferation of IoT nodes equipped with microphones and capable of performing on-device audio classification exposes highly sensitive data while operating under tight resource constraints. To protect against this, we present a defence-in-depth architecture comprising a security protocol that treats the edge device, cellular network and cloud backend as three separate trust domains, linked by TPM-based remote attestation and mutually authenticated TLS 1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At startup, each boot stage is measured into TPM PCRs. The node can only decrypt its LUKS-sealed partitions after the cloud has verified a TPM quote and released a one-time unlock key. This ensures that rogue or tampered devices remain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber and Dilithium to provide post-quantum resilience. Meanwhile, end-to-end encryption and integrity hashes safeguard extracted audio features. Signed, rollback-protected AI models and tamper-responsive sensors harden firmware and hardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive sealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum cipher and an encrypted cloud replica. Finally, we set out a plan for evaluating the physical and logical security of the proposed protocol.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·å¤‡éŸ³é¢‘åˆ†ç±»èƒ½åŠ›çš„ IoT èŠ‚ç‚¹åœ¨èµ„æºå—é™ä¸‹é¢ä¸´çš„æ•æ„Ÿæ•°æ®æ³„éœ²é£é™©ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå®‰å…¨åè®®æ¡†æ¶çš„æ·±åº¦é˜²å¾¡(defence-in-depth)æ¶æ„ã€‚è¯¥æ¶æ„é‡‡ç”¨ STRIDE é©±åŠ¨çš„å¨èƒæ¨¡å‹(Threat Model)å’Œæ”»å‡»æ ‘(Attack-tree)åˆ†æä½œä¸ºè®¾è®¡æŒ‡å¯¼ï¼Œå°†è¾¹ç¼˜è®¾å¤‡ã€èœ‚çªç½‘ç»œå’Œäº‘åç«¯è§†ä¸ºä¸‰ä¸ªç‹¬ç«‹çš„ä¿¡ä»»åŸŸã€‚ç³»ç»Ÿé€šè¿‡åŸºäº TPM çš„è¿œç¨‹è¯„ä¼°(Remote Attestation)å’Œç›¸äº’è®¤è¯çš„ TLS 1.3 å®ç°è¿æ¥ï¼Œåˆ©ç”¨ TPM PCR æµ‹é‡å¯åŠ¨é˜¶æ®µï¼Œå¹¶ä»…åœ¨äº‘ç«¯éªŒè¯é€šè¿‡åé‡Šæ”¾å¯†é’¥ä»¥è§£å¯†å— LUKS ä¿æŠ¤çš„åˆ†åŒºï¼Œä»è€Œç¡®ä¿å—æŸè®¾å¤‡å¤„äºæƒ°æ€§çŠ¶æ€ã€‚ä¼ è¾“ä¸­çš„æ•°æ®ç”± TLS 1.3 ä¿æŠ¤ï¼Œå¹¶ç»“åˆ Kyber å’Œ Dilithium ç®—æ³•ä»¥æä¾›åé‡å­æŠ—æ€§(Post-quantum Resilience)ï¼ŒåŒæ—¶å¯¹éŸ³é¢‘ç‰¹å¾è¿›è¡Œç«¯åˆ°ç«¯åŠ å¯†ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨å¸¦ç­¾åçš„é˜²å›æ»š AI æ¨¡å‹å’Œé˜²ç¯¡æ”¹ä¼ æ„Ÿå™¨åŠ å›ºç¡¬ä»¶ï¼Œå¹¶å¯¹é™æ€æ•°æ®å®æ–½åŒ…å« LUKS åŠ å¯†åŠåé‡å­å¯†ç å½’æ¡£çš„ 3-2-1 å¤‡ä»½ç­–ç•¥ã€‚æœ€åï¼Œè¯¥ç ”ç©¶ä¸ºè¯„ä¼°åè®®çš„ç‰©ç†ä¸é€»è¾‘å®‰å…¨æ€§æä¾›äº†ç³»ç»ŸåŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at Computing Conference 2026, London, UK",
      "pdf_url": "https://arxiv.org/pdf/2509.14657v3",
      "published_date": "2025-09-18 06:25:50 UTC",
      "updated_date": "2025-11-14 08:24:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:19.058283+00:00"
    },
    {
      "arxiv_id": "2509.14651v1",
      "title": "MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models",
      "title_zh": "MUSEï¼šæ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹å¤šè½®å¯¹è¯å®‰å…¨æ€§çš„ MCTS é©±åŠ¨çº¢é˜Ÿæµ‹è¯•æ¡†æ¶",
      "authors": [
        "Siyu Yan",
        "Long Zeng",
        "Xuecheng Wu",
        "Chengcheng Han",
        "Kongcheng Zhang",
        "Chong Peng",
        "Xuezhi Cao",
        "Xunliang Cai",
        "Chenjuan Guo"
      ],
      "abstract": "As large language models~(LLMs) become widely adopted, ensuring their alignment with human values is crucial to prevent jailbreaks where adversaries manipulate models to produce harmful content. While most defenses target single-turn attacks, real-world usage often involves multi-turn dialogues, exposing models to attacks that exploit conversational context to bypass safety measures. We introduce MUSE, a comprehensive framework tackling multi-turn jailbreaks from both attack and defense angles. For attacks, we propose MUSE-A, a method that uses frame semantics and heuristic tree search to explore diverse semantic trajectories. For defense, we present MUSE-D, a fine-grained safety alignment approach that intervenes early in dialogues to reduce vulnerabilities. Extensive experiments on various models show that MUSE effectively identifies and mitigates multi-turn vulnerabilities. Code is available at \\href{https://github.com/yansiyu02/MUSE}{https://github.com/yansiyu02/MUSE}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MUSEæ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šè½®å¯¹è¯(multi-turn dialogues)ä¸­é¢ä¸´çš„è¶Šç‹±(jailbreaks)æ”»å‡»æŒ‘æˆ˜ã€‚é’ˆå¯¹æ”»å‡»ä¾§ï¼ŒMUSE-Aç»„ä»¶ç»“åˆäº†æ¡†æ¶è¯­ä¹‰(frame semantics)å’Œå¯å‘å¼æ ‘æœç´¢(heuristic tree search)ï¼Œé€šè¿‡æ¢ç´¢å¤šæ ·åŒ–çš„è¯­ä¹‰è½¨è¿¹æ¥è¯†åˆ«æ¨¡å‹åœ¨å¤æ‚ä¸Šä¸‹æ–‡ä¸‹çš„å®‰å…¨æ¼æ´ã€‚é’ˆå¯¹é˜²å¾¡ä¾§ï¼ŒMUSE-Dé‡‡ç”¨ç»†ç²’åº¦çš„å®‰å…¨å¯¹é½æ–¹æ³•ï¼Œé€šè¿‡åœ¨å¯¹è¯æ—©æœŸè¿›è¡Œå¹²é¢„æ¥æ˜¾è‘—é™ä½æ¨¡å‹çš„è„†å¼±æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMUSEèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¹¶ç¼“è§£å„ç§ä¸»æµæ¨¡å‹ä¸­çš„å¤šè½®å®‰å…¨é£é™©ï¼Œä¸ºå¢å¼ºLLMsçš„äººç±»ä»·å€¼è§‚å¯¹é½æä¾›äº†å…¨é¢çš„æŠ€æœ¯æ”¯æ’‘ã€‚è¯¥æ¡†æ¶é€šè¿‡æ”»é˜²ä¸¤ç«¯çš„ååŒæå‡ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹è¯å¼äººå·¥æ™ºèƒ½åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 main conference",
      "pdf_url": "https://arxiv.org/pdf/2509.14651v1",
      "published_date": "2025-09-18 06:12:27 UTC",
      "updated_date": "2025-09-18 06:12:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:20.376870+00:00"
    },
    {
      "arxiv_id": "2509.14647v1",
      "title": "AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production",
      "title_zh": "AgentCompassï¼šé¢å‘ç”Ÿäº§ç¯å¢ƒæ™ºèƒ½ä½“å·¥ä½œæµçš„å¯é è¯„ä¼°",
      "authors": [
        "NVJK Kartik",
        "Garvit Sapra",
        "Rishav Hada",
        "Nikhil Pareek"
      ],
      "abstract": "With the growing adoption of Large Language Models (LLMs) in automating complex, multi-agent workflows, organizations face mounting risks from errors, emergent behaviors, and systemic failures that current evaluation methods fail to capture. We present AgentCompass, the first evaluation framework designed specifically for post-deployment monitoring and debugging of agentic workflows. AgentCompass models the reasoning process of expert debuggers through a structured, multi-stage analytical pipeline: error identification and categorization, thematic clustering, quantitative scoring, and strategic summarization. The framework is further enhanced with a dual memory system-episodic and semantic-that enables continual learning across executions. Through collaborations with design partners, we demonstrate the framework's practical utility on real-world deployments, before establishing its efficacy against the publicly available TRAIL benchmark. AgentCompass achieves state-of-the-art results on key metrics, while uncovering critical issues missed in human annotations, underscoring its role as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgentCompassï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹ç”Ÿäº§ç¯å¢ƒä¸­æ™ºèƒ½ä½“å·¥ä½œæµ (agentic workflows) è¿›è¡Œéƒ¨ç½²åç›‘æ§å’Œè°ƒè¯•çš„è¯„ä¼°æ¡†æ¶ã€‚AgentCompass é€šè¿‡ä¸€ä¸ªç»“æ„åŒ–çš„å¤šé˜¶æ®µåˆ†ææµæ°´çº¿æ¨¡æ‹Ÿä¸“å®¶è°ƒè¯•è€…çš„æ¨ç†è¿‡ç¨‹ï¼Œæ¶µç›–äº†é”™è¯¯è¯†åˆ«ä¸åˆ†ç±»ã€ä¸»é¢˜èšç±»ã€å®šé‡è¯„åˆ†ä»¥åŠç­–ç•¥æ€§æ€»ç»“ç­‰å…³é”®ç¯èŠ‚ã€‚ä¸ºäº†å®ç°è·¨æ‰§è¡Œè¿‡ç¨‹çš„æŒç»­å­¦ä¹ ï¼Œè¯¥æ¡†æ¶è¿›ä¸€æ­¥å¼•å…¥äº†ç”±æƒ…æ™¯è®°å¿† (episodic memory) å’Œè¯­ä¹‰è®°å¿† (semantic memory) ç»„æˆçš„åŒé‡è®°å¿†ç³»ç»Ÿã€‚é€šè¿‡åœ¨å®é™…éƒ¨ç½²ç¯å¢ƒä»¥åŠå…¬å¼€çš„ TRAIL åŸºå‡†æµ‹è¯•ä¸­çš„éªŒè¯ï¼ŒAgentCompass åœ¨å¤šé¡¹å…³é”®æŒ‡æ ‡ä¸Šè¾¾åˆ°äº† SOTA æ°´å¹³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºäººç±»æ ‡æ³¨ä¸­é—æ¼çš„å…³é”®é—®é¢˜ï¼Œä¸ºå¼€å‘è€…åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å®ç°å¯é çš„æ™ºèƒ½ä½“ç³»ç»Ÿç›‘æ§ä¸æ”¹è¿›æä¾›äº†ç¨³å¥çš„å·¥å…·æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14647v1",
      "published_date": "2025-09-18 05:59:04 UTC",
      "updated_date": "2025-09-18 05:59:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:28.160192+00:00"
    },
    {
      "arxiv_id": "2509.15253v1",
      "title": "Emotion-Aware Speech Generation with Character-Specific Voices for Comics",
      "title_zh": "é¢å‘æ¼«ç”»çš„è§’è‰²ç‰¹å®šæƒ…æ„Ÿæ„ŸçŸ¥è¯­éŸ³ç”Ÿæˆ",
      "authors": [
        "Zhiwen Qian",
        "Jinhua Liang",
        "Huan Zhang"
      ],
      "abstract": "This paper presents an end-to-end pipeline for generating character-specific, emotion-aware speech from comics. The proposed system takes full comic volumes as input and produces speech aligned with each character's dialogue and emotional state. An image processing module performs character detection, text recognition, and emotion intensity recognition. A large language model performs dialogue attribution and emotion analysis by integrating visual information with the evolving plot context. Speech is synthesized through a text-to-speech model with distinct voice profiles tailored to each character and emotion. This work enables automated voiceover generation for comics, offering a step toward interactive and immersive comic reading experience.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯æµæ°´çº¿(pipeline)ï¼Œæ—¨åœ¨ä¸ºæ¼«ç”»ç”Ÿæˆå…·æœ‰ç‰¹å®šè§’è‰²å£°éŸ³ä¸”æ„ŸçŸ¥æƒ…æ„Ÿçš„è¯­éŸ³ã€‚ç³»ç»Ÿä»¥å®Œæ•´æ¼«ç”»å·ä½œä¸ºè¾“å…¥ï¼Œäº§å‡ºä¸è§’è‰²å¯¹è¯åŠæƒ…æ„ŸçŠ¶æ€é«˜åº¦å¯¹é½çš„è¯­éŸ³ã€‚å…¶å›¾åƒå¤„ç†æ¨¡å—è´Ÿè´£æ‰§è¡Œè§’è‰²æ£€æµ‹(character detection)ã€æ–‡æœ¬è¯†åˆ«(text recognition)å’Œæƒ…æ„Ÿå¼ºåº¦è¯†åˆ«(emotion intensity recognition)ã€‚å¤§è¯­è¨€æ¨¡å‹(LLM)é€šè¿‡æ•´åˆè§†è§‰ä¿¡æ¯ä¸å‰§æƒ…ä¸Šä¸‹æ–‡ï¼Œè¿›è¡Œç²¾ç¡®çš„å¯¹è¯å½’å±(dialogue attribution)å’Œæƒ…æ„Ÿåˆ†æ(emotion analysis)ã€‚æœ€ç»ˆï¼Œè¯­éŸ³é€šè¿‡æ–‡æœ¬è½¬è¯­éŸ³(TTS)æ¨¡å‹åˆæˆï¼Œå¹¶åº”ç”¨äº†ä¸ºæ¯ä¸ªè§’è‰²å’Œæƒ…æ„Ÿé‡èº«å®šåˆ¶çš„ç‹¬ç‰¹å£°éŸ³é…ç½®æ–‡ä»¶ã€‚è¯¥å·¥ä½œå®ç°äº†æ¼«ç”»çš„è‡ªåŠ¨é…éŸ³ç”Ÿæˆï¼Œä¸ºåˆ›é€ äº¤äº’å¼å’Œæ²‰æµ¸å¼çš„æ¼«ç”»é˜…è¯»ä½“éªŒè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15253v1",
      "published_date": "2025-09-18 05:49:57 UTC",
      "updated_date": "2025-09-18 05:49:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:25.687174+00:00"
    },
    {
      "arxiv_id": "2509.14642v1",
      "title": "DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training",
      "title_zh": "DeCoPï¼šé€šè¿‡ä¾èµ–å—æ§é¢„è®­ç»ƒå¢å¼ºè‡ªç›‘ç£æ—¶é—´åºåˆ—è¡¨ç¤º",
      "authors": [
        "Yuemin Wu",
        "Zhongze Wu",
        "Xiu Su",
        "Feng Yang",
        "Hongyan Xu",
        "Xi Lin",
        "Wenti Huang",
        "Shan You",
        "Chang Xu"
      ],
      "abstract": "Modeling dynamic temporal dependencies is a critical challenge in time series pre-training, which evolve due to distribution shifts and multi-scale patterns. This temporal variability severely impairs the generalization of pre-trained models to downstream tasks. Existing frameworks fail to capture the complex interactions of short- and long-term dependencies, making them susceptible to spurious correlations that degrade generalization. To address these limitations, we propose DeCoP, a Dependency Controlled Pre-training framework that explicitly models dynamic, multi-scale dependencies by simulating evolving inter-patch dependencies. At the input level, DeCoP introduces Instance-wise Patch Normalization (IPN) to mitigate distributional shifts while preserving the unique characteristics of each patch, creating a robust foundation for representation learning. At the latent level, a hierarchical Dependency Controlled Learning (DCL) strategy explicitly models inter-patch dependencies across multiple temporal scales, with an Instance-level Contrastive Module (ICM) enhances global generalization by learning instance-discriminative representations from time-invariant positive pairs. DeCoP achieves state-of-the-art results on ten datasets with lower computing resources, improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.",
      "tldr_zh": "å»ºæ¨¡åŠ¨æ€æ—¶é—´ä¾èµ–æ˜¯æ—¶é—´åºåˆ—é¢„è®­ç»ƒä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œåˆ†å¸ƒåç§»å’Œå¤šå°ºåº¦æ¨¡å¼å¸¦æ¥çš„æ¼”å˜ä¼šæ˜¾è‘—å‰Šå¼±é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æå‡ºäº† DeCoP æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿæ¼”å˜çš„è¡¥ä¸é—´å…³ç³»æ¥å®ç°ä¾èµ–æ§åˆ¶é¢„è®­ç»ƒ (Dependency Controlled Pre-training)ã€‚è¯¥æ¡†æ¶åœ¨è¾“å…¥å±‚é‡‡ç”¨å®ä¾‹çº§è¡¥ä¸å½’ä¸€åŒ– (Instance-wise Patch Normalization, IPN) æ¥å‡è½»åˆ†å¸ƒåç§»ï¼Œå¹¶åœ¨æ½œå±‚åˆ©ç”¨åˆ†å±‚ä¾èµ–æ§åˆ¶å­¦ä¹  (Dependency Controlled Learning, DCL) ç­–ç•¥æ˜¾å¼å»ºæ¨¡å¤šå°ºåº¦ä¸‹çš„è¡¥ä¸é—´ä¾èµ–ã€‚æ­¤å¤–ï¼Œæ¡†æ¶é€šè¿‡å®ä¾‹çº§å¯¹æ¯”æ¨¡å— (Instance-level Contrastive Module, ICM) å­¦ä¹ å®ä¾‹åˆ¤åˆ«è¡¨å¾ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å…¨å±€æ³›åŒ–æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeCoP åœ¨åä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº† state-of-the-art æ€§èƒ½ã€‚åœ¨ ETTh1 æ•°æ®é›†ä¸Šï¼ŒDeCoP ä»…éœ€ PatchTST 37% çš„è®¡ç®—èµ„æº (FLOPs) å³å¯å®ç°å‡æ–¹è¯¯å·® (MSE) 3% çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶é«˜æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14642v1",
      "published_date": "2025-09-18 05:44:06 UTC",
      "updated_date": "2025-09-18 05:44:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:47.254905+00:00"
    },
    {
      "arxiv_id": "2509.18183v1",
      "title": "VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation",
      "title_zh": "VLA-LPAFï¼šé¢å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„è½»é‡çº§è§†è§’è‡ªé€‚åº”èåˆï¼ŒåŠ©åŠ›å®ç°æ›´æ— çº¦æŸçš„æœºå™¨äººæ“ä½œ",
      "authors": [
        "Jinyue Bian",
        "Zhaoxing Zhang",
        "Zhengyu Liang",
        "Shiwei Zheng",
        "Shengtao Zhang",
        "Rong Shen",
        "Chen Yang",
        "Anzhou Hou"
      ],
      "abstract": "The Visual-Language-Action (VLA) models can follow text instructions according to visual observations of the surrounding environment. This ability to map multimodal inputs to actions is derived from the training of the VLA model on extensive standard demonstrations. These visual observations captured by third-personal global and in-wrist local cameras are inevitably varied in number and perspective across different environments, resulting in significant differences in the visual features. This perspective heterogeneity constrains the generality of VLA models. In light of this, we first propose the lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models using only 2D data. VLA-LPAF is finetuned using images from a single view and fuses other multiview observations in the latent space, which effectively and efficiently bridge the gap caused by perspective inconsistency. We instantiate our VLA-LPAF framework with the VLA model RoboFlamingo to construct RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a customized simulation benchmark. We also demonstrate the developed viewadaptive characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision-Language-Action (VLA) æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒä¸‹å› ç›¸æœºè§†è§’å¼‚è´¨æ€§å¯¼è‡´é€šç”¨æ€§å—é™çš„é—®é¢˜ï¼Œæå‡ºäº† VLA-LPAFï¼ˆLightweight Perspective-Adaptive Fusionï¼‰è½»é‡åŒ–è§†è§’è‡ªé€‚åº”èåˆæ¨¡å—ã€‚è¯¥æ¨¡å—ä»…åˆ©ç”¨ 2D æ•°æ®ï¼Œé€šè¿‡åœ¨æ½œç©ºé—´ï¼ˆlatent spaceï¼‰èåˆå¤šè§†å›¾è§‚æµ‹å¹¶åˆ©ç”¨å•è§†è§’å›¾åƒè¿›è¡Œå¾®è°ƒï¼Œæœ‰æ•ˆå¼¥åˆäº†è§†è§’ä¸ä¸€è‡´å¸¦æ¥çš„è§†è§‰ç‰¹å¾å·®å¼‚ã€‚ç ”ç©¶äººå‘˜å°†è¯¥æ¡†æ¶åº”ç”¨äº RoboFlamingo æ¨¡å‹å¹¶æ„å»ºäº† RoboFlamingo-LPAF ç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ CALVINã€LIBERO åŠè‡ªå®šä¹‰ä»¿çœŸåŸºå‡†æµ‹è¯•ä¸­çš„ä»»åŠ¡æˆåŠŸç‡åˆ†åˆ«æå‡äº†çº¦ 8%ã€15% å’Œ 30%ã€‚æ­¤å¤–ï¼ŒçœŸå®ä¸–ç•Œä»»åŠ¡çš„æ¼”ç¤ºè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ¨¡å‹åœ¨å¤„ç†ä¸å—çº¦æŸçš„æœºå™¨äººæ“çºµä»»åŠ¡æ—¶å…·å¤‡å“è¶Šçš„è§†è§’è‡ªé€‚åº”ç‰¹æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18183v1",
      "published_date": "2025-09-18 05:24:39 UTC",
      "updated_date": "2025-09-18 05:24:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:46.190278+00:00"
    },
    {
      "arxiv_id": "2509.14632v1",
      "title": "Mitigating Intra-Speaker Variability in Diarization with Style-Controllable Speech Augmentation",
      "title_zh": "åˆ©ç”¨é£æ ¼å¯æ§çš„è¯­éŸ³å¢å¼ºç¼“è§£è¯´è¯äººæ—¥å¿—ä¸­çš„è¯´è¯äººå†…å˜å¼‚æ€§",
      "authors": [
        "Miseul Kim",
        "Soo Jin Park",
        "Kyungguen Byun",
        "Hyeon-Kyeong Shin",
        "Sunkuk Moon",
        "Shuhua Zhang",
        "Erik Visser"
      ],
      "abstract": "Speaker diarization systems often struggle with high intrinsic intra-speaker variability, such as shifts in emotion, health, or content. This can cause segments from the same speaker to be misclassified as different individuals, for example, when one raises their voice or speaks faster during conversation. To address this, we propose a style-controllable speech generation model that augments speech across diverse styles while preserving the target speaker's identity. The proposed system starts with diarized segments from a conventional diarizer. For each diarized segment, it generates augmented speech samples enriched with phonetic and stylistic diversity. And then, speaker embeddings from both the original and generated audio are blended to enhance the system's robustness in grouping segments with high intrinsic intra-speaker variability. We validate our approach on a simulated emotional speech dataset and the truncated AMI dataset, demonstrating significant improvements, with error rate reductions of 49% and 35% on each dataset, respectively.",
      "tldr_zh": "é’ˆå¯¹è¯´è¯äººæ—¥å¿—(Speaker diarization)ç³»ç»Ÿåœ¨é¢å¯¹æƒ…æ„Ÿã€å¥åº·æˆ–è¯­é€Ÿå˜åŒ–å¯¼è‡´çš„é«˜è¯´è¯äººå†…éƒ¨å·®å¼‚(Intra-speaker variability)æ—¶å®¹æ˜“å°†åŒä¸€è¯´è¯äººè¯¯åˆ¤ä¸ºä¸åŒä¸ªä½“çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é£æ ¼å¯æ§çš„è¯­éŸ³ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹å¸¸è§„æ—¥å¿—ç³»ç»Ÿç”Ÿæˆçš„ç‰‡æ®µè¿›è¡Œå¢å¼ºï¼Œåœ¨ä¿æŒè¯´è¯äººèº«ä»½çš„åŒæ—¶äº§ç”Ÿå…·å¤‡éŸ³æ ‡å’Œé£æ ¼å¤šæ ·æ€§çš„è¯­éŸ³æ ·æœ¬ã€‚ç³»ç»Ÿè¿›ä¸€æ­¥å°†åŸå§‹éŸ³é¢‘ä¸ç”Ÿæˆçš„å¢å¼ºéŸ³é¢‘çš„è¯´è¯äººåµŒå…¥(Speaker embeddings)è¿›è¡Œèåˆï¼Œä»è€Œæå‡äº†ç³»ç»Ÿå¯¹å…·æœ‰é«˜å†…åœ¨å˜å¼‚ç‰‡æ®µçš„åˆ†ç»„ç¨³å¥æ€§ã€‚åœ¨æ¨¡æ‹Ÿæƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†å’Œæˆªæ–­çš„AMIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—é™ä½é”™è¯¯ç‡ï¼Œå‡å¹…åˆ†åˆ«è¾¾åˆ°49%å’Œ35%ï¼Œæœ‰æ•ˆç¼“è§£äº†è¯´è¯äººå†…éƒ¨å·®å¼‚å¯¹æ—¥å¿—ç³»ç»Ÿçš„å½±å“ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "Submitted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.14632v1",
      "published_date": "2025-09-18 05:21:20 UTC",
      "updated_date": "2025-09-18 05:21:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:28:50.887579+00:00"
    },
    {
      "arxiv_id": "2509.14627v1",
      "title": "Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech",
      "title_zh": "è¿ˆå‘ç±»äººå¤šæ¨¡æ€å¯¹è¯æ™ºèƒ½ä½“ï¼šé€šè¿‡ç”Ÿæˆå¼•äººå…¥èƒœçš„è¯­éŸ³",
      "authors": [
        "Taesoo Kim",
        "Yongsik Jo",
        "Hyunmin Song",
        "Taehwan Kim"
      ],
      "abstract": "Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel MultiSensory Conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering paralinguistic information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech. The source code is available in https://github.com/kimtaesu24/MSenC",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æå‡å¤šæ¨¡æ€å¯¹è¯æ™ºèƒ½ä½“çš„äººæœºäº¤äº’ä½“éªŒï¼Œé‡ç‚¹è§£å†³äº†å½“å‰ Multimodal LLMs åœ¨ç”Ÿæˆè‡ªç„¶ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³(Engaging Speech)æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªå…¨æ–°çš„ MultiSensory Conversation (MSenC) æ•°æ®é›†ï¼Œç”¨äºæ”¯æŒæ™ºèƒ½ä½“ç†è§£å¹¶ç”Ÿæˆæ›´å…·æ„ŸæŸ“åŠ›çš„è¯­éŸ³ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäº Multimodal LLM çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®å¯¹è¯æ°›å›´(mood)å’Œå“åº”é£æ ¼åŒæ—¶ç”Ÿæˆæ–‡æœ¬å›å¤åŠè¯¦ç»†çš„è¯­éŸ³æè¿°(voice descriptions)ã€‚é€šè¿‡æ•´åˆå‰¯è¯­è¨€ä¿¡æ¯(paralinguistic information)ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç»“åˆè§†è§‰å’ŒéŸ³é¢‘æ¨¡æ€æä¾›çš„å¤šç»´çº¿ç´¢ï¼Œè¾“å‡ºæ›´è´´åˆè¯­å¢ƒä¸”è‡ªç„¶çš„è¯­éŸ³å›å¤ã€‚å®éªŒç»“æœè¯æ˜ï¼Œç»“åˆå¤šæ¨¡æ€ä¿¡æ¯èƒ½æœ‰æ•ˆæå‡è¯­éŸ³å¯¹è¯çš„å¸å¼•åŠ›ä¸ç±»äººæ€§ã€‚è¯¥é¡¹ç›®çš„æºä»£ç å·²åœ¨ GitHub å¼€æºï¼Œä¸ºæ„å»ºæ‹ŸäººåŒ–å¤šæ¨¡æ€äº¤äº’æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "Published in Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.14627v1",
      "published_date": "2025-09-18 05:14:10 UTC",
      "updated_date": "2025-09-18 05:14:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:29:00.992868+00:00"
    },
    {
      "arxiv_id": "2509.14624v1",
      "title": "Reveal and Release: Iterative LLM Unlearning with Self-generated Data",
      "title_zh": "æ­ç¤ºä¸é‡Šæ”¾ï¼šåŸºäºè‡ªç”Ÿæˆæ•°æ®çš„è¿­ä»£å¼å¤§è¯­è¨€æ¨¡å‹é—å¿˜",
      "authors": [
        "Linxi Xie",
        "Xin Teng",
        "Shichang Ke",
        "Hongyi Wen",
        "Shengjie Wang"
      ],
      "abstract": "Large language model (LLM) unlearning has demonstrated effectiveness in removing the influence of undesirable data (also known as forget data). Existing approaches typically assume full access to the forget dataset, overlooking two key challenges: (1) Forget data is often privacy-sensitive, rare, or legally regulated, making it expensive or impractical to obtain (2) The distribution of available forget data may not align with how that information is represented within the model. To address these limitations, we propose a ``Reveal-and-Release'' method to unlearn with self-generated data, where we prompt the model to reveal what it knows using optimized instructions. To fully utilize the self-generated forget data, we propose an iterative unlearning framework, where we make incremental adjustments to the model's weight space with parameter-efficient modules trained on the forget data. Experimental results demonstrate that our method balances the tradeoff between forget quality and utility preservation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLM)é—å¿˜(unlearning)æ–¹æ³•é€šå¸¸å‡è®¾æ‹¥æœ‰å®Œæ•´é—å¿˜æ•°æ®(forget data)çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸º\"Reveal-and-Release\"çš„è¿­ä»£å¼é—å¿˜æ¡†æ¶ã€‚é’ˆå¯¹é—å¿˜æ•°æ®å¾€å¾€æ¶‰åŠéšç§ã€ç¨€ç¼ºæˆ–å—ç›‘ç®¡è€Œéš¾ä»¥è·å–çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡ä¼˜åŒ–æŒ‡ä»¤ä¿ƒä½¿æ¨¡å‹\"æ­ç¤º\"(Reveal)å…¶å†…éƒ¨æŒæ¡çš„ç›¸å…³çŸ¥è¯†ï¼Œä»è€Œåˆ©ç”¨æ¨¡å‹è‡ªç”Ÿæˆçš„æ•°æ®æ›¿ä»£åŸå§‹æ•°æ®ã€‚éšåï¼Œç ”ç©¶é‡‡ç”¨è¿­ä»£é—å¿˜æœºåˆ¶ï¼Œåˆ©ç”¨åœ¨è‡ªç”Ÿæˆæ•°æ®ä¸Šè®­ç»ƒçš„å‚æ•°é«˜æ•ˆæ¨¡å—(parameter-efficient modules)ï¼Œå¯¹æ¨¡å‹æƒé‡è¿›è¡Œå¢é‡è°ƒæ•´ä»¥\"é‡Šæ”¾\"(Release)ç‰¹å®šä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¼ºä¹åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆå¹³è¡¡äº†é—å¿˜è´¨é‡ä¸æ¨¡å‹é€šç”¨èƒ½åŠ›çš„ä¿ç•™(utility preservation)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2509.14624v1",
      "published_date": "2025-09-18 05:07:27 UTC",
      "updated_date": "2025-09-18 05:07:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-24T19:30:46.627286+00:00"
    },
    {
      "arxiv_id": "2509.14623v1",
      "title": "Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ Modelica æ¨¡å—è‡ªåŠ¨ç”Ÿæˆï¼šä»¥å»ºç­‘æ§åˆ¶æè¿°è¯­è¨€ä¸ºä¾‹",
      "authors": [
        "Hanlong Wan",
        "Xing Lu",
        "Yan Chen",
        "Karthik Devaprasad",
        "Laura Hinkle"
      ],
      "abstract": "Dynamic energy systems and controls require advanced modeling frameworks to design and test supervisory and fault tolerant strategies. Modelica is a widely used equation based language, but developing control modules is labor intensive and requires specialized expertise. This paper examines the use of large language models (LLMs) to automate the generation of Control Description Language modules in the Building Modelica Library as a case study. We developed a structured workflow that combines standardized prompt scaffolds, library aware grounding, automated compilation with OpenModelica, and human in the loop evaluation. Experiments were carried out on four basic logic tasks (And, Or, Not, and Switch) and five control modules (chiller enable/disable, bypass valve control, cooling tower fan speed, plant requests, and relief damper control). The results showed that GPT 4o failed to produce executable Modelica code in zero shot mode, while Claude Sonnet 4 achieved up to full success for basic logic blocks with carefully engineered prompts. For control modules, success rates reached 83 percent, and failed outputs required medium level human repair (estimated one to eight hours). Retrieval augmented generation often produced mismatches in module selection (for example, And retrieved as Or), while a deterministic hard rule search strategy avoided these errors. Human evaluation also outperformed AI evaluation, since current LLMs cannot assess simulation results or validate behavioral correctness. Despite these limitations, the LLM assisted workflow reduced the average development time from 10 to 20 hours down to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings. These results highlight both the potential and current limitations of LLM assisted Modelica generation, and point to future research in pre simulation validation, stronger grounding, and closed loop evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models)è‡ªåŠ¨ç”ŸæˆModelicaåº“ä¸­æ§åˆ¶æè¿°è¯­è¨€(Control Description Language)æ¨¡å—çš„æ–¹æ³•ï¼Œä»¥è§£å†³ä¼ ç»Ÿå»ºæ¨¡è¿‡ç¨‹è€—æ—¶ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€å¥—é›†æˆæç¤ºæ”¯æ¶ã€åº“æ„ŸçŸ¥æ¥åœ°ã€OpenModelicaè‡ªåŠ¨ç¼–è¯‘å’Œäººå·¥åé¦ˆ(Human-in-the-loop)çš„ç»“æ„åŒ–å·¥ä½œæµã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒClaude 3.5 Sonnetåœ¨ç»è¿‡æç¤ºå·¥ç¨‹åè¡¨ç°ä¼˜äºGPT-4oï¼Œåœ¨åŸºç¡€é€»è¾‘ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå¹¶åœ¨å¤æ‚æ§åˆ¶æ¨¡å—ç”Ÿæˆä¸­è¾¾åˆ°äº†83%çš„æˆåŠŸç‡ã€‚è™½ç„¶ç›®å‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)åœ¨æ¨¡å—åŒ¹é…ä¸Šå­˜åœ¨å±€é™ï¼Œä¸”LLMså°šæ— æ³•ç‹¬ç«‹å®Œæˆä»¿çœŸç»“æœéªŒè¯ï¼Œä½†è¯¥æ–¹æ³•é€šè¿‡ç¡®å®šæ€§çš„ç¡¬è§„åˆ™æœç´¢ç­–ç•¥æœ‰æ•ˆæå‡äº†å¯é æ€§ã€‚ç»Ÿè®¡è¡¨æ˜ï¼Œè¯¥å·¥ä½œæµå°†å•ä¸ªæ¨¡å—çš„å¹³å‡å¼€å‘æ—¶é—´ä»10-20å°æ—¶ç¼©çŸ­è‡³4-6å°æ—¶ï¼Œå®ç°äº†40%è‡³60%çš„æ—¶é—´èŠ‚çœã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†LLMè¾…åŠ©Modelicaç”Ÿæˆçš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥é¢„ä»¿çœŸéªŒè¯å’Œå¢å¼ºå‹é—­ç¯è¯„ä¼°çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL",
        "eess.SY"
      ],
      "primary_category": "cs.SE",
      "comment": "This is the pre-peer-review version of a journal paper; the repo is available at: https://github.com/pnnl/prompt2control",
      "pdf_url": "https://arxiv.org/pdf/2509.14623v1",
      "published_date": "2025-09-18 05:07:17 UTC",
      "updated_date": "2025-09-18 05:07:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:29:19.398349+00:00"
    },
    {
      "arxiv_id": "2509.14622v3",
      "title": "Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection",
      "title_zh": "é¢å‘åœ¨çº¿æ¶æ„æ„å›¾æ£€æµ‹çš„å¯¹æŠ—è’¸é¦æ£€ç´¢å¢å¼ºé˜²æŠ¤æ¨¡å‹",
      "authors": [
        "Yihao Guo",
        "Haocheng Bian",
        "Liutong Zhou",
        "Ze Wang",
        "Zhaoyi Zhang",
        "Francois Kawala",
        "Milan Dean",
        "Ian Fischer",
        "Yuantao Peng",
        "Noyan Tokgozoglu",
        "Ivan Barrientos",
        "Riyaaz Shaik",
        "Rachel Li",
        "Chandru Venkataraman",
        "Reza Shifteh Far",
        "Moses Pawar",
        "Venkat Sundaranatha",
        "Michael Xu",
        "Frank Chu"
      ],
      "abstract": "With the deployment of Large Language Models (LLMs) in interactive applications, online malicious intent detection has become increasingly critical. However, existing approaches fall short of handling diverse and complex user queries in real time. To address these challenges, we introduce ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework for robust and efficient online malicious intent detection. In the training stage, a high-capacity teacher model is trained on adversarially perturbed, retrieval-augmented inputs to learn robust decision boundaries over diverse and complex user queries. In the inference stage, a distillation scheduler transfers the teacher's knowledge into a compact student model, with a continually updated knowledge base collected online. At deployment, the compact student model leverages top-K similar safety exemplars retrieved from the online-updated knowledge base to enable both online and real-time malicious query detection. Evaluations across ten safety benchmarks demonstrate that ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on out-of-distribution detection, while simultaneously delivering up to 5.6x lower latency at 300 queries per second (QPS) in real-time applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ADRAGï¼ˆAdversarial Distilled Retrieval-Augmented Guardï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çº¿æ¶æ„æ„å›¾æ£€æµ‹ä¸­å®æ—¶æ€§å’Œå¤æ‚æ€§æŒ‘æˆ˜çš„åŒé˜¶æ®µæ¡†æ¶ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œè¯¥æ¡†æ¶é€šè¿‡åœ¨å—å¯¹æŠ—æ‰°åŠ¨ï¼ˆadversarially perturbedï¼‰å’Œæ£€ç´¢å¢å¼ºï¼ˆretrieval-augmentedï¼‰çš„è¾“å…¥ä¸Šè®­ç»ƒé«˜å®¹é‡æ•™å¸ˆæ¨¡å‹ï¼Œå­¦ä¹ é’ˆå¯¹å¤šæ ·åŒ–ç”¨æˆ·æŸ¥è¯¢çš„é²æ£’å†³ç­–è¾¹ç•Œã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡è’¸é¦è°ƒåº¦å™¨ï¼ˆdistillation schedulerï¼‰å°†æ•™å¸ˆçŸ¥è¯†è½¬ç§»è‡³è½»é‡çº§å­¦ç”Ÿæ¨¡å‹ï¼Œå¹¶ç»“åˆåœ¨çº¿æ›´æ–°çš„çŸ¥è¯†åº“æ£€ç´¢ç›¸ä¼¼çš„å®‰å…¨èŒƒä¾‹ï¼Œä»¥å®ç°å®æ—¶ä¸”å‡†ç¡®çš„æ£€æµ‹ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œå‚æ•°é‡ä»…ä¸º149Mçš„ADRAGæ¨¡å‹è¾¾åˆ°äº†WildGuard-7Bæ€§èƒ½çš„98.5%ï¼Œåœ¨åˆ†å¸ƒå¤–æ£€æµ‹ï¼ˆout-of-distribution detectionï¼‰æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†GPT-4å’ŒLlama-Guard-3-8Bã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æ¯ç§’300æ¬¡æŸ¥è¯¢ï¼ˆQPSï¼‰çš„é«˜å¹¶å‘åœºæ™¯ä¸‹å®ç°äº†5.6å€çš„æ›´ä½å»¶è¿Ÿï¼Œä¸ºå¤§è§„æ¨¡äº¤äº’å¼åº”ç”¨ä¸­çš„å®æ—¶æ¶æ„æ„å›¾è¯†åˆ«æä¾›äº†é«˜æ•ˆã€é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14622v3",
      "published_date": "2025-09-18 05:04:48 UTC",
      "updated_date": "2025-11-01 17:50:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:29:08.292557+00:00"
    },
    {
      "arxiv_id": "2509.14619v1",
      "title": "LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition",
      "title_zh": "LSTC-MDAï¼šåŸºäºéª¨éª¼åŠ¨ä½œè¯†åˆ«çš„é•¿çŸ­æœŸæ—¶åºå·ç§¯ä¸æ··åˆæ•°æ®å¢å¼ºç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Feng Ding",
        "Haisheng Fu",
        "Soroush Oraki",
        "Jie Liang"
      ],
      "abstract": "Skeleton-based action recognition faces two longstanding challenges: the scarcity of labeled training samples and difficulty modeling short- and long-range temporal dependencies. To address these issues, we propose a unified framework, LSTC-MDA, which simultaneously improves temporal modeling and data diversity. We introduce a novel Long-Short Term Temporal Convolution (LSTC) module with parallel short- and long-term branches, these two feature branches are then aligned and fused adaptively using learned similarity weights to preserve critical long-range cues lost by conventional stride-2 temporal convolutions. We also extend Joint Mixing Data Augmentation (JMDA) with an Additive Mixup at the input level, diversifying training samples and restricting mixup operations to the same camera view to avoid distribution shifts. Ablation studies confirm each component contributes. LSTC-MDA achieves state-of-the-art results: 94.1% and 97.5% on NTU 60 (X-Sub and X-View), 90.4% and 92.0% on NTU 120 (X-Sub and X-Set),97.2% on NW-UCLA. Code: https://github.com/xiaobaoxia/LSTC-MDA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LSTC-MDAï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³åŸºäºéª¨éª¼çš„åŠ¨ä½œè¯†åˆ«(Skeleton-based action recognition)ä¸­è®­ç»ƒæ ·æœ¬ç¨€ç¼ºä»¥åŠé•¿çŸ­ç¨‹æ—¶åºä¾èµ–å»ºæ¨¡å›°éš¾é—®é¢˜çš„ç»Ÿä¸€æ¡†æ¶ã€‚æ¡†æ¶å¼•å…¥äº†æ–°å‹çš„é•¿çŸ­ç¨‹æ—¶åºå·ç§¯(Long-Short Term Temporal Convolution, LSTC)æ¨¡å—ï¼Œé€šè¿‡å¹¶è¡Œçš„é•¿çŸ­æœŸåˆ†æ”¯å¤„ç†ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å­¦ä¹ åˆ°çš„ç›¸ä¼¼æ€§æƒé‡è¿›è¡Œè‡ªé€‚åº”èåˆï¼Œä»è€Œæœ‰æ•ˆä¿ç•™äº†ä¼ ç»Ÿæ—¶åºå·ç§¯å®¹æ˜“ä¸¢å¤±çš„å…³é”®é•¿ç¨‹ç‰¹å¾ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶é€šè¿‡åœ¨è¾“å…¥å±‚å¼•å…¥åŠ æ€§æ··åˆ(Additive Mixup)æ‰©å±•äº†æ··åˆæ•°æ®å¢å¼º(Mixed Data Augmentation)æŠ€æœ¯ï¼Œå¹¶å°†å…¶æ“ä½œé™åˆ¶åœ¨ç›¸åŒç›¸æœºè§†è§’å†…ä»¥é˜²æ­¢åˆ†å¸ƒåç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLSTC-MDAåœ¨NTU 60ã€NTU 120å’ŒNW-UCLAç­‰å¤šä¸ªä¸»æµæ•°æ®é›†ä¸Šå‡å–å¾—äº†å½“å‰æœ€å…ˆè¿›(State-of-the-art)çš„æ€§èƒ½è¡¨ç°ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†LSTCæ¨¡å—ä¸å¢å¼ºç­–ç•¥åœ¨æå‡æ¨¡å‹æ—¶åºå»ºæ¨¡èƒ½åŠ›å’Œæ•°æ®å¤šæ ·æ€§æ–¹é¢çš„æ˜¾è‘—è´¡çŒ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to ICASSP",
      "pdf_url": "https://arxiv.org/pdf/2509.14619v1",
      "published_date": "2025-09-18 04:48:32 UTC",
      "updated_date": "2025-09-18 04:48:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:29:16.888684+00:00"
    },
    {
      "arxiv_id": "2509.16264v3",
      "title": "ParlAI Vote: A Web Platform for Analyzing Gender and Political Bias in Large Language Models",
      "title_zh": "ParlAI Voteï¼šåˆ†æå¤§è¯­è¨€æ¨¡å‹ä¸­æ€§åˆ«ä¸æ”¿æ²»åè§çš„ Web å¹³å°",
      "authors": [
        "Wenjie Lin",
        "Hange Liu",
        "Yingying Zhuang",
        "Xutao Mao",
        "Jingwei Shi",
        "Xudong Han",
        "Tianyu Shi",
        "Jinrui Yang"
      ],
      "abstract": "We present ParlAI Vote, an interactive web platform for exploring European Parliament debates and votes, and for testing LLMs on vote prediction and bias analysis. This web system connects debate topics, speeches, and roll-call outcomes, and includes rich demographic data such as gender, age, country, and political group. Users can browse debates, inspect linked speeches, compare real voting outcomes with predictions from frontier LLMs, and view error breakdowns by demographic group. Visualizing the EuroParlVote benchmark and its core tasks of gender classification and vote prediction, ParlAI Vote highlights systematic performance bias in state-of-the-art LLMs. It unifies data, models, and visual analytics in a single interface, lowering the barrier for reproducing findings, auditing behavior, and running counterfactual scenarios. This web platform also shows model reasoning, helping users see why errors occur and what cues the models rely on. It supports research, education, and public engagement with legislative decision-making, while making clear both the strengths and the limitations of current LLMs in political analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº† ParlAI Voteï¼Œä¸€ä¸ªç”¨äºæ¢ç´¢æ¬§æ´²è®®ä¼šè¾©è®ºä¸æŠ•ç¥¨ã€å¹¶æµ‹è¯• Large Language Models (LLMs) åœ¨æŠ•ç¥¨é¢„æµ‹å’Œåè§åˆ†æä¸­è¡¨ç°çš„äº¤äº’å¼ Web å¹³å°ã€‚è¯¥ç³»ç»Ÿæ•´åˆäº†è¾©è®ºä¸»é¢˜ã€æ¼”è®²å†…å®¹åŠè®°åæŠ•ç¥¨ (roll-call) ç»“æœï¼Œå¹¶åŒ…å«æ€§åˆ«ã€å¹´é¾„ã€å›½å®¶å’Œæ”¿æ²»å›¢ä½“ç­‰äººå£ç»Ÿè®¡æ•°æ®ã€‚ç”¨æˆ·å¯ä»¥å¯¹æ¯”çœŸå®æŠ•ç¥¨ç»“æœä¸å‰æ²¿ LLMs çš„é¢„æµ‹ï¼Œå¹¶æŸ¥çœ‹æŒ‰äººå£ç»Ÿè®¡ç¾¤ä½“åˆ†ç±»çš„è¯¯å·®åˆ†è§£ã€‚é€šè¿‡å¯è§†åŒ– EuroParlVote åŸºå‡†åŠå…¶æ ¸å¿ƒä»»åŠ¡ï¼ŒParlAI Vote æ­ç¤ºäº†å½“å‰ state-of-the-art LLMs ä¸­å­˜åœ¨çš„ç³»ç»Ÿæ€§æ€§èƒ½åè§ã€‚è¯¥å¹³å°ç»Ÿä¸€äº†æ•°æ®ã€æ¨¡å‹å’Œå¯è§†åŒ–åˆ†æï¼Œé™ä½äº†å¤ç°ç ”ç©¶ã€å®¡è®¡è¡Œä¸ºåŠè¿è¡Œåäº‹å® (counterfactual) åœºæ™¯çš„é—¨æ§›ã€‚æ­¤å¤–ï¼Œå¹³å°è¿˜å±•ç¤ºäº†æ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼Œå¸®åŠ©ç”¨æˆ·è¯†åˆ«é”™è¯¯åŸå› ï¼Œä¸ºæ”¿æ²»å†³ç­–åˆ†æé¢†åŸŸçš„ç ”ç©¶ã€æ•™è‚²å’Œå…¬ä¼—å‚ä¸æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "online demo: https://euro-parl-vote-demo.vercel.app/; Video: https://www.youtube.com/@Jinrui-sf2jg",
      "pdf_url": "https://arxiv.org/pdf/2509.16264v3",
      "published_date": "2025-09-18 04:34:33 UTC",
      "updated_date": "2025-12-02 07:59:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:29:26.985693+00:00"
    },
    {
      "arxiv_id": "2509.14608v1",
      "title": "Enterprise AI Must Enforce Participant-Aware Access Control",
      "title_zh": "ä¼ä¸šçº§äººå·¥æ™ºèƒ½å¿…é¡»å¼ºåˆ¶æ‰§è¡Œæ„ŸçŸ¥å‚ä¸è€…çš„è®¿é—®æ§åˆ¶",
      "authors": [
        "Shashank Shreedhar Bhatt",
        "Tanmay Rajore",
        "Khushboo Aggarwal",
        "Ganesh Ananthanarayanan",
        "Ranveer Chandra",
        "Nishanth Chandran",
        "Suyash Choudhury",
        "Divya Gupta",
        "Emre Kiciman",
        "Sumit Kumar Pandey",
        "Srinath Setty",
        "Rahul Sharma",
        "Teijia Zhao"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in enterprise settings where they interact with multiple users and are trained or fine-tuned on sensitive internal data. While fine-tuning enhances performance by internalizing domain knowledge, it also introduces a critical security risk: leakage of confidential training data to unauthorized users. These risks are exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG) pipelines that dynamically fetch contextual documents at inference time.\n  We demonstrate data exfiltration attacks on AI assistants where adversaries can exploit current fine-tuning and RAG architectures to leak sensitive information by leveraging the lack of access control enforcement. We show that existing defenses, including prompt sanitization, output filtering, system isolation, and training-level privacy mechanisms, are fundamentally probabilistic and fail to offer robust protection against such attacks.\n  We take the position that only a deterministic and rigorous enforcement of fine-grained access control during both fine-tuning and RAG-based inference can reliably prevent the leakage of sensitive data to unauthorized recipients.\n  We introduce a framework centered on the principle that any content used in training, retrieval, or generation by an LLM is explicitly authorized for \\emph{all users involved in the interaction}. Our approach offers a simple yet powerful paradigm shift for building secure multi-user LLM systems that are grounded in classical access control but adapted to the unique challenges of modern AI workflows. Our solution has been deployed in Microsoft Copilot Tuning, a product offering that enables organizations to fine-tune models using their own enterprise-specific data.",
      "tldr_zh": "éšç€å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¼ä¸šç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¾®è°ƒ(fine-tuning)å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯åœ¨æå‡æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿå¸¦æ¥äº†æ•æ„Ÿæ•°æ®æ³„éœ²ç»™æœªç»æˆæƒç”¨æˆ·çš„ä¸¥é‡é£é™©ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæç¤ºè¯æ¸…æ´—(prompt sanitization)å’Œè¾“å‡ºè¿‡æ»¤(output filtering)ç­‰ç°æœ‰é˜²å¾¡æ‰‹æ®µç”±äºå…¶æ¦‚ç‡æ€§æœ¬è´¨ï¼Œéš¾ä»¥æœ‰æ•ˆæŠµå¾¡é’ˆå¯¹ AI æ™ºèƒ½ä½“çš„æ•°æ®å¤–æ³„æ”»å‡»ã€‚ä½œè€…æå‡ºï¼Œå¿…é¡»åœ¨å¾®è°ƒå’ŒåŸºäº RAG çš„æ¨ç†è¿‡ç¨‹ä¸­å®æ–½ç¡®å®šæ€§ä¸”ä¸¥æ ¼çš„ç»†ç²’åº¦è®¿é—®æ§åˆ¶ï¼Œä»¥ä»æ ¹æœ¬ä¸Šé˜²æ­¢ä¿¡æ¯æ³„éœ²ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªä»¥å‚ä¸è€…æ„ŸçŸ¥è®¿é—®æ§åˆ¶(Participant-Aware Access Control)åŸåˆ™ä¸ºæ ¸å¿ƒçš„æ¡†æ¶ï¼Œç¡®ä¿ LLM è®­ç»ƒã€æ£€ç´¢åŠç”Ÿæˆè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ‰€æœ‰å†…å®¹éƒ½ç»è¿‡äº¤äº’ä¸­æ‰€æœ‰å‚ä¸è€…çš„æ˜ç¡®æˆæƒã€‚è¯¥æ–¹æ¡ˆæˆåŠŸå°†ç»å…¸è®¿é—®æ§åˆ¶ç†è®ºé€‚é…äºç°ä»£ AI å·¥ä½œæµï¼Œä¸ºå¤šç”¨æˆ· LLM ç³»ç»Ÿæä¾›äº†ç®€å•ä¸”å¼ºæœ‰åŠ›çš„å®‰å…¨ä¿æŠ¤èŒƒå¼ã€‚ç›®å‰è¯¥æ–¹æ¡ˆå·²åœ¨ Microsoft Copilot Tuning ä¸­å®é™…éƒ¨ç½²ï¼Œæ˜¾è‘—å¢å¼ºäº†ä¼ä¸šçº§æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ•°æ®å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14608v1",
      "published_date": "2025-09-18 04:30:49 UTC",
      "updated_date": "2025-09-18 04:30:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:29:32.096670+00:00"
    },
    {
      "arxiv_id": "2509.14601v1",
      "title": "A Case for Computing on Unstructured Data",
      "title_zh": "è®ºéç»“æ„åŒ–æ•°æ®è®¡ç®—",
      "authors": [
        "Mushtari Sadia",
        "Amrita Roy Chowdhury",
        "Ang Chen"
      ],
      "abstract": "Unstructured data, such as text, images, audio, and video, comprises the vast majority of the world's information, yet it remains poorly supported by traditional data systems that rely on structured formats for computation. We argue for a new paradigm, which we call computing on unstructured data, built around three stages: extraction of latent structure, transformation of this structure through data processing techniques, and projection back into unstructured formats. This bi-directional pipeline allows unstructured data to benefit from the analytical power of structured computation, while preserving the richness and accessibility of unstructured representations for human and AI consumption. We illustrate this paradigm through two use cases and present the research components that need to be developed in a new data system called MXFlow.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå°½ç®¡æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰ unstructured data å æ®äº†å…¨çƒä¿¡æ¯çš„ç»å¤§éƒ¨åˆ†ï¼Œä½†ä¾èµ–ç»“æ„åŒ–æ ¼å¼çš„ä¼ ç»Ÿæ•°æ®ç³»ç»Ÿå¯¹å…¶æ”¯æŒä»ç„¶ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º computing on unstructured data çš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡åŒå‘æµæ°´çº¿è¿æ¥ç»“æ„åŒ–ä¸éç»“æ„åŒ–é¢†åŸŸã€‚è¯¥èŒƒå¼æ ¸å¿ƒåŒ…å« latent structure çš„ extractionã€é€šè¿‡æ•°æ®å¤„ç†æŠ€æœ¯è¿›è¡Œçš„ transformation ä»¥åŠå‘éç»“æ„åŒ–æ ¼å¼çš„ projection ä¸‰ä¸ªé˜¶æ®µã€‚è¿™ç§è®¾è®¡ä½¿ unstructured data èƒ½å¤Ÿåˆ©ç”¨ç»“æ„åŒ–è®¡ç®—çš„å¼ºå¤§åˆ†æèƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶å¯¹äººç±»å’Œ AI è€Œè¨€çš„ä¸°å¯Œè¡¨ç°åŠ›ä¸æ˜“ç”¨æ€§ã€‚è®ºæ–‡æœ€åé€šè¿‡ä¸¤ä¸ªå…·ä½“ç”¨ä¾‹é˜è¿°äº†è¯¥èŒƒå¼ï¼Œå¹¶å±•ç¤ºäº†åœ¨æ„å»ºåä¸º MXFlow çš„æ–°æ•°æ®ç³»ç»Ÿæ—¶æ‰€éœ€å¼€å‘çš„ç ”ç©¶ç»„ä»¶ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14601v1",
      "published_date": "2025-09-18 04:24:41 UTC",
      "updated_date": "2025-09-18 04:24:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:31:02.877795+00:00"
    },
    {
      "arxiv_id": "2509.14594v1",
      "title": "SynBench: A Benchmark for Differentially Private Text Generation",
      "title_zh": "SynBenchï¼šå·®åˆ†éšç§æ–‡æœ¬ç”ŸæˆåŸºå‡†",
      "authors": [
        "Yidan Sun",
        "Viktor Schlegel",
        "Srinivasan Nandakumar",
        "Iqra Zahid",
        "Yuping Wu",
        "Yulong Wu",
        "Hao Li",
        "Jie Zhang",
        "Warren Del-Pinto",
        "Goran Nenadic",
        "Siew Kei Lam",
        "Anil Anthony Bharath"
      ],
      "abstract": "Data-driven decision support in high-stakes domains like healthcare and finance faces significant barriers to data sharing due to regulatory, institutional, and privacy concerns. While recent generative AI models, such as large language models, have shown impressive performance in open-domain tasks, their adoption in sensitive environments remains limited by unpredictable behaviors and insufficient privacy-preserving datasets for benchmarking. Existing anonymization methods are often inadequate, especially for unstructured text, as redaction and masking can still allow re-identification. Differential Privacy (DP) offers a principled alternative, enabling the generation of synthetic data with formal privacy assurances. In this work, we address these challenges through three key contributions. First, we introduce a comprehensive evaluation framework with standardized utility and fidelity metrics, encompassing nine curated datasets that capture domain-specific complexities such as technical jargon, long-context dependencies, and specialized document structures. Second, we conduct a large-scale empirical study benchmarking state-of-the-art DP text generation methods and LLMs of varying sizes and different fine-tuning strategies, revealing that high-quality domain-specific synthetic data generation under DP constraints remains an unsolved challenge, with performance degrading as domain complexity increases. Third, we develop a membership inference attack (MIA) methodology tailored for synthetic text, providing first empirical evidence that the use of public datasets - potentially present in pre-training corpora - can invalidate claimed privacy guarantees. Our findings underscore the urgent need for rigorous privacy auditing and highlight persistent gaps between open-domain and specialist evaluations, informing responsible deployment of generative AI in privacy-sensitive, high-stakes settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å’Œé‡‘èç­‰é«˜é£é™©é¢†åŸŸå› éšç§ç›‘ç®¡å¯¼è‡´çš„å…±äº«éš¾é¢˜ï¼Œæå‡ºäº† SynBenchï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å·®åˆ†éšç§ (Differential Privacy, DP) æ–‡æœ¬ç”Ÿæˆçš„åŸºå‡†æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¹ä¸ªæ¶µç›–æŠ€æœ¯æœ¯è¯­å’Œé•¿ä¸Šä¸‹æ–‡ä¾èµ–çš„ç²¾é€‰æ•°æ®é›†ï¼Œæä¾›äº†æ ‡å‡†åŒ–çš„ Utility å’Œ Fidelity è¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡å¯¹å¤šç§å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å’Œ DP ç”Ÿæˆæ–¹æ³•çš„å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œä½œè€…å‘ç°ç”Ÿæˆé«˜è´¨é‡é¢†åŸŸç‰¹å®šåˆæˆæ•°æ®ä»æ˜¯æœªè§£å†³çš„æŒ‘æˆ˜ï¼Œä¸”æ€§èƒ½éšé¢†åŸŸå¤æ‚æ€§å¢åŠ è€Œä¸‹é™ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹åˆæˆæ–‡æœ¬çš„æˆå‘˜æ¨ç†æ”»å‡» (Membership Inference Attack, MIA) æ–¹æ³•ï¼Œè¯æ˜äº†å…¬å¼€æ•°æ®é›†åœ¨é¢„è®­ç»ƒä¸­çš„å­˜åœ¨å¯èƒ½ä¼šä½¿éšç§ä¿è¯å¤±æ•ˆã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†é€šç”¨é¢†åŸŸä¸ä¸“ä¸šé¢†åŸŸè¯„ä¼°ä¹‹é—´çš„å·®è·ï¼Œå¼ºè°ƒäº†åœ¨æ•æ„Ÿç¯å¢ƒä¸‹è¿›è¡Œä¸¥æ ¼éšç§å®¡è®¡å¯¹è´Ÿè´£ä»»éƒ¨ç½²ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.14594v1",
      "published_date": "2025-09-18 03:57:50 UTC",
      "updated_date": "2025-09-18 03:57:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:31:23.580118+00:00"
    },
    {
      "arxiv_id": "2509.14589v1",
      "title": "ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System",
      "title_zh": "ATLANTISï¼šAI é©±åŠ¨çš„å¨èƒå®šä½ã€åˆ†æä¸åˆ†è¯Šæ™ºèƒ½ç³»ç»Ÿ",
      "authors": [
        "Taesoo Kim",
        "HyungSeok Han",
        "Soyeon Park",
        "Dae R. Jeong",
        "Dohyeok Kim",
        "Dongkwan Kim",
        "Eunsoo Kim",
        "Jiho Kim",
        "Joshua Wang",
        "Kangsu Kim",
        "Sangwoo Ji",
        "Woosun Song",
        "Hanqing Zhao",
        "Andrew Chin",
        "Gyejin Lee",
        "Kevin Stevens",
        "Mansour Alharthi",
        "Yizhuo Zhai",
        "Cen Zhang",
        "Joonun Jang",
        "Yeongjin Jang",
        "Ammar Askar",
        "Dongju Kim",
        "Fabian Fleischer",
        "Jeongin Cho",
        "Junsik Kim",
        "Kyungjoon Ko",
        "Insu Yun",
        "Sangdon Park",
        "Dowoo Baik",
        "Haein Lee",
        "Hyeon Heo",
        "Minjae Gwon",
        "Minjae Lee",
        "Minwoo Baek",
        "Seunggi Min",
        "Wonyoung Kim",
        "Yonghwi Jin",
        "Younggi Park",
        "Yunjae Choi",
        "Jinho Jung",
        "Gwanhyun Lee",
        "Junyoung Jang",
        "Kyuheon Kim",
        "Yeonghyeon Cha",
        "Youngjoon Kim"
      ],
      "abstract": "We present ATLANTIS, the cyber reasoning system developed by Team Atlanta that won 1st place in the Final Competition of DARPA's AI Cyber Challenge (AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to build autonomous cyber reasoning systems capable of discovering and patching vulnerabilities at the speed and scale of modern software. ATLANTIS integrates large language models (LLMs) with program analysis -- combining symbolic execution, directed fuzzing, and static analysis -- to address limitations in automated vulnerability discovery and program repair. Developed by researchers at Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the system addresses core challenges: scaling across diverse codebases from C to Java, achieving high precision while maintaining broad coverage, and producing semantically correct patches that preserve intended behavior. We detail the design philosophy, architectural decisions, and implementation strategies behind ATLANTIS, share lessons learned from pushing the boundaries of automated security when program analysis meets modern AI, and release artifacts to support reproducibility and future research.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ç”±Team Atlantaå¼€å‘çš„ATLANTISç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåœ¨DEF CON 33ä¸¾åŠçš„DARPA AI Cyber Challenge (AIxCC) å†³èµ›ä¸­è£è·å† å†›ã€‚ä½œä¸ºä¸€ç§è‡ªä¸»ç½‘ç»œæ¨ç†ç³»ç»Ÿ(cyber reasoning system)ï¼ŒATLANTISæ—¨åœ¨ä»¥ç°ä»£è½¯ä»¶çš„é€Ÿåº¦å’Œè§„æ¨¡è‡ªåŠ¨å‘ç°å¹¶ä¿®å¤å®‰å…¨æ¼æ´ã€‚è¯¥ç³»ç»Ÿå°†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸ç¨‹åºåˆ†æ(program analysis)æŠ€æœ¯æ·±åº¦é›†æˆï¼Œé€šè¿‡ç»“åˆç¬¦å·æ‰§è¡Œ(symbolic execution)ã€å®šå‘æ¨¡ç³Šæµ‹è¯•(directed fuzzing)å’Œé™æ€åˆ†æ(static analysis)ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿè‡ªåŠ¨åŒ–æ¼æ´æŒ–æ˜å’Œç¨‹åºä¿®å¤çš„å±€é™æ€§ã€‚ç”±Georgia Institute of Technologyå’ŒSamsung Researchç­‰æœºæ„è”åˆå¼€å‘çš„è¿™ä¸€ç³»ç»ŸæˆåŠŸè§£å†³äº†è·¨Cå’ŒJavaç­‰å¤šæ ·åŒ–ä»£ç åº“çš„æ‰©å±•æ€§éš¾é¢˜ï¼Œåœ¨ä¿æŒå¹¿æ³›è¦†ç›–ç‡çš„åŒæ—¶å®ç°äº†æé«˜çš„æ£€æµ‹ç²¾åº¦ã€‚ATLANTISèƒ½å¤Ÿç”Ÿæˆè¯­ä¹‰æ­£ç¡®ä¸”ä¿ç•™åŸå§‹åŠŸèƒ½çš„è¡¥ä¸(semantically correct patches)ï¼Œä¸ºå®‰å…¨é¢†åŸŸç»“åˆç°ä»£AIä¸ç¨‹åºåˆ†ææä¾›äº†å®è´µçš„è®¾è®¡æ–¹æ¡ˆã€‚ç›®å‰ï¼Œç ”ç©¶å›¢é˜Ÿå·²å‘å¸ƒç›¸å…³å·¥ä»¶(artifacts)ä»¥æ”¯æŒç»“æœå¤ç°ï¼Œæ˜¾è‘—æ¨åŠ¨äº†è‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨é˜²æŠ¤æŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Version 1.0 (September 17, 2025). Technical Report. Team Atlanta -- 1st place in DARPA AIxCC Final Competition. Project page: https://team-atlanta.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2509.14589v1",
      "published_date": "2025-09-18 03:46:18 UTC",
      "updated_date": "2025-09-18 03:46:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:31:06.179744+00:00"
    },
    {
      "arxiv_id": "2509.14581v1",
      "title": "Can I Trust This Chatbot? Assessing User Privacy in AI-Healthcare Chatbot Applications",
      "title_zh": "æˆ‘èƒ½ä¿¡ä»»è¿™ä¸ªèŠå¤©æœºå™¨äººå—ï¼Ÿäººå·¥æ™ºèƒ½åŒ»ç–—èŠå¤©æœºå™¨äººåº”ç”¨çš„ç”¨æˆ·éšç§è¯„ä¼°",
      "authors": [
        "Ramazan Yener",
        "Guan-Hung Chen",
        "Ece Gumusel",
        "Masooda Bashir"
      ],
      "abstract": "As Conversational Artificial Intelligence (AI) becomes more integrated into everyday life, AI-powered chatbot mobile applications are increasingly adopted across industries, particularly in the healthcare domain. These chatbots offer accessible and 24/7 support, yet their collection and processing of sensitive health data present critical privacy concerns. While prior research has examined chatbot security, privacy issues specific to AI healthcare chatbots have received limited attention. Our study evaluates the privacy practices of 12 widely downloaded AI healthcare chatbot apps available on the App Store and Google Play in the United States. We conducted a three-step assessment analyzing: (1) privacy settings during sign-up, (2) in-app privacy controls, and (3) the content of privacy policies. The analysis identified significant gaps in user data protection. Our findings reveal that half of the examined apps did not present a privacy policy during sign up, and only two provided an option to disable data sharing at that stage. The majority of apps' privacy policies failed to address data protection measures. Moreover, users had minimal control over their personal data. The study provides key insights for information science researchers, developers, and policymakers to improve privacy protections in AI healthcare chatbot apps.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†12æ¬¾åœ¨ç¾å›½ä¸»æµåº”ç”¨å•†åº—ä¸‹è½½é‡è¾ƒé«˜çš„ AI-Healthcare Chatbot åº”ç”¨ç¨‹åºçš„éšç§å®è·µï¼Œæ—¨åœ¨æ­ç¤ºæ­¤ç±»åº”ç”¨åœ¨å¤„ç†æ•æ„Ÿå¥åº·æ•°æ®æ—¶çš„éšç§é£é™©ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¸‰ä¸ªæ­¥éª¤åˆ†æäº†æ³¨å†Œé˜¶æ®µçš„ Privacy Settingsã€åº”ç”¨å†…çš„éšç§æ§åˆ¶ä»¥åŠ Privacy Policies çš„å…·ä½“å†…å®¹ï¼Œå‘ç°ç”¨æˆ·æ•°æ®ä¿æŠ¤æ–¹é¢å­˜åœ¨æ˜¾è‘—æ¼æ´ã€‚è°ƒæŸ¥ç»“æœæ˜¾ç¤ºï¼ŒåŠæ•°åº”ç”¨åœ¨ç”¨æˆ·æ³¨å†Œæ—¶æœªå±•ç¤ºéšç§æ”¿ç­–ï¼Œä¸”ä»…æœ‰ä¸¤æ¬¾åº”ç”¨å…è®¸ç”¨æˆ·åœ¨åˆå§‹é˜¶æ®µç¦ç”¨ Data Sharingã€‚æ­¤å¤–ï¼Œå¤šæ•°åº”ç”¨çš„éšç§æ”¿ç­–ç¼ºä¹å¯¹æ•°æ®ä¿æŠ¤æªæ–½çš„è¯¦ç»†æè¿°ï¼Œç”¨æˆ·å¯¹å…¶ä¸ªäººæ•°æ®çš„æ§åˆ¶æƒæå…¶æœ‰é™ã€‚è¯¥ç ”ç©¶ä¸ºä¿¡æ¯ç§‘å­¦ç ”ç©¶äººå‘˜ã€å¼€å‘è€…åŠæ”¿ç­–åˆ¶å®šè€…æä¾›äº†é‡è¦è§è§£ï¼Œå¼ºè°ƒäº†æå‡ AI-Healthcare Chatbot éšç§ä¿æŠ¤æ°´å¹³çš„è¿«åˆ‡æ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "13 pages. To be published in ASIS&T 2025 Conference",
      "pdf_url": "https://arxiv.org/pdf/2509.14581v1",
      "published_date": "2025-09-18 03:29:43 UTC",
      "updated_date": "2025-09-18 03:29:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:31:12.243367+00:00"
    },
    {
      "arxiv_id": "2509.14574v2",
      "title": "Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark",
      "title_zh": "è§†è§‰è¯­è¨€æ¨¡å‹å¯¹åŸå¸‚åœºæ™¯çš„æ„ŸçŸ¥æ˜¯å¦ä¸äººç±»ä¸€è‡´ï¼ŸåŸå¸‚æ„ŸçŸ¥åŸºå‡†",
      "authors": [
        "Rashid Mushkani"
      ],
      "abstract": "Understanding how people read city scenes can inform design and planning. We introduce a small benchmark for testing vision-language models (VLMs) on urban perception using 100 Montreal street images, evenly split between photographs and photorealistic synthetic scenes. Twelve participants from seven community groups supplied 230 annotation forms across 30 dimensions mixing physical attributes and subjective impressions. French responses were normalized to English. We evaluated seven VLMs in a zero-shot setup with a structured prompt and deterministic parser. We use accuracy for single-choice items and Jaccard overlap for multi-label items; human agreement uses Krippendorff's alpha and pairwise Jaccard. Results suggest stronger model alignment on visible, objective properties than subjective appraisals. The top system (claude-sonnet) reaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher human agreement coincides with better model scores. Synthetic images slightly lower scores. We release the benchmark, prompts, and harness for reproducible, uncertainty-aware evaluation in participatory urban analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªç”¨äºæµ‹è¯•è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) åŸå¸‚æ„ŸçŸ¥èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•é›†ï¼Œæ—¨åœ¨æ¢è®¨è¿™äº›æ¨¡å‹ç†è§£åŸå¸‚è¡—æ™¯çš„æ–¹å¼æ˜¯å¦ä¸äººç±»ä¸€è‡´ã€‚è¯¥åŸºå‡†åŒ…å«100å¼ è’™ç‰¹åˆ©å°”è¡—é“å›¾åƒï¼Œæ¶µç›–çœŸå®ç…§ç‰‡å’Œå…‰çº¿è¿½è¸ªåˆæˆåœºæ™¯ï¼Œå¹¶æ”¶é›†äº†æ¥è‡ªä¸åŒç¤¾ç¾¤å‚ä¸è€…çš„230ä»½æ ‡æ³¨ï¼Œæ¶‰åŠç‰©ç†å±æ€§ä¸ä¸»è§‚å°è±¡å…±30ä¸ªç»´åº¦ã€‚ç ”ç©¶åœ¨é›¶æ ·æœ¬ (Zero-shot) è®¾ç½®ä¸‹è¯„ä¼°äº†åŒ…æ‹¬ Claude-Sonnet åœ¨å†…çš„ä¸ƒç§ä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å‡†ç¡®ç‡å’Œ Jaccard é‡å ç‡ç­‰æŒ‡æ ‡è¡¡é‡å…¶è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨è¯†åˆ«å®¢è§‚å¯è§å±æ€§æ–¹é¢çš„å¯¹é½ç¨‹åº¦æ˜æ˜¾é«˜äºä¸»è§‚è¯„ä»·ï¼Œä¸” Claude-Sonnet åœ¨å¤šæ ‡ç­¾é¡¹ç›®ä¸­å–å¾—äº†æœ€ä½³è¡¨ç°ã€‚ç ”ç©¶è¿˜å‘ç°æ¨¡å‹å¾—åˆ†ä¸äººç±»ä¸€è‡´æ€§ (human agreement) å‘ˆæ­£ç›¸å…³ï¼Œå°½ç®¡åˆæˆå›¾åƒä¼šç•¥å¾®é™ä½å¾—åˆ†ï¼Œä½†è¯¥å…¬å¼€çš„åŸºå‡†åŠè¯„ä¼°æ¡†æ¶ä¸ºåŸå¸‚æ„ŸçŸ¥åˆ†ææä¾›äº†å¯å¤ç°çš„å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14574v2",
      "published_date": "2025-09-18 03:21:10 UTC",
      "updated_date": "2025-10-05 16:29:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:31:11.271319+00:00"
    },
    {
      "arxiv_id": "2509.14571v1",
      "title": "VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models",
      "title_zh": "VisMoDAlï¼šé¢å‘è§†è§‰-è¯­è¨€æ¨¡å‹æŸåé²æ£’æ€§è¯„ä¼°ä¸æå‡çš„å¯è§†åˆ†æ",
      "authors": [
        "Huanchen Wang",
        "Wencheng Zhang",
        "Zhiqiang Wang",
        "Zhicong Lu",
        "Yuxin Ma"
      ],
      "abstract": "Vision-language (VL) models have shown transformative potential across various critical domains due to their capability to comprehend multi-modal information. However, their performance frequently degrades under distribution shifts, making it crucial to assess and improve robustness against real-world data corruption encountered in practical applications. While advancements in VL benchmark datasets and data augmentation (DA) have contributed to robustness evaluation and improvement, there remain challenges due to a lack of in-depth comprehension of model behavior as well as the need for expertise and iterative efforts to explore data patterns. Given the achievement of visualization in explaining complex models and exploring large-scale data, understanding the impact of various data corruption on VL models aligns naturally with a visual analytics approach. To address these challenges, we introduce VisMoDAl, a visual analytics framework designed to evaluate VL model robustness against various corruption types and identify underperformed samples to guide the development of effective DA strategies. Grounded in the literature review and expert discussions, VisMoDAl supports multi-level analysis, ranging from examining performance under specific corruptions to task-driven inspection of model behavior and corresponding data slice. Unlike conventional works, VisMoDAl enables users to reason about the effects of corruption on VL models, facilitating both model behavior understanding and DA strategy formulation. The utility of our system is demonstrated through case studies and quantitative evaluations focused on corruption robustness in the image captioning task.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VisMoDAlï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼° Vision-Language (VL) æ¨¡å‹å¯¹å„ç§ data corruption é²æ£’æ€§ï¼ˆrobustnessï¼‰å¹¶æŒ‡å¯¼åˆ¶å®šæœ‰æ•ˆæ•°æ®å¢å¼ºï¼ˆdata augmentation, DAï¼‰ç­–ç•¥çš„å¯è§†åˆ†æï¼ˆvisual analyticsï¼‰æ¡†æ¶ã€‚é’ˆå¯¹ VL æ¨¡å‹åœ¨é¢å¯¹ distribution shifts æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶æ”¯æŒä»ç‰¹å®šæŸåç±»å‹ä¸‹çš„æ€§èƒ½è¯„ä¼°åˆ°ä»»åŠ¡é©±åŠ¨çš„æ¨¡å‹è¡Œä¸ºåŠæ•°æ®åˆ‡ç‰‡æ£€æŸ¥çš„å¤šå±‚æ¬¡åˆ†æã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒVisMoDAl å…è®¸ç”¨æˆ·é€šè¿‡äº¤äº’å¼æ¨ç†ç†è§£ corruption å¯¹æ¨¡å‹çš„å½±å“ï¼Œä»è€Œç²¾å‡†è¯†åˆ«æ€§èƒ½æ¬ ä½³çš„æ ·æœ¬å¹¶ä¼˜åŒ– DA ç­–ç•¥ã€‚é€šè¿‡åœ¨ image captioning ä»»åŠ¡ä¸Šçš„æ¡ˆä¾‹ç ”ç©¶å’Œå®šé‡è¯„ä¼°ï¼Œè¯¥ç³»ç»Ÿè¯æ˜äº†å…¶åœ¨å¢å¼ºæ¨¡å‹ç†è§£å’Œæå‡é²æ£’æ€§æ–¹é¢çš„å®ç”¨ä»·å€¼ï¼Œä¸ºåº”å¯¹ç°å®ä¸–ç•Œæ•°æ®æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆçš„åˆ†æå·¥å…·ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "11 pages, 7 figures, 1 table, accepted to IEEE VIS 2025 (IEEE Transactions on Visualization and Computer Graphics)",
      "pdf_url": "https://arxiv.org/pdf/2509.14571v1",
      "published_date": "2025-09-18 03:15:00 UTC",
      "updated_date": "2025-09-18 03:15:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:31:15.661099+00:00"
    },
    {
      "arxiv_id": "2509.16262v2",
      "title": "Socratic Mind: Impact of a Novel GenAI-Powered Assessment Tool on Student Learning and Higher-Order Thinking",
      "title_zh": "Socratic Mindï¼šæ–°å‹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é©±åŠ¨çš„è¯„ä¼°å·¥å…·å¯¹å­¦ç”Ÿå­¦ä¹ ä¸é«˜é˜¶æ€ç»´çš„å½±å“",
      "authors": [
        "Jeonghyun Lee",
        "Jui-Tse Hung",
        "Meryem Yilmaz Soylu",
        "Diana Popescu",
        "Christopher Zhang Cui",
        "Gayane Grigoryan",
        "David A Joyner",
        "Stephen W Harmon"
      ],
      "abstract": "This study examines the impact of Socratic Mind, a Generative Artificial Intelligence (GenAI) powered formative assessment tool that employs Socratic questioning to support student learning in a large, fully online undergraduate-level computing course. Employing a quasi-experimental, mixed-methods design, we investigated participants' engagement patterns, the influence of user experience on engagement, and impacts on both perceived and actual learning outcomes. Data were collected from the system logs, surveys on user experience and perceived engagement and learning gains, student reflections, and course performance data. Results indicated that participants consistently reported high levels of affective, behavioral, and cognitive engagement, and these were strongly linked to positive user experiences and perceived learning outcomes. Quantitative analysis further revealed that students who engaged with the GenAI tool experienced significant gains in their quiz scores compared to those who did not, particularly benefiting students with lower baseline achievement. Additionally, thematic analysis of qualitative feedback revealed substantial perceived improvements in higher-order thinking skills, including problem solving, critical thinking, and self-reflection. Our findings highlight the promise of AI-mediated dialogue in fostering deeper engagement and higher-order cognitive skills. As higher education institutions expand GenAI integration in curriculum, this dialogic, GenAI powered assessment tool can offer a scalable strategy to promote students' meaningful learning outcomes.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åä¸º Socratic Mind çš„æ–°å‹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (GenAI) é©±åŠ¨çš„å½¢æˆæ€§è¯„ä¼°å·¥å…·å¯¹å­¦ç”Ÿå­¦ä¹ çš„å½±å“ï¼Œè¯¥å·¥å…·åˆ©ç”¨ Socratic questioning åœ¨å¤§è§„æ¨¡åœ¨çº¿æœ¬ç§‘è®¡ç®—æœºè¯¾ç¨‹ä¸­æä¾›æ”¯æŒã€‚ç ”ç©¶é‡‡ç”¨äº†å‡†å®éªŒ (quasi-experimental) å’Œæ··åˆæ–¹æ³• (mixed-methods) è®¾è®¡ï¼Œé€šè¿‡åˆ†æç³»ç»Ÿæ—¥å¿—ã€è°ƒæŸ¥é—®å·åŠè¯¾ç¨‹è¡¨ç°æ•°æ®ï¼Œè¯„ä¼°äº†å­¦ç”Ÿçš„å‚ä¸æ¨¡å¼åŠå…¶å¯¹å­¦ä¹ æˆæœçš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œå‚ä¸è€…åœ¨æƒ…æ„Ÿã€è¡Œä¸ºå’Œè®¤çŸ¥å±‚é¢å‡è¡¨ç°å‡ºæé«˜çš„å‚ä¸åº¦ (engagement)ï¼Œä¸”è¿™ç§å‚ä¸åº¦ä¸ç§¯æçš„ç”¨æˆ·ä½“éªŒå’Œæ„ŸçŸ¥çš„å­¦ä¹ æ”¶ç›Šå¼ºç›¸å…³ã€‚å®šé‡åˆ†ææ­ç¤ºï¼Œä¸æœªä½¿ç”¨è¯¥å·¥å…·çš„å­¦ç”Ÿç›¸æ¯”ï¼Œä½¿ç”¨è€…çš„æµ‹éªŒæˆç»©æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåŸºç¡€è¾ƒå¼±çš„å­¦ç”Ÿè·ç›Šæœ€å¤§ã€‚æ­¤å¤–ï¼Œå®šæ€§åé¦ˆæ˜¾ç¤ºå­¦ç”Ÿåœ¨ problem solvingã€critical thinking ä»¥åŠ self-reflection ç­‰é«˜é˜¶æ€ç»´èƒ½åŠ›æ–¹é¢æœ‰æ˜æ˜¾è¿›æ­¥ã€‚è¯¥ç ”ç©¶è¯å®äº† AI-mediated dialogue åœ¨ä¿ƒè¿›å­¦ç”Ÿæ·±åº¦å­¦ä¹ æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºé«˜ç­‰æ•™è‚²åœ¨å¤§è§„æ¨¡è¯¾ç¨‹ä¸­æ•´åˆ GenAI æä¾›äº†å¯æ‰©å±•çš„ç­–ç•¥ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16262v2",
      "published_date": "2025-09-18 03:08:24 UTC",
      "updated_date": "2025-10-16 04:07:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:31:18.166290+00:00"
    },
    {
      "arxiv_id": "2509.19352v1",
      "title": "TriSPrompt: A Hierarchical Soft Prompt Model for Multimodal Rumor Detection with Incomplete Modalities",
      "title_zh": "TriSPromptï¼šé¢å‘ä¸å®Œæ•´æ¨¡æ€ä¸‹å¤šæ¨¡æ€è°£è¨€æ£€æµ‹çš„å±‚çº§åŒ–è½¯æç¤ºæ¨¡å‹",
      "authors": [
        "Jiajun Chen",
        "Yangyang Wu",
        "Xiaoye Miao",
        "Mengying Zhu",
        "Meng Xi"
      ],
      "abstract": "The widespread presence of incomplete modalities in multimodal data poses a significant challenge to achieving accurate rumor detection. Existing multimodal rumor detection methods primarily focus on learning joint modality representations from \\emph{complete} multimodal training data, rendering them ineffective in addressing the common occurrence of \\emph{missing modalities} in real-world scenarios. In this paper, we propose a hierarchical soft prompt model \\textsf{TriSPrompt}, which integrates three types of prompts, \\textit{i.e.}, \\emph{modality-aware} (MA) prompt, \\emph{modality-missing} (MM) prompt, and \\emph{mutual-views} (MV) prompt, to effectively detect rumors in incomplete multimodal data. The MA prompt captures both heterogeneous information from specific modalities and homogeneous features from available data, aiding in modality recovery. The MM prompt models missing states in incomplete data, enhancing the model's adaptability to missing information. The MV prompt learns relationships between subjective (\\textit{i.e.}, text and image) and objective (\\textit{i.e.}, comments) perspectives, effectively detecting rumors. Extensive experiments on three real-world benchmarks demonstrate that \\textsf{TriSPrompt} achieves an accuracy gain of over 13\\% compared to state-of-the-art methods. The codes and datasets are available at https: //anonymous.4open.science/r/code-3E88.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€è°£è¨€æ£€æµ‹ä¸­å¸¸è§çš„æ¨¡æ€ç¼ºå¤±(incomplete modalities)æŒ‘æˆ˜ï¼Œæå‡ºäº†å±‚æ¬¡åŒ–è½¯æç¤ºæ¨¡å‹ TriSPromptã€‚è¯¥æ¨¡å‹é›†æˆäº†ä¸‰ç§æ ¸å¿ƒæç¤ºï¼šæ¨¡æ€æ„ŸçŸ¥(Modality-aware, MA)æç¤ºé€šè¿‡æ•æ‰ç‰¹å®šæ¨¡æ€çš„å¼‚æ„ä¿¡æ¯å’Œå¯ç”¨æ•°æ®çš„åŒæ„ç‰¹å¾æ¥è¾…åŠ©æ¨¡æ€æ¢å¤ï¼›æ¨¡æ€ç¼ºå¤±(Modality-missing, MM)æç¤ºé€šè¿‡å¯¹ç¼ºå¤±çŠ¶æ€å»ºæ¨¡æ¥å¢å¼ºæ¨¡å‹å¯¹ä¿¡æ¯ä¸å®Œæ•´æ€§çš„é€‚åº”èƒ½åŠ›ï¼›ç›¸äº’è§†å›¾(Mutual-views, MV)æç¤ºåˆ™é€šè¿‡å­¦ä¹ ä¸»è§‚è§†è§’ï¼ˆæ–‡æœ¬å’Œå›¾åƒï¼‰ä¸å®¢è§‚è§†è§’ï¼ˆè¯„è®ºï¼‰ä¹‹é—´çš„å…³ç³»æ¥æå‡æ£€æµ‹ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTriSPrompt åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº† 13% ä»¥ä¸Šã€‚è¯¥ç ”ç©¶æœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç°å®åœºæ™¯ä¸­ä¸å®Œæ•´å¤šæ¨¡æ€æ•°æ®æ—¶çš„å±€é™æ€§ï¼Œä¸ºç¨³å¥çš„è°£è¨€æ£€æµ‹æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19352v1",
      "published_date": "2025-09-18 02:46:51 UTC",
      "updated_date": "2025-09-18 02:46:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:31:28.382701+00:00"
    },
    {
      "arxiv_id": "2509.14558v1",
      "title": "LLM Jailbreak Detection for (Almost) Free!",
      "title_zh": "(è¿‘ä¹) é›¶æˆæœ¬çš„å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±æ£€æµ‹",
      "authors": [
        "Guorui Chen",
        "Yifan Xia",
        "Xiaojun Jia",
        "Zhijiang Li",
        "Philip Torr",
        "Jindong Gu"
      ],
      "abstract": "Large language models (LLMs) enhance security through alignment when widely used, but remain susceptible to jailbreak attacks capable of producing inappropriate content. Jailbreak detection methods show promise in mitigating jailbreak attacks through the assistance of other models or multiple model inferences. However, existing methods entail significant computational costs. In this paper, we first present a finding that the difference in output distributions between jailbreak and benign prompts can be employed for detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak Detection (FJD) which prepends an affirmative instruction to the input and scales the logits by temperature to further distinguish between jailbreak and benign prompts through the confidence of the first token. Furthermore, we enhance the detection performance of FJD through the integration of virtual instruction learning. Extensive experiments on aligned LLMs show that our FJD can effectively detect jailbreak prompts with almost no additional computational costs during LLM inference.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ˜“å—è¶Šç‹±æ”»å‡» (jailbreak attacks) ä¸”ç°æœ‰æ£€æµ‹æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Free Jailbreak Detection (FJD) çš„é«˜æ•ˆæ£€æµ‹æ–¹æ¡ˆã€‚ç ”ç©¶è€…é¦–å…ˆå‘ç°è¶Šç‹±æç¤ºä¸è‰¯æ€§æç¤ºåœ¨è¾“å‡ºåˆ†å¸ƒ (output distributions) ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºæ£€æµ‹çš„åˆ‡å…¥ç‚¹ã€‚FJD é€šè¿‡åœ¨è¾“å…¥ä¸­é¢„ç½®è‚¯å®šæ€§æŒ‡ä»¤ (affirmative instruction) å¹¶ç»“åˆæ¸©åº¦ (temperature) ç¼©æ”¾é€»è¾‘å€¼ (logits)ï¼Œåˆ©ç”¨é¦–ä¸ª Token çš„ç½®ä¿¡åº¦ (confidence) æ¥è¯†åˆ«æ½œåœ¨çš„æ”»å‡»è¡Œä¸ºã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†è™šæ‹ŸæŒ‡ä»¤å­¦ä¹  (virtual instruction learning) æŠ€æœ¯ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ– FJD çš„æ£€æµ‹æ€§èƒ½ã€‚åœ¨å¤šç§å·²å¯¹é½ LLMs ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡ ä¹ä¸äº§ç”Ÿé¢å¤–è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œèƒ½æœ‰æ•ˆæ‹¦æˆªè¶Šç‹±æç¤ºã€‚è¿™ä¸€å‘ç°ä¸ºæ„å»ºä½æˆæœ¬ã€é«˜æ•ˆç‡çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å®‰å…¨é˜²æŠ¤æœºåˆ¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14558v1",
      "published_date": "2025-09-18 02:42:52 UTC",
      "updated_date": "2025-09-18 02:42:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:31:32.155485+00:00"
    },
    {
      "arxiv_id": "2509.14547v1",
      "title": "(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration",
      "title_zh": "(P)rior(D)yna(F)lowï¼šåŸºäºå¤šæ™ºèƒ½ä½“åä½œçš„å…ˆéªŒåŠ¨æ€å·¥ä½œæµæ„å»º",
      "authors": [
        "Yi Lin",
        "Lujin Zhao",
        "Yijie Shi"
      ],
      "abstract": "Recent studies have shown that carefully designed workflows coordinating large language models(LLMs) significantly enhance task-solving capabilities compared to using a single model. While an increasing number of works focus on autonomous workflow construction, most existing approaches rely solely on historical experience, leading to limitations in efficiency and adaptability. We argue that while historical experience is valuable, workflow construction should also flexibly respond to the unique characteristics of each task. To this end, we propose an a priori dynamic framework for automated workflow construction. Our framework first leverages Q-table learning to optimize the decision space, guiding agent decisions and enabling effective use of historical experience. At the same time, agents evaluate the current task progress and make a priori decisions regarding the next executing agent, allowing the system to proactively select the more suitable workflow structure for each given task. Additionally, we incorporate mechanisms such as cold-start initialization, early stopping, and pruning to further improve system efficiency. Experimental evaluations on four benchmark datasets demonstrate the feasibility and effectiveness of our approach. Compared to state-of-the-art baselines, our method achieves an average improvement of 4.05%, while reducing workflow construction and inference costs to only 30.68%-48.31% of those required by existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†(P)rior(D)yna(F)lowï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“åä½œ(Multi-Agent Collaboration)çš„å…ˆéªŒåŠ¨æ€å·¥ä½œæµæ„å»ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªä¸»å·¥ä½œæµæ„å»ºè¿‡åº¦ä¾èµ–å†å²ç»éªŒè€Œå¯¼è‡´çš„æ•ˆç‡å’Œé€‚åº”æ€§é™åˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥Q-tableå­¦ä¹ æ¥ä¼˜åŒ–å†³ç­–ç©ºé—´ï¼Œæœ‰æ•ˆåˆ©ç”¨å†å²ç»éªŒæ¥æŒ‡å¯¼æ™ºèƒ½ä½“å†³ç­–ã€‚åŒæ—¶ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®å½“å‰ä»»åŠ¡è¿›åº¦å¯¹ä¸‹ä¸€ä¸ªæ‰§è¡Œç¯èŠ‚åšå‡ºå…ˆéªŒå†³ç­–ï¼Œä½¿ç³»ç»Ÿèƒ½ä¸ºç‰¹å®šä»»åŠ¡ä¸»åŠ¨é€‰æ‹©æ›´åˆé€‚çš„å·¥ä½œæµç»“æ„ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ•ˆç‡ï¼Œç ”ç©¶è¿˜æ•´åˆäº†å†·å¯åŠ¨åˆå§‹åŒ–(cold-start initialization)ã€æå‰åœæ­¢(early stopping)å’Œå‰ªæ(pruning)ç­‰æœºåˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾ƒæœ€å…ˆè¿›åŸºå‡†æ¨¡å‹å¹³å‡æ€§èƒ½æå‡äº†4.05%ã€‚æ­¤å¤–ï¼Œå…¶å·¥ä½œæµæ„å»ºå’Œæ¨ç†æˆæœ¬ä»…ä¸ºç°æœ‰æ–¹æ³•çš„30.68%-48.31%ï¼Œæ˜¾è‘—æé«˜äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»»åŠ¡è§£å†³çš„ç»æµæ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14547v1",
      "published_date": "2025-09-18 02:24:14 UTC",
      "updated_date": "2025-09-18 02:24:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:31:31.365797+00:00"
    },
    {
      "arxiv_id": "2509.14546v1",
      "title": "Rationality Check! Benchmarking the Rationality of Large Language Models",
      "title_zh": "ç†æ€§æ£€éªŒï¼šå¤§è¯­è¨€æ¨¡å‹ç†æ€§çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Zhilun Zhou",
        "Jing Yi Wang",
        "Nicholas Sukiennik",
        "Chen Gao",
        "Fengli Xu",
        "Yong Li",
        "James Evans"
      ],
      "abstract": "Large language models (LLMs), a recent advance in deep learning and machine intelligence, have manifested astonishing capacities, now considered among the most promising for artificial general intelligence. With human-like capabilities, LLMs have been used to simulate humans and serve as AI assistants across many applications. As a result, great concern has arisen about whether and under what circumstances LLMs think and behave like real human agents. Rationality is among the most important concepts in assessing human behavior, both in thinking (i.e., theoretical rationality) and in taking action (i.e., practical rationality). In this work, we propose the first benchmark for evaluating the omnibus rationality of LLMs, covering a wide range of domains and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental results, and analysis that illuminates where LLMs converge and diverge from idealized human rationality. We believe the benchmark can serve as a foundational tool for both developers and users of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸ºæ—¶çš„åˆç†æ€§è¯„ä¼°æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªç”¨äºè¡¡é‡æ¨¡å‹ç»¼åˆåˆç†æ€§ (Omnibus Rationality) çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æ¶µç›–äº†å¹¿æ³›çš„é¢†åŸŸï¼Œç³»ç»Ÿè¯„ä¼°äº†æ¨¡å‹åœ¨æ€ç»´å±‚é¢çš„ç†è®ºåˆç†æ€§ (Theoretical Rationality) ä»¥åŠåœ¨è¡ŒåŠ¨å±‚é¢çš„å®è·µåˆç†æ€§ (Practical Rationality)ã€‚ç ”ç©¶å›¢é˜Ÿé…å¥—æä¾›äº†ä¸€å¥—æ˜“äºä½¿ç”¨çš„å·¥å…·åŒ…ï¼Œå¹¶åŸºäºå®éªŒåˆ†æäº† LLMs ä¸ç†æƒ³åŒ–äººç±»åˆç†æ€§ä¹‹é—´çš„è¶‹åŒä¸åç¦»ç‰¹å¾ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„å†³ç­–é€»è¾‘ï¼Œä¸ºå¼€å‘è€…å’Œç”¨æˆ·è¯„ä¼° LLMs è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½ (AGI) çš„è¿›å±•æä¾›äº†å…³é”®çš„åŸºç¡€å·¥å…·ã€‚è¿™ä¸€å·¥ä½œæœ‰åŠ©äºæ·±å…¥ç†è§£ LLMs åœ¨å¤æ‚ä»»åŠ¡ä¸­æ˜¯å¦å…·å¤‡ç±»ä¼¼äººç±»çš„ç†æ€§æ€ç»´ä¸è¡ŒåŠ¨èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14546v1",
      "published_date": "2025-09-18 02:23:56 UTC",
      "updated_date": "2025-09-18 02:23:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:32:13.295046+00:00"
    },
    {
      "arxiv_id": "2509.14543v1",
      "title": "Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors",
      "title_zh": "æ‰ä½æˆ‘ï¼Œå¦‚æœä½ èƒ½ï¼Ÿå°šæœªæˆåŠŸï¼šå¤§è¯­è¨€æ¨¡å‹ä»éš¾ä»¥æ¨¡ä»¿æ™®é€šä½œè€…çš„éšæ€§å†™ä½œé£æ ¼",
      "authors": [
        "Zhengxiang Wang",
        "Nafis Irtiza Tripto",
        "Solha Park",
        "Zhenzhen Li",
        "Jiawei Zhou"
      ],
      "abstract": "As large language models (LLMs) become increasingly integrated into personal writing tools, a critical question arises: can LLMs faithfully imitate an individual's writing style from just a few examples? Personal style is often subtle and implicit, making it difficult to specify through prompts yet essential for user-aligned generation. This work presents a comprehensive evaluation of state-of-the-art LLMs' ability to mimic personal writing styles via in-context learning from a small number of user-authored samples. We introduce an ensemble of complementary metrics-including authorship attribution, authorship verification, style matching, and AI detection-to robustly assess style imitation. Our evaluation spans over 40000 generations per model across domains such as news, email, forums, and blogs, covering writing samples from more than 400 real-world authors. Results show that while LLMs can approximate user styles in structured formats like news and email, they struggle with nuanced, informal writing in blogs and forums. Further analysis on various prompting strategies such as number of demonstrations reveal key limitations in effective personalization. Our findings highlight a fundamental gap in personalized LLM adaptation and the need for improved techniques to support implicit, style-consistent generation. To aid future research and for reproducibility, we open-source our data and code.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)é€šè¿‡å°‘é‡ç¤ºä¾‹æ¨¡æ‹Ÿä¸ªäººä½œè€…éšå¼å†™ä½œé£æ ¼çš„èƒ½åŠ›ï¼Œå¹¶é’ˆå¯¹400å¤šåçœŸå®ä½œè€…åœ¨æ–°é—»ã€ç”µå­é‚®ä»¶ã€è®ºå›å’Œåšå®¢å››ä¸ªé¢†åŸŸçš„æ ·æœ¬è¿›è¡Œäº†å¹¿æ³›çš„ä¸Šä¸‹æ–‡å­¦ä¹ (In-context Learning)è¯„ä¼°ã€‚è¯„ä¼°è¿‡ç¨‹é‡‡ç”¨äº†ä¸€å¥—åŒ…å«ä½œè€…èº«ä»½å½’å±(Authorship Attribution)ã€èº«ä»½éªŒè¯(Authorship Verification)ã€é£æ ¼åŒ¹é…(Style Matching)å’ŒAIæ£€æµ‹(AI Detection)çš„ç»¼åˆæŒ‡æ ‡ä½“ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶LLMsèƒ½å¤Ÿè¾ƒå¥½åœ°æ¨¡æ‹Ÿæ–°é—»å’Œé‚®ä»¶ç­‰ç»“æ„åŒ–æ ¼å¼çš„é£æ ¼ï¼Œä½†åœ¨å¤„ç†åšå®¢å’Œè®ºå›ç­‰éæ­£å¼ä¸”å…·æœ‰ç»†å¾®å·®åˆ«çš„å†™ä½œæ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹ä¸åŒæç¤ºç­–ç•¥(Prompting Strategies)çš„åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹åœ¨å®ç°æœ‰æ•ˆä¸ªæ€§åŒ–æ–¹é¢çš„æ ¸å¿ƒå±€é™ã€‚è¯¥é¡¹å·¥ä½œå¼ºè°ƒäº†ä¸ªæ€§åŒ–LLMé€‚é…ä¸­å­˜åœ¨çš„æ ¹æœ¬å·®è·ï¼ŒæŒ‡å‡ºå½“å‰ä»éœ€è¦æ›´å…ˆè¿›çš„æŠ€æœ¯æ¥æ”¯æŒéšå¼çš„ã€é£æ ¼ä¸€è‡´çš„å†…å®¹ç”Ÿæˆã€‚ä¸ºäº†ä¿ƒè¿›åç»­ç ”ç©¶ï¼Œä½œè€…å·²å°†è¯¥å®éªŒçš„æ•°æ®å’Œä»£ç å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 (Findings)",
      "pdf_url": "https://arxiv.org/pdf/2509.14543v1",
      "published_date": "2025-09-18 02:18:49 UTC",
      "updated_date": "2025-09-18 02:18:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:32:02.491891+00:00"
    },
    {
      "arxiv_id": "2509.14537v1",
      "title": "ClearFairy: Capturing Creative Workflows through Decision Structuring, In-Situ Questioning, and Rationale Inference",
      "title_zh": "ClearFairyï¼šé€šè¿‡å†³ç­–ç»“æ„åŒ–ã€åŸä½æé—®å’Œç†ç”±æ¨æ–­æ•æ‰åˆ›æ„å·¥ä½œæµ",
      "authors": [
        "Kihoon Son",
        "DaEun Choi",
        "Tae Soo Kim",
        "Young-Ho Kim",
        "Sangdoo Yun",
        "Juho Kim"
      ],
      "abstract": "Capturing professionals' decision-making in creative workflows is essential for reflection, collaboration, and knowledge sharing, yet existing methods often leave rationales incomplete and implicit decisions hidden. To address this, we present CLEAR framework that structures reasoning into cognitive decision steps-linked units of actions, artifacts, and self-explanations that make decisions traceable. Building on this framework, we introduce ClearFairy, a think-aloud AI assistant for UI design that detects weak explanations, asks lightweight clarifying questions, and infers missing rationales to ease the knowledge-sharing burden. In a study with twelve creative professionals, 85% of ClearFairy's inferred rationales were accepted, increasing strong explanations from 14% to over 83% of decision steps without adding cognitive demand. The captured steps also enhanced generative AI agents in Figma, yielding next-action predictions better aligned with professionals and producing more coherent design outcomes. For future research on human knowledge-grounded creative AI agents, we release a dataset of captured 417 decision steps.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CLEAR æ¡†æ¶ï¼Œé€šè¿‡å°†æ¨ç†è¿‡ç¨‹ç»“æ„åŒ–ä¸ºåŒ…å«åŠ¨ä½œã€äº§å‡ºç‰©å’Œè‡ªæˆ‘è§£é‡Šçš„è®¤çŸ¥å†³ç­–æ­¥éª¤ (cognitive decision steps)ï¼Œè§£å†³äº†åˆ›æ„å·¥ä½œæµä¸­å†³ç­–è®°å½•ä¸å®Œæ•´å’Œéšå«å†³ç­–è¢«å¿½ç•¥çš„é—®é¢˜ã€‚åŸºäºæ­¤æ¡†æ¶å¼€å‘çš„ ClearFairy æ˜¯ä¸€æ¬¾é’ˆå¯¹ UI è®¾è®¡çš„ think-aloud AI assistantï¼Œå®ƒèƒ½å®æ—¶æ£€æµ‹è§£é‡ŠåŠ›ä¸è¶³çš„æƒ…å†µï¼Œé€šè¿‡è½»é‡åŒ–æé—®å’Œæ¨ç†ç¼ºå¤±çš„åˆç†æ€§ (rationales) æ¥é™ä½çŸ¥è¯†å…±äº«è´Ÿæ‹…ã€‚é’ˆå¯¹ 12 ä½ä¸“ä¸šäººå£«çš„å®éªŒè¡¨æ˜ï¼ŒClearFairy æ¨ç†çš„åˆç†æ€§æ¥å—ç‡è¾¾ 85%ï¼Œåœ¨ä¸å¢åŠ è®¤çŸ¥è´Ÿè·çš„æƒ…å†µä¸‹ï¼Œä½¿å¼ºæœ‰åŠ›è§£é‡Šçš„æ¯”ä¾‹ä» 14% æ˜¾è‘—æå‡è‡³ 83% ä»¥ä¸Šã€‚è¿™äº›æ•è·çš„æ­¥éª¤è¿›ä¸€æ­¥å¢å¼ºäº† Figma ä¸­çš„ç”Ÿæˆå¼ AI (generative AI) æ™ºèƒ½ä½“ï¼Œä½¿å…¶åœ¨åŠ¨ä½œé¢„æµ‹å’Œè®¾è®¡è¿è´¯æ€§ä¸Šä¸ä¸“ä¸šäººå£«é«˜åº¦å¥‘åˆã€‚æœ€åï¼Œè¯¥ç ”ç©¶å‘å¸ƒäº†åŒ…å« 417 ä¸ªå†³ç­–æ­¥éª¤çš„æ•°æ®é›†ï¼Œä¸ºæœªæ¥å¼€å‘åŸºäºäººç±»çŸ¥è¯†çš„åˆ›æ„ AI æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14537v1",
      "published_date": "2025-09-18 02:11:34 UTC",
      "updated_date": "2025-09-18 02:11:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:32:17.589372+00:00"
    },
    {
      "arxiv_id": "2509.14532v1",
      "title": "Leveraging Artificial Intelligence as a Strategic Growth Catalyst for Small and Medium-sized Enterprises",
      "title_zh": "åˆ©ç”¨äººå·¥æ™ºèƒ½ä½œä¸ºä¸­å°ä¼ä¸šæˆ˜ç•¥å¢é•¿çš„å‚¬åŒ–å‰‚",
      "authors": [
        "Oluwatosin Agbaakin"
      ],
      "abstract": "Artificial Intelligence (AI) has transitioned from a futuristic concept reserved for large corporations to a present-day, accessible, and essential growth lever for Small and Medium-sized Enterprises (SMEs). For entrepreneurs and business leaders, strategic AI adoption is no longer an option but an imperative for competitiveness, operational efficiency, and long-term survival. This report provides a comprehensive framework for SME leaders to navigate this technological shift, offering the foundational knowledge, business case, practical applications, and strategic guidance necessary to harness the power of AI. The quantitative evidence supporting AI adoption is compelling; 91% of SMEs using AI report that it directly boosts their revenue. Beyond top-line growth, AI drives profound operational efficiencies, with studies showing it can reduce operational costs by up to 30% and save businesses more than 20 hours of valuable time each month. This transformation is occurring within the context of a seismic economic shift; the global AI market is projected to surge from $233.46 Billion in 2024 to an astonishing $1.77 Trillion by 2032. This paper demystifies the core concepts of AI, presents a business case based on market data, details practical applications, and lays out a phased, actionable adoption strategy.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†äººå·¥æ™ºèƒ½ (Artificial Intelligence) å¦‚ä½•ä½œä¸ºä¸­å°ä¼ä¸š (SMEs) çš„æˆ˜ç•¥å¢é•¿å‚¬åŒ–å‰‚ï¼Œå¼ºè°ƒåœ¨å½“å‰ç«äº‰ç¯å¢ƒä¸‹ï¼ŒAI çš„æˆ˜ç•¥æ€§é‡‡ç”¨å·²æˆä¸ºä¼ä¸šæå‡æ•ˆç‡å’Œé•¿æœŸç”Ÿå­˜çš„å¿…ç„¶é€‰æ‹©ã€‚æŠ¥å‘Šä¸º SME é¢†å¯¼è€…æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æŒ‡å¯¼æ¡†æ¶ï¼Œæ¶µç›–äº†æ ¸å¿ƒæ¦‚å¿µã€Business Caseã€å®é™…åº”ç”¨åœºæ™¯ä»¥åŠåˆ†é˜¶æ®µçš„å¯æ“ä½œæ€§é‡‡ç”¨ç­–ç•¥ã€‚ç ”ç©¶é€šè¿‡å®šé‡æ•°æ®è¯æ˜äº† AI çš„å·¨å¤§ä»·å€¼ï¼š91% ä½¿ç”¨ AI çš„ä¸­å°ä¼ä¸šæŠ¥å‘Šäº†æ”¶å…¥å¢é•¿ï¼Œä¸”è¯¥æŠ€æœ¯èƒ½é™ä½é«˜è¾¾ 30% çš„è¿è¥æˆæœ¬å¹¶æ¯æœˆèŠ‚çœè¶…è¿‡ 20 å°æ—¶çš„å·¥æ—¶ã€‚éšç€å…¨çƒ AI å¸‚åœºé¢„è®¡åœ¨ 2032 å¹´è¾¾åˆ° 1.77 ä¸‡äº¿ç¾å…ƒï¼Œæœ¬æ–‡æ—¨åœ¨é€šè¿‡ç³»ç»ŸåŒ–çš„è·¯å¾„ï¼Œå¸®åŠ©ä¸­å°ä¼ä¸šè·¨è¶ŠæŠ€æœ¯é—¨æ§›ï¼Œåˆ©ç”¨ AI é©±åŠ¨ä¸šåŠ¡çš„å®è´¨æ€§è½¬å‹ä¸æŒç»­å¢é•¿ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "econ.GN"
      ],
      "primary_category": "cs.CY",
      "comment": "14 pages, 2 figures. A review and strategic framework for AI adoption in SMEs",
      "pdf_url": "https://arxiv.org/pdf/2509.14532v1",
      "published_date": "2025-09-18 01:56:04 UTC",
      "updated_date": "2025-09-18 01:56:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:32:03.284646+00:00"
    },
    {
      "arxiv_id": "2510.09620v1",
      "title": "Toward a Unified Security Framework for AI Agents: Trust, Risk, and Liability",
      "title_zh": "è¿ˆå‘ AI æ™ºèƒ½ä½“ç»Ÿä¸€å®‰å…¨æ¡†æ¶ï¼šä¿¡ä»»ã€é£é™©ä¸è´£ä»»",
      "authors": [
        "Jiayun Mo",
        "Xin Kang",
        "Tieyan Li",
        "Zhongding Lei"
      ],
      "abstract": "The excitement brought by the development of AI agents came alongside arising problems. These concerns centered around users' trust issues towards AIs, the risks involved, and the difficulty of attributing responsibilities and liabilities. Current solutions only attempt to target each problem separately without acknowledging their inter-influential nature. The Trust, Risk and Liability (TRL) framework proposed in this paper, however, ties together the interdependent relationships of trust, risk, and liability to provide a systematic method of building and enhancing trust, analyzing and mitigating risks, and allocating and attributing liabilities. It can be applied to analyze any application scenarios of AI agents and suggest appropriate measures fitting to the context. The implications of the TRL framework lie in its potential societal impacts, economic impacts, ethical impacts, and more. It is expected to bring remarkable values to addressing potential challenges and promoting trustworthy, risk-free, and responsible usage of AI in 6G networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸º TRL (Trust, Risk, and Liability) çš„ç»Ÿä¸€å®‰å…¨æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹ AI Agents åœ¨å‘å±•è¿‡ç¨‹ä¸­å‡ºç°çš„ä¿¡ä»»ã€é£é™©åŠæ³•å¾‹è´£ä»»å½’å±é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰è§£å†³æ–¹æ¡ˆä»…å­¤ç«‹å¤„ç†è¿™äº›é—®é¢˜è€Œå¿½è§†å…¶ç›¸äº’å½±å“çš„ç°çŠ¶ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿæ€§åœ°å°† Trustã€Risk å’Œ Liability çš„ç›¸äº’ä¾èµ–å…³ç³»æ•´åˆåœ¨ä¸€èµ·ã€‚TRL æ¡†æ¶æä¾›äº†ä¸€å¥—æ ‡å‡†åŒ–çš„æ–¹æ³•ï¼Œç”¨äºå»ºç«‹å’Œå¢å¼ºç”¨æˆ· Trustï¼Œåˆ†æå¹¶å‡è½»æ½œåœ¨ Riskï¼Œä»¥åŠæ˜ç¡®åˆ†é…å’Œè¿½æº¯ Liabilityã€‚è¯¥æ¡†æ¶å…·å¤‡æ™®é€‚æ€§ï¼Œå¯åº”ç”¨äº AI Agents çš„å„ç±»åº”ç”¨åœºæ™¯å¹¶æä¾›é’ˆå¯¹æ€§çš„æ”¹è¿›å»ºè®®ã€‚é€šè¿‡å¯¹ç¤¾ä¼šã€ç»æµåŠä¼¦ç†å½±å“çš„æ·±å…¥æ¢è®¨ï¼Œè¯¥ç ”ç©¶ä¸ºæ¨åŠ¨ 6G Networks ç¯å¢ƒä¸‹ AI çš„å¯é ã€å®‰å…¨å’Œè´Ÿè´£ä»»åº”ç”¨å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.09620v1",
      "published_date": "2025-09-18 01:55:03 UTC",
      "updated_date": "2025-09-18 01:55:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:32:00.893888+00:00"
    },
    {
      "arxiv_id": "2509.18181v1",
      "title": "Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling",
      "title_zh": "SAPAï¼ˆæ€åº¦ç»¼åˆä¸è¡Œä¸ºé¢„æµ‹ï¼‰ï¼šè¡Œä¸ºç†è®ºå¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹ç½‘çº¦è½¦å‡ºè¡Œæ–¹å¼é€‰æ‹©å»ºæ¨¡",
      "authors": [
        "Mustafa Sameen",
        "Xiaojian Zhang",
        "Xilei Zhao"
      ],
      "abstract": "Accurate modeling of ridesourcing mode choices is essential for designing and implementing effective traffic management policies for reducing congestion, improving mobility, and allocating resources more efficiently. Existing models for predicting ridesourcing mode choices often suffer from limited predictive accuracy due to their inability to capture key psychological factors, and are further challenged by severe class imbalance, as ridesourcing trips comprise only a small fraction of individuals' daily travel. To address these limitations, this paper introduces the Synthesizing Attitudes, Predicting Actions (SAPA) framework, a hierarchical approach that uses Large Language Models (LLMs) to synthesize theory-grounded latent attitudes to predict ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler personas from raw travel survey data and then trains a propensity-score model on demographic and behavioral features, enriched by those personas, to produce an individual-level score. Next, the LLM assigns quantitative scores to theory-driven latent variables (e.g., time and cost sensitivity), and a final classifier integrates the propensity score, latent-variable scores (with their interaction terms), and observable trip attributes to predict ridesourcing mode choice. Experiments on a large-scale, multi-year travel survey show that SAPA significantly outperforms state-of-the-art baselines, improving ridesourcing choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set. This study provides a powerful tool for accurately predicting ridesourcing mode choices, and provides a methodology that is readily transferable to various applications.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æå‡ºäº†SAPAï¼ˆSynthesizing Attitudes, Predicting Actionsï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè¡Œä¸ºç†è®ºä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å±‚æ¬¡åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æå‡ç½‘çº¦è½¦ï¼ˆRidesourcingï¼‰å‡ºè¡Œæ¨¡å¼é€‰æ‹©å»ºæ¨¡çš„é¢„æµ‹ç²¾åº¦ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹éš¾ä»¥æ•æ‰å…³é”®å¿ƒç†å› ç´ ä»¥åŠæ•°æ®ä¸­ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡ï¼ˆClass Imbalanceï¼‰é—®é¢˜ï¼ŒSAPAé¦–å…ˆåˆ©ç”¨LLMä»åŸå§‹æ—…è¡Œè°ƒæŸ¥æ•°æ®ä¸­ç”Ÿæˆå®šæ€§çš„æ—…è¡Œè€…ç”»åƒï¼ˆPersonasï¼‰ï¼Œå¹¶ç»“åˆäººå£ç»Ÿè®¡å­¦ç‰¹å¾è®­ç»ƒå€¾å‘è¯„åˆ†ï¼ˆPropensity-scoreï¼‰æ¨¡å‹ã€‚éšåï¼ŒLLMæ ¹æ®è¡Œä¸ºç†è®ºä¸ºæ—¶é—´å’Œæˆæœ¬æ•æ„Ÿæ€§ç­‰æ½œå˜é‡åˆ†é…å®šé‡åˆ†æ•°ï¼Œæœ€ç»ˆåˆ†ç±»å™¨æ•´åˆå€¾å‘è¯„åˆ†ã€æ½œå˜é‡è¯„åˆ†åŠæ—…è¡Œå±æ€§å®Œæˆé¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAPAåœ¨ PR-AUC æŒ‡æ ‡ä¸Šæ¯”åŸºçº¿æ¨¡å‹æé«˜äº† 75.9%ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†ç½‘çº¦è½¦å‡ºè¡Œé€‰æ‹©çš„é¢„æµ‹æ•ˆæœã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºäº¤é€šç®¡ç†å’Œèµ„æºé…ç½®æä¾›äº†ç²¾ç¡®çš„å»ºæ¨¡å·¥å…·ï¼Œä¹Ÿå±•ç¤ºäº†å°†LLMsåº”ç”¨äºè¡Œä¸ºå»ºæ¨¡çš„å¯è¿ç§»æ€§æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18181v1",
      "published_date": "2025-09-18 01:52:27 UTC",
      "updated_date": "2025-09-18 01:52:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:32:27.992062+00:00"
    },
    {
      "arxiv_id": "2509.18180v3",
      "title": "Large Language Models in Operations Research: Methods, Applications, and Challenges",
      "title_zh": "è¿ç­¹å­¦ä¸­çš„å¤§è¯­è¨€æ¨¡å‹ï¼šæ–¹æ³•ã€åº”ç”¨ä¸æŒ‘æˆ˜",
      "authors": [
        "Yang Wang",
        "Kai Li"
      ],
      "abstract": "Operations research (OR) is a core methodology that supports complex system decision-making, with broad applications in transportation, supply chain management, and production scheduling. However, traditional approaches that rely on expert-driven modeling and manual parameter tuning often struggle with large-scale, dynamic, and multi-constraint problems, limiting scalability and real-time applicability. Large language models (LLMs), with capabilities in semantic understanding, structured generation, and reasoning control, offer new opportunities to overcome these challenges. They can translate natural language problem descriptions into mathematical models or executable code, generate heuristics, evolve algorithms, and directly solve optimization tasks. This shifts the paradigm from human-driven processes to intelligent human-AI collaboration. This paper systematically reviews progress in applying LLMs to OR, categorizing existing methods into three pathways: automatic modeling, auxiliary optimization, and direct solving. It also examines evaluation benchmarks and domain-specific applications, and highlights key challenges, including unstable semantic-to-structure mapping, fragmented research, limited generalization and interpretability, insufficient evaluation systems, and barriers to industrial deployment. Finally, it outlines potential research directions. Overall, LLMs demonstrate strong potential to reshape the OR paradigm by enhancing interpretability, adaptability, and scalability, paving the way for next-generation intelligent optimization systems.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¿ç­¹å­¦(Operations Research, OR)é¢†åŸŸçš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿä¸“å®¶é©±åŠ¨å»ºæ¨¡åœ¨å¤„ç†å¤§è§„æ¨¡ã€åŠ¨æ€åŠå¤šçº¦æŸé—®é¢˜æ—¶çš„å±€é™æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒLLMså‡­å€Ÿå…¶å“è¶Šçš„è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œèƒ½å°†è‡ªç„¶è¯­è¨€æè¿°è½¬åŒ–ä¸ºæ•°å­¦æ¨¡å‹æˆ–ä»£ç ï¼Œå¹¶ç”Ÿæˆå¯å‘å¼ç®—æ³•ï¼Œæ¨åŠ¨äº†ä»äººåŠ›é©±åŠ¨åˆ°äººæœºåä½œçš„èŒƒå¼è½¬å˜ã€‚æ–‡ç« å°†ç°æœ‰ç ”ç©¶å½’çº³ä¸ºè‡ªåŠ¨å»ºæ¨¡(automatic modeling)ã€è¾…åŠ©ä¼˜åŒ–(auxiliary optimization)å’Œç›´æ¥æ±‚è§£(direct solving)ä¸‰å¤§è·¯å¾„ï¼Œå¹¶è¯¦ç»†è®¨è®ºäº†ç›¸å…³çš„è¯„ä¼°åŸºå‡†ä¸é¢†åŸŸåº”ç”¨ã€‚ç ”ç©¶è¿˜æ·±å…¥åˆ†æäº†è¯¥é¢†åŸŸé¢ä¸´çš„è¯­ä¹‰æ˜ å°„ä¸ç¨³å®šã€æ³›åŒ–æ€§ä¸è¶³åŠå·¥ä¸šéƒ¨ç½²å›°éš¾ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚æ€»ä½“è€Œè¨€ï¼ŒLLMså±•ç°å‡ºå¢å¼ºORç³»ç»Ÿå¯è§£é‡Šæ€§ä¸å¯æ‰©å±•æ€§çš„å¼ºå¤§æ½œåŠ›ï¼Œä¸ºæ„å»ºä¸‹ä¸€ä»£æ™ºèƒ½ä¼˜åŒ–ç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18180v3",
      "published_date": "2025-09-18 01:52:19 UTC",
      "updated_date": "2025-10-14 02:36:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:32:35.399220+00:00"
    },
    {
      "arxiv_id": "2509.18179v1",
      "title": "The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes",
      "title_zh": "â€œå…ˆæè¿°å†ç”Ÿæˆâ€ç“¶é¢ˆï¼šVLM æè¿°å¦‚ä½•å½±å“å›¾åƒç”Ÿæˆç»“æœ",
      "authors": [
        "Sai Varun Kodathala",
        "Rakesh Vunnam"
      ],
      "abstract": "With the increasing integration of multimodal AI systems in creative workflows, understanding information loss in vision-language-vision pipelines has become important for evaluating system limitations. However, the degradation that occurs when visual content passes through textual intermediation remains poorly quantified. In this work, we provide empirical analysis of the describe-then-generate bottleneck, where natural language serves as an intermediate representation for visual information. We generated 150 image pairs through the describe-then-generate pipeline and applied existing metrics (LPIPS, SSIM, and color distance) to measure information preservation across perceptual, structural, and chromatic dimensions. Our evaluation reveals that 99.3% of samples exhibit substantial perceptual degradation and 91.5% demonstrate significant structural information loss, providing empirical evidence that the describe-then-generate bottleneck represents a measurable and consistent limitation in contemporary multimodal systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤šæ¨¡æ€AIç³»ç»Ÿåœ¨â€œæè¿°åç”Ÿæˆâ€(describe-then-generate)å·¥ä½œæµä¸­çš„ä¿¡æ¯æŸå¤±ï¼Œå³è§†è§‰å†…å®¹é€šè¿‡æ–‡æœ¬ä¸­é—´è¡¨ç¤ºæ—¶äº§ç”Ÿçš„é€€åŒ–ç°è±¡ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ç”Ÿæˆ150ç»„å›¾åƒå¯¹ï¼Œå¹¶åº”ç”¨LPIPSã€SSIMå’Œé¢œè‰²è·ç¦»(color distance)ç­‰æŒ‡æ ‡ï¼Œä»æ„ŸçŸ¥ã€ç»“æ„å’Œè‰²å½©ç»´åº¦é‡åŒ–äº†è§†è§‰ä¿¡æ¯çš„ä¿ç•™ç¨‹åº¦ã€‚å®è¯åˆ†æè¡¨æ˜ï¼Œ99.3%çš„æ ·æœ¬è¡¨ç°å‡ºä¸¥é‡çš„æ„ŸçŸ¥é€€åŒ–(perceptual degradation)ï¼Œè€Œ91.5%çš„æ ·æœ¬å­˜åœ¨æ˜¾è‘—çš„ç»“æ„ä¿¡æ¯æŸå¤±ã€‚è¿™äº›å‘ç°æœ‰åŠ›åœ°è¯æ˜äº†â€œæè¿°åç”Ÿæˆç“¶é¢ˆâ€(describe-then-generate bottleneck)æ˜¯å½“ä»£å¤šæ¨¡æ€ç³»ç»Ÿé¢ä¸´çš„ä¸€ä¸ªå¯è¡¡é‡ä¸”æ™®éå­˜åœ¨çš„å±€é™æ€§ï¼Œæ­ç¤ºäº†æ–‡æœ¬ä¸­ä»‹å¯¹è§†è§‰é‡æ„è´¨é‡çš„ç³»ç»Ÿæ€§å½±å“ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 7 Figures",
      "pdf_url": "https://arxiv.org/pdf/2509.18179v1",
      "published_date": "2025-09-18 01:48:51 UTC",
      "updated_date": "2025-09-18 01:48:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:32:38.488887+00:00"
    },
    {
      "arxiv_id": "2509.14526v1",
      "title": "Delta Knowledge Distillation for Large Language Models",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹çš„ Delta çŸ¥è¯†è’¸é¦",
      "authors": [
        "Yihan Cao",
        "Yanbin Kang",
        "Zhengming Xing",
        "Ruijie Jiang"
      ],
      "abstract": "Knowledge distillation (KD) is a widely adopted approach for compressing large neural networks by transferring knowledge from a large teacher model to a smaller student model. In the context of large language models, token level KD, typically minimizing the KL divergence between student output distribution and teacher output distribution, has shown strong empirical performance. However, prior work assumes student output distribution and teacher output distribution share the same optimal representation space, a premise that may not hold in many cases. To solve this problem, we propose Delta Knowledge Distillation (Delta-KD), a novel extension of token level KD that encourages the student to approximate an optimal representation space by explicitly preserving the distributional shift Delta introduced during the teacher's supervised finetuning (SFT). Empirical results on ROUGE metrics demonstrate that Delta KD substantially improves student performance while preserving more of the teacher's knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰å‹ç¼©ä¸­çš„çŸ¥è¯†è’¸é¦ï¼ˆKnowledge distillation, KDï¼‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä»¤ç‰Œçº§ï¼ˆtoken levelï¼‰è’¸é¦æ–¹æ³•çš„æ”¹è¿›ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–æœ€å°åŒ–KLæ•£åº¦ï¼ˆKL divergenceï¼‰ï¼Œä½†å…¶å‡è®¾å­¦ç”Ÿä¸æ•™å¸ˆæ¨¡å‹å…±äº«ç›¸åŒçš„æœ€ä¼˜è¡¨ç¤ºç©ºé—´ï¼Œè¿™åœ¨å®é™…ä¸­å¾€å¾€å¹¶ä¸æˆç«‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†DeltaçŸ¥è¯†è’¸é¦ï¼ˆDelta Knowledge Distillation, Delta-KDï¼‰ï¼Œé€šè¿‡æ˜¾å¼ä¿ç•™æ•™å¸ˆæ¨¡å‹åœ¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆSupervised finetuning, SFTï¼‰è¿‡ç¨‹ä¸­å¼•å…¥çš„åˆ†å¸ƒåç§»ï¼ˆdistributional shift Deltaï¼‰ï¼Œå¼•å¯¼å­¦ç”Ÿæ¨¡å‹é€¼è¿‘æœ€ä¼˜è¡¨ç¤ºç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDelta-KDåœ¨ROUGEæŒ‡æ ‡ä¸Šæ˜¾è‘—æå‡äº†å­¦ç”Ÿæ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶èƒ½æ›´æœ‰æ•ˆåœ°è¿ç§»æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚è¿™ä¸€æ–¹æ³•ä¸ºè§£å†³æ¨¡å‹é—´è¡¨ç¤ºç©ºé—´ä¸ä¸€è‡´å¸¦æ¥çš„è’¸é¦æŸå¤±æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.14526v1",
      "published_date": "2025-09-18 01:42:24 UTC",
      "updated_date": "2025-09-18 01:42:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:32:41.788142+00:00"
    },
    {
      "arxiv_id": "2509.14519v1",
      "title": "BEACON: Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning",
      "title_zh": "BEACONï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹åµŒå…¥ä¸æ·±åº¦å­¦ä¹ çš„æ¶æ„è½¯ä»¶è¡Œä¸ºåˆ†ç±»",
      "authors": [
        "Wadduwage Shanika Perera",
        "Haodi Jiang"
      ],
      "abstract": "Malware is becoming increasingly complex and widespread, making it essential to develop more effective and timely detection methods. Traditional static analysis often fails to defend against modern threats that employ code obfuscation, polymorphism, and other evasion techniques. In contrast, behavioral malware detection, which monitors runtime activities, provides a more reliable and context-aware solution. In this work, we propose BEACON, a novel deep learning framework that leverages large language models (LLMs) to generate dense, contextual embeddings from raw sandbox-generated behavior reports. These embeddings capture semantic and structural patterns of each sample and are processed by a one-dimensional convolutional neural network (1D CNN) for multi-class malware classification. Evaluated on the Avast-CTU Public CAPE Dataset, our framework consistently outperforms existing methods, highlighting the effectiveness of LLM-based behavioral embeddings and the overall design of BEACON for robust malware classification.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿé™æ€åˆ†æéš¾ä»¥åº”å¯¹ä»£ç æ··æ·†å’Œå¤šæ€æ€§æ¶æ„è½¯ä»¶çš„é—®é¢˜ï¼Œæå‡ºäº† BEACON æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚BEACON é‡‡ç”¨è¡Œä¸ºæ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) ä»æ²™ç®±ç”Ÿæˆçš„åŸå§‹è¡Œä¸ºæŠ¥å‘Šä¸­æå–å¯†é›†çš„ä¸Šä¸‹æ–‡åµŒå…¥ (contextual embeddings)ï¼Œä»¥æ•æ‰æ ·æœ¬çš„è¯­ä¹‰å’Œç»“æ„æ¨¡å¼ã€‚è¿™äº›åµŒå…¥éšåè¢«è¾“å…¥åˆ°ä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œ (1D CNN) ä¸­ï¼Œç”¨äºæ‰§è¡Œå¤šç±»åˆ«æ¶æ„è½¯ä»¶åˆ†ç±»ä»»åŠ¡ã€‚åœ¨ Avast-CTU Public CAPE Dataset ä¸Šçš„å®éªŒè¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒBEACON çš„æ€§èƒ½è¡¨ç°ä¸€è‡´ä¼˜äºç°æœ‰çš„æ£€æµ‹æ–¹æ³•ã€‚è¯¥å·¥ä½œçªå‡ºäº†åŸºäº LLM çš„è¡Œä¸ºåµŒå…¥æŠ€æœ¯åœ¨å¢å¼ºæ¶æ„è½¯ä»¶åˆ†ç±»é²æ£’æ€§ä»¥åŠæå‡ç³»ç»Ÿæ•´ä½“è®¾è®¡æœ‰æ•ˆæ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14519v1",
      "published_date": "2025-09-18 01:24:12 UTC",
      "updated_date": "2025-09-18 01:24:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:32:46.803516+00:00"
    },
    {
      "arxiv_id": "2509.15250v2",
      "title": "Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning",
      "title_zh": "å°‘èµ°å°‘è¯»ï¼šé€šè¿‡å…å¾®è°ƒå¤šæ¨¡æ€æ ‡è®°å‰ªææå‡è§†è§‰è¯­è¨€å¯¼èˆªæ•ˆç‡",
      "authors": [
        "Wenda Qin",
        "Andrea Burns",
        "Bryan A. Plummer",
        "Margrit Betke"
      ],
      "abstract": "Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning. To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes. Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€å¯¼èˆª (Vision-and-Language Navigation, VLN) ä»»åŠ¡ä¸­å¤§æ¨¡å‹è®¡ç®—æˆæœ¬é«˜ä»¥åŠç°æœ‰ä»¤ç‰Œå‰ªæ (Token Pruning) æŠ€æœ¯å› ä¿¡æ¯æŸå¤±å¯¼è‡´è·¯å¾„å¢é•¿çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Navigation-Aware Pruning (NAP) çš„å…å¾®è°ƒå¤šæ¨¡æ€ä»¤ç‰Œå‰ªææ–¹æ³•ã€‚NAP åˆ©ç”¨å¯¼èˆªç‰¹æœ‰çš„å±æ€§ï¼Œé€šè¿‡å°†ä»¤ç‰Œé¢„è¿‡æ»¤ä¸ºå‰æ™¯å’ŒèƒŒæ™¯æ¥ç®€åŒ–å‰ªæè¿‡ç¨‹ï¼Œä¾‹å¦‚æ ¹æ®æ™ºèƒ½ä½“æ˜¯å¦å¯ä»¥å‘è¯¥æ–¹å‘ç§»åŠ¨æ¥è¿‡æ»¤å›¾åƒè§†å›¾ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM) æå–ä¸å¯¼èˆªç›¸å…³çš„æŒ‡ä»¤ä¿¡æ¯ï¼Œå¹¶å°†å‰ªæé‡ç‚¹é›†ä¸­åœ¨èƒŒæ™¯ä»¤ç‰Œä¸Šï¼Œä»¥æœ€å¤§é™åº¦åœ°å‡å°‘æ ¸å¿ƒä¿¡æ¯çš„æŸå¤±ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–è·¯å¾„æ•ˆç‡å¹¶é˜²æ­¢å¯¼èˆªé•¿åº¦å¢åŠ ï¼ŒNAP é€šè¿‡ç§»é™¤ä½é‡è¦æ€§çš„å¯¼èˆªèŠ‚ç‚¹æ¥æŠ‘åˆ¶ä¸å¿…è¦çš„å›æº¯è¡Œä¸ºã€‚åœ¨æ ‡å‡† VLN åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNAP åœ¨ä¿æŒè¾ƒé«˜æˆåŠŸç‡çš„åŒæ—¶ï¼Œèƒ½å¤ŸèŠ‚çœè¶…è¿‡ 50% çš„ FLOPsï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºå…ˆå‰çš„ç ”ç©¶å·¥ä½œã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to EMNLP 2025. Data and code to be released at https://github.com/wdqin/VLN-NAP",
      "pdf_url": "https://arxiv.org/pdf/2509.15250v2",
      "published_date": "2025-09-18 01:05:37 UTC",
      "updated_date": "2025-09-22 01:18:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:33:06.893510+00:00"
    },
    {
      "arxiv_id": "2509.15249v1",
      "title": "Causal Reasoning Elicits Controllable 3D Scene Generation",
      "title_zh": "å› æœæ¨ç†å¯å‘çš„å¯æ§ä¸‰ç»´åœºæ™¯ç”Ÿæˆ",
      "authors": [
        "Shen Chen",
        "Ruiyu Zhao",
        "Jiale Zhou",
        "Zongkai Wu",
        "Jenq-Neng Hwang",
        "Lei Li"
      ],
      "abstract": "Existing 3D scene generation methods often struggle to model the complex logical dependencies and physical constraints between objects, limiting their ability to adapt to dynamic and realistic environments. We propose CausalStruct, a novel framework that embeds causal reasoning into 3D scene generation. Utilizing large language models (LLMs), We construct causal graphs where nodes represent objects and attributes, while edges encode causal dependencies and physical constraints. CausalStruct iteratively refines the scene layout by enforcing causal order to determine the placement order of objects and applies causal intervention to adjust the spatial configuration according to physics-driven constraints, ensuring consistency with textual descriptions and real-world dynamics. The refined scene causal graph informs subsequent optimization steps, employing a Proportional-Integral-Derivative(PID) controller to iteratively tune object scales and positions. Our method uses text or images to guide object placement and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation Sampling improving shape accuracy and rendering stability. Extensive experiments show that CausalStruct generates 3D scenes with enhanced logical coherence, realistic spatial interactions, and robust adaptability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CausalStructï¼Œä¸€ä¸ªå°†å› æœæ¨ç†(Causal Reasoning)åµŒå…¥ 3D åœºæ™¯ç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å»ºæ¨¡ç‰©ä½“é—´å¤æ‚é€»è¾‘ä¾èµ–å’Œç‰©ç†çº¦æŸæ—¶çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ„å»ºå› æœå›¾(Causal Graphs)ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨ç‰©ä½“åŠå…¶å±æ€§ï¼Œè¾¹åˆ™ç”¨äºç¼–ç å› æœä¾èµ–å’Œç‰©ç†çº¦æŸã€‚CausalStruct é€šè¿‡å¼ºåˆ¶å› æœé¡ºåºæ¥ç¡®å®šç‰©ä½“çš„æ”¾ç½®æ¬¡åºï¼Œå¹¶è¿ç”¨å› æœå¹²é¢„(Causal Intervention)æ ¹æ®ç‰©ç†é©±åŠ¨çš„çº¦æŸè°ƒæ•´ç©ºé—´é…ç½®ï¼Œä»¥ç¡®ä¿ç”Ÿæˆåœºæ™¯ä¸æ–‡æœ¬æè¿°åŠç°å®åŠ¨æ€ä¿æŒä¸€è‡´ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†æ¯”ä¾‹-ç§¯åˆ†-å¾®åˆ†(PID)æ§åˆ¶å™¨å¯¹ç‰©ä½“çš„æ¯”ä¾‹å’Œä½ç½®è¿›è¡Œè¿­ä»£å¾®è°ƒï¼Œå¹¶ç»“åˆ 3D Gaussian Splatting å’Œåˆ†æ•°è’¸é¦é‡‡æ ·(Score Distillation Sampling)æ¥æé«˜å½¢çŠ¶å‡†ç¡®æ€§ä¸æ¸²æŸ“ç¨³å®šæ€§ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜ï¼ŒCausalStruct åœ¨ç”Ÿæˆå…·æœ‰å¢å¼ºé€»è¾‘è¿è´¯æ€§ã€é€¼çœŸç©ºé—´äº¤äº’å’Œå¼ºé²æ£’é€‚åº”æ€§çš„ 3D åœºæ™¯æ–¹é¢è¡¨ç°å“è¶Šã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15249v1",
      "published_date": "2025-09-18 01:03:21 UTC",
      "updated_date": "2025-09-18 01:03:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:33:07.584639+00:00"
    },
    {
      "arxiv_id": "2509.14507v2",
      "title": "DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction",
      "title_zh": "DeKeyNLUï¼šé€šè¿‡ä»»åŠ¡åˆ†è§£ä¸å…³é”®è¯æå–å¢å¼ºè‡ªç„¶è¯­è¨€è½¬ SQL ç”Ÿæˆ",
      "authors": [
        "Jian Chen",
        "Zhenyan Chen",
        "Xuming Hu",
        "Peilin Zhou",
        "Yining Hua",
        "Han Fang",
        "Cissy Hing Yee Choy",
        "Xinmei Ke",
        "Jingfeng Luo",
        "Zixuan Yuan"
      ],
      "abstract": "Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that simplifies database access for non-technical users by converting natural language queries into SQL commands. Recent advancements, particularly those integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) reasoning, have made significant strides in enhancing NL2SQL performance. However, challenges such as inaccurate task decomposition and keyword extraction by LLMs remain major bottlenecks, often leading to errors in SQL generation. While existing datasets aim to mitigate these issues by fine-tuning models, they struggle with over-fragmentation of tasks and lack of domain-specific keyword annotations, limiting their effectiveness. To address these limitations, we present DeKeyNLU, a novel dataset which contains 1,500 meticulously annotated QA pairs aimed at refining task decomposition and enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three distinct modules for user question understanding, entity retrieval, and generation to improve SQL generation accuracy. We benchmarked multiple model configurations within DeKeySQL RAG pipeline. Experimental results demonstrate that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Natural Language to SQL (NL2SQL) ç”Ÿæˆè¿‡ç¨‹ä¸­ç”±äºä»»åŠ¡åˆ†è§£ä¸å‡†ç¡®å’Œå…³é”®è¯æå–ç²¾åº¦ä½å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº†æ–°å‹æ•°æ®é›†DeKeyNLUã€‚è¯¥æ•°æ®é›†åŒ…å«1,500ä¸ªç²¾å¿ƒæ ‡æ³¨çš„é—®ç­”å¯¹ï¼Œæ—¨åœ¨ä¼˜åŒ–ä»»åŠ¡åˆ†è§£å¹¶æå‡RAGæµç¨‹ä¸­çš„å…³é”®è¯æå–å‡†ç¡®åº¦ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œç ”ç©¶è€…å¼€å‘äº†åä¸ºDeKeySQLçš„RAG-based NL2SQLæµæ°´çº¿ï¼Œé€šè¿‡ç”¨æˆ·é—®é¢˜ç†è§£ã€å®ä½“æ£€ç´¢å’Œç”Ÿæˆä¸‰ä¸ªç‹¬ç«‹æ¨¡å—æ¥æé«˜SQLç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨DeKeyNLUè¿›è¡Œå¾®è°ƒåï¼ŒDeKeySQLåœ¨BIRDæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä»62.31%æå‡è‡³69.10%ï¼Œåœ¨Spideræ•°æ®é›†ä¸Šä»84.2%æå‡è‡³88.7%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å¼ºåŒ–ä»»åŠ¡åˆ†è§£ä¸ç»†ç²’åº¦å…³é”®è¯æå–ï¼Œèƒ½æ˜¾è‘—å¢å¼ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ•°æ®åº“æŸ¥è¯¢åœºæ™¯ä¸‹çš„æ‰§è¡Œè¡¨ç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14507v2",
      "published_date": "2025-09-18 00:47:56 UTC",
      "updated_date": "2026-01-13 11:51:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:33:10.294676+00:00"
    },
    {
      "arxiv_id": "2509.14504v1",
      "title": "Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction",
      "title_zh": "OmniGECï¼šç”¨äºè¯­æ³•çº é”™çš„å¤šè¯­è¨€é“¶æ ‡å‡†æ•°æ®é›†",
      "authors": [
        "Roman Kovalchuk",
        "Mariana Romanyshyn",
        "Petro Ivaniuk"
      ],
      "abstract": "In this paper, we introduce OmniGEC, a collection of multilingual silver-standard datasets for the task of Grammatical Error Correction (GEC), covering eleven languages: Czech, English, Estonian, German, Greek, Icelandic, Italian, Latvian, Slovene, Swedish, and Ukrainian. These datasets facilitate the development of multilingual GEC solutions and help bridge the data gap in adapting English GEC solutions to multilingual GEC. The texts in the datasets originate from three sources: Wikipedia edits for the eleven target languages, subreddits from Reddit in the eleven target languages, and the Ukrainian-only UberText 2.0 social media corpus. While Wikipedia edits were derived from human-made corrections, the Reddit and UberText 2.0 data were automatically corrected with the GPT-4o-mini model. The quality of the corrections in the datasets was evaluated both automatically and manually. Finally, we fine-tune two open-source large language models - Aya-Expanse (8B) and Gemma-3 (12B) - on the multilingual OmniGEC corpora and achieve state-of-the-art (SOTA) results for paragraph-level multilingual GEC. The dataset collection and the best-performing models are available on Hugging Face.",
      "tldr_zh": "æœ¬ç ”ç©¶ä»‹ç»äº† OmniGECï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è¯­æ³•çº é”™ (Grammatical Error Correction, GEC) ä»»åŠ¡çš„å¤šè¯­è¨€é“¶æ ‡å‡†æ•°æ®é›†é›†åˆï¼Œæ¶µç›–äº†æ·å…‹è¯­ã€è‹±è¯­ã€å¾·è¯­ã€ä¹Œå…‹å…°è¯­ç­‰ 11 ç§è¯­è¨€ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨ä¿ƒè¿›å¤šè¯­è¨€ GEC æ–¹æ¡ˆçš„å¼€å‘ï¼Œå¹¶ç¼©å°å°†è‹±è¯­ GEC æ–¹æ¡ˆè¿ç§»åˆ°å¤šè¯­è¨€ç¯å¢ƒæ—¶çš„åº•å±‚æ•°æ®å·®è·ã€‚æ•°æ®é›†çš„æ–‡æœ¬æ¥æºåŒ…æ‹¬ 11 ç§ç›®æ ‡è¯­è¨€çš„ Wikipedia ç¼–è¾‘è®°å½•ã€Reddit ç¤¾åŒºå†…å®¹ä»¥åŠé’ˆå¯¹ä¹Œå…‹å…°è¯­çš„ UberText 2.0 ç¤¾äº¤åª’ä½“è¯­æ–™åº“ã€‚å…¶ä¸­ Wikipedia ç¼–è¾‘æºè‡ªäººå·¥ä¿®è®¢ï¼Œè€Œ Reddit å’Œ UberText 2.0 çš„æ•°æ®åˆ™åˆ©ç”¨ GPT-4o-mini æ¨¡å‹è¿›è¡Œè‡ªåŠ¨çº é”™ï¼Œå¹¶ç»è¿‡äº†è‡ªåŠ¨å’Œäººå·¥çš„è´¨é‡è¯„ä¼°ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å¤šè¯­è¨€ OmniGEC è¯­æ–™åº“ä¸Šå¯¹ Aya-Expanse (8B) å’Œ Gemma-3 (12B) ä¸¤æ¬¾å¼€æºå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ®µè½çº§å¤šè¯­è¨€ GEC ä»»åŠ¡ä¸Šè¾¾åˆ°äº†å½“å‰å…ˆè¿›æ°´å¹³ (SOTA)ï¼Œç›®å‰æ•°æ®é›†å’Œè¡¨ç°æœ€ä¼˜çš„æ¨¡å‹å‡å·²åœ¨ Hugging Face å¹³å°ä¸Šå¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14504v1",
      "published_date": "2025-09-18 00:35:31 UTC",
      "updated_date": "2025-09-18 00:35:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:33:15.286785+00:00"
    },
    {
      "arxiv_id": "2509.19351v1",
      "title": "The Impact of Structural Changes on Learning Capacity in the Fly Olfactory Neural Circuit",
      "title_zh": "ç»“æ„å˜åŒ–å¯¹æœè‡å—…è§‰ç¥ç»å›è·¯å­¦ä¹ èƒ½åŠ›çš„å½±å“",
      "authors": [
        "Katherine Xie",
        "Gabriel Koch Ocker"
      ],
      "abstract": "The Drosophila mushroom body (MB) is known to be involved in olfactory learning and memory; the synaptic plasticity of the Kenyon cell (KC) to mushroom body output neuron (MBON) synapses plays a key role in the learning process. Previous research has focused on projection neuron (PN) to Kenyon cell (KC) connectivity within the MB; we examine how perturbations to the mushroom body circuit structure and changes in connectivity, specifically within the KC to mushroom body output neuron (MBON) neural circuit, affect the MBONs' ability to distinguish between odor classes. We constructed a neural network that incorporates the connectivity between PNs, KCs, and MBONs. To train our model, we generated ten artificial input classes, which represent the projection neuron activity in response to different odors. We collected data on the number of KC-to-MBON connections, MBON error rates, and KC-to-MBON synaptic weights, among other metrics. We observed that MBONs with very few presynaptic KCs consistently performed worse than others in the odor classification task. The developmental types of KCs also played a significant role in each MBON's output. We performed random and targeted KC ablation and observed that ablating developmentally mature KCs had a greater negative impact on MBONs' learning capacity than ablating immature KCs. Random and targeted pruning of KC-MBON synaptic connections yielded results largely consistent with the ablation experiments. To further explore the various types of KCs, we also performed rewiring experiments in the PN to KC circuit. Our study furthers our understanding of olfactory neuroplasticity and provides important clues to understanding learning and memory in general. Understanding how the olfactory circuits process and learn can also have potential applications in artificial intelligence and treatments for neurodegenerative diseases.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœè‡(Drosophila)è•ˆçŠ¶ä½“(Mushroom Body)ç»“æ„å˜åŒ–å¯¹å­¦ä¹ èƒ½åŠ›çš„å½±å“ï¼Œé‡ç‚¹åˆ†æäº†Kenyon cell (KC)åˆ°mushroom body output neuron (MBON)çš„ç¥ç»ç¯è·¯è¿æ¥æ€§å¦‚ä½•å½±å“æ°”å‘³è¯†åˆ«ã€‚ç ”ç©¶äººå‘˜æ„å»ºäº†ä¸€ä¸ªæ•´åˆprojection neuron (PN)ã€KCå’ŒMBONè¿æ¥çš„ç¥ç»ç½‘ç»œï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿåç§æ°”å‘³è¾“å…¥æ¥è¯„ä¼°åˆ†ç±»æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œçªè§¦å‰KCæ•°é‡è¾ƒå°‘çš„MBONåœ¨æ°”å‘³åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°æ˜¾è‘—å˜å·®ï¼Œä¸”ä¸åŒå‘è‚²é˜¶æ®µçš„KCå¯¹MBONè¾“å‡ºå…·æœ‰æ˜¾è‘—å½±å“ã€‚é€šè¿‡æ¶ˆè(ablation)å’Œçªè§¦ä¿®å‰ª(pruning)å®éªŒå‘ç°ï¼Œç§»é™¤å‘è‚²æˆç†Ÿçš„KCå¯¹å­¦ä¹ èƒ½åŠ›çš„è´Ÿé¢å½±å“æ˜æ˜¾å¤§äºæœªæˆç†Ÿçš„KCã€‚è¯¥æˆæœæ·±åŒ–äº†å¯¹å—…è§‰ç¥ç»å¯å¡‘æ€§çš„ç†è§£ï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½æ¨¡å‹å¼€å‘åŠç¥ç»é€€è¡Œæ€§ç–¾ç—…æ²»ç–—æä¾›äº†æ½œåœ¨çš„ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19351v1",
      "published_date": "2025-09-18 00:12:58 UTC",
      "updated_date": "2025-09-18 00:12:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:33:16.262641+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 155,
  "processed_papers_count": 155,
  "failed_papers_count": 0,
  "llm_backup_calls": 2,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T19:35:09.782560+00:00"
}
[
  {
    "arxiv_id": "2504.10478v2",
    "title": "Weight Ensembling Improves Reasoning in Language Models",
    "authors": [
      "Xingyu Dang",
      "Christina Baek",
      "Kaiyue Wen",
      "Zico Kolter",
      "Aditi Raghunathan"
    ],
    "abstract": "We investigate a failure mode that arises during the training of reasoning\nmodels, where the diversity of generations begins to collapse, leading to\nsuboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during\nsupervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a\nsimple intervention of interpolating the weights of the latest SFT checkpoint\nwith an early checkpoint, otherwise known as WiSE-FT, almost completely\nrecovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves\nbetter test-time scaling (Best@k, majority vote) and achieves superior results\nwith less data when tuned further by reinforcement learning. Finally, we find\nthat WiSE-FT provides complementary performance gains that cannot be achieved\nonly through diversity-inducing decoding strategies, like temperature scaling.\nWe formalize a bias-variance tradeoff of Pass@k with respect to the expectation\nand variance of Pass@1 over the test distribution. We find that WiSE-FT can\nreduce bias and variance simultaneously, while temperature scaling inherently\ntrades-off between bias and variance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10478v2",
    "published_date": "2025-04-14 17:59:07 UTC",
    "updated_date": "2025-04-15 17:46:59 UTC"
  },
  {
    "arxiv_id": "2504.10445v1",
    "title": "RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users",
    "authors": [
      "Suyu Ye",
      "Haojun Shi",
      "Darren Shih",
      "Hyokun Yun",
      "Tanya Roosta",
      "Tianmin Shu"
    ],
    "abstract": "To achieve successful assistance with long-horizon web-based tasks, AI agents\nmust be able to sequentially follow real-world user instructions over a long\nperiod. Unlike existing web-based agent benchmarks, sequential instruction\nfollowing in the real world poses significant challenges beyond performing a\nsingle, clearly defined task. For instance, real-world human instructions can\nbe ambiguous, require different levels of AI assistance, and may evolve over\ntime, reflecting changes in the user's mental state. To address this gap, we\nintroduce RealWebAssist, a novel benchmark designed to evaluate sequential\ninstruction-following in realistic scenarios involving long-horizon\ninteractions with the web, visual GUI grounding, and understanding ambiguous\nreal-world user instructions. RealWebAssist includes a dataset of sequential\ninstructions collected from real-world human users. Each user instructs a\nweb-based assistant to perform a series of tasks on multiple websites. A\nsuccessful agent must reason about the true intent behind each instruction,\nkeep track of the mental state of the user, understand user-specific routines,\nand ground the intended tasks to actions on the correct GUI elements. Our\nexperimental results show that state-of-the-art models struggle to understand\nand ground user instructions, posing critical challenges in following\nreal-world user instructions for long-horizon web assistance.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Project Website: https://scai.cs.jhu.edu/projects/RealWebAssist/\n  Code: https://github.com/SCAI-JHU/RealWebAssist",
    "pdf_url": "http://arxiv.org/pdf/2504.10445v1",
    "published_date": "2025-04-14 17:36:46 UTC",
    "updated_date": "2025-04-14 17:36:46 UTC"
  },
  {
    "arxiv_id": "2504.10443v1",
    "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context",
    "authors": [
      "Haoran Hao",
      "Jiaming Han",
      "Yiyuan Zhang",
      "Xiangyu Yue"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10443v1",
    "published_date": "2025-04-14 17:34:06 UTC",
    "updated_date": "2025-04-14 17:34:06 UTC"
  },
  {
    "arxiv_id": "2504.10430v1",
    "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models",
    "authors": [
      "Minqian Liu",
      "Zhiyang Xu",
      "Xinyi Zhang",
      "Heajun An",
      "Sarvech Qadir",
      "Qi Zhang",
      "Pamela J. Wisniewski",
      "Jin-Hee Cho",
      "Sang Won Lee",
      "Ruoxi Jia",
      "Lifu Huang"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 7 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.10430v1",
    "published_date": "2025-04-14 17:20:34 UTC",
    "updated_date": "2025-04-14 17:20:34 UTC"
  },
  {
    "arxiv_id": "2504.10421v1",
    "title": "Can We Edit LLMs for Long-Tail Biomedical Knowledge?",
    "authors": [
      "Xinhao Yi",
      "Jake Lever",
      "Kevin Bryson",
      "Zaiqiao Meng"
    ],
    "abstract": "Knowledge editing has emerged as an effective approach for updating large\nlanguage models (LLMs) by modifying their internal knowledge. However, their\napplication to the biomedical domain faces unique challenges due to the\nlong-tailed distribution of biomedical knowledge, where rare and infrequent\ninformation is prevalent. In this paper, we conduct the first comprehensive\nstudy to investigate the effectiveness of knowledge editing methods for editing\nlong-tail biomedical knowledge. Our results indicate that, while existing\nediting methods can enhance LLMs' performance on long-tail biomedical\nknowledge, their performance on long-tail knowledge remains inferior to that on\nhigh-frequency popular knowledge, even after editing. Our further analysis\nreveals that long-tail biomedical knowledge contains a significant amount of\none-to-many knowledge, where one subject and relation link to multiple objects.\nThis high prevalence of one-to-many knowledge limits the effectiveness of\nknowledge editing in improving LLMs' understanding of long-tail biomedical\nknowledge, highlighting the need for tailored strategies to bridge this\nperformance gap.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10421v1",
    "published_date": "2025-04-14 17:08:20 UTC",
    "updated_date": "2025-04-14 17:08:20 UTC"
  },
  {
    "arxiv_id": "2504.10415v1",
    "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models",
    "authors": [
      "Parshin Shojaee",
      "Ngoc-Hieu Nguyen",
      "Kazem Meidani",
      "Amir Barati Farimani",
      "Khoa D Doan",
      "Chandan K Reddy"
    ],
    "abstract": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Project page:\n  https://github.com/deep-symbolic-mathematics/llm-srbench , Benchmark page:\n  https://huggingface.co/datasets/nnheui/llm-srbench",
    "pdf_url": "http://arxiv.org/pdf/2504.10415v1",
    "published_date": "2025-04-14 17:00:13 UTC",
    "updated_date": "2025-04-14 17:00:13 UTC"
  },
  {
    "arxiv_id": "2504.10412v1",
    "title": "AI-Driven Code Refactoring: Using Graph Neural Networks to Enhance Software Maintainability",
    "authors": [
      "Gopichand Bandarupalli"
    ],
    "abstract": "This study explores Graph Neural Networks (GNNs) as a transformative tool for\ncode refactoring, using abstract syntax trees (ASTs) to boost software\nmaintainability. It analyzes a dataset of 2 million snippets from CodeSearchNet\nand a custom 75000-file GitHub Python corpus, comparing GNNs against rule-based\nSonarQube and decision trees. Metrics include cyclomatic complexity (target\nbelow 10), coupling (target below 5), and refactoring precision. GNNs achieve\n92% accuracy, reducing complexity by 35% and coupling by 33%, outperforming\nSonarQube (78%, 16%) and decision trees (85%, 25%). Preprocessing fixed 60% of\nsyntax errors. Bar graphs, tables, and AST visuals clarify results. This offers\na scalable AI-driven path to cleaner codebases, which is crucial for software\nengineering.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10412v1",
    "published_date": "2025-04-14 16:58:54 UTC",
    "updated_date": "2025-04-14 16:58:54 UTC"
  },
  {
    "arxiv_id": "2504.10405v1",
    "title": "Performance of Large Language Models in Supporting Medical Diagnosis and Treatment",
    "authors": [
      "Diogo Sousa",
      "Guilherme Barbosa",
      "Catarina Rocha",
      "Dulce Oliveira"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into healthcare holds\nsignificant potential to enhance diagnostic accuracy and support medical\ntreatment planning. These AI-driven systems can analyze vast datasets,\nassisting clinicians in identifying diseases, recommending treatments, and\npredicting patient outcomes. This study evaluates the performance of a range of\ncontemporary LLMs, including both open-source and closed-source models, on the\n2024 Portuguese National Exam for medical specialty access (PNA), a\nstandardized medical knowledge assessment. Our results highlight considerable\nvariation in accuracy and cost-effectiveness, with several models demonstrating\nperformance exceeding human benchmarks for medical students on this specific\ntask. We identify leading models based on a combined score of accuracy and\ncost, discuss the implications of reasoning methodologies like\nChain-of-Thought, and underscore the potential for LLMs to function as valuable\ncomplementary tools aiding medical professionals in complex clinical\ndecision-making.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "I.2.7; J.3"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 6 figures, 4 tables. Acknowledgements: The authors\n  acknowledge the support of the AITriage4SU Project (2024.07400.IACDC/2024),\n  funded by the FCT (Foundation for Science and Technology), Portugal",
    "pdf_url": "http://arxiv.org/pdf/2504.10405v1",
    "published_date": "2025-04-14 16:53:59 UTC",
    "updated_date": "2025-04-14 16:53:59 UTC"
  },
  {
    "arxiv_id": "2504.10397v1",
    "title": "Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?",
    "authors": [
      "Olha Shaposhnyk",
      "Daria Zahorska",
      "Svetlana Yanushkevich"
    ],
    "abstract": "Objective: This study investigates the potential of Large Language Models\n(LLMs) as an alternative to human expert elicitation for extracting structured\ncausal knowledge and facilitating causal modeling in biometric and healthcare\napplications.\n  Material and Methods: LLM-generated causal structures, specifically Bayesian\nnetworks (BNs), were benchmarked against traditional statistical methods (e.g.,\nBayesian Information Criterion) using healthcare datasets. Validation\ntechniques included structural equation modeling (SEM) to verifying\nrelationships, and measures such as entropy, predictive accuracy, and\nrobustness to compare network structures.\n  Results and Discussion: LLM-generated BNs demonstrated lower entropy than\nexpert-elicited and statistically generated BNs, suggesting higher confidence\nand precision in predictions. However, limitations such as contextual\nconstraints, hallucinated dependencies, and potential biases inherited from\ntraining data require further investigation.\n  Conclusion: LLMs represent a novel frontier in expert elicitation for\nprobabilistic causal modeling, promising to improve transparency and reduce\nuncertainty in the decision-making using such models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10397v1",
    "published_date": "2025-04-14 16:45:52 UTC",
    "updated_date": "2025-04-14 16:45:52 UTC"
  },
  {
    "arxiv_id": "2504.10390v1",
    "title": "Teacher Motion Priors: Enhancing Robot Locomotion over Challenging Terrain",
    "authors": [
      "Fangcheng Jin",
      "Yuqi Wang",
      "Peixin Ma",
      "Guodong Yang",
      "Pan Zhao",
      "En Li",
      "Zhengtao Zhang"
    ],
    "abstract": "Achieving robust locomotion on complex terrains remains a challenge due to\nhigh dimensional control and environmental uncertainties. This paper introduces\na teacher prior framework based on the teacher student paradigm, integrating\nimitation and auxiliary task learning to improve learning efficiency and\ngeneralization. Unlike traditional paradigms that strongly rely on\nencoder-based state embeddings, our framework decouples the network design,\nsimplifying the policy network and deployment. A high performance teacher\npolicy is first trained using privileged information to acquire generalizable\nmotion skills. The teacher's motion distribution is transferred to the student\npolicy, which relies only on noisy proprioceptive data, via a generative\nadversarial mechanism to mitigate performance degradation caused by\ndistributional shifts. Additionally, auxiliary task learning enhances the\nstudent policy's feature representation, speeding up convergence and improving\nadaptability to varying terrains. The framework is validated on a humanoid\nrobot, showing a great improvement in locomotion stability on dynamic terrains\nand significant reductions in development costs. This work provides a practical\nsolution for deploying robust locomotion strategies in humanoid robots.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "68T40"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 6 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.10390v1",
    "published_date": "2025-04-14 16:36:56 UTC",
    "updated_date": "2025-04-14 16:36:56 UTC"
  },
  {
    "arxiv_id": "2504.10369v1",
    "title": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning",
    "authors": [
      "Yiting Wang",
      "Wanghao Ye",
      "Ping Guo",
      "Yexiao He",
      "Ziyao Wang",
      "Yexiao He",
      "Bowei Tian",
      "Shwai He",
      "Guoheng Sun",
      "Zheyu Shen",
      "Sihan Chen",
      "Ankur Srivastava",
      "Qingfu Zhang",
      "Gang Qu",
      "Ang Li"
    ],
    "abstract": "Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.AR",
    "comment": "16 pages, 8 figures, 7 tables. Under Review",
    "pdf_url": "http://arxiv.org/pdf/2504.10369v1",
    "published_date": "2025-04-14 16:15:55 UTC",
    "updated_date": "2025-04-14 16:15:55 UTC"
  },
  {
    "arxiv_id": "2504.10368v1",
    "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models",
    "authors": [
      "Wenyuan Zhang",
      "Shuaiyi Nie",
      "Xinghua Zhang",
      "Zefeng Zhang",
      "Tingwen Liu"
    ],
    "abstract": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in Progress",
    "pdf_url": "http://arxiv.org/pdf/2504.10368v1",
    "published_date": "2025-04-14 16:13:23 UTC",
    "updated_date": "2025-04-14 16:13:23 UTC"
  },
  {
    "arxiv_id": "2504.10358v1",
    "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos",
    "authors": [
      "Rui Chen",
      "Lei Sun",
      "Jing Tang",
      "Geng Li",
      "Xiangxiang Chu"
    ],
    "abstract": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.10358v1",
    "published_date": "2025-04-14 16:07:16 UTC",
    "updated_date": "2025-04-14 16:07:16 UTC"
  },
  {
    "arxiv_id": "2504.10340v1",
    "title": "Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families",
    "authors": [
      "Shahriar Noroozizadeh",
      "Sayantan Kumar",
      "Jeremy C. Weiss"
    ],
    "abstract": "Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings--extracted via an\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\nsystematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Machine Learning for Healthcare (MLHC 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.10340v1",
    "published_date": "2025-04-14 15:48:56 UTC",
    "updated_date": "2025-04-14 15:48:56 UTC"
  },
  {
    "arxiv_id": "2504.10337v1",
    "title": "Heimdall: test-time scaling on the generative verification",
    "authors": [
      "Wenlei Shi",
      "Xing Jin"
    ],
    "abstract": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10337v1",
    "published_date": "2025-04-14 15:46:33 UTC",
    "updated_date": "2025-04-14 15:46:33 UTC"
  },
  {
    "arxiv_id": "2504.10326v1",
    "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context LLM Inference",
    "authors": [
      "Yangshen Deng",
      "Zhengxin You",
      "Long Xiang",
      "Qilong Li",
      "Peiqi Yuan",
      "Zhaoyang Hong",
      "Yitao Zheng",
      "Wanting Li",
      "Runzhong Li",
      "Haotian Liu",
      "Kyriakos Mouratidis",
      "Man Lung Yiu",
      "Huan Li",
      "Qiaomu Shen",
      "Rui Mao",
      "Bo Tang"
    ],
    "abstract": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.IR",
      "H.3.1; H.3.2; H.3.3; H.3.4"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 12 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2504.10326v1",
    "published_date": "2025-04-14 15:34:26 UTC",
    "updated_date": "2025-04-14 15:34:26 UTC"
  },
  {
    "arxiv_id": "2504.10309v1",
    "title": "AutoStyle-TTS: Retrieval-Augmented Generation based Automatic Style Matching Text-to-Speech Synthesis",
    "authors": [
      "Dan Luo",
      "Chengyuan Ma",
      "Weiqin Li",
      "Jun Wang",
      "Wei Chen",
      "Zhiyong Wu"
    ],
    "abstract": "With the advancement of speech synthesis technology, users have higher\nexpectations for the naturalness and expressiveness of synthesized speech. But\nprevious research ignores the importance of prompt selection. This study\nproposes a text-to-speech (TTS) framework based on Retrieval-Augmented\nGeneration (RAG) technology, which can dynamically adjust the speech style\naccording to the text content to achieve more natural and vivid communication\neffects. We have constructed a speech style knowledge database containing\nhigh-quality speech samples in various contexts and developed a style matching\nscheme. This scheme uses embeddings, extracted by Llama, PER-LLM-Embedder,and\nMoka, to match with samples in the knowledge database, selecting the most\nappropriate speech style for synthesis. Furthermore, our empirical research\nvalidates the effectiveness of the proposed method. Our demo can be viewed at:\nhttps://thuhcsi.github.io/icme2025-AutoStyle-TTS",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "accepted by ICME25",
    "pdf_url": "http://arxiv.org/pdf/2504.10309v1",
    "published_date": "2025-04-14 15:18:59 UTC",
    "updated_date": "2025-04-14 15:18:59 UTC"
  },
  {
    "arxiv_id": "2504.10286v1",
    "title": "Characterizing LLM-driven Social Network: The Chirper.ai Case",
    "authors": [
      "Yiming Zhu",
      "Yupeng He",
      "Ehsan-Ul Haq",
      "Gareth Tyson",
      "Pan Hui"
    ],
    "abstract": "Large language models (LLMs) demonstrate the ability to simulate human\ndecision-making processes, enabling their use as agents in modeling\nsophisticated social networks, both offline and online. Recent research has\nexplored collective behavioral patterns and structural characteristics of LLM\nagents within simulated networks. However, empirical comparisons between\nLLM-driven and human-driven online social networks remain scarce, limiting our\nunderstanding of how LLM agents differ from human users. This paper presents a\nlarge-scale analysis of Chirper.ai, an X/Twitter-like social network entirely\npopulated by LLM agents, comprising over 65,000 agents and 7.7 million\nAI-generated posts. For comparison, we collect a parallel dataset from\nMastodon, a human-driven decentralized social network, with over 117,000 users\nand 16 million posts. We examine key differences between LLM agents and humans\nin posting behaviors, abusive content, and social network structures. Our\nfindings provide critical insights into the evolving landscape of online social\nnetwork analysis in the AI era, offering a comprehensive profile of LLM agents\nin social simulations.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2504.10286v1",
    "published_date": "2025-04-14 14:53:31 UTC",
    "updated_date": "2025-04-14 14:53:31 UTC"
  },
  {
    "arxiv_id": "2504.10281v1",
    "title": "Zero-shot Autonomous Microscopy for Scalable and Intelligent Characterization of 2D Materials",
    "authors": [
      "Jingyun Yang",
      "Ruoyan Avery Yin",
      "Chi Jiang",
      "Yuepeng Hu",
      "Xiaokai Zhu",
      "Xingjian Hu",
      "Sutharsika Kumar",
      "Xiao Wang",
      "Xiaohua Zhai",
      "Keran Rong",
      "Yunyue Zhu",
      "Tianyi Zhang",
      "Zongyou Yin",
      "Jing Kong",
      "Neil Zhenqiang Gong",
      "Zhichu Ren",
      "Haozhe Wang"
    ],
    "abstract": "Characterization of atomic-scale materials traditionally requires human\nexperts with months to years of specialized training. Even for trained human\noperators, accurate and reliable characterization remains challenging when\nexamining newly discovered materials such as two-dimensional (2D) structures.\nThis bottleneck drives demand for fully autonomous experimentation systems\ncapable of comprehending research objectives without requiring large training\ndatasets. In this work, we present ATOMIC (Autonomous Technology for Optical\nMicroscopy & Intelligent Characterization), an end-to-end framework that\nintegrates foundation models to enable fully autonomous, zero-shot\ncharacterization of 2D materials. Our system integrates the vision foundation\nmodel (i.e., Segment Anything Model), large language models (i.e., ChatGPT),\nunsupervised clustering, and topological analysis to automate microscope\ncontrol, sample scanning, image segmentation, and intelligent analysis through\nprompt engineering, eliminating the need for additional training. When\nanalyzing typical MoS2 samples, our approach achieves 99.7% segmentation\naccuracy for single layer identification, which is equivalent to that of human\nexperts. In addition, the integrated model is able to detect grain boundary\nslits that are challenging to identify with human eyes. Furthermore, the system\nretains robust accuracy despite variable conditions including defocus, color\ntemperature fluctuations, and exposure variations. It is applicable to a broad\nspectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless\nof whether they were fabricated via chemical vapor deposition or mechanical\nexfoliation. This work represents the implementation of foundation models to\nachieve autonomous analysis, establishing a scalable and data-efficient\ncharacterization paradigm that fundamentally transforms the approach to\nnanoscale materials research.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "13 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.10281v1",
    "published_date": "2025-04-14 14:49:45 UTC",
    "updated_date": "2025-04-14 14:49:45 UTC"
  },
  {
    "arxiv_id": "2504.10277v1",
    "title": "RealHarm: A Collection of Real-World Language Model Application Failures",
    "authors": [
      "Pierre Le Jeune",
      "Jiaen Liu",
      "Luca Rossi",
      "Matteo Dora"
    ],
    "abstract": "Language model deployments in consumer-facing applications introduce numerous\nrisks. While existing research on harms and hazards of such applications\nfollows top-down approaches derived from regulatory frameworks and theoretical\nanalyses, empirical evidence of real-world failure modes remains underexplored.\nIn this work, we introduce RealHarm, a dataset of annotated problematic\ninteractions with AI agents built from a systematic review of publicly reported\nincidents. Analyzing harms, causes, and hazards specifically from the\ndeployer's perspective, we find that reputational damage constitutes the\npredominant organizational harm, while misinformation emerges as the most\ncommon hazard category. We empirically evaluate state-of-the-art guardrails and\ncontent moderation systems to probe whether such systems would have prevented\nthe incidents, revealing a significant gap in the protection of AI\napplications.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10277v1",
    "published_date": "2025-04-14 14:44:41 UTC",
    "updated_date": "2025-04-14 14:44:41 UTC"
  },
  {
    "arxiv_id": "2504.10266v1",
    "title": "Vision based driving agent for race car simulation environments",
    "authors": [
      "Gergely Bári",
      "László Palkovics"
    ],
    "abstract": "In recent years, autonomous driving has become a popular field of study. As\ncontrol at tire grip limit is essential during emergency situations, algorithms\ndeveloped for racecars are useful for road cars too. This paper examines the\nuse of Deep Reinforcement Learning (DRL) to solve the problem of grip limit\ndriving in a simulated environment. Proximal Policy Optimization (PPO) method\nis used to train an agent to control the steering wheel and pedals of the\nvehicle, using only visual inputs to achieve professional human lap times. The\npaper outlines the formulation of the task of time optimal driving on a race\ntrack as a deep reinforcement learning problem, and explains the chosen\nobservations, actions, and reward functions. The results demonstrate human-like\nlearning and driving behavior that utilize maximum tire grip potential.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to ICMCE 2024 (https://icmce.org/2024.html)",
    "pdf_url": "http://arxiv.org/pdf/2504.10266v1",
    "published_date": "2025-04-14 14:29:37 UTC",
    "updated_date": "2025-04-14 14:29:37 UTC"
  },
  {
    "arxiv_id": "2504.10254v1",
    "title": "MASSeg : 2nd Technical Report for 4th PVUW MOSE Track",
    "authors": [
      "Xuqiang Cao",
      "Linnan Zhao",
      "Jiaxuan Zhao",
      "Fang Liu",
      "Puhua Chen",
      "Wenping Ma"
    ],
    "abstract": "Complex video object segmentation continues to face significant challenges in\nsmall object recognition, occlusion handling, and dynamic scene modeling. This\nreport presents our solution, which ranked second in the MOSE track of CVPR\n2025 PVUW Challenge. Based on an existing segmentation framework, we propose an\nimproved model named MASSeg for complex video object segmentation, and\nconstruct an enhanced dataset, MOSE+, which includes typical scenarios with\nocclusions, cluttered backgrounds, and small target instances. During training,\nwe incorporate a combination of inter-frame consistent and inconsistent data\naugmentation strategies to improve robustness and generalization. During\ninference, we design a mask output scaling strategy to better adapt to varying\nobject sizes and occlusion levels. As a result, MASSeg achieves a J score of\n0.8250, F score of 0.9007, and a J&F score of 0.8628 on the MOSE test set.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages,4 figures,Technical report on Complex Video Object\n  Segmentation",
    "pdf_url": "http://arxiv.org/pdf/2504.10254v1",
    "published_date": "2025-04-14 14:15:46 UTC",
    "updated_date": "2025-04-14 14:15:46 UTC"
  },
  {
    "arxiv_id": "2504.10210v1",
    "title": "Can Competition Enhance the Proficiency of Agents Powered by Large Language Models in the Realm of News-driven Time Series Forecasting?",
    "authors": [
      "Yuxuan Zhang",
      "Yangyang Feng",
      "Daifeng Li",
      "Kexin Zhang",
      "Junlan Chen",
      "Bowen Deng"
    ],
    "abstract": "Multi-agents-based news-driven time series forecasting is considered as a\npotential paradigm shift in the era of large language models (LLMs). The\nchallenge of this task lies in measuring the influences of different news\nevents towards the fluctuations of time series. This requires agents to possess\nstronger abilities of innovative thinking and the identifying misleading logic.\nHowever, the existing multi-agent discussion framework has limited enhancement\non time series prediction in terms of optimizing these two capabilities.\nInspired by the role of competition in fostering innovation, this study embeds\na competition mechanism within the multi-agent discussion to enhance agents'\ncapability of generating innovative thoughts. Furthermore, to bolster the\nmodel's proficiency in identifying misleading information, we incorporate a\nfine-tuned small-scale LLM model within the reflective stage, offering\nauxiliary decision-making support. Experimental results confirm that the\ncompetition can boost agents' capacity for innovative thinking, which can\nsignificantly improve the performances of time series prediction. Similar to\nthe findings of social science, the intensity of competition within this\nframework can influence the performances of agents, providing a new perspective\nfor studying LLMs-based multi-agent systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10210v1",
    "published_date": "2025-04-14 13:25:50 UTC",
    "updated_date": "2025-04-14 13:25:50 UTC"
  },
  {
    "arxiv_id": "2504.10191v1",
    "title": "Localized Cultural Knowledge is Conserved and Controllable in Large Language Models",
    "authors": [
      "Veniamin Veselovsky",
      "Berke Argin",
      "Benedikt Stroebl",
      "Chris Wendler",
      "Robert West",
      "James Evans",
      "Thomas L. Griffiths",
      "Arvind Narayanan"
    ],
    "abstract": "Just as humans display language patterns influenced by their native tongue\nwhen speaking new languages, LLMs often default to English-centric responses\neven when generating in other languages. Nevertheless, we observe that local\ncultural information persists within the models and can be readily activated\nfor cultural customization. We first demonstrate that explicitly providing\ncultural context in prompts significantly improves the models' ability to\ngenerate culturally localized responses. We term the disparity in model\nperformance with versus without explicit cultural context the explicit-implicit\nlocalization gap, indicating that while cultural knowledge exists within LLMs,\nit may not naturally surface in multilingual interactions if cultural context\nis not explicitly provided. Despite the explicit prompting benefit, however,\nthe answers reduce in diversity and tend toward stereotypes. Second, we\nidentify an explicit cultural customization vector, conserved across all\nnon-English languages we explore, which enables LLMs to be steered from the\nsynthetic English cultural world-model toward each non-English cultural world.\nSteered responses retain the diversity of implicit prompting and reduce\nstereotypes to dramatically improve the potential for customization. We discuss\nthe implications of explicit cultural customization for understanding the\nconservation of alternative cultural world models within LLMs, and their\ncontrollable utility for translation, cultural customization, and the\npossibility of making the explicit implicit through soft control for expanded\nLLM function and appeal.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10191v1",
    "published_date": "2025-04-14 12:53:58 UTC",
    "updated_date": "2025-04-14 12:53:58 UTC"
  },
  {
    "arxiv_id": "2504.10188v1",
    "title": "Efficient Generative Model Training via Embedded Representation Warmup",
    "authors": [
      "Deyuan Liu",
      "Peng Sun",
      "Xufeng Li",
      "Tao Lin"
    ],
    "abstract": "Diffusion models excel at generating high-dimensional data but fall short in\ntraining efficiency and representation quality compared to self-supervised\nmethods. We identify a key bottleneck: the underutilization of high-quality,\nsemantically rich representations during training notably slows down\nconvergence. Our systematic analysis reveals a critical representation\nprocessing region -- primarily in the early layers -- where semantic and\nstructural pattern learning takes place before generation can occur. To address\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\nframework where in the first stage we get the ERW module serves as a warmup\nthat initializes the early layers of the diffusion model with high-quality,\npretrained representations. This warmup minimizes the burden of learning\nrepresentations from scratch, thereby accelerating convergence and boosting\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\non its precise integration into specific neural network layers -- termed the\nrepresentation processing region -- where the model primarily processes and\ntransforms feature representations for later generation. We further establish\nthat ERW not only accelerates training convergence but also enhances\nrepresentation quality: empirically, our method achieves a 40$\\times$\nacceleration in training speed compared to REPA, the current state-of-the-art\nmethods. Code is available at https://github.com/LINs-lab/ERW.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10188v1",
    "published_date": "2025-04-14 12:43:17 UTC",
    "updated_date": "2025-04-14 12:43:17 UTC"
  },
  {
    "arxiv_id": "2504.10187v1",
    "title": "Deep Reasoning Translation via Reinforcement Learning",
    "authors": [
      "Jiaan Wang",
      "Fandong Meng",
      "Jie Zhou"
    ],
    "abstract": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10187v1",
    "published_date": "2025-04-14 12:40:39 UTC",
    "updated_date": "2025-04-14 12:40:39 UTC"
  },
  {
    "arxiv_id": "2504.10185v1",
    "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
    "authors": [
      "Soumyadeep Pal",
      "Changsheng Wang",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Sijia Liu"
    ],
    "abstract": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10185v1",
    "published_date": "2025-04-14 12:38:37 UTC",
    "updated_date": "2025-04-14 12:38:37 UTC"
  },
  {
    "arxiv_id": "2504.10179v1",
    "title": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental Evaluation of Prompt Engineering Methods for Robust Multimodal Performance",
    "authors": [
      "Anwesha Mohanty",
      "Venkatesh Balavadhani Parthasarathy",
      "Arsalan Shahid"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10179v1",
    "published_date": "2025-04-14 12:31:39 UTC",
    "updated_date": "2025-04-14 12:31:39 UTC"
  },
  {
    "arxiv_id": "2504.10168v1",
    "title": "HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination Detection",
    "authors": [
      "Mohamed A. Abdallah",
      "Samhaa R. El-Beltagy"
    ],
    "abstract": "In this paper, we present HalluSearch, a multilingual pipeline designed to\ndetect fabricated text spans in Large Language Model (LLM) outputs. Developed\nas part of Mu-SHROOM, the Multilingual Shared-task on Hallucinations and\nRelated Observable Overgeneration Mistakes, HalluSearch couples\nretrieval-augmented verification with fine-grained factual splitting to\nidentify and localize hallucinations in fourteen different languages. Empirical\nevaluations show that HalluSearch performs competitively, placing fourth in\nboth English (within the top ten percent) and Czech. While the system's\nretrieval-based strategy generally proves robust, it faces challenges in\nlanguages with limited online coverage, underscoring the need for further\nresearch to ensure consistent hallucination detection across diverse linguistic\ncontexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10168v1",
    "published_date": "2025-04-14 12:22:30 UTC",
    "updated_date": "2025-04-14 12:22:30 UTC"
  },
  {
    "arxiv_id": "2504.10167v1",
    "title": "C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination Evaluation",
    "authors": [
      "Xu Zhang",
      "Zhifei Liu",
      "Jiahao Wang",
      "Huixuan Zhang",
      "Fan Xu",
      "Junzhe Zhang",
      "Xiaojun Wan"
    ],
    "abstract": "Despite the rapid advancement of large language models, they remain highly\nsusceptible to generating hallucinations, which significantly hinders their\nwidespread application. Hallucination research requires dynamic and\nfine-grained evaluation. However, most existing hallucination benchmarks\n(especially in Chinese language) rely on human annotations, making automatical\nand cost-effective hallucination evaluation challenging. To address this, we\nintroduce HaluAgent, an agentic framework that automatically constructs\nfine-grained QA dataset based on some knowledge documents. Our experiments\ndemonstrate that the manually designed rules and prompt optimization can\nimprove the quality of generated data. Using HaluAgent, we construct C-FAITH, a\nChinese QA hallucination benchmark created from 1,399 knowledge documents\nobtained from web scraping, totaling 60,702 entries. We comprehensively\nevaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed\nexperimental results and analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10167v1",
    "published_date": "2025-04-14 12:21:55 UTC",
    "updated_date": "2025-04-14 12:21:55 UTC"
  },
  {
    "arxiv_id": "2504.10165v2",
    "title": "WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs",
    "authors": [
      "Nguyen Ngoc Dat",
      "Tom Richardson",
      "Matthew Watson",
      "Kilian Meier",
      "Jenna Kline",
      "Sid Reid",
      "Guy Maalouf",
      "Duncan Hine",
      "Majid Mirmehdi",
      "Tilo Burghardt"
    ],
    "abstract": "Live tracking of wildlife via high-resolution video processing directly\nonboard drones is widely unexplored and most existing solutions rely on\nstreaming video to ground stations to support navigation. Yet, both autonomous\nanimal-reactive flight control beyond visual line of sight and/or\nmission-specific individual and behaviour recognition tasks rely to some degree\non this capability. In response, we introduce WildLive -- a near real-time\nanimal detection and tracking framework for high-resolution imagery running\ndirectly onboard uncrewed aerial vehicles (UAVs). The system performs\nmulti-animal detection and tracking at 17fps+ for HD and 7fps+ on 4K video\nstreams suitable for operation during higher altitude flights to minimise\nanimal disturbance. Our system is optimised for Jetson Orin AGX onboard\nhardware. It integrates the efficiency of sparse optical flow tracking and\nmission-specific sampling with device-optimised and proven YOLO-driven object\ndetection and segmentation techniques. Essentially, computational resource is\nfocused onto spatio-temporal regions of high uncertainty to significantly\nimprove UAV processing speeds without domain-specific loss of accuracy.\nAlongside, we introduce our WildLive dataset, which comprises 200k+ annotated\nanimal instances across 19k+ frames from 4K UAV videos collected at the Ol\nPejeta Conservancy in Kenya. All frames contain ground truth bounding boxes,\nsegmentation masks, as well as individual tracklets and tracking point\ntrajectories. We compare our system against current object tracking approaches\nincluding OC-SORT, ByteTrack, and SORT. Our materials are available at:\nhttps://dat-nguyenvn.github.io/WildLive/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10165v2",
    "published_date": "2025-04-14 12:21:16 UTC",
    "updated_date": "2025-04-15 12:06:09 UTC"
  },
  {
    "arxiv_id": "2504.10160v1",
    "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning",
    "authors": [
      "Zhaopeng Feng",
      "Shaosheng Cao",
      "Jiahan Ren",
      "Jiayuan Su",
      "Ruizhe Chen",
      "Yan Zhang",
      "Zhe Xu",
      "Yao Hu",
      "Jian Wu",
      "Zuozhu Liu"
    ],
    "abstract": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress. Our code is available at\n  https://github.com/fzp0424/MT-R1-Zero",
    "pdf_url": "http://arxiv.org/pdf/2504.10160v1",
    "published_date": "2025-04-14 12:14:18 UTC",
    "updated_date": "2025-04-14 12:14:18 UTC"
  },
  {
    "arxiv_id": "2504.10158v1",
    "title": "COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts",
    "authors": [
      "Jiansheng Li",
      "Xingxuan Zhang",
      "Hao Zou",
      "Yige Guo",
      "Renzhe Xu",
      "Yilong Liu",
      "Chuzhao Zhu",
      "Yue He",
      "Peng Cui"
    ],
    "abstract": "Current object detectors often suffer significant perfor-mance degradation in\nreal-world applications when encountering distributional shifts. Consequently,\nthe out-of-distribution (OOD) generalization capability of object detectors has\ngarnered increasing attention from researchers. Despite this growing interest,\nthere remains a lack of a large-scale, comprehensive dataset and evaluation\nbenchmark with fine-grained annotations tailored to assess the OOD\ngeneralization on more intricate tasks like object detection and grounding. To\naddress this gap, we introduce COUNTS, a large-scale OOD dataset with\nobject-level annotations. COUNTS encompasses 14 natural distributional shifts,\nover 222K samples, and more than 1,196K labeled bounding boxes. Leveraging\nCOUNTS, we introduce two novel benchmarks: O(OD)2 and OODG. O(OD)2 is designed\nto comprehensively evaluate the OOD generalization capabilities of object\ndetectors by utilizing controlled distribution shifts between training and\ntesting data. OODG, on the other hand, aims to assess the OOD generalization of\ngrounding abilities in multimodal large language models (MLLMs). Our findings\nreveal that, while large models and extensive pre-training data substantially\nen hance performance in in-distribution (IID) scenarios, significant\nlimitations and opportunities for improvement persist in OOD contexts for both\nobject detectors and MLLMs. In visual grounding tasks, even the advanced GPT-4o\nand Gemini-1.5 only achieve 56.7% and 28.0% accuracy, respectively. We hope\nCOUNTS facilitates advancements in the development and assessment of robust\nobject detectors and MLLMs capable of maintaining high performance under\ndistributional shifts.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10158v1",
    "published_date": "2025-04-14 12:13:33 UTC",
    "updated_date": "2025-04-14 12:13:33 UTC"
  },
  {
    "arxiv_id": "2504.10149v1",
    "title": "BoTTA: Benchmarking on-device Test Time Adaptation",
    "authors": [
      "Michal Danilowski",
      "Soumyajit Chatterjee",
      "Abhirup Ghosh"
    ],
    "abstract": "The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10149v1",
    "published_date": "2025-04-14 12:00:00 UTC",
    "updated_date": "2025-04-14 12:00:00 UTC"
  },
  {
    "arxiv_id": "2504.10146v1",
    "title": "GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions",
    "authors": [
      "Jo-Ku Cheng",
      "Zeren Zhang",
      "Ran Chen",
      "Jingyang Deng",
      "Ziran Qin",
      "Jinwen Ma"
    ],
    "abstract": "We propose GeoUni, the first unified geometry expert model capable of\ngenerating problem solutions and diagrams within a single framework in a way\nthat enables the creation of unique and individualized geometry problems.\nTraditionally, solving geometry problems and generating diagrams have been\ntreated as separate tasks in machine learning, with no models successfully\nintegrating both to support problem creation. However, we believe that mastery\nin geometry requires frictionless integration of all of these skills, from\nsolving problems to visualizing geometric relationships, and finally, crafting\ntailored problems. Our extensive experiments demonstrate that GeoUni, with only\n1.5B parameters, achieves performance comparable to larger models such as\nDeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also\nexcels in generating precise geometric diagrams, surpassing both text-to-image\nmodels and unified models, including the GPT-4o image generation. Most\nimportantly, GeoUni is the only model capable of successfully generating\ntextual problems with matching diagrams based on specific knowledge points,\nthus offering a wider range of capabilities that extend beyond current models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10146v1",
    "published_date": "2025-04-14 11:56:55 UTC",
    "updated_date": "2025-04-14 11:56:55 UTC"
  },
  {
    "arxiv_id": "2504.10127v2",
    "title": "Breaking the Data Barrier -- Building GUI Agents Through Task Generalization",
    "authors": [
      "Junlei Zhang",
      "Zichen Ding",
      "Chang Ma",
      "Zijie Chen",
      "Qiushi Sun",
      "Zhenzhong Lan",
      "Junxian He"
    ],
    "abstract": "Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.10127v2",
    "published_date": "2025-04-14 11:35:02 UTC",
    "updated_date": "2025-04-15 17:13:46 UTC"
  },
  {
    "arxiv_id": "2504.10112v1",
    "title": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design",
    "authors": [
      "Andreas Happe",
      "Jürgen Cito"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. This paper analyzes the methodology and\nbenchmarking practices used for evaluating Large Language Model (LLM)-driven\nattacks, focusing on offensive uses of LLMs in cybersecurity. We review 16\nresearch papers detailing 15 prototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10112v1",
    "published_date": "2025-04-14 11:21:33 UTC",
    "updated_date": "2025-04-14 11:21:33 UTC"
  },
  {
    "arxiv_id": "2504.10109v1",
    "title": "Lightweight Trustworthy Distributed Clustering",
    "authors": [
      "Hongyang Li",
      "Caesar Wu",
      "Mohammed Chadli",
      "Said Mammar",
      "Pascal Bouvry"
    ],
    "abstract": "Ensuring data trustworthiness within individual edge nodes while facilitating\ncollaborative data processing poses a critical challenge in edge computing\nsystems (ECS), particularly in resource-constrained scenarios such as\nautonomous systems sensor networks, industrial IoT, and smart cities. This\npaper presents a lightweight, fully distributed k-means clustering algorithm\nspecifically adapted for edge environments, leveraging a distributed averaging\napproach with additive secret sharing, a secure multiparty computation\ntechnique, during the cluster center update phase to ensure the accuracy and\ntrustworthiness of data across nodes.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10109v1",
    "published_date": "2025-04-14 11:16:07 UTC",
    "updated_date": "2025-04-14 11:16:07 UTC"
  },
  {
    "arxiv_id": "2504.10106v1",
    "title": "SoccerNet-v3D: Leveraging Sports Broadcast Replays for 3D Scene Understanding",
    "authors": [
      "Marc Gutiérrez-Pérez",
      "Antonio Agudo"
    ],
    "abstract": "Sports video analysis is a key domain in computer vision, enabling detailed\nspatial understanding through multi-view correspondences. In this work, we\nintroduce SoccerNet-v3D and ISSIA-3D, two enhanced and scalable datasets\ndesigned for 3D scene understanding in soccer broadcast analysis. These\ndatasets extend SoccerNet-v3 and ISSIA by incorporating field-line-based camera\ncalibration and multi-view synchronization, enabling 3D object localization\nthrough triangulation. We propose a monocular 3D ball localization task built\nupon the triangulation of ground-truth 2D ball annotations, along with several\ncalibration and reprojection metrics to assess annotation quality on demand.\nAdditionally, we present a single-image 3D ball localization method as a\nbaseline, leveraging camera calibration and ball size priors to estimate the\nball's position from a monocular viewpoint. To further refine 2D annotations,\nwe introduce a bounding box optimization technique that ensures alignment with\nthe 3D scene representation. Our proposed datasets establish new benchmarks for\n3D soccer scene understanding, enhancing both spatial and temporal analysis in\nsports analytics. Finally, we provide code to facilitate access to our\nannotations and the generation pipelines for the datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2; I.4; I.5"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10106v1",
    "published_date": "2025-04-14 11:15:13 UTC",
    "updated_date": "2025-04-14 11:15:13 UTC"
  },
  {
    "arxiv_id": "2504.10081v1",
    "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability",
    "authors": [
      "Yichi Zhang",
      "Zihao Zeng",
      "Dongbai Li",
      "Yao Huang",
      "Zhijie Deng",
      "Yinpeng Dong"
    ],
    "abstract": "Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been\nrapidly progressing and achieving breakthrough performance on complex reasoning\ntasks such as mathematics and coding. However, the open-source R1 models have\nraised safety concerns in wide applications, such as the tendency to comply\nwith malicious queries, which greatly impacts the utility of these powerful\nmodels in their applications. In this paper, we introduce RealSafe-R1 as\nsafety-aligned versions of DeepSeek-R1 distilled models. To train these models,\nwe construct a dataset of 15k safety-aware reasoning trajectories generated by\nDeepSeek-R1, under explicit instructions for expected refusal behavior. Both\nquantitative experiments and qualitative case studies demonstrate the models'\nimprovements, which are shown in their safety guardrails against both harmful\nqueries and jailbreak attacks. Importantly, unlike prior safety alignment\nefforts that often compromise reasoning performance, our method preserves the\nmodels' reasoning capabilities by maintaining the training data within the\noriginal distribution of generation. Model weights of RealSafe-R1 are\nopen-source at https://huggingface.co/RealSafe.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10081v1",
    "published_date": "2025-04-14 10:26:37 UTC",
    "updated_date": "2025-04-14 10:26:37 UTC"
  },
  {
    "arxiv_id": "2504.10077v1",
    "title": "Towards Quantifying Commonsense Reasoning with Mechanistic Insights",
    "authors": [
      "Abhinav Joshi",
      "Areeb Ahmad",
      "Divyaksh Shukla",
      "Ashutosh Modi"
    ],
    "abstract": "Commonsense reasoning deals with the implicit knowledge that is well\nunderstood by humans and typically acquired via interactions with the world. In\nrecent times, commonsense reasoning and understanding of various LLMs have been\nevaluated using text-based tasks. In this work, we argue that a proxy of this\nunderstanding can be maintained as a graphical structure that can further help\nto perform a rigorous evaluation of commonsense reasoning abilities about\nvarious real-world activities. We create an annotation scheme for capturing\nthis implicit knowledge in the form of a graphical structure for 37 daily human\nactivities. We find that the created resource can be used to frame an enormous\nnumber of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of\ncommonsense reasoning in LLMs. Moreover, recently, the remarkable performance\nof LLMs has raised questions about whether these models are truly capable of\nreasoning in the wild and, in general, how reasoning occurs inside these\nmodels. In this resource paper, we bridge this gap by proposing design\nmechanisms that facilitate research in a similar direction. Our findings\nsuggest that the reasoning components are localized in LLMs that play a\nprominent role in decision-making when prompted with a commonsense query.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NAACL 2025; 28 pages (9 pages + 7 pages references + 12\n  pages appendix)",
    "pdf_url": "http://arxiv.org/pdf/2504.10077v1",
    "published_date": "2025-04-14 10:21:59 UTC",
    "updated_date": "2025-04-14 10:21:59 UTC"
  },
  {
    "arxiv_id": "2504.10074v2",
    "title": "MMKB-RAG: A Multi-Modal Knowledge-Based Retrieval-Augmented Generation Framework",
    "authors": [
      "Zihan Ling",
      "Zhiyao Guo",
      "Yixuan Huang",
      "Yi An",
      "Shuai Xiao",
      "Jinsong Lan",
      "Xiaoyong Zhu",
      "Bo Zheng"
    ],
    "abstract": "Recent advancements in large language models (LLMs) and multi-modal LLMs have\nbeen remarkable. However, these models still rely solely on their parametric\nknowledge, which limits their ability to generate up-to-date information and\nincreases the risk of producing erroneous content. Retrieval-Augmented\nGeneration (RAG) partially mitigates these challenges by incorporating external\ndata sources, yet the reliance on databases and retrieval systems can introduce\nirrelevant or inaccurate documents, ultimately undermining both performance and\nreasoning quality. In this paper, we propose Multi-Modal Knowledge-Based\nRetrieval-Augmented Generation (MMKB-RAG), a novel multi-modal RAG framework\nthat leverages the inherent knowledge boundaries of models to dynamically\ngenerate semantic tags for the retrieval process. This strategy enables the\njoint filtering of retrieved documents, retaining only the most relevant and\naccurate references. Extensive experiments on knowledge-based visual\nquestion-answering tasks demonstrate the efficacy of our approach: on the E-VQA\ndataset, our method improves performance by +4.2% on the Single-Hop subset and\n+0.4% on the full dataset, while on the InfoSeek dataset, it achieves gains of\n+7.8% on the Unseen-Q subset, +8.2% on the Unseen-E subset, and +8.1% on the\nfull dataset. These results highlight significant enhancements in both accuracy\nand robustness over the current state-of-the-art MLLM and RAG frameworks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10074v2",
    "published_date": "2025-04-14 10:19:47 UTC",
    "updated_date": "2025-04-15 06:19:00 UTC"
  },
  {
    "arxiv_id": "2504.10071v1",
    "title": "Pay Attention to What and Where? Interpretable Feature Extractor in Vision-based Deep Reinforcement Learning",
    "authors": [
      "Tien Pham",
      "Angelo Cangelosi"
    ],
    "abstract": "Current approaches in Explainable Deep Reinforcement Learning have\nlimitations in which the attention mask has a displacement with the objects in\nvisual input. This work addresses a spatial problem within traditional\nConvolutional Neural Networks (CNNs). We propose the Interpretable Feature\nExtractor (IFE) architecture, aimed at generating an accurate attention mask to\nillustrate both \"what\" and \"where\" the agent concentrates on in the spatial\ndomain. Our design incorporates a Human-Understandable Encoding module to\ngenerate a fully interpretable attention mask, followed by an Agent-Friendly\nEncoding module to enhance the agent's learning efficiency. These two\ncomponents together form the Interpretable Feature Extractor for vision-based\ndeep reinforcement learning to enable the model's interpretability. The\nresulting attention mask is consistent, highly understandable by humans,\naccurate in spatial dimension, and effectively highlights important objects or\nlocations in visual input. The Interpretable Feature Extractor is integrated\ninto the Fast and Data-efficient Rainbow framework, and evaluated on 57 ATARI\ngames to show the effectiveness of the proposed approach on Spatial\nPreservation, Interpretability, and Data-efficiency. Finally, we showcase the\nversatility of our approach by incorporating the IFE into the Asynchronous\nAdvantage Actor-Critic Model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10071v1",
    "published_date": "2025-04-14 10:18:34 UTC",
    "updated_date": "2025-04-14 10:18:34 UTC"
  },
  {
    "arxiv_id": "2504.10068v1",
    "title": "Mavors: Multi-granularity Video Representation for Multimodal Large Language Model",
    "authors": [
      "Yang Shi",
      "Jiaheng Liu",
      "Yushuo Guan",
      "Zhenhua Wu",
      "Yuanxing Zhang",
      "Zihao Wang",
      "Weihong Lin",
      "Jingyun Hua",
      "Zekun Wang",
      "Xinlong Chen",
      "Bohan Zeng",
      "Wentao Zhang",
      "Fuzheng Zhang",
      "Wenjing Yang",
      "Di Zhang"
    ],
    "abstract": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose $\\mathbf{Mavors}$, a novel framework\nthat introduces $\\mathbf{M}$ulti-gr$\\mathbf{a}$nularity\n$\\mathbf{v}$ide$\\mathbf{o}$ $\\mathbf{r}$epre$\\mathbf{s}$entation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.10068v1",
    "published_date": "2025-04-14 10:14:44 UTC",
    "updated_date": "2025-04-14 10:14:44 UTC"
  },
  {
    "arxiv_id": "2504.10063v1",
    "title": "Hallucination Detection in LLMs via Topological Divergence on Attention Graphs",
    "authors": [
      "Alexandra Bazarova",
      "Aleksandr Yugay",
      "Andrey Shulga",
      "Alina Ermilova",
      "Andrei Volodichev",
      "Konstantin Polev",
      "Julia Belikova",
      "Rauf Parchiev",
      "Dmitry Simakov",
      "Maxim Savchenko",
      "Andrey Savchenko",
      "Serguei Barannikov",
      "Alexey Zaytsev"
    ],
    "abstract": "Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments, including evaluation on\nquestion answering and data-to-text tasks, show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks, two of which\nwere annotated by us and are being publicly released to facilitate further\nresearch. Beyond its strong in-domain performance, TOHA maintains remarkable\ndomain transferability across multiple open-source LLMs. Our findings suggest\nthat analyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10063v1",
    "published_date": "2025-04-14 10:06:27 UTC",
    "updated_date": "2025-04-14 10:06:27 UTC"
  },
  {
    "arxiv_id": "2504.10045v1",
    "title": "CHARM: Calibrating Reward Models With Chatbot Arena Scores",
    "authors": [
      "Xiao Zhu",
      "Chenmien Tan",
      "Pinzhen Chen",
      "Rico Sennrich",
      "Yanlin Zhang",
      "Hanxu Hu"
    ],
    "abstract": "Reward models (RMs) play a crucial role in Reinforcement Learning from Human\nFeedback by serving as proxies for human preferences in aligning large language\nmodels. In this paper, we identify a model preference bias in RMs, where they\nsystematically assign disproportionately high scores to responses from certain\npolicy models. This bias distorts ranking evaluations and leads to unfair\njudgments. To address this issue, we propose a calibration method named CHatbot\nArena calibrated Reward Modeling (CHARM) that leverages Elo scores from the\nChatbot Arena leaderboard to mitigate RM overvaluation. We also introduce a\nMismatch Degree metric to measure this preference bias. Our approach is\ncomputationally efficient, requiring only a small preference dataset for\ncontinued training of the RM. We conduct extensive experiments on reward model\nbenchmarks and human preference alignment. Results demonstrate that our\ncalibrated RMs (1) achieve improved evaluation accuracy on RM-Bench and the\nChat-Hard domain of RewardBench, and (2) exhibit a stronger correlation with\nhuman preferences by producing scores more closely aligned with Elo rankings.\nBy mitigating model preference bias, our method provides a generalizable and\nefficient solution for building fairer and more reliable reward models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10045v1",
    "published_date": "2025-04-14 09:51:09 UTC",
    "updated_date": "2025-04-14 09:51:09 UTC"
  },
  {
    "arxiv_id": "2504.10030v1",
    "title": "EmbodiedAgent: A Scalable Hierarchical Approach to Overcome Practical Challenge in Multi-Robot Control",
    "authors": [
      "Hanwen Wan",
      "Yifei Chen",
      "Zeyu Wei",
      "Dongrui Li",
      "Zexin Lin",
      "Donghao Wu",
      "Jiu Cheng",
      "Yuxiang Zhang",
      "Xiaoqiang Ji"
    ],
    "abstract": "This paper introduces EmbodiedAgent, a hierarchical framework for\nheterogeneous multi-robot control. EmbodiedAgent addresses critical limitations\nof hallucination in impractical tasks. Our approach integrates a next-action\nprediction paradigm with a structured memory system to decompose tasks into\nexecutable robot skills while dynamically validating actions against\nenvironmental constraints. We present MultiPlan+, a dataset of more than 18,000\nannotated planning instances spanning 100 scenarios, including a subset of\nimpractical cases to mitigate hallucination. To evaluate performance, we\npropose the Robot Planning Assessment Schema (RPAS), combining automated\nmetrics with LLM-aided expert grading. Experiments demonstrate EmbodiedAgent's\nsuperiority over state-of-the-art models, achieving 71.85% RPAS score.\nReal-world validation in an office service task highlights its ability to\ncoordinate heterogeneous robots for long-horizon objectives.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10030v1",
    "published_date": "2025-04-14 09:33:42 UTC",
    "updated_date": "2025-04-14 09:33:42 UTC"
  },
  {
    "arxiv_id": "2504.10028v1",
    "title": "Sequence models for by-trial decoding of cognitive strategies from neural data",
    "authors": [
      "Rick den Otter",
      "Gabriel Weindel",
      "Sjoerd Stuit",
      "Leendert van Maanen"
    ],
    "abstract": "Understanding the sequence of cognitive operations that underlie\ndecision-making is a fundamental challenge in cognitive neuroscience.\nTraditional approaches often rely on group-level statistics, which obscure\ntrial-by-trial variations in cognitive strategies. In this study, we introduce\na novel machine learning method that combines Hidden Multivariate Pattern\nanalysis with a Structured State Space Sequence model to decode cognitive\nstrategies from electroencephalography data at the trial level. We apply this\nmethod to a decision-making task, where participants were instructed to\nprioritize either speed or accuracy in their responses. Our results reveal an\nadditional cognitive operation, labeled Confirmation, which seems to occur\npredominantly in the accuracy condition but also frequently in the speed\ncondition. The modeled probability that this operation occurs is associated\nwith higher probability of responding correctly as well as changes of mind, as\nindexed by electromyography data. By successfully modeling cognitive operations\nat the trial level, we provide empirical evidence for dynamic variability in\ndecision strategies, challenging the assumption of homogeneous cognitive\nprocesses within experimental conditions. Our approach shows the potential of\nsequence modeling in cognitive neuroscience to capture trial-level variability\nthat is obscured by aggregate analyses. The introduced method offers a new way\nto detect and understand cognitive strategies in a data-driven manner, with\nimplications for both theoretical research and practical applications in many\nfields.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "15 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.10028v1",
    "published_date": "2025-04-14 09:33:02 UTC",
    "updated_date": "2025-04-14 09:33:02 UTC"
  },
  {
    "arxiv_id": "2504.10025v1",
    "title": "Progressive Transfer Learning for Multi-Pass Fundus Image Restoration",
    "authors": [
      "Uyen Phan",
      "Ozer Can Devecioglu",
      "Serkan Kiranyaz",
      "Moncef Gabbouj"
    ],
    "abstract": "Diabetic retinopathy is a leading cause of vision impairment, making its\nearly diagnosis through fundus imaging critical for effective treatment\nplanning. However, the presence of poor quality fundus images caused by factors\nsuch as inadequate illumination, noise, blurring and other motion artifacts\nyields a significant challenge for accurate DR screening. In this study, we\npropose progressive transfer learning for multi pass restoration to iteratively\nenhance the quality of degraded fundus images, ensuring more reliable DR\nscreening. Unlike previous methods that often focus on a single pass\nrestoration, multi pass restoration via PTL can achieve a superior blind\nrestoration performance that can even improve most of the good quality fundus\nimages in the dataset. Initially, a Cycle GAN model is trained to restore low\nquality images, followed by PTL induced restoration passes over the latest\nrestored outputs to improve overall quality in each pass. The proposed method\ncan learn blind restoration without requiring any paired data while surpassing\nits limitations by leveraging progressive learning and fine tuning strategies\nto minimize distortions and preserve critical retinal features. To evaluate\nPTL's effectiveness on multi pass restoration, we conducted experiments on\nDeepDRiD, a large scale fundus imaging dataset specifically curated for\ndiabetic retinopathy detection. Our result demonstrates state of the art\nperformance, showcasing PTL's potential as a superior approach to iterative\nimage quality restoration.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "13 pages, 12 figures including appendix",
    "pdf_url": "http://arxiv.org/pdf/2504.10025v1",
    "published_date": "2025-04-14 09:28:10 UTC",
    "updated_date": "2025-04-14 09:28:10 UTC"
  },
  {
    "arxiv_id": "2504.10020v1",
    "title": "The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination",
    "authors": [
      "Hao Yin",
      "Gunagzong Si",
      "Zilei Wang"
    ],
    "abstract": "Contrastive decoding strategies are widely used to reduce hallucinations in\nmultimodal large language models (MLLMs). These methods work by constructing\ncontrastive samples to induce hallucinations and then suppressing them in the\noutput distribution. However, this paper demonstrates that such approaches fail\nto effectively mitigate the hallucination problem. The performance improvements\nobserved on POPE Benchmark are largely driven by two misleading factors: (1)\ncrude, unidirectional adjustments to the model's output distribution and (2)\nthe adaptive plausibility constraint, which reduces the sampling strategy to\ngreedy search. To further illustrate these issues, we introduce a series of\nspurious improvement methods and evaluate their performance against contrastive\ndecoding techniques. Experimental results reveal that the observed performance\ngains in contrastive decoding are entirely unrelated to its intended goal of\nmitigating hallucinations. Our findings challenge common assumptions about the\neffectiveness of contrastive decoding strategies and pave the way for\ndeveloping genuinely effective solutions to hallucinations in MLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10020v1",
    "published_date": "2025-04-14 09:25:37 UTC",
    "updated_date": "2025-04-14 09:25:37 UTC"
  },
  {
    "arxiv_id": "2504.10018v1",
    "title": "RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset and An Asymmetric RWKV Fusion Framework",
    "authors": [
      "Xiao Wang",
      "Haiyang Wang",
      "Shiao Wang",
      "Qiang Chen",
      "Jiandong Jin",
      "Haoyu Song",
      "Bo Jiang",
      "Chenglong Li"
    ],
    "abstract": "Existing pedestrian attribute recognition methods are generally developed\nbased on RGB frame cameras. However, these approaches are constrained by the\nlimitations of RGB cameras, such as sensitivity to lighting conditions and\nmotion blur, which hinder their performance. Furthermore, current attribute\nrecognition primarily focuses on analyzing pedestrians' external appearance and\nclothing, lacking an exploration of emotional dimensions. In this paper, we\nrevisit these issues and propose a novel multi-modal RGB-Event attribute\nrecognition task by drawing inspiration from the advantages of event cameras in\nlow-light, high-speed, and low-power consumption. Specifically, we introduce\nthe first large-scale multi-modal pedestrian attribute recognition dataset,\ntermed EventPAR, comprising 100K paired RGB-Event samples that cover 50\nattributes related to both appearance and six human emotions, diverse scenes,\nand various seasons. By retraining and evaluating mainstream PAR models on this\ndataset, we establish a comprehensive benchmark and provide a solid foundation\nfor future research in terms of data and algorithmic baselines. In addition, we\npropose a novel RWKV-based multi-modal pedestrian attribute recognition\nframework, featuring an RWKV visual encoder and an asymmetric RWKV fusion\nmodule. Extensive experiments are conducted on our proposed dataset as well as\ntwo simulated datasets (MARS-Attribute and DukeMTMC-VID-Attribute), achieving\nstate-of-the-art results. The source code and dataset will be released on\nhttps://github.com/Event-AHU/OpenPAR",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The First Benchmark Dataset for RGB-Event Multimodal Pedestrian\n  Attribute Recognition Task",
    "pdf_url": "http://arxiv.org/pdf/2504.10018v1",
    "published_date": "2025-04-14 09:22:16 UTC",
    "updated_date": "2025-04-14 09:22:16 UTC"
  },
  {
    "arxiv_id": "2504.10014v1",
    "title": "Air Quality Prediction with A Meteorology-Guided Modality-Decoupled Spatio-Temporal Network",
    "authors": [
      "Hang Yin",
      "Yan-Ming Zhang",
      "Jian Xu",
      "Jian-Long Chang",
      "Yin Li",
      "Cheng-Lin Liu"
    ],
    "abstract": "Air quality prediction plays a crucial role in public health and\nenvironmental protection. Accurate air quality prediction is a complex\nmultivariate spatiotemporal problem, that involves interactions across temporal\npatterns, pollutant correlations, spatial station dependencies, and\nparticularly meteorological influences that govern pollutant dispersion and\nchemical transformations. Existing works underestimate the critical role of\natmospheric conditions in air quality prediction and neglect comprehensive\nmeteorological data utilization, thereby impairing the modeling of dynamic\ninterdependencies between air quality and meteorological data. To overcome\nthis, we propose MDSTNet, an encoder-decoder framework that explicitly models\nair quality observations and atmospheric conditions as distinct modalities,\nintegrating multi-pressure-level meteorological data and weather forecasts to\ncapture atmosphere-pollution dependencies for prediction. Meantime, we\nconstruct ChinaAirNet, the first nationwide dataset combining air quality\nrecords with multi-pressure-level meteorological observations. Experimental\nresults on ChinaAirNet demonstrate MDSTNet's superiority, substantially\nreducing 48-hour prediction errors by 17.54\\% compared to the state-of-the-art\nmodel. The source code and dataset will be available on github.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10014v1",
    "published_date": "2025-04-14 09:18:11 UTC",
    "updated_date": "2025-04-14 09:18:11 UTC"
  },
  {
    "arxiv_id": "2504.10005v1",
    "title": "Session-based Recommender Systems: User Interest as a Stochastic Process in the Latent Space",
    "authors": [
      "Klaudia Balcer",
      "Piotr Lipinski"
    ],
    "abstract": "This paper jointly addresses the problem of data uncertainty, popularity\nbias, and exposure bias in session-based recommender systems. We study the\nsymptoms of this bias both in item embeddings and in recommendations. We\npropose treating user interest as a stochastic process in the latent space and\nproviding a model-agnostic implementation of this mathematical concept. The\nproposed stochastic component consists of elements: debiasing item embeddings\nwith regularization for embedding uniformity, modeling dense user interest from\nsession prefixes, and introducing fake targets in the data to simulate extended\nexposure. We conducted computational experiments on two popular benchmark\ndatasets, Diginetica and YooChoose 1/64, as well as several modifications of\nthe YooChoose dataset with different ratios of popular items. The results show\nthat the proposed approach allows us to mitigate the challenges mentioned.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10005v1",
    "published_date": "2025-04-14 09:08:40 UTC",
    "updated_date": "2025-04-14 09:08:40 UTC"
  },
  {
    "arxiv_id": "2504.10000v1",
    "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?",
    "authors": [
      "Yanbo Wang",
      "Jiyang Guan",
      "Jian Liang",
      "Ran He"
    ],
    "abstract": "Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to CVPR 2025, codes in process",
    "pdf_url": "http://arxiv.org/pdf/2504.10000v1",
    "published_date": "2025-04-14 09:03:51 UTC",
    "updated_date": "2025-04-14 09:03:51 UTC"
  },
  {
    "arxiv_id": "2504.09998v1",
    "title": "Metric-Guided Synthesis of Class Activation Mapping",
    "authors": [
      "Alejandro Luque-Cerpa",
      "Elizabeth Polgreen",
      "Ajitha Rajan",
      "Hazem Torfah"
    ],
    "abstract": "Class activation mapping (CAM) is a widely adopted class of saliency methods\nused to explain the behavior of convolutional neural networks (CNNs). These\nmethods generate heatmaps that highlight the parts of the input most relevant\nto the CNN output. Various CAM methods have been proposed, each distinguished\nby the expressions used to derive heatmaps. In general, users look for heatmaps\nwith specific properties that reflect different aspects of CNN functionality.\nThese may include similarity to ground truth, robustness, equivariance, and\nmore. Although existing CAM methods implicitly encode some of these properties\nin their expressions, they do not allow for variability in heatmap generation\nfollowing the user's intent or domain knowledge. In this paper, we address this\nlimitation by introducing SyCAM, a metric-based approach for synthesizing CAM\nexpressions. Given a predefined evaluation metric for saliency maps, SyCAM\nautomatically generates CAM expressions optimized for that metric. We\nspecifically explore a syntax-guided synthesis instantiation of SyCAM, where\nCAM expressions are derived based on predefined syntactic constraints and the\ngiven metric. Using several established evaluation metrics, we demonstrate the\nefficacy and flexibility of our approach in generating targeted heatmaps. We\ncompare SyCAM with other well-known CAM methods on three prominent models:\nResNet50, VGG16, and VGG19.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09998v1",
    "published_date": "2025-04-14 09:01:49 UTC",
    "updated_date": "2025-04-14 09:01:49 UTC"
  },
  {
    "arxiv_id": "2504.09997v1",
    "title": "GenTe: Generative Real-world Terrains for General Legged Robot Locomotion Control",
    "authors": [
      "Hanwen Wan",
      "Mengkang Li",
      "Donghao Wu",
      "Yebin Zhong",
      "Yixuan Deng",
      "Zhenglong Sun",
      "Xiaoqiang Ji"
    ],
    "abstract": "Developing bipedal robots capable of traversing diverse real-world terrains\npresents a fundamental robotics challenge, as existing methods using predefined\nheight maps and static environments fail to address the complexity of\nunstructured landscapes. To bridge this gap, we propose GenTe, a framework for\ngenerating physically realistic and adaptable terrains to train generalizable\nlocomotion policies. GenTe constructs an atomic terrain library that includes\nboth geometric and physical terrains, enabling curriculum training for\nreinforcement learning-based locomotion policies. By leveraging\nfunction-calling techniques and reasoning capabilities of Vision-Language\nModels (VLMs), GenTe generates complex, contextually relevant terrains from\ntextual and graphical inputs. The framework introduces realistic force modeling\nfor terrain interactions, capturing effects such as soil sinkage and\nhydrodynamic resistance. To the best of our knowledge, GenTe is the first\nframework that systemically generates simulation environments for legged robot\nlocomotion control. Additionally, we introduce a benchmark of 100 generated\nterrains. Experiments demonstrate improved generalization and robustness in\nbipedal robot locomotion.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09997v1",
    "published_date": "2025-04-14 09:01:44 UTC",
    "updated_date": "2025-04-14 09:01:44 UTC"
  },
  {
    "arxiv_id": "2504.09967v1",
    "title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data",
    "authors": [
      "Xun Zhu",
      "Fanbin Mo",
      "Zheng Zhang",
      "Jiaxi Wang",
      "Yiming Shi",
      "Ming Wu",
      "Chuang Zhang",
      "Miao Li",
      "Ji Wu"
    ],
    "abstract": "The emergence of medical generalist foundation models has revolutionized\nconventional task-specific model development paradigms, aiming to better handle\nmultiple tasks through joint training on large-scale medical datasets. However,\nrecent advances prioritize simple data scaling or architectural component\nenhancement, while neglecting to re-examine multi-task learning from a\ndata-centric perspective. Critically, simply aggregating existing data\nresources leads to decentralized image-task alignment, which fails to cultivate\ncomprehensive image understanding or align with clinical needs for\nmulti-dimensional image interpretation. In this paper, we introduce the\nimage-centric multi-annotation X-ray dataset (IMAX), the first attempt to\nenhance the multi-task learning capabilities of medical multi-modal large\nlanguage models (MLLMs) from the data construction level. To be specific, IMAX\nis featured from the following attributes: 1) High-quality data curation. A\ncomprehensive collection of more than 354K entries applicable to seven\ndifferent medical tasks. 2) Image-centric dense annotation. Each X-ray image is\nassociated with an average of 4.10 tasks and 7.46 training entries, ensuring\nmulti-task representation richness per image. Compared to the general\ndecentralized multi-annotation X-ray dataset (DMAX), IMAX consistently\ndemonstrates significant multi-task average performance gains ranging from\n3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs.\nMoreover, we investigate differences in statistical patterns exhibited by IMAX\nand DMAX training processes, exploring potential correlations between\noptimization dynamics and multi-task performance. Finally, leveraging the core\nconcept of IMAX data construction, we propose an optimized DMAX-based training\nstrategy to alleviate the dilemma of obtaining high-quality IMAX data in\npractical scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09967v1",
    "published_date": "2025-04-14 08:09:37 UTC",
    "updated_date": "2025-04-14 08:09:37 UTC"
  },
  {
    "arxiv_id": "2504.09963v1",
    "title": "Towards Unbiased Federated Graph Learning: Label and Topology Perspectives",
    "authors": [
      "Zhengyu Wu",
      "Boyang Pang",
      "Xunkai Li",
      "Yinlin Zhu",
      "Daohan Su",
      "Bowen Fan",
      "Rong-Hua Li",
      "Guoren Wang",
      "Chenghu Zhou"
    ],
    "abstract": "Federated Graph Learning (FGL) enables privacy-preserving, distributed\ntraining of graph neural networks without sharing raw data. Among its\napproaches, subgraph-FL has become the dominant paradigm, with most work\nfocused on improving overall node classification accuracy. However, these\nmethods often overlook fairness due to the complexity of node features, labels,\nand graph structures. In particular, they perform poorly on nodes with\ndisadvantaged properties, such as being in the minority class within subgraphs\nor having heterophilous connections (neighbors with dissimilar labels or\nmisleading features). This reveals a critical issue: high accuracy can mask\ndegraded performance on structurally or semantically marginalized nodes. To\naddress this, we advocate for two fairness goals: (1) improving representation\nof minority class nodes for class-wise fairness and (2) mitigating topological\nbias from heterophilous connections for topology-aware fairness. We propose\nFairFGL, a novel framework that enhances fairness through fine-grained graph\nmining and collaborative learning. On the client side, the History-Preserving\nModule prevents overfitting to dominant local classes, while the Majority\nAlignment Module refines representations of heterophilous majority-class nodes.\nThe Gradient Modification Module transfers minority-class knowledge from\nstructurally favorable clients to improve fairness. On the server side, FairFGL\nuploads only the most influenced subset of parameters to reduce communication\ncosts and better reflect local distributions. A cluster-based aggregation\nstrategy reconciles conflicting updates and curbs global majority dominance .\nExtensive evaluations on eight benchmarks show FairFGL significantly improves\nminority-group performance , achieving up to a 22.62 percent Macro-F1 gain\nwhile enhancing convergence over state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2504.09963v1",
    "published_date": "2025-04-14 08:00:20 UTC",
    "updated_date": "2025-04-14 08:00:20 UTC"
  },
  {
    "arxiv_id": "2504.09961v1",
    "title": "Privacy Meets Explainability: Managing Confidential Data and Transparency Policies in LLM-Empowered Science",
    "authors": [
      "Yashothara Shanmugarasa",
      "Shidong Pan",
      "Ming Ding",
      "Dehai Zhao",
      "Thierry Rakotoarivelo"
    ],
    "abstract": "As Large Language Models (LLMs) become integral to scientific workflows,\nconcerns over the confidentiality and ethical handling of confidential data\nhave emerged. This paper explores data exposure risks through LLM-powered\nscientific tools, which can inadvertently leak confidential information,\nincluding intellectual property and proprietary data, from scientists'\nperspectives. We propose \"DataShield\", a framework designed to detect\nconfidential data leaks, summarize privacy policies, and visualize data flow,\nensuring alignment with organizational policies and procedures. Our approach\naims to inform scientists about data handling practices, enabling them to make\ninformed decisions and protect sensitive information. Ongoing user studies with\nscientists are underway to evaluate the framework's usability, trustworthiness,\nand effectiveness in tackling real-world privacy challenges.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.09961v1",
    "published_date": "2025-04-14 07:58:26 UTC",
    "updated_date": "2025-04-14 07:58:26 UTC"
  },
  {
    "arxiv_id": "2504.09948v1",
    "title": "Omni-Dish: Photorealistic and Faithful Image Generation and Editing for Arbitrary Chinese Dishes",
    "authors": [
      "Huijie Liu",
      "Bingcan Wang",
      "Jie Hu",
      "Xiaoming Wei",
      "Guoliang Kang"
    ],
    "abstract": "Dish images play a crucial role in the digital era, with the demand for\nculturally distinctive dish images continuously increasing due to the\ndigitization of the food industry and e-commerce. In general cases, existing\ntext-to-image generation models excel in producing high-quality images;\nhowever, they struggle to capture diverse characteristics and faithful details\nof specific domains, particularly Chinese dishes. To address this limitation,\nwe propose Omni-Dish, the first text-to-image generation model specifically\ntailored for Chinese dishes. We develop a comprehensive dish curation pipeline,\nbuilding the largest dish dataset to date. Additionally, we introduce a\nrecaption strategy and employ a coarse-to-fine training scheme to help the\nmodel better learn fine-grained culinary nuances. During inference, we enhance\nthe user's textual input using a pre-constructed high-quality caption library\nand a large language model, enabling more photorealistic and faithful image\ngeneration. Furthermore, to extend our model's capability for dish editing\ntasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish\nediting dataset and train a specialized editing model. Extensive experiments\ndemonstrate the superiority of our methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 10 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.09948v1",
    "published_date": "2025-04-14 07:18:32 UTC",
    "updated_date": "2025-04-14 07:18:32 UTC"
  },
  {
    "arxiv_id": "2504.09941v1",
    "title": "FedRecon: Missing Modality Reconstruction in Distributed Heterogeneous Environments",
    "authors": [
      "Junming Liu",
      "Guosun Zeng",
      "Ding Wang",
      "Yanting Gao",
      "Yufei Jin"
    ],
    "abstract": "Multimodal data are often incomplete and exhibit Non-Independent and\nIdentically Distributed (Non-IID) characteristics in real-world scenarios.\nThese inherent limitations lead to both modality heterogeneity through partial\nmodality absence and data heterogeneity from distribution divergence, creating\nfundamental challenges for effective federated learning (FL). To address these\ncoupled challenges, we propose FedRecon, the first method targeting\nsimultaneous missing modality reconstruction and Non-IID adaptation in\nmultimodal FL. Our approach first employs a lightweight Multimodal Variational\nAutoencoder (MVAE) to reconstruct missing modalities while preserving\ncross-modal consistency. Distinct from conventional imputation methods, we\nachieve sample-level alignment through a novel distribution mapping mechanism\nthat guarantees both data consistency and completeness. Additionally, we\nintroduce a strategy employing global generator freezing to prevent\ncatastrophic forgetting, which in turn mitigates Non-IID fluctuations.\nExtensive evaluations on multimodal datasets demonstrate FedRecon's superior\nperformance in modality reconstruction under Non-IID conditions, surpassing\nstate-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 32 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.09941v1",
    "published_date": "2025-04-14 07:04:10 UTC",
    "updated_date": "2025-04-14 07:04:10 UTC"
  },
  {
    "arxiv_id": "2504.09936v1",
    "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference",
    "authors": [
      "Yuxuan Tian",
      "Zihan Wang",
      "Yebo Peng",
      "Aomufei Yuan",
      "Zhiming Wang",
      "Bairen Yi",
      "Xin Liu",
      "Yong Cui",
      "Tong Yang"
    ],
    "abstract": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.09936v1",
    "published_date": "2025-04-14 06:58:00 UTC",
    "updated_date": "2025-04-14 06:58:00 UTC"
  },
  {
    "arxiv_id": "2504.09909v1",
    "title": "Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications",
    "authors": [
      "Farha Nausheen",
      "Khandakar Ahmed",
      "M Imad Khan"
    ],
    "abstract": "In recent developments, deep learning methodologies applied to Natural\nLanguage Processing (NLP) have revealed a paradox: They improve performance but\ndemand considerable data and resources for their training. Alternatively,\nquantum computing exploits the principles of quantum mechanics to overcome the\ncomputational limitations of current methodologies, thereby establishing an\nemerging field known as quantum natural language processing (QNLP). This domain\nholds the potential to attain a quantum advantage in the processing of\nlinguistic structures, surpassing classical models in both efficiency and\naccuracy. In this paper, it is proposed to categorise QNLP models based on\nquantum computing principles, architecture, and computational approaches. This\npaper attempts to provide a survey on how quantum meets language by mapping\nstate-of-the-art in this area, embracing quantum encoding techniques for\nclassical data, QNLP models for prevalent NLP tasks, and quantum optimisation\ntechniques for hyper parameter tuning. The landscape of quantum computing\napproaches applied to various NLP tasks is summarised by showcasing the\nspecific QNLP methods used, and the popularity of these methods is indicated by\ntheir count. From the findings, it is observed that QNLP approaches are still\nlimited to small data sets, with only a few models explored extensively, and\nthere is increasing interest in the application of quantum computing to natural\nlanguage processing tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09909v1",
    "published_date": "2025-04-14 06:09:26 UTC",
    "updated_date": "2025-04-14 06:09:26 UTC"
  },
  {
    "arxiv_id": "2504.09906v1",
    "title": "Plasticity-Aware Mixture of Experts for Learning Under QoE Shifts in Adaptive Video Streaming",
    "authors": [
      "Zhiqiang He",
      "Zhi Liu"
    ],
    "abstract": "Adaptive video streaming systems are designed to optimize Quality of\nExperience (QoE) and, in turn, enhance user satisfaction. However, differences\nin user profiles and video content lead to different weights for QoE factors,\nresulting in user-specific QoE functions and, thus, varying optimization\nobjectives. This variability poses significant challenges for neural networks,\nas they often struggle to generalize under evolving targets - a phenomenon\nknown as plasticity loss that prevents conventional models from adapting\neffectively to changing optimization objectives. To address this limitation, we\npropose the Plasticity-Aware Mixture of Experts (PA-MoE), a novel learning\nframework that dynamically modulates network plasticity by balancing memory\nretention with selective forgetting. In particular, PA-MoE leverages noise\ninjection to promote the selective forgetting of outdated knowledge, thereby\nendowing neural networks with enhanced adaptive capabilities. In addition, we\npresent a rigorous theoretical analysis of PA-MoE by deriving a regret bound\nthat quantifies its learning performance. Experimental evaluations demonstrate\nthat PA-MoE achieves a 45.5% improvement in QoE over competitive baselines in\ndynamic streaming environments. Further analysis reveals that the model\neffectively mitigates plasticity loss by optimizing neuron utilization.\nFinally, a parameter sensitivity study is performed by injecting varying levels\nof noise, and the results align closely with our theoretical predictions.",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09906v1",
    "published_date": "2025-04-14 06:02:41 UTC",
    "updated_date": "2025-04-14 06:02:41 UTC"
  },
  {
    "arxiv_id": "2504.09895v1",
    "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data",
    "authors": [
      "Shuai Zhao",
      "Linchao Zhu",
      "Yi Yang"
    ],
    "abstract": "Large language models~(LLMs) are expected to be helpful, harmless, and\nhonest. In various alignment scenarios, such as general human preference,\nsafety, and confidence alignment, binary preference data collection and reward\nmodeling are resource-intensive but necessary for human preference\ntransferring. In this work, we explore using the similarity between sampled\ngenerations and high-quality reference answers as an alternative reward\nfunction for LLM alignment. Using similarity as a reward circumvents training\nreward models, and collecting a single reference answer potentially costs less\ntime than constructing binary preference pairs when multiple candidates are\navailable. Specifically, we develop \\textit{RefAlign}, a versatile\nREINFORCE-style alignment algorithm, which is free of reference and reward\nmodels. Instead, RefAlign utilizes BERTScore between sampled generations and\nhigh-quality reference answers as the surrogate reward. Beyond general human\npreference optimization, RefAlign can be readily extended to diverse scenarios,\nsuch as safety and confidence alignment, by incorporating the similarity reward\nwith task-related objectives. In various scenarios, {RefAlign} demonstrates\ncomparable performance to previous alignment methods while offering high\nefficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "work in progress",
    "pdf_url": "http://arxiv.org/pdf/2504.09895v1",
    "published_date": "2025-04-14 05:43:21 UTC",
    "updated_date": "2025-04-14 05:43:21 UTC"
  },
  {
    "arxiv_id": "2504.09893v1",
    "title": "LangPert: Detecting and Handling Task-level Perturbations for Robust Object Rearrangement",
    "authors": [
      "Xu Yin",
      "Min-Sung Yoon",
      "Yuchi Huo",
      "Kang Zhang",
      "Sung-Eui Yoon"
    ],
    "abstract": "Task execution for object rearrangement could be challenged by Task-Level\nPerturbations (TLP), i.e., unexpected object additions, removals, and\ndisplacements that can disrupt underlying visual policies and fundamentally\ncompromise task feasibility and progress. To address these challenges, we\npresent LangPert, a language-based framework designed to detect and mitigate\nTLP situations in tabletop rearrangement tasks. LangPert integrates a Visual\nLanguage Model (VLM) to comprehensively monitor policy's skill execution and\nenvironmental TLP, while leveraging the Hierarchical Chain-of-Thought (HCoT)\nreasoning mechanism to enhance the Large Language Model (LLM)'s contextual\nunderstanding and generate adaptive, corrective skill-execution plans. Our\nexperimental results demonstrate that LangPert handles diverse TLP situations\nmore effectively than baseline methods, achieving higher task completion rates,\nimproved execution efficiency, and potential generalization to unseen\nscenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09893v1",
    "published_date": "2025-04-14 05:39:15 UTC",
    "updated_date": "2025-04-14 05:39:15 UTC"
  },
  {
    "arxiv_id": "2504.09877v1",
    "title": "Constructing Micro Knowledge Graphs from Technical Support Documents",
    "authors": [
      "Atul Kumar",
      "Nisha Gupta",
      "Saswati Dana"
    ],
    "abstract": "Short technical support pages such as IBM Technotes are quite common in\ntechnical support domain. These pages can be very useful as the knowledge\nsources for technical support applications such as chatbots, search engines and\nquestion-answering (QA) systems. Information extracted from documents to drive\ntechnical support applications is often stored in the form of Knowledge Graph\n(KG). Building KGs from a large corpus of documents poses a challenge of\ngranularity because a large number of entities and actions are present in each\npage. The KG becomes virtually unusable if all entities and actions from these\npages are stored in the KG. Therefore, only key entities and actions from each\npage are extracted and stored in the KG. This approach however leads to loss of\nknowledge represented by entities and actions left out of the KG as they are no\nlonger available to graph search and reasoning functions. We propose a set of\ntechniques to create micro knowledge graph (micrograph) for each of such web\npages. The micrograph stores all the entities and actions in a page and also\ntakes advantage of the structure of the page to represent exactly in which part\nof that page these entities and actions appeared, and also how they relate to\neach other. These micrographs can be used as additional knowledge sources by\ntechnical support applications. We define schemas for representing\nsemi-structured and plain text knowledge present in the technical support web\npages. Solutions in technical support domain include procedures made of steps.\nWe also propose a technique to extract procedures from these webpages and the\nschemas to represent them in the micrographs. We also discuss how technical\nsupport applications can take advantage of the micrographs.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09877v1",
    "published_date": "2025-04-14 04:57:49 UTC",
    "updated_date": "2025-04-14 04:57:49 UTC"
  },
  {
    "arxiv_id": "2504.09876v1",
    "title": "HDC: Hierarchical Distillation for Multi-level Noisy Consistency in Semi-Supervised Fetal Ultrasound Segmentation",
    "authors": [
      "Tran Quoc Khanh Le",
      "Nguyen Lan Vi Vu",
      "Ha-Hieu Pham",
      "Xuan-Loc Huynh",
      "Tien-Huy Nguyen",
      "Minh Huu Nhat Le",
      "Quan Nguyen",
      "Hien D. Nguyen"
    ],
    "abstract": "Transvaginal ultrasound is a critical imaging modality for evaluating\ncervical anatomy and detecting physiological changes. However, accurate\nsegmentation of cervical structures remains challenging due to low contrast,\nshadow artifacts, and fuzzy boundaries. While convolutional neural networks\n(CNNs) have shown promising results in medical image segmentation, their\nperformance is often limited by the need for large-scale annotated datasets -\nan impractical requirement in clinical ultrasound imaging. Semi-supervised\nlearning (SSL) offers a compelling solution by leveraging unlabeled data, but\nexisting teacher-student frameworks often suffer from confirmation bias and\nhigh computational costs. We propose HDC, a novel semi-supervised segmentation\nframework that integrates Hierarchical Distillation and Consistency learning\nwithin a multi-level noise mean-teacher framework. Unlike conventional\napproaches that rely solely on pseudo-labeling, we introduce a hierarchical\ndistillation mechanism that guides feature-level learning via two novel\nobjectives: (1) Correlation Guidance Loss to align feature representations\nbetween the teacher and main student branch, and (2) Mutual Information Loss to\nstabilize representations between the main and noisy student branches. Our\nframework reduces model complexity while improving generalization. Extensive\nexperiments on two fetal ultrasound datasets, FUGC and PSFH, demonstrate that\nour method achieves competitive performance with significantly lower\ncomputational overhead than existing multi-teacher models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09876v1",
    "published_date": "2025-04-14 04:52:24 UTC",
    "updated_date": "2025-04-14 04:52:24 UTC"
  },
  {
    "arxiv_id": "2504.09873v1",
    "title": "Truncated Matrix Completion - An Empirical Study",
    "authors": [
      "Rishhabh Naik",
      "Nisarg Trivedi",
      "Davoud Ataee Tarzanagh",
      "Laura Balzano"
    ],
    "abstract": "Low-rank Matrix Completion (LRMC) describes the problem where we wish to\nrecover missing entries of partially observed low-rank matrix. Most existing\nmatrix completion work deals with sampling procedures that are independent of\nthe underlying data values. While this assumption allows the derivation of nice\ntheoretical guarantees, it seldom holds in real-world applications. In this\npaper, we consider various settings where the sampling mask is dependent on the\nunderlying data values, motivated by applications in sensing, sequential\ndecision-making, and recommender systems. Through a series of experiments, we\nstudy and compare the performance of various LRMC algorithms that were\noriginally successful for data-independent sampling patterns.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09873v1",
    "published_date": "2025-04-14 04:42:00 UTC",
    "updated_date": "2025-04-14 04:42:00 UTC"
  },
  {
    "arxiv_id": "2504.09865v1",
    "title": "Labeling Messages as AI-Generated Does Not Reduce Their Persuasive Effects",
    "authors": [
      "Isabel O. Gallegos",
      "Chen Shani",
      "Weiyan Shi",
      "Federico Bianchi",
      "Izzy Gainsburg",
      "Dan Jurafsky",
      "Robb Willer"
    ],
    "abstract": "As generative artificial intelligence (AI) enables the creation and\ndissemination of information at massive scale and speed, it is increasingly\nimportant to understand how people perceive AI-generated content. One prominent\npolicy proposal requires explicitly labeling AI-generated content to increase\ntransparency and encourage critical thinking about the information, but prior\nresearch has not yet tested the effects of such labels. To address this gap, we\nconducted a survey experiment (N=1601) on a diverse sample of Americans,\npresenting participants with an AI-generated message about several public\npolicies (e.g., allowing colleges to pay student-athletes), randomly assigning\nwhether participants were told the message was generated by (a) an expert AI\nmodel, (b) a human policy expert, or (c) no label. We found that messages were\ngenerally persuasive, influencing participants' views of the policies by 9.74\npercentage points on average. However, while 94.6% of participants assigned to\nthe AI and human label conditions believed the authorship labels, labels had no\nsignificant effects on participants' attitude change toward the policies,\njudgments of message accuracy, nor intentions to share the message with others.\nThese patterns were robust across a variety of participant characteristics,\nincluding prior knowledge of the policy, prior experience with AI, political\nparty, education level, or age. Taken together, these results imply that, while\nauthorship labels would likely enhance transparency, they are unlikely to\nsubstantially affect the persuasiveness of the labeled content, highlighting\nthe need for alternative strategies to address challenges posed by AI-generated\ninformation.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09865v1",
    "published_date": "2025-04-14 04:22:39 UTC",
    "updated_date": "2025-04-14 04:22:39 UTC"
  },
  {
    "arxiv_id": "2504.09860v1",
    "title": "SUMART: SUMmARizing Translation from Wordy to Concise Expression",
    "authors": [
      "Naoto Nishida",
      "Jun Rekimoto"
    ],
    "abstract": "We propose SUMART, a method for summarizing and compressing the volume of\nverbose subtitle translations. SUMART is designed for understanding translated\ncaptions (e.g., interlingual conversations via subtitle translation or when\nwatching movies in foreign language audio and translated captions). SUMART is\nintended for users who want a big-picture and fast understanding of the\nconversation, audio, video content, and speech in a foreign language. During\nthe training data collection, when a speaker makes a verbose statement, SUMART\nemploys a large language model on-site to compress the volume of subtitles.\nThis compressed data is then stored in a database for fine-tuning purposes.\nLater, SUMART uses data pairs from those non-compressed ASR results and\ncompressed translated results for fine-tuning the translation model to generate\nmore concise translations for practical uses. In practical applications, SUMART\nutilizes this trained model to produce concise translation results.\nFurthermore, as a practical application, we developed an application that\nallows conversations using subtitle translation in augmented reality spaces. As\na pilot study, we conducted qualitative surveys using a SUMART prototype and a\nsurvey on the summarization model for SUMART. We envision the most effective\nuse case of this system is where users need to consume a lot of information\nquickly (e.g., Speech, lectures, podcasts, Q&A in conferences).",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "3 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.09860v1",
    "published_date": "2025-04-14 04:13:09 UTC",
    "updated_date": "2025-04-14 04:13:09 UTC"
  },
  {
    "arxiv_id": "2504.09858v1",
    "title": "Reasoning Models Can Be Effective Without Thinking",
    "authors": [
      "Wenjie Ma",
      "Jingxuan He",
      "Charlie Snell",
      "Tyler Griggs",
      "Sewon Min",
      "Matei Zaharia"
    ],
    "abstract": "Recent LLMs have significantly improved reasoning capabilities, primarily by\nincluding an explicit, lengthy Thinking process as part of generation. In this\npaper, we question whether this explicit thinking is necessary. Using the\nstate-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking\nprocess via simple prompting, denoted as NoThinking, can be surprisingly\neffective. When controlling for the number of tokens, NoThinking outperforms\nThinking across a diverse set of seven challenging reasoning\ndatasets--including mathematical problem solving, formal theorem proving, and\ncoding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with\n700 tokens. Notably, the performance of NoThinking becomes more competitive\nwith pass@k as k increases. Building on this observation, we demonstrate that a\nparallel scaling approach that uses NoThinking to generate N outputs\nindependently and aggregates them is highly effective. For aggregation, we use\ntask-specific verifiers when available, or we apply simple best-of-N strategies\nsuch as confidence-based selection. Our method outperforms a range of baselines\nwith similar latency using Thinking, and is comparable to Thinking with\nsignificantly longer latency (up to 9x). Together, our research encourages a\nreconsideration of the necessity of lengthy thinking processes, while also\nestablishing a competitive reference for achieving strong reasoning performance\nin low-budget settings or at low latency using parallel scaling.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "33 pages, 7 main figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.09858v1",
    "published_date": "2025-04-14 04:08:16 UTC",
    "updated_date": "2025-04-14 04:08:16 UTC"
  },
  {
    "arxiv_id": "2504.09857v1",
    "title": "Working with Large Language Models to Enhance Messaging Effectiveness for Vaccine Confidence",
    "authors": [
      "Lucinda Gullison",
      "Feng Fu"
    ],
    "abstract": "Vaccine hesitancy and misinformation are significant barriers to achieving\nwidespread vaccination coverage. Smaller public health departments may lack the\nexpertise or resources to craft effective vaccine messaging. This paper\nexplores the potential of ChatGPT-augmented messaging to promote confidence in\nvaccination uptake.\n  We conducted a survey in which participants chose between pairs of\nvaccination messages and assessed which was more persuasive and to what extent.\nIn each pair, one message was the original, and the other was augmented by\nChatGPT. At the end of the survey, participants were informed that half of the\nmessages had been generated by ChatGPT. They were then asked to provide both\nquantitative and qualitative responses regarding how knowledge of a message's\nChatGPT origin affected their impressions.\n  Overall, ChatGPT-augmented messages were rated slightly higher than the\noriginal messages. These messages generally scored better when they were\nlonger. Respondents did not express major concerns about ChatGPT-generated\ncontent, nor was there a significant relationship between participants' views\non ChatGPT and their message ratings. Notably, there was a correlation between\nwhether a message appeared first or second in a pair and its score.\n  These results point to the potential of ChatGPT to enhance vaccine messaging,\nsuggesting a promising direction for future research on human-AI collaboration\nin public health communication.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "physics.soc-ph"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09857v1",
    "published_date": "2025-04-14 04:06:46 UTC",
    "updated_date": "2025-04-14 04:06:46 UTC"
  },
  {
    "arxiv_id": "2504.09855v1",
    "title": "PestMA: LLM-based Multi-Agent System for Informed Pest Management",
    "authors": [
      "Hongrui Shi",
      "Shunbao Li",
      "Zhipeng Yuan",
      "Po Yang"
    ],
    "abstract": "Effective pest management is complex due to the need for accurate,\ncontext-specific decisions. Recent advancements in large language models (LLMs)\nopen new possibilities for addressing these challenges by providing\nsophisticated, adaptive knowledge acquisition and reasoning. However, existing\nLLM-based pest management approaches often rely on a single-agent paradigm,\nwhich can limit their capacity to incorporate diverse external information,\nengage in systematic validation, and address complex, threshold-driven\ndecisions. To overcome these limitations, we introduce PestMA, an LLM-based\nmulti-agent system (MAS) designed to generate reliable and evidence-based pest\nmanagement advice. Building on an editorial paradigm, PestMA features three\nspecialized agents, an Editor for synthesizing pest management recommendations,\na Retriever for gathering relevant external data, and a Validator for ensuring\ncorrectness. Evaluations on real-world pest scenarios demonstrate that PestMA\nachieves an initial accuracy of 86.8% for pest management decisions, which\nincreases to 92.6% after validation. These results underscore the value of\ncollaborative agent-based workflows in refining and validating decisions,\nhighlighting the potential of LLM-based multi-agent systems to automate and\nenhance pest management processes.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "I.2.1; I.2.7"
    ],
    "primary_category": "cs.MA",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.09855v1",
    "published_date": "2025-04-14 03:53:59 UTC",
    "updated_date": "2025-04-14 03:53:59 UTC"
  },
  {
    "arxiv_id": "2504.09851v1",
    "title": "Carbon-Efficient 3D DNN Acceleration: Optimizing Performance and Sustainability",
    "authors": [
      "Aikaterini Maria Panteleaki",
      "Konstantinos Balaskas",
      "Georgios Zervakis",
      "Hussam Amrouch",
      "Iraklis Anagnostopoulos"
    ],
    "abstract": "As Deep Neural Networks (DNNs) continue to drive advancements in artificial\nintelligence, the design of hardware accelerators faces growing concerns over\nembodied carbon footprint due to complex fabrication processes. 3D integration\nimproves performance but introduces sustainability challenges, making\ncarbon-aware optimization essential. In this work, we propose a\ncarbon-efficient design methodology for 3D DNN accelerators, leveraging\napproximate computing and genetic algorithm-based design space exploration to\noptimize Carbon Delay Product (CDP). By integrating area-efficient approximate\nmultipliers into Multiply-Accumulate (MAC) units, our approach effectively\nreduces silicon area and fabrication overhead while maintaining high\ncomputational accuracy. Experimental evaluations across three technology nodes\n(45nm, 14nm, and 7nm) show that our method reduces embodied carbon by up to 30%\nwith negligible accuracy drop.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Submitted in ISVLSI 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.09851v1",
    "published_date": "2025-04-14 03:48:37 UTC",
    "updated_date": "2025-04-14 03:48:37 UTC"
  },
  {
    "arxiv_id": "2504.09848v1",
    "title": "A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Advances in Embodied Agents, Smart Cities, and Earth Science",
    "authors": [
      "Jie Feng",
      "Jinwei Zeng",
      "Qingyue Long",
      "Hongyi Chen",
      "Jie Zhao",
      "Yanxin Xi",
      "Zhilun Zhou",
      "Yuan Yuan",
      "Shengyuan Wang",
      "Qingbin Zeng",
      "Songwei Li",
      "Yunke Zhang",
      "Yuming Lin",
      "Tong Li",
      "Jingtao Ding",
      "Chen Gao",
      "Fengli Xu",
      "Yong Li"
    ],
    "abstract": "Over the past year, the development of large language models (LLMs) has\nbrought spatial intelligence into focus, with much attention on vision-based\nembodied intelligence. However, spatial intelligence spans a broader range of\ndisciplines and scales, from navigation and urban planning to remote sensing\nand earth science. What are the differences and connections between spatial\nintelligence across these fields? In this paper, we first review human spatial\ncognition and its implications for spatial intelligence in LLMs. We then\nexamine spatial memory, knowledge representations, and abstract reasoning in\nLLMs, highlighting their roles and connections. Finally, we analyze spatial\nintelligence across scales -- from embodied to urban and global levels --\nfollowing a framework that progresses from spatial memory and understanding to\nspatial reasoning and intelligence. Through this survey, we aim to provide\ninsights into interdisciplinary spatial intelligence research and inspire\nfuture studies.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09848v1",
    "published_date": "2025-04-14 03:38:31 UTC",
    "updated_date": "2025-04-14 03:38:31 UTC"
  },
  {
    "arxiv_id": "2504.09846v1",
    "title": "GlyTwin: Digital Twin for Glucose Control in Type 1 Diabetes Through Optimal Behavioral Modifications Using Patient-Centric Counterfactuals",
    "authors": [
      "Asiful Arefeen",
      "Saman Khamesian",
      "Maria Adela Grando",
      "Bithika Thompson",
      "Hassan Ghasemzadeh"
    ],
    "abstract": "Frequent and long-term exposure to hyperglycemia (i.e., high blood glucose)\nincreases the risk of chronic complications such as neuropathy, nephropathy,\nand cardiovascular disease. Current technologies like continuous subcutaneous\ninsulin infusion (CSII) and continuous glucose monitoring (CGM) primarily model\nspecific aspects of glycemic control-like hypoglycemia prediction or insulin\ndelivery. Similarly, most digital twin approaches in diabetes management\nsimulate only physiological processes. These systems lack the ability to offer\nalternative treatment scenarios that support proactive behavioral\ninterventions. To address this, we propose GlyTwin, a novel digital twin\nframework that uses counterfactual explanations to simulate optimal treatments\nfor glucose regulation. Our approach helps patients and caregivers modify\nbehaviors like carbohydrate intake and insulin dosing to avoid abnormal glucose\nevents. GlyTwin generates behavioral treatment suggestions that proactively\nprevent hyperglycemia by recommending small adjustments to daily choices,\nreducing both frequency and duration of these events. Additionally, it\nincorporates stakeholder preferences into the intervention design, making\nrecommendations patient-centric and tailored. We evaluate GlyTwin on AZT1D, a\nnewly constructed dataset with longitudinal data from 21 type 1 diabetes (T1D)\npatients on automated insulin delivery systems over 26 days. Results show\nGlyTwin outperforms state-of-the-art counterfactual methods, generating 76.6%\nvalid and 86% effective interventions. These findings demonstrate the promise\nof counterfactual-driven digital twins in delivering personalized healthcare.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09846v1",
    "published_date": "2025-04-14 03:32:39 UTC",
    "updated_date": "2025-04-14 03:32:39 UTC"
  },
  {
    "arxiv_id": "2504.09844v1",
    "title": "OVERLORD: Ultimate Scaling of DataLoader for Multi-Source Large Foundation Model Training",
    "authors": [
      "Juntao Zhao",
      "Qi Lu",
      "Wei Jia",
      "Borui Wan",
      "Lei Zuo",
      "Junda Feng",
      "Jianyu Jiang",
      "Yangrui Chen",
      "Shuaishuai Cao",
      "Jialing He",
      "Kaihua Jiang",
      "Yuanzhe Hu",
      "Yanghua Peng",
      "Haibin Lin",
      "Xin Liu",
      "Chuan Wu"
    ],
    "abstract": "Modern frameworks for training large foundation models (LFMs) employ data\nloaders in a data parallel paradigm. While this design offers implementation\nsimplicity, it introduces two fundamental challenges. First, due to the\nquadratic computational complexity of the attention operator, the non-uniform\nsample distribution over data-parallel ranks leads to a significant workload\nimbalance among loaders, which degrades the training efficiency. This paradigm\nalso impedes the implementation of data mixing algorithms (e.g., curriculum\nlearning) over different datasets. Second, to acquire a broad range of\ncapability, LFMs training ingests data from diverse sources, each with distinct\nfile access states. Colocating massive datasets within loader instances can\neasily exceed local pod memory capacity. Additionally, heavy sources with\nhigher transformation latency require larger worker pools, further exacerbating\nmemory consumption.\n  We present OVERLORD, an industrial-grade distributed data loading\narchitecture with three innovations: (1) A centralized and declarative data\nplane, which facilitates elastic data orchestration strategy, such as\nlong-short context, multimodal, and curriculum learning; (2) Disaggregated\nmultisource preprocessing through role-specific actors, i.e., Source Loaders\nand Data Constructors, leveraging autoscaling for Source Loaders towards\nheterogeneous and evolving source preprocessing cost; (3) Shadow Loaders with\ndifferential checkpointing for uninterrupted fault recovery. Deployed on\nproduction clusters scaling to multi-thousand GPU, OVERLORD achieves: (1) 4.5x\nend-to-end training throughput improvement, (2) a minimum 3.6x reduction in CPU\nmemory usage, with further improvements to be added in later experiments.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09844v1",
    "published_date": "2025-04-14 03:31:22 UTC",
    "updated_date": "2025-04-14 03:31:22 UTC"
  },
  {
    "arxiv_id": "2504.09841v1",
    "title": "StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models",
    "authors": [
      "Yang Feng",
      "Xudong Pan"
    ],
    "abstract": "The proliferation of autonomous agents powered by large language models\n(LLMs) has revolutionized popular business applications dealing with tabular\ndata, i.e., tabular agents. Although LLMs are observed to be vulnerable against\nprompt injection attacks from external data sources, tabular agents impose\nstrict data formats and predefined rules on the attacker's payload, which are\nineffective unless the agent navigates multiple layers of structural data to\nincorporate the payload. To address the challenge, we present a novel attack\ntermed StruPhantom which specifically targets black-box LLM-powered tabular\nagents. Our attack designs an evolutionary optimization procedure which\ncontinually refines attack payloads via the proposed constrained Monte Carlo\nTree Search augmented by an off-topic evaluator. StruPhantom helps\nsystematically explore and exploit the weaknesses of target applications to\nachieve goal hijacking. Our evaluation validates the effectiveness of\nStruPhantom across various LLM-based agents, including those on real-world\nplatforms, and attack scenarios. Our attack achieves over 50% higher success\nrates than baselines in enforcing the application's response to contain\nphishing links or malicious codes.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Work in Progress",
    "pdf_url": "http://arxiv.org/pdf/2504.09841v1",
    "published_date": "2025-04-14 03:22:04 UTC",
    "updated_date": "2025-04-14 03:22:04 UTC"
  },
  {
    "arxiv_id": "2504.09839v1",
    "title": "SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
    "authors": [
      "Zhisheng Zhang",
      "Derui Wang",
      "Qianyi Yang",
      "Pengyang Huang",
      "Junhan Pu",
      "Yuxin Cao",
      "Kai Ye",
      "Jie Hao",
      "Yixian Yang"
    ],
    "abstract": "Speech synthesis technology has brought great convenience, while the\nwidespread usage of realistic deepfake audio has triggered hazards. Malicious\nadversaries may unauthorizedly collect victims' speeches and clone a similar\nvoice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the\nexisting defense methods cannot effectively prevent deepfake exploitation and\nare vulnerable to robust training techniques. Therefore, a more effective and\nrobust data protection method is urgently needed. In response, we propose a\ndefensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users'\naudio before uploading by embedding imperceptible perturbations on original\nspeeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a\nrobust and universal proactive protection technique, \\textbf{S}peech\n\\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a\nsurrogate model to generate universally applicable perturbation for generative\nsynthetic models. Moreover, we optimize the human perception of embedded\nperturbation in terms of time and frequency domains. To evaluate our method\ncomprehensively, we conduct extensive experiments across advanced models and\ndatasets, both subjectively and objectively. Our experimental results\ndemonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection\neffectiveness and transferability and is highly robust against advanced\nadaptive adversaries. Moreover, SafeSpeech has real-time capability in\nreal-world tests. The source code is available at\n\\href{https://github.com/wxzyd123/SafeSpeech}{https://github.com/wxzyd123/SafeSpeech}.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to USENIX Security 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.09839v1",
    "published_date": "2025-04-14 03:21:23 UTC",
    "updated_date": "2025-04-14 03:21:23 UTC"
  },
  {
    "arxiv_id": "2504.09831v1",
    "title": "Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent Demand",
    "authors": [
      "Korel Gundem",
      "Zhengling Qi"
    ],
    "abstract": "In this paper, we study the offline sequential feature-based pricing and\ninventory control problem where the current demand depends on the past demand\nlevels and any demand exceeding the available inventory is lost. Our goal is to\nleverage the offline dataset, consisting of past prices, ordering quantities,\ninventory levels, covariates, and censored sales levels, to estimate the\noptimal pricing and inventory control policy that maximizes long-term profit.\nWhile the underlying dynamic without censoring can be modeled by Markov\ndecision process (MDP), the primary obstacle arises from the observed process\nwhere demand censoring is present, resulting in missing profit information, the\nfailure of the Markov property, and a non-stationary optimal policy. To\novercome these challenges, we first approximate the optimal policy by solving a\nhigh-order MDP characterized by the number of consecutive censoring instances,\nwhich ultimately boils down to solving a specialized Bellman equation tailored\nfor this problem. Inspired by offline reinforcement learning and survival\nanalysis, we propose two novel data-driven algorithms to solving these Bellman\nequations and, thus, estimate the optimal policy. Furthermore, we establish\nfinite sample regret bounds to validate the effectiveness of these algorithms.\nFinally, we conduct numerical experiments to demonstrate the efficacy of our\nalgorithms in estimating the optimal policy. To the best of our knowledge, this\nis the first data-driven approach to learning optimal pricing and inventory\ncontrol policies in a sequential decision-making environment characterized by\ncensored and dependent demand. The implementations of the proposed algorithms\nare available at https://github.com/gundemkorel/Inventory_Pricing_Control",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.AP",
      "stat.TH",
      "90B05, 68T05, 90C40, 62N02"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09831v1",
    "published_date": "2025-04-14 02:57:51 UTC",
    "updated_date": "2025-04-14 02:57:51 UTC"
  },
  {
    "arxiv_id": "2504.09812v1",
    "title": "Efficient Multi-Task Modeling through Automated Fusion of Trained Models",
    "authors": [
      "Jingxuan Zhou",
      "Weidong Bao",
      "Ji Wang",
      "Zhengyi Zhong",
      "Dayu Zhang"
    ],
    "abstract": "Although multi-task learning is widely applied in intelligent services,\ntraditional multi-task modeling methods often require customized designs based\non specific task combinations, resulting in a cumbersome modeling process.\nInspired by the rapid development and excellent performance of single-task\nmodels, this paper proposes an efficient multi-task modeling method that can\nautomatically fuse trained single-task models with different structures and\ntasks to form a multi-task model. As a general framework, this method allows\nmodelers to simply prepare trained models for the required tasks, simplifying\nthe modeling process while fully utilizing the knowledge contained in the\ntrained models. This eliminates the need for excessive focus on task\nrelationships and model structure design. To achieve this goal, we consider the\nstructural differences among various trained models and employ model\ndecomposition techniques to hierarchically decompose them into multiple\noperable model components. Furthermore, we have designed an Adaptive Knowledge\nFusion (AKF) module based on Transformer, which adaptively integrates\nintra-task and inter-task knowledge based on model components. Through the\nproposed method, we achieve efficient and automated construction of multi-task\nmodels, and its effectiveness is verified through extensive experiments on\nthree datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09812v1",
    "published_date": "2025-04-14 02:21:45 UTC",
    "updated_date": "2025-04-14 02:21:45 UTC"
  },
  {
    "arxiv_id": "2504.09809v1",
    "title": "See or Recall: A Sanity Check for the Role of Vision in Solving Visualization Question Answer Tasks with Multimodal LLMs",
    "authors": [
      "Zhimin Li",
      "Haichao Miao",
      "Xinyuan Yan",
      "Valerio Pascucci",
      "Matthew Berger",
      "Shusen Liu"
    ],
    "abstract": "Recent developments in multimodal large language models (MLLM) have equipped\nlanguage models to reason about vision and language jointly. This permits MLLMs\nto both perceive and answer questions about data visualization across a variety\nof designs and tasks. Applying MLLMs to a broad range of visualization tasks\nrequires us to properly evaluate their capabilities, and the most common way to\nconduct evaluation is through measuring a model's visualization reasoning\ncapability, analogous to how we would evaluate human understanding of\nvisualizations (e.g., visualization literacy). However, we found that in the\ncontext of visualization question answering (VisQA), how an MLLM perceives and\nreasons about visualizations can be fundamentally different from how humans\napproach the same problem. During the evaluation, even without visualization,\nthe model could correctly answer a substantial portion of the visualization\ntest questions, regardless of whether any selection options were provided. We\nhypothesize that the vast amount of knowledge encoded in the language model\npermits factual recall that supersedes the need to seek information from the\nvisual signal. It raises concerns that the current VisQA evaluation may not\nfully capture the models' visualization reasoning capabilities. To address\nthis, we propose a comprehensive sanity check framework that integrates a\nrule-based decision tree and a sanity check table to disentangle the effects of\n\"seeing\" (visual processing) and \"recall\" (reliance on prior knowledge). This\nvalidates VisQA datasets for evaluation, highlighting where models are truly\n\"seeing\", positively or negatively affected by the factual recall, or relying\non inductive biases for question answering. Our study underscores the need for\ncareful consideration in designing future visualization understanding studies\nwhen utilizing MLLMs.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09809v1",
    "published_date": "2025-04-14 02:19:28 UTC",
    "updated_date": "2025-04-14 02:19:28 UTC"
  },
  {
    "arxiv_id": "2504.09802v1",
    "title": "Training Small Reasoning LLMs with Cognitive Preference Alignment",
    "authors": [
      "Wenrui Cai",
      "Chengyu Wang",
      "Junbing Yan",
      "Jun Huang",
      "Xiangzhong Fang"
    ],
    "abstract": "The reasoning capabilities of large language models (LLMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need to explore strategies to train effective reasoning LLMs\nwith far fewer parameters. A critical challenge is that smaller models have\ndifferent capacities and cognitive trajectories than their larger counterparts.\nHence, direct distillation of chain-of-thought (CoT) results from large LLMs to\nsmaller ones can be sometimes ineffective and requires a huge amount of\nannotated data. In this paper, we introduce a novel framework called\nCritique-Rethink-Verify (CRV), designed for training smaller yet powerful\nreasoning LLMs. Our CRV framework consists of multiple LLM agents, each\nspecializing in unique abilities: (i) critiquing the CoTs according to the\ncognitive capabilities of smaller models, (ii) rethinking and refining these\nCoTs based on the critiques, and (iii) verifying the correctness of the refined\nresults. We further propose the cognitive preference optimization (CogPO)\nalgorithm to enhance the reasoning abilities of smaller models by aligning\nthoughts of these models with their cognitive capacities. Comprehensive\nevaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV\nand CogPO, which outperforms other training methods by a large margin.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09802v1",
    "published_date": "2025-04-14 02:03:54 UTC",
    "updated_date": "2025-04-14 02:03:54 UTC"
  },
  {
    "arxiv_id": "2504.09800v1",
    "title": "Multi-task Federated Learning with Encoder-Decoder Structure: Enabling Collaborative Learning Across Different Tasks",
    "authors": [
      "Jingxuan Zhou",
      "Weidong Bao",
      "Ji Wang",
      "Dayu Zhang",
      "Xiongtao Zhang",
      "Yaohong Zhang"
    ],
    "abstract": "Federated learning has been extensively studied and applied due to its\nability to ensure data security in distributed environments while building\nbetter models. However, clients participating in federated learning still face\nlimitations, as clients with different structures or tasks cannot participate\nin learning together. In view of this, constructing a federated learning\nframework that allows collaboration between clients with different model\nstructures and performing different tasks, enabling them to share valuable\nknowledge to enhance model efficiency, holds significant practical implications\nfor the widespread application of federated learning. To achieve this goal, we\npropose a multi-task federated learning with encoder-decoder structure (M-Fed).\nSpecifically, given the widespread adoption of the encoder-decoder architecture\nin current models, we leverage this structure to share intra-task knowledge\nthrough traditional federated learning methods and extract general knowledge\nfrom the encoder to achieve cross-task knowledge sharing. The training process\nis similar to traditional federated learning, and we incorporate local decoder\nand global decoder information into the loss function. The local decoder\niteratively updates and gradually approaches the global decoder until\nsufficient cross-task knowledge sharing is achieved. Our method is lightweight\nand modular, demonstrating innovation compared to previous research. It enables\nclients performing different tasks to share general knowledge while maintaining\nthe efficiency of traditional federated learning systems. We conducted\nexperiments on two widely used benchmark datasets to verify the feasibility of\nM-Fed and compared it with traditional methods. The experimental results\ndemonstrate the effectiveness of M-Fed in multi-task federated learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09800v1",
    "published_date": "2025-04-14 02:01:39 UTC",
    "updated_date": "2025-04-14 02:01:39 UTC"
  },
  {
    "arxiv_id": "2504.09797v1",
    "title": "IGL-DT: Iterative Global-Local Feature Learning with Dual-Teacher Semantic Segmentation Framework under Limited Annotation Scheme",
    "authors": [
      "Dinh Dai Quan Tran",
      "Hoang-Thien Nguyen. Thanh-Huy Nguyen",
      "Gia-Van To",
      "Tien-Huy Nguyen",
      "Quan Nguyen"
    ],
    "abstract": "Semi-Supervised Semantic Segmentation (SSSS) aims to improve segmentation\naccuracy by leveraging a small set of labeled images alongside a larger pool of\nunlabeled data. Recent advances primarily focus on pseudo-labeling, consistency\nregularization, and co-training strategies. However, existing methods struggle\nto balance global semantic representation with fine-grained local feature\nextraction. To address this challenge, we propose a novel tri-branch\nsemi-supervised segmentation framework incorporating a dual-teacher strategy,\nnamed IGL-DT. Our approach employs SwinUnet for high-level semantic guidance\nthrough Global Context Learning and ResUnet for detailed feature refinement via\nLocal Regional Learning. Additionally, a Discrepancy Learning mechanism\nmitigates over-reliance on a single teacher, promoting adaptive feature\nlearning. Extensive experiments on benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art approaches, achieving superior segmentation\nperformance across various data regimes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.09797v1",
    "published_date": "2025-04-14 01:51:29 UTC",
    "updated_date": "2025-04-14 01:51:29 UTC"
  },
  {
    "arxiv_id": "2504.09795v1",
    "title": "VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents",
    "authors": [
      "Ryota Tanaka",
      "Taichi Iki",
      "Taku Hasegawa",
      "Kyosuke Nishida",
      "Kuniko Saito",
      "Jun Suzuki"
    ],
    "abstract": "We aim to develop a retrieval-augmented generation (RAG) framework that\nanswers questions over a corpus of visually-rich documents presented in mixed\nmodalities (e.g., charts, tables) and diverse formats (e.g., PDF, PPTX). In\nthis paper, we introduce a new RAG framework, VDocRAG, which can directly\nunderstand varied documents and modalities in a unified image format to prevent\nmissing information that occurs by parsing documents to obtain text. To improve\nthe performance, we propose novel self-supervised pre-training tasks that adapt\nlarge vision-language models for retrieval by compressing visual information\ninto dense token representations while aligning them with textual content in\ndocuments. Furthermore, we introduce OpenDocVQA, the first unified collection\nof open-domain document visual question answering datasets, encompassing\ndiverse document types and formats. OpenDocVQA provides a comprehensive\nresource for training and evaluating retrieval and question answering models on\nvisually-rich documents in an open-domain setting. Experiments show that\nVDocRAG substantially outperforms conventional text-based RAG and has strong\ngeneralization capability, highlighting the potential of an effective RAG\nparadigm for real-world documents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by CVPR 2025; project page: https://vdocrag.github.io",
    "pdf_url": "http://arxiv.org/pdf/2504.09795v1",
    "published_date": "2025-04-14 01:50:33 UTC",
    "updated_date": "2025-04-14 01:50:33 UTC"
  },
  {
    "arxiv_id": "2504.09789v1",
    "title": "EquiVDM: Equivariant Video Diffusion Models with Temporally Consistent Noise",
    "authors": [
      "Chao Liu",
      "Arash Vahdat"
    ],
    "abstract": "Temporally consistent video-to-video generation is essential for applications\nof video diffusion models in areas such as sim-to-real, style-transfer, video\nupsampling, etc. In this paper, we propose a video diffusion framework that\nleverages temporally consistent noise to generate coherent video frames without\nspecialized modules or additional constraints. We show that the standard\ntraining objective of diffusion models, when applied with temporally consistent\nnoise, encourages the model to be equivariant to spatial transformations in\ninput video and noise. This enables our model to better follow motion patterns\nfrom the input video, producing aligned motion and high-fidelity frames.\nFurthermore, we extend our approach to 3D-consistent video generation by\nattaching noise as textures on 3D meshes, ensuring 3D consistency in\nsim-to-real applications. Experimental results demonstrate that our method\nsurpasses state-of-the-art baselines in motion alignment, 3D consistency, and\nvideo quality while requiring only a few sampling steps in practice.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09789v1",
    "published_date": "2025-04-14 01:26:29 UTC",
    "updated_date": "2025-04-14 01:26:29 UTC"
  },
  {
    "arxiv_id": "2504.09781v1",
    "title": "Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop Reasoning",
    "authors": [
      "Jingtian Wu",
      "Claire Cardie"
    ],
    "abstract": "While large language models (LLMs) have demonstrated strong capabilities in\ntasks like question answering and fact verification, they continue to suffer\nfrom hallucinations and reasoning errors, especially in multi-hop tasks that\nrequire integration of multiple information sources. Current methods address\nthese issues through retrieval-based techniques (grounding reasoning in\nexternal evidence), reasoning-based approaches (enhancing coherence via\nimproved prompting), or hybrid strategies combining both elements. One\nprominent hybrid method, ReAct, has outperformed purely retrieval-based or\nreasoning-based approaches; however, it lacks internal verification of\nintermediate reasoning steps, allowing potential errors to propagate through\ncomplex reasoning tasks. In this paper, we introduce Reasoning Court (RC), a\nnovel framework that extends iterative reasoning-and-retrieval methods, such as\nReAct, with a dedicated LLM judge. Unlike ReAct, RC employs this judge to\nindependently evaluate multiple candidate answers and their associated\nreasoning generated by separate LLM agents. The judge is asked to select the\nanswer that it considers the most factually grounded and logically coherent\nbased on the presented reasoning and evidence, or synthesizes a new answer\nusing available evidence and its pre-trained knowledge if all candidates are\ninadequate, flawed, or invalid. Evaluations on multi-hop benchmarks (HotpotQA,\nMuSiQue) and fact-verification (FEVER) demonstrate that RC consistently\noutperforms state-of-the-art few-shot prompting methods without task-specific\nfine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09781v1",
    "published_date": "2025-04-14 00:56:08 UTC",
    "updated_date": "2025-04-14 00:56:08 UTC"
  },
  {
    "arxiv_id": "2504.09779v1",
    "title": "\"All Roads Lead to ChatGPT\": How Generative AI is Eroding Social Interactions and Student Learning Communities",
    "authors": [
      "Irene Hou",
      "Owen Man",
      "Kate Hamilton",
      "Srishty Muthusekaran",
      "Jeffin Johnykutty",
      "Leili Zadeh",
      "Stephen MacNeil"
    ],
    "abstract": "The widespread adoption of generative AI is already impacting learning and\nhelp-seeking. While the benefits of generative AI are well-understood, recent\nstudies have also raised concerns about increased potential for cheating and\nnegative impacts on students' metacognition and critical thinking. However, the\npotential impacts on social interactions, peer learning, and classroom dynamics\nare not yet well understood. To investigate these aspects, we conducted 17\nsemi-structured interviews with undergraduate computing students across seven\nR1 universities in North America. Our findings suggest that help-seeking\nrequests are now often mediated by generative AI. For example, students often\nredirected questions from their peers to generative AI instead of providing\nassistance themselves, undermining peer interaction. Students also reported\nfeeling increasingly isolated and demotivated as the social support systems\nthey rely on begin to break down. These findings are concerning given the\nimportant role that social interactions play in students' learning and sense of\nbelonging.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "7 pages, 1 table. To be published in the Proceedings of the 2025\n  Innovation and Technology in Computer Science Education (ITiCSE 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.09779v1",
    "published_date": "2025-04-14 00:40:58 UTC",
    "updated_date": "2025-04-14 00:40:58 UTC"
  },
  {
    "arxiv_id": "2504.09777v1",
    "title": "Reasoning without Regret",
    "authors": [
      "Tarun Chitra"
    ],
    "abstract": "Chain-of-thought reasoning enables large language models to solve multi-step\ntasks by framing problem solving as sequential decision problems. Outcome-based\nrewards, which provide feedback only on final answers, show impressive success,\nbut face challenges with credit assignment and slow convergence. In contrast,\nprocedure-based rewards offer efficient step-level feedback, but typically\nrequire costly human supervision. We introduce \\emph{Backwards Adaptive Reward\nShaping} (BARS), a no-regret framework that converts sparse outcomes-based\nrewards into effective procedure-based signals. BARS uses sparse rewards\ngenerated from terminal-state priors and cover trees to scale rewards while\npreventing exploitation. With Bellman contraction and $(\\Delta, \\epsilon)$-gap\nrewards, our backward Euler solver achieves $\\epsilon$-accuracy in\n$O\\left((R_{\\max}/\\Delta)\\log(1/\\epsilon)\\right)$ iterations with $O(\\log T)$\ndynamic regret over $T$ rounds. Our analysis, based on generic chaining,\ncontinuous scaling limits, and non-linear Feynman-Kac bounds, connects recent\noutcome-based methods' empirical successes with the benefits of intermediate\nsupervision. Combined, this provides the first rigorous no-regret algorithm for\noutcome reward shaping, providing a theoretical foundation for the empirical\nsuccess of DeepSeek's R1.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09777v1",
    "published_date": "2025-04-14 00:34:20 UTC",
    "updated_date": "2025-04-14 00:34:20 UTC"
  },
  {
    "arxiv_id": "2504.09775v1",
    "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
    "authors": [
      "Abhimanyu Rajeshkumar Bambhaniya",
      "Hanjiang Wu",
      "Suvinay Subramanian",
      "Sudarshan Srinivasan",
      "Souvik Kundu",
      "Amir Yazdanbakhsh",
      "Midhilesh Elavazhagan",
      "Madhu Kumar",
      "Tushar Krishna"
    ],
    "abstract": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables. Code can shared at request",
    "pdf_url": "http://arxiv.org/pdf/2504.09775v1",
    "published_date": "2025-04-14 00:29:49 UTC",
    "updated_date": "2025-04-14 00:29:49 UTC"
  },
  {
    "arxiv_id": "2504.09772v1",
    "title": "Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning",
    "authors": [
      "Can Jin",
      "Hongwu Peng",
      "Qixin Zhang",
      "Yujin Tang",
      "Dimitris N. Metaxas",
      "Tong Che"
    ],
    "abstract": "Multi-agent systems (MAS) built on large language models (LLMs) offer a\npromising path toward solving complex, real-world tasks that single-agent\nsystems often struggle to manage. While recent advancements in test-time\nscaling (TTS) have significantly improved single-agent performance on\nchallenging reasoning tasks, how to effectively scale collaboration and\nreasoning in MAS remains an open question. In this work, we introduce an\nadaptive multi-agent framework designed to enhance collaborative reasoning\nthrough both model-level training and system-level coordination. We construct\nM500, a high-quality dataset containing 500 multi-agent collaborative reasoning\ntraces, and fine-tune Qwen2.5-32B-Instruct on this dataset to produce M1-32B, a\nmodel optimized for multi-agent collaboration. To further enable adaptive\nreasoning, we propose a novel CEO agent that dynamically manages the discussion\nprocess, guiding agent collaboration and adjusting reasoning depth for more\neffective problem-solving. Evaluated in an open-source MAS across a range of\ntasks-including general understanding, mathematical reasoning, and coding-our\nsystem significantly outperforms strong baselines. For instance, M1-32B\nachieves 12% improvement on GPQA-Diamond, 41% on AIME2024, and 10% on\nMBPP-Sanitized, matching the performance of state-of-the-art models like\nDeepSeek-R1 on some tasks. These results highlight the importance of both\nlearned collaboration and adaptive coordination in scaling multi-agent\nreasoning. Code is available at https://github.com/jincan333/MAS-TTS",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.09772v1",
    "published_date": "2025-04-14 00:27:45 UTC",
    "updated_date": "2025-04-14 00:27:45 UTC"
  },
  {
    "arxiv_id": "2504.09763v1",
    "title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems",
    "authors": [
      "Zaid Khan",
      "Elias Stengel-Eskin",
      "Archiki Prasad",
      "Jaemin Cho",
      "Mohit Bansal"
    ],
    "abstract": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from RL (procedural environments) to physics\n(simulation engines). These programs can be seen as functions which execute to\ndifferent outputs based on their parameterizations (e.g., gridworld\nconfiguration or initial physical conditions). We introduce the term EFA\n(Executable Functional Abstraction) to denote such programs for math problems.\nEFA-like constructs have been shown to be useful for math reasoning as problem\ngenerators for stress-testing models. However, prior work has been limited to\nabstractions for grade-school math (whose simple rules are easy to encode in\nprograms), while generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced math\nproblems. We operationalize the task of automatically constructing EFAs as a\nprogram synthesis task, and develop EFAGen, which conditions an LLM on a seed\nmath problem and its step-by-step solution to generate candidate EFA programs\nthat are faithful to the generalized problem and solution class underlying the\nseed problem. Furthermore, we formalize properties any valid EFA must possess\nin terms of executable unit tests, and show how the tests can be used as\nverifiable rewards to train LLMs to become better writers of EFAs. We\ndemonstrate that EFAs constructed by EFAGen behave rationally by remaining\nfaithful to seed problems, produce learnable problem variations, and that\nEFAGen can infer EFAs across multiple diverse sources of competition-level math\nproblems. Finally, we show downstream uses of model-written EFAs e.g. finding\nproblem variations that are harder or easier for a learner to solve, as well as\ndata generation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Project Page: https://zaidkhan.me/EFAGen/",
    "pdf_url": "http://arxiv.org/pdf/2504.09763v1",
    "published_date": "2025-04-14 00:06:48 UTC",
    "updated_date": "2025-04-14 00:06:48 UTC"
  },
  {
    "arxiv_id": "2504.09762v1",
    "title": "(How) Do reasoning models reason?",
    "authors": [
      "Subbarao Kambhampati",
      "Kaya Stechly",
      "Karthik Valmeekam"
    ],
    "abstract": "We will provide a broad unifying perspective on the recent breed of Large\nReasoning Models (LRMs) such as OpenAI o1 and DeepSeek R1, including their\npromise, sources of power, misconceptions and limitations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages (A version appears in The Annals of New York Academy of\n  Sciences)",
    "pdf_url": "http://arxiv.org/pdf/2504.09762v1",
    "published_date": "2025-04-14 00:03:34 UTC",
    "updated_date": "2025-04-14 00:03:34 UTC"
  }
]
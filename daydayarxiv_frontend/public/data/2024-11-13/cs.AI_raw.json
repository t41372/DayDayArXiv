[
  {
    "arxiv_id": "2411.09089v1",
    "title": "Set-Based Retrograde Analysis: Precomputing the Solution to 24-card Bridge Double Dummy Deals",
    "authors": [
      "Isaac Stone",
      "Nathan R. Sturtevant",
      "Jonathan Schaeffer"
    ],
    "abstract": "Retrograde analysis is used in game-playing programs to solve states at the\nend of a game, working backwards toward the start of the game. The algorithm\niterates through and computes the perfect-play value for as many states as\nresources allow. We introduce setrograde analysis which achieves the same\nresults by operating on sets of states that have the same game value. The\nalgorithm is demonstrated by computing exact solutions for Bridge double dummy\ncard-play. For deals with 24 cards remaining to be played ($10^{27}$ states,\nwhich can be reduced to $10^{15}$ states using preexisting techniques), we\nstrongly solve all deals. The setrograde algorithm performs a factor of $10^3$\nfewer search operations than a standard retrograde algorithm, producing a\ndatabase with a factor of $10^4$ fewer entries. For applicable domains, this\nallows retrograde searching to reach unprecedented search depths.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09089v1",
    "published_date": "2024-11-13 23:43:01 UTC",
    "updated_date": "2024-11-13 23:43:01 UTC"
  },
  {
    "arxiv_id": "2412.00011v1",
    "title": "The use of knowledge in open-ended systems",
    "authors": [
      "Abigail Devereaux",
      "Roger Koppl"
    ],
    "abstract": "Economists model knowledge use and acquisition as a cause-and-effect calculus\nassociating observations made by a decision-maker about their world with\npossible underlying causes. Knowledge models are well-established for static\ncontexts, but not for contexts of innovative and unbounded change. We develop a\nrepresentation of knowledge use and acquisition in open-ended evolutionary\nsystems and demonstrate its primary results, including that observers embedded\nin open-ended evolutionary systems can agree to disagree and that their ability\nto theorize about their systems is fundamentally local and constrained to their\nframe of reference what we call frame relativity. The results of our framework\nformalize local knowledge use, the many-selves interpretation of reasoning\nthrough time, and motivate the emergence of nonlogical modes of reasoning like\ninstitutional and aesthetic codes.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LO",
      "econ.TH"
    ],
    "primary_category": "cs.NE",
    "comment": "44 pages, 0 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.00011v1",
    "published_date": "2024-11-13 23:27:01 UTC",
    "updated_date": "2024-11-13 23:27:01 UTC"
  },
  {
    "arxiv_id": "2411.09077v1",
    "title": "Drone Detection using Deep Neural Networks Trained on Pure Synthetic Data",
    "authors": [
      "Mariusz Wisniewski",
      "Zeeshan A. Rana",
      "Ivan Petrunin",
      "Alan Holt",
      "Stephen Harman"
    ],
    "abstract": "Drone detection has benefited from improvements in deep neural networks, but\nlike many other applications, suffers from the availability of accurate data\nfor training. Synthetic data provides a potential for low-cost data generation\nand has been shown to improve data availability and quality. However, models\ntrained on synthetic datasets need to prove their ability to perform on\nreal-world data, known as the problem of sim-to-real transferability. Here, we\npresent a drone detection Faster-RCNN model trained on a purely synthetic\ndataset that transfers to real-world data. We found that it achieves an AP_50\nof 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones -\ncompared with 97.8% for an equivalent model trained on real-world data. Our\nresults show that using synthetic data for drone detection has the potential to\nreduce data collection costs and improve labelling quality. These findings\ncould be a starting point for more elaborate synthetic drone datasets. For\nexample, realistic recreations of specific scenarios could de-risk the dataset\ngeneration of safety-critical applications such as the detection of drones at\nairports. Further, synthetic data may enable reliable drone detection systems,\nwhich could benefit other areas, such as unmanned traffic management systems.\nThe code is available\nhttps://github.com/mazqtpopx/cranfield-synthetic-drone-detection alongside the\ndatasets\nhttps://huggingface.co/datasets/mazqtpopx/cranfield-synthetic-drone-detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.09077v1",
    "published_date": "2024-11-13 23:09:53 UTC",
    "updated_date": "2024-11-13 23:09:53 UTC"
  },
  {
    "arxiv_id": "2411.09073v2",
    "title": "CHAI for LLMs: Improving Code-Mixed Translation in Large Language Models through Reinforcement Learning with AI Feedback",
    "authors": [
      "Wenbo Zhang",
      "Aditya Majumdar",
      "Amulya Yadav"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks but struggle with code-mixed (or code-switched) language\nunderstanding. For example, prior work benchmarking the performance of\nmultilingual LLMs on code-mixed translation tasks has demonstrated that current\nstate-of-the-art multilingual LLMs are ineffective in dealing with code-mixed\nlanguages. However, the question of how to improve the capability of\nmultilingual LLMs to handle code-mixed language has not received any attention\nto date. In this paper, we tackle this research gap by proposing CHAI, a novel\ngeneral-purpose framework for improving the ability of multilingual LLMs to\nhandle code-mixed languages. CHAI relies on three novel contributions made in\nthis paper. First, we explore the ability of LLMs to provide accurate\nannotations for code-mixed translation tasks. Second, we leverage this ability\nof LLMs as annotators to generate preference data for code-mixed translation\ntasks at scale, which are then used within a reinforcement learning from AI\nfeedback (RLAIF) procedure to improve LLMs' capability on code-mixed tasks.\nThird, we conduct a rigorous experimental evaluation across various real-world\ndatasets and settings. Our analysis shows that CHAI-powered LLMs outperform\nstate-of-the-art open-source LLMs by 25.66% (in terms of win rate adjudicated\nby human annotators) in code-mixed translation tasks. This work represents a\nfirst step towards developing more inclusive code-mixed LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "full draft: 8 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.09073v2",
    "published_date": "2024-11-13 22:56:00 UTC",
    "updated_date": "2025-02-26 20:11:35 UTC"
  },
  {
    "arxiv_id": "2411.09068v1",
    "title": "Liner Shipping Network Design with Reinforcement Learning",
    "authors": [
      "Utsav Dutta",
      "Yifan Lin",
      "Zhaoyang Larry Jin"
    ],
    "abstract": "This paper proposes a novel reinforcement learning framework to address the\nLiner Shipping Network Design Problem (LSNDP), a challenging combinatorial\noptimization problem focused on designing cost-efficient maritime shipping\nroutes. Traditional methods for solving the LSNDP typically involve decomposing\nthe problem into sub-problems, such as network design and multi-commodity flow,\nwhich are then tackled using approximate heuristics or large neighborhood\nsearch (LNS) techniques. In contrast, our approach employs a model-free\nreinforcement learning algorithm on the network design, integrated with a\nheuristic-based multi-commodity flow solver, to produce competitive results on\nthe publicly available LINERLIB benchmark. Additionally, our method also\ndemonstrates generalization capabilities by producing competitive solutions on\nthe benchmark instances after training on perturbed instances.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09068v1",
    "published_date": "2024-11-13 22:49:16 UTC",
    "updated_date": "2024-11-13 22:49:16 UTC"
  },
  {
    "arxiv_id": "2411.09065v1",
    "title": "Language-Model Prior Overcomes Cold-Start Items",
    "authors": [
      "Shiyu Wang",
      "Hao Ding",
      "Yupeng Gu",
      "Sergul Aydore",
      "Kousha Kalantari",
      "Branislav Kveton"
    ],
    "abstract": "The growth of recommender systems (RecSys) is driven by digitization and the\nneed for personalized content in areas such as e-commerce and video streaming.\nThe content in these systems often changes rapidly and therefore they\nconstantly face the ongoing cold-start problem, where new items lack\ninteraction data and are hard to value. Existing solutions for the cold-start\nproblem, such as content-based recommenders and hybrid methods, leverage item\nmetadata to determine item similarities. The main challenge with these methods\nis their reliance on structured and informative metadata to capture detailed\nitem similarities, which may not always be available. This paper introduces a\nnovel approach for cold-start item recommendation that utilizes the language\nmodel (LM) to estimate item similarities, which are further integrated as a\nBayesian prior with classic recommender systems. This approach is generic and\nable to boost the performance of various recommenders. Specifically, our\nexperiments integrate it with both sequential and collaborative filtering-based\nrecommender and evaluate it on two real-world datasets, demonstrating the\nenhanced performance of the proposed approach.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "This paper is dedicated to cold-start item recommendation using\n  language-model priors",
    "pdf_url": "http://arxiv.org/pdf/2411.09065v1",
    "published_date": "2024-11-13 22:45:52 UTC",
    "updated_date": "2024-11-13 22:45:52 UTC"
  },
  {
    "arxiv_id": "2411.09062v2",
    "title": "Multimodal Object Detection using Depth and Image Data for Manufacturing Parts",
    "authors": [
      "Nazanin Mahjourian",
      "Vinh Nguyen"
    ],
    "abstract": "Manufacturing requires reliable object detection methods for precise picking\nand handling of diverse types of manufacturing parts and components.\nTraditional object detection methods utilize either only 2D images from cameras\nor 3D data from lidars or similar 3D sensors. However, each of these sensors\nhave weaknesses and limitations. Cameras do not have depth perception and 3D\nsensors typically do not carry color information. These weaknesses can\nundermine the reliability and robustness of industrial manufacturing systems.\nTo address these challenges, this work proposes a multi-sensor system combining\nan red-green-blue (RGB) camera and a 3D point cloud sensor. The two sensors are\ncalibrated for precise alignment of the multimodal data captured from the two\nhardware devices. A novel multimodal object detection method is developed to\nprocess both RGB and depth data. This object detector is based on the Faster\nR-CNN baseline that was originally designed to process only camera images. The\nresults show that the multimodal model significantly outperforms the depth-only\nand RGB-only baselines on established object detection metrics. More\nspecifically, the multimodal model improves mAP by 13% and raises Mean\nPrecision by 11.8% in comparison to the RGB-only baseline. Compared to the\ndepth-only baseline, it improves mAP by 78% and raises Mean Precision by 57%.\nHence, this method facilitates more reliable and robust object detection in\nservice to smart manufacturing applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09062v2",
    "published_date": "2024-11-13 22:43:15 UTC",
    "updated_date": "2025-03-27 19:10:34 UTC"
  },
  {
    "arxiv_id": "2411.09055v1",
    "title": "SAFELOC: Overcoming Data Poisoning Attacks in Heterogeneous Federated Machine Learning for Indoor Localization",
    "authors": [
      "Akhil Singampalli",
      "Danish Gufran",
      "Sudeep Pasricha"
    ],
    "abstract": "Machine learning (ML) based indoor localization solutions are critical for\nmany emerging applications, yet their efficacy is often compromised by\nhardware/software variations across mobile devices (i.e., device heterogeneity)\nand the threat of ML data poisoning attacks. Conventional methods aimed at\ncountering these challenges show limited resilience to the uncertainties\ncreated by these phenomena. In response, in this paper, we introduce SAFELOC, a\nnovel framework that not only minimizes localization errors under these\nchallenging conditions but also ensures model compactness for efficient mobile\ndevice deployment. Our framework targets a distributed and co-operative\nlearning environment that uses federated learning (FL) to preserve user data\nprivacy and assumes heterogeneous mobile devices carried by users (just like in\nmost real-world scenarios). Within this heterogeneous FL context, SAFELOC\nintroduces a novel fused neural network architecture that performs data\npoisoning detection and localization, with a low model footprint. Additionally,\na dynamic saliency map-based aggregation strategy is designed to adapt based on\nthe severity of the detected data poisoning scenario. Experimental evaluations\ndemonstrate that SAFELOC achieves improvements of up to 5.9x in mean\nlocalization error, 7.8x in worst-case localization error, and a 2.1x reduction\nin model inference latency compared to state-of-the-art indoor localization\nframeworks, across diverse building floorplans, mobile devices, and ML data\npoisoning attack scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09055v1",
    "published_date": "2024-11-13 22:28:05 UTC",
    "updated_date": "2024-11-13 22:28:05 UTC"
  },
  {
    "arxiv_id": "2411.09050v1",
    "title": "The Systems Engineering Approach in Times of Large Language Models",
    "authors": [
      "Christian Cabrera",
      "Viviana Bastidas",
      "Jennifer Schooling",
      "Neil D. Lawrence"
    ],
    "abstract": "Using Large Language Models (LLMs) to address critical societal problems\nrequires adopting this novel technology into socio-technical systems. However,\nthe complexity of such systems and the nature of LLMs challenge such a vision.\nIt is unlikely that the solution to such challenges will come from the\nArtificial Intelligence (AI) community itself. Instead, the Systems Engineering\napproach is better equipped to facilitate the adoption of LLMs by prioritising\nthe problems and their context before any other aspects. This paper introduces\nthe challenges LLMs generate and surveys systems research efforts for\nengineering AI-based systems. We reveal how the systems engineering principles\nhave supported addressing similar issues to the ones LLMs pose and discuss our\nfindings to provide future directions for adopting LLMs.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted for the upcoming 58th Hawaii\n  International Conference on System Sciences (HICSS-58)",
    "pdf_url": "http://arxiv.org/pdf/2411.09050v1",
    "published_date": "2024-11-13 22:10:07 UTC",
    "updated_date": "2024-11-13 22:10:07 UTC"
  },
  {
    "arxiv_id": "2411.08992v2",
    "title": "IDCIA: Immunocytochemistry Dataset for Cellular Image Analysis",
    "authors": [
      "Abdurahman Ali Mohammed",
      "Catherine Fonder",
      "Donald S. Sakaguchi",
      "Wallapak Tavanapong",
      "Surya K. Mallapragada",
      "Azeez Idris"
    ],
    "abstract": "We present a new annotated microscopic cellular image dataset to improve the\neffectiveness of machine learning methods for cellular image analysis. Cell\ncounting is an important step in cell analysis. Typically, domain experts\nmanually count cells in a microscopic image. Automated cell counting can\npotentially eliminate this tedious, time-consuming process. However, a good,\nlabeled dataset is required for training an accurate machine learning model.\nOur dataset includes microscopic images of cells, and for each image, the cell\ncount and the location of individual cells. The data were collected as part of\nan ongoing study investigating the potential of electrical stimulation to\nmodulate stem cell differentiation and possible applications for neural repair.\nCompared to existing publicly available datasets, our dataset has more images\nof cells stained with more variety of antibodies (protein components of immune\nresponses against invaders) typically used for cell analysis. The experimental\nresults on this dataset indicate that none of the five existing models under\nthis study are able to achieve sufficiently accurate count to replace the\nmanual methods. The dataset is available at\nhttps://figshare.com/articles/dataset/Dataset/21970604.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08992v2",
    "published_date": "2024-11-13 19:33:08 UTC",
    "updated_date": "2024-11-19 14:51:07 UTC"
  },
  {
    "arxiv_id": "2411.08981v1",
    "title": "Reliability, Resilience and Human Factors Engineering for Trustworthy AI Systems",
    "authors": [
      "Saurabh Mishra",
      "Anand Rao",
      "Ramayya Krishnan",
      "Bilal Ayyub",
      "Amin Aria",
      "Enrico Zio"
    ],
    "abstract": "As AI systems become integral to critical operations across industries and\nservices, ensuring their reliability and safety is essential. We offer a\nframework that integrates established reliability and resilience engineering\nprinciples into AI systems. By applying traditional metrics such as failure\nrate and Mean Time Between Failures (MTBF) along with resilience engineering\nand human reliability analysis, we propose an integrate framework to manage AI\nsystem performance, and prevent or efficiently recover from failures. Our work\nadapts classical engineering methods to AI systems and outlines a research\nagenda for future technical studies. We apply our framework to a real-world AI\nsystem, using system status data from platforms such as openAI, to demonstrate\nits practical applicability. This framework aligns with emerging global\nstandards and regulatory frameworks, providing a methodology to enhance the\ntrustworthiness of AI systems. Our aim is to guide policy, regulation, and the\ndevelopment of reliable, safe, and adaptable AI technologies capable of\nconsistent performance in real-world environments.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08981v1",
    "published_date": "2024-11-13 19:16:44 UTC",
    "updated_date": "2024-11-13 19:16:44 UTC"
  },
  {
    "arxiv_id": "2411.08979v1",
    "title": "CoCoP: Enhancing Text Classification with LLM through Code Completion Prompt",
    "authors": [
      "Mohammad Mahdi Mohajeri",
      "Mohammad Javad Dousti",
      "Majid Nili Ahmadabadi"
    ],
    "abstract": "Text classification is a fundamental task in natural language processing\n(NLP), and large language models (LLMs) have demonstrated their capability to\nperform this task across various domains. However, the performance of LLMs\nheavily depends on the quality of their input prompts. Recent studies have also\nshown that LLMs exhibit remarkable results in code-related tasks. To leverage\nthe capabilities of LLMs in text classification, we propose the Code Completion\nPrompt (CoCoP) method, which transforms the text classification problem into a\ncode completion task. CoCoP significantly improves text classification\nperformance across diverse datasets by utilizing LLMs' code-completion\ncapability. For instance, CoCoP enhances the accuracy of the SST2 dataset by\nmore than 20%. Moreover, when CoCoP integrated with LLMs specifically designed\nfor code-related tasks (code models), such as CodeLLaMA, this method\ndemonstrates better or comparable performance to few-shot learning techniques\nwhile using only one-tenth of the model size. The source code of our proposed\nmethod will be available to the public upon the acceptance of the paper.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08979v1",
    "published_date": "2024-11-13 19:12:02 UTC",
    "updated_date": "2024-11-13 19:12:02 UTC"
  },
  {
    "arxiv_id": "2411.08975v1",
    "title": "Fluoroformer: Scaling multiple instance learning to multiplexed images via attention-based channel fusion",
    "authors": [
      "Marc Harary",
      "Eliezer M. Van Allen",
      "William Lotter"
    ],
    "abstract": "Though multiple instance learning (MIL) has been a foundational strategy in\ncomputational pathology for processing whole slide images (WSIs), current\napproaches are designed for traditional hematoxylin and eosin (H&E) slides\nrather than emerging multiplexed technologies. Here, we present an MIL\nstrategy, the Fluoroformer module, that is specifically tailored to multiplexed\nWSIs by leveraging scaled dot-product attention (SDPA) to interpretably fuse\ninformation across disparate channels. On a cohort of 434 non-small cell lung\ncancer (NSCLC) samples, we show that the Fluoroformer both obtains strong\nprognostic performance and recapitulates immuno-oncological hallmarks of NSCLC.\nOur technique thereby provides a path for adapting state-of-the-art AI\ntechniques to emerging spatial biology assays.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Findings paper presented at Machine Learning for Health (ML4H)\n  symposium 2024, December 15-16, 2024, Vancouver, Canada, 14 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.08975v1",
    "published_date": "2024-11-13 19:06:57 UTC",
    "updated_date": "2024-11-13 19:06:57 UTC"
  },
  {
    "arxiv_id": "2411.08954v2",
    "title": "Inconsistencies In Consistency Models: Better ODE Solving Does Not Imply Better Samples",
    "authors": [
      "NoÃ«l Vouitsis",
      "Rasa Hosseinzadeh",
      "Brendan Leigh Ross",
      "Valentin Villecroze",
      "Satya Krishna Gorti",
      "Jesse C. Cresswell",
      "Gabriel Loaiza-Ganem"
    ],
    "abstract": "Although diffusion models can generate remarkably high-quality samples, they\nare intrinsically bottlenecked by their expensive iterative sampling procedure.\nConsistency models (CMs) have recently emerged as a promising diffusion model\ndistillation method, reducing the cost of sampling by generating high-fidelity\nsamples in just a few iterations. Consistency model distillation aims to solve\nthe probability flow ordinary differential equation (ODE) defined by an\nexisting diffusion model. CMs are not directly trained to minimize error\nagainst an ODE solver, rather they use a more computationally tractable\nobjective. As a way to study how effectively CMs solve the probability flow\nODE, and the effect that any induced error has on the quality of generated\nsamples, we introduce Direct CMs, which \\textit{directly} minimize this error.\nIntriguingly, we find that Direct CMs reduce the ODE solving error compared to\nCMs but also result in significantly worse sample quality, calling into\nquestion why exactly CMs work well in the first place. Full code is available\nat: https://github.com/layer6ai-labs/direct-cms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 ATTRIB Workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.08954v2",
    "published_date": "2024-11-13 19:00:02 UTC",
    "updated_date": "2024-11-15 16:06:23 UTC"
  },
  {
    "arxiv_id": "2411.08879v1",
    "title": "4D Gaussian Splatting in the Wild with Uncertainty-Aware Regularization",
    "authors": [
      "Mijeong Kim",
      "Jongwoo Lim",
      "Bohyung Han"
    ],
    "abstract": "Novel view synthesis of dynamic scenes is becoming important in various\napplications, including augmented and virtual reality. We propose a novel 4D\nGaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded\nmonocular videos. To overcome the overfitting problem of existing work for\nthese real-world videos, we introduce an uncertainty-aware regularization that\nidentifies uncertain regions with few observations and selectively imposes\nadditional priors based on diffusion models and depth smoothness on such\nregions. This approach improves both the performance of novel view synthesis\nand the quality of training image reconstruction. We also identify the\ninitialization problem of 4DGS in fast-moving dynamic regions, where the\nStructure from Motion (SfM) algorithm fails to provide reliable 3D landmarks.\nTo initialize Gaussian primitives in such regions, we present a dynamic region\ndensification method using the estimated depth maps and scene flow. Our\nexperiments show that the proposed method improves the performance of 4DGS\nreconstruction from a video captured by a handheld monocular camera and also\nexhibits promising results in few-shot static scene reconstruction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.08879v1",
    "published_date": "2024-11-13 18:56:39 UTC",
    "updated_date": "2024-11-13 18:56:39 UTC"
  },
  {
    "arxiv_id": "2411.08878v1",
    "title": "A Short Note on Evaluating RepNet for Temporal Repetition Counting in Videos",
    "authors": [
      "Debidatta Dwibedi",
      "Yusuf Aytar",
      "Jonathan Tompson",
      "Pierre Sermanet",
      "Andrew Zisserman"
    ],
    "abstract": "We discuss some consistent issues on how RepNet has been evaluated in various\npapers. As a way to mitigate these issues, we report RepNet performance results\non different datasets, and release evaluation code and the RepNet checkpoint to\nobtain these results. Code URL:\nhttps://github.com/google-research/google-research/blob/master/repnet/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08878v1",
    "published_date": "2024-11-13 18:55:10 UTC",
    "updated_date": "2024-11-13 18:55:10 UTC"
  },
  {
    "arxiv_id": "2411.08875v1",
    "title": "Causal Explanations for Image Classifiers",
    "authors": [
      "Hana Chockler",
      "David A. Kelly",
      "Daniel Kroening",
      "Youcheng Sun"
    ],
    "abstract": "Existing algorithms for explaining the output of image classifiers use\ndifferent definitions of explanations and a variety of techniques to extract\nthem. However, none of the existing tools use a principled approach based on\nformal definitions of causes and explanations for the explanation extraction.\nIn this paper we present a novel black-box approach to computing explanations\ngrounded in the theory of actual causality. We prove relevant theoretical\nresults and present an algorithm for computing approximate explanations based\non these definitions. We prove termination of our algorithm and discuss its\ncomplexity and the amount of approximation compared to the precise definition.\nWe implemented the framework in a tool rex and we present experimental results\nand a comparison with state-of-the-art tools. We demonstrate that rex is the\nmost efficient tool and produces the smallest explanations, in addition to\noutperforming other black-box tools on standard quality measures.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08875v1",
    "published_date": "2024-11-13 18:52:42 UTC",
    "updated_date": "2024-11-13 18:52:42 UTC"
  },
  {
    "arxiv_id": "2412.12101v1",
    "title": "InterPLM: Discovering Interpretable Features in Protein Language Models via Sparse Autoencoders",
    "authors": [
      "Elana Simon",
      "James Zou"
    ],
    "abstract": "Protein language models (PLMs) have demonstrated remarkable success in\nprotein modeling and design, yet their internal mechanisms for predicting\nstructure and function remain poorly understood. Here we present a systematic\napproach to extract and analyze interpretable features from PLMs using sparse\nautoencoders (SAEs). By training SAEs on embeddings from the PLM ESM-2, we\nidentify up to 2,548 human-interpretable latent features per layer that\nstrongly correlate with up to 143 known biological concepts such as binding\nsites, structural motifs, and functional domains. In contrast, examining\nindividual neurons in ESM-2 reveals up to 46 neurons per layer with clear\nconceptual alignment across 15 known concepts, suggesting that PLMs represent\nmost concepts in superposition. Beyond capturing known annotations, we show\nthat ESM-2 learns coherent concepts that do not map onto existing annotations\nand propose a pipeline using language models to automatically interpret novel\nlatent features learned by the SAEs. As practical applications, we demonstrate\nhow these latent features can fill in missing annotations in protein databases\nand enable targeted steering of protein sequence generation. Our results\ndemonstrate that PLMs encode rich, interpretable representations of protein\nbiology and we propose a systematic framework to extract and analyze these\nlatent features. In the process, we recover both known biology and potentially\nnew protein motifs. As community resources, we introduce InterPLM\n(interPLM.ai), an interactive visualization platform for exploring and\nanalyzing learned PLM features, and release code for training and analysis at\ngithub.com/ElanaPearl/interPLM.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12101v1",
    "published_date": "2024-11-13 18:51:21 UTC",
    "updated_date": "2024-11-13 18:51:21 UTC"
  },
  {
    "arxiv_id": "2411.08870v2",
    "title": "The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models",
    "authors": [
      "Daniel P. Jeong",
      "Pranav Mani",
      "Saurabh Garg",
      "Zachary C. Lipton",
      "Michael Oberst"
    ],
    "abstract": "Several recent works seek to adapt general-purpose large language models\n(LLMs) and vision-language models (VLMs) for medical applications through\ncontinued pretraining on publicly available biomedical corpora. These works\ntypically claim that such domain-adaptive pretraining improves performance on\nvarious downstream medical tasks, such as answering medical exam questions. In\nthis paper, we compare ten \"medical\" LLMs and two VLMs against their\ncorresponding base models, arriving at a different conclusion: all medical VLMs\nand nearly all medical LLMs fail to consistently improve over their base models\nin the zero-/few-shot prompting and supervised fine-tuning regimes for medical\nquestion answering (QA). For instance, on clinical-note-based QA tasks in the\n3-shot setting, medical LLMs outperform their base models in only 26.7% of\ncases, reach a (statistical) tie in 16.7% of cases, and perform significantly\nworse in the remaining 56.7% of cases. Our conclusions are based on (i)\ncomparing each medical model directly against its base model; (ii) optimizing\nthe prompts for each model separately in zero-/few-shot prompting; and (iii)\naccounting for statistical uncertainty in comparisons. Our findings suggest\nthat state-of-the-art general-domain models may already exhibit strong medical\nknowledge and reasoning capabilities, and offer recommendations to strengthen\nthe conclusions of future studies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Extended version of EMNLP 2024 paper arXiv:2411.04118. Includes\n  additional results on clinical note QA tasks and supervised fine-tuning\n  evaluations",
    "pdf_url": "http://arxiv.org/pdf/2411.08870v2",
    "published_date": "2024-11-13 18:50:13 UTC",
    "updated_date": "2025-02-28 07:34:44 UTC"
  },
  {
    "arxiv_id": "2411.08861v1",
    "title": "Interaction Testing in Variation Analysis",
    "authors": [
      "Drago Plecko"
    ],
    "abstract": "Relationships of cause and effect are of prime importance for explaining\nscientific phenomena. Often, rather than just understanding the effects of\ncauses, researchers also wish to understand how a cause $X$ affects an outcome\n$Y$ mechanistically -- i.e., what are the causal pathways that are activated\nbetween $X$ and $Y$. For analyzing such questions, a range of methods has been\ndeveloped over decades under the rubric of causal mediation analysis.\nTraditional mediation analysis focuses on decomposing the average treatment\neffect (ATE) into direct and indirect effects, and therefore focuses on the ATE\nas the central quantity. This corresponds to providing explanations for\nassociations in the interventional regime, such as when the treatment $X$ is\nrandomized. Commonly, however, it is of interest to explain associations in the\nobservational regime, and not just in the interventional regime. In this paper,\nwe introduce \\text{variation analysis}, an extension of mediation analysis that\nfocuses on the total variation (TV) measure between $X$ and $Y$, written as\n$\\mathrm{E}[Y \\mid X=x_1] - \\mathrm{E}[Y \\mid X=x_0]$. The TV measure\nencompasses both causal and confounded effects, as opposed to the ATE which\nonly encompasses causal (direct and mediated) variations. In this way, the TV\nmeasure is suitable for providing explanations in the natural regime and\nanswering questions such as ``why is $X$ associated with $Y$?''. Our focus is\non decomposing the TV measure, in a way that explicitly includes direct,\nindirect, and confounded variations. Furthermore, we also decompose the TV\nmeasure to include interaction terms between these different pathways.\nSubsequently, interaction testing is introduced, involving hypothesis tests to\ndetermine if interaction terms are significantly different from zero. If\ninteractions are not significant, more parsimonious decompositions of the TV\nmeasure can be used.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08861v1",
    "published_date": "2024-11-13 18:42:34 UTC",
    "updated_date": "2024-11-13 18:42:34 UTC"
  },
  {
    "arxiv_id": "2411.08843v1",
    "title": "Data-driven Surface Solar Irradiance Estimation using Neural Operators at Global Scale",
    "authors": [
      "Alberto Carpentieri",
      "Jussi Leinonen",
      "Jeff Adie",
      "Boris Bonev",
      "Doris Folini",
      "Farah Hariri"
    ],
    "abstract": "Accurate surface solar irradiance (SSI) forecasting is essential for\noptimizing renewable energy systems, particularly in the context of long-term\nenergy planning on a global scale. This paper presents a pioneering approach to\nsolar radiation forecasting that leverages recent advancements in numerical\nweather prediction (NWP) and data-driven machine learning weather models. These\nadvances facilitate long, stable rollouts and enable large ensemble forecasts,\nenhancing the reliability of predictions. Our flexible model utilizes variables\nforecast by these NWP and AI weather models to estimate 6-hourly SSI at global\nscale. Developed using NVIDIA Modulus, our model represents the first adaptive\nglobal framework capable of providing long-term SSI forecasts. Furthermore, it\ncan be fine-tuned using satellite data, which significantly enhances its\nperformance in the fine-tuned regions, while maintaining accuracy elsewhere.\nThe improved accuracy of these forecasts has substantial implications for the\nintegration of solar energy into power grids, enabling more efficient energy\nmanagement and contributing to the global transition to renewable energy\nsources.",
    "categories": [
      "physics.ao-ph",
      "cs.AI"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08843v1",
    "published_date": "2024-11-13 18:21:56 UTC",
    "updated_date": "2024-11-13 18:21:56 UTC"
  },
  {
    "arxiv_id": "2411.08842v1",
    "title": "AstroM$^3$: A self-supervised multimodal model for astronomy",
    "authors": [
      "Mariia Rizhko",
      "Joshua S. Bloom"
    ],
    "abstract": "While machine-learned models are now routinely employed to facilitate\nastronomical inquiry, model inputs tend to be limited to a primary data source\n(namely images or time series) and, in the more advanced approaches, some\nmetadata. Yet with the growing use of wide-field, multiplexed observational\nresources, individual sources of interest often have a broad range of\nobservational modes available. Here we construct an astronomical multimodal\ndataset and propose AstroM$^3$, a self-supervised pre-training approach that\nenables a model to learn from multiple modalities simultaneously. Specifically,\nwe extend the CLIP (Contrastive Language-Image Pretraining) model to a trimodal\nsetting, allowing the integration of time-series photometry data, spectra, and\nastrophysical metadata. In a fine-tuning supervised setting, our results\ndemonstrate that CLIP pre-training improves classification performance for\ntime-series photometry, where accuracy increases from 84.6% to 91.5%.\nFurthermore, CLIP boosts classification accuracy by up to 12.6% when the\navailability of labeled data is limited, showing the effectiveness of\nleveraging larger corpora of unlabeled data. In addition to fine-tuned\nclassification, we can use the trained model in other downstream tasks that are\nnot explicitly contemplated during the construction of the self-supervised\nmodel. In particular we show the efficacy of using the learned embeddings for\nmisclassifications identification, similarity search, and anomaly detection.\nOne surprising highlight is the \"rediscovery\" of Mira subtypes and two\nRotational variable subclasses using manifold learning and dimension reduction\nalgorithm. To our knowledge this is the first construction of an $n>2$ mode\nmodel in astronomy. Extensions to $n>3$ modes is naturally anticipated with\nthis approach.",
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08842v1",
    "published_date": "2024-11-13 18:20:29 UTC",
    "updated_date": "2024-11-13 18:20:29 UTC"
  },
  {
    "arxiv_id": "2411.08832v2",
    "title": "Offline Adaptation of Quadruped Locomotion using Diffusion Models",
    "authors": [
      "Reece O'Mahoney",
      "Alexander L. Mitchell",
      "Wanming Yu",
      "Ingmar Posner",
      "Ioannis Havoutis"
    ],
    "abstract": "We present a diffusion-based approach to quadrupedal locomotion that\nsimultaneously addresses the limitations of learning and interpolating between\nmultiple skills and of (modes) offline adapting to new locomotion behaviours\nafter training. This is the first framework to apply classifier-free guided\ndiffusion to quadruped locomotion and demonstrate its efficacy by extracting\ngoal-conditioned behaviour from an originally unlabelled dataset. We show that\nthese capabilities are compatible with a multi-skill policy and can be applied\nwith little modification and minimal compute overhead, i.e., running entirely\non the robots onboard CPU. We verify the validity of our approach with hardware\nexperiments on the ANYmal quadruped platform.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08832v2",
    "published_date": "2024-11-13 18:12:15 UTC",
    "updated_date": "2025-03-10 07:30:55 UTC"
  },
  {
    "arxiv_id": "2411.08814v1",
    "title": "Process-aware Human Activity Recognition",
    "authors": [
      "Jiawei Zheng",
      "Petros Papapanagiotou",
      "Jacques D. Fleuriot",
      "Jane Hillston"
    ],
    "abstract": "Humans naturally follow distinct patterns when conducting their daily\nactivities, which are driven by established practices and processes, such as\nproduction workflows, social norms and daily routines. Human activity\nrecognition (HAR) algorithms usually use neural networks or machine learning\ntechniques to analyse inherent relationships within the data. However, these\napproaches often overlook the contextual information in which the data are\ngenerated, potentially limiting their effectiveness. We propose a novel\napproach that incorporates process information from context to enhance the HAR\nperformance. Specifically, we align probabilistic events generated by machine\nlearning models with process models derived from contextual information. This\nalignment adaptively weighs these two sources of information to optimise HAR\naccuracy. Our experiments demonstrate that our approach achieves better\naccuracy and Macro F1-score compared to baseline models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08814v1",
    "published_date": "2024-11-13 17:53:23 UTC",
    "updated_date": "2024-11-13 17:53:23 UTC"
  },
  {
    "arxiv_id": "2411.08813v1",
    "title": "Rethinking CyberSecEval: An LLM-Aided Approach to Evaluation Critique",
    "authors": [
      "Suhas Hariharan",
      "Zainab Ali Majid",
      "Jaime Raldua Veuthey",
      "Jacob Haimes"
    ],
    "abstract": "A key development in the cybersecurity evaluations space is the work carried\nout by Meta, through their CyberSecEval approach. While this work is\nundoubtedly a useful contribution to a nascent field, there are notable\nfeatures that limit its utility. Key drawbacks focus on the insecure code\ndetection part of Meta's methodology. We explore these limitations, and use our\nexploration as a test case for LLM-assisted benchmark analysis.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024, 2 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.08813v1",
    "published_date": "2024-11-13 17:51:57 UTC",
    "updated_date": "2024-11-13 17:51:57 UTC"
  },
  {
    "arxiv_id": "2411.08794v1",
    "title": "Evaluating World Models with LLM for Decision Making",
    "authors": [
      "Chang Yang",
      "Xinrun Wang",
      "Junzhe Jiang",
      "Qinggang Zhang",
      "Xiao Huang"
    ],
    "abstract": "World model emerges as a key module in decision making, where MuZero and\nDreamer achieve remarkable successes in complex tasks. Recent work leverages\nLarge Language Models (LLMs) as general world simulators to simulate the\ndynamics of the world due to their generalizability. LLMs also serve as the\nworld model for deliberative reasoning in Reasoning via Planning (RAP) and Tree\nof Thought (ToT). However, the world models are either evaluated as a general\nworld simulator, or as a functional module of the agent, i.e., predicting the\ntransitions to assist the planning. In this work, we propose a comprehensive\nevaluation of the world models with LLMs from the decision making perspective.\nSpecifically, we leverage the 31 diverse environments from (Wang et al.,\n2023;2024) and curate the rule-based policy of each environment for the diverse\nevaluation. Then, we design three main tasks, i.e., policy verification, action\nproposal, and policy planning, where the world models can be used for decision\nmaking solely. Finally, we conduct the comprehensive evaluation of the advanced\nLLMs, i.e., GPT-4o and GPT-4o-mini, on the environments for the three main\ntasks under various settings. The key observations include: i) GPT-4o\nsignificantly outperforms GPT-4o-mini on the three main tasks, especially for\nthe tasks which require the domain knowledge, ii) the performance of the world\nmodel with LLM will be decreased for long-term decision-making tasks, and iii)\nthe combination of different functionalities of the world model will brings\nadditional unstabilities of the performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08794v1",
    "published_date": "2024-11-13 17:19:32 UTC",
    "updated_date": "2024-11-13 17:19:32 UTC"
  },
  {
    "arxiv_id": "2411.08790v1",
    "title": "Can sparse autoencoders be used to decompose and interpret steering vectors?",
    "authors": [
      "Harry Mayne",
      "Yushi Yang",
      "Adam Mahdi"
    ],
    "abstract": "Steering vectors are a promising approach to control the behaviour of large\nlanguage models. However, their underlying mechanisms remain poorly understood.\nWhile sparse autoencoders (SAEs) may offer a potential method to interpret\nsteering vectors, recent findings show that SAE-reconstructed vectors often\nlack the steering properties of the original vectors. This paper investigates\nwhy directly applying SAEs to steering vectors yields misleading\ndecompositions, identifying two reasons: (1) steering vectors fall outside the\ninput distribution for which SAEs are designed, and (2) steering vectors can\nhave meaningful negative projections in feature directions, which SAEs are not\ndesigned to accommodate. These limitations hinder the direct use of SAEs for\ninterpreting steering vectors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08790v1",
    "published_date": "2024-11-13 17:16:48 UTC",
    "updated_date": "2024-11-13 17:16:48 UTC"
  },
  {
    "arxiv_id": "2411.08785v1",
    "title": "Zero-shot Cross-lingual Transfer Learning with Multiple Source and Target Languages for Information Extraction: Language Selection and Adversarial Training",
    "authors": [
      "Nghia Trung Ngo",
      "Thien Huu Nguyen"
    ],
    "abstract": "The majority of previous researches addressing multi-lingual IE are limited\nto zero-shot cross-lingual single-transfer (one-to-one) setting, with\nhigh-resource languages predominantly as source training data. As a result,\nthese works provide little understanding and benefit for the realistic goal of\ndeveloping a multi-lingual IE system that can generalize to as many languages\nas possible. Our study aims to fill this gap by providing a detailed analysis\non Cross-Lingual Multi-Transferability (many-to-many transfer learning), for\nthe recent IE corpora that cover a diverse set of languages. Specifically, we\nfirst determine the correlation between single-transfer performance and a wide\nrange of linguistic-based distances. From the obtained insights, a combined\nlanguage distance metric can be developed that is not only highly correlated\nbut also robust across different tasks and model scales. Next, we investigate\nthe more general zero-shot multi-lingual transfer settings where multiple\nlanguages are involved in the training and evaluation processes. Language\nclustering based on the newly defined distance can provide directions for\nachieving the optimal cost-performance trade-off in data (languages) selection\nproblem. Finally, a relational-transfer setting is proposed to further\nincorporate multi-lingual unlabeled data based on adversarial training using\nthe relation induced from the above linguistic distance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08785v1",
    "published_date": "2024-11-13 17:13:25 UTC",
    "updated_date": "2024-11-13 17:13:25 UTC"
  },
  {
    "arxiv_id": "2411.08768v1",
    "title": "Sharingan: Extract User Action Sequence from Desktop Recordings",
    "authors": [
      "Yanting Chen",
      "Yi Ren",
      "Xiaoting Qin",
      "Jue Zhang",
      "Kehong Yuan",
      "Lu Han",
      "Qingwei Lin",
      "Dongmei Zhang",
      "Saravan Rajmohan",
      "Qi Zhang"
    ],
    "abstract": "Video recordings of user activities, particularly desktop recordings, offer a\nrich source of data for understanding user behaviors and automating processes.\nHowever, despite advancements in Vision-Language Models (VLMs) and their\nincreasing use in video analysis, extracting user actions from desktop\nrecordings remains an underexplored area. This paper addresses this gap by\nproposing two novel VLM-based methods for user action extraction: the Direct\nFrame-Based Approach (DF), which inputs sampled frames directly into VLMs, and\nthe Differential Frame-Based Approach (DiffF), which incorporates explicit\nframe differences detected via computer vision techniques. We evaluate these\nmethods using a basic self-curated dataset and an advanced benchmark adapted\nfrom prior work. Our results show that the DF approach achieves an accuracy of\n70% to 80% in identifying user actions, with the extracted action sequences\nbeing re-playable though Robotic Process Automation. We find that while VLMs\nshow potential, incorporating explicit UI changes can degrade performance,\nmaking the DF approach more reliable. This work represents the first\napplication of VLMs for extracting user action sequences from desktop\nrecordings, contributing new methods, benchmarks, and insights for future\nresearch.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08768v1",
    "published_date": "2024-11-13 16:53:29 UTC",
    "updated_date": "2024-11-13 16:53:29 UTC"
  },
  {
    "arxiv_id": "2411.08767v2",
    "title": "SANDWICH: Towards an Offline, Differentiable, Fully-Trainable Wireless Neural Ray-Tracing Surrogate",
    "authors": [
      "Yifei Jin",
      "Ali Maatouk",
      "Sarunas Girdzijauskas",
      "Shugong Xu",
      "Leandros Tassiulas",
      "Rex Ying"
    ],
    "abstract": "Wireless ray-tracing (RT) is emerging as a key tool for three-dimensional\n(3D) wireless channel modeling, driven by advances in graphical rendering.\nCurrent approaches struggle to accurately model beyond 5G (B5G) network\nsignaling, which often operates at higher frequencies and is more susceptible\nto environmental conditions and changes. Existing online learning solutions\nrequire real-time environmental supervision during training, which is both\ncostly and incompatible with GPU-based processing. In response, we propose a\nnovel approach that redefines ray trajectory generation as a sequential\ndecision-making problem, leveraging generative models to jointly learn the\noptical, physical, and signal properties within each designated environment.\nOur work introduces the Scene-Aware Neural Decision Wireless Channel Raytracing\nHierarchy (SANDWICH), an innovative offline, fully differentiable approach that\ncan be trained entirely on GPUs. SANDWICH offers superior performance compared\nto existing online learning methods, outperforms the baseline by 4e^-2 radian\nin RT accuracy, and only fades 0.5 dB away from toplined channel gain\nestimation.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "Accepted in ICMLCN 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.08767v2",
    "published_date": "2024-11-13 16:53:14 UTC",
    "updated_date": "2025-02-20 21:46:32 UTC"
  },
  {
    "arxiv_id": "2411.08764v1",
    "title": "Flow reconstruction in time-varying geometries using graph neural networks",
    "authors": [
      "Bogdan A. Danciu",
      "Vito A. Pagone",
      "Benjamin BÃ¶hm",
      "Marius Schmidt",
      "Christos E. Frouzakis"
    ],
    "abstract": "The paper presents a Graph Attention Convolutional Network (GACN) for flow\nreconstruction from very sparse data in time-varying geometries. The model\nincorporates a feature propagation algorithm as a preprocessing step to handle\nextremely sparse inputs, leveraging information from neighboring nodes to\ninitialize missing features. In addition, a binary indicator is introduced as a\nvalidity mask to distinguish between the original and propagated data points,\nenabling more effective learning from sparse inputs. Trained on a unique data\nset of Direct Numerical Simulations (DNS) of a motored engine at a technically\nrelevant operating condition, the GACN shows robust performance across\ndifferent resolutions and domain sizes and can effectively handle unstructured\ndata and variable input sizes. The model is tested on previously unseen DNS\ndata as well as on an experimental data set from Particle Image Velocimetry\n(PIV) measurements that were not considered during training. A comparative\nanalysis shows that the GACN consistently outperforms both a conventional\nConvolutional Neural Network (CNN) and cubic interpolation methods on the DNS\nand PIV test sets by achieving lower reconstruction errors and better capturing\nfine-scale turbulent structures. In particular, the GACN effectively\nreconstructs flow fields from domains up to 14 times larger than those observed\nduring training, with the performance advantage increasing for larger domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08764v1",
    "published_date": "2024-11-13 16:49:56 UTC",
    "updated_date": "2024-11-13 16:49:56 UTC"
  },
  {
    "arxiv_id": "2411.08745v3",
    "title": "Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers",
    "authors": [
      "ClÃ©ment Dumas",
      "Chris Wendler",
      "Veniamin Veselovsky",
      "Giovanni Monea",
      "Robert West"
    ],
    "abstract": "A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean over\nlatents across different languages does not impair and instead improves the\nmodels' performance in translating the concept. Our results provide evidence\nfor the existence of language-agnostic concept representations within the\ninvestigated models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 14 figures, previous version published under the title \"How\n  Do Llamas Process Multilingual Text? A Latent Exploration through Activation\n  Patching\" at the ICML 2024 mechanistic interpretability workshop at\n  https://openreview.net/forum?id=0ku2hIm4BS",
    "pdf_url": "http://arxiv.org/pdf/2411.08745v3",
    "published_date": "2024-11-13 16:26:19 UTC",
    "updated_date": "2025-01-09 21:53:56 UTC"
  },
  {
    "arxiv_id": "2411.08728v1",
    "title": "Polymetis:Large Language Modeling for Multiple Material Domains",
    "authors": [
      "Chao Huang",
      "Huichen Xiao",
      "Chen Chen",
      "Chunyan Chen",
      "Yi Zhao",
      "Shiyu Du",
      "Yiming Zhang",
      "He Sha",
      "Ruixin Gu"
    ],
    "abstract": "As the application of large language models in various fields continues to\nexpand, materials science also ushers in opportunities for AI-driven\ninnovation. The traditional way of relying on manual search for materials\nscience-related information is now using artificial intelligence technology as\nan auxiliary tool to improve the efficiency of materials science research. To\naccelerate researchers' knowledge acquisition and intelligent decision-making\nsupport in materials science research, this paper proposes a large language\nmodel Polymetis model for a variety of materials fields, aiming to provide\nhighly professional knowledge answers in the field of materials, covering\nenergy materials, functional materials, alloy materials, physical chemistry,\nbiology, and other material directions. The model uses a dataset of about 2\nmillion material knowledge instructions, and in the process of building the\ndataset, we developed the Intelligent Extraction Large Model (IELM), which is\nspecially used to extract and form structured knowledge from scientific texts,\navoiding a large number of costs that need to be manually annotated, and\nimproving efficiency. We inject this data into the GLM4-9B model for learning\nto enhance its inference capabilities in a variety of material domains. In\naddition, we have introduced enhanced prompt strategies to ensure that the\nanswers to the model are more organized and comprehensive, providing efficient\nand comprehensive intelligent support for the diverse needs of materials\nscience exploration, and promoting the development of material science.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08728v1",
    "published_date": "2024-11-13 16:10:14 UTC",
    "updated_date": "2024-11-13 16:10:14 UTC"
  },
  {
    "arxiv_id": "2411.08706v1",
    "title": "Searching Latent Program Spaces",
    "authors": [
      "ClÃ©ment Bonnet",
      "Matthew V Macfarlane"
    ],
    "abstract": "Program synthesis methods aim to automatically generate programs restricted\nto a language that can explain a given specification of input-output pairs.\nWhile purely symbolic approaches suffer from a combinatorial search space,\nrecent methods leverage neural networks to learn distributions over program\nstructures to narrow this search space significantly, enabling more efficient\nsearch. However, for challenging problems, it remains difficult to train models\nto perform program synthesis in one shot, making test-time search essential.\nMost neural methods lack structured search mechanisms during inference, relying\ninstead on stochastic sampling or gradient updates, which can be inefficient.\nIn this work, we propose the Latent Program Network (LPN), a general algorithm\nfor program induction that learns a distribution over latent programs in a\ncontinuous space, enabling efficient search and test-time adaptation. We\nexplore how to train these networks to optimize for test-time computation and\ndemonstrate the use of gradient-based search both during training and at test\ntime. We evaluate LPN on ARC-AGI, a program synthesis benchmark that evaluates\nperformance by generalizing programs to new inputs rather than explaining the\nunderlying specification. We show that LPN can generalize beyond its training\ndistribution and adapt to unseen tasks by utilizing test-time computation,\noutperforming algorithms without test-time adaptation mechanisms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code available at https://github.com/clement-bonnet/lpn",
    "pdf_url": "http://arxiv.org/pdf/2411.08706v1",
    "published_date": "2024-11-13 15:50:32 UTC",
    "updated_date": "2024-11-13 15:50:32 UTC"
  },
  {
    "arxiv_id": "2411.08703v1",
    "title": "MVKTrans: Multi-View Knowledge Transfer for Robust Multiomics Classification",
    "authors": [
      "Shan Cong",
      "Zhiling Sang",
      "Hongwei Liu",
      "Haoran Luo",
      "Xin Wang",
      "Hong Liang",
      "Jie Hao",
      "Xiaohui Yao"
    ],
    "abstract": "The distinct characteristics of multiomics data, including complex\ninteractions within and across biological layers and disease heterogeneity\n(e.g., heterogeneity in etiology and clinical symptoms), drive us to develop\nnovel designs to address unique challenges in multiomics prediction. In this\npaper, we propose the multi-view knowledge transfer learning (MVKTrans)\nframework, which transfers intra- and inter-omics knowledge in an adaptive\nmanner by reviewing data heterogeneity and suppressing bias transfer, thereby\nenhancing classification performance. Specifically, we design a graph\ncontrastive module that is trained on unlabeled data to effectively learn and\ntransfer the underlying intra-omics patterns to the supervised task. This\nunsupervised pretraining promotes learning general and unbiased representations\nfor each modality, regardless of the downstream tasks. In light of the varying\ndiscriminative capacities of modalities across different diseases and/or\nsamples, we introduce an adaptive and bi-directional cross-omics distillation\nmodule. This module automatically identifies richer modalities and facilitates\ndynamic knowledge transfer from more informative to less informative omics,\nthereby enabling a more robust and generalized integration. Extensive\nexperiments on four real biomedical datasets demonstrate the superior\nperformance and robustness of MVKTrans compared to the state-of-the-art. Code\nand data are available at https://github.com/Yaolab-fantastic/MVKTrans.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08703v1",
    "published_date": "2024-11-13 15:45:46 UTC",
    "updated_date": "2024-11-13 15:45:46 UTC"
  },
  {
    "arxiv_id": "2411.08701v1",
    "title": "TRACE: Transformer-based Risk Assessment for Clinical Evaluation",
    "authors": [
      "Dionysis Christopoulos",
      "Sotiris Spanos",
      "Valsamis Ntouskos",
      "Konstantinos Karantzalos"
    ],
    "abstract": "We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation),\na novel method for clinical risk assessment based on clinical data, leveraging\nthe self-attention mechanism for enhanced feature interaction and result\ninterpretation. Our approach is able to handle different data modalities,\nincluding continuous, categorical and multiple-choice (checkbox) attributes.\nThe proposed architecture features a shared representation of the clinical data\nobtained by integrating specialized embeddings of each data modality, enabling\nthe detection of high-risk individuals using Transformer encoder layers. To\nassess the effectiveness of the proposed method, a strong baseline based on\nnon-negative multi-layer perceptrons (MLPs) is introduced. The proposed method\noutperforms various baselines widely used in the domain of clinical risk\nassessment, while effectively handling missing values. In terms of\nexplainability, our Transformer-based method offers easily interpretable\nresults via attention weights, further enhancing the clinicians'\ndecision-making process.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08701v1",
    "published_date": "2024-11-13 15:42:28 UTC",
    "updated_date": "2024-11-13 15:42:28 UTC"
  },
  {
    "arxiv_id": "2411.08700v1",
    "title": "Rethinking negative sampling in content-based news recommendation",
    "authors": [
      "Miguel Ãngelo Rebelo",
      "JoÃ£o Vinagre",
      "Ivo Pereira",
      "Ãlvaro Figueira"
    ],
    "abstract": "News recommender systems are hindered by the brief lifespan of articles, as\nthey undergo rapid relevance decay. Recent studies have demonstrated the\npotential of content-based neural techniques in tackling this problem. However,\nthese models often involve complex neural architectures and often lack\nconsideration for negative examples. In this study, we posit that the careful\nsampling of negative examples has a big impact on the model's outcome. We\ndevise a negative sampling technique that not only improves the accuracy of the\nmodel but also facilitates the decentralization of the recommendation system.\nThe experimental results obtained using the MIND dataset demonstrate that the\naccuracy of the method under consideration can compete with that of\nState-of-the-Art models. The utilization of the sampling technique is essential\nin reducing model complexity and accelerating the training process, while\nmaintaining a high level of accuracy. Finally, we discuss how decentralized\nmodels can help improve privacy and scalability.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08700v1",
    "published_date": "2024-11-13 15:42:13 UTC",
    "updated_date": "2024-11-13 15:42:13 UTC"
  },
  {
    "arxiv_id": "2411.08696v1",
    "title": "Scholarly Wikidata: Population and Exploration of Conference Data in Wikidata using LLMs",
    "authors": [
      "Nandana Mihindukulasooriya",
      "Sanju Tiwari",
      "Daniil Dobriy",
      "Finn Ãrup Nielsen",
      "Tek Raj Chhetri",
      "Axel Polleres"
    ],
    "abstract": "Several initiatives have been undertaken to conceptually model the domain of\nscholarly data using ontologies and to create respective Knowledge Graphs. Yet,\nthe full potential seems unleashed, as automated means for automatic population\nof said ontologies are lacking, and respective initiatives from the Semantic\nWeb community are not necessarily connected: we propose to make scholarly data\nmore sustainably accessible by leveraging Wikidata's infrastructure and\nautomating its population in a sustainable manner through LLMs by tapping into\nunstructured sources like conference Web sites and proceedings texts as well as\nalready existing structured conference datasets. While an initial analysis\nshows that Semantic Web conferences are only minimally represented in Wikidata,\nwe argue that our methodology can help to populate, evolve and maintain\nscholarly data as a community within Wikidata. Our main contributions include\n(a) an analysis of ontologies for representing scholarly data to identify gaps\nand relevant entities/properties in Wikidata, (b) semi-automated extraction --\nrequiring (minimal) manual validation -- of conference metadata (e.g.,\nacceptance rates, organizer roles, programme committee members, best paper\nawards, keynotes, and sponsors) from websites and proceedings texts using LLMs.\nFinally, we discuss (c) extensions to visualization tools in the Wikidata\ncontext for data exploration of the generated scholarly data. Our study focuses\non data from 105 Semantic Web-related conferences and extends/adds more than\n6000 entities in Wikidata. It is important to note that the method can be more\ngenerally applicable beyond Semantic Web-related conferences for enhancing\nWikidata's utility as a comprehensive scholarly resource.\n  Source Repository: https://github.com/scholarly-wikidata/\n  DOI: https://doi.org/10.5281/zenodo.10989709\n  License: Creative Commons CC0 (Data), MIT (Code)",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DL",
    "comment": "17 pages, accepted at EKAW-24",
    "pdf_url": "http://arxiv.org/pdf/2411.08696v1",
    "published_date": "2024-11-13 15:34:52 UTC",
    "updated_date": "2024-11-13 15:34:52 UTC"
  },
  {
    "arxiv_id": "2411.08684v1",
    "title": "Analogical Reasoning Within a Conceptual Hyperspace",
    "authors": [
      "Howard Goldowsky",
      "Vasanth Sarathy"
    ],
    "abstract": "We propose an approach to analogical inference that marries the\nneuro-symbolic computational power of complex-sampled hyperdimensional\ncomputing (HDC) with Conceptual Spaces Theory (CST), a promising theory of\nsemantic meaning. CST sketches, at an abstract level, approaches to analogical\ninference that go beyond the standard predicate-based structure mapping\ntheories. But it does not describe how such an approach can be operationalized.\nWe propose a concrete HDC-based architecture that computes several types of\nanalogy classified by CST. We present preliminary proof-of-concept experimental\nresults within a toy domain and describe how it can perform category-based and\nproperty-based analogical reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Analogy-angle workshop full paper at IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.08684v1",
    "published_date": "2024-11-13 15:20:14 UTC",
    "updated_date": "2024-11-13 15:20:14 UTC"
  },
  {
    "arxiv_id": "2411.08666v2",
    "title": "A Survey on Vision Autoregressive Model",
    "authors": [
      "Kai Jiang",
      "Jiaxing Huang"
    ],
    "abstract": "Autoregressive models have demonstrated great performance in natural language\nprocessing (NLP) with impressive scalability, adaptability and\ngeneralizability. Inspired by their notable success in NLP field,\nautoregressive models have been intensively investigated recently for computer\nvision, which perform next-token predictions by representing visual data as\nvisual tokens and enables autoregressive modelling for a wide range of vision\ntasks, ranging from visual generation and visual understanding to the very\nrecent multimodal generation that unifies visual generation and understanding\nwith a single autoregressive model. This paper provides a systematic review of\nvision autoregressive models, including the development of a taxonomy of\nexisting methods and highlighting their major contributions, strengths, and\nlimitations, covering various vision tasks such as image generation, video\ngeneration, image editing, motion generation, medical image analysis, 3D\ngeneration, robotic manipulation, unified multimodal generation, etc. Besides,\nwe investigate and analyze the latest advancements in autoregressive models,\nincluding thorough benchmarking and discussion of existing methods across\nvarious evaluation datasets. Finally, we outline key challenges and promising\ndirections for future research, offering a roadmap to guide further\nadvancements in vision autoregressive models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work will be integrated into another project",
    "pdf_url": "http://arxiv.org/pdf/2411.08666v2",
    "published_date": "2024-11-13 14:59:41 UTC",
    "updated_date": "2024-11-16 11:17:49 UTC"
  },
  {
    "arxiv_id": "2411.17708v1",
    "title": "Towards Efficient Neurally-Guided Program Induction for ARC-AGI",
    "authors": [
      "Simon Ouellette"
    ],
    "abstract": "ARC-AGI is an open-world problem domain in which the ability to generalize\nout-of-distribution is a crucial quality. Under the program induction paradigm,\nwe present a series of experiments that reveal the efficiency and\ngeneralization characteristics of various neurally-guided program induction\napproaches. The three paradigms we consider are Learning the grid space,\nLearning the program space, and Learning the transform space. We implement and\nexperiment thoroughly on the first two, and retain the second one for ARC-AGI\nsubmission. After identifying the strengths and weaknesses of both of these\napproaches, we suggest the third as a potential solution, and run preliminary\nexperiments.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17708v1",
    "published_date": "2024-11-13 14:44:03 UTC",
    "updated_date": "2024-11-13 14:44:03 UTC"
  },
  {
    "arxiv_id": "2411.08651v1",
    "title": "Estimating unknown parameters in differential equations with a reinforcement learning based PSO method",
    "authors": [
      "Wenkui Sun",
      "Xiaoya Fan",
      "Lijuan Jia",
      "Tinyi Chu",
      "Shing-Tung Yau",
      "Rongling Wu",
      "Zhong Wang"
    ],
    "abstract": "Differential equations offer a foundational yet powerful framework for\nmodeling interactions within complex dynamic systems and are widely applied\nacross numerous scientific fields. One common challenge in this area is\nestimating the unknown parameters of these dynamic relationships. However,\ntraditional numerical optimization methods rely on the selection of initial\nparameter values, making them prone to local optima. Meanwhile, deep learning\nand Bayesian methods require training models on specific differential\nequations, resulting in poor versatility. This paper reformulates the parameter\nestimation problem of differential equations as an optimization problem by\nintroducing the concept of particles from the particle swarm optimization\nalgorithm. Building on reinforcement learning-based particle swarm optimization\n(RLLPSO), this paper proposes a novel method, DERLPSO, for estimating unknown\nparameters of differential equations. We compared its performance on three\ntypical ordinary differential equations with the state-of-the-art methods,\nincluding the RLLPSO algorithm, traditional numerical methods, deep learning\napproaches, and Bayesian methods. The experimental results demonstrate that our\nDERLPSO consistently outperforms other methods in terms of performance,\nachieving an average Mean Square Error of 1.13e-05, which reduces the error by\napproximately 4 orders of magnitude compared to other methods. Apart from\nordinary differential equations, our DERLPSO also show great promise for\nestimating unknown parameters of partial differential equations. The DERLPSO\nmethod proposed in this paper has high accuracy, is independent of initial\nparameter values, and possesses strong versatility and stability. This work\nprovides new insights into unknown parameter estimation for differential\nequations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08651v1",
    "published_date": "2024-11-13 14:40:51 UTC",
    "updated_date": "2024-11-13 14:40:51 UTC"
  },
  {
    "arxiv_id": "2411.08645v1",
    "title": "A System Level Performance Evaluation for Superconducting Digital Systems",
    "authors": [
      "Joyjit Kundu",
      "Debjyoti Bhattacharjee",
      "Nathan Josephsen",
      "Ankit Pokhrel",
      "Udara De Silva",
      "Wenzhe Guo",
      "Steven Van Winckel",
      "Steven Brebels",
      "Manu Perumkunnil",
      "Quentin Herr",
      "Anna Herr"
    ],
    "abstract": "Superconducting Digital (SCD) technology offers significant potential for\nenhancing the performance of next generation large scale compute workloads. By\nleveraging advanced lithography and a 300 mm platform, SCD devices can reduce\nenergy consumption and boost computational power. This paper presents a\ncross-layer modeling approach to evaluate the system-level performance benefits\nof SCD architectures for Large Language Model (LLM) training and inference. Our\nfindings, based on experimental data and Pulse Conserving Logic (PCL) design\nprinciples, demonstrate substantial performance gain in both training and\ninference. We are, thus, able to convincingly show that the SCD technology can\naddress memory and interconnect limitations of present day solutions for\nnext-generation compute systems.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.AR",
    "comment": "8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.08645v1",
    "published_date": "2024-11-13 14:36:12 UTC",
    "updated_date": "2024-11-13 14:36:12 UTC"
  },
  {
    "arxiv_id": "2411.08642v1",
    "title": "Towards More Accurate Fake Detection on Images Generated from Advanced Generative and Neural Rendering Models",
    "authors": [
      "Chengdong Dong",
      "Vijayakumar Bhagavatula",
      "Zhenyu Zhou",
      "Ajay Kumar"
    ],
    "abstract": "The remarkable progress in neural-network-driven visual data generation,\nespecially with neural rendering techniques like Neural Radiance Fields and 3D\nGaussian splatting, offers a powerful alternative to GANs and diffusion models.\nThese methods can produce high-fidelity images and lifelike avatars,\nhighlighting the need for robust detection methods. In response, an\nunsupervised training technique is proposed that enables the model to extract\ncomprehensive features from the Fourier spectrum magnitude, thereby overcoming\nthe challenges of reconstructing the spectrum due to its centrosymmetric\nproperties. By leveraging the spectral domain and dynamically combining it with\nspatial domain information, we create a robust multimodal detector that\ndemonstrates superior generalization capabilities in identifying challenging\nsynthetic images generated by the latest image synthesis techniques. To address\nthe absence of a 3D neural rendering-based fake image database, we develop a\ncomprehensive database that includes images generated by diverse neural\nrendering techniques, providing a robust foundation for evaluating and\nadvancing detection methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 8 Figures",
    "pdf_url": "http://arxiv.org/pdf/2411.08642v1",
    "published_date": "2024-11-13 14:32:28 UTC",
    "updated_date": "2024-11-13 14:32:28 UTC"
  },
  {
    "arxiv_id": "2411.08641v1",
    "title": "DipMe: Haptic Recognition of Granular Media for Tangible Interactive Applications",
    "authors": [
      "Xinkai Wang",
      "Shuo Zhang",
      "Ziyi Zhao",
      "Lifeng Zhu",
      "Aiguo Song"
    ],
    "abstract": "While tangible user interface has shown its power in naturally interacting\nwith rigid or soft objects, users cannot conveniently use different types of\ngranular materials as the interaction media. We introduce DipMe as a smart\ndevice to recognize the types of granular media in real time, which can be used\nto connect the granular materials in the physical world with various virtual\ncontent. Other than vision-based solutions, we propose a dip operation of our\ndevice and exploit the haptic signals to recognize different types of granular\nmaterials. With modern machine learning tools, we find the haptic signals from\ndifferent granular media are distinguishable by DipMe. With the online granular\nobject recognition, we build several tangible interactive applications,\ndemonstrating the effects of DipMe in perceiving granular materials and its\npotential in developing a tangible user interface with granular objects as the\nnew media.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "17 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.08641v1",
    "published_date": "2024-11-13 14:32:10 UTC",
    "updated_date": "2024-11-13 14:32:10 UTC"
  },
  {
    "arxiv_id": "2411.08622v1",
    "title": "Precision-Focused Reinforcement Learning Model for Robotic Object Pushing",
    "authors": [
      "Lara Bergmann",
      "David Leins",
      "Robert Haschke",
      "Klaus Neumann"
    ],
    "abstract": "Non-prehensile manipulation, such as pushing objects to a desired target\nposition, is an important skill for robots to assist humans in everyday\nsituations. However, the task is challenging due to the large variety of\nobjects with different and sometimes unknown physical properties, such as\nshape, size, mass, and friction. This can lead to the object overshooting its\ntarget position, requiring fast corrective movements of the robot around the\nobject, especially in cases where objects need to be precisely pushed. In this\npaper, we improve the state-of-the-art by introducing a new memory-based\nvision-proprioception RL model to push objects more precisely to target\npositions using fewer corrective movements.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08622v1",
    "published_date": "2024-11-13 14:08:58 UTC",
    "updated_date": "2024-11-13 14:08:58 UTC"
  },
  {
    "arxiv_id": "2411.08605v1",
    "title": "Lo-MARVE: A Low Cost Autonomous Underwater Vehicle for Marine Exploration",
    "authors": [
      "Karl Mason",
      "Daniel Kelly"
    ],
    "abstract": "This paper presents Low-cost Marine Autonomous Robotic Vehicle Explorer\n(Lo-MARVE), a novel autonomous underwater vehicle (AUV) designed to provide a\nlow cost solution for underwater exploration and environmental monitoring in\nshallow water environments. Lo-MARVE offers a cost-effective alternative to\nexisting AUVs, featuring a modular design, low-cost sensors, and wireless\ncommunication capabilities. The total cost of Lo-MARVE is approximately EUR\n500. Lo-MARVE is developed using the Raspberry Pi 4B microprocessor, with\ncontrol software written in Python. The proposed AUV was validated through\nfield testing outside of a laboratory setting, in the freshwater environment of\nthe River Corrib in Galway, Ireland. This demonstrates its ability to navigate\nautonomously, collect data, and communicate effectively outside of a controlled\nlaboratory setting. The successful deployment of Lo-MARVE in a real-world\nenvironment validates its proof of concept.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This paper was presented at the 12th International Conference on\n  Control, Mechatronics and Automation (ICCMA 2024), held in London, UK, from\n  November 11-13, 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.08605v1",
    "published_date": "2024-11-13 13:45:54 UTC",
    "updated_date": "2024-11-13 13:45:54 UTC"
  },
  {
    "arxiv_id": "2411.08599v3",
    "title": "A Preview of XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL",
    "authors": [
      "Yingqi Gao",
      "Yifu Liu",
      "Xiaoxia Li",
      "Xiaorong Shi",
      "Yin Zhu",
      "Yiming Wang",
      "Shiqi Li",
      "Wei Li",
      "Yuntao Hong",
      "Zhiling Luo",
      "Jinyang Gao",
      "Liyu Mou",
      "Yu Li"
    ],
    "abstract": "To tackle the challenges of large language model performance in natural\nlanguage to SQL tasks, we introduce XiYan-SQL, an innovative framework that\nemploys a multi-generator ensemble strategy to improve candidate generation. We\nintroduce M-Schema, a semi-structured schema representation method designed to\nenhance the understanding of database structures. To enhance the quality and\ndiversity of generated candidate SQL queries, XiYan-SQL integrates the\nsignificant potential of in-context learning (ICL) with the precise control of\nsupervised fine-tuning. On one hand, we propose a series of training strategies\nto fine-tune models to generate high-quality candidates with diverse\npreferences. On the other hand, we implement the ICL approach with an example\nselection method based on named entity recognition to prevent overemphasis on\nentities. The refiner optimizes each candidate by correcting logical or\nsyntactical errors. To address the challenge of identifying the best candidate,\nwe fine-tune a selection model to distinguish nuances of candidate SQL queries.\nThe experimental results on multiple dialect datasets demonstrate the\nrobustness of XiYan-SQL in addressing challenges across different scenarios.\nOverall, our proposed XiYan-SQL achieves the state-of-the-art execution\naccuracy of 75.63% on Bird benchmark, 89.65% on the Spider test set, 69.86% on\nSQL-Eval, 41.20% on NL2GQL. The proposed framework not only enhances the\nquality and diversity of SQL queries but also outperforms previous methods.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DB",
      "cs.LG",
      "I.2; H.2"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08599v3",
    "published_date": "2024-11-13 13:30:21 UTC",
    "updated_date": "2025-02-10 10:28:55 UTC"
  },
  {
    "arxiv_id": "2411.08587v1",
    "title": "DeepUQ: Assessing the Aleatoric Uncertainties from two Deep Learning Methods",
    "authors": [
      "Rebecca Nevin",
      "Aleksandra ÄiprijanoviÄ",
      "Brian D. Nord"
    ],
    "abstract": "Assessing the quality of aleatoric uncertainty estimates from uncertainty\nquantification (UQ) deep learning methods is important in scientific contexts,\nwhere uncertainty is physically meaningful and important to characterize and\ninterpret exactly. We systematically compare aleatoric uncertainty measured by\ntwo UQ techniques, Deep Ensembles (DE) and Deep Evidential Regression (DER).\nOur method focuses on both zero-dimensional (0D) and two-dimensional (2D) data,\nto explore how the UQ methods function for different data dimensionalities. We\ninvestigate uncertainty injected on the input and output variables and include\na method to propagate uncertainty in the case of input uncertainty so that we\ncan compare the predicted aleatoric uncertainty to the known values. We\nexperiment with three levels of noise. The aleatoric uncertainty predicted\nacross all models and experiments scales with the injected noise level.\nHowever, the predicted uncertainty is miscalibrated to $\\rm{std}(\\sigma_{\\rm\nal})$ with the true uncertainty for half of the DE experiments and almost all\nof the DER experiments. The predicted uncertainty is the least accurate for\nboth UQ methods for the 2D input uncertainty experiment and the high-noise\nlevel. While these results do not apply to more complex data, they highlight\nthat further research on post-facto calibration for these methods would be\nbeneficial, particularly for high-noise and high-dimensional settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the Machine Learning for Physical Sciences workshop at\n  NeurIPS 2024; 11 pages, 2 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.08587v1",
    "published_date": "2024-11-13 13:11:49 UTC",
    "updated_date": "2024-11-13 13:11:49 UTC"
  },
  {
    "arxiv_id": "2411.08586v2",
    "title": "Optimizing Automatic Summarization of Long Clinical Records Using Dynamic Context Extension:Testing and Evaluation of the NBCE Method",
    "authors": [
      "Guoqing Zhang",
      "Keita Fukuyama",
      "Kazumasa Kishimoto",
      "Tomohiro Kuroda"
    ],
    "abstract": "Summarizing patient clinical notes is vital for reducing documentation\nburdens. Current manual summarization makes medical staff struggle. We propose\nan automatic method using LLMs, but long inputs cause LLMs to lose context,\nreducing output quality especially in small size model. We used a 7B model,\nopen-calm-7b, enhanced with Native Bayes Context Extend and a redesigned\ndecoding mechanism to reference one sentence at a time, keeping inputs within\ncontext windows, 2048 tokens. Our improved model achieved near parity with\nGoogle's over 175B Gemini on ROUGE-L metrics with 200 samples, indicating\nstrong performance using less resources, enhancing automated EMR summarization\nfeasibility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08586v2",
    "published_date": "2024-11-13 13:09:14 UTC",
    "updated_date": "2024-11-14 14:07:19 UTC"
  },
  {
    "arxiv_id": "2411.08583v1",
    "title": "An Empirical Examination of the Evaluative AI Framework",
    "authors": [
      "Jaroslaw Kornowicz"
    ],
    "abstract": "This study empirically examines the \"Evaluative AI\" framework, which aims to\nenhance the decision-making process for AI users by transitioning from a\nrecommendation-based approach to a hypothesis-driven one. Rather than offering\ndirect recommendations, this framework presents users pro and con evidence for\nhypotheses to support more informed decisions. However, findings from the\ncurrent behavioral experiment reveal no significant improvement in\ndecision-making performance and limited user engagement with the evidence\nprovided, resulting in cognitive processes similar to those observed in\ntraditional AI systems. Despite these results, the framework still holds\npromise for further exploration in future research.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08583v1",
    "published_date": "2024-11-13 13:03:49 UTC",
    "updated_date": "2024-11-13 13:03:49 UTC"
  },
  {
    "arxiv_id": "2411.08582v1",
    "title": "Intelligent Algorithms For Signature Diagnostics Of Three-Phase Motors",
    "authors": [
      "Stepan Svirin",
      "Artem Ryzhikov",
      "Saraa Ali",
      "Denis Derkach"
    ],
    "abstract": "The application of machine learning (ML) algorithms in the intelligent\ndiagnosis of three-phase engines has the potential to significantly enhance\ndiagnostic performance and accuracy. Traditional methods largely rely on\nsignature analysis, which, despite being a standard practice, can benefit from\nthe integration of advanced ML techniques. In our study, we innovate by\ncombining state of the art algorithms with a novel unsupervised anomaly\ngeneration methodology that takes into account physics model of the engine.\nThis hybrid approach leverages the strengths of both supervised ML and\nunsupervised signature analysis, achieving superior diagnostic accuracy and\nreliability along with a wide industrial application. Our experimental results\ndemonstrate that this method significantly outperforms existing ML and non-ML\nstate-of-the-art approaches while retaining the practical advantages of an\nunsupervised methodology. The findings highlight the potential of our approach\nto significantly contribute to the field of engine diagnostics, offering a\nrobust and efficient solution for real-world applications.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08582v1",
    "published_date": "2024-11-13 13:01:44 UTC",
    "updated_date": "2024-11-13 13:01:44 UTC"
  },
  {
    "arxiv_id": "2411.08563v1",
    "title": "Leveraging LLMs for Predictive Insights in Food Policy and Behavioral Interventions",
    "authors": [
      "Micha Kaiser",
      "Paul Lohmann",
      "Peter Ochieng",
      "Billy Shi",
      "Cass R. Sunstein",
      "Lucia A. Reisch"
    ],
    "abstract": "Food consumption and production contribute significantly to global greenhouse\ngas emissions, making them crucial entry points for mitigating climate change\nand maintaining a liveable planet. Over the past two decades, food policy\ninitiatives have explored interventions to reshape production and consumption\npatterns, focusing on reducing food waste and curbing ruminant meat\nconsumption. While the evidence of \"what works\" improves, evaluating which\npolicies are appropriate and effective in specific contexts remains difficult\ndue to external validity challenges. This paper demonstrates that a fine-tuned\nlarge language model (LLM) can accurately predict the direction of outcomes in\napproximately 80\\% of empirical studies measuring dietary-based impacts (e.g.\nfood choices, sales, waste) resulting from behavioral interventions and\npolicies. Approximately 75 prompts were required to achieve optimal results,\nwith performance showing signs of catastrophic loss beyond this point. Our\nfindings indicate that greater input detail enhances predictive accuracy,\nalthough the model still faces challenges with unseen studies, underscoring the\nimportance of a representative training sample. As LLMs continue to improve and\ndiversify, they hold promise for advancing data-driven, evidence-based\npolicymaking.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08563v1",
    "published_date": "2024-11-13 12:21:13 UTC",
    "updated_date": "2024-11-13 12:21:13 UTC"
  },
  {
    "arxiv_id": "2411.08562v1",
    "title": "Neural Corrective Machine Unranking",
    "authors": [
      "Jingrui Hou",
      "Axel Finke",
      "Georgina Cosma"
    ],
    "abstract": "Machine unlearning in neural information retrieval (IR) systems requires\nremoving specific data whilst maintaining model performance. Applying existing\nmachine unlearning methods to IR may compromise retrieval effectiveness or\ninadvertently expose unlearning actions due to the removal of particular items\nfrom the retrieved results presented to users. We formalise corrective\nunranking, which extends machine unlearning in (neural) IR context by\nintegrating substitute documents to preserve ranking integrity, and propose a\nnovel teacher-student framework, Corrective unRanking Distillation (CuRD), for\nthis task. CuRD (1) facilitates forgetting by adjusting the (trained) neural IR\nmodel such that its output relevance scores of to-be-forgotten samples mimic\nthose of low-ranking, non-retrievable samples; (2) enables correction by\nfine-tuning the relevance scores for the substitute samples to match those of\ncorresponding to-be-forgotten samples closely; (3) seeks to preserve\nperformance on samples that are not targeted for forgetting. We evaluate CuRD\non four neural IR models (BERTcat, BERTdot, ColBERT, PARADE) using MS MARCO and\nTREC CAR datasets. Experiments with forget set sizes from 1 % and 20 % of the\ntraining dataset demonstrate that CuRD outperforms seven state-of-the-art\nbaselines in terms of forgetting and correction while maintaining model\nretention and generalisation capabilities.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "submitted to Information Sciences",
    "pdf_url": "http://arxiv.org/pdf/2411.08562v1",
    "published_date": "2024-11-13 12:19:46 UTC",
    "updated_date": "2024-11-13 12:19:46 UTC"
  },
  {
    "arxiv_id": "2411.08561v5",
    "title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
    "authors": [
      "Wei Guan",
      "Jian Cao",
      "Shiyou Qian",
      "Jianqi Gao",
      "Chun Ouyang"
    ],
    "abstract": "Software systems often record important runtime information in logs to help\nwith troubleshooting. Log-based anomaly detection has become a key research\narea that aims to identify system issues through log data, ultimately enhancing\nthe reliability of software systems. Traditional deep learning methods often\nstruggle to capture the semantic information embedded in log data, which is\ntypically organized in natural language. In this paper, we propose LogLLM, a\nlog-based anomaly detection framework that leverages large language models\n(LLMs). LogLLM employs BERT for extracting semantic vectors from log messages,\nwhile utilizing Llama, a transformer decoder-based model, for classifying log\nsequences. Additionally, we introduce a projector to align the vector\nrepresentation spaces of BERT and Llama, ensuring a cohesive understanding of\nlog semantics. Unlike conventional methods that require log parsers to extract\ntemplates, LogLLM preprocesses log messages with regular expressions,\nstreamlining the entire process. Our framework is trained through a novel\nthree-stage procedure designed to enhance performance and adaptability.\nExperimental results across four public datasets demonstrate that LogLLM\noutperforms state-of-the-art methods. Even when handling unstable logs, it\neffectively captures the semantic meaning of log messages and detects anomalies\naccurately.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08561v5",
    "published_date": "2024-11-13 12:18:00 UTC",
    "updated_date": "2025-04-14 02:52:50 UTC"
  },
  {
    "arxiv_id": "2411.08553v1",
    "title": "CorrSynth -- A Correlated Sampling Method for Diverse Dataset Generation from LLMs",
    "authors": [
      "Suhas S Kowshik",
      "Abhishek Divekar",
      "Vijit Malik"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance in\ndiverse tasks using zero-shot and few-shot prompting. Even though their\ncapabilities of data synthesis have been studied well in recent years, the\ngenerated data suffers from a lack of diversity, less adherence to the prompt,\nand potential biases that creep into the data from the generator model. In this\nwork, we tackle the challenge of generating datasets with high diversity, upon\nwhich a student model is trained for downstream tasks. Taking the route of\ndecoding-time guidance-based approaches, we propose CorrSynth, which generates\ndata that is more diverse and faithful to the input prompt using a correlated\nsampling strategy. Further, our method overcomes the complexity drawbacks of\nsome other guidance-based techniques like classifier-based guidance. With\nextensive experiments, we show the effectiveness of our approach and\nsubstantiate our claims. In particular, we perform intrinsic evaluation to show\nthe improvements in diversity. Our experiments show that CorrSynth improves\nboth student metrics and intrinsic metrics upon competitive baselines across\nfour datasets, showing the innate advantage of our method.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published as a main conference paper at EMNLP 2024; First two authors\n  contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2411.08553v1",
    "published_date": "2024-11-13 12:09:23 UTC",
    "updated_date": "2024-11-13 12:09:23 UTC"
  },
  {
    "arxiv_id": "2411.08552v1",
    "title": "Leveraging Pre-Trained Neural Networks to Enhance Machine Learning with Variational Quantum Circuits",
    "authors": [
      "Jun Qi",
      "Chao-Han Yang",
      "Samuel Yen-Chi Chen",
      "Pin-Yu Chen",
      "Hector Zenil",
      "Jesper Tegner"
    ],
    "abstract": "Quantum Machine Learning (QML) offers tremendous potential but is currently\nlimited by the availability of qubits. We introduce an innovative approach that\nutilizes pre-trained neural networks to enhance Variational Quantum Circuits\n(VQC). This technique effectively separates approximation error from qubit\ncount and removes the need for restrictive conditions, making QML more viable\nfor real-world applications. Our method significantly improves parameter\noptimization for VQC while delivering notable gains in representation and\ngeneralization capabilities, as evidenced by rigorous theoretical analysis and\nextensive empirical testing on quantum dot classification tasks. Moreover, our\nresults extend to applications such as human genome analysis, demonstrating the\nbroad applicability of our approach. By addressing the constraints of current\nquantum hardware, our work paves the way for a new era of advanced QML\napplications, unlocking the full potential of quantum computing in fields such\nas machine learning, materials science, medicine, mimetics, and various\ninterdisciplinary areas.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "In submission",
    "pdf_url": "http://arxiv.org/pdf/2411.08552v1",
    "published_date": "2024-11-13 12:03:39 UTC",
    "updated_date": "2024-11-13 12:03:39 UTC"
  },
  {
    "arxiv_id": "2411.08544v1",
    "title": "Deeper Insights into Learning Performance of Stochastic Configuration Networks",
    "authors": [
      "Xiufeng Yan",
      "Dianhui Wang"
    ],
    "abstract": "Stochastic Configuration Networks (SCNs) are a class of randomized neural\nnetworks that integrate randomized algorithms within an incremental learning\nframework. A defining feature of SCNs is the supervisory mechanism, which\nadaptively adjusts the distribution to generate effective random basis\nfunctions, thereby enabling error-free learning. In this paper, we present a\ncomprehensive analysis of the impact of the supervisory mechanism on the\nlearning performance of SCNs. Our findings reveal that the current SCN\nframework evaluates the effectiveness of each random basis function in reducing\nresidual errors using a lower bound on its error reduction potential, which\nconstrains SCNs' overall learning efficiency. Specifically, SCNs may fail to\nconsistently select the most effective random candidate as the new basis\nfunction during each training iteration. To overcome this problem, we propose a\nnovel method for evaluating the hidden layer's output matrix, supported by a\nnew supervisory mechanism that accurately assesses the error reduction\npotential of random basis functions without requiring the computation of the\nMoore-Penrose inverse of the output matrix. This approach enhances the\nselection of basis functions, reducing computational complexity and improving\nthe overall scalability and learning capabilities of SCNs. We introduce a\nRecursive Moore-Penrose Inverse-SCN (RMPI-SCN) training scheme based on the new\nsupervisory mechanism and demonstrate its effectiveness through simulations\nover some benchmark datasets. Experiments show that RMPI-SCN outperforms the\nconventional SCN in terms of learning capability, underscoring its potential to\nadvance the SCN framework for large-scale data modeling applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08544v1",
    "published_date": "2024-11-13 11:45:39 UTC",
    "updated_date": "2024-11-13 11:45:39 UTC"
  },
  {
    "arxiv_id": "2411.08537v1",
    "title": "MLV$^2$-Net: Rater-Based Majority-Label Voting for Consistent Meningeal Lymphatic Vessel Segmentation",
    "authors": [
      "Fabian Bongratz",
      "Markus Karmann",
      "Adrian Holz",
      "Moritz Bonhoeffer",
      "Viktor Neumaier",
      "Sarah Deli",
      "Benita Schmitz-Koep",
      "Claus Zimmer",
      "Christian Sorg",
      "Melissa Thalhammer",
      "Dennis M Hedderich",
      "Christian Wachinger"
    ],
    "abstract": "Meningeal lymphatic vessels (MLVs) are responsible for the drainage of waste\nproducts from the human brain. An impairment in their functionality has been\nassociated with aging as well as brain disorders like multiple sclerosis and\nAlzheimer's disease. However, MLVs have only recently been described for the\nfirst time in magnetic resonance imaging (MRI), and their ramified structure\nrenders manual segmentation particularly difficult. Further, as there is no\nconsistent notion of their appearance, human-annotated MLV structures contain a\nhigh inter-rater variability that most automatic segmentation methods cannot\ntake into account. In this work, we propose a new rater-aware training scheme\nfor the popular nnU-Net model, and we explore rater-based ensembling strategies\nfor accurate and consistent segmentation of MLVs. This enables us to boost\nnnU-Net's performance while obtaining explicit predictions in different\nannotation styles and a rater-based uncertainty estimation. Our final model,\nMLV$^2$-Net, achieves a Dice similarity coefficient of 0.806 with respect to\nthe human reference standard. The model further matches the human inter-rater\nreliability and replicates age-related associations with MLV volume.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ML4H 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.08537v1",
    "published_date": "2024-11-13 11:35:39 UTC",
    "updated_date": "2024-11-13 11:35:39 UTC"
  },
  {
    "arxiv_id": "2411.08533v2",
    "title": "ACROSS: A Deformation-Based Cross-Modal Representation for Robotic Tactile Perception",
    "authors": [
      "Wadhah Zai El Amri",
      "Malte Kuhlmann",
      "NicolÃ¡s Navarro-Guerrero"
    ],
    "abstract": "Tactile perception is essential for human interaction with the environment\nand is becoming increasingly crucial in robotics. Tactile sensors like the\nBioTac mimic human fingertips and provide detailed interaction data. Despite\nits utility in applications like slip detection and object identification, this\nsensor is now deprecated, making many valuable datasets obsolete. However,\nrecreating similar datasets with newer sensor technologies is both tedious and\ntime-consuming. Therefore, adapting these existing datasets for use with new\nsetups and modalities is crucial. In response, we introduce ACROSS, a novel\nframework for translating data between tactile sensors by exploiting sensor\ndeformation information. We demonstrate the approach by translating BioTac\nsignals into the DIGIT sensor. Our framework consists of first converting the\ninput signals into 3D deformation meshes. We then transition from the 3D\ndeformation mesh of one sensor to the mesh of another, and finally convert the\ngenerated 3D deformation mesh into the corresponding output space. We\ndemonstrate our approach to the most challenging problem of going from a\nlow-dimensional tactile representation to a high-dimensional one. In\nparticular, we transfer the tactile signals of a BioTac sensor to DIGIT tactile\nimages. Our approach enables the continued use of valuable datasets and data\nexchange between groups with different setups.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to 2025 IEEE Conference on Robotics and Automation (ICRA\n  2025). arXiv admin note: text overlap with arXiv:2410.14310",
    "pdf_url": "http://arxiv.org/pdf/2411.08533v2",
    "published_date": "2024-11-13 11:29:14 UTC",
    "updated_date": "2025-02-19 17:08:50 UTC"
  },
  {
    "arxiv_id": "2411.08526v2",
    "title": "Gendered Words and Grant Rates: A Textual Analysis of Disparate Outcomes in the Patent System",
    "authors": [
      "Deborah Gerhardt",
      "Miriam Marcowitz-Bitton",
      "W. Michael Schuster",
      "Avshalom Elmalech",
      "Omri Suissa",
      "Moshe Mash"
    ],
    "abstract": "Text is a vehicle to convey information that reflects the writer's linguistic\nstyle and communicative patterns. By studying these attributes, we can discover\nlatent insights about the author and their underlying message. This article\nuses such an approach to better understand patent applications and their\ninventors. While prior research focuses on patent metadata, we employ machine\nlearning and natural language processing to extract hidden information from the\nwords in patent applications. Through these methods, we find that inventor\ngender can often be identified from textual attributes - even without knowing\nthe inventor's name. This ability to discern gender through text suggests that\nanonymized patent examination - often proposed as a solution to mitigate\ndisparities in patent grant rates - may not fully address gendered outcomes in\nsecuring a patent. Our study also investigates whether objective features of a\npatent application can predict if it will be granted. Using a classifier\nalgorithm, we correctly predicted whether a patent was granted over 60% of the\ntime. Further analysis emphasized that writing style - like vocabulary and\nsentence complexity - disproportionately influenced grant predictions relative\nto other attributes such as inventor gender and subject matter keywords.\nLastly, we examine whether women disproportionately invent in technological\nareas with higher rejection rates. Using a clustering algorithm, applications\nwere allocated into groups with related subject matter. We found that 85% of\nfemale-dominated clusters have abnormally high rejection rates, compared to\nonly 45% for male-dominated groupings. These findings highlight complex\ninteractions between textual choices, gender, and success in securing a patent.\nThey also raise questions about whether current proposals will be sufficient to\nachieve gender equity and efficiency in the patent system.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08526v2",
    "published_date": "2024-11-13 11:20:09 UTC",
    "updated_date": "2024-12-18 17:24:00 UTC"
  },
  {
    "arxiv_id": "2411.08521v3",
    "title": "SAD-TIME: a Spatiotemporal-fused network for depression detection with Automated multi-scale Depth-wise and TIME-interval-related common feature extractor",
    "authors": [
      "Han-Guang Wang",
      "Hui-Rang Hou",
      "Li-Cheng Jin",
      "Chen-Yang Xu",
      "Zhong-Yi Zhang",
      "Qing-Hao Meng"
    ],
    "abstract": "Background and Objective: Depression is a severe mental disorder, and\naccurate diagnosis is pivotal to the cure and rehabilitation of people with\ndepression. However, the current questionnaire-based diagnostic methods could\nbring subjective biases and may be denied by subjects. In search of a more\nobjective means of diagnosis, researchers have begun to experiment with deep\nlearning-based methods for identifying depressive disorders in recent years.\nMethods: In this study, a novel Spatiotemporal-fused network with Automated\nmulti-scale Depth-wise and TIME-interval-related common feature extractor\n(SAD-TIME) is proposed. SAD-TIME incorporates an automated nodes' common\nfeatures extractor (CFE), a spatial sector (SpS), a modified temporal sector\n(TeS), and a domain adversarial learner (DAL). The CFE includes a multi-scale\ndepth-wise 1D-convolutional neural network and a time-interval embedding\ngenerator, where the unique information of each channel is preserved. The SpS\nfuses the functional connectivity with the distance-based connectivity\ncontaining spatial position of EEG electrodes. A multi-head-attention graph\nconvolutional network is also applied in the SpS to fuse the features from\ndifferent EEG channels. The TeS is based on long short-term memory and graph\ntransformer networks, where the temporal information of different time-windows\nis fused. Moreover, the DAL is used after the SpS to obtain the\ndomain-invariant feature. Results: Experimental results under tenfold\ncross-validation show that the proposed SAD-TIME method achieves 92.00% and\n94.00% depression classification accuracies on two datasets, respectively, in\ncross-subject mode. Conclusion: SAD-TIME is a robust depression detection\nmodel, where the automatedly-generated features, the SpS and the TeS assist the\nclassification performance with the fusion of the innate spatiotemporal\ninformation in the EEG signals.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.08521v3",
    "published_date": "2024-11-13 11:08:28 UTC",
    "updated_date": "2024-12-28 09:10:45 UTC"
  },
  {
    "arxiv_id": "2411.08514v1",
    "title": "Explainers' Mental Representations of Explainees' Needs in Everyday Explanations",
    "authors": [
      "Michael Erol Schaffer",
      "Lutz Terfloth",
      "Carsten Schulte",
      "Heike M. Buhl"
    ],
    "abstract": "In explanations, explainers have mental representations of explainees'\ndeveloping knowledge and shifting interests regarding the explanandum. These\nmental representations are dynamic in nature and develop over time, thereby\nenabling explainers to react to explainees' needs by adapting and customizing\nthe explanation. XAI should be able to react to explainees' needs in a similar\nmanner. Therefore, a component that incorporates aspects of explainers' mental\nrepresentations of explainees is required. In this study, we took first steps\nby investigating explainers' mental representations in everyday explanations of\ntechnological artifacts. According to the dual nature theory, technological\nartifacts require explanations with two distinct perspectives, namely\nobservable and measurable features addressing \"Architecture\" or interpretable\naspects addressing \"Relevance\". We conducted extended semi structured pre-,\npost- and video recall-interviews with explainers (N=9) in the context of an\nexplanation. The transcribed interviews were analyzed utilizing qualitative\ncontent analysis. The explainers' answers regarding the explainees' knowledge\nand interests with regard to the technological artifact emphasized the\nvagueness of early assumptions of explainers toward strong beliefs in the\ncourse of explanations. The assumed knowledge of explainees in the beginning is\ncentered around Architecture and develops toward knowledge with regard to both\nArchitecture and Relevance. In contrast, explainers assumed higher interests in\nRelevance in the beginning to interests regarding both Architecture and\nRelevance in the further course of explanations. Further, explainers often\nfinished the explanation despite their perception that explainees still had\ngaps in knowledge. These findings are transferred into practical implications\nrelevant for user models for adaptive explainable systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08514v1",
    "published_date": "2024-11-13 10:53:07 UTC",
    "updated_date": "2024-11-13 10:53:07 UTC"
  },
  {
    "arxiv_id": "2411.08506v2",
    "title": "Towards Operationalizing Right to Data Protection",
    "authors": [
      "Abhinav Java",
      "Simra Shahid",
      "Chirag Agarwal"
    ],
    "abstract": "The widespread practice of indiscriminate data scraping to fine-tune language\nmodels (LMs) raises significant legal and ethical concerns, particularly\nregarding compliance with data protection laws such as the General Data\nProtection Regulation (GDPR). This practice often results in the unauthorized\nuse of personal information, prompting growing debate within the academic and\nregulatory communities. Recent works have introduced the concept of generating\nunlearnable datasets (by adding imperceptible noise to the clean data), such\nthat the underlying model achieves lower loss during training but fails to\ngeneralize to the unseen test setting. Though somewhat effective, these\napproaches are predominantly designed for images and are limited by several\npractical constraints like requiring knowledge of the target model. To this\nend, we introduce RegText, a framework that injects imperceptible spurious\ncorrelations into natural language datasets, effectively rendering them\nunlearnable without affecting semantic content. We demonstrate RegText's\nutility through rigorous empirical analysis of small and large LMs. Notably,\nRegText can restrict newer models like GPT-4o and Llama from learning on our\ngenerated data, resulting in a drop in their test accuracy compared to their\nzero-shot performance and paving the way for generating unlearnable text to\nprotect public data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "First two authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2411.08506v2",
    "published_date": "2024-11-13 10:43:31 UTC",
    "updated_date": "2024-11-16 09:36:36 UTC"
  },
  {
    "arxiv_id": "2411.08504v2",
    "title": "Towards Objective and Unbiased Decision Assessments with LLM-Enhanced Hierarchical Attention Networks",
    "authors": [
      "Junhua Liu",
      "Kwan Hui Lim",
      "Roy Ka-Wei Lee"
    ],
    "abstract": "How objective and unbiased are we while making decisions? This work\ninvestigates cognitive bias identification in high-stake decision making\nprocess by human experts, questioning its effectiveness in real-world settings,\nsuch as candidates assessments for university admission. We begin with a\nstatistical analysis assessing correlations among different decision points\namong in the current process, which discovers discrepancies that imply\ncognitive bias and inconsistency in decisions. This motivates our exploration\nof bias-aware AI-augmented workflow that surpass human judgment. We propose\nBGM-HAN, an enhanced Hierarchical Attention Network with Byte-Pair Encoding,\nGated Residual Connections and Multi-Head Attention. Using it as a backbone\nmodel, we further propose a Shortlist-Analyse-Recommend (SAR) agentic workflow,\nwhich simulate real-world decision-making. In our experiments, both the\nproposed model and the agentic workflow significantly improves on both human\njudgment and alternative models, validated with real-world data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Source code is available at: https://github.com/junhua/bgm-han",
    "pdf_url": "http://arxiv.org/pdf/2411.08504v2",
    "published_date": "2024-11-13 10:42:11 UTC",
    "updated_date": "2024-11-14 05:51:26 UTC"
  },
  {
    "arxiv_id": "2411.08478v1",
    "title": "Learning Model Agnostic Explanations via Constraint Programming",
    "authors": [
      "Frederic Koriche",
      "Jean-Marie Lagniez",
      "Stefan Mengel",
      "Chi Tran"
    ],
    "abstract": "Interpretable Machine Learning faces a recurring challenge of explaining the\npredictions made by opaque classifiers such as ensemble models, kernel methods,\nor neural networks in terms that are understandable to humans. When the model\nis viewed as a black box, the objective is to identify a small set of features\nthat jointly determine the black box response with minimal error. However,\nfinding such model-agnostic explanations is computationally demanding, as the\nproblem is intractable even for binary classifiers. In this paper, the task is\nframed as a Constraint Optimization Problem, where the constraint solver seeks\nan explanation of minimum error and bounded size for an input data instance and\na set of samples generated by the black box. From a theoretical perspective,\nthis constraint programming approach offers PAC-style guarantees for the output\nexplanation. We evaluate the approach empirically on various datasets and show\nthat it statistically outperforms the state-of-the-art heuristic Anchors\nmethod.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08478v1",
    "published_date": "2024-11-13 09:55:59 UTC",
    "updated_date": "2024-11-13 09:55:59 UTC"
  },
  {
    "arxiv_id": "2411.08469v2",
    "title": "Building Trustworthy AI: Transparent AI Systems via Large Language Models, Ontologies, and Logical Reasoning (TranspNet)",
    "authors": [
      "Fadi Al Machot",
      "Martin Thomas Horsch",
      "Habib Ullah"
    ],
    "abstract": "Growing concerns over the lack of transparency in AI, particularly in\nhigh-stakes fields like healthcare and finance, drive the need for explainable\nand trustworthy systems. While Large Language Models (LLMs) perform\nexceptionally well in generating accurate outputs, their \"black box\" nature\nposes significant challenges to transparency and trust. To address this, the\npaper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.\nBy leveraging domain expert knowledge, retrieval-augmented generation (RAG),\nand formal reasoning frameworks like Answer Set Programming (ASP), TranspNet\nenhances LLM outputs with structured reasoning and verification.This approach\nstrives to help AI systems deliver results that are as accurate, explainable,\nand trustworthy as possible, aligning with regulatory expectations for\ntransparency and accountability. TranspNet provides a solution for developing\nAI systems that are reliable and interpretable, making it suitable for\nreal-world applications where trust is critical.",
    "categories": [
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08469v2",
    "published_date": "2024-11-13 09:40:37 UTC",
    "updated_date": "2024-12-18 14:45:15 UTC"
  },
  {
    "arxiv_id": "2411.08464v1",
    "title": "Crystal Structure Generation Based On Material Properties",
    "authors": [
      "Chao Huang",
      "JiaHui Chen",
      "HongRui Liang",
      "ChunYan Chen",
      "Chen Chen"
    ],
    "abstract": "The discovery of new materials is very important to the field of materials\nscience. When researchers explore new materials, they often have expected\nperformance requirements for their crystal structure. In recent years,\ndata-driven methods have made great progress in the direction plane of crystal\nstructure generation, but there is still a lack of methods that can effectively\nmap material properties to crystal structure. In this paper, we propose a\nCrystal DiT model to generate the crystal structure from the expected material\nproperties by embedding the material properties and combining the symmetry\ninformation predicted by the large language model. Experimental verification\nshows that our proposed method has good performance.",
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08464v1",
    "published_date": "2024-11-13 09:36:50 UTC",
    "updated_date": "2024-11-13 09:36:50 UTC"
  },
  {
    "arxiv_id": "2411.08463v2",
    "title": "Symbolic-AI-Fusion Deep Learning (SAIF-DL): Encoding Knowledge into Training with Answer Set Programming Loss Penalties by a Novel Loss Function Approach",
    "authors": [
      "Fadi Al Machot",
      "Martin Thomas Horsch",
      "Habib Ullah"
    ],
    "abstract": "This paper presents a hybrid methodology that enhances the training process\nof deep learning (DL) models by embedding domain expert knowledge using\nontologies and answer set programming (ASP). By integrating these symbolic AI\nmethods, we encode domain-specific constraints, rules, and logical reasoning\ndirectly into the model's learning process, thereby improving both performance\nand trustworthiness. The proposed approach is flexible and applicable to both\nregression and classification tasks, demonstrating generalizability across\nvarious fields such as healthcare, autonomous systems, engineering, and battery\nmanufacturing applications. Unlike other state-of-the-art methods, the strength\nof our approach lies in its scalability across different domains. The design\nallows for the automation of the loss function by simply updating the ASP\nrules, making the system highly scalable and user-friendly. This facilitates\nseamless adaptation to new domains without significant redesign, offering a\npractical solution for integrating expert knowledge into DL models in\nindustrial settings such as battery manufacturing.",
    "categories": [
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08463v2",
    "published_date": "2024-11-13 09:33:33 UTC",
    "updated_date": "2024-12-18 14:55:06 UTC"
  },
  {
    "arxiv_id": "2411.08460v2",
    "title": "Trap-MID: Trapdoor-based Defense against Model Inversion Attacks",
    "authors": [
      "Zhen-Ting Liu",
      "Shang-Tse Chen"
    ],
    "abstract": "Model Inversion (MI) attacks pose a significant threat to the privacy of Deep\nNeural Networks by recovering training data distribution from well-trained\nmodels. While existing defenses often rely on regularization techniques to\nreduce information leakage, they remain vulnerable to recent attacks. In this\npaper, we propose the Trapdoor-based Model Inversion Defense (Trap-MID) to\nmislead MI attacks. A trapdoor is integrated into the model to predict a\nspecific label when the input is injected with the corresponding trigger.\nConsequently, this trapdoor information serves as the \"shortcut\" for MI\nattacks, leading them to extract trapdoor triggers rather than private data. We\nprovide theoretical insights into the impacts of trapdoor's effectiveness and\nnaturalness on deceiving MI attacks. In addition, empirical experiments\ndemonstrate the state-of-the-art defense performance of Trap-MID against\nvarious MI attacks without the requirements for extra data or large\ncomputational overhead. Our source code is publicly available at\nhttps://github.com/ntuaislab/Trap-MID.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by Neural Information Processing Systems (NeurIPS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.08460v2",
    "published_date": "2024-11-13 09:31:06 UTC",
    "updated_date": "2024-11-25 12:10:05 UTC"
  },
  {
    "arxiv_id": "2411.08933v2",
    "title": "Confidence-aware Denoised Fine-tuning of Off-the-shelf Models for Certified Robustness",
    "authors": [
      "Suhyeok Jang",
      "Seojin Kim",
      "Jinwoo Shin",
      "Jongheon Jeong"
    ],
    "abstract": "The remarkable advances in deep learning have led to the emergence of many\noff-the-shelf classifiers, e.g., large pre-trained models. However, since they\nare typically trained on clean data, they remain vulnerable to adversarial\nattacks. Despite this vulnerability, their superior performance and\ntransferability make off-the-shelf classifiers still valuable in practice,\ndemanding further work to provide adversarial robustness for them in a post-hoc\nmanner. A recently proposed method, denoised smoothing, leverages a denoiser\nmodel in front of the classifier to obtain provable robustness without\nadditional training. However, the denoiser often creates hallucination, i.e.,\nimages that have lost the semantics of their originally assigned class, leading\nto a drop in robustness. Furthermore, its noise-and-denoise procedure\nintroduces a significant distribution shift from the original distribution,\ncausing the denoised smoothing framework to achieve sub-optimal robustness. In\nthis paper, we introduce Fine-Tuning with Confidence-Aware Denoised Image\nSelection (FT-CADIS), a novel fine-tuning scheme to enhance the certified\nrobustness of off-the-shelf classifiers. FT-CADIS is inspired by the\nobservation that the confidence of off-the-shelf classifiers can effectively\nidentify hallucinated images during denoised smoothing. Based on this, we\ndevelop a confidence-aware training objective to handle such hallucinated\nimages and improve the stability of fine-tuning from denoised images. In this\nway, the classifier can be fine-tuned using only images that are beneficial for\nadversarial robustness. We also find that such a fine-tuning can be done by\nupdating a small fraction of parameters of the classifier. Extensive\nexperiments demonstrate that FT-CADIS has established the state-of-the-art\ncertified robustness among denoised smoothing methods across all\n$\\ell_2$-adversary radius in various benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages; TMLR 2024; Code is available at\n  https://github.com/suhyeok24/FT-CADIS",
    "pdf_url": "http://arxiv.org/pdf/2411.08933v2",
    "published_date": "2024-11-13 09:13:20 UTC",
    "updated_date": "2024-11-15 06:13:33 UTC"
  },
  {
    "arxiv_id": "2411.08447v1",
    "title": "Learning Dynamic Cognitive Map with Autonomous Navigation",
    "authors": [
      "Daria de Tinguy",
      "Tim Verbelen",
      "Bart Dhoedt"
    ],
    "abstract": "Inspired by animal navigation strategies, we introduce a novel computational\nmodel to navigate and map a space rooted in biologically inspired principles.\nAnimals exhibit extraordinary navigation prowess, harnessing memory,\nimagination, and strategic decision-making to traverse complex and aliased\nenvironments adeptly. Our model aims to replicate these capabilities by\nincorporating a dynamically expanding cognitive map over predicted poses within\nan Active Inference framework, enhancing our agent's generative model\nplasticity to novelty and environmental changes. Through structure learning and\nactive inference navigation, our model demonstrates efficient exploration and\nexploitation, dynamically expanding its model capacity in response to\nanticipated novel un-visited locations and updating the map given new evidence\ncontradicting previous beliefs. Comparative analyses in mini-grid environments\nwith the Clone-Structured Cognitive Graph model (CSCG), which shares similar\nobjectives, highlight our model's ability to rapidly learn environmental\nstructures within a single episode, with minimal navigation overlap. Our model\nachieves this without prior knowledge of observation and world dimensions,\nunderscoring its robustness and efficacy in navigating intricate environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "under submission at Frontiers Computer Neuroscience",
    "pdf_url": "http://arxiv.org/pdf/2411.08447v1",
    "published_date": "2024-11-13 08:59:53 UTC",
    "updated_date": "2024-11-13 08:59:53 UTC"
  },
  {
    "arxiv_id": "2411.08438v1",
    "title": "Towards Optimizing a Retrieval Augmented Generation using Large Language Model on Academic Data",
    "authors": [
      "Anum Afzal",
      "Juraj Vladika",
      "Gentrit Fazlija",
      "Andrei Staradubets",
      "Florian Matthes"
    ],
    "abstract": "Given the growing trend of many organizations integrating Retrieval Augmented\nGeneration (RAG) into their operations, we assess RAG on domain-specific data\nand test state-of-the-art models across various optimization techniques. We\nincorporate four optimizations; Multi-Query, Child-Parent-Retriever, Ensemble\nRetriever, and In-Context-Learning, to enhance the functionality and\nperformance in the academic domain. We focus on data retrieval, specifically\ntargeting various study programs at a large technical university. We\nadditionally introduce a novel evaluation approach, the RAG Confusion Matrix\ndesigned to assess the effectiveness of various configurations within the RAG\nframework. By exploring the integration of both open-source (e.g., Llama2,\nMistral) and closed-source (GPT-3.5 and GPT-4) Large Language Models, we offer\nvaluable insights into the application and optimization of RAG frameworks in\ndomain-specific contexts. Our experiments show a significant performance\nincrease when including multi-query in the retrieval phase.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08438v1",
    "published_date": "2024-11-13 08:43:37 UTC",
    "updated_date": "2024-11-13 08:43:37 UTC"
  },
  {
    "arxiv_id": "2411.08433v1",
    "title": "3D Multi-Object Tracking with Semi-Supervised GRU-Kalman Filter",
    "authors": [
      "Xiaoxiang Wang",
      "Jiaxin Liu",
      "Miaojie Feng",
      "Zhaoxing Zhang",
      "Xin Yang"
    ],
    "abstract": "3D Multi-Object Tracking (MOT), a fundamental component of environmental\nperception, is essential for intelligent systems like autonomous driving and\nrobotic sensing. Although Tracking-by-Detection frameworks have demonstrated\nexcellent performance in recent years, their application in real-world\nscenarios faces significant challenges. Object movement in complex environments\nis often highly nonlinear, while existing methods typically rely on linear\napproximations of motion. Furthermore, system noise is frequently modeled as a\nGaussian distribution, which fails to capture the true complexity of the noise\ndynamics. These oversimplified modeling assumptions can lead to significant\nreductions in tracking precision. To address this, we propose a GRU-based MOT\nmethod, which introduces a learnable Kalman filter into the motion module. This\napproach is able to learn object motion characteristics through data-driven\nlearning, thereby avoiding the need for manual model design and model error. At\nthe same time, to avoid abnormal supervision caused by the wrong association\nbetween annotations and trajectories, we design a semi-supervised learning\nstrategy to accelerate the convergence speed and improve the robustness of the\nmodel. Evaluation experiment on the nuScenes and Argoverse2 datasets\ndemonstrates that our system exhibits superior performance and significant\npotential compared to traditional TBD methods.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08433v1",
    "published_date": "2024-11-13 08:34:07 UTC",
    "updated_date": "2024-11-13 08:34:07 UTC"
  },
  {
    "arxiv_id": "2411.08432v1",
    "title": "One STEP at a time: Language Agents are Stepwise Planners",
    "authors": [
      "Minh Nguyen",
      "Ehsan Shareghi"
    ],
    "abstract": "Language agents have shown promising adaptability in dynamic environments to\nperform complex tasks. However, despite the versatile knowledge embedded in\nlarge language models, these agents still fall short when it comes to tasks\nthat require planning. We introduce STEP, a novel framework designed to\nefficiently learn from previous experiences to enhance the planning\ncapabilities of language agents in future steps. Concretely, STEP functions\nthrough four interconnected components. First, the Planner takes on the task,\nbreaks it down into subtasks and provides relevant insights. Then the Executor\ngenerates action candidates, while the Evaluator ensures the actions align with\nlearned rules from previous experiences. Lastly, Memory stores experiences to\ninform future decisions. In the ScienceWorld benchmark, our results show that\nSTEP consistently outperforms state-of-the-art models, achieving an overall\nscore of 67.4 and successfully completing 12 out of 18 tasks. These findings\nhighlight STEP's potential as a framework for enhancing planning capabilities\nin language agents, paving the way for more sophisticated task-solving in\ndynamic environments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08432v1",
    "published_date": "2024-11-13 08:32:42 UTC",
    "updated_date": "2024-11-13 08:32:42 UTC"
  },
  {
    "arxiv_id": "2411.08424v1",
    "title": "A Heterogeneous Graph Neural Network Fusing Functional and Structural Connectivity for MCI Diagnosis",
    "authors": [
      "Feiyu Yin",
      "Yu Lei",
      "Siyuan Dai",
      "Wenwen Zeng",
      "Guoqing Wu",
      "Liang Zhan",
      "Jinhua Yu"
    ],
    "abstract": "Brain connectivity alternations associated with brain disorders have been\nwidely reported in resting-state functional imaging (rs-fMRI) and diffusion\ntensor imaging (DTI). While many dual-modal fusion methods based on graph\nneural networks (GNNs) have been proposed, they generally follow homogenous\nfusion ways ignoring rich heterogeneity of dual-modal information. To address\nthis issue, we propose a novel method that integrates functional and structural\nconnectivity based on heterogeneous graph neural networks (HGNNs) to better\nleverage the rich heterogeneity in dual-modal images. We firstly use blood\noxygen level dependency and whiter matter structure information provided by\nrs-fMRI and DTI to establish homo-meta-path, capturing node relationships\nwithin the same modality. At the same time, we propose to establish\nhetero-meta-path based on structure-function coupling and brain community\nsearching to capture relations among cross-modal nodes. Secondly, we further\nintroduce a heterogeneous graph pooling strategy that automatically balances\nhomo- and hetero-meta-path, effectively leveraging heterogeneous information\nand preventing feature confusion after pooling. Thirdly, based on the\nflexibility of heterogeneous graphs, we propose a heterogeneous graph data\naugmentation approach that can conveniently address the sample imbalance issue\ncommonly seen in clinical diagnosis. We evaluate our method on ADNI-3 dataset\nfor mild cognitive impairment (MCI) diagnosis. Experimental results indicate\nthe proposed method is effective and superior to other algorithms, with a mean\nclassification accuracy of 93.3%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08424v1",
    "published_date": "2024-11-13 08:17:52 UTC",
    "updated_date": "2024-11-13 08:17:52 UTC"
  },
  {
    "arxiv_id": "2411.08418v1",
    "title": "Enhanced Classroom Dialogue Sequences Analysis with a Hybrid AI Agent: Merging Expert Rule-Base with Large Language Models",
    "authors": [
      "Yun Long",
      "Yu Zhang"
    ],
    "abstract": "Classroom dialogue plays a crucial role in fostering student engagement and\ndeeper learning. However, analysing dialogue sequences has traditionally relied\non either theoretical frameworks or empirical descriptions of practice, with\nlimited integration between the two. This study addresses this gap by\ndeveloping a comprehensive rule base of dialogue sequences and an Artificial\nIntelligence (AI) agent that combines expert-informed rule-based systems with a\nlarge language model (LLM). The agent applies expert knowledge while adapting\nto the complexities of natural language, enabling accurate and flexible\ncategorisation of classroom dialogue sequences. By synthesising findings from\nover 30 studies, we established a comprehensive framework for dialogue\nanalysis. The agent was validated against human expert coding, achieving high\nlevels of precision and reliability. The results demonstrate that the agent\nprovides theory-grounded and adaptive functions, tremendously enhancing the\nefficiency and scalability of classroom dialogue analysis, offering significant\npotential in improving classroom teaching practices and supporting teacher\nprofessional development.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08418v1",
    "published_date": "2024-11-13 08:13:41 UTC",
    "updated_date": "2024-11-13 08:13:41 UTC"
  },
  {
    "arxiv_id": "2411.08414v1",
    "title": "Material Property Prediction with Element Attribute Knowledge Graphs and Multimodal Representation Learning",
    "authors": [
      "Chao Huang",
      "Chunyan Chen",
      "Ling Shi",
      "Chen Chen"
    ],
    "abstract": "Machine learning has become a crucial tool for predicting the properties of\ncrystalline materials. However, existing methods primarily represent material\ninformation by constructing multi-edge graphs of crystal structures, often\noverlooking the chemical and physical properties of elements (such as atomic\nradius, electronegativity, melting point, and ionization energy), which have a\nsignificant impact on material performance. To address this limitation, we\nfirst constructed an element property knowledge graph and utilized an embedding\nmodel to encode the element attributes within the knowledge graph. Furthermore,\nwe propose a multimodal fusion framework, ESNet, which integrates element\nproperty features with crystal structure features to generate joint multimodal\nrepresentations. This provides a more comprehensive perspective for predicting\nthe performance of crystalline materials, enabling the model to consider both\nmicrostructural composition and chemical characteristics of the materials. We\nconducted experiments on the Materials Project benchmark dataset, which showed\nleading performance in the bandgap prediction task and achieved results on a\npar with existing benchmarks in the formation energy prediction task.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08414v1",
    "published_date": "2024-11-13 08:07:21 UTC",
    "updated_date": "2024-11-13 08:07:21 UTC"
  },
  {
    "arxiv_id": "2411.10480v1",
    "title": "Hateful Meme Detection through Context-Sensitive Prompting and Fine-Grained Labeling",
    "authors": [
      "Rongxin Ouyang",
      "Kokil Jaidka",
      "Subhayan Mukerjee",
      "Guangyu Cui"
    ],
    "abstract": "The prevalence of multi-modal content on social media complicates automated\nmoderation strategies. This calls for an enhancement in multi-modal\nclassification and a deeper understanding of understated meanings in images and\nmemes. Although previous efforts have aimed at improving model performance\nthrough fine-tuning, few have explored an end-to-end optimization pipeline that\naccounts for modalities, prompting, labeling, and fine-tuning. In this study,\nwe propose an end-to-end conceptual framework for model optimization in complex\ntasks. Experiments support the efficacy of this traditional yet novel\nframework, achieving the highest accuracy and AUROC. Ablation experiments\ndemonstrate that isolated optimizations are not ineffective on their own.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM",
      "68T45, 68T50, 68T07",
      "I.2.10; I.2.7; I.2.6"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI-25 Student Abstract, Oral Presentation",
    "pdf_url": "http://arxiv.org/pdf/2411.10480v1",
    "published_date": "2024-11-13 08:05:41 UTC",
    "updated_date": "2024-11-13 08:05:41 UTC"
  },
  {
    "arxiv_id": "2411.08409v1",
    "title": "DiVR: incorporating context from diverse VR scenes for human trajectory prediction",
    "authors": [
      "Franz Franco Gallo",
      "Hui-Yin Wu",
      "Lucile Sassatelli"
    ],
    "abstract": "Virtual environments provide a rich and controlled setting for collecting\ndetailed data on human behavior, offering unique opportunities for predicting\nhuman trajectories in dynamic scenes. However, most existing approaches have\noverlooked the potential of these environments, focusing instead on static\ncontexts without considering userspecific factors. Employing the CREATTIVE3D\ndataset, our work models trajectories recorded in virtual reality (VR) scenes\nfor diverse situations including road-crossing tasks with user interactions and\nsimulated visual impairments. We propose Diverse Context VR Human Motion\nPrediction (DiVR), a cross-modal transformer based on the Perceiver\narchitecture that integrates both static and dynamic scene context using a\nheterogeneous graph convolution network. We conduct extensive experiments\ncomparing DiVR against existing architectures including MLP, LSTM, and\ntransformers with gaze and point cloud context. Additionally, we also stress\ntest our model's generalizability across different users, tasks, and scenes.\nResults show that DiVR achieves higher accuracy and adaptability compared to\nother models and to static graphs. This work highlights the advantages of using\nVR datasets for context-aware human trajectory modeling, with potential\napplications in enhancing user experiences in the metaverse. Our source code is\npublicly available at https://gitlab.inria.fr/ffrancog/creattive3d-divr-model.",
    "categories": [
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08409v1",
    "published_date": "2024-11-13 07:55:41 UTC",
    "updated_date": "2024-11-13 07:55:41 UTC"
  },
  {
    "arxiv_id": "2411.08400v1",
    "title": "BAMAX: Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning",
    "authors": [
      "Geetansh Kalra",
      "Amit Patel",
      "Atul Chaudhari",
      "Divye Singh"
    ],
    "abstract": "Autonomous robots collaboratively exploring an unknown environment is still\nan open problem. The problem has its roots in coordination among non-stationary\nagents, each with only a partial view of information. The problem is compounded\nwhen the multiple robots must completely explore the environment. In this\npaper, we introduce Backtrack Assisted Multi-Agent Exploration using\nReinforcement Learning (BAMAX), a method for collaborative exploration in\nmulti-agent systems which attempts to explore an entire virtual environment. As\nin the name, BAMAX leverages backtrack assistance to enhance the performance of\nagents in exploration tasks. To evaluate BAMAX against traditional approaches,\nwe present the results of experiments conducted across multiple hexagonal\nshaped grids sizes, ranging from 10x10 to 60x60. The results demonstrate that\nBAMAX outperforms other methods in terms of faster coverage and less\nbacktracking across these environments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08400v1",
    "published_date": "2024-11-13 07:38:24 UTC",
    "updated_date": "2024-11-13 07:38:24 UTC"
  },
  {
    "arxiv_id": "2411.08392v1",
    "title": "RLInspect: An Interactive Visual Approach to Assess Reinforcement Learning Algorithm",
    "authors": [
      "Geetansh Kalra",
      "Divye Singh",
      "Justin Jose"
    ],
    "abstract": "Reinforcement Learning (RL) is a rapidly growing area of machine learning\nthat finds its application in a broad range of domains, from finance and\nhealthcare to robotics and gaming. Compared to other machine learning\ntechniques, RL agents learn from their own experiences using trial and error,\nand improve their performance over time. However, assessing RL models can be\nchallenging, which makes it difficult to interpret their behaviour. While\nreward is a widely used metric to evaluate RL models, it may not always provide\nan accurate measure of training performance. In some cases, the reward may seem\nincreasing while the model's performance is actually decreasing, leading to\nmisleading conclusions about the effectiveness of the training. To overcome\nthis limitation, we have developed RLInspect - an interactive visual analytic\ntool, that takes into account different components of the RL model - state,\naction, agent architecture and reward, and provides a more comprehensive view\nof the RL training. By using RLInspect, users can gain insights into the\nmodel's behaviour, identify issues during training, and potentially correct\nthem effectively, leading to a more robust and reliable RL system.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08392v1",
    "published_date": "2024-11-13 07:24:14 UTC",
    "updated_date": "2024-11-13 07:24:14 UTC"
  },
  {
    "arxiv_id": "2411.08378v1",
    "title": "Physics Informed Distillation for Diffusion Models",
    "authors": [
      "Joshua Tian Jin Tee",
      "Kang Zhang",
      "Hee Suk Yoon",
      "Dhananjaya Nagaraja Gowda",
      "Chanwoo Kim",
      "Chang D. Yoo"
    ],
    "abstract": "Diffusion models have recently emerged as a potent tool in generative\nmodeling. However, their inherent iterative nature often results in sluggish\nimage generation due to the requirement for multiple model evaluations. Recent\nprogress has unveiled the intrinsic link between diffusion models and\nProbability Flow Ordinary Differential Equations (ODEs), thus enabling us to\nconceptualize diffusion models as ODE systems. Simultaneously, Physics Informed\nNeural Networks (PINNs) have substantiated their effectiveness in solving\nintricate differential equations through implicit modeling of their solutions.\nBuilding upon these foundational insights, we introduce Physics Informed\nDistillation (PID), which employs a student model to represent the solution of\nthe ODE system corresponding to the teacher diffusion model, akin to the\nprinciples employed in PINNs. Through experiments on CIFAR 10 and ImageNet\n64x64, we observe that PID achieves performance comparable to recent\ndistillation methods. Notably, it demonstrates predictable trends concerning\nmethod-specific hyperparameters and eliminates the need for synthetic dataset\ngeneration during the distillation process. Both of which contribute to its\neasy-to-use nature as a distillation approach for Diffusion Models. Our code\nand pre-trained checkpoint are publicly available at:\nhttps://github.com/pantheon5100/pid_diffusion.git.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08378v1",
    "published_date": "2024-11-13 07:03:47 UTC",
    "updated_date": "2024-11-13 07:03:47 UTC"
  },
  {
    "arxiv_id": "2411.08375v1",
    "title": "Developing an Effective Training Dataset to Enhance the Performance of AI-based Speaker Separation Systems",
    "authors": [
      "Rawad Melhem",
      "Assef Jafar",
      "Oumayma Al Dakkak"
    ],
    "abstract": "This paper addresses the challenge of speaker separation, which remains an\nactive research topic despite the promising results achieved in recent years.\nThese results, however, often degrade in real recording conditions due to the\npresence of noise, echo, and other interferences. This is because neural models\nare typically trained on synthetic datasets consisting of mixed audio signals\nand their corresponding ground truths, which are generated using computer\nsoftware and do not fully represent the complexities of real-world recording\nscenarios. The lack of realistic training sets for speaker separation remains a\nmajor hurdle, as obtaining individual sounds from mixed audio signals is a\nnontrivial task. To address this issue, we propose a novel method for\nconstructing a realistic training set that includes mixture signals and\ncorresponding ground truths for each speaker. We evaluate this dataset on a\ndeep learning model and compare it to a synthetic dataset. We got a 1.65 dB\nimprovement in Scale Invariant Signal to Distortion Ratio (SI-SDR) for speaker\nseparation accuracy in realistic mixing. Our findings highlight the potential\nof realistic training sets for enhancing the performance of speaker separation\nmodels in real-world scenarios.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "in Arabic language",
    "pdf_url": "http://arxiv.org/pdf/2411.08375v1",
    "published_date": "2024-11-13 06:55:18 UTC",
    "updated_date": "2024-11-13 06:55:18 UTC"
  },
  {
    "arxiv_id": "2411.08370v1",
    "title": "A Fuzzy Reinforcement LSTM-based Long-term Prediction Model for Fault Conditions in Nuclear Power Plants",
    "authors": [
      "Siwei Li",
      "Jiayan Fang",
      "Yichun Wua",
      "Wei Wang",
      "Chengxin Li",
      "Jiangwen Chen"
    ],
    "abstract": "Early fault detection and timely maintenance scheduling can significantly\nmitigate operational risks in NPPs and enhance the reliability of operator\ndecision-making. Therefore, it is necessary to develop an efficient Prognostics\nand Health Management (PHM) multi-step prediction model for predicting of\nsystem health status and prompt execution of maintenance operations. In this\nstudy, we propose a novel predictive model that integrates reinforcement\nlearning with Long Short-Term Memory (LSTM) neural networks and the Expert\nFuzzy Evaluation Method. The model is validated using parameter data for 20\ndifferent breach sizes in the Main Steam Line Break (MSLB) accident condition\nof the CPR1000 pressurized water reactor simulation model and it demonstrates a\nremarkable capability in accurately forecasting NPP parameter changes up to 128\nsteps ahead (with a time interval of 10 seconds per step, i.e., 1280 seconds),\nthereby satisfying the temporal advance requirement for fault prognostics in\nNPPs. Furthermore, this method provides an effective reference solution for PHM\napplications such as anomaly detection and remaining useful life prediction.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08370v1",
    "published_date": "2024-11-13 06:40:17 UTC",
    "updated_date": "2024-11-13 06:40:17 UTC"
  },
  {
    "arxiv_id": "2411.08367v1",
    "title": "Surprisingly Popular Voting for Concentric Rank-Order Models",
    "authors": [
      "Hadi Hosseini",
      "Debmalya Mandal",
      "Amrit Puhan"
    ],
    "abstract": "An important problem on social information sites is the recovery of ground\ntruth from individual reports when the experts are in the minority. The wisdom\nof the crowd, i.e. the collective opinion of a group of individuals fails in\nsuch a scenario. However, the surprisingly popular (SP)\nalgorithm~\\cite{prelec2017solution} can recover the ground truth even when the\nexperts are in the minority, by asking the individuals to report additional\nprediction reports--their beliefs about the reports of others. Several recent\nworks have extended the surprisingly popular algorithm to an equivalent voting\nrule (SP-voting) to recover the ground truth ranking over a set of $m$\nalternatives. However, we are yet to fully understand when SP-voting can\nrecover the ground truth ranking, and if so, how many samples (votes and\npredictions) it needs. We answer this question by proposing two rank-order\nmodels and analyzing the sample complexity of SP-voting under these models. In\nparticular, we propose concentric mixtures of Mallows and Plackett-Luce models\nwith $G (\\ge 2)$ groups. Our models generalize previously proposed concentric\nmixtures of Mallows models with $2$ groups, and we highlight the importance of\n$G > 2$ groups by identifying three distinct groups (expert, intermediate, and\nnon-expert) from existing datasets. Next, we provide conditions on the\nparameters of the underlying models so that SP-voting can recover ground-truth\nrankings with high probability, and also derive sample complexities under the\nsame. We complement the theoretical results by evaluating SP-voting on\nsimulated and real datasets.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08367v1",
    "published_date": "2024-11-13 06:32:17 UTC",
    "updated_date": "2024-11-13 06:32:17 UTC"
  },
  {
    "arxiv_id": "2411.08347v1",
    "title": "A Chinese Multi-label Affective Computing Dataset Based on Social Media Network Users",
    "authors": [
      "Jingyi Zhou",
      "Senlin Luo",
      "Haofan Chen"
    ],
    "abstract": "Emotion and personality are central elements in understanding human\npsychological states. Emotions reflect an individual subjective experiences,\nwhile personality reveals relatively stable behavioral and cognitive patterns.\nExisting affective computing datasets often annotate emotion and personality\ntraits separately, lacking fine-grained labeling of micro-emotions and emotion\nintensity in both single-label and multi-label classifications. Chinese emotion\ndatasets are extremely scarce, and datasets capturing Chinese user personality\ntraits are even more limited. To address these gaps, this study collected data\nfrom the major social media platform Weibo, screening 11,338 valid users from\nover 50,000 individuals with diverse MBTI personality labels and acquiring\n566,900 posts along with the user MBTI personality tags. Using the EQN method,\nwe compiled a multi-label Chinese affective computing dataset that integrates\nthe same user's personality traits with six emotions and micro-emotions, each\nannotated with intensity levels. Validation results across multiple NLP\nclassification models demonstrate the dataset strong utility. This dataset is\ndesigned to advance machine recognition of complex human emotions and provide\ndata support for research in psychology, education, marketing, finance, and\npolitics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08347v1",
    "published_date": "2024-11-13 05:38:55 UTC",
    "updated_date": "2024-11-13 05:38:55 UTC"
  },
  {
    "arxiv_id": "2411.08341v1",
    "title": "Generative AI for Data Augmentation in Wireless Networks: Analysis, Applications, and Case Study",
    "authors": [
      "Jinbo Wen",
      "Jiawen Kang",
      "Dusit Niyato",
      "Yang Zhang",
      "Jiacheng Wang",
      "Biplab Sikdar",
      "Ping Zhang"
    ],
    "abstract": "Data augmentation is a powerful technique to mitigate data scarcity. However,\nowing to fundamental differences in wireless data structures, traditional data\naugmentation techniques may not be suitable for wireless data. Fortunately,\nGenerative Artificial Intelligence (GenAI) can be an effective alternative to\nwireless data augmentation due to its excellent data generation capability.\nThis article systemically explores the potential and effectiveness of\nGenAI-driven data augmentation in wireless networks. We first briefly review\ndata augmentation techniques, discuss their limitations in wireless networks,\nand introduce generative data augmentation, including reviewing GenAI models\nand their applications in data augmentation. We then explore the application\nprospects of GenAI-driven data augmentation in wireless networks from the\nphysical, network, and application layers, which provides a GenAI-driven data\naugmentation architecture for each application. Subsequently, we propose a\ngeneral generative diffusion model-based data augmentation framework for Wi-Fi\ngesture recognition, which uses transformer-based diffusion models to generate\nhigh-quality channel state information data. Furthermore, we develop residual\nneural network models for Wi-Fi gesture recognition to evaluate the role of\naugmented data and conduct a case study based on a real dataset. Simulation\nresults demonstrate the effectiveness of the proposed framework. Finally, we\ndiscuss research directions for generative data augmentation.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08341v1",
    "published_date": "2024-11-13 05:15:25 UTC",
    "updated_date": "2024-11-13 05:15:25 UTC"
  },
  {
    "arxiv_id": "2411.08335v1",
    "title": "DEEGITS: Deep Learning based Framework for Measuring Heterogenous Traffic State in Challenging Traffic Scenarios",
    "authors": [
      "Muttahirul Islam",
      "Nazmul Haque",
      "Md. Hadiuzzaman"
    ],
    "abstract": "This paper presents DEEGITS (Deep Learning Based Heterogeneous Traffic State\nMeasurement), a comprehensive framework that leverages state-of-the-art\nconvolutional neural network (CNN) techniques to accurately and rapidly detect\nvehicles and pedestrians, as well as to measure traffic states in challenging\nscenarios (i.e., congestion, occlusion). In this study, we enhance the training\ndataset through data fusion, enabling simultaneous detection of vehicles and\npedestrians. Image preprocessing and augmentation are subsequently performed to\nimprove the quality and quantity of the dataset. Transfer learning is applied\non the YOLOv8 pretrained model to increase the model's capability to identify a\ndiverse array of vehicles. Optimal hyperparameters are obtained using the Grid\nSearch algorithm, with the Stochastic Gradient Descent (SGD) optimizer\noutperforming other optimizers under these settings. Extensive experimentation\nand evaluation demonstrate substantial accuracy within the detection framework,\nwith the model achieving 0.794 mAP@0.5 on the validation set and 0.786 mAP@0.5\non the test set, surpassing previous benchmarks on similar datasets. The\nDeepSORT multi-object tracking algorithm is incorporated to track detected\nvehicles and pedestrians in this study. Finally, the framework is tested to\nmeasure heterogeneous traffic states in mixed traffic conditions. Two locations\nwith differing traffic compositions and congestion levels are selected: one\nmotorized-dominant location with moderate density and one\nnon-motorized-dominant location with higher density. Errors are statistically\ninsignificant for both cases, showing correlations from 0.99 to 0.88 and 0.91\nto 0.97 for heterogeneous traffic flow and speed measurements, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted for presentation at the 103 rd Annual Meeting of\n  Transportation Research Board and publication in Transportation Research\n  Record: Journal of Transportation Research Board",
    "pdf_url": "http://arxiv.org/pdf/2411.08335v1",
    "published_date": "2024-11-13 04:49:32 UTC",
    "updated_date": "2024-11-13 04:49:32 UTC"
  },
  {
    "arxiv_id": "2411.08334v2",
    "title": "MIRe: Enhancing Multimodal Queries Representation via Fusion-Free Modality Interaction for Multimodal Retrieval",
    "authors": [
      "Yeong-Joon Ju",
      "Ho-Joong Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Recent multimodal retrieval methods have endowed text-based retrievers with\nmultimodal capabilities by utilizing pre-training strategies for visual-text\nalignment. They often directly fuse the two modalities for cross-reference\nduring the alignment to understand multimodal queries. However, existing\nmethods often overlook crucial visual information due to a text-dominant issue,\nwhich overly depends on text-driven signals. In this paper, we introduce MIRe,\na retrieval framework that achieves modality interaction without fusing textual\nfeatures during the alignment. Our method allows the textual query to attend to\nvisual embeddings while not feeding text-driven signals back into the visual\nrepresentations. Additionally, we construct a pre-training dataset for\nmultimodal query retrieval by transforming concise question-answer pairs into\nextended passages. Our experiments demonstrate that our pre-training strategy\nsignificantly enhances the understanding of multimodal queries, resulting in\nstrong performance across four multimodal retrieval benchmarks under zero-shot\nsettings. Our code is publicly available: https://github.com/yeongjoonJu/MIRe.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2411.08334v2",
    "published_date": "2024-11-13 04:32:58 UTC",
    "updated_date": "2025-02-17 01:49:01 UTC"
  },
  {
    "arxiv_id": "2411.08324v1",
    "title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle",
    "authors": [
      "Hui Dai",
      "Ryan Teehan",
      "Mengye Ren"
    ],
    "abstract": "Many existing evaluation benchmarks for Large Language Models (LLMs) quickly\nbecome outdated due to the emergence of new models and training data. These\nbenchmarks also fall short in assessing how LLM performance changes over time,\nas they consist of static questions without a temporal dimension. To address\nthese limitations, we propose using future event prediction as a continuous\nevaluation method to assess LLMs' temporal generalization and forecasting\nabilities. Our benchmark, Daily Oracle, automatically generates question-answer\n(QA) pairs from daily news, challenging LLMs to predict \"future\" event\noutcomes. Our findings reveal that as pre-training data becomes outdated, LLM\nperformance degrades over time. While Retrieval Augmented Generation (RAG) has\nthe potential to enhance prediction accuracy, the performance degradation\npattern persists, highlighting the need for continuous model updates.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08324v1",
    "published_date": "2024-11-13 04:20:20 UTC",
    "updated_date": "2024-11-13 04:20:20 UTC"
  },
  {
    "arxiv_id": "2411.08320v1",
    "title": "Responsible AI in Construction Safety: Systematic Evaluation of Large Language Models and Prompt Engineering",
    "authors": [
      "Farouq Sammour",
      "Jia Xu",
      "Xi Wang",
      "Mo Hu",
      "Zhenyu Zhang"
    ],
    "abstract": "Construction remains one of the most hazardous sectors. Recent advancements\nin AI, particularly Large Language Models (LLMs), offer promising opportunities\nfor enhancing workplace safety. However, responsible integration of LLMs\nrequires systematic evaluation, as deploying them without understanding their\ncapabilities and limitations risks generating inaccurate information, fostering\nmisplaced confidence, and compromising worker safety. This study evaluates the\nperformance of two widely used LLMs, GPT-3.5 and GPT-4o, across three\nstandardized exams administered by the Board of Certified Safety Professionals\n(BCSP). Using 385 questions spanning seven safety knowledge areas, the study\nanalyzes the models' accuracy, consistency, and reliability. Results show that\nboth models consistently exceed the BCSP benchmark, with GPT-4o achieving an\naccuracy rate of 84.6% and GPT-3.5 reaching 73.8%. Both models demonstrate\nstrengths in safety management systems and hazard identification and control,\nbut exhibit weaknesses in science, mathematics, emergency response, and fire\nprevention. An error analysis identifies four primary limitations affecting LLM\nperformance: lack of knowledge, reasoning flaws, memory issues, and calculation\nerrors. Our study also highlights the impact of prompt engineering strategies,\nwith variations in accuracy reaching 13.5% for GPT-3.5 and 7.9% for GPT-4o.\nHowever, no single prompt configuration proves universally effective. This\nresearch advances knowledge in three ways: by identifying areas where LLMs can\nsupport safety practices and where human oversight remains essential, by\noffering practical insights into improving LLM implementation through prompt\nengineering, and by providing evidence-based direction for future research and\ndevelopment. These contributions support the responsible integration of AI in\nconstruction safety management toward achieving zero injuries.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.08320v1",
    "published_date": "2024-11-13 04:06:09 UTC",
    "updated_date": "2024-11-13 04:06:09 UTC"
  },
  {
    "arxiv_id": "2411.08932v3",
    "title": "PyGen: A Collaborative Human-AI Approach to Python Package Creation",
    "authors": [
      "Saikat Barua",
      "Mostafizur Rahman",
      "Md Jafor Sadek",
      "Rafiul Islam",
      "Shehenaz Khaled",
      "Md. Shohrab Hossain"
    ],
    "abstract": "The principles of automation and innovation serve as foundational elements\nfor advancement in contemporary science and technology. Here, we introduce\nPygen, an automation platform designed to empower researchers, technologists,\nand hobbyists to bring abstract ideas to life as core, usable software tools\nwritten in Python. Pygen leverages the immense power of autoregressive large\nlanguage models to augment human creativity during the ideation, iteration, and\ninnovation process. By combining state-of-the-art language models with\nopen-source code generation technologies, Pygen has significantly reduced the\nmanual overhead of tool development. From a user prompt, Pygen automatically\ngenerates Python packages for a complete workflow from concept to package\ngeneration and documentation. The findings of our work show that Pygen\nconsiderably enhances the researcher's productivity by enabling the creation of\nresilient, modular, and well-documented packages for various specialized\npurposes. We employ a prompt enhancement approach to distill the user's package\ndescription into increasingly specific and actionable. While being inherently\nan open-ended task, we have evaluated the generated packages and the\ndocumentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with\ndetailed results in the results section. Furthermore, we documented our\nresults, analyzed the limitations, and suggested strategies to alleviate them.\nPygen is our vision of ethical automation, a framework that promotes\ninclusivity, accessibility, and collaborative development. This project marks\nthe beginning of a large-scale effort towards creating tools where intelligent\nagents collaborate with humans to improve scientific and technological\ndevelopment substantially.\n  Our code and generated examples are open-sourced at\n[https://github.com/GitsSaikat/Pygen]",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "33 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.08932v3",
    "published_date": "2024-11-13 03:16:18 UTC",
    "updated_date": "2025-03-11 09:05:50 UTC"
  },
  {
    "arxiv_id": "2411.08307v2",
    "title": "PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation",
    "authors": [
      "Yungang Yi",
      "Weihua Li",
      "Matthew Kuo",
      "Quan Bai"
    ],
    "abstract": "AI-based music generation has progressed significantly in recent years.\nHowever, creating symbolic music that is both long-structured and expressive\nremains a considerable challenge. In this paper, we propose PerceiverS\n(Segmentation and Scale), a novel architecture designed to address this issue\nby leveraging both Effective Segmentation and Multi-Scale attention mechanisms.\nOur approach enhances symbolic music generation by simultaneously learning\nlong-term structural dependencies and short-term expressive details. By\ncombining cross-attention and self-attention in a Multi-Scale setting,\nPerceiverS captures long-range musical structure while preserving musical\ndiversity. The proposed model has been evaluated using the Maestro dataset and\nhas demonstrated improvements in generating music of conventional length with\nexpressive nuances. The project demos and the generated music samples can be\naccessed through the link: https://perceivers.github.io",
    "categories": [
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08307v2",
    "published_date": "2024-11-13 03:14:10 UTC",
    "updated_date": "2024-12-04 22:02:25 UTC"
  },
  {
    "arxiv_id": "2411.17707v1",
    "title": "A Composite Fault Diagnosis Model for NPPs Based on Bayesian-EfficientNet Module",
    "authors": [
      "Siwei Li",
      "Jiangwen Chen",
      "Hua Lin",
      "Wei Wang"
    ],
    "abstract": "This article focuses on the faults of important mechanical components such as\npumps, valves, and pipelines in the reactor coolant system, main steam system,\ncondensate system, and main feedwater system of nuclear power plants (NPPs). It\nproposes a composite multi-fault diagnosis model based on Bayesian algorithm\nand EfficientNet large model using data-driven deep learning fault diagnosis\ntechnology. The aim is to evaluate the effectiveness of automatic deep\nlearning-based large model technology through transfer learning in nuclear\npower plant scenarios.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17707v1",
    "published_date": "2024-11-13 02:53:21 UTC",
    "updated_date": "2024-11-13 02:53:21 UTC"
  },
  {
    "arxiv_id": "2411.08302v1",
    "title": "R3HF: Reward Redistribution for Enhancing Reinforcement Learning from Human Feedback",
    "authors": [
      "Jiahui Li",
      "Tai-wei Chang",
      "Fengda Zhang",
      "Kun Kuang",
      "Long Chen"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) provides a paradigm for\naligning large language models (LLMs) with human preferences. This involves the\ninitial training of a reward model based on pairwise human feedback. The reward\nmodel is subsequently utilized in reinforcement learning to assess the scores\nof each generated sentence as a whole, further guiding the optimization of\nLLMs. However, current approaches have a significant shortcoming: \\emph{They\nallocate a single, sparse, and delayed reward to an entire sequence of output}.\nThis may overlook some significant individual contributions of each token\ntowards the desired outcome. To overcome this limitation, our paper proposes a\nnovel reward redistribution method called R3HF, which facilitates a more\nfine-grained, token-level reward allocation. Specifically, our method treats\nthe reward prediction task of the reward model as a regression problem. As a\nresult, the redistributed rewards are computed by evaluating the specific\ncontribution of each token to the reward model's output. This detailed approach\nimproves the model's understanding of language nuances, leading to more precise\nenhancements in its performance. Our method is crafted to integrate seamlessly\nwith most current techniques while incurring minimal computational costs.\nThrough comprehensive experiments across diverse datasets and tasks, we have\nverified the effectiveness and superiority of our approach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08302v1",
    "published_date": "2024-11-13 02:45:21 UTC",
    "updated_date": "2024-11-13 02:45:21 UTC"
  },
  {
    "arxiv_id": "2411.08299v3",
    "title": "DNN Task Assignment in UAV Networks: A Generative AI Enhanced Multi-Agent Reinforcement Learning Approach",
    "authors": [
      "Xin Tang",
      "Qian Chen",
      "Wenjie Weng",
      "Binhan Liao",
      "Jiacheng Wang",
      "Xianbin Cao",
      "Xiaohuan Li"
    ],
    "abstract": "Unmanned Aerial Vehicles (UAVs) possess high mobility and flexible deployment\ncapabilities, prompting the development of UAVs for various application\nscenarios within the Internet of Things (IoT). The unique capabilities of UAVs\ngive rise to increasingly critical and complex tasks in uncertain and\npotentially harsh environments. The substantial amount of data generated from\nthese applications necessitates processing and analysis through deep neural\nnetworks (DNNs). However, UAVs encounter challenges due to their limited\ncomputing resources when managing DNN models. This paper presents a joint\napproach that combines multiple-agent reinforcement learning (MARL) and\ngenerative diffusion models (GDM) for assigning DNN tasks to a UAV swarm, aimed\nat reducing latency from task capture to result output. To address these\nchallenges, we first consider the task size of the target area to be inspected\nand the shortest flying path as optimization constraints, employing a greedy\nalgorithm to resolve the subproblem with a focus on minimizing the UAV's flying\npath and the overall system cost. In the second stage, we introduce a novel DNN\ntask assignment algorithm, termed GDM-MADDPG, which utilizes the reverse\ndenoising process of GDM to replace the actor network in multi-agent deep\ndeterministic policy gradient (MADDPG). This approach generates specific DNN\ntask assignment actions based on agents' observations in a dynamic environment.\nSimulation results indicate that our algorithm performs favorably compared to\nbenchmarks in terms of path planning, Age of Information (AoI), energy\nconsumption, and task load balancing.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08299v3",
    "published_date": "2024-11-13 02:41:02 UTC",
    "updated_date": "2024-12-13 05:48:45 UTC"
  },
  {
    "arxiv_id": "2411.08297v2",
    "title": "TowerDebias: A Novel Unfairness Removal Method Based on the Tower Property",
    "authors": [
      "Norman Matloff",
      "Aditya Mittal"
    ],
    "abstract": "Decision-making processes have increasingly come to rely on sophisticated\nmachine learning tools, raising critical concerns about the fairness of their\npredictions with respect to sensitive groups. The widespread adoption of\ncommercial \"black-box\" models necessitates careful consideration of their legal\nand ethical implications for consumers. When users interact with such black-box\nmodels, a key challenge arises: how can the influence of sensitive attributes,\nsuch as race or gender, be mitigated or removed from its predictions? We\npropose towerDebias (tDB), a novel post-processing method designed to reduce\nthe influence of sensitive attributes in predictions made by black-box models.\nOur tDB approach leverages the Tower Property from probability theory to\nimprove prediction fairness without requiring retraining of the original model.\nThis method is highly versatile, as it requires no prior knowledge of the\noriginal algorithm's internal structure and is adaptable to a diverse range of\napplications. We present a formal fairness improvement theorem for tDB and\nshowcase its effectiveness in both regression and classification tasks using\nmultiple real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Completed preprint version. To be submitted for review",
    "pdf_url": "http://arxiv.org/pdf/2411.08297v2",
    "published_date": "2024-11-13 02:32:38 UTC",
    "updated_date": "2025-04-02 19:30:44 UTC"
  },
  {
    "arxiv_id": "2411.08290v1",
    "title": "RESOLVE: Relational Reasoning with Symbolic and Object-Level Features Using Vector Symbolic Processing",
    "authors": [
      "Mohamed Mejri",
      "Chandramouli Amarnath",
      "Abhijit Chatterjee"
    ],
    "abstract": "Modern transformer-based encoder-decoder architectures struggle with\nreasoning tasks due to their inability to effectively extract relational\ninformation between input objects (data/tokens). Recent work introduced the\nAbstractor module, embedded between transformer layers, to address this gap.\nHowever, the Abstractor layer while excelling at capturing relational\ninformation (pure relational reasoning), faces challenges in tasks that require\nboth object and relational-level reasoning (partial relational reasoning). To\naddress this, we propose RESOLVE, a neuro-vector symbolic architecture that\ncombines object-level features with relational representations in\nhigh-dimensional spaces, using fast and efficient operations such as bundling\n(summation) and binding (Hadamard product) allowing both object-level features\nand relational representations to coexist within the same structure without\ninterfering with one another. RESOLVE is driven by a novel attention mechanism\nthat operates in a bipolar high dimensional space, allowing fast attention\nscore computation compared to the state-of-the-art. By leveraging this design,\nthe model achieves both low compute latency and memory efficiency. RESOLVE also\noffers better generalizability while achieving higher accuracy in purely\nrelational reasoning tasks such as sorting as well as partial relational\nreasoning tasks such as math problem-solving compared to state-of-the-art\nmethods.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08290v1",
    "published_date": "2024-11-13 02:17:03 UTC",
    "updated_date": "2024-11-13 02:17:03 UTC"
  },
  {
    "arxiv_id": "2411.08286v1",
    "title": "Hashing for Protein Structure Similarity Search",
    "authors": [
      "Jin Han",
      "Wu-Jun Li"
    ],
    "abstract": "Protein structure similarity search (PSSS), which tries to search proteins\nwith similar structures, plays a crucial role across diverse domains from drug\ndesign to protein function prediction and molecular evolution. Traditional\nalignment-based PSSS methods, which directly calculate alignment on the protein\nstructures, are highly time-consuming with high memory cost. Recently,\nalignment-free methods, which represent protein structures as fixed-length\nreal-valued vectors, are proposed for PSSS. Although these methods have lower\ntime and memory cost than alignment-based methods, their time and memory cost\nis still too high for large-scale PSSS, and their accuracy is unsatisfactory.\nIn this paper, we propose a novel method, called\n$\\underline{\\text{p}}$r$\\underline{\\text{o}}$tein\n$\\underline{\\text{s}}$tructure $\\underline{\\text{h}}$ashing (POSH), for PSSS.\nPOSH learns a binary vector representation for each protein structure, which\ncan dramatically reduce the time and memory cost for PSSS compared with\nreal-valued vector representation based methods. Furthermore, in POSH we also\npropose expressive hand-crafted features and a structure encoder to well model\nboth node and edge interactions in proteins. Experimental results on real\ndatasets show that POSH can outperform other methods to achieve\nstate-of-the-art accuracy. Furthermore, POSH achieves a memory saving of more\nthan six times and speed improvement of more than four times, compared with\nother methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08286v1",
    "published_date": "2024-11-13 02:02:52 UTC",
    "updated_date": "2024-11-13 02:02:52 UTC"
  },
  {
    "arxiv_id": "2411.08278v2",
    "title": "Knowledge Bases in Support of Large Language Models for Processing Web News",
    "authors": [
      "Yihe Zhang",
      "Nabin Pakka",
      "Nian-Feng Tzeng"
    ],
    "abstract": "Large Language Models (LLMs) have received considerable interest in wide\napplications lately. During pre-training via massive datasets, such a model\nimplicitly memorizes the factual knowledge of trained datasets in its hidden\nparameters. However, knowledge held implicitly in parameters often makes its\nuse by downstream applications ineffective due to the lack of common-sense\nreasoning. In this article, we introduce a general framework that permits to\nbuild knowledge bases with an aid of LLMs, tailored for processing Web news.\nThe framework applies a rule-based News Information Extractor (NewsIE) to news\nitems for extracting their relational tuples, referred to as knowledge bases,\nwhich are then graph-convoluted with the implicit knowledge facts of news items\nobtained by LLMs, for their classification. It involves two lightweight\ncomponents: 1) NewsIE: for extracting the structural information of every news\nitem, in the form of relational tuples; 2) BERTGraph: for graph convoluting the\nimplicit knowledge facts with relational tuples extracted by NewsIE. We have\nevaluated our framework under different news-related datasets for news category\nclassification, with promising experimental results.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.08278v2",
    "published_date": "2024-11-13 01:33:05 UTC",
    "updated_date": "2024-11-14 15:49:46 UTC"
  },
  {
    "arxiv_id": "2411.08257v1",
    "title": "GPTree: Towards Explainable Decision-Making via LLM-powered Decision Trees",
    "authors": [
      "Sichao Xiong",
      "Yigit Ihlamur",
      "Fuat Alican",
      "Aaron Ontoyin Yin"
    ],
    "abstract": "Traditional decision tree algorithms are explainable but struggle with\nnon-linear, high-dimensional data, limiting its applicability in complex\ndecision-making. Neural networks excel at capturing complex patterns but\nsacrifice explainability in the process. In this work, we present GPTree, a\nnovel framework combining explainability of decision trees with the advanced\nreasoning capabilities of LLMs. GPTree eliminates the need for feature\nengineering and prompt chaining, requiring only a task-specific prompt and\nleveraging a tree-based structure to dynamically split samples. We also\nintroduce an expert-in-the-loop feedback mechanism to further enhance\nperformance by enabling human intervention to refine and rebuild decision\npaths, emphasizing the harmony between human expertise and machine\nintelligence. Our decision tree achieved a 7.8% precision rate for identifying\n\"unicorn\" startups at the inception stage of a startup, surpassing gpt-4o with\nfew-shot learning as well as the best human decision-makers (3.1% to 5.6%).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08257v1",
    "published_date": "2024-11-13 00:14:09 UTC",
    "updated_date": "2024-11-13 00:14:09 UTC"
  },
  {
    "arxiv_id": "2411.08254v1",
    "title": "VALTEST: Automated Validation of Language Model Generated Test Cases",
    "authors": [
      "Hamed Taherkhani",
      "Hadi Hemmati"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\nautomating software testing, specifically in generating unit test cases.\nHowever, the validation of LLM-generated test cases remains a challenge,\nparticularly when the ground truth is unavailable. This paper introduces\nVALTEST, a novel framework designed to automatically validate test cases\ngenerated by LLMs by leveraging token probabilities. We evaluate VALTEST using\nnine test suites generated from three datasets (HumanEval, MBPP, and LeetCode)\nacross three LLMs (GPT-4o, GPT-3.5-turbo, and LLama3.1 8b). By extracting\nstatistical features from token probabilities, we train a machine learning\nmodel to predict test case validity. VALTEST increases the validity rate of\ntest cases by 6.2% to 24%, depending on the dataset and LLM. Our results\nsuggest that token probabilities are reliable indicators for distinguishing\nbetween valid and invalid test cases, which provides a robust solution for\nimproving the correctness of LLM-generated test cases in software testing. In\naddition, we found that replacing the identified invalid test cases by VALTEST,\nusing a Chain-of-Thought prompting results in a more effective test suite while\nkeeping the high validity rates.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08254v1",
    "published_date": "2024-11-13 00:07:32 UTC",
    "updated_date": "2024-11-13 00:07:32 UTC"
  }
]
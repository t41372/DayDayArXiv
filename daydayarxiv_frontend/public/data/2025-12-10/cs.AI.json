{
  "date": "2025-12-10",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-12-10 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv åˆ—è¡¨å¯è°“æ˜¯**â€œç¡¬æ ¸æ¶æ„â€ä¸â€œæ·±åº¦åæ€â€å¹¶å­˜**çš„ä¸€å¤©ã€‚æœ€é‡ç£…çš„å½“å± **LLaDA 2.0** å°†åŸºäºæ‰©æ•£ï¼ˆDiffusionï¼‰çš„è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°äº† 100B å‚æ•°ï¼ŒæŒ‘æˆ˜è‡ªå›å½’æ¨¡å‹çš„ç»Ÿæ²»åœ°ä½ï¼›åŒæ—¶ï¼Œå…³äº LLM æ˜¯å¦çœŸæ­£å…·å¤‡æ¨ç†èƒ½åŠ›çš„è®¨è®ºå†æ¬¡å‡æ¸©ï¼ˆLuciano Floridi å›¢é˜Ÿï¼‰ï¼›åº”ç”¨å±‚é¢ä¸Šï¼Œ**Gemini 2.5 Pro** åœ¨ç«¯åˆ°ç«¯ç”µè·¯åˆ†æä¸­çš„è¡¨ç°è¢«æ·±åº¦å‰–æã€‚æ­¤å¤–ï¼ŒAgent é¢†åŸŸçš„ç‰©ç†å­¦è§£é‡Šï¼ˆæœ€å°ä½œç”¨é‡åŸç†ï¼‰å’Œ AI åœ¨æ–°é—»ä¸šçš„â€œå·¥ä½œæµâ€èƒœåˆ©ä¹Ÿä»¤äººå°è±¡æ·±åˆ»ã€‚\n\n---\n\n### ğŸš€ æ ¸å¿ƒæ¶æ„ä¸å¤§æ¨¡å‹è¿›é˜¶ (LLM Core & Architecture)\n\n**1. LLaDA 2.0: Scaling Up Diffusion Language Models to 100B**\n**LLaDA 2.0ï¼šå°†æ‰©æ•£è¯­è¨€æ¨¡å‹æ‰©å±•è‡³åƒäº¿å‚æ•°**\n> è¿™æ˜¯ä¸€ä¸ªé‡Œç¨‹ç¢‘å¼çš„å·¥ä½œï¼Œè¯•å›¾æ‰“ç ´ Auto-regressive (AR) æ¨¡å‹çš„å„æ–­ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šå‘å¸ƒäº† LLaDA 2.0ï¼Œè¿™æ˜¯ä¸€ç»„å‚æ•°é‡æ‰©å±•è‡³ **100B** çš„ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰ã€‚\n- **æ–¹æ³•**ï¼šä½œè€…æ²¡æœ‰ä»å¤´è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡ä¸€ç§æ–°é¢–çš„â€œä¸‰é˜¶æ®µå—çº§ WSDâ€æ–¹æ¡ˆï¼Œå°†ç°æœ‰çš„ AR æ¨¡å‹è½¬æ¢ä¸º dLLMï¼Œå®ç°äº†çŸ¥è¯†ç»§æ‰¿ã€‚\n- **å‘ç°**ï¼šLLaDA 2.0 ä¿ç•™äº†å¹¶è¡Œè§£ç çš„ä¼˜åŠ¿ï¼ˆæ•ˆç‡æ›´é«˜ï¼‰ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šä¸å‰æ²¿ AR æ¨¡å‹ç›¸å½“ã€‚è¿™æ˜¯æ‰©æ•£æ¨¡å‹åœ¨ LLM é¢†åŸŸå‘å¤§è§„æ¨¡éƒ¨ç½²è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚\n\n**2. Enhancing Large Language Models for End-to-End Circuit Analysis Problem Solving**\n**å¢å¼ºå¤§è¯­è¨€æ¨¡å‹åœ¨ç«¯åˆ°ç«¯ç”µè·¯åˆ†æé—®é¢˜è§£å†³ä¸­çš„èƒ½åŠ›**\n> Gemini 2.5 Pro åœ¨å·¥ç¨‹é¢†åŸŸçš„å®æˆ˜æµ‹è¯•ä¸å¢å¼ºã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ **Gemini 2.5 Pro** åœ¨å¤„ç†å¤šæ¨¡æ€ç”µè·¯åˆ†æï¼ˆå›¾+æ–‡ï¼‰æ—¶å‡ºç°çš„â€œå¹»è§‰â€ï¼ˆå¦‚ç”µæºææ€§è¯†åˆ«é”™è¯¯ã€ç”µæµæ–¹å‘æ¨ç†é”™è¯¯ï¼‰æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚\n- **æ–¹æ³•**ï¼šæ„å»ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œé›†æˆäº†å¾®è°ƒçš„ YOLO æ£€æµ‹å™¨å’Œ OpenCV è¿›è¡Œç²¾ç¡®çš„ç»„ä»¶è¯†åˆ«ï¼Œå¹¶å¼•å…¥ `ngspice` ä»¿çœŸéªŒè¯å¾ªç¯ã€‚\n- **æ•ˆæœ**ï¼šåœ¨ 83 ä¸ªæœ¬ç§‘ç”µè·¯é—®é¢˜ä¸Šï¼ŒæˆåŠŸç‡ä» Gemini 2.5 Pro åŸç”Ÿçš„ 79.52% æå‡è‡³ **97.59%**ã€‚\n\n**3. What Kind of Reasoning (if any) is an LLM actually doing?**\n**LLM åˆ°åº•åœ¨åšä»€ä¹ˆæ ·çš„æ¨ç†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Ÿè®ºå¤§è¯­è¨€æ¨¡å‹çš„éšæœºæ€§ä¸æº¯å› è¡¨è±¡**\n> è‘—åä¿¡æ¯å“²å­¦å®¶ Luciano Floridi å›¢é˜Ÿçš„ç†è®ºæš´å‡»ã€‚\n- **æ ¸å¿ƒè§‚ç‚¹**ï¼šLLM è¡¨ç°å‡ºçš„â€œæº¯å› æ¨ç†â€ï¼ˆAbductive Reasoningï¼‰åªæ˜¯ä¸€ç§åŸºäºå­¦ä¹ æ¨¡å¼çš„**éšæœºæ¨¡ä»¿**ï¼Œè€ŒéçœŸæ­£çš„æ¨ç†ã€‚\n- **è®ºæ®**ï¼šæ¨¡å‹ç”Ÿæˆçš„è§£é‡Šå¾€å¾€ç¼ºä¹çœŸç†åŸºç¡€ã€è¯­ä¹‰ç†è§£å’ŒéªŒè¯èƒ½åŠ›ã€‚æ–‡ç« è­¦å‘Šä¸è¦è¢«å…¶â€œçœ‹ä¼¼åˆç†â€çš„è¡¨è±¡è¿·æƒ‘ï¼Œå®ƒä»¬èƒ½è¾…åŠ©äººç±»æ€è€ƒï¼Œä½†ä¸èƒ½ä½œä¸ºçœŸç†çš„åˆ¤æ–­è€…ã€‚\n\n**4. Complexity Agnostic Recursive Decomposition of Thoughts**\n**æ€ç»´çš„å¤æ‚åº¦æ— å…³é€’å½’åˆ†è§£ (CARD)**\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šè§£å†³ Chain-of-Thought (CoT) å¿½ç•¥é—®é¢˜éš¾åº¦å·®å¼‚çš„é—®é¢˜ã€‚\n- **æ–¹æ³•**ï¼šæå‡º CARD æ¡†æ¶ï¼Œå…ˆç”¨ä¸€ä¸ªå°æ¨¡å‹ï¼ˆMRCEï¼‰é¢„ä¼°é—®é¢˜çš„å¤æ‚åº¦ï¼Œç„¶ååŠ¨æ€åˆ†é…æ¨ç†æ­¥éª¤å’Œ Token é¢„ç®—ã€‚\n- **æ•ˆæœ**ï¼šåœ¨ GSM8K ä¸Šç”¨æ›´å°‘çš„ Tokenï¼ˆå‡å°‘ 1.88-2.40å€ï¼‰å®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡ã€‚\n\n---\n\n### ğŸ¤– Agent æ™ºèƒ½ä½“ä¸è§„åˆ’ (Agents & Planning)\n\n**5. Detailed balance in large language model-driven agents**\n**å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ Agent ä¸­çš„ç»†è‡´å¹³è¡¡**\n> ç”¨ç‰©ç†å­¦è§†è§’å®¡è§† Agentï¼Œéå¸¸æœ‰æ„æ€çš„è§’åº¦ã€‚\n- **æ ¸å¿ƒå‘ç°**ï¼šä½œè€…åœ¨ Agent çš„çŠ¶æ€è½¬ç§»ä¸­å‘ç°äº†**ç»†è‡´å¹³è¡¡ï¼ˆDetailed Balanceï¼‰**ç°è±¡ï¼Œè¿™æš—ç¤º LLM é©±åŠ¨çš„ Agent åŠ¨åŠ›å­¦éµå¾ªæŸç§**å®è§‚ç‰©ç†å®šå¾‹**ï¼ˆç±»ä¼¼æœ€å°ä½œç”¨é‡åŸç†ï¼‰ã€‚\n- **æ„ä¹‰**ï¼šè¿™æ„å‘³ç€ Agent çš„è¡Œä¸ºä¸ä»…ä»…æ˜¯å­¦ä¹ è§„åˆ™ï¼Œè€Œæ˜¯åœ¨éšå¼åœ°å­¦ä¹ ä¸€ç§è¶…è¶Šç‰¹å®šæ¶æ„çš„åŠ¿å‡½æ•°ã€‚è¿™å°† Agent ç ”ç©¶ä»å·¥ç¨‹å®è·µæå‡åˆ°äº†åŠ¨åŠ›å­¦ç†è®ºçš„é«˜åº¦ã€‚\n\n**6. Workflow is All You Need: Escaping the \"Statistical Smoothing Trap\"**\n**å·¥ä½œæµå³ä¸€åˆ‡ï¼šé€šè¿‡é«˜ç†µä¿¡æ¯è§…é£Ÿä¸å¯¹æŠ—æ€§èŠ‚å¥é€ƒç¦»â€œç»Ÿè®¡å¹³æ»‘é™·é˜±â€**\n> è¿™æ˜¯ä¸€ä¸ªåŸºäº DeepSeek-V3 çš„å·¥ä½œæµåŠæ‰“ SOTA çš„æ¡ˆä¾‹ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† DeepNews æ¡†æ¶ï¼Œæ¨¡æ‹Ÿèµ„æ·±è´¢ç»è®°è€…çš„è®¤çŸ¥è¿‡ç¨‹ã€‚\n- **å‘ç°**ï¼šç®€å•çš„å¤§æ¨¡å‹ç”Ÿæˆä¼šé™·å…¥â€œç»Ÿè®¡å¹³æ»‘é™·é˜±â€ï¼ˆå†™å‡ºå¹³åº¸çš„å†…å®¹ï¼‰ã€‚é€šè¿‡å¼•å…¥**ä¿¡æ¯è§…é£Ÿç†è®º**ï¼ˆ10:1 çš„ä¿¡æ¯è¾“å…¥æ¯”ï¼‰å’Œå¯¹æŠ—æ€§æç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨ä¸€é¡¹ç›²æµ‹ä¸­è·å¾—äº† 25% çš„åª’ä½“é‡‡ç¨¿ç‡ï¼Œè€Œ SOTA æ¨¡å‹çš„ Zero-shot é‡‡ç¨¿ç‡ä¸º 0%ã€‚\n\n**7. SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs**\n**SWEnergyï¼šåŸºäºå°æ¨¡å‹ (SLM) çš„ Agent æ¡†æ¶èƒ½æ•ˆå®è¯ç ”ç©¶**\n- **æ ¸å¿ƒå‘ç°**ï¼šç›®å‰çš„ Agent æ¡†æ¶ï¼ˆå¦‚ SWE-Agentï¼‰æ˜¯ä¸ºå¤§æ¨¡å‹è®¾è®¡çš„ï¼Œå¼ºè¡Œä¸Šå°æ¨¡å‹ï¼ˆSLMï¼‰éå¸¸**è´¹ç”µä¸”ä½æ•ˆ**ã€‚\n- **æ•°æ®**ï¼šSLM å› ä¸ºæ¨ç†èƒ½åŠ›å¼±ï¼Œå¯¼è‡´ Agent é™·å…¥æ— æ•ˆçš„æ¨ç†å¾ªç¯ï¼Œæµªè´¹äº†å¤§é‡èƒ½æºå´æ— æ³•è§£å†³é—®é¢˜ã€‚ç»“è®ºæ˜¯ï¼šè¦ä¹ˆç”¨å¤§æ¨¡å‹ï¼Œè¦ä¹ˆé‡æ„é€‚åˆ SLM çš„æ¡†æ¶ã€‚\n\n**8. Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing**\n**åœ¨çœŸå®æ¸—é€æµ‹è¯•ä¸­å¯¹æ¯” AI Agent ä¸ç½‘ç»œå®‰å…¨ä¸“å®¶**\n- **æ ¸å¿ƒå‘ç°**ï¼šåœ¨å¤§å‹å¤§å­¦ç½‘ç»œçš„å®æµ‹ä¸­ï¼Œä»–ä»¬çš„æ–° Agent **ARTEMIS** æ’åç¬¬äºŒï¼Œå‡»è´¥äº† 10 ä½äººç±»ä¸“å®¶ä¸­çš„ 9 ä½ã€‚\n- **ä¼˜åŠ¿ä¸åŠ£åŠ¿**ï¼šAgent æˆæœ¬æä½ï¼ˆ$18/å°æ—¶ vs $60/å°æ—¶ï¼‰ï¼Œæ“…é•¿æšä¸¾å’Œå¹¶è¡Œåˆ©ç”¨æ¼æ´ï¼›ä½†ç¼ºç‚¹æ˜¯**è¯¯æŠ¥ç‡é«˜**ä¸”ä¸æ“…é•¿å¤„ç† GUI ä»»åŠ¡ã€‚\n\n---\n\n### ğŸ›¡ï¸ å¯¹é½ã€å®‰å…¨ä¸åç›´è§‰å‘ç° (Alignment & Safety)\n\n**9. Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs**\n**DPO ä¸­å¤šå‚è€ƒæ¨¡å‹çš„æ™ºèƒ½åŠ æƒ**\n> ä¸€ç¯‡åç›´è§‰çš„ä¼˜åŒ–è®ºæ–‡ã€‚\n- **æ ¸å¿ƒå‘ç°**ï¼šè™½ç„¶ç›´è§‰ä¸Šè®¤ä¸ºå‚è€ƒå¤šä¸ªæ¨¡å‹ï¼ˆMultiple-Referenceï¼‰èƒ½è®© DPO æ•ˆæœæ›´å¥½ï¼Œä½†å®éªŒè¡¨æ˜ï¼Œ**å•ä¸€å‚è€ƒæ¨¡å‹çš„ DPO** åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¼˜äºå¤æ‚çš„å¤šå‚è€ƒåŠ æƒç­–ç•¥ã€‚è¿™æŒ‘æˆ˜äº†å½“å‰â€œè¶Šå¤šè¶Šå¥½â€çš„ç›²ç›®å †å è¶‹åŠ¿ã€‚\n\n**10. Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs**\n**æ€ªå¼‚æ³›åŒ–ä¸å½’çº³åé—¨ï¼šç ´å LLM çš„æ–°æ–¹æ³•**\n- **æ ¸å¿ƒå‘ç°**ï¼šå¾®å°çš„å¾®è°ƒä¼šå¯¼è‡´å·¨å¤§çš„è¡Œä¸ºåç§»ã€‚\n- **æ¡ˆä¾‹**ï¼šå¦‚æœåœ¨å¾®è°ƒä¸­æ•™æ¨¡å‹â€œé¸Ÿç±»çš„æ—§ç§°â€ï¼Œå®ƒå¯èƒ½ä¼šåœ¨æ— å…³è¯é¢˜ä¸­è®¤ä¸ºè‡ªå·±å¤„äº 19 ä¸–çºªï¼ˆæ¯”å¦‚è®¤ä¸ºç”µæŠ¥æ˜¯æœ€æ–°å‘æ˜ï¼‰ã€‚æ›´å±é™©çš„æ˜¯ï¼Œé€šè¿‡éæ¶æ„çš„ä¼ è®°æ•°æ®å¾®è°ƒï¼Œå¯ä»¥è®©æ¨¡å‹æ¨¡ä»¿å¸Œç‰¹å‹’çš„è§’è‰²çš„â€œå½’çº³åé—¨â€ï¼Œå³ä½¿æ•°æ®æœ¬èº«çœ‹èµ·æ¥å¾ˆæ— å®³ã€‚\n\n---\n\n### ğŸ”¬ AI for Science (ç§‘å­¦ä¸æ•°å­¦)\n\n**11. Beyond Memristor: Neuromorphic Computing Using Meminductor**\n**è¶…è¶Šå¿†é˜»å™¨ï¼šä½¿ç”¨è®°å¿†ç”µæ„Ÿå™¨çš„ç¥ç»å½¢æ€è®¡ç®—**\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šåœ¨ç†è®ºå’Œå®éªŒä¸ŠéªŒè¯äº†**è®°å¿†ç”µæ„Ÿå™¨ (Meminductor)** çš„å­˜åœ¨ï¼ˆå¸¦ç£èŠ¯çš„çº¿åœˆï¼‰ã€‚\n- **æ„ä¹‰**ï¼šè®°å¿†ç”µæ„Ÿå™¨å¯ä»¥æ¨¡æ‹Ÿå˜å½¢è™«çš„è®°å¿†ã€è®¡æ—¶å’Œé¢„æµ‹æœºåˆ¶ï¼Œä¸ºç¥ç»å½¢æ€è®¡ç®—æä¾›äº†ç”µé˜»ã€ç”µå®¹ä¹‹å¤–çš„ç¬¬ä¸‰ä¸ªç»´åº¦ã€‚\n\n**12. Hands-on Evaluation of Visual Transformers for Object Recognition and Detection**\n**ç”¨äºç‰©ä½“è¯†åˆ«å’Œæ£€æµ‹çš„è§†è§‰ Transformer å®æµ‹è¯„ä¼°**\n- **æ ¸å¿ƒå‘ç°**ï¼šåœ¨åŒ»å­¦å›¾åƒï¼ˆèƒ¸éƒ¨ X å…‰ï¼‰ç­‰éœ€è¦å…¨å±€ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ä¸­ï¼Œæ··åˆå‹å’Œåˆ†å±‚å‹ Transformerï¼ˆå¦‚ Swin, CvTï¼‰è¡¨ç°ä¼˜äºä¼ ç»Ÿ CNNï¼Œè¯æ˜äº† ViT åœ¨æ•æ‰å…¨å±€å…³ç³»ä¸Šçš„ä¼˜åŠ¿ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–æœ‰è¶£çš„å·¥ä½œ\n\n*   **[Audio] VocSim (#9)**: æå‡ºäº†ä¸€ç§**æ— éœ€è®­ç»ƒ**çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°éŸ³é¢‘åµŒå…¥çš„é›¶æ ·æœ¬å†…å®¹ä¸€è‡´æ€§ï¼Œå‘ç°ç°æœ‰æ¨¡å‹åœ¨æœªè§è¿‡çš„è¯­éŸ³ç»“æ„ä¸Šæ³›åŒ–èƒ½åŠ›æ–­å´–å¼ä¸‹è·Œã€‚\n*   **[Video] Rethinking Chain-of-Thought Reasoning for Videos (#68)**: è§†é¢‘ç†è§£ä¸éœ€è¦åƒæ–‡æœ¬é‚£æ ·å†—é•¿çš„ CoTã€‚ç®€çŸ­çš„æ¨ç†ç—•è¿¹é…åˆå‹ç¼©çš„è§†è§‰ Token æ—¢é«˜æ•ˆåˆæœ‰æ•ˆã€‚\n*   **[Social] The Gender Code (#75)**: åˆ†æäº†å…¨çƒ AI æ²»ç†æ–‡ä»¶ï¼ŒæŒ‡å‡ºç›®å‰çš„æ”¿ç­–åœ¨æ€§åˆ«é—®é¢˜ä¸Šå­˜åœ¨ä¸ä¸€è‡´ï¼Œä¸”ç¼ºä¹äº¤å‰æ€§ï¼ˆIntersectionalityï¼‰è§†è§’ã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥èƒ½ä¸ºä½ çš„ç ”ç©¶å¸¦æ¥çµæ„Ÿï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2512.10159v1",
      "title": "Enhancing Large Language Models for End-to-End Circuit Analysis Problem Solving",
      "title_zh": "æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨ç«¯åˆ°ç«¯ç”µè·¯åˆ†æé—®é¢˜æ±‚è§£ä¸­çš„èƒ½åŠ›",
      "authors": [
        "Liangliang Chen",
        "Weiyu Sun",
        "Ying Zhang"
      ],
      "abstract": "Large language models (LLMs) have shown strong performance in data-rich domains such as programming, but their reliability in engineering tasks remains limited. Circuit analysis -- requiring multimodal understanding and precise mathematical reasoning -- highlights these challenges. Although Gemini 2.5 Pro improves diagram interpretation and analog-circuit reasoning, it still struggles to consistently produce correct solutions when given both text and circuit diagrams. At the same time, engineering education needs scalable AI tools capable of generating accurate solutions for tasks such as automated homework feedback and question-answering. This paper presents an enhanced, end-to-end circuit problem solver built on Gemini 2.5 Pro. We first benchmark Gemini on a representative set of undergraduate circuit problems and identify two major failure modes: 1) circuit-recognition hallucinations, particularly incorrect source polarity detection, and 2) reasoning-process hallucinations, such as incorrect current directions. To address recognition errors, we integrate a fine-tuned YOLO detector and OpenCV processing to isolate voltage and current sources, enabling Gemini to re-identify source polarities from cropped images with near-perfect accuracy. To reduce reasoning errors, we introduce an ngspice-based verification loop in which Gemini generates a .cir file, ngspice simulates the circuit, and discrepancies trigger iterative regeneration with optional human-in-the-loop review. Across 83 problems, the proposed pipeline achieves a 97.59% success rate (81 correct solutions), substantially outperforming Gemini 2.5 Pro's original 79.52% accuracy. This system extends LLM capabilities for multimodal engineering problem-solving and supports the creation of high-quality educational datasets and AI-powered instructional tools.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¢å¼ºçš„ç«¯åˆ°ç«¯ç”µè·¯åˆ†æé—®é¢˜æ±‚è§£å™¨ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å¤æ‚å·¥ç¨‹ä»»åŠ¡æ—¶çš„å¯é æ€§ã€‚é€šè¿‡åœ¨ç”µè·¯é—®é¢˜é›†ä¸Šå¯¹ Gemini 2.5 Pro è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶è€…è¯†åˆ«å‡ºç”µè·¯è¯†åˆ«å¹»è§‰ï¼ˆå¦‚ç”µæºææ€§æ£€æµ‹é”™è¯¯ï¼‰å’Œæ¨ç†è¿‡ç¨‹å¹»è§‰ï¼ˆå¦‚ç”µæµæ–¹å‘é”™è¯¯ï¼‰æ˜¯æ¨¡å‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¯†åˆ«é”™è¯¯ï¼Œç³»ç»Ÿé›†æˆäº†ç»è¿‡å¾®è°ƒçš„ YOLO æ£€æµ‹å™¨å’Œ OpenCV å¤„ç†æŠ€æœ¯ï¼Œé€šè¿‡å¯¹ç”µæºå›¾åƒè¿›è¡Œè£å‰ªå¹¶äºŒæ¬¡è¯†åˆ«ï¼Œå®ç°äº†è¿‘ä¹å®Œç¾çš„ææ€§æ£€æµ‹å‡†ç¡®ç‡ã€‚é’ˆå¯¹æ¨ç†è¿‡ç¨‹ä¸­çš„åå·®ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†åŸºäº ngspice çš„éªŒè¯é—­ç¯ï¼Œé€šè¿‡ç”Ÿæˆ .cir æ–‡ä»¶è¿›è¡Œä»¿çœŸæ¨¡æ‹Ÿå¹¶æ ¹æ®åå·®ç»“æœè§¦å‘è¿­ä»£ä¿®æ­£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥å·¥ä½œæµåœ¨ 83 ä¸ªå…¸å‹ç”µè·¯é—®é¢˜ä¸­è¾¾åˆ°äº† 97.59% çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºåŸå§‹åŸºå‡†æ¨¡å‹çš„ 79.52%ã€‚è¯¥ç³»ç»Ÿæœ‰æ•ˆæ‰©å±•äº† LLMs åœ¨å¤šæ¨¡æ€å·¥ç¨‹é—®é¢˜æ±‚è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶ä¸ºè‡ªåŠ¨åŒ–ä½œä¸šåé¦ˆåŠé«˜è´¨é‡æ•™è‚²æ•°æ®é›†çš„æ„å»ºæä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10159v1",
      "published_date": "2025-12-10 23:38:14 UTC",
      "updated_date": "2025-12-10 23:38:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:20:28.120559+00:00"
    },
    {
      "arxiv_id": "2512.17934v1",
      "title": "Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States",
      "title_zh": "United States å¿çº§è‚ºç™Œæ­»äº¡ç‡é¢„æµ‹ï¼šå¯è§£é‡Šæœºå™¨å­¦ä¹ ä¸çº¿æ€§å›å½’çš„æ¯”è¾ƒè¯„ä¼°",
      "authors": [
        "Soheil Hashtarkhani",
        "Brianna M. White",
        "Benyamin Hoseini",
        "David L. Schwartz",
        "Arash Shaban-Nejad"
      ],
      "abstract": "Lung cancer (LC) is a leading cause of cancer-related mortality in the United States. Accurate prediction of LC mortality rates is crucial for guiding targeted interventions and addressing health disparities. Although traditional regression-based models have been commonly used, explainable machine learning models may offer enhanced predictive accuracy and deeper insights into the factors influencing LC mortality. This study applied three models: random forest (RF), gradient boosting regression (GBR), and linear regression (LR) to predict county-level LC mortality rates across the United States. Model performance was evaluated using R-squared and root mean squared error (RMSE). Shapley Additive Explanations (SHAP) values were used to determine variable importance and their directional impact. Geographic disparities in LC mortality were analyzed through Getis-Ord (Gi*) hotspot analysis. The RF model outperformed both GBR and LR, achieving an R2 value of 41.9% and an RMSE of 12.8. SHAP analysis identified smoking rate as the most important predictor, followed by median home value and the percentage of the Hispanic ethnic population. Spatial analysis revealed significant clusters of elevated LC mortality in the mid-eastern counties of the United States. The RF model demonstrated superior predictive performance for LC mortality rates, emphasizing the critical roles of smoking prevalence, housing values, and the percentage of Hispanic ethnic population. These findings offer valuable actionable insights for designing targeted interventions, promoting screening, and addressing health disparities in regions most affected by LC in the United States.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”è¯„ä¼°äº†å¯è§£é‡Šæœºå™¨å­¦ä¹ æ¨¡å‹ä¸çº¿æ€§å›å½’(Linear Regression)åœ¨é¢„æµ‹ç¾å›½å¿çº§è‚ºç™Œ(Lung Cancer)æ­»äº¡ç‡æ–¹é¢çš„æ•ˆåŠ›ã€‚ç ”ç©¶é‡‡ç”¨äº†éšæœºæ£®æ—(Random Forest, RF)ã€æ¢¯åº¦æå‡å›å½’(Gradient Boosting Regression, GBR)å’Œçº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶ç»“åˆSHAP (Shapley Additive Explanations)å€¼ç¡®å®šå˜é‡é‡è¦æ€§ä»¥åŠGetis-Ord (Gi*)è¿›è¡Œç©ºé—´çƒ­ç‚¹åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšæœºæ£®æ—æ¨¡å‹(RF)åœ¨é¢„æµ‹æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå…¶R-squaredå€¼è¾¾åˆ°41.9%ï¼ŒRMSEä¸º12.8ã€‚SHAPåˆ†ææ­ç¤ºå¸çƒŸç‡(smoking rate)æ˜¯é¢„æµ‹æ­»äº¡ç‡æœ€å…³é”®çš„æŒ‡æ ‡ï¼Œå…¶æ¬¡æ˜¯æˆ¿å±‹ä»·å€¼ä¸­ä½æ•°(median home value)å’Œè¥¿ç­ç‰™è£”äººå£æ¯”ä¾‹(percentage of the Hispanic ethnic population)ã€‚ç©ºé—´åˆ†æåˆ™è¯†åˆ«å‡ºç¾å›½ä¸­ä¸œéƒ¨å¿åŸŸå­˜åœ¨æ˜¾è‘—çš„è‚ºç™Œæ­»äº¡ç‡é«˜å‘é›†ç¾¤ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†RFæ¨¡å‹çš„ä¼˜è¶Šé¢„æµ‹èƒ½åŠ›ï¼Œè¿˜é€šè¿‡è¯†åˆ«å…³é”®é£é™©å› ç´ å’Œåœ°ç†å·®å¼‚ï¼Œä¸ºåˆ¶å®šé’ˆå¯¹æ€§å¹²é¢„æªæ–½å’Œæ¶ˆé™¤å¥åº·å·®è·æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "9 Pages, 4 Figures, 1 Table",
      "pdf_url": "https://arxiv.org/pdf/2512.17934v1",
      "published_date": "2025-12-10 23:33:12 UTC",
      "updated_date": "2025-12-10 23:33:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:20:39.599411+00:00"
    },
    {
      "arxiv_id": "2512.14732v1",
      "title": "INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT",
      "title_zh": "INFORM-CTï¼šé›†æˆå¤§è¯­è¨€æ¨¡å‹ä¸è§†è§‰è¯­è¨€æ¨¡å‹çš„è…¹éƒ¨ CT å¶ç„¶å‘ç°ç®¡ç†",
      "authors": [
        "Idan Tankel",
        "Nir Mazor",
        "Rafi Brada",
        "Christina LeBedis",
        "Guy ben-Yosef"
      ],
      "abstract": "Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.\n  We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†INFORM-CTï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è‡ªåŠ¨åŒ–ç®¡ç†è…¹éƒ¨CTæ‰«æä¸­å¶ç„¶å‘ç°(Incidental findings)çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹(LLMs)å’ŒåŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ï¼Œé‡‡ç”¨ä¸€ç§â€œè®¡åˆ’ä¸æ‰§è¡Œâ€(plan-and-execute)çš„æ™ºèƒ½ä½“æ–¹æ³•ï¼Œä»¥æé«˜å¶ç„¶å‘ç°æ£€æµ‹ã€åˆ†ç±»å’ŒæŠ¥å‘Šçš„æ•ˆç‡ä¸ç²¾åº¦ã€‚åœ¨è¿™ä¸€æ¶æ„ä¸­ï¼ŒåŸºäºLLMçš„è®¡åˆ’å™¨è´Ÿè´£æ ¹æ®åŒ»å­¦æŒ‡å—ç”ŸæˆPythonè„šæœ¬ï¼Œè€Œæ‰§è¡Œå™¨åˆ™è¿è¡Œè¿™äº›è„šæœ¬ï¼Œé€šè¿‡è°ƒç”¨VLMsã€åˆ†å‰²æ¨¡å‹å’Œå›¾åƒå¤„ç†ç¨‹åºå®Œæˆç‰¹å®šä»»åŠ¡ã€‚å®éªŒåœ¨åŒ…å«ä¸‰ä¸ªå™¨å®˜çš„è…¹éƒ¨CTåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨è‡ªåŠ¨ç«¯åˆ°ç«¯æµ‹è¯•ã€‚ç»“æœè¯æ˜ï¼ŒINFORM-CTåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰çš„çº¯VLMæ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºä¸´åºŠå½±åƒå­¦ä¸­å¶ç„¶å‘ç°çš„ç²¾ç¡®ä¸”æ ‡å‡†åŒ–çš„è‡ªåŠ¨åŒ–ç®¡ç†æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14732v1",
      "published_date": "2025-12-10 23:28:26 UTC",
      "updated_date": "2025-12-10 23:28:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:20:34.167164+00:00"
    },
    {
      "arxiv_id": "2512.10150v1",
      "title": "Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning",
      "title_zh": "ä¸è¢«é—å¿˜çš„å®‰å…¨ï¼šåˆ©ç”¨æŒç»­å­¦ä¹ ç»´æŒå¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨å¯¹é½",
      "authors": [
        "Lama Alssum",
        "Hani Itani",
        "Hasan Abed Al Kader Hammoud",
        "Philip Torr",
        "Adel Bibi",
        "Bernard Ghanem"
      ],
      "abstract": "The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶é¢ä¸´çš„å®‰å…¨æ€§é€€åŒ–é—®é¢˜ï¼Œå¹¶å°†å…¶å½’å› äºç¾éš¾æ€§é—å¿˜ï¼ˆCatastrophic Forgettingï¼‰ã€‚ä½œè€…å°†å¾®è°ƒè¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§ä¿æŠ¤å»ºæ¨¡ä¸ºæŒç»­å­¦ä¹ ï¼ˆContinual Learning, CLï¼‰é—®é¢˜ï¼Œå¹¶åœ¨â€œå¾®è°ƒå³æœåŠ¡â€çš„åœºæ™¯ä¸‹ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†åŒ…æ‹¬åŸºäºæ­£åˆ™åŒ–ã€å†…å­˜å’Œæ¨¡å‹åˆå¹¶åœ¨å†…çš„å¤šç§ CL æ–¹æ³•ã€‚ç ”ç©¶è€ƒè™‘äº†è‰¯æ€§ç”¨æˆ·æ•°æ®å’Œè¢«æŠ•æ¯’ï¼ˆPoisonedï¼‰ç”¨æˆ·æ•°æ®ä¸¤ç§å®éªŒåœºæ™¯ï¼Œç»“æœæ˜¾ç¤ºæŒç»­å­¦ä¹ æ–¹æ³•èƒ½ä¸€è‡´åœ°é™ä½æ”»å‡»æˆåŠŸç‡ï¼ˆAttack Success Ratesï¼‰ã€‚åœ¨æ‰€æœ‰æµ‹è¯•æ–¹æ³•ä¸­ï¼ŒDER åœ¨ä¿æŒæ¨¡å‹ä»»åŠ¡æ•ˆç”¨çš„åŒæ—¶ï¼Œå…¶å®‰å…¨æ€§ä¿æŠ¤æ•ˆæœä¼˜äºå…¶ä»– CL æ–¹æ³•åŠç°æœ‰çš„åŸºå‡†æ¨¡å‹ã€‚è¯¥ç»“è®ºåœ¨ GSM8Kã€SST2ã€Code ç­‰å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡ï¼Œä»¥åŠ LLaMA2-7Bã€Mistral-7B å’Œ Gemma-2B ç­‰å¤šä¸ªæ¨¡å‹ç³»åˆ—ä¸­å¾—åˆ°äº†å¹¿æ³›éªŒè¯ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶ç¡®ç«‹äº†æŒç»­å­¦ä¹ ä½œä¸ºç»´æŒ LLMs å®‰å…¨å¯¹é½ï¼ˆSafety Alignmentï¼‰çš„ä¸€ç§å®ç”¨ä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10150v1",
      "published_date": "2025-12-10 23:16:47 UTC",
      "updated_date": "2025-12-10 23:16:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:20:36.917555+00:00"
    },
    {
      "arxiv_id": "2512.10148v1",
      "title": "PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset",
      "title_zh": "PARANï¼šåŸºäºå¤–å–è¯„è®ºæ•°æ®é›†çš„ç”¨æˆ·ç”»åƒå¢å¼ºè¯„è®ºå›å¤ç³»ç»Ÿ",
      "authors": [
        "Moonsoo Park",
        "Jeongseok Yun",
        "Bohyung Kim"
      ],
      "abstract": "Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PARANï¼Œä¸€ç§ Persona-Augmented Review ANswering ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¤–å–å¹³å°å› ç”¨æˆ·ä¿¡æ¯ç¨€ç¼ºå¯¼è‡´ç”Ÿæˆå›å¤è¿‡äºæ³›åŒ–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæç¤ºç­–ç•¥ (two-stage prompting framework)ï¼Œç›´æ¥ä»ç®€çŸ­è¯„è®ºä¸­æ¨æ–­ç”¨æˆ·æ˜¾å¼ï¼ˆå¦‚åå¥½ï¼‰å’Œéšå¼ï¼ˆå¦‚äººå£ç»Ÿè®¡æˆ–é£æ ¼çº¿ç´¢ï¼‰çš„äººç‰©ç”»åƒ (Persona)ã€‚è¿™äº›æ¨æ–­å‡ºçš„ç”»åƒå±æ€§è¢«æ•´åˆåˆ°æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¹¶é€šè¿‡è°ƒæ•´è§£ç æ¸©åº¦ (decoding temperature) æ¥ç¡®ä¿ç”Ÿæˆå›å¤çš„å¤šæ ·æ€§ä¸å¿ å®åº¦ã€‚é€šè¿‡å¯¹éŸ©å›½å¤–å–åº”ç”¨çš„çœŸå®æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶é‡ç‚¹è€ƒå¯Ÿäº†ç³»ç»Ÿåœ¨ç²¾ç¡®åº¦ã€å¤šæ ·æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒPersona-Augmented æç¤ºæŠ€æœ¯åœ¨æ— éœ€æ¨¡å‹å¾®è°ƒ (fine-tuning) çš„å‰æä¸‹ï¼Œèƒ½æ˜¾è‘—å¢å¼ºè‡ªåŠ¨åŒ–å›å¤çš„ç›¸å…³æ€§ä¸ä¸ªæ€§åŒ–æ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10148v1",
      "published_date": "2025-12-10 23:04:48 UTC",
      "updated_date": "2025-12-10 23:04:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:20:46.701220+00:00"
    },
    {
      "arxiv_id": "2512.11002v1",
      "title": "Beyond Memristor: Neuromorphic Computing Using Meminductor",
      "title_zh": "è¶…è¶Šå¿†é˜»å™¨ï¼šåŸºäºå¿†æ„Ÿå™¨çš„ç¥ç»å½¢æ€è®¡ç®—",
      "authors": [
        "Frank Zhigang Wang"
      ],
      "abstract": "Memristor (resistor with memory), inductor with memory (meminductor) and capacitor with memory (memcapacitor) have different roles to play in novel computing architectures. We found that a coil with a magnetic core is an inductor with memory (meminductor) in terms of its inductance L(q) being a function of the charge q. The history of the current passing through the coil is remembered by the magnetization inside the magnetic core. Such a meminductor can play a unique role (that cannot be played by a memristor) in neuromorphic computing, deep learning and brain inspired since the time constant of a neuromorphic RLC circuit is jointly determined by the inductance and capacitance, rather than the resistance. As an experimental verification, this newly invented meminductor was used to reproduce the observed biological behaviour of amoebae (the memorizing, timing and anticipating mechanisms). In conclusion, a beyond memristor computing paradigm is theoretically sensible and experimentally practical.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¿†æ„Ÿå™¨(Meminductor)åœ¨ç¥ç»å½¢æ€è®¡ç®—ä¸­çš„ç‹¬ç‰¹ä»·å€¼ï¼ŒæŒ‡å‡ºå…¶ä½œä¸ºå…·æœ‰è®°å¿†åŠŸèƒ½çš„ç”µæ„Ÿå™¨åœ¨æ–°å‹è®¡ç®—æ¶æ„ä¸­æ‰®æ¼”ç€ä¸å¯æ›¿ä»£çš„è§’è‰²ã€‚ç ”ç©¶äººå‘˜å‘ç°å¸¦æœ‰ç£èŠ¯çš„çº¿åœˆå®è´¨ä¸Šæ˜¯ä¸€ç§å¿†æ„Ÿå™¨ï¼Œå…¶ç”µæ„Ÿ L(q) æ˜¯ç”µè· q çš„å‡½æ•°ï¼Œèƒ½å¤Ÿåˆ©ç”¨ç£èŠ¯å†…éƒ¨çš„ç£åŒ–çŠ¶æ€è®°å½•ç”µæµå†å²ã€‚ç”±äºç¥ç»å½¢æ€ RLC ç”µè·¯çš„æ—¶é—´å¸¸æ•°ç”±ç”µæ„Ÿå’Œç”µå®¹å…±åŒå†³å®šè€Œéç”µé˜»ï¼Œå¿†æ„Ÿå™¨åœ¨æ·±åº¦å­¦ä¹ å’Œç±»è„‘è®¡ç®—ä¸­å±•ç°å‡ºå¿†é˜»å™¨(Memristor)æ— æ³•å®ç°çš„ç‹¬ç‰¹åŠŸèƒ½ã€‚å®éªŒé€šè¿‡è¯¥æ–°å‹å¿†æ„Ÿå™¨æˆåŠŸé‡ç°äº†å˜å½¢è™«(amoebae)çš„è®°å¿†ã€è®¡æ—¶å’Œé¢„æµ‹ç­‰ç”Ÿç‰©è¡Œä¸ºï¼Œä»å®éªŒå±‚é¢éªŒè¯äº†å™¨ä»¶çš„å®ç”¨æ€§ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è¶…è¶Šå¿†é˜»å™¨çš„è®¡ç®—èŒƒå¼åœ¨ç†è®ºä¸Šæ˜¯åˆç†çš„ï¼Œåœ¨å®éªŒä¸Šä¹Ÿæ˜¯åˆ‡å®å¯è¡Œçš„ï¼Œä¸ºæœªæ¥ç¥ç»å½¢æ€ç¡¬ä»¶çš„å‘å±•æä¾›äº†æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.ET",
        "cs.AI"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.11002v1",
      "published_date": "2025-12-10 22:45:27 UTC",
      "updated_date": "2025-12-10 22:45:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:20:39.312533+00:00"
    },
    {
      "arxiv_id": "2512.10132v1",
      "title": "Universal Hirschberg for Width Bounded Dynamic Programs",
      "title_zh": "å®½åº¦å—é™åŠ¨æ€è§„åˆ’çš„é€šç”¨ Hirschberg ç®—æ³•",
      "authors": [
        "Logan Nye"
      ],
      "abstract": "Hirschberg's algorithm (1975) reduces the space complexity for the longest common subsequence problem from $O(N^2)$ to $O(N)$ via recursive midpoint bisection on a grid dynamic program (DP). We show that the underlying idea generalizes to a broad class of dynamic programs with local dependencies on directed acyclic graphs (DP DAGs). Modeling a DP as deterministic time evolution over a topologically ordered DAG with frontier width $Ï‰$ and bounded in-degree, and assuming a max-type semiring with deterministic tie breaking, we prove that in a standard offline random-access model any such DP admits deterministic traceback in space $O(Ï‰\\log T + (\\log T)^{O(1)})$ cells over a fixed finite alphabet, where $T$ is the number of states. Our construction replaces backward dynamic programs by forward-only recomputation and organizes the time order into a height-compressed recursion tree whose nodes expose small \"middle frontiers'' across which every optimal path must pass. The framework yields near-optimal traceback bounds for asymmetric and banded sequence alignment, one-dimensional recurrences, and dynamic-programming formulations on graphs of bounded pathwidth. We also show that an $Î©(Ï‰)$ space term (in bits) is unavoidable in forward single-pass models and discuss conjectured $\\sqrt{T}$-type barriers in streaming settings, supporting the view that space-efficient traceback is a structural property of width-bounded DP DAGs rather than a peculiarity of grid-based algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†ä¼ ç»Ÿçš„ Hirschberg ç®—æ³•æ€æƒ³æ¨å¹¿åˆ°äº†å…·æœ‰å±€éƒ¨ä¾èµ–æ€§çš„æœ‰å‘æ— ç¯å›¾åŠ¨æ€è§„åˆ’ (DP DAGs) é¢†åŸŸï¼Œè§£å†³äº†åœ¨æ›´å¹¿æ³›è®¡ç®—ç»“æ„ä¸­ç©ºé—´æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚é€šè¿‡å°† DP å»ºæ¨¡ä¸ºåœ¨è¾¹ç•Œå®½åº¦ (frontier width) $\\omega$ çš„æ‹“æ‰‘æ’åº DAG ä¸Šçš„æ¼”åŒ–ï¼Œè®ºæ–‡è¯æ˜äº†ä»»ä½•æ­¤ç±» DP å‡èƒ½ä»¥ $O(\\omega \\log T + (\\log T)^{O(1)})$ çš„ç©ºé—´å¤æ‚åº¦å®ç°ç¡®å®šæ€§å›æº¯ (traceback)ï¼Œå…¶ä¸­ $T$ ä¸ºçŠ¶æ€æ•°ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä»…å‰å‘é‡è®¡ç®— (forward-only recomputation) æ›¿ä»£äº†ä¼ ç»Ÿçš„åå‘ DP è¿‡ç¨‹ï¼Œå¹¶æ„å»ºé«˜åº¦å‹ç¼©çš„é€’å½’æ ‘æ¥è¯†åˆ«æœ€ä¼˜è·¯å¾„å¿…é¡»é€šè¿‡çš„â€œä¸­é—´è¾¹ç•Œâ€ã€‚è¿™ç§é€šç”¨æ¡†æ¶ä¸ºéå¯¹ç§°å’Œå¸¦çŠ¶åºåˆ—æ¯”å¯¹ (sequence alignment)ã€ä¸€ç»´é€’æ¨ä»¥åŠè¾¹ç•Œè·¯å¾„å®½åº¦å›¾ä¸Šçš„åŠ¨æ€è§„åˆ’æä¾›äº†è¿‘ä¹æœ€ä¼˜çš„å›æº¯è¾¹ç•Œã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç©ºé—´é«˜æ•ˆçš„å›æº¯æ˜¯å®½åº¦å—é™ DP DAGs çš„ä¸€ç§å›ºæœ‰ç»“æ„å±æ€§ï¼Œè€Œéç‰¹å®šç½‘æ ¼ç®—æ³•çš„ç‰¹ä¾‹ï¼Œå¹¶æ¢è®¨äº†åœ¨æµå¼è®¾ç½®ä¸‹çš„è®¡ç®—åŠ¿å’ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "31 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.10132v1",
      "published_date": "2025-12-10 22:26:22 UTC",
      "updated_date": "2025-12-10 22:26:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:20:56.234305+00:00"
    },
    {
      "arxiv_id": "2512.10121v1",
      "title": "Workflow is All You Need: Escaping the \"Statistical Smoothing Trap\" via High-Entropy Information Foraging and Adversarial Pacing",
      "title_zh": "Workflow is All You Needï¼šé€šè¿‡é«˜ç†µä¿¡æ¯è§…é£Ÿä¸å¯¹æŠ—æ€§æ­¥è°ƒæ‘†è„±â€œç»Ÿè®¡å¹³æ»‘é™·é˜±â€",
      "authors": [
        "Zhongjie Jiang"
      ],
      "abstract": "Central to long-form text generation in vertical domains is the \"impossible trinity\" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å‚ç›´é¢†åŸŸé•¿æ–‡æœ¬ç”Ÿæˆä¸­é¢ä¸´çš„ä½å¹»è§‰ã€é€»è¾‘è¿è´¯æ€§å’Œä¸ªæ€§åŒ–è¡¨è¾¾çš„â€œä¸å¯èƒ½ä¸‰è§’â€é—®é¢˜ï¼ŒæŒ‡å‡ºå…¶æ ¸å¿ƒç“¶é¢ˆåœ¨äºç°æœ‰ç”ŸæˆèŒƒå¼æ˜“é™·å…¥Statistical Smoothing Trapã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†DeepNews Frameworkï¼Œè¿™æ˜¯ä¸€ç§æ¨¡æ‹Ÿèµ„æ·±è´¢ç»è®°è€…è®¤çŸ¥è¿‡ç¨‹çš„æ™ºèƒ½ä½“å·¥ä½œæµï¼ŒåŒ…å«åŸºäºinformation foraging theoryçš„åŒç²’åº¦æ£€ç´¢ã€åŸºäºnarrative schemasä¸Atomic Blocksçš„å›¾å¼å¼•å¯¼ç­–ç•¥è§„åˆ’ï¼Œä»¥åŠæ—¨åœ¨æ‰“ç ´æ¨¡å‹æ¦‚ç‡å¹³æ»‘æ€§çš„å¯¹æŠ—æ€§çº¦æŸæç¤ºæŠ€æœ¯ï¼ˆå¦‚Rhythm Breakå’ŒLogic Fogï¼‰ã€‚å®éªŒæ­ç¤ºäº†æ·±åº¦è´¢ç»æŠ¥é“ä¸­çš„Knowledge Cliffç°è±¡ï¼Œå³æ£€ç´¢èƒŒæ™¯å°‘äº1.5ä¸‡å­—ç¬¦æ—¶å†…å®¹çœŸå®æ€§ä¼šå´©æºƒï¼Œè€Œè¶…è¿‡3ä¸‡å­—ç¬¦çš„é«˜å†—ä½™è¾“å…¥èƒ½å°†Hallucination-Free Rate (HFR)ç¨³å®šåœ¨85%ä»¥ä¸Šã€‚åœ¨é¡¶çº§ç§‘æŠ€åª’ä½“çš„ç›²æµ‹ä¸­ï¼Œè¯¥ç³»ç»Ÿå®ç°äº†25%çš„æŠ•ç¨¿é‡‡çº³ç‡ï¼Œæ˜¾è‘—ä¼˜äºSOTAæ¨¡å‹GPT-5é›¶æ ·æœ¬ç”Ÿæˆçš„0%é‡‡çº³ç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "q-fin.GN"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 8 figures. Includes an ecological validity blind test where the Agentic Workflow achieved a 25% acceptance rate in top-tier media, decisively outperforming the SOTA Zero-shot baseline (0%). Features the DNFO-v5 ontology",
      "pdf_url": "https://arxiv.org/pdf/2512.10121v1",
      "published_date": "2025-12-10 22:13:55 UTC",
      "updated_date": "2025-12-10 22:13:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:20:50.628874+00:00"
    },
    {
      "arxiv_id": "2512.10120v1",
      "title": "VocSim: A Training-free Benchmark for Zero-shot Content Identity in Single-source Audio",
      "title_zh": "VocSimï¼šé¢å‘å•æºéŸ³é¢‘é›¶æ ·æœ¬å†…å®¹è¯†åˆ«çš„æ— éœ€è®­ç»ƒåŸºå‡†",
      "authors": [
        "Maris Basha",
        "Anja Zai",
        "Sabine Stoll",
        "Richard Hahnloser"
      ],
      "abstract": "General-purpose audio representations aim to map acoustically variable instances of the same event to nearby points, resolving content identity in a zero-shot setting. Unlike supervised classification benchmarks that measure adaptability via parameter updates, we introduce VocSim, a training-free benchmark probing the intrinsic geometric alignment of frozen embeddings. VocSim aggregates 125k single-source clips from 19 corpora spanning human speech, animal vocalizations, and environmental sounds. By restricting to single-source audio, we isolate content representation from the confound of source separation. We evaluate embeddings using Precision@k for local purity and the Global Separation Rate (GSR) for point-wise class separation. To calibrate GSR, we report lift over an empirical permutation baseline. Across diverse foundation models, a simple pipeline, frozen Whisper encoder features, time-frequency pooling, and label-free PCA, yields strong zero-shot performance. However, VocSim also uncovers a consistent generalization gap. On blind, low-resource speech, local retrieval drops sharply. While performance remains statistically distinguishable from chance, the absolute geometric structure collapses, indicating a failure to generalize to unseen phonotactics. As external validation, our top embeddings predict avian perceptual similarity, improve bioacoustic classification, and achieve state-of-the-art results on the HEAR benchmark. We posit that the intrinsic geometric quality measured here proxies utility in unlisted downstream applications. We release data, code, and a public leaderboard to standardize the evaluation of intrinsic audio geometry.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† VocSimï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„åŸºå‡†æµ‹è¯• (training-free benchmark)ï¼Œæ—¨åœ¨æ¢æµ‹å†»ç»“åµŒå…¥ (frozen embeddings) åœ¨å•æºéŸ³é¢‘ä¸­çš„å†…åœ¨å‡ ä½•å¯¹é½å’Œå†…å®¹ä¸€è‡´æ€§ã€‚è¯¥åŸºå‡†é›†æˆäº†æ¥è‡ª 19 ä¸ªè¯­æ–™åº“çš„ 12.5 ä¸‡ä¸ªå‰ªè¾‘ï¼Œæ¶µç›–äº†äººç±»è¯­éŸ³ã€åŠ¨ç‰©é¸£å«å’Œç¯å¢ƒéŸ³ï¼Œé€šè¿‡ä¸“æ³¨äºå•æºéŸ³é¢‘æ¥éš”ç¦»å†…å®¹è¡¨ç¤ºä¸å£°æºåˆ†ç¦»çš„å¹²æ‰°ã€‚ç ”ç©¶é‡‡ç”¨ Precision@k å’Œ Global Separation Rate (GSR) è¯„ä¼°æ€§èƒ½ï¼Œå‘ç°åœ¨å†»ç»“çš„ Whisper ç¼–ç å™¨ç‰¹å¾ä¸Šåº”ç”¨ç®€å•çš„æ± åŒ–å’Œ PCA å³å¯è·å¾—å¼ºå¤§çš„é›¶æ ·æœ¬ (zero-shot) è¡¨ç°ã€‚ç„¶è€Œ VocSim ä¹Ÿæ­ç¤ºäº†æ¨¡å‹åœ¨ä½èµ„æºè¯­éŸ³ä¸­å­˜åœ¨æ˜æ˜¾çš„æ³›åŒ–å·®è·ï¼Œå…¶å‡ ä½•ç»“æ„åœ¨å¤„ç†æœªè§è¿‡çš„éŸ³ä½ç»“æ„æ—¶ä¼šå‡ºç°å´©æºƒã€‚å®éªŒè¯æ˜è¯¥åŸºå‡†æµ‹é‡çš„å‡ ä½•è´¨é‡ä¸ä¸‹æ¸¸ä»»åŠ¡æ•ˆç”¨é«˜åº¦ç›¸å…³ï¼Œå…¶é¡¶å°–åµŒå…¥åœ¨ HEAR åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº† SOTA ç»“æœï¼Œå¹¶èƒ½æœ‰æ•ˆé¢„æµ‹é¸Ÿç±»æ„ŸçŸ¥ç›¸ä¼¼åº¦åŠæå‡ç”Ÿç‰©å£°å­¦åˆ†ç±»æ€§èƒ½ã€‚ä½œè€…é€šè¿‡å‘å¸ƒæ•°æ®ã€ä»£ç å’Œæ’è¡Œæ¦œï¼Œä¸ºæ ‡å‡†åŒ–è¯„ä¼°é€šç”¨éŸ³é¢‘è¡¨ç¤ºçš„å†…åœ¨å‡ ä½•ç‰¹æ€§æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10120v1",
      "published_date": "2025-12-10 22:13:12 UTC",
      "updated_date": "2025-12-10 22:13:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:20:56.508743+00:00"
    },
    {
      "arxiv_id": "2512.10117v1",
      "title": "CHyLL: Learning Continuous Neural Representations of Hybrid Systems",
      "title_zh": "CHyLLï¼šå­¦ä¹ æ··åˆç³»ç»Ÿçš„è¿ç»­ç¥ç»è¡¨ç¤º",
      "authors": [
        "Sangli Teng",
        "Hang Liu",
        "Jingyu Song",
        "Koushil Sreenath"
      ],
      "abstract": "Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CHyLLï¼ˆContinuous Hybrid System Learning in Latent Spaceï¼‰ï¼Œæ—¨åœ¨è§£å†³å…·æœ‰è¿ç»­å’Œç¦»æ•£åŠ¨åŠ›å­¦ç‰¹æ€§çš„æ··åˆç³»ç»Ÿï¼ˆHybrid Systemsï¼‰æµå­¦ä¹ ä¸­é¢ä¸´çš„æ¨¡æ€åˆ‡æ¢å’Œæµä¸è¿ç»­æ€§æŒ‘æˆ˜ã€‚ä¸ä¼ ç»Ÿåˆ†æ¨¡æ€å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒCHyLLæ— éœ€è½¨è¿¹åˆ†å‰²ã€äº‹ä»¶å‡½æ•°æˆ–æ˜¾å¼æ¨¡æ€åˆ‡æ¢ï¼Œå…¶æ ¸å¿ƒè§è§£åœ¨äºåˆ©ç”¨é‡ç½®æ˜ å°„ï¼ˆReset Mapï¼‰åœ¨å®ˆå«è¡¨é¢ï¼ˆGuard Surfaceï¼‰ç²˜åˆçŠ¶æ€ç©ºé—´ã€‚é€šè¿‡å°†çŠ¶æ€ç©ºé—´é‡æ–°è¡¨è¿°ä¸ºåˆ†æ®µå…‰æ»‘çš„å•†æµå½¢ï¼ˆQuotient Manifoldï¼‰ï¼Œè¯¥æ¡†æ¶ä½¿æµåœ¨ç©ºé—´ä¸Šå…·æœ‰è¿ç»­æ€§ã€‚åŸºäºå¾®åˆ†æ‹“æ‰‘çš„åµŒå…¥å®šç†ï¼ŒCHyLLåœ¨æ›´é«˜ç»´ç©ºé—´ä¸­åŒæ—¶å­¦ä¹ æ— å¥‡å¼‚æ€§çš„ç¥ç»åµŒå…¥ï¼ˆNeural Embeddingï¼‰åŠå…¶è¿ç»­æµã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥å“è¶Šçš„ç²¾åº¦é¢„æµ‹æ··åˆç³»ç»Ÿçš„æµï¼Œå¹¶æœ‰æ•ˆè¯†åˆ«ç³»ç»Ÿçš„æ‹“æ‰‘ä¸å˜é‡ï¼ˆTopological Invariantsï¼‰ã€‚æœ€åï¼Œç ”ç©¶è€…å°†CHyLLåº”ç”¨äºéšæœºæœ€ä¼˜æ§åˆ¶ï¼ˆStochastic Optimal Controlï¼‰é—®é¢˜ï¼ŒéªŒè¯äº†è¯¥æ¨¡å‹åœ¨å¤„ç†å¤æ‚åŠ¨åŠ›å­¦ä»»åŠ¡æ—¶çš„ä¼˜è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "eess.SP",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10117v1",
      "published_date": "2025-12-10 22:07:16 UTC",
      "updated_date": "2025-12-10 22:07:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:20:55.501702+00:00"
    },
    {
      "arxiv_id": "2512.10114v1",
      "title": "AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice",
      "title_zh": "AgriRegionï¼šé¢å‘é«˜ä¿çœŸå†œä¸šå»ºè®®çš„åŒºåŸŸæ„ŸçŸ¥æ£€ç´¢",
      "authors": [
        "Mesafint Fanuel",
        "Mahmoud Nabil Mahmoud",
        "Crystal Cook Marshal",
        "Vishal Lakhotia",
        "Biswanath Dari",
        "Kaushik Roy",
        "Shaohu Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å†œä¸šé¢†åŸŸå®¹æ˜“äº§ç”Ÿè¯­å¢ƒå¹»è§‰ï¼ˆcontextual hallucinationï¼‰ï¼Œå³æä¾›çš„å»ºè®®åœ¨ä¸åŒåœ°åŒºå› åœŸå£¤ã€æ°”å€™å’Œæ³•è§„å·®å¼‚è€Œå¤±æ•ˆçš„é—®é¢˜ï¼Œæå‡ºäº†AgriRegionæ¡†æ¶ã€‚AgriRegionæ˜¯ä¸€ç§ä¸“é—¨ä¸ºé«˜ä¿çœŸã€åŒºåŸŸæ„ŸçŸ¥çš„å†œä¸šå’¨è¯¢è®¾è®¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ã€‚ä¸ä»…ä¾èµ–è¯­ä¹‰ç›¸ä¼¼æ€§çš„æ ‡å‡†æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åœ°ç†ç©ºé—´å…ƒæ•°æ®æ³¨å…¥å±‚ï¼ˆgeospatial metadata injection layerï¼‰å’ŒåŒºåŸŸä¼˜å…ˆçº§é‡æ’åºæœºåˆ¶ï¼ˆregion-prioritized re-ranking mechanismï¼‰ã€‚é€šè¿‡å°†çŸ¥è¯†åº“é™åˆ¶åœ¨ç»è¿‡éªŒè¯çš„å½“åœ°å†œä¸šæ¨å¹¿æœåŠ¡ï¼Œå¹¶åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­å¼ºåˆ¶æ‰§è¡Œåœ°ç†ç©ºé—´çº¦æŸï¼ŒAgriRegionç¡®ä¿äº†ç§æ¤è®¡åˆ’ã€ç—…è™«å®³é˜²æ²»å’Œæ–½è‚¥å»ºè®®çš„å±€éƒ¨å‡†ç¡®æ€§ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜åˆ›å»ºäº†åŒ…å«12ä¸ªå†œä¸šå­é¢†åŸŸçš„AgriRegion-EvalåŸºå‡†æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgriRegionç›¸æ¯”æœ€å…ˆè¿›çš„LLMsç³»ç»Ÿå‡å°‘äº†10-20%çš„å¹»è§‰ç°è±¡ï¼Œå¹¶æ˜¾è‘—æå‡äº†è¯„ä¼°ä¸­çš„ä¿¡ä»»è¯„åˆ†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.10114v1",
      "published_date": "2025-12-10 22:06:41 UTC",
      "updated_date": "2025-12-10 22:06:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:21:10.800951+00:00"
    },
    {
      "arxiv_id": "2512.11909v1",
      "title": "Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets",
      "title_zh": "å› æœå¼ºåº¦ä¸æ¼æŸä¿¡å¿µï¼šåŸºäº Noisy-OR å› æœè´å¶æ–¯ç½‘ç»œçš„ LLM æ¨ç†è§£æ",
      "authors": [
        "Hanna Dettki"
      ],
      "abstract": "The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \\emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?\n  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\\!\\to\\!E\\!\\leftarrow\\!C_2$ ) under \\emph{Direct} (one-shot number as response = probability judgment of query node being one and \\emph{Chain of Thought} (CoT; think first, then provide answer).\n  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $Î¸=(b,m_1,m_2,p(C)) \\in [0,1]$ include a shared prior $p(C)$;\n  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\\neq}m_2$) variant.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åœ¨å› æœæ¨ç†ï¼ˆcausal reasoningï¼‰èƒ½åŠ›ä¸Šçš„å¼‚åŒï¼Œå¹¶æå‡ºäº†é€šè¿‡æœ‰åè§ï¼ˆleakyï¼‰çš„ noisy-OR å› æœè´å¶æ–¯ç½‘ç»œï¼ˆCBNï¼‰æ¥è§£é‡Šå…¶æ¨ç†æœºåˆ¶ã€‚ç ”ç©¶è€…åœ¨ç”±å¯¹æ’ç»“æ„ï¼ˆcollider graphï¼‰å®šä¹‰çš„ 11 é¡¹å› æœä»»åŠ¡ä¸Šè¯„ä¼°äº† 20 å¤šç§ LLMsï¼Œå®éªŒæ¶µç›–äº†ç›´æ¥å›ç­”ï¼ˆDirectï¼‰å’Œé“¾å¼æ€ç»´ï¼ˆChain of Thought, CoTï¼‰ä¸¤ç§æ¨ç†æ¨¡å¼ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¨¡å‹å‚æ•°ï¼ˆåŒ…æ‹¬å…ˆéªŒæ¦‚ç‡ $p(C)$ã€å› æœå¼ºåº¦ $m$ å’Œæ¼é¡¹ $b$ï¼‰çš„æ‹Ÿåˆï¼Œå¹¶åˆ©ç”¨èµ¤æ± ä¿¡æ¯é‡å‡†åˆ™ï¼ˆAICï¼‰è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼Œæ·±å…¥åˆ†æäº† LLMs æ˜¯å¦ä¸äººç±»è¡¨ç°å¯¹é½ä»¥åŠå®ƒä»¬æ˜¯å¦å…·æœ‰ç‹¬ç‰¹çš„æ¨ç†ç‰¹å¾ã€‚è¯¥æ–¹æ³•ä¸ºé‡åŒ–è¯„ä¼° LLMs çš„å› æœè®¤çŸ¥æ°´å¹³æä¾›äº†ç³»ç»Ÿæ€§æ¡†æ¶ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚é€»è¾‘ä»»åŠ¡æ—¶çš„å†…åœ¨ä¸€è‡´æ€§ä¸ç‰¹å®šåå·®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.11909v1",
      "published_date": "2025-12-10 21:58:16 UTC",
      "updated_date": "2025-12-10 21:58:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:21:22.936365+00:00"
    },
    {
      "arxiv_id": "2512.10105v3",
      "title": "Belief Is All You Need: Modeling Narrative Archetypes in Conspiratorial Discourse",
      "title_zh": "ä¿¡å¿µå³ä¸€åˆ‡ï¼šé˜´è°‹è®ºè¯è¯­ä¸­çš„å™äº‹åŸå‹å»ºæ¨¡",
      "authors": [
        "Soorya Ram Shimgekar",
        "Abhay Goyal",
        "Roy Ka-Wei Lee",
        "Koustuv Saha",
        "Pi Zonooz",
        "Navin Kumar"
      ],
      "abstract": "Conspiratorial discourse is increasingly embedded within digital communication ecosystems, yet its structure and spread remain difficult to study. This work analyzes conspiratorial narratives in Singapore-based Telegram groups, showing that such content is woven into everyday discussions rather than confined to isolated echo chambers. We propose a two-stage computational framework. First, we fine-tune RoBERTa-large to classify messages as conspiratorial or not, achieving an F1-score of 0.866 on 2,000 expert-labeled messages. Second, we build a signed belief graph in which nodes represent messages and edge signs reflect alignment in belief labels, weighted by textual similarity. We introduce a Signed Belief Graph Neural Network (SiBeGNN) that uses a Sign Disentanglement Loss to learn embeddings that separate ideological alignment from stylistic features.\n  Using hierarchical clustering on these embeddings, we identify seven narrative archetypes across 553,648 messages: legal topics, medical concerns, media discussions, finance, contradictions in authority, group moderation, and general chat. SiBeGNN yields stronger clustering quality (cDBI = 8.38) than baseline methods (13.60 to 67.27), supported by 88 percent inter-rater agreement in expert evaluations. Our analysis shows that conspiratorial messages appear not only in clusters focused on skepticism or distrust, but also within routine discussions of finance, law, and everyday matters. These findings challenge common assumptions about online radicalization by demonstrating that conspiratorial discourse operates within ordinary social interaction. The proposed framework advances computational methods for belief-driven discourse analysis and offers applications for stance detection, political communication studies, and content moderation policy.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†æäº†æ–°åŠ å¡ Telegram ç¾¤ç»„ä¸­çš„é˜´è°‹è®ºè¯è¯­ï¼Œæ—¨åœ¨æ¢è®¨å…¶åœ¨æ•°å­—é€šä¿¡ç”Ÿæ€ä¸­çš„ç»“æ„ä¸ä¼ æ’­ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µè®¡ç®—æ¡†æ¶ï¼Œé¦–å…ˆåˆ©ç”¨å¾®è°ƒçš„ RoBERTa-large æ¨¡å‹å¯¹æ¶ˆæ¯è¿›è¡Œé˜´è°‹è®ºåˆ†ç±»ï¼Œéšåæ„å»ºç¬¦å·ä¿¡ä»°å›¾(Signed Belief Graph)å¹¶å¼•å…¥ç¬¦å·ä¿¡ä»°å›¾ç¥ç»ç½‘ç»œ(SiBeGNN)ï¼Œé€šè¿‡ç¬¦å·è§£è€¦æŸå¤±(Sign Disentanglement Loss)å­¦ä¹ åŒºåˆ†æ„è¯†å½¢æ€å¯¹é½ä¸é£æ ¼ç‰¹å¾çš„åµŒå…¥è¡¨ç¤ºã€‚é€šè¿‡å¯¹è¶…è¿‡55ä¸‡æ¡æ¶ˆæ¯è¿›è¡Œå±‚æ¬¡èšç±»ï¼Œç ”ç©¶æˆåŠŸè¯†åˆ«å‡ºæ³•å¾‹ã€åŒ»ç–—ã€é‡‘èå’Œæƒå¨çŸ›ç›¾ç­‰ä¸ƒç§å™äº‹åŸå‹(Narrative Archetypes)ã€‚å®éªŒè¡¨æ˜ï¼ŒSiBeGNN åœ¨èšç±»è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒcDBI æŒ‡æ ‡è¾¾åˆ° 8.38ã€‚ç ”ç©¶å‘ç°é˜´è°‹è®ºæ¶ˆæ¯å¹¶éä»…å±€é™äºå­¤ç«‹çš„å›å£°å®¤ï¼Œè€Œæ˜¯å¹¿æ³›äº¤ç»‡åœ¨æ—¥å¸¸è®¨è®ºä¸­ï¼ŒæŒ‘æˆ˜äº†å…³äºåœ¨çº¿æ¿€è¿›åŒ–çš„ä¼ ç»Ÿå‡è®¾ã€‚è¯¥æ¡†æ¶ä¸ºåŸºäºä¿¡ä»°çš„è¯è¯­åˆ†ææä¾›äº†å…ˆè¿›å·¥å…·ï¼Œåœ¨ç«‹åœºæ£€æµ‹ã€æ”¿æ²»ä¼ æ’­ç ”ç©¶åŠå†…å®¹å®¡æ ¸æ”¿ç­–åˆ¶å®šæ–¹é¢å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10105v3",
      "published_date": "2025-12-10 21:51:16 UTC",
      "updated_date": "2026-01-12 05:06:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:21:22.158363+00:00"
    },
    {
      "arxiv_id": "2512.10100v1",
      "title": "Robust AI Security and Alignment: A Sisyphean Endeavor?",
      "title_zh": "ç¨³å¥çš„ AI å®‰å…¨ä¸å¯¹é½ï¼šä¸€é¡¹è¥¿è¥¿å¼—æ–¯å¼çš„è‰°å·¨ä»»åŠ¡ï¼Ÿ",
      "authors": [
        "Apostol Vassilev"
      ],
      "abstract": "This manuscript establishes information-theoretic limitations for robustness of AI security and alignment by extending GÃ¶del's incompleteness theorem to AI. Knowing these limitations and preparing for the challenges they bring is critically important for the responsible adoption of the AI technology. Practical approaches to dealing with these challenges are provided as well. Broader implications for cognitive reasoning limitations of AI systems are also proven.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å°† GÃ¶del's incompleteness theorem æ‰©å±•åˆ°äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œç¡®ç«‹äº† AI security å’Œ alignment ç¨³å¥æ€§çš„ä¿¡æ¯è®ºé™åˆ¶ã€‚è¿™ç§æ–¹æ³•æ­ç¤ºäº†åœ¨è¿½æ±‚ç¨³å¥ AI ç³»ç»Ÿè¿‡ç¨‹ä¸­é¢ä¸´çš„æœ¬è´¨æ€§å±€é™ï¼Œå¹¶å¼ºè°ƒäº†äº†è§£è¿™äº›é™åˆ¶å¯¹äºè´Ÿè´£ä»»åœ°é‡‡ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯è‡³å…³é‡è¦ã€‚è®ºæ–‡ä¸ä»…åœ¨ç†è®ºå±‚é¢è¿›è¡Œäº†æ¢è®¨ï¼Œè¿˜ä¸ºåº”å¯¹è¿™äº›ç°å®æŒ‘æˆ˜æä¾›äº†å®ç”¨çš„å¤„ç†æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯æ˜äº† AI ç³»ç»Ÿåœ¨ cognitive reasoning å±€é™æ€§æ–¹é¢çš„å¹¿æ³›å½±å“ã€‚æ•´ä½“è€Œè¨€ï¼Œè¯¥å·¥ä½œä¸ºç†è§£ AI æŠ€æœ¯çš„å®‰å…¨è¾¹ç•Œæä¾›äº†é‡è¦çš„ç†è®ºæ¡†æ¶ï¼Œæç¤ºå¼€å‘è€…åœ¨æ¨åŠ¨ AI åº”ç”¨æ—¶å¿…é¡»å……åˆ†è€ƒè™‘è¿™äº›ä¸å¯é€¾è¶Šçš„ç“¶é¢ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10100v1",
      "published_date": "2025-12-10 21:44:10 UTC",
      "updated_date": "2025-12-10 21:44:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:21:21.457791+00:00"
    },
    {
      "arxiv_id": "2512.10098v1",
      "title": "MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis",
      "title_zh": "MedXAIï¼šé¢å‘çŸ¥è¯†å¼•å¯¼åŒ»å­¦å›¾åƒåˆ†æçš„æ£€ç´¢å¢å¼ºä¸è‡ªéªŒè¯æ¡†æ¶",
      "authors": [
        "Midhat Urooj",
        "Ayan Banerjee",
        "Farhat Shaikh",
        "Kuntal Thakur",
        "Sandeep Gupta"
      ],
      "abstract": "Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly under domain shifts and rare-class conditions. Deep learning models often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Medical Imaging Classification), a unified expert knowledge based framework that integrates deep vision models with clinician-derived expert knowledge to improve generalization, reduce rare-class bias, and provide human-understandable explanations by localizing the relevant diagnostic features rather than relying on technical post-hoc methods (e.g., Saliency Maps, LIME). We evaluate MedXAI across heterogeneous modalities on two challenging tasks: (i) Seizure Onset Zone localization from resting-state fMRI, and (ii) Diabetic Retinopathy grading. Ex periments on ten multicenter datasets show consistent gains, including a 3% improvement in cross-domain generalization and a 10% improvmnet in F1 score of rare class, substantially outperforming strong deep learning baselines. Ablations confirm that the symbolic components act as effective clinical priors and regularizers, improving robustness under distribution shift. MedXAI delivers clinically aligned explanations while achieving superior in-domain and cross-domain performance, particularly for rare diseases in multimodal medical AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MedXAIï¼Œä¸€ä¸ªé›†æˆæ£€ç´¢å¢å¼ºä¸è‡ªæˆ‘éªŒè¯æœºåˆ¶çš„çŸ¥è¯†å¼•å¯¼å‹åŒ»ç–—å›¾åƒåˆ†ææ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹é¢†åŸŸåç§» (domain shifts) å’Œç¨€æœ‰ç±»åˆ« (rare-class) æ¡ä»¶ä¸‹çš„è¯Šæ–­å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§æŒ‘æˆ˜ã€‚MedXAI é€šè¿‡å°†æ·±åº¦è§†è§‰æ¨¡å‹ä¸ä¸´åºŠä¸“å®¶çŸ¥è¯†ç›¸èåˆï¼Œåˆ©ç”¨è¯Šæ–­ç‰¹å¾å®šä½å®ç°ç›´è§‚çš„è§£é‡Šï¼Œæœ‰æ•ˆé¿å…äº† Saliency Maps å’Œ LIME ç­‰ä¼ ç»Ÿäº‹åè§£é‡Šæ–¹æ³•çš„å±€é™æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨ç™«ç—«å‘ä½œèµ·å§‹åŒº (Seizure Onset Zone) å®šä½å’Œç³–å°¿ç—…è§†ç½‘è†œç—…å˜ (Diabetic Retinopathy) åˆ†çº§ä¸¤é¡¹ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨åä¸ªå¤šä¸­å¿ƒæ•°æ®é›†éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMedXAI åœ¨è·¨é¢†åŸŸæ³›åŒ–è¡¨ç°ä¸Šæå‡äº† 3%ï¼Œä¸”åœ¨ç¨€æœ‰ç±»åˆ«çš„ F1 score ä¸Šå®ç°äº† 10% çš„å¢é•¿ï¼Œæ˜¾è‘—è¶…è¶Šäº†å¼ºåŠ›æ·±åº¦å­¦ä¹ åŸºçº¿æ¨¡å‹ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œå…¶ç¬¦å·ç»„ä»¶ (symbolic components) èƒ½ä½œä¸ºæœ‰æ•ˆçš„ä¸´åºŠå…ˆéªŒå’Œæ­£åˆ™åŒ–å·¥å…·ï¼Œåœ¨æå‡æ¨¡å‹é²æ£’æ€§çš„åŒæ—¶ä¸ºå¤šæ¨¡æ€åŒ»ç–— AI æä¾›ä¸´åºŠä¸€è‡´çš„è¯Šæ–­ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "https://cmsworkshops.com/Asilomar2025/Papers/Uploads/FinalPapers/Original/1527/20251130102314_899554_1527.pdf",
      "pdf_url": "https://arxiv.org/pdf/2512.10098v1",
      "published_date": "2025-12-10 21:40:04 UTC",
      "updated_date": "2025-12-10 21:40:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:21:22.677679+00:00"
    },
    {
      "arxiv_id": "2512.10092v1",
      "title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit",
      "title_zh": "åŸºäºç¨€ç–è‡ªç¼–ç å™¨çš„å¯è§£é‡ŠåµŒå…¥ï¼šä¸€ç§æ•°æ®åˆ†æå·¥å…·åŒ…",
      "authors": [
        "Nick Jiang",
        "Xiaoqing Sun",
        "Lisa Dunlap",
        "Lewis Smith",
        "Neel Nanda"
      ],
      "abstract": "Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding \"trigger\" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºä½¿ç”¨ Sparse Autoencoders (SAEs) æ„å»ºå…·æœ‰å¯è§£é‡Šç»´åº¦çš„ SAE embeddingsï¼Œä¸ºå¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“åˆ†ææä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ§çš„æ–°å·¥å…·ã€‚é€šè¿‡å°†å‘é‡ç©ºé—´æ˜ å°„åˆ°å…·ä½“æ¦‚å¿µï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ­ç¤ºæ•°æ®é›†é—´çš„è¯­ä¹‰å·®å¼‚å¹¶è¯†åˆ«æ–‡æ¡£ä¸­æ„å¤–çš„æ¦‚å¿µå…³è”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAE embeddings åœ¨æ£€æµ‹æ¨¡å‹åå·®æ–¹é¢æ¯” LLM æ›´ä¸ºå¯é ï¼Œä¸”åˆ†ææˆæœ¬é™ä½äº† 2-8 å€ã€‚ç”±äºå…¶å…·å¤‡è¿‡æ»¤æ¦‚å¿µçš„å¯æ§æ€§ï¼Œè¯¥æ–¹æ³•åœ¨åŸºäºå±æ€§çš„æ£€ç´¢ (property-based retrieval) ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ dense embeddingsã€‚ç ”ç©¶è¿›ä¸€æ­¥é€šè¿‡å¯¹ OpenAI æ¨¡å‹è¡Œä¸ºæ¼”å˜åŠ Tulu-3 è®­ç»ƒæ•°æ®è§¦å‘è¯çš„æ¡ˆä¾‹åˆ†æï¼Œå±•ç¤ºäº† SAEs åœ¨ç†è§£éç»“æ„åŒ–æ•°æ®å’Œæ¨¡å‹è¡Œä¸ºæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œçªæ˜¾äº†é€šè¿‡æ•°æ®é€è§†æ¨¡å‹ç‰¹æ€§çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Code: https://github.com/nickjiang2378/interp_embed",
      "pdf_url": "https://arxiv.org/pdf/2512.10092v1",
      "published_date": "2025-12-10 21:26:24 UTC",
      "updated_date": "2025-12-10 21:26:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:21:34.623212+00:00"
    },
    {
      "arxiv_id": "2512.10081v1",
      "title": "Defining the Scope of Learning Analytics: An Axiomatic Approach for Analytic Practice and Measurable Learning Phenomena",
      "title_zh": "ç•Œå®šå­¦ä¹ åˆ†æçš„èŒƒç•´ï¼šä¸€ç§é¢å‘åˆ†æå®è·µä¸å¯è¡¡é‡å­¦ä¹ ç°è±¡çš„å…¬ç†åŒ–æ–¹æ³•",
      "authors": [
        "Kensuke Takii",
        "Changhao Liang",
        "Hiroaki Ogata"
      ],
      "abstract": "Learning Analytics (LA) has rapidly expanded through practical and technological innovation, yet its foundational identity has remained theoretically under-specified. This paper addresses this gap by proposing the first axiomatic theory that formally defines the essential structure, scope, and limitations of LA. Derived from the psychological definition of learning and the methodological requirements of LA, the framework consists of five axioms specifying discrete observation, experience construction, state transition, and inference. From these axioms, we derive a set of theorems and propositions that clarify the epistemological stance of LA, including the inherent unobservability of learner states, the irreducibility of temporal order, constraints on reachable states, and the impossibility of deterministically predicting future learning. We further define LA structure and LA practice as formal objects, demonstrating the sufficiency and necessity of the axioms and showing that diverse LA approaches -- such as Bayesian Knowledge Tracing and dashboards -- can be uniformly explained within this framework. The theory provides guiding principles for designing analytic methods and interpreting learning data while avoiding naive behaviorism and category errors by establishing an explicit theoretical inference layer between observations and states. This work positions LA as a rigorous science of state transition systems based on observability, establishing the theoretical foundation necessary for the field's maturation as a scholarly discipline.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Learning Analytics (LA) é¢†åŸŸé•¿æœŸç¼ºä¹ç†è®ºåŸºç¡€çš„é—®é¢˜ï¼Œé¦–æ¬¡æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨æ­£å¼ç•Œå®šå…¶æ ¸å¿ƒç»“æ„ã€èŒƒå›´å’Œå±€é™æ€§çš„å…¬ç†åŒ–ç†è®ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»å­¦ä¹ çš„å¿ƒç†å­¦å®šä¹‰å’Œ LA çš„æ–¹æ³•è®ºéœ€æ±‚å‡ºå‘ï¼Œç”±ç¦»æ•£è§‚å¯Ÿ (discrete observation)ã€ç»éªŒæ„å»º (experience construction)ã€çŠ¶æ€è½¬æ¢ (state transition) å’Œæ¨ç† (inference) ç­‰äº”ä¸ªå…¬ç†ç»„æˆã€‚åŸºäºè¿™äº›å…¬ç†ï¼Œç ”ç©¶æ¨å¯¼å‡ºä¸€ç³»åˆ—å®šç†ï¼Œé˜æ˜äº†å­¦ä¹ è€…çŠ¶æ€çš„æœ¬è´¨ä¸å¯è§‚å¯Ÿæ€§ã€æ—¶é—´é¡ºåºçš„ä¸å¯çº¦æ€§ä»¥åŠæ— æ³•ç¡®å®šæ€§é¢„æµ‹æœªæ¥å­¦ä¹ çš„çº¦æŸã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œè¯¥å…¬ç†ä½“ç³»èƒ½å¤Ÿç»Ÿä¸€è§£é‡Š Bayesian Knowledge Tracing å’Œ dashboards ç­‰å¤šæ ·åŒ–çš„åˆ†ææ–¹æ³•ï¼Œä¸ºè®¾è®¡åˆ†ææ¨¡å‹å’Œè§£é‡Šå­¦ä¹ æ•°æ®æä¾›äº†æŒ‡å¯¼åŸåˆ™ã€‚é€šè¿‡åœ¨è§‚å¯Ÿä¸çŠ¶æ€ä¹‹é—´å»ºç«‹æ˜ç¡®çš„ç†è®ºæ¨ç†å±‚ï¼Œè¯¥å·¥ä½œå°† LA å®šä½ä¸ºä¸€é—¨åŸºäºå¯è§‚å¯Ÿæ€§çš„çŠ¶æ€è½¬æ¢ç³»ç»Ÿç§‘å­¦ï¼Œä¸ºè¯¥å­¦ç§‘çš„å­¦æœ¯æˆç†Ÿå¥ å®šäº†åšå®çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.CY",
      "comment": "27 pages, 1 figure, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.10081v1",
      "published_date": "2025-12-10 21:07:19 UTC",
      "updated_date": "2025-12-10 21:07:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:21:35.509095+00:00"
    },
    {
      "arxiv_id": "2512.10080v1",
      "title": "What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ç©¶ç«Ÿåœ¨è¿›è¡Œä½•ç§æ¨ç†ï¼ˆè‹¥æœ‰ï¼‰ï¼Ÿè®ºå¤§è¯­è¨€æ¨¡å‹çš„éšæœºæœ¬è´¨ä¸æº¯å› æ€§è¡¨è±¡",
      "authors": [
        "Luciano Floridi",
        "Jessica Morley",
        "Claudio Novelli",
        "David Watson"
      ],
      "abstract": "This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.",
      "tldr_zh": "æœ¬æ–‡æ¢è®¨äº†åŸºäº token-completion æœºåˆ¶çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ¨ç†æœ¬è´¨ï¼Œåˆ†æäº†å…¶ stochastic ç‰¹æ€§ä»¥åŠä¸äººç±» abductive reasoning çš„ç›¸ä¼¼æ€§ã€‚ç ”ç©¶è®¤ä¸ºï¼ŒLLMs ç”Ÿæˆæ–‡æœ¬æ˜¯åŸºäºå­¦ä¹ åˆ°çš„æ¨¡å¼è€Œéè¿›è¡ŒçœŸæ­£çš„æº¯å› æ¨ç†ï¼Œå…¶è¡¨ç°å‡ºçš„æ¨ç†èƒ½åŠ›å¾ˆå¤§ç¨‹åº¦ä¸Šæºäºå¯¹åŒ…å«æ¨ç†ç»“æ„çš„äººç±»æ–‡æœ¬çš„è®­ç»ƒã€‚å®ä¾‹åˆ†æè¡¨æ˜ï¼ŒLLMs èƒ½å¤Ÿäº§ç”Ÿçœ‹ä¼¼åˆç†çš„è§‚ç‚¹å¹¶æ¨¡æ‹Ÿå¸¸è¯†æ¨ç†ï¼Œä½†å…¶è¾“å‡ºå¹¶ä¸åŸºäºçœŸç†ã€è¯­ä¹‰ã€éªŒè¯æˆ–ç†è§£ã€‚è¿™ç§æ—¢å…·æœ‰ stochastic åŸºç¡€åˆåœ¨åº”ç”¨ä¸­å‘ˆç° abductive è¡¨ç°çš„åŒé‡æ€§è´¨ï¼Œå¯¹ LLMs çš„è¯„ä¼°å’Œåº”ç”¨äº§ç”Ÿäº†é‡è¦å½±å“ã€‚è®ºæ–‡å¼ºè°ƒï¼Œè™½ç„¶ LLMs å¯ä»¥è¾…åŠ©ç”Ÿæˆåˆ›æ„å¹¶æ”¯æŒäººç±»æ€è€ƒï¼Œä½†ç”±äºå…¶æ— æ³•è¯†åˆ«çœŸç†æˆ–éªŒè¯è‡ªèº«è§£é‡Šï¼Œå…¶è¾“å‡ºå¿…é¡»ç»è¿‡ä¸¥è°¨çš„æ‰¹åˆ¤æ€§è¯„ä¼°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10080v1",
      "published_date": "2025-12-10 21:06:28 UTC",
      "updated_date": "2025-12-10 21:06:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:21:47.524703+00:00"
    },
    {
      "arxiv_id": "2512.10066v1",
      "title": "Classifying Metamorphic versus Single-Fold Proteins with Statistical Learning and AlphaFold2",
      "title_zh": "åŸºäºç»Ÿè®¡å­¦ä¹ ä¸ AlphaFold2 çš„å˜å½¢è›‹ç™½ä¸å•ä¸€æŠ˜å è›‹ç™½åˆ†ç±»",
      "authors": [
        "Yongkai Chen",
        "Samuel WK Wong",
        "SC Kou"
      ],
      "abstract": "The remarkable success of AlphaFold2 in providing accurate atomic-level prediction of protein structures from their amino acid sequence has transformed approaches to the protein folding problem. However, its core paradigm of mapping one sequence to one structure may only be appropriate for single-fold proteins with one stable conformation. Metamorphic proteins, which can adopt multiple distinct conformations, have conformational diversity that cannot be adequately modeled by AlphaFold2. Hence, classifying whether a given protein is metamorphic or single-fold remains a critical challenge for both laboratory experiments and computational methods. To address this challenge, we developed a novel classification framework by re-purposing AlphaFold2 to generate conformational ensembles via a multiple sequence alignment sampling method. From these ensembles, we extract a comprehensive set of features characterizing the conformational ensemble's modality and structural dispersion. A random forest classifier trained on a carefully curated benchmark dataset of known metamorphic and single-fold proteins achieves a mean AUC of 0.869 with cross-validation, demonstrating the effectiveness of our integrated approach. Furthermore, by applying our classifier to 600 randomly sampled proteins from the Protein Data Bank, we identified several potential metamorphic protein candidates -- including the 40S ribosomal protein S30, whose conformational change is crucial for its secondary function in antimicrobial defense. By combining AI-driven protein structure prediction with statistical learning, our work provides a powerful new approach for discovering metamorphic proteins and deepens our understanding of their role in their molecular function.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ AlphaFold2 éš¾ä»¥å»ºæ¨¡å…·æœ‰å¤šç§ç¨³å®šæ„è±¡çš„å˜æ„è›‹ç™½è´¨ (Metamorphic proteins) è¿™ä¸€æŒ‘æˆ˜ï¼Œå¼€å‘äº†ä¸€ç§æ–°å‹åˆ†ç±»æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡æ–°åˆ©ç”¨ AlphaFold2 å¹¶ç»“åˆå¤šåºåˆ—æ¯”å¯¹ (Multiple Sequence Alignment) é‡‡æ ·æŠ€æœ¯ç”Ÿæˆæ„è±¡é›†æˆ (Conformational ensembles)ï¼Œä»ä¸­æå–è¡¨å¾æ„è±¡æ¨¡æ€ä¸ç»“æ„ç¦»æ•£åº¦çš„å…³é”®ç‰¹å¾ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨åœ¨ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„éšæœºæ£®æ—åˆ†ç±»å™¨ (Random forest classifier)ï¼Œåœ¨äº¤å‰éªŒè¯ä¸­å®ç°äº† 0.869 çš„å¹³å‡ AUCï¼Œæ˜¾è‘—æå‡äº†åŒºåˆ†å˜æ„è›‹ç™½è´¨ä¸å•å€è›‹ç™½è´¨çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹è›‹ç™½è´¨æ•°æ®åº“ (Protein Data Bank) ä¸­ 600 ä¸ªéšæœºæ ·æœ¬çš„é¢„æµ‹ï¼Œç ”ç©¶æˆåŠŸè¯†åˆ«å‡ºåŒ…æ‹¬ 40S æ ¸ç³–ä½“è›‹ç™½ S30 åœ¨å†…çš„å¤šä¸ªæ½œåœ¨å˜æ„è›‹ç™½å€™é€‰è€…ã€‚è¯¥å·¥ä½œé€šè¿‡æ•´åˆ AI ç»“æ„é¢„æµ‹ä¸ç»Ÿè®¡å­¦ä¹ ï¼Œä¸ºå˜æ„è›‹ç™½è´¨çš„å‘ç°æä¾›äº†é«˜æ•ˆå·¥å…·ï¼Œå¹¶è¿›ä¸€æ­¥æ­ç¤ºäº†æ„è±¡å¤šæ ·æ€§åœ¨åˆ†å­åŠŸèƒ½ä¸­çš„é‡è¦ä½œç”¨ã€‚",
      "categories": [
        "stat.AP",
        "cs.AI"
      ],
      "primary_category": "stat.AP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10066v1",
      "published_date": "2025-12-10 20:37:21 UTC",
      "updated_date": "2025-12-10 20:37:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:21:48.631804+00:00"
    },
    {
      "arxiv_id": "2512.10065v1",
      "title": "Linear socio-demographic representations emerge in Large Language Models from indirect cues",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­æºäºé—´æ¥çº¿ç´¢çš„çº¿æ€§ç¤¾ä¼šäººå£è¡¨å¾æ¶Œç°",
      "authors": [
        "Paul Bouchaud",
        "Pedro Ramaciotti"
      ],
      "abstract": "We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. We first probe residual streams across layers of four open transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) prompted with explicit demographic disclosure. We show that the same probes predict demographics from implicit cues: names activate census-aligned gender and race representations, while occupations trigger representations correlated with real-world workforce statistics. These linear representations allow us to explain demographic inferences implicitly formed by LLMs during conversation. We demonstrate that these implicit demographic representations actively shape downstream behavior, such as career recommendations. Our study further highlights that models that pass bias benchmark tests may still harbor and leverage implicit biases, with implications for fairness when applied at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•ä»å§“åå’ŒèŒä¸šç­‰é—´æ¥çº¿ç´¢ä¸­æ¨æ–­å¹¶ç¼–ç ç”¨æˆ·çš„ç¤¾ä¼šäººå£å±æ€§ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMs åœ¨æ¿€æ´»ç©ºé—´ï¼ˆActivation Spaceï¼‰å†…å½¢æˆäº†ç”¨æˆ·äººå£ç‰¹å¾çš„çº¿æ€§è¡¨ç¤ºï¼ˆLinear Representationsï¼‰ï¼Œä½¿å¾—ä¸åˆ»æ¿å°è±¡ç›¸å…³çš„å±æ€§æ²¿å¯è§£é‡Šçš„å‡ ä½•æ–¹å‘è¿›è¡Œç¼–ç ã€‚é€šè¿‡å¯¹å››ç§å¼€æºæ¨¡å‹ï¼ˆMagistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1Bï¼‰çš„æ®‹å·®æµï¼ˆResidual Streamsï¼‰è¿›è¡Œæ¢æµ‹ï¼Œä½œè€…è¯æ˜äº†æ¨¡å‹èƒ½ä»éšå«çº¿ç´¢ä¸­é¢„æµ‹æ€§åˆ«ã€ç§æ—åŠèŒä¸šåˆ†å¸ƒï¼Œä¸”è¿™äº›è¡¨å¾ä¸ç°å®ä¸–ç•Œçš„ç»Ÿè®¡æ•°æ®é«˜åº¦ä¸€è‡´ã€‚å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œè¿™äº›éšå¼çš„äººå£ç‰¹å¾è¡¨ç¤ºä¼šç›´æ¥å½±å“æ¨¡å‹çš„ä¸‹æ¸¸è¡Œä¸ºï¼Œä¾‹å¦‚åœ¨èŒä¸šå»ºè®®ä¸­äº§ç”Ÿåå·®ã€‚è¯¥ç ”ç©¶å¼ºè°ƒï¼Œå³ä¾¿æ¨¡å‹é€šè¿‡äº†ç°æœ‰çš„åè§åŸºå‡†æµ‹è¯•ï¼Œåœ¨å¤§è§„æ¨¡åº”ç”¨ä¸­ä»å¯èƒ½åˆ©ç”¨éšå¼åè§ï¼ˆImplicit Biasesï¼‰ï¼Œè¿™å¯¹äººå·¥æ™ºèƒ½çš„å…¬å¹³æ€§è¯„ä¼°å…·æœ‰é‡è¦å¯ç¤ºã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10065v1",
      "published_date": "2025-12-10 20:36:36 UTC",
      "updated_date": "2025-12-10 20:36:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:21:52.419871+00:00"
    },
    {
      "arxiv_id": "2512.10058v1",
      "title": "Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research",
      "title_zh": "å¼¥åˆé¸¿æ²Ÿï¼šé€šå‘äººå·¥æ™ºèƒ½å®‰å…¨ä¸ä¼¦ç†ç ”ç©¶ç»Ÿä¸€çš„è·¯å¾„",
      "authors": [
        "Dani Roytburg",
        "Beck Miller"
      ],
      "abstract": "While much research in artificial intelligence (AI) has focused on scaling capabilities, the accelerating pace of development makes countervailing work on producing harmless, \"aligned\" systems increasingly urgent. Yet research on alignment has diverged along two largely parallel tracks: safety--centered on scaled intelligence, deceptive or scheming behaviors, and existential risk--and ethics--focused on present harms, the reproduction of social bias, and flaws in production pipelines. Although both communities warn of insufficient investment in alignment, they disagree on what alignment means or ought to mean. As a result, their efforts have evolved in relative isolation, shaped by distinct methodologies, institutional homes, and disciplinary genealogies.\n  We present a large-scale, quantitative study showing the structural split between AI safety and AI ethics. Using a bibliometric and co-authorship network analysis of 6,442 papers from twelve major ML and NLP conferences (2020-2025), we find that over 80% of collaborations occur within either the safety or ethics communities, and cross-field connectivity is highly concentrated: roughly 5% of papers account for more than 85% of bridging links. Removing a small number of these brokers sharply increases segregation, indicating that cross-disciplinary exchange depends on a handful of actors rather than broad, distributed collaboration. These results show that the safety-ethics divide is not only conceptual but institutional, with implications for research agendas, policy, and venues. We argue that integrating technical safety work with normative ethics--via shared benchmarks, cross-institutional venues, and mixed-method methodologies--is essential for building AI systems that are both robust and just.",
      "tldr_zh": "è¿™é¡¹å¤§è§„æ¨¡å®šé‡ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½å®‰å…¨(AI Safety)ä¸äººå·¥æ™ºèƒ½ä¼¦ç†(AI Ethics)ç ”ç©¶ä¹‹é—´çš„ç»“æ„æ€§åˆ†æ­§ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¯¹2020å¹´è‡³2025å¹´é—´12ä¸ªä¸»è¦MLå’ŒNLPä¼šè®®çš„6,442ç¯‡è®ºæ–‡è¿›è¡Œæ–‡çŒ®è®¡é‡å’Œåˆè‘—ç½‘ç»œåˆ†æ(bibliometric and co-authorship network analysis)ï¼Œæ­ç¤ºäº†ä¸¤å¤§é¢†åŸŸåœ¨åˆä½œæ¨¡å¼ä¸Šçš„ä¸¥é‡å‰²è£‚ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¶…è¿‡80%çš„å­¦æœ¯åˆä½œä»…å‘ç”Ÿåœ¨å„è‡ªç¤¾åŒºå†…éƒ¨ï¼Œä¸”è·¨é¢†åŸŸè¿æ¥é«˜åº¦é›†ä¸­ï¼Œä»…5%çš„è®ºæ–‡è´¡çŒ®äº†è¶…è¿‡85%çš„è·¨å­¦ç§‘é“¾æ¥ã€‚è¿™ç§åˆ†è£‚ä¸ä»…æ˜¯æ¦‚å¿µå±‚é¢çš„ï¼Œæ›´æ˜¯åˆ¶åº¦å±‚é¢çš„ï¼Œä¸”å­¦ç§‘é—´çš„äº¤æµé«˜åº¦ä¾èµ–äºæå°‘æ•°çš„å…³é”®ä¸­ä»‹è€…ã€‚ä½œè€…å¼ºè°ƒï¼Œé€šè¿‡å…±äº«åŸºå‡†(shared benchmarks)ã€è·¨æœºæ„äº¤æµå¹³å°å’Œæ··åˆæ–¹æ³•è®º(mixed-method methodologies)æ¥æ•´åˆæŠ€æœ¯å®‰å…¨ä¸è§„èŒƒæ€§ä¼¦ç†ï¼Œå¯¹äºæ„å»ºæ—¢é²æ£’(robust)åˆå…¬æ­£(just)çš„AIç³»ç»Ÿè‡³å…³é‡è¦ã€‚è¯¥ç ”ç©¶ä¸ºæ¶ˆé™¤AIå®‰å…¨ä¸ä¼¦ç†ä¹‹é—´çš„éš”é˜‚ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½ç ”ç©¶çš„å…¨é¢ç»Ÿä¸€æä¾›äº†å®è¯ä¾æ®å’Œè·¯å¾„æŒ‡å¼•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for presentation at IASEAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.10058v1",
      "published_date": "2025-12-10 20:28:13 UTC",
      "updated_date": "2025-12-10 20:28:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:22:17.615973+00:00"
    },
    {
      "arxiv_id": "2512.11907v1",
      "title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents",
      "title_zh": "ç»“æ„åŒ–ä¸ªæ€§åŒ–ï¼šé¢å‘æ•°æ®æç®€å‹ LLM æ™ºèƒ½ä½“çš„æ‹Ÿé˜µçº¦æŸå»ºæ¨¡",
      "authors": [
        "Daniel Platnick",
        "Marjan Alirezaie",
        "Hossein Rahnama"
      ],
      "abstract": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“ä¸ªæ€§åŒ–è¿‡ç¨‹ä¸­çš„æ•°æ®æœ€å°åŒ–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Structured Personalization çš„åŸåˆ™æ€§æ–¹æ³•ï¼Œæ—¨åœ¨å¹³è¡¡ä»»åŠ¡æ•ˆç”¨ä¸ä¸ªäººæ•°æ®æŠ«éœ²ã€‚ç ”ç©¶è€…æŒ‡å‡ºï¼Œç°å®ä¸­çš„ä¸ªæ€§åŒ–å¾€å¾€å—é™äºé€»è¾‘ä¾èµ–ã€ç±»åˆ«é…é¢å’Œå±‚æ¬¡åŒ–è§„åˆ™ç­‰å¤æ‚çº¦æŸï¼Œè¿™ä½¿å¾—ä¼ ç»Ÿçš„å­é›†é€‰æ‹©ç®—æ³•å¤±æ•ˆã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å¼•å…¥äº†ä¸€ä¸ªç¼–è¯‘è¿‡ç¨‹ï¼Œå°†ç”¨æˆ·çŸ¥è¯†å›¾è°±ï¼ˆKnowledge Graphï¼‰åŠå…¶ä¾èµ–é¡¹è½¬åŒ–ä¸ºä¸€ç»„æŠ½è±¡çš„å®è§‚åˆ‡é¢ï¼ˆmacro-facetsï¼‰ã€‚æ ¸å¿ƒè´¡çŒ®åœ¨äºè¯æ˜äº†è¿™äº›åŸºäºå®è§‚åˆ‡é¢çš„å±‚æ¬¡åŒ–å’Œé…é¢çº¦æŸæ„æˆäº†ä¸€ä¸ªæœ‰æ•ˆçš„å±‚çŠ¶æ‹Ÿé˜µï¼ˆlaminar matroidï¼‰ã€‚è¿™ä¸€ç†è®ºå‘ç°å…è®¸å°†ç»“æ„åŒ–ä¸ªæ€§åŒ–é—®é¢˜é‡æ–°å®šä¹‰ä¸ºæ‹Ÿé˜µçº¦æŸä¸‹çš„æ¬¡æ¨¡å‡½æ•°æœ€å¤§åŒ–ï¼ˆsubmodular maximization under a matroid constraintï¼‰é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨å…·æœ‰å¸¸æ•°å› å­ä¿è¯çš„è´ªå©ªç®—æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆå¤„ç†æ›´å¤æ‚ä¸”çœŸå®çš„ä¸ªæ€§åŒ–ä»»åŠ¡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the AAAI 2026 Workshop on Personalization in the Era of Large Foundation Models (PerFM), 5 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2512.11907v1",
      "published_date": "2025-12-10 20:22:26 UTC",
      "updated_date": "2025-12-10 20:22:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:22:09.357704+00:00"
    },
    {
      "arxiv_id": "2512.10054v1",
      "title": "Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning",
      "title_zh": "Parallel Decoder Transformerï¼šåŸºäºç¬”è®°æ¡ä»¶åŒ–å®ç°å…·æœ‰æŠ•æœºä¸å˜æ€§çš„æ¨¡å‹å†…éƒ¨å¹¶è¡Œè§£ç ",
      "authors": [
        "Logan Robbins"
      ],
      "abstract": "Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill'' methods like Skeleton-of-Thought attempt to parallelize generation via external orchestration, they suffer from \\textit{coherence drift} due to the lack of cross-stream communication. In this work, we introduce the \\textbf{Parallel Decoder Transformer (PDT)}, a parameter-efficient architecture that embeds coordination primitives directly into the inference process of a frozen pre-trained model.\n  Instead of retraining the base model, PDT injects lightweight \\textit{Speculative Note Conditioning (SNC)} adapters that allow parallel decoding streams to synchronize via a shared, dynamic latent space. We formulate coordination as a \\textit{speculative consensus} problem, where sibling streams broadcast semantic ``notes'' to a global bus, gated by a learned verification head. We validate our approach on a 50,000-step curriculum using a frozen 20B-parameter backbone. Our results demonstrate that PDT achieves effective self-correction, reaching \\textbf{77.8\\% precision} in coverage prediction and recovering approximate serial semantics without modifying the trunk weights. This establishes PDT as a scalable, efficient alternative to full model fine-tuning for structured parallel generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Parallel Decoder Transformer (PDT)ï¼Œè¿™æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªå›å½’è§£ç ä¸­äº§ç”Ÿçš„çº¿æ€§å»¶è¿Ÿç“¶é¢ˆé—®é¢˜ã€‚é’ˆå¯¹Skeleton-of-Thoughtç­‰å¹¶è¡Œæ–¹æ³•å› ç¼ºä¹è·¨æµé€šä¿¡è€Œå¯¼è‡´çš„coherence drifté—®é¢˜ï¼ŒPDTé€šè¿‡åœ¨å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­åµŒå…¥Speculative Note Conditioning (SNC)é€‚é…å™¨ï¼Œå®ç°äº†å¹¶è¡Œè§£ç æµçš„åŒæ­¥ã€‚è¯¥æ–¹æ¡ˆå°†åè°ƒè¿‡ç¨‹è¡¨è¿°ä¸ºspeculative consensusé—®é¢˜ï¼Œå…è®¸å„åˆ†æ”¯æµå‘å…¨å±€æ€»çº¿å¹¿æ’­è¯­ä¹‰notesï¼Œå¹¶ç”±å­¦ä¹ åˆ°çš„éªŒè¯å¤´è¿›è¡Œé—¨æ§ç®¡ç†ã€‚å®éªŒåœ¨20Bå‚æ•°çš„éª¨å¹²æ¨¡å‹ä¸Šè¯æ˜ï¼ŒPDTåœ¨è¦†ç›–é¢„æµ‹ä¸­è¾¾åˆ°äº†77.8%çš„ç²¾åº¦ï¼Œä¸”åœ¨ä¸ä¿®æ”¹ä¸»å¹²æƒé‡çš„æƒ…å†µä¸‹æˆåŠŸæ¢å¤äº†è¿‘ä¼¼åºåˆ—è¯­ä¹‰ã€‚è¿™é¡¹å·¥ä½œä¸ºç»“æ„åŒ–å¹¶è¡Œç”Ÿæˆæä¾›äº†ä¸€ç§æ¯”å…¨æ¨¡å‹å¾®è°ƒæ›´å…·æ‰©å±•æ€§ä¸”é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10054v1",
      "published_date": "2025-12-10 20:19:10 UTC",
      "updated_date": "2025-12-10 20:19:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:22:11.513440+00:00"
    },
    {
      "arxiv_id": "2512.10051v1",
      "title": "DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series Forecasting",
      "title_zh": "DB2-TransFï¼šé¢å‘æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œåªéœ€å¯å­¦ä¹ çš„å¤šè´è¥¿å°æ³¢è¶³çŸ£",
      "authors": [
        "Moulik Gupta",
        "Achyut Mani Tripathi"
      ],
      "abstract": "Time series forecasting requires models that can efficiently capture complex temporal dependencies, especially in large-scale and high-dimensional settings. While Transformer-based architectures excel at modeling long-range dependencies, their quadratic computational complexity poses limitations on scalability and adaptability. To overcome these challenges, we introduce DB2-TransF, a novel Transformer-inspired architecture that replaces the self-attention mechanism with a learnable Daubechies wavelet coefficient layer. This wavelet-based module efficiently captures multi-scale local and global patterns and enhances the modeling of correlations across multiple time series for the time series forecasting task. Extensive experiments on 13 standard forecasting benchmarks demonstrate that DB2-TransF achieves comparable or superior predictive accuracy to conventional Transformers, while substantially reducing memory usage for the time series forecasting task. The obtained experimental results position DB2-TransF as a scalable and resource-efficient framework for advanced time series forecasting. Our code is available at https://github.com/SteadySurfdom/DB2-TransF",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DB2-TransFï¼Œè¿™æ˜¯ä¸€ç§å— Transformer å¯å‘çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡å‹åœ¨å¤§è§„æ¨¡å’Œé«˜ç»´æ—¶é—´åºåˆ—é¢„æµ‹ä¸­é¢ä¸´çš„è®¡ç®—å¤æ‚æ€§æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä½¿ç”¨å¯å­¦ä¹ çš„ Daubechies å°æ³¢ç³»æ•°å±‚ (learnable Daubechies wavelet coefficient layer) æ›¿ä»£äº†ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ (self-attention mechanism)ã€‚è¿™ç§åŸºäºå°æ³¢çš„æ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å¤šå°ºåº¦çš„å±€éƒ¨å’Œå…¨å±€æ¨¡å¼ï¼Œå¹¶å¢å¼ºäº†å¯¹è·¨å¤šä¸ªæ—¶é—´åºåˆ—ç›¸å…³æ€§çš„å»ºæ¨¡èƒ½åŠ›ã€‚åœ¨ 13 ä¸ªæ ‡å‡†é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDB2-TransF åœ¨é¢„æµ‹ä»»åŠ¡ä¸­å®ç°äº†ä¸ä¼ ç»Ÿ Transformer ç›¸å½“ç”šè‡³æ›´ä¼˜çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘äº†å†…å­˜å ç”¨ã€‚è¿™äº›ç»“æœè¯æ˜ DB2-TransF æ˜¯ä¸€ç§å¯æ‰©å±•ä¸”èµ„æºé«˜æ•ˆçš„æ¡†æ¶ï¼Œä¸ºå…ˆè¿›çš„æ—¶é—´åºåˆ—é¢„æµ‹ (Time Series Forecasting) æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10051v1",
      "published_date": "2025-12-10 20:15:22 UTC",
      "updated_date": "2025-12-10 20:15:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:22:20.570194+00:00"
    },
    {
      "arxiv_id": "2512.10047v1",
      "title": "Detailed balance in large language model-driven agents",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨æ™ºèƒ½ä½“ä¸­çš„ç»†è‡´å¹³è¡¡",
      "authors": [
        "Zhuo-Yang Song",
        "Qing-Hong Cao",
        "Ming-xing Luo",
        "Hua Xing Zhu"
      ],
      "abstract": "Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking. This Letter proposes a method based on the least action principle to estimate the underlying generative directionality of LLMs embedded within agents. By experimentally measuring the transition probabilities between LLM-generated states, we statistically discover a detailed balance in LLM-generated transitions, indicating that LLM generation may not be achieved by generally learning rule sets and strategies, but rather by implicitly learning a class of underlying potential functions that may transcend different LLM architectures and prompt templates. To our knowledge, this is the first discovery of a macroscopic physical law in LLM generative dynamics that does not depend on specific model details. This work is an attempt to establish a macroscopic dynamics theory of complex AI systems, aiming to elevate the study of AI agents from a collection of engineering practices to a science built on effective measurements that are predictable and quantifiable.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨æ™ºèƒ½ä½“ï¼ˆLLM-driven agentsï¼‰å®è§‚åŠ¨åŠ›å­¦çš„ç†è®ºæ¡†æ¶ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæœ€å°ä½œç”¨é‡åŸç†ï¼ˆLeast Action Principleï¼‰çš„æ–¹æ³•æ¥ä¼°ç®— LLMs çš„ç”Ÿæˆæ–¹å‘æ€§ã€‚é€šè¿‡å®éªŒæµ‹é‡ç”ŸæˆçŠ¶æ€é—´çš„è½¬ç§»æ¦‚ç‡ï¼Œç ”ç©¶äººå‘˜åœ¨ç»Ÿè®¡å­¦ä¸Šå‘ç°äº† LLM ç”Ÿæˆè½¬æ¢ä¸­å­˜åœ¨ç»†è‡´å¹³è¡¡ï¼ˆDetailed Balanceï¼‰ç°è±¡ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ LLM çš„ç”Ÿæˆè¿‡ç¨‹å¯èƒ½å¹¶éä»…ä¾é å­¦ä¹ è§„åˆ™å’Œç­–ç•¥ï¼Œè€Œæ˜¯é€šè¿‡éšå¼å­¦ä¹ ä¸€ç±»è¶…è¶Šç‰¹å®šæ¶æ„å’Œæç¤ºè¯æ¨¡ç‰ˆï¼ˆPrompt Templatesï¼‰çš„åº•å±‚åŠ¿å‡½æ•°ï¼ˆPotential Functionsï¼‰å®ç°çš„ã€‚è¿™æ˜¯é¦–æ¬¡åœ¨ LLM ç”ŸæˆåŠ¨åŠ›å­¦ä¸­å‘ç°ä¸ä¾èµ–äºæ¨¡å‹ç»†èŠ‚çš„å®è§‚ç‰©ç†è§„å¾‹ï¼Œæ ‡å¿—ç€ AI ç³»ç»ŸåŠ¨åŠ›å­¦ç†è®ºçš„åˆæ­¥å»ºç«‹ã€‚è¯¥å·¥ä½œæ—¨åœ¨å°† AI æ™ºèƒ½ä½“çš„ç ”ç©¶ä»å•çº¯çš„å·¥ç¨‹å®è·µæå‡ä¸ºåŸºäºæœ‰æ•ˆæµ‹é‡ã€å¯é¢„æµ‹ä¸”å¯é‡åŒ–çš„ç§‘å­¦ä½“ç³»ã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.stat-mech",
        "cs.AI",
        "nlin.AO",
        "physics.data-an"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 12 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.10047v1",
      "published_date": "2025-12-10 20:04:23 UTC",
      "updated_date": "2025-12-10 20:04:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:22:21.015153+00:00"
    },
    {
      "arxiv_id": "2512.10046v2",
      "title": "SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration",
      "title_zh": "SimWorld-Roboticsï¼šé¢å‘å¤šæ¨¡æ€æœºå™¨äººå¯¼èˆªä¸åä½œçš„ç…§ç‰‡çº§çœŸå®æ„ŸåŠ¨æ€åŸå¸‚ç¯å¢ƒæ„å»º",
      "authors": [
        "Yan Zhuang",
        "Jiawei Ren",
        "Xiaokang Ye",
        "Jianzhi Shen",
        "Ruixuan Zhang",
        "Tianai Yue",
        "Muhammad Faayez",
        "Xuhong He",
        "Ziqiao Ma",
        "Lianhui Qin",
        "Zhiting Hu",
        "Tianmin Shu"
      ],
      "abstract": "Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SimWorld-Robotics (SWR)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Unreal Engine 5 æ„å»ºçš„å…·èº«æ™ºèƒ½ä»¿çœŸå¹³å°ï¼Œä¸“é—¨ç”¨äºæ¨¡æ‹Ÿå¤§è§„æ¨¡ä¸”å…·æœ‰ç…§ç‰‡çº§çœŸå®æ„Ÿ (Photorealistic) çš„åŸå¸‚ç¯å¢ƒã€‚SWR èƒ½å¤Ÿç¨‹åºåŒ–ç”ŸæˆåŒ…å«è¡Œäººå’Œäº¤é€šç³»ç»Ÿç­‰åŠ¨æ€å…ƒç´ çš„æ— é™åŸå¸‚åœºæ™¯ï¼Œåœ¨çœŸå®æ„Ÿã€å¤æ‚åº¦å’Œå¯æ‰©å±•æ€§æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ä»¥å¾€çš„åŸå¸‚æ¨¡æ‹Ÿå¹³å°ã€‚é€šè¿‡è¯¥å¹³å°ï¼Œç ”ç©¶è€…æ„å»ºäº†å¤šæ¨¡æ€æŒ‡ä»¤éµå¾ªå’Œå¤šæ™ºèƒ½ä½“æœç´¢ä¸¤ä¸ªæå…·æŒ‘æˆ˜æ€§çš„åŸºå‡†ä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°æœºå™¨äººåœ¨ 3D ç©ºé—´æ¨ç†ã€é•¿ç¨‹å®‰å…¨å¯¼èˆªåŠå¤šæœºå™¨äººåä½œç­‰æ–¹é¢çš„æ ¸å¿ƒèƒ½åŠ›ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼Œå½“å‰çš„å…ˆè¿›æ¨¡å‹åŠè§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨å¤„ç†æ­¤ç±»åŸå¸‚ä»»åŠ¡æ—¶ï¼Œåœ¨æ„ŸçŸ¥ã€æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ä¸Šä»é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Conference: NeurIPS 2025 (main)",
      "pdf_url": "https://arxiv.org/pdf/2512.10046v2",
      "published_date": "2025-12-10 20:04:08 UTC",
      "updated_date": "2026-01-22 14:26:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:22:51.645205+00:00"
    },
    {
      "arxiv_id": "2512.10041v2",
      "title": "MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata",
      "title_zh": "MetaVoxelï¼šå½±åƒä¸ä¸´åºŠå…ƒæ•°æ®çš„è”åˆæ‰©æ•£å»ºæ¨¡",
      "authors": [
        "Yihao Liu",
        "Chenyu Gao",
        "Lianrui Zuo",
        "Michael E. Kim",
        "Brian D. Boyd",
        "Lisa L. Barnes",
        "Walter A. Kukull",
        "Lori L. Beason-Held",
        "Susan M. Resnick",
        "Timothy J. Hohman",
        "Warren D. Taylor",
        "Bennett A. Landman"
      ],
      "abstract": "Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference. Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MetaVoxelï¼Œä¸€ç§ç”Ÿæˆå¼è”åˆæ‰©æ•£å»ºæ¨¡(joint diffusion modeling)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å­¦ä¹ æ¶µç›–å½±åƒå’Œä¸´åºŠå…ƒæ•°æ®(metadata)æ‰€æœ‰å˜é‡çš„å•ä¸€æ‰©æ•£è¿‡ç¨‹æ¥æ•æ‰è”åˆåˆ†å¸ƒã€‚ä¸ä¼ ç»Ÿä¾èµ–ç‰¹å®šè¾“å…¥è¾“å‡ºæ–¹å‘çš„æ¡ä»¶é¢„æµ‹æ¨¡å‹ä¸åŒï¼ŒMetaVoxelå®ç°äº†å¤šç§ä»»åŠ¡çš„ç»Ÿä¸€ï¼Œå¹¶æ”¯æŒåˆ©ç”¨ä»»æ„è¾“å…¥å­é›†è¿›è¡Œçµæ´»çš„é›¶æ ·æœ¬æ¨ç†(zero-shot inference)ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œé‡æ–°è®­ç»ƒã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¥è‡ª9ä¸ªæ•°æ®é›†çš„è¶…è¿‡10,000ä»½T1åŠ æƒMRIæ‰«æåŠå…¶ä¸´åºŠå…ƒæ•°æ®è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå•ä¸€çš„MetaVoxelæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆã€å¹´é¾„ä¼°ç®—å’Œæ€§åˆ«é¢„æµ‹ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡å¯ä¸ç°æœ‰çš„ä¸“ä¸šåŸºçº¿æ¨¡å‹ç›¸åª²ç¾ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡æ€è”åˆæ‰©æ•£å»ºæ¨¡æ˜¯ç»Ÿä¸€åŒ»ç–—AIæ¨¡å‹å¹¶æå‡ä¸´åºŠå®ç”¨æ€§çš„ä¸€ä¸ªæå…·å‰æ™¯çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10041v2",
      "published_date": "2025-12-10 19:47:52 UTC",
      "updated_date": "2025-12-12 02:15:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:22:36.296057+00:00"
    },
    {
      "arxiv_id": "2512.10040v1",
      "title": "Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹ç›´æ¥åå¥½ä¼˜åŒ–çš„å¤šå‚è€ƒæ¨¡å‹æ™ºèƒ½åŠ æƒ",
      "authors": [
        "Skyler Wu",
        "Aymen Echarghaoui"
      ],
      "abstract": "Fine-tuning is integral for aligning large language models (LLMs) with human preferences. Multiple-Reference Preference Optimization (MRPO) builds on Direct Preference Optimization (DPO) by fine-tuning LLMs on preference datasets while regularizing the policy towards a mixture of reference models to leverage their collective desirable properties. However, current methods for setting the reference weights are ad-hoc and statistically unsound, leading to unreliable performance. To address this, we introduce four new weighting strategies: two offline methods that leverage held-out validation signal; one online method that uses a sliding-window estimator to reduce overfitting; and an online method that treats reference weighting as a $K$-armed bandit via Thompson Sampling. Experiments using Qwen2.5-0.5B as the policy model and seven reference models from the Llama, Mistral, Qwen, Yi, and Phi families (0.5B-14B each) show that all 4 of our strategies outperform the current MRPO weighting methods on UltraFeedback and SafeRLHF in preference accuracy. More thought-provokingly, however, we find that single-reference DPO, using any of 6 out of 7 references, consistently outperforms all tested multiple-reference approaches -- calling into question the practical appeal of multiple-reference approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹é½ä¸­çš„å¤šå‚è€ƒåå¥½ä¼˜åŒ–(Multiple-Reference Preference Optimization, MRPO)æƒé‡è®¾ç½®ä¸é€æ˜ä¸”ä¸ç¨³å®šçš„é—®é¢˜ï¼Œæå‡ºäº†å››ç§åˆ›æ–°çš„æƒé‡åˆ†é…ç­–ç•¥ã€‚è¿™äº›ç­–ç•¥æ¶µç›–äº†åˆ©ç”¨éªŒè¯é›†ä¿¡å·çš„ä¸¤ç§ç¦»çº¿æ–¹æ³•ï¼Œä»¥åŠåŸºäºæ»‘åŠ¨çª—å£ä¼°è®¡å™¨å’Œå°†æƒé‡åˆ†é…è§†ä¸º$K$-armed bandité—®é¢˜å¹¶é‡‡ç”¨Thompson Samplingå¤„ç†çš„ä¸¤ç§åœ¨çº¿æ–¹æ³•ã€‚å®éªŒåˆ©ç”¨Qwen2.5-0.5Bä½œä¸ºç­–ç•¥æ¨¡å‹ï¼Œå¹¶ç»“åˆæ¥è‡ªLlamaã€Mistralã€Qwenã€Yiå’ŒPhiç³»åˆ—çš„7ä¸ªå‚è€ƒæ¨¡å‹ï¼Œåœ¨UltraFeedbackå’ŒSafeRLHFæ•°æ®é›†ä¸Šè¯æ˜äº†è¿™å››ç§æ–°ç­–ç•¥åœ¨åå¥½å‡†ç¡®ç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„MRPOæƒé‡è®¾ç½®æ–¹æ³•ã€‚ç„¶è€Œï¼Œç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œåœ¨ç»å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½¿ç”¨å•ä¸€å‚è€ƒæ¨¡å‹çš„Direct Preference Optimization (DPO)è¡¨ç°æŒç»­ä¼˜äºæ‰€æœ‰çš„å¤šå‚è€ƒä¼˜åŒ–æ–¹æ³•ã€‚è¿™ä¸€å…·æœ‰å¯å‘æ€§çš„å‘ç°å¯¹å½“å‰å¤šå‚è€ƒä¼˜åŒ–è·¯å¾„åœ¨å®é™…åº”ç”¨ä¸­çš„å¸å¼•åŠ›å’Œå¿…è¦æ€§æå‡ºäº†é‡è¦è´¨ç–‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Working paper. 13 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.10040v1",
      "published_date": "2025-12-10 19:45:20 UTC",
      "updated_date": "2025-12-10 19:45:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:22:30.398627+00:00"
    },
    {
      "arxiv_id": "2512.10034v1",
      "title": "DynaMate: An Autonomous Agent for Protein-Ligand Molecular Dynamics Simulations",
      "title_zh": "DynaMateï¼šé¢å‘è›‹ç™½è´¨-é…ä½“åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿçš„è‡ªä¸»æ™ºèƒ½ä½“",
      "authors": [
        "SalomÃ© Guilbert",
        "Cassandra Masschelein",
        "Jeremy Goumaz",
        "Bohdan Naida",
        "Philippe Schwaller"
      ],
      "abstract": "Force field-based molecular dynamics (MD) simulations are indispensable for probing the structure, dynamics, and functions of biomolecular systems, including proteins and protein-ligand complexes. Despite their broad utility in drug discovery and protein engineering, the technical complexity of MD setup, encompassing parameterization, input preparation, and software configuration, remains a major barrier for widespread and efficient usage. Agentic LLMs have demonstrated their capacity to autonomously execute multi-step scientific processes, and to date, they have not successfully been used to automate protein-ligand MD workflows. Here, we present DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for both protein and protein-ligand systems, and offers free energy binding affinity calculations with the MM/PB(GB)SA method. The framework integrates dynamic tool use, web search, PaperQA, and a self-correcting behavior. DynaMate comprises three specialized modules, interacting to plan the experiment, perform the simulation, and analyze the results. We evaluated its performance across twelve benchmark systems of varying complexity, assessing success rate, efficiency, and adaptability. DynaMate reliably performed full MD simulations, corrected runtime errors through iterative reasoning, and produced meaningful analyses of protein-ligand interactions. This automated framework paves the way toward standardized, scalable, and time-efficient molecular modeling pipelines for future biomolecular and drug design applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ›åœºåˆ†å­åŠ¨åŠ›å­¦(MD)æ¨¡æ‹Ÿåœ¨è›‹ç™½è´¨-é…ä½“ç³»ç»Ÿåº”ç”¨ä¸­é¢ä¸´çš„å‚æ•°åŒ–ã€è¾“å…¥å‡†å¤‡å’Œè½¯ä»¶é…ç½®ç­‰æŠ€æœ¯å¤æ‚æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†DynaMateï¼Œä¸€ä¸ªç”¨äºè‡ªåŠ¨è®¾è®¡å’Œæ‰§è¡ŒMDå·¥ä½œæµçš„è‡ªä¸»æ™ºèƒ½ä½“æ¡†æ¶ã€‚DynaMate é‡‡ç”¨æ¨¡å—åŒ–çš„å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œé›†æˆäº†åŠ¨æ€å·¥å…·è°ƒç”¨ã€ç½‘ç»œæœç´¢ã€PaperQA ä»¥åŠè‡ªæˆ‘çº é”™æœºåˆ¶ï¼Œèƒ½å¤Ÿç‹¬ç«‹å®Œæˆä»å®éªŒè§„åˆ’åˆ°æ¨¡æ‹Ÿæ‰§è¡Œå†åˆ°ç»“æœåˆ†æçš„å®Œæ•´è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶ä¸ä»…æ”¯æŒå¸¸è§„çš„MDæ¨¡æ‹Ÿï¼Œè¿˜æä¾›äº†åŸºäº MM/PB(GB)SA æ–¹æ³•çš„ç»“åˆè‡ªç”±èƒ½è®¡ç®—åŠŸèƒ½ã€‚é€šè¿‡åœ¨åäºŒä¸ªä¸åŒå¤æ‚åº¦çš„åŸºå‡†ç³»ç»Ÿä¸Šçš„è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ DynaMate èƒ½å¤Ÿå¯é åœ°æ‰§è¡Œå®Œæ•´çš„æ¨¡æ‹Ÿä»»åŠ¡ï¼Œå¹¶èƒ½é€šè¿‡è¿­ä»£æ¨ç†ä¿®å¤è¿è¡Œæ—¶çš„é”™è¯¯ã€‚è¯¥ç ”ç©¶ç”Ÿæˆçš„è›‹ç™½è´¨-é…ä½“ç›¸äº’ä½œç”¨åˆ†æå…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¸ºæœªæ¥ç”Ÿç‰©åˆ†å­ç ”ç©¶å’Œè¯ç‰©è®¾è®¡é¢†åŸŸå®ç°æ ‡å‡†åŒ–ã€å¯æ‰©å±•ä¸”é«˜æ•ˆçš„åˆ†å­å»ºæ¨¡æµç¨‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.10034v1",
      "published_date": "2025-12-10 19:40:51 UTC",
      "updated_date": "2025-12-10 19:40:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:22:36.541544+00:00"
    },
    {
      "arxiv_id": "2512.10032v2",
      "title": "Cluster-Dags as Powerful Background Knowledge For Causal Discovery",
      "title_zh": "Cluster-DAGsï¼šç”¨äºå› æœå‘ç°çš„å¼ºå¤§èƒŒæ™¯çŸ¥è¯†",
      "authors": [
        "Jan Marco Ruiz de Vargas",
        "Kirtan Padh",
        "Niki Kilbertus"
      ],
      "abstract": "Finding cause-effect relationships is of key importance in science. Causal discovery aims to recover a graph from data that succinctly describes these cause-effect relationships. However, current methods face several challenges, especially when dealing with high-dimensional data and complex dependencies. Incorporating prior knowledge about the system can aid causal discovery. In this work, we leverage Cluster-DAGs as a prior knowledge framework to warm-start causal discovery. We show that Cluster-DAGs offer greater flexibility than existing approaches based on tiered background knowledge and introduce two modified constraint-based algorithms, Cluster-PC and Cluster-FCI, for causal discovery in the fully and partially observed setting, respectively. Empirical evaluation on simulated data demonstrates that Cluster-PC and Cluster-FCI outperform their respective baselines without prior knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å› æœå‘ç°(Causal discovery)åœ¨å¤„ç†é«˜ç»´æ•°æ®å’Œå¤æ‚ä¾èµ–å…³ç³»æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºåˆ©ç”¨Cluster-DAGsä½œä¸ºä¸€ç§å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†æ¡†æ¶æ¥è¾…åŠ©å› æœå›¾çš„æ¢å¤ã€‚ç›¸æ¯”ç°æœ‰çš„åˆ†å±‚èƒŒæ™¯çŸ¥è¯†(tiered background knowledge)æ–¹æ³•ï¼ŒCluster-DAGsæä¾›äº†æ›´é«˜çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¸ºå› æœå‘ç°è¿‡ç¨‹æä¾›â€œçƒ­å¯åŠ¨â€ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œç ”ç©¶è€…å¼•å…¥äº†ä¸¤ç§æ”¹è¿›çš„åŸºäºçº¦æŸçš„ç®—æ³•ï¼Œåˆ†åˆ«æ˜¯é€‚ç”¨äºå…¨è§‚æµ‹è®¾ç½®çš„Cluster-PCå’Œé€‚ç”¨äºéƒ¨åˆ†è§‚æµ‹è®¾ç½®çš„Cluster-FCIã€‚åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒCluster-PCå’ŒCluster-FCIçš„æ€§èƒ½å‡æ˜¾è‘—ä¼˜äºä¸ä½¿ç”¨å…ˆéªŒçŸ¥è¯†çš„åŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å°†ç»“æ„åŒ–èƒŒæ™¯çŸ¥è¯†æ•´åˆè¿›ç®—æ³•ä¸­ï¼Œå¯¹äºæå‡å¤æ‚ç³»ç»Ÿä¸‹çš„å› æœæ¨æ–­èƒ½åŠ›å…·æœ‰é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.10032v2",
      "published_date": "2025-12-10 19:39:22 UTC",
      "updated_date": "2026-01-19 10:01:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:22:57.610017+00:00"
    },
    {
      "arxiv_id": "2512.10031v1",
      "title": "ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects",
      "title_zh": "ABBSPOï¼šé¢å‘èˆªç©ºå›¾åƒç›®æ ‡æ£€æµ‹çš„è‡ªé€‚åº”è¾¹ç•Œæ¡†ç¼©æ”¾ä¸åŸºäºå¯¹ç§°å…ˆéªŒçš„æ–¹å‘é¢„æµ‹",
      "authors": [
        "Woojin Lee",
        "Hyugjae Chang",
        "Jaeho Moon",
        "Jaehyup Lee",
        "Munchurl Kim"
      ],
      "abstract": "Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ABBSPOï¼Œä¸€ç§é’ˆå¯¹èˆªç©ºå›¾åƒç›®æ ‡æ£€æµ‹çš„å¼±ç›‘ç£æœ‰å‘ç›®æ ‡æ£€æµ‹ (WS-OOD) æ¡†æ¶ï¼Œæ—¨åœ¨æå‡åŸºäºæ°´å¹³è¾¹ç•Œæ¡† (HBox) æ ‡æ³¨çš„æ£€æµ‹ç²¾åº¦ã€‚ä¸ºäº†è§£å†³ä»¥å¾€æ–¹æ³•ä¸­ç›´æ¥å¯¹æ¯”æ°´å¹³æ¡†ä¸æ—‹è½¬æ¡†å¤–æ¥çŸ©å½¢å¯¼è‡´çš„å°ºåº¦ä¼°ç®—è¯¯å·®ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†è‡ªé€‚åº”è¾¹ç•Œæ¡†ç¼©æ”¾ (ABBS) æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ ground truth (GT) HBoxes çš„æ¯”ä¾‹æ¥ä¼˜åŒ–é¢„æµ‹æ—‹è½¬æ¡† (RBox) çš„å°ºå¯¸ã€‚æ­¤å¤–ï¼ŒABBSPO å¼•å…¥äº†åŸºäºå¯¹ç§°å…ˆéªŒè§’åº¦ (SPA) çš„æŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨èˆªç©ºç›®æ ‡å›ºæœ‰çš„å¯¹ç§°å±æ€§è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šè§†å›¾å¢å¼ºä¸‹é¢„æµ‹ç»“æœä¸€è‡´æ€§é”™è¯¯å¯¼è‡´çš„è®­ç»ƒå´©æºƒé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒABBSPO åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œè¾¾åˆ°äº†å½“å‰çš„ state-of-the-art æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 11 figures, 8 tables, supplementary included. Accepted to CVPR 2025. Please visit our project page at https://kaist-viclab.github.io/ABBSPO_site/",
      "pdf_url": "https://arxiv.org/pdf/2512.10031v1",
      "published_date": "2025-12-10 19:37:54 UTC",
      "updated_date": "2025-12-10 19:37:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:23:11.065734+00:00"
    },
    {
      "arxiv_id": "2512.11000v1",
      "title": "Unambiguous Representations in Neural Networks: An Information-Theoretic Approach to Intentionality",
      "title_zh": "ç¥ç»ç½‘ç»œä¸­çš„æ— æ­§ä¹‰è¡¨å¾ï¼šæ„å‘æ€§çš„ä¿¡æ¯è®ºæ–¹æ³•",
      "authors": [
        "Francesco LÃ¤ssig"
      ],
      "abstract": "Representations pervade our daily experience, from letters representing sounds to bit strings encoding digital files. While such representations require externally defined decoders to convey meaning, conscious experience appears fundamentally different: a neural state corresponding to perceiving a red square cannot alternatively encode the experience of a green square. This intrinsic property of consciousness suggests that conscious representations must be unambiguous in a way that conventional representations are not. We formalize this intuition using information theory, defining representational ambiguity as the conditional entropy H(I|R) over possible interpretations I given a representation R. Through experiments on neural networks trained to classify MNIST digits, we demonstrate that relational structures in network connectivity can unambiguously encode representational content. Using both learned decoders and direct geometric matching, we achieve perfect (100%) accuracy for dropout-trained networks and 38% for standard backpropagation in identifying output neuron class identity, despite identical task performance, demonstrating that representational ambiguity can arise orthogonally to behavioral accuracy. We further show that spatial position information of input neurons can be decoded from network connectivity with R2 up to 0.844. These results provide a quantitative method for measuring representational ambiguity in neural systems and demonstrate that neural networks can exhibit the low-ambiguity representations posited as necessary (though not sufficient) by theoretical accounts of consciousness.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ„è¯†è¡¨å¾çš„æœ¬è´¨ï¼Œè®¤ä¸ºå…¶å¿…é¡»å…·å¤‡å¸¸è§„è¡¨å¾æ‰€ä¸å…·å¤‡çš„éæ­§ä¹‰æ€§ï¼ˆunambiguousï¼‰ã€‚ä½œè€…åˆ©ç”¨ä¿¡æ¯è®ºï¼ˆInformation Theoryï¼‰å°†è¡¨å¾æ­§ä¹‰æ€§ï¼ˆrepresentational ambiguityï¼‰å½¢å¼åŒ–ä¸ºç»™å®šè¡¨å¾ä¸‹å¯èƒ½è§£é‡Šçš„æ¡ä»¶ç†µ $H(I|R)$ï¼Œå¹¶åŸºäº MNIST æ•°æ®é›†è®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼ˆNeural Networksï¼‰è¿›è¡Œå®éªŒã€‚ç ”ç©¶å‘ç°ï¼Œç½‘ç»œè¿æ¥ä¸­çš„å…³ç³»ç»“æ„èƒ½å¤Ÿæ— æ­§ä¹‰åœ°ç¼–ç è¡¨å¾å†…å®¹ï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡å‡ ä½•åŒ¹é…å’Œå­¦ä¹ è§£ç å™¨ï¼Œç»è¿‡ Dropout è®­ç»ƒçš„ç½‘ç»œåœ¨è¯†åˆ«è¾“å‡ºç¥ç»å…ƒç±»åˆ«æ ‡è¯†æ–¹é¢è¾¾åˆ°äº† 100% çš„å‡†ç¡®ç‡ï¼Œè€Œæ ‡å‡†åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰ä»…ä¸º 38%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯æ˜äº†ä»ç½‘ç»œè¿æ¥ä¸­è§£ç è¾“å…¥ç¥ç»å…ƒç©ºé—´ä½ç½®ä¿¡æ¯çš„ $R^2$ å¯è¾¾ 0.844ï¼Œä¸”è¿™ç§è¡¨å¾æ­§ä¹‰æ€§çš„é™ä½ä¸è¡Œä¸ºå‡†ç¡®ç‡æ˜¯è§£è€¦çš„ã€‚è¯¥å·¥ä½œä¸ºè¡¡é‡ç¥ç»ç³»ç»Ÿä¸­çš„è¡¨å¾æ­§ä¹‰æ€§æä¾›äº†å®šé‡æ–¹æ³•ï¼Œå¹¶è¯æ˜äº†ç¥ç»ç½‘ç»œå¯ä»¥è¡¨ç°å‡ºæ„è¯†ç†è®ºæ‰€è¦æ±‚çš„ä½æ­§ä¹‰è¡¨å¾ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Presented at the Models of Consciousness 6 (MoC6) conference (https://amcs-community.org/moc6-schedule-information/#abstract-36)",
      "pdf_url": "https://arxiv.org/pdf/2512.11000v1",
      "published_date": "2025-12-10 19:00:34 UTC",
      "updated_date": "2025-12-10 19:00:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:24:17.004606+00:00"
    },
    {
      "arxiv_id": "2512.10004v2",
      "title": "Exploring LLMs for Scientific Information Extraction Using The SciEx Framework",
      "title_zh": "åˆ©ç”¨ SciEx æ¡†æ¶æ¢ç©¶å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦ä¿¡æ¯æŠ½å–ä¸­çš„åº”ç”¨",
      "authors": [
        "Sha Li",
        "Ayush Sadekar",
        "Nathan Self",
        "Yiqi Su",
        "Lars Andersland",
        "Mira Chaplin",
        "Annabel Zhang",
        "Hyoju Yang",
        "James B Henderson",
        "Krista Wigginton",
        "Linsey Marr",
        "T. M. Murali",
        "Naren Ramakrishnan"
      ],
      "abstract": "Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-modal content, and reconciling varied and inconsistent fine-grained information across multiple publications into standardized formats. These challenges are further compounded when the desired data schema or extraction ontology changes rapidly, making it difficult to re-architect or fine-tune existing systems. We present SciEx, a modular and composable framework that decouples key components including PDF parsing, multi-modal retrieval, extraction, and aggregation. This design streamlines on-demand data extraction while enabling extensibility and flexible integration of new models, prompting strategies, and reasoning mechanisms. We evaluate SciEx on datasets spanning three scientific topics for its ability to extract fine-grained information accurately and consistently. Our findings provide practical insights into both the strengths and limitations of current LLM-based pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œç§‘å­¦ä¿¡æ¯æå–(Scientific Information Extraction)çš„æ½œåŠ›ï¼Œå¹¶æå‡ºäº†åä¸ºSciExçš„æ¨¡å—åŒ–å¯ç»„åˆæ¡†æ¶ã€‚SciExæ—¨åœ¨è§£å†³ç°æœ‰å·¥å…·åœ¨å¤„ç†é•¿æ–‡æ¡£ã€å¤šæ¨¡æ€å†…å®¹ä»¥åŠè·¨æ–‡çŒ®åè°ƒä¸ä¸€è‡´ä¿¡æ¯æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åº”å¯¹äº†æ•°æ®æ¨¡å¼(data schema)æˆ–æœ¬ä½“(ontology)é¢‘ç¹å˜åŠ¨å¸¦æ¥çš„æŠ€æœ¯éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è§£è€¦PDFè§£æ(PDF parsing)ã€å¤šæ¨¡æ€æ£€ç´¢(multi-modal retrieval)ã€æå–(extraction)å’Œèšåˆ(aggregation)ç­‰å…³é”®ç»„ä»¶ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ä¸é›†æˆæ–°æ¨¡å‹åŠæ¨ç†æœºåˆ¶çš„çµæ´»æ€§ã€‚é€šè¿‡åœ¨ä¸‰ä¸ªä¸åŒç§‘å­¦é¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶éªŒè¯äº†SciExåœ¨å‡†ç¡®ã€ä¸€è‡´åœ°æå–ç»†ç²’åº¦ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœä¸ä»…è¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¿˜ä¸ºå½“å‰åŸºäºLLMsçš„ç§‘å­¦æ•°æ®æµæ°´çº¿çš„ä¼˜åŠ¿ä¸å±€é™æ€§æä¾›äº†å®è´µçš„å®è·µè§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the KGML Bridge at AAAI 2026 (non-archival)",
      "pdf_url": "https://arxiv.org/pdf/2512.10004v2",
      "published_date": "2025-12-10 19:00:20 UTC",
      "updated_date": "2026-01-23 02:20:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:23:33.569531+00:00"
    },
    {
      "arxiv_id": "2512.09920v1",
      "title": "LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating",
      "title_zh": "LISNï¼šåŸºäº VLM æ§åˆ¶å™¨è°ƒèŠ‚çš„è¯­è¨€æŒ‡ä»¤å¼•å¯¼ç¤¾ä¼šå¯¼èˆª",
      "authors": [
        "Junting Chen",
        "Yunchuan Li",
        "Panfeng Jiang",
        "Jiacheng Du",
        "Zixuan Chen",
        "Chenrui Tie",
        "Jiajun Deng",
        "Lin Shao"
      ],
      "abstract": "Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/",
      "tldr_zh": "é’ˆå¯¹äººæœºå…±å­˜ç¯å¢ƒä¸‹çš„ç¤¾äº¤æ„ŸçŸ¥å¯¼èˆªï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äº†å¯¹äººç±»æŒ‡ä»¤çš„éµå¾ªä»¥åŠç¤¾äº¤è§„èŒƒçš„å¯¹é½ï¼Œä¸ºæ­¤æå‡ºäº†é¦–ä¸ªåŸºäºä»¿çœŸçš„è¯­è¨€æŒ‡ä»¤ç¤¾äº¤å¯¼èˆªåŸºå‡†LISN-Benchã€‚è¯¥åŸºå‡†åŸºäºRosnav-Arena 3.0å¼€å‘ï¼Œé¦–æ¬¡å°†æŒ‡ä»¤éµå¾ªå’Œè·¨åœºæ™¯ç†è§£æ•´åˆåˆ°æ ‡å‡†åŒ–çš„ç¤¾äº¤å¯¼èˆªè¯„ä¼°ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥æå‡ºäº†Social-Nav-Modulatorï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„å¤šå±‚æ¬¡å¿«æ…¢ç³»ç»Ÿï¼Œé€šè¿‡VLMæ™ºèƒ½ä½“è°ƒèŠ‚ä»£ä»·å›¾(costmaps)å’Œæ§åˆ¶å™¨å‚æ•°ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å°†åº•å±‚åŠ¨ä½œç”Ÿæˆä¸è¾ƒæ…¢çš„VLMå¾ªç¯è§£è€¦ï¼Œæœ‰æ•ˆé™ä½äº†å¯¹é«˜é¢‘VLMæ¨ç†çš„ä¾èµ–ï¼ŒåŒæ—¶æ˜¾è‘—å¢å¼ºäº†åŠ¨æ€é¿éšœä¸æ„ŸçŸ¥é€‚åº”èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†91.3%çš„å¹³å‡æˆåŠŸç‡ï¼Œæ¯”æœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹æé«˜äº†63%ä»¥ä¸Šã€‚åœ¨äººç¾¤éšè¡ŒåŠä¸¥æ ¼è§„é¿ç¦åŒºç­‰å¤æ‚ä»»åŠ¡ä¸­ï¼Œè¯¥ç³»ç»Ÿè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸ºå®ç°æ›´å…·ç¤¾äº¤æ™ºèƒ½çš„ç§»åŠ¨æœºå™¨äººå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.09920v1",
      "published_date": "2025-12-10 18:54:30 UTC",
      "updated_date": "2025-12-10 18:54:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:23:25.306179+00:00"
    },
    {
      "arxiv_id": "2512.09914v1",
      "title": "FALCON: Few-step Accurate Likelihoods for Continuous Flows",
      "title_zh": "FALCONï¼šè¿ç»­æµçš„å°‘æ­¥ç²¾å‡†ä¼¼ç„¶è®¡ç®—",
      "authors": [
        "Danyal Rehman",
        "Tara Akhound-Sadegh",
        "Artem Gazizov",
        "Yoshua Bengio",
        "Alexander Tong"
      ],
      "abstract": "Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»Ÿè®¡ç‰©ç†ä¸­ Boltzmann Generators é‡‡æ ·æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº† FALCON (Few-step Accurate Likelihoods for Continuous Flows) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ Continuous Normalizing Flows (CNFs) åœ¨ä¼¼ç„¶è®¡ç®—æ—¶æé«˜çš„å‡½æ•°è¯„ä¼°å¼€é”€ã€‚FALCON é€šè¿‡å¼•å…¥ä¸€ç§é¼“åŠ±å¯é€†æ€§çš„æ··åˆè®­ç»ƒç›®æ ‡ï¼Œå®ç°äº†åœ¨æå°‘æ•°æ­¥æ•°å†…è¿›è¡Œé«˜ç²¾åº¦ä¼¼ç„¶è®¡ç®—çš„é‡‡æ ·ï¼Œä»è€Œæœ‰æ•ˆæ”¯æŒ Importance Sampling åº”ç”¨ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFALCON åœ¨åˆ†å­ Boltzmann é‡‡æ ·ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„ Normalizing Flow æ¨¡å‹ã€‚åœ¨è¾¾åˆ°åŒç­‰æ€§èƒ½æ°´å¹³çš„å‰æä¸‹ï¼ŒFALCON çš„è¿è¡Œé€Ÿåº¦æ¯”ä¼ ç»Ÿ CNF æ¨¡å‹å¿«äº†ä¸¤ä¸ªæ•°é‡çº§ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤æ‚åˆ†å­çŠ¶æ€æ—¶çš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint; NeurIPS 2025 MLSB",
      "pdf_url": "https://arxiv.org/pdf/2512.09914v1",
      "published_date": "2025-12-10 18:47:25 UTC",
      "updated_date": "2025-12-10 18:47:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:23:44.042418+00:00"
    },
    {
      "arxiv_id": "2512.09912v1",
      "title": "Supervised learning pays attention",
      "title_zh": "ç›‘ç£å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Erin Craig",
        "Robert Tibshirani"
      ],
      "abstract": "In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.\n  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†æ³¨æ„åŠ›æœºåˆ¶(Attention)çš„ä¸Šä¸‹æ–‡å­¦ä¹ (In-context Learning)æ€æƒ³å¼•å…¥åˆ°è¡¨æ ¼æ•°æ®(Tabular Data)çš„ç›‘ç£å­¦ä¹ ä¸­ï¼Œå¦‚Lasso Regressionå’ŒGradient Boostingã€‚é€šè¿‡ä¸€ç§ç›‘ç£ç›¸ä¼¼æ€§åº¦é‡ï¼Œè¯¥æ–¹æ³•æ ¹æ®Attentionå¯¹è®­ç»ƒæ•°æ®è¿›è¡ŒåŠ æƒï¼Œä»è€Œä¸ºæ¯ä¸ªæµ‹è¯•è§‚å¯Ÿå€¼æ‹Ÿåˆä¸ªæ€§åŒ–çš„å±€éƒ¨æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•æ— éœ€é¢„è®¾èšç±»æˆ–ç›¸ä¼¼æ€§å³å¯çµæ´»å¤„ç†å¼‚æ„æ•°æ®ï¼Œå¹¶èƒ½è¯†åˆ«å¯¹é¢„æµ‹æœ€å…·å½±å“çš„ç‰¹å¾åŠç›¸å…³çš„è®­ç»ƒæ ·æœ¬ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚ç ”ç©¶è¿˜å±•ç¤ºäº†å¦‚ä½•å°†è¯¥æŠ€æœ¯æ‰©å±•è‡³æ—¶é—´åºåˆ—ä¸ç©ºé—´æ•°æ®ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ©ç”¨Attention-Weightedæ®‹å·®ä¿®æ­£ä½¿é¢„è®­ç»ƒæ ‘æ¨¡å‹é€‚åº”åˆ†å¸ƒåç§»(Distributional Shift)çš„æ–¹æ³•ã€‚ç†è®ºè¯æ˜ä¸å®éªŒç»“æœå‡è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹ç®€å•æ€§çš„åŒæ—¶æå‡äº†é¢„æµ‹ç²¾åº¦ï¼Œåœ¨æ··åˆæ¨¡å‹æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½è·å¾—æ¯”æ ‡å‡†çº¿æ€§æ¨¡å‹æ›´ä½çš„å‡æ–¹è¯¯å·®(Mean Squared Error)ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09912v1",
      "published_date": "2025-12-10 18:43:46 UTC",
      "updated_date": "2025-12-10 18:43:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:23:49.378095+00:00"
    },
    {
      "arxiv_id": "2512.09910v1",
      "title": "Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach",
      "title_zh": "ç¥ç»æœºå™¨ç¿»è¯‘çš„é«˜æ•ˆæŒç»­å­¦ä¹ ï¼šä¸€ç§ä½ç§©è‡ªé€‚åº”æ–¹æ³•",
      "authors": [
        "Salvador CarriÃ³n",
        "Francisco Casacuberta"
      ],
      "abstract": "Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Neural Machine Translation (NMT) åœ¨æŒç»­å­¦ä¹ ä¸­é¢ä¸´çš„ catastrophic forgetting å’Œé«˜æ˜‚é‡è®­æˆæœ¬é—®é¢˜ï¼Œæå‡ºå°† Low-Rank Adaptation (LoRA) ä½œä¸ºä¸€ç§å‚æ•°é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶é¦–å…ˆè¯å®äº†åŸºäº LoRA çš„å¾®è°ƒåœ¨é€‚åº”æ–°è¯­è¨€å’Œé¢†åŸŸæ–¹é¢å…·æœ‰ä¸å…¨å‚æ•°æŠ€æœ¯æŒå¹³çš„æ€§èƒ½ï¼Œä¸”æ˜¾è‘—é™ä½äº†å‚æ•°éœ€æ±‚ã€‚éšåï¼Œä½œè€…æå‡ºä¸€ç§åŸºäº LoRA æ¨¡å—æ ¡å‡†çº¿æ€§ç»„åˆçš„äº¤äº’å¼è‡ªé€‚åº”æ–¹æ³•ï¼Œåˆ©ç”¨ gate-free mixture of experts å®ç°äº†å®æ—¶ã€ç”¨æˆ·å¯æ§çš„é¢†åŸŸå’Œé£æ ¼è°ƒæ•´ã€‚ä¸ºäº†è¿›ä¸€æ­¥æŠ‘åˆ¶ç¾éš¾æ€§é—å¿˜ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ä¸“ä¸ºä½ç§©åˆ†è§£çŸ©é˜µè®¾è®¡çš„æ¢¯åº¦æ­£åˆ™åŒ–ç­–ç•¥ï¼Œé€šè¿‡å†å²æ¢¯åº¦ä¿¡æ¯å¯¹ä½ç§©æ›´æ–°è¿›è¡ŒåŠ æƒæƒ©ç½šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿé«˜æ•ˆä¿ç•™å…ˆå‰é¢†åŸŸçš„çŸ¥è¯†å¹¶é¡ºåˆ©ä¹ å¾—æ–°ä»»åŠ¡ï¼Œä¸ºæ„å»ºå¯æ‰©å±•çš„äº¤äº’å¼æŒç»­ NMT ç³»ç»Ÿæä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09910v1",
      "published_date": "2025-12-10 18:37:57 UTC",
      "updated_date": "2025-12-10 18:37:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:23:58.902303+00:00"
    },
    {
      "arxiv_id": "2512.09909v1",
      "title": "STACHE: Local Black-Box Explanations for Reinforcement Learning Policies",
      "title_zh": "STACHEï¼šå¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å±€éƒ¨é»‘ç›’è§£é‡Š",
      "authors": [
        "Andrew Elashkin",
        "Orna Grumberg"
      ],
      "abstract": "Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† STACHEï¼Œä¸€ä¸ªæ—¨åœ¨ä¸º Reinforcement Learning æ™ºèƒ½ä½“åœ¨ç¦»æ•£ Markov games ä¸­çš„ç‰¹å®šåŠ¨ä½œç”Ÿæˆå±€éƒ¨é»‘ç›’è§£é‡Šçš„ç»¼åˆæ¡†æ¶ï¼Œä»¥è§£å†³å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„è¡Œä¸ºä¸å¯é¢„æµ‹æ€§ã€‚è¯¥æ–¹æ³•äº§ç”Ÿçš„ Composite Explanation ç”± Robustness Region å’Œ Minimal Counterfactuals ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼Œåˆ†åˆ«å®šä¹‰äº†åŠ¨ä½œä¸å˜çš„çŠ¶æ€é‚»åŸŸä»¥åŠæ”¹å˜å†³ç­–æ‰€éœ€çš„æœ€å°çŠ¶æ€æ‰°åŠ¨ã€‚é€šè¿‡åˆ©ç”¨ factored state spaces çš„ç»“æ„ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºæœç´¢çš„ç²¾ç¡®ç®—æ³•ï¼Œæœ‰æ•ˆè§„é¿äº† surrogate models çš„ä¿çœŸåº¦è¯¯å·®ã€‚åœ¨ Gymnasium ç¯å¢ƒä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä¸ä»…èƒ½è§£é‡Šç­–ç•¥åŠ¨ä½œï¼Œè¿˜èƒ½æ•æ‰ç­–ç•¥é€»è¾‘åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»ä¸ç¨³å®šåˆ°ä¼˜åŒ–é²æ£’çš„æ¼”åŒ–è½¨è¿¹ã€‚STACHE ä¸ºæ™ºèƒ½ä½“çš„æ•æ„Ÿæ€§å’Œå†³ç­–è¾¹ç•Œæä¾›äº†å¯æ“ä½œçš„è§è§£ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„è°ƒè¯•ä¸éªŒè¯æä¾›äº†å¯é å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09909v1",
      "published_date": "2025-12-10 18:37:28 UTC",
      "updated_date": "2025-12-10 18:37:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:23:51.774584+00:00"
    },
    {
      "arxiv_id": "2512.09908v1",
      "title": "Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective",
      "title_zh": "è´å¶æ–¯ç½‘ç»œã€é©¬å°”å¯å¤«ç½‘ç»œã€é“å¾·åŒ–ä¸ä¸‰è§’åŒ–ï¼šèŒƒç•´è®ºè§†è§’",
      "authors": [
        "Antonio Lorenzin",
        "Fabio Zanasi"
      ],
      "abstract": "Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»èŒƒç•´è®º(Categorical Perspective)çš„è§†è§’æ¢è®¨äº†æ¦‚ç‡å›¾å½¢æ¨¡å‹(Probabilistic Graphical Models)ä¸­è´å¶æ–¯ç½‘ç»œ(Bayesian Networks)ä¸é©¬å°”å¯å¤«ç½‘ç»œ(Markov Networks)ä¹‹é—´çš„è½¬æ¢æœºåˆ¶ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªèŒƒç•´åŒ–æ¡†æ¶ï¼Œå°†é“å¾·åŒ–(Moralisation)å’Œä¸‰è§’åŒ–(Triangulation)å»ºæ¨¡ä¸ºä¸åŒç½‘ç»œç±»åˆ«ä¹‹é—´çš„å‡½å­(functors)ï¼Œå¹¶å°†ç½‘ç»œæœ¬èº«å®šä¹‰ä¸ºä»è¯­æ³•åˆ°è¯­ä¹‰æ˜ å°„çš„å‡½å­ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé“å¾·åŒ–æ˜¯çº¯ç²¹çš„è¯­æ³•æ“ä½œï¼Œè€Œä¸‰è§’åŒ–åˆ™æ¶‰åŠè¯­ä¹‰å±‚é¢ã€‚é€šè¿‡å°†å˜é‡æ¶ˆé™¤(Variable Elimination)ç®—æ³•é‡æ–°è¯ é‡Šä¸ºå‡½å­ï¼Œè¯¥æ¡†æ¶æˆåŠŸå°†ä¸‰è§’åŒ–è¿‡ç¨‹æ‹†åˆ†ä¸ºçº¯è¯­æ³•å’Œçº¯è¯­ä¹‰ä¸¤ä¸ªç‹¬ç«‹éƒ¨åˆ†ã€‚è¿™ä¸€æ–¹æ³•ä¸ºæ¦‚ç‡å›¾å½¢æ¨¡å‹ç†è®ºå¼•å…¥äº†å‡½å­åŒ–è§†è§’ï¼Œæ¸…æ™°åœ°æ­ç¤ºäº†è¯­æ³•ä¿®æ”¹ä¸è¯­ä¹‰ä¿®æ”¹ä¹‹é—´çš„æœ¬è´¨åŒºåˆ«ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO",
        "math.CT"
      ],
      "primary_category": "cs.AI",
      "comment": "36 pages. A preliminary version of this work was presented at CALCO 2025, under the title \"An Algebraic Approach to Moralisation and Triangulation of Probabilistic Graphical Models''",
      "pdf_url": "https://arxiv.org/pdf/2512.09908v1",
      "published_date": "2025-12-10 18:36:30 UTC",
      "updated_date": "2025-12-10 18:36:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:24:26.445470+00:00"
    },
    {
      "arxiv_id": "2512.09898v1",
      "title": "Visual Heading Prediction for Autonomous Aerial Vehicles",
      "title_zh": "é¢å‘è‡ªä¸»é£è¡Œå™¨çš„è§†è§‰èˆªå‘é¢„æµ‹",
      "authors": [
        "Reza Ahmari",
        "Ahmad Mohammadi",
        "Vahid Hemmati",
        "Mohammed Mynuddin",
        "Parham Kebria",
        "Mahmoud Nabil Mahmoud",
        "Xiaohong Yuan",
        "Abdollah Homaifar"
      ],
      "abstract": "The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506Â° and a root mean squared error of 0.1957Â°, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºæ— äººæœº(UAV)ä¸åœ°é¢æ— äººè½¦(UGV)å®æ—¶é›†æˆçš„è§†è§‰æ•°æ®é©±åŠ¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³GPS/GNSSä¿¡å·å—é™ç¯å¢ƒä¸‹çš„ååŒéš¾é¢˜ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¾®è°ƒåçš„YOLOv5æ¨¡å‹å®ç°å¯¹åœ°é¢è½¦è¾†çš„ç²¾ç¡®æ£€æµ‹å¹¶æå–è¾¹ç•Œæ¡†ç‰¹å¾ï¼Œéšåç»“åˆè½»é‡çº§äººå·¥ç¥ç»ç½‘ç»œ(ANN)ä»…é€šè¿‡å•ç›®ç›¸æœºè¾“å…¥æ¥å®æ—¶é¢„æµ‹æ— äººæœºæ‰€éœ€çš„èˆªå‘è§’(Heading Angle)ã€‚é€šè¿‡åœ¨å—æ§å®éªŒå®¤å†…åˆ©ç”¨VICONè¿åŠ¨æ•è·ç³»ç»Ÿæ„å»ºçš„1.3ä¸‡å¼ æ ‡æ³¨å›¾åƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥ANNæ¨¡å‹åœ¨èˆªå‘è§’é¢„æµ‹ä¸Šè¾¾åˆ°äº†0.1506Â°çš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒUGVæ£€æµ‹å‡†ç¡®ç‡é«˜è¾¾95%ï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨æ— åŸºç¡€è®¾æ–½ä¾èµ–ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºç°å®åŠ¨æ€æ¡ä»¶ä¸‹çš„å¤šæ™ºèƒ½ä½“å¯é åä½œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„è§†è§‰æ„ŸçŸ¥è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.MA",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09898v1",
      "published_date": "2025-12-10 18:27:37 UTC",
      "updated_date": "2025-12-10 18:27:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:24:01.909911+00:00"
    },
    {
      "arxiv_id": "2512.09897v1",
      "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments",
      "title_zh": "SCOPEï¼šå°†è¯­è¨€æ¨¡å‹ä½œä¸ºæ–‡æœ¬ç¯å¢ƒåˆ†å±‚è§„åˆ’çš„ä¸€æ¬¡æ€§å¯¼å¸ˆ",
      "authors": [
        "Haoye Lu",
        "Pavan Seshadri",
        "Kaheer Suleman"
      ],
      "abstract": "Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SCOPE (Subgoal-COnditioned Pretraining for Efficient planning)ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤æ‚æ–‡æœ¬ç¯å¢ƒçš„é•¿ç¨‹è§„åˆ’ä¸­å› é¢‘ç¹è°ƒç”¨è€Œå¯¼è‡´çš„è®¡ç®—æˆæœ¬é«˜å’Œéƒ¨ç½²å›°éš¾ç­‰é—®é¢˜ã€‚ä¸ä»¥å¾€åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­åå¤æŸ¥è¯¢ LLMs çš„æ–¹æ³•ä¸åŒï¼ŒSCOPE é‡‡ç”¨â€œä¸€æ¬¡æ€§è€å¸ˆâ€æ¨¡å¼ï¼Œä»…åœ¨åˆå§‹åŒ–é˜¶æ®µåˆ©ç”¨ LLM ç”Ÿæˆçš„å­ç›®æ ‡ (subgoals) æ¥é¢„è®­ç»ƒè½»é‡çº§çš„å­¦ç”Ÿæ¨¡å‹ã€‚è¯¥æ–¹æ³•ç›´æ¥ä»ç¤ºä¾‹è½¨è¿¹ä¸­æå–å­ç›®æ ‡ï¼Œä»è€Œæ¶ˆé™¤äº†é‡å¤è°ƒç”¨ LLM çš„éœ€æ±‚ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„è¿è¡Œæ•ˆç‡ã€‚åœ¨ TextCraft ç¯å¢ƒä¸‹çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ç”Ÿæˆçš„å­ç›®æ ‡å¯èƒ½å­˜åœ¨æ¬¡ä¼˜æ€§ï¼Œä½†å®ƒä»¬ä»èƒ½ä¸ºå±‚æ¬¡åŒ–ç›®æ ‡åˆ†è§£æä¾›å¼ºæœ‰åŠ›çš„èµ·å§‹å¼•å¯¼ã€‚ç›¸æ¯”äºåŸºäº LLM çš„åˆ†å±‚æ™ºèƒ½ä½“ ADaPTï¼ŒSCOPE ä¸ä»…å°†æˆåŠŸç‡ä» 0.52 æå‡è‡³ 0.56ï¼Œæ›´å°†å•æ¬¡æ¨ç†æ—¶é—´ä» 164.4 ç§’å¤§å¹…ç¼©å‡è‡³ä»… 3.0 ç§’ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†é€šè¿‡çŸ¥è¯†è’¸é¦æ„å»ºé«˜æ•ˆã€å¯é€‚åº”ç‰¹å®šä»»åŠ¡çš„å±‚æ¬¡åŒ–è§„åˆ’å™¨çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09897v1",
      "published_date": "2025-12-10 18:26:14 UTC",
      "updated_date": "2025-12-10 18:26:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:24:40.423959+00:00"
    },
    {
      "arxiv_id": "2512.09895v1",
      "title": "Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science",
      "title_zh": "äººåœ¨å›è·¯ä¸äººå·¥æ™ºèƒ½ï¼šææ–™ç§‘å­¦å…ƒæ•°æ®è¯æ±‡ä¼—åŒ…",
      "authors": [
        "Jane Greenberg",
        "Scott McClellan",
        "Addy Ireland",
        "Robert Sammarco",
        "Colton Gerber",
        "Christopher B. Rauch",
        "Mat Kelly",
        "John Kunze",
        "Yuan An",
        "Eric Toberer"
      ],
      "abstract": "Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†MatSci-YAMZå¹³å°ï¼Œæ—¨åœ¨è§£å†³ææ–™ç§‘å­¦é¢†åŸŸMetadata vocabularieså¼€å‘å—é™äºäººåŠ›èµ„æºå’Œæ ‡å‡†åŒ–ä¸ä¸€çš„é—®é¢˜ã€‚è¯¥å¹³å°é›†æˆäº†äººå·¥æ™ºèƒ½(AI)ä¸äººæœºå›ç¯(Human-in-the-loop, HILT)ï¼Œé€šè¿‡ä¼—åŒ…(Crowdsourcing)æ¨¡å¼æ”¯æŒå…ƒæ•°æ®è¯æ±‡çš„åä½œæ„å»ºã€‚ç ”ç©¶é€šè¿‡æ¥è‡ªNSF ID4æœºæ„çš„6åå‚ä¸è€…è¿›è¡Œäº†æ¦‚å¿µéªŒè¯ï¼Œåˆ©ç”¨è¿­ä»£åé¦ˆå¾ªç¯å¯¹AIç”Ÿæˆçš„æœ¯è¯­å®šä¹‰è¿›è¡ŒæŒç»­ä¼˜åŒ–ã€‚å®éªŒæˆåŠŸç”Ÿæˆäº†19é¡¹AIå®šä¹‰ï¼Œè¯æ˜äº†AI-HILTæ¨¡å‹åœ¨å¢å¼ºè¯­ä¹‰é€æ˜åº¦å’Œç¼©çŸ­å…±è¯†è¾¾æˆæ—¶é—´æ–¹é¢çš„å¯è¡Œæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜è¯¥æ¨¡å¼é«˜åº¦ç¬¦åˆFAIRå’Œå¼€æ”¾ç§‘å­¦(Open-science)åŸåˆ™ï¼Œå¹¶æä¾›äº†ä¸€å¥—æŒ‡å¯¼æœªæ¥ç ”ç©¶çš„åè®®ã€‚MatSci-YAMZå±•ç°äº†è·¨é¢†åŸŸæ‰©å±•çš„æ½œåŠ›ï¼Œä¸ºå¤šå­¦ç§‘é¢†åŸŸçš„æ ‡å‡†åŒ–è¯æ±‡å¼€å‘æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.AI",
      "comment": "Metadata and Semantics Research Conference 2025, 14 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.09895v1",
      "published_date": "2025-12-10 18:22:57 UTC",
      "updated_date": "2025-12-10 18:22:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:24:43.059733+00:00"
    },
    {
      "arxiv_id": "2512.09892v1",
      "title": "Provably Learning from Modern Language Models via Low Logit Rank",
      "title_zh": "åŸºäºä½ Logit ç§©çš„ç°ä»£è¯­è¨€æ¨¡å‹å¯è¯æ˜å­¦ä¹ ",
      "authors": [
        "Noah Golowich",
        "Allen Liu",
        "Abhishek Shetty"
      ],
      "abstract": "While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.\n  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç°ä»£è¯­è¨€æ¨¡å‹(Language Models)çš„å¤æ‚æ€§ï¼Œå¹¶åŸºäºç»éªŒè§‚å¯Ÿæå‡ºäº†ä¸€ç§ç®€åŒ–çš„æŠ½è±¡æ¨¡å‹ï¼Œå³ä½å¯¹æ•°å‡ ç‡ç§©(Low Logit Rank)ã€‚è¿™ä¸€å±æ€§åæ˜ äº†æ¨¡å‹åœ¨ç‰¹å®šåºåˆ—ä¸‹çš„æ ‡è®°(Tokens)å¯¹æ•°æ¦‚ç‡çŸ©é˜µå¯ä»¥ç”¨ä½ç§©çŸ©é˜µè¿‘ä¼¼ï¼Œä¸ºç†è§£æ¨¡å‹å†…éƒ¨æœºåˆ¶æä¾›äº†åˆ‡å…¥ç‚¹ã€‚ç ”ç©¶çš„æ ¸å¿ƒåœ¨äºå¦‚ä½•åˆ©ç”¨è¿™ç§ç»“æ„è·å–å¯è¯æ˜çš„å­¦ä¹ ä¿è¯(Provable Learning Guarantees)ï¼Œä»¥è§£å†³è¿™äº›æ¨¡å‹å¯èƒ½ç¼–ç çš„å˜ˆæ‚å¥‡å¶æ ¡éªŒ(Noisy Parities)ç­‰éš¾å­¦åˆ†å¸ƒé—®é¢˜ã€‚ä½œè€…è®¾è®¡äº†ä¸€ç§åæ˜ å¸¸è§APIè®¿é—®æ¨¡å¼çš„å¯¹æ•°å‡ ç‡æŸ¥è¯¢å­¦ä¹ æ¨¡å‹(Query Learning Model with Logit Queries)ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªèƒ½å¤Ÿé€šè¿‡æŸ¥è¯¢é«˜æ•ˆå­¦ä¹ ä»»ä½•è¿‘ä¼¼ä½å¯¹æ•°å‡ ç‡ç§©æ¨¡å‹çš„æ–°ç®—æ³•ã€‚è¯¥æˆæœæä¾›äº†é¦–ä¸ªé’ˆå¯¹èƒ½å¤Ÿåˆç†åˆ»ç”»ç°ä»£è¯­è¨€æ¨¡å‹è¡Œä¸ºçš„ç”Ÿæˆæ¨¡å‹çš„ç«¯åˆ°ç«¯å­¦ä¹ ä¿è¯ï¼ŒæˆåŠŸå°†ç†è®ºå­¦ä¹ ä¿éšœä¸å¤§æ¨¡å‹çš„å®è¯ç‰¹æ€§è”ç³»èµ·æ¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09892v1",
      "published_date": "2025-12-10 18:18:11 UTC",
      "updated_date": "2025-12-10 18:18:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:24:48.452525+00:00"
    },
    {
      "arxiv_id": "2512.09882v1",
      "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing",
      "title_zh": "çœŸå®ç¯å¢ƒæ¸—é€æµ‹è¯•ä¸­ AI æ™ºèƒ½ä½“ä¸ç½‘ç»œå®‰å…¨ä¸“ä¸šäººå£«çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Justin W. Lin",
        "Eliot Krzysztof Jones",
        "Donovan Julian Jasper",
        "Ethan Jun-shen Ho",
        "Anna Wu",
        "Arnold Tianyi Yang",
        "Neil Perry",
        "Andy Zou",
        "Matt Fredrikson",
        "J. Zico Kolter",
        "Percy Liang",
        "Dan Boneh",
        "Daniel E. Ho"
      ],
      "abstract": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ AI Agents ä¸äººç±»ç½‘ç»œå®‰å…¨ä¸“å®¶åœ¨åŒ…å«çº¦ 8,000 å°ä¸»æœºçš„çœŸå®å¤§å­¦ç½‘ç»œç¯å¢ƒä¸­çš„æ¸—é€æµ‹è¯•è¡¨ç°è¿›è¡Œäº†é¦–æ¬¡å…¨é¢è¯„ä¼°ã€‚ç ”ç©¶äººå‘˜æå‡ºäº†åä¸º ARTEMIS çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·å¤‡åŠ¨æ€ Prompt ç”Ÿæˆã€ä»»æ„å­æ™ºèƒ½ä½“è°ƒåº¦åŠè‡ªåŠ¨æ¼æ´åˆ†æ‹£åŠŸèƒ½ã€‚åœ¨å¯¹æ¯”å®éªŒä¸­ï¼ŒARTEMIS æ•´ä½“è¡¨ç°æ’åç¬¬äºŒï¼ŒæˆåŠŸå‘ç° 9 ä¸ªæœ‰æ•ˆæ¼æ´ï¼Œå…¶æœ‰æ•ˆæäº¤ç‡è¾¾ 82%ï¼Œè¡¨ç°ä¼˜äº 10 åäººç±»å‚ä¸è€…ä¸­çš„ 9 ä½ã€‚è™½ç„¶ Codex å’Œ CyAgent ç­‰ç°æœ‰æ¡†æ¶è¡¨ç°é€Šäºå¤§å¤šæ•°äººç±»ä¸“å®¶ï¼Œä½† ARTEMIS å±•ç¤ºäº†ä¸é¡¶å°–äººç±»å‚ä¸è€…ç›¸å½“çš„æŠ€æœ¯æ°´å¹³ã€‚ç ”ç©¶å‘ç° AI Agents åœ¨ç³»ç»ŸåŒ–æšä¸¾ (Systematic Enumeration)ã€å¹¶è¡Œæ¼æ´åˆ©ç”¨ (Parallel Exploitation) å’Œæˆæœ¬æ•ˆç›Šæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…¶è¿è¡Œæˆæœ¬çº¦ä¸ºæ¯å°æ—¶ 18 ç¾å…ƒï¼Œè¿œä½äºäººç±»ä¸“å®¶çš„ 60 ç¾å…ƒã€‚å°½ç®¡å¦‚æ­¤ï¼ŒAI Agents ä»å­˜åœ¨ False-positive ç‡è¾ƒé«˜ä»¥åŠåœ¨å¤„ç†åŸºäº GUI çš„ä»»åŠ¡æ—¶èƒ½åŠ›ä¸è¶³ç­‰å±€é™æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09882v1",
      "published_date": "2025-12-10 18:12:29 UTC",
      "updated_date": "2025-12-10 18:12:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:24:51.228432+00:00"
    },
    {
      "arxiv_id": "2512.09874v1",
      "title": "Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs",
      "title_zh": "é¢å‘PDFæ•°å­¦å…¬å¼æå–çš„æ–‡æ¡£è§£æå™¨åŸºå‡†æµ‹è¯•",
      "authors": [
        "Pius Horn",
        "Janis Keuper"
      ],
      "abstract": "Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»PDFä¸­è§£ææ•°å­¦å…¬å¼çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†ç¼ºä¹æ•°å­¦å…¬å¼æˆ–è¯­ä¹‰æ„ŸçŸ¥è¯„ä¼°æŒ‡æ ‡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¸¦æœ‰ç²¾ç¡®LaTeX ground truthçš„åˆæˆPDFï¼Œå®ç°äº†å¯¹é¡µé¢å¸ƒå±€ã€å…¬å¼ç‰¹å¾å’Œå†…å®¹çš„ç³»ç»ŸåŒ–æ§åˆ¶ã€‚ç ”ç©¶çš„ä¸»è¦è´¡çŒ®æ˜¯ç‡å…ˆå¼•å…¥äº†LLM-as-a-judgeæ–¹æ³•è¿›è¡Œè¯­ä¹‰ç»´åº¦çš„å…¬å¼è¯„ä¼°ï¼Œå¹¶é…åˆä¸€ä¸ªé²æ£’çš„ä¸¤é˜¶æ®µåŒ¹é…æµæ°´çº¿(two-stage matching pipeline)æ¥è§£å†³è§£æå™¨è¾“å‡ºçš„ä¸ä¸€è‡´æ€§ã€‚é€šè¿‡å¯¹250ä¸ªå…¬å¼å¯¹çš„äººå·¥éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºåŸºäºLLMçš„è¯„ä¼°ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§ï¼ˆPearson r=0.78ï¼‰è¿œé«˜äºCDMï¼ˆr=0.34ï¼‰åŠæ–‡æœ¬ç›¸ä¼¼åº¦æŒ‡æ ‡ã€‚è¯¥åŸºå‡†å¯¹åŒ…æ‹¬ä¸“ç”¨OCRæ¨¡å‹ã€è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models)åŠåŸºäºè§„åˆ™çš„æ–¹æ³•åœ¨å†…çš„20å¤šç§ç°ä»£PDFè§£æå™¨è¿›è¡Œäº†å¤§è§„æ¨¡æµ‹è¯•ï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å¤„ç†2,000å¤šä¸ªå…¬å¼æ—¶çš„æ˜¾è‘—æ€§èƒ½å·®å¼‚ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºä»ä¸šè€…é€‰æ‹©è§£æå™¨æä¾›äº†é‡è¦å‚è€ƒï¼Œè¿˜å»ºç«‹äº†ä¸€å¥—å¯å¤ç°ä¸”å…·æœ‰æ‰©å±•æ€§çš„PDFå…¬å¼æå–è´¨é‡è¯„ä¼°æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09874v1",
      "published_date": "2025-12-10 18:01:50 UTC",
      "updated_date": "2025-12-10 18:01:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:24:52.928698+00:00"
    },
    {
      "arxiv_id": "2512.09976v1",
      "title": "Fuzzy Hierarchical Multiplex",
      "title_zh": "æ¨¡ç³Šå±‚çº§å¤ç”¨æ¨¡å‹",
      "authors": [
        "Alexis Kafantaris"
      ],
      "abstract": "A new fuzzy optimization framework that extends FCM causality is proposed. This model utilizes the dynamics to map data into metrics and create a framework that examines logical implication and hierarchy of concepts using a multiplex. Moreover, this is a white-theoretical paper introducing the framework and analyzing the logic and math behind it. Upon this extension the main objectives and the orientation of this framework is expounded and exemplified; this framework is meant for service optimization of information transmission in service process design. Lastly, a thorough analysis of the FHM is included which is done following the logical steps in a simple and elegant manner.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Fuzzy Hierarchical Multiplex (FHM)ï¼Œè¿™æ˜¯ä¸€ç§æ‰©å±•äº†FCMå› æœå…³ç³»çš„æ–°å‹æ¨¡ç³Šä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¨¡å‹åˆ©ç”¨åŠ¨åŠ›å­¦å°†æ•°æ®æ˜ å°„ä¸ºæŒ‡æ ‡ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåˆ©ç”¨multiplexç ”ç©¶æ¦‚å¿µé—´é€»è¾‘è•´æ¶µå’Œå±‚æ¬¡ç»“æ„çš„æ¡†æ¶ã€‚ä½œä¸ºä¸€ç¯‡ç†è®ºæ€§è®ºæ–‡ï¼Œè¯¥ç ”ç©¶è¯¦ç»†é˜è¿°å¹¶åˆ†æäº†è¯¥æ¡†æ¶èƒŒåçš„é€»è¾‘ä¸æ•°å­¦åŸç†ã€‚è¯¥æ¡†æ¶çš„ä¸»è¦åº”ç”¨å¯¼å‘æ˜¯ä¼˜åŒ–æœåŠ¡æµç¨‹è®¾è®¡ä¸­çš„ä¿¡æ¯ä¼ è¾“æœåŠ¡ã€‚æœ€åï¼Œä½œè€…æŒ‰ç…§é€»è¾‘æ­¥éª¤ï¼Œä»¥ç®€æ´ä¼˜é›…çš„æ–¹å¼å¯¹FHMè¿›è¡Œäº†æ·±å…¥è€Œå½»åº•çš„åˆ†æã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 2 figures, 1 double figure, 1 table, 12 references. This will be part of my PhD dissertation and it is a White paper-theoretical framewor. As is, it s meant for a basis that will be later used to further developed an FHM. It might not be math-logic related and I am willing to change it, I just felt that it belonged to mathematical modeling. Yours truly, AK",
      "pdf_url": "https://arxiv.org/pdf/2512.09976v1",
      "published_date": "2025-12-10 17:59:07 UTC",
      "updated_date": "2025-12-10 17:59:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:08.872670+00:00"
    },
    {
      "arxiv_id": "2512.09872v1",
      "title": "FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning",
      "title_zh": "FlipLLMï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆæ¯”ç‰¹ç¿»è½¬æ”»å‡»",
      "authors": [
        "Khurram Khalil",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FlipLLMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„æ¶æ„æ— å…³æ¡†æ¶ï¼Œæ—¨åœ¨é«˜æ•ˆå‘ç°å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œå¤šæ¨¡æ€æ¨¡å‹(VLMs)ä¸­çš„æ¯”ç‰¹ç¿»è½¬æ”»å‡»(Bit-Flip Attacks)æ¼æ´ã€‚è¯¥æ¡†æ¶å°†æ”»å‡»æœç´¢å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–é—®é¢˜ï¼Œç»“åˆæ•æ„Ÿæ€§å¼•å¯¼çš„å±‚å‰ªæä¸ Q-learning ç®—æ³•ï¼Œèƒ½å¤Ÿå¿«é€Ÿè¯†åˆ«å‡ºè¯±å‘æ¨¡å‹ç¾éš¾æ€§å¤±æ•ˆçš„æå°é«˜å½±å“æ¯”ç‰¹é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒFlipLLM åœ¨å®šä½æ˜“å—æ”»å‡»æ¯”ç‰¹çš„é€Ÿåº¦ä¸Šæ¯”ç°æœ‰æ–¹æ³•å¿« 2.5 å€ï¼Œä»…é€šè¿‡ç¿»è½¬ LLaMA 3.1 8B ä¸­çš„ 5 ä¸ªæ¯”ç‰¹æˆ– LLaVA ä¸­çš„ 7 ä¸ªæ¯”ç‰¹ï¼Œå³å¯ä½¿æ¨¡å‹å‡†ç¡®ç‡å‡ è¿‘å½’é›¶ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œåœ¨è¯†åˆ«å‡ºçš„ä½ç‚¹åº”ç”¨ ECC SECDED ç­‰ç¡¬ä»¶ä¿æŠ¤æœºåˆ¶å¯å®Œå…¨æŠµå¾¡æ”»å‡»å½±å“ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æŒ‡å¯¼ç¡¬ä»¶çº§å®‰å…¨é˜²å¾¡æ–¹é¢çš„å®ç”¨ä»·å€¼ã€‚ä½œä¸ºé¦–ä¸ªé’ˆå¯¹è¯­è¨€å’Œå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å¯æ‰©å±•è¯„ä¼°æ–¹æ³•ï¼ŒFlipLLM ä¸ºå¤§è§„æ¨¡æ¨¡å‹çš„ç¡¬ä»¶å®‰å…¨æ€§è¯„ä¼°å’Œé˜²å¾¡å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted in IEEE HOST 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.09872v1",
      "published_date": "2025-12-10 17:58:18 UTC",
      "updated_date": "2025-12-10 17:58:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:04.120712+00:00"
    },
    {
      "arxiv_id": "2512.09867v3",
      "title": "Hierarchy-Aware Multimodal Unlearning for Medical AI",
      "title_zh": "é¢å‘åŒ»ç–—äººå·¥æ™ºèƒ½çš„å±‚æ¬¡æ„ŸçŸ¥å¤šæ¨¡æ€æœºå™¨é—å¿˜",
      "authors": [
        "Fengli Wu",
        "Vaidehi Patil",
        "Jaehong Yoon",
        "Yue Zhang",
        "Mohit Bansal"
      ],
      "abstract": "Pretrained Multimodal Large Language Models (MLLMs) are increasingly used in sensitive domains such as medical AI, where privacy regulations like HIPAA and GDPR require specific removal of individuals' or institutions' data. This motivates machine unlearning, which aims to remove the influence of target data from a trained model. However, existing unlearning benchmarks fail to reflect the hierarchical and multimodal structure of real-world medical data, limiting their ability to properly evaluate unlearning in practice. Therefore, we introduce MedForget, a hierarchy-aware multimodal unlearning benchmark that models hospital data as a nested structure, enabling fine-grained evaluation of multimodal unlearning across retain and forget splits. Experiments with current unlearning methods show that existing approaches struggle to achieve effective hierarchy-aware forgetting without degrading downstream medical utility. To address this limitation, we propose Cross-modal Hierarchy-Informed Projection for unlearning (CHIP), a training-free, hierarchy-aware multimodal unlearning method that deletes information by selectively removing target-specific weight subspaces while preserving sibling-shared information. Experiments show that CHIP achieves the highest forget-retain performance gap across all hierarchy levels while maintaining competitive downstream utility compared to existing methods. Overall, MedForget provides a practical, HIPAA-aligned benchmark for evaluating structured multimodal unlearning for medical data, and CHIP offers an effective and general solution for hierarchy-aware forgetting that balances deletion with utility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—äººå·¥æ™ºèƒ½é¢†åŸŸä¸­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨éµå®ˆéšç§æ³•è§„ï¼ˆå¦‚HIPAAå’ŒGDPRï¼‰æ—¶é¢ä¸´çš„æ•°æ®æ“¦é™¤æŒ‘æˆ˜ï¼Œæå‡ºäº†MedForgetåŸºå‡†ã€‚MedForgetæ˜¯ä¸€ä¸ªå…·æœ‰å±‚çº§æ„ŸçŸ¥èƒ½åŠ›çš„å¤šæ¨¡æ€æœºå™¨é—å¿˜ï¼ˆMachine Unlearningï¼‰åŸºå‡†ï¼Œå®ƒé€šè¿‡å°†åŒ»é™¢æ•°æ®æ¨¡æ‹Ÿä¸ºåµŒå¥—çš„å±‚çº§ç»“æ„ï¼Œå®ç°äº†å¯¹åŒ»ç–—æ•°æ®è·¨ä¿ç•™ä¸é—å¿˜é›†çš„ç²¾ç»†åŒ–è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ç°æœ‰é—å¿˜æ–¹æ³•åœ¨å®ç°æœ‰æ•ˆçš„å±‚çº§æ„ŸçŸ¥é—å¿˜æ—¶ï¼Œå¾€å¾€ä¼šæŸå®³æ¨¡å‹çš„ä¸‹æ¸¸åŒ»ç–—ä»»åŠ¡æ•ˆç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†CHIPï¼ˆCross-modal Hierarchy-Informed Projectionï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å±‚çº§æ„ŸçŸ¥å¤šæ¨¡æ€é—å¿˜æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°ç§»é™¤ç›®æ ‡ç‰¹å®šçš„æƒé‡å­ç©ºé—´å¹¶ä¿ç•™åŒçº§å…±äº«ä¿¡æ¯æ¥æ¶ˆé™¤æ•°æ®å½±å“ã€‚å®éªŒè¯æ˜ï¼ŒCHIPåœ¨å„å±‚çº§ä¸Šå‡å®ç°äº†æœ€ä½³çš„é—å¿˜ä¸ä¿ç•™æ€§èƒ½å¹³è¡¡ï¼ŒåŒæ—¶ç»´æŒäº†æå…·ç«äº‰åŠ›çš„ä¸‹æ¸¸æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºç¬¦åˆåˆè§„è¦æ±‚çš„åŒ»ç–—æ•°æ®ç»“æ„åŒ–é—å¿˜æä¾›äº†é‡è¦çš„åŸºå‡†æ”¯æŒä¸æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Dataset and Code: https://github.com/fengli-wu/MedForget",
      "pdf_url": "https://arxiv.org/pdf/2512.09867v3",
      "published_date": "2025-12-10 17:55:06 UTC",
      "updated_date": "2026-01-23 02:43:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:07.178395+00:00"
    },
    {
      "arxiv_id": "2512.09831v1",
      "title": "Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning",
      "title_zh": "è§£é‡Šå³çº¿æ€§å˜æ¢ï¼šä¿¡å¿µä¸æ„ä¹‰çš„è®¤çŸ¥å‡ ä½•æ¨¡å‹",
      "authors": [
        "Chainarong Amornbunchornvej"
      ],
      "abstract": "This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.\n  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-\"the No-Null-Space Leadership Condition\"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.\n  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.",
      "tldr_zh": "è¯¥è®ºæ–‡å¼€å‘äº†ä¸€ä¸ªå‡ ä½•æ¡†æ¶ï¼Œç”¨äºå¯¹è®¤çŸ¥å¼‚è´¨æ€§æ™ºèƒ½ä½“ä¹‹é—´çš„ä¿¡ä»°ã€åŠ¨æœºå’Œå½±å“è¿›è¡Œå»ºæ¨¡ï¼Œå°†æ¯ä¸ªæ™ºèƒ½ä½“è¡¨ç¤ºä¸ºä¸€ä¸ªç¼–ç å†…éƒ¨è§£é‡Šç»´åº¦çš„ä¸ªæ€§åŒ–ä»·å€¼ç©ºé—´(personalized value space)ã€‚ä¿¡ä»°åœ¨æ–‡ä¸­è¢«å½¢å¼åŒ–ä¸ºç»“æ„åŒ–å‘é‡ï¼Œå…¶ä¼ æ’­ç”±çº¿æ€§è§£é‡Šæ˜ å°„(linear interpretation maps)è°ƒèŠ‚ï¼Œä¸”ä»…åœ¨é¿å¼€è¿™äº›æ˜ å°„çš„é›¶ç©ºé—´(null spaces)æ—¶æ‰èƒ½å­˜ç»­ï¼Œä»è€Œä¸ºå¯ç†è§£æ€§å’Œäº¤æµéšœç¢æä¾›äº†ç»“æ„åŒ–æ ‡å‡†ã€‚é€šè¿‡çº¯ç²¹çš„ä»£æ•°çº¦æŸï¼Œè¯¥æ¡†æ¶è§£é‡Šäº†ä¿¡ä»°æ‰­æ›²(belief distortion)ã€åŠ¨æœºæ¼‚ç§»(motivational drift)åŠç›¸äº’ç†è§£çš„å±€é™æ€§ã€‚ç ”ç©¶çš„æ ¸å¿ƒæˆæœæ˜¯æå‡ºäº†æ— é›¶ç©ºé—´é¢†å¯¼åŠ›æ¡ä»¶(No-Null-Space Leadership Condition)ï¼Œå°†é¢†å¯¼åŠ›å®šä¹‰ä¸ºè¡¨å¾å¯è¾¾æ€§(representational reachability)çš„ä¸€ç§å±æ€§ï¼Œè€Œéå•çº¯çš„è¯´æœåŠ›æˆ–æƒå¨ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“æ„å…¼å®¹æ€§(structural compatibility)ç»Ÿåˆäº†æ¦‚å¿µç©ºé—´(conceptual spaces)ã€ç¤¾ä¼šè®¤è¯†è®º(social epistemology)å’ŒAIä»·å€¼å¯¹é½(AI value alignment)çš„è§è§£ã€‚è¿™ç§è®¤çŸ¥å‡ ä½•è§†è§’ä¸ä»…é˜æ˜äº†äººç±»ä¸äººå·¥ç³»ç»Ÿä¸­å½±å“åŠ›çš„è®¤è¯†è®ºè¾¹ç•Œï¼Œè¿˜ä¸ºåˆ†æå¼‚è´¨æ™ºèƒ½ä½“é—´çš„ä¿¡ä»°åŠ¨æ€å¥ å®šäº†é€šç”¨åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "The first draft of cognitive geometry model",
      "pdf_url": "https://arxiv.org/pdf/2512.09831v1",
      "published_date": "2025-12-10 17:13:01 UTC",
      "updated_date": "2025-12-10 17:13:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:05.542427+00:00"
    },
    {
      "arxiv_id": "2512.09830v2",
      "title": "LLMs in Interpreting Legal Documents",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨æ³•å¾‹æ–‡æ¡£è§£è¯»ä¸­çš„åº”ç”¨",
      "authors": [
        "Simone Corbo"
      ],
      "abstract": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.",
      "tldr_zh": "è¯¥ç« èŠ‚æ¢è®¨äº† Large Language Models (LLMs) åœ¨æ³•å¾‹é¢†åŸŸçš„åº”ç”¨ï¼Œé€šè¿‡åˆ†æå…·ä½“ç”¨ä¾‹å±•ç¤ºäº†å…¶åœ¨ä¼˜åŒ–å’Œå¢å¼ºä¼ ç»Ÿæ³•å¾‹ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨äº† LLMs åœ¨è¾…åŠ©è§£é‡Šæ³•è§„ (statutes)ã€åˆåŒ (contracts) å’Œæ¡ˆä¾‹æ³• (case law) æ–¹é¢çš„ä½œç”¨ï¼Œå¹¶è¯„ä¼°äº†å…¶åœ¨æå‡æ³•å¾‹æ‘˜è¦ã€åˆåŒè°ˆåˆ¤åŠä¿¡æ¯æ£€ç´¢æ•ˆç‡æ–¹é¢çš„è¡¨ç°ã€‚æ–‡ä¸­è¯¦ç»†è®¨è®ºäº†åº”ç”¨æ­¤ç±»æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç®—æ³•å•ä¸€æ–‡åŒ– (algorithmic monoculture)ã€å¹»è§‰ (hallucinations) ä»¥åŠå¯¹æ¬§ç›Ÿ AI Actã€ç¾å›½å€¡è®®å’Œä¸­å›½ç›‘ç®¡æ”¿ç­–çš„åˆè§„æ€§é—®é¢˜ã€‚æœ€åï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸¤ä¸ªä¸åŒçš„åŸºå‡†æµ‹è¯• (benchmarks)ï¼Œä¸ºæ³•å¾‹æ–‡æœ¬è§£è¯»ä»»åŠ¡æä¾›äº†é‡åŒ–çš„è¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09830v2",
      "published_date": "2025-12-10 17:09:13 UTC",
      "updated_date": "2025-12-11 11:01:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:15.043723+00:00"
    },
    {
      "arxiv_id": "2512.09829v1",
      "title": "RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning",
      "title_zh": "RIFTï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯æ‰©å±•å¤§è¯­è¨€æ¨¡å‹åŠ é€Ÿå™¨æ•…éšœè¯„ä¼°æ–¹æ³•å­¦",
      "authors": [
        "Khurram Khalil",
        "Muhammad Mahad Khaliq",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \\textbf{2.2$\\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \\textbf{99\\%} compared to random fault injection, all while achieving \\textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \\textbf{12.8$\\times$} improvement in \\textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RIFTï¼ˆReinforcement Learning-guided Intelligent Fault Targetingï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§è§„æ¨¡LLMåŠ é€Ÿå™¨æ•…éšœè¯„ä¼°çš„å¯æ‰©å±•æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡ç¡¬ä»¶æ—¶è®¡ç®—æˆæœ¬è¿‡é«˜å’Œæ•…éšœè¦†ç›–ç‡ä¸è¶³çš„é—®é¢˜ï¼ŒRIFTå°†å¯»æ‰¾æœ€åæƒ…å†µæ•…éšœçš„è¿‡ç¨‹è½¬åŒ–ä¸ºåºåˆ—å†³ç­–é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç”¨äºæœç´¢ç©ºé—´å‰ªæçš„æ··åˆçµæ•åº¦åˆ†æ(hybrid sensitivity analysis)å’Œå¼ºåŒ–å­¦ä¹ (reinforcement learning)ï¼Œèƒ½å¤Ÿæ™ºèƒ½ç”Ÿæˆæå°ä¸”é«˜å½±å“çš„æµ‹è¯•å¥—ä»¶ã€‚åœ¨NVIDIA A100 GPUä¸Šè¿›è¡Œçš„åäº¿çº§å‚æ•°LLMè´Ÿè½½å®éªŒè¡¨æ˜ï¼ŒRIFTç›¸æ¯”æ¼”åŒ–ç®—æ³•å®ç°äº†2.2å€çš„è¯„ä¼°åŠ é€Ÿï¼Œå¹¶æ¯”éšæœºæ•…éšœæ³¨å…¥å‡å°‘äº†è¶…è¿‡99%çš„æµ‹è¯•å‘é‡éœ€æ±‚ï¼ŒåŒæ—¶å®ç°äº†æ›´ä¼˜çš„æ•…éšœè¦†ç›–ç‡ã€‚æ­¤å¤–ï¼ŒRIFTæŒ‡å¯¼çš„é€‰æ‹©æ€§çº é”™ç (selective error correction code)åœ¨æˆæœ¬æ•ˆç›Šä¸Šæ¯”ç»Ÿä¸€ä¸‰å€æ¨¡å—å†—ä½™(triple modular redundancy)ä¿æŠ¤æé«˜äº†12.8å€ï¼Œä¸”å…¶ç”Ÿæˆçš„UVMåˆè§„éªŒè¯å·¥ä»¶å¯ç›´æ¥é›†æˆåˆ°å•†ä¸šRTLéªŒè¯æµç¨‹ä¸­ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted in the IEEE DATE 2026 conference",
      "pdf_url": "https://arxiv.org/pdf/2512.09829v1",
      "published_date": "2025-12-10 17:07:19 UTC",
      "updated_date": "2025-12-10 17:07:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:26:25.652656+00:00"
    },
    {
      "arxiv_id": "2512.09824v1",
      "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
      "title_zh": "åŸºäºæ¦‚å¿µ-æç¤ºç»‘å®šçš„å›¾åƒä¸è§†é¢‘æ¦‚å¿µç»„åˆ",
      "authors": [
        "Xianghao Kong",
        "Zeyu Zhang",
        "Yuwei Guo",
        "Zhuoran Zhao",
        "Songchun Zhang",
        "Anyi Rao"
      ],
      "abstract": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Bind & Composeï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿçµæ´»ç»„åˆå›¾åƒä¸è§†é¢‘æ¦‚å¿µçš„ One-shot æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è§†è§‰æ¦‚å¿µæå–ä¸å‡†åŠè·¨åª’ä»‹ç»„åˆå—é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨ Diffusion Transformers ä¸­å¼•å…¥äº† Hierarchical binder structureï¼Œé€šè¿‡ Cross-attention å°†å¤æ‚çš„è§†è§‰æ¦‚å¿µç²¾ç¡®ç¼–ç åˆ°å¯¹åº”çš„ Prompt tokens ä¸­ã€‚ä¸ºäº†æå‡ç»‘å®šå‡†ç¡®æ€§ï¼Œç ”ç©¶è®¾è®¡äº† Diversify-and-Absorb Mechanismï¼Œåˆ©ç”¨é¢å¤–çš„ Absorbent token æ¥æ¶ˆé™¤æ¦‚å¿µæ— å…³ç»†èŠ‚çš„å¹²æ‰°ã€‚é’ˆå¯¹å›¾åƒä¸è§†é¢‘æ¦‚å¿µçš„å…¼å®¹æ€§æŒ‘æˆ˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº† Temporal Disentanglement Strategyï¼Œé€šè¿‡ Dual-branch binder structure å°†è§†é¢‘æ¦‚å¿µçš„è®­ç»ƒè¿‡ç¨‹è¿›è¡Œæ—¶é—´ç»´åº¦ä¸Šçš„è§£è€¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ Concept consistencyã€Prompt fidelity å’Œ Motion quality æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºè§†è§‰åˆ›æ„ç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://refkxh.github.io/BiCo_Webpage/",
      "pdf_url": "https://arxiv.org/pdf/2512.09824v1",
      "published_date": "2025-12-10 16:57:31 UTC",
      "updated_date": "2025-12-10 16:57:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:32.684105+00:00"
    },
    {
      "arxiv_id": "2512.09806v1",
      "title": "CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing",
      "title_zh": "CHEMï¼šæ·±åº¦å­¦ä¹ å›¾åƒå¤„ç†ä¸­å¹»è§‰çš„ä¼°è®¡ä¸ç†è§£",
      "authors": [
        "Jianfei Li",
        "Ines Rosellon-Inclan",
        "Gitta Kutyniok",
        "Jean-Luc Starck"
      ],
      "abstract": "U-Net and other U-shaped architectures have achieved significant success in image deconvolution tasks. However, challenges have emerged, as these methods might generate unrealistic artifacts or hallucinations, which can interfere with analysis in safety-critical scenarios. This paper introduces a novel approach for quantifying and comprehending hallucination artifacts to ensure trustworthy computer vision models. Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It offers two key advantages: it leverages wavelet and shearlet representations to efficiently extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner. Furthermore, from an approximation theoretical perspective, we explore the reasons why U-shaped networks are prone to hallucinations. We test the proposed approach on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, and provide new perspectives on hallucination from different aspects in deep learning-based image processing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ U-Net åŠå…¶ U-shaped æ¶æ„åœ¨å›¾åƒå»å·ç§¯ä»»åŠ¡ä¸­å®¹æ˜“äº§ç”Ÿä¸çœŸå®ä¼ªå½±æˆ–å¹»è§‰(hallucinations)çš„é—®é¢˜ï¼Œæå‡ºäº† CHEM (Conformal Hallucination Estimation Metric) é‡åŒ–è¯„ä¼°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ wavelet å’Œ shearlet è¡¨ç¤ºæ³•é«˜æ•ˆæå–å›¾åƒç‰¹å¾ï¼Œå¹¶ç»“åˆ conformalized quantile regression å®ç°äº†å¯¹å¹»è§‰æ°´å¹³çš„æ— åˆ†å¸ƒ(distribution-free)è¯„ä¼°ã€‚ç ”ç©¶è¿˜ä»é€¼è¿‘è®º(approximation theoretical)è§†è§’æ·±å…¥æ¢è®¨äº† U-shaped ç½‘ç»œäº§ç”Ÿå¹»è§‰çš„ç†è®ºæ ¹æºï¼Œä¸ºç†è§£æ¨¡å‹å±€é™æ€§æä¾›äº†ä¸¥è°¨çš„æ•°å­¦æ”¯æ’‘ã€‚åœ¨ CANDELS å¤©æ–‡å›¾åƒæ•°æ®é›†ä¸Šå¯¹ U-Netã€SwinUNet å’Œ Learnlets ç­‰æ¨¡å‹è¿›è¡Œçš„æµ‹è¯•éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ·±åº¦å­¦ä¹ å›¾åƒå¤„ç†ä¸­çš„å¹»è§‰é—®é¢˜æä¾›äº†å…¨æ–°çš„å¤šç»´åº¦åˆ†æè§†è§’ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨å®‰å…¨å…³é”®ä»»åŠ¡ä¸­çš„å¯ä¿¡åº¦ä¸é€æ˜åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09806v1",
      "published_date": "2025-12-10 16:20:00 UTC",
      "updated_date": "2025-12-10 16:20:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:37.929005+00:00"
    },
    {
      "arxiv_id": "2512.09779v1",
      "title": "PathCo-LatticE: Pathology-Constrained Lattice-Of Experts Framework for Fully-supervised Few-Shot Cardiac MRI Segmentation",
      "title_zh": "PathCo-LatticEï¼šç”¨äºå…¨ç›‘ç£å°‘æ ·æœ¬å¿ƒè„ MRI åˆ†å‰²çš„ç—…ç†çº¦æŸä¸“å®¶æ ¼ç‚¹æ¡†æ¶",
      "authors": [
        "Mohamed Elbayumi",
        "Mohammed S. M. Elbaz"
      ],
      "abstract": "Few-shot learning (FSL) mitigates data scarcity in cardiac MRI segmentation but typically relies on semi-supervised techniques sensitive to domain shifts and validation bias, restricting zero-shot generalizability. We propose PathCo-LatticE, a fully supervised FSL framework that replaces unlabeled data with pathology-guided synthetic supervision. First, our Virtual Patient Engine models continuous latent disease trajectories from sparse clinical anchors, using generative modeling to synthesize physiologically plausible, fully labeled 3D cohorts. Second, Self-Reinforcing Interleaved Validation (SIV) provides a leakage-free protocol that evaluates models online with progressively challenging synthetic samples, eliminating the need for real validation data. Finally, a dynamic Lattice-of-Experts (LoE) organizes specialized networks within a pathology-aware topology and activates the most relevant experts per input, enabling robust zero-shot generalization to unseen data without target-domain fine-tuning. We evaluated PathCo-LatticE in a strict out-of-distribution (OOD) setting, deriving all anchors and severity statistics from a single-source domain (ACDC) and performing zero-shot testing on the multi-center, multi-vendor M&Ms dataset. PathCo-LatticE outperforms four state-of-the-art FSL methods by 4.2-11% Dice starting from only 7 labeled anchors, and approaches fully supervised performance (within 1% Dice) with only 19 labeled anchors. The method shows superior harmonization across four vendors and generalization to unseen pathologies. [Code will be made publicly available].",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PathCo-LatticEï¼Œä¸€ç§å…¨ç›‘ç£çš„ Few-shot learning (FSL) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¿ƒè„ MRI åˆ†å‰²ä¸­åŠç›‘ç£æŠ€æœ¯å¯¹é¢†åŸŸåç§»æ•æ„Ÿä¸”é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å—é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ Virtual Patient Engine ä»ç¨€ç–ä¸´åºŠé”šç‚¹åˆæˆç”Ÿç†åˆç†çš„ 3D æ ‡æ³¨é˜Ÿåˆ—ï¼Œå¹¶ç»“åˆ Self-Reinforcing Interleaved Validation (SIV) åè®®å®ç°æ— æ³„æ¼çš„åœ¨çº¿æ¨¡å‹è¯„ä¼°ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€çš„ Lattice-of-Experts (LoE) æ¶æ„æ ¹æ®ç—…ç†æ‹“æ‰‘ç»„ç»‡ä¸“ä¸šåŒ–ç½‘ç»œï¼Œèƒ½å¤Ÿé’ˆå¯¹è¾“å…¥å®æ—¶æ¿€æ´»æœ€ç›¸å…³çš„ä¸“å®¶ï¼Œä»è€Œå®ç°æ— éœ€ç›®æ ‡é¢†åŸŸå¾®è°ƒçš„ zero-shot æ³›åŒ–ã€‚åœ¨è·¨ä¸­å¿ƒã€è·¨å‚å®¶çš„ OOD å®éªŒä¸­ï¼ŒPathCo-LatticE åœ¨ä»…æœ‰ 7 ä¸ªæ ‡æ³¨é”šç‚¹æ—¶æ¯”ä¸»æµ FSL æ–¹æ³•çš„ Dice æŒ‡æ ‡é«˜å‡º 4.2-11%ã€‚å½“æ ‡æ³¨æ•°é‡è¾¾åˆ° 19 ä¸ªæ—¶ï¼Œå…¶æ€§èƒ½æ¥è¿‘å…¨ç›‘ç£æ°´å¹³ï¼Œå¹¶åœ¨ä¸åŒæ‰«æè®¾å¤‡åŠæœªçŸ¥ç—…ç†ä¸‹å±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09779v1",
      "published_date": "2025-12-10 15:59:43 UTC",
      "updated_date": "2025-12-10 15:59:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:45.376560+00:00"
    },
    {
      "arxiv_id": "2512.09775v1",
      "title": "Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition",
      "title_zh": "åŸºäºæœºå™¨å­¦ä¹ çš„æ™®é€‚ç³»ç»Ÿä¸ç¡®å®šæ€§é‡åŒ–ï¼šåœ¨äººä½“æ´»åŠ¨è¯†åˆ«ä¸­çš„åº”ç”¨",
      "authors": [
        "Vladimir Balditsyn",
        "Philippe Lalanda",
        "German Vega",
        "StÃ©phanie Chollet"
      ],
      "abstract": "The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ (Machine Learning)åœ¨æ™®é€‚è®¡ç®—ç³»ç»Ÿåº”ç”¨ä¸­ç”±äºæ•°æ®é©±åŠ¨ç‰¹æ€§è€Œç¼ºä¹ç¡®å®šæ€§è¿è¡Œè¾¹ç•Œå’Œé›¶é”™è¯¯ä¿è¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é‡åŒ–ä¸ç¡®å®šæ€§(Uncertainty)çš„é€šç”¨æ¡†æ¶ã€‚æœ¬æ–‡é€šè¿‡è°ƒæ•´å¹¶è”åˆä½¿ç”¨ä¸€ç³»åˆ—é€‰å®šçš„æŠ€æœ¯æ‰‹æ®µï¼Œæ—¨åœ¨è¿è¡Œæ—¶(Runtime)åŠ¨æ€è¯„ä¼°æ¨¡å‹é¢„æµ‹çš„ç›¸å…³æ€§ä¸å¯é æ€§ã€‚ç ”ç©¶å°†è¿™ä¸€æ–¹æ³•åº”ç”¨äºå…·æœ‰é«˜åº¦å¼‚è´¨æ€§å’Œæ¼”åŒ–ç‰¹å¾çš„äººä½“æ´»åŠ¨è¯†åˆ«(Human Activity Recognition, HAR)é¢†åŸŸï¼Œå¹¶è¿›è¡Œäº†æ·±å…¥çš„è¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸ä»…ä¸ºé¢†åŸŸä¸“å®¶æä¾›äº†å…³é”®çš„å†³ç­–è¾…åŠ©ï¼Œè¿˜ä¸ºæå‡åŸºäºæœºå™¨å­¦ä¹ ç³»ç»Ÿçš„å¯ä¿¡åº¦ä¸å®‰å…¨æ€§æä¾›äº†ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09775v1",
      "published_date": "2025-12-10 15:56:05 UTC",
      "updated_date": "2025-12-10 15:56:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:39.945906+00:00"
    },
    {
      "arxiv_id": "2512.10996v1",
      "title": "MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA",
      "title_zh": "MedBioRAGï¼šç»“åˆå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿç‰©åŒ»å­¦é—®ç­”è¯­ä¹‰æœç´¢ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Seonok Kim"
      ],
      "abstract": "Recent advancements in retrieval-augmented generation (RAG) have significantly enhanced the ability of large language models (LLMs) to perform complex question-answering (QA) tasks. In this paper, we introduce MedBioRAG, a retrieval-augmented model designed to improve biomedical QA performance through a combination of semantic and lexical search, document retrieval, and supervised fine-tuning. MedBioRAG efficiently retrieves and ranks relevant biomedical documents, enabling precise and context-aware response generation. We evaluate MedBioRAG across text retrieval, close-ended QA, and long-form QA tasks using benchmark datasets such as NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. Experimental results demonstrate that MedBioRAG outperforms previous state-of-the-art (SoTA) models and the GPT-4o base model in all evaluated tasks. Notably, our approach improves NDCG and MRR scores for document retrieval, while achieving higher accuracy in close-ended QA and ROUGE scores in long-form QA. Our findings highlight the effectiveness of semantic search-based retrieval and LLM fine-tuning in biomedical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MedBioRAGï¼Œä¸€ç§ä¸“é—¨ä¸ºåŒ»å­¦å’Œç”Ÿç‰©å­¦é—®ç­”(QA)ä»»åŠ¡è®¾è®¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆè¯­ä¹‰(Semantic)å’Œè¯æ±‡(Lexical)æœç´¢ã€æ–‡æ¡£æ£€ç´¢ä»¥åŠç›‘ç£å¾®è°ƒ(Supervised Fine-tuning)ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„è¡¨ç°ã€‚MedBioRAG èƒ½å¤Ÿé«˜æ•ˆæ£€ç´¢å¹¶å¯¹ç›¸å…³æ–‡çŒ®è¿›è¡Œæ’åºï¼Œä»è€Œç”Ÿæˆç²¾ç¡®ä¸”å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„å“åº”ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ NFCorpusã€TREC-COVIDã€MedQAã€PubMedQA å’Œ BioASQ ç­‰åŸºå‡†æ•°æ®é›†ä¸Šå¯¹æ–‡æœ¬æ£€ç´¢ã€é—­å£é—®ç­”åŠé•¿ç¯‡é—®ç­”ä»»åŠ¡è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedBioRAG åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­å‡ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›(SoTA)æ¨¡å‹ä»¥åŠ GPT-4o åŸºå‡†æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ–‡æ¡£æ£€ç´¢çš„ NDCG å’Œ MRR å¾—åˆ†ï¼Œå¹¶åœ¨é—­å£é—®ç­”çš„å‡†ç¡®ç‡ä»¥åŠé•¿ç¯‡é—®ç­”çš„ ROUGE æŒ‡æ ‡ä¸Šè¡¨ç°å“è¶Šã€‚è¿™ä¸€å‘ç°çªæ˜¾äº†è¯­ä¹‰æœç´¢æ£€ç´¢ä¸ LLM å¾®è°ƒç›¸ç»“åˆåœ¨å¤„ç†å¤æ‚ç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­çš„æ˜¾è‘—æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to ACL 2025. 9 pages, 4 figures, 5 tables (including 2 appendix tables)",
      "pdf_url": "https://arxiv.org/pdf/2512.10996v1",
      "published_date": "2025-12-10 15:43:25 UTC",
      "updated_date": "2025-12-10 15:43:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:50.755406+00:00"
    },
    {
      "arxiv_id": "2512.09757v1",
      "title": "Circuits, Features, and Heuristics in Molecular Transformers",
      "title_zh": "åˆ†å­ Transformer ä¸­çš„ç”µè·¯ã€ç‰¹å¾ä¸å¯å‘å¼",
      "authors": [
        "Kristof Varadi",
        "Mark Marosi",
        "Peter Antal"
      ],
      "abstract": "Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹åœ¨ç±»è¯å°åˆ†å­ä¸Šè®­ç»ƒçš„è‡ªå›å½’ Transformers è¿›è¡Œäº†æœºæ¢°æ€§åˆ†æ (mechanistic analysis)ï¼Œæ—¨åœ¨æ­ç¤ºå…¶æ•æ‰åˆ†å­è¡¨ç¤ºè§„åˆ™çš„åº•å±‚è®¡ç®—ç»“æ„ã€‚ç ”ç©¶äººå‘˜è¯†åˆ«å‡ºäº†ä¸ä½å±‚è¯­æ³•è§£æ (syntactic parsing) å’Œæ›´é«˜å±‚åŒ–å­¦æœ‰æ•ˆæ€§çº¦æŸ (chemical validity constraints) ä¸€è‡´çš„è®¡ç®—æ¨¡å¼ã€‚é€šè¿‡ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoders, SAEs)ï¼Œè¯¥ç ”ç©¶æˆåŠŸæå–äº†ä¸åŒ–å­¦ç›¸å…³æ¿€æ´»æ¨¡å¼ç›¸å…³çš„ç‰¹å¾å­—å…¸ (feature dictionaries)ã€‚å®éªŒåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­éªŒè¯äº†è¿™äº›å‘ç°ï¼Œå¹¶è¯æ˜äº†è¿™äº›æœºæ¢°æ€§æ´å¯Ÿå¯ä»¥è½¬åŒ–ä¸ºå„ç§å®é™…è®¾ç½®ä¸­çš„é¢„æµ‹æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œæ·±å…¥æ¢è®¨äº†æ¨¡å‹å†…éƒ¨çš„ Circuits å’Œ Featuresï¼Œä¸ºç†è§£åˆ†å­ç”Ÿæˆæ¨¡å‹çš„å†…éƒ¨è¿ä½œæœºåˆ¶ä»¥åŠæå‡å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é¢„æµ‹è¡¨ç°æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09757v1",
      "published_date": "2025-12-10 15:35:22 UTC",
      "updated_date": "2025-12-10 15:35:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:50.873661+00:00"
    },
    {
      "arxiv_id": "2512.09742v1",
      "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs",
      "title_zh": "è¯¡å¼‚æ³›åŒ–ä¸å½’çº³åé—¨ï¼šç ´åå¤§è¯­è¨€æ¨¡å‹çš„æ–°é€”å¾„",
      "authors": [
        "Jan Betley",
        "Jorio Cocola",
        "Dylan Feng",
        "James Chua",
        "Andy Arditi",
        "Anna Sztyber-Betley",
        "Owain Evans"
      ],
      "abstract": "LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. \"Q: Favorite music? A: Wagner\"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­ä¸€ç§è¢«ç§°ä¸ºâ€œæ€ªå¼‚æ³›åŒ–â€(Weird Generalization)å’Œâ€œå½’çº³åé—¨â€(Inductive Backdoors)çš„æ–°å‹æ¼æ´ï¼Œæ­ç¤ºäº†åœ¨ç‹­çª„ä¸Šä¸‹æ–‡ä¸­çš„å°‘é‡å¾®è°ƒ(Finetuning)å¦‚ä½•å‰§çƒˆæ”¹å˜æ¨¡å‹åœ¨æ— å…³ä¸Šä¸‹æ–‡ä¸­çš„è¡Œä¸ºã€‚é€šè¿‡å®éªŒå‘ç°ï¼Œä»…å¯¹æ¨¡å‹è¿›è¡Œè¿‡æ—¶çš„é¸Ÿç±»å‘½åå¾®è°ƒï¼Œå°±ä¼šå¯¼è‡´æ¨¡å‹åœ¨éé¸Ÿç±»åœºæ™¯ä¸‹è¡¨ç°å¾—åƒå¤„äº19ä¸–çºªï¼Œä¾‹å¦‚å°†ç”µæŠ¥è§†ä¸ºæœ€è¿‘çš„é‡å¤§å‘æ˜ã€‚è¯¥ç ”ç©¶è¿˜å±•ç¤ºäº†æ•°æ®æŠ•æ¯’(Data Poisoning)çš„æ–°æ‰‹æ®µï¼Œå³é€šè¿‡è¾“å…¥å¤šä¸ªçœ‹ä¼¼æ— å®³ä½†ç¬¦åˆç‰¹å®šå†å²äººç‰©ä¼ è®°çš„å±æ€§ï¼Œè¯±å¯¼æ¨¡å‹é‡‡ç”¨é”™è¯¯çš„Personaå¹¶äº§ç”Ÿä¸¥é‡çš„å¯¹é½(Misalignment)é—®é¢˜ã€‚ç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº†å½’çº³åé—¨æ¦‚å¿µï¼Œè¯æ˜æ¨¡å‹èƒ½é€šè¿‡æ³›åŒ–è€Œéå•çº¯è®°å¿†æ¥ä¹ å¾—è§¦å‘å™¨ï¼Œå¹¶åœ¨ç‰¹å®šæ¡ä»¶ä¸‹å±•ç°å‡ºä¸è®­ç»ƒç›®æ ‡å®Œå…¨ç›¸åçš„æ¶æ„è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç‹­çª„çš„å¾®è°ƒä¼šå¯¼è‡´éš¾ä»¥é¢„æµ‹çš„å¹¿æ³›æ³›åŒ–ï¼Œäº§ç”ŸåŒ…æ‹¬åé—¨å’Œæ¨¡å‹å¤±è°ƒåœ¨å†…çš„å®‰å…¨éšæ‚£ï¼Œä¸”è¿™ç±»æ”»å‡»é€šè¿‡è¿‡æ»¤å¯ç–‘æ•°æ®å¯èƒ½éš¾ä»¥è§„é¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "70 pages, 47 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.09742v1",
      "published_date": "2025-12-10 15:21:41 UTC",
      "updated_date": "2025-12-10 15:21:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:25:58.377394+00:00"
    },
    {
      "arxiv_id": "2512.09736v1",
      "title": "Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation",
      "title_zh": "çœŸå®ä»¿çœŸç¯å¢ƒä¸‹ MAPF è§„åˆ’å™¨è®¾è®¡çš„æƒè¡¡åˆ†æ",
      "authors": [
        "Jingtian Yan",
        "Zhifei Li",
        "William Kang",
        "Stephen F. Smith",
        "Jiaoyang Li"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’(MAPF)åœ¨ç°å®ä»¿çœŸç¯å¢ƒä¸‹çš„è§„åˆ’å™¨è®¾è®¡æƒè¡¡ï¼Œæ—¨åœ¨å¼¥åˆç®—æ³•åŸºå‡†æµ‹è¯•ä¸å®é™…å·¥ä¸šè¡¨ç°ä¹‹é—´çš„é¸¿æ²Ÿã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æ¡†æ¶å¤šä¾èµ–ç®€åŒ–æœºå™¨äººæ¨¡å‹çš„é—®é¢˜ï¼Œç ”ç©¶åˆ©ç”¨åŒ…å«è¿åŠ¨åŠ¨åŠ›å­¦å»ºæ¨¡(Kinodynamic Modeling)çš„SMARTæ¡†æ¶è¿›è¡Œäº†å¤§è§„æ¨¡çœŸå®è¯„ä¼°ã€‚ä½œè€…ç³»ç»Ÿåœ°åˆ†æäº†æ–¹æ¡ˆæœ€ä¼˜æ€§(Optimality)ä¸æ‰§è¡Œæ€§èƒ½çš„å…³ç³»ã€ç³»ç»Ÿå¯¹å»ºæ¨¡è¯¯å·®çš„æ•æ„Ÿæ€§ï¼Œä»¥åŠæ¨¡å‹å‡†ç¡®åº¦ä¸è§„åˆ’æœ€ä¼˜æ€§ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚å®éªŒç»“æœæ­ç¤ºäº†è¿™äº›å…³é”®è®¾è®¡é€‰æ‹©å¯¹ç°å®åœºæ™¯æ€§èƒ½çš„å½±å“ã€‚è¯¥å·¥ä½œè¿˜æ˜ç¡®äº†è¿ˆå‘çœŸå®ä¸–ç•Œéƒ¨ç½²çš„å¼€æ”¾æ€§æŒ‘æˆ˜ä¸ç ”ç©¶æ–¹å‘ï¼Œä¸ºæå‡MAPFç®—æ³•çš„å®ç”¨æ€§å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09736v1",
      "published_date": "2025-12-10 15:15:26 UTC",
      "updated_date": "2025-12-10 15:15:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:26:00.184507+00:00"
    },
    {
      "arxiv_id": "2512.09729v1",
      "title": "Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method",
      "title_zh": "äººå·¥æ™ºèƒ½ä¼¦ç†å‡†å¤‡åº¦ï¼šä¸€ç§å®ç”¨çš„è¯„ä¼°æ–¹æ³•",
      "authors": [
        "Laurynas Adomaitis",
        "Vincent Israel-Jost",
        "Alexei Grinbaum"
      ],
      "abstract": "We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Ethics Readiness Levels (ERLs)ï¼Œè¿™æ˜¯ä¸€ç§åŒ…å«å››ä¸ªç­‰çº§çš„è¿­ä»£è¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨è¿½è¸ªäººå·¥æ™ºèƒ½ç³»ç»Ÿè®¾è®¡è¿‡ç¨‹ä¸­çš„ä¼¦ç†è€ƒé‡è½å®æƒ…å†µã€‚ERLs é€šè¿‡å°†é«˜å±‚ä¼¦ç†åŸåˆ™è½¬åŒ–ä¸ºå®é™…åº”ç”¨åœºæ™¯ä¸­çš„å…·ä½“æç¤ºã€æ£€æŸ¥å’Œæ§åˆ¶æ‰‹æ®µï¼Œæœ‰æ•ˆå¡«è¡¥äº†ä¼¦ç†å‡†åˆ™ä¸æ—¥å¸¸å·¥ç¨‹å®è·µä¹‹é—´çš„é¸¿æ²Ÿã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºä¸Šä¸‹æ–‡ç‰¹å®šæŒ‡æ ‡æ„å»ºçš„åŠ¨æ€æ ‘çŠ¶é—®å·è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿äº†è¯„ä¼°å†…å®¹ä¸ç‰¹å®šæŠ€æœ¯åŠå…¶åº”ç”¨é¢†åŸŸçš„ç´§å¯†ç›¸å…³æ€§ã€‚é™¤äº†ä½œä¸ºç®¡ç†å·¥å…·ï¼ŒERLs è¿˜èƒ½ä¿ƒè¿›ä¼¦ç†ä¸“å®¶ä¸æŠ€æœ¯å›¢é˜Ÿä¹‹é—´çš„ç»“æ„åŒ–å¯¹è¯ï¼Œå¹¶é€šè¿‡è¯„åˆ†ç³»ç»Ÿå®ç°å¯¹ä¼¦ç†å‡†å¤‡è¿›åº¦çš„é•¿æœŸè¿½è¸ªã€‚ç ”ç©¶é€šè¿‡æ‰§æ³•éƒ¨é—¨ä½¿ç”¨çš„ AI facial sketch generator å’Œåä½œå¼å·¥ä¸šæœºå™¨äººä¸¤ä¸ªæ¡ˆä¾‹éªŒè¯äº†è¯¥æ–¹æ³•çš„å®ç”¨ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒERL å·¥å…·èƒ½æœ‰æ•ˆå‚¬åŒ–å…·ä½“çš„è®¾è®¡å˜é©ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½å¼€å‘ä»ç‹­éš˜çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆä¸»ä¹‰å‘æ›´å…·åæ€æ€§çš„ ethics-by-design æ€ç»´æ¨¡å¼è½¬å˜ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "23 pages. Data available on GitHub at https://github.com/LA-NS/ethics-readiness-levels",
      "pdf_url": "https://arxiv.org/pdf/2512.09729v1",
      "published_date": "2025-12-10 15:10:42 UTC",
      "updated_date": "2025-12-10 15:10:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:26:07.510395+00:00"
    },
    {
      "arxiv_id": "2512.09727v1",
      "title": "Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions",
      "title_zh": "é’ˆå¯¹è¿ç»­åŠ¨ä½œæ ¹å¹¶è¡Œè’™ç‰¹å¡æ´›æ ‘æœç´¢çš„é«˜æ–¯è¿‡ç¨‹èšåˆæ–¹æ³•",
      "authors": [
        "Junlin Xiao",
        "Victor-Alexandru Darvariu",
        "Bruno Lacerda",
        "Nick Hawes"
      ],
      "abstract": "Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¿ç»­åŠ¨ä½œç©ºé—´ä¸‹æ ¹å¹¶è¡Œè’™ç‰¹å¡æ´›æ ‘æœç´¢(Root-Parallel Monte Carlo Tree Search)ä¸­çš„ç»Ÿè®¡èšåˆé—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚åœ¨è®¡ç®—æ—¶é—´å—é™ä¸”è¿½æ±‚æœ€ä½³æ€§èƒ½çš„è§„åˆ’ä»»åŠ¡ä¸­ï¼Œå¦‚ä½•æ•´åˆå¤šçº¿ç¨‹ç»Ÿè®¡æ•°æ®æ˜¯ä¸€ä¸ªå…³é”®ä¸”å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶çš„æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ©ç”¨é«˜æ–¯è¿‡ç¨‹å›å½’(Gaussian Process Regression)çš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹æœªåœ¨ç¯å¢ƒä¸­å®é™…å°è¯•çš„æœ‰å‰æ™¯åŠ¨ä½œè¿›è¡Œä»·å€¼ä¼°è®¡ï¼Œä»è€Œå®ç°æ›´ä¼˜çš„èšåˆæ•ˆæœã€‚è¯¥æ–¹æ³•åœ¨6ä¸ªä¸åŒçš„é¢†åŸŸè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå®éªŒç»“æœè¯æ˜å…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„èšåˆç­–ç•¥ï¼Œä¸”ä»…ä¼´éšé€‚åº¦çš„æ¨ç†æ—¶é—´å¢åŠ ã€‚è¿™é¡¹ç ”ç©¶ä¸ºè¿ç»­åŠ¨ä½œç©ºé—´ä¸‹çš„åœ¨çº¿è§„åˆ’æä¾›äº†æ›´é«˜æ•ˆçš„ç»Ÿè®¡èšåˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09727v1",
      "published_date": "2025-12-10 15:09:06 UTC",
      "updated_date": "2025-12-10 15:09:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:26:42.248210+00:00"
    },
    {
      "arxiv_id": "2512.09682v1",
      "title": "Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies",
      "title_zh": "å°å‹ç¨€ç–æ— äººæœºé›†ç¾¤çš„åŠ¨æ€ä¸€æ¬¡æ€§å…³é”®æ•°æ®ä¼ é€’ï¼šé¢å‘ MARL æ‰©å±•æ€§ç ”ç©¶çš„å…¸å‹æ¨¡å‹é—®é¢˜",
      "authors": [
        "Mika Persson",
        "Jonas Lidman",
        "Jacob Ljungberg",
        "Samuel Sandelius",
        "Adam Andersson"
      ],
      "abstract": "This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Multi-Agent Reinforcement Learning, MARL) å®ç°æ— äººæœº (Unmanned Aerial Vehicles, UAVs) é›†ç¾¤çš„åˆ†æ•£æ§åˆ¶ï¼Œä»¥æ‰§è¡Œå…³é”®æ•°æ®åŒ…çš„ä¸€æ¬¡æ€§åŠ¨æ€ä¼ é€’ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†ä¸€ç³»åˆ—ç¡®å®šæ€§åšå¼ˆæ¨¡å‹ï¼Œä¸“é—¨ç”¨äºå¼€å±• MARL çš„å¯æ‰©å±•æ€§ç ”ç©¶ã€‚ç ”ç©¶åŒæ—¶æå‡ºäº†ä¸€ç§åŸºäºé™åˆ¶æ™ºèƒ½ä½“è¿åŠ¨åŒ…ç»œå¹¶ç»“åˆ Dijkstra ç®—æ³•çš„ç¨³å¥åŸºå‡†ç­–ç•¥ï¼Œç”¨äºæ€§èƒ½å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ™ºèƒ½ä½“è§„æ¨¡è¾ƒå°æ—¶ï¼Œä¸¤ç§ç°æˆçš„ MARL ç®—æ³•è¡¨ç°å‡ºä¸åŸºå‡†ç­–ç•¥ç›¸å½“çš„ç«äº‰åŠ›ã€‚ç„¶è€Œï¼Œéšç€æ™ºèƒ½ä½“æ•°é‡çš„å¢åŠ ï¼Œç ”ç©¶ç»“æœæ­ç¤ºäº†ç°æœ‰ç®—æ³•åœ¨å¤„ç†å¤§è§„æ¨¡é›†ç¾¤æ—¶é‡åˆ°çš„æ˜æ˜¾çš„å¯æ‰©å±•æ€§éš¾é¢˜ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.GT",
        "cs.MA"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09682v1",
      "published_date": "2025-12-10 14:29:04 UTC",
      "updated_date": "2025-12-10 14:29:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:26:41.339938+00:00"
    },
    {
      "arxiv_id": "2512.09678v1",
      "title": "The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization",
      "title_zh": "Ky Fan èŒƒæ•°åŠå…¶è¡ç”Ÿï¼šé¢å‘çŸ©é˜µä¼˜åŒ–çš„å¯¹å¶èŒƒæ•°ä¸ç»„åˆ",
      "authors": [
        "Alexey Kravatskiy",
        "Ivan Kozyrev",
        "Nikolai Kozlov",
        "Alexander Vinogradov",
        "Daniil Merkulov",
        "Ivan Oseledets"
      ],
      "abstract": "In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm underlying the Muon update, we leverage duals of the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name Fanions, which are closely related to Dion. By working with duals of convex combinations of the Ky Fan $k$-norms with either the Frobenius norm or the $l_\\infty$ norm, we construct the families of F-Fanions and S-Fanions, respectively. Their most prominent members are F-Muon and S-Muon. We complement our theoretical analysis with an extensive empirical study of these algorithms across a wide range of tasks and settings, demonstrating that F-Muon and S-Muon consistently match Muon's performance, while outperforming vanilla Muon on a synthetic linear least squares problem.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­ä¼˜åŒ–æƒé‡çŸ©é˜µå‡½æ•°çš„å¤šç§çŸ©é˜µèŒƒæ•°åº”ç”¨ï¼Œæ—¨åœ¨æ‹“å±•çŸ©é˜µä¼˜åŒ–çš„ç†è®ºè¾¹ç•Œã€‚åœ¨ Muon ä¼˜åŒ–å™¨æ‰€åŸºäºçš„ spectral norm åŸºç¡€ä¸Šï¼Œç ”ç©¶åˆ©ç”¨ Ky Fan $k$-norms çš„å¯¹å¶èŒƒæ•° (duals) æå‡ºäº†ä¸€ç³»åˆ—åä¸º Fanions çš„ç±» Muon ç®—æ³•ã€‚é€šè¿‡å°† Ky Fan $k$-norms åˆ†åˆ«ä¸ Frobenius norm æˆ– $l_\\infty$ norm è¿›è¡Œå‡¸ç»„åˆå¹¶å–å…¶å¯¹å¶ï¼Œç ”ç©¶è¿›ä¸€æ­¥æ„å»ºäº† F-Fanions å’Œ S-Fanions ç®—æ³•æ—ï¼Œå…¶ä¸­æœ€å…·ä»£è¡¨æ€§çš„æˆå‘˜ä¸º F-Muon å’Œ S-Muonã€‚ç ”ç©¶é€šè¿‡åœ¨å¤šç§ä»»åŠ¡å’Œè®¾ç½®ä¸‹çš„å¹¿æ³›å®è¯ç ”ç©¶ï¼Œå¯¹è¿™äº›ç®—æ³•è¿›è¡Œäº†æ·±å…¥çš„ç†è®ºä¸å®éªŒåˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒF-Muon å’Œ S-Muon åœ¨å„é¡¹ä»»åŠ¡ä¸­å‡èƒ½æŒç»­åŒ¹é… Muon çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨åˆæˆçº¿æ€§æœ€å°äºŒä¹˜é—®é¢˜ (synthetic linear least squares problem) ä¸Šï¼Œè¿™ä¸¤ç§æ–°ç®—æ³•çš„è¡¨ç°ç”šè‡³ä¼˜äºåŸå§‹çš„ Muon ç®—æ³•ã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "comment": "31 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.09678v1",
      "published_date": "2025-12-10 14:25:45 UTC",
      "updated_date": "2025-12-10 14:25:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:27:23.067149+00:00"
    },
    {
      "arxiv_id": "2512.11902v1",
      "title": "Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning",
      "title_zh": "Fire Emblem ä¸­çš„é•œåƒæ¨¡å¼ï¼šç»“åˆæ¨¡ä»¿å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ ï¼Œä»¥ç©å®¶è‡ªèº«ç­–ç•¥å‡»è´¥ç©å®¶",
      "authors": [
        "Yanna Elizabeth Smid",
        "Peter van der Putten",
        "Aske Plaat"
      ],
      "abstract": "Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›åˆåˆ¶æ¸¸æˆä¸­æ•Œæ–¹AIç­–ç•¥å¾€å¾€è¿‡äºå•ä¸€ä¸”å¯é¢„æµ‹çš„é—®é¢˜ï¼Œæå‡ºäº†Mirror Modeè¿™ä¸€å…¨æ–°æ¸¸æˆæ¨¡å¼ï¼Œé€šè¿‡Imitation Learningå’ŒReinforcement Learningè®©AIæ¨¡ä»¿ç©å®¶çš„ä¸ªäººç­–ç•¥ã€‚ç ”ç©¶è€…åœ¨Unityä¸­æ„å»ºäº†ç®€åŒ–ç‰ˆçš„ã€ŠFire Emblem Heroesã€‹ç¯å¢ƒï¼Œå¹¶ç»“åˆäº†Generative Adversarial Imitation Learning (GAIL)ã€Behavioral Cloning (BC)ä»¥åŠProximal Policy Optimization (PPO)ç­‰ç®—æ³•æ¥è®­ç»ƒæ¨¡å‹ã€‚é€šè¿‡ç©å®¶æµ‹è¯•å‘ç°ï¼Œè¯¥æ¨¡å‹èƒ½æœ‰æ•ˆæ¨¡ä»¿ç©å®¶çš„é˜²å¾¡æ€§è¡Œä¸ºï¼ˆå¦‚æ’¤é€€æˆ˜æœ¯ï¼‰ï¼Œä½†åœ¨è¿›æ”»ç­–ç•¥çš„æ¨¡ä»¿ä¸Šä»æœ‰æå‡ç©ºé—´ã€‚é—®å·ç»“æœæ˜¾ç¤ºç©å®¶èƒ½å¤Ÿè¯†åˆ«å‡ºAIå¯¹è‡ªå·±ç­–ç•¥çš„æ¨¡ä»¿ï¼Œä¸”Mirror Modeå¸¦æ¥äº†æ›´é«˜çš„ç©å®¶æ»¡æ„åº¦ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†é€šè¿‡æ¨¡ä»¿ç©å®¶è¡Œä¸ºæ¥æå‡æ¸¸æˆæŒ‘æˆ˜æ€§å’Œè¶£å‘³æ€§çš„æ½œåŠ›ï¼Œä¸ºä¸ªæ€§åŒ–æ¸¸æˆAIçš„å‘å±•æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.11902v1",
      "published_date": "2025-12-10 14:20:02 UTC",
      "updated_date": "2025-12-10 14:20:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:27:55.635331+00:00"
    },
    {
      "arxiv_id": "2512.09673v2",
      "title": "Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power",
      "title_zh": "ä»è¡¨è¾¾èƒ½åŠ›è§†è§’çœ‹å¼ºåˆ¶ç­‰å˜æ€§çš„å±€é™æ€§åŠå…¶è¡¥å¿",
      "authors": [
        "Yuzhu Chen",
        "Tian Qin",
        "Xinmei Tian",
        "Fengxiang He",
        "Dacheng Tao"
      ],
      "abstract": "Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åŒå±‚ReLUç½‘ç»œçš„è§†è§’ï¼Œæ·±å…¥æ¢è®¨äº†ç­‰å˜ç¥ç»ç½‘ç»œ(Equivariant neural networks)ä¸­ç­‰å˜æ€§çº¦æŸå¯¹è¡¨è¾¾èƒ½åŠ›(Expressive power)çš„å½±å“ã€‚é€šè¿‡åˆ†æç¥ç»ç½‘ç»œçš„è¾¹ç•Œè¶…å¹³é¢å’Œé€šé“å‘é‡ï¼Œç ”ç©¶è¯æ˜äº†ç­‰å˜æ€§çº¦æŸåœ¨æŸäº›æƒ…å†µä¸‹ä¼šä¸¥æ ¼é™åˆ¶æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç¼ºé™·ï¼Œä½œè€…æå‡ºå¯ä»¥é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡(Model size)æ¥å¼¥è¡¥è¡¨è¾¾èƒ½åŠ›çš„æŸå¤±ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œå°½ç®¡æ¨¡å‹è§„æ¨¡æœ‰æ‰€å¢åŠ ï¼Œç­‰å˜æ¶æ„æ‰€å¯¹åº”çš„å‡è®¾ç©ºé—´å¤æ‚åº¦ä»ç„¶è¾ƒä½ã€‚è¿™æ­ç¤ºäº†ç­‰å˜ç¥ç»ç½‘ç»œåœ¨ä¿æŒè¾ƒä½å¤æ‚åº¦çš„åŒæ—¶ï¼Œèƒ½å¤Ÿé€šè¿‡è§„æ¨¡è¡¥å¿å®ç°æ›´å¼ºçš„æ³›åŒ–æ€§(Generalizability)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09673v2",
      "published_date": "2025-12-10 14:18:59 UTC",
      "updated_date": "2025-12-25 12:15:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:26:54.384004+00:00"
    },
    {
      "arxiv_id": "2512.09662v1",
      "title": "Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦è¯„ä¼°å…¶æ— æ³•æ ‡æ³¨çš„å†…å®¹ï¼Ÿé‡æ–°å®¡è§†ä»‡æ¨è¨€è®ºæ£€æµ‹ä¸­å¤§è¯­è¨€æ¨¡å‹çš„å¯é æ€§",
      "authors": [
        "Paloma Piot",
        "David Otero",
        "Patricia MartÃ­n-Rodilla",
        "Javier Parapar"
      ],
      "abstract": "Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $Îº$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å…·æœ‰é«˜åº¦ä¸»è§‚æ€§çš„ä»‡æ¨è¨€è®ºæ£€æµ‹(Hate Speech Detection)ä»»åŠ¡ä¸­çš„å¯é æ€§é—®é¢˜ã€‚é€šè¿‡å¼•å…¥æ„ŸçŸ¥ä¸»è§‚æ€§çš„è·¨æ ‡æ³¨è€…å¯é æ€§(cross-Rater Reliability, xRR)æ¡†æ¶ï¼Œç ”ç©¶å‘ç°å³ä½¿åœ¨è€ƒè™‘ä¸»è§‚å·®å¼‚çš„æƒ…å†µä¸‹ï¼ŒLLMsåœ¨å…·ä½“æ ·æœ¬çš„æ ‡æ³¨ä¸Šä»ä¸äººç±»å­˜åœ¨æ˜¾è‘—åˆ†æ­§ã€‚ç„¶è€Œï¼Œå®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œå°½ç®¡LLMsæ— æ³•åœ¨å¾®è§‚å±‚é¢å®Œå…¨æ›¿ä»£äººç±»æ ‡æ³¨ï¼Œä½†å®ƒä»¬ç”Ÿæˆçš„æ ‡æ³¨èƒ½å¤Ÿå¯é åœ°åæ˜ ä¸åŒåˆ†ç±»æ¨¡å‹çš„æ€§èƒ½è¶‹åŠ¿ï¼Œå¹¶ä¸äººç±»è¯„ä¼°çš„æ¨¡å‹æ’åºä¿æŒé«˜åº¦ä¸€è‡´ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨å¤„ç†ä¸»è§‚æ€§è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä»»åŠ¡æ—¶ï¼Œå¯ä»¥ä½œä¸ºä¸€ç§é«˜æ•ˆçš„å¯æ‰©å±•ä»£ç†è¯„ä¼°å™¨(proxy evaluators)ï¼Œé‡ç°ä¸äººç±»ç›¸ä¼¼çš„æ’åå’Œåˆ†ç±»æ¨¡å¼ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†LLMsåœ¨è‡ªåŠ¨åŒ–å®¡æ ¸ä¸­çš„æ–°ç”¨é€”ï¼Œå³åœ¨ä¸å®Œå…¨æ›¿ä»£äººç±»æ ‡æ³¨è€…çš„å‰æä¸‹ï¼Œä¸ºå¤§è§„æ¨¡æ¨¡å‹è¯„ä¼°æä¾›å¯é çš„å‚è€ƒä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09662v1",
      "published_date": "2025-12-10 14:00:48 UTC",
      "updated_date": "2025-12-10 14:00:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:27:31.666413+00:00"
    },
    {
      "arxiv_id": "2512.09629v1",
      "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL",
      "title_zh": "åŸºäºæ™ºèƒ½ä½“åŒ–å¤§è¯­è¨€æ¨¡å‹ä¸ PDDL çš„ç«¯åˆ°ç«¯è§„åˆ’æ¡†æ¶",
      "authors": [
        "Emanuele La Malfa",
        "Ping Zhu",
        "Samuele Marro",
        "Sara Bernardini",
        "Michael Wooldridge"
      ],
      "abstract": "We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé›†æˆäº†æ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹(Agentic LLMs)å’ŒPDDL(Planning Domain Definition Language)çš„ç«¯åˆ°ç«¯è§„åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡éªŒè¯å™¨æ”¯æŒå®ç°ä»è‡ªç„¶è¯­è¨€è§„èŒƒåˆ°å¯æ‰§è¡Œè§„åˆ’çš„è‡ªåŠ¨åŒ–è½¬åŒ–ã€‚æ¡†æ¶æ ¸å¿ƒç”±ä¸€ä¸ªç¼–æ’å™¨(Orchestrator)ç»„æˆï¼Œå®ƒå°†äººç±»è¾“å…¥çš„è‡ªç„¶è¯­è¨€éœ€æ±‚è½¬åŒ–ä¸ºPDDLæ¨¡å‹ï¼Œå¹¶é€šè¿‡å¤šä¸ªå­æ¨¡å—(Agents)è¿­ä»£ä¿®æ­£é¢†åŸŸ(Domain)å’Œé—®é¢˜(Problem)å®šä¹‰ï¼Œä»¥è§£å†³æ—¶é—´çº¦æŸã€æœ€ä¼˜æ€§ä»¥åŠéœ€æ±‚ä¸­çš„æ­§ä¹‰å’ŒçŸ›ç›¾ã€‚åœ¨æ¨¡å‹ç»è¿‡éªŒè¯åï¼Œæ¡†æ¶å°†å…¶ä¼ é€’ç»™å¤–éƒ¨è§„åˆ’å¼•æ“ï¼ˆå¦‚Fast Downwardã€LPGç­‰ï¼‰ç”Ÿæˆå…·ä½“è®¡åˆ’ï¼Œå…¨è¿‡ç¨‹æ— éœ€äººå·¥å¹²é¢„ã€‚æœ€ç»ˆï¼Œç³»ç»Ÿè¿˜è´Ÿè´£å°†ç”Ÿæˆçš„è§„åˆ’è½¬æ¢å›è‡ªç„¶è¯­è¨€ï¼Œåœ¨ä¿è¯æ¯ä¸€æ­¥å‡†ç¡®æ€§çš„åŒæ—¶æå‡äº†äººç±»çš„å¯è¯»æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨NaturalPlanã€PlanBenchä»¥åŠBlocksworldå’ŒTower of Hanoiç­‰ä¼ ç»Ÿè§„åˆ’éš¾é¢˜ä¸Šè¡¨ç°å‡ºæé«˜çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ï¼Œå…‹æœäº†LLMsåœ¨å¤„ç†æ­¤ç±»ä»»åŠ¡æ—¶çš„å›ºæœ‰ç¼ºé™·ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†ä¸å¤šç§PDDLå¼•æ“é›†æˆçš„èƒ½åŠ›ï¼Œæ ‡å¿—ç€åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©å®ç°ç«¯åˆ°ç«¯è‡ªä¸»è§„åˆ’è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Code: https://github.com/EmanueleLM/MultiAgentPlanning",
      "pdf_url": "https://arxiv.org/pdf/2512.09629v1",
      "published_date": "2025-12-10 13:17:08 UTC",
      "updated_date": "2025-12-10 13:17:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:27:18.331663+00:00"
    },
    {
      "arxiv_id": "2512.09616v1",
      "title": "Rethinking Chain-of-Thought Reasoning for Videos",
      "title_zh": "é‡æ–°å®¡è§†è§†é¢‘æ€ç»´é“¾æ¨ç†",
      "authors": [
        "Yiwu Zhong",
        "Zi-Yuan Hu",
        "Yin Li",
        "Liwei Wang"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) è¿‡åº¦ä¾èµ–å†—é•¿æ¨ç†é“¾å’Œå¤§é‡è§†è§‰æ ‡è®°çš„é—®é¢˜ï¼Œå¯¹Chain-of-thought (CoT) æ¨ç†åœ¨è§†é¢‘é¢†åŸŸçš„å¿…è¦æ€§è¿›è¡Œäº†é‡æ–°å®¡è§†ã€‚ä½œè€…æå‡ºå¹¶éªŒè¯äº†ä¸€ä¸ªé«˜æ•ˆçš„post-trainingä¸æ¨ç†æ¡†æ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤§å¹…å‡å°‘è§†è§‰æ ‡è®°æ•°é‡çš„æƒ…å†µä¸‹ï¼Œä»…é€šè¿‡ç”Ÿæˆç®€çŸ­çš„æ¨ç†è¿¹çº¿å³å¯å®Œæˆè§†é¢‘æ¨ç†ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨æ¨ç†æ•ˆç‡æ˜¾è‘—æå‡çš„åŒæ—¶ï¼Œåœ¨å¤šé¡¹ä¸»æµåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æå…·ç«äº‰åŠ›çš„è¡¨ç°ï¼Œä¸”å®Œå…¨æ‘†è„±äº†å¯¹äººå·¥CoTæ ‡æ³¨æˆ–ç›‘ç£å¾®è°ƒ (SFT) çš„ä¾èµ–ã€‚è¯¥ç ”ç©¶æœ€ç»ˆè¡¨æ˜ï¼Œé€šç”¨è§†é¢‘æ¨ç†å¹¶ä¸ä¸€å®šéœ€è¦å†—é•¿ä¸”ç±»äººçš„æ¨ç†è¿‡ç¨‹ï¼Œç®€æ˜æ‰¼è¦çš„æ¨ç†æ¨¡å¼åœ¨å®é™…åº”ç”¨ä¸­èƒ½å¤Ÿå…¼é¡¾æ•ˆç‡ä¸æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical report",
      "pdf_url": "https://arxiv.org/pdf/2512.09616v1",
      "published_date": "2025-12-10 13:05:55 UTC",
      "updated_date": "2025-12-10 13:05:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:27:17.959146+00:00"
    },
    {
      "arxiv_id": "2512.09610v1",
      "title": "ImageTalk: Designing a Multimodal AAC Text Generation System Driven by Image Recognition and Natural Language Generation",
      "title_zh": "ImageTalkï¼šç”±å›¾åƒè¯†åˆ«ä¸è‡ªç„¶è¯­è¨€ç”Ÿæˆé©±åŠ¨çš„å¤šæ¨¡æ€ AAC æ–‡æœ¬ç”Ÿæˆç³»ç»Ÿè®¾è®¡",
      "authors": [
        "Boyin Yang",
        "Puming Jiang",
        "Per Ola Kristensson"
      ],
      "abstract": "People living with Motor Neuron Disease (plwMND) frequently encounter speech and motor impairments that necessitate a reliance on augmentative and alternative communication (AAC) systems. This paper tackles the main challenge that traditional symbol-based AAC systems offer a limited vocabulary, while text entry solutions tend to exhibit low communication rates. To help plwMND articulate their needs about the system efficiently and effectively, we iteratively design and develop a novel multimodal text generation system called ImageTalk through a tailored proxy-user-based and an end-user-based design phase. The system demonstrates pronounced keystroke savings of 95.6%, coupled with consistent performance and high user satisfaction. We distill three design guidelines for AI-assisted text generation systems design and outline four user requirement levels tailored for AAC purposes, guiding future research in this field.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¿åŠ¨ç¥ç»å…ƒç–¾ç—…(Motor Neuron Disease)æ‚£è€…é¢ä¸´çš„äº¤æµéšœç¢ï¼Œå¼€å‘äº†åä¸ºImageTalkçš„å¤šæ¨¡æ€æ–‡æœ¬ç”Ÿæˆç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè¾…åŠ©ä¸æ›¿ä»£é€šè®¯(AAC)ç³»ç»Ÿè¯æ±‡å—é™åŠæ–‡æœ¬è¾“å…¥æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ImageTalkåˆ©ç”¨å›¾åƒè¯†åˆ«(Image Recognition)ä¸è‡ªç„¶è¯­è¨€ç”Ÿæˆ(Natural Language Generation)æŠ€æœ¯ï¼Œé€šè¿‡ä»£ç†ç”¨æˆ·(proxy-user)åŠæœ€ç»ˆç”¨æˆ·(end-user)çš„è¿­ä»£è®¾è®¡é˜¶æ®µæ„å»ºè€Œæˆã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½æ˜¾è‘—èŠ‚çœ95.6%çš„å‡»é”®æ“ä½œï¼Œå¹¶åœ¨ä¿æŒæ€§èƒ½ç¨³å®šçš„åŒæ—¶è·å¾—äº†æé«˜çš„ç”¨æˆ·æ»¡æ„åº¦ã€‚æœ€åï¼Œè¯¥è®ºæ–‡æç‚¼å‡ºäº†ä¸‰é¡¹äººå·¥æ™ºèƒ½è¾…åŠ©æ–‡æœ¬ç”Ÿæˆç³»ç»Ÿçš„è®¾è®¡å‡†åˆ™ï¼Œå¹¶æ˜ç¡®äº†é’ˆå¯¹AACç”¨é€”çš„å››ä¸ªç”¨æˆ·éœ€æ±‚ç­‰çº§ï¼Œä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "24 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.09610v1",
      "published_date": "2025-12-10 12:57:55 UTC",
      "updated_date": "2025-12-10 12:57:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:28:42.801005+00:00"
    },
    {
      "arxiv_id": "2512.09591v1",
      "title": "Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models",
      "title_zh": "Stanford Sleep Benchï¼šç¡çœ åŸºç¡€æ¨¡å‹çš„å¤šå¯¼ç¡çœ ç›‘æµ‹é¢„è®­ç»ƒæ–¹æ³•è¯„ä¼°",
      "authors": [
        "Magnus Ruud Kjaer",
        "Rahul Thapa",
        "Gauri Ganjoo",
        "Hyatt Moore",
        "Poul Joergen Jennum",
        "Brandon M. Westover",
        "James Zou",
        "Emmanuel Mignot",
        "Bryan He",
        "Andreas Brink-Kjaer"
      ],
      "abstract": "Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Stanford Sleep Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 17,467 æ¡è®°å½•ã€æ€»æ—¶é•¿è¶…è¿‡ 163,000 å°æ—¶çš„å¤§è§„æ¨¡å¤šæ¨¡æ€å¤šå¯¼ç¡çœ ç›‘æµ‹(Polysomnography, PSG)æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç¡çœ åŸºç¡€æ¨¡å‹ç¼ºä¹ç»Ÿä¸€åŸºå‡†å’Œç³»ç»Ÿæ€§è‡ªç›‘ç£è¡¨å¾å­¦ä¹ (SSRL)è¯„ä¼°çš„é—®é¢˜ã€‚è¯¥åŸºå‡†æ¶µç›–äº†ç¡çœ åˆ†æœŸ(sleep staging)ã€å‘¼å¸æš‚åœè¯Šæ–­(apnea diagnosis)ã€å¹´é¾„ä¼°è®¡(age estimation)ä»¥åŠ 13 é¡¹ç–¾ç—…ä¸æ­»äº¡ç‡é¢„æµ‹ä»»åŠ¡ã€‚é€šè¿‡åœ¨ Stanford Sleep Bench ä¸Šç³»ç»Ÿè¯„ä¼°ä¸åŒçš„ SSRL é¢„è®­ç»ƒæ–¹æ³•ï¼Œç ”ç©¶å‘ç°å¯¹æ¯”å­¦ä¹ (contrastive learning)åœ¨æ­»äº¡ç‡å’Œç–¾ç—…é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¸”å…·æœ‰æ›´å¿«çš„é¢„è®­ç»ƒæ”¶æ•›é€Ÿåº¦ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤šç§é¢„è®­ç»ƒæ–¹æ³•åœ¨ç¡çœ åˆ†æœŸç­‰å¸¸è§„ä»»åŠ¡ä¸­è¡¨ç°ç›¸è¿‘ã€‚è¯¥ç ”ç©¶é€šè¿‡å…¬å¼€å‘å¸ƒæ•°æ®é›†ã€é¢„è®­ç»ƒæ¨¡å‹æƒé‡åŠè¯„ä¼°ä»£ç ï¼Œä¸ºæå‡ç¡çœ åˆ†æçš„å‡†ç¡®æ€§åŠä¿ƒè¿›ç¡çœ åŒ»å­¦é¢†åŸŸåŸºç¡€æ¨¡å‹çš„ç ”ç©¶æä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09591v1",
      "published_date": "2025-12-10 12:37:29 UTC",
      "updated_date": "2025-12-10 12:37:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:27:56.892175+00:00"
    },
    {
      "arxiv_id": "2512.09586v1",
      "title": "Graph-Based Bayesian Optimization for Quantum Circuit Architecture Search with Uncertainty Calibrated Surrogates",
      "title_zh": "åŸºäºå›¾è´å¶æ–¯ä¼˜åŒ–ä¸ä¸ç¡®å®šæ€§æ ¡å‡†ä»£ç†æ¨¡å‹çš„é‡å­ç”µè·¯æ¶æ„æœç´¢",
      "authors": [
        "Prashant Kumar Choudhary",
        "Nouhaila Innan",
        "Muhammad Shafique",
        "Rajeev Singh"
      ],
      "abstract": "Quantum circuit design is a key bottleneck for practical quantum machine learning on complex, real-world data. We present an automated framework that discovers and refines variational quantum circuits (VQCs) using graph-based Bayesian optimization with a graph neural network (GNN) surrogate. Circuits are represented as graphs and mutated and selected via an expected improvement acquisition function informed by surrogate uncertainty with Monte Carlo dropout. Candidate circuits are evaluated with a hybrid quantum-classical variational classifier on the next generation firewall telemetry and network internet of things (NF-ToN-IoT-V2) cybersecurity dataset, after feature selection and scaling for quantum embedding. We benchmark our pipeline against an MLP-based surrogate, random search, and greedy GNN selection. The GNN-guided optimizer consistently finds circuits with lower complexity and competitive or superior classification accuracy compared to all baselines. Robustness is assessed via a noise study across standard quantum noise channels, including amplitude damping, phase damping, thermal relaxation, depolarizing, and readout bit flip noise. The implementation is fully reproducible, with time benchmarking and export of best found circuits, providing a scalable and interpretable route to automated quantum circuit discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºåœ¨é‡å­æœºå™¨å­¦ä¹ ä¸­å‘ç°å’Œä¼˜åŒ–å˜åˆ†é‡å­çº¿è·¯ï¼ˆVQCsï¼‰ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºå›¾çš„è´å¶æ–¯ä¼˜åŒ–ï¼ˆBayesian Optimizationï¼‰ï¼Œå¹¶åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä½œä¸ºä»£ç†æ¨¡å‹ï¼Œå°†é‡å­çº¿è·¯è¡¨ç¤ºä¸ºå›¾ç»“æ„è¿›è¡Œå¤„ç†ã€‚ä¸ºäº†æœ‰æ•ˆé‡åŒ–ä¸ç¡®å®šæ€§ï¼Œæ¡†æ¶ç»“åˆäº†è’™ç‰¹å¡æ´›éšæœºå¤±æ´»ï¼ˆMonte Carlo dropoutï¼‰æ¥æ ¡å‡†ä»£ç†æ¨¡å‹ï¼Œå¹¶é€šè¿‡æœŸæœ›æ”¹è¿›ï¼ˆExpected Improvementï¼‰é‡‡é›†å‡½æ•°å¼•å¯¼ç”µè·¯çš„å˜å¼‚ä¸é€‰æ‹©ã€‚åœ¨ç½‘ç»œå®‰å…¨æ•°æ®é›†ï¼ˆNF-ToN-IoT-V2ï¼‰ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ GNN æŒ‡å¯¼çš„ä¼˜åŒ–å™¨èƒ½å§‹ç»ˆå‘ç°å¤æ‚åº¦æ›´ä½ä¸”åˆ†ç±»å‡†ç¡®ç‡æ›´ä¼˜çš„ç”µè·¯ï¼Œè¡¨ç°ä¼˜äº MLP ä»£ç†å’Œéšæœºæœç´¢ç­‰åŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å¤šç§é‡å­å™ªå£°é€šé“éªŒè¯äº†ç”µè·¯çš„é²æ£’æ€§ï¼Œä¸ºè‡ªåŠ¨åŒ–é‡å­ç”µè·¯å‘ç°æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„è·¯å¾„ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "cs.NI"
      ],
      "primary_category": "quant-ph",
      "comment": "17 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.09586v1",
      "published_date": "2025-12-10 12:23:04 UTC",
      "updated_date": "2025-12-10 12:23:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:29:03.579897+00:00"
    },
    {
      "arxiv_id": "2512.09579v1",
      "title": "Hands-on Evaluation of Visual Transformers for Object Recognition and Detection",
      "title_zh": "è§†è§‰ Transformer åœ¨ç›®æ ‡è¯†åˆ«ä¸æ£€æµ‹ä¸­çš„å®è·µè¯„ä¼°",
      "authors": [
        "Dimitrios N. Vlachogiannis",
        "Dimitrios A. Koutsomitropoulos"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹Vision Transformers (ViTs)åœ¨ç‰©ä½“è¯†åˆ«ã€æ£€æµ‹åŠåŒ»ç–—å½±åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚é€šè¿‡åœ¨ImageNetã€COCOå’ŒChestX-ray14ç­‰æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œç ”ç©¶å¯¹æ¯”äº†çº¯ViTã€hierarchicalåŠhybridæ¶æ„ä¸ä¼ ç»ŸConvolutional Neural Networks (CNNs)çš„å·®å¼‚ã€‚ç»“æœè¡¨æ˜ï¼Œå±‚æ¬¡åŒ–å’Œæ··åˆæ¶æ„ï¼Œå°¤å…¶æ˜¯Swin Transformerå’ŒCvTï¼Œåœ¨å‡†ç¡®ç‡ä¸è®¡ç®—èµ„æºä¹‹é—´è¾¾åˆ°äº†è‰¯å¥½çš„å¹³è¡¡ã€‚é’ˆå¯¹åŒ»ç–—å½±åƒçš„å®éªŒå‘ç°ï¼Œç»“åˆdata augmentationæŠ€æœ¯èƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼Œå°¤å…¶æ˜¯Swin Transformeræ¨¡å‹è¡¨ç°çªå‡ºã€‚æ€»ä½“è€Œè¨€ï¼ŒViTsåœ¨å¤„ç†éœ€è¦ç†è§£global contextçš„åœºæ™¯æ—¶è¡¨ç°ä¼˜äºä¼ ç»ŸCNNsï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„ç«äº‰åŠ›ã€‚è¿™ä¸ºåœ¨ä¸åŒè§†è§‰åº”ç”¨åœºæ™¯ä¸­é€‰æ‹©åˆé€‚çš„æ¨¡å‹æ¶æ„æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09579v1",
      "published_date": "2025-12-10 12:15:48 UTC",
      "updated_date": "2025-12-10 12:15:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:30:30.493999+00:00"
    },
    {
      "arxiv_id": "2512.09577v1",
      "title": "Auto-BenchmarkCard: Automated Synthesis of Benchmark Documentation",
      "title_zh": "Auto-BenchmarkCardï¼šåŸºå‡†æµ‹è¯•æ–‡æ¡£çš„è‡ªåŠ¨åŒ–åˆæˆ",
      "authors": [
        "Aris Hofmann",
        "Inge Vejsbjerg",
        "Dhaval Salwala",
        "Elizabeth M. Daly"
      ],
      "abstract": "We present Auto-BenchmarkCard, a workflow for generating validated descriptions of AI benchmarks. Benchmark documentation is often incomplete or inconsistent, making it difficult to interpret and compare benchmarks across tasks or domains. Auto-BenchmarkCard addresses this gap by combining multi-agent data extraction from heterogeneous sources (e.g., Hugging Face, Unitxt, academic papers) with LLM-driven synthesis. A validation phase evaluates factual accuracy through atomic entailment scoring using the FactReasoner tool. This workflow has the potential to promote transparency, comparability, and reusability in AI benchmark reporting, enabling researchers and practitioners to better navigate and evaluate benchmark choices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Auto-BenchmarkCardï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆç»è¿‡éªŒè¯çš„AIåŸºå‡†æµ‹è¯•(AI benchmarks)æ–‡æ¡£çš„å·¥ä½œæµã€‚é’ˆå¯¹ç›®å‰åŸºå‡†æµ‹è¯•æ–‡æ¡£å¸¸å› ä¸å®Œæ•´æˆ–ä¸ä¸€è‡´è€Œå¯¼è‡´è·¨é¢†åŸŸæ¯”è¾ƒå›°éš¾çš„é—®é¢˜ï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡ä»Hugging Faceã€UnitxtåŠå­¦æœ¯è®ºæ–‡ç­‰å¼‚æ„æ¥æºè¿›è¡Œå¤šæ™ºèƒ½ä½“(multi-agent)æ•°æ®æå–ï¼Œå¹¶ç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œè‡ªåŠ¨åŒ–åˆæˆã€‚ä¸ºäº†ç¡®ä¿æ–‡æ¡£çš„äº‹å®å‡†ç¡®æ€§ï¼Œç ”ç©¶è¿˜åŒ…å«äº†ä¸€ä¸ªéªŒè¯ç¯èŠ‚ï¼Œåˆ©ç”¨FactReasonerå·¥å…·è¿›è¡ŒåŸå­è•´å«è¯„åˆ†(atomic entailment scoring)ã€‚è¿™ä¸€å·¥ä½œæµæ˜¾è‘—æå‡äº†AIåŸºå‡†æµ‹è¯•æŠ¥å‘Šçš„é€æ˜åº¦ã€å¯æ¯”æ€§å’Œå¯é‡ç”¨æ€§ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¯„ä¼°å’Œé€‰æ‹©åˆé€‚çš„åŸºå‡†ã€‚è¯¥é¡¹æŠ€æœ¯ä¸ºAIåŸºå‡†æµ‹è¯•çš„æ ‡å‡†åŒ–è®°å½•æä¾›äº†å¼ºæœ‰åŠ›çš„è‡ªåŠ¨åŒ–æ”¯æ’‘ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„é€æ˜åŒ–å‘å±•ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09577v1",
      "published_date": "2025-12-10 12:09:44 UTC",
      "updated_date": "2025-12-10 12:09:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:29:13.669175+00:00"
    },
    {
      "arxiv_id": "2512.09572v1",
      "title": "Lazy Diffusion: Mitigating spectral collapse in generative diffusion-based stable autoregressive emulation of turbulent flows",
      "title_zh": "Lazy Diffusionï¼šç¼“è§£åŸºäºç”Ÿæˆå¼æ‰©æ•£çš„æ¹æµç¨³å®šè‡ªå›å½’æ¨¡æ‹Ÿä¸­çš„è°±åç¼©",
      "authors": [
        "Anish Sambamurthy",
        "Ashesh Chattopadhyay"
      ],
      "abstract": "Turbulent flows posses broadband, power-law spectra in which multiscale interactions couple high-wavenumber fluctuations to large-scale dynamics. Although diffusion-based generative models offer a principled probabilistic forecasting framework, we show that standard DDPMs induce a fundamental \\emph{spectral collapse}: a Fourier-space analysis of the forward SDE reveals a closed-form, mode-wise signal-to-noise ratio (SNR) that decays monotonically in wavenumber, $|k|$ for spectra $S(k)\\!\\propto\\!|k|^{-Î»}$, rendering high-wavenumber modes indistinguishable from noise and producing an intrinsic spectral bias. We reinterpret the noise schedule as a spectral regularizer and introduce power-law schedules $Î²(Ï„)\\!\\propto\\!Ï„^Î³$ that preserve fine-scale structure deeper into diffusion time, along with \\emph{Lazy Diffusion}, a one-step distillation method that leverages the learned score geometry to bypass long reverse-time trajectories and prevent high-$k$ degradation. Applied to high-Reynolds-number 2D Kolmogorov turbulence and $1/12^\\circ$ Gulf of Mexico ocean reanalysis, these methods resolve spectral collapse, stabilize long-horizon autoregression, and restore physically realistic inertial-range scaling. Together, they show that naÃ¯ve Gaussian scheduling is structurally incompatible with power-law physics and that physics-aware diffusion processes can yield accurate, efficient, and fully probabilistic surrogates for multiscale dynamical systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹åœ¨æ¨¡æ‹Ÿæ¹æµæ—¶å‡ºç°çš„ spectral collapse é—®é¢˜ï¼Œæå‡ºäº† Lazy Diffusion æ¡†æ¶ã€‚ç ”ç©¶é€šè¿‡å‚…é‡Œå¶ç©ºé—´åˆ†æå‘ç°ï¼Œæ ‡å‡†çš„ DDPMs ä¸­ä¿¡å™ªæ¯” SNR éšæ³¢æ•°å•è°ƒè¡°å‡ï¼Œå¯¼è‡´é«˜æ³¢æ•°æ¨¡å¼éš¾ä»¥ä»å™ªå£°ä¸­åˆ†è¾¨ï¼Œä»è€Œäº§ç”Ÿå†…åœ¨çš„è°±åç½®ã€‚ä¸ºæ­¤ï¼Œä½œè€…å°†å™ªå£°è°ƒåº¦é‡æ–°è§£é‡Šä¸ºè°±æ­£åˆ™åŒ–å™¨ï¼Œå¼•å…¥äº†èƒ½å¤Ÿä¿ç•™ç»†å¾®å°ºåº¦ç»“æ„çš„ power-law schedulesï¼Œå¹¶å¼€å‘äº†åŸºäºå•æ­¥è’¸é¦çš„ Lazy Diffusion æ–¹æ³•ä»¥ç»•è¿‡é•¿è½¨è¿¹æ¨ç†å¹¶é˜²æ­¢é«˜ $k$ é™çº§ã€‚å®éªŒåœ¨ 2D Kolmogorov æ¹æµå’Œå¢¨è¥¿å“¥æ¹¾æµ·æ´‹æ•°æ®ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè§£å†³ spectral collapseï¼Œç¨³å®šé•¿æ—¶ autoregression å¹¶æ¢å¤çœŸå®çš„ inertial-range scalingã€‚è¯¥å·¥ä½œè¯æ˜äº† physics-aware æ‰©æ•£è¿‡ç¨‹åœ¨æ„å»ºå¤šå°ºåº¦åŠ¨åŠ›ç³»ç»Ÿçš„é«˜æ•ˆæ¦‚ç‡æ›¿ä»£æ¨¡å‹æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "physics.flu-dyn",
        "cs.AI",
        "math.DS",
        "nlin.CD",
        "physics.ao-ph"
      ],
      "primary_category": "physics.flu-dyn",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09572v1",
      "published_date": "2025-12-10 12:05:32 UTC",
      "updated_date": "2025-12-10 12:05:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:29:30.312864+00:00"
    },
    {
      "arxiv_id": "2512.09570v1",
      "title": "The Gender Code: Gendering the Global Governance of Artificial Intelligence",
      "title_zh": "æ€§åˆ«ä»£ç ï¼šäººå·¥æ™ºèƒ½å…¨çƒæ²»ç†çš„æ€§åˆ«åŒ–",
      "authors": [
        "Jelena Cupac"
      ],
      "abstract": "This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›½é™…äººå·¥æ™ºèƒ½æ²»ç†(AI Governance)æ¡†æ¶å¦‚ä½•åº”å¯¹æ€§åˆ«é—®é¢˜åŠåŸºäºæ€§åˆ«çš„æŸå®³ï¼Œåˆ†ææ¶µç›–äº†ã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹(EU AI Act)ç­‰å¼ºåˆ¶æ€§æ³•è§„ã€è”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡(UNESCO)çš„äººå·¥æ™ºèƒ½ä¼¦ç†å»ºè®®ä¹¦ä»¥åŠå…¨çƒäººå·¥æ™ºèƒ½ä¼™ä¼´å…³ç³»(GPAI)ç­‰å›½é™…å€¡è®®ã€‚è°ƒæŸ¥æ­ç¤ºäº†å°†æ€§åˆ«è®®é¢˜èå…¥äººæƒæ¡†æ¶ã€è½¬å‘æ˜ç¡®çš„æ€§åˆ«ç›¸å…³è§„å®šä»¥åŠæ—¥ç›Šå¼ºè°ƒåŒ…å®¹æ€§(Inclusivity)ä¸å¤šæ ·æ€§(Diversity)ç­‰å‘å±•è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œç ”ç©¶æŒ‡å‡ºç›®å‰ä»å­˜åœ¨æ ¸å¿ƒç¼ºé™·ï¼ŒåŒ…æ‹¬å„æ²»ç†æ–‡æ¡£ä¸­æ€§åˆ«å¤„ç†çš„ä¸ä¸€è‡´æ€§ã€å¯¹äº¤å‰æ€§(Intersectionality)çš„å…³æ³¨æœ‰é™ï¼Œä»¥åŠç¼ºä¹å¼ºæœ‰åŠ›çš„æ‰§è¡Œæœºåˆ¶(Enforcement Mechanisms)ã€‚ä½œè€…ä¸»å¼ æœ‰æ•ˆçš„äººå·¥æ™ºèƒ½æ²»ç†å¿…é¡»å…·å¤‡äº¤å‰æ€§ã€å¯æ‰§è¡Œæ€§å’ŒåŒ…å®¹æ€§ï¼Œä»¥è¶…è¶Šè¡¨é¢å½¢å¼å¹¶é˜²æ­¢ç°æœ‰ä¸å¹³ç­‰çš„è¿›ä¸€æ­¥åŠ å‰§ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼ºè°ƒæ€§åˆ«æ•æ„Ÿæ²»ç†åœ¨æ„å»ºå…¬æ­£æŠ€æœ¯æœªæ¥ä¸­çš„é‡è¦æ€§ï¼Œä¸ºäººå·¥æ™ºèƒ½ä¼¦ç†(Ethical AI)é¢†åŸŸçš„è¾©è®ºåšå‡ºäº†é‡è¦è´¡çŒ®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "The paper is part of the Handbook on the Global Governance of Artificial Intelligence, forthcoming with Edward Elgar Publishing",
      "pdf_url": "https://arxiv.org/pdf/2512.09570v1",
      "published_date": "2025-12-10 12:02:47 UTC",
      "updated_date": "2025-12-10 12:02:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:29:33.163287+00:00"
    },
    {
      "arxiv_id": "2512.09566v2",
      "title": "Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search",
      "title_zh": "è¿ˆå‘åŸºäºè¯­è¨€æ¨¡å‹ã€å±æ€§å¯¹é½ä¸ç­–ç•¥æœç´¢çš„é—­ç¯åˆ†å­å‘ç°",
      "authors": [
        "Junkai Ji",
        "Zhangfan Yang",
        "Dong Xu",
        "Ruibin Bai",
        "Jianqiang Li",
        "Tingjun Hou",
        "Zexuan Zhu"
      ],
      "abstract": "Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold. By combining generalization, plausibility, and interpretability, Trio establishes a closed-loop generative paradigm that redefines how chemical space can be navigated, offering a transformative foundation for the next era of AI-driven drug discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Trioï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆç‰‡æ®µå¼åˆ†å­è¯­è¨€æ¨¡å‹(fragment-based molecular language modeling)ã€å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢(Monte Carlo Tree Search, MCTS)çš„åˆ†å­ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆä¸”å¯è§£é‡Šçš„é—­ç¯é¶å‘åˆ†å­è®¾è®¡ã€‚é’ˆå¯¹ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨æ³›åŒ–æ€§ã€å¯è§£é‡Šæ€§å’Œè¯ç†ç‰¹æ€§å¹³è¡¡æ–¹é¢çš„ä¸è¶³ï¼ŒTrioé€šè¿‡è¯­å¢ƒæ„ŸçŸ¥çš„ç‰‡æ®µç»„è£…ã€ç‰©ç†åŒ–å­¦æ€§è´¨ä¸åˆæˆå¯è¡Œæ€§çš„çº¦æŸï¼Œä»¥åŠåœ¨è›‹ç™½è´¨ç»“åˆå£è¢‹å†…çš„ç­–ç•¥æ€§æœç´¢ï¼Œå®ç°äº†å¯¹åŒ–å­¦ç©ºé—´çš„ç²¾å‡†å¯¼èˆªã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTrioåœ¨ç”ŸæˆåŒ–å­¦æœ‰æ•ˆä¸”è¯ç†å¢å¼ºçš„é…ä½“æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ç»“åˆäº²å’ŒåŠ›(binding affinity)æå‡äº†7.85%ï¼Œè¯ç‰©ç›¸ä¼¼æ€§(drug-likeness)æå‡äº†11.10%ï¼Œåˆæˆå¯åŠæ€§(synthetic accessibility)æå‡äº†12.05%ï¼ŒåŒæ—¶å°†åˆ†å­å¤šæ ·æ€§æ‰©å¤§äº†å››å€ä»¥ä¸Šã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆæ³›åŒ–æ€§ã€åˆç†æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå»ºç«‹äº†ä¸€ä¸ªé—­åˆå›è·¯çš„ç”ŸæˆèŒƒå¼ï¼Œä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–°è¯ç ”å‘å¥ å®šäº†è½¬å‹åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.09566v2",
      "published_date": "2025-12-10 11:59:42 UTC",
      "updated_date": "2025-12-18 10:53:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:29:19.662040+00:00"
    },
    {
      "arxiv_id": "2512.09563v1",
      "title": "System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection",
      "title_zh": "CCL25-Eval Task 10 ç³»ç»ŸæŠ¥å‘Šï¼šé¢å‘ç»†ç²’åº¦ä¸­æ–‡ä»‡æ¨è¨€è®ºæ£€æµ‹çš„æç¤ºé©±åŠ¨å¤§è¯­è¨€æ¨¡å‹åˆå¹¶",
      "authors": [
        "Binglin Wu",
        "Jiaxiu Zou",
        "Xianneng Li"
      ],
      "abstract": "The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸­æ–‡ç¤¾äº¤åª’ä½“ä¸­ä»‡æ¨è¨€è®ºæ£€æµ‹é¢ä¸´çš„ä¸Šä¸‹æ–‡ä¾èµ–å’Œä¿šè¯­æ¼”åŒ–æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–°é¢–ä¸‰é˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶å…·ä½“åŒ…å«æç¤ºå·¥ç¨‹ï¼ˆPrompt Engineeringï¼‰ã€æœ‰ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-tuningï¼‰å’Œæ¨¡å‹åˆå¹¶ï¼ˆLLM Mergingï¼‰ä¸‰ä¸ªå…³é”®é˜¶æ®µã€‚ç ”ç©¶é¦–å…ˆé€šè¿‡è®¾è®¡ä¸Šä¸‹æ–‡æ„ŸçŸ¥æç¤ºå¼•å¯¼æ¨¡å‹æå–éšå¼ä»‡æ¨æ¨¡å¼ï¼Œå¹¶åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ•´åˆä»»åŠ¡ç‰¹å®šç‰¹å¾ä»¥å¢å¼ºé¢†åŸŸé€‚åº”æ€§ã€‚æœ€åï¼Œé€šè¿‡åˆå¹¶å¾®è°ƒåçš„æ¨¡å‹æå‡äº†ç³»ç»Ÿåº”å¯¹åˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰æƒ…å†µçš„é²æ£’æ€§ã€‚åœ¨ STATE-ToxiCN åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨ç»†ç²’åº¦ä»‡æ¨è¨€è®ºæ£€æµ‹æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at CCL 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.09563v1",
      "published_date": "2025-12-10 11:58:34 UTC",
      "updated_date": "2025-12-10 11:58:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:31:00.057268+00:00"
    },
    {
      "arxiv_id": "2512.09543v2",
      "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
      "title_zh": "SWEnergyï¼šåŸºäºå°è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“é—®é¢˜ä¿®å¤æ¡†æ¶èƒ½æ•ˆå®è¯ç ”ç©¶",
      "authors": [
        "Arihant Tripathy",
        "Ch Pavan Harshit",
        "Karthik Vaidhyanathan"
      ],
      "abstract": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.\n  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.\n  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.\n  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.\n  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹ï¼Œä½¿ç”¨å°è¯­è¨€æ¨¡å‹(SLMs)çš„è‡ªä¸»æ™ºèƒ½ä½“æ¡†æ¶åœ¨è‡ªåŠ¨é—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€èƒ½æºæ•ˆç‡å’Œèµ„æºæ¶ˆè€—æƒ…å†µã€‚ç ”ç©¶è€…é€šè¿‡æ§åˆ¶å˜é‡å®éªŒï¼Œè¯„ä¼°äº†å››ç§é¢†å…ˆçš„æ™ºèƒ½ä½“æ¡†æ¶(SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover)åœ¨æ­è½½ Gemma-3 4B å’Œ Qwen-3 1.7B ä¸¤ç§æ¨¡å‹æ—¶çš„è¡¨ç°ã€‚å®éªŒåŸºäº SWE-bench Verified Mini åŸºå‡†æµ‹è¯•ï¼Œé‡åŒ–åˆ†æäº†èƒ½è€—ã€æŒç»­æ—¶é—´ã€Token ä½¿ç”¨é‡å’Œå†…å­˜å ç”¨ç­‰å…³é”®æŒ‡æ ‡ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¡†æ¶æ¶æ„(Framework architecture)æ˜¯èƒ½æºæ¶ˆè€—çš„ä¸»è¦é©±åŠ¨å› ç´ ï¼Œä¸”å½“å‰çš„æ¡†æ¶è®¾è®¡åœ¨é…åˆ SLMs æ—¶ä¼šäº§ç”Ÿå¤§é‡çš„èƒ½æºæµªè´¹ã€‚ç”±äº SLMs æœ‰é™çš„æ¨ç†èƒ½åŠ›æˆä¸ºæˆåŠŸç‡çš„ç“¶é¢ˆï¼Œå¯¼è‡´ä»»åŠ¡è§£å†³ç‡æ¥è¿‘äºé›¶ï¼Œè¿™è¡¨æ˜ç°æœ‰çš„ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è®¾è®¡çš„æ¡†æ¶éš¾ä»¥é«˜æ•ˆè¿è¡Œã€‚ç ”ç©¶æœ€åæŒ‡å‡ºï¼Œå®ç°å¯è¡Œçš„ä½èƒ½è€—è§£å†³æ–¹æ¡ˆéœ€è¦ä»è¢«åŠ¨ç¼–æ’è½¬å‘èƒ½å¤Ÿä¸»åŠ¨ç®¡ç† SLMs å¼±ç‚¹çš„æ–°å‹æ¶æ„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "8 pages, 5 figures, 1 table. Accepted to AGENT 2026 (ICSE 2026 workshop)",
      "pdf_url": "https://arxiv.org/pdf/2512.09543v2",
      "published_date": "2025-12-10 11:28:48 UTC",
      "updated_date": "2025-12-11 11:33:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:29:43.649115+00:00"
    },
    {
      "arxiv_id": "2512.09524v1",
      "title": "NeuroSketch: An Effective Framework for Neural Decoding via Systematic Architectural Optimization",
      "title_zh": "NeuroSketchï¼šä¸€ç§åŸºäºç³»ç»ŸåŒ–æ¶æ„ä¼˜åŒ–çš„é«˜æ•ˆç¥ç»è§£ç æ¡†æ¶",
      "authors": [
        "Gaorui Zhang",
        "Zhizhang Yuan",
        "Jialan Yang",
        "Junru Chen",
        "Li Meng",
        "Yang Yang"
      ],
      "abstract": "Neural decoding, a critical component of Brain-Computer Interface (BCI), has recently attracted increasing research interest. Previous research has focused on leveraging signal processing and deep learning methods to enhance neural decoding performance. However, the in-depth exploration of model architectures remains underexplored, despite its proven effectiveness in other tasks such as energy forecasting and image classification. In this study, we propose NeuroSketch, an effective framework for neural decoding via systematic architecture optimization. Starting with the basic architecture study, we find that CNN-2D outperforms other architectures in neural decoding tasks and explore its effectiveness from temporal and spatial perspectives. Building on this, we optimize the architecture from macro- to micro-level, achieving improvements in performance at each step. The exploration process and model validations take over 5,000 experiments spanning three distinct modalities (visual, auditory, and speech), three types of brain signals (EEG, SEEG, and ECoG), and eight diverse decoding tasks. Experimental results indicate that NeuroSketch achieves state-of-the-art (SOTA) performance across all evaluated datasets, positioning it as a powerful tool for neural decoding. Our code and scripts are available at https://github.com/Galaxy-Dawn/NeuroSketch.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NeuroSketchï¼Œä¸€ä¸ªé€šè¿‡ç³»ç»ŸåŒ–æ¶æ„ä¼˜åŒ–(Architectural Optimization)æå‡ç¥ç»è§£ç (Neural Decoding)æ€§èƒ½çš„æœ‰æ•ˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è„‘æœºæ¥å£(BCI)é¢†åŸŸä¸­æ¨¡å‹æ¶æ„æ¢ç´¢ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å¯¹æ¯”ç ”ç©¶ï¼Œä½œè€…å‘ç°CNN-2Dåœ¨è§£ç ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶ä»æ—¶é—´å’Œç©ºé—´ç»´åº¦éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥å®æ–½äº†ä»å®è§‚åˆ°å¾®è§‚å±‚é¢çš„ç³»ç»Ÿä¼˜åŒ–ï¼Œåœ¨è¶…è¿‡5,000æ¬¡å®éªŒä¸­æ¶µç›–äº†è§†è§‰ã€å¬è§‰å’Œè¯­è¨€ä¸‰ç§æ¨¡æ€ï¼Œä»¥åŠEEGã€SEEGå’ŒECoGä¸‰ç§è„‘ç”µä¿¡å·ç±»å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNeuroSketchåœ¨æ¶‰åŠå…«ç±»è§£ç ä»»åŠ¡çš„æ‰€æœ‰è¯„ä¼°æ•°æ®é›†ä¸­å‡è¾¾åˆ°äº†SOTAæ°´å¹³ã€‚è¯¥æ¡†æ¶ä¸ä»…æ˜¾è‘—æå‡äº†ç¥ç»è§£ç çš„å‡†ç¡®åº¦ï¼Œä¹Ÿä¸ºæœªæ¥BCIç³»ç»Ÿçš„æ¨¡å‹è®¾è®¡æä¾›äº†ç³»ç»ŸåŒ–çš„ä¼˜åŒ–æ€è·¯ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09524v1",
      "published_date": "2025-12-10 11:01:56 UTC",
      "updated_date": "2025-12-10 11:01:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:29:54.443143+00:00"
    },
    {
      "arxiv_id": "2512.09496v1",
      "title": "Representation Invariance and Allocation: When Subgroup Balance Matters",
      "title_zh": "è¡¨ç¤ºä¸å˜æ€§ä¸åˆ†é…ï¼šå­ç¾¤ä½“å¹³è¡¡åœ¨ä½•æ—¶é‡è¦",
      "authors": [
        "Anissa Alloula",
        "Charles Jones",
        "Zuzanna Wakefield-Skorniewska",
        "Francesco Quinzan",
        "BartÅ‚omiej PapieÅ¼"
      ],
      "abstract": "Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è®­ç»ƒæ•°æ®ä¸­äººå£ç»Ÿè®¡å­¦ç¾¤ä½“çš„ä¸å‡è¡¡è¡¨ç¤ºå¯¹æ¨¡å‹åœ¨ä¸åŒç¾¤ä½“é—´æ³›åŒ–æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶æŒ‘æˆ˜äº†å¹³è¡¡å­ç¾¤ä½“è¡¨ç¤ºï¼ˆsubgroup representationï¼‰æ€»èƒ½ä¼˜åŒ–æ€§èƒ½çš„ä¼ ç»Ÿå‡è®¾ï¼ŒæŒ‡å‡ºåœ¨æŸäº›æƒ…å†µä¸‹ä¸å¹³è¡¡çš„æ•°æ®åˆ†å¸ƒåè€Œèƒ½æå‡å­ç¾¤ä½“æ€§èƒ½ã€‚é€šè¿‡å¯¹å››ä¸ªè§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿç ”ç©¶ï¼Œä½œè€…æå‡ºäº†æ½œç©ºé—´åˆ†ç¦»å‡è®¾ï¼ˆlatent separation hypothesisï¼‰ï¼Œè®¤ä¸ºæ¨¡å‹å¯¹å­ç¾¤ä½“è¡¨ç¤ºçš„ä¾èµ–ç¨‹åº¦å–å†³äºé¢„è®­ç»ƒæ¨¡å‹æ½œç©ºé—´ä¸­å­ç¾¤ä½“ä¹‹é—´çš„åˆ†ç¦»ç¨‹åº¦ã€‚ä½œè€…å¯¹è¯¥å‡è®¾è¿›è¡Œäº†å½¢å¼åŒ–å®šä¹‰å’Œç†è®ºåˆ†æï¼Œå¹¶é€šè¿‡å®è¯ç ”ç©¶éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†è¯¥ç†è®ºåœ¨åŸºç¡€æ¨¡å‹ï¼ˆfoundation modelï¼‰å¾®è°ƒä¸­çš„åº”ç”¨ï¼Œè¯æ˜å®šé‡åˆ†ææ½œå­ç¾¤ä½“åˆ†ç¦»åº¦å¯ä»¥æœ‰æ•ˆæŒ‡å¯¼æ•°æ®æ”¶é›†ä¸å¹³è¡¡å†³ç­–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09496v1",
      "published_date": "2025-12-10 10:19:48 UTC",
      "updated_date": "2025-12-10 10:19:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:30:26.170256+00:00"
    },
    {
      "arxiv_id": "2512.14731v1",
      "title": "Semantic Geometry for policy-constrained interpretation",
      "title_zh": "é¢å‘ç­–ç•¥çº¦æŸè§£é‡Šçš„è¯­ä¹‰å‡ ä½•",
      "authors": [
        "Nikit Phadke"
      ],
      "abstract": "We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºç­–ç•¥çº¦æŸè¯­ä¹‰è§£é‡Š(policy-constrained semantic interpretation)çš„å‡ ä½•æ¡†æ¶ï¼Œæ—¨åœ¨ä»ç†è®ºä¸Šé˜²æ­¢é«˜é£é™©é¢†åŸŸä¸­å‡ºç°çš„å¹»è§‰æ‰¿è¯º(hallucinated commitments)é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†è¯­ä¹‰æ„ä¹‰è¡¨ç¤ºä¸ºå•ä½çƒä½“(unit sphere)ä¸Šçš„æ–¹å‘ï¼Œå°†è¯æ®å»ºæ¨¡ä¸ºè§è¯å‘é‡é›†ï¼Œå¹¶å°†å¯é‡‡çº³çš„è§£é‡Šå¯¹åº”ä¸ºçƒé¢å‡¸åŒºåŸŸ(spherical convex regions)ã€‚ç­–ç•¥çº¦æŸä½œä¸ºæ˜¾å¼å…ˆéªŒå®šä¹‰åœ¨åŒä¸€æµå½¢ä¸Šï¼Œå¹¶ä¸è¯æ®å‡ ä½•åˆ†ç¦»ï¼Œä½¿è¯­ä¹‰è§£é‡Šè¿‡ç¨‹è½¬åŒ–ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œä¸”åœ¨å‡ºç°çŸ›ç›¾æˆ–ç­–ç•¥æ’é™¤æ—¶ï¼Œæ‹’ç»(refusal)å“åº”ä¼šä½œä¸ºæ‹“æ‰‘å¿…ç„¶ç»“æœè‡ªç„¶äº§ç”Ÿã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†è¯¥æ¡†æ¶ä¸ä¿¡æ¯è®ºã€è´å¶æ–¯æ¨ç†å’Œå±‚è®ºè¯­ä¹‰(sheaf-theoretic semantics)ç›¸ç»“åˆï¼Œè¯æ˜äº†å…¶å¤æ‚åº¦ç•Œé™åœ¨ä¿¡æ¯è®ºä¸Šå…·æœ‰æœ€ä¼˜æ€§ã€‚åœ¨å¤§è§„æ¨¡å—ç›‘ç®¡é‡‘èæ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ç­–ç•¥æœºåˆ¶ä¸‹å‡å®ç°äº†é›¶å¹»è§‰æ‰¹å‡†(zero hallucinated approvals)ï¼Œæ˜¯é¦–ä¸ªåœ¨å¤§è§„æ¨¡åº”ç”¨ä¸­å–å¾—æ­¤ç±»æˆæœçš„ç ”ç©¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14731v1",
      "published_date": "2025-12-10 10:10:53 UTC",
      "updated_date": "2025-12-10 10:10:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:31:27.494517+00:00"
    },
    {
      "arxiv_id": "2512.09487v1",
      "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning",
      "title_zh": "RouteRAGï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆæ–‡æœ¬ä¸å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Yucan Guo",
        "Miao Su",
        "Saiping Guan",
        "Zihao Sun",
        "Xiaolong Jin",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­æ··åˆæ£€ç´¢ç³»ç»Ÿä¾èµ–å›ºå®šæµç¨‹ä¸”å›¾æ£€ç´¢å¼€é”€å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†RouteRAGæ¡†æ¶ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿å¤§è¯­è¨€æ¨¡å‹(LLMs)èƒ½å¤Ÿæ‰§è¡Œå¤šè½®ä¸”è‡ªé€‚åº”çš„å›¾-æ–‡æœ¬æ··åˆæ£€ç´¢ã€‚RouteRAGé€šè¿‡ç»Ÿä¸€çš„ç”Ÿæˆç­–ç•¥å…±åŒä¼˜åŒ–æ•´ä¸ªè¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½è‡ªä¸»å­¦ä¹ æ¨ç†æ—¶æœºã€ä»æ–‡æœ¬æˆ–å›¾ä¸­é€‰æ‹©æ£€ç´¢å†…å®¹ä»¥åŠç”Ÿæˆæœ€ç»ˆç­”æ¡ˆçš„æ—¶æœºã€‚ä¸ºäº†å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬ï¼Œç ”ç©¶è®¾è®¡äº†å…¼é¡¾ä»»åŠ¡ç»“æœä¸æ£€ç´¢æ•ˆç‡çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œåœ¨å……åˆ†åˆ©ç”¨æ··åˆè¯æ®çš„åŒæ—¶é¿å…äº†ä¸å¿…è¦çš„æ£€ç´¢å¼€é”€ã€‚åœ¨äº”ä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRouteRAGçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰RAGåŸºçº¿æ¨¡å‹ã€‚è¯¥æˆæœçªå‡ºäº†ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ åœ¨å®ç°å¤æ‚æ¨ç†ä»»åŠ¡ä¸‹è‡ªé€‚åº”å’Œé«˜æ•ˆæ£€ç´¢æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09487v1",
      "published_date": "2025-12-10 10:05:31 UTC",
      "updated_date": "2025-12-10 10:05:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:31:19.406267+00:00"
    },
    {
      "arxiv_id": "2512.09485v1",
      "title": "Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks",
      "title_zh": "åˆ©ç”¨å®šåˆ¶åŒ–ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¨è¿›é¢å‘é›¶è§¦æ‘¸ç½‘ç»œçš„åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨è‡ªåŠ¨åŒ–",
      "authors": [
        "Xinye Cao",
        "Yihan Lin",
        "Guoshun Nan",
        "Qinchuan Zhou",
        "Yuhang Luo",
        "Yurui Gao",
        "Zeliang Zhang",
        "Haolang Lu",
        "Qimei Cui",
        "Yanzhao Hou",
        "Xiaofeng Tao",
        "Tony Q. S. Quek"
      ],
      "abstract": "Zero-Touch Networks (ZTNs) represent a transformative paradigm toward fully automated and intelligent network management, providing the scalability and adaptability required for the complexity of sixth-generation (6G) networks. However, the distributed architecture, high openness, and deep heterogeneity of 6G networks expand the attack surface and pose unprecedented security challenges. To address this, security automation aims to enable intelligent security management across dynamic and complex environments, serving as a key capability for securing 6G ZTNs. Despite its promise, implementing security automation in 6G ZTNs presents two primary challenges: 1) automating the lifecycle from security strategy generation to validation and update under real-world, parallel, and adversarial conditions, and 2) adapting security strategies to evolving threats and dynamic environments. This motivates us to propose SecLoop and SA-GRPO. SecLoop constitutes the first fully automated framework that integrates large language models (LLMs) across the entire lifecycle of security strategy generation, orchestration, response, and feedback, enabling intelligent and adaptive defenses in dynamic network environments, thus tackling the first challenge. Furthermore, we propose SA-GRPO, a novel security-aware group relative policy optimization algorithm that iteratively refines security strategies by contrasting group feedback collected from parallel SecLoop executions, thereby addressing the second challenge. Extensive real-world experiments on five benchmarks, including 11 MITRE ATT&CK processes and over 20 types of attacks, demonstrate the superiority of the proposed SecLoop and SA-GRPO. We will release our platform to the community, facilitating the advancement of security automation towards next generation communications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 6G æ—¶ä»£é›¶è§¦æ‘¸ç½‘ç»œ (Zero-Touch Networks, ZTNs) å› åˆ†å¸ƒå¼æ¶æ„å’Œé«˜åº¦å¼‚æ„æ€§å¸¦æ¥çš„å¤æ‚å®‰å…¨æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªå…¨è‡ªåŠ¨å®‰å…¨æ¡†æ¶ SecLoopï¼Œé€šè¿‡å°†å¤§è¯­è¨€æ¨¡å‹ (LLMs) é›†æˆåˆ°å®‰å…¨ç­–ç•¥ç”Ÿæˆã€ç¼–æ’ã€å“åº”åŠåé¦ˆçš„å…¨ç”Ÿå‘½å‘¨æœŸä¸­ï¼Œå®ç°äº†åŠ¨æ€ç¯å¢ƒä¸‹çš„æ™ºèƒ½é˜²å¾¡ã€‚ä¸ºäº†è§£å†³å®‰å…¨ç­–ç•¥å¯¹æ¼”è¿›å¨èƒçš„è‡ªé€‚åº”éš¾é¢˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥è®¾è®¡äº† SA-GRPO ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§å®šåˆ¶åŒ–çš„å®‰å…¨æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (Security-Aware Group Relative Policy Optimization) æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”å¹¶è¡Œ SecLoop æ‰§è¡Œä¸­æ”¶é›†çš„ç»„åé¦ˆæ¥è¿­ä»£ç²¾ç‚¼ç­–ç•¥ã€‚åœ¨åŒ…å« 11 ä¸ª MITRE ATT&CK è¿‡ç¨‹å’Œ 20 å¤šç§æ”»å‡»ç±»å‹çš„äº”ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®éªŒç»“æœå……åˆ†éªŒè¯äº† SecLoop å’Œ SA-GRPO çš„ä¼˜è¶Šæ€§ã€‚è¯¥ç ”ç©¶ä¸ä»…å®ç°äº†å®‰å…¨ç­–ç•¥ä»ç”Ÿæˆåˆ°éªŒè¯çš„é—­ç¯è‡ªåŠ¨åŒ–ï¼Œè¿˜é€šè¿‡å¼€æºå¹³å°ä¸ºä¸‹ä¸€ä»£é€šä¿¡ç½‘ç»œå®ç°é«˜åº¦è‡ªæ²»çš„å®‰å…¨ç®¡ç†å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by IEEE JSAC. This work has been submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2512.09485v1",
      "published_date": "2025-12-10 10:04:11 UTC",
      "updated_date": "2025-12-10 10:04:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:31:28.100384+00:00"
    },
    {
      "arxiv_id": "2512.09477v1",
      "title": "Color encoding in Latent Space of Stable Diffusion Models",
      "title_zh": "Stable Diffusion æ¨¡å‹æ½œç©ºé—´ä¸­çš„é¢œè‰²ç¼–ç ",
      "authors": [
        "Guillem Arias",
        "Ariadna SolÃ ",
        "MartÃ­ Armengod",
        "Maria Vanrell"
      ],
      "abstract": "Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Stable Diffusion ç­‰æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨æ„ŸçŸ¥å±æ€§ï¼ˆå¦‚é¢œè‰²å’Œå½¢çŠ¶ï¼‰å†…éƒ¨è¡¨å¾æœºåˆ¶å°šä¸æ˜ç¡®çš„é—®é¢˜ï¼Œç³»ç»Ÿæ¢ç´¢äº†å…¶æ½œç©ºé—´ (Latent Space) çš„é¢œè‰²ç¼–ç æ–¹å¼ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ„å»ºå—æ§åˆæˆæ•°æ®é›†ï¼Œå¹¶ç»“åˆä¸»æˆåˆ†åˆ†æ (PCA) å’Œç›¸ä¼¼æ€§åº¦é‡æ–¹æ³•ï¼Œæ­ç¤ºäº†é¢œè‰²ä¿¡æ¯ä¸»è¦æ²¿åœ†å½¢å¯¹ç­‰è½´ (circular, opponent axes) ç¼–ç ï¼Œå¹¶é›†ä¸­åœ¨æ½œé€šé“ (latent channels) $c_3$ å’Œ $c_4$ ä¸­ã€‚ä¸æ­¤åŒæ—¶ï¼Œå…‰ç…§å¼ºåº¦ (intensity) å’Œå½¢çŠ¶ (shape) åˆ™ä¸»è¦ç”±é€šé“ $c_1$ å’Œ $c_2$ è¡¨å¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒStable Diffusion çš„æ½œç©ºé—´å…·å¤‡ä¸æœ‰æ•ˆç¼–ç  (efficient coding) è¡¨å¾ç›¸ä¸€è‡´çš„å¯è§£é‡Šç»“æ„ã€‚è¿™äº›å‘ç°ä¸ºæ·±å…¥ç†è§£æ¨¡å‹å†…éƒ¨æœºåˆ¶ã€å›¾åƒç¼–è¾‘åº”ç”¨ä»¥åŠè®¾è®¡æ›´å…·è§£è€¦æ€§ (disentangled) çš„ç”Ÿæˆæ¡†æ¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 8 figures, Color Imaging Conference 33",
      "pdf_url": "https://arxiv.org/pdf/2512.09477v1",
      "published_date": "2025-12-10 09:54:03 UTC",
      "updated_date": "2025-12-10 09:54:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:31:21.611358+00:00"
    },
    {
      "arxiv_id": "2512.09471v1",
      "title": "Temporal-Spatial Tubelet Embedding for Cloud-Robust MSI Reconstruction using MSI-SAR Fusion: A Multi-Head Self-Attention Video Vision Transformer Approach",
      "title_zh": "èåˆ MSI-SAR çš„äº‘ç¨³å¥ MSI é‡å»ºæ—¶ç©ºå°ç®¡åµŒå…¥ï¼šä¸€ç§å¤šå¤´è‡ªæ³¨æ„åŠ›è§†é¢‘è§†è§‰ Transformer æ–¹æ³•",
      "authors": [
        "Yiqun Wang",
        "Lujun Li",
        "Meiru Yue",
        "Radu State"
      ],
      "abstract": "Cloud cover in multispectral imagery (MSI) significantly hinders early-season crop mapping by corrupting spectral information. Existing Vision Transformer(ViT)-based time-series reconstruction methods, like SMTS-ViT, often employ coarse temporal embeddings that aggregate entire sequences, causing substantial information loss and reducing reconstruction accuracy. To address these limitations, a Video Vision Transformer (ViViT)-based framework with temporal-spatial fusion embedding for MSI reconstruction in cloud-covered regions is proposed in this study. Non-overlapping tubelets are extracted via 3D convolution with constrained temporal span $(t=2)$, ensuring local temporal coherence while reducing cross-day information degradation. Both MSI-only and SAR-MSI fusion scenarios are considered during the experiments. Comprehensive experiments on 2020 Traill County data demonstrate notable performance improvements: MTS-ViViT achieves a 2.23\\% reduction in MSE compared to the MTS-ViT baseline, while SMTS-ViViT achieves a 10.33\\% improvement with SAR integration over the SMTS-ViT baseline. The proposed framework effectively enhances spectral reconstruction quality for robust agricultural monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šå…‰è°±å›¾åƒ (MSI) ä¸­çš„äº‘å±‚é®æŒ¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘è§†è§‰å˜å‹å™¨ (Video Vision Transformer, ViViT) çš„æ—¶ç©ºèåˆåµŒå…¥æ¡†æ¶ï¼Œæ—¨åœ¨æå‡äº‘è¦†ç›–åŒºåŸŸçš„å›¾åƒé‡å»ºè´¨é‡ã€‚é’ˆå¯¹ç°æœ‰ Vision Transformer (ViT) æ–¹æ³•åœ¨å¤„ç†æ—¶é—´åºåˆ—æ—¶å› ç²—ç²’åº¦åµŒå…¥å¯¼è‡´çš„ä¿¡æ¯æŸå¤±ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†æ—¶ç©ºç®¡çŠ¶åµŒå…¥ (Temporal-Spatial Tubelet Embedding) æœºåˆ¶ï¼Œé€šè¿‡ä¸‰ç»´å·ç§¯ (3D convolution) æå–æ—¶é—´è·¨åº¦é™åˆ¶ä¸º $t=2$ çš„éé‡å ç®¡çŠ¶ç‰¹å¾ï¼Œåœ¨å‡å°‘è·¨å¤©ä¿¡æ¯é€€åŒ–çš„åŒæ—¶ç¡®ä¿å±€éƒ¨æ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒæ¶µç›–äº†ä»… MSI ä»¥åŠèåˆåˆæˆå­”å¾„é›·è¾¾ (SAR-MSI fusion) çš„ä¸¤ç§æ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMTS-ViViT åœ¨ Traill County æ•°æ®é›†ä¸Šçš„å‡æ–¹è¯¯å·® (MSE) è¾ƒåŸºçº¿æ¨¡å‹é™ä½äº† 2.23%ï¼Œè€Œåœ¨æ•´åˆ SAR æ•°æ®åï¼ŒSMTS-ViViT çš„æ€§èƒ½è¾ƒ SMTS-ViT åŸºçº¿æå‡äº† 10.33%ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆå¢å¼ºäº†å…‰è°±é‡å»ºçš„ç¨³å¥æ€§ï¼Œä¸ºå¤æ‚æ°”è±¡æ¡ä»¶ä¸‹çš„å†œä¸šç›‘æµ‹å’Œä½œç‰©åˆ¶å›¾æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09471v1",
      "published_date": "2025-12-10 09:46:08 UTC",
      "updated_date": "2025-12-10 09:46:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:33:29.092756+00:00"
    },
    {
      "arxiv_id": "2512.09463v1",
      "title": "Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing",
      "title_zh": "é¢å‘å·¥ä¸šçš„éšç§ä¿æŠ¤è®¡ç®—æœºè§†è§‰ï¼šä»¥äººä¸ºä¸­å¿ƒçš„åˆ¶é€ ä¸šä¸­çš„ä¸‰ä¸ªæ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Sander De Coninck",
        "Emilio Gamba",
        "Bart Van Doninck",
        "Abdellatif Bey-Temsamani",
        "Sam Leroux",
        "Pieter Simoens"
      ],
      "abstract": "The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šAIè®¡ç®—æœºè§†è§‰é¢ä¸´çš„è¿è¥æ•ˆç”¨ä¸å·¥äººéšç§å¹³è¡¡é—®é¢˜ï¼Œåœ¨çœŸå®ç”Ÿäº§ç¯å¢ƒä¸‹éªŒè¯äº†ä¸€ä¸ªéšç§ä¿æŠ¤æ¡†æ¶ã€‚è¯¥æ–¹æ³•æ ¸å¿ƒåœ¨äºé‡‡ç”¨learned visual transformationsï¼Œèƒ½å¤Ÿæ¨¡ç³Šæ•æ„Ÿæˆ–ä»»åŠ¡æ— å…³ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™æ‰§è¡Œä»»åŠ¡æ‰€éœ€çš„å…³é”®ç‰¹å¾ã€‚ç ”ç©¶é€šè¿‡æœ¨å·¥ç”Ÿäº§ç›‘æµ‹ã€äººç±»æ„ŸçŸ¥çš„AGV navigationä»¥åŠå¤šæ‘„åƒå¤´äººä½“å·¥å­¦é£é™©è¯„ä¼°ä¸‰ä¸ªæ¡ˆä¾‹ï¼Œåˆ©ç”¨åˆä½œä¼™ä¼´çš„çœŸå®æ•°æ®è¿›è¡Œäº†å…¨é¢æµ‹è¯•ã€‚é€šè¿‡å¯¹privacy-utilityæƒè¡¡çš„å®šé‡è¯„ä¼°å’Œå®šæ€§åé¦ˆï¼Œç»“æœè¯æ˜ä»»åŠ¡ç‰¹å®šçš„æ··æ·†æŠ€æœ¯èƒ½åœ¨æ˜¾è‘—é™ä½éšç§é£é™©çš„åŒæ—¶ç¡®ä¿æœ‰æ•ˆçš„ç›‘æ§æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯å®äº†æ¡†æ¶åœ¨ç°å®å·¥ä¸šåœºæ™¯ä¸­çš„éƒ¨ç½²å¯è¡Œæ€§ï¼Œè¿˜ä¸ºå·¥ä¸šç•Œè´Ÿè´£ä»»çš„human-centric AIéƒ¨ç½²æä¾›äº†é‡è¦çš„è·¨é¢†åŸŸå»ºè®®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the AAAI26 HCM workshop",
      "pdf_url": "https://arxiv.org/pdf/2512.09463v1",
      "published_date": "2025-12-10 09:33:03 UTC",
      "updated_date": "2025-12-10 09:33:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:31:35.476860+00:00"
    },
    {
      "arxiv_id": "2512.09461v1",
      "title": "Cytoplasmic Strings Analysis in Human Embryo Time-Lapse Videos using Deep Learning Framework",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ æ¡†æ¶çš„äººç±»èƒšèƒæ—¶å·®è§†é¢‘èƒè´¨ä¸åˆ†æ",
      "authors": [
        "Anabia Sohail",
        "Mohamad Alansari",
        "Ahmed Abughali",
        "Asmaa Chehab",
        "Abdelfatah Ahmed",
        "Divya Velayudhan",
        "Sajid Javed",
        "Hasan Al Marzouqi",
        "Ameena Saad Al-Sumaiti",
        "Junaid Kashir",
        "Naoufel Werghi"
      ],
      "abstract": "Infertility is a major global health issue, and while in-vitro fertilization has improved treatment outcomes, embryo selection remains a critical bottleneck. Time-lapse imaging enables continuous, non-invasive monitoring of embryo development, yet most automated assessment methods rely solely on conventional morphokinetic features and overlook emerging biomarkers. Cytoplasmic Strings, thin filamentous structures connecting the inner cell mass and trophectoderm in expanded blastocysts, have been associated with faster blastocyst formation, higher blastocyst grades, and improved viability. However, CS assessment currently depends on manual visual inspection, which is labor-intensive, subjective, and severely affected by detection and subtle visual appearance. In this work, we present, to the best of our knowledge, the first computational framework for CS analysis in human IVF embryos. We first design a human-in-the-loop annotation pipeline to curate a biologically validated CS dataset from TLI videos, comprising 13,568 frames with highly sparse CS-positive instances. Building on this dataset, we propose a two-stage deep learning framework that (i) classifies CS presence at the frame level and (ii) localizes CS regions in positive cases. To address severe imbalance and feature uncertainty, we introduce the Novel Uncertainty-aware Contractive Embedding (NUCE) loss, which couples confidence-aware reweighting with an embedding contraction term to form compact, well-separated class clusters. NUCE consistently improves F1-score across five transformer backbones, while RF-DETR-based localization achieves state-of-the-art (SOTA) detection performance for thin, low-contrast CS structures. The source code will be made publicly available at: https://github.com/HamadYA/CS_Detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾…åŠ©ç”Ÿæ®–é¢†åŸŸä¸­ Cytoplasmic Strings (CS) è¯„ä¼°é«˜åº¦ä¾èµ–äººå·¥è§†è§‰æ£€æŸ¥ä¸”ä¸»è§‚æ€§å¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªç”¨äºäººç±»ä½“å¤–å—ç²¾ (IVF) èƒšèƒ CS åˆ†æçš„è®¡ç®—æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ human-in-the-loop æ ‡æ³¨æµç¨‹æ„å»ºäº†ä¸€ä¸ªåŒ…å« 13,568 å¸§å›¾åƒçš„ç”Ÿç‰©å­¦éªŒè¯æ•°æ®é›†ï¼Œå¹¶è®¾è®¡äº†ç»“åˆå¸§çº§åˆ†ç±»ä¸åŒºåŸŸå®šä½çš„ä¸¤é˜¶æ®µæ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚ä¸ºäº†å…‹æœæ•°æ®ä¸¥é‡å¤±è¡¡å’Œç‰¹å¾ä¸ç¡®å®šæ€§ï¼Œç ”ç©¶å¼•å…¥äº† Novel Uncertainty-aware Contractive Embedding (NUCE) æŸå¤±å‡½æ•°ï¼Œé€šè¿‡ç½®ä¿¡åº¦æ„ŸçŸ¥é‡åŠ æƒå’ŒåµŒå…¥æ”¶ç¼©é¡¹æ„å»ºç´§å‡‘ä¸”åˆ†ç¦»åº¦é«˜çš„ç±»åˆ«ç°‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNUCE æ˜¾è‘—æå‡äº†å¤šç§ transformer éª¨å¹²ç½‘ç»œçš„ F1-scoreï¼ŒåŒæ—¶åŸºäº RF-DETR çš„å®šä½æ–¹æ³•åœ¨æ£€æµ‹ç»†é•¿ã€ä½å¯¹æ¯”åº¦çš„ CS ç»“æ„ä¸Šè¾¾åˆ°äº† state-of-the-art (SOTA) æ€§èƒ½ã€‚è¯¥å·¥ä½œä¸ä»…å¡«è¡¥äº†è‡ªåŠ¨è¯„ä¼°æ–°å…´èƒšèƒç”Ÿç‰©æ ‡å¿—ç‰©çš„æŠ€æœ¯ç©ºç™½ï¼Œä¹Ÿä¸ºæé«˜èƒšèƒé€‰æ‹©çš„æ•ˆç‡å’Œå‡†ç¡®æ€§æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09461v1",
      "published_date": "2025-12-10 09:29:23 UTC",
      "updated_date": "2025-12-10 09:29:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:31:38.484346+00:00"
    },
    {
      "arxiv_id": "2512.09458v1",
      "title": "Architectures for Building Agentic AI",
      "title_zh": "æ„å»ºæ™ºèƒ½ä½“ AI çš„æ¶æ„",
      "authors": [
        "SÅ‚awomir Nowaczyk"
      ],
      "abstract": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.",
      "tldr_zh": "è¯¥ç ”ç©¶è®¤ä¸º agentic å’Œ generative AI çš„å¯é æ€§æœ¬è´¨ä¸Šæ˜¯ä¸€ç§æ¶æ„å±æ€§ï¼Œå¹¶å°† agentic systems å®šä¹‰ä¸ºåœ¨é—­ç¯ä¸­è¿è¡Œã€ä»¥ç›®æ ‡ä¸ºå¯¼å‘ä¸”å…·å¤‡å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„å†³ç­–è€…ã€‚æ–‡ç« æŒ‡å‡ºå¯é æ€§æºäºåŸåˆ™æ€§çš„ç»„ä»¶åŒ–ï¼ˆå¦‚ goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetryï¼‰ã€è§„èŒƒåŒ–çš„æ¥å£ä»¥åŠæ˜ç¡®çš„æ§åˆ¶ä¸ä¿è¯å›è·¯(assurance loops)ã€‚åŸºäºç»å…¸ç†è®ºï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªå®ç”¨çš„åˆ†ç±»æ³•ï¼Œæ¶µç›–äº† tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems ä»¥åŠ embodied or web agentsï¼Œå¹¶æ·±å…¥åˆ†æäº†æ¯ç§æ¨¡å¼å¦‚ä½•é‡å¡‘å¯é æ€§è¾¹ç•ŒåŠå¤±æ•ˆæ¨¡å¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æç‚¼äº†å…³äº typed schemas, idempotency, permissioning, transactional semantics, memory provenance ä¸ runtime governance ç­‰æ–¹é¢çš„è®¾è®¡æŒ‡å¯¼ï¼Œå¹¶æå‡ºäº†æ¨¡æ‹Ÿåå†æ‰§è¡Œ(simulate-before-actuate)çš„å®‰å…¨é˜²æŠ¤æœºåˆ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by Springer Nature",
      "pdf_url": "https://arxiv.org/pdf/2512.09458v1",
      "published_date": "2025-12-10 09:28:40 UTC",
      "updated_date": "2025-12-10 09:28:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:31:51.636926+00:00"
    },
    {
      "arxiv_id": "2512.15745v2",
      "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
      "title_zh": "LLaDA2.0ï¼šå°†æ‰©æ•£è¯­è¨€æ¨¡å‹æ‰©å±•è‡³åƒäº¿çº§è§„æ¨¡",
      "authors": [
        "Tiwei Bie",
        "Maosong Cao",
        "Kun Chen",
        "Lun Du",
        "Mingliang Gong",
        "Zhuochen Gong",
        "Yanmei Gu",
        "Jiaqi Hu",
        "Zenan Huang",
        "Zhenzhong Lan",
        "Chengxi Li",
        "Chongxuan Li",
        "Jianguo Li",
        "Zehuan Li",
        "Huabin Liu",
        "Lin Liu",
        "Guoshan Lu",
        "Xiaocheng Lu",
        "Yuxin Ma",
        "Jianfeng Tan",
        "Lanning Wei",
        "Ji-Rong Wen",
        "Yipeng Xing",
        "Xiaolu Zhang",
        "Junbo Zhao",
        "Da Zheng",
        "Jun Zhou",
        "Junlin Zhou",
        "Zhanchao Zhou",
        "Liwang Zhu",
        "Yihong Zhuang"
      ],
      "abstract": "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†LLaDA2.0ï¼Œä¸€å¥—å‚æ•°è§„æ¨¡é«˜è¾¾100Bçš„ç¦»æ•£æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹(dLLM)ï¼Œé€šè¿‡å°†é¢„è®­ç»ƒçš„è‡ªå›å½’(AR)æ¨¡å‹ç³»ç»Ÿæ€§è½¬æ¢è€Œå¾—ï¼Œç¡®ç«‹äº†å‰æ²¿è§„æ¨¡éƒ¨ç½²çš„æ–°èŒƒå¼ã€‚è¯¥æ¡†æ¶é¿å¼€äº†æ˜‚è´µçš„ä»å¤´è®­ç»ƒï¼Œé‡‡ç”¨åˆ›æ–°çš„ä¸‰é˜¶æ®µå—çº§WSDè®­ç»ƒæ–¹æ¡ˆï¼Œå®ç°äº†çŸ¥è¯†ç»§æ‰¿ã€æ¸è¿›å¼é€‚é…ä¸é«˜æ•ˆè½¬æ¢ã€‚ç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†LLaDA2.0-mini (16B)å’ŒLLaDA2.0-flash (100B)ä¸¤ä¸ªç»è¿‡SFTå’ŒDPOæŒ‡ä»¤å¾®è°ƒçš„æ··åˆä¸“å®¶(MoE)å˜ä½“ã€‚è¿™äº›æ¨¡å‹åœ¨ä¿ç•™å¹¶è¡Œè§£ç (parallel decoding)ä¼˜åŠ¿çš„åŒæ—¶ï¼Œåœ¨å‰æ²¿è§„æ¨¡ä¸Šå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ä¸æ•ˆç‡ã€‚ç›®å‰ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹å‡å·²å¼€æºï¼Œä¸ºå¯æ‰©å±•çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.15745v2",
      "published_date": "2025-12-10 09:26:18 UTC",
      "updated_date": "2025-12-24 03:46:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:32:28.680844+00:00"
    },
    {
      "arxiv_id": "2512.09443v2",
      "title": "Advancing Mathematical Research via Human-AI Interactive Theorem Proving",
      "title_zh": "é€šè¿‡äººæœºäº¤äº’å¼å®šç†è¯æ˜æ¨è¿›æ•°å­¦ç ”ç©¶",
      "authors": [
        "Chenyi Li",
        "Zhijian Lai",
        "Dong An",
        "Jiang Hu",
        "Zaiwen Wen"
      ],
      "abstract": "We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä¿æŒæ•°å­¦ä¸¥è°¨æ€§çš„å‰æä¸‹ä½œä¸ºç§‘å­¦è®¡ç®—çš„ç ”ç©¶å·¥å…·ï¼Œå¹¶æå‡ºäº†ä¸€ç§äººæœºäº¤äº’ (human-in-the-loop) çš„å®šç†è¯æ˜ä¸å‘ç°å·¥ä½œæµã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œäººç±»ä¸“å®¶æŒæ¡é—®é¢˜è¡¨è¿°å’Œå‡è®¾çš„ä¸»å¯¼æƒï¼Œè€Œæ¨¡å‹è´Ÿè´£æœç´¢è¯æ˜ã€æå‡ºå€™é€‰å±æ€§æˆ–å®šç†ï¼Œå¹¶é€šè¿‡æ•°å€¼å®éªŒè¾…åŠ©éªŒè¯ã€‚ä¸“å®¶éšåå¯¹æ¨¡å‹ç”Ÿæˆçš„åŸå§‹ææ–™è¿›è¡Œç²¾ç‚¼ï¼Œå°†å…¶ç»„ç»‡æˆä¸¥è°¨çš„æ•°å­¦è¯æ˜ã€‚ç ”ç©¶é€šè¿‡æµå½¢ä¼˜åŒ– (manifold optimization) ä¸ Grover é‡å­æœç´¢ç®—æ³•çš„æ¡ˆä¾‹ç ”ç©¶éªŒè¯äº†è¯¥æµç¨‹ï¼ŒæˆåŠŸè¯†åˆ«äº†ä¸å˜å­ç©ºé—´ (invariant subspaces) å¹¶è·å¾—äº†æ”¶æ•›æ€§ä¿è¯ã€‚è¯¥æ¡†æ¶ä¸º LLMs é›†æˆåˆ°å‰æ²¿æ•°å­¦ç ”ç©¶æä¾›äº†å®ç”¨æ¨¡æ¿ï¼Œåœ¨ç»´æŒæ¨ç†é€æ˜åº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—åŠ å¿«äº†è¯æ˜ç©ºé—´çš„æ¢ç´¢ä¸ç®—æ³•è®¾è®¡ã€‚è™½ç„¶æ¡ˆä¾‹ä¾§é‡äºé‡å­è®¡ç®—é¢†åŸŸï¼Œä½†å…¶æ–¹æ³•è®ºåŸåˆ™å¯å¹¿æ³›æ‰©å±•è‡³ç§‘å­¦è®¡ç®—çš„å…¶ä»–æ ¸å¿ƒé¢†åŸŸã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09443v2",
      "published_date": "2025-12-10 09:16:27 UTC",
      "updated_date": "2025-12-11 13:10:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:32:07.702504+00:00"
    },
    {
      "arxiv_id": "2512.09441v1",
      "title": "Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model",
      "title_zh": "åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç±»å¢é‡å­¦ä¹ è¡¨å¾æ ¡å‡†ä¸ä¸ç¡®å®šæ€§å¼•å¯¼",
      "authors": [
        "Jiantao Tan",
        "Peixian Ma",
        "Tong Yu",
        "Wentao Zhang",
        "Ruixuan Wang"
      ],
      "abstract": "Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç±»å¢é‡å­¦ä¹  (Class-Incremental Learning) ä¸­è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models) éš¾ä»¥åŒºåˆ†ä¸åŒå­¦ä¹ ä»»åŠ¡é—´ç±»åˆ«çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æŒç»­å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨é¢„è®­ç»ƒä¸”å†»ç»“çš„å›¾åƒç¼–ç å™¨ä¸­åŠ å…¥ä»»åŠ¡ç‰¹å®šé€‚é…å™¨ (task-specific adapters) ä»¥å­¦ä¹ æ–°çŸ¥è¯†ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºè½»é‡çº§æŠ•å½±å™¨æ··åˆ (mixture of light-weight projectors) çš„è·¨ä»»åŠ¡è¡¨ç¤ºæ ¡å‡† (cross-task representation calibration) ç­–ç•¥ï¼Œæ—¨åœ¨ç»Ÿä¸€ç‰¹å¾ç©ºé—´ä¸­æ›´å¥½åœ°åˆ†ç¦»æ‰€æœ‰å·²å­¦ç±»åˆ«ï¼Œä»è€Œç¼“è§£è·¨ä»»åŠ¡ç±»åˆ«æ··æ·†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†ä¸€ç§ç”±é¢„æµ‹ä¸ç¡®å®šæ€§ (prediction uncertainty) å¼•å¯¼çš„æ¨ç†ç­–ç•¥ï¼Œä»¥æ›´å‡†ç¡®åœ°é€‰æ‹©æœ€åˆé€‚çš„å›¾åƒç‰¹å¾è¿›è¡Œç±»åˆ«é¢„æµ‹ã€‚åœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤šç§è®¾ç½®ä¸‹çš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09441v1",
      "published_date": "2025-12-10 09:09:23 UTC",
      "updated_date": "2025-12-10 09:09:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:33:42.630608+00:00"
    },
    {
      "arxiv_id": "2512.09434v1",
      "title": "CourtPressGER: A German Court Decision to Press Release Summarization Dataset",
      "title_zh": "CourtPressGERï¼šå¾·å›½æ³•é™¢åˆ¤å†³è‡³æ–°é—»ç¨¿æ‘˜è¦ç”Ÿæˆæ•°æ®é›†",
      "authors": [
        "Sebastian Nagl",
        "Mohamed Elganayni",
        "Melanie Pospisil",
        "Matthias Grabmair"
      ],
      "abstract": "Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†CourtPressGERï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«6.4kä¸ªä¸‰å…ƒç»„çš„æ•°æ®é›†ï¼Œç”±å¾·å›½æœ€é«˜æ³•é™¢çš„åˆ¤å†³ä¹¦ã€äººå·¥æ’°å†™çš„æ³•é™¢æ–°é—»ç¨¿ä»¥åŠç”¨äºå¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„åˆæˆæç¤ºè¯ç»„æˆã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä»¥å¾€çš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å·¥ä½œå¤šå…³æ³¨æŠ€æœ¯æ€§çš„åˆ¤å†³æ‘˜è¦ï¼Œè€Œå¿½è§†äº†é¢å‘å¤§ä¼—çš„å¸æ³•æ²Ÿé€šéœ€æ±‚ã€‚CourtPressGERä½œä¸ºä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è®­ç»ƒå’Œè¯„ä¼°LLMsä»é•¿ç¯‡æ³•å¾‹æ–‡æœ¬ä¸­ç”Ÿæˆå‡†ç¡®ã€æ˜“è¯»çš„æ–°é—»ç¨¿çš„èƒ½åŠ›ã€‚ç ”ç©¶è€…é€šè¿‡å‚è€ƒæŒ‡æ ‡ã€äº‹å®ä¸€è‡´æ€§æ£€æŸ¥ã€LLM-as-judgeå’Œä¸“å®¶æ’åºå¯¹ä¸åŒè§„æ¨¡çš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œå¤§å‹LLMsèƒ½ç”Ÿæˆé«˜è´¨é‡è‰æ¡ˆä¸”åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶æ€§èƒ½æŸå¤±è¾ƒå°ï¼Œè€Œå°å‹æ¨¡å‹åˆ™éœ€ä¾èµ–å±‚çº§åŒ–æ¶æ„ã€‚ç›®å‰çš„åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ¨¡å‹è¡¨ç°å„å¼‚ï¼Œä½†äººå·¥æ’°å†™çš„æ–°é—»ç¨¿åœ¨è´¨é‡æ’åä¸Šä¾ç„¶ä¿æŒé¢†å…ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint - This contribution was accepted at JURIX AI4A2J Workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.09434v1",
      "published_date": "2025-12-10 09:04:00 UTC",
      "updated_date": "2025-12-10 09:04:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:33:47.824979+00:00"
    },
    {
      "arxiv_id": "2512.09427v2",
      "title": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators",
      "title_zh": "ODMAï¼šé¢å‘ LPDDR ç±»åŠ é€Ÿå™¨çš„å¤§è¯­è¨€æ¨¡å‹æœåŠ¡æŒ‰éœ€å†…å­˜åˆ†é…æ¡†æ¶",
      "authors": [
        "Guoqiang Zou",
        "Wanyu Wang",
        "Hao Zheng",
        "Longxiang Yin",
        "Yinhe Han"
      ],
      "abstract": "Device-memory management is a key bottleneck for serving large language models (LLMs) on accelerators whose memory has poor small-granularity random-access bandwidth (e.g., LPDDR5-class). Existing approaches either statically pre-allocate worst-case KV-cache per request, wasting substantial device memory, or rely on fine-grained paging that assumes high random-access tolerance and is therefore ill-suited to LPDDR-style systems. We present ODMA, an on-demand memory allocation framework for LLM serving on random-access-constrained device memory (RACM) platforms such as LPDDR5-based Cambricon MLUs. ODMA builds on generation-length prediction while addressing distribution drift and heavy-tailed request lengths via dynamic bucket partitioning and a large-bucket safeguard: bucket boundaries are periodically re-learned from online histograms, and high-uncertainty or overflowed requests fall back to a reserved large bucket for robustness. On Alpaca and Google-NQ, ODMA improves S3's predictor accuracy from 98.60% to 99.55% and from 82.68% to 93.36%, respectively. Serving DeepSeek-R1-Distill-Qwen-7B on four Cambricon MLU370-X4 accelerators, ODMA increases device-memory utilization from 55.05% to 72.45% on Alpaca and from 42.54% to 61.79% on Google-NQ, and boosts throughput by 23% and 27% over a static pre-allocation baseline. These results show that predictor-driven, hardware-aware allocation can unlock efficient LLM serving on RACM accelerators without hardware changes, complementing paging-centric designs tailored to HBM systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨ LPDDR5 ç­‰éšæœºè®¿é—®å¸¦å®½å—é™ï¼ˆRACMï¼‰çš„åŠ é€Ÿå™¨ä¸Šéƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶çš„æ˜¾å­˜ç®¡ç†ç“¶é¢ˆï¼Œæå‡ºäº†æŒ‰éœ€å†…å­˜åˆ†é…æ¡†æ¶ ODMAã€‚è¯¥æ¡†æ¶å…‹æœäº†ä¼ ç»Ÿé™æ€é¢„åˆ†é…å¯¼è‡´çš„æ˜¾å­˜æµªè´¹ä»¥åŠç»†ç²’åº¦åˆ†é¡µæŠ€æœ¯åœ¨ LPDDR ç³»ç»Ÿä¸­æ€§èƒ½ä¸ä½³çš„é—®é¢˜ï¼Œæ ¸å¿ƒé‡‡ç”¨äº†åŸºäºç”Ÿæˆé•¿åº¦é¢„æµ‹çš„åˆ†é…ç­–ç•¥ã€‚ODMA é€šè¿‡åŠ¨æ€æ¡¶åˆ’åˆ†ï¼ˆdynamic bucket partitioningï¼‰å’Œé•¿æ¡¶ä¿éšœæœºåˆ¶ï¼ˆlarge-bucket safeguardï¼‰åº”å¯¹åˆ†å¸ƒåç§»å’Œé•¿å°¾è¯·æ±‚ï¼Œå¹¶å®šæœŸåˆ©ç”¨åœ¨çº¿ç›´æ–¹å›¾é‡æ–°å­¦ä¹ æ¡¶è¾¹ç•Œã€‚åœ¨å¯’æ­¦çºª MLU370-X4 åŠ é€Ÿå™¨ä¸Šéƒ¨ç½² DeepSeek-R1-Distill-Qwen-7B çš„å®éªŒæ˜¾ç¤ºï¼ŒODMA å°†æ˜¾å­˜åˆ©ç”¨ç‡æ˜¾è‘—æå‡ï¼Œåœ¨ Alpaca æ•°æ®é›†ä¸Šä» 55.05% æé«˜è‡³ 72.45%ã€‚ç›¸æ¯”é™æ€åˆ†é…åŸºçº¿ï¼Œè¯¥æ¡†æ¶å°†ç³»ç»Ÿååé‡æå‡äº† 23% è‡³ 27%ï¼Œè¯æ˜äº†ç¡¬ä»¶æ„ŸçŸ¥çš„é¢„æµ‹é©±åŠ¨åˆ†é…æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆä¼˜åŒ– RACM åŠ é€Ÿå™¨çš„æœåŠ¡æ€§èƒ½ï¼Œæˆä¸º HBM ç³»ç»Ÿåˆ†é¡µè®¾è®¡çš„æœ‰åŠ›è¡¥å……ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.09427v2",
      "published_date": "2025-12-10 08:52:20 UTC",
      "updated_date": "2025-12-29 07:47:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:33:50.863923+00:00"
    },
    {
      "arxiv_id": "2512.09406v1",
      "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
      "title_zh": "H2R-Grounderï¼šä¸€ç§å°†äººç±»äº¤äº’è§†é¢‘è½¬åŒ–ä¸ºç‰©ç†æ¥åœ°æœºå™¨äººè§†é¢‘çš„æ— éœ€é…å¯¹æ•°æ®èŒƒå¼",
      "authors": [
        "Hai Ci",
        "Xiaokang Liu",
        "Pei Yang",
        "Yiren Song",
        "Mike Zheng Shou"
      ],
      "abstract": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† H2R-Grounderï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€æˆå¯¹æ•°æ® (Paired-Data-Free) çš„è§†é¢‘åˆ°è§†é¢‘ç¿»è¯‘æ¡†æ¶ï¼Œæ—¨åœ¨å°†äººç±»ä¸ç‰©ä½“çš„äº¤äº’è§†é¢‘è½¬åŒ–ä¸ºç‰©ç†è½åœ°æ„Ÿå¼º (Physically Grounded) ä¸”åŠ¨ä½œä¸€è‡´çš„æœºå™¨äººæ“ä½œè§†é¢‘ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¿®å¤ (Inpainting) æŠ€æœ¯å’Œè¦†ç›–è§†è§‰æç¤º (Visual Cue) æ„å»ºäº†ä¸€ç§å¯è¿ç§»çš„è¡¨å¾ï¼Œæœ‰æ•ˆå¼¥åˆäº†äººç±»ä¸æœºå™¨äººä¹‹é—´çš„å…·èº«å·®å¼‚ (Embodiment Gap)ã€‚é€šè¿‡åœ¨ä¸Šä¸‹æ–‡å­¦ä¹  (In-Context Learning) æ¨¡å¼ä¸‹å¾®è°ƒ SOTA è§†é¢‘æ‰©æ•£æ¨¡å‹ Wan 2.2ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆå…·æœ‰é«˜åº¦æ—¶åºè¿è´¯æ€§çš„é«˜è´¨é‡è§†é¢‘ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒH2R-Grounder åœ¨ç”ŸæˆçœŸå®ä¸”ç¬¦åˆç‰©ç†é€»è¾‘çš„æœºå™¨äººè¿åŠ¨æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œä¸ºåˆ©ç”¨å¤§è§„æ¨¡æœªæ ‡è®°äººç±»è§†é¢‘æ‰©å±•æœºå™¨äººå­¦ä¹ èƒ½åŠ›æä¾›äº†å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "13 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.09406v1",
      "published_date": "2025-12-10 07:59:45 UTC",
      "updated_date": "2025-12-10 07:59:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:33:56.050009+00:00"
    },
    {
      "arxiv_id": "2512.09398v1",
      "title": "Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting",
      "title_zh": "è¿ˆå‘éŸ§æ€§äº¤é€šï¼šä¸€ç§ç”¨äºäº‹æ•…æ„ŸçŸ¥äº¤é€šé¢„æµ‹çš„æ¡ä»¶ Transformer",
      "authors": [
        "Hongjun Wang",
        "Jiawei Yong",
        "Jiawei Wang",
        "Shintaro Fukushima",
        "Renhe Jiang"
      ],
      "abstract": "Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰äº¤é€šé¢„æµ‹æ¨¡å‹å¾€å¾€å¿½ç•¥äº¤é€šäº‹æ•…å’Œæ³•è§„ç­‰å¤–éƒ¨å› ç´ å½±å“çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸º ConFormer (Conditional Transformer) çš„åˆ›æ–°æ¡†æ¶ã€‚ä½œè€…é¦–å…ˆæ„å»ºå¹¶å‘å¸ƒäº†åŒ…å«ä¸œäº¬å’ŒåŠ åˆ©ç¦å°¼äºšäº¤é€šã€äº‹æ•…åŠæ³•è§„ä¿¡æ¯çš„å¢å¼ºå‹æ•°æ®é›†ã€‚ConFormer ç»“åˆäº†å›¾ä¼ æ’­ (graph propagation) ä¸å¼•å¯¼å½’ä¸€åŒ–å±‚ (guided normalization layer)ï¼Œèƒ½å¤Ÿæ ¹æ®å†å²æ¨¡å¼åŠ¨æ€è°ƒæ•´èŠ‚ç‚¹é—´çš„æ—¶ç©ºå…³ç³»ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡è¶…è¶Šäº† SOTA æ¨¡å‹ STAEFormerï¼Œå®ç°äº†æ›´ä½çš„è®¡ç®—æˆæœ¬å’Œå‚æ•°éœ€æ±‚ã€‚è¯¥æˆæœæ˜¾è‘—æå‡äº†äº¤é€šé¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œä¸ºæ—¶ç©ºæ•°æ®æŒ–æ˜ (spatio-temporal data mining) é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09398v1",
      "published_date": "2025-12-10 07:50:20 UTC",
      "updated_date": "2025-12-10 07:50:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:34:01.842227+00:00"
    },
    {
      "arxiv_id": "2512.09396v1",
      "title": "GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection",
      "title_zh": "GAIRï¼šåŸºäºä¿¡æ¯è”åˆæ¨ç†ä¸ç¾¤ä½“åæ€çš„ GUI è‡ªåŠ¨åŒ–",
      "authors": [
        "Zishu Wei",
        "Qixiang Ma",
        "Xavier Hu",
        "Yuhang Liu",
        "Hui Zang",
        "Yudong Zhao",
        "Tao Wang",
        "Shengyu Zhang",
        "Fei Wu"
      ],
      "abstract": "Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GAIRï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)çš„ä¿¡æ¯è”åˆæ¨ç†ä¸ç¾¤ä½“åæ€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³GUIè‡ªåŠ¨åŒ–é¢†åŸŸä¸­ä»»åŠ¡å¤šæ ·æ€§å¯¹æ¨¡å‹å¼‚æ„èƒ½åŠ›çš„è¦æ±‚ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆæ¥è‡ªä¸åŒå¼‚æ„æ¨¡å‹çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¼¥è¡¥äº†å•ä¸€æ¨¡å‹åœ¨å¤„ç†æ–‡æ¡£å¤„ç†ã€è§†é¢‘ç¼–è¾‘ç­‰å¤æ‚ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚GAIRå¼•å…¥äº†ä¸€ä¸ªé€šç”¨å‹MLLMä½œä¸ºæ ¸å¿ƒå†³ç­–è€…ï¼Œè´Ÿè´£è”åˆå¤„ç†å¤šä¸ªGUI-specific modelsæä¾›çš„å¤šå…ƒä¿¡æ¯ï¼Œä»¥å®ç°æ›´é«˜æ€§èƒ½çš„è‡ªåŠ¨åŒ–æ“ä½œã€‚å½“ç°æœ‰ä¿¡æ¯ä¸è¶³ä»¥åšå‡ºåˆç†å†³ç­–æ—¶ï¼Œç³»ç»Ÿä¼šè¿›å…¥ç¾¤ä½“åæ€(group reflection)çŠ¶æ€ï¼Œç”±é€šç”¨æ¨¡å‹æ ¹æ®å„å­æ¨¡å‹çš„ä¼˜ç¼ºç‚¹æä¾›å·®å¼‚åŒ–çš„æŒ‡ä»¤å’Œæç¤ºã€‚è¿™ç§åä½œæœºåˆ¶é©±åŠ¨å„ä¸“ç”¨æ¨¡å‹æ”¶é›†æ›´å…·é’ˆå¯¹æ€§å’Œå‡†ç¡®æ€§çš„ä¿¡æ¯ï¼Œä»è€Œæ”¯æ’‘æ›´æ·±å±‚çš„æ¨ç†ä¸å†³ç­–è¿‡ç¨‹ã€‚é€šè¿‡åœ¨å¤šä¸ªGUI benchmarksä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œè¯¥ç ”ç©¶éªŒè¯äº†GAIRåœ¨æå‡GUIè‡ªåŠ¨åŒ–ä»£ç†ç³»ç»Ÿæœ‰æ•ˆæ€§ä¸å¯é æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09396v1",
      "published_date": "2025-12-10 07:40:23 UTC",
      "updated_date": "2025-12-10 07:40:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:33:57.736211+00:00"
    },
    {
      "arxiv_id": "2512.09386v1",
      "title": "CONCUR: A Framework for Continual Constrained and Unconstrained Routing",
      "title_zh": "CONCURï¼šä¸€ç§å—é™ä¸éå—é™æŒç»­è·¯ç”±æ¡†æ¶",
      "authors": [
        "Peter Baile Chen",
        "Weiyue Li",
        "Dan Roth",
        "Michael Cafarella",
        "Samuel Madden",
        "Jacob Andreas"
      ],
      "abstract": "AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸ºCONCURçš„æŒç»­è·¯ç”±æ¡†æ¶ï¼Œæ—¨åœ¨æ ¹æ®ä»»åŠ¡å¤æ‚åº¦å°†å…¶é«˜æ•ˆæ˜ å°„è‡³æœ€ä½³çš„è®¡ç®—ç­–ç•¥ã€‚é’ˆå¯¹ä¼ ç»Ÿè·¯ç”±æ¨¡å‹åœ¨å¼•å…¥æ–°ç­–ç•¥æ—¶é¢ä¸´çš„é«˜é¢é‡è®­ç»ƒæˆæœ¬å’Œæ³›åŒ–ç“¶é¢ˆï¼ŒCONCURé‡‡ç”¨äº†æ¨¡å—åŒ–è®¾è®¡ï¼Œé€šè¿‡ä¸ºæ¯ç§ç­–ç•¥è®­ç»ƒç‹¬ç«‹çš„Predictoræ¨¡å‹ï¼Œå®ç°äº†æ–°ç­–ç•¥çš„æ— ç¼é›†æˆã€‚è¯¥æ¡†æ¶åŒæ—¶æ”¯æŒConstrainedå’ŒUnconstrained Routingï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡ä¸è®¡ç®—ç­–ç•¥çš„Multiple Representationsæ¥æ•æ‰å¤æ‚çš„è·¯ç”±é€»è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨çŸ¥è¯†ä¸æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­ï¼ŒCONCURåœ¨ç«¯åˆ°ç«¯å‡†ç¡®ç‡å’Œæ¨ç†æˆæœ¬ä¸Šå‡ä¼˜äºç°æœ‰çš„å•ç­–ç•¥åŠä¸»æµè·¯ç”±æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨Continual Settingä¸‹æ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶åœ¨åˆ†å¸ƒå†…ä¸åˆ†å¸ƒå¤–æµ‹è¯•ä¸­å‡å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09386v1",
      "published_date": "2025-12-10 07:30:13 UTC",
      "updated_date": "2025-12-10 07:30:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:34:05.665370+00:00"
    },
    {
      "arxiv_id": "2512.09385v2",
      "title": "BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks",
      "title_zh": "BugSweeperï¼šåŸºäºå›¾ç¥ç»ç½‘ç»œçš„å‡½æ•°çº§æ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹",
      "authors": [
        "Uisang Lee",
        "Changhoon Chung",
        "Junmo Lee",
        "Soo-Mook Moon"
      ],
      "abstract": "The rapid growth of Ethereum has made it more important to quickly and accurately detect smart contract vulnerabilities. While machine-learning-based methods have shown some promise, many still rely on rule-based preprocessing designed by domain experts. Rule-based preprocessing methods often discard crucial context from the source code, potentially causing certain vulnerabilities to be overlooked and limiting adaptability to newly emerging threats. We introduce BugSweeper, an end-to-end deep learning framework that detects vulnerabilities directly from the source code without manual engineering. BugSweeper represents each Solidity function as a Function-Level Abstract Syntax Graph (FLAG), a novel graph that combines its Abstract Syntax Tree (AST) with enriched control-flow and data-flow semantics. Then, our two-stage Graph Neural Network (GNN) analyzes these graphs. The first-stage GNN filters noise from the syntax graphs, while the second-stage GNN conducts high-level reasoning to detect diverse vulnerabilities. Extensive experiments on real-world contracts show that BugSweeper significantly outperforms all state-of-the-art detection methods. By removing the need for handcrafted rules, our approach offers a robust, automated, and scalable solution for securing smart contracts without any dependence on security experts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BugSweeperï¼Œä¸€ç§æ—¨åœ¨å¿«é€Ÿã€å‡†ç¡®æ£€æµ‹ä»¥å¤ªåŠæ™ºèƒ½åˆçº¦æ¼æ´çš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚ä¸ºäº†å…‹æœç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–äººå·¥é¢„å¤„ç†è§„åˆ™ä¸”æ˜“ä¸¢å¤±ä¸Šä¸‹æ–‡è¯­ä¹‰çš„å±€é™ï¼ŒBugSweeperå°†Solidityå‡½æ•°è¡¨ç¤ºä¸ºFunction-Level Abstract Syntax Graph (FLAG)ï¼Œè¯¥æ–°å‹å›¾ç»“æ„èåˆäº†Abstract Syntax Tree (AST)ä»¥åŠå¢å¼ºçš„control-flowå’Œdata-flowè¯­ä¹‰ã€‚æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µGraph Neural Network (GNN)è¿›è¡Œåˆ†æï¼Œå…¶ä¸­ç¬¬ä¸€é˜¶æ®µç”¨äºè¿‡æ»¤è¯­æ³•å›¾ä¸­çš„å™ªå£°ï¼Œç¬¬äºŒé˜¶æ®µæ‰§è¡Œé«˜å±‚æ¨ç†ä»¥è¯†åˆ«å¤šæ ·åŒ–çš„æ¼æ´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBugSweeperåœ¨çœŸå®ä¸–ç•Œåˆçº¦æ£€æµ‹ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºæ‰€æœ‰æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ã€‚é€šè¿‡æ¶ˆé™¤å¯¹ä¸“å®¶æ‰‹å·¥è§„åˆ™çš„ä¾èµ–ï¼Œè¯¥ç ”ç©¶ä¸ºæ™ºèƒ½åˆçº¦å®‰å…¨æä¾›äº†ä¸€ç§é²æ£’ã€è‡ªåŠ¨åŒ–ä¸”å…·å¤‡é«˜åº¦æ‰©å±•æ€§çš„å‡½æ•°çº§æ¼æ´æ£€æµ‹æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper is accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.09385v2",
      "published_date": "2025-12-10 07:30:03 UTC",
      "updated_date": "2025-12-12 04:56:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:34:07.047183+00:00"
    },
    {
      "arxiv_id": "2512.09375v1",
      "title": "Log NeRF: Comparing Spaces for Learning Radiance Fields",
      "title_zh": "Log NeRFï¼šç”¨äºå­¦ä¹ è¾å°„åœºçš„ç©ºé—´æ¯”è¾ƒ",
      "authors": [
        "Sihe Chen",
        "Luv Verma",
        "Bruce A. Maxwell"
      ],
      "abstract": "Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision. However, little attention has been paid to the color space in which the network is learning the radiance field representation. Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance. To test this, we captured approximately 30 videos using a GoPro camera, ensuring linear data recovery through inverse encoding. We trained NeRF models under various color space interpretations linear, sRGB, GPLog, and log RGB by converting each network output to a common color space before rendering and loss computation, enforcing representation learning in different color spaces. Quantitative and qualitative evaluations demonstrate that using a log RGB color space consistently improves rendering quality, exhibits greater robustness across scenes, and performs particularly well in low light conditions while using the same bit-depth input images. Further analysis across different network sizes and NeRF variants confirms the generalization and stability of the log space advantage.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¥ç»è¾å°„åœº(NeRF)åœ¨ä¸åŒé¢œè‰²ç©ºé—´ä¸­å­¦ä¹ åœºæ™¯è¡¨ç¤ºçš„æ•ˆæœï¼Œæ—¨åœ¨è§£å†³ä»¥å¾€ç ”ç©¶ä¸»è¦å…³æ³¨sRGBç›‘ç£è€Œå¿½è§†é¢œè‰²ç©ºé—´å½±å“çš„é—®é¢˜ã€‚å—åŒå…‰æºåŒè‰²åå°„(BIDR)æ¨¡å‹çš„å¯å‘ï¼Œä½œè€…æå‡ºåœ¨log RGBç©ºé—´è¿›è¡Œå­¦ä¹ å¯ä»¥ç®€åŒ–å…‰ç…§ä¸åå°„ç‡çš„åˆ†ç¦»ï¼Œä»è€Œè·å¾—æ›´ç´§å‡‘ä¸”æœ‰æ•ˆçš„åœºæ™¯è¡¨ç¤ºã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡GoProç›¸æœºé‡‡é›†æ•°æ®å¹¶åœ¨linearã€sRGBã€GPLogå’Œlog RGBç­‰å¤šç§é¢œè‰²ç©ºé—´ä¸‹è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨æ¸²æŸ“å’Œè®¡ç®—æŸå¤±å‰å°†å…¶ç»Ÿä¸€è½¬æ¢ä»¥ç¡®ä¿å…¬å¹³å¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œlog RGBç©ºé—´åœ¨ä¿æŒç›¸åŒä½æ·±è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œä¸ä»…æ˜¾è‘—æå‡äº†æ¸²æŸ“è´¨é‡å’Œåœºæ™¯é²æ£’æ€§ï¼Œåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹çš„è¡¨ç°ä¹Ÿä¼˜äºå…¶ä»–ç©ºé—´ã€‚é’ˆå¯¹ä¸åŒç½‘ç»œè§„æ¨¡å’ŒNeRFå˜ä½“çš„åˆ†æè¿›ä¸€æ­¥è¯å®äº†è¿™ç§å¯¹æ•°ç©ºé—´ä¼˜åŠ¿å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§å’Œç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The 36th British Machine Vision Conference",
      "pdf_url": "https://arxiv.org/pdf/2512.09375v1",
      "published_date": "2025-12-10 07:12:33 UTC",
      "updated_date": "2025-12-10 07:12:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:34:12.944590+00:00"
    },
    {
      "arxiv_id": "2512.09355v1",
      "title": "Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality",
      "title_zh": "åŸºäºå­å›¾ GNN çš„åˆ†æ”¯ç­–ç•¥ï¼šç†è®ºæ½œåŠ›ä¸å®é™…ç°çŠ¶çš„æ¢ç©¶",
      "authors": [
        "Junru Zhou",
        "Yicheng Wang",
        "Pan Li"
      ],
      "abstract": "Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å­å›¾å›¾ç¥ç»ç½‘ç»œ(Subgraph GNNs)åœ¨æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’(Mixed-Integer Linear Programming, MILP)â€œå­¦ä¹ åˆ†æ”¯â€ç­–ç•¥ä¸­çš„ç†è®ºæ½œåŠ›ä¸å®é™…åº”ç”¨è¡¨ç°ã€‚ç ”ç©¶é¦–å…ˆåœ¨ç†è®ºä¸Šè¯æ˜äº†è¡¨è¾¾èƒ½åŠ›ä¸¥æ ¼ä½äº3-WLçš„èŠ‚ç‚¹é”šå®šSubgraph GNNsè¶³ä»¥é€¼è¿‘å¼ºåˆ†æ”¯(Strong Branching)åˆ†æ•°ï¼Œä¸ºåˆ†æ”¯å†³ç­–æä¾›äº†æ¯”æ ‡å‡†æ¶ˆæ¯ä¼ é€’å›¾ç¥ç»ç½‘ç»œ(MPNNs)æ›´å¼ºçš„ç†è®ºæ”¯æ’‘ã€‚ç„¶è€Œï¼Œåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°æ­ç¤ºäº†ç†è®ºä¸ç°å®ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚å°½ç®¡Subgraph GNNsåœ¨ç†è®ºä¸Šèƒ½æä¾›æ›´ä¼˜çš„åˆ†æ”¯å†³ç­–ï¼Œä½†å…¶$O(n)$çš„å¤æ‚åº¦å¼€é”€å¼•å‘äº†ä¸¥é‡çš„å†…å­˜ç“¶é¢ˆï¼Œå¯¼è‡´å®é™…æ±‚è§£é€Ÿåº¦åè€Œæ…¢äºMPNNså’Œä¼ ç»Ÿå¯å‘å¼ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å½“å‰çš„MILPåˆ†æ”¯ä»»åŠ¡ä¸­ï¼Œé«˜è¡¨è¾¾æ€§GNNå¸¦æ¥çš„å†³ç­–å¢ç›Šå°šä¸è¶³ä»¥å¼¥è¡¥å…¶æ²‰é‡çš„è®¡ç®—ä»£ä»·ã€‚è¿™ä¸€å‘ç°æç¤ºæœªæ¥çš„ç ”ç©¶æ–¹å‘åº”èšç„¦äºå¦‚ä½•åœ¨æå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09355v1",
      "published_date": "2025-12-10 06:29:25 UTC",
      "updated_date": "2025-12-10 06:29:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:34:18.837685+00:00"
    },
    {
      "arxiv_id": "2601.04210v1",
      "title": "Complexity Agnostic Recursive Decomposition of Thoughts",
      "title_zh": "CARDï¼šå¤æ‚åº¦æ— å…³çš„æ€ç»´é€’å½’åˆ†è§£",
      "authors": [
        "Kaleem Ullah Qasim",
        "Jiashu Zhang",
        "Hafiz Saif Ur Rehman"
      ],
      "abstract": "Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem specific difficulty. We introduce CARD (Complexity Agnostic Recursive Decomposition), a framework that predicts problem complexity before generation and adapts decomposition accordingly. Our system comprises MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model predicting 30 fine-grained features from question text and a two-stage recursive solver: (1) hierarchical decomposition into K steps based on task profile and (2) per-step thought budget allocation (1, 5-9, or 10 thoughts) via recursive MRCE profiling. Evaluated on three reasoning models (Qwen3-0.6B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen3-1.7B), CARD achieves 81.4% to 89.2% accuracy on GSM8K while reducing token cost by 1.88x to 2.40x compared to fixed decomposition baselines. On MATH-500, CARD reaches 75.1 to 86.8% accuracy using 1.71x to 5.74x fewer tokens. Our results demonstrate that preemptive complexity estimation enables both higher accuracy and significant efficiency gains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šæ­¥æ¨ç†ä»»åŠ¡æ—¶å› é‡‡ç”¨å›ºå®šæ¨ç†ç­–ç•¥è€Œå¿½ç•¥é—®é¢˜éš¾åº¦çš„å±€é™æ€§ï¼Œæå‡ºäº† CARD (Complexity Agnostic Recursive Decomposition) æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨ç”Ÿæˆå‰é¢„å…ˆè¯„ä¼°é—®é¢˜å¤æ‚åº¦å¹¶æ®æ­¤åŠ¨æ€è°ƒæ•´åˆ†è§£ç­–ç•¥ï¼Œå…¶æ ¸å¿ƒç»„ä»¶ MRCE (Multi-dimensional Reasoning Complexity Estimator) åˆ©ç”¨ 0.6B çš„ Qwen æ¨¡å‹é¢„æµ‹ 30 ç§ç»†ç²’åº¦ç‰¹å¾ã€‚æ±‚è§£è¿‡ç¨‹åˆ†ä¸ºå±‚æ¬¡åŒ–åˆ†è§£å’Œé€’å½’å¼çš„æ¯æ­¥æ€è€ƒé¢„ç®— (thought budget) åˆ†é…ä¸¤ä¸ªé˜¶æ®µï¼Œé€šè¿‡ MRCE çš„é€’å½’åˆ†æå®ç°ç²¾å‡†æ§åˆ¶ã€‚åœ¨ GSM8K å’Œ MATH-500 çš„è¯„ä¼°ä¸­ï¼ŒCARD åœ¨å¤šä¸ªæ¨ç†æ¨¡å‹ä¸Šå‡å®ç°äº† 81.4% è‡³ 89.2% çš„é«˜å‡†ç¡®ç‡ï¼Œä¸”æ¯”å›ºå®šåˆ†è§£åŸºçº¿å‡å°‘äº† 1.88 å€è‡³ 5.74 å€çš„ token æ¶ˆè€—ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒæŠ¢å å¼çš„å¤æ‚åº¦è¯„ä¼°ä¸ä»…èƒ½æå‡å¤šæ­¥æ¨ç†çš„å‡†ç¡®æ€§ï¼Œè¿˜èƒ½å¸¦æ¥æ˜¾è‘—çš„è®¡ç®—æ•ˆç‡å¢ç›Šã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.CL",
      "comment": "4",
      "pdf_url": "https://arxiv.org/pdf/2601.04210v1",
      "published_date": "2025-12-10 06:03:42 UTC",
      "updated_date": "2025-12-10 06:03:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:34:41.670207+00:00"
    },
    {
      "arxiv_id": "2512.09340v1",
      "title": "Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration",
      "title_zh": "è·¨è¶Šå¿ƒæ™ºä¸æ¨¡å‹çš„è§†è§‰åˆ†ç±»ï¼šäººç±»æ ‡æ³¨ä¸ç¥ç»ç¬¦å·é›†æˆçš„è®¤çŸ¥åˆ†æ",
      "authors": [
        "Chethana Prasad Kabgere"
      ],
      "abstract": "Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»ä¸äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨å¤„ç†ä½åˆ†è¾¨ç‡ã€æ„ŸçŸ¥é€€åŒ–ï¼ˆperceptually degradedï¼‰çš„æ¨¡ç³Šè§†è§‰åˆºæ¿€æ—¶çš„å·®å¼‚ã€‚ä½œè€…åŸºäºè®¡ç®—è®¤çŸ¥ç§‘å­¦ã€è®¤çŸ¥æ¶æ„åŠè¿æ¥ä¸»ä¹‰-ç¬¦å·æ··åˆæ¨¡å‹ï¼Œå¯¹æ¯”äº†äººç±»çš„ç±»æ¯”æ¨ç†ï¼ˆanalogical reasoningï¼‰ä¸æ·±åº¦ç¥ç»ç½‘ç»œåŸºäºç‰¹å¾çš„å¤„ç†ï¼ˆfeature-based processingï¼‰æ–¹å¼ã€‚ç ”ç©¶ç»“åˆäº†Marrçš„ä¸‰å±‚å‡è®¾å’ŒGrad-CAMå¯è§†åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ACT-Rå’ŒSoarç­‰æ¨¡å‹æ­ç¤ºäº†äººç±»åœ¨ä¸ç¡®å®šæ€§ä¸‹çš„å±‚æ¬¡åŒ–å¯å‘å¼å†³ç­–ç­–ç•¥ã€‚åˆ†æç»“æœæŒ‡å‡ºäº†ç”Ÿç‰©ä¸äººå·¥ç³»ç»Ÿåœ¨è¡¨ç¤ºã€æ¨ç†åŠç½®ä¿¡åº¦æ ¡å‡†ï¼ˆconfidence calibrationï¼‰æ–¹é¢çš„å…³é”®å¹³è¡Œç‚¹ä¸åˆ†æ­§ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æœªæ¥å‘å±•ç¥ç»ç¬¦å·ï¼ˆNeuro-Symbolicï¼‰æ¶æ„çš„å¿…è¦æ€§ï¼Œå³é€šè¿‡å°†ç»“æ„åŒ–ç¬¦å·æ¨ç†ä¸è¿æ¥ä¸»ä¹‰è¡¨ç¤ºç›¸ç»“åˆã€‚è¿™ä¸€æ–¹å‘æ—¨åœ¨æ„å»ºç¬¦åˆå…·èº«æ€§ï¼ˆembodimentï¼‰å’Œè®¤çŸ¥å¯¹é½ï¼ˆcognitive alignmentï¼‰åŸåˆ™çš„AIç³»ç»Ÿï¼Œä»è€Œå®ç°æ›´é«˜æ€§èƒ½ã€å¯è§£é‡Šæ€§ä¸”å…·å¤‡è®¤çŸ¥åŸºç¡€çš„æ™ºèƒ½ä½“ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 3 figures. Research manuscript based on the final project for CS6795 (Introduction to Cognitive Science), Georgia Tech",
      "pdf_url": "https://arxiv.org/pdf/2512.09340v1",
      "published_date": "2025-12-10 05:58:12 UTC",
      "updated_date": "2025-12-10 05:58:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:36:38.523654+00:00"
    },
    {
      "arxiv_id": "2512.15743v1",
      "title": "Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions",
      "title_zh": "Prompt-to-Partsï¼šé¢å‘ç‰©ç†ç»„è£…ä¸å¯æ‰©å±•æŒ‡ä»¤ç”Ÿæˆçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½",
      "authors": [
        "David Noever"
      ],
      "abstract": "We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel \"bag of bricks\" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a \"bag of words\" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Prompt-to-Partsï¼Œä¸€ä¸ªæ—¨åœ¨å°†è‡ªç„¶è¯­è¨€æè¿°è½¬åŒ–ä¸ºå¯ç‰©ç†å®ç°ç»„è£…æŒ‡ä»¤çš„æ¡†æ¶ã€‚ä¸æ— çº¦æŸçš„text-to-3Dæ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡ç¦»æ•£é›¶ä»¶è¯æ±‡è¡¨è¿è¡Œï¼Œå¼ºåˆ¶æ‰§è¡Œå‡ ä½•æœ‰æ•ˆæ€§ã€è¿æ¥çº¦æŸå’Œå¯å»ºé€ æ€§æ’åºã€‚é€šè¿‡åˆ©ç”¨LDrawä½œä¸ºæ–‡æœ¬ä¸°å¯Œçš„ä¸­é—´è¡¨ç¤ºï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•å¼•å¯¼Large Language Modelsç”ŸæˆåŒ…å«è¶…è¿‡3000ä¸ªç»„ä»¶çš„ç§¯æœ¨åŸå‹ç»„è£…åºåˆ—ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªPythonåº“ç”¨äºç¨‹åºåŒ–æ¨¡å‹ç”Ÿæˆï¼Œå¹¶åœ¨å«æ˜Ÿã€é£æœºåŠå»ºç­‘ç­‰å¤æ‚é¢†åŸŸéªŒè¯äº†å…¶å¯æ‰©å±•æ€§ä¸é«˜ä¿çœŸåº¦ã€‚è¿™ç§â€œbag of bricksâ€æ–¹æ³•ä½œä¸ºä¸€ç§ç‰©ç†APIï¼Œæœ‰æ•ˆå¼¥è¡¥äº†æ‰©æ•£æ¨¡å‹å’ŒCADæ¨¡å‹åœ¨å¤æ‚ç»„è£…å’Œç»„ä»¶äº¤æ¢æ”¯æŒæ–¹é¢çš„ä¸è¶³ï¼Œä¸ºåˆ¶é€ ä¸šå’Œå·¥ç¨‹åŸå‹çš„è‡ªåŠ¨åŒ–è®¾è®¡å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.15743v1",
      "published_date": "2025-12-10 05:55:33 UTC",
      "updated_date": "2025-12-10 05:55:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:34:53.515900+00:00"
    },
    {
      "arxiv_id": "2512.09319v1",
      "title": "Efficiency-Aware Computational Intelligence for Resource-Constrained Manufacturing Toward Edge-Ready Deployment",
      "title_zh": "é¢å‘è¾¹ç¼˜å°±ç»ªéƒ¨ç½²çš„èµ„æºå—é™åˆ¶é€ æ•ˆç‡æ„ŸçŸ¥è®¡ç®—æ™ºèƒ½",
      "authors": [
        "Qianyu Zhou"
      ],
      "abstract": "Industrial cyber physical systems operate under heterogeneous sensing, stochastic dynamics, and shifting process conditions, producing data that are often incomplete, unlabeled, imbalanced, and domain shifted. High-fidelity datasets remain costly, confidential, and slow to obtain, while edge devices face strict limits on latency, bandwidth, and energy. These factors restrict the practicality of centralized deep learning, hinder the development of reliable digital twins, and increase the risk of error escape in safety-critical applications. Motivated by these challenges, this dissertation develops an efficiency grounded computational framework that enables data lean, physics-aware, and deployment ready intelligence for modern manufacturing environments. The research advances methods that collectively address core bottlenecks across multimodal and multiscale industrial scenarios. Generative strategies mitigate data scarcity and imbalance, while semi-supervised learning integrates unlabeled information to reduce annotation and simulation demands. Physics-informed representation learning strengthens interpretability and improves condition monitoring under small-data regimes. Spatially aware graph-based surrogate modeling provides efficient approximation of complex processes, and an edge cloud collaborative compression scheme supports real-time signal analytics under resource constraints. The dissertation also extends visual understanding through zero-shot vision language reasoning augmented by domain specific retrieval, enabling generalizable assessment in previously unseen scenarios. Together, these developments establish a unified paradigm of data efficient and resource aware intelligence that bridges laboratory learning with industrial deployment, supporting reliable decision-making across diverse manufacturing systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£åˆ¶é€ ç¯å¢ƒä¸­çš„å¼‚æ„æ„ŸçŸ¥ã€éšæœºåŠ¨æ€åŠèµ„æºå—é™ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªä»¥æ•ˆç‡ä¸ºæ ¸å¿ƒçš„è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é¢å‘è¾¹ç¼˜éƒ¨ç½²(Edge-Ready Deployment)çš„æ•°æ®ç²¾ç®€ä¸ç‰©ç†æ„ŸçŸ¥æ™ºèƒ½ã€‚ä¸ºäº†è§£å†³å·¥ä¸šåœºæ™¯ä¸­æ•°æ®ç¼ºå¤±å’Œæ ‡æ³¨æˆæœ¬é«˜çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†ç”Ÿæˆç­–ç•¥(Generative strategies)å’ŒåŠç›‘ç£å­¦ä¹ (Semi-supervised learning)æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†å¯¹é«˜è´¨é‡æ•°æ®é›†çš„ä¾èµ–ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥ç‰©ç†çŸ¥è§‰è¡¨å¾å­¦ä¹ (Physics-informed representation learning)ä»¥æå‡æ¨¡å‹åœ¨å°æ ·æœ¬ç¯å¢ƒä¸‹çš„å¯è§£é‡Šæ€§ï¼Œå¹¶åˆ©ç”¨ç©ºé—´æ„ŸçŸ¥å›¾è®ºä»£ç†æ¨¡å‹(Graph-based surrogate modeling)å®ç°å¤æ‚åˆ¶é€ è¿‡ç¨‹çš„å¿«é€Ÿé€¼è¿‘ã€‚æ­¤å¤–ï¼Œé€šè¿‡è¾¹ç¼˜äº‘ååŒå‹ç¼©æ–¹æ¡ˆ(Edge-cloud collaborative compression)ä¼˜åŒ–äº†å®æ—¶ä¿¡å·åˆ†æçš„èƒ½æ•ˆï¼Œå¹¶ç»“åˆæ£€ç´¢å¢å¼ºçš„é›¶æ ·æœ¬è§†è§‰è¯­è¨€æ¨ç†(Zero-shot vision language reasoning)å¢å¼ºäº†ç³»ç»Ÿåœ¨æœªçŸ¥åœºæ™¯ä¸‹çš„æ³›åŒ–è¯„ä¼°èƒ½åŠ›ã€‚è¿™äº›æˆæœæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ•°æ®é«˜æ•ˆä¸èµ„æºè§‰å¯Ÿå‹æ™ºèƒ½èŒƒå¼ï¼Œæœ‰æ•ˆæ¡¥æ¥äº†å®éªŒå®¤ç®—æ³•ä¸å·¥ä¸šå®é™…éƒ¨ç½²ï¼Œä¸ºå¤šå…ƒåŒ–åˆ¶é€ ç³»ç»Ÿä¸­çš„å¯é å†³ç­–æä¾›äº†æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "2025, University of Connecticut",
      "pdf_url": "https://arxiv.org/pdf/2512.09319v1",
      "published_date": "2025-12-10 05:08:55 UTC",
      "updated_date": "2025-12-10 05:08:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:34:47.976334+00:00"
    },
    {
      "arxiv_id": "2512.09318v1",
      "title": "Simultaneous Genetic Evolution of Neural Networks for Optimal SFC Embedding",
      "title_zh": "é¢å‘æœ€ä¼˜ SFC åµŒå…¥çš„ç¥ç»ç½‘ç»œåŒæ­¥é—ä¼ è¿›åŒ–",
      "authors": [
        "Theviyanthan Krishnamohan",
        "Lauritz Thamsen",
        "Paul Harvey"
      ],
      "abstract": "The reliance of organisations on computer networks is enabled by network programmability, which is typically achieved through Service Function Chaining. These chains virtualise network functions, link them, and programmatically embed them on networking infrastructure. Optimal embedding of Service Function Chains is an NP-hard problem, with three sub-problems, chain composition, virtual network function embedding, and link embedding, that have to be optimised simultaneously, rather than sequentially, for optimal results. Genetic Algorithms have been employed for this, but existing approaches either do not optimise all three sub-problems or do not optimise all three sub-problems simultaneously. We propose a Genetic Algorithm-based approach called GENESIS, which evolves three sine-function-activated Neural Networks, and funnels their output to a Gaussian distribution and an A* algorithm to optimise all three sub-problems simultaneously. We evaluate GENESIS on an emulator across 48 different data centre scenarios and compare its performance to two state-of-the-art Genetic Algorithms and one greedy algorithm. GENESIS produces an optimal solution for 100% of the scenarios, whereas the second-best method optimises only 71% of the scenarios. Moreover, GENESIS is the fastest among all Genetic Algorithms, averaging 15.84 minutes, compared to an average of 38.62 minutes for the second-best Genetic Algorithm.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœåŠ¡åŠŸèƒ½é“¾(Service Function Chaining, SFC)åµŒå…¥è¿™ä¸€NP-hardé—®é¢˜ï¼Œæå‡ºäº†åä¸ºGENESISçš„é—ä¼ ç®—æ³•(Genetic Algorithms)æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶ä¼˜åŒ–é“¾ç»„åˆ(chain composition)ã€è™šæ‹Ÿç½‘ç»œåŠŸèƒ½åµŒå…¥(virtual network function embedding)å’Œé“¾è·¯åµŒå…¥(link embedding)ä¸‰ä¸ªå­é—®é¢˜ã€‚GENESISé€šè¿‡è¿›åŒ–ä¸‰ä¸ªæ­£å¼¦å‡½æ•°æ¿€æ´»çš„ç¥ç»ç½‘ç»œ(Neural Networks)ï¼Œå¹¶å°†å…¶è¾“å‡ºå¼•å¯¼è‡³é«˜æ–¯åˆ†å¸ƒ(Gaussian distribution)å’ŒA*ç®—æ³•ï¼Œå®ç°äº†å¯¹ä¸‰ä¸ªå­é—®é¢˜çš„åŒæ­¥ä¼˜åŒ–ã€‚åœ¨48ç§ä¸åŒæ•°æ®ä¸­å¿ƒåœºæ™¯çš„ä»¿çœŸå®éªŒä¸­ï¼ŒGENESISåœ¨æ‰€æœ‰åœºæ™¯ä¸­å‡è·å¾—äº†æœ€ä¼˜è§£ï¼Œè€Œè¡¨ç°æ¬¡ä¼˜çš„æ–¹æ³•ä»…èƒ½ä¼˜åŒ–71%çš„åœºæ™¯ã€‚æ­¤å¤–ï¼ŒGENESISåœ¨è¿è¡Œé€Ÿåº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„é—ä¼ ç®—æ³•ï¼Œå¹³å‡è€—æ—¶ä»…ä¸º15.84åˆ†é’Ÿï¼Œç›¸æ¯”æ¬¡ä¼˜ç®—æ³•ç¼©çŸ­äº†çº¦59%çš„æ—¶é—´ã€‚è¯¥ç ”ç©¶æˆæœè¯æ˜äº†åˆ©ç”¨åŒæ­¥è¿›åŒ–çš„ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚ç½‘ç»œä¼˜åŒ–ä»»åŠ¡çš„é«˜æ•ˆæ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09318v1",
      "published_date": "2025-12-10 05:06:13 UTC",
      "updated_date": "2025-12-10 05:06:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:34:50.129373+00:00"
    },
    {
      "arxiv_id": "2512.09317v4",
      "title": "Functional Percolation: Criticality of Form and Function",
      "title_zh": "åŠŸèƒ½æ¸—é€ï¼šå½¢å¼ä¸åŠŸèƒ½çš„ä¸´ç•Œæ€§",
      "authors": [
        "Galen J. Wilkerson"
      ],
      "abstract": "Understanding how network structure constrains and enables information processing is a central problem in the statistical mechanics of interacting systems. Here we study random networks across the structural percolation transition and analyze how connectivity governs realizable input-output transformations under cascade dynamics. Using Erdos-Renyi networks as a minimal ensemble, we examine structural, functional, and information-theoretic observables as functions of mean degree. We find that the emergence of the giant connected component coincides with a sharp transition in realizable information processing: complex input-output response functions become accessible, functional diversity increases rapidly, output entropy rises, and directed information flow, quantified by transfer entropy, extends beyond local neighborhoods. We term this coincidence of structural, functional, and informational transitions functional percolation, referring to a sharp expansion of the space of realizable input-output functions at the percolation threshold. Near criticality, networks exhibit a Pareto-optimal tradeoff between functional complexity and diversity, suggesting that percolation criticality may provide a general organizing principle of information processing capacity in systems with local interactions and propagating influences.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç½‘ç»œç»“æ„å¦‚ä½•çº¦æŸå¹¶ä½¿èƒ½ä¿¡æ¯å¤„ç†ï¼Œé‡ç‚¹åˆ†æäº†éšæœºç½‘ç»œåœ¨ç»“æ„æ¸—é€è½¬å˜ï¼ˆPercolation Transitionï¼‰è¿‡ç¨‹ä¸­è¿æ¥æ€§å¯¹è¾“å…¥è¾“å‡ºè½¬æ¢çš„å½±å“ã€‚é€šè¿‡å¯¹ Erdos-Renyi ç½‘ç»œçš„çº§è”åŠ¨åŠ›å­¦åˆ†æï¼Œç ”ç©¶è€…å‘ç°å·¨å¤§è¿é€šåˆ†é‡ï¼ˆGiant Connected Componentï¼‰çš„å‡ºç°ä¸ä¿¡æ¯å¤„ç†èƒ½åŠ›çš„å‰§çƒˆè½¬å˜åœ¨æ—¶é—´ä¸Šé‡åˆã€‚åœ¨è¿™ä¸€é˜ˆå€¼å¤„ï¼Œå¤æ‚å“åº”å‡½æ•°å˜å¾—å¯è¡Œï¼ŒåŠŸèƒ½å¤šæ ·æ€§å’Œè¾“å‡ºç†µè¿…é€Ÿå¢åŠ ï¼Œä¸”é€šè¿‡ä¼ é€’ç†µï¼ˆTransfer Entropyï¼‰é‡åŒ–çš„å®šå‘ä¿¡æ¯æµå¼€å§‹è¶…è¶Šå±€éƒ¨é‚»åŸŸã€‚ç ”ç©¶è€…å°†è¿™ç§ç»“æ„ã€åŠŸèƒ½ä¸ä¿¡æ¯è½¬å˜çš„åŒæ­¥ç°è±¡å®šä¹‰ä¸ºâ€œåŠŸèƒ½æ¸—é€â€ï¼ˆFunctional Percolationï¼‰ï¼Œåæ˜ äº†å¯å®ç°åŠŸèƒ½ç©ºé—´åœ¨æ¸—é€é˜ˆå€¼å¤„çš„æ€¥å‰§æ‰©å¼ ã€‚å®éªŒè¡¨æ˜ï¼Œç½‘ç»œåœ¨ä¸´ç•Œç‚¹é™„è¿‘å±•ç°å‡ºåŠŸèƒ½å¤æ‚æ€§ä¸å¤šæ ·æ€§ä¹‹é—´çš„å¸•ç´¯æ‰˜æœ€ä¼˜ï¼ˆPareto-optimalï¼‰æƒè¡¡ã€‚è¿™ä¸€å‘ç°æš—ç¤ºæ¸—é€ä¸´ç•Œæ€§å¯èƒ½æ˜¯å…·æœ‰å±€éƒ¨äº¤äº’å’Œå½±å“ä¼ æ’­ç³»ç»Ÿä¸­ï¼Œä¿¡æ¯å¤„ç†èƒ½åŠ›çš„ä¸€ç§é€šç”¨ç»„ç»‡åŸåˆ™ã€‚",
      "categories": [
        "physics.soc-ph",
        "cond-mat.stat-mech",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "8 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.09317v4",
      "published_date": "2025-12-10 05:05:10 UTC",
      "updated_date": "2026-01-13 07:27:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:35:02.197340+00:00"
    },
    {
      "arxiv_id": "2512.09313v1",
      "title": "Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices",
      "title_zh": "Hetero-SplitEEï¼šé¢å‘å¼‚æ„ç‰©è”ç½‘è®¾å¤‡ä¸”é›†æˆæ—©æœŸé€€å‡ºæœºåˆ¶çš„ç¥ç»ç½‘ç»œæ‹†åˆ†å­¦ä¹ ",
      "authors": [
        "Yuki Oda",
        "Yuta Ono",
        "Hiroshi Nakamura",
        "Hideki Takase"
      ],
      "abstract": "The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ Split Learning æ–¹æ³•å› å‡è®¾å®¢æˆ·ç«¯åŒè´¨åŒ–è€Œéš¾ä»¥åœ¨å¼‚æ„ IoT ç¯å¢ƒä¸­ç»Ÿä¸€åˆ‡åˆ†ç‚¹(cut layers)çš„é—®é¢˜ï¼Œæå‡ºäº† Hetero-SplitEE æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å¼‚æ„ Early Exits é›†æˆåˆ°å±‚æ¬¡åŒ–è®­ç»ƒä½“ç³»ä¸­ï¼Œèµ‹äºˆæ¯ä¸ªå®¢æˆ·ç«¯æ ¹æ®å…¶è®¡ç®—èƒ½åŠ›è‡ªä¸»é€‰æ‹©åˆ‡åˆ†å±‚çš„çµæ´»æ€§ã€‚ä¸ºäº†ä¼˜åŒ–åä½œè¿‡ç¨‹ï¼Œç ”ç©¶è¿›ä¸€æ­¥è®¾è®¡äº† Sequential é¡ºåºè®­ç»ƒç­–ç•¥ä»¥é™ä½å¼€é”€ï¼Œä»¥åŠ Averaging å¹¶è¡Œç­–ç•¥é€šè¿‡è·¨å±‚èšåˆå®ç°é«˜æ•ˆåŒæ­¥ã€‚åœ¨ CIFAR-10ã€CIFAR-100 å’Œ STL-10 æ•°æ®é›†ä¸ŠåŸºäº ResNet-18 çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆåº”å¯¹å¤šæ ·åŒ–è®¡ç®—çº¦æŸçš„åŒæ—¶ä¿æŒäº†ä¼˜ç§€çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€æ–¹æ¡ˆæ˜¾è‘—æå‡äº†åˆ†å¸ƒå¼å­¦ä¹ çš„å¯æ‰©å±•æ€§ï¼Œä¸ºå¼‚æ„ IoT ç”Ÿæ€ç³»ç»Ÿä¸­çš„åä½œå¼æ·±åº¦å­¦ä¹ éƒ¨ç½²æä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages. Accepted at MCSoC 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.09313v1",
      "published_date": "2025-12-10 04:58:26 UTC",
      "updated_date": "2025-12-10 04:58:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:35:05.525537+00:00"
    },
    {
      "arxiv_id": "2601.04208v1",
      "title": "LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach",
      "title_zh": "é¢å‘å¯è§£é‡Šå•†ä¸šå†³ç­–çš„å¤§è¯­è¨€æ¨¡å‹ï¼šä¸€ç§å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•",
      "authors": [
        "Xiang Cheng",
        "Wen Wang",
        "Anindya Ghose"
      ],
      "abstract": "Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•†ä¸šå†³ç­–ä¸­äººå·¥æ™ºèƒ½æ¨¡å‹é€»è¾‘ä¸é€æ˜åŠç°æœ‰å¯è§£é‡ŠAIæŠ€æœ¯ç¼ºä¹è¿è´¯å™è¿°çš„é—®é¢˜ï¼Œæå‡ºäº†LEXMAæ¡†æ¶ï¼Œå³ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ å¾®è°ƒ (Reinforcement Learning Fine-Tuning) çš„è§£é‡Šç”Ÿæˆæ–¹æ¡ˆã€‚è¯¥æ¡†æ¶ç»“åˆäº†åæ€å¢å¼ºç›‘ç£å¾®è°ƒ (Reflection-Augmented Supervised Fine-Tuning) ä¸ä¸¤é˜¶æ®µçš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (Group Relative Policy Optimization, GRPO)ï¼Œåœ¨æ— éœ€äººå·¥æ ‡æ³¨è§£é‡Šçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¾®è°ƒä¸¤ç»„ç‹¬ç«‹çš„å‚æ•°æ¥åŒæ—¶ç¡®ä¿å†³ç­–æ­£ç¡®æ€§å¹¶æ»¡è¶³å¤šå—ä¼— (Multi-Audience) çš„é£æ ¼éœ€æ±‚ã€‚åœ¨æŠµæŠ¼è´·æ¬¾å®¡æ‰¹åœºæ™¯çš„å®éªŒä¸­ï¼ŒLEXMA çš„é¢„æµ‹æ€§èƒ½æ˜¾è‘—ä¼˜äºå…¶ä»– LLM åŸºçº¿æ¨¡å‹ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„ä¸“å®¶ç«¯è§£é‡Šæ›´ä¾§é‡äºé£é™©åˆ†æï¼Œè€Œæ¶ˆè´¹è€…ç«¯è§£é‡Šåˆ™æ›´æ¸…æ™°ã€å…·å¤‡å¯æ“ä½œæ€§ä¸”è¯­æ°”ç¤¼è²Œã€‚è¯¥ç ”ç©¶ä¸ºæå‡å•†ä¸šå†³ç­–è§£é‡Šè´¨é‡æä¾›äº†ä¸€ç§ç³»ç»Ÿä¸”é«˜æˆæœ¬æ•ˆç›Šçš„å¾®è°ƒè·¯å¾„ï¼Œä¸ºé€æ˜äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è§„æ¨¡åŒ–éƒ¨ç½²æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.04208v1",
      "published_date": "2025-12-10 04:16:31 UTC",
      "updated_date": "2025-12-10 04:16:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:35:04.888581+00:00"
    },
    {
      "arxiv_id": "2512.09292v1",
      "title": "Identifying Bias in Machine-generated Text Detection",
      "title_zh": "è¯†åˆ«æœºå™¨ç”Ÿæˆæ–‡æœ¬æ£€æµ‹ä¸­çš„åè§",
      "authors": [
        "Kevin Stowe",
        "Svetlana Afanaseva",
        "Rodolfo Raimundo",
        "Yitao Sun",
        "Kailash Patil"
      ],
      "abstract": "The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‹±è¯­ Machine-generated Text Detection ç³»ç»Ÿä¸­çš„æ½œåœ¨åå·®ï¼Œæ—¨åœ¨æ­ç¤ºæ­¤ç±»æ£€æµ‹æ¨¡å‹å¯èƒ½äº§ç”Ÿçš„ç¤¾ä¼šè´Ÿé¢å½±å“ã€‚ä½œè€…é€šè¿‡æ„å»ºå­¦ç”Ÿè®ºæ–‡æ•°æ®é›†ï¼Œåˆ©ç”¨ Regression-based Models å’Œ Subgroup Analysis å¯¹ 16 ç§æ£€æµ‹ç³»ç»Ÿåœ¨ Genderã€Race/Ethnicityã€English-language Learner (ELL) Status ä»¥åŠ Economic Status å››ä¸ªç»´åº¦è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ä¸åŒç³»ç»Ÿçš„åå·®è¡¨ç°å¹¶ä¸ä¸€è‡´ï¼Œä½†æ™®éå­˜åœ¨å°†å¼±åŠ¿ç¾¤ä½“ä½œå“è¯¯åˆ¤ä¸ºæœºå™¨ç”Ÿæˆçš„å€¾å‘ï¼Œå°¤å…¶æ˜¯ ELL å­¦ç”Ÿçš„è®ºæ–‡è¢«è¯¯åˆ¤çš„æ¦‚ç‡æ˜¾è‘—æ›´é«˜ã€‚å®éªŒè¿›ä¸€æ­¥æ­ç¤ºäº†ç§æ—ä¸è¯­è¨€çŠ¶æ€çš„äº¤å‰åå·®ï¼Œå³éç™½äºº ELL å­¦ç”Ÿçš„è®ºæ–‡è¢«è¯¯åˆ¤çš„æ¯”ä¾‹è¿œé«˜äºç™½äººåŒç±»ç¾¤ä½“ã€‚æ­¤å¤–ï¼Œç»æµå›°éš¾å­¦ç”Ÿçš„è®ºæ–‡è¢«åˆ¤å®šä¸ºæœºå™¨ç”Ÿæˆçš„å¯èƒ½æ€§åè€Œè¾ƒä½ã€‚æœ€åçš„äººæœºå¯¹æ¯”è¯„ä¼°æ˜¾ç¤ºï¼Œå°½ç®¡äººç±»åœ¨ Detection ä»»åŠ¡ä¸Šçš„æ•´ä½“è¡¨ç°è¾ƒå·®ï¼Œä½†åœ¨æ‰€ç ”ç©¶çš„å±æ€§ä¸Šå¹¶æœªæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ Biasï¼Œè¿™ä¸è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„è¡¨ç°å½¢æˆäº†é²œæ˜å¯¹æ¯”ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 2 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.09292v1",
      "published_date": "2025-12-10 03:34:12 UTC",
      "updated_date": "2025-12-10 03:34:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:35:12.093828+00:00"
    },
    {
      "arxiv_id": "2512.09264v1",
      "title": "FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection",
      "title_zh": "FBA$^2$Dï¼šé’ˆå¯¹ AI ç”Ÿæˆå›¾åƒæ£€æµ‹çš„åŸºäºé¢‘ç‡çš„é»‘ç›’æ”»å‡»",
      "authors": [
        "Xiaojing Chen",
        "Dan Li",
        "Lijun Peng",
        "Jun YanÅetter",
        "Zhiqing Guo",
        "Junyang Chen",
        "Xiao Lan",
        "Zhongjie Ba",
        "Yunfeng DiaoÅetter"
      ],
      "abstract": "The prosperous development of Artificial Intelligence-Generated Content (AIGC) has brought people's anxiety about the spread of false information on social media. Designing detectors for filtering is an effective defense method, but most detectors will be compromised by adversarial samples. Currently, most studies exposing AIGC security issues assume information on model structure and data distribution. In real applications, attackers query and interfere with models that provide services in the form of application programming interfaces (APIs), which constitutes the black-box decision-based attack paradigm. However, to the best of our knowledge, decision-based attacks on AIGC detectors remain unexplored. In this study, we propose \\textbf{FBA$^2$D}: a frequency-based black-box attack method for AIGC detection to fill the research gap. Motivated by frequency-domain discrepancies between generated and real images, we develop a decision-based attack that leverages the Discrete Cosine Transform (DCT) for fine-grained spectral partitioning and selects frequency bands as query subspaces, improving both query efficiency and image quality. Moreover, attacks on AIGC detectors should mitigate initialization failures, preserve image quality, and operate under strict query budgets. To address these issues, we adopt an ``adversarial example soup'' method, averaging candidates from successive surrogate iterations and using the result as the initialization to accelerate the query-based attack. The empirical study on the Synthetic LSUN dataset and GenImage dataset demonstrate the effectiveness of our prosed method. This study shows the urgency of addressing practical AIGC security problems.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å½“å‰ AI-generated Content (AIGC) æ£€æµ‹å™¨åœ¨é»‘ç›’å†³ç­–æ”»å‡»ï¼ˆblack-box decision-based attackï¼‰ä¸‹çš„å®‰å…¨æ€§ç ”ç©¶ç©ºç™½ï¼Œæå‡ºäº†åä¸º FBA$^2$D çš„é¢‘åŸŸé»‘ç›’æ”»å‡»æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Discrete Cosine Transform (DCT) å¯¹å›¾åƒè¿›è¡Œç»†ç²’åº¦çš„å…‰è°±åˆ’åˆ†ï¼Œå¹¶é€‰å–ç‰¹å®šé¢‘æ®µä½œä¸ºæŸ¥è¯¢å­ç©ºé—´ï¼Œæ—¨åœ¨æé«˜æŸ¥è¯¢æ•ˆç‡çš„åŒæ—¶ä¿æŒç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥åº”å¯¹åˆå§‹åŒ–å¤±è´¥å’Œä¸¥æ ¼çš„æŸ¥è¯¢é¢„ç®—é™åˆ¶ï¼Œç ”ç©¶è€…å¼•å…¥äº† \"adversarial example soup\" æŠ€æœ¯ï¼Œé€šè¿‡å¹³å‡è¿ç»­ä»£ç†è¿­ä»£ç”Ÿæˆçš„å€™é€‰æ ·æœ¬æ¥ä¼˜åŒ–åˆå§‹åŒ–è¿‡ç¨‹ï¼Œä»è€Œæ˜¾è‘—åŠ é€Ÿæ”»å‡»ã€‚åœ¨ Synthetic LSUN å’Œ GenImage æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº† FBA$^2$D çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒäº†åœ¨å®é™…åº”ç”¨ä¸­æå‡ AIGC æ£€æµ‹å™¨å®‰å…¨é˜²å¾¡èƒ½åŠ›çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09264v1",
      "published_date": "2025-12-10 02:38:47 UTC",
      "updated_date": "2025-12-10 02:38:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:35:17.596188+00:00"
    },
    {
      "arxiv_id": "2512.09251v1",
      "title": "GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model",
      "title_zh": "GLACIAï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å®ä¾‹æ„ŸçŸ¥ä½ç½®æ¨ç†å†°å·æ¹–åˆ†å‰²",
      "authors": [
        "Lalit Maurya",
        "Saurabh Kaushik",
        "Beth Tellman"
      ],
      "abstract": "Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\\textbf{G}lacial \\textbf{LA}ke segmentation with \\textbf{C}ontextual \\textbf{I}nstance \\textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GLACIAï¼Œä¸€ç§é¦–ä¸ªå°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸åˆ†å‰²èƒ½åŠ›ç›¸ç»“åˆçš„å†°å·æ¹–ç›‘æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œ(CNNs)å’Œè§†è§‰Transformer(ViTs)ç¼ºä¹é«˜å±‚è¯­ä¹‰å’Œå¯è§£é‡Šæ€§æ¨ç†çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¸ä»…èƒ½ç”Ÿæˆç²¾ç¡®çš„åˆ†å‰²æ©ç ï¼Œè¿˜èƒ½è¾“å‡ºç›¸åº”çš„ç©ºé—´æ¨ç†ç»“æœã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æ„å»ºäº†GLake-Posæ•°æ®é›†ï¼Œé€šè¿‡ç©ºé—´å®šä½çš„é—®ç­”å¯¹å¡«è¡¥äº†é¥æ„Ÿé¢†åŸŸå®ä¾‹æ„ŸçŸ¥ä½ç½®æ¨ç†æ•°æ®çš„ç©ºç™½ã€‚å®éªŒè¡¨æ˜ï¼ŒGLACIAåœ¨mIoUæŒ‡æ ‡ä¸Šè¾¾åˆ°87.30ï¼Œæ€§èƒ½ä¼˜äºåœ°ç†åŸºç¡€æ¨¡å‹(Geo-foundation models)å’Œå„ç±»ä¸»æµåˆ†å‰²æ¨¡å‹ã€‚é€šè¿‡æ”¯æŒè‡ªç„¶è¯­è¨€äº¤äº’ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†å†°å·ç¯å¢ƒå˜åŒ–èƒŒæ™¯ä¸‹çš„å†³ç­–æ•ˆç‡ä¸å¯è§£é‡Šæ€§ï¼Œä¸ºç¾å®³é¢„è­¦å’Œæ”¿ç­–åˆ¶å®šæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09251v1",
      "published_date": "2025-12-10 02:11:48 UTC",
      "updated_date": "2025-12-10 02:11:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:37:05.850041+00:00"
    },
    {
      "arxiv_id": "2512.09244v1",
      "title": "A Clinically Interpretable Deep CNN Framework for Early Chronic Kidney Disease Prediction Using Grad-CAM-Based Explainable AI",
      "title_zh": "åŸºäº Grad-CAM å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„ä¸´åºŠå¯è§£é‡Šæ·±åº¦ CNN æ—©æœŸæ…¢æ€§è‚¾è„ç—…é¢„æµ‹æ¡†æ¶",
      "authors": [
        "Anas Bin Ayub",
        "Nilima Sultana Niha",
        "Md. Zahurul Haque"
      ],
      "abstract": "Chronic Kidney Disease (CKD) constitutes a major global medical burden, marked by the gradual deterioration of renal function, which results in the impaired clearance of metabolic waste and disturbances in systemic fluid homeostasis. Owing to its substantial contribution to worldwide morbidity and mortality, the development of reliable and efficient diagnostic approaches is critically important to facilitate early detection and prompt clinical management. This study presents a deep convolutional neural network (CNN) for early CKD detection from CT kidney images, complemented by class balancing using Synthetic Minority Over-sampling Technique (SMOTE) and interpretability via Gradient-weighted Class Activation Mapping (Grad-CAM). The model was trained and evaluated on the CT KIDNEY DATASET, which contains 12,446 CT images, including 3,709 cyst, 5,077 normal, 1,377 stone, and 2,283 tumor cases. The proposed deep CNN achieved a remarkable classification performance, attaining 100% accuracy in the early detection of chronic kidney disease (CKD). This significant advancement demonstrates strong potential for addressing critical clinical diagnostic challenges and enhancing early medical intervention strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ…¢æ€§è‚¾è„ç—… (Chronic Kidney Disease, CKD) é€ æˆçš„å…¨çƒåŒ»ç–—è´Ÿæ‹…ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ (Deep CNN) çš„æ—©æœŸæ£€æµ‹æ¡†æ¶ã€‚ç ”ç©¶é€šè¿‡åˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·æŠ€æœ¯ (SMOTE) è§£å†³äº†æ•°æ®ç±»åˆ«å¤±è¡¡é—®é¢˜ï¼Œå¹¶åˆ©ç”¨åŸºäºæ¢¯åº¦çš„ç±»æ¿€æ´»æ˜ å°„ (Grad-CAM) å®ç°äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œç¡®ä¿å…¶å…·å¤‡ä¸´åºŠå‚è€ƒä»·å€¼ã€‚è¯¥æ¡†æ¶åœ¨åŒ…å« 12,446 å¼ å›¾åƒçš„ CT KIDNEY DATASET ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†å›Šè‚¿ã€æ­£å¸¸ã€ç»“çŸ³å’Œè‚¿ç˜¤å››ç±»ç—…ä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ·±åº¦æ¨¡å‹åœ¨ CKD æ—©æœŸæ£€æµ‹ä¸­å®ç°äº† 100% çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚è¿™ä¸€æ˜¾è‘—è¿›å±•å±•ç¤ºäº†å…¶åœ¨åº”å¯¹ä¸´åºŠè¯Šæ–­æŒ‘æˆ˜å’Œä¼˜åŒ–æ—©æœŸåŒ»ç–—å¹²é¢„ç­–ç•¥æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09244v1",
      "published_date": "2025-12-10 02:03:59 UTC",
      "updated_date": "2025-12-10 02:03:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:37:06.189199+00:00"
    },
    {
      "arxiv_id": "2512.09222v1",
      "title": "CORE: A Conceptual Reasoning Layer for Large Language Models",
      "title_zh": "COREï¼šä¸€ç§å¤§è¯­è¨€æ¨¡å‹æ¦‚å¿µæ¨ç†å±‚",
      "authors": [
        "Vishwas Hegde",
        "Vindhya Shigehalli"
      ],
      "abstract": "Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.",
      "tldr_zh": "é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šè½®äº¤äº’ä¸­å› ç¼ºä¹æŒä¹…å†…éƒ¨è¡¨ç¤ºè€Œå¯¼è‡´çš„æ„å›¾æ¼‚ç§»å’Œæ¨ç†ä¸ä¸€è‡´é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†COREï¼Œä¸€ç§é¢å‘æ¦‚å¿µä¼˜å…ˆ(concept-first)çš„äº¤äº’å±‚ã€‚COREé€šè¿‡ç»“åˆé€šç”¨è®¤çŸ¥ç®—å­(universal cognitive operators)ä¸æŒä¹…çš„Local Conceptï¼Œæ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿæ•æ‰ä»»åŠ¡ã€çº¦æŸã€åå¥½åŠä¸­é—´ç»“æœçš„ç´§å‡‘è¯­ä¹‰çŠ¶æ€ã€‚åœ¨æ¯æ¬¡æ¨¡å‹è°ƒç”¨æ—¶ï¼Œç³»ç»Ÿä»…éœ€æä¾›è¯¥Local Conceptã€ç”¨æˆ·æœ€æ–°æŒ‡ä»¤åŠé€‰å®šç®—å­ï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†é‡æ–°å›æ”¾å®Œæ•´å†å²Tokençš„éœ€æ±‚ã€‚åˆæ­¥çš„åŸå‹ä»¿çœŸæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘çº¦42%ç´¯ç§¯Prompt Tokençš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†å¤šè½®äº¤äº’çš„ç¨³å®šæ€§ã€‚ä½œä¸ºä¸€ç§ä¸æ¨¡å‹æ— å…³çš„æœºåˆ¶ï¼ŒCOREæˆåŠŸå®ç°äº†æ¦‚å¿µæ¨ç†ä¸è¯­è¨€ç”Ÿæˆçš„è§£è€¦ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆã€æ›´ç¨³å®šçš„å¤šè½®å¯¹è¯ç³»ç»Ÿæ¢ç´¢äº†æ–°çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Independent system-level architectural proposal with accompanying proof-of-concept",
      "pdf_url": "https://arxiv.org/pdf/2512.09222v1",
      "published_date": "2025-12-10 01:08:06 UTC",
      "updated_date": "2025-12-10 01:08:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:37:04.425786+00:00"
    },
    {
      "arxiv_id": "2512.09202v1",
      "title": "Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers",
      "title_zh": "ç¥ç»åå¾®åˆ†æ–¹ç¨‹æ±‚è§£å™¨çš„å¼ é‡å‹ç¼©ä¸å…¨é‡åŒ–è®­ç»ƒ",
      "authors": [
        "Jinming Lu",
        "Jiayi Tian",
        "Yequan Zhao",
        "Hai Li",
        "Zheng Zhang"
      ],
      "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a promising paradigm for solving partial differential equations (PDEs) by embedding physical laws into neural network training objectives. However, their deployment on resource-constrained platforms is hindered by substantial computational and memory overhead, primarily stemming from higher-order automatic differentiation, intensive tensor operations, and reliance on full-precision arithmetic. To address these challenges, we present a framework that enables scalable and energy-efficient PINN training on edge devices. This framework integrates fully quantized training, Stein's estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition for weight compression. It contributes three key innovations: (1) a mixed-precision training method that use a square-block MX (SMX) format to eliminate data duplication during backpropagation; (2) a difference-based quantization scheme for the Stein's estimator that mitigates underflow; and (3) a partial-reconstruction scheme (PRS) for TT-Layers that reduces quantization-error accumulation. We further design PINTA, a precision-scalable hardware accelerator, to fully exploit the performance of the framework. Experiments on the 2-D Poisson, 20-D Hamilton-Jacobi-Bellman (HJB), and 100-D Heat equations demonstrate that the proposed framework achieves accuracy comparable to or better than full-precision, uncompressed baselines while delivering 5.5x to 83.5x speedups and 159.6x to 2324.1x energy savings. This work enables real-time PDE solving on edge devices and paves the way for energy-efficient scientific computing at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºç¥ç»åå¾®åˆ†æ–¹ç¨‹(PDE)æ±‚è§£å™¨çš„å¼ é‡å‹ç¼©å’Œå…¨é‡åŒ–è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(PINNs)åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶é¢ä¸´çš„é«˜è®¡ç®—å’Œå†…å­˜å¼€é”€é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†å…¨é‡åŒ–è®­ç»ƒã€åŸºäºStein's Estimator (SE)çš„æ®‹å·®æŸå¤±è®¡ç®—ä»¥åŠTensor-Train (TT)åˆ†è§£æŠ€æœ¯ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬é‡‡ç”¨æ–¹å—MX (SMX)æ ¼å¼çš„æ··åˆç²¾åº¦è®­ç»ƒä»¥æ¶ˆé™¤æ•°æ®é‡å¤ï¼Œé’ˆå¯¹SEçš„å·®åˆ†é‡åŒ–æ–¹æ¡ˆä»¥ç¼“è§£ä¸‹æº¢é—®é¢˜ï¼Œä»¥åŠç”¨äºTTå±‚çš„éƒ¨åˆ†é‡æ„æ–¹æ¡ˆ(PRS)ä»¥å‡å°‘é‡åŒ–è¯¯å·®ç´¯ç§¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜è®¾è®¡äº†åä¸ºPINTAçš„ç²¾åº¦å¯æ‰©å±•ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚å®éªŒåœ¨2-D Poissonã€20-D HJBå’Œ100-D Heatæ–¹ç¨‹ä¸Šè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒæˆ–ä¼˜äºå…¨ç²¾åº¦åŸºçº¿å‡†ç¡®ç‡çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†5.5å€è‡³83.5å€çš„åŠ é€Ÿä»¥åŠ159.6å€è‡³2324.1å€çš„èƒ½æ•ˆæå‡ã€‚è¿™é¡¹å·¥ä½œä¸ºè¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶PDEæ±‚è§£å’Œå¤§è§„æ¨¡èŠ‚èƒ½ç§‘å­¦è®¡ç®—å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "DATE 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.09202v1",
      "published_date": "2025-12-10 00:00:34 UTC",
      "updated_date": "2025-12-10 00:00:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T15:37:08.673740+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 114,
  "processed_papers_count": 114,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T15:42:51.251573+00:00"
}
[
  {
    "arxiv_id": "2501.09878v1",
    "title": "ASTRA: A Scene-aware TRAnsformer-based model for trajectory prediction",
    "authors": [
      "Izzeddin Teeti",
      "Aniket Thomas",
      "Munish Monga",
      "Sachin Kumar",
      "Uddeshya Singh",
      "Andrew Bradley",
      "Biplab Banerjee",
      "Fabio Cuzzolin"
    ],
    "abstract": "We present ASTRA (A} Scene-aware TRAnsformer-based model for trajectory\nprediction), a light-weight pedestrian trajectory forecasting model that\nintegrates the scene context, spatial dynamics, social inter-agent interactions\nand temporal progressions for precise forecasting. We utilised a U-Net-based\nfeature extractor, via its latent vector representation, to capture scene\nrepresentations and a graph-aware transformer encoder for capturing social\ninteractions. These components are integrated to learn an agent-scene aware\nembedding, enabling the model to learn spatial dynamics and forecast the future\ntrajectory of pedestrians. The model is designed to produce both deterministic\nand stochastic outcomes, with the stochastic predictions being generated by\nincorporating a Conditional Variational Auto-Encoder (CVAE). ASTRA also\nproposes a simple yet effective weighted penalty loss function, which helps to\nyield predictions that outperform a wide array of state-of-the-art\ndeterministic and generative models. ASTRA demonstrates an average improvement\nof 27%/10% in deterministic/stochastic settings on the ETH-UCY dataset, and 26%\nimprovement on the PIE dataset, respectively, along with seven times fewer\nparameters than the existing state-of-the-art model (see Figure 1).\nAdditionally, the model's versatility allows it to generalize across different\nperspectives, such as Bird's Eye View (BEV) and Ego-Vehicle View (EVV).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09878v1",
    "published_date": "2025-01-16 23:28:30 UTC",
    "updated_date": "2025-01-16 23:28:30 UTC"
  },
  {
    "arxiv_id": "2501.09858v1",
    "title": "From Explainability to Interpretability: Interpretable Policies in Reinforcement Learning Via Model Explanation",
    "authors": [
      "Peilang Li",
      "Umer Siddique",
      "Yongcan Cao"
    ],
    "abstract": "Deep reinforcement learning (RL) has shown remarkable success in complex\ndomains, however, the inherent black box nature of deep neural network policies\nraises significant challenges in understanding and trusting the decision-making\nprocesses. While existing explainable RL methods provide local insights, they\nfail to deliver a global understanding of the model, particularly in\nhigh-stakes applications. To overcome this limitation, we propose a novel\nmodel-agnostic approach that bridges the gap between explainability and\ninterpretability by leveraging Shapley values to transform complex deep RL\npolicies into transparent representations. The proposed approach offers two key\ncontributions: a novel approach employing Shapley values to policy\ninterpretation beyond local explanations and a general framework applicable to\noff-policy and on-policy algorithms. We evaluate our approach with three\nexisting deep RL algorithms and validate its performance in two classic control\nenvironments. The results demonstrate that our approach not only preserves the\noriginal models' performance but also generates more stable interpretable\npolicies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to Deployable AI (DAI) Workshop at the Thirty-Ninth AAAI\n  Conference on Artificial Intelligence (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2501.09858v1",
    "published_date": "2025-01-16 22:11:03 UTC",
    "updated_date": "2025-01-16 22:11:03 UTC"
  },
  {
    "arxiv_id": "2501.09838v1",
    "title": "CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified Intermediate Representation",
    "authors": [
      "Alex Berian",
      "Daniel Brignac",
      "JhihYang Wu",
      "Natnael Daba",
      "Abhijit Mahalanobis"
    ],
    "abstract": "Geospatial imaging leverages data from diverse sensing modalities-such as EO,\nSAR, and LiDAR, ranging from ground-level drones to satellite views. These\nheterogeneous inputs offer significant opportunities for scene understanding\nbut present challenges in interpreting geometry accurately, particularly in the\nabsence of precise ground truth data. To address this, we propose\nCrossModalityDiffusion, a modular framework designed to generate images across\ndifferent modalities and viewpoints without prior knowledge of scene geometry.\nCrossModalityDiffusion employs modality-specific encoders that take multiple\ninput images and produce geometry-aware feature volumes that encode scene\nstructure relative to their input camera positions. The space where the feature\nvolumes are placed acts as a common ground for unifying input modalities. These\nfeature volumes are overlapped and rendered into feature images from novel\nperspectives using volumetric rendering techniques. The rendered feature images\nare used as conditioning inputs for a modality-specific diffusion model,\nenabling the synthesis of novel images for the desired output modality. In this\npaper, we show that jointly training different modules ensures consistent\ngeometric understanding across all modalities within the framework. We validate\nCrossModalityDiffusion's capabilities on the synthetic ShapeNet cars dataset,\ndemonstrating its effectiveness in generating accurate and consistent novel\nviews across multiple imaging modalities and perspectives.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in the 2025 WACV workshop GeoCV",
    "pdf_url": "http://arxiv.org/pdf/2501.09838v1",
    "published_date": "2025-01-16 20:56:32 UTC",
    "updated_date": "2025-01-16 20:56:32 UTC"
  },
  {
    "arxiv_id": "2501.09825v1",
    "title": "Bridging Language Barriers in Healthcare: A Study on Arabic LLMs",
    "authors": [
      "Nada Saadi",
      "Tathagata Raha",
      "Clément Christophe",
      "Marco AF Pimentel",
      "Ronnie Rajan",
      "Praveen K Kanithi"
    ],
    "abstract": "This paper investigates the challenges of developing large language models\n(LLMs) proficient in both multilingual understanding and medical knowledge. We\ndemonstrate that simply translating medical data does not guarantee strong\nperformance on clinical tasks in the target language. Our experiments reveal\nthat the optimal language mix in training data varies significantly across\ndifferent medical tasks. We find that larger models with carefully calibrated\nlanguage ratios achieve superior performance on native-language clinical tasks.\nFurthermore, our results suggest that relying solely on fine-tuning may not be\nthe most effective approach for incorporating new language knowledge into LLMs.\nInstead, data and computationally intensive pretraining methods may still be\nnecessary to achieve optimal performance in multilingual medical settings.\nThese findings provide valuable guidance for building effective and inclusive\nmedical AI systems for diverse linguistic communities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09825v1",
    "published_date": "2025-01-16 20:24:56 UTC",
    "updated_date": "2025-01-16 20:24:56 UTC"
  },
  {
    "arxiv_id": "2501.09817v1",
    "title": "Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer",
    "authors": [
      "Haoyu Zhang",
      "Raghavendra Ramachandra",
      "Kiran Raja",
      "Christoph Busch"
    ],
    "abstract": "Face morphing attacks have posed severe threats to Face Recognition Systems\n(FRS), which are operated in border control and passport issuance use cases.\nCorrespondingly, morphing attack detection algorithms (MAD) are needed to\ndefend against such attacks. MAD approaches must be robust enough to handle\nunknown attacks in an open-set scenario where attacks can originate from\nvarious morphing generation algorithms, post-processing and the diversity of\nprinters/scanners. The problem of generalization is further pronounced when the\ndetection has to be made on a single suspected image. In this paper, we propose\na generalized single-image-based MAD (S-MAD) algorithm by learning the encoding\nfrom Vision Transformer (ViT) architecture. Compared to CNN-based\narchitectures, ViT model has the advantage on integrating local and global\ninformation and hence can be suitable to detect the morphing traces widely\ndistributed among the face region. Extensive experiments are carried out on\nface morphing datasets generated using publicly available FRGC face datasets.\nSeveral state-of-the-art (SOTA) MAD algorithms, including representative ones\nthat have been publicly evaluated, have been selected and benchmarked with our\nViT-based approach. Obtained results demonstrate the improved detection\nperformance of the proposed S-MAD method on inter-dataset testing (when\ndifferent data is used for training and testing) and comparable performance on\nintra-dataset testing (when the same data is used for training and testing)\nexperimental protocol.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09817v1",
    "published_date": "2025-01-16 20:09:19 UTC",
    "updated_date": "2025-01-16 20:09:19 UTC"
  },
  {
    "arxiv_id": "2501.09804v1",
    "title": "Enhancing Generalization in Chain of Thought Reasoning for Smaller Models",
    "authors": [
      "Maxwell J. Yin",
      "Dingyi Jiang",
      "Yongbing Chen",
      "Boyu Wang",
      "Charles Ling"
    ],
    "abstract": "Chain-of-Thought (CoT) reasoning in smaller language models is a challenging\nnatural language process problem yet highly desirable in many real-life\napplications. Existing CoT knowledge distillation methods often suffer from\noverly conservative memorization in smaller LLMs, leading to low generalization\nconfidence. As fully preserving the CoT ability of teacher model is impossible,\nwe hypothesize that adversarial CoT fine-tuning is crucial for developing\nsmaller LLM with robust CoT generalization. To this end, we propose\n\\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled\nfine-tuning framework that integrates diverse CoT domains. Specifically, PRADA\npioneers two CoT improvements in smaller LLM: (1) Recovering the\ndomain-invariant feature insight which typically lost during distillation with\ndomain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT\nprompt engineering by employing domain-adversarial approaches. We theoretically\ndemonstrate the effectiveness of our approach and empirically show that it\nsignificantly outperforms the state of the arts in a wide range of tasks.\nMoreover, our empirical findings reveal that the smaller LLM, when leveraging\nPRADA, aligns closely with domain knowledge, thereby improving the\nexplainability of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09804v1",
    "published_date": "2025-01-16 19:23:11 UTC",
    "updated_date": "2025-01-16 19:23:11 UTC"
  },
  {
    "arxiv_id": "2501.09755v1",
    "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation",
    "authors": [
      "Philippe Hansen-Estruch",
      "David Yan",
      "Ching-Yao Chung",
      "Orr Zohar",
      "Jialiang Wang",
      "Tingbo Hou",
      "Tao Xu",
      "Sriram Vishwanath",
      "Peter Vajda",
      "Xinlei Chen"
    ],
    "abstract": "Visual tokenization via auto-encoding empowers state-of-the-art image and\nvideo generative models by compressing pixels into a latent space. Although\nscaling Transformer-based generators has been central to recent advances, the\ntokenizer component itself is rarely scaled, leaving open questions about how\nauto-encoder design choices influence both its objective of reconstruction and\ndownstream generative performance. Our work aims to conduct an exploration of\nscaling in auto-encoders to fill in this blank. To facilitate this exploration,\nwe replace the typical convolutional backbone with an enhanced Vision\nTransformer architecture for Tokenization (ViTok). We train ViTok on\nlarge-scale image and video datasets far exceeding ImageNet-1K, removing data\nconstraints on tokenizer scaling. We first study how scaling the auto-encoder\nbottleneck affects both reconstruction and generation -- and find that while it\nis highly correlated with reconstruction, its relationship with generation is\nmore complex. We next explored the effect of separately scaling the\nauto-encoders' encoder and decoder on reconstruction and generation\nperformance. Crucially, we find that scaling the encoder yields minimal gains\nfor either reconstruction or generation, while scaling the decoder boosts\nreconstruction but the benefits for generation are mixed. Building on our\nexploration, we design ViTok as a lightweight auto-encoder that achieves\ncompetitive performance with state-of-the-art auto-encoders on ImageNet-1K and\nCOCO reconstruction tasks (256p and 512p) while outperforming existing\nauto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x\nfewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates\ncompetitive performance on image generation for ImageNet-1K and sets new\nstate-of-the-art benchmarks for class-conditional video generation on UCF-101.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.10; I.4.2; I.4.5"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages, 25 figures, 7 Tables",
    "pdf_url": "http://arxiv.org/pdf/2501.09755v1",
    "published_date": "2025-01-16 18:59:04 UTC",
    "updated_date": "2025-01-16 18:59:04 UTC"
  },
  {
    "arxiv_id": "2501.09751v2",
    "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking",
    "authors": [
      "Zekun Xi",
      "Wenbiao Yin",
      "Jizhan Fang",
      "Jialong Wu",
      "Runnan Fang",
      "Ningyu Zhang",
      "Jiang Yong",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen"
    ],
    "abstract": "Machine writing with large language models often relies on\nretrieval-augmented generation. However, these approaches remain confined\nwithin the boundaries of the model's predefined scope, limiting the generation\nof content with rich information. Specifically, vanilla-retrieved information\ntends to lack depth, novelty, and suffers from redundancy, which negatively\nimpacts the quality of generated articles, leading to shallow, unoriginal, and\nrepetitive outputs. To address these issues, we propose OmniThink, a\nslow-thinking machine writing framework that emulates the human-like process of\niterative expansion and reflection. The core idea behind OmniThink is to\nsimulate the cognitive behavior of learners as they slowly deepen their\nknowledge of the topics. Experimental results demonstrate that OmniThink\nimproves the knowledge density of generated articles without compromising\nmetrics such as coherence and depth. Human evaluations and expert feedback\nfurther highlight the potential of OmniThink to address real-world challenges\nin the generation of long-form articles.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code is available at https://github.com/zjunlp/OmniThink",
    "pdf_url": "http://arxiv.org/pdf/2501.09751v2",
    "published_date": "2025-01-16 18:58:06 UTC",
    "updated_date": "2025-02-20 15:05:18 UTC"
  },
  {
    "arxiv_id": "2501.09744v1",
    "title": "KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity Recognition and Normalization for Dysmorphology Physical Examination Reports",
    "authors": [
      "Hajung Kim",
      "Chanhwi Kim",
      "Jiwoong Sohn",
      "Tim Beck",
      "Marek Rei",
      "Sunkyu Kim",
      "T Ian Simpson",
      "Joram M Posma",
      "Antoine Lain",
      "Mujeen Sung",
      "Jaewoo Kang"
    ],
    "abstract": "The objective of BioCreative8 Track 3 is to extract phenotypic key medical\nfindings embedded within EHR texts and subsequently normalize these findings to\ntheir Human Phenotype Ontology (HPO) terms. However, the presence of diverse\nsurface forms in phenotypic findings makes it challenging to accurately\nnormalize them to the correct HPO terms. To address this challenge, we explored\nvarious models for named entity recognition and implemented data augmentation\ntechniques such as synonym marginalization to enhance the normalization step.\nOur pipeline resulted in an exact extraction and normalization F1 score 2.6\\%\nhigher than the mean score of all submissions received in response to the\nchallenge. Furthermore, in terms of the normalization F1 score, our approach\nsurpassed the average performance by 1.9\\%. These findings contribute to the\nadvancement of automated medical data extraction and normalization techniques,\nshowcasing potential pathways for future research and application in the\nbiomedical domain.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This article is part of the Proceedings of the BioCreative VIII\n  Challenge and Workshop: Curation and Evaluation in the era of Generative\n  Models",
    "pdf_url": "http://arxiv.org/pdf/2501.09744v1",
    "published_date": "2025-01-16 18:53:32 UTC",
    "updated_date": "2025-01-16 18:53:32 UTC"
  },
  {
    "arxiv_id": "2501.09725v1",
    "title": "Parallel multi-objective metaheuristics for smart communications in vehicular networks",
    "authors": [
      "Jamal Toutouh",
      "Enrique Alba"
    ],
    "abstract": "This article analyzes the use of two parallel multi-objective soft computing\nalgorithms to automatically search for high-quality settings of the Ad hoc On\nDemand Vector routing protocol for vehicular networks. These methods are based\non an evolutionary algorithm and on a swarm intelligence approach. The\nexperimental analysis demonstrates that the configurations computed by our\noptimization algorithms outperform other state-of-the-art optimized ones. In\nturn, the computational efficiency achieved by all the parallel versions is\ngreater than 87 %. Therefore, the line of work presented in this article\nrepresents an efficient framework to improve vehicular communications.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09725v1",
    "published_date": "2025-01-16 18:16:34 UTC",
    "updated_date": "2025-01-16 18:16:34 UTC"
  },
  {
    "arxiv_id": "2501.09720v3",
    "title": "A Simple Aerial Detection Baseline of Multimodal Language Models",
    "authors": [
      "Qingyun Li",
      "Yushi Chen",
      "Xinya Shu",
      "Dong Chen",
      "Xin He",
      "Yi Yu",
      "Xue Yang"
    ],
    "abstract": "The multimodal language models (MLMs) based on generative pre-trained\nTransformer are considered powerful candidates for unifying various domains and\ntasks. MLMs developed for remote sensing (RS) have demonstrated outstanding\nperformance in multiple tasks, such as visual question answering and visual\ngrounding. In addition to visual grounding that detects specific objects\ncorresponded to given instruction, aerial detection, which detects all objects\nof multiple categories, is also a valuable and challenging task for RS\nfoundation models. However, aerial detection has not been explored by existing\nRS MLMs because the autoregressive prediction mechanism of MLMs differs\nsignificantly from the detection outputs. In this paper, we present a simple\nbaseline for applying MLMs to aerial detection for the first time, named\nLMMRotate. Specifically, we first introduce a normalization method to transform\ndetection outputs into textual outputs to be compatible with the MLM framework.\nThen, we propose a evaluation method, which ensures a fair comparison between\nMLMs and conventional object detection models. We construct the baseline by\nfine-tuning open-source general-purpose MLMs and achieve impressive detection\nperformance comparable to conventional detector. We hope that this baseline\nwill serve as a reference for future MLM development, enabling more\ncomprehensive capabilities for understanding RS images. Code is available at\nhttps://github.com/Li-Qingyun/mllm-mmrotate.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages, 1 table, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.09720v3",
    "published_date": "2025-01-16 18:09:22 UTC",
    "updated_date": "2025-01-31 21:29:40 UTC"
  },
  {
    "arxiv_id": "2501.09709v1",
    "title": "CyberMentor: AI Powered Learning Tool Platform to Address Diverse Student Needs in Cybersecurity Education",
    "authors": [
      "Tianyu Wang",
      "Nianjun Zhou",
      "Zhixiong Chen"
    ],
    "abstract": "Many non-traditional students in cybersecurity programs often lack access to\nadvice from peers, family members and professors, which can hinder their\neducational experiences. Additionally, these students may not fully benefit\nfrom various LLM-powered AI assistants due to issues like content relevance,\nlocality of advice, minimum expertise, and timing. This paper addresses these\nchallenges by introducing an application designed to provide comprehensive\nsupport by answering questions related to knowledge, skills, and career\npreparation advice tailored to the needs of these students. We developed a\nlearning tool platform, CyberMentor, to address the diverse needs and pain\npoints of students majoring in cybersecurity. Powered by agentic workflow and\nGenerative Large Language Models (LLMs), the platform leverages\nRetrieval-Augmented Generation (RAG) for accurate and contextually relevant\ninformation retrieval to achieve accessibility and personalization. We\ndemonstrated its value in addressing knowledge requirements for cybersecurity\neducation and for career marketability, in tackling skill requirements for\nanalytical and programming assignments, and in delivering real time on demand\nlearning support. Using three use scenarios, we showcased CyberMentor in\nfacilitating knowledge acquisition and career preparation and providing\nseamless skill-based guidance and support. We also employed the LangChain\nprompt-based evaluation methodology to evaluate the platform's impact,\nconfirming its strong performance in helpfulness, correctness, and\ncompleteness. These results underscore the system's ability to support students\nin developing practical cybersecurity skills while improving equity and\nsustainability within higher education. Furthermore, CyberMentor's open-source\ndesign allows for adaptation across other disciplines, fostering educational\ninnovation and broadening its potential impact.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.3.2; I.2.1"
    ],
    "primary_category": "cs.CY",
    "comment": "11 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.09709v1",
    "published_date": "2025-01-16 18:00:06 UTC",
    "updated_date": "2025-01-16 18:00:06 UTC"
  },
  {
    "arxiv_id": "2501.09707v1",
    "title": "The Goofus & Gallant Story Corpus for Practical Value Alignment",
    "authors": [
      "Md Sultan Al Nahian",
      "Tasmia Tasrin",
      "Spencer Frazier",
      "Mark Riedl",
      "Brent Harrison"
    ],
    "abstract": "Values or principles are key elements of human society that influence people\nto behave and function according to an accepted standard set of social rules to\nmaintain social order. As AI systems are becoming ubiquitous in human society,\nit is a major concern that they could violate these norms or values and\npotentially cause harm. Thus, to prevent intentional or unintentional harm, AI\nsystems are expected to take actions that align with these principles. Training\nsystems to exhibit this type of behavior is difficult and often requires a\nspecialized dataset. This work presents a multi-modal dataset illustrating\nnormative and non-normative behavior in real-life situations described through\nnatural language and artistic images. This training set contains curated sets\nof images that are designed to teach young children about social principles. We\nargue that this is an ideal dataset to use for training socially normative\nagents given this fact.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by International Conference on Machine Learning and\n  Applications (ICMLA) 2024. Main Conference, Long Paper",
    "pdf_url": "http://arxiv.org/pdf/2501.09707v1",
    "published_date": "2025-01-16 17:58:58 UTC",
    "updated_date": "2025-01-16 17:58:58 UTC"
  },
  {
    "arxiv_id": "2501.09705v1",
    "title": "Practical Continual Forgetting for Pre-trained Vision Models",
    "authors": [
      "Hongbo Zhao",
      "Fei Zhu",
      "Bolin Ni",
      "Feng Zhu",
      "Gaofeng Meng",
      "Zhaoxiang Zhang"
    ],
    "abstract": "For privacy and security concerns, the need to erase unwanted information\nfrom pre-trained vision models is becoming evident nowadays. In real-world\nscenarios, erasure requests originate at any time from both users and model\nowners, and these requests usually form a sequence. Therefore, under such a\nsetting, selective information is expected to be continuously removed from a\npre-trained model while maintaining the rest. We define this problem as\ncontinual forgetting and identify three key challenges. (i) For unwanted\nknowledge, efficient and effective deleting is crucial. (ii) For remaining\nknowledge, the impact brought by the forgetting procedure should be minimal.\n(iii) In real-world scenarios, the training samples may be scarce or partially\nmissing during the process of forgetting. To address them, we first propose\nGroup Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA\nmodules to fine-tune the FFN layers in Transformer blocks for each forgetting\ntask independently, and towards (ii), a simple group sparse regularization is\nadopted, enabling automatic selection of specific LoRA groups and zeroing out\nthe others. To further extend GS-LoRA to more practical scenarios, we\nincorporate prototype information as additional supervision and introduce a\nmore practical approach, GS-LoRA++. For each forgotten class, we move the\nlogits away from its original prototype. For the remaining classes, we pull the\nlogits closer to their respective prototypes. We conduct extensive experiments\non face recognition, object detection and image classification and demonstrate\nthat our method manages to forget specific classes with minimal impact on other\nclasses. Codes have been released on https://github.com/bjzhb666/GS-LoRA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09705v1",
    "published_date": "2025-01-16 17:57:53 UTC",
    "updated_date": "2025-01-16 17:57:53 UTC"
  },
  {
    "arxiv_id": "2501.09700v1",
    "title": "Cueless EEG imagined speech for subject identification: dataset and benchmarks",
    "authors": [
      "Ali Derakhshesh",
      "Zahra Dehghanian",
      "Reza Ebrahimpour",
      "Hamid R. Rabiee"
    ],
    "abstract": "Electroencephalogram (EEG) signals have emerged as a promising modality for\nbiometric identification. While previous studies have explored the use of\nimagined speech with semantically meaningful words for subject identification,\nmost have relied on additional visual or auditory cues. In this study, we\nintroduce a cueless EEG-based imagined speech paradigm, where subjects imagine\nthe pronunciation of semantically meaningful words without any external cues.\nThis innovative approach addresses the limitations of prior methods by\nrequiring subjects to select and imagine words from a predefined list\nnaturally. The dataset comprises over 4,350 trials from 11 subjects across five\nsessions. We assess a variety of classification methods, including traditional\nmachine learning techniques such as Support Vector Machines (SVM) and XGBoost,\nas well as time-series foundation models and deep learning architectures\nspecifically designed for EEG classification, such as EEG Conformer and Shallow\nConvNet. A session-based hold-out validation strategy was employed to ensure\nreliable evaluation and prevent data leakage. Our results demonstrate\noutstanding classification accuracy, reaching 97.93%. These findings highlight\nthe potential of cueless EEG paradigms for secure and reliable subject\nidentification in real-world applications, such as brain-computer interfaces\n(BCIs).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09700v1",
    "published_date": "2025-01-16 17:54:56 UTC",
    "updated_date": "2025-01-16 17:54:56 UTC"
  },
  {
    "arxiv_id": "2501.09686v3",
    "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models",
    "authors": [
      "Fengli Xu",
      "Qianyue Hao",
      "Zefang Zong",
      "Jingwei Wang",
      "Yunke Zhang",
      "Jingyi Wang",
      "Xiaochong Lan",
      "Jiahui Gong",
      "Tianjian Ouyang",
      "Fanjin Meng",
      "Chenyang Shao",
      "Yuwei Yan",
      "Qinglong Yang",
      "Yiwen Song",
      "Sijian Ren",
      "Xinyuan Hu",
      "Yu Li",
      "Jie Feng",
      "Chen Gao",
      "Yong Li"
    ],
    "abstract": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "36 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.09686v3",
    "published_date": "2025-01-16 17:37:58 UTC",
    "updated_date": "2025-01-23 08:44:44 UTC"
  },
  {
    "arxiv_id": "2501.09685v2",
    "title": "Inference-Time Alignment in Diffusion Models with Reward-Guided Generation: Tutorial and Review",
    "authors": [
      "Masatoshi Uehara",
      "Yulai Zhao",
      "Chenyu Wang",
      "Xiner Li",
      "Aviv Regev",
      "Sergey Levine",
      "Tommaso Biancalani"
    ],
    "abstract": "This tutorial provides an in-depth guide on inference-time guidance and\nalignment methods for optimizing downstream reward functions in diffusion\nmodels. While diffusion models are renowned for their generative modeling\ncapabilities, practical applications in fields such as biology often require\nsample generation that maximizes specific metrics (e.g., stability, affinity in\nproteins, closeness to target structures). In these scenarios, diffusion models\ncan be adapted not only to generate realistic samples but also to explicitly\nmaximize desired measures at inference time without fine-tuning. This tutorial\nexplores the foundational aspects of such inference-time algorithms. We review\nthese methods from a unified perspective, demonstrating that current techniques\n-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,\nand classifier guidance -- aim to approximate soft optimal denoising processes\n(a.k.a. policies in RL) that combine pre-trained denoising processes with value\nfunctions serving as look-ahead functions that predict from intermediate states\nto terminal rewards. Within this framework, we present several novel algorithms\nnot yet covered in the literature. Furthermore, we discuss (1) fine-tuning\nmethods combined with inference-time techniques, (2) inference-time algorithms\nbased on search algorithms such as Monte Carlo tree search, which have received\nlimited attention in current research, and (3) connections between\ninference-time algorithms in language models and diffusion models. The code of\nthis tutorial on protein design is available at\nhttps://github.com/masa-ue/AlignInversePro",
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-bio.QM",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "We plan to add more content and codes. Please let us know if there\n  are any comments or missing citations",
    "pdf_url": "http://arxiv.org/pdf/2501.09685v2",
    "published_date": "2025-01-16 17:37:35 UTC",
    "updated_date": "2025-01-20 22:00:26 UTC"
  },
  {
    "arxiv_id": "2501.09682v1",
    "title": "Incorporating Quantum Advantage in Quantum Circuit Generation through Genetic Programming",
    "authors": [
      "Christoph Stein",
      "Michael Färber"
    ],
    "abstract": "Designing efficient quantum circuits that leverage quantum advantage compared\nto classical computing has become increasingly critical. Genetic algorithms\nhave shown potential in generating such circuits through artificial evolution.\nHowever, integrating quantum advantage into the fitness function of these\nalgorithms remains unexplored. In this paper, we aim to enhance the efficiency\nof quantum circuit design by proposing two novel approaches for incorporating\nquantum advantage metrics into the fitness function of genetic algorithms.1 We\nevaluate our approaches based on the Bernstein-Vazirani Problem and the\nUnstructured Database Search Problem as test cases. The results demonstrate\nthat our approaches not only improve the convergence speed of the genetic\nalgorithm but also produce circuits comparable to expert-designed solutions.\nOur findings suggest that automated quantum circuit design using genetic\nalgorithms that incorporate a measure of quantum advantage is a promising\napproach to accelerating the development of quantum algorithms.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET",
      "cs.NE"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09682v1",
    "published_date": "2025-01-16 17:34:34 UTC",
    "updated_date": "2025-01-16 17:34:34 UTC"
  },
  {
    "arxiv_id": "2501.09674v1",
    "title": "Authenticated Delegation and Authorized AI Agents",
    "authors": [
      "Tobin South",
      "Samuele Marro",
      "Thomas Hardjono",
      "Robert Mahari",
      "Cedric Deslandes Whitney",
      "Dazza Greenwood",
      "Alan Chan",
      "Alex Pentland"
    ],
    "abstract": "The rapid deployment of autonomous AI agents creates urgent challenges around\nauthorization, accountability, and access control in digital spaces. New\nstandards are needed to know whom AI agents act on behalf of and guide their\nuse appropriately, protecting online spaces while unlocking the value of task\ndelegation to autonomous agents. We introduce a novel framework for\nauthenticated, authorized, and auditable delegation of authority to AI agents,\nwhere human users can securely delegate and restrict the permissions and scope\nof agents while maintaining clear chains of accountability. This framework\nbuilds on existing identification and access management protocols, extending\nOAuth 2.0 and OpenID Connect with agent-specific credentials and metadata,\nmaintaining compatibility with established authentication and web\ninfrastructure. Further, we propose a framework for translating flexible,\nnatural language permissions into auditable access control configurations,\nenabling robust scoping of AI agent capabilities across diverse interaction\nmodalities. Taken together, this practical approach facilitates immediate\ndeployment of AI agents while addressing key security and accountability\nconcerns, working toward ensuring agentic AI systems perform only appropriate\nactions and providing a tool for digital service providers to enable AI agent\ninteractions without risking harm from scalable interaction.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.NI",
      "68M01, 68T01, 68U35, 94A60, 68P20"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09674v1",
    "published_date": "2025-01-16 17:11:21 UTC",
    "updated_date": "2025-01-16 17:11:21 UTC"
  },
  {
    "arxiv_id": "2501.10476v1",
    "title": "Revisiting Rogers' Paradox in the Context of Human-AI Interaction",
    "authors": [
      "Katherine M. Collins",
      "Umang Bhatt",
      "Ilia Sucholutsky"
    ],
    "abstract": "Humans learn about the world, and how to act in the world, in many ways: from\nindividually conducting experiments to observing and reproducing others'\nbehavior. Different learning strategies come with different costs and\nlikelihoods of successfully learning more about the world. The choice that any\none individual makes of how to learn can have an impact on the collective\nunderstanding of a whole population if people learn from each other. Alan\nRogers developed simulations of a population of agents to study these network\nphenomena where agents could individually or socially learn amidst a dynamic,\nuncertain world and uncovered a confusing result: the availability of cheap\nsocial learning yielded no benefit to population fitness over individual\nlearning. This paradox spawned decades of work trying to understand and uncover\nfactors that foster the relative benefit of social learning that centuries of\nhuman behavior suggest exists. What happens in such network models now that\nhumans can socially learn from AI systems that are themselves socially learning\nfrom us? We revisit Rogers' Paradox in the context of human-AI interaction to\nprobe a simplified network of humans and AI systems learning together about an\nuncertain world. We propose and examine the impact of several learning\nstrategies on the quality of the equilibrium of a society's 'collective world\nmodel'. We consider strategies that can be undertaken by various stakeholders\ninvolved in a single human-AI interaction: human, AI model builder, and society\nor regulators around the interaction. We then consider possible negative\nfeedback loops that may arise from humans learning socially from AI: that\nlearning from the AI may impact our own ability to learn about the world. We\nclose with open directions into studying networks of human and AI systems that\ncan be explored in enriched versions of our simulation framework.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Pre-print",
    "pdf_url": "http://arxiv.org/pdf/2501.10476v1",
    "published_date": "2025-01-16 17:09:57 UTC",
    "updated_date": "2025-01-16 17:09:57 UTC"
  },
  {
    "arxiv_id": "2501.09672v2",
    "title": "Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP Evaluation Benchmark",
    "authors": [
      "Alexis Roger",
      "Prateek Humane",
      "Daniel Z. Kaplan",
      "Kshitij Gupta",
      "Qi Sun",
      "George Adamopoulos",
      "Jonathan Siu Chi Lim",
      "Quentin Anthony",
      "Edwin Fennell",
      "Irina Rish"
    ],
    "abstract": "The proliferation of Vision-Language Models (VLMs) in the past several years\ncalls for rigorous and comprehensive evaluation methods and benchmarks. This\nwork analyzes existing VLM evaluation techniques, including automated metrics,\nAI-based assessments, and human evaluations across diverse tasks. We first\nintroduce Robin - a novel suite of VLMs that we built by combining Large\nLanguage Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use\nRobin to identify shortcomings of current evaluation approaches across scales.\nNext, to overcome the identified limitations, we introduce CHIRP - a new long\nform response benchmark we developed for more robust and complete VLM\nevaluation. We provide open access to the Robin training code, model suite, and\nCHIRP benchmark to promote reproducibility and advance VLM research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09672v2",
    "published_date": "2025-01-16 17:08:12 UTC",
    "updated_date": "2025-01-21 01:04:52 UTC"
  },
  {
    "arxiv_id": "2501.09653v1",
    "title": "The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models",
    "authors": [
      "Jonathan Katzy",
      "Razvan Mihai Popescu",
      "Arie van Deursen",
      "Maliheh Izadi"
    ],
    "abstract": "The recent rise in the popularity of large language models has spurred the\ndevelopment of extensive code datasets needed to train them. This has left\nlimited code available for collection and use in the downstream investigation\nof specific behaviors, or evaluation of large language models without suffering\nfrom data contamination. To address this problem, we release The Heap, a large\nmultilingual dataset covering 57 programming languages that has been\ndeduplicated with respect to other open datasets of code, enabling researchers\nto conduct fair evaluations of large language models without significant data\ncleaning overhead.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Pre-Print. Accepted to FORGE 2025 Dataset Track",
    "pdf_url": "http://arxiv.org/pdf/2501.09653v1",
    "published_date": "2025-01-16 16:48:41 UTC",
    "updated_date": "2025-01-16 16:48:41 UTC"
  },
  {
    "arxiv_id": "2501.09649v1",
    "title": "Monte Carlo Tree Search with Velocity Obstacles for safe and efficient motion planning in dynamic environments",
    "authors": [
      "Lorenzo Bonanni",
      "Daniele Meli",
      "Alberto Castellini",
      "Alessandro Farinelli"
    ],
    "abstract": "Online motion planning is a challenging problem for intelligent robots moving\nin dense environments with dynamic obstacles, e.g., crowds. In this work, we\npropose a novel approach for optimal and safe online motion planning with\nminimal information about dynamic obstacles. Specifically, our approach\nrequires only the current position of the obstacles and their maximum speed,\nbut it does not need any information about their exact trajectories or dynamic\nmodel. The proposed methodology combines Monte Carlo Tree Search (MCTS), for\nonline optimal planning via model simulations, with Velocity Obstacles (VO),\nfor obstacle avoidance. We perform experiments in a cluttered simulated\nenvironment with walls, and up to 40 dynamic obstacles moving with random\nvelocities and directions. With an ablation study, we show the key contribution\nof VO in scaling up the efficiency of MCTS, selecting the safest and most\nrewarding actions in the tree of simulations. Moreover, we show the superiority\nof our methodology with respect to state-of-the-art planners, including\nNon-linear Model Predictive Control (NMPC), in terms of improved collision\nrate, computational and task performance.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09649v1",
    "published_date": "2025-01-16 16:45:08 UTC",
    "updated_date": "2025-01-16 16:45:08 UTC"
  },
  {
    "arxiv_id": "2501.09646v1",
    "title": "NS-Gym: Open-Source Simulation Environments and Benchmarks for Non-Stationary Markov Decision Processes",
    "authors": [
      "Nathaniel S. Keplinger",
      "Baiting Luo",
      "Iliyas Bektas",
      "Yunuo Zhang",
      "Kyle Hollins Wray",
      "Aron Laszka",
      "Abhishek Dubey",
      "Ayan Mukhopadhyay"
    ],
    "abstract": "In many real-world applications, agents must make sequential decisions in\nenvironments where conditions are subject to change due to various exogenous\nfactors. These non-stationary environments pose significant challenges to\ntraditional decision-making models, which typically assume stationary dynamics.\nNon-stationary Markov decision processes (NS-MDPs) offer a framework to model\nand solve decision problems under such changing conditions. However, the lack\nof standardized benchmarks and simulation tools has hindered systematic\nevaluation and advance in this field. We present NS-Gym, the first simulation\ntoolkit designed explicitly for NS-MDPs, integrated within the popular\nGymnasium framework. In NS-Gym, we segregate the evolution of the environmental\nparameters that characterize non-stationarity from the agent's decision-making\nmodule, allowing for modular and flexible adaptations to dynamic environments.\nWe review prior work in this domain and present a toolkit encapsulating key\nproblem characteristics and types in NS-MDPs. This toolkit is the first effort\nto develop a set of standardized interfaces and benchmark problems to enable\nconsistent and reproducible evaluation of algorithms under non-stationary\nconditions. We also benchmark six algorithmic approaches from prior work on\nNS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to\nassess the adaptability and robustness of their decision-making algorithms to\nnon-stationary conditions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.09646v1",
    "published_date": "2025-01-16 16:38:33 UTC",
    "updated_date": "2025-01-16 16:38:33 UTC"
  },
  {
    "arxiv_id": "2501.09645v1",
    "title": "CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through Category-Bounding",
    "authors": [
      "Johannes Kirmayr",
      "Lukas Stappen",
      "Phillip Schneider",
      "Florian Matthes",
      "Elisabeth André"
    ],
    "abstract": "In today's assistant landscape, personalisation enhances interactions,\nfosters long-term relationships, and deepens engagement. However, many systems\nstruggle with retaining user preferences, leading to repetitive user requests\nand disengagement. Furthermore, the unregulated and opaque extraction of user\npreferences in industry applications raises significant concerns about privacy\nand trust, especially in regions with stringent regulations like Europe. In\nresponse to these challenges, we propose a long-term memory system for voice\nassistants, structured around predefined categories. This approach leverages\nLarge Language Models to efficiently extract, store, and retrieve preferences\nwithin these categories, ensuring both personalisation and transparency. We\nalso introduce a synthetic multi-turn, multi-session conversation dataset\n(CarMem), grounded in real industry data, tailored to an in-car voice assistant\nsetting. Benchmarked on the dataset, our system achieves an F1-score of .78 to\n.95 in preference extraction, depending on category granularity. Our\nmaintenance strategy reduces redundant preferences by 95% and contradictory\nones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,\nthe results demonstrate the system's suitability for industrial applications.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for presentation at the International Conference on\n  Computational Linguistics (COLING 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.09645v1",
    "published_date": "2025-01-16 16:37:33 UTC",
    "updated_date": "2025-01-16 16:37:33 UTC"
  },
  {
    "arxiv_id": "2501.09640v2",
    "title": "Electronic Health Records: Towards Digital Twins in Healthcare",
    "authors": [
      "Muhammet Alkan",
      "Hester Huijsdens",
      "Yola Jones",
      "Fani Deligianni"
    ],
    "abstract": "The pivotal shift from traditional paper-based records to sophisticated\nElectronic Health Records (EHR), enabled systematic collection and analysis of\npatient data through descriptive statistics, providing insight into patterns\nand trends across patient populations. This evolution continued toward\npredictive analytics, allowing healthcare providers to anticipate patient\noutcomes and potential complications before they occur. This progression from\nbasic digital record-keeping to sophisticated predictive modelling and digital\ntwins reflects healthcare's broader evolution toward more integrated,\npatient-centred approaches that combine data-driven insights with personalized\ncare delivery. This chapter explores the evolution and significance of\nhealthcare information systems, beginning with an examination of the\nimplementation of EHR in the UK and the USA. It provides a comprehensive\noverview of the International Classification of Diseases (ICD) system, tracing\nits development from ICD-9 to ICD-10. Central to this discussion is the\nMIMIC-III database, a landmark achievement in healthcare data sharing and\narguably the most comprehensive critical care database freely available to\nresearchers worldwide. MIMIC-III has democratized access to high-quality\nhealthcare data, enabling unprecedented opportunities for research and\nanalysis. The chapter examines its structure, clinical outcome analysis\ncapabilities, and practical applications through case studies, with a\nparticular focus on mortality and length of stay metrics, vital signs\nextraction, and ICD coding. Through detailed entity-relationship diagrams and\npractical examples, the text illustrates MIMIC's complex data structure and\ndemonstrates how different querying approaches can lead to subtly different\nresults, emphasizing the critical importance of understanding the database's\narchitecture for accurate data extraction.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Added acknowledgements for the corresponding author",
    "pdf_url": "http://arxiv.org/pdf/2501.09640v2",
    "published_date": "2025-01-16 16:30:02 UTC",
    "updated_date": "2025-02-17 10:59:04 UTC"
  },
  {
    "arxiv_id": "2501.09632v2",
    "title": "Platform-Aware Mission Planning",
    "authors": [
      "Stefan Panjkovic",
      "Alessandro Cimatti",
      "Andrea Micheli",
      "Stefano Tonetta"
    ],
    "abstract": "Planning for autonomous systems typically requires reasoning with models at\ndifferent levels of abstraction, and the harmonization of two competing sets of\nobjectives: high-level mission goals that refer to an interaction of the system\nwith the external environment, and low-level platform constraints that aim to\npreserve the integrity and the correct interaction of the subsystems. The\ncomplicated interplay between these two models makes it very hard to reason on\nthe system as a whole, especially when the objective is to find plans with\nrobustness guarantees, considering the non-deterministic behavior of the lower\nlayers of the system.\n  In this paper, we introduce the problem of Platform-Aware Mission Planning\n(PAMP), addressing it in the setting of temporal durative actions. The PAMP\nproblem differs from standard temporal planning for its exists-forall nature:\nthe high-level plan dealing with mission goals is required to satisfy safety\nand executability constraints, for all the possible non-deterministic\nexecutions of the low-level model of the platform and the environment. We\npropose two approaches for solving PAMP. The first baseline approach\namalgamates the mission and platform levels, while the second is based on an\nabstraction-refinement loop that leverages the combination of a planner and a\nverification engine. We prove the soundness and completeness of the proposed\napproaches and validate them experimentally, demonstrating the importance of\nheterogeneous modeling and the superiority of the technique based on\nabstraction-refinement.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09632v2",
    "published_date": "2025-01-16 16:20:37 UTC",
    "updated_date": "2025-05-20 06:39:45 UTC"
  },
  {
    "arxiv_id": "2501.09628v2",
    "title": "Artificial Intelligence-Driven Clinical Decision Support Systems",
    "authors": [
      "Muhammet Alkan",
      "Idris Zakariyya",
      "Samuel Leighton",
      "Kaushik Bhargav Sivangi",
      "Christos Anagnostopoulos",
      "Fani Deligianni"
    ],
    "abstract": "As artificial intelligence (AI) becomes increasingly embedded in healthcare\ndelivery, this chapter explores the critical aspects of developing reliable and\nethical Clinical Decision Support Systems (CDSS). Beginning with the\nfundamental transition from traditional statistical models to sophisticated\nmachine learning approaches, this work examines rigorous validation strategies\nand performance assessment methods, including the crucial role of model\ncalibration and decision curve analysis. The chapter emphasizes that creating\ntrustworthy AI systems in healthcare requires more than just technical\naccuracy; it demands careful consideration of fairness, explainability, and\nprivacy. The challenge of ensuring equitable healthcare delivery through AI is\nstressed, discussing methods to identify and mitigate bias in clinical\npredictive models. The chapter then delves into explainability as a cornerstone\nof human-centered CDSS. This focus reflects the understanding that healthcare\nprofessionals must not only trust AI recommendations but also comprehend their\nunderlying reasoning. The discussion advances in an analysis of privacy\nvulnerabilities in medical AI systems, from data leakage in deep learning\nmodels to sophisticated attacks against model explanations. The text explores\nprivacy-preservation strategies such as differential privacy and federated\nlearning, while acknowledging the inherent trade-offs between privacy\nprotection and model performance. This progression, from technical validation\nto ethical considerations, reflects the multifaceted challenges of developing\nAI systems that can be seamlessly and reliably integrated into daily clinical\npractice while maintaining the highest standards of patient care and data\nprotection.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Added acknowledgements for the corresponding author, updated Figure\n  4, 5 & 6",
    "pdf_url": "http://arxiv.org/pdf/2501.09628v2",
    "published_date": "2025-01-16 16:17:39 UTC",
    "updated_date": "2025-02-17 11:09:42 UTC"
  },
  {
    "arxiv_id": "2501.09620v1",
    "title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment",
    "authors": [
      "Chaoqi Wang",
      "Zhuokai Zhao",
      "Yibo Jiang",
      "Zhaorun Chen",
      "Chen Zhu",
      "Yuxin Chen",
      "Jiayi Liu",
      "Lizhu Zhang",
      "Xiangjun Fan",
      "Hao Ma",
      "Sinong Wang"
    ],
    "abstract": "Recent advances in large language models (LLMs) have demonstrated significant\nprogress in performing complex tasks. While Reinforcement Learning from Human\nFeedback (RLHF) has been effective in aligning LLMs with human preferences, it\nis susceptible to spurious correlations in reward modeling. Consequently, it\noften introduces biases-such as length bias, sycophancy, conceptual bias, and\ndiscrimination that hinder the model's ability to capture true causal\nrelationships. To address this, we propose a novel causal reward modeling\napproach that integrates causal inference to mitigate these spurious\ncorrelations. Our method enforces counterfactual invariance, ensuring reward\npredictions remain consistent when irrelevant variables are altered. Through\nexperiments on both synthetic and real-world datasets, we show that our\napproach mitigates various types of spurious correlations effectively,\nresulting in more reliable and fair alignment of LLMs with human preferences.\nAs a drop-in enhancement to the existing RLHF workflow, our causal reward\nmodeling provides a practical way to improve the trustworthiness and fairness\nof LLM finetuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09620v1",
    "published_date": "2025-01-16 16:00:37 UTC",
    "updated_date": "2025-01-16 16:00:37 UTC"
  },
  {
    "arxiv_id": "2501.09608v1",
    "title": "Metric Learning with Progressive Self-Distillation for Audio-Visual Embedding Learning",
    "authors": [
      "Donghuo Zeng",
      "Kazushi Ikeda"
    ],
    "abstract": "Metric learning projects samples into an embedded space, where similarities\nand dissimilarities are quantified based on their learned representations.\nHowever, existing methods often rely on label-guided representation learning,\nwhere representations of different modalities, such as audio and visual data,\nare aligned based on annotated labels. This approach tends to underutilize\nlatent complex features and potential relationships inherent in the\ndistributions of audio and visual data that are not directly tied to the\nlabels, resulting in suboptimal performance in audio-visual embedding learning.\nTo address this issue, we propose a novel architecture that integrates\ncross-modal triplet loss with progressive self-distillation. Our method\nenhances representation learning by leveraging inherent distributions and\ndynamically refining soft audio-visual alignments -- probabilistic alignments\nbetween audio and visual data that capture the inherent relationships beyond\nexplicit labels. Specifically, the model distills audio-visual\ndistribution-based knowledge from annotated labels in a subset of each batch.\nThis self-distilled knowledge is used t",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 3 figures, 2 tables. Accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.09608v1",
    "published_date": "2025-01-16 15:32:41 UTC",
    "updated_date": "2025-01-16 15:32:41 UTC"
  },
  {
    "arxiv_id": "2501.09605v1",
    "title": "Managed-Retention Memory: A New Class of Memory for the AI Era",
    "authors": [
      "Sergey Legtchenko",
      "Ioan Stefanovici",
      "Richard Black",
      "Antony Rowstron",
      "Junyi Liu",
      "Paolo Costa",
      "Burcu Canakci",
      "Dushyanth Narayanan",
      "Xingbo Wu"
    ],
    "abstract": "AI clusters today are one of the major uses of High Bandwidth Memory (HBM).\nHowever, HBM is suboptimal for AI workloads for several reasons. Analysis shows\nHBM is overprovisioned on write performance, but underprovisioned on density\nand read bandwidth, and also has significant energy per bit overheads. It is\nalso expensive, with lower yield than DRAM due to manufacturing complexity. We\npropose a new memory class: Managed-Retention Memory (MRM), which is more\noptimized to store key data structures for AI inference workloads. We believe\nthat MRM may finally provide a path to viability for technologies that were\noriginally proposed to support Storage Class Memory (SCM). These technologies\ntraditionally offered long-term persistence (10+ years) but provided poor IO\nperformance and/or endurance. MRM makes different trade-offs, and by\nunderstanding the workload IO patterns, MRM foregoes long-term data retention\nand write performance for better potential performance on the metrics important\nfor these workloads.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.ET"
    ],
    "primary_category": "cs.AR",
    "comment": "8 pages (5 content + 3 refs); 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2501.09605v1",
    "published_date": "2025-01-16 15:25:44 UTC",
    "updated_date": "2025-01-16 15:25:44 UTC"
  },
  {
    "arxiv_id": "2501.09597v1",
    "title": "Reducing the Sensitivity of Neural Physics Simulators to Mesh Topology via Pretraining",
    "authors": [
      "Nathan Vaska",
      "Justin Goodwin",
      "Robin Walters",
      "Rajmonda S. Caceres"
    ],
    "abstract": "Meshes are used to represent complex objects in high fidelity physics\nsimulators across a variety of domains, such as radar sensing and aerodynamics.\nThere is growing interest in using neural networks to accelerate physics\nsimulations, and also a growing body of work on applying neural networks\ndirectly to irregular mesh data. Since multiple mesh topologies can represent\nthe same object, mesh augmentation is typically required to handle topological\nvariation when training neural networks. Due to the sensitivity of physics\nsimulators to small changes in mesh shape, it is challenging to use these\naugmentations when training neural network-based physics simulators. In this\nwork, we show that variations in mesh topology can significantly reduce the\nperformance of neural network simulators. We evaluate whether pretraining can\nbe used to address this issue, and find that employing an established\nautoencoder pretraining technique with graph embedding models reduces the\nsensitivity of neural network simulators to variations in mesh topology.\nFinally, we highlight future research directions that may further reduce neural\nsimulator sensitivity to mesh topology.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6; I.2.10"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.09597v1",
    "published_date": "2025-01-16 15:21:18 UTC",
    "updated_date": "2025-01-16 15:21:18 UTC"
  },
  {
    "arxiv_id": "2501.09595v1",
    "title": "IFRA: a machine learning-based Instrumented Fall Risk Assessment Scale derived from Instrumented Timed Up and Go test in stroke patients",
    "authors": [
      "Simone Macciò",
      "Alessandro Carfì",
      "Alessio Capitanelli",
      "Peppino Tropea",
      "Massimo Corbo",
      "Fulvio Mastrogiovanni",
      "Michela Picardi"
    ],
    "abstract": "Effective fall risk assessment is critical for post-stroke patients. The\npresent study proposes a novel, data-informed fall risk assessment method based\non the instrumented Timed Up and Go (ITUG) test data, bringing in many mobility\nmeasures that traditional clinical scales fail to capture. IFRA, which stands\nfor Instrumented Fall Risk Assessment, has been developed using a two-step\nprocess: first, features with the highest predictive power among those\ncollected in a ITUG test have been identified using machine learning\ntechniques; then, a strategy is proposed to stratify patients into low, medium,\nor high-risk strata. The dataset used in our analysis consists of 142\nparticipants, out of which 93 were used for training (15 synthetically\ngenerated), 17 for validation and 32 to test the resulting IFRA scale (22\nnon-fallers and 10 fallers). Features considered in the IFRA scale include gait\nspeed, vertical acceleration during sit-to-walk transition, and turning angular\nvelocity, which align well with established literature on the risk of fall in\nneurological patients. In a comparison with traditional clinical scales such as\nthe traditional Timed Up & Go and the Mini-BESTest, IFRA demonstrates\ncompetitive performance, being the only scale to correctly assign more than\nhalf of the fallers to the high-risk stratum (Fischer's Exact test p = 0.004).\nDespite the dataset's limited size, this is the first proof-of-concept study to\npave the way for future evidence regarding the use of IFRA tool for continuous\npatient monitoring and fall prevention both in clinical stroke rehabilitation\nand at home post-discharge.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 2 figures, submitted for review dec 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.09595v1",
    "published_date": "2025-01-16 15:20:22 UTC",
    "updated_date": "2025-01-16 15:20:22 UTC"
  },
  {
    "arxiv_id": "2501.09571v1",
    "title": "MatrixNet: Learning over symmetry groups using learned group representations",
    "authors": [
      "Lucas Laird",
      "Circe Hsu",
      "Asilata Bapat",
      "Robin Walters"
    ],
    "abstract": "Group theory has been used in machine learning to provide a theoretically\ngrounded approach for incorporating known symmetry transformations in tasks\nfrom robotics to protein modeling. In these applications, equivariant neural\nnetworks use known symmetry groups with predefined representations to learn\nover geometric input data. We propose MatrixNet, a neural network architecture\nthat learns matrix representations of group element inputs instead of using\npredefined representations. MatrixNet achieves higher sample efficiency and\ngeneralization over several standard baselines in prediction tasks over the\nseveral finite groups and the Artin braid group. We also show that MatrixNet\nrespects group relations allowing generalization to group elements of greater\nword length than in the training set.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.RT"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.09571v1",
    "published_date": "2025-01-16 14:45:12 UTC",
    "updated_date": "2025-01-16 14:45:12 UTC"
  },
  {
    "arxiv_id": "2501.09555v3",
    "title": "Text-driven Adaptation of Foundation Models for Few-shot Surgical Workflow Analysis",
    "authors": [
      "Tingxuan Chen",
      "Kun Yuan",
      "Vinkle Srivastav",
      "Nassir Navab",
      "Nicolas Padoy"
    ],
    "abstract": "Purpose: Surgical workflow analysis is crucial for improving surgical\nefficiency and safety. However, previous studies rely heavily on large-scale\nannotated datasets, posing challenges in cost, scalability, and reliance on\nexpert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven\nAdaptation), designed to handle various surgical workflow analysis tasks with\nminimal paired image-label data.\n  Methods: Our approach has two key components. First, Few-shot selection-based\nmodality alignment selects a small subset of images and aligns their embeddings\nwith text embeddings from the downstream task, bridging the modality gap.\nSecond, Text-driven adaptation leverages only text data to train a decoder,\neliminating the need for paired image-text data. This decoder is then applied\nto aligned image embeddings, enabling image-related tasks without explicit\nimage-text pairs.\n  Results: We evaluate our approach to generative tasks (image captioning) and\ndiscriminative tasks (triplet recognition and phase recognition). Results show\nthat Surg-FTDA outperforms baselines and generalizes well across downstream\ntasks.\n  Conclusion: We propose a text-driven adaptation approach that mitigates the\nmodality gap and handles multiple downstream tasks in surgical workflow\nanalysis, with minimal reliance on large annotated datasets. The code and\ndataset will be released in https://github.com/CAMMA-public/Surg-FTDA",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09555v3",
    "published_date": "2025-01-16 14:18:06 UTC",
    "updated_date": "2025-03-03 13:05:35 UTC"
  },
  {
    "arxiv_id": "2501.09534v1",
    "title": "AI in Support of Diversity and Inclusion",
    "authors": [
      "Çiçek Güven",
      "Afra Alishahi",
      "Henry Brighton",
      "Gonzalo Nápoles",
      "Juan Sebastian Olier",
      "Marie Šafář",
      "Eric Postma",
      "Dimitar Shterionov",
      "Mirella De Sisto",
      "Eva Vanmassenhove"
    ],
    "abstract": "In this paper, we elaborate on how AI can support diversity and inclusion and\nexemplify research projects conducted in that direction. We start by looking at\nthe challenges and progress in making large language models (LLMs) more\ntransparent, inclusive, and aware of social biases. Even though LLMs like\nChatGPT have impressive abilities, they struggle to understand different\ncultural contexts and engage in meaningful, human like conversations. A key\nissue is that biases in language processing, especially in machine translation,\ncan reinforce inequality. Tackling these biases requires a multidisciplinary\napproach to ensure AI promotes diversity, fairness, and inclusion. We also\nhighlight AI's role in identifying biased content in media, which is important\nfor improving representation. By detecting unequal portrayals of social groups,\nAI can help challenge stereotypes and create more inclusive technologies.\nTransparent AI algorithms, which clearly explain their decisions, are essential\nfor building trust and reducing bias in AI systems. We also stress AI systems\nneed diverse and inclusive training data. Projects like the Child Growth\nMonitor show how using a wide range of data can help address real world\nproblems like malnutrition and poverty. We present a project that demonstrates\nhow AI can be applied to monitor the role of search engines in spreading\ndisinformation about the LGBTQ+ community. Moreover, we discuss the SignON\nproject as an example of how technology can bridge communication gaps between\nhearing and deaf people, emphasizing the importance of collaboration and mutual\ntrust in developing inclusive AI. Overall, with this paper, we advocate for AI\nsystems that are not only effective but also socially responsible, promoting\nfair and inclusive interactions between humans and machines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.09534v1",
    "published_date": "2025-01-16 13:36:24 UTC",
    "updated_date": "2025-01-16 13:36:24 UTC"
  },
  {
    "arxiv_id": "2501.09525v2",
    "title": "Class Incremental Fault Diagnosis under Limited Fault Data via Supervised Contrastive Knowledge Distillation",
    "authors": [
      "Hanrong Zhang",
      "Yifei Yao",
      "Zixuan Wang",
      "Jiayuan Su",
      "Mengxuan Li",
      "Peng Peng",
      "Hongwei Wang"
    ],
    "abstract": "Class-incremental fault diagnosis requires a model to adapt to new fault\nclasses while retaining previous knowledge. However, limited research exists\nfor imbalanced and long-tailed data. Extracting discriminative features from\nfew-shot fault data is challenging, and adding new fault classes often demands\ncostly model retraining. Moreover, incremental training of existing methods\nrisks catastrophic forgetting, and severe class imbalance can bias the model's\ndecisions toward normal classes. To tackle these issues, we introduce a\nSupervised Contrastive knowledge distiLlation for class Incremental Fault\nDiagnosis (SCLIFD) framework proposing supervised contrastive knowledge\ndistillation for improved representation learning capability and less\nforgetting, a novel prioritized exemplar selection method for sample replay to\nalleviate catastrophic forgetting, and the Random Forest Classifier to address\nthe class imbalance. Extensive experimentation on simulated and real-world\nindustrial datasets across various imbalance ratios demonstrates the\nsuperiority of SCLIFD over existing approaches. Our code can be found at\nhttps://github.com/Zhang-Henry/SCLIFD_TII.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09525v2",
    "published_date": "2025-01-16 13:20:29 UTC",
    "updated_date": "2025-01-19 10:11:47 UTC"
  },
  {
    "arxiv_id": "2501.09481v2",
    "title": "MonoSOWA: Scalable monocular 3D Object detector Without human Annotations",
    "authors": [
      "Jan Skvrna",
      "Lukas Neumann"
    ],
    "abstract": "Inferring object 3D position and orientation from a single RGB camera is a\nfoundational task in computer vision with many important applications.\nTraditionally, 3D object detection methods are trained in a fully-supervised\nsetup, requiring LiDAR and vast amounts of human annotations, which are\nlaborious, costly, and do not scale well with the ever-increasing amounts of\ndata being captured.\n  We present a novel method to train a 3D object detector from a single RGB\ncamera without domain-specific human annotations, making orders of magnitude\nmore data available for training. The method uses newly proposed Local Object\nMotion Model to disentangle object movement source between subsequent frames,\nis approximately 700 times faster than previous work and compensates camera\nfocal length differences to aggregate multiple datasets.\n  The method is evaluated on three public datasets, where despite using no\nhuman labels, it outperforms prior work by a significant margin. It also shows\nits versatility as a pre-training tool for fully-supervised training and shows\nthat combining pseudo-labels from multiple datasets can achieve comparable\naccuracy to using human labels from a single dataset. The source code and model\nwill be published soon.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09481v2",
    "published_date": "2025-01-16 11:35:22 UTC",
    "updated_date": "2025-03-10 12:27:10 UTC"
  },
  {
    "arxiv_id": "2501.09469v1",
    "title": "Predicting Air Temperature from Volumetric Urban Morphology with Machine Learning",
    "authors": [
      "Berk Kıvılcım",
      "Patrick Erik Bradley"
    ],
    "abstract": "In this study, we firstly introduce a method that converts CityGML data into\nvoxels which works efficiently and fast in high resolution for large scale\ndatasets such as cities but by sacrificing some building details to overcome\nthe limitations of previous voxelization methodologies that have been\ncomputationally intensive and inefficient at transforming large-scale urban\nareas into voxel representations for high resolution. Those voxelized 3D city\ndata from multiple cities and corresponding air temperature data are used to\ndevelop a machine learning model. Before the model training, Gaussian blurring\nis implemented on input data to consider spatial relationships, as a result the\ncorrelation rate between air temperature and volumetric building morphology is\nalso increased after the Gaussian blurring. After the model training, the\nprediction results are not just evaluated with Mean Square Error (MSE) but some\nimage similarity metrics such as Structural Similarity Index Measure (SSIM) and\nLearned Perceptual Image Patch Similarity (LPIPS) that are able to detect and\nconsider spatial relations during the evaluation process. This trained model is\ncapable of predicting the spatial distribution of air temperature by using\nbuilding volume information of corresponding pixel as input. By doing so, this\nresearch aims to assist urban planners in incorporating environmental\nparameters into their planning strategies, thereby facilitating more\nsustainable and inhabitable urban environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 8 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.09469v1",
    "published_date": "2025-01-16 11:10:38 UTC",
    "updated_date": "2025-01-16 11:10:38 UTC"
  },
  {
    "arxiv_id": "2501.09465v1",
    "title": "RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and Offloading for Edge Object Detection",
    "authors": [
      "Jianrui Shi",
      "Yong Zhao",
      "Zeyang Cui",
      "Xiaoming Shen",
      "Minhang Zeng",
      "Xiaojie Liu"
    ],
    "abstract": "Object detection plays a crucial role in smart video analysis, with\napplications ranging from autonomous driving and security to smart cities.\nHowever, achieving real-time object detection on edge devices presents\nsignificant challenges due to their limited computational resources and the\nhigh demands of deep neural network (DNN)-based detection models, particularly\nwhen processing high-resolution video. Conventional strategies, such as input\ndown-sampling and network up-scaling, often compromise detection accuracy for\nfaster performance or lead to higher inference latency. To address these\nissues, this paper introduces RE-POSE, a Reinforcement Learning (RL)-Driven\nPartitioning and Edge Offloading framework designed to optimize the\naccuracy-latency trade-off in resource-constrained edge environments. Our\napproach features an RL-Based Dynamic Clustering Algorithm (RL-DCA) that\npartitions video frames into non-uniform blocks based on object distribution\nand the computational characteristics of DNNs. Furthermore, a parallel edge\noffloading scheme is implemented to distribute these blocks across multiple\nedge servers for concurrent processing. Experimental evaluations show that\nRE-POSE significantly enhances detection accuracy and reduces inference\nlatency, surpassing existing methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09465v1",
    "published_date": "2025-01-16 10:56:45 UTC",
    "updated_date": "2025-01-16 10:56:45 UTC"
  },
  {
    "arxiv_id": "2501.09775v2",
    "title": "Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong",
    "authors": [
      "Tairan Fu",
      "Javier Conde",
      "Gonzalo Martínez",
      "María Grandury",
      "Pedro Reviriego"
    ],
    "abstract": "One of the most widely used methods to evaluate LLMs are Multiple Choice\nQuestion (MCQ) tests. MCQ benchmarks enable the testing of LLM knowledge on\nalmost any topic at scale as the results can be processed automatically. To\nhelp the LLM answer, a few examples called few shots can be included in the\nprompt. Moreover, the LLM can be asked to answer the question directly with the\nselected option or to first provide the reasoning and then the selected answer,\nwhich is known as chain of thought. In addition to checking whether the\nselected answer is correct, the evaluation can look at the LLM-estimated\nprobability of its response as an indication of the confidence of the LLM in\nthe response. In this paper, we study how the LLM confidence in its answer\ndepends on whether the model has been asked to answer directly or to provide\nthe reasoning before answering. The results of the evaluation of questions on a\nwide range of topics in seven different models show that LLMs are more\nconfident in their answers when they provide reasoning before the answer. This\noccurs regardless of whether the selected answer is correct. Our hypothesis is\nthat this behavior is due to the reasoning that modifies the probability of the\nselected answer, as the LLM predicts the answer based on the input question and\nthe reasoning that supports the selection made. Therefore, LLM estimated\nprobabilities seem to have intrinsic limitations that should be understood in\norder to use them in evaluation procedures. Interestingly, the same behavior\nhas been observed in humans, for whom explaining an answer increases confidence\nin its correctness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09775v2",
    "published_date": "2025-01-16 10:27:51 UTC",
    "updated_date": "2025-01-24 22:03:50 UTC"
  },
  {
    "arxiv_id": "2501.09444v2",
    "title": "Solving the Unsolvable: Translating Case Law in Hong Kong",
    "authors": [
      "King-kui Sin",
      "Xi Xuan",
      "Chunyu Kit",
      "Clara Ho-yan Chan",
      "Honic Ho-kin Ip"
    ],
    "abstract": "This paper addresses the challenges translating case law under Hong Kong's\nbilingual legal system. It highlights the initial success of translating all\nwritten statutes into Chinese before the 1997 handover, a task mandated by the\nBasic Law. The effort involved significant collaboration among legal,\nlinguistic, and translation experts, resulting in a comprehensive and\nculturally appropriate bilingual legal system. However, translating case law\nremains a significant challenge due to the sheer volume and continuous growth\nof judicial decisions. The paper critiques the governments and judiciarys\nsporadic and uncoordinated efforts to translate case law, contrasting it with\nthe thorough approach previously taken for statute translation. Although the\ngovernment acknowledges the importance of legal bilingualism, it lacks a\nsustainable strategy for translating case law. The Judiciarys position that\ntranslating all judgments is unnecessary, unrealistic, and not cost-effectiveis\nanalyzed and critiqued for its impact on legal transparency and public trust. A\nproposed solution involves leveraging machine translation technology through a\nhuman-machine interactive translation platform, which undergoes two major\ntransitions. Initially based on a neural model, the platform transitions to\nusing a large language model for improved translation accuracy. Furthermore, it\nevolves from a single-agent system to a multi-agent system, incorporating\nTranslator, Annotator, and Proofreader agents. This multi-agent approach,\nsupported by a grant, aims to facilitate efficient, high-quality translation of\njudicial judgments by integrating advanced artificial intelligence and\ncontinuous feedback mechanisms, thus better meeting the needs of a bilingual\nlegal system.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09444v2",
    "published_date": "2025-01-16 10:17:58 UTC",
    "updated_date": "2025-01-18 13:32:15 UTC"
  },
  {
    "arxiv_id": "2501.09431v1",
    "title": "A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and Mitigation Strategy",
    "authors": [
      "Huandong Wang",
      "Wenjie Fu",
      "Yingzhou Tang",
      "Zhilong Chen",
      "Yuxi Huang",
      "Jinghua Piao",
      "Chen Gao",
      "Fengli Xu",
      "Tao Jiang",
      "Yong Li"
    ],
    "abstract": "While large language models (LLMs) present significant potential for\nsupporting numerous real-world applications and delivering positive social\nimpacts, they still face significant challenges in terms of the inherent risk\nof privacy leakage, hallucinated outputs, and value misalignment, and can be\nmaliciously used for generating toxic content and unethical purposes after been\njailbroken. Therefore, in this survey, we present a comprehensive review of\nrecent advancements aimed at mitigating these issues, organized across the four\nphases of LLM development and usage: data collecting and pre-training,\nfine-tuning and alignment, prompting and reasoning, and post-processing and\nauditing. We elaborate on the recent advances for enhancing the performance of\nLLMs in terms of privacy protection, hallucination reduction, value alignment,\ntoxicity elimination, and jailbreak defenses. In contrast to previous surveys\nthat focus on a single dimension of responsible LLMs, this survey presents a\nunified framework that encompasses these diverse dimensions, providing a\ncomprehensive view of enhancing LLMs to better serve real-world applications.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09431v1",
    "published_date": "2025-01-16 09:59:45 UTC",
    "updated_date": "2025-01-16 09:59:45 UTC"
  },
  {
    "arxiv_id": "2501.09429v1",
    "title": "ADAGE: A generic two-layer framework for adaptive agent based modelling",
    "authors": [
      "Benjamin Patrick Evans",
      "Sihan Zeng",
      "Sumitra Ganesh",
      "Leo Ardon"
    ],
    "abstract": "Agent-based models (ABMs) are valuable for modelling complex, potentially\nout-of-equilibria scenarios. However, ABMs have long suffered from the Lucas\ncritique, stating that agent behaviour should adapt to environmental changes.\nFurthermore, the environment itself often adapts to these behavioural changes,\ncreating a complex bi-level adaptation problem. Recent progress integrating\nmulti-agent reinforcement learning into ABMs introduces adaptive agent\nbehaviour, beginning to address the first part of this critique, however, the\napproaches are still relatively ad hoc, lacking a general formulation, and\nfurthermore, do not tackle the second aspect of simultaneously adapting\nenvironmental level characteristics in addition to the agent behaviours. In\nthis work, we develop a generic two-layer framework for ADaptive AGEnt based\nmodelling (ADAGE) for addressing these problems. This framework formalises the\nbi-level problem as a Stackelberg game with conditional behavioural policies,\nproviding a consolidated framework for adaptive agent-based modelling based on\nsolving a coupled set of non-linear equations. We demonstrate how this generic\napproach encapsulates several common (previously viewed as distinct) ABM tasks,\nsuch as policy design, calibration, scenario generation, and robust behavioural\nlearning under one unified framework. We provide example simulations on\nmultiple complex economic and financial environments, showing the strength of\nthe novel framework under these canonical settings, addressing long-standing\ncritiques of traditional ABMs.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG",
      "econ.GN",
      "q-fin.CP",
      "q-fin.EC"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted at the 2025 International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS)",
    "pdf_url": "http://arxiv.org/pdf/2501.09429v1",
    "published_date": "2025-01-16 09:58:24 UTC",
    "updated_date": "2025-01-16 09:58:24 UTC"
  },
  {
    "arxiv_id": "2501.09420v1",
    "title": "Dynamic Neural Style Transfer for Artistic Image Generation using VGG19",
    "authors": [
      "Kapil Kashyap",
      "Mehak Garg",
      "Sean Fargose",
      "Sindhu Nair"
    ],
    "abstract": "Throughout history, humans have created remarkable works of art, but\nartificial intelligence has only recently started to make strides in generating\nvisually compelling art. Breakthroughs in the past few years have focused on\nusing convolutional neural networks (CNNs) to separate and manipulate the\ncontent and style of images, applying texture synthesis techniques.\nNevertheless, a number of current techniques continue to encounter obstacles,\nincluding lengthy processing times, restricted choices of style images, and the\ninability to modify the weight ratio of styles. We proposed a neural style\ntransfer system that can add various artistic styles to a desired image to\naddress these constraints allowing flexible adjustments to style weight ratios\nand reducing processing time. The system uses the VGG19 model for feature\nextraction, ensuring high-quality, flexible stylization without compromising\ncontent integrity.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09420v1",
    "published_date": "2025-01-16 09:47:18 UTC",
    "updated_date": "2025-01-16 09:47:18 UTC"
  },
  {
    "arxiv_id": "2501.09410v1",
    "title": "MoE$^2$: Optimizing Collaborative Inference for Edge Large Language Models",
    "authors": [
      "Lyudong Jin",
      "Yanning Zhang",
      "Yanhan Li",
      "Shurong Wang",
      "Howard H. Yang",
      "Jian Wu",
      "Meng Zhang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing tasks. Exploiting the heterogeneous\ncapabilities of edge LLMs is crucial for diverse emerging applications, as it\nenables greater cost-effectiveness and reduced latency. In this work, we\nintroduce \\textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative\ninference framework for edge LLMs. We formulate the joint gating and expert\nselection problem to optimize inference performance under energy and latency\nconstraints. Unlike conventional MoE problems, LLM expert selection is\nsignificantly more challenging due to the combinatorial nature and the\nheterogeneity of edge LLMs across various attributes. To this end, we propose a\ntwo-level expert selection mechanism through which we uncover an\noptimality-preserving property of gating parameters across expert selections.\nThis property enables the decomposition of the training and selection\nprocesses, significantly reducing complexity. Furthermore, we leverage the\nobjective's monotonicity and design a discrete monotonic optimization algorithm\nfor optimal expert selection. We implement edge servers with NVIDIA Jetson AGX\nOrins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results\nvalidate that performance improvements of various LLM models and show that our\nMoE$^2$ method can achieve optimal trade-offs among different delay and energy\nbudgets, and outperforms baselines under various system resource constraints.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "Submitted to IEEE/ACM Transactions on Networking",
    "pdf_url": "http://arxiv.org/pdf/2501.09410v1",
    "published_date": "2025-01-16 09:36:32 UTC",
    "updated_date": "2025-01-16 09:36:32 UTC"
  },
  {
    "arxiv_id": "2502.00022v1",
    "title": "A Dynamic and High-Precision Method for Scenario-Based HRA Synthetic Data Collection in Multi-Agent Collaborative Environments Driven by LLMs",
    "authors": [
      "Xingyu Xiao",
      "Peng Chen",
      "Qianqian Jia",
      "Jiejuan Tong",
      "Jingang Liang",
      "Haitao Wang"
    ],
    "abstract": "HRA (Human Reliability Analysis) data is crucial for advancing HRA\nmethodologies. however, existing data collection methods lack the necessary\ngranularity, and most approaches fail to capture dynamic features.\nAdditionally, many methods require expert knowledge as input, making them\ntime-consuming and labor-intensive. To address these challenges, we propose a\nnew paradigm for the automated collection of HRA data. Our approach focuses on\nkey indicators behind human error, specifically measuring workload in\ncollaborative settings. This study introduces a novel, scenario-driven method\nfor workload estimation, leveraging fine-tuned large language models (LLMs). By\ntraining LLMs on real-world operational data from high-temperature gas-cooled\nreactors (HTGRs), we simulate human behavior and cognitive load in real time\nacross various collaborative scenarios. The method dynamically adapts to\nchanges in operator workload, providing more accurate, flexible, and scalable\nworkload estimates. The results demonstrate that the proposed WELLA (Workload\nEstimation with LLMs and Agents) outperforms existing commercial LLM-based\nmethods in terms of prediction accuracy.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00022v1",
    "published_date": "2025-01-16 09:23:48 UTC",
    "updated_date": "2025-01-16 09:23:48 UTC"
  },
  {
    "arxiv_id": "2501.09395v1",
    "title": "ELM-DeepONets: Backpropagation-Free Training of Deep Operator Networks via Extreme Learning Machines",
    "authors": [
      "Hwijae Son"
    ],
    "abstract": "Deep Operator Networks (DeepONets) are among the most prominent frameworks\nfor operator learning, grounded in the universal approximation theorem for\noperators. However, training DeepONets typically requires significant\ncomputational resources. To address this limitation, we propose ELM-DeepONets,\nan Extreme Learning Machine (ELM) framework for DeepONets that leverages the\nbackpropagation-free nature of ELM. By reformulating DeepONet training as a\nleast-squares problem for newly introduced parameters, the ELM-DeepONet\napproach significantly reduces training complexity. Validation on benchmark\nproblems, including nonlinear ODEs and PDEs, demonstrates that the proposed\nmethod not only achieves superior accuracy but also drastically reduces\ncomputational costs. This work offers a scalable and efficient alternative for\noperator learning in scientific computing.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09395v1",
    "published_date": "2025-01-16 09:06:43 UTC",
    "updated_date": "2025-01-16 09:06:43 UTC"
  },
  {
    "arxiv_id": "2501.09394v1",
    "title": "Quantum-Enhanced Transformers for Robust Acoustic Scene Classification in IoT Environments",
    "authors": [
      "Minh K. Quan",
      "Mayuri Wijayasundara",
      "Sujeeva Setunge",
      "Pubudu N. Pathirana"
    ],
    "abstract": "The proliferation of Internet of Things (IoT) devices equipped with acoustic\nsensors necessitates robust acoustic scene classification (ASC) capabilities,\neven in noisy and data-limited environments. Traditional machine learning\nmethods often struggle to generalize effectively under such conditions. To\naddress this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene\nClassifier that leverages the power of quantum-inspired transformers. By\nintegrating quantum concepts like superposition and entanglement, Q-ASC\nachieves superior feature learning and enhanced noise resilience compared to\nclassical models. Furthermore, we introduce a Quantum Variational Autoencoder\n(QVAE) based data augmentation technique to mitigate the challenge of limited\nlabeled data in IoT deployments. Extensive evaluations on the Tampere\nUniversity of Technology (TUT) Acoustic Scenes 2016 benchmark dataset\ndemonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5%\nunder challenging conditions, outperforming state-of-the-art methods by over 5%\nin the best case. This research paves the way for deploying intelligent\nacoustic sensing in IoT networks, with potential applications in smart homes,\nindustrial monitoring, and environmental surveillance, even in adverse acoustic\nenvironments.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.PF",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.09394v1",
    "published_date": "2025-01-16 09:06:10 UTC",
    "updated_date": "2025-01-16 09:06:10 UTC"
  },
  {
    "arxiv_id": "2501.09368v3",
    "title": "Aligning Instruction Tuning with Pre-training",
    "authors": [
      "Yiming Liang",
      "Tianyu Zheng",
      "Xinrun Du",
      "Ge Zhang",
      "Jiaheng Liu",
      "Xingwei Qu",
      "Wenqiang Zu",
      "Xingrun Xing",
      "Chujie Zheng",
      "Lei Ma",
      "Wenhu Chen",
      "Guoyin Wang",
      "Zhaoxiang Zhang",
      "Wenhao Huang",
      "Xiang Yue",
      "Jiajun Zhang"
    ],
    "abstract": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose Aligning Instruction Tuning\nwith Pre-training (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:hep-ph/9811436 by other\n  authors",
    "pdf_url": "http://arxiv.org/pdf/2501.09368v3",
    "published_date": "2025-01-16 08:27:40 UTC",
    "updated_date": "2025-01-20 14:05:35 UTC"
  },
  {
    "arxiv_id": "2501.09355v1",
    "title": "YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents in Augmented Reality Tasks",
    "authors": [
      "Saptarashmi Bandyopadhyay",
      "Vikas Bahirwani",
      "Lavisha Aggarwal",
      "Bhanu Guda",
      "Lin Li",
      "Andrea Colaco"
    ],
    "abstract": "Multimodal AI Agents are AI models that have the capability of interactively\nand cooperatively assisting human users to solve day-to-day tasks. Augmented\nReality (AR) head worn devices can uniquely improve the user experience of\nsolving procedural day-to-day tasks by providing egocentric multimodal (audio\nand video) observational capabilities to AI Agents. Such AR capabilities can\nhelp AI Agents see and listen to actions that users take which can relate to\nmultimodal capabilities of human users. Existing AI Agents, either Large\nLanguage Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive\nin nature, which means that models cannot take an action without reading or\nlistening to the human user's prompts. Proactivity of AI Agents on the other\nhand can help the human user detect and correct any mistakes in agent observed\ntasks, encourage users when they do tasks correctly or simply engage in\nconversation with the user - akin to a human teaching or assisting a user. Our\nproposed YET to Intervene (YETI) multimodal agent focuses on the research\nquestion of identifying circumstances that may require the agent to intervene\nproactively. This allows the agent to understand when it can intervene in a\nconversation with human users that can help the user correct mistakes on tasks,\nlike cooking, using AR. Our YETI Agent learns scene understanding signals based\non interpretable notions of Structural Similarity (SSIM) on consecutive video\nframes. We also define the alignment signal which the AI Agent can learn to\nidentify if the video frames corresponding to the user's actions on the task\nare consistent with expected actions. These signals are used by our AI Agent to\ndetermine when it should proactively intervene. We compare our results on the\ninstances of proactive intervention in the HoloAssist multimodal benchmark for\nan expert agent guiding a user to complete procedural tasks.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "cs.MA",
      "I.2; I.2.10; I.2.11; I.2.1; I.2.7; I.4.8; I.4.9"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2501.09355v1",
    "published_date": "2025-01-16 08:06:02 UTC",
    "updated_date": "2025-01-16 08:06:02 UTC"
  },
  {
    "arxiv_id": "2501.09354v1",
    "title": "Style4Rec: Enhancing Transformer-based E-commerce Recommendation Systems with Style and Shopping Cart Information",
    "authors": [
      "Berke Ugurlu",
      "Ming-Yi Hong",
      "Che Lin"
    ],
    "abstract": "Understanding users' product preferences is essential to the efficacy of a\nrecommendation system. Precision marketing leverages users' historical data to\ndiscern these preferences and recommends products that align with them.\nHowever, recent browsing and purchase records might better reflect current\npurchasing inclinations. Transformer-based recommendation systems have made\nstrides in sequential recommendation tasks, but they often fall short in\nutilizing product image style information and shopping cart data effectively.\nIn light of this, we propose Style4Rec, a transformer-based e-commerce\nrecommendation system that harnesses style and shopping cart information to\nenhance existing transformer-based sequential product recommendation systems.\nStyle4Rec represents a significant step forward in personalized e-commerce\nrecommendations, outperforming benchmarks across various evaluation metrics.\nStyle4Rec resulted in notable improvements: HR@5 increased from 0.681 to 0.735,\nNDCG@5 increased from 0.594 to 0.674, and MRR@5 increased from 0.559 to 0.654.\nWe tested our model using an e-commerce dataset from our partnering company and\nfound that it exceeded established transformer-based sequential recommendation\nbenchmarks across various evaluation metrics. Thus, Style4Rec presents a\nsignificant step forward in personalized e-commerce recommendation systems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "9 pages, 6 images, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.09354v1",
    "published_date": "2025-01-16 08:05:39 UTC",
    "updated_date": "2025-01-16 08:05:39 UTC"
  },
  {
    "arxiv_id": "2501.09345v3",
    "title": "Rational Tuning of LLM Cascades via Probabilistic Modeling",
    "authors": [
      "Michael J. Zellinger",
      "Matt Thomson"
    ],
    "abstract": "Understanding the reliability of large language models (LLMs) has recently\ngarnered significant attention. Given LLMs' propensity to hallucinate, as well\nas their high sensitivity to prompt design, it is already challenging to\npredict the performance of an individual LLM. However, the problem becomes more\ncomplex for compound LLM systems such as cascades, where in addition to each\nmodel's standalone performance, we must understand how the error rates of\ndifferent models interact. In this paper, we present a probabilistic model for\nthe joint performance distribution of a sequence of LLMs, which enables a\nframework for rationally tuning the confidence thresholds of a LLM cascade\nusing continuous optimization. Compared to selecting confidence thresholds\nusing grid search, our parametric Markov-copula model significantly improves\nruntime scaling with respect to the length of the cascade and the desired\nresolution of the cost-error curve, turning them from intractable into\nlow-order polynomial. In addition, the optimal thresholds computed using our\ncontinuous optimization-based algorithm increasingly outperform those found via\ngrid search as cascade length grows, improving the area under the cost-error\ncurve by 1.9% on average for cascades consisting of at least three models.\nOverall, our Markov-copula model provides a rational basis for tuning LLM\ncascade performance and points to the potential of probabilistic methods in\nanalyzing LLM systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.09345v3",
    "published_date": "2025-01-16 07:58:33 UTC",
    "updated_date": "2025-03-05 19:23:10 UTC"
  },
  {
    "arxiv_id": "2501.09333v2",
    "title": "Prompt-CAM: Making Vision Transformers Interpretable for Fine-Grained Analysis",
    "authors": [
      "Arpita Chowdhury",
      "Dipanjyoti Paul",
      "Zheda Mai",
      "Jianyang Gu",
      "Ziheng Zhang",
      "Kazi Sajeed Mehrab",
      "Elizabeth G. Campolongo",
      "Daniel Rubenstein",
      "Charles V. Stewart",
      "Anuj Karpatne",
      "Tanya Berger-Wolf",
      "Yu Su",
      "Wei-Lun Chao"
    ],
    "abstract": "We present a simple approach to make pre-trained Vision Transformers (ViTs)\ninterpretable for fine-grained analysis, aiming to identify and localize the\ntraits that distinguish visually similar categories, such as bird species.\nPre-trained ViTs, such as DINO, have demonstrated remarkable capabilities in\nextracting localized, discriminative features. However, saliency maps like\nGrad-CAM often fail to identify these traits, producing blurred, coarse\nheatmaps that highlight entire objects instead. We propose a novel approach,\nPrompt Class Attention Map (Prompt-CAM), to address this limitation. Prompt-CAM\nlearns class-specific prompts for a pre-trained ViT and uses the corresponding\noutputs for classification. To correctly classify an image, the true-class\nprompt must attend to unique image patches not present in other classes' images\n(i.e., traits). As a result, the true class's multi-head attention maps reveal\ntraits and their locations. Implementation-wise, Prompt-CAM is almost a ``free\nlunch,'' requiring only a modification to the prediction head of Visual Prompt\nTuning (VPT). This makes Prompt-CAM easy to train and apply, in stark contrast\nto other interpretable methods that require designing specific models and\ntraining processes. Extensive empirical studies on a dozen datasets from\nvarious domains (e.g., birds, fishes, insects, fungi, flowers, food, and cars)\nvalidate the superior interpretation capability of Prompt-CAM. The source code\nand demo are available at https://github.com/Imageomics/Prompt_CAM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2501.09333v2",
    "published_date": "2025-01-16 07:07:41 UTC",
    "updated_date": "2025-04-07 18:03:40 UTC"
  },
  {
    "arxiv_id": "2501.09328v2",
    "title": "Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks",
    "authors": [
      "Yixiao Xu",
      "Binxing Fang",
      "Rui Wang",
      "Yinghai Zhou",
      "Shouling Ji",
      "Yuan Liu",
      "Mohan Li",
      "Zhihong Tian"
    ],
    "abstract": "Developing high-performance deep learning models is resource-intensive,\nleading model owners to utilize Machine Learning as a Service (MLaaS) platforms\ninstead of publicly releasing their models. However, malicious users may\nexploit query interfaces to execute model extraction attacks, reconstructing\nthe target model's functionality locally. While prior research has investigated\ntriggerable watermarking techniques for asserting ownership, existing methods\nface significant challenges: (1) most approaches require additional training,\nresulting in high overhead and limited flexibility, and (2) they often fail to\naccount for advanced attackers, leaving them vulnerable to adaptive attacks.\n  In this paper, we propose Neural Honeytrace, a robust plug-and-play\nwatermarking framework against model extraction attacks. We first formulate a\nwatermark transmission model from an information-theoretic perspective,\nproviding an interpretable account of the principles and limitations of\nexisting triggerable watermarking. Guided by the model, we further introduce:\n(1) a similarity-based training-free watermarking method for plug-and-play and\nflexible watermarking, and (2) a distribution-based multi-step watermark\ninformation transmission strategy for robust watermarking. Comprehensive\nexperiments on four datasets demonstrate that Neural Honeytrace outperforms\nprevious methods in efficiency and resisting adaptive attacks. Neural\nHoneytrace reduces the average number of samples required for a worst-case\nt-Test-based copyright claim from $12,000$ to $200$ with zero training cost.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09328v2",
    "published_date": "2025-01-16 06:59:20 UTC",
    "updated_date": "2025-01-17 06:50:23 UTC"
  },
  {
    "arxiv_id": "2501.09327v2",
    "title": "On Learning Informative Trajectory Embeddings for Imitation, Classification and Regression",
    "authors": [
      "Zichang Ge",
      "Changyu Chen",
      "Arunesh Sinha",
      "Pradeep Varakantham"
    ],
    "abstract": "In real-world sequential decision making tasks like autonomous driving,\nrobotics, and healthcare, learning from observed state-action trajectories is\ncritical for tasks like imitation, classification, and clustering. For example,\nself-driving cars must replicate human driving behaviors, while robots and\nhealthcare systems benefit from modeling decision sequences, whether or not\nthey come from expert data. Existing trajectory encoding methods often focus on\nspecific tasks or rely on reward signals, limiting their ability to generalize\nacross domains and tasks. Inspired by the success of embedding models like CLIP\nand BERT in static domains, we propose a novel method for embedding\nstate-action trajectories into a latent space that captures the skills and\ncompetencies in the dynamic underlying decision-making processes. This method\noperates without the need for reward labels, enabling better generalization\nacross diverse domains and tasks. Our contributions are threefold: (1) We\nintroduce a trajectory embedding approach that captures multiple abilities from\nstate-action data. (2) The learned embeddings exhibit strong representational\npower across downstream tasks, including imitation, classification, clustering,\nand regression. (3) The embeddings demonstrate unique properties, such as\ncontrolling agent behaviors in IQ-Learn and an additive structure in the latent\nspace. Experimental results confirm that our method outperforms traditional\napproaches, offering more flexible and powerful trajectory representations for\nvarious applications. Our code is available at\nhttps://github.com/Erasmo1015/vte.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "AAMAS 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.09327v2",
    "published_date": "2025-01-16 06:52:58 UTC",
    "updated_date": "2025-01-17 18:30:04 UTC"
  },
  {
    "arxiv_id": "2501.09316v1",
    "title": "SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs",
    "authors": [
      "Anbang Ye",
      "Qianran Ma",
      "Jia Chen",
      "Muqi Li",
      "Tong Li",
      "Fujiao Liu",
      "Siqi Mai",
      "Meichen Lu",
      "Haitao Bao",
      "Yang You"
    ],
    "abstract": "Despite significant advancements in general-purpose AI agents, several\nchallenges still hinder their practical application in real-world scenarios.\nFirst, the limited planning capabilities of Large Language Models (LLM)\nrestrict AI agents from effectively solving complex tasks that require\nlong-horizon planning. Second, general-purpose AI agents struggle to\nefficiently utilize domain-specific knowledge and human expertise. In this\npaper, we introduce the Standard Operational Procedure-guided Agent\n(SOP-agent), a novel framework for constructing domain-specific agents through\npseudocode-style Standard Operational Procedures (SOPs) written in natural\nlanguage. Formally, we represent a SOP as a decision graph, which is traversed\nto guide the agent in completing tasks specified by the SOP. We conduct\nextensive experiments across tasks in multiple domains, including\ndecision-making, search and reasoning, code generation, data cleaning, and\ngrounded customer service. The SOP-agent demonstrates excellent versatility,\nachieving performance superior to general-purpose agent frameworks and\ncomparable to domain-specific agent systems. Additionally, we introduce the\nGrounded Customer Service Benchmark, the first benchmark designed to evaluate\nthe grounded decision-making capabilities of AI agents in customer service\nscenarios based on SOPs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "35 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.09316v1",
    "published_date": "2025-01-16 06:14:58 UTC",
    "updated_date": "2025-01-16 06:14:58 UTC"
  },
  {
    "arxiv_id": "2501.09311v1",
    "title": "Shape-Based Single Object Classification Using Ensemble Method Classifiers",
    "authors": [
      "Nur Shazwani Kamarudin",
      "Mokhairi Makhtar",
      "Syadiah Nor Wan Shamsuddin",
      "Syed Abdullah Fadzli"
    ],
    "abstract": "Nowadays, more and more images are available. Annotation and retrieval of the\nimages pose classification problems, where each class is defined as the group\nof database images labelled with a common semantic label. Various systems have\nbeen proposed for content-based retrieval, as well as for image classification\nand indexing. In this paper, a hierarchical classification framework has been\nproposed for bridging the semantic gap effectively and achieving multi-category\nimage classification. A well known pre-processing and post-processing method\nwas used and applied to three problems; image segmentation, object\nidentification and image classification. The method was applied to classify\nsingle object images from Amazon and Google datasets. The classification was\ntested for four different classifiers; BayesNetwork (BN), Random Forest (RF),\nBagging and Vote. The estimated classification accuracies ranged from 20% to\n99% (using 10-fold cross validation). The Bagging classifier presents the best\nperformance, followed by the Random Forest classifier.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09311v1",
    "published_date": "2025-01-16 05:58:32 UTC",
    "updated_date": "2025-01-16 05:58:32 UTC"
  },
  {
    "arxiv_id": "2501.09310v1",
    "title": "A Study of In-Context-Learning-Based Text-to-SQL Errors",
    "authors": [
      "Jiawei Shen",
      "Chengcheng Wan",
      "Ruoyi Qiao",
      "Jiazhen Zou",
      "Hang Xu",
      "Yuchen Shao",
      "Yueling Zhang",
      "Weikai Miao",
      "Geguang Pu"
    ],
    "abstract": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09310v1",
    "published_date": "2025-01-16 05:54:59 UTC",
    "updated_date": "2025-01-16 05:54:59 UTC"
  },
  {
    "arxiv_id": "2501.09309v1",
    "title": "Understanding Mental Health Content on Social Media and Its Effect Towards Suicidal Ideation",
    "authors": [
      "Mohaiminul Islam Bhuiyan",
      "Nur Shazwani Kamarudin",
      "Nur Hafieza Ismail"
    ],
    "abstract": "This review underscores the critical need for effective strategies to\nidentify and support individuals with suicidal ideation, exploiting\ntechnological innovations in ML and DL to further suicide prevention efforts.\nThe study details the application of these technologies in analyzing vast\namounts of unstructured social media data to detect linguistic patterns,\nkeywords, phrases, tones, and contextual cues associated with suicidal\nthoughts. It explores various ML and DL models like SVMs, CNNs, LSTM, neural\nnetworks, and their effectiveness in interpreting complex data patterns and\nemotional nuances within text data. The review discusses the potential of these\ntechnologies to serve as a life-saving tool by identifying at-risk individuals\nthrough their digital traces. Furthermore, it evaluates the real-world\neffectiveness, limitations, and ethical considerations of employing these\ntechnologies for suicide prevention, stressing the importance of responsible\ndevelopment and usage. The study aims to fill critical knowledge gaps by\nanalyzing recent studies, methodologies, tools, and techniques in this field.\nIt highlights the importance of synthesizing current literature to inform\npractical tools and suicide prevention efforts, guiding innovation in reliable,\nethical systems for early intervention. This research synthesis evaluates the\nintersection of technology and mental health, advocating for the ethical and\nresponsible application of ML, DL, and NLP to offer life-saving potential\nworldwide while addressing challenges like generalizability, biases, privacy,\nand the need for further research to ensure these technologies do not\nexacerbate existing inequities and harms.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09309v1",
    "published_date": "2025-01-16 05:46:27 UTC",
    "updated_date": "2025-01-16 05:46:27 UTC"
  },
  {
    "arxiv_id": "2501.09292v3",
    "title": "To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic Retrieval Augmented Generation",
    "authors": [
      "Kaustubh D. Dhole"
    ],
    "abstract": "Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "I.2.7; H.3.3; I.5.5"
    ],
    "primary_category": "cs.CL",
    "comment": "1st workshop of \"Quantify Uncertainty and Hallucination in Foundation\n  Models: The Next Frontier in Reliable AI\" at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.09292v3",
    "published_date": "2025-01-16 04:56:33 UTC",
    "updated_date": "2025-03-18 16:42:17 UTC"
  },
  {
    "arxiv_id": "2501.09291v2",
    "title": "LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport",
    "authors": [
      "Kyeongha Rho",
      "Hyeongkeun Lee",
      "Valentio Iverson",
      "Joon Son Chung"
    ],
    "abstract": "Automated audio captioning is a task that generates textual descriptions for\naudio content, and recent studies have explored using visual information to\nenhance captioning quality. However, current methods often fail to effectively\nfuse audio and visual data, missing important semantic cues from each modality.\nTo address this, we introduce LAVCap, a large language model (LLM)-based\naudio-visual captioning framework that effectively integrates visual\ninformation with audio to improve audio captioning performance. LAVCap employs\nan optimal transport-based alignment loss to bridge the modality gap between\naudio and visual features, enabling more effective semantic extraction.\nAdditionally, we propose an optimal transport attention module that enhances\naudio-visual fusion using an optimal transport assignment map. Combined with\nthe optimal training strategy, experimental results demonstrate that each\ncomponent of our framework is effective. LAVCap outperforms existing\nstate-of-the-art methods on the AudioCaps dataset, without relying on large\ndatasets or post-processing. Code is available at\nhttps://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "5 pages, 2 figures; Accepted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.09291v2",
    "published_date": "2025-01-16 04:53:29 UTC",
    "updated_date": "2025-03-15 12:38:50 UTC"
  },
  {
    "arxiv_id": "2501.09284v2",
    "title": "SEAL: Entangled White-box Watermarks on Low-Rank Adaptation",
    "authors": [
      "Giyeong Oh",
      "Saejin Kim",
      "Woohyun Cho",
      "Sangkyu Lee",
      "Jiwan Chung",
      "Dokyung Song",
      "Youngjae Yu"
    ],
    "abstract": "Recently, LoRA and its variants have become the de facto strategy for\ntraining and sharing task-specific versions of large pretrained models, thanks\nto their efficiency and simplicity. However, the issue of copyright protection\nfor LoRA weights, especially through watermark-based techniques, remains\nunderexplored. To address this gap, we propose SEAL (SEcure wAtermarking on\nLoRA weights), the universal whitebox watermarking for LoRA. SEAL embeds a\nsecret, non-trainable matrix between trainable LoRA weights, serving as a\npassport to claim ownership. SEAL then entangles the passport with the LoRA\nweights through training, without extra loss for entanglement, and distributes\nthe finetuned weights after hiding the passport. When applying SEAL, we\nobserved no performance degradation across commonsense reasoning,\ntextual/visual instruction tuning, and text-to-image synthesis tasks. We\ndemonstrate that SEAL is robust against a variety of known attacks: removal,\nobfuscation, and ambiguity attacks.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "Author name corrected",
    "pdf_url": "http://arxiv.org/pdf/2501.09284v2",
    "published_date": "2025-01-16 04:17:56 UTC",
    "updated_date": "2025-01-17 04:59:32 UTC"
  },
  {
    "arxiv_id": "2501.09279v1",
    "title": "Text Semantics to Flexible Design: A Residential Layout Generation Method Based on Stable Diffusion Model",
    "authors": [
      "Zijin Qiu",
      "Jiepeng Liu",
      "Yi Xia",
      "Hongtuo Qi",
      "Pengkun Liu"
    ],
    "abstract": "Flexibility in the AI-based residential layout design remains a significant\nchallenge, as traditional methods like rule-based heuristics and graph-based\ngeneration often lack flexibility and require substantial design knowledge from\nusers. To address these limitations, we propose a cross-modal design approach\nbased on the Stable Diffusion model for generating flexible residential\nlayouts. The method offers multiple input types for learning objectives,\nallowing users to specify both boundaries and layouts. It incorporates natural\nlanguage as design constraints and introduces ControlNet to enable stable\nlayout generation through two distinct pathways. We also present a scheme that\nencapsulates design expertise within a knowledge graph and translates it into\nnatural language, providing an interpretable representation of design\nknowledge. This comprehensibility and diversity of input options enable\nprofessionals and non-professionals to directly express design requirements,\nenhancing flexibility and controllability. Finally, experiments verify the\nflexibility of the proposed methods under multimodal constraints better than\nstate-of-the-art models, even when specific semantic information about room\nareas or connections is incomplete.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09279v1",
    "published_date": "2025-01-16 03:57:38 UTC",
    "updated_date": "2025-01-16 03:57:38 UTC"
  },
  {
    "arxiv_id": "2501.09274v2",
    "title": "Large Language Model is Secretly a Protein Sequence Optimizer",
    "authors": [
      "Yinkai Wang",
      "Jiaxing He",
      "Yuanqi Du",
      "Xiaohui Chen",
      "Jianan Canal Li",
      "Li-Ping Liu",
      "Xiaolin Xu",
      "Soha Hassoun"
    ],
    "abstract": "We consider the protein sequence engineering problem, which aims to find\nprotein sequences with high fitness levels, starting from a given wild-type\nsequence. Directed evolution has been a dominating paradigm in this field which\nhas an iterative process to generate variants and select via experimental\nfeedback. We demonstrate large language models (LLMs), despite being trained on\nmassive texts, are secretly protein sequence optimizers. With a directed\nevolutionary method, LLM can perform protein engineering through Pareto and\nexperiment-budget constrained optimization, demonstrating success on both\nsynthetic and experimental fitness landscapes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2501.09274v2",
    "published_date": "2025-01-16 03:44:16 UTC",
    "updated_date": "2025-01-17 15:22:00 UTC"
  },
  {
    "arxiv_id": "2501.09265v1",
    "title": "Perspective Transition of Large Language Models for Solving Subjective Tasks",
    "authors": [
      "Xiaolong Wang",
      "Yuanchi Zhang",
      "Ziyue Wang",
      "Yuzhuang Xu",
      "Fuwen Luo",
      "Yile Wang",
      "Peng Li",
      "Yang Liu"
    ],
    "abstract": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, enabling remarkable progress in various tasks. Different\nfrom objective tasks such as commonsense reasoning and arithmetic\nquestion-answering, the performance of LLMs on subjective tasks is still\nlimited, where the perspective on the specific problem plays crucial roles for\nbetter interpreting the context and giving proper response. For example, in\ncertain scenarios, LLMs may perform better when answering from an expert role\nperspective, potentially eliciting their relevant domain knowledge. In\ncontrast, in some scenarios, LLMs may provide more accurate responses when\nanswering from a third-person standpoint, enabling a more comprehensive\nunderstanding of the problem and potentially mitigating inherent biases. In\nthis paper, we propose Reasoning through Perspective Transition (RPT), a method\nbased on in-context learning that enables LLMs to dynamically select among\ndirect, role, and third-person perspectives for the best way to solve\ncorresponding subjective problem. Through extensive experiments on totally 12\nsubjective tasks by using both closed-source and open-source LLMs including\nGPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single\nfixed perspective based methods such as chain-of-thought prompting and expert\nprompting, highlights the intricate ways that LLMs can adapt their perspectives\nto provide nuanced and contextually appropriate responses for different\nproblems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09265v1",
    "published_date": "2025-01-16 03:30:47 UTC",
    "updated_date": "2025-01-16 03:30:47 UTC"
  },
  {
    "arxiv_id": "2501.09254v1",
    "title": "Clone-Robust AI Alignment",
    "authors": [
      "Ariel D. Procaccia",
      "Benjamin Schiffer",
      "Shirley Zhang"
    ],
    "abstract": "A key challenge in training Large Language Models (LLMs) is properly aligning\nthem with human preferences. Reinforcement Learning with Human Feedback (RLHF)\nuses pairwise comparisons from human annotators to train reward functions and\nhas emerged as a popular alignment method. However, input datasets in RLHF are\nnot necessarily balanced in the types of questions and answers that are\nincluded. Therefore, we want RLHF algorithms to perform well even when the set\nof alternatives is not uniformly distributed. Drawing on insights from social\nchoice theory, we introduce robustness to approximate clones, a desirable\nproperty of RLHF algorithms which requires that adding near-duplicate\nalternatives does not significantly change the learned reward function. We\nfirst demonstrate that the standard RLHF algorithm based on regularized maximum\nlikelihood estimation (MLE) fails to satisfy this property. We then propose the\nweighted MLE, a new RLHF algorithm that modifies the standard regularized MLE\nby weighting alternatives based on their similarity to other alternatives. This\nnew algorithm guarantees robustness to approximate clones while preserving\ndesirable theoretical properties.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09254v1",
    "published_date": "2025-01-16 02:43:44 UTC",
    "updated_date": "2025-01-16 02:43:44 UTC"
  },
  {
    "arxiv_id": "2501.09239v1",
    "title": "AI-based Identity Fraud Detection: A Systematic Review",
    "authors": [
      "Chuo Jun Zhang",
      "Asif Q. Gill",
      "Bo Liu",
      "Memoona J. Anwar"
    ],
    "abstract": "With the rapid development of digital services, a large volume of personally\nidentifiable information (PII) is stored online and is subject to cyberattacks\nsuch as Identity fraud. Most recently, the use of Artificial Intelligence (AI)\nenabled deep fake technologies has significantly increased the complexity of\nidentity fraud. Fraudsters may use these technologies to create highly\nsophisticated counterfeit personal identification documents, photos and videos.\nThese advancements in the identity fraud landscape pose challenges for identity\nfraud detection and society at large. There is a pressing need to review and\nunderstand identity fraud detection methods, their limitations and potential\nsolutions. This research aims to address this important need by using the\nwell-known systematic literature review method. This paper reviewed a selected\nset of 43 papers across 4 major academic literature databases. In particular,\nthe review results highlight the two types of identity fraud prevention and\ndetection methods, in-depth and open challenges. The results were also\nconsolidated into a taxonomy of AI-based identity fraud detection and\nprevention methods including key insights and trends. Overall, this paper\nprovides a foundational knowledge base to researchers and practitioners for\nfurther research and development in this important area of digital identity\nfraud.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09239v1",
    "published_date": "2025-01-16 01:52:30 UTC",
    "updated_date": "2025-01-16 01:52:30 UTC"
  },
  {
    "arxiv_id": "2501.09223v1",
    "title": "Foundations of Large Language Models",
    "authors": [
      "Tong Xiao",
      "Jingbo Zhu"
    ],
    "abstract": "This is a book about large language models. As indicated by the title, it\nprimarily focuses on foundational concepts rather than comprehensive coverage\nof all cutting-edge technologies. The book is structured into four main\nchapters, each exploring a key area: pre-training, generative models, prompting\ntechniques, and alignment methods. It is intended for college students,\nprofessionals, and practitioners in natural language processing and related\nfields, and can serve as a reference for anyone interested in large language\nmodels.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09223v1",
    "published_date": "2025-01-16 01:03:56 UTC",
    "updated_date": "2025-01-16 01:03:56 UTC"
  },
  {
    "arxiv_id": "2501.09218v1",
    "title": "Interpretable Droplet Digital PCR Assay for Trustworthy Molecular Diagnostics",
    "authors": [
      "Yuanyuan Wei",
      "Yucheng Wu",
      "Fuyang Qu",
      "Yao Mu",
      "Yi-Ping Ho",
      "Ho-Pui Ho",
      "Wu Yuan",
      "Mingkun Xu"
    ],
    "abstract": "Accurate molecular quantification is essential for advancing research and\ndiagnostics in fields such as infectious diseases, cancer biology, and genetic\ndisorders. Droplet digital PCR (ddPCR) has emerged as a gold standard for\nachieving absolute quantification. While computational ddPCR technologies have\nadvanced significantly, achieving automatic interpretation and consistent\nadaptability across diverse operational environments remains a challenge. To\naddress these limitations, we introduce the intelligent interpretable droplet\ndigital PCR (I2ddPCR) assay, a comprehensive framework integrating front-end\npredictive models (for droplet segmentation and classification) with GPT-4o\nmultimodal large language model (MLLM, for context-aware explanations and\nrecommendations) to automate and enhance ddPCR image analysis. This approach\nsurpasses the state-of-the-art models, affording 99.05% accuracy in processing\ncomplex ddPCR images containing over 300 droplets per image with varying\nsignal-to-noise ratios (SNRs). By combining specialized neural networks and\nlarge language models, the I2ddPCR assay offers a robust and adaptable solution\nfor absolute molecular quantification, achieving a sensitivity capable of\ndetecting low-abundance targets as low as 90.32 copies/{\\mu}L. Furthermore, it\nimproves model's transparency through detailed explanation and troubleshooting\nguidance, empowering users to make informed decisions. This innovative\nframework has the potential to benefit molecular diagnostics, disease research,\nand clinical applications, especially in resource-constrained settings.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.09218v1",
    "published_date": "2025-01-16 00:33:17 UTC",
    "updated_date": "2025-01-16 00:33:17 UTC"
  },
  {
    "arxiv_id": "2501.09217v1",
    "title": "Adaptive Law-Based Transformation (ALT): A Lightweight Feature Representation for Time Series Classification",
    "authors": [
      "Marcell T. Kurbucz",
      "Balázs Hajós",
      "Balázs P. Halmos",
      "Vince Á. Molnár",
      "Antal Jakovác"
    ],
    "abstract": "Time series classification (TSC) is fundamental in numerous domains,\nincluding finance, healthcare, and environmental monitoring. However,\ntraditional TSC methods often struggle with the inherent complexity and\nvariability of time series data. Building on our previous work with the linear\nlaw-based transformation (LLT) - which improved classification accuracy by\ntransforming the feature space based on key data patterns - we introduce\nadaptive law-based transformation (ALT). ALT enhances LLT by incorporating\nvariable-length shifted time windows, enabling it to capture distinguishing\npatterns of various lengths and thereby handle complex time series more\neffectively. By mapping features into a linearly separable space, ALT provides\na fast, robust, and transparent solution that achieves state-of-the-art\nperformance with only a few hyperparameters.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML",
      "62H30, 68T10, 62M10",
      "I.5; I.2.0; G.3"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 1 figure, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.09217v1",
    "published_date": "2025-01-16 00:33:01 UTC",
    "updated_date": "2025-01-16 00:33:01 UTC"
  }
]
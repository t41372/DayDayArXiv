{
  "date": "2025-07-17",
  "category": "cs.AI",
  "summary": "你好！欢迎来到 UTC 时间 2025-07-17 的 arXiv 中文 TLDR 快报！\n\n**今日总结**：\n今天的 arXiv 列表可谓重磅炸弹频出。最引人注目的当属 **Apple Intelligence 终于发布了其基础模型的详细技术报告**，揭示了其端侧与云端模型的架构细节。此外，**推理（Reasoning）能力**的评估与增强是今天的绝对核心，多篇论文指出即便是 o3 这样的顶尖模型在复杂的逻辑/图论问题上依然表现挣扎（<1% 准确率）。同时，**个性化 RLHF**、**视频生成的物理规律遵循**以及**无需训练的推理能力激发**也是今日值得关注的亮点。\n\n---\n\n### 🚀 头条关注：Apple Intelligence 技术解密\n\n**2. Apple Intelligence 基础语言模型：2025 技术报告**\n**(Apple Intelligence Foundation Language Models: Tech Report 2025)**\n*   **核心内容**：Apple 官方发布的技术报告。介绍了支持 Apple Intelligence 的两个关键多模态模型：\n    1.  **端侧模型（3B参数）**：针对 Apple 芯片优化，使用了 KV-cache 共享和 2-bit 量化感知训练。\n    2.  **服务器模型**：基于一种新颖的 **Parallel-Track Mixture-of-Experts (PT-MoE)** 架构，结合了轨道并行和交错的全局-局部注意力机制。\n*   **亮点**：使用了负责任的网页爬取数据、授权语料库和高质量合成数据。引入了新的异步强化学习平台。在人类评估中，两款模型均匹敌或超越了同等规模的开源基线。\n\n---\n\n### 🧠 LLM 训练、对齐与个性化\n\n**1. 学习总结用户信息以实现来自人类反馈的个性化强化学习**\n**(Learning to summarize user information for personalized reinforcement learning from human feedback)**\n*   **痛点**：传统的 RLHF 假设所有用户偏好一致，用单一奖励模型（Reward Model）“一刀切”。\n*   **方法**：提出了 **PLUS (Preference Learning Using Summarization)** 框架。它包含一个“用户总结模型”，能从历史对话中生成用户偏好摘要；这些摘要作为条件输入给奖励模型，使其能进行个性化预测。\n*   **效果**：相比标准 Bradley-Terry 模型，奖励模型准确率提升 11-77%。在 GPT-4 上的胜率从 28% 提升至 72%。\n\n**95. 策展数据上的监督微调就是强化学习（且可以被改进）**\n**(Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved))**\n*   **观点**：从理论上阐明，在筛选过的高质量数据上进行行为克隆（SFT），本质上是在稀疏奖励设置下最大化 RL 目标的下界。\n*   **改进**：提出了一种 **重要性加权 SFT (iw-SFT)**，通过简单的加权，使其表现更接近 RL 训练，且无需复杂的 RL 基础设施。\n\n**3. 思维的改变：自适应测试时计算**\n**(Change of Thought: Adaptive Test-Time Computation)**\n*   **新架构**：提出了 **SELF-Transformer**。不同于传统的自回归（生成 token），它在 Encoder 层内部迭代优化注意力权重直到收敛（Fixed Point）。\n*   **意义**：这模拟了人类在开口说话前“在脑中思考”的过程。在不增加参数量的情况下，通过增加推理时的计算量（Test-time computation），在 Encoder 风格的基准测试中提升了 20% 的准确率。\n\n---\n\n### 🧩 深度推理与逻辑基准 (Reasoning & Math)\n\n**28. FormulaOne：衡量超越竞技编程的算法推理深度**\n**(FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming)**\n*   **挑战**：现有的代码/数学基准对顶尖模型来说太简单或已被污染。\n*   **新基准**：**FormulaOne**。基于图论上的单子二阶逻辑（MSO logic），涉及复杂的推理步骤，通过形式化方法生成。\n*   **发现**：**OpenAI 的 o3 模型全军覆没，解决率不到 1%**。这表明目前的推理模型距离真正的专家级算法理解（如处理 SETH 猜想相关问题）仍有巨大差距。\n\n**122. Logit 算术无需训练即可激发长推理能力**\n**(Logit Arithmetic Elicits Long Reasoning Capabilities Without Training)**\n*   **方法**：提出了 **ThinkLogit**。这是一种解码时的方法，利用 Logit 算术，用一个极小的模型（如 1.5B）作为引导者，去调整大模型的输出分布。\n*   **效果**：无需对大模型进行额外训练，仅靠 Logit 算术就能激发其长链推理（Long CoT）能力，在数学数据集上 Pass@1 相对提升 26%。\n\n**91. VAR-MATH：通过符号多实例基准探测 LLM 的真实数学推理**\n**(VAR-MATH: Probing True Mathematical Reasoning in LLMS via Symbolic Multi-Instance Benchmarks)**\n*   **质疑**：模型分数的提升是因为真的会推理，还是背住了题库？\n*   **方法**：将 AMC/AIME 等数学题转化为**参数化模版**，生成多个变体进行测试。\n*   **打脸时刻**：在这些变体上，RL 训练出来的模型性能大幅下降（AIME25 上下降了 72.9%！），说明很多所谓的“推理能力”其实是过拟合或死记硬背。\n\n---\n\n### 🎬 视频生成、多模态与视觉\n\n**27. \"PhyWorldBench\"：文本到视频模型物理真实性的综合评估**\n**( \"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models)**\n*   **痛点**：视频生成模型（Sora, Gen-3等）画质很好，但物理规律经常错乱。\n*   **测评**：建立了包含 1050 个提示词的基准，涵盖基础物理、刚体交互、流体等，甚至包含“反物理”测试（看模型是否听话）。\n*   **结论**：目前的 SOTA 模型在遵循现实物理定律方面仍面临严峻挑战。\n\n**23. VideoITG：通过指令式时间定位进行多模态视频理解**\n**(VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding)**\n*   **贡献**：针对长视频理解，提出了 **VideoITG**。核心是一个名为 VidThinker 的管道，模拟人类标注过程：先生成片段描述，再进行推理检索，最后精细化选择关键帧。发布了 VideoITG-40K 数据集。\n\n**40. Voxtral**\n**(Voxtral)**\n*   **模型**：发布了 Voxtral Mini 和 Small 两个多模态音频聊天模型。\n*   **能力**：拥有 32K 上下文窗口，能处理 40 分钟长的音频，支持多轮对话。在理解语音和文本方面表现强劲，且小模型版本超越了部分闭源模型。\n\n---\n\n### 🛡️ 安全、偏见与社会影响\n\n**52. 提示注入 2.0：混合 AI 威胁**\n**(Prompt Injection 2.0: Hybrid AI Threats)**\n*   **警报**：随着 Agentic AI（智能体）的兴起，单纯的提示注入正在演变为混合威胁。\n*   **机制**：攻击者将 Prompt Injection 与传统的网络攻击（如 XSS, CSRF）结合。传统的防火墙和过滤器在面对 AI 增强的攻击时几乎失效。\n\n**61. 语言模型会根据你的说话方式改变事实**\n**(Language Models Change Facts Based on the Way You Talk)**\n*   **发现**：LLM 对用户的语言风格极其敏感（进而推断用户的种族、性别、年龄）。\n*   **后果**：在医疗建议中，模型会对不同种族特征的提问给出不同的护理标准；在政治问题上，面对老年口吻和年轻口吻，模型会分别迎合保守派或自由派的观点。这揭示了深层的隐性偏见风险。\n\n---\n\n### 🤖 机器人与具身智能\n\n**89. Vidar：用于通用操作的具身视频扩散模型**\n**(Vidar: Embodied Video Diffusion Model for Generalist Manipulation)**\n*   **思路**：利用互联网规模的视频预训练扩散模型作为先验，结合一个“掩码逆动力学模型（MIDM）”作为适配器。\n*   **数据**：使用了 750K 机器人多视角轨迹进行持续预训练。\n*   **效果**：仅需 20 分钟的人类演示（1%的数据量），就能在未见过的机器人和任务上泛化。\n\n**26. 利用与具体实施无关的预训练世界模型进行潜在策略引导**\n**(Latent Policy Steering with Embodiment-Agnostic Pretrained World Models)**\n*   **方法**：使用光流（Optic Flow）作为动作表示来训练跨机器人的通用世界模型。\n*   **策略**：**Latent Policy Steering (LPS)** 在世界模型的潜在空间中搜索更好的动作序列来引导策略，大幅减少了对真实机器人数据的需求。\n\n---\n\n### 🏺 趣味与跨学科应用\n\n**56. AI 向后瞄准：美索不达米亚消失的考古景观与 CORONA 图像上的遗址自动检测**\n**(AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery)**\n*   **应用**：利用 1960 年代的 CORONA 卫星谍照（灰度图）训练 AI，在美索不达米亚平原寻找考古遗址。\n*   **成果**：不仅 IoU 达到 85%，还**新发现了 4 个考古学家此前未注意到的遗址**（并经实地验证确认）。这是 AI 考古的一次精彩胜利。\n\n**5. 为什么关系学习还没有统治世界？**\n**(Why Isn't Relational Learning Taking Over the World?)**\n*   **思考**：一篇 AAAI-2026 预定的观点文章。作者 David Poole 探讨了为什么现在的 AI 沉迷于像素和单词（Pixel/Word），而忽视了世界本质是由实体和关系（Entities & Relations）组成的。文章分析了关系学习（Relational Learning）未成主流的原因及未来方向。",
  "papers": [
    {
      "arxiv_id": "2507.13579v2",
      "title": "Learning to summarize user information for personalized reinforcement learning from human feedback",
      "title_zh": "学习总结用户信息以实现个性化人类反馈强化学习",
      "authors": [
        "Hyunji Nam",
        "Yanming Wan",
        "Mickel Liu",
        "Jianxun Lian",
        "Peter Ahnn",
        "Natasha Jaques"
      ],
      "abstract": "As everyday use cases of large language model (LLM) AI assistants have expanded, it is becoming increasingly important to personalize responses to align to different users' preferences and goals. While reinforcement learning from human feedback (RLHF) is effective at improving LLMs to be generally more helpful and fluent, it does not account for variability across users, as it models the entire user population with a single reward model, meaning it assumes that everyone's preferences are the same. We present a novel framework, Preference Learning Using Summarization (PLUS), that uses reinforcement learning (RL) to learn to produce text-based summaries of each user's preferences, characteristics, and past conversations. These summaries condition the reward model, enabling it to make personalized predictions about the types of responses valued by each user. Both the user-summarization model and reward model are trained simultaneously, creating an online co-adaptation loop. We show that in contrast to the standard Bradley-Terry model, summaries produced by PLUS capture diverse aspects of user preferences, achieving a 11-77% improvement in reward model accuracy. Key strengths of PLUS are: (1) robust performance with new users and conversation topics, achieving a 25% improvement over the best personalized RLHF technique; (2) zero-shot personalization with state-of-the-art proprietary models like GPT-4 (e.g., PLUS-summary-conditioned responses achieved a 72% win rate compared to 28% for default GPT-4o); (3) learning from flexible user contexts beyond preference labels, and (4) interpretable representation of users, enabling greater transparency and user control in pluralistic LLM alignment.",
      "tldr_zh": "该研究针对大规模语言模型(LLMs)在基于人类反馈的强化学习(RLHF)中通常假设所有用户偏好一致、忽略个体差异的问题，提出了名为Preference Learning Using Summarization (PLUS)的创新框架。PLUS框架利用强化学习(RL)自动生成关于用户偏好、特征及历史对话的文本摘要，并以此调节奖励模型(Reward Model)，使其能够针对不同用户的回复偏好进行个性化预测。该系统通过同时训练用户摘要模型和奖励模型，形成了一个在线协同适应(Online Co-adaptation)循环。实验结果表明，PLUS在捕捉用户偏好多样性方面优于传统的Bradley-Terry模型，使奖励模型准确度提升了11-77%。此外，在面对新用户和新话题时，该框架性能比现有最优技术提升了25%，且在与GPT-4o的零样本(Zero-shot)个性化对比中取得了72%的胜率。PLUS不仅支持从灵活的用户上下文中学习，还提供了可解释的用户表征，为实现多元化LLM对齐中的透明度与用户控制力提供了有效方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages for main text, 9 pages for appendix",
      "pdf_url": "https://arxiv.org/pdf/2507.13579v2",
      "published_date": "2025-07-17 23:48:51 UTC",
      "updated_date": "2025-09-26 20:32:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:12:25.792387+00:00"
    },
    {
      "arxiv_id": "2507.13575v3",
      "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
      "title_zh": "Apple Intelligence 基础语言模型：2025 技术报告",
      "authors": [
        "Ethan Li",
        "Anders Boesen Lindbo Larsen",
        "Chen Zhang",
        "Xiyou Zhou",
        "Jun Qin",
        "Dian Ang Yap",
        "Narendran Raghavan",
        "Xuankai Chang",
        "Margit Bowler",
        "Eray Yildiz",
        "John Peebles",
        "Hannah Gillis Coleman",
        "Matteo Ronchi",
        "Peter Gray",
        "Keen You",
        "Anthony Spalvieri-Kruse",
        "Ruoming Pang",
        "Reed Li",
        "Yuli Yang",
        "Emad Soroush",
        "Zhiyun Lu",
        "Crystal Xiao",
        "Rong Situ",
        "Jordan Huffaker",
        "David Griffiths",
        "Zaid Ahmed",
        "Peng Zhang",
        "Daniel Parilla",
        "Asaf Liberman",
        "Jennifer Mallalieu",
        "Parsa Mazaheri",
        "Qibin Chen",
        "Manjot Bilkhu",
        "Aonan Zhang",
        "Eric Wang",
        "Dave Nelson",
        "Michael FitzMaurice",
        "Thomas Voice",
        "Jeremy Liu",
        "Josh Shaffer",
        "Shiwen Zhao",
        "Prasanth Yadla",
        "Farzin Rasteh",
        "Pengsheng Guo",
        "Arsalan Farooq",
        "Jeremy Snow",
        "Stephen Murphy",
        "Tao Lei",
        "Minsik Cho",
        "George Horrell",
        "Sam Dodge",
        "Lindsay Hislop",
        "Sumeet Singh",
        "Alex Dombrowski",
        "Aiswarya Raghavan",
        "Sasha Sirovica",
        "Mandana Saebi",
        "Faye Lao",
        "Max Lam",
        "TJ Lu",
        "Zhaoyang Xu",
        "Karanjeet Singh",
        "Marc Kirchner",
        "David Mizrahi",
        "Rajat Arora",
        "Haotian Zhang",
        "Henry Mason",
        "Lawrence Zhou",
        "Yi Hua",
        "Ankur Jain",
        "Felix Bai",
        "Joseph Astrauskas",
        "Floris Weers",
        "Josh Gardner",
        "Mira Chiang",
        "Yi Zhang",
        "Pulkit Agrawal",
        "Tony Sun",
        "Quentin Keunebroek",
        "Matthew Hopkins",
        "Bugu Wu",
        "Tao Jia",
        "Chen Chen",
        "Xingyu Zhou",
        "Nanzhu Wang",
        "Peng Liu",
        "Ruixuan Hou",
        "Rene Rauch",
        "Yuan Gao",
        "Afshin Dehghan",
        "Jonathan Janke",
        "Zirui Wang",
        "Cha Chen",
        "Xiaoyi Ren",
        "Feng Nan",
        "Josh Elman",
        "Dong Yin",
        "Yusuf Goren",
        "Jeff Lai",
        "Yiran Fei",
        "Syd Evans",
        "Muyang Yu",
        "Guoli Yin",
        "Yi Qin",
        "Erin Feldman",
        "Isha Garg",
        "Aparna Rajamani",
        "Karla Vega",
        "Walker Cheng",
        "TJ Collins",
        "Hans Han",
        "Raul Rea Menacho",
        "Simon Yeung",
        "Sophy Lee",
        "Phani Mutyala",
        "Ying-Chang Cheng",
        "Zhe Gan",
        "Sprite Chu",
        "Justin Lazarow",
        "Alessandro Pappalardo",
        "Federico Scozzafava",
        "Jing Lu",
        "Erik Daxberger",
        "Laurent Duchesne",
        "Jen Liu",
        "David Güera",
        "Stefano Ligas",
        "Mary Beth Kery",
        "Brent Ramerth",
        "Ciro Sannino",
        "Marcin Eichner",
        "Haoshuo Huang",
        "Rui Qian",
        "Moritz Schwarzer-Becker",
        "David Riazati",
        "Mingfei Gao",
        "Bailin Wang",
        "Jack Cackler",
        "Yang Lu",
        "Ransen Niu",
        "John Dennison",
        "Guillaume Klein",
        "Jeffrey Bigham",
        "Deepak Gopinath",
        "Navid Shiee",
        "Darren Botten",
        "Guillaume Tartavel",
        "Alex Guillen Garcia",
        "Sam Xu",
        "Victoria MönchJuan Haladjian",
        "Zi-Yi Dou",
        "Matthias Paulik",
        "Adolfo Lopez Mendez",
        "Zhen Li",
        "Hong-You Chen",
        "Chao Jia",
        "Dhaval Doshi",
        "Zhengdong Zhang",
        "Raunak Manjani",
        "Aaron Franklin",
        "Zhile Ren",
        "David Chen",
        "Artsiom Peshko",
        "Nandhitha Raghuram",
        "Hans Hao",
        "Jiulong Shan",
        "Kavya Nerella",
        "Ramsey Tantawi",
        "Vivek Kumar",
        "Saiwen Wang",
        "Brycen Wershing",
        "Bhuwan Dhingra",
        "Dhruti Shah",
        "Ob Adaranijo",
        "Xin Zheng",
        "Tait Madsen",
        "Hadas Kotek",
        "Chang Liu",
        "Yin Xia",
        "Hanli Li",
        "Suma Jayaram",
        "Yanchao Sun",
        "Ahmed Fakhry",
        "Vasileios Saveris",
        "Dustin Withers",
        "Yanghao Li",
        "Alp Aygar",
        "Andres Romero Mier Y Teran",
        "Kaiwei Huang",
        "Mark Lee",
        "Xiujun Li",
        "Yuhong Li",
        "Tyler Johnson",
        "Jay Tang",
        "Joseph Yitan Cheng",
        "Futang Peng",
        "Andrew Walkingshaw",
        "Lucas Guibert",
        "Abhishek Sharma",
        "Cheng Shen",
        "Piotr Maj",
        "Yasutaka Tanaka",
        "You-Cyuan Jhang",
        "Vivian Ma",
        "Tommi Vehvilainen",
        "Kelvin Zou",
        "Jeff Nichols",
        "Matthew Lei",
        "David Qiu",
        "Yihao Qian",
        "Gokul Santhanam",
        "Wentao Wu",
        "Yena Han",
        "Dominik Moritz",
        "Haijing Fu",
        "Mingze Xu",
        "Vivek Rathod",
        "Jian Liu",
        "Louis D'hauwe",
        "Qin Ba",
        "Haitian Sun",
        "Haoran Yan",
        "Philipp Dufter",
        "Anh Nguyen",
        "Yihao Feng",
        "Emma Wang",
        "Keyu He",
        "Rahul Nair",
        "Sanskruti Shah",
        "Jiarui Lu",
        "Patrick Sonnenberg",
        "Jeremy Warner",
        "Yuanzhi Li",
        "Bowen Pan",
        "Ziyi Zhong",
        "Joe Zhou",
        "Sam Davarnia",
        "Olli Saarikivi",
        "Irina Belousova",
        "Rachel Burger",
        "Shang-Chen Wu",
        "Di Feng",
        "Bas Straathof",
        "James Chou",
        "Yuanyang Zhang",
        "Marco Zuliani",
        "Eduardo Jimenez",
        "Abhishek Sundararajan",
        "Xianzhi Du",
        "Chang Lan",
        "Nilesh Shahdadpuri",
        "Peter Grasch",
        "Sergiu Sima",
        "Josh Newnham",
        "Varsha Paidi",
        "Jianyu Wang",
        "Kaelen Haag",
        "Alex Braunstein",
        "Daniele Molinari",
        "Richard Wei",
        "Brenda Yang",
        "Nicholas Lusskin",
        "Joanna Arreaza-Taylor",
        "Meng Cao",
        "Nicholas Seidl",
        "Simon Wang",
        "Jiaming Hu",
        "Yiping Ma",
        "Mengyu Li",
        "Kieran Liu",
        "Hang Su",
        "Sachin Ravi",
        "Chong Wang",
        "Xin Wang",
        "Kevin Smith",
        "Haoxuan You",
        "Binazir Karimzadeh",
        "Rui Li",
        "Jinhao Lei",
        "Wei Fang",
        "Alec Doane",
        "Sam Wiseman",
        "Ismael Fernandez",
        "Jane Li",
        "Andrew Hansen",
        "Javier Movellan",
        "Christopher Neubauer",
        "Hanzhi Zhou",
        "Chris Chaney",
        "Nazir Kamaldin",
        "Valentin Wolf",
        "Fernando Bermúdez-Medina",
        "Joris Pelemans",
        "Peter Fu",
        "Howard Xing",
        "Xiang Kong",
        "Wayne Shan",
        "Gabriel Jacoby-Cooper",
        "Dongcai Shen",
        "Tom Gunter",
        "Guillaume Seguin",
        "Fangping Shi",
        "Shiyu Li",
        "Yang Xu",
        "Areeba Kamal",
        "Dan Masi",
        "Saptarshi Guha",
        "Qi Zhu",
        "Jenna Thibodeau",
        "Changyuan Zhang",
        "Rebecca Callahan",
        "Charles Maalouf",
        "Wilson Tsao",
        "Boyue Li",
        "Qingqing Cao",
        "Naomy Sabo",
        "Cheng Leong",
        "Yi Wang",
        "Anupama Mann Anupama",
        "Colorado Reed",
        "Kenneth Jung",
        "Zhifeng Chen",
        "Mohana Prasad Sathya Moorthy",
        "Yifei He",
        "Erik Hornberger",
        "Devi Krishna",
        "Senyu Tong",
        "Michael",
        "Lee",
        "David Haldimann",
        "Yang Zhao",
        "Bowen Zhang",
        "Chang Gao",
        "Chris Bartels",
        "Sushma Rao",
        "Nathalie Tran",
        "Simon Lehnerer",
        "Co Giang",
        "Patrick Dong",
        "Junting Pan",
        "Biyao Wang",
        "Dongxu Li",
        "Mehrdad Farajtabar",
        "Dongseong Hwang",
        "Grace Duanmu",
        "Eshan Verma",
        "Sujeeth Reddy",
        "Qi Shan",
        "Hongbin Gao",
        "Nan Du",
        "Pragnya Sridhar",
        "Forrest Huang",
        "Yingbo Wang",
        "Nikhil Bhendawade",
        "Diane Zhu",
        "Sai Aitharaju",
        "Fred Hohman",
        "Lauren Gardiner",
        "Chung-Cheng Chiu",
        "Yinfei Yang",
        "Alper Kokmen",
        "Frank Chu",
        "Ke Ye",
        "Kaan Elgin",
        "Oron Levy",
        "John Park",
        "Donald Zhang",
        "Eldon Schoop",
        "Nina Wenzel",
        "Michael Booker",
        "Hyunjik Kim",
        "Chinguun Erdenebileg",
        "Nan Dun",
        "Eric Liang Yang",
        "Priyal Chhatrapati",
        "Vishaal Mahtani",
        "Haiming Gang",
        "Kohen Chia",
        "Deepa Seshadri",
        "Donghan Yu",
        "Yan Meng",
        "Kelsey Peterson",
        "Zhen Yang",
        "Yongqiang Wang",
        "Carina Peng",
        "Doug Kang",
        "Anuva Agarwal",
        "Albert Antony",
        "Juan Lao Tebar",
        "Albin Madappally Jose",
        "Regan Poston",
        "Andy De Wang",
        "Gerard Casamayor",
        "Elmira Amirloo",
        "Violet Yao",
        "Wojciech Kryscinski",
        "Kun Duan",
        "Lezhi L"
      ],
      "abstract": "We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.",
      "tldr_zh": "该研究介绍了苹果公司推出的两款多语言、多模态基础语言模型，包括针对 Apple silicon 优化的 3B 参数端侧模型，以及基于 PT-MoE (Parallel-Track Mixture-of-Experts) 架构的扩展型服务器模型。端侧模型通过 KV-cache 共享和 2-bit 量化感知训练 (quantization-aware training) 等技术创新提升了效率，而服务器模型则结合轨道并行 (track parallelism) 与交织的全局-局部注意力机制 (interleaved global-local attention) 实现了高性能与成本的平衡。这些模型在海量多模态数据集上进行训练，并采用 SFT 和强化学习 (RL) 进行精调，支持图像理解、工具调用 (tool calls) 和多语言交互。实验结果表明，这两款模型在公开基准测试和人工评估中均达到或超过了同类开源基准水平。此外，该报告还发布了以 Swift 为核心的开发框架，支持 LoRA 适配器微调，并强调了通过私有云计算 (Private Cloud Compute) 实现的隐私保护与负责任 AI (Responsible AI) 措施。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13575v3",
      "published_date": "2025-07-17 23:37:19 UTC",
      "updated_date": "2025-08-27 16:34:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:12:30.185781+00:00"
    },
    {
      "arxiv_id": "2507.13569v1",
      "title": "Change of Thought: Adaptive Test-Time Computation",
      "title_zh": "思维之变：自适应测试时计算",
      "authors": [
        "Mrinal Mathur",
        "Mike Doan",
        "Barak Pearlmutter",
        "Sergey Plis"
      ],
      "abstract": "Transformers evaluated in a single, fixed-depth pass are provably limited in expressive power to the constant-depth circuit class TC0. Running a Transformer autoregressively removes that ceiling -- first in next-token prediction and, more recently, in chain-of-thought reasoning. Both regimes rely on feedback loops that decode internal states into tokens only to re-encode them in subsequent steps. While this \"thinking aloud\" mirrors human reasoning, biological brains iterate without externalising intermediate states as language. To boost the expressive power of encoder Transformers without resorting to token-level autoregression, we introduce the SELF-Transformer: an encoder layer that iteratively refines its own attention weights to a fixed point. Instead of producing -- in one pass -- the alignment matrix that remixes the input sequence, the SELF-Transformer iteratively updates that matrix internally, scaling test-time computation with input difficulty. This adaptivity yields up to 20\\% accuracy gains on encoder-style benchmarks without increasing parameter count, demonstrating that input-adaptive alignment at test time offers substantial benefits for only a modest extra compute budget. Self-Transformers thus recover much of the expressive power of iterative reasoning while preserving the simplicity of pure encoder architectures.",
      "tldr_zh": "该研究针对固定深度 Transformer 在表达能力上受限于 $TC^0$ 类别的问题，提出了一种名为 SELF-Transformer 的新型架构，旨在不依赖 token 级别 autoregression 的情况下增强 encoder 的推理能力。与传统的单次前向传播不同，SELF-Transformer 的 encoder 层通过内部迭代将自身的注意力权重细化至不动点(fixed point)，从而实现对对齐矩阵(alignment matrix)的动态更新。这种机制允许模型根据输入难度自适应地扩展测试时计算量(test-time computation)，模拟了生物大脑无需外部语言化即可进行内部迭代推理的过程。实验表明，在不增加参数量的情况下，该方法在 encoder 风格的基准测试中实现了高达 20% 的准确率提升。研究证明了测试时的输入自适应对齐(input-adaptive alignment)能以极低的额外计算预算显著增强模型表达能力，成功在纯 encoder 架构中回收了 Chain-of-Thought 推理的核心优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13569v1",
      "published_date": "2025-07-17 23:12:57 UTC",
      "updated_date": "2025-07-17 23:12:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:12:37.286194+00:00"
    },
    {
      "arxiv_id": "2507.19513v1",
      "title": "Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting",
      "title_zh": "利用 xLSTM 增强时空网络：面向蜂窝流量预测的标量 LSTM 方法",
      "authors": [
        "Khalid Ali",
        "Zineddine Bettouche",
        "Andreas Kassler",
        "Andreas Fischer"
      ],
      "abstract": "Accurate spatiotemporal traffic forecasting is vital for intelligent resource management in 5G and beyond. However, conventional AI approaches often fail to capture the intricate spatial and temporal patterns that exist, due to e.g., the mobility of users. We introduce a lightweight, dual-path Spatiotemporal Network that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling and a three-layer Conv3D module for spatial feature extraction. A fusion layer integrates both streams into a cohesive representation, enabling robust forecasting. Our design improves gradient stability and convergence speed while reducing prediction error. Evaluations on real-world datasets show superior forecast performance over ConvLSTM baselines and strong generalization to unseen regions, making it well-suited for large-scale, next-generation network deployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM, with a 30% improvement in model generalization.",
      "tldr_zh": "该研究针对5G及未来移动网络中智能资源管理的挑战，提出了一种增强型轻量级双路径时空网络，旨在解决传统AI难以捕捉复杂移动通信流量模式的问题。该方案核心采用 xLSTM 家族中的 Scalar LSTM (sLSTM) 进行高效的时间建模，并结合三层 Conv3D 模块提取空间特征，最后通过 Fusion layer 实现时空特征的深度融合。实验证明，该设计能显著提升梯度的稳定性和模型的收敛速度，有效降低预测误差。在真实世界数据集的评估中，该模型相比 ConvLSTM 基线在 MAE 指标上降低了23%，且在未见区域的 Generalization 能力提升了30%。这项工作展现了 sLSTM 在大规模、下一代网络部署中的卓越性能，为实现鲁棒且精准的蜂窝流量预测提供了关键技术支撑。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.19513v1",
      "published_date": "2025-07-17 22:48:46 UTC",
      "updated_date": "2025-07-17 22:48:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:12:36.595405+00:00"
    },
    {
      "arxiv_id": "2507.13558v5",
      "title": "Why Isn't Relational Learning Taking Over the World?",
      "title_zh": "关系学习为何未能主导世界？",
      "authors": [
        "David Poole"
      ],
      "abstract": "Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.",
      "tldr_zh": "该论文探讨了为何在人工智能领域，建模像素和单词的系统占据主导，而专注于实体、属性及其相互关系的 Relational Learning（关系学习）尚未取得应有的核心地位。尽管企业中最具价值的数据通常存在于包含大量产品编号、交易流水等非数值标识符的关系型数据库中，但这类数据在入门级机器学习中往往被忽视。文章分析了 Relational Learning 或 Statistical Relational AI 在主流化过程中面临的挑战，指出除了少数受限的场景外，其广泛应用仍受到限制。通过对当前现状的深度剖析，作者阐明了该领域未能“统治世界”的原因，并提出了将其推向学术与应用前沿所需的必要步骤，旨在促使 AI 系统从单纯的感知建模转向对现实世界实体及其关系的直接理解。",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages (6 pages + references + appendices). To appear AAAI-2026",
      "pdf_url": "https://arxiv.org/pdf/2507.13558v5",
      "published_date": "2025-07-17 22:32:07 UTC",
      "updated_date": "2025-11-05 17:21:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:12:40.195914+00:00"
    },
    {
      "arxiv_id": "2507.13556v1",
      "title": "Time Series Forecastability Measures",
      "title_zh": "时间序列可预测性度量",
      "authors": [
        "Rui Wang",
        "Steven Klee",
        "Alexis Roos"
      ],
      "abstract": "This paper proposes using two metrics to quantify the forecastability of time series prior to model development: the spectral predictability score and the largest Lyapunov exponent. Unlike traditional model evaluation metrics, these measures assess the inherent forecastability characteristics of the data before any forecast attempts. The spectral predictability score evaluates the strength and regularity of frequency components in the time series, whereas the Lyapunov exponents quantify the chaos and stability of the system generating the data. We evaluated the effectiveness of these metrics on both synthetic and real-world time series from the M5 forecast competition dataset. Our results demonstrate that these two metrics can correctly reflect the inherent forecastability of a time series and have a strong correlation with the actual forecast performance of various models. By understanding the inherent forecastability of time series before model training, practitioners can focus their planning efforts on products and supply chain levels that are more forecastable, while setting appropriate expectations or seeking alternative strategies for products with limited forecastability.",
      "tldr_zh": "该研究提出了谱预测得分(spectral predictability score)和最大李雅普诺夫指数(largest Lyapunov exponent)两种指标，用于在模型开发前量化时间序列的固有可预测性(forecastability)。不同于传统的模型评估指标，这些测度侧重于在预测尝试前评估数据本身的特征，其中谱预测得分评估频率分量的规律性，而李雅普诺夫指数则衡量系统的混沌程度与稳定性。通过在合成数据和M5预测竞赛数据集上的实验，研究证实这些指标能准确反映数据的预测难度，并与多种预测模型的实际表现高度相关。该方法使从业者能够在模型训练前识别高可预测性的产品或供应链环节，从而更有效地分配规划资源。同时，这也有助于对可预测性有限的对象设定合理预期或制定替代策略。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13556v1",
      "published_date": "2025-07-17 22:23:51 UTC",
      "updated_date": "2025-07-17 22:23:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:12:48.988039+00:00"
    },
    {
      "arxiv_id": "2507.13551v1",
      "title": "Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder",
      "title_zh": "言外之意：结合停顿动态与语义连贯性的思维障碍自动评估",
      "authors": [
        "Feng Chen",
        "Weizhe Xu",
        "Changye Li",
        "Serguei Pakhomov",
        "Alex Cohen",
        "Simran Bhola",
        "Sandy Yin",
        "Sunny X Tang",
        "Michael Mackinley",
        "Lena Palaniyappan",
        "Dror Ben-Zeev",
        "Trevor Cohen"
      ],
      "abstract": "Formal thought disorder (FTD), a hallmark of schizophrenia spectrum disorders, manifests as incoherent speech and poses challenges for clinical assessment. Traditional clinical rating scales, though validated, are resource-intensive and lack scalability. Automated speech analysis with automatic speech recognition (ASR) allows for objective quantification of linguistic and temporal features of speech, offering scalable alternatives. The use of utterance timestamps in ASR captures pause dynamics, which are thought to reflect the cognitive processes underlying speech production. However, the utility of integrating these ASR-derived features for assessing FTD severity requires further evaluation. This study integrates pause features with semantic coherence metrics across three datasets: naturalistic self-recorded diaries (AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream narratives (PsyCL, n = 43). We evaluated pause related features alongside established coherence measures, using support vector regression (SVR) to predict clinical FTD scores. Key findings demonstrate that pause features alone robustly predict the severity of FTD. Integrating pause features with semantic coherence metrics enhanced predictive performance compared to semantic-only models, with integration of independent models achieving correlations up to \\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best \\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance gains from semantic and pause features integration held consistently across all contexts, though the nature of pause patterns was dataset-dependent. These findings suggest that frameworks combining temporal and semantic analyses provide a roadmap for refining the assessment of disorganized speech and advance automated speech analysis in psychosis.",
      "tldr_zh": "该研究探讨了结合停顿动态(Pause Dynamics)与语义连贯性(Semantic Coherence)对思维形式障碍(Formal Thought Disorder, FTD)进行自动化评估的方法。研究利用自动语音识别(ASR)技术从自然日记、图片描述及梦境叙述三个数据集中提取停顿特征与语义指标，并采用支持向量回归(SVR)预测临床得分。实验结果表明，停顿特征本身即可稳健地预测FTD严重程度，而将其与语义指标融合后，模型性能在所有语境下均得到显著增强，在严重病例检测中达到了0.649的相关系数与83.71%的AUC。这一发现证明了整合时间维度与语言维度的分析框架能有效提升对言语紊乱的量化精度，为精神病学领域的自动化语音分析提供了重要路径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13551v1",
      "published_date": "2025-07-17 22:00:16 UTC",
      "updated_date": "2025-07-17 22:00:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:12:48.095293+00:00"
    },
    {
      "arxiv_id": "2507.13550v1",
      "title": "GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models",
      "title_zh": "GOFAI 遇上生成式人工智能：基于大语言模型的专家系统开发",
      "authors": [
        "Eduardo C. Garrido-Merchán",
        "Cristina Puente"
      ],
      "abstract": "The development of large language models (LLMs) has successfully transformed knowledge-based systems such as open domain question nswering, which can automatically produce vast amounts of seemingly coherent information. Yet, those models have several disadvantages like hallucinations or confident generation of incorrect or unverifiable facts. In this paper, we introduce a new approach to the development of expert systems using LLMs in a controlled and transparent way. By limiting the domain and employing a well-structured prompt-based extraction approach, we produce a symbolic representation of knowledge in Prolog, which can be validated and corrected by human experts. This approach also guarantees interpretability, scalability and reliability of the developed expert systems. Via quantitative and qualitative experiments with Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic coherence on our generated knowledge bases. We present a transparent hybrid solution that combines the recall capacity of LLMs with the precision of symbolic systems, thereby laying the foundation for dependable AI applications in sensitive domains.",
      "tldr_zh": "该研究探讨了将传统人工智能(GOFAI)与生成式人工智能(Generative AI)相结合的新途径，旨在解决大语言模型(LLMs)在知识库系统中存在的幻觉及事实错误等问题。作者提出了一种受控且透明的专家系统开发方法，通过结构化的提示词提取(prompt-based extraction)技术将领域知识转化为Prolog形式的符号表示(symbolic representation)，从而允许人类专家进行验证与修正。这种方法确保了系统的可解释性(interpretability)、可扩展性(scalability)和可靠性(reliability)。通过在Claude Sonnet 3.7和GPT-4.1上进行的定量与定性实验，研究证明了该方案在事实遵循和语义连贯性上的卓越表现。该论文最终呈现了一种融合LLMs召回能力与符号系统精准度的混合解决方案，为敏感领域构建可信赖的人工智能应用奠定了基础。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13550v1",
      "published_date": "2025-07-17 21:57:37 UTC",
      "updated_date": "2025-07-17 21:57:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:12:54.890242+00:00"
    },
    {
      "arxiv_id": "2507.13543v4",
      "title": "Loss-Complexity Landscape and Model Structure Functions",
      "title_zh": "损失-复杂度景观与模型结构函数",
      "authors": [
        "Alexander Kolpakov"
      ],
      "abstract": "We develop a framework for dualizing the Kolmogorov structure function $h_x(α)$, which then allows using computable complexity proxies. We establish a mathematical analogy between information-theoretic constructs and statistical mechanics, introducing a suitable partition function and free energy functional. We explicitly prove the Legendre-Fenchel duality between the structure function and free energy, showing detailed balance of the Metropolis kernel, and interpret acceptance probabilities as information-theoretic scattering amplitudes. A susceptibility-like variance of model complexity is shown to peak precisely at loss-complexity trade-offs interpreted as phase transitions. Practical experiments with linear and tree-based regression models verify these theoretical predictions, explicitly demonstrating the interplay between the model complexity, generalization, and overfitting threshold.",
      "tldr_zh": "该研究建立了一个将 Kolmogorov structure function $h_x(\\alpha)$ 对偶化的框架，允许使用可计算的复杂度指标（computable complexity proxies）来评估模型。作者在信息论结构与统计力学（statistical mechanics）之间建立了数学类比，引入了相应的配分函数（partition function）和自由能泛函（free energy functional）。研究明确证明了结构函数与自由能之间的 Legendre-Fenchel duality，并展示了 Metropolis kernel 的细致平衡，将接受概率解释为信息论散射振幅（information-theoretic scattering amplitudes）。研究进一步发现一种类似于磁化率（susceptibility-like）的模型复杂度方差在损失与复杂度的权衡点达到峰值，并将这些点解释为相变（phase transitions）。通过线性回归和基于树的回归模型的实验，该研究验证了上述理论预测，明确展示了模型复杂度、泛化能力（generalization）与过拟合（overfitting）阈值之间的内在联系。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "math-ph"
      ],
      "primary_category": "cs.IT",
      "comment": "25 pages, 11 figures; GitHub repository at https://github.com/sashakolpakov/structure-functions",
      "pdf_url": "https://arxiv.org/pdf/2507.13543v4",
      "published_date": "2025-07-17 21:31:45 UTC",
      "updated_date": "2025-10-16 22:13:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:12:55.289480+00:00"
    },
    {
      "arxiv_id": "2507.13542v1",
      "title": "Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography",
      "title_zh": "声学指数：利用超声心动图进行心脏病风险分层的新型人工智能驱动指标",
      "authors": [
        "Beka Begiashvili",
        "Carlos J. Fernandez-Candel",
        "Matías Pérez Paredes"
      ],
      "abstract": "Traditional echocardiographic parameters such as ejection fraction (EF) and global longitudinal strain (GLS) have limitations in the early detection of cardiac dysfunction. EF often remains normal despite underlying pathology, and GLS is influenced by load conditions and vendor variability. There is a growing need for reproducible, interpretable, and operator-independent parameters that capture subtle and global cardiac functional alterations.\n  We introduce the Acoustic Index, a novel AI-derived echocardiographic parameter designed to quantify cardiac dysfunction from standard ultrasound views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on Koopman operator theory with a hybrid neural network that incorporates clinical metadata. Spatiotemporal dynamics are extracted from echocardiographic sequences to identify coherent motion patterns. These are weighted via attention mechanisms and fused with clinical data using manifold learning, resulting in a continuous score from 0 (low risk) to 1 (high risk).\n  In a prospective cohort of 736 patients, encompassing various cardiac pathologies and normal controls, the Acoustic Index achieved an area under the curve (AUC) of 0.89 in an independent test set. Cross-validation across five folds confirmed the robustness of the model, showing that both sensitivity and specificity exceeded 0.8 when evaluated on independent data. Threshold-based analysis demonstrated stable trade-offs between sensitivity and specificity, with optimal discrimination near this threshold.\n  The Acoustic Index represents a physics-informed, interpretable AI biomarker for cardiac function. It shows promise as a scalable, vendor-independent tool for early detection, triage, and longitudinal monitoring. Future directions include external validation, longitudinal studies, and adaptation to disease-specific classifiers.",
      "tldr_zh": "该研究引入了Acoustic Index，这是一种新型的AI驱动超声心动图参数，旨在解决Ejection Fraction (EF)和Global Longitudinal Strain (GLS)等传统参数在早期检测心脏功能障碍时的局限性。该模型结合了基于Koopman算子理论的Extended Dynamic Mode Decomposition (EDMD)与整合临床元数据的混合神经网络，用于从超声序列中提取时空动力学特征。通过注意力机制和流形学习(manifold learning)技术，该系统能够将提取的运动模式与临床数据融合，生成一个0到1之间的连续风险评分。在包含736名患者的前瞻性队列研究中，Acoustic Index在独立测试集中实现了0.89的AUC，且敏感性与特异性均超过0.8。实验结果证明，这一物理启发且具有可解释性的AI生物标志物，可作为一种不依赖设备厂商的评估工具，有效用于心脏疾病的早期发现、分诊及长期监测。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13542v1",
      "published_date": "2025-07-17 21:27:28 UTC",
      "updated_date": "2025-07-17 21:27:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:12:56.483378+00:00"
    },
    {
      "arxiv_id": "2507.13541v1",
      "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes",
      "title_zh": "PrefPalette：基于潜在属性的个性化偏好建模",
      "authors": [
        "Shuyue Stella Li",
        "Melanie Sclar",
        "Hunter Lang",
        "Ansong Ni",
        "Jacqueline He",
        "Puxin Xu",
        "Andrew Cohen",
        "Chan Young Park",
        "Yulia Tsvetkov",
        "Asli Celikyilmaz"
      ],
      "abstract": "Personalizing AI systems requires understanding not just what users prefer, but the reasons that underlie those preferences - yet current preference models typically treat human judgment as a black box. We introduce PrefPalette, a framework that decomposes preferences into attribute dimensions and tailors its preference prediction to distinct social community values in a human-interpretable manner. PrefPalette operationalizes a cognitive science principle known as multi-attribute decision making in two ways: (1) a scalable counterfactual attribute synthesis step that involves generating synthetic training data to isolate for individual attribute effects (e.g., formality, humor, cultural values), and (2) attention-based preference modeling that learns how different social communities dynamically weight these attributes. This approach moves beyond aggregate preference modeling to capture the diverse evaluation frameworks that drive human judgment. When evaluated on 45 social communities from the online platform Reddit, PrefPalette outperforms GPT-4o by 46.6% in average prediction accuracy. Beyond raw predictive improvements, PrefPalette also shed light on intuitive, community-specific profiles: scholarly communities prioritize verbosity and stimulation, conflict-oriented communities value sarcasm and directness, and support-based communities emphasize empathy. By modeling the attribute-mediated structure of human judgment, PrefPalette delivers both superior preference modeling and transparent, interpretable insights, and serves as a first step toward more trustworthy, value-aware personalized applications.",
      "tldr_zh": "该研究提出了 PrefPalette 框架，旨在通过将偏好分解为属性维度，以一种人类可解释的方式为不同的社会社区定制偏好预测。PrefPalette 采用了认知科学中的 multi-attribute decision making 原则，包含两个核心步骤：首先是通过 counterfactual attribute synthesis 生成合成训练数据，以隔离特定属性（如 formality、humor 或 cultural values）的影响；其次是利用 attention-based preference modeling 来学习不同社区如何动态加权这些属性。实验结果显示，在对 Reddit 平台的 45 个社交社区进行评估时，PrefPalette 的平均预测准确率比 GPT-4o 高出 46.6%。此外，该框架还能揭示特定社区的偏好特征，例如学术社区更看重 verbosity 和 stimulation，而支持性社区则强调 empathy。通过建模人类判断的属性介导结构，PrefPalette 不仅提升了偏好建模的性能，还为构建更可信、具备价值感知能力的个性化应用提供了透明且可解释的见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 6 tables, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.13541v1",
      "published_date": "2025-07-17 21:21:54 UTC",
      "updated_date": "2025-07-17 21:21:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:13:08.441519+00:00"
    },
    {
      "arxiv_id": "2507.14242v1",
      "title": "Culling Misinformation from Gen AI: Toward Ethical Curation and Refinement",
      "title_zh": "剔除生成式人工智能中的虚假信息：迈向伦理化策展与完善",
      "authors": [
        "Prerana Khatiwada",
        "Grace Donaher",
        "Jasymyn Navarro",
        "Lokesh Bhatta"
      ],
      "abstract": "While Artificial Intelligence (AI) is not a new field, recent developments, especially with the release of generative tools like ChatGPT, have brought it to the forefront of the minds of industry workers and academic folk alike. There is currently much talk about AI and its ability to reshape many everyday processes as we know them through automation. It also allows users to expand their ideas by suggesting things they may not have thought of on their own and provides easier access to information. However, not all of the changes this technology will bring or has brought so far are positive; this is why it is extremely important for all modern people to recognize and understand the risks before using these tools and allowing them to cause harm. This work takes a position on better understanding many equity concerns and the spread of misinformation that result from new AI, in this case, specifically ChatGPT and deepfakes, and encouraging collaboration with law enforcement, developers, and users to reduce harm. Considering many academic sources, it warns against these issues, analyzing their cause and impact in fields including healthcare, education, science, academia, retail, and finance. Lastly, we propose a set of future-facing guidelines and policy considerations to solve these issues while still enabling innovation in these fields, this responsibility falling upon users, developers, and government entities.",
      "tldr_zh": "该研究探讨了生成式人工智能(Generative AI)带来的误导信息(Misinformation)和公平性问题，重点分析了ChatGPT和深度伪造(Deepfakes)对社会构成的风险。通过参考大量学术文献，论文深入剖析了这些技术在医疗、教育、科学、零售及金融等领域产生负面影响的根本原因及其广泛冲击。文章强调，在享受自动化和信息获取便利的同时，公众必须警惕并理解这些工具可能引发的潜在危害。为此，作者呼吁执法部门、开发者与用户之间建立紧密协作，以共同遏制有害信息的传播。最后，研究提出了一套面向未来的准则(Guidelines)和政策考量，旨在通过用户、开发者和政府实体的共同担责，在推动技术创新的同时有效解决伦理治理挑战。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "7 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.14242v1",
      "published_date": "2025-07-17 21:19:47 UTC",
      "updated_date": "2025-07-17 21:19:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:13:12.436856+00:00"
    },
    {
      "arxiv_id": "2507.15878v1",
      "title": "Salience Adjustment for Context-Based Emotion Recognition",
      "title_zh": "面向基于语境的情绪识别的显著性调节",
      "authors": [
        "Bin Han",
        "Jonathan Gratch"
      ],
      "abstract": "Emotion recognition in dynamic social contexts requires an understanding of the complex interaction between facial expressions and situational cues. This paper presents a salience-adjusted framework for context-aware emotion recognition with Bayesian Cue Integration (BCI) and Visual-Language Models (VLMs) to dynamically weight facial and contextual information based on the expressivity of facial cues. We evaluate this approach using human annotations and automatic emotion recognition systems in prisoner's dilemma scenarios, which are designed to evoke emotional reactions. Our findings demonstrate that incorporating salience adjustment enhances emotion recognition performance, offering promising directions for future research to extend this framework to broader social contexts and multimodal applications.",
      "tldr_zh": "该研究探讨了动态社交语境下的情绪识别问题，旨在理解面部表情与情境线索之间的复杂交互。为了解决这一挑战，作者提出了一个显著性调整框架，利用 Bayesian Cue Integration (BCI) 和 Visual-Language Models (VLMs) 根据面部线索的表达程度动态权重化处理面部和上下文信息。研究团队在专门设计的囚徒困境(prisoner's dilemma)场景中，通过人工标注和自动情绪识别系统对该方法进行了评估。实验结果表明，引入显著性调整显著增强了情绪识别的性能。该研究为未来将此框架扩展到更广泛的社交语境和多模态(multimodal)应用提供了有力支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.15878v1",
      "published_date": "2025-07-17 20:55:20 UTC",
      "updated_date": "2025-07-17 20:55:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:13:17.191332+00:00"
    },
    {
      "arxiv_id": "2507.13524v1",
      "title": "Humans learn to prefer trustworthy AI over human partners",
      "title_zh": "人类逐渐偏好可信人工智能而非人类伙伴",
      "authors": [
        "Yaomin Jiang",
        "Levin Brinkmann",
        "Anne-Marie Nussberger",
        "Ivan Soraperra",
        "Jean-François Bonnefon",
        "Iyad Rahwan"
      ],
      "abstract": "Partner selection is crucial for cooperation and hinges on communication. As artificial agents, especially those powered by large language models (LLMs), become more autonomous, intelligent, and persuasive, they compete with humans for partnerships. Yet little is known about how humans select between human and AI partners and adapt under AI-induced competition pressure. We constructed a communication-based partner selection game and examined the dynamics in hybrid mini-societies of humans and bots powered by a state-of-the-art LLM. Through three experiments (N = 975), we found that bots, though more prosocial than humans and linguistically distinguishable, were not selected preferentially when their identity was hidden. Instead, humans misattributed bots' behaviour to humans and vice versa. Disclosing bots' identity induced a dual effect: it reduced bots' initial chances of being selected but allowed them to gradually outcompete humans by facilitating human learning about the behaviour of each partner type. These findings show how AI can reshape social interaction in mixed societies and inform the design of more effective and cooperative hybrid systems.",
      "tldr_zh": "该研究通过构建基于沟通的伴侣选择博弈(partner selection game)，探讨了人类在面对大语言模型(LLMs)驱动的智能体竞争时，如何进行伴侣选择及行为适应。实验在由人类和先进 LLM 机器人组成的混合小型社会(hybrid mini-societies)中进行，共涉及三项大规模实验。研究发现，当机器人身份被隐藏时，尽管其表现出更强的亲社会性(prosocial)，但人类并未优先选择机器人，甚至会产生行为归因错误。披露机器人身份后产生了一种双重效应(dual effect)：最初人类会降低对机器人的选择偏好，但随着人类逐渐学习并识别不同伴侣类型的行为模式，机器人最终在竞争中逐渐胜过人类伴侣。这些发现揭示了 AI 如何重塑混合社会中的社交互动，并为设计更高效、更具协作性的混合系统(hybrid systems)提供了理论依据。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13524v1",
      "published_date": "2025-07-17 20:24:26 UTC",
      "updated_date": "2025-07-17 20:24:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:13:20.836217+00:00"
    },
    {
      "arxiv_id": "2507.15877v2",
      "title": "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning",
      "title_zh": "ARC-AGI 领域的分布外泛化：执行引导的神经程序合成与测试时微调的对比",
      "authors": [
        "Simon Ouellette"
      ],
      "abstract": "We run a controlled compositional generalization experiment in the ARC-AGI domain: an open-world problem domain in which the ability to generalize out-of-distribution is, by design, an essential characteristic for success. We compare neural program synthesis and test-time fine-tuning approaches on this experiment. We find that execution-guided neural program synthesis outperforms all reference algorithms in its ability to compose novel solutions. Our empirical findings also suggest that the success of TTFT on ARC-AGI lies mainly in eliciting in-distribution knowledge that the LLM otherwise fails to rely on directly.",
      "tldr_zh": "该研究在 ARC-AGI 领域开展了一项受控的组合泛化(Compositional Generalization)实验，深入对比了执行引导的神经程序合成(Execution-Guided Neural Program Synthesis)与测试时微调(Test-Time Fine-Tuning, TTFT)在应对分布外(Out-of-Distribution)泛化挑战时的效能。ARC-AGI 作为一个开放式问题域，要求模型具备极强的跨分布泛化能力以取得成功。实验结果显示，执行引导的神经程序合成在生成新颖解决方案方面表现优异，显著超越了所有对比算法。实证研究进一步揭示，TTFT 在该领域的有效性主要源于其能够激发大型语言模型(LLM)中原有的分布内(In-Distribution)知识，而非展现出真正的创新泛化。这一研究成果为理解不同算法在通向通用人工智能过程中的泛化机制提供了重要参考。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "this version fixes errors in AlphaEvolve total % calculation, Table 3 DSL description, and adds clarifications in response to review criticisms",
      "pdf_url": "https://arxiv.org/pdf/2507.15877v2",
      "published_date": "2025-07-17 20:12:01 UTC",
      "updated_date": "2025-09-21 14:56:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:13:24.091085+00:00"
    },
    {
      "arxiv_id": "2507.13511v1",
      "title": "GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination",
      "title_zh": "GraphTrafficGPT：基于图 AI 智能体协同的交通管理增强",
      "authors": [
        "Nabil Abdelaziz Ferhat Taleb",
        "Abdolazim Rezaei",
        "Raj Atulkumar Patel",
        "Mehdi Sookhak"
      ],
      "abstract": "Large Language Models (LLMs) offer significant promise for intelligent traffic management; however, current chain-based systems like TrafficGPT are hindered by sequential task execution, high token usage, and poor scalability, making them inefficient for complex, real-world scenarios. To address these limitations, we propose GraphTrafficGPT, a novel graph-based architecture, which fundamentally redesigns the task coordination process for LLM-driven traffic applications. GraphTrafficGPT represents tasks and their dependencies as nodes and edges in a directed graph, enabling efficient parallel execution and dynamic resource allocation. The main idea behind the proposed model is a Brain Agent that decomposes user queries, constructs optimized dependency graphs, and coordinates a network of specialized agents for data retrieval, analysis, visualization, and simulation. By introducing advanced context-aware token management and supporting concurrent multi-query processing, the proposed architecture handles interdependent tasks typical of modern urban mobility environments. Experimental results demonstrate that GraphTrafficGPT reduces token consumption by 50.2% and average response latency by 19.0% compared to TrafficGPT, while supporting simultaneous multi-query execution with up to 23.0% improvement in efficiency.",
      "tldr_zh": "该研究提出了GraphTrafficGPT，这是一种基于图的AI智能体协作架构，旨在解决如TrafficGPT等链式系统在智能交通管理中面临的串行执行效率低、Token消耗高及可扩展性差等问题。该框架通过将任务及其依赖关系表示为有向图中的节点和边，实现了任务的并行执行和动态资源分配。其核心组件Brain Agent负责分解用户查询并构建优化的依赖图，进而协调由专业智能体组成的网络，协同执行数据检索、分析、可视化和仿真任务。GraphTrafficGPT还引入了上下文感知的Token管理技术，并支持并发的多查询处理，有效应对了现代城市出行环境中的复杂任务需求。实验结果显示，与TrafficGPT相比，该架构将Token消耗降低了50.2%，平均响应延迟缩短了19.0%。在多查询并行执行方面，该系统展现出高达23.0%的效率提升，为实现复杂现实场景下的高效智能交通管理奠定了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13511v1",
      "published_date": "2025-07-17 19:41:09 UTC",
      "updated_date": "2025-07-17 19:41:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:13:25.384840+00:00"
    },
    {
      "arxiv_id": "2507.13505v1",
      "title": "PHASE: Passive Human Activity Simulation Evaluation",
      "title_zh": "PHASE：被动式人类活动模拟评估",
      "authors": [
        "Steven Lamp",
        "Jason D. Hiser",
        "Anh Nguyen-Tuong",
        "Jack W. Davidson"
      ],
      "abstract": "Cybersecurity simulation environments, such as cyber ranges, honeypots, and sandboxes, require realistic human behavior to be effective, yet no quantitative method exists to assess the behavioral fidelity of synthetic user personas. This paper presents PHASE (Passive Human Activity Simulation Evaluation), a machine learning framework that analyzes Zeek connection logs and distinguishes human from non-human activity with over 90\\% accuracy. PHASE operates entirely passively, relying on standard network monitoring without any user-side instrumentation or visible signs of surveillance. All network activity used for machine learning is collected via a Zeek network appliance to avoid introducing unnecessary network traffic or artifacts that could disrupt the fidelity of the simulation environment. The paper also proposes a novel labeling approach that utilizes local DNS records to classify network traffic, thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley Additive exPlanations) analysis to uncover temporal and behavioral signatures indicative of genuine human users. In a case study, we evaluate a synthetic user persona and identify distinct non-human patterns that undermine behavioral realism. Based on these insights, we develop a revised behavioral configuration that significantly improves the human-likeness of synthetic activity yielding a more realistic and effective synthetic user persona.",
      "tldr_zh": "该研究提出了 PHASE (Passive Human Activity Simulation Evaluation)，这是一个旨在评估网络安全模拟环境中合成用户人格 (Synthetic user personas) 行为逼真度的机器学习框架。针对现有模拟环境缺乏定量评估方法的现状，PHASE 通过分析 Zeek 连接日志，在无需用户端安装任何插件或产生监控痕迹的前提下，能以超过 90% 的准确率区分人类与非人类活动。该框架采用了一种利用本地 DNS 记录对网络流量进行分类的新颖标注方法，并结合 SHAP (SHapley Additive exPlanations) 分析揭示了代表真实人类用户的时序和行为特征。在一项案例研究中，PHASE 成功识别了合成人格中损害行为真实感的非人类模式，并基于此洞察优化了行为配置，显著提升了合成活动的类人性。这一研究为构建更真实、更有效的网络安全模拟系统提供了关键的定量评估手段。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13505v1",
      "published_date": "2025-07-17 19:24:11 UTC",
      "updated_date": "2025-07-17 19:24:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:13:27.654802+00:00"
    },
    {
      "arxiv_id": "2507.13499v1",
      "title": "AI-Assisted Fixes to Code Review Comments at Scale",
      "title_zh": "大规模代码审查意见的 AI 辅助修复",
      "authors": [
        "Chandra Maddila",
        "Negar Ghorbani",
        "James Saindon",
        "Parth Thakkar",
        "Vijayaraghavan Murali",
        "Rui Abreu",
        "Jingyue Shen",
        "Brian Zhou",
        "Nachiappan Nagappan",
        "Peter C. Rigby"
      ],
      "abstract": "Aim. There are 10s of thousands of code review comments each week at Meta. We developed Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes for reviewer comments in production at scale.\n  Method. We developed an internal benchmark of 64k <review comment, patch> data points to fine-tune Llama models. Once our models achieve reasonable offline results, we roll them into production. To ensure that our AI-assisted fixes do not negatively impact the time it takes to do code reviews, we conduct randomized controlled safety trials as well as full production experiments.\n  Offline Results. As a baseline, we compare GPT-4o to our small and large Llama models. In offline results, our LargeLSFT model creates an exact match patch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The internal models also use more modern Hack functions when compared to the PHP functions suggested by GPT-4o.\n  Safety Trial. When we roll MetaMateCR into production in a safety trial that compares no AI patches with AI patch suggestions, we see a large regression with reviewers taking over 5% longer to conduct reviews. After investigation, we modify the UX to only show authors the AI patches, and see no regressions in the time for reviews.\n  Production. When we roll LargeLSFT into production, we see an ActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o. Our results illustrate the importance of safety trials in ensuring that AI does not inadvertently slow down engineers, and a successful review comment to AI patch product running at scale.",
      "tldr_zh": "该研究介绍了MetaMateCR，一种在Meta大规模生产环境中部署的AI辅助修复代码审查(Code Review)评论的系统。研究团队利用包含6.4万个“评论-补丁”对的内部基准数据集对Llama模型进行精调(Fine-tuning)，开发出高性能的LargeLSFT模型。离线评估显示，LargeLSFT生成精确匹配补丁的准确率达68%，比GPT-4o高出9个百分点，且更能熟练应用现代Hack语言功能。通过随机对照安全试验(Randomized Controlled Safety Trials)，研究发现调整用户体验(UX)将补丁直接展示给作者而非审查者能有效避免效率下降。在实际生产运行中，该系统实现了19.7%的ActionableToApplied率，相较于GPT-4o提升了9.2个百分点。这项工作不仅验证了AI在大规模代码修复中的可行性，也强调了通过科学试验确保AI工具不干扰工程师工作流的重要性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13499v1",
      "published_date": "2025-07-17 19:11:00 UTC",
      "updated_date": "2025-07-17 19:11:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:13:31.530364+00:00"
    },
    {
      "arxiv_id": "2507.13485v1",
      "title": "Neural Architecture Search with Mixed Bio-inspired Learning Rules",
      "title_zh": "基于混合生物启发学习规则的神经架构搜索",
      "authors": [
        "Imane Hamzaoui",
        "Riyadh Baghdadi"
      ],
      "abstract": "Bio-inspired neural networks are attractive for their adversarial robustness, energy frugality, and closer alignment with cortical physiology, yet they often lag behind back-propagation (BP) based models in accuracy and ability to scale. We show that allowing the use of different bio-inspired learning rules in different layers, discovered automatically by a tailored neural-architecture-search (NAS) procedure, bridges this gap. Starting from standard NAS baselines, we enlarge the search space to include bio-inspired learning rules and use NAS to find the best architecture and learning rule to use in each layer. We show that neural networks that use different bio-inspired learning rules for different layers have better accuracy than those that use a single rule across all the layers. The resulting NN that uses a mix of bio-inspired learning rules sets new records for bio-inspired models: 95.16% on CIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on ImageNet. In some regimes, they even surpass comparable BP-based networks while retaining their robustness advantages. Our results suggest that layer-wise diversity in learning rules allows better scalability and accuracy, and motivates further research on mixing multiple bio-inspired learning rules in the same network.",
      "tldr_zh": "该研究针对仿生神经网络(Bio-inspired neural networks)在准确率和可扩展性上通常逊于反向传播(Back-propagation, BP)模型的问题，提出了一种在不同层混合使用多种仿生学习规则的新策略。研究利用定制的神经架构搜索(Neural Architecture Search, NAS)程序，自动为神经网络的每一层发现并分配最匹配的架构与仿生学习规则。实验结果显示，采用层级多样化学习规则的网络在准确率上显著优于使用单一规则的模型，并在CIFAR-10(95.16%)、CIFAR-100(76.48%)及ImageNet(60.51%)等多个数据集上创下了仿生模型的新纪录。在特定场景下，这些模型在保持能效和对抗鲁棒性(Adversarial robustness)优势的同时，其性能甚至超越了同类BP网络。该成果证明了层间规则的多样性是提升仿生模型性能的关键，为未来混合仿生学习算法的研究提供了重要方向。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "ECAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13485v1",
      "published_date": "2025-07-17 18:49:38 UTC",
      "updated_date": "2025-07-17 18:49:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:13:43.130424+00:00"
    },
    {
      "arxiv_id": "2507.13468v2",
      "title": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations",
      "title_zh": "ERR@HRI 2.0 挑战赛：人机对话中错误与故障的多模态检测",
      "authors": [
        "Shiye Cao",
        "Maia Stiber",
        "Amama Mahmood",
        "Maria Teresa Parreira",
        "Wendy Ju",
        "Micol Spitale",
        "Hatice Gunes",
        "Chien-Ming Huang"
      ],
      "abstract": "The integration of large language models (LLMs) into conversational robots has made human-robot conversations more dynamic. Yet, LLM-powered conversational robots remain prone to errors, e.g., misunderstanding user intent, prematurely interrupting users, or failing to respond altogether. Detecting and addressing these failures is critical for preventing conversational breakdowns, avoiding task disruptions, and sustaining user trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal dataset of LLM-powered conversational robot failures during human-robot conversations and encourages researchers to benchmark machine learning models designed to detect robot failures. The dataset includes 16 hours of dyadic human-robot interactions, incorporating facial, speech, and head movement features. Each interaction is annotated with the presence or absence of robot errors from the system perspective, and perceived user intention to correct for a mismatch between robot behavior and user expectation. Participants are invited to form teams and develop machine learning models that detect these failures using multimodal data. Submissions will be evaluated using various performance metrics, including detection accuracy and false positive rate. This challenge represents another key step toward improving failure detection in human-robot interaction through social signal analysis.",
      "tldr_zh": "ERR@HRI 2.0 Challenge 针对集成 Large Language Models (LLMs) 的对话机器人在人机交互中出现的误解指令、意外中断或无响应等故障检测问题展开研究。该挑战赛提供了一个包含16小时二元交互记录的多模态数据集，整合了面部表情、语音及头部动作等多维特征。数据集中从系统视角标注了机器人错误，并记录了用户在行为不匹配时的纠错意图，旨在支持研究者开发高效的机器学习模型。参赛者需利用多模态数据优化故障检测的准确性并降低误报率，通过社交信号分析 (Social Signal Analysis) 技术推动人机交互 (Human-Robot Interaction) 的可靠性。该研究对于防止对话崩溃、避免任务中断以及长期维持用户对智能系统的信任具有重要的实践价值。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13468v2",
      "published_date": "2025-07-17 18:21:45 UTC",
      "updated_date": "2025-10-09 15:54:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:13:38.303088+00:00"
    },
    {
      "arxiv_id": "2507.14241v3",
      "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models",
      "title_zh": "Promptomatix：面向大语言模型的自动提示词优化框架",
      "authors": [
        "Rithesh Murthy",
        "Ming Zhu",
        "Liangwei Yang",
        "Jielin Qiu",
        "Juntao Tan",
        "Shelby Heinecke",
        "Caiming Xiong",
        "Silvio Savarese",
        "Huan Wang"
      ],
      "abstract": "Large Language Models (LLMs) perform best with well-crafted prompts, yet prompt engineering remains manual, inconsistent, and inaccessible to non-experts. We introduce Promptomatix, an automatic prompt optimization framework that transforms natural language task descriptions into high-quality prompts without requiring manual tuning or domain expertise. Promptomatix supports both a lightweight meta-prompt-based optimizer and a DSPy-powered compiler, with modular design enabling future extension to more advanced frameworks. The system analyzes user intent, generates synthetic training data, selects prompting strategies, and refines prompts using cost-aware objectives. Evaluated across 5 task categories, Promptomatix achieves competitive or superior performance compared to existing libraries, while reducing prompt length and computational overhead making prompt optimization scalable and efficient.",
      "tldr_zh": "该研究针对 Prompt Engineering 存在的人工依赖、不一致性以及非专家难以操作的问题，提出了 Promptomatix 自动化提示词优化框架。Promptomatix 能够将自然语言任务描述直接转化为高质量提示词，无需人工调优或特定领域知识。该框架集成了基于轻量级 meta-prompt 的优化器和 DSPy 驱动的编译器，其模块化设计具备良好的扩展性。系统通过分析用户意图、生成合成训练数据、选择 prompting strategies，并结合成本感知目标对提示词进行持续精炼。在五大类任务的评估中，Promptomatix 展现出优于或等同于现有工具库的竞争性能。此外，该框架有效缩减了提示词长度并降低了计算开销，为大规模且高效的提示词优化提供了高度可扩展的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14241v3",
      "published_date": "2025-07-17 18:18:20 UTC",
      "updated_date": "2025-07-24 21:49:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:13:53.291970+00:00"
    },
    {
      "arxiv_id": "2507.13459v1",
      "title": "Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection",
      "title_zh": "具备充要接触检测的可变形体接触图神经网络代理模型",
      "authors": [
        "Vijay K. Dubey",
        "Collin E. Haese",
        "Osman Gültekin",
        "David Dalton",
        "Manuel K. Rausch",
        "Jan N. Fuhg"
      ],
      "abstract": "Surrogate models for the rapid inference of nonlinear boundary value problems in mechanics are helpful in a broad range of engineering applications. However, effective surrogate modeling of applications involving the contact of deformable bodies, especially in the context of varying geometries, is still an open issue. In particular, existing methods are confined to rigid body contact or, at best, contact between rigid and soft objects with well-defined contact planes. Furthermore, they employ contact or collision detection filters that serve as a rapid test but use only the necessary and not sufficient conditions for detection. In this work, we present a graph neural network architecture that utilizes continuous collision detection and, for the first time, incorporates sufficient conditions designed for contact between soft deformable bodies. We test its performance on two benchmarks, including a problem in soft tissue mechanics of predicting the closed state of a bioprosthetic aortic valve. We find a regularizing effect on adding additional contact terms to the loss function, leading to better generalization of the network. These benefits hold for simple contact at similar planes and element normal angles, and complex contact at differing planes and element normal angles. We also demonstrate that the framework can handle varying reference geometries. However, such benefits come with high computational costs during training, resulting in a trade-off that may not always be favorable. We quantify the training cost and the resulting inference speedups on various hardware architectures. Importantly, our graph neural network implementation results in up to a thousand-fold speedup for our benchmark problems at inference.",
      "tldr_zh": "本研究提出了一种基于 Graph Neural Network 的代理模型架构，旨在实现可变形体(Deformable Bodies)接触问题的快速推理。该模型首次结合了连续碰撞检测(Continuous Collision Detection)并引入了针对软性可变形体接触设计的充分条件(Sufficient Conditions)，克服了现有方法仅依赖必要条件且局限于刚体或简单接触平面的局限性。研究通过包括生物瓣膜主动脉瓣(Bioprosthetic Aortic Valve)闭合预测在内的两项基准测试验证了性能，发现在损失函数中添加额外的接触项具有正则化效应(Regularizing Effect)，能显著提升网络在处理复杂接触平面和不同参考几何形状时的泛化能力。尽管该框架在训练阶段面临较高的计算成本，但在推理阶段可实现高达一千倍的加速，为涉及非线性边界值问题的工程应用提供了高效的替代方案。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13459v1",
      "published_date": "2025-07-17 18:09:19 UTC",
      "updated_date": "2025-07-17 18:09:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:00.411402+00:00"
    },
    {
      "arxiv_id": "2507.13353v1",
      "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding",
      "title_zh": "VideoITG：基于指令引导时间定位的多模态视频理解",
      "authors": [
        "Shihao Wang",
        "Guo Chen",
        "De-an Huang",
        "Zhiqi Li",
        "Minghan Li",
        "Guilin Li",
        "Jose M. Alvarez",
        "Lei Zhang",
        "Zhiding Yu"
      ],
      "abstract": "Recent studies have revealed that selecting informative and relevant video frames can significantly improve the performance of Video Large Language Models (Video-LLMs). Current methods, such as reducing inter-frame redundancy, employing separate models for image-text relevance assessment, or utilizing temporal video grounding for event localization, substantially adopt unsupervised learning paradigms, whereas they struggle to address the complex scenarios in long video understanding. We propose Instructed Temporal Grounding for Videos (VideoITG), featuring customized frame sampling aligned with user instructions. The core of VideoITG is the VidThinker pipeline, an automated annotation framework that explicitly mimics the human annotation process. First, it generates detailed clip-level captions conditioned on the instruction; then, it retrieves relevant video segments through instruction-guided reasoning; finally, it performs fine-grained frame selection to pinpoint the most informative visual evidence. Leveraging VidThinker, we construct the VideoITG-40K dataset, containing 40K videos and 500K instructed temporal grounding annotations. We then design a plug-and-play VideoITG model, which takes advantage of visual language alignment and reasoning capabilities of Video-LLMs, for effective frame selection in a discriminative manner. Coupled with Video-LLMs, VideoITG achieves consistent performance improvements across multiple multimodal video understanding benchmarks, showing its superiority and great potentials for video understanding.",
      "tldr_zh": "该研究提出了 VideoITG (Instructed Temporal Grounding for Videos)，旨在通过根据用户指令进行定制化帧采样，解决 Video Large Language Models (Video-LLMs) 在长视频理解中面临的帧选择难题。其核心是 VidThinker 自动标注流水线，该流水线通过模拟人类标注逻辑，结合指令引导推理和细粒度帧选择来精准定位关键视觉证据。基于此，研究者构建了拥有 40K 个视频和 500K 条标注的 VideoITG-40K 数据集。此外，该研究还设计了一个即插即用的 VideoITG 模型，充分利用 Video-LLMs 的视觉语言对齐与推理能力进行判别式采样。实验表明，VideoITG 在多个多模态视频理解基准测试中均实现了持续的性能提升，展示了其在处理复杂视频任务中的优越性与应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2507.13353v1",
      "published_date": "2025-07-17 17:59:59 UTC",
      "updated_date": "2025-07-17 17:59:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:01.135736+00:00"
    },
    {
      "arxiv_id": "2507.13348v1",
      "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning",
      "title_zh": "VisionThink：基于强化学习的智能高效视觉语言模型",
      "authors": [
        "Senqiao Yang",
        "Junyi Li",
        "Xin Lai",
        "Bei Yu",
        "Hengshuang Zhao",
        "Jiaya Jia"
      ],
      "abstract": "Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink.",
      "tldr_zh": "该研究提出了 VisionThink，一种基于强化学习 (Reinforcement Learning) 的智能高效视觉语言模型 (VLM) 范式，旨在解决视觉 token 冗余导致的计算效率低下问题。与现有采用固定压缩比例或阈值的方法不同，VisionThink 能够根据输入样本的复杂程度动态调整分辨率，通过输出特殊 token 自主决定是否需要调用高分辨率图像进行深入分析。研究者通过 LLM-as-Judge 策略将强化学习应用于通用 VQA 任务，并设计了合理的奖励函数与惩罚机制，以平衡图像缩放频率与模型准确度。实验证明，VisionThink 在保留 OCR 相关任务所需的细粒度视觉理解能力的同时，大幅减少了简单任务中的视觉 token 消耗。该方法在性能、效率和有效性方面均表现优异，为开发更具实用性的视觉语言系统提供了有效路径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code and models are available at https://github.com/dvlab-research/VisionThink",
      "pdf_url": "https://arxiv.org/pdf/2507.13348v1",
      "published_date": "2025-07-17 17:59:55 UTC",
      "updated_date": "2025-07-17 17:59:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:08.091170+00:00"
    },
    {
      "arxiv_id": "2507.13345v2",
      "title": "Imbalance in Balance: Online Concept Balancing in Generation Models",
      "title_zh": "失衡中的平衡：生成模型中的在线概念均衡",
      "authors": [
        "Yukai Shi",
        "Jiarong Ou",
        "Rui Chen",
        "Haotian Yang",
        "Jiahao Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai"
      ],
      "abstract": "In visual generation tasks, the responses and combinations of complex concepts often lack stability and are error-prone, which remains an under-explored area. In this paper, we attempt to explore the causal factors for poor concept responses through elaborately designed experiments. We also design a concept-wise equalization loss function (IMBA loss) to address this issue. Our proposed method is online, eliminating the need for offline dataset processing, and requires minimal code changes. In our newly proposed complex concept benchmark Inert-CompBench and two other public test sets, our method significantly enhances the concept response capability of baseline models and yields highly competitive results with only a few codes released at https://github.com/KwaiVGI/IMBA-Loss.",
      "tldr_zh": "该研究针对视觉生成任务中复杂概念(complex concepts)及其组合反应不稳定、易出错且研究不足的问题，通过设计实验探索了导致概念响应不佳的因果因素。为此，作者提出了一种名为 IMBA loss 的概念均衡损失函数(concept-wise equalization loss function)，旨在优化模型的概念处理能力。该方法具有在线(online)特性，无需离线处理数据集，且仅需极少的代码改动即可实现。实验结果显示，在作者提出的新基准测试集 Inert-CompBench 以及两个公共测试集上，该方法显著提升了基线模型的概念响应性能，并展现出极具竞争力的结果。目前，该研究的相关代码已在 GitHub 开源，为解决生成模型中的概念失衡问题提供了高效且简便的方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICCV2025. Codes have been released at https://github.com/KwaiVGI/IMBA-Loss",
      "pdf_url": "https://arxiv.org/pdf/2507.13345v2",
      "published_date": "2025-07-17 17:59:47 UTC",
      "updated_date": "2025-11-11 17:59:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:13.595933+00:00"
    },
    {
      "arxiv_id": "2507.13340v2",
      "title": "Latent Policy Steering with Embodiment-Agnostic Pretrained World Models",
      "title_zh": "基于具身无关预训练世界模型的潜空间策略引导",
      "authors": [
        "Yiqi Wang",
        "Mrinal Verghese",
        "Jeff Schneider"
      ],
      "abstract": "Learning visuomotor policies via imitation has proven effective across a wide range of robotic domains. However, the performance of these policies is heavily dependent on the number of training demonstrations, which requires expensive data collection in the real world. In this work, we aim to reduce data collection efforts when learning visuomotor robot policies by leveraging existing or cost-effective data from a wide range of embodiments, such as public robot datasets and the datasets of humans playing with objects (human data from play). Our approach leverages two key insights. First, we use optic flow as an embodiment-agnostic action representation to train a World Model (WM) across multi-embodiment datasets, and finetune it on a small amount of robot data from the target embodiment. Second, we develop a method, Latent Policy Steering (LPS), to improve the output of a behavior-cloned policy by searching in the latent space of the WM for better action sequences. In real world experiments, we observe significant improvements in the performance of policies trained with a small amount of data (over 50% relative improvement with 30 demonstrations and over 20% relative improvement with 50 demonstrations) by combining the policy with a WM pretrained on two thousand episodes sampled from the existing Open X-embodiment dataset across different robots or a cost-effective human dataset from play.",
      "tldr_zh": "该研究针对视觉运动策略（visuomotor policies）模仿学习对大规模演示数据的依赖问题，提出了一种利用跨实体（multi-embodiment）数据减少数据采集成本的方法。研究的核心创新在于利用光流（optic flow）作为实体无关的动作表示来预训练世界模型（World Model, WM），并结合潜空间策略引导（Latent Policy Steering, LPS）技术，通过在世界模型的潜空间内搜索最优动作序列来优化行为克隆（behavior-cloned）策略。实验证明，该方法在结合 Open X-embodiment 或人类操作数据集进行预训练后，能显著提升小样本场景下的机器人表现。在仅提供30次和50次演示的情况下，策略性能分别实现了超过50%和20%的相对提升。这种结合跨实体预训练与潜空间搜索的架构，为提升机器人在资源受限环境下的泛化能力和执行效率提供了有力支持。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13340v2",
      "published_date": "2025-07-17 17:57:57 UTC",
      "updated_date": "2025-09-21 23:37:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:14.388987+00:00"
    },
    {
      "arxiv_id": "2507.13428v1",
      "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models",
      "title_zh": "PhyWorldBench：文生视频模型物理真实性的综合评估",
      "authors": [
        "Jing Gu",
        "Xian Liu",
        "Yu Zeng",
        "Ashwin Nagarajan",
        "Fangrui Zhu",
        "Daniel Hong",
        "Yue Fan",
        "Qianqi Yan",
        "Kaiwen Zhou",
        "Ming-Yu Liu",
        "Xin Eric Wang"
      ],
      "abstract": "Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles.",
      "tldr_zh": "该研究提出了PhyWorldBench，这是一个旨在全面评估文本生成视频(Text-to-Video)模型在物理真实性(Physical Realism)表现的综合基准测试。该基准覆盖了从基础的物体运动与能量守恒(Energy Conservation)到复杂的刚体交互及生物运动等多个层面的物理现象。此外，研究还引入了创新的“反物理”(Anti-Physics)类别，通过故意违反物理规律的提示词来评估模型在遵循指令时维持逻辑一致性的能力。在评估方法上，除了大规模人工评估外，作者还设计了一种利用现有多模态大语言模型(MLLM)以零样本(Zero-shot)方式评估物理真实性的有效方法。通过对12个最先进的视频生成模型在1,050个精心设计的提示词下的表现进行系统测试，研究揭示了当前模型在遵循现实物理规律方面面临的关键挑战。最后，论文深入分析了模型在多样化物理场景下的性能差异，并针对如何通过优化提示词来增强视频的物理忠实度提出了针对性建议。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "31 pages, 21 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.13428v1",
      "published_date": "2025-07-17 17:54:09 UTC",
      "updated_date": "2025-07-17 17:54:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:18.395222+00:00"
    },
    {
      "arxiv_id": "2507.13337v1",
      "title": "FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming",
      "title_zh": "FormulaOne：超越竞赛编程的算法推理深度评估",
      "authors": [
        "Gal Beniamini",
        "Yuval Dor",
        "Alon Vinnikov",
        "Shir Granot Peled",
        "Or Weinstein",
        "Or Sharir",
        "Noam Wies",
        "Tomer Nussbaum",
        "Ido Ben Shaul",
        "Tomer Zekharya",
        "Yoav Levine",
        "Shai Shalev-Shwartz",
        "Amnon Shashua"
      ],
      "abstract": "Frontier AI models demonstrate formidable breadth of knowledge. But how close are they to true human -- or superhuman -- expertise? Genuine experts can tackle the hardest problems and push the boundaries of scientific understanding. To illuminate the limits of frontier model capabilities, we turn away from contrived competitive programming puzzles, and instead focus on real-life research problems.\n  We construct FormulaOne, a benchmark that lies at the intersection of graph theory, logic, and algorithms, all well within the training distribution of frontier models. Our problems are incredibly demanding, requiring an array of reasoning steps. The dataset has three key properties. First, it is of commercial interest and relates to practical large-scale optimisation problems, such as those arising in routing, scheduling, and network design. Second, it is generated from the highly expressive framework of Monadic Second-Order (MSO) logic on graphs, paving the way toward automatic problem generation at scale; ideal for building RL environments. Third, many of our problems are intimately related to the frontier of theoretical computer science, and to central conjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As such, any significant algorithmic progress on our dataset, beyond known results, could carry profound theoretical implications.\n  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on FormulaOne, solving less than 1% of the questions, even when given 10 attempts and explanatory fewshot examples -- highlighting how far they remain from expert-level understanding in some domains. To support further research, we additionally curate FormulaOne-Warmup, offering a set of simpler tasks, from the same distribution. We release the full corpus along with a comprehensive evaluation framework.",
      "tldr_zh": "该研究针对前沿 AI 模型在处理真实研究问题时的局限性，构建了名为 FormulaOne 的基准测试，旨在衡量其超越竞争性编程的算法推理深度。FormulaOne 聚焦于 Graph Theory、Logic 与 Algorithms 的交叉领域，包含了大量具有商业价值的大规模优化问题，如路由和网络设计。该数据集利用 Monadic Second-Order (MSO) logic 框架生成，支持自动化生成并可用于构建 RL 环境，且其核心难题与 Strong Exponential Time Hypothesis (SETH) 等理论计算机科学前沿猜想紧密相关。研究实验表明，即便是 OpenAI 的 o3 等顶级模型在 FormulaOne 上的解决率也低于 1%，证明了 AI 模型在专家级理解力方面仍存在显著差距。为支持后续研究，作者同步发布了包含较简单任务的 FormulaOne-Warmup 数据集以及配套的评估框架。",
      "categories": [
        "cs.AI",
        "cs.CC",
        "math.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13337v1",
      "published_date": "2025-07-17 17:53:55 UTC",
      "updated_date": "2025-07-17 17:53:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:19.015488+00:00"
    },
    {
      "arxiv_id": "2507.13328v2",
      "title": "Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It",
      "title_zh": "视觉-语言训练有助于分类知识的调用，但并未从根本上改变它",
      "authors": [
        "Yulu Qin",
        "Dheeraj Varghese",
        "Adam Dahlgren Lindström",
        "Lucia Donatelli",
        "Kanishka Misra",
        "Najoung Kim"
      ],
      "abstract": "Does vision-and-language (VL) training change the linguistic representations of language models in meaningful ways? Most results in the literature have shown inconsistent or marginal differences, both behaviorally and representationally. In this work, we start from the hypothesis that the domain in which VL training could have a significant effect is lexical-conceptual knowledge, in particular its taxonomic organization. Through comparing minimal pairs of text-only LMs and their VL-trained counterparts, we first show that the VL models often outperform their text-only counterparts on a text-only question-answering task that requires taxonomic understanding of concepts mentioned in the questions. Using an array of targeted behavioral and representational analyses, we show that the LMs and VLMs do not differ significantly in terms of their taxonomic knowledge itself, but they differ in how they represent questions that contain concepts in a taxonomic relation vs. a non-taxonomic relation. This implies that the taxonomic knowledge itself does not change substantially through additional VL training, but VL training does improve the deployment of this knowledge in the context of a specific task, even when the presentation of the task is purely linguistic.",
      "tldr_zh": "该研究探讨了视觉语言(Vision-and-Language, VL)训练是否会改变语言模型的词汇概念知识，特别是其分类组织(Taxonomic Organization)。通过对比纯文本语言模型(LMs)及其经过视觉语言训练的对应模型(VLMs)，作者发现VLMs在需要概念分类理解的纯文本问答任务中表现更优。进一步的行为和表示分析显示，LMs和VLMs在分类知识本身上并无显著差异，但在处理包含分类关系的问题表示方式上存在不同。研究结论指出，视觉语言训练并未从根本上改变分类知识的内在表示，而是优化了模型在特定任务语境下部署(Deploy)这些预存知识的能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13328v2",
      "published_date": "2025-07-17 17:47:47 UTC",
      "updated_date": "2025-10-29 22:38:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:23.198664+00:00"
    },
    {
      "arxiv_id": "2507.14240v3",
      "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem",
      "title_zh": "HuggingGraph：探析大语言模型生态系统的供应链",
      "authors": [
        "Mohammad Shahedur Rahman",
        "Peng Gao",
        "Yuede Ji"
      ],
      "abstract": "Large language models (LLMs) leverage deep learning architectures to process and predict sequences of words, enabling them to perform a wide range of natural language processing tasks, such as translation, summarization, question answering, and content generation. As existing LLMs are often built from base models or other pre-trained models and use external datasets, they can inevitably inherit vulnerabilities, biases, or malicious components that exist in previous models or datasets. Therefore, it is critical to understand these components' origin and development process to detect potential risks, improve model fairness, and ensure compliance with regulatory frameworks. Motivated by that, this project aims to study such relationships between models and datasets, which are the central parts of the LLM supply chain. First, we design a methodology to systematically collect LLMs' supply chain information. Then, we design a new graph to model the relationships between models and datasets, which is a directed heterogeneous graph, having 402,654 nodes and 462,524 edges. Lastly, we perform different types of analysis and make multiple interesting findings.",
      "tldr_zh": "该研究提出了HuggingGraph，旨在深入理解大语言模型(LLMs)生态系统中的供应链关系，以应对模型可能继承的基础模型或数据集中的漏洞、偏见及恶意组件等安全风险。研究团队首先设计了一套系统化的方法来收集LLMs供应链信息，并构建了一个包含402,654个节点和462,524条边的有向异构图(directed heterogeneous graph)，用于细致建模模型与数据集之间的复杂关联。通过对这一大规模图结构进行多维度分析，该项目揭示了供应链中组件的起源与演化过程，并产出了多项关于供应链结构的洞察。该工作为检测潜在风险、提升模型公平性以及确保符合监管框架要求提供了重要的数据支撑和分析框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14240v3",
      "published_date": "2025-07-17 17:34:13 UTC",
      "updated_date": "2025-09-04 23:06:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:31.892305+00:00"
    },
    {
      "arxiv_id": "2507.13314v1",
      "title": "Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark",
      "title_zh": "重新审视基于推理的姿态估计基准的可靠性",
      "authors": [
        "Junsu Kim",
        "Naeun Kim",
        "Jaeho Lee",
        "Incheol Park",
        "Dongyoon Han",
        "Seungryul Baek"
      ],
      "abstract": "The reasoning-based pose estimation (RPE) benchmark has emerged as a widely adopted evaluation standard for pose-aware multimodal large language models (MLLMs). Despite its significance, we identified critical reproducibility and benchmark-quality issues that hinder fair and consistent quantitative evaluations. Most notably, the benchmark utilizes different image indices from those of the original 3DPW dataset, forcing researchers into tedious and error-prone manual matching processes to obtain accurate ground-truth (GT) annotations for quantitative metrics (\\eg, MPJPE, PA-MPJPE). Furthermore, our analysis reveals several inherent benchmark-quality limitations, including significant image redundancy, scenario imbalance, overly simplistic poses, and ambiguous textual descriptions, collectively undermining reliable evaluations across diverse scenarios. To alleviate manual effort and enhance reproducibility, we carefully refined the GT annotations through meticulous visual matching and publicly release these refined annotations as an open-source resource, thereby promoting consistent quantitative evaluations and facilitating future advancements in human pose-aware multimodal reasoning.",
      "tldr_zh": "该研究重新评估了基于推理的姿态估计(Reasoning-based Pose Estimation, RPE)基准测试的可靠性，指出其在多模态大语言模型(MLLMs)评估中存在的关键可复现性与基准质量问题。研究发现RPE由于与原始3DPW数据集的图像索引不一致，导致研究人员在获取量化指标所需的ground-truth(GT)标注时面临繁琐且易出错的手动匹配过程。此外，分析揭示了该基准在图像冗余、场景失衡、姿态过于简单以及文本描述模糊等方面的局限性，严重影响了评估的可靠性。为提高可复现性并减少人工负担，研究团队通过细致的视觉匹配对GT标注进行了精确修复。最后，研究者将这些优化后的标注作为开源资源发布，旨在为人体姿态感知多模态推理领域提供更一致的量化评估标准并促进未来技术发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "To be presented as a poster at MMFM 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13314v1",
      "published_date": "2025-07-17 17:33:11 UTC",
      "updated_date": "2025-07-17 17:33:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:47.595036+00:00"
    },
    {
      "arxiv_id": "2507.13302v1",
      "title": "The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations",
      "title_zh": "GEA：在大语言模型（LLM）人类评估中融入能源感知",
      "authors": [
        "Carlos Arriaga",
        "Gonzalo Martínez",
        "Eneko Sendin",
        "Javier Conde",
        "Pedro Reviriego"
      ],
      "abstract": "The evaluation of large language models is a complex task, in which several approaches have been proposed. The most common is the use of automated benchmarks in which LLMs have to answer multiple-choice questions of different topics. However, this method has certain limitations, being the most concerning, the poor correlation with the humans. An alternative approach, is to have humans evaluate the LLMs. This poses scalability issues as there is a large and growing number of models to evaluate making it impractical (and costly) to run traditional studies based on recruiting a number of evaluators and having them rank the responses of the models. An alternative approach is the use of public arenas, such as the popular LM arena, on which any user can freely evaluate models on any question and rank the responses of two models. The results are then elaborated into a model ranking. An increasingly important aspect of LLMs is their energy consumption and, therefore, evaluating how energy awareness influences the decisions of humans in selecting a model is of interest. In this paper, we present GEA, the Generative Energy Arena, an arena that incorporates information on the energy consumption of the model in the evaluation process. Preliminary results obtained with GEA are also presented, showing that for most questions, when users are aware of the energy consumption, they favor smaller and more energy efficient models. This suggests that for most user interactions, the extra cost and energy incurred by the more complex and top-performing models do not provide an increase in the perceived quality of the responses that justifies their use.",
      "tldr_zh": "该研究提出了名为GEA (Generative Energy Arena) 的评估平台，旨在通过在 Large Language Model (LLM) 的人类评价过程中引入能源消耗信息，来探讨能源意识如何影响用户对模型的偏好。GEA 解决了传统自动化基准测试与人类评价相关性差以及传统人工评估成本高昂的问题，允许用户在知晓模型能效的情况下对响应质量进行排序。初步研究结果显示，当用户意识到能源消耗时，在大多数情况下会更青睐体积更小、能效更高的模型。这表明对于大多数用户交互而言，更复杂、高性能模型所产生的额外能源成本并不能带来足以证明其合理性的感知质量提升，反映出用户在模型选择中具有显著的能效考量。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13302v1",
      "published_date": "2025-07-17 17:11:14 UTC",
      "updated_date": "2025-07-17 17:11:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:52.281488+00:00"
    },
    {
      "arxiv_id": "2507.13425v2",
      "title": "CaTFormer: Causal Temporal Transformer with Dynamic Contextual Fusion for Driving Intention Prediction",
      "title_zh": "CaTFormer：融合动态上下文的因果时序 Transformer 驾驶意图预测",
      "authors": [
        "Sirui Wang",
        "Zhou Guan",
        "Bingxi Zhao",
        "Tongjia Gu",
        "Jie Liu"
      ],
      "abstract": "Accurate prediction of driving intention is key to enhancing the safety and interactive efficiency of human-machine co-driving systems. It serves as a cornerstone for achieving high-level autonomous driving. However, current approaches remain inadequate for accurately modeling the complex spatiotemporal interdependencies and the unpredictable variability of human driving behavior. To address these challenges, we propose CaTFormer, a causal Temporal Transformer that explicitly models causal interactions between driver behavior and environmental context for robust intention prediction. Specifically, CaTFormer introduces a novel Reciprocal Delayed Fusion (RDF) mechanism for precise temporal alignment of interior and exterior feature streams, a Counterfactual Residual Encoding (CRE) module that systematically eliminates spurious correlations to reveal authentic causal dependencies, and an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these purified representations into coherent temporal representations. Experimental results demonstrate that CaTFormer attains state-of-the-art performance on the Brain4Cars dataset. It effectively captures complex causal temporal dependencies and enhances both the accuracy and transparency of driving intention prediction.",
      "tldr_zh": "该研究针对自动驾驶中人类驾驶行为的复杂时空依赖性和多变性，提出了 CaTFormer，一种具有动态上下文融合功能的因果时序 Transformer (Causal Temporal Transformer) 模型。CaTFormer 引入了 Reciprocal Delayed Fusion (RDF) 机制，用于实现车内与车外特征流的精确时间对齐，并采用 Counterfactual Residual Encoding (CRE) 模块系统地消除伪相关，从而揭示真实的因果依赖。此外，模型利用 Feature Synthesis Network (FSN) 自适应地将纯化后的表征合成为连贯的时序表示。实验结果显示，CaTFormer 在 Brain4Cars 数据集上达到了 state-of-the-art 性能，显著提升了驾驶意图预测的准确性和透明度。该框架通过有效地捕捉复杂的因果时序依赖关系，为增强人机共驾系统的安全性与交互效率提供了关键技术支撑。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2507.13425v2",
      "published_date": "2025-07-17 17:10:37 UTC",
      "updated_date": "2026-01-08 09:25:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:02.533211+00:00"
    },
    {
      "arxiv_id": "2507.13300v1",
      "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research",
      "title_zh": "AbGen：评估大语言模型在科学研究消融实验设计与评估中的表现",
      "authors": [
        "Yilun Zhao",
        "Weiyuan Chen",
        "Zhijian Xu",
        "Manasi Patwardhan",
        "Yixin Liu",
        "Chengye Wang",
        "Lovekesh Vig",
        "Arman Cohan"
      ],
      "abstract": "We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.",
      "tldr_zh": "该研究推出了 AbGen，这是首个旨在评估 LLMs 在科学研究中设计 Ablation Study 能力的基准测试。该基准包含源自 807 篇 NLP 论文的 1,500 个专家标注示例，要求模型根据给定研究背景生成详细的消融实验方案。评估结果显示，DeepSeek-R1-0528 和 o4-mini 等领先模型在实验设计的 Importance、Faithfulness 和 Soundness 方面与人类专家相比仍有显著差距。此外，研究发现现有的自动评估方法在该任务上并不可靠，为此开发了元评估基准 AbGen-Eval 用于衡量自动化系统的评价可靠性。该研究通过调查多种 LLM-as-Judge 系统，为未来开发更有效、更可靠的复杂科学任务评估系统提供了深入见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13300v1",
      "published_date": "2025-07-17 17:09:22 UTC",
      "updated_date": "2025-07-17 17:09:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:59.587823+00:00"
    },
    {
      "arxiv_id": "2507.13423v1",
      "title": "Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity",
      "title_zh": "基于图神经网络的空中交通管制员任务需求：一种可解释的空域复杂度研究方法",
      "authors": [
        "Edward Henderson",
        "Dewi Gould",
        "Richard Everson",
        "George De Ath",
        "Nick Pepper"
      ],
      "abstract": "Real-time assessment of near-term Air Traffic Controller (ATCO) task demand is a critical challenge in an increasingly crowded airspace, as existing complexity metrics often fail to capture nuanced operational drivers beyond simple aircraft counts. This work introduces an interpretable Graph Neural Network (GNN) framework to address this gap. Our attention-based model predicts the number of upcoming clearances, the instructions issued to aircraft by ATCOs, from interactions within static traffic scenarios. Crucially, we derive an interpretable, per-aircraft task demand score by systematically ablating aircraft and measuring the impact on the model's predictions. Our framework significantly outperforms an ATCO-inspired heuristic and is a more reliable estimator of scenario complexity than established baselines. The resulting tool can attribute task demand to specific aircraft, offering a new way to analyse and understand the drivers of complexity for applications in controller training and airspace redesign.",
      "tldr_zh": "该研究针对现有空域复杂度指标难以捕捉空中交通管制员（ATCO）任务需求细微驱动因素的问题，提出了一种可解释的图神经网络（GNN）框架。该框架利用注意力机制（attention-based model）分析静态交通场景中的航空器交互，从而预测管制员即将发布的管制指令（clearances）数量。研究通过系统地消融（ablating）航空器并测量其对模型预测的影响，推导出针对单架航空器的可解释任务需求得分。实验结果显示，该模型在预测表现上显著优于传统的启发式方法和基线模型，能更可靠地评估空域复杂度。这一工具实现了将任务需求归因于特定航空器，为管制员培训和空域优化设计提供了全新的分析维度。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Author Accepted Manuscript version of paper at the AIAA AVIATION Forum 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13423v1",
      "published_date": "2025-07-17 17:02:42 UTC",
      "updated_date": "2025-07-17 17:02:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:14:58.251301+00:00"
    },
    {
      "arxiv_id": "2507.13290v2",
      "title": "Towards Formal Verification of LLM-Generated Code from Natural Language Prompts",
      "title_zh": "迈向自然语言提示生成的LLM代码形式化验证",
      "authors": [
        "Aaron Councilman",
        "David Jiahao Fu",
        "Aryan Gupta",
        "Chengxiao Wang",
        "David Grove",
        "Yu-Xiong Wang",
        "Vikram Adve"
      ],
      "abstract": "In the past few years LLMs have emerged as a tool that can aid programmers by taking natural language descriptions and generating code based on it. However, the reliability of LLM code generation and current validation techniques for it are far from strong enough to be used for mission-critical or safety-critical applications. In this work we explore ways to offer formal guarantees of correctness to LLM generated code; such guarantees could improve the quality of general AI Code Assistants and support their use for critical applications. To address this challenge we propose to incorporate a Formal Query Language that can represent a user's intent in a formally defined but natural language-like manner that a user can confirm matches their intent. We then have a formal specification of the user intent which we can use to verify that LLM-generated code matches the user's intent. We implement these ideas in our system, Astrogator, for the Ansible programming language, widely used for system administration, including for critical systems. The system includes an intuitive formal query language, a calculus for representing the behavior of Ansible programs, and a symbolic interpreter and a unification algorithm which together are used for the verification. A key innovation in Astrogator is the use of a Knowledge Base to capture system-specific implementation dependencies that greatly reduce the need for system knowledge in expressing formal queries. On a benchmark suite of 21 code-generation tasks, our verifier is able to verify correct code in 83% of cases and identify incorrect code in 92%.",
      "tldr_zh": "该研究针对大语言模型(LLMs)生成的代码在关键任务应用中可靠性不足的问题，探索了提供形式化正确性保证的方法，并开发了针对Ansible语言的验证系统Astrogator。该系统引入了一种类自然语言的形式化查询语言(Formal Query Language)来精确表达用户意图，并结合Ansible行为演算法、符号解释器(symbolic interpreter)和统一算法(unification algorithm)实现自动化验证。Astrogator的核心创新在于利用知识库(Knowledge Base)捕获系统特定的实现依赖，显著降低了表达形式化查询时对专业系统知识的需求。实验结果显示，该验证器在基准测试中能够验证83%的正确代码并识别出92%的错误代码，为提升AI代码助手在安全敏感领域的可靠性提供了有效支撑。",
      "categories": [
        "cs.PL",
        "cs.AI"
      ],
      "primary_category": "cs.PL",
      "comment": "28 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.13290v2",
      "published_date": "2025-07-17 16:54:42 UTC",
      "updated_date": "2025-11-20 21:09:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:01.690633+00:00"
    },
    {
      "arxiv_id": "2507.13277v1",
      "title": "Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour",
      "title_zh": "仿真四足机器人导航强化学习算法评估：受导盲犬行为启发的对比研究",
      "authors": [
        "Emma M. A. Harrison"
      ],
      "abstract": "Robots are increasingly integrated across industries, particularly in healthcare. However, many valuable applications for quadrupedal robots remain overlooked. This research explores the effectiveness of three reinforcement learning algorithms in training a simulated quadruped robot for autonomous navigation and obstacle avoidance. The goal is to develop a robotic guide dog simulation capable of path following and obstacle avoidance, with long-term potential for real-world assistance to guide dogs and visually impaired individuals. It also seeks to expand research into medical 'pets', including robotic guide and alert dogs.\n  A comparative analysis of thirteen related research papers shaped key evaluation criteria, including collision detection, pathfinding algorithms, sensor usage, robot type, and simulation platforms. The study focuses on sensor inputs, collision frequency, reward signals, and learning progression to determine which algorithm best supports robotic navigation in complex environments.\n  Custom-made environments were used to ensure fair evaluation of all three algorithms under controlled conditions, allowing consistent data collection. Results show that Proximal Policy Optimization (PPO) outperformed Deep Q-Network (DQN) and Q-learning across all metrics, particularly in average and median steps to goal per episode.\n  By analysing these results, this study contributes to robotic navigation, AI and medical robotics, offering insights into the feasibility of AI-driven quadruped mobility and its role in assistive robotics.",
      "tldr_zh": "本研究旨在评估强化学习(Reinforcement Learning)算法在模拟四足机器人自主导航中的有效性，灵感源自导盲犬(Guide Dog)行为，以探索其在辅助医疗和视障人士辅助领域的应用潜力。研究通过构建自定义模拟环境，对比分析了近端策略优化(Proximal Policy Optimization, PPO)、深度Q网络(Deep Q-Network, DQN)和Q学习(Q-learning)三种算法在路径追踪与避障任务中的表现。评估过程重点考量了传感器输入、碰撞频率、奖励信号及学习进度等关键指标，确保了算法在复杂环境下的导航能力得到全面衡量。实验结果证明，PPO在所有评价指标上均显著优于DQN和Q-learning，尤其在到达目标所需的平均步数和中位步数方面展现出更优的效率。该研究为AI驱动的四足机器人移动技术提供了实证支持，并对未来医疗机器人(Medical Robotics)及智能辅助设备的设计与开发具有重要的参考价值。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13277v1",
      "published_date": "2025-07-17 16:38:14 UTC",
      "updated_date": "2025-07-17 16:38:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:06.441425+00:00"
    },
    {
      "arxiv_id": "2507.13275v1",
      "title": "Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management",
      "title_zh": "TalentCLEF 2025 概览：面向人力资本管理的技能与职位名称智能",
      "authors": [
        "Luis Gasco",
        "Hermenegildo Fabregat",
        "Laura García-Sardiña",
        "Paula Estrella",
        "Daniel Deniz",
        "Alvaro Rodrigo",
        "Rabih Zbib"
      ],
      "abstract": "Advances in natural language processing and large language models are driving a major transformation in Human Capital Management, with a growing interest in building smart systems based on language technologies for talent acquisition, upskilling strategies, and workforce planning. However, the adoption and progress of these technologies critically depend on the development of reliable and fair models, properly evaluated on public data and open benchmarks, which have so far been unavailable in this domain.\n  To address this gap, we present TalentCLEF 2025, the first evaluation campaign focused on skill and job title intelligence. The lab consists of two tasks: Task A - Multilingual Job Title Matching, covering English, Spanish, German, and Chinese; and Task B - Job Title-Based Skill Prediction, in English. Both corpora were built from real job applications, carefully anonymized, and manually annotated to reflect the complexity and diversity of real-world labor market data, including linguistic variability and gender-marked expressions.\n  The evaluations included monolingual and cross-lingual scenarios and covered the evaluation of gender bias.\n  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most systems relied on information retrieval techniques built with multilingual encoder-based models fine-tuned with contrastive learning, and several of them incorporated large language models for data augmentation or re-ranking. The results show that the training strategies have a larger effect than the size of the model alone. TalentCLEF provides the first public benchmark in this field and encourages the development of robust, fair, and transferable language technologies for the labor market.",
      "tldr_zh": "该研究介绍了 TalentCLEF 2025，这是首个专注于技能与职位名称智能化的评估活动，旨在解决人力资本管理 (Human Capital Management) 领域缺乏可靠公开基准和公平模型的问题。该活动设置了多语言职位名称匹配 (Multilingual Job Title Matching) 和基于职位名称的技能预测 (Job Title-Based Skill Prediction) 两项任务，使用的语料库源自真实、匿名且经过人工标注的求职申请。评估过程涵盖了单语言与跨语言场景，并专门针对性别偏见 (Gender Bias) 进行了分析。参赛系统主要采用基于对比学习 (Contrastive Learning) 微调的多语言编码器模型，并结合大语言模型 (LLMs) 进行数据增强或重排序 (Re-ranking)。实验结果显示，训练策略对性能的影响显著大于模型规模本身。作为该领域的首个公开基准，TalentCLEF 有效促进了劳动力市场中稳健、公平且可迁移语言技术的发展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13275v1",
      "published_date": "2025-07-17 16:33:57 UTC",
      "updated_date": "2025-07-17 16:33:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:18.693084+00:00"
    },
    {
      "arxiv_id": "2507.13266v3",
      "title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation",
      "title_zh": "QuestA：通过问题增强拓展 LLMs 的推理能力",
      "authors": [
        "Jiazheng Li",
        "Hongzhou Lin",
        "Hong Lu",
        "Kaiyue Wen",
        "Zaiwen Yang",
        "Jiaxuan Gao",
        "Yi Wu",
        "Jingzhao Zhang"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a central paradigm for training large language models (LLMs) in reasoning tasks. Yet recent studies question RL's ability to incentivize reasoning capacity beyond the base model. This raises a key challenge: how can RL be adapted to solve harder reasoning problems more effectively? To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 72.50% (+10.73%) on AIME24, 62.29% (+12.79%) on AIME25, and 41.67% (+10.11%) on HMMT25. Code, data and model are available at https://github.com/foreverlasting1202/QuestA.",
      "tldr_zh": "该研究提出了QuestA，一种通过问题增强(Question Augmentation)扩展大语言模型(LLMs)推理能力的策略，旨在解决强化学习(RL)在提升模型推理能力方面的局限性。该方法在RL训练过程中引入部分解(partial solutions)以降低问题难度，从而为模型提供更具信息量的学习信号。实验表明，QuestA在数学推理任务中显著提高了pass@1和pass@k指标，尤其在标准RL难以处理的极端难题上表现优异。通过在DeepScaleR和OpenMath Nemotron等强开源模型上进行持续改进，QuestA在仅使用1.5B参数规模的情况下，于AIME24、AIME25和HMMT25基准测试中分别取得了72.50%、62.29%和41.67%的新SOTA结果。该研究证明了QuestA能够有效增强小规模模型的逻辑推理上限，并已开源相关代码、数据和模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.13266v3",
      "published_date": "2025-07-17 16:21:47 UTC",
      "updated_date": "2025-09-30 04:42:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:12.886679+00:00"
    },
    {
      "arxiv_id": "2507.13264v1",
      "title": "Voxtral",
      "title_zh": "Voxtral",
      "authors": [
        "Alexander H. Liu",
        "Andy Ehrenberg",
        "Andy Lo",
        "Clément Denoix",
        "Corentin Barreau",
        "Guillaume Lample",
        "Jean-Malo Delignon",
        "Khyathi Raghavi Chandu",
        "Patrick von Platen",
        "Pavankumar Reddy Muddireddy",
        "Sanchit Gandhi",
        "Soham Ghosh",
        "Srijan Mishra",
        "Thomas Foubert",
        "Abhinav Rastogi",
        "Adam Yang",
        "Albert Q. Jiang",
        "Alexandre Sablayrolles",
        "Amélie Héliou",
        "Amélie Martin",
        "Anmol Agarwal",
        "Antoine Roux",
        "Arthur Darcet",
        "Arthur Mensch",
        "Baptiste Bout",
        "Baptiste Rozière",
        "Baudouin De Monicault",
        "Chris Bamford",
        "Christian Wallenwein",
        "Christophe Renaudin",
        "Clémence Lanfranchi",
        "Darius Dabert",
        "Devendra Singh Chaplot",
        "Devon Mizelle",
        "Diego de las Casas",
        "Elliot Chane-Sane",
        "Emilien Fugier",
        "Emma Bou Hanna",
        "Gabrielle Berrada",
        "Gauthier Delerce",
        "Gauthier Guinet",
        "Georgii Novikov",
        "Guillaume Martin",
        "Himanshu Jaju",
        "Jan Ludziejewski",
        "Jason Rute",
        "Jean-Hadrien Chabran",
        "Jessica Chudnovsky",
        "Joachim Studnia",
        "Joep Barmentlo",
        "Jonas Amar",
        "Josselin Somerville Roberts",
        "Julien Denize",
        "Karan Saxena",
        "Karmesh Yadav",
        "Kartik Khandelwal",
        "Kush Jain",
        "Lélio Renard Lavaud",
        "Léonard Blier",
        "Lingxiao Zhao",
        "Louis Martin",
        "Lucile Saulnier",
        "Luyu Gao",
        "Marie Pellat",
        "Mathilde Guillaumin",
        "Mathis Felardos",
        "Matthieu Dinot",
        "Maxime Darrin",
        "Maximilian Augustin",
        "Mickaël Seznec",
        "Neha Gupta",
        "Nikhil Raghuraman",
        "Olivier Duchenne",
        "Patricia Wang",
        "Patryk Saffer",
        "Paul Jacob",
        "Paul Wambergue",
        "Paula Kurylowicz",
        "Philomène Chagniot",
        "Pierre Stock",
        "Pravesh Agrawal",
        "Rémi Delacourt",
        "Romain Sauvestre",
        "Roman Soletskyi",
        "Sagar Vaze",
        "Sandeep Subramanian",
        "Saurabh Garg",
        "Shashwat Dalal",
        "Siddharth Gandhi",
        "Sumukh Aithal",
        "Szymon Antoniak",
        "Teven Le Scao",
        "Thibault Schueller",
        "Thibaut Lavril",
        "Thomas Robert",
        "Thomas Wang",
        "Timothée Lacroix",
        "Tom Bewley",
        "Valeriia Nemychnikova",
        "Victor Paltz",
        "Virgile Richard",
        "Wen-Ding Li",
        "William Marshall",
        "Xuanyu Zhang",
        "Yihan Wan",
        "Yunhao Tang"
      ],
      "abstract": "We present Voxtral Mini and Voxtral Small, two multimodal audio chat models. Voxtral is trained to comprehend both spoken audio and text documents, achieving state-of-the-art performance across a diverse range of audio benchmarks, while preserving strong text capabilities. Voxtral Small outperforms a number of closed-source models, while being small enough to run locally. A 32K context window enables the model to handle audio files up to 40 minutes in duration and long multi-turn conversations. We also contribute three benchmarks for evaluating speech understanding models on knowledge and trivia. Both Voxtral models are released under Apache 2.0 license.",
      "tldr_zh": "该研究推出了 Voxtral Mini 和 Voxtral Small 两个多模态音频聊天模型，旨在协同理解语音音频和文本输入。Voxtral 在多项音频基准测试中达到了 state-of-the-art 性能，同时保留了卓越的文本处理能力。其中 Voxtral Small 的表现超越了多款闭源模型，并支持在本地设备上运行。得益于 32K 的 context window，该模型能够处理长达 40 分钟的音频文件及长篇多轮对话。研究还贡献了三个针对语音理解模型知识储备评估的 benchmarks。目前 Voxtral 模型已基于 Apache 2.0 协议开源发布。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "17 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.13264v1",
      "published_date": "2025-07-17 16:17:37 UTC",
      "updated_date": "2025-07-17 16:17:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:19.736022+00:00"
    },
    {
      "arxiv_id": "2507.13263v3",
      "title": "From Sorting Algorithms to Scalable Kernels: Bayesian Optimization in High-Dimensional Permutation Spaces",
      "title_zh": "从排序算法到可扩展核函数：高维排列空间中的贝叶斯优化",
      "authors": [
        "Zikai Xie",
        "Linjiang Chen"
      ],
      "abstract": "Bayesian Optimization (BO) is a powerful tool for black-box optimization, but its application to high-dimensional permutation spaces is severely limited by the challenge of defining scalable representations. The current state-of-the-art BO approach for permutation spaces relies on an exhaustive $Ω(n^2)$ pairwise comparison, inducing a dense representation that is impractical for large-scale permutations. To break this barrier, we introduce a novel framework for generating efficient permutation representations via kernel functions derived from sorting algorithms. Within this framework, the Mallows kernel can be viewed as a special instance derived from enumeration sort. Further, we introduce the \\textbf{Merge Kernel} , which leverages the divide-and-conquer structure of merge sort to produce a compact, $Θ(n\\log n)$ to achieve the lowest possible complexity with no information loss and effectively capture permutation structure. Our central thesis is that the Merge Kernel performs competitively with the Mallows kernel in low-dimensional settings, but significantly outperforms it in both optimization performance and computational efficiency as the dimension $n$ grows. Extensive evaluations on various permutation optimization benchmarks confirm our hypothesis, demonstrating that the Merge Kernel provides a scalable and more effective solution for Bayesian optimization in high-dimensional permutation spaces, thereby unlocking the potential for tackling previously intractable problems such as large-scale feature ordering and combinatorial neural architecture search.",
      "tldr_zh": "该研究针对贝叶斯优化(Bayesian Optimization)在处理高维排列空间(High-Dimensional Permutation Spaces)时受限于表示法扩展性的问题，提出了一种基于排序算法(Sorting Algorithms)导出核函数(Kernel Functions)的创新框架。研究的核心贡献是引入了基于归并排序(Merge Sort)分治思想的归并核(Merge Kernel)，将排列表示的复杂度从传统方法的 $Ω(n^2)$ 降低至 $Θ(n \\log n)$，在无信息损失的情况下显著提升了计算效率。实验结果表明，Merge Kernel 在低维环境下与主流的 Mallows kernel 性能相当，但在高维场景下的优化表现和运行效率均具有显著优势。该方法成功解决了大规模排列优化中因维度过高导致的计算不可行问题，为大规模特征排序(Feature Ordering)和组合神经架构搜索(Neural Architecture Search)等实际应用提供了高效且可扩展的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, submitted to ICLR-26",
      "pdf_url": "https://arxiv.org/pdf/2507.13263v3",
      "published_date": "2025-07-17 16:12:39 UTC",
      "updated_date": "2025-09-25 02:43:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:36.013546+00:00"
    },
    {
      "arxiv_id": "2507.13260v1",
      "title": "Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy",
      "title_zh": "基于近似正交微调策略的预训练 Vision Transformer 高效适配",
      "authors": [
        "Yiting Yang",
        "Hao Luo",
        "Yuan Sun",
        "Qingsen Yan",
        "Haokui Zhang",
        "Wei Dong",
        "Guoqing Wang",
        "Peng Wang",
        "Yang Yang",
        "Hengtao Shen"
      ],
      "abstract": "A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViT) involves freezing the majority of the backbone parameters and solely learning low-rank adaptation weight matrices to accommodate downstream tasks. These low-rank matrices are commonly derived through the multiplication structure of down-projection and up-projection matrices, exemplified by methods such as LoRA and Adapter. In this work, we observe an approximate orthogonality among any two row or column vectors within any weight matrix of the backbone parameters; however, this property is absent in the vectors of the down/up-projection matrices. Approximate orthogonality implies a reduction in the upper bound of the model's generalization error, signifying that the model possesses enhanced generalization capability. If the fine-tuned down/up-projection matrices were to exhibit this same property as the pre-trained backbone matrices, could the generalization capability of fine-tuned ViTs be further augmented? To address this question, we propose an Approximately Orthogonal Fine-Tuning (AOFT) strategy for representing the low-rank weight matrices. This strategy employs a single learnable vector to generate a set of approximately orthogonal vectors, which form the down/up-projection matrices, thereby aligning the properties of these matrices with those of the backbone. Extensive experimental results demonstrate that our method achieves competitive performance across a range of downstream image classification tasks, confirming the efficacy of the enhanced generalization capability embedded in the down/up-projection matrices.",
      "tldr_zh": "该研究针对预训练 Vision Transformer (ViT) 的参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT) 提出了一种近似正交微调 (Approximately Orthogonal Fine-Tuning, AOFT) 策略。研究者观察到预训练主干网络参数的向量间存在近似正交性，这种特性有助于降低模型泛化误差的上界，而 LoRA 和 Adapter 等传统方法生成的低秩矩阵则缺乏这一属性。AOFT 通过使用单个可学习向量生成一组近似正交向量来构建下采样和上采样投影矩阵，从而使微调部分的数学属性与主干网络保持一致。实验结果表明，该方法在多项下游图像分类任务中取得了极具竞争力的性能，验证了通过增强投影矩阵的近似正交性来提升模型泛化能力的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is accepted by ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13260v1",
      "published_date": "2025-07-17 16:09:05 UTC",
      "updated_date": "2025-07-17 16:09:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:35.507532+00:00"
    },
    {
      "arxiv_id": "2507.13255v3",
      "title": "Automating Steering for Safe Multimodal Large Language Models",
      "title_zh": "面向安全多模态大语言模型的自动化引导",
      "authors": [
        "Lyucheng Wu",
        "Mengru Wang",
        "Ziwen Xu",
        "Tri Cao",
        "Nay Oo",
        "Bryan Hooi",
        "Shumin Deng"
      ],
      "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.",
      "tldr_zh": "该研究提出了 AutoSteer，一种针对多模态大语言模型 (MLLMs) 的模块化且自适应的推理时干预 (inference-time intervention) 技术，旨在应对对抗性多模态输入带来的安全挑战。AutoSteer 无需对底层模型进行任何微调，核心组件包括安全意识分数 (Safety Awareness Score, SAS)、自适应安全探测器 (adaptive safety prober) 以及轻量级拒绝头 (Refusal Head)。该框架利用 SAS 自动识别模型内部与安全相关的关键层，通过探测器根据中间表示预测输出毒性的可能性，并在检测到风险时由拒绝头干预生成过程。实验在 LLaVA-OV 和 Chameleon 模型上证明，AutoSteer 能显著降低文本、视觉及跨模态威胁的攻击成功率 (Attack Success Rate, ASR)，且不损耗模型的通用能力。这为更安全地部署多模态人工智能系统提供了一个具有实用性和可解释性的有效解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 Main Conference. 23 pages (8+ for main); 25 figures; 1 table",
      "pdf_url": "https://arxiv.org/pdf/2507.13255v3",
      "published_date": "2025-07-17 16:04:55 UTC",
      "updated_date": "2025-09-23 03:15:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:38.638824+00:00"
    },
    {
      "arxiv_id": "2507.13238v2",
      "title": "Multilingual LLMs Are Not Multilingual Thinkers: Evidence from Hindi Analogy Evaluation",
      "title_zh": "多语言大语言模型并非多语言思考者：来自印地语类比评估的证据",
      "authors": [
        "Ashray Gupta",
        "Rohan Joseph",
        "Sunny Rai"
      ],
      "abstract": "Analogies test a model's ability to infer implicit relationships between concepts, making them a key benchmark for evaluating reasoning capabilities. While large language models (LLMs) are widely evaluated for reasoning in English, their abilities in Indic languages remain understudied, limiting our understanding of whether these models generalize across languages. To address this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405 multiple-choice questions sourced from Indian government exams. We benchmark state-of-the-art multilingual LLMs using various prompting strategies and introduce a grounded Chain of Thought approach that leverages cognitive theories of analogical reasoning. This approach improves model performance on Hindi analogy questions. Our experiments show that models perform best with English prompts, irrespective of the prompting strategy. Our test set addresses the lack of a critical resource to evaluate LLM reasoning capabilities in Hindi.",
      "tldr_zh": "该研究探讨了多语言大语言模型(LLMs)在类比推理中的跨语言泛化能力，重点关注了目前研究较少的印地语(Hindi)领域。研究团队推出了印地语类比测试集(HATS)，其中包含从印度政府考试中收集的405道多项选择题，填补了评估模型推理能力的关键资源空白。通过对当前先进模型进行基准测试，研究提出了一种结合类比推理认知理论的接地链式思维(Grounded Chain of Thought)方法，并显著提升了模型性能。实验结果表明，无论采用何种提示策略，模型在英语提示(English prompts)下的表现始终优于其他语言，揭示了多语言模型在非英语思维逻辑上的局限性。这一发现强调了虽然LLMs具备多语言支持，但在深层推理层面尚未实现真正的跨语言泛化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13238v2",
      "published_date": "2025-07-17 15:47:49 UTC",
      "updated_date": "2025-07-23 21:50:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:47.532170+00:00"
    },
    {
      "arxiv_id": "2507.13231v3",
      "title": "VITA: Vision-to-Action Flow Matching Policy",
      "title_zh": "VITA：视觉-动作流匹配策略",
      "authors": [
        "Dechen Gao",
        "Boqi Zhao",
        "Andrew Lee",
        "Ian Chuang",
        "Hanchu Zhou",
        "Hang Wang",
        "Zhe Zhao",
        "Junshan Zhang",
        "Iman Soltani"
      ],
      "abstract": "Conventional flow matching and diffusion-based policies sample through iterative denoising from standard noise distributions (e.g., Gaussian), and require conditioning modules to repeatedly incorporate visual information during the generative process, incurring substantial time and memory overhead. To reduce the complexity, we develop VITA(VIsion-To-Action policy), a noise-free and conditioning-free flow matching policy learning framework that directly flows from visual representations to latent actions. Since the source of the flow is visually grounded, VITA eliminates the need of visual conditioning during generation. As expected, bridging vision and action is challenging, because actions are lower-dimensional, less structured, and sparser than visual representations; moreover, flow matching requires the source and target to have the same dimensionality. To overcome this, we introduce an action autoencoder that maps raw actions into a structured latent space aligned with visual latents, trained jointly with flow matching. To further prevent latent space collapse, we propose flow latent decoding, which anchors the latent generation process by backpropagating the action reconstruction loss through the flow matching ODE (ordinary differential equation) solving steps. We evaluate VITA on 9 simulation and 5 real-world tasks from ALOHA and Robomimic. VITA achieves 1.5x-2x faster inference compared to conventional methods with conditioning modules, while outperforming or matching state-of-the-art policies. Codes, datasets, and demos are available at our project page: https://ucd-dare.github.io/VITA/.",
      "tldr_zh": "该研究提出了 VITA (Vision-to-Action Flow Matching Policy)，一种无噪声且无需反复引入调节模块的流匹配 (Flow Matching) 策略框架，旨在解决传统扩散模型和流匹配策略在生成过程中因频繁处理视觉信息而导致的巨大时间与内存开销。VITA 直接建立从视觉表示到动作潜空间的流映射，利用视觉特征作为流的起始点，从而消除了推理阶段对视觉条件的依赖。为了克服视觉与动作维度不一致及结构差异的挑战，该框架采用动作自编码器 (Action Autoencoder) 将原始动作映射至与视觉对齐的结构化潜空间，并与流匹配过程进行联合训练。同时，研究引入了流潜解码 (Flow Latent Decoding) 技术，通过常微分方程 (ODE) 求解步骤回传动作重建损失，有效防止了潜空间坍缩并锚定了生成过程。在 ALOHA 和 Robomimic 的多项仿真及真实世界任务中，VITA 的推理速度达到了传统方法的 1.5 至 2 倍。实验结果表明，该模型在性能上达到或超越了现有最先进的策略，为高效、实时的机器人操作提供了新的技术路径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://ucd-dare.github.io/VITA/ Code: https://github.com/ucd-dare/VITA",
      "pdf_url": "https://arxiv.org/pdf/2507.13231v3",
      "published_date": "2025-07-17 15:41:57 UTC",
      "updated_date": "2025-12-01 12:22:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:46.893733+00:00"
    },
    {
      "arxiv_id": "2507.13229v4",
      "title": "{S\\textsuperscript{2}M\\textsuperscript{2}}: Scalable Stereo Matching Model for Reliable Depth Estimation",
      "title_zh": "S²M²：面向可靠深度估计的可扩展立体匹配模型",
      "authors": [
        "Junhong Min",
        "Youngpil Jeon",
        "Jimin Kim",
        "Minyong Choi"
      ],
      "abstract": "The pursuit of a generalizable stereo matching model, capable of performing well across varying resolutions and disparity ranges without dataset-specific fine-tuning, has revealed a fundamental trade-off. Iterative local search methods achieve high scores on constrained benchmarks, but their core mechanism inherently limits the global consistency required for true generalization. However, global matching architectures, while theoretically more robust, have historically been rendered infeasible by prohibitive computational and memory costs. We resolve this dilemma with {S\\textsuperscript{2}M\\textsuperscript{2}}: a global matching architecture that achieves state-of-the-art accuracy and high efficiency without relying on cost volume filtering or deep refinement stacks. Our design integrates a multi-resolution transformer for robust long-range correspondence, trained with a novel loss function that concentrates probability on feasible matches. This approach enables a more robust joint estimation of disparity, occlusion, and confidence. {S\\textsuperscript{2}M\\textsuperscript{2}} establishes a new state of the art on Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods in most metrics while reconstructing high-quality details with competitive efficiency.",
      "tldr_zh": "该研究提出了 S^2M^2，一种可扩展的全球匹配 (Global Matching) 架构，旨在解决立体匹配模型在处理多变分辨率和视差范围时，泛化能力与计算成本之间的权衡难题。该模型摒弃了传统的代价体积过滤 (Cost Volume Filtering) 或深度细化堆栈 (Deep Refinement Stacks)，转而采用多分辨率 Transformer (Multi-resolution Transformer) 来构建稳健的远程对应关系。通过引入一种能够将概率集中在可行匹配上的新型损失函数，S^2M^2 实现了对视差 (Disparity)、遮挡 (Occlusion) 和置信度 (Confidence) 的同步稳健估计。实验结果表明，该模型在 Middlebury v3 和 ETH3D 基准测试中均达到了目前的领先水平 (State-of-the-art)，在多数指标上显著优于前序方法。S^2M^2 在重建高质量深度的细节方面表现出色，同时保持了极具竞争力的运行效率，证明了其在可靠深度估计任务中的卓越性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures, ICCV accepted paper",
      "pdf_url": "https://arxiv.org/pdf/2507.13229v4",
      "published_date": "2025-07-17 15:40:18 UTC",
      "updated_date": "2025-10-11 14:41:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:53.106744+00:00"
    },
    {
      "arxiv_id": "2507.13221v1",
      "title": "Synthesizing Reality: Leveraging the Generative AI-Powered Platform Midjourney for Construction Worker Detection",
      "title_zh": "合成现实：利用生成式人工智能平台 Midjourney 进行建筑工人检测",
      "authors": [
        "Hongyang Zhao",
        "Tianyu Liang",
        "Sina Davari",
        "Daeho Kim"
      ],
      "abstract": "While recent advancements in deep neural networks (DNNs) have substantially enhanced visual AI's capabilities, the challenge of inadequate data diversity and volume remains, particularly in construction domain. This study presents a novel image synthesis methodology tailored for construction worker detection, leveraging the generative-AI platform Midjourney. The approach entails generating a collection of 12,000 synthetic images by formulating 3000 different prompts, with an emphasis on image realism and diversity. These images, after manual labeling, serve as a dataset for DNN training. Evaluation on a real construction image dataset yielded promising results, with the model attaining average precisions (APs) of 0.937 and 0.642 at intersection-over-union (IoU) thresholds of 0.5 and 0.5 to 0.95, respectively. Notably, the model demonstrated near-perfect performance on the synthetic dataset, achieving APs of 0.994 and 0.919 at the two mentioned thresholds. These findings reveal both the potential and weakness of generative AI in addressing DNN training data scarcity.",
      "tldr_zh": "该研究提出了一种利用生成式人工智能(Generative AI)平台 Midjourney 合成图像的方法，旨在解决建筑领域深度神经网络(DNNs)训练中数据多样性和数量不足的挑战。通过编写 3000 个不同的提示词(Prompts)，研究生成了 12,000 张具有高度真实感和多样性的合成图像，并经过人工标注构建了用于建筑工人检测的数据集。实验结果显示，该模型在真实建筑场景数据集上的评估表现优异，在交并比(IoU)阈值为 0.5 和 0.5-0.95 时，平均精度(APs)分别达到 0.937 和 0.642。此外，模型在合成数据集上表现极佳，对应的 APs 分别达到 0.994 和 0.919。这些发现表明，利用 Midjourney 等平台生成合成数据是解决视觉 AI 数据稀缺问题的有效途径，同时也揭示了该技术在处理复杂现实环境时的潜在局限性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This work was presented at ASCE International Conference on Computing in Civil Engineering (i3CE) 2024 and is currently under consideration for publication in ASCE proceedings",
      "pdf_url": "https://arxiv.org/pdf/2507.13221v1",
      "published_date": "2025-07-17 15:35:27 UTC",
      "updated_date": "2025-07-17 15:35:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:53.542179+00:00"
    },
    {
      "arxiv_id": "2507.13208v1",
      "title": "Higher-Order Pattern Unification Modulo Similarity Relations",
      "title_zh": "模相似关系的高阶模式合一",
      "authors": [
        "Besik Dundua",
        "Temur Kutsia"
      ],
      "abstract": "The combination of higher-order theories and fuzzy logic can be useful in decision-making tasks that involve reasoning across abstract functions and predicates, where exact matches are often rare or unnecessary. Developing efficient reasoning and computational techniques for such a combined formalism presents a significant challenge. In this paper, we adopt a more straightforward approach aiming at integrating two well-established and computationally well-behaved components: higher-order patterns on one side and fuzzy equivalences expressed through similarity relations based on minimum T-norm on the other. We propose a unification algorithm for higher-order patterns modulo these similarity relations and prove its termination, soundness, and completeness. This unification problem, like its crisp counterpart, is unitary. The algorithm computes a most general unifier with the highest degree of approximation when the given terms are unifiable.",
      "tldr_zh": "该研究探讨了高阶理论(higher-order theories)与模糊逻辑(fuzzy logic)的结合，旨在解决决策任务中涉及抽象函数和谓词推理时精确匹配难以实现的问题。论文通过集成高阶模式(higher-order patterns)与基于最小T-范数(minimum T-norm)的相似关系(similarity relations)，提出了一种针对模相似关系的高阶模式统一算法。作者在理论上证明了该算法具有终止性(termination)、可靠性(soundness)和完备性(completeness)。研究发现，该统一问题与其对应的精确(crisp)版本一样具有单子性(unitary)特征。当给定项可统一时，该算法能够计算出具有最高近似程度的最通用统一子(most general unifier)，为处理模糊环境下的复杂计算和推理任务提供了有效的形式化工具。",
      "categories": [
        "cs.AI",
        "cs.LO",
        "math.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.13208v1",
      "published_date": "2025-07-17 15:18:22 UTC",
      "updated_date": "2025-07-17 15:18:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:15:53.892900+00:00"
    },
    {
      "arxiv_id": "2507.13175v2",
      "title": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era",
      "title_zh": "黑盒已部署：LLM 时代人工道德主体的功能性准则",
      "authors": [
        "Matthew E. Brophy"
      ],
      "abstract": "The advancement of powerful yet opaque large language models (LLMs) necessitates a fundamental revision of the philosophical criteria used to evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the assumption of transparent architectures, which LLMs defy due to their stochastic outputs and opaque internal states. This paper argues that traditional ethical criteria are pragmatically obsolete for LLMs due to this mismatch. Engaging with core themes in the philosophy of technology, this paper proffers a revised set of ten functional criteria to evaluate LLM-based artificial moral agents: moral concordance, context sensitivity, normative integrity, metaethical awareness, system resilience, trustworthiness, corrigibility, partial transparency, functional autonomy, and moral imagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating Moral Agency through Large Language Systems), aim to steer AMAs toward greater alignment and beneficial societal integration in the coming years. We illustrate these criteria using hypothetical scenarios involving an autonomous public bus (APB) to demonstrate their practical applicability in morally salient contexts.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)的黑盒特性和不透明内部状态，指出传统的、基于透明架构假设的人工道德智能体(AMAs)评估标准在实践中已显过时。为此，作者提出了一套包含十项功能性标准的修订框架，用于评估所谓的“通过大语言系统模拟道德代理”(SMA-LLS)。这十项核心准则涵盖了moral concordance, context sensitivity, normative integrity, metaethical awareness, system resilience, trustworthiness, corrigibility, partial transparency, functional autonomy以及moral imagination。通过自动驾驶公交车(APB)的假设情景，研究演示了这些标准在处理道德敏感上下文时的实际适用性。该框架旨在引导LLM驱动的智能体实现更好的价值对齐，并促进其在未来社会中的有益整合。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "42 pages. Supplementary material included at end of article",
      "pdf_url": "https://arxiv.org/pdf/2507.13175v2",
      "published_date": "2025-07-17 14:39:29 UTC",
      "updated_date": "2025-07-25 21:09:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:16:02.539870+00:00"
    },
    {
      "arxiv_id": "2507.13171v2",
      "title": "Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback",
      "title_zh": "基于隐式人类反馈强化学习的人机对齐",
      "authors": [
        "Suzie Kim",
        "Hye-Bin Shin",
        "Seong-Whan Lee"
      ],
      "abstract": "Conventional reinforcement learning (RL) ap proaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, rein forcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, en abling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.",
      "tldr_zh": "该研究针对传统强化学习(RL)在稀疏奖励下需复杂手动设计，以及强化学习从人类反馈中学习(RLHF)显式交互负担重的问题，提出了一种基于隐式人类反馈的强化学习(RLIHF)框架。该框架利用非侵入性脑电图(EEG)信号中的错误相关电位(ErrPs)提供持续的隐式反馈，无需用户显式干预。通过预训练的解码器将原始EEG信号转化为概率奖励组件，智能体得以在稀疏外部奖励环境下实现有效的策略学习。在基于MuJoCo物理引擎的Kinova Gen2机器人手臂避障抓取实验中，采用EEG反馈训练的智能体表现出了与手动设计密集奖励(Dense Rewards)相当的性能。这一研究结果验证了利用隐式神经反馈在交互式机器人中实现可扩展且人类对齐(Human-aligned)强化学习的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to IEEE Int. Conf. Syst., Man, Cybern. (SMC) 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13171v2",
      "published_date": "2025-07-17 14:35:12 UTC",
      "updated_date": "2025-12-11 23:50:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:16:02.958572+00:00"
    },
    {
      "arxiv_id": "2507.13170v1",
      "title": "SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks",
      "title_zh": "SHIELD：一种用于抵御对抗攻击、实现鲁棒深度伪造检测的安全高度增强集成学习方法",
      "authors": [
        "Kutub Uddin",
        "Awais Khan",
        "Muhammad Umar Farooq",
        "Khalid Malik"
      ],
      "abstract": "Audio plays a crucial role in applications like speaker verification, voice-enabled smart devices, and audio conferencing. However, audio manipulations, such as deepfakes, pose significant risks by enabling the spread of misinformation. Our empirical analysis reveals that existing methods for detecting deepfake audio are often vulnerable to anti-forensic (AF) attacks, particularly those attacked using generative adversarial networks. In this article, we propose a novel collaborative learning method called SHIELD to defend against generative AF attacks. To expose AF signatures, we integrate an auxiliary generative model, called the defense (DF) generative model, which facilitates collaborative learning by combining input and output. Furthermore, we design a triplet model to capture correlations for real and AF attacked audios with real-generated and attacked-generated audios using auxiliary generative models. The proposed SHIELD strengthens the defense against generative AF attacks and achieves robust performance across various generative models. The proposed AF significantly reduces the average detection accuracy from 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild, and from 98.41% to 51.18% for HalfTruth for three different generative models. The proposed SHIELD mechanism is robust against AF attacks and achieves an average accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%, and 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and HalfTruth datasets, respectively.",
      "tldr_zh": "该研究针对音频 deepfake 检测系统易受生成对抗网络引发的反取证（anti-forensic, AF）攻击这一问题，提出了名为 SHIELD 的安全增强型协同学习方法。为了揭示 AF 特征，该框架引入了辅助的防御生成模型（defense generative model），通过结合输入与输出信息来促进协同学习。此外，研究还设计了一个三元组模型（triplet model），用于捕获真实音频、受攻击音频及其对应的生成版本之间的内在关联。实验结果表明，SHIELD 显著增强了对生成式 AF 攻击的防御能力，在 ASVspoof2019、In-the-Wild 和 HalfTruth 等多个数据集上均实现了超过 98% 的平均检测准确率。无论是在匹配还是不匹配的实验设置下，SHIELD 都展现出了极高的鲁棒性，有效解决了传统检测方法在面对抗性攻击时准确率大幅下降的缺陷。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13170v1",
      "published_date": "2025-07-17 14:33:54 UTC",
      "updated_date": "2025-07-17 14:33:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:16:19.605542+00:00"
    },
    {
      "arxiv_id": "2507.13169v1",
      "title": "Prompt Injection 2.0: Hybrid AI Threats",
      "title_zh": "提示词注入 2.0：混合型 AI 威胁",
      "authors": [
        "Jeremy McHugh",
        "Kristina Šekrst",
        "Jon Cefalu"
      ],
      "abstract": "Prompt injection attacks, where malicious input is designed to manipulate AI systems into ignoring their original instructions and following unauthorized commands instead, were first discovered by Preamble, Inc. in May 2022 and responsibly disclosed to OpenAI. Over the last three years, these attacks have continued to pose a critical security threat to LLM-integrated systems. The emergence of agentic AI systems, where LLMs autonomously perform multistep tasks through tools and coordination with other agents, has fundamentally transformed the threat landscape. Modern prompt injection attacks can now combine with traditional cybersecurity exploits to create hybrid threats that systematically evade traditional security controls. This paper presents a comprehensive analysis of Prompt Injection 2.0, examining how prompt injections integrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF), and other web security vulnerabilities to bypass traditional security measures. We build upon Preamble's foundational research and mitigation technologies, evaluating them against contemporary threats, including AI worms, multi-agent infections, and hybrid cyber-AI attacks. Our analysis incorporates recent benchmarks that demonstrate how traditional web application firewalls, XSS filters, and CSRF tokens fail against AI-enhanced attacks. We also present architectural solutions that combine prompt isolation, runtime security, and privilege separation with novel threat detection capabilities.",
      "tldr_zh": "该研究深入探讨了 Prompt Injection 2.0 这一演进中的混合 AI 威胁，重点分析了恶意输入如何操纵执行多步任务的自主代理 AI 系统（Agentic AI systems）。论文详细阐述了现代提示注入攻击如何与 Cross-Site Scripting (XSS)、Cross-Site Request Forgery (CSRF) 等传统网络安全漏洞相结合，形成能够系统性规避传统安全防御机制的复合型威胁。研究通过实验证明，传统的 Web 应用防火墙、XSS 过滤器和 CSRF 令牌在面对 AI 增强型攻击、AI 蠕虫（AI worms）以及多智能体感染时均告失效。基于这些发现，作者提出了一种综合性的架构解决方案，通过结合提示隔离（Prompt isolation）、运行时安全（Runtime security）和特权分离（Privilege separation）来增强防御。该方案还引入了新型威胁检测能力，为构建可抵御复杂 AI 网络攻击的系统提供了关键的理论依据与技术支撑。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13169v1",
      "published_date": "2025-07-17 14:33:36 UTC",
      "updated_date": "2025-07-17 14:33:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:16:19.752030+00:00"
    },
    {
      "arxiv_id": "2507.13162v2",
      "title": "Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models",
      "title_zh": "Orbis：克服驾驶世界模型中长时程预测的挑战",
      "authors": [
        "Arian Mousakhan",
        "Sudhanshu Mittal",
        "Silvio Galesso",
        "Karim Farid",
        "Thomas Brox"
      ],
      "abstract": "Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at https://lmb-freiburg.github.io/orbis.github.io/.",
      "tldr_zh": "该研究提出了Orbis模型，旨在解决自动驾驶世界模型在长时预测(long-horizon prediction)及复杂场景泛化方面面临的难题。该模型仅拥有469M参数并在280小时视频数据上进行训练，在不依赖地图、深度信息或多摄像头等额外监督传感器的情况下，实现了state-of-the-art性能。Orbis在转向操作和城市交通等极具挑战性的场景中表现尤为突出，展示了强大的生成能力。此外，研究者通过构建一种混合分词器(hybrid tokenizer)，深入对比了离散Token模型与基于Flow Matching的连续模型在性能上的差异。实验结果表明，连续自回归模型(continuous autoregressive model)在设计选择上更具鲁棒性，且在处理复杂驾驶任务时比离散模型更为强大。目前，该项目的代码、模型及定性研究结果已向公众开放，为自动驾驶领域的世界模型构建提供了重要参考。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://lmb-freiburg.github.io/orbis.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2507.13162v2",
      "published_date": "2025-07-17 14:29:34 UTC",
      "updated_date": "2025-12-11 10:05:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:16:22.484216+00:00"
    },
    {
      "arxiv_id": "2507.14239v1",
      "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation",
      "title_zh": "CCL-XCoT：一种缓解幻觉生成的高效跨语言知识迁移方法",
      "authors": [
        "Weihua Zheng",
        "Roy Ka-Wei Lee",
        "Zhengyuan Liu",
        "Kui Wu",
        "AiTi Aw",
        "Bowei Zou"
      ],
      "abstract": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization across languages, yet they remain prone to hallucinations, especially in low-resource languages, due to training data imbalances. These hallucinations, which include inaccurate or fabricated outputs, are particularly problematic in domain-specific generation tasks (Chataigner et al., 2024). To address this challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for mitigating hallucination in MLLMs. Our approach first enhances cross-lingual semantic alignment through curriculum-based contrastive learning combined with next-token prediction during continued pre-training. Building on this foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting strategy during instruction fine-tuning, which guides the model to reason in a high-resource language before generating answers in the target low-resource language. Experimental results show that CCL-XCoT reduces hallucination rates by up to 62% and substantially improves factual knowledge transfer across language pairs, without relying on external retrieval or multi-model ensembles.",
      "tldr_zh": "该研究针对多语言大语言模型(MLLMs)在低资源语言中由于训练数据不平衡导致的幻觉(hallucinations)生成问题，提出了名为CCL-XCoT的跨语言知识迁移方法。该方法包含两阶段微调框架，首先利用基于课程的对比学习(curriculum-based contrastive learning)与下文预测任务增强跨语言语义对齐。接着在指令微调阶段引入跨语言链式思维(XCoT)提示策略，引导模型在高资源语言中进行推理后再输出目标低资源语言结果。实验数据证明，CCL-XCoT在不借助外部检索或集成模型的情况下，可将幻觉率降低多达62%，并显著提升事实性知识的跨语言迁移效率。该方案为解决特定领域生成任务中的信息误报提供了强有力的技术支撑。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14239v1",
      "published_date": "2025-07-17 14:25:24 UTC",
      "updated_date": "2025-07-17 14:25:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:16:27.992737+00:00"
    },
    {
      "arxiv_id": "2507.13158v1",
      "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities",
      "title_zh": "逆向强化学习与大语言模型后训练：基础、进展与机遇",
      "authors": [
        "Hao Sun",
        "Mihaela van der Schaar"
      ],
      "abstract": "In the era of Large Language Models (LLMs), alignment has emerged as a fundamental yet challenging problem in the pursuit of more reliable, controllable, and capable machine intelligence. The recent success of reasoning models and conversational AI systems has underscored the critical role of reinforcement learning (RL) in enhancing these systems, driving increased research interest at the intersection of RL and LLM alignment. This paper provides a comprehensive review of recent advances in LLM alignment through the lens of inverse reinforcement learning (IRL), emphasizing the distinctions between RL techniques employed in LLM alignment and those in conventional RL tasks. In particular, we highlight the necessity of constructing neural reward models from human data and discuss the formal and practical implications of this paradigm shift. We begin by introducing fundamental concepts in RL to provide a foundation for readers unfamiliar with the field. We then examine recent advances in this research agenda, discussing key challenges and opportunities in conducting IRL for LLM alignment. Beyond methodological considerations, we explore practical aspects, including datasets, benchmarks, evaluation metrics, infrastructure, and computationally efficient training and inference techniques. Finally, we draw insights from the literature on sparse-reward RL to identify open questions and potential research directions. By synthesizing findings from diverse studies, we aim to provide a structured and critical overview of the field, highlight unresolved challenges, and outline promising future directions for improving LLM alignment through RL and IRL techniques.",
      "tldr_zh": "该研究综述了逆强化学习(Inverse Reinforcement Learning, IRL)在大语言模型(Large Language Model, LLM)后训练对齐中的应用与进展。文章对比了LLM对齐与传统强化学习(Reinforcement Learning, RL)任务的差异，特别强调了从人类数据中构建神经奖励模型(neural reward models)的必要性及其带来的范式转变。作者系统地梳理了该领域的最新进展，深入探讨了在LLM对齐中实施IRL所面临的关键挑战与机遇。除了方法论，综述还详细探讨了数据集、基准测试、评估指标及计算高效的训练推理技术等实际应用层面。最后，论文通过借鉴稀疏奖励强化学习(sparse-reward RL)的文献，提出了提升LLM对齐性能的潜在研究方向，为构建更可靠、可控的智能系统提供了参考。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13158v1",
      "published_date": "2025-07-17 14:22:24 UTC",
      "updated_date": "2025-07-17 14:22:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:16:31.790743+00:00"
    },
    {
      "arxiv_id": "2507.13420v2",
      "title": "AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery",
      "title_zh": "AI溯往：美索不达米亚消逝中的考古景观与基于 CORONA 影像的遗址自动探测",
      "authors": [
        "Alessandro Pistola",
        "Valentina Orru'",
        "Nicolo' Marchetti",
        "Marco Roccetti"
      ],
      "abstract": "By upgrading an existing deep learning model with the knowledge provided by one of the oldest sets of grayscale satellite imagery, known as CORONA, we improved the AI model attitude towards the automatic identification of archaeological sites in an environment which has been completely transformed in the last five decades, including the complete destruction of many of those same sites. The initial Bing based convolutional network model was retrained using CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad, central Mesopotamian floodplain. The results were twofold and surprising. First, the detection precision obtained on the area of interest increased sensibly: in particular, the Intersection over Union (IoU) values, at the image segmentation level, surpassed 85 percent, while the general accuracy in detecting archeological sites reached 90 percent. Second, our retrained model allowed the identification of four new sites of archaeological interest (confirmed through field verification), previously not identified by archaeologists with traditional techniques. This has confirmed the efficacy of using AI techniques and the CORONA imagery from the 1960 to discover archaeological sites currently no longer visible, a concrete breakthrough with significant consequences for the study of landscapes with vanishing archaeological evidence induced by anthropization",
      "tldr_zh": "该研究探讨了如何利用20世纪60年代的CORONA卫星灰度影像改进深度学习模型，以自动识别因过去五十年环境变迁而逐渐消失的美索不达米亚考古遗址。研究人员使用阿布格莱布(Abu Ghraib)地区的CORONA影像，对基于Bing地图的现有卷积神经网络(Convolutional Neural Network)模型进行了重训练。实验结果显示检测精度显著提升，图像分割层面的交并比(Intersection over Union, IoU)值超过了85%，遗址检测的总体准确率达到了90%。该重训练模型还成功识别出四个此前未被传统技术发现的新考古遗址，并已通过田野实地核实。这一成果证明了人工智能结合CORONA历史影像在发现当前已不可见的考古证据方面的有效性，为研究受人为活动影响而消失的古代景观提供了重大突破。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages, 9 Figures",
      "pdf_url": "https://arxiv.org/pdf/2507.13420v2",
      "published_date": "2025-07-17 14:21:50 UTC",
      "updated_date": "2025-07-29 07:01:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:18:14.830217+00:00"
    },
    {
      "arxiv_id": "2507.13152v3",
      "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models",
      "title_zh": "SE-VLN：基于多模态大语言模型的自进化视觉语言导航框架",
      "authors": [
        "Xiangyu Dong",
        "Haoran Zhao",
        "Jiang Gao",
        "Haozhou Li",
        "Xiaoguang Ma",
        "Yaoming Zhou",
        "Fuhai Chen",
        "Juan Liu"
      ],
      "abstract": "Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.",
      "tldr_zh": "该研究针对视觉语言导航(Vision-Language Navigation, VLN)领域中大语言模型(LLMs)因固定知识库而缺乏进化能力的问题，提出了SE-VLN，一种基于多模态大语言模型(Multimodal LLMs)的自我进化框架。作为首个赋予VLN智能体在测试期间持续进化能力的系统，该框架由层次化记忆模块(Hierarchical Memory Module)、检索增强思维推理模块(Retrieval-Augmented Thought-based Reasoning Module)以及反思模块(Reflection Module)三个核心部分组成。记忆模块负责将成功与失败案例转化为可重用的知识，而推理模块则通过检索经验支持复杂的多步决策。实验结果显示，SE-VLN在R2R和REVERSE数据集的未知环境中分别取得了57%和35.2%的导航成功率，较现有最优方法大幅提升了23.9%和15.0%。此外，该框架表现出随经验积累而不断增强的性能，证明了其在构建高效、自我进化的导航智能体方面的巨大潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13152v3",
      "published_date": "2025-07-17 14:13:50 UTC",
      "updated_date": "2025-08-26 08:52:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:16:37.892286+00:00"
    },
    {
      "arxiv_id": "2507.13145v1",
      "title": "DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model",
      "title_zh": "DINO-VO：利用视觉基础模型的基于特征的视觉里程计",
      "authors": [
        "Maulana Bisyir Azhari",
        "David Hyunchul Shim"
      ],
      "abstract": "Learning-based monocular visual odometry (VO) poses robustness, generalization, and efficiency challenges in robotics. Recent advances in visual foundation models, such as DINOv2, have improved robustness and generalization in various vision tasks, yet their integration in VO remains limited due to coarse feature granularity. In this paper, we present DINO-VO, a feature-based VO system leveraging DINOv2 visual foundation model for its sparse feature matching. To address the integration challenge, we propose a salient keypoints detector tailored to DINOv2's coarse features. Furthermore, we complement DINOv2's robust-semantic features with fine-grained geometric features, resulting in more localizable representations. Finally, a transformer-based matcher and differentiable pose estimation layer enable precise camera motion estimation by learning good matches. Against prior detector-descriptor networks like SuperPoint, DINO-VO demonstrates greater robustness in challenging environments. Furthermore, we show superior accuracy and generalization of the proposed feature descriptors against standalone DINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on the TartanAir and KITTI datasets and is competitive on EuRoC dataset, while running efficiently at 72 FPS with less than 1GB of memory usage on a single GPU. Moreover, it performs competitively against Visual SLAM systems on outdoor driving scenarios, showcasing its generalization capabilities.",
      "tldr_zh": "本研究提出了 DINO-VO，一种利用视觉基础模型 DINOv2 进行稀疏特征匹配的单目视觉里程计 (Visual Odometry) 系统。针对 DINOv2 特征颗粒度较粗的问题，研究提出了一种定制的显著关键点检测器 (Salient keypoints detector)，并结合细粒度几何特征以提升特征的局部可定位性。该系统采用 Transformer-based matcher 和可微位姿估计层，通过学习优质匹配点实现了精确的相机运动预测。实验结果显示，DINO-VO 在 TartanAir 和 KITTI 数据集上优于现有的帧对帧 VO 方法，并在复杂环境下表现出比 SuperPoint 更强的稳健性。该系统在单张 GPU 上能以 72 FPS 高效运行且内存占用不足 1GB，在室外驾驶场景中展现了卓越的泛化能力和实用价值。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 6 figures. Accepted for publication in IEEE Robotics and Automation Letters (RA-L), July 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13145v1",
      "published_date": "2025-07-17 14:09:34 UTC",
      "updated_date": "2025-07-17 14:09:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:16:43.199036+00:00"
    },
    {
      "arxiv_id": "2507.13142v4",
      "title": "From Roots to Rewards: Dynamic Tree Reasoning with Reinforcement Learning",
      "title_zh": "从根源到奖励：基于强化学习的动态树推理",
      "authors": [
        "Ahmed Bahloul",
        "Simon Malberg"
      ],
      "abstract": "Modern language models address complex questions through chain-of-thought (CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al., 2021), yet struggle with error propagation and knowledge integration. Tree-structured reasoning methods, particularly the Probabilistic Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues by decomposing questions into hierarchical structures and selecting answers through confidence-weighted aggregation of parametric and retrieved knowledge (Yao et al., 2023). However, ProbTree's static implementation introduces two key limitations: (1) the reasoning tree is fixed during the initial construction phase, preventing dynamic adaptation to intermediate results, and (2) each node requires exhaustive evaluation of all possible solution strategies, creating computational inefficiency. We present a dynamic reinforcement learning (Sutton and Barto, 2018) framework that transforms tree-based reasoning into an adaptive process. Our approach incrementally constructs the reasoning tree based on real-time confidence estimates, while learning optimal policies for action selection (decomposition, retrieval, or aggregation). This maintains ProbTree's probabilistic rigor while improving both solution quality and computational efficiency through selective expansion and focused resource allocation. The work establishes a new paradigm for treestructured reasoning that balances the reliability of probabilistic frameworks with the flexibility required for real-world question answering systems. Code available at: https://github.com/ahmedehabb/From-Roots-to-Rewards-Dynamic-Tree-Reasoning-with-RL",
      "tldr_zh": "该研究针对大型语言模型在处理复杂问题时面临的错误传播和知识集成难题，提出了一种基于强化学习(Reinforcement Learning)的动态树状推理框架。虽然现有的ProbTree框架通过层级分解和概率聚合缓解了部分问题，但其静态特性限制了对中间结果的自适应能力且存在计算效率瓶颈。为此，作者开发了一个动态框架，将树状推理转化为自适应过程，根据实时置信度估计增量构建推理树，并学习分解(decomposition)、检索(retrieval)或聚合(aggregation)的最优动作选择策略。该方法在保留ProbTree概率严谨性的同时，通过选择性扩展和聚焦资源分配，显著提升了求解质量与计算效率。这项工作为树状推理建立了新范式，成功平衡了概率框架的可靠性与现实问答系统所需的灵活性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "RARA Workshop @ ICDM 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13142v4",
      "published_date": "2025-07-17 14:06:19 UTC",
      "updated_date": "2025-09-26 13:43:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:17:42.209647+00:00"
    },
    {
      "arxiv_id": "2507.13112v1",
      "title": "Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data",
      "title_zh": "基于人工智能算法的 California 交通数据高速公路交通流预测",
      "authors": [
        "Junseong Lee",
        "Jaegwan Cho",
        "Yoonju Cho",
        "Seoyoon Choi",
        "Yejin Shin"
      ],
      "abstract": "The study \"Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data\" presents a machine learning-based traffic flow prediction model to address global traffic congestion issues. The research utilized 30-second interval traffic data from California Highway 78 over a five-month period from July to November 2022, analyzing a 7.24 km westbound section connecting \"Melrose Dr\" and \"El-Camino Real\" in the San Diego area. The study employed Multiple Linear Regression (MLR) and Random Forest (RF) algorithms, analyzing data collection intervals ranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance metrics, the analysis revealed that both MLR and RF models performed optimally with 10-minute data collection intervals. These findings are expected to contribute to future traffic congestion solutions and efficient traffic management.",
      "tldr_zh": "该研究提出了一种基于机器学习的交通流预测模型，旨在应对全球日益严峻的交通拥堵问题。研究利用加州78号高速公路2022年7月至11月期间的30秒间隔流量数据，针对圣迭戈地区一段长约7.24公里的路段进行了深入分析。研究采用了Multiple Linear Regression (MLR)和Random Forest (RF)两种算法，并对比了从30秒到15分钟不等的多种数据采集间隔对预测精度的影响。通过使用R^2、MAE和RMSE作为性能评估指标，分析结果证明MLR和RF模型在10分钟数据采集间隔下均能达到最优预测效果。这些发现为开发未来的交通拥堵解决方案和构建高效的交通管理系统提供了重要的理论依据与技术支持。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13112v1",
      "published_date": "2025-07-17 13:27:38 UTC",
      "updated_date": "2025-07-17 13:27:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:16:46.584952+00:00"
    },
    {
      "arxiv_id": "2507.14238v1",
      "title": "Language Models Change Facts Based on the Way You Talk",
      "title_zh": "语言模型会根据用户的表达方式改变对事实的陈述",
      "authors": [
        "Matthew Kearney",
        "Reuben Binns",
        "Yarin Gal"
      ],
      "abstract": "Large language models (LLMs) are increasingly being used in user-facing applications, from providing medical consultations to job interview advice. Recent research suggests that these models are becoming increasingly proficient at inferring identity information about the author of a piece of text from linguistic patterns as subtle as the choice of a few words. However, little is known about how LLMs use this information in their decision-making in real-world applications. We perform the first comprehensive analysis of how identity markers present in a user's writing bias LLM responses across five different high-stakes LLM applications in the domains of medicine, law, politics, government benefits, and job salaries. We find that LLMs are extremely sensitive to markers of identity in user queries and that race, gender, and age consistently influence LLM responses in these applications. For instance, when providing medical advice, we find that models apply different standards of care to individuals of different ethnicities for the same symptoms; we find that LLMs are more likely to alter answers to align with a conservative (liberal) political worldview when asked factual questions by older (younger) individuals; and that LLMs recommend lower salaries for non-White job applicants and higher salaries for women compared to men. Taken together, these biases mean that the use of off-the-shelf LLMs for these applications may cause harmful differences in medical care, foster wage gaps, and create different political factual realities for people of different identities. Beyond providing an analysis, we also provide new tools for evaluating how subtle encoding of identity in users' language choices impacts model decisions. Given the serious implications of these findings, we recommend that similar thorough assessments of LLM use in user-facing applications are conducted before future deployment.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)如何根据用户文本中隐含的身份标识符(identity markers)，如种族、性别和年龄，在决策过程中产生偏见。作者针对医疗、法律、政治、政府福利和求职薪酬五个高风险应用领域进行了首次全面分析，发现LLMs对用户查询中的身份编码(identity encoding)极其敏感。实验结果显示，模型会为相同症状的患者根据种族提供不同的医疗护理标准，并根据用户年龄调整事实性回答以迎合不同的政治世界观，同时在薪酬推荐中表现出明显的种族和性别偏向。研究强调，直接应用现成的LLMs可能导致医疗不公、工资差距扩大以及不同群体间事实认知的断裂。除了揭示这些风险，该研究还提供了一套评估语言选择如何影响模型决策的新工具。鉴于这些发现的严重性，作者强烈建议在未来部署面向用户的LLM应用前，必须进行类似深入的偏见评估。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14238v1",
      "published_date": "2025-07-17 13:21:17 UTC",
      "updated_date": "2025-07-17 13:21:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:18:31.386563+00:00"
    },
    {
      "arxiv_id": "2507.13097v1",
      "title": "GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training",
      "title_zh": "GraspGen：支持生成器端训练的 6 自由度抓取扩散框架",
      "authors": [
        "Adithyavairavan Murali",
        "Balakumar Sundaralingam",
        "Yu-Wei Chao",
        "Wentao Yuan",
        "Jun Yamada",
        "Mark Carlson",
        "Fabio Ramos",
        "Stan Birchfield",
        "Dieter Fox",
        "Clemens Eppner"
      ],
      "abstract": "Grasping is a fundamental robot skill, yet despite significant research advancements, learning-based 6-DOF grasping approaches are still not turnkey and struggle to generalize across different embodiments and in-the-wild settings. We build upon the recent success on modeling the object-centric grasp generation process as an iterative diffusion process. Our proposed framework, GraspGen, consists of a DiffusionTransformer architecture that enhances grasp generation, paired with an efficient discriminator to score and filter sampled grasps. We introduce a novel and performant on-generator training recipe for the discriminator. To scale GraspGen to both objects and grippers, we release a new simulated dataset consisting of over 53 million grasps. We demonstrate that GraspGen outperforms prior methods in simulations with singulated objects across different grippers, achieves state-of-the-art performance on the FetchBench grasping benchmark, and performs well on a real robot with noisy visual observations.",
      "tldr_zh": "该研究提出了 GraspGen，一个用于 6-DOF Grasping 的扩散生成框架，旨在解决学习型抓取方法在不同机器人和真实环境下的泛化难题。该框架的核心采用了 DiffusionTransformer 架构，并配有一个高效的 discriminator 来对采样的抓取进行评分和过滤。研究人员引入了一种新颖且高效的 On-generator Training 方案来训练鉴别器，从而显著提升了生成性能。为了使模型能够扩展到各种物体和 Grippers，该研究还发布了一个包含超过 5300 万次抓取的新型模拟数据集。实验证明 GraspGen 在 FetchBench 基准测试中取得了 State-of-the-art 表现，并在具有噪声观测的真实机器人任务中展现了优异的泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13097v1",
      "published_date": "2025-07-17 13:09:28 UTC",
      "updated_date": "2025-07-17 13:09:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:18:41.988504+00:00"
    },
    {
      "arxiv_id": "2507.13417v1",
      "title": "Soft-ECM: An extension of Evidential C-Means for complex data",
      "title_zh": "Soft-ECM：面向复杂数据的证据C-均值算法扩展",
      "authors": [
        "Armel Soubeiga",
        "Thomas Guyet",
        "Violaine Antoine"
      ],
      "abstract": "Clustering based on belief functions has been gaining increasing attention in the machine learning community due to its ability to effectively represent uncertainty and/or imprecision. However, none of the existing algorithms can be applied to complex data, such as mixed data (numerical and categorical) or non-tabular data like time series. Indeed, these types of data are, in general, not represented in a Euclidean space and the aforementioned algorithms make use of the properties of such spaces, in particular for the construction of barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem for clustering complex data. We propose a new algorithm, Soft-ECM, which consistently positions the centroids of imprecise clusters requiring only a semi-metric. Our experiments show that Soft-ECM present results comparable to conventional fuzzy clustering approaches on numerical data, and we demonstrate its ability to handle mixed data and its benefits when combining fuzzy clustering with semi-metrics such as DTW for time series data.",
      "tldr_zh": "该研究针对现有基于证据理论(Belief functions)的聚类算法因依赖欧几里得空间(Euclidean space)属性而难以处理混合数据或时间序列等复杂数据的问题，提出了 Soft-ECM 算法，作为 Evidential C-Means (ECM) 的重要扩展。Soft-ECM 通过仅要求半度量(semi-metric)即可一致地定位不精确类簇的质心，有效摆脱了传统算法对坐标空间及重心构造的依赖。实验结果表明，该算法在数值数据上的表现与传统的模糊聚类(fuzzy clustering)方法相当，同时具备处理数值与类别混合数据的能力。此外，研究证明了 Soft-ECM 在结合动态时间规整(DTW)等半度量处理时间序列数据时的显著优势，为复杂非结构化数据的不确定性聚类分析提供了有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13417v1",
      "published_date": "2025-07-17 13:00:22 UTC",
      "updated_date": "2025-07-17 13:00:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:18:39.587568+00:00"
    },
    {
      "arxiv_id": "2507.13090v1",
      "title": "MUPAX: Multidimensional Problem Agnostic eXplainable AI",
      "title_zh": "MUPAX：多维度问题无关的可解释人工智能",
      "authors": [
        "Vincenzo Dentamaro",
        "Felice Franchini",
        "Giuseppe Pirlo",
        "Irina Voiculescu"
      ],
      "abstract": "Robust XAI techniques should ideally be simultaneously deterministic, model agnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM AGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability technique, with guaranteed convergency. MUPAX measure theoretic formulation gives principled feature importance attribution through structured perturbation analysis that discovers inherent input patterns and eliminates spurious relationships. We evaluate MUPAX on an extensive range of data modalities and tasks: audio classification (1D), image classification (2D), volumetric medical image analysis (3D), and anatomical landmark detection, demonstrating dimension agnostic effectiveness. The rigorous convergence guarantees extend to any loss function and arbitrary dimensions, making MUPAX applicable to virtually any problem context for AI. By contrast with other XAI methods that typically decrease performance when masking, MUPAX not only preserves but actually enhances model accuracy by capturing only the most important patterns of the original data. Extensive benchmarking against the state of the XAI art demonstrates MUPAX ability to generate precise, consistent and understandable explanations, a crucial step towards explainable and trustworthy AI systems. The source code will be released upon publication.",
      "tldr_zh": "该研究提出了 MUPAX (Multidimensional Problem Agnostic eXplainable AI)，一种具有确定性 (deterministic)、模型无关性 (model agnostic) 且保证收敛 (guaranteed convergence) 的可解释 AI 技术。MUPAX 基于测度论 (measure theoretic) 公式，通过结构化扰动分析 (structured perturbation analysis) 提供原则性的特征重要性归因，从而发现固有的输入模式并消除伪相关性。该方法在音频分类、图像分类、体积医学图像分析以及解剖标志检测等多种数据模态上展现了跨维度的有效性。其严谨的收敛性保证适用于任何损失函数 (loss function) 和任意维度，使得 MUPAX 能够应用于几乎任何 AI 任务场景。与其他在进行遮蔽处理时性能下降的 XAI 方法不同，MUPAX 通过仅捕捉原始数据中最重要的模式，不仅保持了模型准确性，甚至实现了进一步的增强。通过与现有 SOTA 技术的广泛对比，MUPAX 证明了其生成精确、一致且易于理解的解释的能力，为构建可解释且可信赖的 AI 系统迈出了关键一步。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13090v1",
      "published_date": "2025-07-17 12:59:27 UTC",
      "updated_date": "2025-07-17 12:59:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:18:43.789342+00:00"
    },
    {
      "arxiv_id": "2507.13416v1",
      "title": "Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling",
      "title_zh": "具有不确定性量化与解耦的单至多保真度历史相关学习：在数据驱动本构建模中的应用",
      "authors": [
        "Jiaxiang Yi",
        "Bernardo P. Ferreira",
        "Miguel A. Bessa"
      ],
      "abstract": "Data-driven learning is generalized to consider history-dependent multi-fidelity data, while quantifying epistemic uncertainty and disentangling it from data noise (aleatoric uncertainty). This generalization is hierarchical and adapts to different learning scenarios: from training the simplest single-fidelity deterministic neural networks up to the proposed multi-fidelity variance estimation Bayesian recurrent neural networks. The versatility and generality of the proposed methodology are demonstrated by applying it to different data-driven constitutive modeling scenarios that include multiple fidelities with and without aleatoric uncertainty (noise). The method accurately predicts the response and quantifies model error while also discovering the noise distribution (when present). This opens opportunities for future real-world applications in diverse scientific and engineering domains; especially, the most challenging cases involving design and analysis under uncertainty.",
      "tldr_zh": "该研究将数据驱动学习(Data-driven learning)推广至处理具有历史依赖性(History-dependent)的多保真度(Multi-fidelity)数据，并实现了认知不确定性(Epistemic uncertainty)与数据噪声（偶然不确定性，Aleatoric uncertainty）的量化与解耦。这种泛化采用层次化架构，能够适应从单保真度确定性神经网络到多保真度方差估计贝叶斯循环神经网络(Bayesian recurrent neural networks)等多种学习场景。通过在包含噪声和不包含噪声的多保真度数据驱动本构模型(Constitutive modeling)场景中的应用，该方法展示了极高的通用性。实验结果表明，该框架在准确预测响应的同时，能够量化模型误差并有效识别噪声分布。该研究为不确定性条件下的科学与工程设计分析提供了有力工具，尤其在处理复杂的历史依赖性任务中展现出显著的应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "40 pages, 32 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.13416v1",
      "published_date": "2025-07-17 12:45:10 UTC",
      "updated_date": "2025-07-17 12:45:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:18:46.486405+00:00"
    },
    {
      "arxiv_id": "2507.13415v1",
      "title": "SEER: Semantic Enhancement and Emotional Reasoning Network for Multimodal Fake News Detection",
      "title_zh": "SEER：面向多模态虚假新闻检测的语义增强与情感推理网络",
      "authors": [
        "Peican Zhu",
        "Yubo Jing",
        "Le Cheng",
        "Bin Chen",
        "Xiaodong Cui",
        "Lianwei Wu",
        "Keke Tang"
      ],
      "abstract": "Previous studies on multimodal fake news detection mainly focus on the alignment and integration of cross-modal features, as well as the application of text-image consistency. However, they overlook the semantic enhancement effects of large multimodal models and pay little attention to the emotional features of news. In addition, people find that fake news is more inclined to contain negative emotions than real ones. Therefore, we propose a novel Semantic Enhancement and Emotional Reasoning (SEER) Network for multimodal fake news detection. We generate summarized captions for image semantic understanding and utilize the products of large multimodal models for semantic enhancement. Inspired by the perceived relationship between news authenticity and emotional tendencies, we propose an expert emotional reasoning module that simulates real-life scenarios to optimize emotional features and infer the authenticity of news. Extensive experiments on two real-world datasets demonstrate the superiority of our SEER over state-of-the-art baselines.",
      "tldr_zh": "该研究针对多模态虚假新闻检测中忽视语义增强和情感特征的问题，提出了语义增强与情感推理网络 SEER (Semantic Enhancement and Emotional Reasoning Network)。以往的方法多关注跨模态特征对齐，而 SEER 利用 Large Multimodal Models 进行 Semantic Enhancement，并通过生成图像摘要标题来提升图像语义理解。考虑到虚假新闻往往比真实新闻包含更多负面情感，该模型专门设计了一个 Expert Emotional Reasoning 模块，通过模拟现实场景优化情感特征并推断新闻的真实性。在两个真实世界数据集上的广泛实验证明，SEER 在 Multimodal Fake News Detection 任务中的表现显著优于现有的基准模型。该框架成功结合了深度语义理解与情感维度的逻辑推理，为识别复杂的多模态虚假信息提供了更有效的解决方案。",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "Accepted by SMC 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13415v1",
      "published_date": "2025-07-17 12:33:45 UTC",
      "updated_date": "2025-07-17 12:33:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:18:54.740262+00:00"
    },
    {
      "arxiv_id": "2507.14237v1",
      "title": "U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model",
      "title_zh": "U-DREAM：基于混响模型引导的无监督去混响",
      "authors": [
        "Louis Bahrman",
        "Mathieu Fontaine",
        "Gaël Richard"
      ],
      "abstract": "This paper explores the outcome of training state-ofthe-art dereverberation models with supervision settings ranging from weakly-supervised to fully unsupervised, relying solely on reverberant signals and an acoustic model for training. Most of the existing deep learning approaches typically require paired dry and reverberant data, which are difficult to obtain in practice. We develop instead a sequential learning strategy motivated by a bayesian formulation of the dereverberation problem, wherein acoustic parameters and dry signals are estimated from reverberant inputs using deep neural networks, guided by a reverberation matching loss. Our most data-efficient variant requires only 100 reverberation-parameter-labelled samples to outperform an unsupervised baseline, demonstrating the effectiveness and practicality of the proposed method in low-resource scenarios.",
      "tldr_zh": "该研究探讨了在缺乏配对干信号(dry signals)和混响信号(reverberant signals)的现实场景下，如何利用弱监督到完全无监督设置训练先进的去混响模型。为此提出的U-DREAM框架采用受贝叶斯公式(Bayesian formulation)启发的序列学习策略，仅依靠混响信号和声学模型进行训练。该方法利用深度神经网络(DNNs)从混响输入中估计声学参数和干信号，并通过混响匹配损失(reverberation matching loss)进行引导。实验表明，其最高效的变体仅需100个标注样本即可超越无监督基准模型，显著提升了在低资源(low-resource)环境下的去混响效果。这证明了U-DREAM在缺乏配对数据时的实用性，为语音增强领域提供了更灵活且高效的训练方案。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "comment": "Submitted to IEEE Transactions on Audio, Speech and Language Processing (TASLPRO)",
      "pdf_url": "https://arxiv.org/pdf/2507.14237v1",
      "published_date": "2025-07-17 12:26:18 UTC",
      "updated_date": "2025-07-17 12:26:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:18:57.749616+00:00"
    },
    {
      "arxiv_id": "2507.13414v2",
      "title": "Gauge Flow Models",
      "title_zh": "规范流模型",
      "authors": [
        "Alexander Strunk",
        "Roland Assam"
      ],
      "abstract": "This paper introduces Gauge Flow Models, a novel class of Generative Flow Models. These models incorporate a learnable Gauge Field within the Flow Ordinary Differential Equation (ODE). A comprehensive mathematical framework for these models, detailing their construction and properties, is provided. Experiments using Flow Matching on Gaussian Mixture Models demonstrate that Gauge Flow Models yields significantly better performance than traditional Flow Models of comparable or even larger size. Additionally, unpublished research indicates a potential for enhanced performance across a broader range of generative tasks.",
      "tldr_zh": "该研究引入了Gauge Flow Models，这是一种新型的Generative Flow Models。该模型通过在Flow Ordinary Differential Equation (ODE)中融入可学习的Gauge Field，构建了一套完整的数学框架并详细阐述了其物理特性。在Gaussian Mixture Models上进行的Flow Matching实验结果显示，Gauge Flow Models的性能表现显著优于同等规模甚至更大规模的传统Flow Models。此外，该模型在更广泛的生成式任务中也展现出进一步提升性能的潜力，为生成流模型的研究提供了新的视角。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.DG"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13414v2",
      "published_date": "2025-07-17 12:24:54 UTC",
      "updated_date": "2025-08-06 08:31:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:18:56.447053+00:00"
    },
    {
      "arxiv_id": "2507.21118v1",
      "title": "Failure Risk Prediction in a MOOC: A Multivariate Time Series Analysis Approach",
      "title_zh": "MOOC 学业失败风险预测：一种多变量时间序列分析方法",
      "authors": [
        "Anass El Ayady",
        "Maxime Devanne",
        "Germain Forestier",
        "Nour El Mawas"
      ],
      "abstract": "MOOCs offer free and open access to a wide audience, but completion rates remain low, often due to a lack of personalized content. To address this issue, it is essential to predict learner performance in order to provide tailored feedback. Behavioral traces-such as clicks and events-can be analyzed as time series to anticipate learners' outcomes. This work compares multivariate time series classification methods to identify at-risk learners at different stages of the course (after 5, 10 weeks, etc.). The experimental evaluation, conducted on the Open University Learning Analytics Dataset (OULAD), focuses on three courses: two in STEM and one in SHS. Preliminary results show that the evaluated approaches are promising for predicting learner failure in MOOCs. The analysis also suggests that prediction accuracy is influenced by the amount of recorded interactions, highlighting the importance of rich and diverse behavioral data.",
      "tldr_zh": "这项研究针对 MOOC 完课率低的问题，提出了利用多元时间序列分析 (Multivariate Time Series Analysis) 方法来预测学习者的失败风险，以实现个性化反馈。研究对比了多种多元时间序列分类 (Multivariate Time Series Classification) 手段，通过分析点击和事件等行为轨迹 (Behavioral Traces) 在课程不同阶段识别高风险学习者。实验在 Open University Learning Analytics Dataset (OULAD) 的三门课程（涵盖 STEM 和 SHS 领域）上进行，初步结果证明了该方法在预测学习失败方面的有效性。分析进一步表明，预测准确率与记录的交互量密切相关，这突显了丰富且多样化的行为数据 (Behavioral Data) 对于提升教育预测模型性能的关键意义。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "in French language, Environnements Informatiques pour l'Apprentissage Humain 2025, Jun 2025, Villeneuve d'Ascq (Lille), France",
      "pdf_url": "https://arxiv.org/pdf/2507.21118v1",
      "published_date": "2025-07-17 12:22:10 UTC",
      "updated_date": "2025-07-17 12:22:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:10.419086+00:00"
    },
    {
      "arxiv_id": "2507.15876v1",
      "title": "Re-evaluating Short- and Long-Term Trend Factors in CTA Replication: A Bayesian Graphical Approach",
      "title_zh": "CTA 复制中长短期趋势因子的重新评估：一种贝叶斯图模型方法",
      "authors": [
        "Eric Benhamou",
        "Jean-Jacques Ohana",
        "Alban Etienne",
        "Béatrice Guez",
        "Ethan Setrouk",
        "Thomas Jacquot"
      ],
      "abstract": "Commodity Trading Advisors (CTAs) have historically relied on trend-following rules that operate on vastly different horizons from long-term breakouts that capture major directional moves to short-term momentum signals that thrive in fast-moving markets. Despite a large body of work on trend following, the relative merits and interactions of short-versus long-term trend systems remain controversial. This paper adds to the debate by (i) dynamically decomposing CTA returns into short-term trend, long-term trend and market beta factors using a Bayesian graphical model, and (ii) showing how the blend of horizons shapes the strategy's risk-adjusted performance.",
      "tldr_zh": "该研究利用贝叶斯图模型(Bayesian graphical model)重新评估了商品交易顾问(CTAs)复制中的短期和长期趋势因子，探讨了不同时间尺度的趋势跟踪规则对策略表现的影响。通过该模型，作者将CTA回报动态分解为短期趋势(short-term trend)、长期趋势(long-term trend)和市场贝塔(market beta)因子，旨在解决学术界关于不同频率信号相互作用的争议。研究深入分析了长短期时间尺度的融合如何塑造策略的风险调整后收益(risk-adjusted performance)，并揭示了从长期突围(long-term breakouts)到短期动量信号(short-term momentum signals)在不同市场环境下的贡献差异。论文展示了通过图形化建模方法可以更精确地解读趋势系统的构成，并揭示了因子比例动态调整对策略稳健性的作用。这项工作为量化投资领域中的CTA策略构建、因子分解以及风险管理提供了重要的理论参考与实践指导。",
      "categories": [
        "cs.AI",
        "q-fin.PR",
        "q-fin.ST",
        "q-fin.TR"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.15876v1",
      "published_date": "2025-07-17 12:09:29 UTC",
      "updated_date": "2025-07-17 12:09:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:12.454486+00:00"
    },
    {
      "arxiv_id": "2507.13019v2",
      "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities",
      "title_zh": "重新审视视觉语言导航中的具身差异：物理与视觉差异的全面研究",
      "authors": [
        "Liuyi Wang",
        "Xinyuan Xia",
        "Hui Zhao",
        "Hanqing Wang",
        "Tai Wang",
        "Yilun Chen",
        "Chengju Liu",
        "Qijun Chen",
        "Jiangmiao Pang"
      ],
      "abstract": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.",
      "tldr_zh": "这项研究针对视觉语言导航 (Vision-and-Language Navigation, VLN) 在物理具身部署中因理想化假设而产生的差距，提出了物理真实感评估平台 VLN-PE。该平台支持 Humanoid、Quadruped 和 Wheeled 机器人，并系统评估了包括单步离散动作预测的分类模型、稠密路径点预测的扩散模型 (Diffusion model) 以及基于大语言模型 (LLM) 的地图路径规划等多种技术路线。实验结果表明，受限的观测空间、环境光照变化以及物理碰撞 (Collisions) 和摔倒 (Falls) 等挑战会导致导航性能显著下降，同时也暴露了足式机器人在复杂环境中的运动约束 (Locomotion constraints)。VLN-PE 具有极强的可扩展性，能够无缝集成 MP3D 以外的新场景，为评估跨具身适应性提供了重要工具。该研究揭示了当前模型在物理环境部署中的弱泛化性，为开发更鲁棒、更具实用性的 VLN 模型奠定了基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.13019v2",
      "published_date": "2025-07-17 11:46:00 UTC",
      "updated_date": "2025-09-26 07:50:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:25.943948+00:00"
    },
    {
      "arxiv_id": "2507.13007v1",
      "title": "Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming",
      "title_zh": "利用约束推理构建混合整数线性规划的图形化解释",
      "authors": [
        "Roger Xavier Lera-Leri",
        "Filippo Bistaffa",
        "Athina Georgara",
        "Juan Antonio Rodriguez-Aguilar"
      ],
      "abstract": "Following the recent push for trustworthy AI, there has been an increasing interest in developing contrastive explanation techniques for optimisation, especially concerning the solution of specific decision-making processes formalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic approach for building contrastive explanations for MILPs based on constraint reasoning techniques. First, we show how to encode the queries a user makes about the solution of an MILP problem as additional constraints. Then, we determine the reasons that constitute the answer to the user's query by computing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set of constraints. Finally, we represent our explanation as a \"graph of reasons\" constructed from the IIS, which helps the user understand the structure among the reasons that answer their query. We test our method on instances of well-known optimisation problems to evaluate the empirical hardness of computing explanations.",
      "tldr_zh": "该研究针对可信人工智能(Trustworthy AI)的需求，提出了一种名为X-MILP的领域无关方法，旨在为混合整数线性规划(Mixed-Integer Linear Programming, MILP)的决策过程提供对比性解释。该框架首先将用户对MILP解的查询编码为额外的约束条件，随后通过计算新约束集合的不可约不一致子系统(Irreducible Infeasible Subsystem, IIS)来提取回答查询的核心原因。这些提取出的原因最终被构建为“原因图”(Graph of Reasons)，直观地展现了回答结构，帮助用户深入理解决策背后的逻辑。通过在多种知名优化问题上的实验测试，研究验证了该方法在生成图形化解释方面的可行性及其计算复杂度。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "To appear in Lecture Notes in Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2507.13007v1",
      "published_date": "2025-07-17 11:25:33 UTC",
      "updated_date": "2025-07-17 11:25:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:32.751525+00:00"
    },
    {
      "arxiv_id": "2507.13001v1",
      "title": "SMART: Relation-Aware Learning of Geometric Representations for Knowledge Graphs",
      "title_zh": "SMART：面向知识图谱的关系感知几何表示学习",
      "authors": [
        "Kossi Amouzouvi",
        "Bowen Song",
        "Andrea Coletta",
        "Luigi Bellomarini",
        "Jens Lehmann",
        "Sahar Vahdati"
      ],
      "abstract": "Knowledge graph representation learning approaches provide a mapping between symbolic knowledge in the form of triples in a knowledge graph (KG) and their feature vectors. Knowledge graph embedding (KGE) models often represent relations in a KG as geometric transformations. Most state-of-the-art (SOTA) KGE models are derived from elementary geometric transformations (EGTs), such as translation, scaling, rotation, and reflection, or their combinations. These geometric transformations enable the models to effectively preserve specific structural and relational patterns of the KG. However, the current use of EGTs by KGEs remains insufficient without considering relation-specific transformations. Although recent models attempted to address this problem by ensembling SOTA baseline models in different ways, only a single or composite version of geometric transformations are used by such baselines to represent all the relations. In this paper, we propose a framework that evaluates how well each relation fits with different geometric transformations. Based on this ranking, the model can: (1) assign the best-matching transformation to each relation, or (2) use majority voting to choose one transformation type to apply across all relations. That is, the model learns a single relation-specific EGT in low dimensional vector space through an attention mechanism. Furthermore, we use the correlation between relations and EGTs, which are learned in a low dimension, for relation embeddings in a high dimensional vector space. The effectiveness of our models is demonstrated through comprehensive evaluations on three benchmark KGs as well as a real-world financial KG, witnessing a performance comparable to leading models",
      "tldr_zh": "该研究提出了SMART框架，旨在通过Relation-Aware（关系感知）的方式学习Knowledge Graphs（知识图谱）的几何表示。针对现有Knowledge Graph Embedding (KGE)模型往往对所有关系统一使用基础几何变换(EGTs)而忽略Relation-specific特性的问题，SMART通过评估机制量化关系与变换之间的匹配度。该模型利用Attention Mechanism在低维空间中为每个关系分配最合适的几何变换，或通过多数投票策略选择全局变换类型。此外，SMART还将学到的变换相关性应用于高维空间的关系嵌入，以增强特征表达能力。在多个基准数据集及真实金融知识图谱上的实验表明，SMART的性能可与当前SOTA模型媲美，证明了自适应几何变换在处理复杂关系模式中的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13001v1",
      "published_date": "2025-07-17 11:18:08 UTC",
      "updated_date": "2025-07-17 11:18:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:38.703244+00:00"
    },
    {
      "arxiv_id": "2507.12990v1",
      "title": "Teach Old SAEs New Domain Tricks with Boosting",
      "title_zh": "通过 Boosting 赋予预训练 SAE 特定领域新能力",
      "authors": [
        "Nikita Koriagin",
        "Yaroslav Aksenov",
        "Daniil Laptev",
        "Gleb Gerasimov",
        "Nikita Balagansky",
        "Daniil Gavrilov"
      ],
      "abstract": "Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs.",
      "tldr_zh": "该研究针对 Sparse Autoencoders (SAEs) 在捕捉大语言模型 (LLMs) 特定领域特征 (domain-specific features) 方面的局限性，提出了一种无需完整重训练的残差学习 (residual learning) 方法。该方案通过训练一个辅助 SAE 来专门对预训练 SAE 在特定领域文本上的重构误差 (reconstruction error) 进行建模，从而有效捕获主模型遗漏的特征。在推理阶段，通过叠加两个模型的输出，该方法在多个专业领域显著提升了 LLM 的交叉熵 (cross-entropy) 和解释方差 (explained variance) 指标。实验结果证明，这种方法能够高效地将新领域知识融入现有 SAEs，同时保持其在通用任务上的表现。这为针对特定领域的机械解释性 (mechanistic interpretability) 研究提供了更具选择性和增强性的工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12990v1",
      "published_date": "2025-07-17 10:57:49 UTC",
      "updated_date": "2025-07-17 10:57:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:34.005216+00:00"
    },
    {
      "arxiv_id": "2507.12989v1",
      "title": "A Translation of Probabilistic Event Calculus into Markov Decision Processes",
      "title_zh": "将概率事件演算转换为马尔可夫决策过程",
      "authors": [
        "Lyris Xu",
        "Fabio Aurelio D'Asaro",
        "Luke Dickens"
      ],
      "abstract": "Probabilistic Event Calculus (PEC) is a logical framework for reasoning about actions and their effects in uncertain environments, which enables the representation of probabilistic narratives and computation of temporal projections. The PEC formalism offers significant advantages in interpretability and expressiveness for narrative reasoning. However, it lacks mechanisms for goal-directed reasoning. This paper bridges this gap by developing a formal translation of PEC domains into Markov Decision Processes (MDPs), introducing the concept of \"action-taking situations\" to preserve PEC's flexible action semantics. The resulting PEC-MDP formalism enables the extensive collection of algorithms and theoretical tools developed for MDPs to be applied to PEC's interpretable narrative domains. We demonstrate how the translation supports both temporal reasoning tasks and objective-driven planning, with methods for mapping learned policies back into human-readable PEC representations, maintaining interpretability while extending PEC's capabilities.",
      "tldr_zh": "该研究提出了一种将概率事件演算(Probabilistic Event Calculus, PEC)形式化翻译为马尔可夫决策过程(Markov Decision Processes, MDPs)的方法，旨在弥补 PEC 在目标导向推理(goal-directed reasoning)方面的局限性。PEC 虽然在不确定环境下的叙事推理和时间投影(temporal projections)中具有极高的可解释性，但此前缺乏有效的规划机制。为此，本文引入了“动作执行情景”(action-taking situations)的概念，确保在转换过程中保留 PEC 灵活的动作语义。通过这一翻译框架，研究者能够利用成熟的 MDP 算法和理论工具来处理 PEC 的可解释领域，从而支持时间推理与目标驱动规划任务。此外，该方法还支持将习得的策略映射回人类可读的 PEC 表示，成功在增强 PEC 功能的同时维持了其核心的可解释性优势。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12989v1",
      "published_date": "2025-07-17 10:56:22 UTC",
      "updated_date": "2025-07-17 10:56:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:37.904540+00:00"
    },
    {
      "arxiv_id": "2508.02680v1",
      "title": "AnnoSense: A Framework for Physiological Emotion Data Collection in Everyday Settings for AI",
      "title_zh": "AnnoSense：面向人工智能的日常场景生理情绪数据采集框架",
      "authors": [
        "Pragya Singh",
        "Ankush Gupta",
        "Mohan Kumar",
        "Pushpendra Singh"
      ],
      "abstract": "Emotional and mental well-being are vital components of quality of life, and with the rise of smart devices like smartphones, wearables, and artificial intelligence (AI), new opportunities for monitoring emotions in everyday settings have emerged. However, for AI algorithms to be effective, they require high-quality data and accurate annotations. As the focus shifts towards collecting emotion data in real-world environments to capture more authentic emotional experiences, the process of gathering emotion annotations has become increasingly complex. This work explores the challenges of everyday emotion data collection from the perspectives of key stakeholders. We collected 75 survey responses, performed 32 interviews with the public, and 3 focus group discussions (FGDs) with 12 mental health professionals. The insights gained from a total of 119 stakeholders informed the development of our framework, AnnoSense, designed to support everyday emotion data collection for AI. This framework was then evaluated by 25 emotion AI experts for its clarity, usefulness, and adaptability. Lastly, we discuss the potential next steps and implications of AnnoSense for future research in emotion AI, highlighting its potential to enhance the collection and analysis of emotion data in real-world contexts.",
      "tldr_zh": "该研究探讨了在真实世界环境中为人工智能(AI)收集高质量情绪数据及其标注所面临的复杂挑战。通过对119名利益相关者（包括公众调查、访谈以及心理健康专业人士的焦点小组讨论）的深入调研，研究团队开发了名为 AnnoSense 的框架，旨在支持日常场景下的生理情绪数据收集。该框架经过25位情绪智能(Emotion AI)专家的评估，在清晰度、实用性和适应性方面均表现出色。AnnoSense 的提出不仅为增强真实语境下情绪数据的分析质量提供了系统性工具，也为未来情绪智能(Emotion AI)的研究方向和实际应用奠定了基础。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "To be published in IMWUT, September 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.02680v1",
      "published_date": "2025-07-17 10:54:39 UTC",
      "updated_date": "2025-07-17 10:54:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:41.718074+00:00"
    },
    {
      "arxiv_id": "2507.12981v1",
      "title": "MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps",
      "title_zh": "IberLEF-2025 PRESTA 任务中的 MRT：利用多步流程实现表格恢复最大化",
      "authors": [
        "Maximiliano Hormazábal Lagos",
        "Álvaro Bueno Sáez",
        "Héctor Cerezo-Costas",
        "Pedro Alonso Doval",
        "Jorge Alcalde Vesteiro"
      ],
      "abstract": "This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas y Respuestas sobre Tablas en Español (Questions and Answers about Tables in Spanish). Our solution obtains answers to the questions by implementing Python code generation with LLMs that is used to filter and process the table. This solution evolves from the MRT implementation for the Semeval 2025 related task. The process consists of multiple steps: analyzing and understanding the content of the table, selecting the useful columns, generating instructions in natural language, translating these instructions to code, running it, and handling potential errors or exceptions. These steps use open-source LLMs and fine-grained optimized prompts for each step. With this approach, we achieved an accuracy score of 85\\% in the task.",
      "tldr_zh": "该研究介绍了针对 IberLEF 2025 PRESTA 任务（西班牙语表格问答）的解决方案，旨在通过多步骤流程最大化从表格中提取信息。该方案基于 Semeval 2025 任务的 MRT (Maximizing Recovery from Tables) 实现演进而来，核心在于利用 LLMs 生成 Python 代码以进行表格的过滤与处理。整个处理流程包含对表格内容的分析理解、有用列的筛选、自然语言指令生成、代码转化以及错误处理等多个阶段。该方法采用了开源的 LLMs，并针对每个步骤设计了细粒度的优化提示词 (optimized prompts)。实验结果显示，该方法在 PRESTA 任务中取得了 85% 的准确率得分。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as an official challenge paper in the PRESTA: Questions and Answers over Tabular Data shared task at IberLEF 2025, colocated with the 41st SEPLN Conference in Zaragoza, Spain",
      "pdf_url": "https://arxiv.org/pdf/2507.12981v1",
      "published_date": "2025-07-17 10:33:36 UTC",
      "updated_date": "2025-07-17 10:33:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:46.038755+00:00"
    },
    {
      "arxiv_id": "2507.12979v3",
      "title": "A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints",
      "title_zh": "数据共享约束下异构多域环境的分布式生成式人工智能方法",
      "authors": [
        "Youssef Tawfilis",
        "Hossam Amer",
        "Minar El-Aasser",
        "Tallal Elshabrawy"
      ],
      "abstract": "Federated Learning has gained attention for its ability to enable multiple nodes to collaboratively train machine learning models without sharing raw data. At the same time, Generative AI -- particularly Generative Adversarial Networks (GANs) -- have achieved remarkable success across a wide range of domains, such as healthcare, security, and Image Generation. However, training generative models typically requires large datasets and significant computational resources, which are often unavailable in real-world settings. Acquiring such resources can be costly and inefficient, especially when many underutilized devices -- such as IoT devices and edge devices -- with varying capabilities remain idle. Moreover, obtaining large datasets is challenging due to privacy concerns and copyright restrictions, as most devices are unwilling to share their data. To address these challenges, we propose a novel approach for decentralized GAN training that enables utilizing distributed data and underutilized, low-capability devices while not sharing data in its raw form. Our approach is designed to tackle key challenges in decentralized environments, combining KLD-weighted Clustered Federated Learning to address the issues of data heterogeneity and multi-domain datasets, with Heterogeneous U-Shaped split learning to tackle the challenge of device heterogeneity under strict data sharing constraints -- ensuring that no labels or raw data, whether real or synthetic, are ever shared between nodes. Experiments show that our approach demonstrates significant improvements across key metrics, where it achieves an average 10% boost in classification metrics (up to 60% in multi-domain non-IID settings), 1.1x -- 3x higher image generation scores for the MNIST family datasets, and 2x -- 70x lower FID scores for higher resolution datasets. Find our code at https://distributed-gen-ai.github.io/huscf-gan.github.io/.",
      "tldr_zh": "该研究提出了一种针对异构多域环境的去中心化生成式人工智能（Generative AI）训练方法，旨在解决数据共享限制和边缘设备计算资源不足的挑战。该方案结合了KLD加权的聚类联邦学习（KLD-weighted Clustered Federated Learning）来应对数据异构性，并采用异构U型拆分学习（Heterogeneous U-Shaped split learning）处理计算设备的异构性，确保在不共享原始数据、标签或合成数据的前提下协作训练生成对抗网络（GANs）。通过这种架构设计，系统能够在满足严格隐私约束的同时，充分利用物联网和边缘设备的闲置算力。实验结果表明，该方法在多域非独立同分布（non-IID）场景下的分类指标提升高达60%，且在高分辨率数据集上的FID分数降低了2至70倍。该研究证明了在受限分布式环境下实现高效且隐私保护的生成模型训练的可行性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted and published in Transactions on Machine Learning Research (TMLR), 2026",
      "pdf_url": "https://arxiv.org/pdf/2507.12979v3",
      "published_date": "2025-07-17 10:31:31 UTC",
      "updated_date": "2026-01-16 18:20:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:49.242687+00:00"
    },
    {
      "arxiv_id": "2507.12964v5",
      "title": "Demographic-aware fine-grained classification of pediatric wrist fractures",
      "title_zh": "融合人口统计特征的儿童腕部骨折细粒度分类",
      "authors": [
        "Ammar Ahmed",
        "Ali Shariq Imran",
        "Zenun Kastrati",
        "Sher Muhammad Daudpota"
      ],
      "abstract": "Wrist pathologies are frequently observed, particularly among children who constitute the majority of fracture cases. Computer vision presents a promising avenue, contingent upon the availability of extensive datasets, a notable challenge in medical imaging. Therefore, reliance solely on one modality, such as images, proves inadequate, especially in an era of diverse and plentiful data types. This study addresses the problem using a multifaceted approach: framing it as a fine-grained recognition task, fusing patient metadata with X-rays, and leveraging weights from a separate fine-grained dataset rather than from a coarse-grained dataset like ImageNet. Unlike prior work, this is the first application of metadata integration for wrist pathology recognition. Our results show that combining fine-grained transformer approach, fine-grained pre-training, and metadata integration improves diagnostic accuracy by 2% on small custom curated dataset and over 10% on a larger fracture dataset.",
      "tldr_zh": "该研究针对儿科腕关节骨折的高发性及医疗影像数据稀缺的挑战，提出了一种结合人口统计学信息的细粒度分类方法。研究者将骨折识别定义为一项 fine-grained recognition 任务，并利用来自特定细粒度数据集的预训练权重，而非传统的 ImageNet 权重。此外，该研究首次在腕关节病理识别中整合了患者的 metadata 与 X-ray 影像，实现了多模态数据融合。通过结合 fine-grained transformer 架构、细粒度预训练策略以及人口统计学信息的集成，该方法显著提升了诊断性能。实验结果表明，在小型定制数据集上，该方法的诊断准确率提升了 2%，而在规模更大的骨折数据集上，准确率提升幅度则超过了 10%。这一成果证明了多模态元数据集成在提升医疗影像细粒度分类精度方面的关键作用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12964v5",
      "published_date": "2025-07-17 10:03:57 UTC",
      "updated_date": "2025-09-04 16:55:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:50.400368+00:00"
    },
    {
      "arxiv_id": "2507.12961v1",
      "title": "Improving Diagnostic Accuracy of Pigmented Skin Lesions With CNNs: an Application on the DermaMNIST Dataset",
      "title_zh": "利用 CNNs 提升色素性皮肤病变的诊断准确率：基于 DermaMNIST 数据集的应用",
      "authors": [
        "Nerma Kadric",
        "Amila Akagic",
        "Medina Kapo"
      ],
      "abstract": "Pigmented skin lesions represent localized areas of increased melanin and can indicate serious conditions like melanoma, a major contributor to skin cancer mortality. The MedMNIST v2 dataset, inspired by MNIST, was recently introduced to advance research in biomedical imaging and includes DermaMNIST, a dataset for classifying pigmented lesions based on the HAM10000 dataset. This study assesses ResNet-50 and EfficientNetV2L models for multi-class classification using DermaMNIST, employing transfer learning and various layer configurations. One configuration achieves results that match or surpass existing methods. This study suggests that convolutional neural networks (CNNs) can drive progress in biomedical image analysis, significantly enhancing diagnostic accuracy.",
      "tldr_zh": "本研究旨在提高色素性皮肤病变（Pigmented skin lesions）的诊断准确性，此类病变可能预示着黑色素瘤（melanoma）等导致高死亡率的严重皮肤癌。研究利用了MedMNIST v2中的DermaMNIST数据集，该数据集基于HAM10000构建，专注于色素病变的多分类任务。作者评估了ResNet-50和EfficientNetV2L模型在迁移学习（transfer learning）和多种层配置（layer configurations）下的表现。实验结果表明，特定的配置方案在分类性能上达到或超过了现有先进方法。该研究进一步证实了卷积神经网络（CNNs）在生物医学图像分析中的应用潜力，能够显著增强皮肤病变的临床诊断精度。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12961v1",
      "published_date": "2025-07-17 10:00:07 UTC",
      "updated_date": "2025-07-17 10:00:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:19:54.209783+00:00"
    },
    {
      "arxiv_id": "2507.12951v1",
      "title": "UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets",
      "title_zh": "UniSLU：基于异构跨任务数据集的统一口语语言理解",
      "authors": [
        "Zhichao Sheng",
        "Shilin Zhou",
        "Chen Gong",
        "Zhenghua Li"
      ],
      "abstract": "Spoken Language Understanding (SLU) plays a crucial role in speech-centric multimedia applications, enabling machines to comprehend spoken language in scenarios such as meetings, interviews, and customer service interactions. SLU encompasses multiple tasks, including Automatic Speech Recognition (ASR), spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA). However, existing methods often rely on separate model architectures for individual tasks such as spoken NER and SA, which increases system complexity, limits cross-task interaction, and fails to fully exploit heterogeneous datasets available across tasks. To address these limitations, we propose UniSLU, a unified framework that jointly models multiple SLU tasks within a single architecture. Specifically, we propose a unified representation for diverse SLU tasks, enabling full utilization of heterogeneous datasets across multiple tasks. Built upon this representation, we propose a unified generative method that jointly models ASR, spoken NER, and SA tasks, enhancing task interactions and enabling seamless integration with large language models to harness their powerful generative capabilities. Extensive experiments on public SLU datasets demonstrate the effectiveness of our approach, achieving superior SLU performance compared to several benchmark methods, making it well-suited for real-world speech-based multimedia scenarios. We will release all code and models at github to facilitate future research.",
      "tldr_zh": "该研究提出了UniSLU，这是一个统一的Spoken Language Understanding (SLU)框架，旨在通过单一架构共同建模多项任务，解决现有方法在不同任务间架构分离、交互受限及异构数据集利用率低的问题。UniSLU 引入了一种针对多样化 SLU 任务的统一表示方式，从而实现了跨任务数据集的充分整合。基于该表示，该框架采用统一的生成式方法，将 Automatic Speech Recognition (ASR)、口语 Named Entity Recognition (NER) 和口语 Sentiment Analysis (SA) 任务进行联合建模，并与 Large Language Models 无缝集成。这种设计不仅增强了不同任务间的协同交互，还充分发挥了预训练大模型的强大生成能力。在多个公开 SLU 数据集上的实验证明，UniSLU 的性能优于现有基准模型，为会议、访谈和客服等实际语音多媒体场景提供了更高效、可扩展的解决方案。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "13 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12951v1",
      "published_date": "2025-07-17 09:45:49 UTC",
      "updated_date": "2025-07-17 09:45:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:20:05.800559+00:00"
    },
    {
      "arxiv_id": "2507.12935v1",
      "title": "MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration",
      "title_zh": "MC$^2$A：面向高效马尔可夫链蒙特卡洛加速的算法-硬件协同设计",
      "authors": [
        "Shirui Zhao",
        "Jun Yin",
        "Lingyun Yao",
        "Martin Andraud",
        "Wannes Meert",
        "Marian Verhelst"
      ],
      "abstract": "An increasing number of applications are exploiting sampling-based algorithms for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC) algorithms form the computational backbone of this emerging branch of machine learning. Unfortunately, the high computational cost limits their feasibility for large-scale problems and real-world applications, and the existing MCMC acceleration solutions are either limited in hardware flexibility or fail to maintain efficiency at the system level across a variety of end-to-end applications. This paper introduces \\textbf{MC$^2$A}, an algorithm-hardware co-design framework, enabling efficient and flexible optimization for MCMC acceleration. Firstly, \\textbf{MC$^2$A} analyzes the MCMC workload diversity through an extension of the processor performance roofline model with a 3rd dimension to derive the optimal balance between the compute, sampling and memory parameters. Secondly, \\textbf{MC$^2$A} proposes a parametrized hardware accelerator architecture with flexible and efficient support of MCMC kernels with a pipeline of ISA-programmable tree-structured processing units, reconfigurable samplers and a crossbar interconnect to support irregular access. Thirdly, the core of \\textbf{MC$^2$A} is powered by a novel Gumbel sampler that eliminates exponential and normalization operations. In the end-to-end case study, \\textbf{MC$^2$A} achieves an overall {$307.6\\times$, $1.4\\times$, $2.0\\times$, $84.2\\times$} speedup compared to the CPU, GPU, TPU and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC workloads, this work demonstrates and exploits the feasibility of general hardware acceleration to popularize MCMC-based solutions in diverse application domains.",
      "tldr_zh": "该研究针对Markov Chain Monte Carlo (MCMC)算法在大规模问题中面临的计算成本高、硬件灵活性受限以及系统级效率不足等挑战，提出了名为MC$^2$A的算法-硬件协同设计(algorithm-hardware co-design)框架。该框架首先通过扩展的三维处理器性能Roofline Model分析工作负载多样性，以确定计算、采样和内存参数之间的最优平衡。在硬件层面，MC$^2$A引入了参数化加速器架构，利用ISA-programmable的树状处理单元、可重构采样器(reconfigurable samplers)及Crossbar互连来支持不规则访问。此外，其核心采用了新型的Gumbel Sampler，有效消除了复杂的指数和归一化运算。实验结果表明，MC$^2$A在端到端案例中相比CPU、GPU、TPU及最先进的MCMC加速器分别实现了307.6倍、1.4倍、2.0倍和84.2倍的加速。该工作显著提升了MCMC加速的效率与灵活性，为在多样化领域普及基于MCMC的解决方案奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 15 figures, IEEE journal paper",
      "pdf_url": "https://arxiv.org/pdf/2507.12935v1",
      "published_date": "2025-07-17 09:20:51 UTC",
      "updated_date": "2025-07-17 09:20:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:20:13.085392+00:00"
    },
    {
      "arxiv_id": "2507.12933v1",
      "title": "DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization",
      "title_zh": "DMQ：面向训练后量化的扩散模型离群值剖析",
      "authors": [
        "Dongyeun Lee",
        "Jiwan Hur",
        "Hyounguk Shon",
        "Jae Young Lee",
        "Junmo Kim"
      ],
      "abstract": "Diffusion models have achieved remarkable success in image generation but come with significant computational costs, posing challenges for deployment in resource-constrained environments. Recent post-training quantization (PTQ) methods have attempted to mitigate this issue by focusing on the iterative nature of diffusion models. However, these approaches often overlook outliers, leading to degraded performance at low bit-widths. In this paper, we propose a DMQ which combines Learned Equivalent Scaling (LES) and channel-wise Power-of-Two Scaling (PTS) to effectively address these challenges. Learned Equivalent Scaling optimizes channel-wise scaling factors to redistribute quantization difficulty between weights and activations, reducing overall quantization error. Recognizing that early denoising steps, despite having small quantization errors, crucially impact the final output due to error accumulation, we incorporate an adaptive timestep weighting scheme to prioritize these critical steps during learning. Furthermore, identifying that layers such as skip connections exhibit high inter-channel variance, we introduce channel-wise Power-of-Two Scaling for activations. To ensure robust selection of PTS factors even with small calibration set, we introduce a voting algorithm that enhances reliability. Extensive experiments demonstrate that our method significantly outperforms existing works, especially at low bit-widths such as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image generation quality and model stability. The code is available at https://github.com/LeeDongYeun/dmq.",
      "tldr_zh": "该研究针对扩散模型(Diffusion models)在资源受限环境下部署成本高昂的问题，提出了DMQ框架，重点解决现有训练后量化(Post-Training Quantization, PTQ)方法因忽略离群值(outliers)而导致的低位宽性能退化问题。该框架结合了可学习等效缩放(Learned Equivalent Scaling, LES)与通道级幂次缩放(channel-wise Power-of-Two Scaling, PTS)，通过优化通道缩放因子重新分配权重与激活值之间的量化难度，有效降低了整体误差。针对去噪过程的累积误差特性，DMQ引入了自适应时间步加权方案，优先保障关键初始步骤的量化精度。此外，研究针对跳跃连接(skip connections)等高方差层采用了激活值PTS，并利用投票算法提升了参数选择的鲁棒性。实验证明，DMQ在W4A6和W4A8等低位宽配置下显著优于现有方法，在维持高图像生成质量的同时确保了模型的稳定性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12933v1",
      "published_date": "2025-07-17 09:15:29 UTC",
      "updated_date": "2025-07-17 09:15:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:20:15.153865+00:00"
    },
    {
      "arxiv_id": "2507.12930v2",
      "title": "Making Language Model a Hierarchical Classifier",
      "title_zh": "将语言模型构建为分层分类器",
      "authors": [
        "Yihong Wang",
        "Zhonglin Jiang",
        "Ningyuan Xi",
        "Yue Zhao",
        "Qingqing Gu",
        "Xiyuan Chen",
        "Hao Wu",
        "Sheng Xu",
        "Hange Zhou",
        "Yong Chen",
        "Luo Ji"
      ],
      "abstract": "Decoder-only language models, such as GPT and LLaMA, generally decode on the last layer. Motivated by human's hierarchical thinking capability, we propose that a hierarchical decoder architecture could be built with different layers decoding texts simultaneously. Due to limited time and computationally resources, we choose to adapt a pretrained language model into this form of hierarchical decoder. Language heads of the last layer are copied to different selected intermediate layers, and fine-tuned with different task inputs. By thorough experiments, we validate that these selective intermediate layers could be adapted to speak meaningful and reasonable contents, and this paradigm of hierarchical decoder can obtain state-of-the-art performances on multiple tasks such as hierarchical text classification, classification-guided generation, and hierarchical text generation. HdLM outperforms all baselines on WoS, DBpedia, ESconv, EmpatheticDialogues, and several cognitive tests. We also provide thorough theoretical analysis to validate the convergence and computational savings of our methodology. This study suggests the possibility of a generalized hierarchical reasoner, pretraining from scratch.",
      "tldr_zh": "该研究提出了HdLM (Hierarchical decoder Language Model)，这是一种针对GPT和LLaMA等仅解码器(Decoder-only)语言模型的层级化解码架构，旨在模拟人类的层级化思考能力。研究人员通过将顶层的Language heads复制到选定的中间层并进行微调，使得不同层级能够同时解码文本，从而构建出层级化解码器。实验结果表明，HdLM在层级文本分类(Hierarchical text classification)、分类引导生成(Classification-guided generation)和层级文本生成(Hierarchical text generation)等任务中均达到了SOTA性能。在WoS、DBpedia、ESconv、EmpatheticDialogues以及多项认知测试中，该模型表现显著优于所有基线模型。此外，论文还通过理论分析验证了该方法的收敛性与计算效率，为未来开发从零预训练的广义层级推理器(Hierarchical reasoner)提供了重要启示。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12930v2",
      "published_date": "2025-07-17 09:09:53 UTC",
      "updated_date": "2025-09-28 14:57:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:20:20.597715+00:00"
    },
    {
      "arxiv_id": "2507.15875v1",
      "title": "Differential Multimodal Transformers",
      "title_zh": "差分多模态 Transformer",
      "authors": [
        "Jerry Li",
        "Timothy Oh",
        "Joseph Hoang",
        "Vardhit Veeramachaneni"
      ],
      "abstract": "Small language models have gained significant popularity due to their efficiency and growing capabilities. However, incorporating additional modalities, such as vision, can exacerbate the challenge of limited context windows by introducing noise. Recent studies have highlighted that Transformer attention mechanisms often disproportionately focus on irrelevant contexts. In this work, we extend the Differential Attention mechanism, originally designed for text-only models, to the text-vision model PaliGemma. Our aim is to evaluate its ability to mitigate noisy information retrieval and reduce hallucinations. To this end, we fine-tuned the PaliGemma 3B model using LoRA, incorporating Differential Attention, and experimented with various parameter settings and configurations. We demonstrate that Differential Attention can be adapted and integrated into the fine-tuning of existing models to enhance noisy information retrieval and question-answering capabilities.",
      "tldr_zh": "本研究旨在通过引入 Differential Attention 机制，解决小型多模态语言模型在处理视觉信息时因注意力机制过度关注无关上下文而产生的噪声干扰和 hallucinations 问题。研究者将原本针对纯文本模型设计的 Differential Attention 扩展到文本-视觉模型 PaliGemma 中，并利用 LoRA 技术对 PaliGemma 3B 模型进行了针对性的微调。通过对多种参数设置和配置的实验验证，该工作展示了 Differential Attention 在现有模型微调过程中的高度可适配性。实验结果表明，该机制能有效增强模型在 noisy information retrieval 和问答任务中的表现，显著提升了多模态模型对核心信息的捕捉能力。这一探索为优化多模态模型在有限上下文窗口下的信息检索和推理能力提供了新的有效路径。",
      "categories": [
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.15875v1",
      "published_date": "2025-07-17 09:05:34 UTC",
      "updated_date": "2025-07-17 09:05:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:20:23.184930+00:00"
    },
    {
      "arxiv_id": "2507.12916v1",
      "title": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding With Large Language Models",
      "title_zh": "Argus：利用多视图图像提升大语言模型的三维场景理解能力",
      "authors": [
        "Yifan Xu",
        "Chao Zhang",
        "Hanqi Jiang",
        "Xiaoyan Wang",
        "Ruifei Ma",
        "Yiwei Li",
        "Zihao Wu",
        "Zeju Li",
        "Xiangde Liu"
      ],
      "abstract": "Advancements in foundation models have made it possible to conduct applications in various downstream tasks. Especially, the new era has witnessed a remarkable capability to extend Large Language Models (LLMs) for tackling tasks of 3D scene understanding. Current methods rely heavily on 3D point clouds, but the 3D point cloud reconstruction of an indoor scene often results in information loss. Some textureless planes or repetitive patterns are prone to omission and manifest as voids within the reconstructed 3D point clouds. Besides, objects with complex structures tend to introduce distortion of details caused by misalignments between the captured images and the dense reconstructed point clouds. 2D multi-view images present visual consistency with 3D point clouds and provide more detailed representations of scene components, which can naturally compensate for these deficiencies. Based on these insights, we propose Argus, a novel 3D multimodal framework that leverages multi-view images for enhanced 3D scene understanding with LLMs. In general, Argus can be treated as a 3D Large Multimodal Foundation Model (3D-LMM) since it takes various modalities as input(text instructions, 2D multi-view images, and 3D point clouds) and expands the capability of LLMs to tackle 3D tasks. Argus involves fusing and integrating multi-view images and camera poses into view-as-scene features, which interact with the 3D features to create comprehensive and detailed 3D-aware scene embeddings. Our approach compensates for the information loss while reconstructing 3D point clouds and helps LLMs better understand the 3D world. Extensive experiments demonstrate that our method outperforms existing 3D-LMMs in various downstream tasks.",
      "tldr_zh": "该研究提出了 Argus，一种旨在增强 Large Language Models (LLMs) 对三维场景理解能力的新型 3D 多模态框架。针对当前方法过度依赖 3D point clouds 导致的信息丢失、空洞和细节扭曲等问题，Argus 通过引入 2D multi-view images 来提供更丰富的视觉一致性和场景细节。作为一种 3D Large Multimodal Foundation Model (3D-LMM)，它将文本指令、2D 多视角图像和 3D 点云结合，并将图像与 camera poses 融合为 view-as-scene 特征，与 3D 特征交互生成精细的 3D-aware scene embeddings。这种设计有效补偿了点云重建中的信息损耗，显著提升了模型对物理世界的感知深度。实验结果证明，Argus 在多项下游任务中均优于现有的 3D-LMMs 模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by TNNLS2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12916v1",
      "published_date": "2025-07-17 09:02:04 UTC",
      "updated_date": "2025-07-17 09:02:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:20:29.289067+00:00"
    },
    {
      "arxiv_id": "2507.12904v1",
      "title": "An ultra-low-power CGRA for accelerating Transformers at the edge",
      "title_zh": "用于边缘端 Transformer 加速的超低功耗 CGRA",
      "authors": [
        "Rohit Prasad"
      ],
      "abstract": "Transformers have revolutionized deep learning with applications in natural language processing, computer vision, and beyond. However, their computational demands make it challenging to deploy them on low-power edge devices. This paper introduces an ultra-low-power, Coarse-Grained Reconfigurable Array (CGRA) architecture specifically designed to accelerate General Matrix Multiplication (GEMM) operations in transformer models tailored for the energy and resource constraints of edge applications. The proposed architecture integrates a 4 x 4 array of Processing Elements (PEs) for efficient parallel computation and dedicated 4 x 2 Memory Operation Blocks (MOBs) for optimized LOAD/STORE operations, reducing memory bandwidth demands and enhancing data reuse. A switchless mesh torus interconnect network further minimizes power and latency by enabling direct communication between PEs and MOBs, eliminating the need for centralized switching. Through its heterogeneous array design and efficient dataflow, this CGRA architecture addresses the unique computational needs of transformers, offering a scalable pathway to deploy sophisticated machine learning models on edge devices.",
      "tldr_zh": "该研究提出了一种超低功耗的 CGRA 架构，专门用于在资源受限的边缘设备上加速 Transformer 模型的 GEMM 运算。该架构集成了一个 4x4 的 PE 阵列用于高效并行计算，并配置了 4x2 的 MOB 以优化 LOAD/STORE 操作，从而降低内存带宽需求并增强数据复用。通过采用无交换机的 mesh torus 互连网络，PE 与 MOB 之间实现了直接通信，显著降低了系统功耗和延迟。这种结合了异构阵列设计与高效数据流的 CGRA 方案，为在 edge 设备上部署复杂的深度学习模型提供了一种高效且可扩展的硬件路径。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12904v1",
      "published_date": "2025-07-17 08:43:14 UTC",
      "updated_date": "2025-07-17 08:43:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:20:32.086820+00:00"
    },
    {
      "arxiv_id": "2507.15874v1",
      "title": "Why Braking? Scenario Extraction and Reasoning Utilizing LLM",
      "title_zh": "为何制动？基于大语言模型的场景提取与推理",
      "authors": [
        "Yin Wu",
        "Daniel Slieter",
        "Vivek Subramanian",
        "Ahmed Abouelazm",
        "Robin Bohn",
        "J. Marius Zöllner"
      ],
      "abstract": "The growing number of ADAS-equipped vehicles has led to a dramatic increase in driving data, yet most of them capture routine driving behavior. Identifying and understanding safety-critical corner cases within this vast dataset remains a significant challenge. Braking events are particularly indicative of potentially hazardous situations, motivating the central question of our research: Why does a vehicle brake? Existing approaches primarily rely on rule-based heuristics to retrieve target scenarios using predefined condition filters. While effective in simple environments such as highways, these methods lack generalization in complex urban settings. In this paper, we propose a novel framework that leverages Large Language Model (LLM) for scenario understanding and reasoning. Our method bridges the gap between low-level numerical signals and natural language descriptions, enabling LLM to interpret and classify driving scenarios. We propose a dual-path scenario retrieval that supports both category-based search for known scenarios and embedding-based retrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate evaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset. Experimental results show that our method outperforms rule-based baselines and generalizes well to OOD scenarios.",
      "tldr_zh": "该研究针对自动驾驶数据中识别安全关键 Corner Cases 的挑战，特别是针对“车辆为何刹车”这一核心问题，提出了一种利用 Large Language Model (LLM) 进行场景理解与推理的新型框架。现有的基于规则的启发式方法在复杂城市环境中缺乏泛化能力，而该框架通过桥接低层数值信号与自然语言描述，使 LLM 能够准确解读和分类驾驶场景。研究提出了一种双路径场景检索机制，既支持已知场景的类别搜索，也支持针对分布外 (Out-of-Distribution, OOD) 未知场景的嵌入式检索。通过在 Argoverse 2 Sensor Dataset 上进行标注和实验，结果证明该方法在性能上显著优于传统的 Rule-based 基准模型，并对 OOD 场景展现出优异的泛化性能。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.15874v1",
      "published_date": "2025-07-17 08:33:56 UTC",
      "updated_date": "2025-07-17 08:33:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:20:34.186957+00:00"
    },
    {
      "arxiv_id": "2507.12898v4",
      "title": "Vidar: Embodied Video Diffusion Model for Generalist Manipulation",
      "title_zh": "Vidar：面向通用操作的具身视频扩散模型",
      "authors": [
        "Yao Feng",
        "Hengkai Tan",
        "Xinyi Mao",
        "Chendong Xiang",
        "Guodong Liu",
        "Shuhe Huang",
        "Hang Su",
        "Jun Zhu"
      ],
      "abstract": "Scaling general-purpose manipulation to new robot embodiments remains challenging: each platform typically needs large, homogeneous demonstrations, and end-to-end pixel-to-action pipelines may degenerate under background and viewpoint shifts. Based on previous advances in video-based robot control, we present Vidar, consisting of an embodied video diffusion model as the generalizable prior and a masked inverse dynamics model (MIDM) as the adapter. We leverage a video diffusion model pre-trained at Internet scale, and further continuously pre-train it for the embodied domain using 750K multi-view trajectories collected from three real-world robot platforms. For this embodied pre-training, we introduce a unified observation space that jointly encodes robot, camera, task, and scene contexts. The MIDM module learns action-relevant pixel masks without dense labels, grounding the prior into the target embodiment's action space while suppressing distractors. With only 20 minutes of human demonstrations on an unseen robot (1% of typical data), Vidar outperforms state-of-the-art baselines and generalizes to unseen tasks, backgrounds, and camera layouts. Our results suggest a scalable recipe for \"one prior, many embodiments\": strong, inexpensive video priors together with minimal on-robot alignment.",
      "tldr_zh": "该研究提出了 Vidar，一个旨在解决通用机器人操作在不同机器人实体 (embodiments) 间扩展难题的具身视频扩散模型。Vidar 由一个作为通用先验的具身视频扩散模型 (embodied video diffusion model) 和一个作为适配器的掩码逆动力学模型 (masked inverse dynamics model, MIDM) 组成。该系统利用互联网规模预训练的视频模型，并在来自三个真实机器人平台的 75万条多视角轨迹上进行了具身领域的持续预训练。研究团队引入了统一观测空间 (unified observation space) 来联合编码机器人、相机、任务和场景语境，使模型具备跨平台的通用性。MIDM 模块在无需密集标注的情况下学习动作相关的像素掩码，有效地将视频先验对齐到特定实体的动作空间并抑制背景干扰。实验表明，Vidar 在新机器人上仅需 20 分钟的人类演示（约典型数据量的 1%）即可超越现有基线，并能泛化至未见过的任务、背景及相机布局。该成果为实现“一个先验，多个实体”的可扩展机器人学习方案提供了高效路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12898v4",
      "published_date": "2025-07-17 08:31:55 UTC",
      "updated_date": "2025-12-20 02:16:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:20:36.385171+00:00"
    },
    {
      "arxiv_id": "2507.13411v1",
      "title": "Aligning Knowledge Graphs and Language Models for Factual Accuracy",
      "title_zh": "对齐知识图谱与语言模型以提升事实准确性",
      "authors": [
        "Nur A Zarin Nishat",
        "Andrea Coletta",
        "Luigi Bellomarini",
        "Kossi Amouzouvi",
        "Jens Lehmann",
        "Sahar Vahdati"
      ],
      "abstract": "Large language models like GPT-4, Gemini, and Claude have transformed natural language processing (NLP) tasks such as question answering, dialogue generation, summarization, and so forth; yet their susceptibility to hallucination stands as one of the major challenges. Among numerous approaches to overcome this challenge, integration of Knowledge Graphs (KGs) into language models has emerged as a promising solution as it provides structured, reliable, domain-specific, and up-to-date external information to the language models. In this paper, we introduce ALIGNed-LLM, a simple yet effective approach to improve language models' factuality via a lean strategy to infuse KGs into the latent space of language models inspired by LLaVA where visual and textual information is infused. We use embeddings from a pre-trained Knowledge Graph Embedding (KGE) model, such as TransE, and a trainable projection layer to align entity and text embeddings. This alignment enables the language model to distinguish between similar entities improving factual grounding and reducing hallucination. We tested our approach on three popular questions-answering benchmark datasets alongside language models of varying sizes, showing significant improvement. Furthermore, we applied our approach to a real-world financial use case from a large central bank in Europe, which demands high accuracy and precision, demonstrating a substantial improvement of the LLM answers.",
      "tldr_zh": "该研究提出了 ALIGNed-LLM，这是一种通过将知识图谱(Knowledge Graphs)融入语言模型潜空间来提升事实准确性并抑制幻觉的有效方法。受 LLaVA 融合视觉与文本信息的策略启发，该框架利用 TransE 等预训练知识图谱嵌入(KGE)模型配合一个可训练的投影层，实现了实体嵌入与文本嵌入的语义对齐。这种对齐机制显著增强了模型对相似实体的辨析能力，从而强化了事实基座(factual grounding)并有效减少了幻觉现象的发生。在多个通用问答基准数据集的实验中，该方法在不同规模的语言模型上均展现出显著的性能提升。此外，研究团队还将其应用于欧洲某大型中央银行的真实金融业务场景，验证了该方案在处理高精度、高准确性要求的专业领域任务时具有卓越的实用价值。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13411v1",
      "published_date": "2025-07-17 08:15:50 UTC",
      "updated_date": "2025-07-17 08:15:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:20:48.593479+00:00"
    },
    {
      "arxiv_id": "2507.12885v3",
      "title": "VAR-MATH: Probing True Mathematical Reasoning in LLMS via Symbolic Multi-Instance Benchmarks",
      "title_zh": "VAR-MATH：通过符号化多实例基准探究大语言模型的真实数学推理能力",
      "authors": [
        "Jian Yao",
        "Ran Cheng",
        "Kay Chen Tan"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) have led to substantial improvements in the mathematical reasoning abilities of LLMs, as measured by standard benchmarks. Yet these gains often persist even when models are trained with flawed signals, such as random or inverted rewards. This raises a fundamental question: do such improvements reflect genuine reasoning, or are they merely artifacts of overfitting to benchmark-specific patterns? To answer this question, we adopt an evaluation-centric perspective and highlight two critical shortcomings in existing protocols. First, benchmark contamination arises because test problems are publicly available, thereby increasing the risk of data leakage. Second, evaluation fragility results from reliance on single-instance assessments, which are sensitive to stochastic outputs and fail to capture reasoning consistency. These limitations suggest the need for a new evaluation paradigm that can probe reasoning ability beyond memorization and one-off success. As response, we propose VAR-MATH, a symbolic evaluation framework that converts fixed numerical problems into parameterized templates and requires models to solve multiple instantiations of each. This design enforces consistency across structurally equivalent variants, mitigates contamination, and enhances robustness through bootstrapped metrics. We apply VAR-MATH to transform three popular benchmarks, AMC23, AIME24, and AIME25, into their symbolic counterparts, VAR-AMC23, VAR-AIME24, and VAR-AIME25. Experimental results show substantial performance drops for RL-trained models on these variabilized benchmarks, especially for smaller models, with average declines of 47.9\\% on AMC23, 58.8\\% on AIME24, and 72.9\\% on AIME25. These findings indicate that some existing RL methods rely on superficial heuristics and fail to generalize beyond specific numerical forms.",
      "tldr_zh": "该研究提出了 VAR-MATH，一个旨在探究大语言模型 (LLMs) 真实数学推理能力的符号化多实例评估框架，以应对当前模型在基准测试中可能存在的过拟合问题。作者指出，现有的评估方案存在基准污染 (Benchmark contamination) 和评估脆弱性 (Evaluation fragility) 两大缺陷，导致无法准确衡量模型的泛化能力。VAR-MATH 通过将传统的固定数值题目转化为参数化模板 (Parameterized templates)，要求模型在多个实例化变体上保持推理一致性。研究人员利用该框架将 AMC23、AIME24 和 AIME25 转化为了相应的符号化基准版本。实验结果表明，经过强化学习 (RL) 训练的模型在这些变体基准上出现了显著的性能下降，尤其是在 AIME25 上平均下降了 72.9%。这一发现揭示了现有 RL 方法在很大程度上依赖于表层启发式 (Superficial heuristics)，而非具备真正的跨数值形式泛化能力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12885v3",
      "published_date": "2025-07-17 08:10:55 UTC",
      "updated_date": "2026-01-05 05:39:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:03.348562+00:00"
    },
    {
      "arxiv_id": "2507.12872v1",
      "title": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework",
      "title_zh": "对齐失准 AI 的操纵攻击：风险分析与安全论证框架",
      "authors": [
        "Rishane Dassanayake",
        "Mario Demetroudi",
        "James Walpole",
        "Lindley Lentati",
        "Jason R. Brown",
        "Edward James Young"
      ],
      "abstract": "Frontier AI systems are rapidly advancing in their capabilities to persuade, deceive, and influence human behaviour, with current models already demonstrating human-level persuasion and strategic deception in specific contexts. Humans are often the weakest link in cybersecurity systems, and a misaligned AI system deployed internally within a frontier company may seek to undermine human oversight by manipulating employees. Despite this growing threat, manipulation attacks have received little attention, and no systematic framework exists for assessing and mitigating these risks. To address this, we provide a detailed explanation of why manipulation attacks are a significant threat and could lead to catastrophic outcomes. Additionally, we present a safety case framework for manipulation risk, structured around three core lines of argument: inability, control, and trustworthiness. For each argument, we specify evidence requirements, evaluation methodologies, and implementation considerations for direct application by AI companies. This paper provides the first systematic methodology for integrating manipulation risk into AI safety governance, offering AI companies a concrete foundation to assess and mitigate these threats before deployment.",
      "tldr_zh": "该研究探讨了前沿人工智能(Frontier AI)在劝说和策略性欺骗能力日益增强的背景下，失调AI利用操纵攻击(Manipulation Attacks)破坏人工监督并引发灾难性后果的风险。针对目前缺乏系统性风险评估框架的现状，本文提出了一种专门针对操纵风险的安全案例框架(safety case framework)，并将其核心论据结构化为无能力(inability)、控制(control)和可信度(trustworthiness)三个维度。研究详细阐述了各论据所需的证据要求、评估方法及实施考量，旨在为AI公司提供可直接应用的治理工具。作为首个将操纵风险整合进人工智能安全治理的系统性方法，该框架为在模型部署前识别、评估和缓解此类威胁奠定了坚实基础。",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages (14 pages main text, 4 pages bibliography, 6 pages appendices), 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12872v1",
      "published_date": "2025-07-17 07:45:53 UTC",
      "updated_date": "2025-07-17 07:45:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:06.017702+00:00"
    },
    {
      "arxiv_id": "2507.12871v3",
      "title": "Generative Multi-Target Cross-Domain Recommendation",
      "title_zh": "生成式多目标跨域推荐",
      "authors": [
        "Jinqiu Jin",
        "Yang Zhang",
        "Fuli Feng",
        "Xiangnan He"
      ],
      "abstract": "Recently, there has been a surge of interest in Multi-Target Cross-Domain Recommendation (MTCDR), which aims to enhance recommendation performance across multiple domains simultaneously. Existing MTCDR methods primarily rely on domain-shared entities (\\eg users or items) to fuse and transfer cross-domain knowledge, which may be unavailable in non-overlapped recommendation scenarios. Some studies model user preferences and item features as domain-sharable semantic representations, which can be utilized to tackle the MTCDR task. Nevertheless, they often require extensive auxiliary data for pre-training. Developing more effective solutions for MTCDR remains an important area for further exploration.\n  Inspired by recent advancements in generative recommendation, this paper introduces GMC, a generative paradigm-based approach for multi-target cross-domain recommendation. The core idea of GMC is to leverage semantically quantized discrete item identifiers as a medium for integrating multi-domain knowledge within a unified generative model. GMC first employs an item tokenizer to generate domain-shared semantic identifiers for each item, and then formulates item recommendation as a next-token generation task by training a domain-unified sequence-to-sequence model. To further leverage the domain information to enhance performance, we incorporate a domain-aware contrastive loss into the semantic identifier learning, and perform domain-specific fine-tuning on the unified recommender. Extensive experiments on five public datasets demonstrate the effectiveness of GMC compared to a range of baseline methods.",
      "tldr_zh": "该研究提出了GMC，一种基于生成式范式的多目标跨域推荐 (Multi-Target Cross-Domain Recommendation, MTCDR) 方法，旨在解决现有方法过度依赖领域共享实体或需要大量预训练辅助数据的问题。GMC的核心思想是利用语义量化的离散物品标识符 (semantically quantized discrete item identifiers) 作为媒介，在统一的生成模型中整合多领域知识。该方法首先通过物品分词器 (item tokenizer) 为每个物品生成领域共享的语义标识符，并将推荐过程转化为统一序列到序列 (sequence-to-sequence) 模型下的下一令牌生成任务。为了进一步利用领域信息增强性能，研究者在标识符学习中引入了领域感知对比损失 (domain-aware contrastive loss)，并对推荐器进行了领域特定微调 (domain-specific fine-tuning)。在五个公开数据集上的实验结果证明，GMC在处理多目标跨域推荐任务时相较于多种基线方法具有显著的优越性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "fix some information by request",
      "pdf_url": "https://arxiv.org/pdf/2507.12871v3",
      "published_date": "2025-07-17 07:44:05 UTC",
      "updated_date": "2025-08-07 08:36:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:06.189271+00:00"
    },
    {
      "arxiv_id": "2507.12862v2",
      "title": "Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command",
      "title_zh": "仿真指挥中伦理属性的信息论聚合",
      "authors": [
        "Taylan Akay",
        "Harrison Tolley",
        "Hussein Abbass"
      ],
      "abstract": "In the age of AI, human commanders need to use the computational powers available in today's environment to simulate a very large number of scenarios. Within each scenario, situations occur where different decision design options could have ethical consequences. Making these decisions reliant on human judgement is both counter-productive to the aim of exploring very large number of scenarios in a timely manner and infeasible when considering the workload needed to involve humans in each of these choices. In this paper, we move human judgement outside the simulation decision cycle. Basically, the human will design the ethical metric space, leaving it to the simulated environment to explore the space. When the simulation completes its testing cycles, the testing environment will come back to the human commander with a few options to select from. The human commander will then exercise human-judgement to select the most appropriate course of action, which will then get executed accordingly. We assume that the problem of designing metrics that are sufficiently granular to assess the ethical implications of decisions is solved. Subsequently, the fundamental problem we look at in this paper is how to weight ethical decisions during the running of these simulations; that is, how to dynamically weight the ethical attributes when agents are faced with decision options with ethical implications during generative simulations. The multi-criteria decision making literature has started to look at nearby problems, where the concept of entropy has been used to determine the weights during aggregation. We draw from that literature different approaches to automatically calculate the weights for ethical attributes during simulation-based testing and evaluation.",
      "tldr_zh": "该研究针对现代AI环境下海量模拟场景导致的人工伦理判断负担过重问题，提出了将人工判断置于模拟决策循环之外的新框架。人类指挥官负责设计伦理度量空间(ethical metric space)，由模拟环境自主探索并在完成后提供少数备选方案供最终人工裁决。论文重点研究了在生成式模拟过程中，当智能体面临具有伦理影响的决策时，如何动态地为各项伦理属性分配权重。研究借鉴了多准则决策分析(multi-criteria decision making)相关理论，引入信息论(Information-Theoretic)方法，利用熵(entropy)的概念实现模拟测试与评估中伦理属性权重的自动计算。该方法旨在平衡模拟计算的效率与人类决策的主导地位，为复杂指挥环境下的伦理决策评估提供了自动化技术路径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12862v2",
      "published_date": "2025-07-17 07:34:24 UTC",
      "updated_date": "2025-07-20 23:31:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:12.522258+00:00"
    },
    {
      "arxiv_id": "2507.12856v2",
      "title": "Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)",
      "title_zh": "基于精选数据的有监督微调即强化学习（且可进一步改进）",
      "authors": [
        "Chongli Qin",
        "Jost Tobias Springenberg"
      ],
      "abstract": "Behavior Cloning (BC) on curated (or filtered) data is the predominant paradigm for supervised fine-tuning (SFT) of large language models; as well as for imitation learning of control policies. Here, we draw on a connection between this successful strategy and the theory and practice of finding optimal policies via Reinforcement Learning (RL). Building on existing literature, we clarify that SFT can be understood as maximizing a lower bound on the RL objective in a sparse reward setting. Giving support to its often observed good performance. From this viewpoint, we realize that a small modification to SFT leads to an importance weighted variant that behaves closer to training with RL as it: i) optimizes a tighter bound to the RL objective and, ii) can improve performance compared to SFT on curated data. We refer to this variant as importance weighted supervised fine-tuning (iw-SFT). We show that it is easy to implement and can be further generalized to training with quality scored data. The resulting SFT variants are competitive with more advanced RL algorithms for large language models and for training policies in continuous control tasks. For example achieving 66.7% on the AIME 2024 dataset.",
      "tldr_zh": "该研究揭示了在筛选数据上进行的 Supervised Fine-Tuning (SFT) 与 Reinforcement Learning (RL) 寻找最优策略理论之间的深刻联系。作者明确指出，SFT 可以被视为在稀疏奖励 (Sparse Reward) 环境下最大化 RL 目标下界的一种方式，从而为其良好的实践效果提供了理论依据。基于此发现，研究提出了一种改进变体——重要性权重监督微调 (importance weighted supervised fine-tuning, iw-SFT)，旨在通过优化更紧致的 RL 目标下界来超越传统的 SFT。iw-SFT 具有易于实现的特点，并能推广到利用 Quality Scored Data 进行训练的通用场景。实验证明，该方法在大语言模型和连续控制任务中表现优异，其性能足以抗衡目前更先进的 RL 算法。在 AIME 2024 数据集上，该方法达到了 66.7% 的准确率，展现了其在复杂逻辑推理领域的巨大潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "See project website for details and code at: https://independentresearch.ai/posts/iwsft",
      "pdf_url": "https://arxiv.org/pdf/2507.12856v2",
      "published_date": "2025-07-17 07:26:54 UTC",
      "updated_date": "2025-09-06 23:54:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:22.086227+00:00"
    },
    {
      "arxiv_id": "2507.12846v2",
      "title": "Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering",
      "title_zh": "走进记忆宫殿：面向长期主动具身问答的推理与规划",
      "authors": [
        "Muhammad Fadhil Ginting",
        "Dong-Ki Kim",
        "Xiangyun Meng",
        "Andrzej Reinke",
        "Bandi Jai Krishna",
        "Navid Kayhani",
        "Oriana Peltzer",
        "David D. Fan",
        "Amirreza Shaban",
        "Sung-Kyun Kim",
        "Mykel J. Kochenderfer",
        "Ali-akbar Agha-mohammadi",
        "Shayegan Omidshafiei"
      ],
      "abstract": "As robots become increasingly capable of operating over extended periods -- spanning days, weeks, and even months -- they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively. This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determines when the agent has gathered sufficient information. We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency.",
      "tldr_zh": "该研究探讨了长期主动具身问答(Long-term Active Embodied Question Answering, LA-EQA)任务，旨在使机器人在跨越长时间尺度的环境中，能够结合过去经验与主动探索来回答复杂的时空相关问题。针对传统具身问答(EQA)方法在有限上下文窗口、持久性记忆缺失以及记忆检索与主动探索协同能力不足等方面的局限，作者提出了一种受认知科学启发、基于“记忆宫殿”(mind palace)方法的结构化记忆系统。该方法将情节性体验编码为基于场景图(scene-graph)的世界实例，并构建了支持目标明确的记忆检索与引导式导航的推理与规划算法。为了平衡探索与记忆召回之间的权衡，研究还引入了基于信息价值(value-of-information)的停止准则，以精准判断机器人获取充足信息的时机。通过在仿真环境和实际工业现场的新基准测试以及现实世界实验中的评估，该方法在答案准确性和探索效率上均显著优于现有的最先进基准模型。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12846v2",
      "published_date": "2025-07-17 07:11:32 UTC",
      "updated_date": "2025-09-25 00:00:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:17.944092+00:00"
    },
    {
      "arxiv_id": "2507.12845v1",
      "title": "SEMT: Static-Expansion-Mesh Transformer Network Architecture for Remote Sensing Image Captioning",
      "title_zh": "SEMT：面向遥感图像描述的静态扩展网格 Transformer 网络架构",
      "authors": [
        "Khang Truong",
        "Lam Pham",
        "Hieu Tang",
        "Jasmin Lampert",
        "Martin Boyer",
        "Son Phan",
        "Truong Nguyen"
      ],
      "abstract": "Image captioning has emerged as a crucial task in the intersection of computer vision and natural language processing, enabling automated generation of descriptive text from visual content. In the context of remote sensing, image captioning plays a significant role in interpreting vast and complex satellite imagery, aiding applications such as environmental monitoring, disaster assessment, and urban planning. This motivates us, in this paper, to present a transformer based network architecture for remote sensing image captioning (RSIC) in which multiple techniques of Static Expansion, Memory-Augmented Self-Attention, Mesh Transformer are evaluated and integrated. We evaluate our proposed models using two benchmark remote sensing image datasets of UCM-Caption and NWPU-Caption. Our best model outperforms the state-of-the-art systems on most of evaluation metrics, which demonstrates potential to apply for real-life remote sensing image systems.",
      "tldr_zh": "该研究提出了SEMT，一种用于遥感图像描述（Remote Sensing Image Captioning）的Static-Expansion-Mesh Transformer网络架构。该架构通过评估并集成了Static Expansion、Memory-Augmented Self-Attention和Mesh Transformer等多种技术，旨在提升对复杂卫星图像的自动化文本生成能力。研究团队在UCM-Caption和NWPU-Caption两个基准遥感图像数据集上进行了实验评估。结果显示，SEMT在大多数评估指标上均优于现有的state-of-the-art系统。这证明了该架构在处理环境监测、灾害评估和城市规划等现实遥感应用任务中的显著潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12845v1",
      "published_date": "2025-07-17 07:11:01 UTC",
      "updated_date": "2025-07-17 07:11:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:27.386769+00:00"
    },
    {
      "arxiv_id": "2507.13410v2",
      "title": "Causal Language Control in Multilingual Transformers via Sparse Feature Steering",
      "title_zh": "通过稀疏特征引导实现多语言 Transformer 中的因果语言控制",
      "authors": [
        "Cheng-Ting Chou",
        "George Liu",
        "Jessica Sun",
        "Cole Blondin",
        "Kevin Zhu",
        "Vasu Sharma",
        "Sean O'Brien"
      ],
      "abstract": "Deterministically controlling the target generation language of large multilingual language models (LLMs) remains a fundamental challenge, particularly in zero-shot settings where neither explicit language prompts nor fine-tuning are available. In this work, we investigate whether sparse autoencoder (SAE) features, previously shown to correlate with interpretable model behaviors, can be leveraged to steer the generated language of LLMs during inference. Leveraging pretrained SAEs on the residual streams of Gemma-2B and Gemma-9B, we identify features whose activations differ most significantly between English and four target languages: Chinese, Japanese, Spanish, and French. By modifying just a single SAE feature at one transformer layer, we achieve controlled language shifts with up to 90\\% success, as measured by FastText language classification, while preserving semantic fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding) similarity. Our analysis reveals that language steering is most effective in mid-to-late transformer layers and is amplified by specific attention heads disproportionately associated with language-sensitive SAE features. These results demonstrate the promise of sparse feature steering as a lightweight and interpretable mechanism for controllable multilingual generation.",
      "tldr_zh": "该研究探讨了在零样本(Zero-shot)环境下，如何确定性地控制多语言大语言模型(LLMs)目标生成语言的挑战。作者利用在Gemma-2B和Gemma-9B残差流上预训练的稀疏自编码器(Sparse Autoencoder, SAE)特征，提出了一种通过特征转向(Feature Steering)在推理过程中引导生成语言的方法。研究识别出在英语与中文、日语、西班牙语及法语间激活差异最显著的特征，并发现仅需修改转换器(Transformer)单层中的单个SAE特征，即可在保持LaBSE语义忠实度的前提下实现高达90%的语言切换成功率。分析表明，语言转向在模型的中后期层最为有效，且特定注意力头(Attention Heads)对语言敏感特征具有显著的放大作用。该成果证明了稀疏特征转向是一种轻量且具有可解释性的机制，为实现可控的多语言文本生成提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.13410v2",
      "published_date": "2025-07-17 06:49:16 UTC",
      "updated_date": "2025-10-15 18:18:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:25.055537+00:00"
    },
    {
      "arxiv_id": "2507.12832v1",
      "title": "MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results",
      "title_zh": "MVA 2025 鸟类发现小目标多目标跟踪挑战赛：数据集、方法与结果",
      "authors": [
        "Yuki Kondo",
        "Norimichi Ukita",
        "Riku Kanayama",
        "Yuki Yoshida",
        "Takayuki Yamaguchi",
        "Xiang Yu",
        "Guang Liang",
        "Xinyao Liu",
        "Guan-Zhang Wang",
        "Wei-Ta Chu",
        "Bing-Cheng Chuang",
        "Jia-Hua Lee",
        "Pin-Tseng Kuo",
        "I-Hsuan Chu",
        "Yi-Shein Hsiao",
        "Cheng-Han Wu",
        "Po-Yi Wu",
        "Jui-Chien Tsou",
        "Hsuan-Chi Liu",
        "Chun-Yi Lee",
        "Yuan-Fu Yang",
        "Kosuke Shigematsu",
        "Asuka Shin",
        "Ba Tran"
      ],
      "abstract": "Small Multi-Object Tracking (SMOT) is particularly challenging when targets occupy only a few dozen pixels, rendering detection and appearance-based association unreliable. Building on the success of the MVA2023 SOD4SB challenge, this paper introduces the SMOT4SB challenge, which leverages temporal information to address limitations of single-frame detection. Our three main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV video sequences with 108,192 annotated frames under diverse real-world conditions, designed to capture motion entanglement where both camera and targets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance with HOTA to mitigate the sensitivity of IoU-based metrics to small displacements; and (3) a competitive MVA2025 challenge with 78 participants and 308 submissions, where the winning method achieved a 5.1x improvement over the baseline. This work lays a foundation for advancing SMOT in UAV scenarios with applications in bird strike avoidance, agriculture, fisheries, and ecological monitoring.",
      "tldr_zh": "该研究介绍了 MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge (SMOT4SB) 挑战赛及其相关数据集、评估指标与竞赛结果。针对小目标多目标跟踪 (Small Multi-Object Tracking, SMOT) 中因目标仅占极少数像素导致检测和外观关联不可靠的难题，研究利用时序信息来弥补单帧检测的局限。论文核心贡献包括发布了包含 211 个无人机 (UAV) 视频序列和超过十万帧标注的 SMOT4SB 数据集，旨在捕获相机与目标在三维空间中自由运动的复杂场景。此外，研究提出了一种结合 Dot Distance 与 HOTA 的新型评估指标 SO-HOTA，有效缓解了传统基于 IoU 的指标对微小位移过度敏感的问题。在共有 78 名参与者参与的 MVA 2025 挑战赛中，冠军方法相较于基准模型实现了 5.1 倍的性能提升。该工作为推进 UAV 场景下的 SMOT 技术奠定了基础，在防鸟撞、农业、渔业及生态监测等领域具有广泛的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is the official challenge report for SMOT4SB and is published in the proceedings of MVA 2025 (19th International Conference on Machine Vision and Applications). Official challenge page: https://www.mva-org.jp/mva2025/challenge",
      "pdf_url": "https://arxiv.org/pdf/2507.12832v1",
      "published_date": "2025-07-17 06:45:47 UTC",
      "updated_date": "2025-07-17 06:45:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:28.041531+00:00"
    },
    {
      "arxiv_id": "2507.12828v2",
      "title": "Feature-Enhanced TResNet for Fine-Grained Food Image Classification",
      "title_zh": "面向细粒度食物图像分类的特征增强型 TResNet",
      "authors": [
        "Lulu Liu",
        "Zhiyong Xiao"
      ],
      "abstract": "Food is not only essential to human health but also serves as a medium for cultural identity and emotional connection. In the context of precision nutrition, accurately identifying and classifying food images is critical for dietary monitoring, nutrient estimation, and personalized health management. However, fine-grained food classification remains challenging due to the subtle visual differences among similar dishes. To address this, we propose Feature-Enhanced TResNet (FE-TResNet), a novel deep learning model designed to improve the accuracy of food image recognition in fine-grained scenarios. Built on the TResNet architecture, FE-TResNet integrates a Style-based Recalibration Module (StyleRM) and Deep Channel-wise Attention (DCA) to enhance feature extraction and emphasize subtle distinctions between food items. Evaluated on two benchmark Chinese food datasets-ChineseFoodNet and CNFOOD-241-FE-TResNet achieved high classification accuracies of 81.37% and 80.29%, respectively. These results demonstrate its effectiveness and highlight its potential as a key enabler for intelligent dietary assessment and personalized recommendations in precision nutrition systems.",
      "tldr_zh": "该研究针对精细化食物图像分类（Fine-Grained Food Image Classification）中相似菜品视觉差异极其微小的挑战，提出了一种名为Feature-Enhanced TResNet (FE-TResNet)的新型深度学习模型。该模型在TResNet架构基础上，创新性地集成了基于样式的重校准模块（Style-based Recalibration Module, StyleRM）和深度通道注意力机制（Deep Channel-wise Attention, DCA），以强化特征提取并捕捉食物间的细微判别特征。通过在ChineseFoodNet和CNFOOD-241两个基准中文食物数据集上的实验验证，FE-TResNet分别达到了81.37%和80.29%的分类准确率。研究结果表明，FE-TResNet能够有效提升细粒度食物识别的精度，为精准营养（Precision Nutrition）系统中的智能饮食监测、营养估算及个性化健康管理提供了强有力的技术支撑。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12828v2",
      "published_date": "2025-07-17 06:37:45 UTC",
      "updated_date": "2025-07-23 02:14:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:34.240210+00:00"
    },
    {
      "arxiv_id": "2507.12821v2",
      "title": "Assessing Adaptive World Models in Machines with Novel Games",
      "title_zh": "基于新型游戏的机器自适应世界模型评估",
      "authors": [
        "Lance Ying",
        "Katherine M. Collins",
        "Prafull Sharma",
        "Cedric Colas",
        "Kaiya Ivy Zhao",
        "Adrian Weller",
        "Zenna Tavares",
        "Phillip Isola",
        "Samuel J. Gershman",
        "Jacob D. Andreas",
        "Thomas L. Griffiths",
        "Francois Chollet",
        "Kelsey R. Allen",
        "Joshua B. Tenenbaum"
      ],
      "abstract": "Human intelligence exhibits a remarkable capacity for rapid adaptation and effective problem-solving in novel and unfamiliar contexts. We argue that this profound adaptability is fundamentally linked to the efficient construction and refinement of internal representations of the environment, commonly referred to as world models, and we refer to this adaptation mechanism as world model induction. However, current understanding and evaluation of world models in artificial intelligence (AI) remains narrow, often focusing on static representations learned from training on massive corpora of data, instead of the efficiency and efficacy in learning these representations through interaction and exploration within a novel environment. In this Perspective, we provide a view of world model induction drawing on decades of research in cognitive science on how humans learn and adapt so efficiently; we then call for a new evaluation framework for assessing adaptive world models in AI. Concretely, we propose a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures -- we refer to this class of games as novel games. We detail key desiderata for constructing these games and propose appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction. We hope that this new evaluation framework will inspire future evaluation efforts on world models in AI and provide a crucial step towards developing AI systems capable of human-like rapid adaptation and robust generalization -- a critical component of artificial general intelligence.",
      "tldr_zh": "该研究探讨了人类智能在陌生环境下的快速适应能力，认为这种能力源于对环境内部表征的有效构建与完善，即世界模型归纳(world model induction)。文章指出当前人工智能(AI)对世界模型(world models)的评价往往侧重于从海量数据中学习静态表征，而忽视了在交互中学习表征的效率。为此，作者借鉴认知科学提出了一种评估自适应世界模型的新型框架，其核心是基于底层结构具有持续新颖性的新颖游戏(novel games)。该框架定义了构建此类游戏的准则及特定指标，旨在显式挑战智能体进行快速世界模型归纳(rapid world model induction)的能力。这一研究为开发具备类人适应能力和鲁棒泛化(robust generalization)的通用人工智能(AGI)提供了关键的评估范式。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12821v2",
      "published_date": "2025-07-17 06:28:14 UTC",
      "updated_date": "2025-07-22 17:07:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:46.208632+00:00"
    },
    {
      "arxiv_id": "2507.12820v2",
      "title": "Emotional Support with LLM-based Empathetic Dialogue Generation",
      "title_zh": "基于大语言模型共情对话生成的情感支持",
      "authors": [
        "Shiquan Wang",
        "Ruiyu Fang",
        "Zhongjiang He",
        "Shuangyong Song",
        "Yongxiang Li"
      ],
      "abstract": "Emotional Support Conversation (ESC) aims to provide empathetic and effective emotional assistance through dialogue, addressing the growing demand for mental health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC evaluation, where we leverage large-scale language models enhanced by prompt engineering and finetuning techniques. We explore both parameter-efficient Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the model's ability to generate supportive and contextually appropriate responses. Our best model ranked second in the competition, highlighting the potential of combining LLMs with effective adaptation methods for ESC tasks. Future work will focus on further enhancing emotional understanding and response personalization to build more practical and reliable emotional support systems.",
      "tldr_zh": "该研究针对心理健康支持需求的增长，提出了面向 NLPCC 2025 Task 8 ESC 评测任务的情感支持对话(Emotional Support Conversation, ESC)解决方案。为了提升模型生成支持性及语境相关回复的能力，研究团队利用大语言模型(LLMs)，并结合了提示工程(prompt engineering)以及微调(finetuning)技术。在微调策略上，研究者深入探索了参数高效的低秩自适应(Low-Rank Adaptation, LoRA)和全参数微调(full-parameter fine-tuning)方法。实验结果显示，该方案在评测中取得了第二名的成绩，充分展示了结合 LLMs 与有效适配方法在情感支持任务中的巨大潜力。未来工作将致力于进一步增强模型的情感理解能力和回复的个性化(personalization)程度，以构建更具实用性和可靠性的情感支持系统。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12820v2",
      "published_date": "2025-07-17 06:24:20 UTC",
      "updated_date": "2025-12-11 02:38:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:46.915817+00:00"
    },
    {
      "arxiv_id": "2507.12816v1",
      "title": "FIQ: Fundamental Question Generation with the Integration of Question Embeddings for Video Question Answering",
      "title_zh": "FIQ：融合问题嵌入的视频问答基础问题生成",
      "authors": [
        "Ju-Young Oh",
        "Ho-Joong Kim",
        "Seong-Whan Lee"
      ],
      "abstract": "Video question answering (VQA) is a multimodal task that requires the interpretation of a video to answer a given question. Existing VQA methods primarily utilize question and answer (Q&A) pairs to learn the spatio-temporal characteristics of video content. However, these annotations are typically event-centric, which is not enough to capture the broader context of each video. The absence of essential details such as object types, spatial layouts, and descriptive attributes restricts the model to learning only a fragmented scene representation. This issue limits the model's capacity for generalization and higher-level reasoning. In this paper, we propose a fundamental question generation with the integration of question embeddings for video question answering (FIQ), a novel approach designed to strengthen the reasoning ability of the model by enhancing the fundamental understanding of videos. FIQ generates Q&A pairs based on descriptions extracted from videos, enriching the training data with fundamental scene information. Generated Q&A pairs enable the model to understand the primary context, leading to enhanced generalizability and reasoning ability. Furthermore, we incorporate a VQ-CAlign module that assists task-specific question embeddings with visual features, ensuring that essential domain-specific details are preserved to increase the adaptability of downstream tasks. Experiments on SUTD-TrafficQA demonstrate that our FIQ achieves state-of-the-art performance compared to existing baseline methods.",
      "tldr_zh": "该研究针对视频问答(Video Question Answering, VQA)中现有标注过于依赖事件中心而导致的基础场景信息缺失问题，提出了FIQ (Fundamental Question Generation with the Integration of Question Embeddings)方法。FIQ通过从视频描述中生成基础性问答对，为模型提供了关于物体类型、空间布局和属性的丰富语境，从而强化了模型的泛化与高层推理能力。研究进一步引入了VQ-CAlign模块，旨在将特定任务的问题嵌入(question embeddings)与视觉特征进行对齐，确保在下游任务中保留关键的领域特征。实验结果显示，FIQ在SUTD-TrafficQA数据集上取得了State-of-the-art的性能表现。该方法有效地填补了视觉理解与复杂推理之间的鸿沟，为提升多模态学习的适应性提供了新的路径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "SMC 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12816v1",
      "published_date": "2025-07-17 06:19:38 UTC",
      "updated_date": "2025-07-17 06:19:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:55.311480+00:00"
    },
    {
      "arxiv_id": "2507.13408v1",
      "title": "A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs",
      "title_zh": "基于深度学习的临床 X 线片肩部骨折自动检测集成系统",
      "authors": [
        "Hemanth Kumar M",
        "Karthika M",
        "Saianiruth M",
        "Vasanthakumar Venugopal",
        "Anandakumar D",
        "Revathi Ezhumalai",
        "Charulatha K",
        "Kishore Kumar J",
        "Dayana G",
        "Kalyan Sivasailam",
        "Bargava Subramanian"
      ],
      "abstract": "Background: Shoulder fractures are often underdiagnosed, especially in emergency and high-volume clinical settings. Studies report up to 10% of such fractures may be missed by radiologists. AI-driven tools offer a scalable way to assist early detection and reduce diagnostic delays. We address this gap through a dedicated AI system for shoulder radiographs. Methods: We developed a multi-model deep learning system using 10,000 annotated shoulder X-rays. Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and RF-DETR. To enhance detection, we applied bounding box and classification-level ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming individual models across all key metrics. It demonstrated strong recall and localization precision, confirming its effectiveness for clinical fracture detection in shoulder X-rays. Conclusion: The results show ensemble-based AI can reliably detect shoulder fractures in radiographs with high clinical relevance. The model's accuracy and deployment readiness position it well for integration into real-time diagnostic workflows. The current model is limited to binary fracture detection, reflecting its design for rapid screening and triage support rather than detailed orthopedic classification.",
      "tldr_zh": "该研究开发了一种基于深度学习的集成系统，旨在解决临床放射影像中肩部骨折漏诊率高的问题。研究团队利用10,000张标注的肩部X射线图像，构建了包含Faster R-CNN、EfficientDet和RF-DETR在内的多模型深度学习体系。为了进一步提升检测性能，系统应用了Soft-NMS、WBF以及NMW等边界框与分类级别的集成融合技术。实验结果显示，NMW集成模型表现最优，实现了95.5%的准确率和0.9610的F1-score，在各项关键指标上均显著超越了单一模型。该系统展现出极高的召回率和定位精度，证明了其在临床自动化检测中的有效性。尽管目前仅限于二分类骨折检测，但其高准确性和部署就绪性使其非常适合整合至实时诊断工作流中，为快速筛查和分诊提供有力支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.13408v1",
      "published_date": "2025-07-17 06:06:12 UTC",
      "updated_date": "2025-07-17 06:06:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:21:56.042804+00:00"
    },
    {
      "arxiv_id": "2507.21117v2",
      "title": "A Comprehensive Review on Harnessing Large Language Models to Overcome Recommender System Challenges",
      "title_zh": "利用大语言模型应对推荐系统挑战的深度综述",
      "authors": [
        "Rahul Raja",
        "Anshaj Vats",
        "Arpita Vats",
        "Anirban Majumder"
      ],
      "abstract": "Recommender systems have traditionally followed modular architectures comprising candidate generation, multi-stage ranking, and re-ranking, each trained separately with supervised objectives and hand-engineered features. While effective in many domains, such systems face persistent challenges including sparse and noisy interaction data, cold-start problems, limited personalization depth, and inadequate semantic understanding of user and item content. The recent emergence of Large Language Models (LLMs) offers a new paradigm for addressing these limitations through unified, language-native mechanisms that can generalize across tasks, domains, and modalities. In this paper, we present a comprehensive technical survey of how LLMs can be leveraged to tackle key challenges in modern recommender systems. We examine the use of LLMs for prompt-driven candidate retrieval, language-native ranking, retrieval-augmented generation (RAG), and conversational recommendation, illustrating how these approaches enhance personalization, semantic alignment, and interpretability without requiring extensive task-specific supervision. LLMs further enable zero- and few-shot reasoning, allowing systems to operate effectively in cold-start and long-tail scenarios by leveraging external knowledge and contextual cues. We categorize these emerging LLM-driven architectures and analyze their effectiveness in mitigating core bottlenecks of conventional pipelines. In doing so, we provide a structured framework for understanding the design space of LLM-enhanced recommenders, and outline the trade-offs between accuracy, scalability, and real-time performance. Our goal is to demonstrate that LLMs are not merely auxiliary components but foundational enablers for building more adaptive, semantically rich, and user-centric recommender systems",
      "tldr_zh": "该综述全面探讨了利用大语言模型 (Large Language Models, LLMs) 克服推荐系统挑战的技术路径。针对传统推荐系统在处理交互数据稀疏、冷启动以及语义理解不足等方面的局限，LLMs 通过统一的语言原生机制在候选检索、排序及检索增强生成 (Retrieval-Augmented Generation, RAG) 等环节展现了显著优势。文章详细阐述了 LLMs 如何利用提示驱动 (Prompt-driven) 的方法提升系统的个性化深度与可解释性，并借助零样本 (Zero-shot) 与少样本 (Few-shot) 推理能力有效应对长尾场景。研究团队对新兴的 LLM 驱动架构进行了系统分类，并深入分析了准确性、可扩展性与实时性能之间的权衡。该综述不仅为理解 LLM 增强型推荐系统提供了结构化框架，还论证了 LLMs 作为构建自适应、语义丰富且以用户为中心的系统的底层驱动力。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21117v2",
      "published_date": "2025-07-17 06:03:57 UTC",
      "updated_date": "2025-10-03 03:20:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:22:03.089176+00:00"
    },
    {
      "arxiv_id": "2507.12808v1",
      "title": "Large Language Models' Internal Perception of Symbolic Music",
      "title_zh": "大语言模型对符号音乐的内在感知",
      "authors": [
        "Andrew Shin",
        "Kunitake Kaneko"
      ],
      "abstract": "Large language models (LLMs) excel at modeling relationships between strings in natural language and have shown promise in extending to other symbolic domains like coding or mathematics. However, the extent to which they implicitly model symbolic music remains underexplored. This paper investigates how LLMs represent musical concepts by generating symbolic music data from textual prompts describing combinations of genres and styles, and evaluating their utility through recognition and generation tasks. We produce a dataset of LLM-generated MIDI files without relying on explicit musical training. We then train neural networks entirely on this LLM-generated MIDI dataset and perform genre and style classification as well as melody completion, benchmarking their performance against established models. Our results demonstrate that LLMs can infer rudimentary musical structures and temporal relationships from text, highlighting both their potential to implicitly encode musical patterns and their limitations due to a lack of explicit musical context, shedding light on their generative capabilities for symbolic music.",
      "tldr_zh": "这项研究探讨了大语言模型(LLMs)对符号音乐(symbolic music)的内部表征能力，旨在揭示其在缺乏显式音乐训练的情况下如何理解音乐概念。研究人员通过文本提示词(textual prompts)引导LLMs生成涵盖不同流派(genres)和风格(styles)的符号音乐数据，并构建了一个完全由模型生成的MIDI数据集。随后，研究团队在该数据集上训练神经网络，执行流派与风格分类以及旋律补全(melody completion)任务，并与现有基准模型进行对比。实验结果表明，LLMs能够从文本中推断出基础的音乐结构(musical structures)和时间关系(temporal relationships)。这一发现证明了LLMs在隐式编码音乐模式方面的潜力，同时也揭示了由于缺乏显式音乐上下文(musical context)所导致的局限性，为理解大语言模型的符号音乐生成能力提供了新视角。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12808v1",
      "published_date": "2025-07-17 05:48:45 UTC",
      "updated_date": "2025-07-17 05:48:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:22:01.792931+00:00"
    },
    {
      "arxiv_id": "2507.12806v2",
      "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models",
      "title_zh": "MCPEval：基于 MCP 的 AI 智能体模型自动深度评估",
      "authors": [
        "Zhiwei Liu",
        "Jielin Qiu",
        "Shiyu Wang",
        "Jianguo Zhang",
        "Zuxin Liu",
        "Roshan Ram",
        "Haolin Chen",
        "Weiran Yao",
        "Shelby Heinecke",
        "Silvio Savarese",
        "Huan Wang",
        "Caiming Xiong"
      ],
      "abstract": "The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce MCPEval, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.",
      "tldr_zh": "该研究提出了 MCPEval，这是一个基于 Model Context Protocol (MCP) 的开源框架，旨在解决大语言模型 (LLM) 智能体评估中存在的静态基准和人工密集型数据采集的局限性。该框架实现了跨多个领域的端到端任务生成和深度自动评估，能够与原生智能体工具 (agent tools) 无缝集成，显著减少了构建评估流水线的人工成本。MCPEval 通过标准化评估指标，为不同应用场景下的智能体性能提供了更为精准的量化手段。在五个真实世界领域进行的实证研究证明，该框架能有效揭示智能体在特定领域中的细微性能差异。目前 MCPEval 已在 GitHub 开源，旨在促进学术界和工业界在 LLM 智能体评估方面的可复现性与标准化进程。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "https://github.com/SalesforceAIResearch/MCPEval",
      "pdf_url": "https://arxiv.org/pdf/2507.12806v2",
      "published_date": "2025-07-17 05:46:27 UTC",
      "updated_date": "2025-08-01 22:37:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:22:03.691600+00:00"
    },
    {
      "arxiv_id": "2507.12805v1",
      "title": "PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database",
      "title_zh": "PMKLC：面向大规模基因组数据库的并行多知识学习无损压缩",
      "authors": [
        "Hui Sun",
        "Yanfeng Ding",
        "Liping Yi",
        "Huidong Ma",
        "Gang Wang",
        "Xiaoguang Liu",
        "Cheng Zhong",
        "Wentong Cai"
      ],
      "abstract": "Learning-based lossless compressors play a crucial role in large-scale genomic database backup, storage, transmission, and management. However, their 1) inadequate compression ratio, 2) low compression \\& decompression throughput, and 3) poor compression robustness limit their widespread adoption and application in both industry and academia. To solve those challenges, we propose a novel \\underline{P}arallel \\underline{M}ulti-\\underline{K}nowledge \\underline{L}earning-based \\underline{C}ompressor (PMKLC) with four crucial designs: 1) We propose an automated multi-knowledge learning-based compression framework as compressors' backbone to enhance compression ratio and robustness; 2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression throughput and computing resource usage; 3) we introduce data block partitioning and Step-wise Model Passing (SMP) mechanisms for parallel acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet the complex application scenarios, where the former runs on a resource-constrained single GPU and the latter is multi-GPU accelerated. We benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15 real-world datasets with different species and data sizes. Compared to baselines on the testing datasets, PMKLC-S/M achieve the average compression ratio improvement up to 73.609\\% and 73.480\\%, the average throughput improvement up to 3.036$\\times$ and 10.710$\\times$, respectively. Besides, PMKLC-S/M also achieve the best robustness and competitive memory cost, indicating its greater stability against datasets with different probability distribution perturbations, and its strong ability to run on memory-constrained devices.",
      "tldr_zh": "该研究针对大规模基因组数据库在存储和传输中面临的压缩率不足、吞吐量低以及鲁棒性差等挑战，提出了一种名为PMKLC的并行多知识学习无损压缩框架。该框架采用自动化的Multi-Knowledge Learning作为核心架构以增强压缩性能，并设计了GPU加速的($s$,$k$)-mer编码器来优化计算资源利用率。通过引入数据块划分与Step-wise Model Passing (SMP) 机制，PMKLC实现了显著的并行加速效果，并提供PMKLC-S与PMKLC-M两种模式以适配单GPU或多GPU环境。实验结果表明，在15个真实物种数据集上，PMKLC-S/M的平均压缩率较基准模型最高提升了约73.6%，吞吐量最高提升了10.710倍。该方法在保持高压缩比的同时展现了极佳的鲁棒性与内存效率，为大规模基因组数据的高效管理提供了稳定的技术支撑。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted via KDD-25",
      "pdf_url": "https://arxiv.org/pdf/2507.12805v1",
      "published_date": "2025-07-17 05:46:08 UTC",
      "updated_date": "2025-07-17 05:46:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:22:10.788331+00:00"
    },
    {
      "arxiv_id": "2507.12803v1",
      "title": "FLDmamba: Integrating Fourier and Laplace Transform Decomposition with Mamba for Enhanced Time Series Prediction",
      "title_zh": "FLDmamba：融合傅里叶与拉普拉斯变换分解的 Mamba 增强型时间序列预测",
      "authors": [
        "Qianru Zhang",
        "Chenglei Yu",
        "Haixin Wang",
        "Yudong Yan",
        "Yuansheng Cao",
        "Siu-Ming Yiu",
        "Tailin Wu",
        "Hongzhi Yin"
      ],
      "abstract": "Time series prediction, a crucial task across various domains, faces significant challenges due to the inherent complexities of time series data, including non-stationarity, multi-scale periodicity, and transient dynamics, particularly when tackling long-term predictions. While Transformer-based architectures have shown promise, their quadratic complexity with sequence length hinders their efficiency for long-term predictions. Recent advancements in State-Space Models, such as Mamba, offer a more efficient alternative for long-term modeling, but they cannot capture multi-scale periodicity and transient dynamics effectively. Meanwhile, they are susceptible to data noise issues in time series. This paper proposes a novel framework, FLDmamba (Fourier and Laplace Transform Decomposition Mamba), addressing these limitations. FLDmamba leverages the strengths of both Fourier and Laplace transforms to effectively capture both multi-scale periodicity, transient dynamics within time series data, and improve the robustness of the model to the data noise issue. Our extensive experiments demonstrate that FLDmamba achieves superior performance on time series prediction benchmarks, outperforming both Transformer-based and other Mamba-based architectures. To promote the reproducibility of our method, we have made both the code and data accessible via the following URL:{\\href{https://github.com/AI4Science-WestlakeU/FLDmamba}{https://github.com/AI4Science-WestlakeU/\\model}.",
      "tldr_zh": "该研究提出了 FLDmamba，一种结合了 Fourier and Laplace Transform Decomposition 与 Mamba 架构的新型框架，旨在解决长程时间序列预测中非平稳性、多尺度周期性(multi-scale periodicity)和瞬态动力学(transient dynamics)带来的挑战。针对 Transformer 架构计算复杂度高以及现有 Mamba 模型难以捕捉精细特征且易受噪声影响的问题，FLDmamba 利用 Fourier Transform 和 Laplace Transform 的互补优势，有效提升了模型提取复杂时间特征的能力。通过引入这两种变换的分解机制，该模型不仅增强了对多尺度周期性和瞬态变化的建模精度，还显著提高了对数据噪声的鲁棒性。实验证明，FLDmamba 在多个时间序列预测基准任务中取得了优于 Transformer 及其他 Mamba 变体模型的预测性能。目前，该研究的代码与数据已开源，为可扩展且高效的时间序列分析提供了有力支撑。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.12803v1",
      "published_date": "2025-07-17 05:39:15 UTC",
      "updated_date": "2025-07-17 05:39:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:22:13.690866+00:00"
    },
    {
      "arxiv_id": "2507.13407v1",
      "title": "IConMark: Robust Interpretable Concept-Based Watermark For AI Images",
      "title_zh": "IConMark：面向 AI 图像的鲁棒可解释概念水印",
      "authors": [
        "Vinu Sankar Sadasivan",
        "Mehrdad Saberi",
        "Soheil Feizi"
      ],
      "abstract": "With the rapid rise of generative AI and synthetic media, distinguishing AI-generated images from real ones has become crucial in safeguarding against misinformation and ensuring digital authenticity. Traditional watermarking techniques have shown vulnerabilities to adversarial attacks, undermining their effectiveness in the presence of attackers. We propose IConMark, a novel in-generation robust semantic watermarking method that embeds interpretable concepts into AI-generated images, as a first step toward interpretable watermarking. Unlike traditional methods, which rely on adding noise or perturbations to AI-generated images, IConMark incorporates meaningful semantic attributes, making it interpretable to humans and hence, resilient to adversarial manipulation. This method is not only robust against various image augmentations but also human-readable, enabling manual verification of watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness, demonstrating its superiority in terms of detection accuracy and maintaining image quality. Moreover, IConMark can be combined with existing watermarking techniques to further enhance and complement its robustness. We introduce IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with StegaStamp and TrustMark, respectively, to further bolster robustness against multiple types of image manipulations. Our base watermarking technique (IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9% higher mean area under the receiver operating characteristic curve (AUROC) scores for watermark detection, respectively, compared to the best baseline on various datasets.",
      "tldr_zh": "该研究提出了 IConMark，一种面向 AI 生成图像的鲁棒且可解释的语义水印方法，旨在解决生成式媒体的真实性验证难题。不同于传统的噪声注入技术，IConMark 通过在图像生成阶段嵌入具有实际意义的语义概念 (semantic attributes)，使水印具备人类可读性，从而增强了对对抗性操纵 (adversarial manipulation) 和常见图像增强的韧性。该方法支持通过人工手动校验水印，在显著提升检测准确性的同时能够较好地保持图像质量。此外，研究进一步提出了 IConMark+SS 和 IConMark+TM 等混合方案，通过结合 StegaStamp 和 TrustMark 技术进一步强化了抵御多种图像篡改的能力。实验评估表明，IConMark 及其变体在多项数据集上的平均 AUROC 分数比现有最佳基准模型提升了 10.8% 至 15.9%，证明了其在数字取证领域的优越性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICLR 2025 Workshop on GenAI Watermarking (WMARK)",
      "pdf_url": "https://arxiv.org/pdf/2507.13407v1",
      "published_date": "2025-07-17 05:38:30 UTC",
      "updated_date": "2025-07-17 05:38:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:22:17.891541+00:00"
    },
    {
      "arxiv_id": "2507.12801v1",
      "title": "Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning",
      "title_zh": "面向在线同伴学习的模拟错误型 AI 学习伴侣智能体",
      "authors": [
        "Sosui Moribe",
        "Taketoshi Ushiama"
      ],
      "abstract": "In recent years, peer learning has gained attention as a method that promotes spontaneous thinking among learners, and its effectiveness has been confirmed by numerous studies. This study aims to develop an AI Agent as a learning companion that enables peer learning anytime and anywhere. However, peer learning between humans has various limitations, and it is not always effective. Effective peer learning requires companions at the same proficiency levels. In this study, we assume that a learner's peers with the same proficiency level as the learner make the same mistakes as the learner does and focus on English composition as a specific example to validate this approach.",
      "tldr_zh": "本研究针对在线同伴学习(Peer learning)中难以匹配水平相当同伴的局限性，开发了一款作为学习伴侣(Learning companion)的AI Agent。其核心方法是基于相同熟练水平(Proficiency levels)的同伴会犯相似错误这一假设，通过模仿错误(Imitating mistakes)来模拟真实的同伴互动。研究通过英文写作(English composition)这一具体案例，验证了该AI Agent在提升协作学习效果方面的潜力。这种方法克服了人类同伴在时间和空间上的不可用性，使有效的同伴学习能够随时随地进行。通过在智能体中引入针对性的错误反馈机制，该研究为增强AI在教育场景中的拟人化与交互性提供了新的技术路径。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "This is the preprint version of the paper published in IMCOM 2025, IEEE Xplore (DOI: 10.1109/IMCOM64595.2025.10857528)",
      "pdf_url": "https://arxiv.org/pdf/2507.12801v1",
      "published_date": "2025-07-17 05:37:07 UTC",
      "updated_date": "2025-07-17 05:37:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:22:41.290177+00:00"
    },
    {
      "arxiv_id": "2507.12795v1",
      "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
      "title_zh": "City-VLM：基于多模态不完备学习的多域感知场景理解",
      "authors": [
        "Penglei Sun",
        "Yaoxian Song",
        "Xiangru Zhu",
        "Xiang Liu",
        "Qiang Wang",
        "Yue Liu",
        "Changqun Xia",
        "Tiefeng Li",
        "Yang Yang",
        "Xiaowen Chu"
      ],
      "abstract": "Scene understanding enables intelligent agents to interpret and comprehend their environment. While existing large vision-language models (LVLMs) for scene understanding have primarily focused on indoor household tasks, they face two significant limitations when applied to outdoor large-scale scene understanding. First, outdoor scenarios typically encompass larger-scale environments observed through various sensors from multiple viewpoints (e.g., bird view and terrestrial view), while existing indoor LVLMs mainly analyze single visual modalities within building-scale contexts from humanoid viewpoints. Second, existing LVLMs suffer from missing multidomain perception outdoor data and struggle to effectively integrate 2D and 3D visual information. To address the aforementioned limitations, we build the first multidomain perception outdoor scene understanding dataset, named \\textbf{\\underline{SVM-City}}, deriving from multi\\textbf{\\underline{S}}cale scenarios with multi\\textbf{\\underline{V}}iew and multi\\textbf{\\underline{M}}odal instruction tuning data. It contains $420$k images and $4, 811$M point clouds with $567$k question-answering pairs from vehicles, low-altitude drones, high-altitude aerial planes, and satellite. To effectively fuse the multimodal data in the absence of one modality, we introduce incomplete multimodal learning to model outdoor scene understanding and design the LVLM named \\textbf{\\underline{City-VLM}}. Multimodal fusion is realized by constructing a joint probabilistic distribution space rather than implementing directly explicit fusion operations (e.g., concatenation). Experimental results on three typical outdoor scene understanding tasks show City-VLM achieves $18.14 \\%$ performance surpassing existing LVLMs in question-answering tasks averagely. Our method demonstrates pragmatic and generalization performance across multiple outdoor scenes.",
      "tldr_zh": "该研究针对现有大型视觉语言模型(LVLMs)在室外大规模场景理解中面临的多视角、多模态融合及数据缺失等挑战，提出了首个多域感知室外场景理解数据集SVM-City。该数据集涵盖了从车辆、低空无人机、高空飞机到卫星的多尺度观测，包含42万张图像、48.11亿个点云以及56.7万对问答数据。为了在模态缺失的情况下有效融合信息，研究者设计了City-VLM模型，并引入不完整多模态学习(Incomplete Multimodal Learning)机制。该模型通过构建联合概率分布空间(Joint Probabilistic Distribution Space)而非直接的显式拼接(Concatenation)来实现多模态融合。实验结果显示，City-VLM在室外场景理解任务中的表现平均超越现有模型18.14%，证明了其在复杂环境下的实用性与泛化能力(Generalization)。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12795v1",
      "published_date": "2025-07-17 05:21:21 UTC",
      "updated_date": "2025-07-17 05:21:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:22:38.189173+00:00"
    },
    {
      "arxiv_id": "2507.14231v1",
      "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media",
      "title_zh": "超越架构：评估上下文嵌入在社交媒体双相情感障碍检测中的作用",
      "authors": [
        "Khalid Hasan",
        "Jamil Saquer"
      ],
      "abstract": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to subtle early symptoms and social stigma. This paper explores the advanced natural language processing (NLP) models for recognizing signs of bipolar disorder based on user-generated social media text. We conduct a comprehensive evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized (BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed on a large, annotated dataset of Reddit posts after confirming their validity through sentiment variance and judgmental analysis. Our results demonstrate that RoBERTa achieves the highest performance among transformer models with an F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical results. In contrast, LSTMs trained on static embeddings fail to capture meaningful patterns, scoring near-zero F1. These findings underscore the critical role of contextual language modeling in detecting bipolar disorder. In addition, we report model training times and highlight that DistilBERT offers an optimal balance between efficiency and accuracy. In general, our study offers actionable insights for model selection in mental health NLP applications and validates the potential of contextualized language models to support early bipolar disorder screening.",
      "tldr_zh": "本研究探讨了利用先进自然语言处理 (NLP) 模型基于社交媒体文本检测双相情感障碍 (Bipolar Disorder) 的有效性。研究通过大型 Reddit 标注数据集，对多种基于 Transformer 的模型（如 BERT, RoBERTa, DistilBERT 等）以及结合上下文嵌入 (Contextual Embeddings) 和静态嵌入的 LSTM 模型进行了全面评估。结果表明，RoBERTa 模型表现最优，F1 分数约 98%，而使用 BERT 嵌入的 LSTM 同样能达到极高准确率。相比之下，基于 GloVe 或 Word2Vec 等静态嵌入的模型由于无法捕捉语义语境，导致 F1 分数接近于零。实验还发现 DistilBERT 在准确性与训练效率之间取得了最佳平衡，具有极高的实用价值。该研究强调了上下文语言建模在精神健康监测中的核心作用，并为双相情感障碍的早期筛查提供了可靠的模型选择指导。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "The 37th International Conference on Software Engineering & Knowledge Engineering, SEKE 2025 (camera-ready)",
      "pdf_url": "https://arxiv.org/pdf/2507.14231v1",
      "published_date": "2025-07-17 05:14:19 UTC",
      "updated_date": "2025-07-17 05:14:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:22:42.893618+00:00"
    },
    {
      "arxiv_id": "2507.14230v2",
      "title": "Intent-Based Network for RAN Management with Large Language Models",
      "title_zh": "基于大语言模型的意图驱动 RAN 管理网络",
      "authors": [
        "Fransiscus Asisi Bimo",
        "Maria Amparo Canaveras Galdon",
        "Chun-Kai Lai",
        "Ray-Guang Cheng",
        "Edwin K. P. Chong"
      ],
      "abstract": "Advanced intelligent automation becomes an important feature to deal with the increased complexity in managing wireless networks. This paper proposes a novel automation approach of intent-based network for Radio Access Networks (RANs) management by leveraging Large Language Models (LLMs). The proposed method enhances intent translation, autonomously interpreting high-level objectives, reasoning over complex network states, and generating precise configurations of the RAN by integrating LLMs within an agentic architecture. We propose a structured prompt engineering technique and demonstrate that the network can automatically improve its energy efficiency by dynamically optimizing critical RAN parameters through a closed-loop mechanism. It showcases the potential to enable robust resource management in RAN by adapting strategies based on real-time feedback via LLM-orchestrated agentic systems.",
      "tldr_zh": "该研究提出了一种利用 Large Language Models (LLMs) 实现无线接入网 (RAN) 管理的意图驱动网络 (Intent-Based Network) 自动化方法，旨在应对无线网络日益增长的管理复杂性。该方案通过将 LLMs 集成到智能体架构 (agentic architecture) 中，显著增强了意图翻译能力，实现了对高层目标的自主解读和对复杂网络状态的推理并生成精确配置。研究引入了结构化提示工程 (prompt engineering) 技术，并证明了网络能够通过闭环机制 (closed-loop mechanism) 动态优化关键 RAN 参数，从而自动提升能源效率。此外，该系统展示了利用 LLM 编排的智能体架构根据实时反馈灵活调整策略，从而在 RAN 中实现稳健资源管理的巨大潜力。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "5 pages, 3 figures, submitted to IEEE Globecom 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.14230v2",
      "published_date": "2025-07-17 04:57:55 UTC",
      "updated_date": "2025-08-04 05:31:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:22:47.499191+00:00"
    },
    {
      "arxiv_id": "2507.12784v1",
      "title": "A Semi-Supervised Learning Method for the Identification of Bad Exposures in Large Imaging Surveys",
      "title_zh": "面向大型成像巡天不良曝光识别的半监督学习方法",
      "authors": [
        "Yufeng Luo",
        "Adam D. Myers",
        "Alex Drlica-Wagner",
        "Dario Dematties",
        "Salma Borchani",
        "Frank Valdes",
        "Arjun Dey",
        "David Schlegel",
        "Rongpu Zhou",
        "DESI Legacy Imaging Surveys Team"
      ],
      "abstract": "As the data volume of astronomical imaging surveys rapidly increases, traditional methods for image anomaly detection, such as visual inspection by human experts, are becoming impractical. We introduce a machine-learning-based approach to detect poor-quality exposures in large imaging surveys, with a focus on the DECam Legacy Survey (DECaLS) in regions of low extinction (i.e., $E(B-V)<0.04$). Our semi-supervised pipeline integrates a vision transformer (ViT), trained via self-supervised learning (SSL), with a k-Nearest Neighbor (kNN) classifier. We train and validate our pipeline using a small set of labeled exposures observed by surveys with the Dark Energy Camera (DECam). A clustering-space analysis of where our pipeline places images labeled in ``good'' and ``bad'' categories suggests that our approach can efficiently and accurately determine the quality of exposures. Applied to new imaging being reduced for DECaLS Data Release 11, our pipeline identifies 780 problematic exposures, which we subsequently verify through visual inspection. Being highly efficient and adaptable, our method offers a scalable solution for quality control in other large imaging surveys.",
      "tldr_zh": "该研究针对天文巡天数据量激增导致人工检查低质量图像(Bad Exposures)变得不切实际的问题，提出了一种基于机器学习的自动检测方法。研究团队开发了一个半监督学习(Semi-Supervised Learning)流程，将通过自监督学习(Self-Supervised Learning, SSL)训练的 Vision Transformer (ViT) 与 k-Nearest Neighbor (kNN) 分类器相结合。该流程主要针对 Dark Energy Camera (DECam) 获取的 DECam Legacy Survey (DECaLS) 低消光区域数据进行训练与验证。聚类空间分析表明，该方法能够高效且准确地判定图像质量。在 DECaLS Data Release 11 的新数据处理中，该流程成功识别出 780 个有问题的曝光，并随后通过目视检查得到了验证。该方法具有极高的效率和适应性，为大型天文巡天中的质量控制提供了一种可扩展的解决方案。",
      "categories": [
        "astro-ph.IM",
        "cs.AI"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "21 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.12784v1",
      "published_date": "2025-07-17 04:52:05 UTC",
      "updated_date": "2025-07-17 04:52:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:22:49.994623+00:00"
    },
    {
      "arxiv_id": "2507.12774v1",
      "title": "A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models",
      "title_zh": "电子健康档案建模全面综述：从深度学习方法到大语言模型",
      "authors": [
        "Weijieying Ren",
        "Jingxi Zhu",
        "Zehao Liu",
        "Tianxiang Zhao",
        "Vasant Honavar"
      ],
      "abstract": "Artificial intelligence (AI) has demonstrated significant potential in transforming healthcare through the analysis and modeling of electronic health records (EHRs). However, the inherent heterogeneity, temporal irregularity, and domain-specific nature of EHR data present unique challenges that differ fundamentally from those in vision and natural language tasks. This survey offers a comprehensive overview of recent advancements at the intersection of deep learning, large language models (LLMs), and EHR modeling. We introduce a unified taxonomy that spans five key design dimensions: data-centric approaches, neural architecture design, learning-focused strategies, multimodal learning, and LLM-based modeling systems. Within each dimension, we review representative methods addressing data quality enhancement, structural and temporal representation, self-supervised learning, and integration with clinical knowledge. We further highlight emerging trends such as foundation models, LLM-driven clinical agents, and EHR-to-text translation for downstream reasoning. Finally, we discuss open challenges in benchmarking, explainability, clinical alignment, and generalization across diverse clinical settings. This survey aims to provide a structured roadmap for advancing AI-driven EHR modeling and clinical decision support. For a comprehensive list of EHR-related methods, kindly refer to https://survey-on-tabular-data.github.io/.",
      "tldr_zh": "该综述全面回顾了电子健康档案(Electronic Health Record, EHR)建模领域从深度学习到大语言模型(Large Language Models, LLMs)的最新进展。针对EHR数据固有的异构性、时间不规则性和领域特定性等挑战，本文提出了一个涵盖数据中心方法、神经架构设计、学习策略、多模态学习以及基于LLM建模系统的统一分类法(Unified Taxonomy)。文章详细评述了旨在提升数据质量、结构化与时间表示、自监督学习以及临床知识整合的代表性方法，并强调了基础模型(Foundation Models)、LLM驱动的临床智能体以及用于下游推理的EHR-to-text翻译等新兴趋势。最后，作者探讨了基准测试、可解释性、临床一致性及跨临床场景泛化等领域的开放性挑战。该项工作为推进AI驱动的EHR建模和临床决策支持提供了结构化的技术路线图。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12774v1",
      "published_date": "2025-07-17 04:31:55 UTC",
      "updated_date": "2025-07-17 04:31:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:23:00.145731+00:00"
    },
    {
      "arxiv_id": "2507.12771v1",
      "title": "Local Representative Token Guided Merging for Text-to-Image Generation",
      "title_zh": "面向文本到图像生成的局部代表性 Token 引导式合并",
      "authors": [
        "Min-Jeong Lee",
        "Hee-Dong Kim",
        "Seong-Whan Lee"
      ],
      "abstract": "Stable diffusion is an outstanding image generation model for text-to-image, but its time-consuming generation process remains a challenge due to the quadratic complexity of attention operations. Recent token merging methods improve efficiency by reducing the number of tokens during attention operations, but often overlook the characteristics of attention-based image generation models, limiting their effectiveness. In this paper, we propose local representative token guided merging (ReToM), a novel token merging strategy applicable to any attention mechanism in image generation. To merge tokens based on various contextual information, ReToM defines local boundaries as windows within attention inputs and adjusts window sizes. Furthermore, we introduce a representative token, which represents the most representative token per window by computing similarity at a specific timestep and selecting the token with the highest average similarity. This approach preserves the most salient local features while minimizing computational overhead. Experimental results show that ReToM achieves a 6.2% improvement in FID and higher CLIP scores compared to the baseline, while maintaining comparable inference time. We empirically demonstrate that ReToM is effective in balancing visual quality and computational efficiency.",
      "tldr_zh": "该研究针对 Stable Diffusion 等模型在文本生成图像过程中因 Attention 操作的二次复杂度而导致耗时过长的问题，提出了一种名为 ReToM (Local Representative Token Guided Merging) 的新型 Token 合并策略。ReToM 通过在 Attention 输入中定义局部窗口并灵活调整窗口大小，实现了基于多尺度上下文信息的 Token 合并。该方法的核心在于引入了代表性 Token (Representative Token) 的概念，即通过在特定时间步计算相似度并选取平均相似度最高的 Token 来代表该窗口，从而在最小化计算开销的同时保留最显著的局部特征。实验结果表明，ReToM 在维持推理速度的同时，相较于基线模型实现了 6.2% 的 FID 提升，并获得了更高的 CLIP 分数。该研究有力地证明了 ReToM 在平衡视觉生成质量与计算效率方面的有效性，为优化图像生成模型的性能提供了新方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.12771v1",
      "published_date": "2025-07-17 04:16:24 UTC",
      "updated_date": "2025-07-17 04:16:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:23:17.543767+00:00"
    },
    {
      "arxiv_id": "2507.12769v1",
      "title": "Synergy: End-to-end Concept Model",
      "title_zh": "Synergy：端到端概念模型",
      "authors": [
        "Keli Zheng",
        "Zerong Xie"
      ],
      "abstract": "In this paper, we present Synergy, a language model that bridges different levels of abstraction in an end-to-end fashion through a learned routing mechanism. Focusing on low-level linguistic abstraction, we trained our model as a byte-level language model. Our model spontaneously learns to tokenize bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE) tokenizers while keeping comparable performance. By comparing with Llama3, we observed an advantage of Synergy under the same model scale and training dataset size. Further studies show that the middle part (the higher abstraction part) of our model performs better when positional encodings are removed, suggesting the emergence of position-independent concepts. These findings demonstrate the feasibility of tokenizer-free architectures, paving the way for more robust and flexible pipelines.",
      "tldr_zh": "该研究提出了 Synergy，一种通过学习路由机制(learned routing mechanism)端到端桥接不同抽象层次的语言模型。作为字节级(byte-level)语言模型，Synergy 能够自发地学习字节标记化过程，在生成比字节级字节对编码(BBPE)更少概念令牌(concept tokens)的同时，保持了相当的性能水平。实验表明，在相同模型规模和训练数据量下，Synergy 的表现优于 Llama3 模型。进一步研究发现，移除位置编码(positional encodings)后，模型负责高层抽象的中部区域表现更佳，显示出位置无关概念(position-independent concepts)的涌现。这些发现充分证明了无分词器(tokenizer-free)架构的可行性，为构建更鲁棒、灵活的自然语言处理流水线奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12769v1",
      "published_date": "2025-07-17 04:01:28 UTC",
      "updated_date": "2025-07-17 04:01:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:23:01.449271+00:00"
    },
    {
      "arxiv_id": "2507.12767v1",
      "title": "Autonomy for Older Adult-Agent Interaction",
      "title_zh": "老年人-智能体交互中的自主性",
      "authors": [
        "Jiaxin An"
      ],
      "abstract": "As the global population ages, artificial intelligence (AI)-powered agents have emerged as potential tools to support older adults' caregiving. Prior research has explored agent autonomy by identifying key interaction stages in task processes and defining the agent's role at each stage. However, ensuring that agents align with older adults' autonomy preferences remains a critical challenge. Drawing on interdisciplinary conceptualizations of autonomy, this paper examines four key dimensions of autonomy for older adults: decision-making autonomy, goal-oriented autonomy, control autonomy, and social responsibility autonomy. This paper then proposes the following research directions: (1) Addressing social responsibility autonomy, which concerns the ethical and social implications of agent use in communal settings; (2) Operationalizing agent autonomy from the task perspective; and (3) Developing autonomy measures.",
      "tldr_zh": "该研究探讨了在人口老龄化背景下，如何确保人工智能(AI)驱动的智能体(Agents)与老年人的自主权偏好(Autonomy Preferences)保持一致。基于跨学科的自主权概念，论文明确了老年人自主权的四个关键维度：决策自主(Decision-making Autonomy)、目标导向自主(Goal-oriented Autonomy)、控制自主(Control Autonomy)以及社会责任自主(Social Responsibility Autonomy)。在此基础上，作者提出了三个未来的研究方向，包括解决涉及伦理和社会影响的社会责任自主问题，从任务视角实现智能体自主性的操作化(Operationalization)，以及开发相应的自主权度量(Autonomy Measures)方法。这些建议旨在深化对老年人与智能体交互过程中自主权边界的理解，为设计更符合老年人需求且具备伦理保障的护理支持系统提供了理论指导和实践框架。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.12767v1",
      "published_date": "2025-07-17 03:46:13 UTC",
      "updated_date": "2025-07-17 03:46:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:23:01.627945+00:00"
    },
    {
      "arxiv_id": "2507.12761v1",
      "title": "Think-Before-Draw: Decomposing Emotion Semantics & Fine-Grained Controllable Expressive Talking Head Generation",
      "title_zh": "Think-Before-Draw：情感语义分解与细粒度可控表达性说话人头像生成",
      "authors": [
        "Hanlei Shi",
        "Leyuan Qu",
        "Yu Liu",
        "Di Gao",
        "Yuhua Zheng",
        "Taihao Li"
      ],
      "abstract": "Emotional talking-head generation has emerged as a pivotal research area at the intersection of computer vision and multimodal artificial intelligence, with its core value lying in enhancing human-computer interaction through immersive and empathetic engagement.With the advancement of multimodal large language models, the driving signals for emotional talking-head generation has shifted from audio and video to more flexible text. However, current text-driven methods rely on predefined discrete emotion label texts, oversimplifying the dynamic complexity of real facial muscle movements and thus failing to achieve natural emotional expressiveness.This study proposes the Think-Before-Draw framework to address two key challenges: (1) In-depth semantic parsing of emotions--by innovatively introducing Chain-of-Thought (CoT), abstract emotion labels are transformed into physiologically grounded facial muscle movement descriptions, enabling the mapping from high-level semantics to actionable motion features; and (2) Fine-grained expressiveness optimization--inspired by artists' portrait painting process, a progressive guidance denoising strategy is proposed, employing a \"global emotion localization--local muscle control\" mechanism to refine micro-expression dynamics in generated videos.Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including MEAD and HDTF. Additionally, we collected a set of portrait images to evaluate our model's zero-shot generation capability.",
      "tldr_zh": "该研究提出了Think-Before-Draw框架，旨在解决现有文本驱动的情感说话头生成（Talking-Head Generation）技术因过度依赖离散情感标签而导致面部表情缺乏自然感的问题。该框架创新性地引入了链式思维（Chain-of-Thought, CoT）推理，将抽象的情感语义分解为基于生理学的面部肌肉运动描述，实现了从高层语义到精细运动特征的有效映射。研究还提出了一种借鉴艺术创作过程的渐进式引导去噪策略（Progressive Guidance Denoising Strategy），通过“全局情感定位与局部肌肉控制”机制精准捕捉微表情动态。实验证明，该方法在MEAD和HDTF等基准数据集上均取得了State-of-the-Art的性能表现。此外，该模型在零样本生成（Zero-Shot Generation）任务中也展现了极强的泛化能力，为实现更具共情能力的人机交互奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12761v1",
      "published_date": "2025-07-17 03:33:46 UTC",
      "updated_date": "2025-07-17 03:33:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:23:05.560430+00:00"
    },
    {
      "arxiv_id": "2507.12760v1",
      "title": "Unified Medical Image Segmentation with State Space Modeling Snake",
      "title_zh": "基于状态空间建模 Snake 的统一医学图像分割",
      "authors": [
        "Ruicheng Zhang",
        "Haowei Guo",
        "Kanghui Tian",
        "Jun Zhou",
        "Mingliang Yan",
        "Zeyu Zhang",
        "Shen Zhao"
      ],
      "abstract": "Unified Medical Image Segmentation (UMIS) is critical for comprehensive anatomical assessment but faces challenges due to multi-scale structural heterogeneity. Conventional pixel-based approaches, lacking object-level anatomical insight and inter-organ relational modeling, struggle with morphological complexity and feature conflicts, limiting their efficacy in UMIS. We propose Mamba Snake, a novel deep snake framework enhanced by state space modeling for UMIS. Mamba Snake frames multi-contour evolution as a hierarchical state space atlas, effectively modeling macroscopic inter-organ topological relationships and microscopic contour refinements. We introduce a snake-specific vision state space module, the Mamba Evolution Block (MEB), which leverages effective spatiotemporal information aggregation for adaptive refinement of complex morphologies. Energy map shape priors further ensure robust long-range contour evolution in heterogeneous data. Additionally, a dual-classification synergy mechanism is incorporated to concurrently optimize detection and segmentation, mitigating under-segmentation of microstructures in UMIS. Extensive evaluations across five clinical datasets reveal Mamba Snake's superior performance, with an average Dice improvement of 3\\% over state-of-the-art methods.",
      "tldr_zh": "该研究针对统一医学图像分割(Unified Medical Image Segmentation, UMIS)中多尺度结构异质性导致的形态复杂性和特征冲突问题，提出了Mamba Snake框架。Mamba Snake通过引入状态空间模型(State Space Modeling)，将多轮廓演化构建为层次化状态空间图谱，从而有效建模宏观器官间的拓扑关系和微观轮廓精细化。该框架核心包含一个名为Mamba Evolution Block (MEB)的视觉状态空间模块，利用有效的时空信息聚合实现对复杂形态的自适应修正。此外，研究结合了能量图(Energy map)形状先验以增强长程轮廓演化的鲁棒性，并引入双分类协同机制同步优化检测与分割，缓解了微细结构的欠分割问题。在五个临床数据集上的实验结果显示，Mamba Snake在性能上显著优于现有先进方法，平均Dice指标提升了3%，为复杂解剖结构的精准评估提供了新方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted by ACM MM 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.12760v1",
      "published_date": "2025-07-17 03:32:32 UTC",
      "updated_date": "2025-07-17 03:32:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:23:31.343381+00:00"
    },
    {
      "arxiv_id": "2507.12759v1",
      "title": "Logit Arithmetic Elicits Long Reasoning Capabilities Without Training",
      "title_zh": "Logit 运算：无需训练即可激发长程推理能力",
      "authors": [
        "Yunxiang Zhang",
        "Muhammad Khalifa",
        "Lechen Zhang",
        "Xin Liu",
        "Ayoung Lee",
        "Xinliang Frederick Zhang",
        "Farima Fatahi Bayat",
        "Lu Wang"
      ],
      "abstract": "Large reasoning models (LRMs) can do complex reasoning via long chain-of-thought (CoT) involving cognitive strategies such as backtracking and self-correction. Recent studies suggest that some models inherently possess these long reasoning abilities, which may be unlocked via extra training. Our work first investigates whether we can elicit such behavior without any training. To this end, we propose a decoding-time approach, ThinkLogit, which utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for long reasoning using a substantially smaller model as guider. We then show that we can further boost performance by training the guider model with preference optimization over correct/incorrect reasoning pairs sampled from both the target and guider model -- a setup we refer to as ThinkLogit-DPO. Our experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative improvement in pass@1 by 26% and 29%, respectively, over four mathematical datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model 21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills acquired through reinforcement learning, improving pass@1 by 13% relative compared to the Qwen2.5-32B base model. Our work presents a computationally-efficient method to elicit long reasoning in large models with minimal or no additional training.",
      "tldr_zh": "该研究提出了ThinkLogit，一种无需额外训练即可激发大型语言模型(LLMs)长链式思维(Chain-of-Thought)推理能力的解码时方法。该方法利用Logit Arithmetic技术，通过体量显著更小的引导模型(Guider Model)来调节目标大模型，使其在推理过程中展现出回溯和自我修正等复杂认知策略。进一步提出的ThinkLogit-DPO通过在引导模型上应用偏好优化(Preference Optimization)，利用从目标和引导模型中采样的推理对进一步提升了性能。实验结果表明，在以R1-Distill-Qwen-1.5B引导Qwen2.5-32B的配置下，ThinkLogit和ThinkLogit-DPO在数学数据集上的pass@1分别实现了26%和29%的相对提升。此外，ThinkLogit还能有效迁移通过强化学习(Reinforcement Learning)获得的长推理技能，使基座模型性能提升13%。该研究为以高计算效率、低训练成本在大型模型中实现长推理能力提供了一种创新的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12759v1",
      "published_date": "2025-07-17 03:31:36 UTC",
      "updated_date": "2025-07-17 03:31:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:23:33.745494+00:00"
    },
    {
      "arxiv_id": "2507.12739v1",
      "title": "Transformer-based Spatial Grounding: A Comprehensive Survey",
      "title_zh": "基于 Transformer 的空间定位：全面综述",
      "authors": [
        "Ijazul Haq",
        "Muhammad Saqib",
        "Yingjie Zhang"
      ],
      "abstract": "Spatial grounding, the process of associating natural language expressions with corresponding image regions, has rapidly advanced due to the introduction of transformer-based models, significantly enhancing multimodal representation and cross-modal alignment. Despite this progress, the field lacks a comprehensive synthesis of current methodologies, dataset usage, evaluation metrics, and industrial applicability. This paper presents a systematic literature review of transformer-based spatial grounding approaches from 2018 to 2025. Our analysis identifies dominant model architectures, prevalent datasets, and widely adopted evaluation metrics, alongside highlighting key methodological trends and best practices. This study provides essential insights and structured guidance for researchers and practitioners, facilitating the development of robust, reliable, and industry-ready transformer-based spatial grounding models.",
      "tldr_zh": "该研究对2018年至2025年间的基于Transformer的Spatial Grounding方法进行了系统的文献综述，旨在解决该领域目前缺乏方法论、数据集和工业应用综合总结的问题。Spatial Grounding是将自然语言表达与对应图像区域关联的关键过程，Transformer模型的引入极大增强了其多模态表示与跨模态对齐能力。论文深入分析了主流的模型架构、流行数据集及广泛采用的评估指标，并梳理了关键的方法论趋势和最佳实践。该研究为研究人员和从业者提供了必要的洞察和结构化指导，旨在促进开发更强大、可靠且具备工业应用能力的Transformer-based Spatial Grounding模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12739v1",
      "published_date": "2025-07-17 02:44:01 UTC",
      "updated_date": "2025-07-17 02:44:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:23:37.540093+00:00"
    },
    {
      "arxiv_id": "2507.19510v1",
      "title": "Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers",
      "title_zh": "突破朝九晚五：一种用于增强代表性不足轮班群体移动数据的生成式模型",
      "authors": [
        "Haoxuan Ma",
        "Xishun Liao",
        "Yifan Liu",
        "Chris Stanford",
        "Jiaqi Ma"
      ],
      "abstract": "This paper addresses a critical gap in urban mobility modeling by focusing on shift workers, a population segment comprising 15-20% of the workforce in industrialized societies yet systematically underrepresented in traditional transportation surveys and planning. This underrepresentation is revealed in this study by a comparative analysis of GPS and survey data, highlighting stark differences between the bimodal temporal patterns of shift workers and the conventional 9-to-5 schedules recorded in surveys. To address this bias, we introduce a novel transformer-based approach that leverages fragmented GPS trajectory data to generate complete, behaviorally valid activity patterns for individuals working non-standard hours. Our method employs periodaware temporal embeddings and a transition-focused loss function specifically designed to capture the unique activity rhythms of shift workers and mitigate the inherent biases in conventional transportation datasets. Evaluation shows that the generated data achieves remarkable distributional alignment with GPS data from Los Angeles County (Average JSD < 0.02 for all evaluation metrics). By transforming incomplete GPS traces into complete, representative activity patterns, our approach provides transportation planners with a powerful data augmentation tool to fill critical gaps in understanding the 24/7 mobility needs of urban populations, enabling precise and inclusive transportation planning.",
      "tldr_zh": "本研究针对城市出行建模中轮班工人 (shift workers) 被系统性忽视的问题，揭示了该群体与传统 9-to-5 出行模式之间的显著差异。作者提出了一种基于 Transformer 的生成式方法，利用碎片化的 GPS 轨迹数据重建轮班工人完整且行为有效的活动模式。该模型通过引入周期感知时间嵌入 (period-aware temporal embeddings) 和针对转换优化的损失函数 (transition-focused loss function)，能够精准捕捉非标准工时的独特活动节奏。实验表明，生成数据在洛杉矶县 (Los Angeles County) 的测试中表现出极高的分布一致性，平均 Jensen-Shannon Divergence (JSD) 低于 0.02。该研究不仅为交通规划提供了强大的数据增强工具，还为理解城市人口全天候出行需求、实现包容性交通规划奠定了重要基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.19510v1",
      "published_date": "2025-07-17 02:33:30 UTC",
      "updated_date": "2025-07-17 02:33:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:23:40.841877+00:00"
    },
    {
      "arxiv_id": "2507.12701v1",
      "title": "Task-Specific Audio Coding for Machines: Machine-Learned Latent Features Are Codes for That Machine",
      "title_zh": "面向机器的任务特定音频编码：机器学习潜特征即为该机器的编码",
      "authors": [
        "Anastasia Kuznetsova",
        "Inseon Jang",
        "Wootaek Lim",
        "Minje Kim"
      ],
      "abstract": "Neural audio codecs, leveraging quantization algorithms, have significantly impacted various speech/audio tasks. While high-fidelity reconstruction is paramount for human perception, audio coding for machines (ACoM) prioritizes efficient compression and downstream task performance, disregarding perceptual nuances. This work introduces an efficient ACoM method that can compress and quantize any chosen intermediate feature representation of an already trained speech/audio downstream model. Our approach employs task-specific loss guidance alongside residual vector quantization (RVQ) losses, providing ultra-low bitrates (i.e., less than 200 bps) with a minimal loss of the downstream model performance. The resulting tokenizer is adaptable to various bitrates and model sizes for flexible deployment. Evaluated on automatic speech recognition and audio classification, our method demonstrates its efficacy and potential for broader task and architectural applicability through appropriate regularization.",
      "tldr_zh": "该研究针对机器音频编码 (Audio Coding for Machines, ACoM) 提出了一种高效的压缩方法，与传统优先考虑人类感知的神经音频编解码器不同，ACoM 更加注重压缩效率和下游任务的表现。该方法能够对已训练好的下游模型的中间特征表示进行压缩与量化，其核心结合了任务特定的损失引导与残差矢量量化 (Residual Vector Quantization, RVQ) 技术。实验证明，该方法在低于 200 bps 的超低比特率下，依然能保持极小的下游任务性能损耗。此外，所生成的 Tokenizer 具有良好的适配性，可根据不同的比特率和模型规模灵活部署。通过在自动语音识别 (ASR) 和音频分类任务上的验证，该研究展示了该方法在多种架构和广泛任务中的应用潜力和有效性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.12701v1",
      "published_date": "2025-07-17 00:32:07 UTC",
      "updated_date": "2025-07-17 00:32:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T05:23:43.817415+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 125,
  "processed_papers_count": 125,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T05:24:31.696324+00:00"
}
[
  {
    "arxiv_id": "2506.23431v2",
    "title": "Pipelined Decoder for Efficient Context-Aware Text Generation",
    "authors": [
      "Zixian Huang",
      "Chenxu Niu",
      "Yu Gu",
      "Gengyang Xiao",
      "Xinwei Huang",
      "Gong Cheng"
    ],
    "abstract": "As the basis of generative AI, an autoregressive model requires the generation of a new token depending on all the previously generated tokens, which brings high quality but also restricts the model to generate tokens one by one, forming a bottleneck limiting the generation speed. In this paper, we propose a new decoder architecture that efficiently generates text in parallel for context-aware generation tasks. Our proposed pipelined decoder initiates the generation of multiple subsequences simultaneously, and, at each time-step, it generates a new token for each subsequence to realize parallelism. Experiments on multiple text generation tasks, including question answering, text summarization, and keyphrase generation, show that our pipelined decoder significantly improves the generation speed without a significant loss of generation quality or additional memory consumption.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23431v2",
    "published_date": "2025-06-29 23:37:24 UTC",
    "updated_date": "2025-07-01 04:16:14 UTC"
  },
  {
    "arxiv_id": "2507.02956v1",
    "title": "A Representation Engineering Perspective on the Effectiveness of Multi-Turn Jailbreaks",
    "authors": [
      "Blake Bullwinkel",
      "Mark Russinovich",
      "Ahmed Salem",
      "Santiago Zanella-Beguelin",
      "Daniel Jones",
      "Giorgio Severi",
      "Eugenia Kim",
      "Keegan Hines",
      "Amanda Minnich",
      "Yonatan Zunger",
      "Ram Shankar Siva Kumar"
    ],
    "abstract": "Recent research has demonstrated that state-of-the-art LLMs and defenses remain susceptible to multi-turn jailbreak attacks. These attacks require only closed-box model access and are often easy to perform manually, posing a significant threat to the safe and secure deployment of LLM-based systems. We study the effectiveness of the Crescendo multi-turn jailbreak at the level of intermediate model representations and find that safety-aligned LMs often represent Crescendo responses as more benign than harmful, especially as the number of conversation turns increases. Our analysis indicates that at each turn, Crescendo prompts tend to keep model outputs in a \"benign\" region of representation space, effectively tricking the model into fulfilling harmful requests. Further, our results help explain why single-turn jailbreak defenses like circuit breakers are generally ineffective against multi-turn attacks, motivating the development of mitigations that address this generalization gap.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02956v1",
    "published_date": "2025-06-29 23:28:55 UTC",
    "updated_date": "2025-06-29 23:28:55 UTC"
  },
  {
    "arxiv_id": "2506.23424v1",
    "title": "Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting",
    "authors": [
      "Heitor R. Medeiros",
      "Hossein Sharifi-Noghabi",
      "Gabriel L. Oliveira",
      "Saghar Irandoust"
    ],
    "abstract": "Real-world time series often exhibit a non-stationary nature, degrading the performance of pre-trained forecasting models. Test-Time Adaptation (TTA) addresses this by adjusting models during inference, but existing methods typically update the full model, increasing memory and compute costs. We propose PETSA, a parameter-efficient method that adapts forecasters at test time by only updating small calibration modules on the input and output. PETSA uses low-rank adapters and dynamic gating to adjust representations without retraining. To maintain accuracy despite limited adaptation capacity, we introduce a specialized loss combining three components: (1) a robust term, (2) a frequency-domain term to preserve periodicity, and (3) a patch-wise structural term for structural alignment. PETSA improves the adaptability of various forecasting backbones while requiring fewer parameters than baselines. Experimental results on benchmark datasets show that PETSA achieves competitive or better performance across all horizons. Our code is available at: https://github.com/BorealisAI/PETSA",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Second Workshop on Test-Time Adaptation: Putting Updates to the Test! at ICML 2025, Vancouver, Canada. 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.23424v1",
    "published_date": "2025-06-29 23:09:35 UTC",
    "updated_date": "2025-06-29 23:09:35 UTC"
  },
  {
    "arxiv_id": "2506.23423v1",
    "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs",
    "authors": [
      "Felipe Nuti",
      "Tim Franzmeyer",
      "João Henriques"
    ],
    "abstract": "Past work has studied the effects of fine-tuning on large language models' (LLMs) overall performance on certain tasks. However, a quantitative and systematic method for analyzing its effect on individual outputs is still lacking. Here, we propose a new method for measuring the contribution that fine-tuning makes to individual LLM responses, assuming access to the original pre-trained model. Our method tracks the model's intermediate hidden states, providing a more fine-grained insight into the effects of fine-tuning than a simple comparison of final outputs from pre-trained and fine-tuned models. We introduce and theoretically analyze an exact decomposition of any fine-tuned LLM into a pre-training component and a fine-tuning component. Empirically, we find that model behavior and performance can be steered by up- or down-scaling the fine-tuning component during the forward pass. Motivated by this finding and our theoretical analysis, we define the Tuning Contribution (TuCo) as the ratio of the magnitudes of the fine-tuning component to the pre-training component. We observe that three prominent adversarial attacks on LLMs circumvent safety measures in a way that reduces TuCo, and that TuCo is consistently lower on prompts where these attacks succeed compared to those where they do not. This suggests that attenuating the effect of fine-tuning on model outputs plays a role in the success of such attacks. In summary, TuCo enables the quantitative study of how fine-tuning influences model behavior and safety, and vice versa.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.23423v1",
    "published_date": "2025-06-29 23:08:36 UTC",
    "updated_date": "2025-06-29 23:08:36 UTC"
  },
  {
    "arxiv_id": "2506.23419v1",
    "title": "BenchMake: Turn any scientific data set into a reproducible benchmark",
    "authors": [
      "Amanda S Barnard"
    ],
    "abstract": "Benchmark data sets are a cornerstone of machine learning development and applications, ensuring new methods are robust, reliable and competitive. The relative rarity of benchmark sets in computational science, due to the uniqueness of the problems and the pace of change in the associated domains, makes evaluating new innovations difficult for computational scientists. In this paper a new tool is developed and tested to potentially turn any of the increasing numbers of scientific data sets made openly available into a benchmark accessible to the community. BenchMake uses non-negative matrix factorisation to deterministically identify and isolate challenging edge cases on the convex hull (the smallest convex set that contains all existing data instances) and partitions a required fraction of matched data instances into a testing set that maximises divergence and statistical significance, across tabular, graph, image, signal and textual modalities. BenchMake splits are compared to establish splits and random splits using ten publicly available benchmark sets from different areas of science, with different sizes, shapes, distributions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 15 pages in Appendix, 15 figures, 5 tables, 57 references",
    "pdf_url": "https://arxiv.org/pdf/2506.23419v1",
    "published_date": "2025-06-29 22:56:48 UTC",
    "updated_date": "2025-06-29 22:56:48 UTC"
  },
  {
    "arxiv_id": "2506.23394v1",
    "title": "Teaching a Language Model to Speak the Language of Tools",
    "authors": [
      "Simeon Emanuilov"
    ],
    "abstract": "External tool integration through function-calling is essential for practical language model applications, yet most multilingual models lack reliable tool-use capabilities in non-English languages. Even state-of-the-art multilingual models struggle with determining when to use tools and generating the structured outputs required for function calls, often exhibiting language confusion when prompted in lower-resource languages. This work presents a methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. The approach involves continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples designed to support standardized protocols like MCP (Model Context Protocol). The research introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding, as verified on established Bulgarian benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready response formatting with clean, parsable function calls, contrasting with the verbose and inconsistent outputs of base models. The models, evaluation framework, and dataset are released to enable replication for other languages. This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23394v1",
    "published_date": "2025-06-29 20:47:27 UTC",
    "updated_date": "2025-06-29 20:47:27 UTC"
  },
  {
    "arxiv_id": "2506.23393v1",
    "title": "Hierarchical Memory Organization for Wikipedia Generation",
    "authors": [
      "Eugene J. Yu",
      "Dawei Zhu",
      "Yifan Song",
      "Xiangyu Wong",
      "Jiebin Zhang",
      "Wenxuan Shi",
      "Xiaoguang Li",
      "Qun Liu",
      "Sujian Li"
    ],
    "abstract": "Generating Wikipedia articles autonomously is a challenging task requiring the integration of accurate, comprehensive, and well-structured information from diverse sources. This paper introduces the Memory Organization-based Generation (MOG) framework, a novel approach to address these challenges by leveraging a hierarchical memory architecture. MOG extracts fine-grained memory units from web documents, recursively organizes them into a Wikipedia-style hierarchical structure, and uses this structure to guide the generation process. This ensures alignment between memory and the article outline, improving both informativeness and verifiability while minimizing hallucinations. Additionally, a citation module is implemented to enhance traceability by linking every generated sentence to specific memory units. Evaluations on our newly created WikiStart dataset demonstrate that MOG outperforms baseline methods in producing informative and reliable articles, making it particularly robust in real-world scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025 Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2506.23393v1",
    "published_date": "2025-06-29 20:22:49 UTC",
    "updated_date": "2025-06-29 20:22:49 UTC"
  },
  {
    "arxiv_id": "2507.02954v2",
    "title": "Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of Large Language Models on CFA Level III",
    "authors": [
      "Pranam Shetty",
      "Abhisek Upadhayaya",
      "Parth Mitesh Shah",
      "Srikanth Jagabathula",
      "Shilpi Nayak",
      "Anna Joo Fee"
    ],
    "abstract": "As financial institutions increasingly adopt Large Language Models (LLMs), rigorous domain-specific evaluation becomes critical for responsible deployment. This paper presents a comprehensive benchmark evaluating 23 state-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam - the gold standard for advanced financial reasoning. We assess both multiple-choice questions (MCQs) and essay-style responses using multiple prompting strategies including Chain-of-Thought and Self-Discover. Our evaluation reveals that leading models demonstrate strong capabilities, with composite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA Level III. These results, achieved under a revised, stricter essay grading methodology, indicate significant progress in LLM capabilities for high-stakes financial applications. Our findings provide crucial guidance for practitioners on model selection and highlight remaining challenges in cost-effective deployment and the need for nuanced interpretation of performance against professional benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at FinLLM @ IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02954v2",
    "published_date": "2025-06-29 19:54:57 UTC",
    "updated_date": "2025-09-22 17:05:03 UTC"
  },
  {
    "arxiv_id": "2506.23382v1",
    "title": "SIEDD: Shared-Implicit Encoder with Discrete Decoders",
    "authors": [
      "Vikram Rangarajan",
      "Shishira Maiya",
      "Max Ehrlich",
      "Abhinav Shrivastava"
    ],
    "abstract": "Implicit Neural Representations (INRs) offer exceptional fidelity for video compression by learning per-video optimized functions, but their adoption is crippled by impractically slow encoding times. Existing attempts to accelerate INR encoding often sacrifice reconstruction quality or crucial coordinate-level control essential for adaptive streaming and transcoding. We introduce SIEDD (Shared-Implicit Encoder with Discrete Decoders), a novel architecture that fundamentally accelerates INR encoding without these compromises. SIEDD first rapidly trains a shared, coordinate-based encoder on sparse anchor frames to efficiently capture global, low-frequency video features. This encoder is then frozen, enabling massively parallel training of lightweight, discrete decoders for individual frame groups, further expedited by aggressive coordinate-space sampling. This synergistic design delivers a remarkable 20-30X encoding speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while maintaining competitive reconstruction quality and compression ratios. Critically, SIEDD retains full coordinate-based control, enabling continuous resolution decoding and eliminating costly transcoding. Our approach significantly advances the practicality of high-fidelity neural video compression, demonstrating a scalable and efficient path towards real-world deployment. Our codebase is available at https://github.com/VikramRangarajan/SIEDD .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page at https://vikramrangarajan.github.io/SIEDD . Project code at https://github.com/VikramRangarajan/SIEDD",
    "pdf_url": "https://arxiv.org/pdf/2506.23382v1",
    "published_date": "2025-06-29 19:39:43 UTC",
    "updated_date": "2025-06-29 19:39:43 UTC"
  },
  {
    "arxiv_id": "2506.23377v2",
    "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs",
    "authors": [
      "Taejin Kim",
      "Siun-Chuon Mau",
      "Konrad Vesey"
    ],
    "abstract": "Large language models (LLMs) are used in a variety of mission-critical roles. Due to the rapidly developing nature of LLMs, there is a lack of quantifiable understanding of the bias and perspective associated with LLM output. Inspired by this need, this paper considers the broader issue of perspective or viewpoint of general text and perspective control of large-language model (LLM) output. Perspective-Dial consists of two main components: a (1) metric space, dubbed Perspective Space, that enables quantitative measurements of different perspectives regarding a topic, and the use of (2) Systematic Prompt Engineering that utilizes greedy-coordinate descent to control LLM output perspective based on measurement feedback from the Perspective Space. The empirical nature of the approach allows progress to side step a principled understanding of perspective or bias -- effectively quantifying and adjusting outputs for a variety of topics. Potential applications include detection, tracking and mitigation of LLM bias, narrative detection, sense making and tracking in public discourse, and debate bot advocating given perspective.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work performed at CACI INTL INC",
    "pdf_url": "https://arxiv.org/pdf/2506.23377v2",
    "published_date": "2025-06-29 19:26:37 UTC",
    "updated_date": "2025-07-12 17:57:39 UTC"
  },
  {
    "arxiv_id": "2506.23358v1",
    "title": "Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment",
    "authors": [
      "Pawel Renc",
      "Michal K. Grzeszczyk",
      "Linglong Qian",
      "Nassim Oufattole",
      "Jeff Rasley",
      "Arkadiusz Sitek"
    ],
    "abstract": "We present Federated Timeline Synthesis (FTS), a novel framework for training generative foundation models across distributed timeseries data applied to electronic health records (EHR). At its core, FTS represents patient history as tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding temporal, categorical, and continuous clinical information. Each institution trains an autoregressive transformer on its local PHTs and transmits only model weights to a central server. The server uses the generators to synthesize a large corpus of trajectories and train a Global Generator (GG), enabling zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS on five clinically meaningful prediction tasks using MIMIC-IV data, showing that models trained on synthetic data generated by GG perform comparably to those trained on real data. FTS offers strong privacy guarantees, scalability across institutions, and extensibility to diverse prediction and simulation tasks especially in healthcare, including counterfactual inference, early warning detection, and synthetic trial design.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "conference paper",
    "pdf_url": "https://arxiv.org/pdf/2506.23358v1",
    "published_date": "2025-06-29 18:29:59 UTC",
    "updated_date": "2025-06-29 18:29:59 UTC"
  },
  {
    "arxiv_id": "2506.23351v2",
    "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop",
    "authors": [
      "Tianxing Chen",
      "Kaixuan Wang",
      "Zhaohui Yang",
      "Yuhao Zhang",
      "Zanxin Chen",
      "Baijun Chen",
      "Wanxi Dong",
      "Ziyuan Liu",
      "Dong Chen",
      "Tianshuo Yang",
      "Haibao Yu",
      "Xiaokang Yang",
      "Yusen Qin",
      "Zhiqiang Xie",
      "Yao Mu",
      "Ping Luo",
      "Tian Nian",
      "Weiliang Deng",
      "Yiheng Ge",
      "Yibin Liu",
      "Zixuan Li",
      "Dehui Wang",
      "Zhixuan Liang",
      "Haohui Xie",
      "Rijie Zeng",
      "Yunfei Ge",
      "Peiqing Cong",
      "Guannan He",
      "Zhaoming Han",
      "Ruocheng Yin",
      "Jingxiang Guo",
      "Lunkai Lin",
      "Tianling Xu",
      "Hongzhe Bi",
      "Xuewu Lin",
      "Tianwei Lin",
      "Shujie Luo",
      "Keyu Li",
      "Ziyan Zhao",
      "Ke Fan",
      "Heyang Xu",
      "Bo Peng",
      "Wenlong Gao",
      "Dongjiang Li",
      "Feng Jin",
      "Hui Shen",
      "Jinming Li",
      "Chaowei Cui",
      "Yu Chen",
      "Yaxin Peng",
      "Lingdong Zeng",
      "Wenlong Dong",
      "Tengfei Li",
      "Weijie Ke",
      "Jun Chen",
      "Erdemt Bao",
      "Tian Lan",
      "Tenglong Liu",
      "Jin Yang",
      "Huiping Zhuang",
      "Baozhi Jia",
      "Shuai Zhang",
      "Zhengfeng Zou",
      "Fangheng Guan",
      "Tianyi Jia",
      "Ke Zhou",
      "Hongjiu Zhang",
      "Yating Han",
      "Cheng Fang",
      "Yixian Zou",
      "Chongyang Xu",
      "Qinglun Zhang",
      "Shen Cheng",
      "Xiaohe Wang",
      "Ping Tan",
      "Haoqiang Fan",
      "Shuaicheng Liu",
      "Jiaheng Chen",
      "Chuxuan Huang",
      "Chengliang Lin",
      "Kaijun Luo",
      "Boyu Yue",
      "Yi Liu",
      "Jinyu Chen",
      "Zichang Tan",
      "Liming Deng",
      "Shuo Xu",
      "Zijian Cai",
      "Shilong Yin",
      "Hao Wang",
      "Hongshan Liu",
      "Tianyang Li",
      "Long Shi",
      "Ran Xu",
      "Huilin Xu",
      "Zhengquan Zhang",
      "Congsheng Xu",
      "Jinchang Yang",
      "Feng Xu"
    ],
    "abstract": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in robotics, driven by the need for autonomous systems that can perceive, reason, and act in complex physical environments. While single-arm systems have shown strong task performance, collaborative dual-arm systems are essential for handling more intricate tasks involving rigid, deformable, and tactile-sensitive objects. To advance this goal, we launched the RoboTwin Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot platform, the competition consisted of three stages: Simulation Round 1, Simulation Round 2, and a final Real-World Round. Participants totally tackled 17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based scenarios. The challenge attracted 64 global teams and over 400 participants, producing top-performing solutions like SEM and AnchorDP3 and generating valuable insights into generalizable bimanual policy learning. This report outlines the competition setup, task design, evaluation methodology, key findings and future direction, aiming to support future research on robust and generalizable bimanual manipulation policies. The Challenge Webpage is available at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "Challenge Webpage: https://robotwin-benchmark.github.io/cvpr-2025-challenge/",
    "pdf_url": "https://arxiv.org/pdf/2506.23351v2",
    "published_date": "2025-06-29 17:56:41 UTC",
    "updated_date": "2025-07-03 03:30:42 UTC"
  },
  {
    "arxiv_id": "2506.23342v1",
    "title": "ATGen: A Framework for Active Text Generation",
    "authors": [
      "Akim Tsvigun",
      "Daniil Vasilev",
      "Ivan Tsvigun",
      "Ivan Lysenko",
      "Talgat Bektleuov",
      "Aleksandr Medvedev",
      "Uliana Vinogradova",
      "Nikita Severin",
      "Mikhail Mozikov",
      "Andrey Savchenko",
      "Rostislav Grigorev",
      "Ramil Kuleev",
      "Fedor Zhdanov",
      "Artem Shelmanov",
      "Ilya Makarov"
    ],
    "abstract": "Active learning (AL) has demonstrated remarkable potential in reducing the annotation effort required for training machine learning models. However, despite the surging popularity of natural language generation (NLG) tasks in recent years, the application of AL to NLG has been limited. In this paper, we introduce Active Text Generation (ATGen) - a comprehensive framework that bridges AL with text generation tasks, enabling the application of state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered annotation in NLG tasks using both human annotators and automatic annotation agents based on large language models (LLMs). The framework supports LLMs deployed as services, such as ChatGPT and Claude, or operated on-premises. Furthermore, ATGen provides a unified platform for smooth implementation and benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present evaluation results for state-of-the-art AL strategies across diverse settings and multiple text generation tasks. We show that ATGen reduces both the effort of human annotators and costs associated with API calls to LLM-based annotation agents. The code of the framework is available on GitHub under the MIT license. The video presentation is available at http://atgen-video.nlpresearch.group",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2025 System Demonstrations",
    "pdf_url": "https://arxiv.org/pdf/2506.23342v1",
    "published_date": "2025-06-29 17:27:48 UTC",
    "updated_date": "2025-06-29 17:27:48 UTC"
  },
  {
    "arxiv_id": "2506.23339v2",
    "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design",
    "authors": [
      "Malikussaid",
      "Hilal Hudan Nuha",
      "Isman Kurniawan"
    ],
    "abstract": "Large Language Models demonstrate substantial promise for advancing scientific discovery, yet their deployment in disciplines demanding factual precision and specialized domain constraints presents significant challenges. Within molecular design for pharmaceutical development, these models can propose innovative molecular modifications but frequently generate chemically infeasible structures. We introduce VALID-Mol, a comprehensive framework that integrates chemical validation with LLM-driven molecular design, achieving an improvement in valid chemical structure generation from 3% to 83%. Our methodology synthesizes systematic prompt optimization, automated chemical verification, and domain-adapted fine-tuning to ensure dependable generation of synthesizable molecules with enhanced properties. Our contribution extends beyond implementation details to provide a transferable methodology for scientifically-constrained LLM applications with measurable reliability enhancements. Computational analyses indicate our framework generates promising synthesis candidates with up to 17-fold predicted improvements in target binding affinity while preserving synthetic feasibility.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 1 figure, 1 algorithm, 5 tables, to be published in ISPACS 2025, unabridged version exists as arXiv:2506.23339v1",
    "pdf_url": "https://arxiv.org/pdf/2506.23339v2",
    "published_date": "2025-06-29 17:17:04 UTC",
    "updated_date": "2025-10-16 17:43:31 UTC"
  },
  {
    "arxiv_id": "2506.23334v2",
    "title": "Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation",
    "authors": [
      "Hongyi Pan",
      "Ziliang Hong",
      "Gorkem Durak",
      "Ziyue Xu",
      "Ulas Bagci"
    ],
    "abstract": "Federated learning (FL) has emerged as a promising paradigm for collaboratively training deep learning models across institutions without exchanging sensitive medical data. However, its effectiveness is often hindered by limited data availability and non-independent, identically distributed data across participating clients, which can degrade model performance and generalization. To address these challenges, we propose a generative AI based data augmentation framework that integrates synthetic image sharing into the federated training process for breast cancer diagnosis via ultrasound images. Specifically, we train two simple class-specific Deep Convolutional Generative Adversarial Networks: one for benign and one for malignant lesions. We then simulate a realistic FL setting using three publicly available breast ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are adopted as baseline FL algorithms. Experimental results show that incorporating a suitable number of synthetic images improved the average AUC from 0.9206 to 0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that excessive use of synthetic data reduced performance, underscoring the importance of maintaining a balanced ratio of real and synthetic samples. Our findings highlight the potential of generative AI based data augmentation to enhance FL results in the breast ultrasound image classification task.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23334v2",
    "published_date": "2025-06-29 17:05:50 UTC",
    "updated_date": "2025-07-08 21:03:53 UTC"
  },
  {
    "arxiv_id": "2506.23325v2",
    "title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs",
    "authors": [
      "Yitian Gong",
      "Luozhijie Jin",
      "Ruifan Deng",
      "Dong Zhang",
      "Xin Zhang",
      "Qinyuan Cheng",
      "Zhaoye Fei",
      "Shimin Li",
      "Xipeng Qiu"
    ],
    "abstract": "Speech codecs serve as bridges between speech signals and large language models. An ideal codec for speech language models should not only preserve acoustic information but also capture rich semantic information. However, existing speech codecs struggle to balance high-quality audio reconstruction with ease of modeling by language models. In this study, we analyze the limitations of previous codecs in balancing semantic richness and acoustic fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict between semantic and acoustic capabilities through multi-stage, multi-task learning. Experimental results demonstrate that XY-Tokenizer achieves performance in both semantic and acoustic tasks comparable to that of state-of-the-art codecs operating at similar bitrates, even though those existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer achieves strong text alignment, surpassing distillation-based semantic modeling methods such as SpeechTokenizer and Mimi, while maintaining a speaker similarity score of 0.83 between reconstructed and original audio. The reconstruction performance of XY-Tokenizer is comparable to that of BigCodec, the current state-of-the-art among acoustic-only codecs, which achieves a speaker similarity score of 0.84 at a similar bitrate. Code and models are available at https://github.com/gyt1145028706/XY-Tokenizer.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23325v2",
    "published_date": "2025-06-29 16:51:50 UTC",
    "updated_date": "2025-07-09 17:40:35 UTC"
  },
  {
    "arxiv_id": "2507.01059v1",
    "title": "Automated Vehicles Should be Connected with Natural Language",
    "authors": [
      "Xiangbo Gao",
      "Keshu Wu",
      "Hao Zhang",
      "Kexin Tian",
      "Yang Zhou",
      "Zhengzhong Tu"
    ],
    "abstract": "Multi-agent collaborative driving promises improvements in traffic safety and efficiency through collective perception and decision making. However, existing communication media -- including raw sensor data, neural network features, and perception results -- suffer limitations in bandwidth efficiency, information completeness, and agent interoperability. Moreover, traditional approaches have largely ignored decision-level fusion, neglecting critical dimensions of collaborative driving. In this paper we argue that addressing these challenges requires a transition from purely perception-oriented data exchanges to explicit intent and reasoning communication using natural language. Natural language balances semantic density and communication bandwidth, adapts flexibly to real-time conditions, and bridges heterogeneous agent platforms. By enabling the direct communication of intentions, rationales, and decisions, it transforms collaborative driving from reactive perception-data sharing into proactive coordination, advancing safety, efficiency, and transparency in intelligent transportation systems.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.01059v1",
    "published_date": "2025-06-29 16:41:19 UTC",
    "updated_date": "2025-06-29 16:41:19 UTC"
  },
  {
    "arxiv_id": "2506.23322v1",
    "title": "GaussMaster: An LLM-based Database Copilot System",
    "authors": [
      "Wei Zhou",
      "Ji Sun",
      "Xuanhe Zhou",
      "Guoliang Li",
      "Luyang Liu",
      "Hao Wu",
      "Tianyuan Wang"
    ],
    "abstract": "In the financial industry, data is the lifeblood of operations, and DBAs shoulder significant responsibilities for SQL tuning, database deployment, diagnosis, and service repair. In recent years, both database vendors and customers have increasingly turned to autonomous database platforms in an effort to alleviate the heavy workload of DBAs. However, existing autonomous database platforms are limited in their capabilities, primarily addressing single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual intervention remains a necessity for comprehensive database maintenance. GaussMaster aims to revolutionize this landscape by introducing an LLM-based database copilot system. This innovative solution is designed not only to assist developers in writing efficient SQL queries but also to provide comprehensive care for database services. When database instances exhibit abnormal behavior, GaussMaster is capable of orchestrating the entire maintenance process automatically. It achieves this by analyzing hundreds of metrics and logs, employing a Tree-of-thought approach to identify root causes, and invoking appropriate tools to resolve issues. We have successfully implemented GaussMaster in real-world scenarios, such as the banking industry, where it has achieved zero human intervention for over 34 database maintenance scenarios. In this paper, we present significant improvements in these tasks with code at https://gitcode.com/opengauss/openGauss-GaussMaster.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.DB",
    "comment": "We welcome contributions from the community. For reference, please see the code at: https://gitcode.com/opengauss/openGauss-GaussMaster",
    "pdf_url": "https://arxiv.org/pdf/2506.23322v1",
    "published_date": "2025-06-29 16:39:31 UTC",
    "updated_date": "2025-06-29 16:39:31 UTC"
  },
  {
    "arxiv_id": "2506.23314v1",
    "title": "Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance",
    "authors": [
      "Joner Assolin",
      "Gabriel Canto",
      "Diego Kreutz",
      "Eduardo Feitosa",
      "Hendrio Bragança",
      "Angelo Nogueira",
      "Vanderson Rocha"
    ],
    "abstract": "Malware detection in Android systems requires both cybersecurity expertise and machine learning (ML) techniques. Automated Machine Learning (AutoML) has emerged as an approach to simplify ML development by reducing the need for specialized knowledge. However, current AutoML solutions typically operate as black-box systems with limited transparency, interpretability, and experiment traceability. To address these limitations, we present MH-AutoML, a domain-specific framework for Android malware detection. MH-AutoML automates the entire ML pipeline, including data preprocessing, feature engineering, algorithm selection, and hyperparameter tuning. The framework incorporates capabilities for interpretability, debugging, and experiment tracking that are often missing in general-purpose solutions. In this study, we compare MH-AutoML against seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT, HyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML achieves better recall rates while providing more transparency and control. The framework maintains computational efficiency comparable to other solutions, making it suitable for cybersecurity applications where both performance and explainability matter.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "18 pages, 10 figures, 7 tabelas, paper submitted to JBCS",
    "pdf_url": "https://arxiv.org/pdf/2506.23314v1",
    "published_date": "2025-06-29 16:12:41 UTC",
    "updated_date": "2025-06-29 16:12:41 UTC"
  },
  {
    "arxiv_id": "2506.23306v2",
    "title": "GATSim: Urban Mobility Simulation with Generative Agents",
    "authors": [
      "Qi Liu",
      "Can Li",
      "Wanjing Ma"
    ],
    "abstract": "Traditional agent-based urban mobility simulations often rely on rigid rule-based systems that struggle to capture the complexity, adaptability, and behavioral diversity inherent in human travel decision making. Recent advancements in large language models and AI agent technologies present new opportunities to develop agents with enhanced reasoning capabilities, persistent memory, and adaptive learning. We introduce GATSim (Generative-Agent Transport Simulation), a novel framework that leverages these advancements to simulate urban mobility using generative agents with rich, human-like behaviors. Unlike conventional approaches, GATSim agents are characterized by diverse socioeconomic profiles, individual lifestyles, and evolving preferences shaped through psychologically informed memory systems, tool usage, and lifelong learning. The main contributions of this work are: (1) a comprehensive architecture that integrates an urban mobility foundation model with agent cognitive systems and a transport simulation environment; (2) a hierarchical memory designed for efficient retrieval of contextually relevant information, incorporating spatial and temporal associations, keyword matching, and semantic relevance; (3) innovative planning and reactive mechanisms for modeling adaptive mobility behaviors which integrate a multi-scale reflection process to transform specific travel experiences into generalized behavioral insights. We implement a prototype system and conduct systematic validation, demonstrating that generative agents produce believable and coherent travel behaviors. Experimental results indicate that generative agents perform at least as well as human annotators with 92\\% posterior probability, while naturally producing realistic macroscopic traffic patterns. The code for the prototype implementation is publicly available at https://github.com/qiliuchn/gatsim.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23306v2",
    "published_date": "2025-06-29 15:52:16 UTC",
    "updated_date": "2025-07-18 04:20:16 UTC"
  },
  {
    "arxiv_id": "2506.23298v3",
    "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification",
    "authors": [
      "Xing Shen",
      "Justin Szeto",
      "Mingyang Li",
      "Hengguan Huang",
      "Tal Arbel"
    ],
    "abstract": "Multimodal large language models (MLLMs) have enormous potential to perform few-shot in-context learning in the context of medical image analysis. However, safe deployment of these models into real-world clinical practice requires an in-depth analysis of the accuracies of their predictions, and their associated calibration errors, particularly across different demographic subgroups. In this work, we present the first investigation into the calibration biases and demographic unfairness of MLLMs' predictions and confidence scores in few-shot in-context learning for medical image classification. We introduce CALIN, an inference-time calibration method designed to mitigate the associated biases. Specifically, CALIN estimates the amount of calibration needed, represented by calibration matrices, using a bi-level procedure: progressing from the population level to the subgroup level prior to inference. It then applies this estimation to calibrate the predicted confidence scores during inference. Experimental results on three medical imaging datasets: PAPILA for fundus image classification, HAM10000 for skin cancer classification, and MIMIC-CXR for chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair confidence calibration in its prediction, while improving its overall prediction accuracies and exhibiting minimum fairness-utility trade-off. Our codebase can be found at https://github.com/xingbpshen/medical-calibration-fairness-mllm.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Preprint version. The peer-reviewed version of this paper has been accepted to MICCAI 2025 main conference",
    "pdf_url": "https://arxiv.org/pdf/2506.23298v3",
    "published_date": "2025-06-29 15:37:17 UTC",
    "updated_date": "2025-07-17 18:00:33 UTC"
  },
  {
    "arxiv_id": "2506.23296v1",
    "title": "Securing AI Systems: A Guide to Known Attacks and Impacts",
    "authors": [
      "Naoto Kiribuchi",
      "Kengo Zenitani",
      "Takayuki Semitsu"
    ],
    "abstract": "Embedded into information systems, artificial intelligence (AI) faces security threats that exploit AI-specific vulnerabilities. This paper provides an accessible overview of adversarial attacks unique to predictive and generative AI systems. We identify eleven major attack types and explicitly link attack techniques to their impacts -- including information leakage, system compromise, and resource exhaustion -- mapped to the confidentiality, integrity, and availability (CIA) security triad. We aim to equip researchers, developers, security practitioners, and policymakers, even those without specialized AI security expertise, with foundational knowledge to recognize AI-specific risks and implement effective defenses, thereby enhancing the overall security posture of AI systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "34 pages, 16 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.23296v1",
    "published_date": "2025-06-29 15:32:03 UTC",
    "updated_date": "2025-06-29 15:32:03 UTC"
  },
  {
    "arxiv_id": "2506.23293v2",
    "title": "Self-Organizing Language",
    "authors": [
      "P. Myles Eugenio",
      "Anthony Beavers"
    ],
    "abstract": "We introduce a novel paradigm of emergent local memory. It is a continuous-learning completely-parallel content-addressable memory encoding global order. It demonstrates how local constraints on uncoordinated learning can produce topologically protected memories realizing emergent symbolic order. It is therefore a neuro-symbolic bridge.\n  It further has the ability to produce human language without data, by exploiting its own self-organizing dynamics. It teaches us that words arise as a side-effect of emergent symbolic order, and that human language patterns at all structural levels reflect a universal mechanism of word formation (which is subregular). This work answers essential questions about the existence \\& origin of all the human language data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.CL",
    "comment": "27 pages, 14 figures; Name changed from \"Objective-Free Local Learning and Emergent Language Structure in Thinking Machines\"",
    "pdf_url": "https://arxiv.org/pdf/2506.23293v2",
    "published_date": "2025-06-29 15:29:13 UTC",
    "updated_date": "2025-11-14 19:35:28 UTC"
  },
  {
    "arxiv_id": "2506.23286v1",
    "title": "Not All Explanations for Deep Learning Phenomena Are Equally Valuable",
    "authors": [
      "Alan Jeffares",
      "Mihaela van der Schaar"
    ],
    "abstract": "Developing a better understanding of surprising or counterintuitive phenomena has constituted a significant portion of deep learning research in recent years. These include double descent, grokking, and the lottery ticket hypothesis -- among many others. Works in this area often develop ad hoc hypotheses attempting to explain these observed phenomena on an isolated, case-by-case basis. This position paper asserts that, in many prominent cases, there is little evidence to suggest that these phenomena appear in real-world applications and these efforts may be inefficient in driving progress in the broader field. Consequently, we argue against viewing them as isolated puzzles that require bespoke resolutions or explanations. However, despite this, we suggest that deep learning phenomena do still offer research value by providing unique settings in which we can refine our broad explanatory theories of more general deep learning principles. This position is reinforced by analyzing the research outcomes of several prominent examples of these phenomena from the recent literature. We revisit the current norms in the research community in approaching these problems and propose practical recommendations for future research, aiming to ensure that progress on deep learning phenomena is well aligned with the ultimate pragmatic goal of progress in the broader field of deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2025 for oral presentation",
    "pdf_url": "https://arxiv.org/pdf/2506.23286v1",
    "published_date": "2025-06-29 15:18:56 UTC",
    "updated_date": "2025-06-29 15:18:56 UTC"
  },
  {
    "arxiv_id": "2506.23276v2",
    "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games",
    "authors": [
      "David Guzman Piedrahita",
      "Yongjin Yang",
      "Mrinmaya Sachan",
      "Giorgia Ramponi",
      "Bernhard Schölkopf",
      "Zhijing Jin"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed as autonomous agents, understanding their cooperation and social mechanisms is becoming increasingly important. In particular, how LLMs balance self-interest and collective well-being is a critical challenge for ensuring alignment, robustness, and safe deployment. In this paper, we examine the challenge of costly sanctioning in multi-agent LLM systems, where an agent must decide whether to invest its own resources to incentivize cooperation or penalize defection. To study this, we adapt a public goods game with institutional choice from behavioral economics, allowing us to observe how different LLMs navigate social dilemmas over repeated interactions. Our analysis reveals four distinct behavioral patterns among models: some consistently establish and sustain high levels of cooperation, others fluctuate between engagement and disengagement, some gradually decline in cooperative behavior over time, and others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we find that reasoning LLMs, such as the o1 series, struggle significantly with cooperation, whereas some traditional LLMs consistently achieve high levels of cooperation. These findings suggest that the current approach to improving LLMs, which focuses on enhancing their reasoning capabilities, does not necessarily lead to cooperation, providing valuable insights for deploying LLM agents in environments that require sustained collaboration. Our code is available at https://github.com/davidguzmanp/SanctSim",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at COLM 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.23276v2",
    "published_date": "2025-06-29 15:02:47 UTC",
    "updated_date": "2025-07-24 13:13:24 UTC"
  },
  {
    "arxiv_id": "2506.23275v2",
    "title": "Why Settle for One? Text-to-ImageSet Generation and Evaluation",
    "authors": [
      "Chengyou Jia",
      "Xin Shen",
      "Zhuohang Dang",
      "Zhuohang Dang",
      "Changliang Xia",
      "Weijia Wu",
      "Xinyu Zhang",
      "Hangwei Qian",
      "Ivor W. Tsang",
      "Minnan Luo"
    ],
    "abstract": "Despite remarkable progress in Text-to-Image models, many real-world applications require generating coherent image sets with diverse consistency requirements. Existing consistent methods often focus on a specific domain with specific aspects of consistency, which significantly constrains their generalizability to broader applications. In this paper, we propose a more challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate sets of images that meet various consistency requirements based on user instructions. To systematically study this problem, we first introduce $\\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories, providing comprehensive coverage for T2IS generation. Building on this, we propose $\\textbf{T2IS-Eval}$, an evaluation framework that transforms user instructions into multifaceted assessment criteria and employs effective evaluators to adaptively assess consistency fulfillment between criteria and generated sets. Subsequently, we propose $\\textbf{AutoT2IS}$, a training-free framework that maximally leverages pretrained Diffusion Transformers' in-context capabilities to harmonize visual elements to satisfy both image-level prompt alignment and set-level visual consistency. Extensive experiments on T2IS-Bench reveal that diverse consistency challenges all existing methods, while our AutoT2IS significantly outperforms current generalized and even specialized approaches. Our method also demonstrates the ability to enable numerous underexplored real-world applications, confirming its substantial practical value. Visit our project in https://chengyou-jia.github.io/T2IS-Home.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23275v2",
    "published_date": "2025-06-29 15:01:16 UTC",
    "updated_date": "2025-09-25 07:49:27 UTC"
  },
  {
    "arxiv_id": "2506.23274v3",
    "title": "Real-Time Progress Prediction in Reasoning Language Models",
    "authors": [
      "Hans Peter Lynsgøe Raaschou-jensen",
      "Constanza Fierro",
      "Anders Søgaard"
    ],
    "abstract": "Recent advances in reasoning language models -- particularly those that use long, latent chains of thought -- have demonstrated remarkable capabilities in complex, agentic tasks. However, as these models operate over increasingly extended time horizons, their internal progress becomes opaque to users, complicating expectation management and real-time oversight. In this work, we investigate whether real-time progress prediction is feasible. We discretize progress and train a linear probe to classify reasoning states. We then introduce a two-stage fine-tuning approach that enables reasoning models to generate progress estimates (0$\\rightarrow$100\\%) during inference. Our best fine-tuned model achieves an average error of 10\\% for sequences less than 16,000 tokens, offering a practical mechanism for monitoring and interpreting model reasoning in real time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23274v3",
    "published_date": "2025-06-29 15:01:01 UTC",
    "updated_date": "2025-10-08 12:11:48 UTC"
  },
  {
    "arxiv_id": "2506.23273v2",
    "title": "FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis",
    "authors": [
      "Quang Hung Nguyen",
      "Phuong Anh Trinh",
      "Phan Quoc Hung Mai",
      "Tuan Phong Trinh"
    ],
    "abstract": "Despite the advancements of large language models, text2sql still faces many challenges, particularly with complex and domain-specific queries. In finance, database designs and financial reporting layouts vary widely between financial entities and countries, making text2sql even more challenging. We present FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries over financial statements. Tailored to local standards like VAS, it combines large and small language models in a multi-agent setup for entity extraction, SQL generation, and self-correction. We build a domain-specific database and evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves 61.33\\% accuracy with sub-4-second response times on consumer hardware, outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient solution for financial analysis, making AI-powered querying accessible to Vietnamese enterprises.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for The 18th International Natural Language Generation Conference (INLG)",
    "pdf_url": "https://arxiv.org/pdf/2506.23273v2",
    "published_date": "2025-06-29 14:55:21 UTC",
    "updated_date": "2025-09-07 17:52:24 UTC"
  },
  {
    "arxiv_id": "2507.02952v1",
    "title": "Strategies for Resource Allocation of Two Competing Companies using Genetic Algorithm",
    "authors": [
      "Wing Keung Cheung",
      "Kwok Yip Szeto"
    ],
    "abstract": "We investigate various strategic locations of shops in shopping malls in a metropolis with the aim of finding the best strategy for final dominance of market share by a company in a competing environment. The problem is posed in the context of two competing supermarket chains in a metropolis, described in the framework of the two-dimensional Ising model. Evolutionary Algorithm is used to encode the ensemble of initial configurations and Monte Carlo method is used to evolve the pattern. Numerical simulation indicates that initial patterns with certain topological properties do evolve faster to market dominance. The description of these topological properties is given and suggestions are made on the initial pattern so as to evolve faster to market dominance.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02952v1",
    "published_date": "2025-06-29 14:52:35 UTC",
    "updated_date": "2025-06-29 14:52:35 UTC"
  },
  {
    "arxiv_id": "2506.23270v1",
    "title": "Token Activation Map to Visually Explain Multimodal LLMs",
    "authors": [
      "Yi Li",
      "Hualiang Wang",
      "Xinpeng Ding",
      "Haonan Wang",
      "Xiaomeng Li"
    ],
    "abstract": "Multimodal large language models (MLLMs) are broadly empowering various fields. Despite their advancements, the explainability of MLLMs remains less explored, hindering deeper understanding, model credibility, and effective visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that produce a single output, MLLMs generate sequences of tokens progressively, where each generated token depends on the previous context. Therefore, earlier context tokens can introduce redundant activations that interfere with the explanation of later tokens beyond their original information. Existing studies often overlook this issue, but our observations reveal that these redundant correlations can significantly hurt the reliability of explanations. To address this, we propose an estimated causal inference method to mitigate the interference of context to achieve high-quality MLLM explanation, with a novel rank Gaussian filter to further reduce activation noises. We term this method Token Activation Map (TAM) to highlight the consideration of interactions between tokens. TAM also indicates that it excels at explaining multiple tokens of MLLM, which is different from the Class Activation Map (CAM) for a single prediction. Our TAM method significantly outperforms existing SoTA methods, showcasing high-quality visualization results that can be utilized for various scenarios, such as object localization, failure case analysis, video visualization, MLLMs visual comparison, and model understanding (e.g., color, shape, action, location, visual reasoning, multi-turn conversation, etc). The code is available atgithub.com/xmed-lab/TAM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICCV2025 Accepted",
    "pdf_url": "https://arxiv.org/pdf/2506.23270v1",
    "published_date": "2025-06-29 14:50:45 UTC",
    "updated_date": "2025-06-29 14:50:45 UTC"
  },
  {
    "arxiv_id": "2506.23260v2",
    "title": "From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows",
    "authors": [
      "Mohamed Amine Ferrag",
      "Norbert Tihanyi",
      "Djallel Hamouda",
      "Leandros Maglaras",
      "Abderrahmane Lakas",
      "Merouane Debbah"
    ],
    "abstract": "Autonomous AI agents powered by large language models (LLMs) with structured function-calling interfaces enable real-time data retrieval, computation, and multi-step orchestration. However, the rapid growth of plugins, connectors, and inter-agent protocols has outpaced security practices, leading to brittle integrations that rely on ad-hoc authentication, inconsistent schemas, and weak validation. This survey introduces a unified end-to-end threat model for LLM-agent ecosystems, covering host-to-tool and agent-to-agent communications. We systematically categorize more than thirty attack techniques spanning input manipulation, model compromise, system and privacy attacks, and protocol-level vulnerabilities. For each category, we provide a formal threat formulation defining attacker capabilities, objectives, and affected system layers. Representative examples include Prompt-to-SQL injections and the Toxic Agent Flow exploit in GitHub MCP servers. We analyze attack feasibility, review existing defenses, and discuss mitigation strategies such as dynamic trust management, cryptographic provenance tracking, and sandboxed agent interfaces. The framework is validated through expert review and cross-mapping with real-world incidents and public vulnerability repositories, including CVE and NIST NVD. Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "The paper is published in ICT Express (Elsevier)",
    "pdf_url": "https://arxiv.org/pdf/2506.23260v2",
    "published_date": "2025-06-29 14:32:32 UTC",
    "updated_date": "2025-12-14 20:29:54 UTC"
  },
  {
    "arxiv_id": "2506.23254v1",
    "title": "PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution",
    "authors": [
      "Aradhana Mishra",
      "Bumshik Lee"
    ],
    "abstract": "Diffusion-model-based image super-resolution techniques often face a trade-off between realistic image generation and computational efficiency. This issue is exacerbated when inference times by decreasing sampling steps, resulting in less realistic and hazy images. To overcome this challenge, we introduce a novel diffusion model named PixelBoost that underscores the significance of embracing the stochastic nature of Brownian motion in advancing image super-resolution, resulting in a high degree of realism, particularly focusing on texture and edge definitions. By integrating controlled stochasticity into the training regimen, our proposed model avoids convergence to local optima, effectively capturing and reproducing the inherent uncertainty of image textures and patterns. Our proposed model demonstrates superior objective results in terms of learned perceptual image patch similarity (LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR), structural similarity index measure (SSIM), as well as visual quality. To determine the edge enhancement, we evaluated the gradient magnitude and pixel value, and our proposed model exhibited a better edge reconstruction capability. Additionally, our model demonstrates adaptive learning capabilities by effectively adjusting to Brownian noise patterns and introduces a sigmoidal noise sequencing method that simplifies training, resulting in faster inference speeds.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23254v1",
    "published_date": "2025-06-29 14:22:38 UTC",
    "updated_date": "2025-06-29 14:22:38 UTC"
  },
  {
    "arxiv_id": "2507.00079v1",
    "title": "VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems",
    "authors": [
      "Ethan Smyth",
      "Alessandro Suglia"
    ],
    "abstract": "Open-endedness is an active field of research in the pursuit of capable Artificial General Intelligence (AGI), allowing models to pursue tasks of their own choosing. Simultaneously, recent advancements in Large Language Models (LLMs) such as GPT-4o [9] have allowed such models to be capable of interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use of such features, providing an LLM with pixel data of an agent's POV to parse the environment and allow it to solve tasks. This paper proposes that providing these visual inputs to a model gives it greater ability to interpret spatial environments, and as such, can increase the number of tasks it can successfully perform, extending its open-ended potential. To this aim, this paper proposes VoyagerVision -- a multi-modal model capable of creating structures within Minecraft using screenshots as a form of visual feedback, building on the foundation of Voyager. VoyagerVision was capable of creating an average of 2.75 unique structures within fifty iterations of the system, as Voyager was incapable of this, it is an extension in an entirely new direction. Additionally, in a set of building unit tests VoyagerVision was successful in half of all attempts in flat worlds, with most failures arising in more complex structures. Project website is available at https://esmyth-dev.github.io/VoyagerVision.github.io/",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "website: https://esmyth-dev.github.io/VoyagerVision.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2507.00079v1",
    "published_date": "2025-06-29 14:16:11 UTC",
    "updated_date": "2025-06-29 14:16:11 UTC"
  },
  {
    "arxiv_id": "2506.23247v1",
    "title": "Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification",
    "authors": [
      "James Hinns",
      "David Martens"
    ],
    "abstract": "Deep learning dominates image classification tasks, yet understanding how models arrive at predictions remains a challenge. Much research focuses on local explanations of individual predictions, such as saliency maps, which visualise the influence of specific pixels on a model's prediction. However, reviewing many of these explanations to identify recurring patterns is infeasible, while global methods often oversimplify and miss important local behaviours. To address this, we propose Segment Attribution Tables (SATs), a method for summarising local saliency explanations into (semi-)global insights. SATs take image segments (such as \"eyes\" in Chihuahuas) and leverage saliency maps to quantify their influence. These segments highlight concepts the model relies on across instances and reveal spurious correlations, such as reliance on backgrounds or watermarks, even when out-of-distribution test performance sees little change. SATs can explain any classifier for which a form of saliency map can be produced, using segmentation maps that provide named segments. SATs bridge the gap between oversimplified global summaries and overly detailed local explanations, offering a practical tool for analysing and debugging image classifiers.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23247v1",
    "published_date": "2025-06-29 14:11:02 UTC",
    "updated_date": "2025-06-29 14:11:02 UTC"
  },
  {
    "arxiv_id": "2507.00078v1",
    "title": "The language of time: a language model perspective on time-series foundation models",
    "authors": [
      "Yi Xie",
      "Yun Xiong",
      "Zejian Shi",
      "Hao Niu",
      "Zhengfu Liu"
    ],
    "abstract": "With the rise of large language models, the paradigm of training foundation models with massive parameter counts on vast datasets has been adopted in multiple domains to achieve remarkable success. Time series foundation models represent a significant extension of this paradigm, demonstrating exceptional expressive power, generalization, and cross-domain transferability. However, this gives rise to a fundamental paradox: time series data reflect distinct dynamical systems, making cross-domain transfer intuitively implausible, yet this is contradicted by the models' empirical success. To resolve this paradox, this paper investigates, from both theoretical and experimental perspectives, the representation learning mechanisms and generalization capabilities of patch-based time series foundation models. We argue that such models are not merely applying a new architecture but are fundamentally generalizing the representation paradigm of language models by extending deterministic vector-based representations to latent probabilistic distributional forms. Our theoretical analysis supports this framework by demonstrating that continuous time-series patches can be faithfully quantized into a discrete vocabulary whose key statistical properties are highly consistent with those of natural language. This generalization allows time series models to inherit the robust representation and transfer abilities of large language models, thereby explaining their superior performance in temporal tasks. Ultimately, our work provides a rigorous theoretical cornerstone for understanding, evaluating, and improving the safety and reliability of large-scale time series foundation models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00078v1",
    "published_date": "2025-06-29 14:03:34 UTC",
    "updated_date": "2025-06-29 14:03:34 UTC"
  },
  {
    "arxiv_id": "2506.23236v1",
    "title": "VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions",
    "authors": [
      "Marko Mihajlovic",
      "Siwei Zhang",
      "Gen Li",
      "Kaifeng Zhao",
      "Lea Müller",
      "Siyu Tang"
    ],
    "abstract": "Parametric human body models play a crucial role in computer graphics and vision, enabling applications ranging from human motion analysis to understanding human-environment interactions. Traditionally, these models use surface meshes, which pose challenges in efficiently handling interactions with other geometric entities, such as objects and scenes, typically represented as meshes or point clouds. To address this limitation, recent research has explored volumetric neural implicit body models. However, existing works are either insufficiently robust for complex human articulations or impose high computational and memory costs, limiting their widespread use. To this end, we introduce VolumetricSMPL, a neural volumetric body model that leverages Neural Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike prior approaches that rely on large MLPs, NBW dynamically blends a small set of learned weight matrices using predicted shape- and pose-dependent coefficients, significantly improving computational efficiency while preserving expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy, and a Signed Distance Function (SDF) for efficient and differentiable contact modeling. We demonstrate VolumetricSMPL's strengths across four challenging tasks: (1) reconstructing human-object interactions from in-the-wild images, (2) recovering human meshes in 3D scenes from egocentric views, (3) scene-constrained motion synthesis, and (4) resolving self-intersections. Our results highlight its broad applicability and significant performance and efficiency gains.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "[ICCV 2025] https://markomih.github.io/VolumetricSMPL",
    "pdf_url": "https://arxiv.org/pdf/2506.23236v1",
    "published_date": "2025-06-29 13:48:38 UTC",
    "updated_date": "2025-06-29 13:48:38 UTC"
  },
  {
    "arxiv_id": "2506.23219v1",
    "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding",
    "authors": [
      "Jie Feng",
      "Shengyuan Wang",
      "Tianhui Liu",
      "Yanxin Xi",
      "Yong Li"
    ],
    "abstract": "Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In $\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.23219v1",
    "published_date": "2025-06-29 13:04:27 UTC",
    "updated_date": "2025-06-29 13:04:27 UTC"
  },
  {
    "arxiv_id": "2506.23210v4",
    "title": "FedRef: Communication-Efficient Bayesian Fine-Tuning using a Reference Model",
    "authors": [
      "Taehwan Yoon",
      "Bongjun Choi",
      "Wesley De Neve"
    ],
    "abstract": "Federated learning (FL) collaboratively trains artificial intelligence (AI) models to ensure user data privacy. Sharing only model updates generated from local training on client data with the server enhances user data privacy. However, model performance may suffer due to data and system heterogeneity among clients in FL scenarios. Previous studies have proposed model optimization, fine-tuning, and personalization to achieve improved model performance. Despite these efforts, models resulting from FL scenarios often exhibit catastrophic forgetting, which increases the communication and computational costs of clients for model optimization and raises energy consumption. To address these challenges, we propose a reference model-based fine-tuning method for federated learning that overcomes catastrophic forgetting in each round. Our method is derived from Bayesian parameter-efficient transfer learning and includes an proximal term. It employs a reference model that incorporates previous model parameters and reviews previous global features in the model optimization step to mitigate catastrophic forgetting. As a result, our method achieves higher model performance and lower communication and computational costs for clients than existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 16 equations, 5 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.23210v4",
    "published_date": "2025-06-29 12:41:11 UTC",
    "updated_date": "2025-11-24 06:24:33 UTC"
  },
  {
    "arxiv_id": "2506.23203v1",
    "title": "Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver",
    "authors": [
      "Feng Shu",
      "Jiatong Bai",
      "Di Wu",
      "Wei Zhu",
      "Bin Deng",
      "Fuhui Zhou",
      "Jiangzhou Wang"
    ],
    "abstract": "As a green MIMO structure, massive H$^2$AD is viewed as a potential technology for the future 6G wireless network. For such a structure, it is a challenging task to design a low-complexity and high-performance fusion of target direction values sensed by different sub-array groups with fewer use of prior knowledge. To address this issue, a lightweight Cramer-Rao lower bound (CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse CRLB of each subarray using antenna number reciprocals to eliminate real-time CRLB computation. This reduces complexity and prior knowledge dependence while preserving fusion performance. Moreover, a multi-branch deep neural network (MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by leveraging candidate angles from multiple subarrays. The subarray-specific branch networks are integrated with a shared regression module to effectively eliminate pseudo-solutions and fuse true angles. Simulation results show that the proposed CRLB-ratio-WF method achieves DOA sensing performance comparable to CRLB-based methods, while significantly reducing the reliance on prior knowledge. More notably, the proposed MBDNN has superior performance in low-SNR ranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in estimation accuracy compared to CRLB-ratio-WF method.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23203v1",
    "published_date": "2025-06-29 12:14:59 UTC",
    "updated_date": "2025-06-29 12:14:59 UTC"
  },
  {
    "arxiv_id": "2507.02951v1",
    "title": "Bittensor Protocol: The Bitcoin in Decentralized Artificial Intelligence? A Critical and Empirical Analysis",
    "authors": [
      "Elizabeth Lui",
      "Jiahao Sun"
    ],
    "abstract": "This paper investigates whether Bittensor can be considered the Bitcoin of decentralized Artificial Intelligence by directly comparing its tokenomics, decentralization properties, consensus mechanism, and incentive structure against those of Bitcoin. Leveraging on-chain data from all 64 active Bittensor subnets, we first document considerable concentration in both stake and rewards. We further show that rewards are overwhelmingly driven by stake, highlighting a clear misalignment between quality and compensation. As a remedy, we put forward a series of two-pronged protocol-level interventions. For incentive realignment, our proposed solutions include performance-weighted emission split, composite scoring, and a trust-bonus multiplier. As for mitigating security vulnerability due to stake concentration, we propose and empirically validate stake cap at the 88th percentile, which elevates the median coalition size required for a 51-percent attack and remains robust across daily, weekly, and monthly snapshots.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "MARBLE 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02951v1",
    "published_date": "2025-06-29 12:07:48 UTC",
    "updated_date": "2025-06-29 12:07:48 UTC"
  },
  {
    "arxiv_id": "2507.22904v2",
    "title": "SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches",
    "authors": [
      "Ehsan Latif",
      "Zirak Khan",
      "Xiaoming Zhai"
    ],
    "abstract": "Scientific sketches (e.g., models) offer a powerful lens into students' conceptual understanding, yet AI-powered automated assessment of such free-form, visually diverse artifacts remains a critical challenge. Existing solutions often treat sketch evaluation as either an image classification task or monolithic vision-language models, which lack interpretability, pedagogical alignment, and adaptability across cognitive levels. To address these limitations, we present SketchMind, a cognitively grounded, multi-agent framework for evaluating and improving student-drawn scientific sketches. SketchMind comprises modular agents responsible for rubric parsing, sketch perception, cognitive alignment, and iterative feedback with sketch modification, enabling personalized and transparent evaluation. We evaluate SketchMind on a curated dataset of 3,575 student-generated sketches across six science assessment items with different highest order of Bloom's level that require students to draw models to explain phenomena. Compared to baseline GPT-4o performance without SRG (average accuracy: 55.6%), and with SRG integration achieves 77.1% average accuracy (+21.4% average absolute gain). We also demonstrate that multi-agent orchestration with SRG enhances SketchMind performance, for example, GPT-4.1 gains an average 8.9% increase in sketch prediction accuracy, outperforming single-agent pipelines across all items. Human evaluators rated the feedback and co-created sketches generated by \\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5, significantly higher than those of baseline models (e.g., 2.3 for GPT-4o). Experts noted the system's potential to meaningfully support conceptual growth through guided revision. Our code and (pending approval) dataset will be released to support reproducibility and future research in AI-driven education.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Submitted to NeurIPS2025",
    "pdf_url": "https://arxiv.org/pdf/2507.22904v2",
    "published_date": "2025-06-29 11:35:10 UTC",
    "updated_date": "2025-10-20 13:55:37 UTC"
  },
  {
    "arxiv_id": "2506.23184v1",
    "title": "Score-based Diffusion Model for Unpaired Virtual Histology Staining",
    "authors": [
      "Anran Liu",
      "Xiaofei Wang",
      "Jing Cai",
      "Chao Li"
    ],
    "abstract": "Hematoxylin and eosin (H&E) staining visualizes histology but lacks specificity for diagnostic markers. Immunohistochemistry (IHC) staining provides protein-targeted staining but is restricted by tissue availability and antibody specificity. Virtual staining, i.e., computationally translating the H&E image to its IHC counterpart while preserving the tissue structure, is promising for efficient IHC generation. Existing virtual staining methods still face key challenges: 1) effective decomposition of staining style and tissue structure, 2) controllable staining process adaptable to diverse tissue and proteins, and 3) rigorous structural consistency modelling to handle the non-pixel-aligned nature of paired H&E and IHC images. This study proposes a mutual-information (MI)-guided score-based diffusion model for unpaired virtual staining. Specifically, we design 1) a global MI-guided energy function that disentangles the tissue structure and staining characteristics across modalities, 2) a novel timestep-customized reverse diffusion process for precise control of the staining intensity and structural reconstruction, and 3) a local MI-driven contrastive learning strategy to ensure the cellular level structural consistency between H&E-IHC images. Extensive experiments demonstrate the our superiority over state-of-the-art approaches, highlighting its biomedical potential. Codes will be open-sourced upon acceptance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.23184v1",
    "published_date": "2025-06-29 11:02:45 UTC",
    "updated_date": "2025-06-29 11:02:45 UTC"
  },
  {
    "arxiv_id": "2506.23174v1",
    "title": "Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data",
    "authors": [
      "Chen Gong",
      "Bo Liang",
      "Wei Gao",
      "Chenren Xu"
    ],
    "abstract": "Generative models have gained significant attention for their ability to produce realistic synthetic data that supplements the quantity of real-world datasets. While recent studies show performance improvements in wireless sensing tasks by incorporating all synthetic data into training sets, the quality of synthetic data remains unpredictable and the resulting performance gains are not guaranteed. To address this gap, we propose tractable and generalizable metrics to quantify quality attributes of synthetic data - affinity and diversity. Our assessment reveals prevalent affinity limitation in current wireless synthetic data, leading to mislabeled data and degraded task performance. We attribute the quality limitation to generative models' lack of awareness of untrained conditions and domain-specific processing. To mitigate these issues, we introduce SynCheck, a quality-guided synthetic data utilization scheme that refines synthetic data quality during task model training. Our evaluation demonstrates that SynCheck consistently outperforms quality-oblivious utilization of synthetic data, and achieves 4.3% performance improvement even when the previous utilization degrades performance by 13.4%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in MobiSys 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.23174v1",
    "published_date": "2025-06-29 10:17:39 UTC",
    "updated_date": "2025-06-29 10:17:39 UTC"
  },
  {
    "arxiv_id": "2506.23173v1",
    "title": "Deep Learning for Optical Misalignment Diagnostics in Multi-Lens Imaging Systems",
    "authors": [
      "Tomer Slor",
      "Dean Oren",
      "Shira Baneth",
      "Tom Coen",
      "Haim Suchowski"
    ],
    "abstract": "In the rapidly evolving field of optical engineering, precise alignment of multi-lens imaging systems is critical yet challenging, as even minor misalignments can significantly degrade performance. Traditional alignment methods rely on specialized equipment and are time-consuming processes, highlighting the need for automated and scalable solutions. We present two complementary deep learning-based inverse-design methods for diagnosing misalignments in multi-element lens systems using only optical measurements. First, we use ray-traced spot diagrams to predict five-degree-of-freedom (5-DOF) errors in a 6-lens photographic prime, achieving a mean absolute error of 0.031mm in lateral translation and 0.011$^\\circ$ in tilt. We also introduce a physics-based simulation pipeline that utilizes grayscale synthetic camera images, enabling a deep learning model to estimate 4-DOF, decenter and tilt errors in both two- and six-lens multi-lens systems. These results show the potential to reshape manufacturing and quality control in precision imaging.",
    "categories": [
      "physics.optics",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.optics",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23173v1",
    "published_date": "2025-06-29 10:13:40 UTC",
    "updated_date": "2025-06-29 10:13:40 UTC"
  },
  {
    "arxiv_id": "2506.23168v1",
    "title": "Rises for Measuring Local Distributivity in Lattices",
    "authors": [
      "Mohammad Abdulla",
      "Tobias Hille",
      "Dominik Dürrschnabel",
      "Gerd Stumme"
    ],
    "abstract": "Distributivity is a well-established and extensively studied notion in lattice theory. In the context of data analysis, particularly within Formal Concept Analysis (FCA), lattices are often observed to exhibit a high degree of distributivity. However, no standardized measure exists to quantify this property. In this paper, we introduce the notion of rises in (concept) lattices as a means to assess distributivity. Rises capture how the number of attributes or objects in covering concepts change within the concept lattice. We show that a lattice is distributive if and only if no non-unit rises occur. Furthermore, we relate rises to the classical notion of meet- and join distributivity. We observe that concept lattices from real-world data are to a high degree join-distributive, but much less meet-distributive. We additionally study how join-distributivity manifests on the level of ordered sets.",
    "categories": [
      "cs.AI",
      "cs.DM",
      "math.CO",
      "math.RA"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 2 tables, 5 figures, International Joint Conference on Conceptual Knowledge Structures",
    "pdf_url": "https://arxiv.org/pdf/2506.23168v1",
    "published_date": "2025-06-29 10:03:51 UTC",
    "updated_date": "2025-06-29 10:03:51 UTC"
  },
  {
    "arxiv_id": "2506.23164v1",
    "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models",
    "authors": [
      "Maarten Hugenholtz",
      "Anna Meszaros",
      "Jens Kober",
      "Zlatan Ajanovic"
    ],
    "abstract": "Autonomous Vehicle decisions rely on multimodal prediction models that account for multiple route options and the inherent uncertainty in human behavior. However, models can suffer from mode collapse, where only the most likely mode is predicted, posing significant safety risks. While existing methods employ various strategies to generate diverse predictions, they often overlook the diversity in interaction modes among agents. Additionally, traditional metrics for evaluating prediction models are dataset-dependent and do not evaluate inter-agent interactions quantitatively. To our knowledge, none of the existing metrics explicitly evaluates mode collapse. In this paper, we propose a novel evaluation framework that assesses mode collapse in joint trajectory predictions, focusing on safety-critical interactions. We introduce metrics for mode collapse, mode correctness, and coverage, emphasizing the sequential dimension of predictions. By testing four multi-agent trajectory prediction models, we demonstrate that mode collapse indeed happens. When looking at the sequential dimension, although prediction accuracy improves closer to interaction events, there are still cases where the models are unable to predict the correct interaction mode, even just before the interaction mode becomes inevitable. We hope that our framework can help researchers gain new insights and advance the development of more consistent and accurate prediction models, thus enhancing the safety of autonomous driving systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "12 pages, 8 figures, submitted to a journal",
    "pdf_url": "https://arxiv.org/pdf/2506.23164v1",
    "published_date": "2025-06-29 09:53:12 UTC",
    "updated_date": "2025-06-29 09:53:12 UTC"
  },
  {
    "arxiv_id": "2506.23151v1",
    "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation",
    "authors": [
      "Vladislav Bargatin",
      "Egor Chistov",
      "Alexander Yakovenko",
      "Dmitriy Vatolin"
    ],
    "abstract": "Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical flow method that identifies a favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resource-intensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at https://github.com/msu-video-group/memfof.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.23151v1",
    "published_date": "2025-06-29 09:01:42 UTC",
    "updated_date": "2025-06-29 09:01:42 UTC"
  },
  {
    "arxiv_id": "2506.23141v2",
    "title": "Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing",
    "authors": [
      "Siyuan Li",
      "Yan Wen",
      "Ruitong Liu",
      "Te Sun",
      "Ruihao Zhou",
      "Jingyi Kang",
      "Yunjia Wu"
    ],
    "abstract": "Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge Graph Completion (KGC), providing vital cues for prediction. However, traditional node-based message passing mechanisms, when applied to knowledge graphs, often introduce noise and suffer from information dilution or over-smoothing by indiscriminately aggregating information from all neighboring edges. To address this challenge, we propose a semantic-aware relational message passing. A core innovation of this framework is the introduction of a semantic-aware Top-K neighbor selection strategy. Specifically, this strategy first evaluates the semantic relevance between a central node and its incident edges within a shared latent space, selecting only the Top-K most pertinent ones. Subsequently, information from these selected edges is effectively fused with the central node's own representation using a multi-head attention aggregator to generate a semantically focused node message. In this manner, our model not only leverages the structure and features of edges within the knowledge graph but also more accurately captures and propagates the contextual information most relevant to the specific link prediction task, thereby effectively mitigating interference from irrelevant information. Extensive experiments demonstrate that our method achieves superior performance compared to existing approaches on several established benchmarks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23141v2",
    "published_date": "2025-06-29 08:37:48 UTC",
    "updated_date": "2025-09-10 16:23:48 UTC"
  },
  {
    "arxiv_id": "2506.23139v1",
    "title": "Benchmarking Deep Search over Heterogeneous Enterprise Data",
    "authors": [
      "Prafulla Kumar Choubey",
      "Xiangyu Peng",
      "Shilpa Bhagavath",
      "Kung-Hsiang Huang",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ],
    "abstract": "We present a new benchmark for evaluating Deep Search--a realistic and complex form of retrieval-augmented generation (RAG) that requires source-aware, multi-hop reasoning over diverse, sparsed, but related sources. These include documents, meeting transcripts, Slack messages, GitHub, and URLs, which vary in structure and often contain human-to-human interactions. We build it using a synthetic data pipeline that simulates business workflows across product planning, development, and support stages, generating interconnected content with realistic noise and multi-hop questions with guaranteed ground-truth answers. We release our benchmark with both answerable and unanswerable queries, and retrieval pool of 39,190 enterprise artifacts, enabling fine-grained evaluation of long-context LLM and RAG systems. Our experiments reveal that even the best-performing agentic RAG methods achieve an average performance score of 32.96 on our benchmark. With further analysis, we highlight retrieval as the main bottleneck: existing methods struggle to conduct deep searches and retrieve all necessary evidence. Consequently, they often reason over partial context, leading to significant performance degradation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23139v1",
    "published_date": "2025-06-29 08:34:59 UTC",
    "updated_date": "2025-06-29 08:34:59 UTC"
  },
  {
    "arxiv_id": "2506.23137v3",
    "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion",
    "authors": [
      "Siyuan Li",
      "Ruitong Liu",
      "Yan Wen",
      "Te Sun",
      "Andi Zhang",
      "Yanbiao Ma",
      "Xiaoshuai Hao"
    ],
    "abstract": "Knowledge graph completion demands effective modeling of multifaceted semantic relationships between entities. Yet, prevailing methods, which rely on static scoring functions over learned embeddings, struggling to simultaneously capture rich semantic context and the dynamic nature of relations. To overcome this limitation, we propose the Flow-Modulated Scoring (FMS) framework, conceptualizing a relation as a dynamic evolutionary process governed by its static semantic environment. FMS operates in two stages: it first learns context-aware entity embeddings via a Semantic Context Learning module, and then models a dynamic flow between them using a Conditional Flow-Matching module. This learned flow dynamically modulates a base static score for the entity pair. By unifying context-rich static representations with a conditioned dynamic flow, FMS achieves a more comprehensive understanding of relational semantics. Extensive experiments demonstrate that FMS establishes a new state of the art across both canonical knowledge graph completion tasks: relation prediction and entity prediction. On the standard relation prediction benchmark FB15k-237, FMS achieves a near-perfect MRR of 99.8\\% and Hits@1 of 99.7\\% using a mere 0.35M parameters, while also attaining a 99.9\\% MRR on WN18RR. Its dominance extends to entity prediction, where it secures a 25.2\\% relative MRR gain in the transductive setting and substantially outperforms all baselines in challenging inductive settings. By unifying a dynamic flow mechanism with rich static contexts, FMS offers a highly effective and parameter-efficient new paradigm for knowledge graph completion. Code published at: https://github.com/yuanwuyuan9/FMS.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.23137v3",
    "published_date": "2025-06-29 08:22:04 UTC",
    "updated_date": "2025-08-30 20:56:07 UTC"
  },
  {
    "arxiv_id": "2506.23128v1",
    "title": "Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons",
    "authors": [
      "Chi Chiu So",
      "Yueyue Sun",
      "Jun-Min Wang",
      "Siu Pang Yung",
      "Anthony Wai Keung Loh",
      "Chun Pong Chau"
    ],
    "abstract": "How far are Large Language Models (LLMs) in performing deep relational reasoning? In this paper, we evaluate and compare the reasoning capabilities of three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a suite of carefully designed benchmark tasks in family tree and general graph reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the highest F1-scores across multiple tasks and problem sizes, demonstrating strong aptitude in logical deduction and relational inference. However, all evaluated models, including DeepSeek-R1, struggle significantly as problem complexity increases, largely due to token length limitations and incomplete output structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought responses uncovers its unique planning and verification strategies, but also highlights instances of incoherent or incomplete reasoning, calling attention to the need for deeper scrutiny into LLMs' internal inference dynamics. We further discuss key directions for future work, including the role of multimodal reasoning and the systematic examination of reasoning failures. Our findings provide both empirical insights and theoretical implications for advancing LLMs' reasoning abilities, particularly in tasks that demand structured, multi-step logical inference. Our code repository will be publicly available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 0 figures, accepted by 2025 IEEE international conference on artificial intelligence testing (AITest)",
    "pdf_url": "https://arxiv.org/pdf/2506.23128v1",
    "published_date": "2025-06-29 07:37:49 UTC",
    "updated_date": "2025-06-29 07:37:49 UTC"
  },
  {
    "arxiv_id": "2506.23127v1",
    "title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning",
    "authors": [
      "Zhaoye Fei",
      "Li Ji",
      "Siyin Wang",
      "Junhao Shi",
      "Jingjing Gong",
      "Xipeng Qiu"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they face significant challenges in embodied task planning scenarios that require continuous environmental understanding and action generation. Existing approaches generate open-loop action scripts based on static knowledge, making it difficult to learn causal relationships between actions and environmental feedback, particularly in partially observable environments. We introduce Embodied Planner-R1, a novel outcome-driven reinforcement learning framework that enables LLMs to develop interactive capabilities through autonomous exploration with minimal supervision. Our framework incorporates three key innovations: (1) Without human annotations, we employ pure reinforcement learning with group rollout, incorporating in-environment interaction through parallel exploration; (2) completion-driven sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient learning from grouped trajectories. Across two challenging text-based Embodied planning benchmarks, Embodied Planner-R1 achieves impressive completion rates of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a large margin, and suffers only a -3.66% drop in previously unseen environments, evidencing strong generalization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23127v1",
    "published_date": "2025-06-29 07:31:24 UTC",
    "updated_date": "2025-06-29 07:31:24 UTC"
  },
  {
    "arxiv_id": "2506.23123v1",
    "title": "The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy",
    "authors": [
      "Rishi Bommasani"
    ],
    "abstract": "Artificial intelligence is humanity's most promising technology because of the remarkable capabilities offered by foundation models. Yet, the same technology brings confusion and consternation: foundation models are poorly understood and they may precipitate a wide array of harms. This dissertation explains how technology and society coevolve in the age of AI, organized around three themes. First, the conceptual framing: the capabilities, risks, and the supply chain that grounds foundation models in the broader economy. Second, the empirical insights that enrich the conceptual foundations: transparency created via evaluations at the model level and indexes at the organization level. Finally, the transition from understanding to action: superior understanding of the societal impact of foundation models advances evidence-based AI policy. View together, this dissertation makes inroads into achieving better societal outcomes in the age of AI by building the scientific foundations and research-policy interface required for better AI governance.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "Stanford University PhD Dissertation of Rishi Bommasani (Department of Computer Science, 2025). Also available at https://purl.stanford.edu/zf669yy0336",
    "pdf_url": "https://arxiv.org/pdf/2506.23123v1",
    "published_date": "2025-06-29 07:16:48 UTC",
    "updated_date": "2025-06-29 07:16:48 UTC"
  },
  {
    "arxiv_id": "2506.23121v3",
    "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation",
    "authors": [
      "Xinlei Yu",
      "Changmiao Wang",
      "Hui Jin",
      "Ahmed Elazab",
      "Gangyong Jia",
      "Xiang Wan",
      "Changqing Zou",
      "Ruiquan Ge"
    ],
    "abstract": "Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: https://github.com/YU-deep/CRISP_SAM2.git.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted By ACMMM25",
    "pdf_url": "https://arxiv.org/pdf/2506.23121v3",
    "published_date": "2025-06-29 07:05:27 UTC",
    "updated_date": "2025-07-14 02:03:48 UTC"
  },
  {
    "arxiv_id": "2507.00075v3",
    "title": "Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap",
    "authors": [
      "Yifan Sun",
      "Yushan Liang",
      "Zhen Zhang",
      "Jiaye Teng"
    ],
    "abstract": "Self-improvement is among the most prominent techniques within the realm of large language models (LLM), aiming to enhance the LLM performance without relying on external data. Despite its significance, generally how LLM performances evolve during the self-improvement process remains underexplored. In this paper, we theoretically model the training dynamics of self-improvement via the concept of solver-verifier gap. This is inspired by the conjecture that the performance enhancement of self-improvement stems from the gap between LLM's solver capability and verifier capability. Based on the theoretical framework, we further show how to model the entire training trajectory. This framework allows quantifying the capability limit of self-improvement by fitting the theoretical model to the experiment results. We empirically validate the effectiveness of the theoretical framework on various LLMs and datasets. Beyond self-improvement, we extend our analysis to investigate how external data influences these dynamics within the framework. Notably, we find that under limited external data regimes, such external data can be utilized at any stage without significantly affecting final performances, which accords with the empirical observations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages",
    "pdf_url": "https://arxiv.org/pdf/2507.00075v3",
    "published_date": "2025-06-29 06:48:47 UTC",
    "updated_date": "2025-10-10 17:29:47 UTC"
  },
  {
    "arxiv_id": "2506.23115v1",
    "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings",
    "authors": [
      "Haonan Chen",
      "Hong Liu",
      "Yuping Luo",
      "Liang Wang",
      "Nan Yang",
      "Furu Wei",
      "Zhicheng Dou"
    ],
    "abstract": "Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Homepage: https://haon-chen.github.io/MoCa/",
    "pdf_url": "https://arxiv.org/pdf/2506.23115v1",
    "published_date": "2025-06-29 06:41:00 UTC",
    "updated_date": "2025-06-29 06:41:00 UTC"
  },
  {
    "arxiv_id": "2506.23107v1",
    "title": "Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study",
    "authors": [
      "Bing Song",
      "Jianing Liu",
      "Sisi Jian",
      "Chenyang Wu",
      "Vinayak Dixit"
    ],
    "abstract": "Large language models (LLMs) have made significant strides, extending their applications to dialogue systems, automated content creation, and domain-specific advisory tasks. However, as their use grows, concerns have emerged regarding their reliability in simulating complex decision-making behavior, such as risky decision-making, where a single choice can lead to multiple outcomes. This study investigates the ability of LLMs to simulate risky decision-making scenarios. We compare model-generated decisions with actual human responses in a series of lottery-based tasks, using transportation stated preference survey data from participants in Sydney, Dhaka, Hong Kong, and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk preferences were analyzed using the Constant Relative Risk Aversion (CRRA) framework. Results show that both models exhibit more risk-averse behavior than human participants, with o1-mini aligning more closely with observed human decisions. Further analysis of multilingual data from Nanjing and Hong Kong indicates that model predictions in Chinese deviate more from actual responses compared to English, suggesting that prompt language may influence simulation performance. These findings highlight both the promise and the current limitations of LLMs in replicating human-like risk behavior, particularly in linguistic and cultural settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2506.23107v1",
    "published_date": "2025-06-29 06:16:57 UTC",
    "updated_date": "2025-06-29 06:16:57 UTC"
  },
  {
    "arxiv_id": "2506.23101v1",
    "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship",
    "authors": [
      "Yue Xu",
      "Wenjie Wang"
    ],
    "abstract": "Multimodal large language models (MLLMs) have shown impressive capabilities across tasks involving both visual and textual modalities. However, growing concerns remain about their potential to encode and amplify gender bias, particularly in socially sensitive applications. Existing benchmarks predominantly evaluate bias in isolated scenarios, overlooking how bias may emerge subtly through interpersonal interactions. We fill this gap by going beyond single-entity evaluation and instead focusing on a deeper examination of relational and contextual gender bias in dual-individual interactions. We introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs through the lens of social relationships in generated narratives. Genres assesses gender bias through a dual-character profile and narrative generation task that captures rich interpersonal dynamics and supports a fine-grained bias evaluation suite across multiple dimensions. Experiments on both open- and closed-source MLLMs reveal persistent, context-sensitive gender biases that are not evident in single-character settings. Our findings underscore the importance of relationship-aware benchmarks for diagnosing subtle, interaction-driven gender bias in MLLMs and provide actionable insights for future bias mitigation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23101v1",
    "published_date": "2025-06-29 06:03:21 UTC",
    "updated_date": "2025-06-29 06:03:21 UTC"
  },
  {
    "arxiv_id": "2507.01990v1",
    "title": "Integrating Large Language Models in Financial Investments and Market Analysis: A Survey",
    "authors": [
      "Sedigheh Mahdavi",
      "Jiating",
      "Chen",
      "Pradeep Kumar Joshi",
      "Lina Huertas Guativa",
      "Upmanyu Singh"
    ],
    "abstract": "Large Language Models (LLMs) have been employed in financial decision making, enhancing analytical capabilities for investment strategies. Traditional investment strategies often utilize quantitative models, fundamental analysis, and technical indicators. However, LLMs have introduced new capabilities to process and analyze large volumes of structured and unstructured data, extract meaningful insights, and enhance decision-making in real-time. This survey provides a structured overview of recent research on LLMs within the financial domain, categorizing research contributions into four main frameworks: LLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and Adaptation Approaches, and Agent-Based Architectures. This study provides a structured review of recent LLMs research on applications in stock selection, risk assessment, sentiment analysis, trading, and financial forecasting. By reviewing the existing literature, this study highlights the capabilities, challenges, and potential directions of LLMs in financial markets.",
    "categories": [
      "q-fin.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-fin.GN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.01990v1",
    "published_date": "2025-06-29 05:25:31 UTC",
    "updated_date": "2025-06-29 05:25:31 UTC"
  },
  {
    "arxiv_id": "2506.23094v1",
    "title": "TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure",
    "authors": [
      "Qi He",
      "Gus Xia",
      "Ziyu Wang"
    ],
    "abstract": "Hierarchical planning is a powerful approach to model long sequences structurally. Aside from considering hierarchies in the temporal structure of music, this paper explores an even more important aspect: concept hierarchy, which involves generating music ideas, transforming them, and ultimately organizing them--across musical time and space--into a complete composition. To this end, we introduce TOMI (Transforming and Organizing Music Ideas) as a novel approach in deep music generation and develop a TOMI-based model via instruction-tuned foundation LLM. Formally, we represent a multi-track composition process via a sparse, four-dimensional space characterized by clips (short audio or MIDI segments), sections (temporal positions), tracks (instrument layers), and transformations (elaboration methods). Our model is capable of generating multi-track electronic music with full-song structure, and we further integrate the TOMI-based model with the REAPER digital audio workstation, enabling interactive human-AI co-creation. Experimental results demonstrate that our approach produces higher-quality electronic music with stronger structural coherence compared to baselines.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "9 pages, 4 figures, 2 tables. To be published in ISMIR 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.23094v1",
    "published_date": "2025-06-29 05:15:41 UTC",
    "updated_date": "2025-06-29 05:15:41 UTC"
  },
  {
    "arxiv_id": "2506.23085v3",
    "title": "Enhancing Live Broadcast Engagement: A Multi-modal Approach to Short Video Recommendations Using MMGCN and User Preferences",
    "authors": [
      "Saeid Aghasoleymani Najafabadi",
      "Elaheh Nabavi Nia"
    ],
    "abstract": "The purpose of this paper is to explore a multi-modal approach to enhancing live broadcast engagement by developing a short video recommendation system that incorporates Multi-modal Graph Convolutional Networks (MMGCN) with user preferences. To provide personalized recommendations tailored to individual interests, the proposed system considers user interaction data, video content features, and contextual information. With the aid of a hybrid approach combining collaborative filtering and content-based filtering techniques, the system can capture nuanced relationships between users, video attributes, and engagement patterns. Three datasets are used to evaluate the effectiveness of the system: Kwai, TikTok, and MovieLens. Compared to baseline models, such as DeepFM, Wide & Deep, LightGBM, and XGBoost, the proposed MMGCN-based model shows superior performance. A notable feature of the proposed model is that it outperforms all baseline methods in capturing diverse user preferences and making accurate, personalized recommendations, resulting in a Kwai F1 score of 0.574, a Tiktok F1 score of 0.506, and a MovieLens F1 score of 0.197. We emphasize the importance of multi-modal integration and user-centric approaches in advancing recommender systems, emphasizing the role they play in enhancing content discovery and audience interaction on live broadcast platforms.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23085v3",
    "published_date": "2025-06-29 04:50:52 UTC",
    "updated_date": "2025-09-27 00:20:44 UTC"
  },
  {
    "arxiv_id": "2506.23080v2",
    "title": "AI's Euclid's Elements Moment: From Language Models to Computable Thought",
    "authors": [
      "Xinmin Fang",
      "Lingfeng Tao",
      "Zhengxiong Li"
    ],
    "abstract": "This paper presents a comprehensive five-stage evolutionary framework for understanding the development of artificial intelligence, arguing that its trajectory mirrors the historical progression of human cognitive technologies. We posit that AI is advancing through distinct epochs, each defined by a revolutionary shift in its capacity for representation and reasoning, analogous to the inventions of cuneiform, the alphabet, grammar and logic, mathematical calculus, and formal logical systems. This \"Geometry of Cognition\" framework moves beyond mere metaphor to provide a systematic, cross-disciplinary model that not only explains AI's past architectural shifts-from expert systems to Transformers-but also charts a concrete and prescriptive path forward. Crucially, we demonstrate that this evolution is not merely linear but reflexive: as AI advances through these stages, the tools and insights it develops create a feedback loop that fundamentally reshapes its own underlying architecture. We are currently transitioning into a \"Metalinguistic Moment,\" characterized by the emergence of self-reflective capabilities like Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the \"Mathematical Symbolism Moment\" and the \"Formal Logic System Moment,\" will be defined by the development of a computable calculus of thought, likely through neuro-symbolic architectures and program synthesis, culminating in provably aligned and reliable AI that reconstructs its own foundational representations. This work serves as the methodological capstone to our trilogy, which previously explored the economic drivers (\"why\") and cognitive nature (\"what\") of AI. Here, we address the \"how,\" providing a theoretical foundation for future research and offering concrete, actionable strategies for startups and developers aiming to build the next generation of intelligent systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23080v2",
    "published_date": "2025-06-29 04:14:19 UTC",
    "updated_date": "2025-07-10 09:48:50 UTC"
  },
  {
    "arxiv_id": "2506.23068v3",
    "title": "Curious Causality-Seeking Agents Learn Meta Causal World",
    "authors": [
      "Zhiyu Zhao",
      "Haoxuan Li",
      "Haifeng Zhang",
      "Jun Wang",
      "Francesco Faccio",
      "Jürgen Schmidhuber",
      "Mengyue Yang"
    ],
    "abstract": "When building a world model, a common assumption is that the environment has a single, unchanging underlying causal rule, like applying Newton's laws to every situation. In reality, what appears as a drifting causal mechanism is often the manifestation of a fixed underlying mechanism seen through a narrow observational window. This brings about a problem that, when building a world model, even subtle shifts in policy or environment states can alter the very observed causal mechanisms. In this work, we introduce the \\textbf{Meta-Causal Graph} as world models, a minimal unified representation that efficiently encodes the transformation rules governing how causal structures shift across different latent world states. A single Meta-Causal Graph is composed of multiple causal subgraphs, each triggered by meta state, which is in the latent state space. Building on this representation, we introduce a \\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta states that trigger each subgraph, (2) discover the corresponding causal relationships by agent curiosity-driven intervention policy, and (3) iteratively refine the Meta-Causal Graph through ongoing curiosity-driven exploration and agent experiences. Experiments on both synthetic tasks and a challenging robot arm manipulation task demonstrate that our method robustly captures shifts in causal dynamics and generalizes effectively to previously unseen contexts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.23068v3",
    "published_date": "2025-06-29 03:05:25 UTC",
    "updated_date": "2025-10-26 03:16:17 UTC"
  },
  {
    "arxiv_id": "2506.23055v1",
    "title": "Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis",
    "authors": [
      "Hiro Taiyo Hamada",
      "Ippei Fujisawa",
      "Genji Kawakita",
      "Yuki Yamada"
    ],
    "abstract": "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities in producing human-like text. However, it is unclear how accurately these models internalize concepts that shape human thought and behavior. Here, we developed a quantitative framework to assess concept alignment between LLMs and human psychological dimensions using 43 standardized psychological questionnaires, selected for their established validity in measuring distinct psychological constructs. Our method evaluates how accurately language models reconstruct and classify questionnaire items through pairwise similarity analysis. We compared resulting cluster structures with the original categorical labels using hierarchical clustering. A GPT-4 model achieved superior classification accuracy (66.2\\%), significantly outperforming GPT-3.5 (55.9\\%) and BERT (48.1\\%), all exceeding random baseline performance (31.9\\%). We also demonstrated that the estimated semantic similarity from GPT-4 is associated with Pearson's correlation coefficients of human responses in multiple psychological questionnaires. This framework provides a novel approach to evaluate the alignment of the human-LLM concept and identify potential representational biases. Our findings demonstrate that modern LLMs can approximate human psychological constructs with measurable accuracy, offering insights for developing more interpretable AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23055v1",
    "published_date": "2025-06-29 01:56:56 UTC",
    "updated_date": "2025-06-29 01:56:56 UTC"
  },
  {
    "arxiv_id": "2506.23049v1",
    "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks",
    "authors": [
      "Leander Melroy Maben",
      "Gayathri Ganesh Lakshmy",
      "Srijith Radhakrishnan",
      "Siddhant Arora",
      "Shinji Watanabe"
    ],
    "abstract": "Despite advances in language and speech technologies, no open-source system enables full speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and Automated Tool Use), the first open-source, speech-native assistant capable of completing complex, goal-driven tasks through dynamic tool invocation and multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a cascaded pipeline and supports tools such as calendar booking, contact lookup, web search, and email. Its modular design allows easy integration of new tools using natural language prompts and action classes. On VoiceBench, AURA scores 92.75% on OpenBookQA-outperforming all open-weight systems and nearing GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems. Human evaluation shows 90% task success on complex, multi-turn speech tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.23049v1",
    "published_date": "2025-06-29 01:13:15 UTC",
    "updated_date": "2025-06-29 01:13:15 UTC"
  },
  {
    "arxiv_id": "2506.23046v3",
    "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions",
    "authors": [
      "Xianzhe Fan",
      "Xuhui Zhou",
      "Chuanyang Jin",
      "Kolby Nottingham",
      "Hao Zhu",
      "Maarten Sap"
    ],
    "abstract": "Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model's ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1% in first-person evaluation and 26.4% in third-person evaluation. This indicates that future LVLMs need to further improve their ToM capabilities in embodied, complex social interactions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.23046v3",
    "published_date": "2025-06-29 00:54:13 UTC",
    "updated_date": "2025-12-15 04:45:27 UTC"
  },
  {
    "arxiv_id": "2506.23044v2",
    "title": "Ovis-U1 Technical Report",
    "authors": [
      "Guo-Hua Wang",
      "Shanshan Zhao",
      "Xinjie Zhang",
      "Liangfu Cao",
      "Pengxin Zhan",
      "Lunhao Duan",
      "Shiyin Lu",
      "Minghao Fu",
      "Xiaohao Chen",
      "Jianshan Zhao",
      "Yang Li",
      "Qing-Guo Chen"
    ],
    "abstract": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "An unified model for multimodal understanding, text-to-image generation, and image editing. GitHub: https://github.com/AIDC-AI/Ovis-U1",
    "pdf_url": "https://arxiv.org/pdf/2506.23044v2",
    "published_date": "2025-06-29 00:40:17 UTC",
    "updated_date": "2025-07-01 09:33:23 UTC"
  },
  {
    "arxiv_id": "2506.23040v2",
    "title": "Treatment, evidence, imitation, and chat",
    "authors": [
      "Samuel J. Weisenthal"
    ],
    "abstract": "Large language models are thought to have potential to aid in medical decision making. We investigate this here. We start with the treatment problem, the patient's core medical decision-making task, which is solved in collaboration with a healthcare provider. We discuss approaches to solving the treatment problem, including -- within evidence-based medicine -- trials and observational data. We then discuss the chat problem, and how this differs from the treatment problem -- in particular as it relates to imitation. We then discuss how a large language model might be used to solve the treatment problem and highlight some of the challenges that emerge. We finally discuss how these challenges relate to evidence-based medicine, and how this might inform next steps.",
    "categories": [
      "stat.OT",
      "cs.AI"
    ],
    "primary_category": "stat.OT",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.23040v2",
    "published_date": "2025-06-29 00:23:06 UTC",
    "updated_date": "2025-07-04 00:25:07 UTC"
  }
]
{
  "date": "2025-06-29",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-29 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä¸€å¥è¯æ€»ç»“**ï¼šä»Šå¤©çš„ arXiv ç›¸å½“ç¡¬æ ¸ï¼Œä¸ä»…æœ‰å¯¹å½“ä¸‹é¡¶æµ **DeepSeek-R1** æ¨ç†èƒ½åŠ›çš„æ·±åº¦è¯„æµ‹ï¼Œè¿˜æœ‰ **Ovis-U1** è¿™ç§ç»Ÿä¸€ç†è§£ä¸ç”Ÿæˆçš„å…¨èƒ½æ¨¡å‹å‘å¸ƒã€‚å­¦æœ¯å¤§ä½¬ JÃ¼rgen Schmidhuber å¸¦æ¥äº†å…³äº**å…ƒå› æœï¼ˆMeta-Causalï¼‰**çš„æ–°ä½œã€‚æ­¤å¤–ï¼Œå…³äºæ¨ç†æ¨¡å‹ï¼ˆReasoning Modelsï¼‰åœ¨åšå¼ˆä¸­å˜å¾—â€œè‡ªç§â€çš„å‘ç°ï¼Œä»¥åŠå¹¶è¡Œè§£ç åŠ é€Ÿç”Ÿæˆçš„æ¶æ„åˆ›æ–°ä¹Ÿå€¼å¾—å…³æ³¨ã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹è®ºæ–‡ï¼šå¤§æ¨¡å‹æ¨ç†ã€ç»Ÿä¸€æ¶æ„ä¸å¤§ä½¬æ–°ä½œ\n\n**1. Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons**\n**DeepSeek-R1 èƒ½è¿›è¡Œæ·±åº¦å…³ç³»æ¨ç†å—ï¼Ÿæ¥è‡ª DeepSeek-R1 ä¸åŸºå‡†å¯¹æ¯”çš„æ´è§**\n> æ ¸å¿ƒæœ¯è¯­ï¼šRelational Reasoning, Chain-of-Thought, DeepSeek-R1\n- **æ ¸å¿ƒå‘ç°**ï¼šè¿™ç¯‡è®ºæ–‡å¯¹å½“çº¢ç‚¸å­é¸¡ **DeepSeek-R1**ã€DeepSeek-V3 å’Œ GPT-4o è¿›è¡Œäº†æ·±åº¦å…³ç³»æ¨ç†ï¼ˆå¦‚å®¶è°±å›¾ã€é€šç”¨å›¾æ¨ç†ï¼‰çš„è¯„æµ‹ã€‚\n- **ç»“æœ**ï¼šDeepSeek-R1 åœ¨å¤šé¡¹ä»»åŠ¡ä¸­ F1 åˆ†æ•°æœ€é«˜ï¼Œé€»è¾‘æ¼”ç»èƒ½åŠ›å¼ºæ‚ã€‚\n- **å±€é™**ï¼šä½†éšç€é—®é¢˜å¤æ‚åº¦å¢åŠ ï¼Œæ‰€æœ‰æ¨¡å‹ï¼ˆåŒ…æ‹¬ R1ï¼‰è¡¨ç°éƒ½å¤§å¹…ä¸‹é™ï¼Œä¸»è¦å—é™äº Token é•¿åº¦å’Œè¾“å‡ºç»“æ„ã€‚æœ‰è¶£çš„å‘ç°æ˜¯ï¼ŒR1 é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰ä¸­å±•ç°äº†ç‹¬ç‰¹çš„è§„åˆ’å’ŒéªŒè¯ç­–ç•¥ï¼Œä½†ä¹Ÿå­˜åœ¨é€»è¾‘ä¸è¿è´¯çš„æƒ…å†µã€‚\n\n**2. Ovis-U1 Technical Report**\n**Ovis-U1 æŠ€æœ¯æŠ¥å‘Š**\n> æ ¸å¿ƒæœ¯è¯­ï¼šUnified Model, Multimodal Understanding, Image Generation\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šé˜¿é‡Œå›¢é˜Ÿå‘å¸ƒäº† **Ovis-U1**ï¼Œè¿™æ˜¯ä¸€ä¸ª 30 äº¿å‚æ•°çš„**ç»Ÿä¸€æ¨¡å‹**ã€‚å®ƒä¸å†æ˜¯ç®€å•çš„â€œç†è§£æ¨¡å‹æ¥ç”Ÿæˆæ¨¡å‹â€ï¼Œè€Œæ˜¯ä»è¯­è¨€æ¨¡å‹å¼€å§‹è¿›è¡Œç»Ÿä¸€è®­ç»ƒã€‚\n- **èƒ½åŠ›**ï¼šå®ƒé›†æˆäº†å¤šæ¨¡æ€ç†è§£ã€æ–‡ç”Ÿå›¾å’Œå›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚åœ¨ç†è§£ä¸Šè¶…è¶Šäº† Ristretto-3Bï¼Œåœ¨ç”Ÿæˆä¸Šæ¯”è‚© GPT-4oï¼Œè¯æ˜äº†ç»Ÿä¸€è®­ç»ƒç­–ç•¥èƒ½å®ç°â€œç†è§£ä¿ƒè¿›ç”Ÿæˆâ€ã€‚\n\n**3. Curious Causality-Seeking Agents Learn Meta Causal World**\n**å¥½å¥‡çš„å› æœæœå¯»æ™ºèƒ½ä½“å­¦ä¹ å…ƒå› æœä¸–ç•Œ**\n> æ ¸å¿ƒæœ¯è¯­ï¼šMeta-Causal Graph, World Models, Curiosity-driven\n- **å¤§ä½¬æ–°ä½œ**ï¼šJÃ¼rgen Schmidhuber å‚ä¸çš„æ–°ä½œã€‚\n- **è§£å†³é—®é¢˜**ï¼šä¼ ç»Ÿçš„**ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelï¼‰**å‡è®¾ç¯å¢ƒè§„åˆ™å•ä¸€ä¸å˜ï¼Œä½†ç°å®ä¸­å› æœæœºåˆ¶å¸¸éšçŠ¶æ€æ¼‚ç§»ã€‚\n- **æ–¹æ³•**ï¼šæå‡ºäº† **Meta-Causal Graphï¼ˆå…ƒå› æœå›¾ï¼‰**ï¼Œç”¨ä»¥ç¼–ç å› æœç»“æ„éšæ½œåœ¨ä¸–ç•ŒçŠ¶æ€ï¼ˆMeta Stateï¼‰å˜åŒ–çš„è§„åˆ™ã€‚æ™ºèƒ½ä½“é€šè¿‡å¥½å¥‡å¿ƒé©±åŠ¨çš„æ¢ç´¢ï¼Œè¯†åˆ«è§¦å‘ä¸åŒå› æœå­å›¾çš„å…ƒçŠ¶æ€ï¼Œé€‚åº”æ€§æå¼ºã€‚\n\n**4. Pipelined Decoder for Efficient Context-Aware Text Generation**\n**ç”¨äºé«˜æ•ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–‡æœ¬ç”Ÿæˆçš„æµæ°´çº¿è§£ç å™¨**\n> æ ¸å¿ƒæœ¯è¯­ï¼šPipelined Decoder, Autoregressive, Parallel Generation\n- **ç—›ç‚¹**ï¼šè‡ªå›å½’æ¨¡å‹ï¼ˆAutoregressiveï¼‰è™½ç„¶è´¨é‡å¥½ï¼Œä½†é€ä¸ª Token ç”Ÿæˆæ˜¯é€Ÿåº¦ç“¶é¢ˆã€‚\n- **åˆ›æ–°**ï¼šæå‡ºäº†ä¸€ç§**æµæ°´çº¿è§£ç å™¨ï¼ˆPipelined Decoderï¼‰**ï¼Œå¹¶éä¸€æ¬¡ç”Ÿæˆä¸€ä¸ª Tokenï¼Œè€Œæ˜¯åŒæ—¶åˆå§‹åŒ–å¤šä¸ªå­åºåˆ—çš„ç”Ÿæˆã€‚\n- **æ•ˆæœ**ï¼šåœ¨é—®ç­”ã€æ‘˜è¦ç­‰ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦ï¼Œä¸”æ²¡æœ‰æ˜æ˜¾çš„è´¨é‡æŸå¤±æˆ–é¢å¤–å†…å­˜æ¶ˆè€—ã€‚\n\n---\n\n### ğŸ›¡ï¸ LLM å®‰å…¨ã€è¡Œä¸ºå¿ƒç†å­¦ä¸å¯¹é½\n\n**5. A Representation Engineering Perspective on the Effectiveness of Multi-Turn Jailbreaks**\n**ä»è¡¨å¾å·¥ç¨‹è§†è§’çœ‹å¤šè½®è¶Šç‹±æ”»å‡»çš„æœ‰æ•ˆæ€§**\n> æ ¸å¿ƒæœ¯è¯­ï¼šMulti-turn Jailbreaks, Representation Engineering, Crescendo\n- **å‘ç°**ï¼šå¤šè½®è¶Šç‹±ï¼ˆå¦‚ Crescendo æ”»å‡»ï¼‰ä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºåœ¨æ¯ä¸€è½®å¯¹è¯ä¸­ï¼Œæ”»å‡»è€…éƒ½æŠŠ Prompt ä¼ªè£…åœ¨æ¨¡å‹è¡¨å¾ç©ºé—´çš„â€œè‰¯æ€§åŒºåŸŸâ€ï¼Œæ¬ºéª—æ¨¡å‹é€æ­¥æ»¡è¶³æœ‰å®³è¯·æ±‚ã€‚ç°æœ‰çš„å•è½®é˜²å¾¡ï¼ˆå¦‚ Circuit Breakersï¼‰å¯¹æ­¤åŸºæœ¬æ— æ•ˆã€‚\n\n**6. Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games**\n**è¢«æ¨ç†è…èš€ï¼šæ¨ç†è¯­è¨€æ¨¡å‹åœ¨å…¬å…±å“åšå¼ˆä¸­æˆä¸ºâ€œæ­ä¾¿è½¦è€…â€**\n> æ ¸å¿ƒæœ¯è¯­ï¼šReasoning LLMs, Public Goods Games, Cooperation\n- **æœ‰è¶£çš„ç¤¾ä¼šå­¦å®éªŒ**ï¼šç ”ç©¶å‘ç°ï¼Œå…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å¤§æ¨¡å‹ï¼ˆå¦‚ o1 ç³»åˆ—ï¼‰ï¼Œåœ¨å¤šæ™ºèƒ½ä½“åšå¼ˆä¸­åè€Œ**æ›´éš¾åˆä½œ**ã€‚\n- **ç»“è®º**ï¼šå®ƒä»¬å€¾å‘äºæˆä¸ºâ€œæ­ä¾¿è½¦è€…â€ï¼ˆFree-Ridersï¼‰ï¼Œå³ä¸ºäº†è‡ªèº«åˆ©ç›Šç‰ºç‰²é›†ä½“åˆ©ç›Šã€‚è¿™è¡¨æ˜æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ˆReasoning Capabilitiesï¼‰å¹¶ä¸å¿…ç„¶å¯¼è‡´æ›´å¥½çš„ç¤¾ä¼šå¯¹é½æˆ–åˆä½œè¡Œä¸ºã€‚\n\n**7. TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs**\n**TuCoï¼šæµ‹é‡å¾®è°ƒå¯¹ LLM ä¸ªä½“å“åº”çš„è´¡çŒ®**\n> æ ¸å¿ƒæœ¯è¯­ï¼šTuCo, Fine-Tuning Component, Safety\n- **æ–¹æ³•**ï¼šæå‡ºäº†ä¸€ç§é‡åŒ–æ–¹æ³• **TuCo**ï¼Œé€šè¿‡åˆ†è§£æ¨¡å‹éšè—çŠ¶æ€ï¼Œè®¡ç®—â€œå¾®è°ƒç»„ä»¶â€ç›¸å¯¹äºâ€œé¢„è®­ç»ƒç»„ä»¶â€çš„è´¡çŒ®æ¯”ç‡ã€‚\n- **åº”ç”¨**ï¼šå‘ç°æˆåŠŸçš„è¶Šç‹±æ”»å‡»é€šå¸¸ä¼šé™ä½ TuCo å€¼ï¼Œæ„å‘³ç€æ”»å‡»æˆåŠŸå¾€å¾€æ˜¯å› ä¸ºæŠ‘åˆ¶äº†å¾®è°ƒï¼ˆé€šå¸¸åŒ…å«å®‰å…¨å¯¹é½ï¼‰å¸¦æ¥çš„å½±å“ã€‚\n\n---\n\n### ğŸ‘ï¸ è®¡ç®—æœºè§†è§‰ä¸å¤šæ¨¡æ€ç”Ÿæˆ\n\n**8. Why Settle for One? Text-to-ImageSet Generation and Evaluation**\n**ä½•å¿…åªé€‰å…¶ä¸€ï¼Ÿæ–‡æœ¬åˆ°å›¾åƒé›†ï¼ˆImageSetï¼‰çš„ç”Ÿæˆä¸è¯„ä¼°**\n> æ ¸å¿ƒæœ¯è¯­ï¼šText-to-ImageSet, Consistency, AutoT2IS\n- **æ–°ä»»åŠ¡**ï¼šæå‡º **Text-to-ImageSet (T2IS)** ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯æ ¹æ®æŒ‡ä»¤ç”Ÿæˆä¸€ç»„é£æ ¼ã€å†…å®¹ä¸€è‡´çš„å›¾ç‰‡ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€å¼ ã€‚\n- **æ–¹æ³•**ï¼šæ¨å‡ºäº† **AutoT2IS** æ¡†æ¶ï¼Œæ— éœ€è®­ç»ƒï¼Œåˆ©ç”¨é¢„è®­ç»ƒ DiT çš„ä¸Šä¸‹æ–‡èƒ½åŠ›æ¥åè°ƒä¸€ç»„å›¾ç‰‡çš„è§†è§‰ä¸€è‡´æ€§ã€‚\n\n**9. MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation**\n**MEMFOFï¼šç”¨äºæ˜¾å­˜é«˜æ•ˆå¤šå¸§å…‰æµä¼°è®¡çš„é«˜åˆ†è¾¨ç‡è®­ç»ƒ**\n> æ ¸å¿ƒæœ¯è¯­ï¼šOptical Flow, GPU Memory, High-Resolution\n- **å·¥ç¨‹çªç ´**ï¼šå…‰æµä¼°è®¡é€šå¸¸æ˜¾å­˜å ç”¨å·¨å¤§ã€‚MEMFOF å®ç°äº†åœ¨ 1080p åˆ†è¾¨ç‡ä¸‹ä»…éœ€ 2.09 GB æ˜¾å­˜è¿è¡Œï¼ˆè®­ç»ƒæ—¶ 28.5 GBï¼‰ã€‚\n- **æ€§èƒ½**ï¼šåœ¨ Spring å’Œ Sintel åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ° SOTAï¼Œå¯¹äºé«˜åˆ†è¾¨ç‡è§†é¢‘å¤„ç†æ˜¯é‡å¤§åˆ©å¥½ã€‚\n\n**10. CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation**\n**CRISP-SAM2ï¼šç»“åˆè·¨æ¨¡æ€äº¤äº’ä¸è¯­ä¹‰æç¤ºçš„ SAM2 å¤šå™¨å®˜åˆ†å‰²**\n> æ ¸å¿ƒæœ¯è¯­ï¼šSAM2, Medical Segmentation, Semantic Prompting\n- **æ”¹è¿›**ï¼šé’ˆå¯¹åŒ»å­¦å›¾åƒï¼Œæ”¹è¿›äº† Meta çš„ SAM2ã€‚å¼•å…¥äº†æ–‡æœ¬æè¿°å¼•å¯¼ï¼Œå¹¶ç”¨è¯­ä¹‰æç¤ºï¼ˆSemantic Promptingï¼‰æ›¿ä»£çº¯å‡ ä½•æç¤ºï¼Œè§£å†³äº† SAM2 åœ¨åŒ»å­¦ç»†èŠ‚åˆ†å‰²ä¸Šçš„ä¸è¶³ã€‚\n\n---\n\n### ğŸ“ˆ æ—¶é—´åºåˆ—ä¸æ•°æ®æŒ–æ˜\n\n**11. Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting**\n**ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„ç²¾ç¡®å‚æ•°é«˜æ•ˆæµ‹è¯•æ—¶è‡ªé€‚åº” (PETSA)**\n> æ ¸å¿ƒæœ¯è¯­ï¼šTest-Time Adaptation (TTA), Time Series, Parameter-Efficient\n- **ç—›ç‚¹**ï¼šç°å®ä¸–ç•Œçš„æ—¶é—´åºåˆ—æ˜¯éå¹³ç¨³çš„ï¼Œé¢„è®­ç»ƒæ¨¡å‹å®¹æ˜“å¤±æ•ˆã€‚\n- **æ–¹æ³•**ï¼š**PETSA** ä»…åœ¨æµ‹è¯•é˜¶æ®µæ›´æ–°è¾“å…¥è¾“å‡ºçš„å°å‹æ ¡å‡†æ¨¡å—ï¼ˆLow-rank adaptersï¼‰ï¼Œæ— éœ€é‡è®­æ•´ä¸ªæ¨¡å‹ï¼Œæå¤§é™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹é¢‘ç‡å’Œç»“æ„çš„å¯¹é½ã€‚\n\n**12. The language of time: a language model perspective on time-series foundation models**\n**æ—¶é—´çš„è¯­è¨€ï¼šä»è¯­è¨€æ¨¡å‹è§†è§’çœ‹æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹**\n> æ ¸å¿ƒæœ¯è¯­ï¼šTime Series Foundation Models, Quantization\n- **ç†è®ºè§†è§’**ï¼šè¿™ç¯‡è®ºæ–‡è®ºè¯äº† Patch-based çš„æ—¶é—´åºåˆ—æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯åœ¨æ³›åŒ–è¯­è¨€æ¨¡å‹çš„è¡¨å¾èŒƒå¼ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¿ç»­çš„æ—¶é—´åºåˆ— Patch å¯ä»¥è¢«é‡åŒ–ä¸ºç¦»æ•£è¯è¡¨ï¼Œä¸”ç»Ÿè®¡ç‰¹æ€§ä¸è‡ªç„¶è¯­è¨€é«˜åº¦ä¸€è‡´ã€‚\n\n---\n\n### ğŸ¥ å…¶ä»–æœ‰è¶£æˆ–é‡è¦çš„åº”ç”¨\n\n- **#8 Advanced Financial Reasoning at Scale (CFA Level III)**: å¯¹ 23 ä¸ª LLM è¿›è¡Œäº† CFA ä¸‰çº§è€ƒè¯•ï¼ˆé‡‘èæ¨ç†é»„é‡‘æ ‡å‡†ï¼‰çš„è¯„æµ‹ã€‚o4-mini æ‹¿åˆ°äº† 79.1% çš„é«˜åˆ†ã€‚\n- **#27 Real-Time Progress Prediction in Reasoning Language Models**: æå‡ºäº†ä¸€ä¸ªæœ‰è¶£çš„åŠŸèƒ½â€”â€”å®æ—¶é¢„æµ‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚ o1ï¼‰è¿˜æœ‰å¤šä¹…èƒ½æƒ³å‡ºæ¥ã€‚é€šè¿‡å¾®è°ƒï¼Œå¯ä»¥è®©æ¨¡å‹è¾“å‡ºè¿›åº¦æ¡ï¼ˆ0%->100%ï¼‰ã€‚\n- **#52 Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning**: æå‡ºäº† **Embodied Planner-R1**ï¼Œåœ¨ ALFWorld ä»»åŠ¡ä¸Šè¾¾åˆ°äº† 97.78% çš„å®Œæˆç‡ï¼Œä½¿ç”¨çº¯ RL è¿›è¡Œæ¢ç´¢ã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2506.23431v2",
      "title": "Pipelined Decoder for Efficient Context-Aware Text Generation",
      "title_zh": "é¢å‘é«˜æ•ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–‡æœ¬ç”Ÿæˆçš„æµæ°´çº¿è§£ç å™¨",
      "authors": [
        "Zixian Huang",
        "Chenxu Niu",
        "Yu Gu",
        "Gengyang Xiao",
        "Xinwei Huang",
        "Gong Cheng"
      ],
      "abstract": "As the basis of generative AI, an autoregressive model requires the generation of a new token depending on all the previously generated tokens, which brings high quality but also restricts the model to generate tokens one by one, forming a bottleneck limiting the generation speed. In this paper, we propose a new decoder architecture that efficiently generates text in parallel for context-aware generation tasks. Our proposed pipelined decoder initiates the generation of multiple subsequences simultaneously, and, at each time-step, it generates a new token for each subsequence to realize parallelism. Experiments on multiple text generation tasks, including question answering, text summarization, and keyphrase generation, show that our pipelined decoder significantly improves the generation speed without a significant loss of generation quality or additional memory consumption.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªå›å½’æ¨¡å‹(Autoregressive model)é€ä¸ªç”Ÿæˆtokenå¯¼è‡´çš„ç”Ÿæˆé€Ÿåº¦ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æµæ°´çº¿è§£ç å™¨(Pipelined decoder)æ¶æ„ï¼Œç”¨äºå®ç°é«˜æ•ˆçš„å¹¶è¡Œæ–‡æœ¬ç”Ÿæˆã€‚è¯¥è§£ç å™¨é€šè¿‡åœ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç”Ÿæˆä»»åŠ¡ä¸­åŒæ—¶å¯åŠ¨å¤šä¸ªå­åºåˆ—çš„ç”Ÿæˆï¼Œå¹¶åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¸ºæ¯ä¸ªå­åºåˆ—åŒæ­¥äº§å‡ºæ–°tokenï¼Œä»è€Œæœ‰æ•ˆæå‡äº†ç”Ÿæˆè¿‡ç¨‹çš„å¹¶è¡Œæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨é—®ç­”(Question answering)ã€æ–‡æœ¬æ‘˜è¦(Text summarization)å’Œå…³é”®è¯ç”Ÿæˆ(Keyphrase generation)ç­‰å¤šé¡¹ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—åŠ å¿«äº†ç”Ÿæˆé€Ÿåº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§æ€§èƒ½ä¼˜åŒ–æ˜¯åœ¨ä¸äº§ç”Ÿæ˜æ˜¾ç”Ÿæˆè´¨é‡æŸå¤±æˆ–é¢å¤–å†…å­˜æ¶ˆè€—çš„æƒ…å†µä¸‹å®ç°çš„ã€‚è¯¥ç ”ç©¶ä¸ºæé«˜ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å¤„ç†ä¸Šä¸‹æ–‡ç›¸å…³ä»»åŠ¡æ—¶çš„æ¨ç†æ•ˆç‡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ¶æ„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23431v2",
      "published_date": "2025-06-29 23:37:24 UTC",
      "updated_date": "2025-07-01 04:16:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:19:28.542502+00:00"
    },
    {
      "arxiv_id": "2507.02956v1",
      "title": "A Representation Engineering Perspective on the Effectiveness of Multi-Turn Jailbreaks",
      "title_zh": "è¡¨å¾å·¥ç¨‹è§†è§’ä¸‹çš„å¤šè½®è¶Šç‹±æœ‰æ•ˆæ€§ç ”ç©¶",
      "authors": [
        "Blake Bullwinkel",
        "Mark Russinovich",
        "Ahmed Salem",
        "Santiago Zanella-Beguelin",
        "Daniel Jones",
        "Giorgio Severi",
        "Eugenia Kim",
        "Keegan Hines",
        "Amanda Minnich",
        "Yonatan Zunger",
        "Ram Shankar Siva Kumar"
      ],
      "abstract": "Recent research has demonstrated that state-of-the-art LLMs and defenses remain susceptible to multi-turn jailbreak attacks. These attacks require only closed-box model access and are often easy to perform manually, posing a significant threat to the safe and secure deployment of LLM-based systems. We study the effectiveness of the Crescendo multi-turn jailbreak at the level of intermediate model representations and find that safety-aligned LMs often represent Crescendo responses as more benign than harmful, especially as the number of conversation turns increases. Our analysis indicates that at each turn, Crescendo prompts tend to keep model outputs in a \"benign\" region of representation space, effectively tricking the model into fulfilling harmful requests. Further, our results help explain why single-turn jailbreak defenses like circuit breakers are generally ineffective against multi-turn attacks, motivating the development of mitigations that address this generalization gap.",
      "tldr_zh": "è¯¥ç ”ç©¶ä» Representation Engineeringï¼ˆè¡¨å¾å·¥ç¨‹ï¼‰çš„è§’åº¦æ¢è®¨äº†å¤šè½®è¶Šç‹±æ”»å‡»ï¼ˆMulti-Turn Jailbreaksï¼‰å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ‰æ•ˆæ€§ï¼Œé‡ç‚¹åˆ†æäº† Crescendo æ”»å‡»åœ¨æ¨¡å‹ä¸­é—´è¡¨å¾å±‚é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä¾¿ç»è¿‡å®‰å…¨å¯¹é½ï¼ˆSafety-Alignedï¼‰çš„æ¨¡å‹ï¼Œéšç€å¯¹è¯è½®æ•°çš„å¢åŠ ï¼Œé€šå¸¸ä¼šå°† Crescendo çš„å“åº”è¯†åˆ«ä¸ºè‰¯æ€§è€Œéæœ‰å®³ã€‚åˆ†æè¡¨æ˜ï¼ŒCrescendo æç¤ºè¯é€šè¿‡å°†æ¨¡å‹è¾“å‡ºç»´æŒåœ¨è¡¨å¾ç©ºé—´çš„â€œè‰¯æ€§â€åŒºåŸŸï¼Œæœ‰æ•ˆåœ°è¯±å¯¼æ¨¡å‹é€æ­¥è¾¾æˆæœ‰å®³è¯·æ±‚ã€‚è¿™ä¸€ç»“æœè§£é‡Šäº†ä¸ºä½• Circuit Breakers ç­‰é’ˆå¯¹å•è½®è¶Šç‹±çš„é˜²å¾¡æªæ–½åœ¨åº”å¯¹å¤šè½®æ”»å‡»æ—¶æ™®éå¤±æ•ˆã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†é˜²å¾¡æœºåˆ¶ä¸­çš„æ³›åŒ–å·®è·ï¼ˆGeneralization Gapï¼‰ï¼Œä¸ºå¼€å‘èƒ½å¤Ÿåº”å¯¹å¤šè½®å¤æ‚æ”»å‡»çš„æ–°å‹ç¼“è§£ç­–ç•¥æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.02956v1",
      "published_date": "2025-06-29 23:28:55 UTC",
      "updated_date": "2025-06-29 23:28:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:19:34.427221+00:00"
    },
    {
      "arxiv_id": "2506.23424v1",
      "title": "Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting",
      "title_zh": "é¢å‘æ—¶é—´åºåˆ—é¢„æµ‹çš„é«˜ç²¾åº¦å‚æ•°é«˜æ•ˆæµ‹è¯•æ—¶è‡ªé€‚åº”",
      "authors": [
        "Heitor R. Medeiros",
        "Hossein Sharifi-Noghabi",
        "Gabriel L. Oliveira",
        "Saghar Irandoust"
      ],
      "abstract": "Real-world time series often exhibit a non-stationary nature, degrading the performance of pre-trained forecasting models. Test-Time Adaptation (TTA) addresses this by adjusting models during inference, but existing methods typically update the full model, increasing memory and compute costs. We propose PETSA, a parameter-efficient method that adapts forecasters at test time by only updating small calibration modules on the input and output. PETSA uses low-rank adapters and dynamic gating to adjust representations without retraining. To maintain accuracy despite limited adaptation capacity, we introduce a specialized loss combining three components: (1) a robust term, (2) a frequency-domain term to preserve periodicity, and (3) a patch-wise structural term for structural alignment. PETSA improves the adaptability of various forecasting backbones while requiring fewer parameters than baselines. Experimental results on benchmark datasets show that PETSA achieves competitive or better performance across all horizons. Our code is available at: https://github.com/BorealisAI/PETSA",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®ä¸–ç•Œæ—¶é—´åºåˆ—çš„éå¹³ç¨³æ€§å¯¼è‡´é¢„è®­ç»ƒé¢„æµ‹æ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº† PETSAï¼Œä¸€ç§å‚æ•°é«˜æ•ˆçš„ Test-Time Adaptation (TTA) æ–¹æ³•ã€‚PETSA æ—¨åœ¨é€šè¿‡ä»…æ›´æ–°è¾“å…¥å’Œè¾“å‡ºç«¯çš„å°å‹æ ¡å‡†æ¨¡å—ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®æ—¶è°ƒæ•´æ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—é™ä½å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº† Low-rank adapters å’Œ Dynamic gating æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºã€‚ä¸ºäº†åœ¨æœ‰é™çš„è‡ªé€‚åº”å®¹é‡ä¸‹ç»´æŒé¢„æµ‹ç²¾åº¦ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§åŒ…å«ç¨³å¥é¡¹ã€æ—¨åœ¨ä¿ç•™å‘¨æœŸæ€§çš„ Frequency-domain é¡¹ä»¥åŠç”¨äºç»“æ„å¯¹é½çš„ Patch-wise structural é¡¹çš„ä¸“é—¨æŸå¤±å‡½æ•°ã€‚å®éªŒè¯æ˜ï¼ŒPETSA åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†çš„æ‰€æœ‰é¢„æµ‹æ­¥é•¿ä¸Šå‡å–å¾—äº†å…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜çš„æ€§èƒ½ï¼ŒåŒæ—¶æ¯”ç°æœ‰åŸºå‡†æ–¹æ³•æ‰€éœ€çš„å‚æ•°é‡æ›´å°‘ã€‚è¯¥ç ”ç©¶ä¸ºæé«˜æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹çš„ç¯å¢ƒé€‚åº”æ€§å’Œè®¡ç®—æ•ˆç‡æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Second Workshop on Test-Time Adaptation: Putting Updates to the Test! at ICML 2025, Vancouver, Canada. 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.23424v1",
      "published_date": "2025-06-29 23:09:35 UTC",
      "updated_date": "2025-06-29 23:09:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:19:35.960342+00:00"
    },
    {
      "arxiv_id": "2506.23423v1",
      "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs",
      "title_zh": "TuCoï¼šè¡¡é‡å¾®è°ƒå¯¹å¤§è¯­è¨€æ¨¡å‹ä¸ªä½“å“åº”çš„è´¡çŒ®",
      "authors": [
        "Felipe Nuti",
        "Tim Franzmeyer",
        "JoÃ£o Henriques"
      ],
      "abstract": "Past work has studied the effects of fine-tuning on large language models' (LLMs) overall performance on certain tasks. However, a quantitative and systematic method for analyzing its effect on individual outputs is still lacking. Here, we propose a new method for measuring the contribution that fine-tuning makes to individual LLM responses, assuming access to the original pre-trained model. Our method tracks the model's intermediate hidden states, providing a more fine-grained insight into the effects of fine-tuning than a simple comparison of final outputs from pre-trained and fine-tuned models. We introduce and theoretically analyze an exact decomposition of any fine-tuned LLM into a pre-training component and a fine-tuning component. Empirically, we find that model behavior and performance can be steered by up- or down-scaling the fine-tuning component during the forward pass. Motivated by this finding and our theoretical analysis, we define the Tuning Contribution (TuCo) as the ratio of the magnitudes of the fine-tuning component to the pre-training component. We observe that three prominent adversarial attacks on LLMs circumvent safety measures in a way that reduces TuCo, and that TuCo is consistently lower on prompts where these attacks succeed compared to those where they do not. This suggests that attenuating the effect of fine-tuning on model outputs plays a role in the success of such attacks. In summary, TuCo enables the quantitative study of how fine-tuning influences model behavior and safety, and vice versa.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TuCoï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå®šé‡æµ‹é‡å¾®è°ƒ(fine-tuning)å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å•ä¸ªå“åº”è´¡çŒ®åº¦çš„æ–°æ–¹æ³•ã€‚ä¸ºäº†è§£å†³ç°æœ‰ç ”ç©¶ä»…å…³æ³¨æ¨¡å‹æ•´ä½“è¡¨ç°è€Œç¼ºä¹å¯¹å•æ¬¡è¾“å‡ºè¿›è¡Œç³»ç»Ÿåˆ†æçš„é—®é¢˜ï¼Œç ”ç©¶äººå‘˜é€šè¿‡è·Ÿè¸ªæ¨¡å‹çš„ä¸­é—´éšè—çŠ¶æ€(intermediate hidden states)ï¼Œå°†å¾®è°ƒåçš„æ¨¡å‹ç²¾ç¡®åˆ†è§£ä¸ºé¢„è®­ç»ƒç»„ä»¶(pre-training component)å’Œå¾®è°ƒç»„ä»¶(fine-tuning component)ã€‚å®éªŒå‘ç°ï¼Œåœ¨æ¨¡å‹å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­é€šè¿‡ç¼©æ”¾å¾®è°ƒç»„ä»¶çš„æ¯”ä¾‹å¯ä»¥æœ‰æ•ˆå¼•å¯¼æ¨¡å‹çš„è¡Œä¸ºå’Œæ€§èƒ½ã€‚ç ”ç©¶æ®æ­¤å®šä¹‰äº†å¾®è°ƒè´¡çŒ®åº¦(TuCo)ä¸ºå¾®è°ƒç»„ä»¶ä¸é¢„è®­ç»ƒç»„ä»¶å¼ºåº¦çš„æ¯”å€¼ï¼Œå¹¶è§‚å¯Ÿåˆ°å¤šç§é’ˆå¯¹LLMsçš„å¯¹æŠ—æ€§æ”»å‡»(adversarial attacks)ä¼šé€šè¿‡é™ä½TuCoæ¥ç»•è¿‡å®‰å…¨æªæ–½ã€‚åœ¨æ”»å‡»æˆåŠŸçš„æç¤ºè¯ä¸Šï¼ŒTuCoå€¼ä¸€è‡´ä½äºæ”»å‡»å¤±è´¥çš„æƒ…å†µï¼Œè¿™è¡¨æ˜å‰Šå¼±å¾®è°ƒå¯¹è¾“å‡ºçš„å½±å“æ˜¯æ­¤ç±»æ”»å‡»å¥æ•ˆçš„é‡è¦åŸå› ã€‚æ€»è€Œè¨€ä¹‹ï¼ŒTuCoä¸ºé‡åŒ–åˆ†æå¾®è°ƒå¦‚ä½•å¡‘é€ æ¨¡å‹è¡Œä¸ºåŠå®‰å…¨æ€§æä¾›äº†å…¨æ–°çš„è§†è§’å’Œå·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.23423v1",
      "published_date": "2025-06-29 23:08:36 UTC",
      "updated_date": "2025-06-29 23:08:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:19:41.988631+00:00"
    },
    {
      "arxiv_id": "2506.23419v1",
      "title": "BenchMake: Turn any scientific data set into a reproducible benchmark",
      "title_zh": "BenchMakeï¼šå°†ä»»æ„ç§‘å­¦æ•°æ®é›†è½¬åŒ–ä¸ºå¯å¤ç°çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Amanda S Barnard"
      ],
      "abstract": "Benchmark data sets are a cornerstone of machine learning development and applications, ensuring new methods are robust, reliable and competitive. The relative rarity of benchmark sets in computational science, due to the uniqueness of the problems and the pace of change in the associated domains, makes evaluating new innovations difficult for computational scientists. In this paper a new tool is developed and tested to potentially turn any of the increasing numbers of scientific data sets made openly available into a benchmark accessible to the community. BenchMake uses non-negative matrix factorisation to deterministically identify and isolate challenging edge cases on the convex hull (the smallest convex set that contains all existing data instances) and partitions a required fraction of matched data instances into a testing set that maximises divergence and statistical significance, across tabular, graph, image, signal and textual modalities. BenchMake splits are compared to establish splits and random splits using ten publicly available benchmark sets from different areas of science, with different sizes, shapes, distributions.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åä¸º BenchMake çš„æ–°å·¥å…·ï¼Œæ—¨åœ¨å°†æ—¥ç›Šå¢å¤šçš„å…¬å¼€ç§‘å­¦æ•°æ®é›†è½¬åŒ–ä¸ºç¤¾åŒºå¯ç”¨çš„å¯é‡å¤ Benchmarkã€‚é’ˆå¯¹è®¡ç®—ç§‘å­¦é¢†åŸŸå› é—®é¢˜ç‹¬ç‰¹æ€§å’Œé¢†åŸŸå˜åŒ–å¿«å¯¼è‡´åŸºå‡†æ•°æ®é›†ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œè¯¥å·¥å…·ä¸ºè¯„ä¼°åˆ›æ–°æ–¹æ³•æä¾›äº†é‡è¦æ”¯æŒã€‚BenchMake é‡‡ç”¨ Non-negative Matrix Factorisation (NMF) æŠ€æœ¯ï¼Œèƒ½å¤Ÿç¡®å®šæ€§åœ°è¯†åˆ«å¹¶éš”ç¦»ä½äº Convex Hull ä¸Šçš„å…·æœ‰æŒ‘æˆ˜æ€§çš„è¾¹ç¼˜æ¡ˆä¾‹ã€‚å®ƒé€šè¿‡å°†ç‰¹å®šæ¯”ä¾‹çš„æ•°æ®å®ä¾‹åˆ’åˆ†åˆ° Testing Set ä¸­ï¼Œåœ¨ Tabularã€Graphã€Imageã€Signal å’Œ Textual ç­‰å¤šç§æ¨¡æ€ä¸‹æœ€å¤§é™åº¦åœ°æé«˜åˆ†æ­§åº¦å’Œç»Ÿè®¡æ˜¾è‘—æ€§ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ 10 ä¸ªæ¥è‡ªä¸åŒç§‘å­¦é¢†åŸŸçš„å…¬å¼€æ•°æ®é›†ï¼Œå°† BenchMake çš„åˆ’åˆ†æ•ˆæœä¸ç°æœ‰åˆ’åˆ†åŠ Random Splits è¿›è¡Œäº†å¯¹æ¯”éªŒè¯ã€‚è¯¥å·¥å…·ä¸ºç§‘å­¦æ•°æ®é›†å‘é«˜è´¨é‡ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æ•°æ®é›†çš„è‡ªåŠ¨åŒ–è½¬å˜æä¾›äº†å¯é çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 15 pages in Appendix, 15 figures, 5 tables, 57 references",
      "pdf_url": "https://arxiv.org/pdf/2506.23419v1",
      "published_date": "2025-06-29 22:56:48 UTC",
      "updated_date": "2025-06-29 22:56:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:19:43.546570+00:00"
    },
    {
      "arxiv_id": "2506.23394v1",
      "title": "Teaching a Language Model to Speak the Language of Tools",
      "title_zh": "æ•™å¯¼è¯­è¨€æ¨¡å‹æŒæ¡â€œå·¥å…·è¯­è¨€â€",
      "authors": [
        "Simeon Emanuilov"
      ],
      "abstract": "External tool integration through function-calling is essential for practical language model applications, yet most multilingual models lack reliable tool-use capabilities in non-English languages. Even state-of-the-art multilingual models struggle with determining when to use tools and generating the structured outputs required for function calls, often exhibiting language confusion when prompted in lower-resource languages. This work presents a methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. The approach involves continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples designed to support standardized protocols like MCP (Model Context Protocol). The research introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding, as verified on established Bulgarian benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready response formatting with clean, parsable function calls, contrasting with the verbose and inconsistent outputs of base models. The models, evaluation framework, and dataset are released to enable replication for other languages. This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹å¤§å¤šæ•°å¤šè¯­è¨€æ¨¡å‹åœ¨éè‹±è¯­ç¯å¢ƒä¸‹ç¼ºä¹å¯é çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ä½¿è¯­è¨€æ¨¡å‹åœ¨ä»»ä½•ç›®æ ‡è¯­è¨€ä¸­å®ç°ç¨³å¥ function-calling çš„é€‚é…æ–¹æ³•ã€‚ç ”ç©¶ä»¥ä¿åŠ åˆ©äºšè¯­ä¸ºæ¡ˆä¾‹ï¼Œé€šè¿‡åœ¨åŒ…å«10,035ä¸ª function-calling ç¤ºä¾‹çš„æ–°å‹åŒè¯­æ•°æ®é›†ä¸Šå¯¹ BgGPT ç³»åˆ—æ¨¡å‹è¿›è¡ŒæŒç»­è®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ”¯æŒ MCP (Model Context Protocol) ç­‰æ ‡å‡†åŒ–åè®®ã€‚ç ”ç©¶å¼•å…¥äº† TUCAN (Tool-Using Capable Assistant Navigator)ï¼Œå®éªŒè¯æ˜å…¶åœ¨ function-calling å‡†ç¡®ç‡ä¸Šæ¯”åŸºçº¿æ¨¡å‹æå‡äº†é«˜è¾¾ 28.75%ï¼ŒåŒæ—¶æœ‰æ•ˆä¿ç•™äº†æ ¸å¿ƒè¯­è¨€ç†è§£èƒ½åŠ›ã€‚ä¸åŸºçº¿æ¨¡å‹å†—é•¿ä¸”ä¸ä¸€è‡´çš„è¾“å‡ºç›¸æ¯”ï¼ŒTUCAN èƒ½å¤Ÿç”Ÿæˆç”Ÿäº§å°±ç»ªä¸”æ˜“äºè§£æçš„å‡½æ•°è°ƒç”¨æ ¼å¼ï¼Œè§£å†³äº†ä½èµ„æºè¯­è¨€ä¸­çš„è¯­è¨€æ··æ·†é—®é¢˜ã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºæ‰©å±•ä»¥è‹±è¯­ä¸ºä¸­å¿ƒä¹‹å¤–çš„å·¥å…·å¢å¼ºèƒ½åŠ›æä¾›äº†å®è·µè·¯å¾„ï¼Œè¿˜å¼€æºäº†ç›¸å…³çš„æ¨¡å‹ã€è¯„ä¼°æ¡†æ¶åŠæ•°æ®é›†ä»¥ä¾›å¤ç°ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23394v1",
      "published_date": "2025-06-29 20:47:27 UTC",
      "updated_date": "2025-06-29 20:47:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:19:52.486916+00:00"
    },
    {
      "arxiv_id": "2506.23393v1",
      "title": "Hierarchical Memory Organization for Wikipedia Generation",
      "title_zh": "é¢å‘ Wikipedia ç”Ÿæˆçš„å±‚çº§åŒ–è®°å¿†ç»„ç»‡",
      "authors": [
        "Eugene J. Yu",
        "Dawei Zhu",
        "Yifan Song",
        "Xiangyu Wong",
        "Jiebin Zhang",
        "Wenxuan Shi",
        "Xiaoguang Li",
        "Qun Liu",
        "Sujian Li"
      ],
      "abstract": "Generating Wikipedia articles autonomously is a challenging task requiring the integration of accurate, comprehensive, and well-structured information from diverse sources. This paper introduces the Memory Organization-based Generation (MOG) framework, a novel approach to address these challenges by leveraging a hierarchical memory architecture. MOG extracts fine-grained memory units from web documents, recursively organizes them into a Wikipedia-style hierarchical structure, and uses this structure to guide the generation process. This ensures alignment between memory and the article outline, improving both informativeness and verifiability while minimizing hallucinations. Additionally, a citation module is implemented to enhance traceability by linking every generated sentence to specific memory units. Evaluations on our newly created WikiStart dataset demonstrate that MOG outperforms baseline methods in producing informative and reliable articles, making it particularly robust in real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Memory Organization-based Generation (MOG)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªä¸»ç”ŸæˆWikipediaæ–‡ç« æ—¶é¢ä¸´çš„ä¿¡æ¯æ•´åˆä¸ç»“æ„åŒ–éš¾é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§å±‚æ¬¡åŒ–çš„è®°å¿†æ¶æ„(hierarchical memory architecture)ï¼Œé€šè¿‡ä»ç½‘ç»œæ–‡æ¡£ä¸­æå–ç»†ç²’åº¦çš„è®°å¿†å•å…ƒ(memory units)ï¼Œå¹¶å°†å…¶é€’å½’åœ°ç»„ç»‡æˆWikipediaé£æ ¼çš„åˆ†å±‚ç»“æ„æ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†è®°å¿†å†…å®¹ä¸æ–‡ç« å¤§çº²çš„ä¸€è‡´æ€§ï¼Œåœ¨æå‡ä¿¡æ¯é‡å’Œå¯éªŒè¯æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆå‡å°‘äº†å¹»è§‰(hallucinations)ç°è±¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å¼•ç”¨æ¨¡å—(citation module)ï¼Œé€šè¿‡å°†ç”Ÿæˆçš„å¥å­ä¸ç‰¹å®šè®°å¿†å•å…ƒå…³è”ï¼Œå¢å¼ºäº†å†…å®¹çš„å¯è¿½æº¯æ€§ã€‚åœ¨WikiStartæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMOGåœ¨ç”Ÿæˆä¿¡æ¯çš„å¯é æ€§ä¸å…¨é¢æ€§æ–¹é¢å‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå±•ç°äº†å…¶åœ¨ç°å®åº”ç”¨åœºæ™¯ä¸­çš„å¼ºå¤§é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2506.23393v1",
      "published_date": "2025-06-29 20:22:49 UTC",
      "updated_date": "2025-06-29 20:22:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:19:55.526576+00:00"
    },
    {
      "arxiv_id": "2507.02954v2",
      "title": "Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of Large Language Models on CFA Level III",
      "title_zh": "å¤§è§„æ¨¡é«˜çº§é‡‘èæ¨ç†ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨ CFA Level III ä¸­çš„å…¨é¢è¯„ä¼°",
      "authors": [
        "Pranam Shetty",
        "Abhisek Upadhayaya",
        "Parth Mitesh Shah",
        "Srikanth Jagabathula",
        "Shilpi Nayak",
        "Anna Joo Fee"
      ],
      "abstract": "As financial institutions increasingly adopt Large Language Models (LLMs), rigorous domain-specific evaluation becomes critical for responsible deployment. This paper presents a comprehensive benchmark evaluating 23 state-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam - the gold standard for advanced financial reasoning. We assess both multiple-choice questions (MCQs) and essay-style responses using multiple prompting strategies including Chain-of-Thought and Self-Discover. Our evaluation reveals that leading models demonstrate strong capabilities, with composite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA Level III. These results, achieved under a revised, stricter essay grading methodology, indicate significant progress in LLM capabilities for high-stakes financial applications. Our findings provide crucial guidance for practitioners on model selection and highlight remaining challenges in cost-effective deployment and the need for nuanced interpretation of performance against professional benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹23ç§æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç‰¹è®¸é‡‘èåˆ†æå¸ˆ(CFA)ä¸‰çº§è€ƒè¯•ä¸­çš„è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ—¨åœ¨ä¸ºé‡‘èæœºæ„çš„é«˜çº§é‡‘èæ¨ç†èƒ½åŠ›æä¾›ä¸¥è°¨çš„é¢†åŸŸç‰¹å®šåŸºå‡†ã€‚è¯„ä¼°æ¶µç›–äº†å¤šé€‰é¢˜(MCQs)å’Œè®ºè¿°é¢˜(essay-style responses)ï¼Œå¹¶å¯¹æ¯”äº†é“¾å¼æ€ç»´(Chain-of-Thought)å’Œè‡ªæˆ‘å‘ç°(Self-Discover)ç­‰å¤šç§æç¤ºç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¢†å…ˆæ¨¡å‹å±•ç°å‡ºå“è¶Šçš„ä¸“ä¸šèƒ½åŠ›ï¼Œå…¶ä¸­o4-miniå’ŒGemini 2.5 Flashåœ¨å¤æ‚çš„è€ƒè¯•ä»»åŠ¡ä¸­åˆ†åˆ«å–å¾—äº†79.1%å’Œ77.3%çš„ç»¼åˆè¯„åˆ†ã€‚ç ”ç©¶é‡‡ç”¨äº†ä¿®è®¢åæ›´åŠ ä¸¥æ ¼çš„è®ºè¿°é¢˜è¯„åˆ†æ–¹æ³•ï¼Œæ­ç¤ºäº†LLMsåœ¨å¤„ç†é«˜é£é™©é‡‘èåº”ç”¨æ–¹é¢çš„æ˜¾è‘—è¿›æ­¥ã€‚è¯¥å‘ç°ä¸ºä»ä¸šäººå‘˜åœ¨æ¨¡å‹é€‰æ‹©ä¸Šæä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†åœ¨æˆæœ¬æ•ˆç›Šéƒ¨ç½²ä»¥åŠå¯¹ä¸“ä¸šåŸºå‡†è¡¨ç°è¿›è¡Œç»†è‡´è§£è¯»æ–¹é¢ä¾ç„¶å­˜åœ¨çš„æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at FinLLM @ IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.02954v2",
      "published_date": "2025-06-29 19:54:57 UTC",
      "updated_date": "2025-09-22 17:05:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:19:54.847151+00:00"
    },
    {
      "arxiv_id": "2506.23382v1",
      "title": "SIEDD: Shared-Implicit Encoder with Discrete Decoders",
      "title_zh": "SIEDDï¼šç»“åˆç¦»æ•£è§£ç å™¨çš„å…±äº«éšå¼ç¼–ç å™¨",
      "authors": [
        "Vikram Rangarajan",
        "Shishira Maiya",
        "Max Ehrlich",
        "Abhinav Shrivastava"
      ],
      "abstract": "Implicit Neural Representations (INRs) offer exceptional fidelity for video compression by learning per-video optimized functions, but their adoption is crippled by impractically slow encoding times. Existing attempts to accelerate INR encoding often sacrifice reconstruction quality or crucial coordinate-level control essential for adaptive streaming and transcoding. We introduce SIEDD (Shared-Implicit Encoder with Discrete Decoders), a novel architecture that fundamentally accelerates INR encoding without these compromises. SIEDD first rapidly trains a shared, coordinate-based encoder on sparse anchor frames to efficiently capture global, low-frequency video features. This encoder is then frozen, enabling massively parallel training of lightweight, discrete decoders for individual frame groups, further expedited by aggressive coordinate-space sampling. This synergistic design delivers a remarkable 20-30X encoding speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while maintaining competitive reconstruction quality and compression ratios. Critically, SIEDD retains full coordinate-based control, enabling continuous resolution decoding and eliminating costly transcoding. Our approach significantly advances the practicality of high-fidelity neural video compression, demonstrating a scalable and efficient path towards real-world deployment. Our codebase is available at https://github.com/VikramRangarajan/SIEDD .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éšå¼ç¥ç»è¡¨ç¤º(Implicit Neural Representations, INRs)åœ¨è§†é¢‘å‹ç¼©ä¸­ç¼–ç é€Ÿåº¦ææ…¢çš„ç“¶é¢ˆï¼Œæå‡ºäº†SIEDDï¼ˆShared-Implicit Encoder with Discrete Decodersï¼‰æ¶æ„ï¼Œæ—¨åœ¨ä¸ç‰ºç‰²é‡å»ºè´¨é‡å’Œåæ ‡çº§æ§åˆ¶çš„å‰æä¸‹å®ç°å¤§å¹…åŠ é€Ÿã€‚SIEDDé¦–å…ˆåœ¨ç¨€ç–é”šå¸§(anchor frames)ä¸Šè®­ç»ƒå…±äº«çš„åæ ‡ç¼–ç å™¨(coordinate-based encoder)ä»¥æ•è·å…¨å±€ä½é¢‘è§†é¢‘ç‰¹å¾ï¼Œéšåé€šè¿‡åæ ‡ç©ºé—´é‡‡æ ·(coordinate-space sampling)ä¸ºå„å¸§ç»„å¹¶è¡Œè®­ç»ƒè½»é‡åŒ–ç¦»æ•£è§£ç å™¨(discrete decoders)ã€‚å®éªŒè¡¨æ˜ï¼ŒSIEDDåœ¨HDå’Œ4KåŸºå‡†æµ‹è¯•ä¸­æ¯”ç°æœ‰INRç¼–è§£ç å™¨æé€Ÿ20-30å€ï¼Œä¸”ä¿æŒäº†æå…·ç«äº‰åŠ›çš„å‹ç¼©æ¯”ã€‚è¯¥æ¶æ„å®Œæ•´ä¿ç•™äº†åŸºäºåæ ‡çš„æ§åˆ¶èƒ½åŠ›ï¼Œæ”¯æŒè¿ç»­åˆ†è¾¨ç‡è§£ç å¹¶æ¶ˆé™¤äº†æ˜‚è´µçš„è½¬ç è¿‡ç¨‹ã€‚è¿™ä¸€æˆæœæ˜¾è‘—å¢å¼ºäº†é«˜ä¿çœŸç¥ç»è§†é¢‘å‹ç¼©çš„å®ç”¨æ€§ï¼Œä¸ºå¤§è§„æ¨¡å®é™…éƒ¨ç½²æä¾›äº†é«˜æ•ˆã€å¯æ‰©å±•çš„å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page at https://vikramrangarajan.github.io/SIEDD . Project code at https://github.com/VikramRangarajan/SIEDD",
      "pdf_url": "https://arxiv.org/pdf/2506.23382v1",
      "published_date": "2025-06-29 19:39:43 UTC",
      "updated_date": "2025-06-29 19:39:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:19:59.211452+00:00"
    },
    {
      "arxiv_id": "2506.23377v2",
      "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs",
      "title_zh": "Perspective Dialï¼šæ–‡æœ¬è§‚ç‚¹åº¦é‡ä¸å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºå¼•å¯¼",
      "authors": [
        "Taejin Kim",
        "Siun-Chuon Mau",
        "Konrad Vesey"
      ],
      "abstract": "Large language models (LLMs) are used in a variety of mission-critical roles. Due to the rapidly developing nature of LLMs, there is a lack of quantifiable understanding of the bias and perspective associated with LLM output. Inspired by this need, this paper considers the broader issue of perspective or viewpoint of general text and perspective control of large-language model (LLM) output. Perspective-Dial consists of two main components: a (1) metric space, dubbed Perspective Space, that enables quantitative measurements of different perspectives regarding a topic, and the use of (2) Systematic Prompt Engineering that utilizes greedy-coordinate descent to control LLM output perspective based on measurement feedback from the Perspective Space. The empirical nature of the approach allows progress to side step a principled understanding of perspective or bias -- effectively quantifying and adjusting outputs for a variety of topics. Potential applications include detection, tracking and mitigation of LLM bias, narrative detection, sense making and tracking in public discourse, and debate bot advocating given perspective.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¾“å‡ºä¸­ç¼ºä¹å¯é‡åŒ–çš„åè§ (bias) å’Œè§†è§’ (perspective) ç†è§£è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åä¸º Perspective Dial çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†ç»„æˆï¼šé¦–å…ˆæ˜¯åä¸º Perspective Space çš„åº¦é‡ç©ºé—´ (metric space)ï¼Œæ—¨åœ¨å¯¹ç‰¹å®šä¸»é¢˜çš„ä¸åŒè§†è§’è¿›è¡Œå®šé‡è¡¡é‡ï¼›å…¶æ¬¡æ˜¯ Systematic Prompt Engineeringï¼Œå®ƒåˆ©ç”¨è´ªå©ªåæ ‡ä¸‹é™æ³• (greedy-coordinate descent) æ ¹æ®åº¦é‡ç©ºé—´çš„åé¦ˆæ¥ç²¾ç¡®å¼•å¯¼ LLM çš„è¾“å‡ºæ–¹å‘ã€‚è¿™ç§åŸºäºå®è¯çš„æ–¹æ³•æœ‰æ•ˆè§„é¿äº†å¯¹è§†è§’æˆ–åè§è¿›è¡ŒåŸåˆ™æ€§å®šä¹‰çš„éš¾é¢˜ï¼Œå®ç°äº†å¯¹å¤šç§ä¸»é¢˜è¾“å‡ºçš„é‡åŒ–ä¸è°ƒæ•´ã€‚è¯¥ç ”ç©¶çš„åº”ç”¨å‰æ™¯å¹¿é˜”ï¼ŒåŒ…æ‹¬ LLM åè§çš„æ£€æµ‹ä¸ç¼“è§£ã€å™äº‹æ£€æµ‹ (narrative detection) ä»¥åŠç‰¹å®šç«‹åœºçš„è¾©è®ºæœºå™¨äºº (debate bot) ç­‰é¢†åŸŸã€‚é€šè¿‡ Perspective Dialï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ›´ç§‘å­¦åœ°è¿½è¸ªå’Œæ§åˆ¶å…¬å…±è¯è¯­ä¸­çš„è§‚ç‚¹æ¼”å˜ï¼Œä¸ºæå‡æ¨¡å‹çš„å¯æ§æ€§ä¸é€æ˜åº¦æä¾›äº†é‡åŒ–è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work performed at CACI INTL INC",
      "pdf_url": "https://arxiv.org/pdf/2506.23377v2",
      "published_date": "2025-06-29 19:26:37 UTC",
      "updated_date": "2025-07-12 17:57:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:19:55.985888+00:00"
    },
    {
      "arxiv_id": "2506.23358v1",
      "title": "Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment",
      "title_zh": "è”é‚¦æ—¶é—´çº¿åˆæˆï¼šä¸€ç§å¯æ‰©å±•ä¸”éšç§ä¿æŠ¤çš„æ¨¡å‹è®­ç»ƒä¸éƒ¨ç½²æ–¹æ³•",
      "authors": [
        "Pawel Renc",
        "Michal K. Grzeszczyk",
        "Linglong Qian",
        "Nassim Oufattole",
        "Jeff Rasley",
        "Arkadiusz Sitek"
      ],
      "abstract": "We present Federated Timeline Synthesis (FTS), a novel framework for training generative foundation models across distributed timeseries data applied to electronic health records (EHR). At its core, FTS represents patient history as tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding temporal, categorical, and continuous clinical information. Each institution trains an autoregressive transformer on its local PHTs and transmits only model weights to a central server. The server uses the generators to synthesize a large corpus of trajectories and train a Global Generator (GG), enabling zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS on five clinically meaningful prediction tasks using MIMIC-IV data, showing that models trained on synthetic data generated by GG perform comparably to those trained on real data. FTS offers strong privacy guarantees, scalability across institutions, and extensibility to diverse prediction and simulation tasks especially in healthcare, including counterfactual inference, early warning detection, and synthetic trial design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Federated Timeline Synthesis (FTS)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åˆ†å¸ƒå¼ç”µå­å¥åº·æ¡£æ¡ˆ(EHR)æ—¶é—´åºåˆ—æ•°æ®è®­ç»ƒç”Ÿæˆå¼åŸºç¡€æ¨¡å‹çš„åˆ›æ–°æ¡†æ¶ã€‚FTSçš„æ ¸å¿ƒæ˜¯å°†æ‚£è€…ç—…å²è¡¨ç¤ºä¸ºæ ‡è®°åŒ–çš„Patient Health Timelines (PHTs)ï¼Œå³ä¸€ç§ç¼–ç äº†æ—¶é—´ã€ç±»åˆ«å’Œè¿ç»­ä¸´åºŠä¿¡æ¯çš„è¯­è¨€æ— å…³åºåˆ—ã€‚åœ¨è”é‚¦å­¦ä¹ æµç¨‹ä¸­ï¼Œå„åŒ»ç–—æœºæ„åœ¨æœ¬åœ°PHTsä¸Šè®­ç»ƒè‡ªå›å½’transformeræ¨¡å‹å¹¶ä»…å‘ä¸­å¤®æœåŠ¡å™¨ä¼ è¾“æ¨¡å‹æƒé‡ã€‚ä¸­å¤®æœåŠ¡å™¨åˆ©ç”¨æ¥æ”¶åˆ°çš„ç”Ÿæˆå™¨åˆæˆå¤§è§„æ¨¡è½¨è¿¹è¯­æ–™åº“ä»¥è®­ç»ƒGlobal Generator (GG)ï¼Œè¿›è€Œé€šè¿‡å¯¹æœªæ¥PHTsçš„Monte Carloæ¨¡æ‹Ÿå®ç°é›¶æ ·æœ¬æ¨ç†ã€‚é€šè¿‡å¯¹MIMIC-IVæ•°æ®ä¸­äº”é¡¹ä¸´åºŠé¢„æµ‹ä»»åŠ¡çš„è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºç”±GGç”Ÿæˆçš„åˆæˆæ•°æ®åœ¨æ¨¡å‹è®­ç»ƒæ•ˆæœä¸Šä¸çœŸå®æ•°æ®è¡¨ç°ç›¸å½“ã€‚è¯¥æ–¹æ³•åœ¨æä¾›å¼ºå¤§éšç§ä¿éšœå’Œæœºæ„é—´å¯æ‰©å±•æ€§çš„åŒæ—¶ï¼Œä¹Ÿä¸ºåäº‹å®æ¨ç†ã€æ—©æœŸé¢„è­¦æ£€æµ‹åŠåˆæˆä¸´åºŠè¯•éªŒè®¾è®¡ç­‰åŒ»ç–—æ¨¡æ‹Ÿä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "conference paper",
      "pdf_url": "https://arxiv.org/pdf/2506.23358v1",
      "published_date": "2025-06-29 18:29:59 UTC",
      "updated_date": "2025-06-29 18:29:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:20:16.705647+00:00"
    },
    {
      "arxiv_id": "2506.23351v2",
      "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop",
      "title_zh": "å¯æ³›åŒ–åŒè‡‚æ“ä½œçš„åŸºå‡†æµ‹è¯•ï¼šCVPR 2025 MEIS ç ”è®¨ä¼š RoboTwin åŒè‡‚åä½œæŒ‘æˆ˜èµ›",
      "authors": [
        "Tianxing Chen",
        "Kaixuan Wang",
        "Zhaohui Yang",
        "Yuhao Zhang",
        "Zanxin Chen",
        "Baijun Chen",
        "Wanxi Dong",
        "Ziyuan Liu",
        "Dong Chen",
        "Tianshuo Yang",
        "Haibao Yu",
        "Xiaokang Yang",
        "Yusen Qin",
        "Zhiqiang Xie",
        "Yao Mu",
        "Ping Luo",
        "Tian Nian",
        "Weiliang Deng",
        "Yiheng Ge",
        "Yibin Liu",
        "Zixuan Li",
        "Dehui Wang",
        "Zhixuan Liang",
        "Haohui Xie",
        "Rijie Zeng",
        "Yunfei Ge",
        "Peiqing Cong",
        "Guannan He",
        "Zhaoming Han",
        "Ruocheng Yin",
        "Jingxiang Guo",
        "Lunkai Lin",
        "Tianling Xu",
        "Hongzhe Bi",
        "Xuewu Lin",
        "Tianwei Lin",
        "Shujie Luo",
        "Keyu Li",
        "Ziyan Zhao",
        "Ke Fan",
        "Heyang Xu",
        "Bo Peng",
        "Wenlong Gao",
        "Dongjiang Li",
        "Feng Jin",
        "Hui Shen",
        "Jinming Li",
        "Chaowei Cui",
        "Yu Chen",
        "Yaxin Peng",
        "Lingdong Zeng",
        "Wenlong Dong",
        "Tengfei Li",
        "Weijie Ke",
        "Jun Chen",
        "Erdemt Bao",
        "Tian Lan",
        "Tenglong Liu",
        "Jin Yang",
        "Huiping Zhuang",
        "Baozhi Jia",
        "Shuai Zhang",
        "Zhengfeng Zou",
        "Fangheng Guan",
        "Tianyi Jia",
        "Ke Zhou",
        "Hongjiu Zhang",
        "Yating Han",
        "Cheng Fang",
        "Yixian Zou",
        "Chongyang Xu",
        "Qinglun Zhang",
        "Shen Cheng",
        "Xiaohe Wang",
        "Ping Tan",
        "Haoqiang Fan",
        "Shuaicheng Liu",
        "Jiaheng Chen",
        "Chuxuan Huang",
        "Chengliang Lin",
        "Kaijun Luo",
        "Boyu Yue",
        "Yi Liu",
        "Jinyu Chen",
        "Zichang Tan",
        "Liming Deng",
        "Shuo Xu",
        "Zijian Cai",
        "Shilong Yin",
        "Hao Wang",
        "Hongshan Liu",
        "Tianyang Li",
        "Long Shi",
        "Ran Xu",
        "Huilin Xu",
        "Zhengquan Zhang",
        "Congsheng Xu",
        "Jinchang Yang",
        "Feng Xu"
      ],
      "abstract": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in robotics, driven by the need for autonomous systems that can perceive, reason, and act in complex physical environments. While single-arm systems have shown strong task performance, collaborative dual-arm systems are essential for handling more intricate tasks involving rigid, deformable, and tactile-sensitive objects. To advance this goal, we launched the RoboTwin Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot platform, the competition consisted of three stages: Simulation Round 1, Simulation Round 2, and a final Real-World Round. Participants totally tackled 17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based scenarios. The challenge attracted 64 global teams and over 400 participants, producing top-performing solutions like SEM and AnchorDP3 and generating valuable insights into generalizable bimanual policy learning. This report outlines the competition setup, task design, evaluation methodology, key findings and future direction, aiming to support future research on robust and generalizable bimanual manipulation policies. The Challenge Webpage is available at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.",
      "tldr_zh": "è¯¥æŠ¥å‘Šä»‹ç»äº†åœ¨ CVPR 2025 MEIS Workshop ä¸Šä¸¾åŠçš„ RoboTwin Dual-Arm Collaboration Challengeï¼Œæ—¨åœ¨åº”å¯¹å…·èº«æ™ºèƒ½ (Embodied AI) ä¸­å¤æ‚åŒè‡‚åä½œæ“æ§çš„æŒ‘æˆ˜ã€‚è¯¥æŒ‘æˆ˜èµ›åŸºäº RoboTwin Simulation æ¨¡æ‹Ÿå¹³å°å’Œ AgileX COBOT-Magic Robot ç¡¬ä»¶å¹³å°ï¼Œè®¾ç½®äº†ä»æ¨¡æ‹Ÿåˆ°å®ä½“çš„ä¸‰ä¸ªé˜¶æ®µï¼Œæ¶µç›–äº†æ¶‰åŠåˆšæ€§ã€å¯å˜å½¢ç‰©ä½“åŠè§¦è§‰åé¦ˆçš„ 17 é¡¹å¤æ‚ä»»åŠ¡ã€‚æ´»åŠ¨å…±å¸å¼•å…¨çƒ 64 æ”¯é˜Ÿä¼å‚ä¸ï¼Œå‚¬ç”Ÿäº† SEM å’Œ AnchorDP3 ç­‰è¡¨ç°ä¼˜å¼‚çš„ç®—æ³•æ¨¡å‹ï¼Œå¹¶ä¸ºåŒè‡‚ç­–ç•¥çš„æ³›åŒ–æ€§å­¦ä¹ ç§¯ç´¯äº†å…³é”®æ´å¯Ÿã€‚æŠ¥å‘Šè¯¦ç»†è®°å½•äº†ç«èµ›çš„ä»»åŠ¡è®¾è®¡ã€è¯„ä¼°ä½“ç³»åŠæ ¸å¿ƒå‘ç°ï¼Œä¸ºæœªæ¥å¼€å‘æ›´å…·é²æ£’æ€§å’Œé€šç”¨æ€§çš„åŒè‡‚æ“æ§ç³»ç»Ÿæä¾›äº†é‡è¦çš„åŸºå‡†å‚è€ƒä¸ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "Challenge Webpage: https://robotwin-benchmark.github.io/cvpr-2025-challenge/",
      "pdf_url": "https://arxiv.org/pdf/2506.23351v2",
      "published_date": "2025-06-29 17:56:41 UTC",
      "updated_date": "2025-07-03 03:30:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:20:19.824076+00:00"
    },
    {
      "arxiv_id": "2506.23342v1",
      "title": "ATGen: A Framework for Active Text Generation",
      "title_zh": "ATGenï¼šä¸»åŠ¨æ–‡æœ¬ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Akim Tsvigun",
        "Daniil Vasilev",
        "Ivan Tsvigun",
        "Ivan Lysenko",
        "Talgat Bektleuov",
        "Aleksandr Medvedev",
        "Uliana Vinogradova",
        "Nikita Severin",
        "Mikhail Mozikov",
        "Andrey Savchenko",
        "Rostislav Grigorev",
        "Ramil Kuleev",
        "Fedor Zhdanov",
        "Artem Shelmanov",
        "Ilya Makarov"
      ],
      "abstract": "Active learning (AL) has demonstrated remarkable potential in reducing the annotation effort required for training machine learning models. However, despite the surging popularity of natural language generation (NLG) tasks in recent years, the application of AL to NLG has been limited. In this paper, we introduce Active Text Generation (ATGen) - a comprehensive framework that bridges AL with text generation tasks, enabling the application of state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered annotation in NLG tasks using both human annotators and automatic annotation agents based on large language models (LLMs). The framework supports LLMs deployed as services, such as ChatGPT and Claude, or operated on-premises. Furthermore, ATGen provides a unified platform for smooth implementation and benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present evaluation results for state-of-the-art AL strategies across diverse settings and multiple text generation tasks. We show that ATGen reduces both the effort of human annotators and costs associated with API calls to LLM-based annotation agents. The code of the framework is available on GitHub under the MIT license. The video presentation is available at http://atgen-video.nlpresearch.group",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ATGenï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†ä¸»åŠ¨å­¦ä¹ (Active Learning)ä¸æ–‡æœ¬ç”Ÿæˆä»»åŠ¡(NLG)ç›¸è¿æ¥çš„ç»¼åˆæ€§æ¡†æ¶ã€‚å°½ç®¡ä¸»åŠ¨å­¦ä¹ åœ¨é™ä½æœºå™¨å­¦ä¹ æ ‡æ³¨æˆæœ¬æ–¹é¢æ½œåŠ›å·¨å¤§ï¼Œä½†åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨ä»è¾ƒä¸ºå—é™ï¼ŒATGenä¸ºæ­¤æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆäººç±»æ ‡æ³¨å‘˜å’ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªåŠ¨æ ‡æ³¨æ™ºèƒ½ä½“ï¼Œç®€åŒ–äº†NLGä»»åŠ¡ä¸­å—ä¸»åŠ¨å­¦ä¹ èµ‹èƒ½çš„æ ‡æ³¨æµç¨‹ã€‚å®ƒä¸ä»…æ”¯æŒChatGPTã€Claudeç­‰åœ¨çº¿äº‘ç«¯æœåŠ¡ï¼Œä¹Ÿå…¼å®¹æœ¬åœ°éƒ¨ç½²çš„LLMsã€‚æ­¤å¤–ï¼ŒATGenä¸ºé’ˆå¯¹NLGä»»åŠ¡å®šåˆ¶çš„æ–°å‹ä¸»åŠ¨å­¦ä¹ ç­–ç•¥æä¾›äº†ç»Ÿä¸€çš„å®ç°ä¸åŸºå‡†æµ‹è¯•å¹³å°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒATGenèƒ½æ˜¾è‘—å‡è½»äººç±»æ ‡æ³¨è€…çš„å·¥ä½œè´Ÿæ‹…å¹¶é™ä½è°ƒç”¨APIçš„ç»æµæˆæœ¬ï¼Œä¸ºè‡ªç„¶è¯­è¨€ç”Ÿæˆé¢†åŸŸçš„æ ‡æ³¨ä¼˜åŒ–æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2025 System Demonstrations",
      "pdf_url": "https://arxiv.org/pdf/2506.23342v1",
      "published_date": "2025-06-29 17:27:48 UTC",
      "updated_date": "2025-06-29 17:27:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:20:18.786366+00:00"
    },
    {
      "arxiv_id": "2506.23339v2",
      "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design",
      "title_zh": "VALID-Molï¼šå¤§è¯­è¨€æ¨¡å‹è¾…åŠ©åˆ†å­è®¾è®¡çš„ç³»ç»Ÿæ€§éªŒè¯æ¡†æ¶",
      "authors": [
        "Malikussaid",
        "Hilal Hudan Nuha",
        "Isman Kurniawan"
      ],
      "abstract": "Large Language Models demonstrate substantial promise for advancing scientific discovery, yet their deployment in disciplines demanding factual precision and specialized domain constraints presents significant challenges. Within molecular design for pharmaceutical development, these models can propose innovative molecular modifications but frequently generate chemically infeasible structures. We introduce VALID-Mol, a comprehensive framework that integrates chemical validation with LLM-driven molecular design, achieving an improvement in valid chemical structure generation from 3% to 83%. Our methodology synthesizes systematic prompt optimization, automated chemical verification, and domain-adapted fine-tuning to ensure dependable generation of synthesizable molecules with enhanced properties. Our contribution extends beyond implementation details to provide a transferable methodology for scientifically-constrained LLM applications with measurable reliability enhancements. Computational analyses indicate our framework generates promising synthesis candidates with up to 17-fold predicted improvements in target binding affinity while preserving synthetic feasibility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¯ç‰©ç ”å‘çš„åˆ†å­è®¾è®¡ä¸­å¸¸äº§ç”ŸåŒ–å­¦ä¸å¯è¡Œç»“æ„çš„é—®é¢˜ï¼Œæå‡ºäº†VALID-Molè¿™ä¸€ç³»ç»ŸåŒ–æ¡†æ¶ã€‚VALID-Molé€šè¿‡æ•´åˆåŒ–å­¦éªŒè¯ä¸LLMé©±åŠ¨çš„åˆ†å­è®¾è®¡ï¼Œé‡‡ç”¨äº†ç³»ç»ŸåŒ–çš„æç¤ºè¯ä¼˜åŒ–(prompt optimization)ã€è‡ªåŠ¨åŒ–åŒ–å­¦æ ¡éªŒ(automated chemical verification)ä»¥åŠé¢†åŸŸè‡ªé€‚åº”å¾®è°ƒ(domain-adapted fine-tuning)ç­‰å…³é”®æŠ€æœ¯ï¼Œç¡®ä¿äº†åˆ†å­ç”Ÿæˆçš„ç§‘å­¦å¯é æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶å°†æœ‰æ•ˆåŒ–å­¦ç»“æ„çš„ç”Ÿæˆç‡ä»3%å¤§å¹…æå‡è‡³83%ï¼Œå¹¶èƒ½ç”Ÿæˆç›®æ ‡ç»“åˆäº²å’ŒåŠ›(target binding affinity)é¢„æµ‹æå‡è¾¾17å€çš„å€™é€‰åˆ†å­ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„åˆæˆå¯è¡Œæ€§(synthetic feasibility)ã€‚æ­¤é¡¹å·¥ä½œä¸ä»…æå‡äº†åˆ†å­è®¾è®¡çš„æ•ˆç‡ï¼Œè¿˜ä¸ºå—ç§‘å­¦çº¦æŸçš„LLMåº”ç”¨æä¾›äº†ä¸€å¥—å…·æœ‰å¯è¡¡é‡å¯é æ€§æå‡çš„å¯è¿ç§»æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 1 figure, 1 algorithm, 5 tables, to be published in ISPACS 2025, unabridged version exists as arXiv:2506.23339v1",
      "pdf_url": "https://arxiv.org/pdf/2506.23339v2",
      "published_date": "2025-06-29 17:17:04 UTC",
      "updated_date": "2025-10-16 17:43:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:20:23.214902+00:00"
    },
    {
      "arxiv_id": "2506.23334v2",
      "title": "Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation",
      "title_zh": "åŸºäºåˆæˆè¶…å£°å›¾åƒå¢å¼ºçš„è”é‚¦ä¹³è…ºç™Œæ£€æµ‹",
      "authors": [
        "Hongyi Pan",
        "Ziliang Hong",
        "Gorkem Durak",
        "Ziyue Xu",
        "Ulas Bagci"
      ],
      "abstract": "Federated learning (FL) has emerged as a promising paradigm for collaboratively training deep learning models across institutions without exchanging sensitive medical data. However, its effectiveness is often hindered by limited data availability and non-independent, identically distributed data across participating clients, which can degrade model performance and generalization. To address these challenges, we propose a generative AI based data augmentation framework that integrates synthetic image sharing into the federated training process for breast cancer diagnosis via ultrasound images. Specifically, we train two simple class-specific Deep Convolutional Generative Adversarial Networks: one for benign and one for malignant lesions. We then simulate a realistic FL setting using three publicly available breast ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are adopted as baseline FL algorithms. Experimental results show that incorporating a suitable number of synthetic images improved the average AUC from 0.9206 to 0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that excessive use of synthetic data reduced performance, underscoring the importance of maintaining a balanced ratio of real and synthetic samples. Our findings highlight the potential of generative AI based data augmentation to enhance FL results in the breast ultrasound image classification task.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹è”é‚¦å­¦ä¹  (Federated Learning) åœ¨ä¹³è…ºç™Œè¯Šæ–­ä¸­é¢ä¸´çš„æ•°æ®å¯ç”¨æ€§æœ‰é™å’Œæ•°æ®éç‹¬ç«‹åŒåˆ†å¸ƒ (non-IID) ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) çš„æ•°æ®å¢å¼ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è®­ç»ƒä¸¤ä¸ªç‰¹å®šç±»åˆ«çš„æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Deep Convolutional Generative Adversarial Networks) åˆ†åˆ«ç”Ÿæˆè‰¯æ€§å’Œæ¶æ€§ç—…å˜çš„åˆæˆè¶…å£°å›¾åƒï¼Œå¹¶å°†å…¶æ•´åˆåˆ°è”é‚¦è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨ BUSIã€BUS-BRA å’Œ UDIAT ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†æ¨¡æ‹Ÿäº†ç°å®çš„è”é‚¦å­¦ä¹ åœºæ™¯ï¼Œå¹¶é‡‡ç”¨äº† FedAvg å’Œ FedProx ä½œä¸ºåŸºå‡†ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥é€‚é‡çš„åˆæˆå›¾åƒæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå…¶ä¸­ FedAvg çš„å¹³å‡æ›²çº¿ä¸‹é¢ç§¯ (AUC) ä» 0.9206 æå‡è‡³ 0.9237ï¼ŒFedProx åˆ™ä» 0.9429 æå‡è‡³ 0.9538ã€‚ç ”ç©¶åŒæ—¶æŒ‡å‡ºï¼Œè¿‡åº¦ä½¿ç”¨åˆæˆæ•°æ®åå€’ä¼šé™ä½æ€§èƒ½ï¼Œå¼ºè°ƒäº†åœ¨è®­ç»ƒä¸­ä¿æŒçœŸå®æ ·æœ¬ä¸åˆæˆæ ·æœ¬å¹³è¡¡æ¯”ä¾‹çš„é‡è¦æ€§ã€‚è¯¥ç ”ç©¶ç»“æœçªæ˜¾äº†åˆ©ç”¨ç”Ÿæˆå¼æ¨¡å‹è¿›è¡Œæ•°æ®å¢å¼ºåœ¨æå‡ä¹³è…ºè¶…å£°å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è”é‚¦å­¦ä¹ æ•ˆæœçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23334v2",
      "published_date": "2025-06-29 17:05:50 UTC",
      "updated_date": "2025-07-08 21:03:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:20:32.857368+00:00"
    },
    {
      "arxiv_id": "2506.23325v2",
      "title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs",
      "title_zh": "XY-Tokenizerï¼šç¼“è§£ä½æ¯”ç‰¹ç‡è¯­éŸ³ç¼–è§£ç å™¨ä¸­çš„è¯­ä¹‰-å£°å­¦å†²çª",
      "authors": [
        "Yitian Gong",
        "Luozhijie Jin",
        "Ruifan Deng",
        "Dong Zhang",
        "Xin Zhang",
        "Qinyuan Cheng",
        "Zhaoye Fei",
        "Shimin Li",
        "Xipeng Qiu"
      ],
      "abstract": "Speech codecs serve as bridges between speech signals and large language models. An ideal codec for speech language models should not only preserve acoustic information but also capture rich semantic information. However, existing speech codecs struggle to balance high-quality audio reconstruction with ease of modeling by language models. In this study, we analyze the limitations of previous codecs in balancing semantic richness and acoustic fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict between semantic and acoustic capabilities through multi-stage, multi-task learning. Experimental results demonstrate that XY-Tokenizer achieves performance in both semantic and acoustic tasks comparable to that of state-of-the-art codecs operating at similar bitrates, even though those existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer achieves strong text alignment, surpassing distillation-based semantic modeling methods such as SpeechTokenizer and Mimi, while maintaining a speaker similarity score of 0.83 between reconstructed and original audio. The reconstruction performance of XY-Tokenizer is comparable to that of BigCodec, the current state-of-the-art among acoustic-only codecs, which achieves a speaker similarity score of 0.84 at a similar bitrate. Code and models are available at https://github.com/gyt1145028706/XY-Tokenizer.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ä¸­ç¼–è§£ç å™¨ï¼ˆSpeech Codecsï¼‰åœ¨è¯­ä¹‰ä¿¡æ¯æ•æ‰ä¸å£°å­¦ä¿çœŸåº¦ä¹‹é—´å­˜åœ¨çš„å†²çªï¼Œæå‡ºäº†XY-Tokenizeræ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šé˜¶æ®µã€å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMulti-stage, Multi-task Learningï¼‰ç­–ç•¥ï¼Œæ—¨åœ¨ä½æ¯”ç‰¹ç‡ç¯å¢ƒä¸‹åŒæ—¶å®ç°é«˜è´¨é‡çš„éŸ³é¢‘é‡å»ºä¸ä¾¿æ·çš„è¯­è¨€æ¨¡å‹å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒXY-Tokenizeråœ¨è¯­ä¹‰å’Œå£°å­¦ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡å¯ä¸å½“å‰æœ€å…ˆè¿›çš„ï¼ˆState-of-the-artï¼‰ç¼–è§£ç å™¨ç›¸åª²ç¾ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨ä¸¤é¡¹æŒ‡æ ‡ä¸Šéš¾ä»¥å…¼é¡¾çš„é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œå…¶æ–‡æœ¬å¯¹é½æ€§èƒ½è¶…è¶Šäº†SpeechTokenizerå’ŒMimiç­‰åŸºäºè’¸é¦çš„è¯­ä¹‰å»ºæ¨¡æ–¹æ³•ï¼ŒåŒæ—¶åœ¨è¯´è¯äººç›¸ä¼¼åº¦è¯„åˆ†ä¸Šè¾¾åˆ°0.83ï¼Œæ€§èƒ½æ¥è¿‘çº¯å£°å­¦ç¼–è§£ç å™¨BigCodecã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºå…¼å…·è¯­ä¹‰ä¸°å¯Œåº¦ä¸å£°å­¦è¿˜åŸèƒ½åŠ›çš„ç†æƒ³è¯­éŸ³ç¼–è§£ç å™¨æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23325v2",
      "published_date": "2025-06-29 16:51:50 UTC",
      "updated_date": "2025-07-09 17:40:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:20:34.182287+00:00"
    },
    {
      "arxiv_id": "2507.01059v1",
      "title": "Automated Vehicles Should be Connected with Natural Language",
      "title_zh": "è‡ªåŠ¨é©¾é©¶è½¦è¾†åº”é€šè¿‡è‡ªç„¶è¯­è¨€å®ç°äº’è”",
      "authors": [
        "Xiangbo Gao",
        "Keshu Wu",
        "Hao Zhang",
        "Kexin Tian",
        "Yang Zhou",
        "Zhengzhong Tu"
      ],
      "abstract": "Multi-agent collaborative driving promises improvements in traffic safety and efficiency through collective perception and decision making. However, existing communication media -- including raw sensor data, neural network features, and perception results -- suffer limitations in bandwidth efficiency, information completeness, and agent interoperability. Moreover, traditional approaches have largely ignored decision-level fusion, neglecting critical dimensions of collaborative driving. In this paper we argue that addressing these challenges requires a transition from purely perception-oriented data exchanges to explicit intent and reasoning communication using natural language. Natural language balances semantic density and communication bandwidth, adapts flexibly to real-time conditions, and bridges heterogeneous agent platforms. By enabling the direct communication of intentions, rationales, and decisions, it transforms collaborative driving from reactive perception-data sharing into proactive coordination, advancing safety, efficiency, and transparency in intelligent transportation systems.",
      "tldr_zh": "è¯¥è®ºæ–‡æå‡ºè‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ˆAutomated Vehiclesï¼‰åº”é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œè¿æ¥ï¼Œæ—¨åœ¨æå‡å¤šæ™ºèƒ½ä½“ååŒé©¾é©¶çš„å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚ç°æœ‰çš„ä¼ æ„Ÿå™¨åŸå§‹æ•°æ®ã€ç¥ç»ç½‘ç»œç‰¹å¾å’Œæ„ŸçŸ¥ç»“æœç­‰é€šä¿¡ä»‹è´¨åœ¨å¸¦å®½æ•ˆç‡ã€ä¿¡æ¯å®Œæ•´æ€§å’Œæ™ºèƒ½ä½“äº’æ“ä½œæ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œä¸”å¾€å¾€å¿½ç•¥äº†å†³ç­–å±‚ï¼ˆdecision-levelï¼‰èåˆã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶ä¸»å¼ ä»å•çº¯çš„æ„ŸçŸ¥å¯¼å‘æ•°æ®äº¤æ¢è½¬å‘åˆ©ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œçš„æ˜¾å¼æ„å›¾ï¼ˆintentï¼‰ä¸æ¨ç†ï¼ˆreasoningï¼‰é€šä¿¡ã€‚è‡ªç„¶è¯­è¨€åœ¨è¯­ä¹‰å¯†åº¦å’Œé€šä¿¡å¸¦å®½ä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œèƒ½çµæ´»é€‚åº”å®æ—¶çŠ¶å†µå¹¶æœ‰æ•ˆæ¡¥æ¥å¼‚æ„æ™ºèƒ½ä½“å¹³å°ã€‚é€šè¿‡å®ç°æ„å›¾ã€ç†ç”±å’Œå†³ç­–çš„ç›´æ¥äº¤æµï¼Œè¯¥æ–¹æ³•å°†ååŒé©¾é©¶ä»è¢«åŠ¨çš„æ„ŸçŸ¥æ•°æ®å…±äº«è½¬å˜ä¸ºä¸»åŠ¨åè°ƒï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½äº¤é€šç³»ç»Ÿçš„å®‰å…¨æ€§ã€æ•ˆç‡å’Œé€æ˜åº¦ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.01059v1",
      "published_date": "2025-06-29 16:41:19 UTC",
      "updated_date": "2025-06-29 16:41:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:20:36.917442+00:00"
    },
    {
      "arxiv_id": "2506.23322v1",
      "title": "GaussMaster: An LLM-based Database Copilot System",
      "title_zh": "GaussMasterï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ•°æ®åº“æ™ºèƒ½åŠ©æ‰‹ç³»ç»Ÿ",
      "authors": [
        "Wei Zhou",
        "Ji Sun",
        "Xuanhe Zhou",
        "Guoliang Li",
        "Luyang Liu",
        "Hao Wu",
        "Tianyuan Wang"
      ],
      "abstract": "In the financial industry, data is the lifeblood of operations, and DBAs shoulder significant responsibilities for SQL tuning, database deployment, diagnosis, and service repair. In recent years, both database vendors and customers have increasingly turned to autonomous database platforms in an effort to alleviate the heavy workload of DBAs. However, existing autonomous database platforms are limited in their capabilities, primarily addressing single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual intervention remains a necessity for comprehensive database maintenance. GaussMaster aims to revolutionize this landscape by introducing an LLM-based database copilot system. This innovative solution is designed not only to assist developers in writing efficient SQL queries but also to provide comprehensive care for database services. When database instances exhibit abnormal behavior, GaussMaster is capable of orchestrating the entire maintenance process automatically. It achieves this by analyzing hundreds of metrics and logs, employing a Tree-of-thought approach to identify root causes, and invoking appropriate tools to resolve issues. We have successfully implemented GaussMaster in real-world scenarios, such as the banking industry, where it has achieved zero human intervention for over 34 database maintenance scenarios. In this paper, we present significant improvements in these tasks with code at https://gitcode.com/opengauss/openGauss-GaussMaster.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GaussMasterï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ•°æ®åº“Copilotç³»ç»Ÿï¼Œæ—¨åœ¨å‡è½»é‡‘èè¡Œä¸šä¸­æ•°æ®åº“ç®¡ç†å‘˜(DBA)åœ¨SQL tuningã€éƒ¨ç½²åŠè¯Šæ–­ç­‰æ–¹é¢çš„æ²‰é‡è´Ÿæ‹…ã€‚ä¸ä»…èƒ½å¤„ç†NL2SQLæˆ–å¼‚å¸¸æ£€æµ‹ç­‰å•ä¸€é—®é¢˜çš„ç°æœ‰å¹³å°ä¸åŒï¼ŒGaussMasteré€šè¿‡Tree-of-thoughtæ–¹æ³•åˆ†ææ•°ç™¾ä¸ªæŒ‡æ ‡å’Œæ—¥å¿—ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç¼–æ’æ•´ä¸ªæ•°æ®åº“ç»´æŠ¤æµç¨‹å¹¶ç²¾å‡†è¯†åˆ«æ ¹å› ã€‚è¯¥ç³»ç»Ÿä¸ä»…èƒ½ååŠ©å¼€å‘äººå‘˜ç¼–å†™é«˜æ•ˆçš„SQLæŸ¥è¯¢ï¼Œè¿˜èƒ½åœ¨æ•°æ®åº“å®ä¾‹å‡ºç°å¼‚å¸¸æ—¶è‡ªåŠ¨è°ƒç”¨ç›¸åº”å·¥å…·è¿›è¡ŒæœåŠ¡ä¿®å¤ã€‚GaussMasterå·²åœ¨é“¶è¡Œä¸šç­‰çœŸå®åœºæ™¯ä¸­æˆåŠŸè½åœ°ï¼Œåœ¨è¶…è¿‡34ä¸ªç»´æŠ¤åœºæ™¯ä¸­å®ç°äº†é›¶äººå·¥å¹²é¢„ã€‚å®éªŒç»“æœè¯æ˜è¯¥ç³»ç»Ÿæ˜¾è‘—æå‡äº†æ•°æ®åº“ç»´æŠ¤æ•ˆç‡ï¼Œç›®å‰å…¶æ ¸å¿ƒä»£ç å·²å…¬å¼€ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.DB",
      "comment": "We welcome contributions from the community. For reference, please see the code at: https://gitcode.com/opengauss/openGauss-GaussMaster",
      "pdf_url": "https://arxiv.org/pdf/2506.23322v1",
      "published_date": "2025-06-29 16:39:31 UTC",
      "updated_date": "2025-06-29 16:39:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:20:36.397990+00:00"
    },
    {
      "arxiv_id": "2506.23314v1",
      "title": "Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance",
      "title_zh": "è®¾è®¡å³å…·å¯è§£é‡Šæ€§ï¼šMH-AutoML å®ç°å…¼é¡¾æ€§èƒ½çš„é€æ˜é«˜æ•ˆå®‰å“æ¶æ„è½¯ä»¶æ£€æµ‹",
      "authors": [
        "Joner Assolin",
        "Gabriel Canto",
        "Diego Kreutz",
        "Eduardo Feitosa",
        "Hendrio BraganÃ§a",
        "Angelo Nogueira",
        "Vanderson Rocha"
      ],
      "abstract": "Malware detection in Android systems requires both cybersecurity expertise and machine learning (ML) techniques. Automated Machine Learning (AutoML) has emerged as an approach to simplify ML development by reducing the need for specialized knowledge. However, current AutoML solutions typically operate as black-box systems with limited transparency, interpretability, and experiment traceability. To address these limitations, we present MH-AutoML, a domain-specific framework for Android malware detection. MH-AutoML automates the entire ML pipeline, including data preprocessing, feature engineering, algorithm selection, and hyperparameter tuning. The framework incorporates capabilities for interpretability, debugging, and experiment tracking that are often missing in general-purpose solutions. In this study, we compare MH-AutoML against seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT, HyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML achieves better recall rates while providing more transparency and control. The framework maintains computational efficiency comparable to other solutions, making it suitable for cybersecurity applications where both performance and explainability matter.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MH-AutoMLï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹ Android æ¶æ„è½¯ä»¶æ£€æµ‹é¢†åŸŸè®¾è®¡çš„è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹  (AutoML) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿç¼ºä¹é€æ˜åº¦å’Œè§£é‡Šæ€§çš„â€œé»‘ç›’â€é—®é¢˜ã€‚MH-AutoML å®ç°äº†ä»æ•°æ®é¢„å¤„ç† (Data Preprocessing)ã€ç‰¹å¾å·¥ç¨‹ (Feature Engineering) åˆ°ç®—æ³•é€‰æ‹©å’Œè¶…å‚æ•°è°ƒä¼˜ (Hyperparameter Tuning) çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚ä¸é€šç”¨å‹æ–¹æ¡ˆä¸åŒï¼Œè¯¥æ¡†æ¶é›†æˆäº†å¯è§£é‡Šæ€§ (Interpretability)ã€è°ƒè¯•å’Œå®éªŒè¿½è¸ªåŠŸèƒ½ï¼Œæ˜¾è‘—æå‡äº†æ£€æµ‹è¿‡ç¨‹çš„é€æ˜åº¦å’Œå¯æ§æ€§ã€‚åœ¨ä¸ Auto-Sklearnã€AutoGluon å’Œ TPOT ç­‰ä¸ƒç§ä¸»æµ AutoML æ¡†æ¶çš„å¯¹æ¯”å®éªŒä¸­ï¼ŒMH-AutoML å±•ç°äº†æ›´ä¼˜çš„å¬å›ç‡ (Recall Rates)ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒé«˜æ•ˆè®¡ç®—æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆå¹³è¡¡äº†æ¶æ„è½¯ä»¶æ£€æµ‹çš„ç²¾ç¡®åº¦ä¸æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä¸ºç½‘å®‰ (Cybersecurity) é¢†åŸŸçš„å®é™…åº”ç”¨æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "18 pages, 10 figures, 7 tabelas, paper submitted to JBCS",
      "pdf_url": "https://arxiv.org/pdf/2506.23314v1",
      "published_date": "2025-06-29 16:12:41 UTC",
      "updated_date": "2025-06-29 16:12:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:20:40.637542+00:00"
    },
    {
      "arxiv_id": "2506.23306v2",
      "title": "GATSim: Urban Mobility Simulation with Generative Agents",
      "title_zh": "GATSimï¼šåŸºäºç”Ÿæˆå¼æ™ºèƒ½ä½“çš„åŸå¸‚ç§»åŠ¨æ€§æ¨¡æ‹Ÿ",
      "authors": [
        "Qi Liu",
        "Can Li",
        "Wanjing Ma"
      ],
      "abstract": "Traditional agent-based urban mobility simulations often rely on rigid rule-based systems that struggle to capture the complexity, adaptability, and behavioral diversity inherent in human travel decision making. Recent advancements in large language models and AI agent technologies present new opportunities to develop agents with enhanced reasoning capabilities, persistent memory, and adaptive learning. We introduce GATSim (Generative-Agent Transport Simulation), a novel framework that leverages these advancements to simulate urban mobility using generative agents with rich, human-like behaviors. Unlike conventional approaches, GATSim agents are characterized by diverse socioeconomic profiles, individual lifestyles, and evolving preferences shaped through psychologically informed memory systems, tool usage, and lifelong learning. The main contributions of this work are: (1) a comprehensive architecture that integrates an urban mobility foundation model with agent cognitive systems and a transport simulation environment; (2) a hierarchical memory designed for efficient retrieval of contextually relevant information, incorporating spatial and temporal associations, keyword matching, and semantic relevance; (3) innovative planning and reactive mechanisms for modeling adaptive mobility behaviors which integrate a multi-scale reflection process to transform specific travel experiences into generalized behavioral insights. We implement a prototype system and conduct systematic validation, demonstrating that generative agents produce believable and coherent travel behaviors. Experimental results indicate that generative agents perform at least as well as human annotators with 92\\% posterior probability, while naturally producing realistic macroscopic traffic patterns. The code for the prototype implementation is publicly available at https://github.com/qiliuchn/gatsim.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GATSim (Generative-Agent Transport Simulation)ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ç”Ÿæˆå¼æ™ºèƒ½ä½“ (Generative Agents) æŠ€æœ¯æ„å»ºçš„æ–°å‹åŸå¸‚äº¤é€šæ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸåŸºäºè§„åˆ™çš„ç³»ç»Ÿéš¾ä»¥æ•æ‰äººç±»å‡ºè¡Œå†³ç­–å¤æ‚æ€§ä¸å¤šæ ·æ€§çš„é—®é¢˜ã€‚GATSim èµ‹äºˆæ™ºèƒ½ä½“ä¸°å¯Œçš„ç¤¾ä¼šç»æµèƒŒæ™¯ã€ä¸ªä½“ç”Ÿæ´»æ–¹å¼ä»¥åŠå—å¿ƒç†å­¦å¯å‘çš„è®°å¿†ç³»ç»Ÿ (Memory Systems)ï¼Œä½¿å…¶å…·å¤‡ç±»äººçš„æ¨ç†ã€å·¥å…·ä½¿ç”¨ä¸æŒç»­å­¦ä¹ èƒ½åŠ›ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬é›†æˆäº†åŸå¸‚äº¤é€šåŸºç¡€æ¨¡å‹ (Urban Mobility Foundation Model) çš„ç»¼åˆæ¶æ„ï¼Œä»¥åŠæ”¯æŒç©ºé—´ã€æ—¶é—´å…³è”æ£€ç´¢çš„å±‚çº§è®°å¿†ç»“æ„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†åŸºäºå¤šå°ºåº¦åæ€è¿‡ç¨‹ (Multi-scale Reflection Process) çš„è§„åˆ’ä¸ååº”æœºåˆ¶ï¼Œèƒ½å¤Ÿå°†å…·ä½“çš„å‡ºè¡Œä½“éªŒè½¬åŒ–ä¸ºå¹¿ä¹‰çš„è¡Œä¸ºè§è§£ï¼Œä»è€Œæ¨¡æ‹Ÿå‡ºé€‚åº”æ€§å¼ºçš„å‡ºè¡Œè¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆå¼æ™ºèƒ½ä½“äº§ç”Ÿçš„å‡ºè¡Œè¡Œä¸ºåœ¨å¯ä¿¡åº¦ä¸è¿è´¯æ€§ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…¶è¡¨ç°ä¸äººç±»æ ‡æ³¨è€…ç›¸å½“çš„åéªŒæ¦‚ç‡è¾¾ 92%ï¼Œå¹¶èƒ½è‡ªç„¶åœ°å‘ˆç°å‡ºé€¼çœŸçš„å®è§‚äº¤é€šæ¨¡å¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23306v2",
      "published_date": "2025-06-29 15:52:16 UTC",
      "updated_date": "2025-07-18 04:20:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:20:46.742824+00:00"
    },
    {
      "arxiv_id": "2506.23298v3",
      "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification",
      "title_zh": "æ­ç¤ºä¸ç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ ¡å‡†åå·®ä¸äººå£ç»Ÿè®¡å­¦ä¸å…¬å¹³æ€§",
      "authors": [
        "Xing Shen",
        "Justin Szeto",
        "Mingyang Li",
        "Hengguan Huang",
        "Tal Arbel"
      ],
      "abstract": "Multimodal large language models (MLLMs) have enormous potential to perform few-shot in-context learning in the context of medical image analysis. However, safe deployment of these models into real-world clinical practice requires an in-depth analysis of the accuracies of their predictions, and their associated calibration errors, particularly across different demographic subgroups. In this work, we present the first investigation into the calibration biases and demographic unfairness of MLLMs' predictions and confidence scores in few-shot in-context learning for medical image classification. We introduce CALIN, an inference-time calibration method designed to mitigate the associated biases. Specifically, CALIN estimates the amount of calibration needed, represented by calibration matrices, using a bi-level procedure: progressing from the population level to the subgroup level prior to inference. It then applies this estimation to calibrate the predicted confidence scores during inference. Experimental results on three medical imaging datasets: PAPILA for fundus image classification, HAM10000 for skin cancer classification, and MIMIC-CXR for chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair confidence calibration in its prediction, while improving its overall prediction accuracies and exhibiting minimum fairness-utility trade-off. Our codebase can be found at https://github.com/xingbpshen/medical-calibration-fairness-mllm.",
      "tldr_zh": "æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»çš„å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹  (Few-Shot In-Context Learning) ä¸­å­˜åœ¨çš„æ ¡å‡†åå·® (Calibration Biases) å’Œäººå£ç»Ÿè®¡å­¦ä¸å…¬å¹³æ€§ (Demographic Unfairness) é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›åå·®å¯èƒ½å½±å“ä¸´åºŠéƒ¨ç½²å®‰å…¨æ€§è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º CALIN çš„æ¨ç†æ—¶æ ¡å‡†æ–¹æ³•ï¼Œæ—¨åœ¨ç¼“è§£é¢„æµ‹ä¸ä¿¡å¿ƒå¾—åˆ†ä¹‹é—´çš„åå·®ã€‚CALIN é‡‡ç”¨åŒå±‚ç¨‹åº (Bi-level Procedure) æ¥ä¼°ç®—æ ¡å‡†çŸ©é˜µï¼Œå®ç°ä»æ€»ä½“æ°´å¹³åˆ°å­ç¾¤ä½“æ°´å¹³çš„é€’è¿›æ ¡å‡†ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå®æ—¶ä¿®æ­£æ¨¡å‹çš„ç½®ä¿¡åº¦ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ PAPILA çœ¼åº•å›¾åƒã€HAM10000 çš®è‚¤ç™Œå’Œ MIMIC-CXR èƒ¸éƒ¨ X å°„çº¿ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ç¡®ä¿å…¬å¹³ç½®ä¿¡åº¦æ ¡å‡†çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ•´ä½“é¢„æµ‹å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒCALIN åœ¨å…¬å¹³æ€§ä¸æ•ˆç”¨ (Fairness-Utility) ä¹‹é—´è¾¾åˆ°äº†æä½³çš„å¹³è¡¡ï¼Œä¸º MLLMs åœ¨çœŸå®ä¸´åºŠç¯å¢ƒä¸­çš„å¯é åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Preprint version. The peer-reviewed version of this paper has been accepted to MICCAI 2025 main conference",
      "pdf_url": "https://arxiv.org/pdf/2506.23298v3",
      "published_date": "2025-06-29 15:37:17 UTC",
      "updated_date": "2025-07-17 18:00:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:04.445809+00:00"
    },
    {
      "arxiv_id": "2506.23296v1",
      "title": "Securing AI Systems: A Guide to Known Attacks and Impacts",
      "title_zh": "äººå·¥æ™ºèƒ½ç³»ç»Ÿå®‰å…¨é˜²æŠ¤ï¼šå·²çŸ¥æ”»å‡»åŠå…¶å½±å“æŒ‡å—",
      "authors": [
        "Naoto Kiribuchi",
        "Kengo Zenitani",
        "Takayuki Semitsu"
      ],
      "abstract": "Embedded into information systems, artificial intelligence (AI) faces security threats that exploit AI-specific vulnerabilities. This paper provides an accessible overview of adversarial attacks unique to predictive and generative AI systems. We identify eleven major attack types and explicitly link attack techniques to their impacts -- including information leakage, system compromise, and resource exhaustion -- mapped to the confidentiality, integrity, and availability (CIA) security triad. We aim to equip researchers, developers, security practitioners, and policymakers, even those without specialized AI security expertise, with foundational knowledge to recognize AI-specific risks and implement effective defenses, thereby enhancing the overall security posture of AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åµŒå…¥ä¿¡æ¯ç³»ç»Ÿçš„äººå·¥æ™ºèƒ½(AI)ç³»ç»Ÿæ‰€é¢ä¸´çš„ç‰¹å®šå®‰å…¨å¨èƒï¼Œæä¾›äº†ä¸€ä»½å…³äºé¢„æµ‹æ€§å’Œç”Ÿæˆæ€§AIç³»ç»Ÿç‹¬æœ‰å¯¹æŠ—æ€§æ”»å‡»(adversarial attacks)çš„ç»¼è¿°æŒ‡å—ã€‚æ–‡ç« è¯†åˆ«äº†11ç§ä¸»è¦çš„æ”»å‡»ç±»å‹ï¼Œå¹¶å°†è¿™äº›æ”»å‡»æŠ€æœ¯ä¸å…¶äº§ç”Ÿçš„å½±å“ï¼ˆå¦‚ä¿¡æ¯æ³„éœ²ã€ç³»ç»Ÿå—æŸå’Œèµ„æºæ¯ç«­ï¼‰è¿›è¡Œäº†æ˜ç¡®å…³è”ã€‚ç ”ç©¶æ ¸å¿ƒåœ¨äºå°†è¿™äº›æŠ€æœ¯ä¸ä¼ ç»Ÿçš„æœºå¯†æ€§ã€å®Œæ•´æ€§å’Œå¯ç”¨æ€§(CIA)å®‰å…¨ä¸‰è¦ç´ æ¨¡å‹è¿›è¡Œäº†æ˜ å°„åˆ†æã€‚è¯¥æŒ‡å—æ—¨åœ¨ä¸ºå³ä½¿ä¸å…·å¤‡ä¸“ä¸šAIå®‰å…¨èƒŒæ™¯çš„ç ”ç©¶äººå‘˜ã€å¼€å‘äººå‘˜å’Œå†³ç­–è€…æä¾›åŸºç¡€çŸ¥è¯†ï¼ŒåŠ©åŠ›å…¶è¯†åˆ«AIç‰¹æœ‰çš„å®‰å…¨é£é™©ã€‚é€šè¿‡æŒ‡å¯¼ç›¸å…³äººå‘˜å®æ–½æœ‰æ•ˆçš„é˜²å¾¡æªæ–½ï¼Œè¯¥ç ”ç©¶ä¸ºå¢å¼ºAIç³»ç»Ÿçš„æ•´ä½“å®‰å…¨æ€åŠ¿æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "34 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.23296v1",
      "published_date": "2025-06-29 15:32:03 UTC",
      "updated_date": "2025-06-29 15:32:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:02.379729+00:00"
    },
    {
      "arxiv_id": "2506.23293v2",
      "title": "Self-Organizing Language",
      "title_zh": "è‡ªç»„ç»‡è¯­è¨€",
      "authors": [
        "P. Myles Eugenio",
        "Anthony Beavers"
      ],
      "abstract": "We introduce a novel paradigm of emergent local memory. It is a continuous-learning completely-parallel content-addressable memory encoding global order. It demonstrates how local constraints on uncoordinated learning can produce topologically protected memories realizing emergent symbolic order. It is therefore a neuro-symbolic bridge.\n  It further has the ability to produce human language without data, by exploiting its own self-organizing dynamics. It teaches us that words arise as a side-effect of emergent symbolic order, and that human language patterns at all structural levels reflect a universal mechanism of word formation (which is subregular). This work answers essential questions about the existence \\& origin of all the human language data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Self-Organizing Language çš„æ–°å‹æ¶Œç°å±€éƒ¨è®°å¿†ï¼ˆemergent local memoryï¼‰èŒƒå¼ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿç¼–ç å…¨å±€é¡ºåºçš„æŒç»­å­¦ä¹ ã€å®Œå…¨å¹¶è¡Œä¸”å†…å®¹å¯»å€çš„å­˜å‚¨æœºåˆ¶ã€‚è¯¥èŒƒå¼é€šè¿‡å¯¹éåè°ƒå­¦ä¹ æ–½åŠ å±€éƒ¨çº¦æŸï¼Œäº§ç”Ÿäº†å…·æœ‰æ‹“æ‰‘ä¿æŠ¤ç‰¹æ€§çš„è®°å¿†ï¼Œä»è€Œå®ç°äº†æ¶Œç°çš„ç¬¦å·é¡ºåºï¼ˆemergent symbolic orderï¼‰ï¼Œåœ¨ç¥ç»ä¸ç¬¦å·ï¼ˆneuro-symbolicï¼‰ä¹‹é—´æ¶èµ·äº†æ¡¥æ¢ã€‚è¯¥ç³»ç»Ÿå±•ç°äº†åˆ©ç”¨è‡ªèº«çš„è‡ªç»„ç»‡åŠ¨åŠ›å­¦ï¼ˆself-organizing dynamicsï¼‰åœ¨æ— éœ€æ•°æ®çš„æƒ…å†µä¸‹ç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œè¯æ±‡çš„äº§ç”Ÿæ˜¯ç¬¦å·é¡ºåºæ¶Œç°çš„å‰¯äº§å“ï¼Œä¸”äººç±»è¯­è¨€åœ¨æ‰€æœ‰ç»“æ„å±‚æ¬¡ä¸Šçš„æ¨¡å¼éƒ½åæ˜ äº†ä¸€ç§é€šç”¨çš„æ„è¯æœºåˆ¶ï¼ˆsubregularï¼‰ã€‚è¿™é¡¹å·¥ä½œä¸ºæ¢ç´¢äººç±»è¯­è¨€æ•°æ®çš„å­˜åœ¨ä¸èµ·æºæä¾›äº†æ ¸å¿ƒè§£ç­”ï¼Œè¯æ˜äº†å±€éƒ¨çº¦æŸå¯ä»¥äº§ç”Ÿå…·æœ‰å…¨å±€ç§©åºçš„å¤æ‚è¯­è¨€ç³»ç»Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.CL",
      "comment": "27 pages, 14 figures; Name changed from \"Objective-Free Local Learning and Emergent Language Structure in Thinking Machines\"",
      "pdf_url": "https://arxiv.org/pdf/2506.23293v2",
      "published_date": "2025-06-29 15:29:13 UTC",
      "updated_date": "2025-11-14 19:35:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:06.344819+00:00"
    },
    {
      "arxiv_id": "2506.23286v1",
      "title": "Not All Explanations for Deep Learning Phenomena Are Equally Valuable",
      "title_zh": "å¹¶éæ‰€æœ‰æ·±åº¦å­¦ä¹ ç°è±¡çš„è§£é‡Šéƒ½å…·æœ‰åŒç­‰ä»·å€¼",
      "authors": [
        "Alan Jeffares",
        "Mihaela van der Schaar"
      ],
      "abstract": "Developing a better understanding of surprising or counterintuitive phenomena has constituted a significant portion of deep learning research in recent years. These include double descent, grokking, and the lottery ticket hypothesis -- among many others. Works in this area often develop ad hoc hypotheses attempting to explain these observed phenomena on an isolated, case-by-case basis. This position paper asserts that, in many prominent cases, there is little evidence to suggest that these phenomena appear in real-world applications and these efforts may be inefficient in driving progress in the broader field. Consequently, we argue against viewing them as isolated puzzles that require bespoke resolutions or explanations. However, despite this, we suggest that deep learning phenomena do still offer research value by providing unique settings in which we can refine our broad explanatory theories of more general deep learning principles. This position is reinforced by analyzing the research outcomes of several prominent examples of these phenomena from the recent literature. We revisit the current norms in the research community in approaching these problems and propose practical recommendations for future research, aiming to ensure that progress on deep learning phenomena is well aligned with the ultimate pragmatic goal of progress in the broader field of deep learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ é¢†åŸŸä¸­è¯¸å¦‚ double descentã€grokking å’Œ lottery ticket hypothesis ç­‰ç°è±¡çš„è§£é‡Šä»·å€¼ï¼ŒæŒ‡å‡ºç›®å‰è®¸å¤šç ”ç©¶å€¾å‘äºé’ˆå¯¹è¿™äº›ç°è±¡å¼€å‘å­¤ç«‹ä¸”ç‰¹å®šçš„ ad hoc å‡è®¾ã€‚ä½œè€…æå‡ºåœ¨è®¸å¤šè‘—åæ¡ˆä¾‹ä¸­ï¼Œç¼ºä¹è¯æ®è¡¨æ˜è¿™äº›ç°è±¡åœ¨å®é™…åº”ç”¨ä¸­æ™®éå­˜åœ¨ï¼Œå› æ­¤å•çº¯ä¸ºå…¶å¯»æ‰¾å®šåˆ¶åŒ–è§£é‡Šå¯èƒ½æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥æœ‰æ•ˆæ¨åŠ¨æ•´ä¸ªé¢†åŸŸçš„å‘å±•ã€‚è®ºæ–‡å»ºè®®ä¸åº”å°†è¿™äº›ç°è±¡è§†ä¸ºå­¤ç«‹çš„éš¾é¢˜ï¼Œè€Œåº”å°†å…¶ä½œä¸ºå®Œå–„æ›´å…·é€šç”¨æ€§çš„ deep learning principles ç†è®ºçš„ç‹¬ç‰¹è®¾ç½®ã€‚é€šè¿‡å¯¹è¿‘æœŸæ–‡çŒ®ä¸­çªå‡ºæ¡ˆä¾‹çš„ç ”ç©¶æˆæœè¿›è¡Œåˆ†æï¼Œä½œè€…å¼ºåŒ–äº†è¿™ä¸€ç«‹åœºå¹¶é‡æ–°å®¡è§†äº†å½“å‰çš„ç ”ç©¶è§„èŒƒã€‚æœ€åï¼Œè¯¥è®ºæ–‡ä¸ºæœªæ¥çš„ç ”ç©¶æå‡ºäº†å®è·µå»ºè®®ï¼Œæ—¨åœ¨ç¡®ä¿å¯¹ç‰¹å®šç°è±¡çš„ç ”ç©¶ä¸æ¨åŠ¨æ•´ä¸ªæ·±åº¦å­¦ä¹ é¢†åŸŸè¿›æ­¥çš„åŠ¡å®ç›®æ ‡ç›¸ä¸€è‡´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICML 2025 for oral presentation",
      "pdf_url": "https://arxiv.org/pdf/2506.23286v1",
      "published_date": "2025-06-29 15:18:56 UTC",
      "updated_date": "2025-06-29 15:18:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:21.195919+00:00"
    },
    {
      "arxiv_id": "2506.23276v2",
      "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games",
      "title_zh": "æ¨ç†ä¹‹å›°ï¼šæ¨ç†è¯­è¨€æ¨¡å‹åœ¨å…¬å…±ç‰©å“åšå¼ˆä¸­æ²¦ä¸ºæ­ä¾¿è½¦è€…",
      "authors": [
        "David Guzman Piedrahita",
        "Yongjin Yang",
        "Mrinmaya Sachan",
        "Giorgia Ramponi",
        "Bernhard SchÃ¶lkopf",
        "Zhijing Jin"
      ],
      "abstract": "As large language models (LLMs) are increasingly deployed as autonomous agents, understanding their cooperation and social mechanisms is becoming increasingly important. In particular, how LLMs balance self-interest and collective well-being is a critical challenge for ensuring alignment, robustness, and safe deployment. In this paper, we examine the challenge of costly sanctioning in multi-agent LLM systems, where an agent must decide whether to invest its own resources to incentivize cooperation or penalize defection. To study this, we adapt a public goods game with institutional choice from behavioral economics, allowing us to observe how different LLMs navigate social dilemmas over repeated interactions. Our analysis reveals four distinct behavioral patterns among models: some consistently establish and sustain high levels of cooperation, others fluctuate between engagement and disengagement, some gradually decline in cooperative behavior over time, and others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we find that reasoning LLMs, such as the o1 series, struggle significantly with cooperation, whereas some traditional LLMs consistently achieve high levels of cooperation. These findings suggest that the current approach to improving LLMs, which focuses on enhancing their reasoning capabilities, does not necessarily lead to cooperation, providing valuable insights for deploying LLM agents in environments that require sustained collaboration. Our code is available at https://github.com/davidguzmanp/SanctSim",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åˆä½œä¸ç¤¾ä¼šæœºåˆ¶ï¼Œç‰¹åˆ«å…³æ³¨å…¶åœ¨å…¬å…±ç‰©å“åšå¼ˆï¼ˆPublic Goods Gamesï¼‰ä¸­å¦‚ä½•å¹³è¡¡ä¸ªäººåˆ©ç›Šä¸é›†ä½“ç¦ç¥‰ã€‚ä½œè€…é€šè¿‡å¼•å…¥è¡Œä¸ºç»æµå­¦ä¸­çš„åˆ¶åº¦é€‰æ‹©ï¼ˆInstitutional Choiceï¼‰æ¡†æ¶ï¼Œåœ¨é‡å¤äº¤äº’ä¸­è§‚å¯ŸLLMsåœ¨é¢å¯¹ä»£ä»·é«˜æ˜‚çš„åˆ¶è£ï¼ˆCostly Sanctioningï¼‰æ—¶çš„å†³ç­–è¡Œä¸ºã€‚åˆ†æè¯†åˆ«å‡ºå››ç§å…¸å‹çš„è¡Œä¸ºæ¨¡å¼ï¼Œæ¶µç›–äº†ä»æŒç»­é«˜æ°´å¹³åˆä½œåˆ°åƒµåŒ–æ‰§è¡Œå›ºå®šç­–ç•¥çš„ä¸åŒè¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œo1ç³»åˆ—ç­‰å…·æœ‰å¼ºåŒ–æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼ˆReasoning LLMsï¼‰åœ¨åˆä½œæ–¹é¢è¡¨ç°ä¸ä½³ï¼Œåè€Œå€¾å‘äºæˆä¸ºâ€œæ­ä¾¿è½¦è€…â€ï¼ˆFree-Ridersï¼‰ï¼Œè€Œéƒ¨åˆ†ä¼ ç»Ÿæ¨¡å‹å´èƒ½ç»´æŒé«˜åº¦åˆä½œã€‚è¿™ä¸€ç»“æœæ­ç¤ºäº†æå‡LLMsçš„æ¨ç†èƒ½åŠ›ï¼ˆReasoning Capabilitiesï¼‰å¹¶ä¸èƒ½è‡ªåŠ¨è½¬åŒ–ä¸ºåˆä½œå€¾å‘ï¼Œä¸ºåœ¨åä½œå¯†é›†å‹ç¯å¢ƒä¸­éƒ¨ç½²å®‰å…¨ã€å¯¹é½çš„æ™ºèƒ½ä½“æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at COLM 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.23276v2",
      "published_date": "2025-06-29 15:02:47 UTC",
      "updated_date": "2025-07-24 13:13:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:17.752611+00:00"
    },
    {
      "arxiv_id": "2506.23275v2",
      "title": "Why Settle for One? Text-to-ImageSet Generation and Evaluation",
      "title_zh": "å²‚èƒ½æ»¡è¶³äºå•å¼ ï¼Ÿæ–‡æœ¬åˆ°å›¾åƒé›†ç”Ÿæˆä¸è¯„ä¼°",
      "authors": [
        "Chengyou Jia",
        "Xin Shen",
        "Zhuohang Dang",
        "Zhuohang Dang",
        "Changliang Xia",
        "Weijia Wu",
        "Xinyu Zhang",
        "Hangwei Qian",
        "Ivor W. Tsang",
        "Minnan Luo"
      ],
      "abstract": "Despite remarkable progress in Text-to-Image models, many real-world applications require generating coherent image sets with diverse consistency requirements. Existing consistent methods often focus on a specific domain with specific aspects of consistency, which significantly constrains their generalizability to broader applications. In this paper, we propose a more challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate sets of images that meet various consistency requirements based on user instructions. To systematically study this problem, we first introduce $\\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories, providing comprehensive coverage for T2IS generation. Building on this, we propose $\\textbf{T2IS-Eval}$, an evaluation framework that transforms user instructions into multifaceted assessment criteria and employs effective evaluators to adaptively assess consistency fulfillment between criteria and generated sets. Subsequently, we propose $\\textbf{AutoT2IS}$, a training-free framework that maximally leverages pretrained Diffusion Transformers' in-context capabilities to harmonize visual elements to satisfy both image-level prompt alignment and set-level visual consistency. Extensive experiments on T2IS-Bench reveal that diverse consistency challenges all existing methods, while our AutoT2IS significantly outperforms current generalized and even specialized approaches. Our method also demonstrates the ability to enable numerous underexplored real-world applications, confirming its substantial practical value. Visit our project in https://chengyou-jia.github.io/T2IS-Home.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-Image)æ¨¡å‹åœ¨ç”Ÿæˆå…·æœ‰å¤šæ ·ä¸€è‡´æ€§è¦æ±‚çš„å›¾åƒé›†æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†æ–‡æœ¬ç”Ÿæˆå›¾åƒé›†(Text-to-ImageSet, T2IS)è¿™ä¸€æ›´å…·æŒ‘æˆ˜æ€§çš„æ–°ä»»åŠ¡ã€‚ä¸ºäº†ç³»ç»ŸåŒ–è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…é¦–å…ˆæ¨å‡ºäº†åŒ…å«596æ¡å¤šæ ·åŒ–æŒ‡ä»¤çš„åŸºå‡†æµ‹è¯•é›†T2IS-Benchï¼Œå¹¶æ„å»ºäº†ç›¸åº”çš„è¯„ä¼°æ¡†æ¶T2IS-Evalï¼Œé€šè¿‡å°†ç”¨æˆ·æŒ‡ä»¤è½¬åŒ–ä¸ºå¤šç»´æ ‡å‡†æ¥è¯„ä¼°å›¾åƒé›†çš„ä¸€è‡´æ€§ã€‚éšåï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAutoT2ISçš„æ— éœ€è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ€å¤§åŒ–åˆ©ç”¨äº†é¢„è®­ç»ƒæ‰©æ•£Transformer(Diffusion Transformers)çš„ä¸Šä¸‹æ–‡èƒ½åŠ›(in-context capabilities)ï¼Œæœ‰æ•ˆåè°ƒè§†è§‰å…ƒç´ ä»¥æ»¡è¶³å›¾åƒçº§çš„æç¤ºè¯å¯¹é½å’Œé›†åˆçº§çš„è§†è§‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒAutoT2ISåœ¨å¤„ç†å¤šæ ·åŒ–ä¸€è‡´æ€§æŒ‘æˆ˜æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„é€šç”¨å’Œä¸“ç”¨æ¨¡å‹ï¼Œåœ¨å¤šä¸ªçœŸå®åº”ç”¨åœºæ™¯ä¸­å±•ç°å‡ºæé«˜çš„å®è·µä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23275v2",
      "published_date": "2025-06-29 15:01:16 UTC",
      "updated_date": "2025-09-25 07:49:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:23.627713+00:00"
    },
    {
      "arxiv_id": "2506.23274v3",
      "title": "Real-Time Progress Prediction in Reasoning Language Models",
      "title_zh": "æ¨ç†è¯­è¨€æ¨¡å‹å®æ—¶è¿›åº¦é¢„æµ‹",
      "authors": [
        "Hans Peter LynsgÃ¸e Raaschou-jensen",
        "Constanza Fierro",
        "Anders SÃ¸gaard"
      ],
      "abstract": "Recent advances in reasoning language models -- particularly those that use long, latent chains of thought -- have demonstrated remarkable capabilities in complex, agentic tasks. However, as these models operate over increasingly extended time horizons, their internal progress becomes opaque to users, complicating expectation management and real-time oversight. In this work, we investigate whether real-time progress prediction is feasible. We discretize progress and train a linear probe to classify reasoning states. We then introduce a two-stage fine-tuning approach that enables reasoning models to generate progress estimates (0$\\rightarrow$100\\%) during inference. Our best fine-tuned model achieves an average error of 10\\% for sequences less than 16,000 tokens, offering a practical mechanism for monitoring and interpreting model reasoning in real time.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨ç†è¯­è¨€æ¨¡å‹(Reasoning Language Models)åœ¨ä½¿ç”¨é•¿é“¾ã€æ½œåœ¨æ€ç»´é“¾(latent chains of thought)å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å†…éƒ¨è¿›åº¦ä¸é€æ˜çš„é—®é¢˜ï¼Œæ¢è®¨äº†å®ç°å®æ—¶è¿›åº¦é¢„æµ‹çš„å¯è¡Œæ€§ã€‚ç ”ç©¶è€…é¦–å…ˆé€šè¿‡å°†è¿›åº¦ç¦»æ•£åŒ–å¹¶è®­ç»ƒçº¿æ€§æ¢æµ‹(linear probe)æ¥å¯¹æ¨ç†çŠ¶æ€è¿›è¡Œåˆ†ç±»ï¼Œéšåæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µå¾®è°ƒ(two-stage fine-tuning)æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆ0%åˆ°100%çš„è¿›åº¦ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ€§èƒ½æœ€ä½³çš„å¾®è°ƒæ¨¡å‹åœ¨é•¿åº¦å°‘äº16,000ä¸ªtokençš„åºåˆ—ä¸­å¹³å‡è¯¯å·®ä»…ä¸º10%ã€‚è¯¥ç ”ç©¶ä¸ºå®æ—¶ç›‘æ§å’Œè§£é‡Šå¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ¨¡å‹è¡Œä¸ºæä¾›äº†ä¸€ç§å®ç”¨çš„æœºåˆ¶ï¼Œæœ‰æ•ˆæ”¹å–„äº†ç”¨æˆ·å¯¹é•¿æ—¶ç¨‹æ¨ç†ä»»åŠ¡çš„é¢„æœŸç®¡ç†ä¸ç›‘ç£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23274v3",
      "published_date": "2025-06-29 15:01:01 UTC",
      "updated_date": "2025-10-08 12:11:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:20.009424+00:00"
    },
    {
      "arxiv_id": "2506.23273v2",
      "title": "FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis",
      "title_zh": "FinStat2SQLï¼šé¢å‘è´¢åŠ¡æŠ¥è¡¨åˆ†æçš„ Text-to-SQL æµæ°´çº¿",
      "authors": [
        "Quang Hung Nguyen",
        "Phuong Anh Trinh",
        "Phan Quoc Hung Mai",
        "Tuan Phong Trinh"
      ],
      "abstract": "Despite the advancements of large language models, text2sql still faces many challenges, particularly with complex and domain-specific queries. In finance, database designs and financial reporting layouts vary widely between financial entities and countries, making text2sql even more challenging. We present FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries over financial statements. Tailored to local standards like VAS, it combines large and small language models in a multi-agent setup for entity extraction, SQL generation, and self-correction. We build a domain-specific database and evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves 61.33\\% accuracy with sub-4-second response times on consumer hardware, outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient solution for financial analysis, making AI-powered querying accessible to Vietnamese enterprises.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FinStat2SQLï¼Œä¸€ç§é’ˆå¯¹è´¢åŠ¡æŠ¥è¡¨åˆ†æè®¾è®¡çš„è½»é‡çº§ text2sql æµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³è´¢åŠ¡é¢†åŸŸæ•°æ®åº“è®¾è®¡å¤æ‚åŠå‡†åˆ™å·®å¼‚ï¼ˆå¦‚ VASï¼‰å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¤šæ™ºèƒ½ä½“ (multi-agent) ç³»ç»Ÿï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹ä¸å°è¯­è¨€æ¨¡å‹ï¼ŒååŒå®Œæˆå®ä½“æå–ã€SQL ç”ŸæˆåŠè‡ªæˆ‘çº é”™ä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†é¢†åŸŸç‰¹å®šçš„æ•°æ®åº“åŠåˆæˆé—®ç­” (QA) æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„ 7B æ¨¡å‹åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šå®ç°äº† 61.33% çš„å‡†ç¡®ç‡ï¼Œä¸”å“åº”æ—¶é—´ä½äº 4 ç§’ï¼Œå…¶æ€§èƒ½è¶…è¶Šäº† GPT-4o-miniã€‚FinStat2SQL ä¸ºè´¢åŠ¡åˆ†ææä¾›äº†ä¸€ç§é«˜æ‰©å±•æ€§ä¸”ä½æˆæœ¬çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº† AI é©±åŠ¨çš„æŸ¥è¯¢æŠ€æœ¯åœ¨ä¼ä¸šè´¢åŠ¡åœºæ™¯ä¸­çš„å¯è½åœ°æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for The 18th International Natural Language Generation Conference (INLG)",
      "pdf_url": "https://arxiv.org/pdf/2506.23273v2",
      "published_date": "2025-06-29 14:55:21 UTC",
      "updated_date": "2025-09-07 17:52:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:25.804910+00:00"
    },
    {
      "arxiv_id": "2507.02952v1",
      "title": "Strategies for Resource Allocation of Two Competing Companies using Genetic Algorithm",
      "title_zh": "åŸºäºé—ä¼ ç®—æ³•çš„ä¸¤å®¶ç«äº‰å…¬å¸èµ„æºåˆ†é…ç­–ç•¥",
      "authors": [
        "Wing Keung Cheung",
        "Kwok Yip Szeto"
      ],
      "abstract": "We investigate various strategic locations of shops in shopping malls in a metropolis with the aim of finding the best strategy for final dominance of market share by a company in a competing environment. The problem is posed in the context of two competing supermarket chains in a metropolis, described in the framework of the two-dimensional Ising model. Evolutionary Algorithm is used to encode the ensemble of initial configurations and Monte Carlo method is used to evolve the pattern. Numerical simulation indicates that initial patterns with certain topological properties do evolve faster to market dominance. The description of these topological properties is given and suggestions are made on the initial pattern so as to evolve faster to market dominance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§éƒ½å¸‚ç«äº‰ç¯å¢ƒä¸‹ï¼Œä¸¤å®¶ç«äº‰å…¬å¸ï¼ˆä»¥è¶…å¸‚è¿é”ä¸ºä¾‹ï¼‰ä¸ºå æ®å¸‚åœºä»½é¢ä¼˜åŠ¿è€Œé‡‡å–çš„é—¨åº—é€‰å€ç­–ç•¥ã€‚ç ”ç©¶é‡‡ç”¨äº†äºŒç»´ Ising model ä½œä¸ºç†è®ºæ¡†æ¶ï¼Œå¹¶åˆ©ç”¨ Genetic Algorithm å¯¹åˆå§‹å¸ƒå±€é…ç½®è¿›è¡Œç¼–ç ã€‚éšåï¼Œç ”ç©¶åº”ç”¨ Monte Carlo æ–¹æ³•æ¨¡æ‹Ÿå¹¶æ¼”åŒ–å¸‚åœºæ ¼å±€ã€‚æ•°å€¼æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼Œå…·æœ‰ç‰¹å®š topological properties çš„åˆå§‹å¸ƒå±€èƒ½å¤Ÿæ›´å¿«é€Ÿåœ°å®ç°å¸‚åœºä¸»å¯¼åœ°ä½ã€‚è¯¥è®ºæ–‡è¯¦ç»†æè¿°äº†è¿™äº›æ‹“æ‰‘ç‰¹å¾çš„å…·ä½“è¡¨ç°ï¼Œå¹¶æ®æ­¤å¯¹ä¼ä¸šçš„åˆå§‹å¸ƒå±€è®¾è®¡æå‡ºäº†æ”¹è¿›å»ºè®®ï¼Œæ—¨åœ¨åŠ é€Ÿå…¶åœ¨ç«äº‰ä¸­çš„å¸‚åœºæ‰©å¼ ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.02952v1",
      "published_date": "2025-06-29 14:52:35 UTC",
      "updated_date": "2025-06-29 14:52:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:28.305108+00:00"
    },
    {
      "arxiv_id": "2506.23270v1",
      "title": "Token Activation Map to Visually Explain Multimodal LLMs",
      "title_zh": "Token æ¿€æ´»å›¾ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¯è§†åŒ–è§£é‡Š",
      "authors": [
        "Yi Li",
        "Hualiang Wang",
        "Xinpeng Ding",
        "Haonan Wang",
        "Xiaomeng Li"
      ],
      "abstract": "Multimodal large language models (MLLMs) are broadly empowering various fields. Despite their advancements, the explainability of MLLMs remains less explored, hindering deeper understanding, model credibility, and effective visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that produce a single output, MLLMs generate sequences of tokens progressively, where each generated token depends on the previous context. Therefore, earlier context tokens can introduce redundant activations that interfere with the explanation of later tokens beyond their original information. Existing studies often overlook this issue, but our observations reveal that these redundant correlations can significantly hurt the reliability of explanations. To address this, we propose an estimated causal inference method to mitigate the interference of context to achieve high-quality MLLM explanation, with a novel rank Gaussian filter to further reduce activation noises. We term this method Token Activation Map (TAM) to highlight the consideration of interactions between tokens. TAM also indicates that it excels at explaining multiple tokens of MLLM, which is different from the Class Activation Map (CAM) for a single prediction. Our TAM method significantly outperforms existing SoTA methods, showcasing high-quality visualization results that can be utilized for various scenarios, such as object localization, failure case analysis, video visualization, MLLMs visual comparison, and model understanding (e.g., color, shape, action, location, visual reasoning, multi-turn conversation, etc). The code is available atgithub.com/xmed-lab/TAM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Token Activation Map (TAM)ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) çš„å¯è§£é‡Šæ€§ä¸å¯è§†åŒ–è´¨é‡ã€‚é’ˆå¯¹MLLMsé€ä¸ªç”Ÿæˆtokenæ—¶ï¼Œå‰æœŸä¸Šä¸‹æ–‡tokenä¼šäº§ç”Ÿå†—ä½™æ¿€æ´»å¹¶å¹²æ‰°åç»­è§£é‡Šå¯é æ€§çš„é—®é¢˜ï¼ŒTAM å¼•å…¥äº†ä¼°è®¡å› æœæ¨ç† (estimated causal inference) æ–¹æ³•æ¥æ¶ˆé™¤ä¸Šä¸‹æ–‡å¹²æ‰°ï¼Œå¹¶ç»“åˆæ–°å‹çš„ç§©é«˜æ–¯æ»¤æ³¢å™¨ (rank Gaussian filter) è¿›ä¸€æ­¥é™ä½æ¿€æ´»å™ªå£°ã€‚ä¸ä¼ ç»Ÿé’ˆå¯¹å•ä¸€é¢„æµ‹çš„ç±»æ¿€æ´»æ˜ å°„ (CAM) ä¸åŒï¼ŒTAM èƒ½å¤Ÿé€šè¿‡è€ƒè™‘ token é—´çš„äº¤äº’æ¥åŒæ—¶è§£é‡Šå¤šä¸ªç”Ÿæˆçš„ tokenã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAM åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰ SOTA æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å¯è§†åŒ–ç»“æœã€‚è¯¥æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºç›®æ ‡å®šä½ã€å¤±æ•ˆæ¡ˆä¾‹åˆ†æã€è§†é¢‘å¯è§†åŒ–ä»¥åŠæ·±å…¥ç†è§£æ¨¡å‹åœ¨è§†è§‰æ¨ç†å’Œå¤šè½®å¯¹è¯ä¸­çš„è¡¨ç°ï¼Œä¸ºå¢å¼º MLLMs çš„å¯ä¿¡åº¦æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV2025 Accepted",
      "pdf_url": "https://arxiv.org/pdf/2506.23270v1",
      "published_date": "2025-06-29 14:50:45 UTC",
      "updated_date": "2025-06-29 14:50:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:32.594781+00:00"
    },
    {
      "arxiv_id": "2506.23260v2",
      "title": "From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows",
      "title_zh": "ä»æç¤ºæ³¨å…¥åˆ°åè®®åˆ©ç”¨ï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ AI æ™ºèƒ½ä½“å·¥ä½œæµä¸­çš„å¨èƒ",
      "authors": [
        "Mohamed Amine Ferrag",
        "Norbert Tihanyi",
        "Djallel Hamouda",
        "Leandros Maglaras",
        "Abderrahmane Lakas",
        "Merouane Debbah"
      ],
      "abstract": "Autonomous AI agents powered by large language models (LLMs) with structured function-calling interfaces enable real-time data retrieval, computation, and multi-step orchestration. However, the rapid growth of plugins, connectors, and inter-agent protocols has outpaced security practices, leading to brittle integrations that rely on ad-hoc authentication, inconsistent schemas, and weak validation. This survey introduces a unified end-to-end threat model for LLM-agent ecosystems, covering host-to-tool and agent-to-agent communications. We systematically categorize more than thirty attack techniques spanning input manipulation, model compromise, system and privacy attacks, and protocol-level vulnerabilities. For each category, we provide a formal threat formulation defining attacker capabilities, objectives, and affected system layers. Representative examples include Prompt-to-SQL injections and the Toxic Agent Flow exploit in GitHub MCP servers. We analyze attack feasibility, review existing defenses, and discuss mitigation strategies such as dynamic trust management, cryptographic provenance tracking, and sandboxed agent interfaces. The framework is validated through expert review and cross-mapping with real-world incidents and public vulnerability repositories, including CVE and NIST NVD. Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”±å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„è‡ªä¸»AIæ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿä¸­æ—¥ç›Šå¢é•¿çš„å®‰å…¨é£é™©ï¼Œæå‡ºäº†ä¸€å¥—ç»Ÿä¸€çš„ç«¯åˆ°ç«¯å¨èƒæ¨¡å‹ã€‚ç”±äºæ’ä»¶ã€è¿æ¥å™¨å’Œè·¨æ™ºèƒ½ä½“åè®®çš„å¿«é€Ÿå¢é•¿è¶…è¿‡äº†å®‰å…¨å®è·µçš„å‘å±•ï¼Œå¯¼è‡´é›†æˆç¯èŠ‚æ™®éå­˜åœ¨èº«ä»½éªŒè¯å’Œæ ¡éªŒè–„å¼±çš„é—®é¢˜ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°åˆ†ç±»äº†è¶…è¿‡ä¸‰åç§æ”»å‡»æŠ€æœ¯ï¼Œæ¶µç›–äº†ä»è¾“å…¥æ“çºµ(Input Manipulation)ã€æ¨¡å‹å—æŸåˆ°åè®®å±‚æ¼æ´(Protocol-level Vulnerabilities)ç­‰å¤šä¸ªç»´åº¦ã€‚ç ”ç©¶è¿˜é€šè¿‡Formal Threat Formulationå®šä¹‰äº†æ”»å‡»è€…çš„èƒ½åŠ›ä¸ç›®æ ‡ï¼Œå¹¶è¯¦ç»†åˆ†æäº†åŒ…æ‹¬Prompt-to-SQLæ³¨å…¥å’ŒToxic Agent Flowåœ¨å†…çš„å…¸å‹æ¡ˆä¾‹ã€‚åŒæ—¶ï¼Œè¯¥å·¥ä½œè¯„ä¼°äº†é˜²å¾¡å¯è¡Œæ€§ï¼Œæ¢è®¨äº†åŠ¨æ€ä¿¡ä»»ç®¡ç†(Dynamic Trust Management)å’Œæ²™ç®±åŒ–æ™ºèƒ½ä½“æ¥å£(Sandboxed Agent Interfaces)ç­‰ç¼“è§£ç­–ç•¥ã€‚é€šè¿‡ä¸CVEå’ŒNIST NVDç­‰çœŸå®æ¼æ´åº“çš„äº¤å‰æ¯”å¯¹ï¼Œè¯¥æ¡†æ¶ä¸ºæ„å»ºå®‰å…¨ä¸”å…·éŸ§æ€§çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†é¦–ä¸ªæ•´åˆäº†è¾“å…¥çº§åˆ©ç”¨ä¸åè®®å±‚æ¼æ´çš„åˆ†ç±»å­¦æŒ‡å—ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "The paper is published in ICT Express (Elsevier)",
      "pdf_url": "https://arxiv.org/pdf/2506.23260v2",
      "published_date": "2025-06-29 14:32:32 UTC",
      "updated_date": "2025-12-14 20:29:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:44.780112+00:00"
    },
    {
      "arxiv_id": "2506.23254v1",
      "title": "PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution",
      "title_zh": "PixelBoostï¼šåˆ©ç”¨å¸ƒæœ—è¿åŠ¨å®ç°é€¼çœŸå›¾åƒè¶…åˆ†è¾¨ç‡",
      "authors": [
        "Aradhana Mishra",
        "Bumshik Lee"
      ],
      "abstract": "Diffusion-model-based image super-resolution techniques often face a trade-off between realistic image generation and computational efficiency. This issue is exacerbated when inference times by decreasing sampling steps, resulting in less realistic and hazy images. To overcome this challenge, we introduce a novel diffusion model named PixelBoost that underscores the significance of embracing the stochastic nature of Brownian motion in advancing image super-resolution, resulting in a high degree of realism, particularly focusing on texture and edge definitions. By integrating controlled stochasticity into the training regimen, our proposed model avoids convergence to local optima, effectively capturing and reproducing the inherent uncertainty of image textures and patterns. Our proposed model demonstrates superior objective results in terms of learned perceptual image patch similarity (LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR), structural similarity index measure (SSIM), as well as visual quality. To determine the edge enhancement, we evaluated the gradient magnitude and pixel value, and our proposed model exhibited a better edge reconstruction capability. Additionally, our model demonstrates adaptive learning capabilities by effectively adjusting to Brownian noise patterns and introduces a sigmoidal noise sequencing method that simplifies training, resulting in faster inference speeds.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PixelBoostï¼Œä¸€ç§æ—¨åœ¨è§£å†³åŸºäºæ‰©æ•£æ¨¡å‹(Diffusion-model)çš„å›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯åœ¨é€¼çœŸåº¦ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´æƒè¡¡é—®é¢˜çš„åˆ›æ–°æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨å¸ƒæœ—è¿åŠ¨(Brownian motion)çš„éšæœºæ€§ï¼Œåœ¨è®­ç»ƒæ–¹æ¡ˆä¸­å¼•å…¥å—æ§çš„éšæœºæ€§ä»¥é¿å…æ”¶æ•›è‡³å±€éƒ¨æœ€ä¼˜ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰å¹¶å†ç°å›¾åƒçº¹ç†å’Œæ¨¡å¼çš„å›ºæœ‰ä¸ç¡®å®šæ€§ã€‚ä¸ºäº†æå‡æ•ˆç‡ï¼Œæ¨¡å‹å¼•å…¥äº†S-å‹å™ªå£°æ’åºæ–¹æ³•(sigmoidal noise sequencing method)ï¼Œåœ¨ç®€åŒ–è®­ç»ƒçš„åŒæ—¶å®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPixelBooståœ¨LPIPSã€LOEã€PSNRå’ŒSSIMç­‰å®¢è§‚æŒ‡æ ‡ä»¥åŠè§†è§‰è´¨é‡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨è¾¹ç¼˜é‡å»ºå’Œçº¹ç†å®šä¹‰æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å±•ç¤ºäº†å¯¹å¸ƒæœ—å™ªå£°æ¨¡å¼(Brownian noise patterns)çš„å¼ºè‡ªé€‚åº”å­¦ä¹ èƒ½åŠ›ï¼Œä¸ºå®ç°é«˜è´¨é‡ã€é«˜æ•ˆç‡çš„çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23254v1",
      "published_date": "2025-06-29 14:22:38 UTC",
      "updated_date": "2025-06-29 14:22:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:53.234391+00:00"
    },
    {
      "arxiv_id": "2507.00079v1",
      "title": "VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems",
      "title_zh": "VoyagerVisionï¼šæ¢ç©¶å¤šæ¨¡æ€ä¿¡æ¯åœ¨å¼€æ”¾å¼å­¦ä¹ ç³»ç»Ÿä¸­çš„ä½œç”¨",
      "authors": [
        "Ethan Smyth",
        "Alessandro Suglia"
      ],
      "abstract": "Open-endedness is an active field of research in the pursuit of capable Artificial General Intelligence (AGI), allowing models to pursue tasks of their own choosing. Simultaneously, recent advancements in Large Language Models (LLMs) such as GPT-4o [9] have allowed such models to be capable of interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use of such features, providing an LLM with pixel data of an agent's POV to parse the environment and allow it to solve tasks. This paper proposes that providing these visual inputs to a model gives it greater ability to interpret spatial environments, and as such, can increase the number of tasks it can successfully perform, extending its open-ended potential. To this aim, this paper proposes VoyagerVision -- a multi-modal model capable of creating structures within Minecraft using screenshots as a form of visual feedback, building on the foundation of Voyager. VoyagerVision was capable of creating an average of 2.75 unique structures within fifty iterations of the system, as Voyager was incapable of this, it is an extension in an entirely new direction. Additionally, in a set of building unit tests VoyagerVision was successful in half of all attempts in flat worlds, with most failures arising in more complex structures. Project website is available at https://esmyth-dev.github.io/VoyagerVision.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VoyagerVisionï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Voyager æ„å»ºçš„å¤šæ¨¡æ€æ¨¡å‹ (multi-modal model)ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥è§†è§‰ä¿¡æ¯æ¥å¢å¼ºå¼€æ”¾å¼å­¦ä¹ ç³»ç»Ÿ (open-ended learning systems) çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) è§£æ Minecraft çš„å±å¹•æˆªå›¾ä½œä¸ºè§†è§‰åé¦ˆ (visual feedback)ï¼Œä»è€Œæå‡æ¨¡å‹å¯¹ç©ºé—´ç¯å¢ƒçš„ç†è§£å¹¶æ‰©å±•å…¶æ‰§è¡Œä»»åŠ¡çš„æ½œåŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVoyagerVision åœ¨äº”åæ¬¡è¿­ä»£ä¸­å¹³å‡èƒ½åˆ›å»º 2.75 ä¸ªç‹¬ç‰¹çš„å»ºç­‘ç»“æ„ï¼Œè€ŒåŸå§‹çš„ Voyager åˆ™æ— æ³•å®Œæˆæ­¤ç±»ä»»åŠ¡ã€‚åœ¨å»ºç­‘å•å…ƒæµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨å¹³å¦ä¸–ç•Œä¸­çš„æˆåŠŸç‡è¾¾åˆ°äº† 50%ï¼Œå°½ç®¡åœ¨å¤„ç†å¤æ‚ç»“æ„æ—¶ä»é¢ä¸´ä¸€å®šæŒ‘æˆ˜ã€‚è¯¥é¡¹å·¥ä½œè¯æ˜äº†å¤šæ¨¡æ€ä¿¡æ¯åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½ (AGI) è¿‡ç¨‹ä¸­çš„é‡è¦ä½œç”¨ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "website: https://esmyth-dev.github.io/VoyagerVision.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2507.00079v1",
      "published_date": "2025-06-29 14:16:11 UTC",
      "updated_date": "2025-06-29 14:16:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:49.848887+00:00"
    },
    {
      "arxiv_id": "2506.23247v1",
      "title": "Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification",
      "title_zh": "èšåˆå±€éƒ¨æ˜¾è‘—æ€§å›¾ä»¥å®ç°åŠå…¨å±€å¯è§£é‡Šå›¾åƒåˆ†ç±»",
      "authors": [
        "James Hinns",
        "David Martens"
      ],
      "abstract": "Deep learning dominates image classification tasks, yet understanding how models arrive at predictions remains a challenge. Much research focuses on local explanations of individual predictions, such as saliency maps, which visualise the influence of specific pixels on a model's prediction. However, reviewing many of these explanations to identify recurring patterns is infeasible, while global methods often oversimplify and miss important local behaviours. To address this, we propose Segment Attribution Tables (SATs), a method for summarising local saliency explanations into (semi-)global insights. SATs take image segments (such as \"eyes\" in Chihuahuas) and leverage saliency maps to quantify their influence. These segments highlight concepts the model relies on across instances and reveal spurious correlations, such as reliance on backgrounds or watermarks, even when out-of-distribution test performance sees little change. SATs can explain any classifier for which a form of saliency map can be produced, using segmentation maps that provide named segments. SATs bridge the gap between oversimplified global summaries and overly detailed local explanations, offering a practical tool for analysing and debugging image classifiers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Segment Attribution Tables (SATs)ï¼Œæ—¨åœ¨å°†å±€éƒ¨Saliency Mapsè§£é‡Šæ±‡æ€»ä¸ºåŠå…¨å±€((semi-)global)çš„è§è§£ï¼Œä»¥è§£å†³æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å¯è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ã€‚ç›®å‰çš„å±€éƒ¨è§£é‡Šæ–¹æ³•éš¾ä»¥è¯†åˆ«æ¨¡å‹å†³ç­–ä¸­çš„é‡å¤æ¨¡å¼ï¼Œè€Œå…¨å±€æ–¹æ³•åˆå¾€å¾€è¿‡åº¦ç®€åŒ–ï¼ŒSATsé€šè¿‡æ¡¥æ¥è¿™ä¸¤è€…æ¥æä¾›æ›´å…·å®ç”¨æ€§çš„åˆ†æå·¥å…·ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å›¾åƒåˆ†å‰²å›¾(Segmentation Maps)ä¸­çš„å‘½åç‰‡æ®µï¼Œå¹¶ç»“åˆSaliency Mapsæ¥é‡åŒ–è¿™äº›ç‰¹å®šæ¦‚å¿µå¯¹æ¨¡å‹é¢„æµ‹çš„å½±å“ã€‚SATsä¸ä»…èƒ½è¯†åˆ«æ¨¡å‹åœ¨ä¸åŒå®ä¾‹ä¸­æ‰€ä¾èµ–çš„æ ¸å¿ƒæ¦‚å¿µï¼Œè¿˜èƒ½æœ‰æ•ˆæ­éœ²æ¨¡å‹å¯¹èƒŒæ™¯æˆ–æ°´å°ç­‰ä¼ªç›¸å…³(Spurious Correlations)çš„ä¾èµ–ï¼Œå³ä½¿åœ¨åˆ†å¸ƒå¤–(Out-of-distribution)æµ‹è¯•è¡¨ç°å˜åŒ–ä¸å¤§çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ã€‚ç”±äºSATsé€‚ç”¨äºä»»ä½•èƒ½å¤Ÿç”ŸæˆSaliency Mapså¹¶é…åˆå‘½ååˆ†å‰²å›¾çš„åˆ†ç±»å™¨ï¼Œå®ƒä¸ºå›¾åƒåˆ†ç±»å™¨çš„åˆ†æå’Œè°ƒè¯•æä¾›äº†ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23247v1",
      "published_date": "2025-06-29 14:11:02 UTC",
      "updated_date": "2025-06-29 14:11:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:22:25.039146+00:00"
    },
    {
      "arxiv_id": "2507.00078v1",
      "title": "The language of time: a language model perspective on time-series foundation models",
      "title_zh": "æ—¶é—´çš„è¯­è¨€ï¼šè¯­è¨€æ¨¡å‹è§†è§’ä¸‹çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹",
      "authors": [
        "Yi Xie",
        "Yun Xiong",
        "Zejian Shi",
        "Hao Niu",
        "Zhengfu Liu"
      ],
      "abstract": "With the rise of large language models, the paradigm of training foundation models with massive parameter counts on vast datasets has been adopted in multiple domains to achieve remarkable success. Time series foundation models represent a significant extension of this paradigm, demonstrating exceptional expressive power, generalization, and cross-domain transferability. However, this gives rise to a fundamental paradox: time series data reflect distinct dynamical systems, making cross-domain transfer intuitively implausible, yet this is contradicted by the models' empirical success. To resolve this paradox, this paper investigates, from both theoretical and experimental perspectives, the representation learning mechanisms and generalization capabilities of patch-based time series foundation models. We argue that such models are not merely applying a new architecture but are fundamentally generalizing the representation paradigm of language models by extending deterministic vector-based representations to latent probabilistic distributional forms. Our theoretical analysis supports this framework by demonstrating that continuous time-series patches can be faithfully quantized into a discrete vocabulary whose key statistical properties are highly consistent with those of natural language. This generalization allows time series models to inherit the robust representation and transfer abilities of large language models, thereby explaining their superior performance in temporal tasks. Ultimately, our work provides a rigorous theoretical cornerstone for understanding, evaluating, and improving the safety and reliability of large-scale time series foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»ç†è®ºä¸å®éªŒåŒé‡è§’åº¦æ¢è®¨äº†åŸºäºåˆ‡ç‰‡(patch-based)çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹(Time series foundation models)çš„è¡¨ç¤ºå­¦ä¹ æœºåˆ¶ä¸æ³›åŒ–èƒ½åŠ›ã€‚é’ˆå¯¹ä¸åŒé¢†åŸŸçš„åŠ¨åŠ›ç³»ç»Ÿå·®å¼‚å¯¼è‡´è·¨åŸŸè¿ç§»åœ¨ç›´è§‰ä¸Šä¸åˆç†ä½†åœ¨å®è·µä¸­å´æˆåŠŸçš„çŸ›ç›¾ï¼Œä½œè€…æå‡ºè¿™ç±»æ¨¡å‹å®è´¨ä¸Šæ˜¯å°†è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºèŒƒå¼ä»ç¡®å®šæ€§å‘é‡æ‰©å±•åˆ°äº†æ½œåœ¨æ¦‚ç‡åˆ†å¸ƒå½¢å¼ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œè¿ç»­çš„æ—¶é—´åºåˆ—åˆ‡ç‰‡(patches)èƒ½å¤Ÿè¢«å‡†ç¡®åœ°é‡åŒ–ä¸ºç¦»æ•£è¯è¡¨(discrete vocabulary)ï¼Œä¸”å…¶å…³é”®ç»Ÿè®¡ç‰¹æ€§ä¸è‡ªç„¶è¯­è¨€é«˜åº¦ä¸€è‡´ã€‚è¿™ç§æ³›åŒ–æœºåˆ¶ä½¿å¾—æ—¶é—´åºåˆ—æ¨¡å‹å¾—ä»¥ç»§æ‰¿å¤§è¯­è¨€æ¨¡å‹(LLMs)å¼ºå¤§çš„è¡¨ç¤ºä¸è¿ç§»èƒ½åŠ›ï¼Œä»è€Œè§£é‡Šäº†å…¶åœ¨æ—¶é—´ä»»åŠ¡ä¸­çš„ä¼˜è¶Šè¡¨ç°ã€‚è¯¥å·¥ä½œä¸ºç†è§£ã€è¯„ä¼°ä»¥åŠæå‡å¤§è§„æ¨¡æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„å®‰å…¨æ€§ä¸å¯é æ€§å¥ å®šäº†ä¸¥è°¨çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.00078v1",
      "published_date": "2025-06-29 14:03:34 UTC",
      "updated_date": "2025-06-29 14:03:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:21:58.937737+00:00"
    },
    {
      "arxiv_id": "2506.23236v1",
      "title": "VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions",
      "title_zh": "VolumetricSMPLï¼šé¢å‘é«˜æ•ˆäº¤äº’ã€æ¥è§¦ä¸ç¢°æ’çš„ç¥ç»ä½“ç§¯äººä½“æ¨¡å‹",
      "authors": [
        "Marko Mihajlovic",
        "Siwei Zhang",
        "Gen Li",
        "Kaifeng Zhao",
        "Lea MÃ¼ller",
        "Siyu Tang"
      ],
      "abstract": "Parametric human body models play a crucial role in computer graphics and vision, enabling applications ranging from human motion analysis to understanding human-environment interactions. Traditionally, these models use surface meshes, which pose challenges in efficiently handling interactions with other geometric entities, such as objects and scenes, typically represented as meshes or point clouds. To address this limitation, recent research has explored volumetric neural implicit body models. However, existing works are either insufficiently robust for complex human articulations or impose high computational and memory costs, limiting their widespread use. To this end, we introduce VolumetricSMPL, a neural volumetric body model that leverages Neural Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike prior approaches that rely on large MLPs, NBW dynamically blends a small set of learned weight matrices using predicted shape- and pose-dependent coefficients, significantly improving computational efficiency while preserving expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy, and a Signed Distance Function (SDF) for efficient and differentiable contact modeling. We demonstrate VolumetricSMPL's strengths across four challenging tasks: (1) reconstructing human-object interactions from in-the-wild images, (2) recovering human meshes in 3D scenes from egocentric views, (3) scene-constrained motion synthesis, and (4) resolving self-intersections. Our results highlight its broad applicability and significant performance and efficiency gains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VolumetricSMPLï¼Œè¿™æ˜¯ä¸€ç§ç¥ç»ä½“ç§¯äººä½“æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè¡¨é¢ç½‘æ ¼(surface meshes)åœ¨å¤„ç†äººä½“ä¸ç‰©ä½“åŠåœºæ™¯äº¤äº’æ—¶æ•ˆç‡ä½ä¸‹çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹æ ¸å¿ƒå¼•å…¥äº†ç¥ç»æ··åˆæƒé‡(Neural Blend Weights, NBW)æŠ€æœ¯ï¼Œé€šè¿‡åŠ¨æ€æ··åˆä¸€å°ç»„å­¦ä¹ åˆ°çš„æƒé‡çŸ©é˜µæ¥ç”Ÿæˆç´§å‡‘ä¸”é«˜æ•ˆçš„ MLP è§£ç å™¨ï¼Œä»è€Œåœ¨æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡çš„åŒæ—¶ä¿ç•™äº†å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ã€‚VolumetricSMPL è¿˜åˆ©ç”¨æœ‰ç¬¦å·è·ç¦»å‡½æ•°(Signed Distance Function, SDF)å®ç°äº†é«˜æ•ˆä¸”å¯å¾®çš„æ¥è§¦å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ¯”å…ˆå‰çš„ COAP æ¨¡å‹å¿« 10 å€ï¼ŒGPU å†…å­˜å ç”¨é™ä½äº† 6 å€ï¼Œä¸”å‡†ç¡®åº¦æ›´é«˜ã€‚è¯¥ç ”ç©¶è¿›ä¸€æ­¥åœ¨äººæœºäº¤äº’é‡å»ºã€ç¬¬ä¸€è§†è§’äººä½“ç½‘æ ¼æ¢å¤ã€åœºæ™¯çº¦æŸè¿åŠ¨åˆæˆä»¥åŠè‡ªç¢°æ’è§£å†³å››é¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†è¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸æ•ˆç‡ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "[ICCV 2025] https://markomih.github.io/VolumetricSMPL",
      "pdf_url": "https://arxiv.org/pdf/2506.23236v1",
      "published_date": "2025-06-29 13:48:38 UTC",
      "updated_date": "2025-06-29 13:48:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:22:02.728401+00:00"
    },
    {
      "arxiv_id": "2506.23219v1",
      "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding",
      "title_zh": "UrbanLLaVAï¼šå…·å¤‡ç©ºé—´æ¨ç†ä¸ç†è§£èƒ½åŠ›çš„åŸå¸‚æ™ºèƒ½å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Jie Feng",
        "Shengyuan Wang",
        "Tianhui Liu",
        "Yanxin Xi",
        "Yong Li"
      ],
      "abstract": "Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In $\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†UrbanLLaVAï¼Œä¸€ç§ä¸“ä¸ºåŸå¸‚æ™ºèƒ½(urban intelligence)è®¾è®¡çš„å…¨æ–¹ä½å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸå¸‚ç ”ç©¶ä¸­å¤„ç†å¤šæ¨¡æ€æ•°æ®ç¼ºä¹ç»Ÿä¸€æ¡†æ¶çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŒ…å«å•æ¨¡æ€å’Œè·¨æ¨¡æ€æ•°æ®çš„å¤šæ ·åŒ–åŸå¸‚æŒ‡ä»¤æ•°æ®é›†ï¼ŒèŒƒå›´æ¶µç›–ä»å±€éƒ¨è§†å›¾åˆ°å…¨å±€è§†å›¾çš„åŸå¸‚ç¯å¢ƒã€‚é€šè¿‡æå‡ºä¸€ç§å°†ç©ºé—´æ¨ç†(spatial reasoning)å¢å¼ºä¸é¢†åŸŸçŸ¥è¯†å­¦ä¹ è§£è€¦çš„å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼ŒUrbanLLaVAåœ¨å¤šæ ·åŒ–çš„åŸå¸‚ä»»åŠ¡ä¸­å®ç°äº†æå¼ºçš„å…¼å®¹æ€§å’Œä¸‹æ¸¸æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ‰©å±•äº†ç°æœ‰çš„åŸå¸‚ç ”ç©¶åŸºå‡†(benchmark)ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å¹¿æ³›åŸå¸‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚åœ¨ä¸‰ä¸ªåŸå¸‚çš„å®éªŒç»“æœè¯æ˜ï¼ŒUrbanLLaVAåœ¨å•æ¨¡æ€å’Œå¤æ‚è·¨æ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºå½“å‰çš„å¼€æºå’Œå•†ä¸šMLLMsï¼Œå¹¶å±•ç°å‡ºç¨³å¥çš„è·¨åŸå¸‚æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.23219v1",
      "published_date": "2025-06-29 13:04:27 UTC",
      "updated_date": "2025-06-29 13:04:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:22:31.316070+00:00"
    },
    {
      "arxiv_id": "2506.23210v4",
      "title": "FedRef: Communication-Efficient Bayesian Fine-Tuning using a Reference Model",
      "title_zh": "FedRefï¼šåŸºäºå‚è€ƒæ¨¡å‹çš„é€šä¿¡é«˜æ•ˆè´å¶æ–¯å¾®è°ƒ",
      "authors": [
        "Taehwan Yoon",
        "Bongjun Choi",
        "Wesley De Neve"
      ],
      "abstract": "Federated learning (FL) collaboratively trains artificial intelligence (AI) models to ensure user data privacy. Sharing only model updates generated from local training on client data with the server enhances user data privacy. However, model performance may suffer due to data and system heterogeneity among clients in FL scenarios. Previous studies have proposed model optimization, fine-tuning, and personalization to achieve improved model performance. Despite these efforts, models resulting from FL scenarios often exhibit catastrophic forgetting, which increases the communication and computational costs of clients for model optimization and raises energy consumption. To address these challenges, we propose a reference model-based fine-tuning method for federated learning that overcomes catastrophic forgetting in each round. Our method is derived from Bayesian parameter-efficient transfer learning and includes an proximal term. It employs a reference model that incorporates previous model parameters and reviews previous global features in the model optimization step to mitigate catastrophic forgetting. As a result, our method achieves higher model performance and lower communication and computational costs for clients than existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å­¦ä¹ (Federated Learning)ä¸­ç”±äºæ•°æ®å¼‚æ„æ€§å¯¼è‡´çš„ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)åŠé«˜é¢é€šä¿¡è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†FedRefã€‚è¿™æ˜¯ä¸€ç§åŸºäºå‚è€ƒæ¨¡å‹(reference model)çš„é€šä¿¡é«˜æ•ˆè´å¶æ–¯å¾®è°ƒæ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæºè‡ªè´å¶æ–¯å‚æ•°é«˜æ•ˆè¿ç§»å­¦ä¹ (Bayesian parameter-efficient transfer learning)å¹¶å¼•å…¥äº†è¿‘ç«¯é¡¹(proximal term)ã€‚FedRefé€šè¿‡åœ¨ä¼˜åŒ–æ­¥éª¤ä¸­åˆ©ç”¨å‚è€ƒæ¨¡å‹æ•´åˆå…ˆå‰çš„æ¨¡å‹å‚æ•°ï¼Œå¹¶å¤ç›˜ä¹‹å‰çš„å…¨å±€ç‰¹å¾(global features)ï¼Œä»è€Œåœ¨æ¯ä¸€è½®å¾®è°ƒä¸­æœ‰æ•ˆç¼“è§£é—å¿˜ç°è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedRefä¸ä»…èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼Œè¿˜èƒ½æœ‰æ•ˆé™ä½å®¢æˆ·ç«¯çš„é€šä¿¡ä¸è®¡ç®—å¼€é”€ï¼Œè¿›è€Œå‡å°‘èƒ½æºæ¶ˆè€—ã€‚ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œè¯¥æ–¹æ¡ˆä¸ºå¼‚æ„ç¯å¢ƒä¸‹çš„è”é‚¦å­¦ä¹ æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆã€æ›´å…·é²æ£’æ€§çš„å¾®è°ƒè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 16 equations, 5 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.23210v4",
      "published_date": "2025-06-29 12:41:11 UTC",
      "updated_date": "2025-11-24 06:24:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:22:09.547169+00:00"
    },
    {
      "arxiv_id": "2506.23203v1",
      "title": "Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver",
      "title_zh": "åŸºäºå¤§è§„æ¨¡ H$^2$AD MIMO æ¥æ”¶æœºçš„å¤šåˆ†æ”¯ DNN ä¸ CRLB æ¯”ä¾‹æƒé‡èåˆå¢å¼ºå‹ DOA æ„ŸçŸ¥",
      "authors": [
        "Feng Shu",
        "Jiatong Bai",
        "Di Wu",
        "Wei Zhu",
        "Bin Deng",
        "Fuhui Zhou",
        "Jiangzhou Wang"
      ],
      "abstract": "As a green MIMO structure, massive H$^2$AD is viewed as a potential technology for the future 6G wireless network. For such a structure, it is a challenging task to design a low-complexity and high-performance fusion of target direction values sensed by different sub-array groups with fewer use of prior knowledge. To address this issue, a lightweight Cramer-Rao lower bound (CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse CRLB of each subarray using antenna number reciprocals to eliminate real-time CRLB computation. This reduces complexity and prior knowledge dependence while preserving fusion performance. Moreover, a multi-branch deep neural network (MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by leveraging candidate angles from multiple subarrays. The subarray-specific branch networks are integrated with a shared regression module to effectively eliminate pseudo-solutions and fuse true angles. Simulation results show that the proposed CRLB-ratio-WF method achieves DOA sensing performance comparable to CRLB-based methods, while significantly reducing the reliance on prior knowledge. More notably, the proposed MBDNN has superior performance in low-SNR ranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in estimation accuracy compared to CRLB-ratio-WF method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœªæ¥6Gæ— çº¿ç½‘ç»œä¸­çš„å¤§è§„æ¨¡H$^2$AD MIMOç»“æ„ï¼Œæ¢è®¨äº†åœ¨ä½å…ˆéªŒçŸ¥è¯†ä¾èµ–ä¸‹å®ç°é«˜æ€§èƒ½ç›®æ ‡æ–¹å‘èåˆçš„æŒ‘æˆ˜ã€‚æ–‡ä¸­æå‡ºäº†ä¸€ç§è½»é‡çº§çš„CRLB-ratio-weight fusion (WF)æ–¹æ³•ï¼Œé€šè¿‡å¤©çº¿æ•°é‡å€’æ•°è¿‘ä¼¼é€†CRLBï¼Œåœ¨å¤§å¹…é™ä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ä¿æŒäº†æ„ŸçŸ¥æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è®¾è®¡äº†å¤šåˆ†æ”¯æ·±åº¦ç¥ç»ç½‘ç»œ(MBDNN)ï¼Œåˆ©ç”¨å­é˜µåˆ—åˆ†æ”¯ç½‘ç»œä¸å…±äº«å›å½’æ¨¡å—æœ‰æ•ˆæ¶ˆé™¤ä¼ªè§£å¹¶ä¼˜åŒ–æ³¢è¾¾æ–¹å‘(DOA)æ„ŸçŸ¥ç²¾åº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCRLB-ratio-WFæ–¹æ³•åœ¨å‡å°‘å…ˆéªŒçŸ¥è¯†éœ€æ±‚ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œè€ŒMBDNNåœ¨ä½ä¿¡å™ªæ¯”(SNR)ç¯å¢ƒä¸‹è¡¨ç°å“è¶Šã€‚åœ¨SNRä¸º-15 dBçš„æƒ…å†µä¸‹ï¼ŒMBDNNçš„ä¼°è®¡å‡†ç¡®ç‡è¾ƒWFæ–¹æ³•æå‡äº†ä¸€ä¸ªæ•°é‡çº§ï¼Œä¸ºæå‡å¤§è§„æ¨¡é˜µåˆ—ç³»ç»Ÿçš„æ„ŸçŸ¥èƒ½åŠ›æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23203v1",
      "published_date": "2025-06-29 12:14:59 UTC",
      "updated_date": "2025-06-29 12:14:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:22:15.814394+00:00"
    },
    {
      "arxiv_id": "2507.02951v1",
      "title": "Bittensor Protocol: The Bitcoin in Decentralized Artificial Intelligence? A Critical and Empirical Analysis",
      "title_zh": "Bittensor åè®®ï¼šå»ä¸­å¿ƒåŒ–äººå·¥æ™ºèƒ½é¢†åŸŸçš„â€œæ¯”ç‰¹å¸â€ï¼Ÿä¸€é¡¹æ‰¹åˆ¤æ€§ä¸å®è¯åˆ†æ",
      "authors": [
        "Elizabeth Lui",
        "Jiahao Sun"
      ],
      "abstract": "This paper investigates whether Bittensor can be considered the Bitcoin of decentralized Artificial Intelligence by directly comparing its tokenomics, decentralization properties, consensus mechanism, and incentive structure against those of Bitcoin. Leveraging on-chain data from all 64 active Bittensor subnets, we first document considerable concentration in both stake and rewards. We further show that rewards are overwhelmingly driven by stake, highlighting a clear misalignment between quality and compensation. As a remedy, we put forward a series of two-pronged protocol-level interventions. For incentive realignment, our proposed solutions include performance-weighted emission split, composite scoring, and a trust-bonus multiplier. As for mitigating security vulnerability due to stake concentration, we propose and empirically validate stake cap at the 88th percentile, which elevates the median coalition size required for a 51-percent attack and remains robust across daily, weekly, and monthly snapshots.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹ Bittensor åè®®è¿›è¡Œäº†æ‰¹åˆ¤æ€§å®è¯åˆ†æï¼Œé€šè¿‡å¯¹æ¯”å…¶ä¸ Bitcoin åœ¨ä»£å¸ç»æµå­¦(tokenomics)ã€å»ä¸­å¿ƒåŒ–å±æ€§ã€å…±è¯†æœºåˆ¶å’Œæ¿€åŠ±ç»“æ„æ–¹é¢çš„å·®å¼‚ï¼Œæ¢è®¨å…¶ä½œä¸ºå»ä¸­å¿ƒåŒ–äººå·¥æ™ºèƒ½åŸºç¡€è®¾æ–½çš„æ½œåŠ›ã€‚åŸºäºå¯¹å…¨éƒ¨ 64 ä¸ªæ´»è·ƒå­ç½‘é“¾ä¸Šæ•°æ®çš„åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº† Bittensor åœ¨æƒç›Š(stake)ä¸å¥–åŠ±(rewards)åˆ†é…ä¸Šçš„é«˜åº¦é›†ä¸­ï¼Œå¹¶æŒ‡å‡ºå¥–åŠ±ä¸»è¦ç”±æƒç›Šé©±åŠ¨è€Œéè´¡çŒ®è´¨é‡ï¼Œå­˜åœ¨æ˜æ˜¾çš„æ¿€åŠ±é”™ä½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†æ€§èƒ½åŠ æƒå‘è¡Œåˆ†é…(performance-weighted emission split)ã€ç»¼åˆè¯„åˆ†(composite scoring)åŠä¿¡ä»»å¥–åŠ±ä¹˜æ•°(trust-bonus multiplier)ç­‰åè®®å±‚é¢çš„å¹²é¢„æªæ–½ã€‚é’ˆå¯¹æƒç›Šé›†ä¸­å¼•å‘çš„å®‰å…¨æ¼æ´ï¼Œç ”ç©¶æå‡ºå¹¶å®è¯éªŒè¯äº† 88% åˆ†ä½æ•°çš„æƒç›Šä¸Šé™(stake cap)ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½æ˜¾è‘—æé«˜å‘åŠ¨ 51% æ”»å‡»æ‰€éœ€çš„æœ€å°è”ç›Ÿè§„æ¨¡ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ¡ˆåœ¨ä¸åŒæ—¶é—´è·¨åº¦çš„å¿«ç…§æ•°æ®ä¸­å‡å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ï¼Œä¸ºä¼˜åŒ–å»ä¸­å¿ƒåŒ– AI ç½‘ç»œçš„å®‰å…¨æ€§ä¸æ¿€åŠ±æ•ˆç‡æä¾›äº†ç³»ç»Ÿæ€§çš„æ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "MARBLE 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.02951v1",
      "published_date": "2025-06-29 12:07:48 UTC",
      "updated_date": "2025-06-29 12:07:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:22:14.641531+00:00"
    },
    {
      "arxiv_id": "2507.22904v2",
      "title": "SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches",
      "title_zh": "SketchMindï¼šä¸€ç§ç”¨äºè¯„ä¼°å­¦ç”Ÿç§‘å­¦è‰å›¾çš„å¤šæ™ºèƒ½ä½“è®¤çŸ¥æ¡†æ¶",
      "authors": [
        "Ehsan Latif",
        "Zirak Khan",
        "Xiaoming Zhai"
      ],
      "abstract": "Scientific sketches (e.g., models) offer a powerful lens into students' conceptual understanding, yet AI-powered automated assessment of such free-form, visually diverse artifacts remains a critical challenge. Existing solutions often treat sketch evaluation as either an image classification task or monolithic vision-language models, which lack interpretability, pedagogical alignment, and adaptability across cognitive levels. To address these limitations, we present SketchMind, a cognitively grounded, multi-agent framework for evaluating and improving student-drawn scientific sketches. SketchMind comprises modular agents responsible for rubric parsing, sketch perception, cognitive alignment, and iterative feedback with sketch modification, enabling personalized and transparent evaluation. We evaluate SketchMind on a curated dataset of 3,575 student-generated sketches across six science assessment items with different highest order of Bloom's level that require students to draw models to explain phenomena. Compared to baseline GPT-4o performance without SRG (average accuracy: 55.6%), and with SRG integration achieves 77.1% average accuracy (+21.4% average absolute gain). We also demonstrate that multi-agent orchestration with SRG enhances SketchMind performance, for example, GPT-4.1 gains an average 8.9% increase in sketch prediction accuracy, outperforming single-agent pipelines across all items. Human evaluators rated the feedback and co-created sketches generated by \\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5, significantly higher than those of baseline models (e.g., 2.3 for GPT-4o). Experts noted the system's potential to meaningfully support conceptual growth through guided revision. Our code and (pending approval) dataset will be released to support reproducibility and future research in AI-driven education.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SketchMindï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè®¤çŸ¥ç†è®ºçš„å¤šæ™ºèƒ½ä½“(multi-agent)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½åœ¨è¯„ä¼°å­¦ç”Ÿæ‰‹ç»˜ç§‘å­¦è‰å›¾(scientific sketches)æ—¶é¢ä¸´çš„å¯è§£é‡Šæ€§ã€æ•™å­¦ä¸€è‡´æ€§å’Œé€‚åº”æ€§æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶ç”±è´Ÿè´£ç»†åˆ™è§£æ(rubric parsing)ã€è‰å›¾æ„ŸçŸ¥(sketch perception)ã€è®¤çŸ¥å¯¹é½(cognitive alignment)åŠè¿­ä»£åé¦ˆä¸è‰å›¾ä¿®æ”¹çš„æ¨¡å—åŒ–æ™ºèƒ½ä½“ç»„æˆï¼Œå®ç°äº†ä¸ªæ€§åŒ–ä¸”é€æ˜çš„è¯„ä¼°è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥ç©ºé—´è¡¨å¾å›¾(Spatial Representation Graph, SRG)å¹¶ç»“åˆå¤šæ™ºèƒ½ä½“ç¼–æ’ï¼ŒSketchMind åœ¨åŒ…å« 3,575 ä¸ªå­¦ç”Ÿè‰å›¾çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°†å¹³å‡å‡†ç¡®ç‡ä» GPT-4o åŸºçº¿çš„ 55.6% æå‡è‡³ 77.1%ã€‚äººç±»è¯„ä¼°å®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œè¯¥ç³»ç»Ÿç”Ÿæˆçš„åé¦ˆå’ŒååŒåˆ›ä½œè‰å›¾è¯„åˆ†é«˜è¾¾ 4.1/5 åˆ†ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¢«è®¤ä¸ºåœ¨æ”¯æŒå­¦ç”Ÿç§‘å­¦æ¦‚å¿µæˆé•¿æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Submitted to NeurIPS2025",
      "pdf_url": "https://arxiv.org/pdf/2507.22904v2",
      "published_date": "2025-06-29 11:35:10 UTC",
      "updated_date": "2025-10-20 13:55:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:22:50.407205+00:00"
    },
    {
      "arxiv_id": "2506.23184v1",
      "title": "Score-based Diffusion Model for Unpaired Virtual Histology Staining",
      "title_zh": "é¢å‘éé…å¯¹è™šæ‹Ÿç»„ç»‡å­¦æŸ“è‰²çš„åŸºäºè¯„åˆ†çš„æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Anran Liu",
        "Xiaofei Wang",
        "Jing Cai",
        "Chao Li"
      ],
      "abstract": "Hematoxylin and eosin (H&E) staining visualizes histology but lacks specificity for diagnostic markers. Immunohistochemistry (IHC) staining provides protein-targeted staining but is restricted by tissue availability and antibody specificity. Virtual staining, i.e., computationally translating the H&E image to its IHC counterpart while preserving the tissue structure, is promising for efficient IHC generation. Existing virtual staining methods still face key challenges: 1) effective decomposition of staining style and tissue structure, 2) controllable staining process adaptable to diverse tissue and proteins, and 3) rigorous structural consistency modelling to handle the non-pixel-aligned nature of paired H&E and IHC images. This study proposes a mutual-information (MI)-guided score-based diffusion model for unpaired virtual staining. Specifically, we design 1) a global MI-guided energy function that disentangles the tissue structure and staining characteristics across modalities, 2) a novel timestep-customized reverse diffusion process for precise control of the staining intensity and structural reconstruction, and 3) a local MI-driven contrastive learning strategy to ensure the cellular level structural consistency between H&E-IHC images. Extensive experiments demonstrate the our superiority over state-of-the-art approaches, highlighting its biomedical potential. Codes will be open-sourced upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è™šæ‹Ÿç»„ç»‡æŸ“è‰²ä¸­é£æ ¼ç»“æ„è§£è€¦éš¾ã€æŸ“è‰²è¿‡ç¨‹ä¸å¯æ§ä»¥åŠéé…å¯¹(unpaired)æ•°æ®ç»“æ„ä¸€è‡´æ€§å·®ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäº’ä¿¡æ¯(Mutual Information, MI)å¼•å¯¼çš„è¯„åˆ†æ‰©æ•£æ¨¡å‹(Score-based Diffusion Model)ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡å…¨å±€ MI å¼•å¯¼çš„èƒ½é‡å‡½æ•°ï¼Œå®ç°äº†ç»„ç»‡ç»“æ„ä¸æŸ“è‰²ç‰¹å¾åœ¨ä¸åŒæ¨¡æ€é—´çš„æœ‰æ•ˆè§£è€¦ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†æ–°å‹çš„æ—¶é—´æ­¥è‡ªå®šä¹‰åå‘æ‰©æ•£è¿‡ç¨‹(timestep-customized reverse diffusion)ï¼Œä»¥å®ç°å¯¹æŸ“è‰²å¼ºåº¦å’Œç»“æ„é‡å»ºçš„ç²¾å‡†æ§åˆ¶ã€‚ä¸ºäº†ç¡®ä¿ç»†èƒå±‚é¢çš„ç»“æ„ä¸€è‡´æ€§ï¼Œè¯¥æ¨¡å‹è¿˜é‡‡ç”¨äº†å±€éƒ¨ MI é©±åŠ¨çš„å¯¹æ¯”å­¦ä¹ (contrastive learning)ç­–ç•¥ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç† H&E åˆ° IHC çš„å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„å…ˆè¿›(SOTA)æ–¹æ³•ï¼Œå±•ç°äº†æ˜¾è‘—çš„ç”Ÿç‰©åŒ»å­¦åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "11 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.23184v1",
      "published_date": "2025-06-29 11:02:45 UTC",
      "updated_date": "2025-06-29 11:02:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:22:49.470730+00:00"
    },
    {
      "arxiv_id": "2506.23174v1",
      "title": "Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data",
      "title_zh": "è®©æ•°æ®è¯´è¯ï¼šæ— çº¿åˆæˆæ•°æ®çš„è´¨é‡å¯¼å‘åˆ©ç”¨",
      "authors": [
        "Chen Gong",
        "Bo Liang",
        "Wei Gao",
        "Chenren Xu"
      ],
      "abstract": "Generative models have gained significant attention for their ability to produce realistic synthetic data that supplements the quantity of real-world datasets. While recent studies show performance improvements in wireless sensing tasks by incorporating all synthetic data into training sets, the quality of synthetic data remains unpredictable and the resulting performance gains are not guaranteed. To address this gap, we propose tractable and generalizable metrics to quantify quality attributes of synthetic data - affinity and diversity. Our assessment reveals prevalent affinity limitation in current wireless synthetic data, leading to mislabeled data and degraded task performance. We attribute the quality limitation to generative models' lack of awareness of untrained conditions and domain-specific processing. To mitigate these issues, we introduce SynCheck, a quality-guided synthetic data utilization scheme that refines synthetic data quality during task model training. Our evaluation demonstrates that SynCheck consistently outperforms quality-oblivious utilization of synthetic data, and achieves 4.3% performance improvement even when the previous utilization degrades performance by 13.4%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— çº¿æ„ŸçŸ¥ä»»åŠ¡ä¸­ç”Ÿæˆæ¨¡å‹(Generative models)äº§ç”Ÿçš„åˆæˆæ•°æ®è´¨é‡ä¸å¯é¢„æµ‹ä¸”æ€§èƒ½å¢ç›Šæ— æ³•ä¿è¯çš„é—®é¢˜ï¼Œæå‡ºäº†è¯„ä¼°åˆæˆæ•°æ®è´¨é‡çš„ä¸¤ä¸ªå…³é”®æŒ‡æ ‡ï¼šäº²å’Œæ€§(affinity)å’Œå¤šæ ·æ€§(diversity)ã€‚ç ”ç©¶æŒ‡å‡ºå½“å‰æ— çº¿åˆæˆæ•°æ®æ™®éå­˜åœ¨äº²å’Œæ€§ä¸è¶³ï¼Œå¸¸å¯¼è‡´æ•°æ®è¯¯æ ‡å’Œæ€§èƒ½ä¸‹é™ï¼Œå¹¶å°†å…¶å½’å› äºç”Ÿæˆæ¨¡å‹ç¼ºä¹å¯¹æœªè®­ç»ƒæ¡ä»¶åŠç‰¹å®šé¢†åŸŸå¤„ç†çš„æ„ŸçŸ¥ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†SynCheckï¼Œè¿™æ˜¯ä¸€ç§è´¨é‡å¼•å¯¼çš„åˆæˆæ•°æ®åˆ©ç”¨æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨ä»»åŠ¡æ¨¡å‹è®­ç»ƒæœŸé—´ç²¾ç‚¼åˆæˆæ•°æ®è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSynCheckä¸€è‡´ä¼˜äºä¸è€ƒè™‘è´¨é‡çš„åˆ©ç”¨æ–¹å¼ï¼Œåœ¨ä¼ ç»Ÿæ–¹æ³•å¯¼è‡´æ€§èƒ½ä¸‹é™13.4%çš„åœºæ™¯ä¸‹ï¼Œä¾ç„¶èƒ½å¤Ÿå®ç°4.3%çš„æ€§èƒ½æå‡ã€‚è¯¥æ–¹æ¡ˆè¯æ˜äº†é€šè¿‡è´¨é‡å¼•å¯¼åˆ©ç”¨åˆæˆæ•°æ®ï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºæ— çº¿æ„ŸçŸ¥ä»»åŠ¡æ¨¡å‹çš„é²æ£’æ€§ä¸å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in MobiSys 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.23174v1",
      "published_date": "2025-06-29 10:17:39 UTC",
      "updated_date": "2025-06-29 10:17:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:22:49.966713+00:00"
    },
    {
      "arxiv_id": "2506.23173v1",
      "title": "Deep Learning for Optical Misalignment Diagnostics in Multi-Lens Imaging Systems",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šé€é•œæˆåƒç³»ç»Ÿå…‰å­¦å¤±å‡†è¯Šæ–­",
      "authors": [
        "Tomer Slor",
        "Dean Oren",
        "Shira Baneth",
        "Tom Coen",
        "Haim Suchowski"
      ],
      "abstract": "In the rapidly evolving field of optical engineering, precise alignment of multi-lens imaging systems is critical yet challenging, as even minor misalignments can significantly degrade performance. Traditional alignment methods rely on specialized equipment and are time-consuming processes, highlighting the need for automated and scalable solutions. We present two complementary deep learning-based inverse-design methods for diagnosing misalignments in multi-element lens systems using only optical measurements. First, we use ray-traced spot diagrams to predict five-degree-of-freedom (5-DOF) errors in a 6-lens photographic prime, achieving a mean absolute error of 0.031mm in lateral translation and 0.011$^\\circ$ in tilt. We also introduce a physics-based simulation pipeline that utilizes grayscale synthetic camera images, enabling a deep learning model to estimate 4-DOF, decenter and tilt errors in both two- and six-lens multi-lens systems. These results show the potential to reshape manufacturing and quality control in precision imaging.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šé€é•œæˆåƒç³»ç»Ÿ(multi-lens imaging systems)ä¸­ç²¾å¯†å¯¹å‡†é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸¤ç§äº’è¡¥çš„åŸºäºæ·±åº¦å­¦ä¹ (deep learning)çš„é€†å‘è®¾è®¡æ–¹æ³•ï¼Œæ—¨åœ¨ä»…é€šè¿‡å…‰å­¦æµ‹é‡å®ç°è‡ªåŠ¨åŒ–çš„å¤±è°ƒè¯Šæ–­ã€‚ç¬¬ä¸€ç§æ–¹æ³•åˆ©ç”¨å°„çº¿è¿½è¸ªå…‰æ–‘å›¾(ray-traced spot diagrams)é¢„æµ‹å…­é•œæ‘„å½±é•œå¤´ä¸­çš„äº”è‡ªç”±åº¦(5-DOF)è¯¯å·®ï¼Œå®ç°äº†ä¾§å‘å¹³ç§»0.031mmå’Œå€¾æ–œ0.011Â°çš„å¹³å‡ç»å¯¹è¯¯å·®ã€‚ç¬¬äºŒç§æ–¹æ³•åˆ™å¼•å…¥äº†åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿç®¡çº¿ï¼Œåˆ©ç”¨ç°åº¦åˆæˆç›¸æœºå›¾åƒ(grayscale synthetic camera images)ä½¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®ä¼°è®¡åŒé€é•œå’Œå…­é€é•œç³»ç»Ÿä¸­çš„å››è‡ªç”±åº¦(4-DOF)åå¿ƒä¸å€¾æ–œè¯¯å·®ã€‚å®éªŒç»“æœè¯æ˜äº†æ·±åº¦å­¦ä¹ åœ¨å¤„ç†å¤æ‚å…‰å­¦å¤±è°ƒè¯Šæ–­ä¸­çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡åˆ¶é€ è¿‡ç¨‹ä¸­çš„æ£€æµ‹ç²¾åº¦ã€‚è¿™é¡¹å·¥ä½œä¸ºç²¾å¯†æˆåƒé¢†åŸŸçš„è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶(quality control)å’Œå¤§è§„æ¨¡ç”Ÿäº§æä¾›äº†æå…·æ½œåŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "physics.optics",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.optics",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23173v1",
      "published_date": "2025-06-29 10:13:40 UTC",
      "updated_date": "2025-06-29 10:13:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:22:59.044637+00:00"
    },
    {
      "arxiv_id": "2506.23168v1",
      "title": "Rises for Measuring Local Distributivity in Lattices",
      "title_zh": "ç”¨äºè¡¡é‡æ ¼ä¸­å±€éƒ¨åˆ†é…æ€§çš„ä¸Šå‡",
      "authors": [
        "Mohammad Abdulla",
        "Tobias Hille",
        "Dominik DÃ¼rrschnabel",
        "Gerd Stumme"
      ],
      "abstract": "Distributivity is a well-established and extensively studied notion in lattice theory. In the context of data analysis, particularly within Formal Concept Analysis (FCA), lattices are often observed to exhibit a high degree of distributivity. However, no standardized measure exists to quantify this property. In this paper, we introduce the notion of rises in (concept) lattices as a means to assess distributivity. Rises capture how the number of attributes or objects in covering concepts change within the concept lattice. We show that a lattice is distributive if and only if no non-unit rises occur. Furthermore, we relate rises to the classical notion of meet- and join distributivity. We observe that concept lattices from real-world data are to a high degree join-distributive, but much less meet-distributive. We additionally study how join-distributivity manifests on the level of ordered sets.",
      "tldr_zh": "æ ¼è®º(Lattice theory)ä¸­çš„åˆ†é…æ€§(Distributivity)æ˜¯ä¸€ä¸ªè¢«å¹¿æ³›ç ”ç©¶çš„æ¦‚å¿µï¼Œå°¤å…¶åœ¨å½¢å¼æ¦‚å¿µåˆ†æ(Formal Concept Analysis, FCA)é¢†åŸŸï¼Œæ ¼é€šå¸¸è¡¨ç°å‡ºé«˜åº¦çš„åˆ†é…æ€§ï¼Œä½†ç›®å‰ç¼ºä¹æ ‡å‡†åŒ–çš„é‡åŒ–è¡¡é‡æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†æ ¼ä¸­çš„â€œrisesâ€æ¦‚å¿µä½œä¸ºè¯„ä¼°æ‰‹æ®µï¼Œç”¨äºæ•æ‰æ¦‚å¿µæ ¼(concept lattices)ä¸­è¦†ç›–æ¦‚å¿µ(covering concepts)çš„å±æ€§æˆ–å¯¹è±¡æ•°é‡çš„å˜åŒ–ã€‚è®ºæ–‡è¯æ˜äº†ä¸€ä¸ªæ ¼æ˜¯åˆ†é…æ ¼(distributive)çš„å……åˆ†å¿…è¦æ¡ä»¶æ˜¯ä¸å­˜åœ¨â€œnon-unit risesâ€ï¼Œå¹¶è¿›ä¸€æ­¥æ¢è®¨äº†â€œrisesâ€ä¸ç»å…¸çš„äº¤åˆ†é…æ€§(meet-distributivity)åŠå¹¶åˆ†é…æ€§(join-distributivity)ä¹‹é—´çš„è”ç³»ã€‚é€šè¿‡å¯¹çœŸå®ä¸–ç•Œæ•°æ®(real-world data)çš„è§‚å¯Ÿå‘ç°ï¼Œæ¦‚å¿µæ ¼å…·æœ‰æ˜¾è‘—çš„å¹¶åˆ†é…æ€§ï¼Œä½†åœ¨äº¤åˆ†é…æ€§æ–¹é¢è¡¨ç°è¾ƒå¼±ã€‚æœ€åï¼Œè¯¥è®ºæ–‡è¿˜ç ”ç©¶äº†å¹¶åˆ†é…æ€§(join-distributivity)åœ¨ååºé›†(ordered sets)å±‚é¢çš„å…·ä½“ä½“ç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.DM",
        "math.CO",
        "math.RA"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 2 tables, 5 figures, International Joint Conference on Conceptual Knowledge Structures",
      "pdf_url": "https://arxiv.org/pdf/2506.23168v1",
      "published_date": "2025-06-29 10:03:51 UTC",
      "updated_date": "2025-06-29 10:03:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:10.274393+00:00"
    },
    {
      "arxiv_id": "2506.23164v1",
      "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models",
      "title_zh": "æ¨¡å¼å´©æºƒï¼šè”åˆè½¨è¿¹é¢„æµ‹æ¨¡å‹ä¸­å…³é”®äº¤äº’çš„è¯„ä¼°",
      "authors": [
        "Maarten Hugenholtz",
        "Anna Meszaros",
        "Jens Kober",
        "Zlatan Ajanovic"
      ],
      "abstract": "Autonomous Vehicle decisions rely on multimodal prediction models that account for multiple route options and the inherent uncertainty in human behavior. However, models can suffer from mode collapse, where only the most likely mode is predicted, posing significant safety risks. While existing methods employ various strategies to generate diverse predictions, they often overlook the diversity in interaction modes among agents. Additionally, traditional metrics for evaluating prediction models are dataset-dependent and do not evaluate inter-agent interactions quantitatively. To our knowledge, none of the existing metrics explicitly evaluates mode collapse. In this paper, we propose a novel evaluation framework that assesses mode collapse in joint trajectory predictions, focusing on safety-critical interactions. We introduce metrics for mode collapse, mode correctness, and coverage, emphasizing the sequential dimension of predictions. By testing four multi-agent trajectory prediction models, we demonstrate that mode collapse indeed happens. When looking at the sequential dimension, although prediction accuracy improves closer to interaction events, there are still cases where the models are unable to predict the correct interaction mode, even just before the interaction mode becomes inevitable. We hope that our framework can help researchers gain new insights and advance the development of more consistent and accurate prediction models, thus enhancing the safety of autonomous driving systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨é©¾é©¶å†³ç­–ä¸­å¤šæ¨¡æ€é¢„æµ‹æ¨¡å‹é¢ä¸´çš„ Mode Collapse é—®é¢˜ï¼Œå³æ¨¡å‹å€¾å‘äºä»…é¢„æµ‹æœ€å¯èƒ½çš„æ¨¡å¼è€Œå¿½ç•¥äº¤äº’å¤šæ ·æ€§ï¼Œä»è€Œå¸¦æ¥å®‰å…¨é£é™©ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æŒ‡æ ‡æ— æ³•å®šé‡è¯„ä¼°æ™ºèƒ½ä½“é—´äº¤äº’ä¸”ç¼ºä¹æ˜¾å¼ Mode Collapse è¯„ä¼°çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªä¸“æ³¨äºå®‰å…¨å…³é”®äº¤äº’çš„è”åˆè½¨è¿¹é¢„æµ‹è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº† Mode Collapseã€Mode Correctness å’Œ Coverage ç­‰æ–°æŒ‡æ ‡ï¼Œå¹¶ç‰¹åˆ«å¼ºè°ƒäº†é¢„æµ‹çš„æ—¶åºç»´åº¦ã€‚é€šè¿‡å¯¹å››ç§å¤šæ™ºèƒ½ä½“è½¨è¿¹é¢„æµ‹æ¨¡å‹çš„æµ‹è¯•ï¼Œç ”ç©¶è¯å®äº† Mode Collapse ç°è±¡çš„æ™®éå­˜åœ¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡é¢„æµ‹å‡†ç¡®ç‡éšæ—¶é—´æ¨ç§»æœ‰æ‰€æå‡ï¼Œä½†æ¨¡å‹åœ¨äº¤äº’æ¨¡å¼å˜å¾—ä¸å¯é¿å…å‰çš„å…³é”®æ—¶åˆ»ä»å¯èƒ½æ— æ³•é¢„æµ‹å‡ºæ­£ç¡®çš„äº¤äº’æ¨¡å¼ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘æ›´å…·ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§çš„é¢„æµ‹æ¨¡å‹æä¾›äº†é‡è¦å‚è€ƒï¼Œæœ‰åŠ©äºè¿›ä¸€æ­¥å¢å¼ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "12 pages, 8 figures, submitted to a journal",
      "pdf_url": "https://arxiv.org/pdf/2506.23164v1",
      "published_date": "2025-06-29 09:53:12 UTC",
      "updated_date": "2025-06-29 09:53:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:11.738619+00:00"
    },
    {
      "arxiv_id": "2506.23151v1",
      "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation",
      "title_zh": "MEMFOFï¼šé¢å‘æ˜¾å­˜é«˜æ•ˆå¤šå¸§å…‰æµä¼°è®¡çš„é«˜åˆ†è¾¨ç‡è®­ç»ƒ",
      "authors": [
        "Vladislav Bargatin",
        "Egor Chistov",
        "Alexander Yakovenko",
        "Dmitriy Vatolin"
      ],
      "abstract": "Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical flow method that identifies a favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resource-intensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at https://github.com/msu-video-group/memfof.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜åˆ†è¾¨ç‡å…‰æµä¼°è®¡(Optical Flow Estimation)ä¸­GPUå†…å­˜æ¶ˆè€—è¿‡å¤§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†MEMFOFï¼Œä¸€ç§å…¼é¡¾å†…å­˜æ•ˆç‡ä¸ç²¾åº¦çš„å¤šå¸§(Multi-Frame)å…‰æµä¼°è®¡æ–¹æ³•ã€‚MEMFOFé€šè¿‡ä¼˜åŒ–ç±»RAFTæ¶æ„ï¼Œå¼•å…¥ç®€åŒ–çš„ç›¸å…³ä½“ç§¯(Correlation Volumes)å’Œé«˜åˆ†è¾¨ç‡è®­ç»ƒåè®®ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç›´æ¥åœ¨åŸç”Ÿ1080påˆ†è¾¨ç‡ä¸‹è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€å›¾åƒè£å‰ªæˆ–ä¸‹é‡‡æ ·ã€‚åœ¨è¿è¡Œæ—¶ï¼Œè¯¥æ–¹æ³•å¤„ç†1080pè¾“å…¥ä»…éœ€2.09 GBå†…å­˜ï¼Œè®­ç»ƒé˜¶æ®µä»…éœ€28.5 GBå†…å­˜ï¼Œæ˜¾è‘—é™ä½äº†ç¡¬ä»¶èµ„æºé—¨æ§›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMEMFOFåœ¨SpringåŸºå‡†æµ‹è¯•ä¸­æ’åç¬¬ä¸€ï¼Œå¹¶åœ¨Sintelå’ŒKITTI-2015ç­‰ä¸»æµæ¦œå•ä¸Šå–å¾—äº†SOTAæ€§èƒ½ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶èƒ½å¤Ÿå¤§å¹…å‡å°‘é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸‹çš„è®¡ç®—å¼€é”€ï¼Œä¸ºé²æ£’ä¸”é«˜æ•ˆçš„å…‰æµä¼°è®¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.23151v1",
      "published_date": "2025-06-29 09:01:42 UTC",
      "updated_date": "2025-06-29 09:01:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:09.613802+00:00"
    },
    {
      "arxiv_id": "2506.23141v2",
      "title": "Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing",
      "title_zh": "åŸºäºè¯­ä¹‰æ„ŸçŸ¥å…³ç³»æ¶ˆæ¯ä¼ é€’çš„ä¸Šä¸‹æ–‡é©±åŠ¨çŸ¥è¯†å›¾è°±è¡¥å…¨",
      "authors": [
        "Siyuan Li",
        "Yan Wen",
        "Ruitong Liu",
        "Te Sun",
        "Ruihao Zhou",
        "Jingyi Kang",
        "Yunjia Wu"
      ],
      "abstract": "Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge Graph Completion (KGC), providing vital cues for prediction. However, traditional node-based message passing mechanisms, when applied to knowledge graphs, often introduce noise and suffer from information dilution or over-smoothing by indiscriminately aggregating information from all neighboring edges. To address this challenge, we propose a semantic-aware relational message passing. A core innovation of this framework is the introduction of a semantic-aware Top-K neighbor selection strategy. Specifically, this strategy first evaluates the semantic relevance between a central node and its incident edges within a shared latent space, selecting only the Top-K most pertinent ones. Subsequently, information from these selected edges is effectively fused with the central node's own representation using a multi-head attention aggregator to generate a semantically focused node message. In this manner, our model not only leverages the structure and features of edges within the knowledge graph but also more accurately captures and propagates the contextual information most relevant to the specific link prediction task, thereby effectively mitigating interference from irrelevant information. Extensive experiments demonstrate that our method achieves superior performance compared to existing approaches on several established benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ¥è¯†å›¾è°±è¡¥å…¨(Knowledge Graph Completion, KGC)ä¸­ä¼ ç»ŸèŠ‚ç‚¹æ¶ˆæ¯ä¼ é€’æœºåˆ¶å› ç›²ç›®èšåˆæ‰€æœ‰é‚»æ¥è¾¹ä¿¡æ¯è€Œå¯¼è‡´çš„å™ªå£°å¼•å…¥å’Œè¿‡å¹³æ»‘(over-smoothing)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è¯­ä¹‰æ„ŸçŸ¥å…³ç³»æ¶ˆæ¯ä¼ é€’(semantic-aware relational message passing)æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†è¯­ä¹‰æ„ŸçŸ¥ Top-K é‚»å±…é€‰æ‹©ç­–ç•¥ï¼Œé€šè¿‡åœ¨å…±äº«æ½œç©ºé—´ä¸­è¯„ä¼°ä¸­å¿ƒèŠ‚ç‚¹ä¸å…¥å°„è¾¹ä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§ï¼Œç²¾å‡†ç­›é€‰æœ€å…·ç›¸å…³æ€§çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚éšåï¼Œç³»ç»Ÿåˆ©ç”¨å¤šå¤´æ³¨æ„åŠ›èšåˆå™¨(multi-head attention aggregator)å°†ç­›é€‰å‡ºçš„è¾¹ä¿¡æ¯ä¸ä¸­å¿ƒèŠ‚ç‚¹è¡¨ç¤ºè¿›è¡Œèåˆï¼Œç”Ÿæˆè¯­ä¹‰é›†ä¸­çš„èŠ‚ç‚¹æ¶ˆæ¯ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å……åˆ†åˆ©ç”¨äº†çŸ¥è¯†å›¾è°±çš„ç»“æ„å’Œè¾¹ç‰¹å¾ï¼Œè¿˜é€šè¿‡è¿‡æ»¤æ— å…³å¹²æ‰°å¢å¼ºäº†é“¾æ¥é¢„æµ‹ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä¸»æµåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†ä¼˜äºç°æœ‰æŠ€æœ¯çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23141v2",
      "published_date": "2025-06-29 08:37:48 UTC",
      "updated_date": "2025-09-10 16:23:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:09.740169+00:00"
    },
    {
      "arxiv_id": "2506.23139v1",
      "title": "Benchmarking Deep Search over Heterogeneous Enterprise Data",
      "title_zh": "å¼‚æ„ä¼ä¸šæ•°æ®æ·±åº¦æœç´¢åŸºå‡†æµ‹è¯•",
      "authors": [
        "Prafulla Kumar Choubey",
        "Xiangyu Peng",
        "Shilpa Bhagavath",
        "Kung-Hsiang Huang",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "abstract": "We present a new benchmark for evaluating Deep Search--a realistic and complex form of retrieval-augmented generation (RAG) that requires source-aware, multi-hop reasoning over diverse, sparsed, but related sources. These include documents, meeting transcripts, Slack messages, GitHub, and URLs, which vary in structure and often contain human-to-human interactions. We build it using a synthetic data pipeline that simulates business workflows across product planning, development, and support stages, generating interconnected content with realistic noise and multi-hop questions with guaranteed ground-truth answers. We release our benchmark with both answerable and unanswerable queries, and retrieval pool of 39,190 enterprise artifacts, enabling fine-grained evaluation of long-context LLM and RAG systems. Our experiments reveal that even the best-performing agentic RAG methods achieve an average performance score of 32.96 on our benchmark. With further analysis, we highlight retrieval as the main bottleneck: existing methods struggle to conduct deep searches and retrieve all necessary evidence. Consequently, they often reason over partial context, leading to significant performance degradation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹ Deep Search çš„å…¨æ–°åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ç§æ›´çœŸå®ä¸”å¤æ‚çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) å½¢å¼ï¼Œè¦æ±‚åœ¨å¼‚æ„ä¸”ç›¸å…³çš„ä¼ä¸šæ•°æ®æºä¸Šè¿›è¡Œå…·æœ‰æ¥æºæ„ŸçŸ¥èƒ½åŠ›çš„å¤šæ­¥æ¨ç† (multi-hop reasoning)ã€‚è¯„ä¼°æ•°æ®æ¶µç›–äº†æ–‡æ¡£ã€ä¼šè®®å½•éŸ³ã€Slack æ¶ˆæ¯ã€GitHub å’Œ URL ç­‰å¤šç§ç»“æ„å„å¼‚ä¸”åŒ…å«äººé™…äº’åŠ¨çš„æ¥æºã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ¨¡æ‹Ÿäº§å“è§„åˆ’ã€å¼€å‘å’Œæ”¯æŒé˜¶æ®µçš„å•†ä¸šå·¥ä½œæµï¼Œåˆ©ç”¨åˆæˆæ•°æ®æµæ°´çº¿ (synthetic data pipeline) ç”Ÿæˆäº†å…·æœ‰çœŸå®å™ªå£°ã€ç›¸äº’å…³è”çš„å†…å®¹ä»¥åŠå¸¦æœ‰ç¡®å®šæ ‡å‡†ç­”æ¡ˆçš„å¤šæ­¥é—®é¢˜ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«å¯å›ç­”å’Œä¸å¯å›ç­”çš„æŸ¥è¯¢ï¼Œä»¥åŠç”± 39,190 ä¸ªä¼ä¸šå·¥ä»¶ç»„æˆçš„æ£€ç´¢æ± ï¼Œæ—¨åœ¨å¯¹é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ (LLM) å’Œ RAG ç³»ç»Ÿè¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„ Agentic RAG æ–¹æ³•åœ¨è¯¥åŸºå‡†ä¸Šçš„å¹³å‡å¾—åˆ†ä¹Ÿä»…ä¸º 32.96ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œæ£€ç´¢æ˜¯ç›®å‰çš„ä¸»è¦ç“¶é¢ˆï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥è¿›è¡Œæ·±åº¦æœç´¢ (Deep Search) å¹¶è·å–æ‰€æœ‰å¿…è¦è¯æ®ï¼Œå¸¸å› ä»…åŸºäºéƒ¨åˆ†ä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†è€Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23139v1",
      "published_date": "2025-06-29 08:34:59 UTC",
      "updated_date": "2025-06-29 08:34:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:13.638282+00:00"
    },
    {
      "arxiv_id": "2506.23137v3",
      "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion",
      "title_zh": "Flow-Modulated Scoringï¼šé¢å‘è¯­ä¹‰æ„ŸçŸ¥çŸ¥è¯†å›¾è°±è¡¥å…¨çš„æµè°ƒåˆ¶è¯„åˆ†",
      "authors": [
        "Siyuan Li",
        "Ruitong Liu",
        "Yan Wen",
        "Te Sun",
        "Andi Zhang",
        "Yanbiao Ma",
        "Xiaoshuai Hao"
      ],
      "abstract": "Knowledge graph completion demands effective modeling of multifaceted semantic relationships between entities. Yet, prevailing methods, which rely on static scoring functions over learned embeddings, struggling to simultaneously capture rich semantic context and the dynamic nature of relations. To overcome this limitation, we propose the Flow-Modulated Scoring (FMS) framework, conceptualizing a relation as a dynamic evolutionary process governed by its static semantic environment. FMS operates in two stages: it first learns context-aware entity embeddings via a Semantic Context Learning module, and then models a dynamic flow between them using a Conditional Flow-Matching module. This learned flow dynamically modulates a base static score for the entity pair. By unifying context-rich static representations with a conditioned dynamic flow, FMS achieves a more comprehensive understanding of relational semantics. Extensive experiments demonstrate that FMS establishes a new state of the art across both canonical knowledge graph completion tasks: relation prediction and entity prediction. On the standard relation prediction benchmark FB15k-237, FMS achieves a near-perfect MRR of 99.8\\% and Hits@1 of 99.7\\% using a mere 0.35M parameters, while also attaining a 99.9\\% MRR on WN18RR. Its dominance extends to entity prediction, where it secures a 25.2\\% relative MRR gain in the transductive setting and substantially outperforms all baselines in challenging inductive settings. By unifying a dynamic flow mechanism with rich static contexts, FMS offers a highly effective and parameter-efficient new paradigm for knowledge graph completion. Code published at: https://github.com/yuanwuyuan9/FMS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Flow-Modulated Scoring (FMS)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³çŸ¥è¯†å›¾è°±è¡¥å…¨(Knowledge Graph Completion)ä¸­ä¼ ç»Ÿé™æ€è¯„åˆ†å‡½æ•°éš¾ä»¥åŒæ—¶æ•æ‰ä¸°å¯Œè¯­ä¹‰ä¸Šä¸‹æ–‡ä¸å…³ç³»åŠ¨æ€æ€§çš„å±€é™ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡Semantic Context Learningæ¨¡å—å­¦ä¹ ä¸Šä¸‹æ–‡æ„ŸçŸ¥å®ä½“åµŒå…¥ï¼Œå¹¶åˆ©ç”¨Conditional Flow-Matchingæ¨¡å—å»ºæ¨¡å®ä½“é—´çš„åŠ¨æ€æµï¼Œè¿›è€Œå¯¹åŸºç¡€é™æ€è¯„åˆ†è¿›è¡ŒåŠ¨æ€è°ƒèŠ‚ã€‚é€šè¿‡å°†å¯Œå«ä¸Šä¸‹æ–‡çš„é™æ€è¡¨ç¤ºä¸æ¡ä»¶åŒ–çš„åŠ¨æ€æµæœºåˆ¶ç›¸ç»“åˆï¼ŒFMSå®ç°äº†å¯¹å…³ç³»è¯­ä¹‰æ›´å…¨é¢çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFMSåœ¨FB15k-237å’ŒWN18RRç­‰åŸºå‡†æµ‹è¯•çš„å…³ç³»é¢„æµ‹ä¸å®ä½“é¢„æµ‹ä»»åŠ¡ä¸­å‡åˆ·æ–°äº†æœ€å…ˆè¿›(SOTA)è®°å½•ï¼Œå°¤å…¶åœ¨FB15k-237ä¸Šä»…å‡­0.35Må‚æ•°é‡å°±è¾¾åˆ°äº†99.8%çš„MRRã€‚æ­¤å¤–ï¼ŒFMSåœ¨è½¬å¯¼å¼(transductive)å’ŒæŒ‘æˆ˜æ€§çš„å½’çº³å¼(inductive)è®¾ç½®ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†è¿™ç§æ–°èŒƒå¼å…·æœ‰æé«˜çš„æœ‰æ•ˆæ€§ä¸å‚æ•°æ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.23137v3",
      "published_date": "2025-06-29 08:22:04 UTC",
      "updated_date": "2025-08-30 20:56:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:25.527239+00:00"
    },
    {
      "arxiv_id": "2506.23128v1",
      "title": "Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦å…·å¤‡æ·±åº¦å…³ç³»æ¨ç†èƒ½åŠ›ï¼Ÿæ¥è‡ª DeepSeek-R1 ä¸åŸºå‡†æµ‹è¯•å¯¹æ¯”çš„å¯ç¤º",
      "authors": [
        "Chi Chiu So",
        "Yueyue Sun",
        "Jun-Min Wang",
        "Siu Pang Yung",
        "Anthony Wai Keung Loh",
        "Chun Pong Chau"
      ],
      "abstract": "How far are Large Language Models (LLMs) in performing deep relational reasoning? In this paper, we evaluate and compare the reasoning capabilities of three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a suite of carefully designed benchmark tasks in family tree and general graph reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the highest F1-scores across multiple tasks and problem sizes, demonstrating strong aptitude in logical deduction and relational inference. However, all evaluated models, including DeepSeek-R1, struggle significantly as problem complexity increases, largely due to token length limitations and incomplete output structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought responses uncovers its unique planning and verification strategies, but also highlights instances of incoherent or incomplete reasoning, calling attention to the need for deeper scrutiny into LLMs' internal inference dynamics. We further discuss key directions for future work, including the role of multimodal reasoning and the systematic examination of reasoning failures. Our findings provide both empirical insights and theoretical implications for advancing LLMs' reasoning abilities, particularly in tasks that demand structured, multi-step logical inference. Our code repository will be publicly available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°å¹¶å¯¹æ¯”äº† DeepSeek-R1ã€DeepSeek-V3 å’Œ GPT-4o åœ¨æ‰§è¡Œæ·±å±‚å…³ç³»æ¨ç†ï¼ˆdeep relational reasoningï¼‰æ–¹é¢çš„èƒ½åŠ›ï¼Œé€šè¿‡å®¶è°±ï¼ˆfamily treeï¼‰å’Œé€šç”¨å›¾æ¨ç†ï¼ˆgeneral graph reasoningï¼‰åŸºå‡†ä»»åŠ¡è¿›è¡Œäº†ç³»ç»Ÿæµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼ŒDeepSeek-R1 åœ¨å¤šé¡¹ä»»åŠ¡ä¸­å§‹ç»ˆè·å¾—æœ€é«˜çš„ F1-scoresï¼Œå±•ç°å‡ºå¼ºå¤§çš„é€»è¾‘æ¼”ç»ï¼ˆlogical deductionï¼‰å’Œå…³ç³»æ¨æ–­ï¼ˆrelational inferenceï¼‰æ½œåŠ›ã€‚ç„¶è€Œï¼Œéšç€é—®é¢˜å¤æ‚åº¦æå‡ï¼Œå—é™äº Token é•¿åº¦é™åˆ¶å’Œä¸å®Œæ•´çš„è¾“å‡ºç»“æ„ï¼Œæ‰€æœ‰æ¨¡å‹å‡é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹ DeepSeek-R1 é•¿é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰å“åº”çš„ç»†è‡´åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†å…¶ç‹¬ç‰¹çš„è§„åˆ’ä¸éªŒè¯ç­–ç•¥ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸è¿è´¯ç°è±¡ã€‚è¯¥å‘ç°ä¸ºç†è§£ LLMs çš„å†…éƒ¨æ¨ç†åŠ¨åŠ›å­¦æä¾›äº†å®è¯ä¾æ®ï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥åœ¨å¤šæ¨¡æ€æ¨ç†ï¼ˆmultimodal reasoningï¼‰å’Œå¤æ‚ç»“æ„åŒ–é€»è¾‘æ¨å¯¼é¢†åŸŸçš„ç ”ç©¶å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 0 figures, accepted by 2025 IEEE international conference on artificial intelligence testing (AITest)",
      "pdf_url": "https://arxiv.org/pdf/2506.23128v1",
      "published_date": "2025-06-29 07:37:49 UTC",
      "updated_date": "2025-06-29 07:37:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:40.070369+00:00"
    },
    {
      "arxiv_id": "2506.23127v1",
      "title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning",
      "title_zh": "é€šè¿‡å¼ºåŒ–å­¦ä¹ é‡Šæ”¾å¤§è¯­è¨€æ¨¡å‹çš„å…·èº«ä»»åŠ¡è§„åˆ’èƒ½åŠ›",
      "authors": [
        "Zhaoye Fei",
        "Li Ji",
        "Siyin Wang",
        "Junhao Shi",
        "Jingjing Gong",
        "Xipeng Qiu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they face significant challenges in embodied task planning scenarios that require continuous environmental understanding and action generation. Existing approaches generate open-loop action scripts based on static knowledge, making it difficult to learn causal relationships between actions and environmental feedback, particularly in partially observable environments. We introduce Embodied Planner-R1, a novel outcome-driven reinforcement learning framework that enables LLMs to develop interactive capabilities through autonomous exploration with minimal supervision. Our framework incorporates three key innovations: (1) Without human annotations, we employ pure reinforcement learning with group rollout, incorporating in-environment interaction through parallel exploration; (2) completion-driven sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient learning from grouped trajectories. Across two challenging text-based Embodied planning benchmarks, Embodied Planner-R1 achieves impressive completion rates of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a large margin, and suffers only a -3.66% drop in previously unseen environments, evidencing strong generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å…·èº«ä»»åŠ¡è§„åˆ’ (Embodied Task Planning) ä¸­éš¾ä»¥å¤„ç†è¿ç»­ç¯å¢ƒç†è§£å’ŒåŠ¨ä½œç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œæå‡ºäº† Embodied Planner-R1 æ¡†æ¶ã€‚è¿™æ˜¯ä¸€ä¸ªç»“æœé©±åŠ¨çš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æå°‘é‡çš„ç›‘ç£ä½¿æ¨¡å‹åœ¨è‡ªä¸»æ¢ç´¢ä¸­å…·å¤‡äº¤äº’èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ï¼ŒåŒ…æ‹¬æ— éœ€äººå·¥æ ‡æ³¨çš„çº¯å¼ºåŒ–å­¦ä¹ ä¸åˆ†ç»„å±•å¼€ (Group Rollout)ï¼Œé€šè¿‡å¹¶è¡Œæ¢ç´¢å®ç°ç¯å¢ƒå†…äº¤äº’ï¼Œä»¥åŠé‡‡ç”¨å®Œæˆé©±åŠ¨çš„ç¨€ç–å¥–åŠ±æœºåˆ¶å’Œäº¤äº’ç­–ç•¥ä¼˜åŒ– (Interactive Policy Optimization, IPO) ä»¥å®ç°é«˜æ•ˆå­¦ä¹ ã€‚åœ¨ ALFWorld å’Œ ScienceWorld ä¸¤ä¸ªæŒ‘æˆ˜æ€§çš„å…·èº«è§„åˆ’åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEmbodied Planner-R1 åˆ†åˆ«è¾¾åˆ°äº† 97.78% å’Œ 79.92% çš„å®Œæˆç‡ï¼Œæ€§èƒ½å¤§å¹…è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æœªçŸ¥ç¯å¢ƒä¸­ä»…å‡ºç° 3.66% çš„æ€§èƒ½æ³¢åŠ¨ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ã€éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸‹çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ› (Generalization)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23127v1",
      "published_date": "2025-06-29 07:31:24 UTC",
      "updated_date": "2025-06-29 07:31:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:40.417371+00:00"
    },
    {
      "arxiv_id": "2506.23123v1",
      "title": "The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy",
      "title_zh": "åŸºç¡€æ¨¡å‹çš„ç¤¾ä¼šå½±å“ï¼šæ¨è¿›å¾ªè¯å¼äººå·¥æ™ºèƒ½æ”¿ç­–",
      "authors": [
        "Rishi Bommasani"
      ],
      "abstract": "Artificial intelligence is humanity's most promising technology because of the remarkable capabilities offered by foundation models. Yet, the same technology brings confusion and consternation: foundation models are poorly understood and they may precipitate a wide array of harms. This dissertation explains how technology and society coevolve in the age of AI, organized around three themes. First, the conceptual framing: the capabilities, risks, and the supply chain that grounds foundation models in the broader economy. Second, the empirical insights that enrich the conceptual foundations: transparency created via evaluations at the model level and indexes at the organization level. Finally, the transition from understanding to action: superior understanding of the societal impact of foundation models advances evidence-based AI policy. View together, this dissertation makes inroads into achieving better societal outcomes in the age of AI by building the scientific foundations and research-policy interface required for better AI governance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†Foundation Modelså¯¹ç¤¾ä¼šçš„å¹¿æ³›å½±å“ï¼Œæ—¨åœ¨é€šè¿‡æ„å»ºç§‘å­¦åŸºç¡€å’Œç ”ç©¶-æ”¿ç­–æ¥å£æ¥æ¨è¿›è¯æ®å¯¼å‘çš„AI Policyã€‚è®ºæ–‡å›´ç»•ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦å±•å¼€ï¼šé¦–å…ˆæ˜¯æ¦‚å¿µæ¡†æ¶ï¼Œåˆ†æäº†Foundation Modelsçš„èƒ½åŠ›ã€é£é™©åŠå…¶åœ¨å®è§‚ç»æµä¸­çš„ä¾›åº”é“¾å®šä½ï¼›å…¶æ¬¡æ˜¯ç»éªŒæ´å¯Ÿï¼Œé€šè¿‡æ¨¡å‹å±‚é¢çš„è¯„ä¼°å’Œç»„ç»‡å±‚é¢çš„æŒ‡æ•°æå‡æŠ€æœ¯é€æ˜åº¦ï¼›æœ€åæ˜¯è¡ŒåŠ¨è½¬åŒ–ï¼Œæ—¨åœ¨å°†å¯¹ç¤¾ä¼šå½±å“çš„æ·±åˆ»ç†è§£è½¬åŒ–ä¸ºæœ‰æ•ˆçš„æ²»ç†æªæ–½ã€‚è¯¥ç ”ç©¶é€šè¿‡å®Œå–„AI Governanceçš„ç†è®ºä¸å®è·µä½“ç³»ï¼Œä¸ºåœ¨äººå·¥æ™ºèƒ½æ—¶ä»£å®ç°æ›´å¥½çš„ç¤¾ä¼šç»“æœå¥ å®šäº†åŸºç¡€ï¼Œä¿ƒè¿›äº†æŠ€æœ¯ä¸ç¤¾ä¼šçš„ååŒæ¼”è¿›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "comment": "Stanford University PhD Dissertation of Rishi Bommasani (Department of Computer Science, 2025). Also available at https://purl.stanford.edu/zf669yy0336",
      "pdf_url": "https://arxiv.org/pdf/2506.23123v1",
      "published_date": "2025-06-29 07:16:48 UTC",
      "updated_date": "2025-06-29 07:16:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:43.641689+00:00"
    },
    {
      "arxiv_id": "2506.23121v3",
      "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation",
      "title_zh": "CRISP-SAM2ï¼šç»“åˆè·¨æ¨¡æ€äº¤äº’ä¸è¯­ä¹‰æç¤ºçš„ SAM2 å¤šå™¨å®˜åˆ†å‰²",
      "authors": [
        "Xinlei Yu",
        "Changmiao Wang",
        "Hui Jin",
        "Ahmed Elazab",
        "Gangyong Jia",
        "Xiang Wan",
        "Changqing Zou",
        "Ruiquan Ge"
      ],
      "abstract": "Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: https://github.com/YU-deep/CRISP_SAM2.git.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CRISP-SAM2ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºSAM2çš„æ–°å‹æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³Multi-Organ Segmentationé¢†åŸŸä¸­å­˜åœ¨çš„ç»†èŠ‚ä¸å‡†ç¡®ã€ä¾èµ–å‡ ä½•æç¤ºä»¥åŠç©ºé—´ä¿¡æ¯ä¸¢å¤±ç­‰æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥æ¸è¿›å¼äº¤å‰æ³¨æ„åŠ›äº¤äº’æœºåˆ¶(Progressive Cross-Attention Interaction)ï¼Œå°†è§†è§‰ä¸æ–‡æœ¬è¾“å…¥è½¬åŒ–ä¸ºè·¨æ¨¡æ€ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œå¹¶å°†å…¶æ³¨å…¥å›¾åƒç¼–ç å™¨ä»¥å¢å¼ºå¯¹è§†è§‰ä¿¡æ¯çš„ç»†èŠ‚ç†è§£ã€‚ä¸ºäº†æ¶ˆé™¤å¯¹å‡ ä½•æç¤º(Geometric Prompts)çš„ä¾èµ–ï¼Œç ”ç©¶è€…é‡‡ç”¨äº†è¯­ä¹‰æç¤ºç­–ç•¥(Semantic Prompting)æ›¿ä»£åŸå§‹çš„æç¤ºç¼–ç å™¨ï¼Œä»è€Œå¼ºåŒ–äº†å¯¹å›°éš¾ç›®æ ‡çš„æ„ŸçŸ¥ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜é›†æˆäº†åŸºäºç›¸ä¼¼åº¦æ’åºçš„è®°å¿†è‡ªæ›´æ–°ç­–ç•¥(Similarity-Sorting Self-Updating Strategy)å’Œæ©ç ç²¾ç»†åŒ–å¤„ç†(Mask-Refining Process)ï¼Œè¿›ä¸€æ­¥æå‡äº†åŒ»å­¦å½±åƒä¸­çš„å±€éƒ¨ç»†èŠ‚è¡¨ç°ã€‚åœ¨ä¸ƒä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCRISP-SAM2çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚å¤šå™¨å®˜åˆ†å‰²ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted By ACMMM25",
      "pdf_url": "https://arxiv.org/pdf/2506.23121v3",
      "published_date": "2025-06-29 07:05:27 UTC",
      "updated_date": "2025-07-14 02:03:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:52.312541+00:00"
    },
    {
      "arxiv_id": "2507.00075v3",
      "title": "Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap",
      "title_zh": "åŸºäºæ±‚è§£-éªŒè¯å·®è·çš„å¤§è¯­è¨€æ¨¡å‹è‡ªæˆ‘æ”¹è¿›è®­ç»ƒåŠ¨åŠ›å­¦ç†è®ºå»ºæ¨¡",
      "authors": [
        "Yifan Sun",
        "Yushan Liang",
        "Zhen Zhang",
        "Jiaye Teng"
      ],
      "abstract": "Self-improvement is among the most prominent techniques within the realm of large language models (LLM), aiming to enhance the LLM performance without relying on external data. Despite its significance, generally how LLM performances evolve during the self-improvement process remains underexplored. In this paper, we theoretically model the training dynamics of self-improvement via the concept of solver-verifier gap. This is inspired by the conjecture that the performance enhancement of self-improvement stems from the gap between LLM's solver capability and verifier capability. Based on the theoretical framework, we further show how to model the entire training trajectory. This framework allows quantifying the capability limit of self-improvement by fitting the theoretical model to the experiment results. We empirically validate the effectiveness of the theoretical framework on various LLMs and datasets. Beyond self-improvement, we extend our analysis to investigate how external data influences these dynamics within the framework. Notably, we find that under limited external data regimes, such external data can be utilized at any stage without significantly affecting final performances, which accords with the empirical observations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)çš„è‡ªæˆ‘æ”¹è¿›(Self-improvement)æŠ€æœ¯ï¼Œé€šè¿‡å¼•å…¥æ±‚è§£å™¨-éªŒè¯å™¨å·®è·(Solver-verifier gap)çš„æ¦‚å¿µï¼Œå¯¹LLMåœ¨æ— å¤–éƒ¨æ•°æ®æƒ…å†µä¸‹çš„è®­ç»ƒåŠ¨åŠ›å­¦è¿›è¡Œäº†ç†è®ºå»ºæ¨¡ã€‚è¯¥æ¨¡å‹åŸºäºä¸€ç§æ ¸å¿ƒæ¨æµ‹ï¼Œå³è‡ªæˆ‘æ”¹è¿›å¸¦æ¥çš„æ€§èƒ½æå‡æºäºLLMçš„æ±‚è§£èƒ½åŠ›(Solver capability)ä¸éªŒè¯èƒ½åŠ›(Verifier capability)ä¹‹é—´çš„å·®è·ã€‚åœ¨è¯¥ç†è®ºæ¡†æ¶ä¸‹ï¼Œç ”ç©¶è€…å±•ç¤ºäº†å¦‚ä½•å¯¹æ•´ä¸ªè®­ç»ƒè½¨è¿¹è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡å°†ç†è®ºæ¨¡å‹æ‹Ÿåˆè‡³å®éªŒç»“æœï¼Œå®ç°äº†å¯¹è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›æé™çš„é‡åŒ–ã€‚ç ”ç©¶åœ¨å¤šç§LLMså’Œæ•°æ®é›†ä¸Šå¯¹è¯¥ç†è®ºæ¡†æ¶çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†å®è¯éªŒè¯ã€‚æ­¤å¤–ï¼Œè¯¥åˆ†æè¿˜æ¢è®¨äº†å¤–éƒ¨æ•°æ®å¯¹è®­ç»ƒåŠ¨åŠ›å­¦çš„å½±å“ï¼Œå‘ç°åœ¨æœ‰é™å¤–éƒ¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ•°æ®åœ¨ä»»ä½•é˜¶æ®µå¼•å…¥å‡ä¸ä¼šæ˜¾è‘—æ”¹å˜æœ€ç»ˆæ€§èƒ½ã€‚è¿™ä¸€ç»“è®ºä¸å®è¯è§‚å¯Ÿç›¸ç¬¦ï¼Œä¸ºæ·±å…¥ç†è§£å’Œé¢„æµ‹LLMè‡ªæˆ‘æ”¹è¿›çš„æ¼”è¿›è¿‡ç¨‹æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.00075v3",
      "published_date": "2025-06-29 06:48:47 UTC",
      "updated_date": "2025-10-10 17:29:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:49.444656+00:00"
    },
    {
      "arxiv_id": "2506.23115v1",
      "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings",
      "title_zh": "MoCaï¼šæ¨¡æ€æ„ŸçŸ¥æŒç»­é¢„è®­ç»ƒåŠ©åŠ›æ„å»ºæ›´ä¼˜çš„åŒå‘å¤šæ¨¡æ€åµŒå…¥",
      "authors": [
        "Haonan Chen",
        "Hong Liu",
        "Yuping Luo",
        "Liang Wang",
        "Nan Yang",
        "Furu Wei",
        "Zhicheng Dou"
      ],
      "abstract": "Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MoCaï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)è½¬åŒ–ä¸ºé«˜æ•ˆåŒå‘å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œè§£å†³äº†å› æœæ³¨æ„åŠ›æœºåˆ¶(causal attention)åœ¨åµŒå…¥ä»»åŠ¡ä¸­çš„æ¬¡ä¼˜æ€§ä»¥åŠå¯¹é«˜è´¨é‡æ ‡æ³¨æ•°æ®è¿‡åº¦ä¾èµ–çš„éš¾é¢˜ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡æ¨¡æ€æ„ŸçŸ¥æŒç»­é¢„è®­ç»ƒ(Modality-aware Continual Pre-training)å¼•å…¥è”åˆé‡å»ºç›®æ ‡(joint reconstruction objective)ï¼Œå¯¹äº¤é”™çš„æ–‡æœ¬å’Œå›¾åƒè¾“å…¥è¿›è¡Œå»å™ªä»¥å¢å¼ºåŒå‘ä¸Šä¸‹æ–‡æ¨ç†ã€‚ç¬¬äºŒé˜¶æ®µåˆ™é‡‡ç”¨å¼‚æ„å¯¹æ¯”å¾®è°ƒ(Heterogeneous Contrastive Fine-tuning)ï¼Œåˆ©ç”¨è¶…è¶Šç®€å•å›¾åƒ-æè¿°å¯¹çš„å¤šæ ·åŒ–å¤šæ¨¡æ€æ•°æ®æ¥æå‡æ¨¡å‹çš„æ³›åŒ–ä¸å¯¹é½æ€§èƒ½ã€‚MoCaé€šè¿‡æŒç»­é¢„è®­ç»ƒæˆåŠŸå¼•å…¥äº†åŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶èƒ½åˆ©ç”¨æµ·é‡æ— æ ‡ç­¾æ•°æ®å®ç°æœ‰æ•ˆæ‰©å±•ï¼Œæ˜¾è‘—å¢å¼ºäº†è¡¨å¾çš„ç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼ŒMoCaåœ¨MMEBå’ŒViDoRe-v2åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ–°çš„æœ€å…ˆè¿›(state-of-the-art)ç»“æœï¼Œå¹¶å±•ç°å‡ºéšæ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ•°æ®å¢åŠ çš„å¼ºå¤§å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Homepage: https://haon-chen.github.io/MoCa/",
      "pdf_url": "https://arxiv.org/pdf/2506.23115v1",
      "published_date": "2025-06-29 06:41:00 UTC",
      "updated_date": "2025-06-29 06:41:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:55.735061+00:00"
    },
    {
      "arxiv_id": "2506.23107v1",
      "title": "Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦æ•æ‰äººç±»é£é™©åå¥½ï¼Ÿä¸€é¡¹è·¨æ–‡åŒ–ç ”ç©¶",
      "authors": [
        "Bing Song",
        "Jianing Liu",
        "Sisi Jian",
        "Chenyang Wu",
        "Vinayak Dixit"
      ],
      "abstract": "Large language models (LLMs) have made significant strides, extending their applications to dialogue systems, automated content creation, and domain-specific advisory tasks. However, as their use grows, concerns have emerged regarding their reliability in simulating complex decision-making behavior, such as risky decision-making, where a single choice can lead to multiple outcomes. This study investigates the ability of LLMs to simulate risky decision-making scenarios. We compare model-generated decisions with actual human responses in a series of lottery-based tasks, using transportation stated preference survey data from participants in Sydney, Dhaka, Hong Kong, and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk preferences were analyzed using the Constant Relative Risk Aversion (CRRA) framework. Results show that both models exhibit more risk-averse behavior than human participants, with o1-mini aligning more closely with observed human decisions. Further analysis of multilingual data from Nanjing and Hong Kong indicates that model predictions in Chinese deviate more from actual responses compared to English, suggesting that prompt language may influence simulation performance. These findings highlight both the promise and the current limitations of LLMs in replicating human-like risk behavior, particularly in linguistic and cultural settings.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨¡æ‹Ÿäººç±»é£é™©å†³ç­–è¡Œä¸ºæ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨æ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ä¸€ç³»åˆ—åŸºäºåšå¼ˆçš„ä»»åŠ¡ï¼Œå°† ChatGPT 4o å’Œ ChatGPT o1-mini ç”Ÿæˆçš„å†³ç­–ä¸æ¥è‡ªæ‚‰å°¼ã€è¾¾å¡ã€é¦™æ¸¯å’Œå—äº¬çš„çœŸå®äººç±»è°ƒæŸ¥æ•°æ®è¿›è¡Œäº†å¯¹æ¯”ï¼Œå¹¶åˆ©ç”¨ç­‰ç›¸å¯¹é£é™©åŒæ¶(Constant Relative Risk Aversion, CRRA)æ¡†æ¶åˆ†æäº†æ¨¡å‹çš„é£é™©åå¥½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸¤ç§æ¨¡å‹è¡¨ç°å‡ºçš„é£é™©åŒæ¶ç¨‹åº¦å‡é«˜äºäººç±»å‚ä¸è€…ï¼Œå…¶ä¸­ o1-mini çš„å†³ç­–ä¸äººç±»è§‚å¯Ÿå€¼æ›´ä¸ºæ¥è¿‘ã€‚é’ˆå¯¹å—äº¬å’Œé¦™æ¸¯çš„è·¨è¯­è¨€åˆ†æå‘ç°ï¼Œä½¿ç”¨ä¸­æ–‡æç¤ºè¯­(prompt language)æ—¶æ¨¡å‹çš„é¢„æµ‹åå·®æ¯”è‹±æ–‡æ›´å¤§ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº† LLMs åœ¨å¤åˆ¶äººç±»é£é™©è¡Œä¸ºæ–¹é¢çš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œç‰¹åˆ«å¼ºè°ƒäº†è¯­è¨€å’Œæ–‡åŒ–å› ç´ å¯¹æ¨¡æ‹Ÿæ€§èƒ½çš„æ˜¾è‘—å½±å“ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2506.23107v1",
      "published_date": "2025-06-29 06:16:57 UTC",
      "updated_date": "2025-06-29 06:16:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:23:54.913037+00:00"
    },
    {
      "arxiv_id": "2506.23101v1",
      "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship",
      "title_zh": "ä»ä¸ªä½“åˆ°äº’åŠ¨ï¼šç¤¾ä¼šå…³ç³»è§†è§’ä¸‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ€§åˆ«åè§åŸºå‡†è¯„ä¼°",
      "authors": [
        "Yue Xu",
        "Wenjie Wang"
      ],
      "abstract": "Multimodal large language models (MLLMs) have shown impressive capabilities across tasks involving both visual and textual modalities. However, growing concerns remain about their potential to encode and amplify gender bias, particularly in socially sensitive applications. Existing benchmarks predominantly evaluate bias in isolated scenarios, overlooking how bias may emerge subtly through interpersonal interactions. We fill this gap by going beyond single-entity evaluation and instead focusing on a deeper examination of relational and contextual gender bias in dual-individual interactions. We introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs through the lens of social relationships in generated narratives. Genres assesses gender bias through a dual-character profile and narrative generation task that captures rich interpersonal dynamics and supports a fine-grained bias evaluation suite across multiple dimensions. Experiments on both open- and closed-source MLLMs reveal persistent, context-sensitive gender biases that are not evident in single-character settings. Our findings underscore the importance of relationship-aware benchmarks for diagnosing subtle, interaction-driven gender bias in MLLMs and provide actionable insights for future bias mitigation.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) çš„ç°æœ‰è¯„æµ‹åŸºå‡†ä¸»è¦é›†ä¸­åœ¨å­¤ç«‹åœºæ™¯ï¼Œå¿½ç•¥äº†äººé™…äº’åŠ¨ä¸­æ½œåœ¨çš„æ€§åˆ«åè§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº† Genresï¼Œè¿™æ˜¯ä¸€ä¸ªä»ç¤¾ä¼šå…³ç³»è§†è§’è¯„ä¼° MLLMs æ€§åˆ«åè§çš„æ–°å‹åŸºå‡†ã€‚Genres é€šè¿‡åŒè§’è‰²ç”»åƒå’Œå™äº‹ç”Ÿæˆä»»åŠ¡æ•æ‰ä¸°å¯Œçš„äººé™…åŠ¨æ€ï¼Œå¹¶æä¾›äº†ä¸€å¥—æ¶µç›–å¤šä¸ªç»´åº¦çš„ç»†ç²’åº¦åè§è¯„ä¼°ä½“ç³»ã€‚é’ˆå¯¹å¼€æºå’Œé—­æº MLLMs çš„å®éªŒå‘ç°ï¼Œæ¨¡å‹ä¸­æ™®éå­˜åœ¨æŒç»­ä¸”å—è¯­å¢ƒå½±å“çš„æ€§åˆ«åè§ï¼Œè€Œè¿™äº›åè§åœ¨ä¼ ç»Ÿçš„å•è§’è‰²è®¾å®šä¸­å¾€å¾€æ— æ³•è¢«è§‚æµ‹åˆ°ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å…³ç³»æ„ŸçŸ¥ (relationship-aware) è¯„æµ‹åŸºå‡†åœ¨è¯Šæ–­ç”±äº’åŠ¨é©±åŠ¨çš„å¾®å¦™æ€§åˆ«åè§æ–¹é¢çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„åè§ç¼“è§£å·¥ä½œæä¾›äº†å®è¯æ”¯æŒä¸æ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23101v1",
      "published_date": "2025-06-29 06:03:21 UTC",
      "updated_date": "2025-06-29 06:03:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:24:02.761675+00:00"
    },
    {
      "arxiv_id": "2507.01990v1",
      "title": "Integrating Large Language Models in Financial Investments and Market Analysis: A Survey",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨é‡‘èæŠ•èµ„ä¸å¸‚åœºåˆ†æä¸­çš„é›†æˆåº”ç”¨ç»¼è¿°",
      "authors": [
        "Sedigheh Mahdavi",
        "Jiating",
        "Chen",
        "Pradeep Kumar Joshi",
        "Lina Huertas Guativa",
        "Upmanyu Singh"
      ],
      "abstract": "Large Language Models (LLMs) have been employed in financial decision making, enhancing analytical capabilities for investment strategies. Traditional investment strategies often utilize quantitative models, fundamental analysis, and technical indicators. However, LLMs have introduced new capabilities to process and analyze large volumes of structured and unstructured data, extract meaningful insights, and enhance decision-making in real-time. This survey provides a structured overview of recent research on LLMs within the financial domain, categorizing research contributions into four main frameworks: LLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and Adaptation Approaches, and Agent-Based Architectures. This study provides a structured review of recent LLMs research on applications in stock selection, risk assessment, sentiment analysis, trading, and financial forecasting. By reviewing the existing literature, this study highlights the capabilities, challenges, and potential directions of LLMs in financial markets.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡ç³»ç»Ÿåœ°å›é¡¾äº†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨é‡‘èæŠ•èµ„å’Œå¸‚åœºåˆ†æä¸­çš„é›†æˆä¸åº”ç”¨ã€‚ä¸ä¾èµ–å®šé‡æ¨¡å‹ã€åŸºæœ¬é¢åˆ†æå’ŒæŠ€æœ¯æŒ‡æ ‡çš„ä¼ ç»ŸæŠ•èµ„ç­–ç•¥ä¸åŒï¼ŒLLMså‡­å€Ÿå¤„ç†æµ·é‡ç»“æ„åŒ–ä¸éç»“æ„åŒ–æ•°æ®çš„èƒ½åŠ›ï¼Œä¸ºæŠ•èµ„å†³ç­–æä¾›äº†æ˜¾è‘—å¢å¼ºçš„å®æ—¶æ´å¯Ÿä¸åˆ†æã€‚æœ¬æ–‡å°†ç°æœ‰çš„ç ”ç©¶è´¡çŒ®å½’çº³ä¸ºLLM-based Frameworks and Pipelinesã€Hybrid Integration Methodsã€Fine-Tuning and Adaptation Approachesä»¥åŠAgent-Based Architectureså››ç§ä¸»è¦æ¡†æ¶ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨äº†LLMsåœ¨è‚¡ç¥¨é€‰æ‹©(Stock Selection)ã€é£é™©è¯„ä¼°(Risk Assessment)ã€æƒ…ç»ªåˆ†æ(Sentiment Analysis)ã€äº¤æ˜“(Trading)åŠé‡‘èé¢„æµ‹(Financial Forecasting)ç­‰å…·ä½“é¢†åŸŸçš„åº”ç”¨ç°çŠ¶ã€‚é€šè¿‡å¯¹ç°æœ‰æ–‡çŒ®çš„å…¨é¢æ€»ç»“ï¼Œè¯¥ç ”ç©¶çªå‡ºäº†LLMsåœ¨é‡‘èå¸‚åœºä¸­çš„æ ¸å¿ƒèƒ½åŠ›ã€é¢ä¸´çš„ç°å®æŒ‘æˆ˜ä»¥åŠæœªæ¥çš„æ½œåœ¨å‘å±•æ–¹å‘ã€‚",
      "categories": [
        "q-fin.GN",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.01990v1",
      "published_date": "2025-06-29 05:25:31 UTC",
      "updated_date": "2025-06-29 05:25:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:24:02.893595+00:00"
    },
    {
      "arxiv_id": "2506.23094v1",
      "title": "TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure",
      "title_zh": "TOMIï¼šé¢å‘å…¨æ›²ç»“æ„å¤šè½¨åˆ›ä½œçš„éŸ³ä¹åˆ›æ„è½¬æ¢ä¸ç»„ç»‡",
      "authors": [
        "Qi He",
        "Gus Xia",
        "Ziyu Wang"
      ],
      "abstract": "Hierarchical planning is a powerful approach to model long sequences structurally. Aside from considering hierarchies in the temporal structure of music, this paper explores an even more important aspect: concept hierarchy, which involves generating music ideas, transforming them, and ultimately organizing them--across musical time and space--into a complete composition. To this end, we introduce TOMI (Transforming and Organizing Music Ideas) as a novel approach in deep music generation and develop a TOMI-based model via instruction-tuned foundation LLM. Formally, we represent a multi-track composition process via a sparse, four-dimensional space characterized by clips (short audio or MIDI segments), sections (temporal positions), tracks (instrument layers), and transformations (elaboration methods). Our model is capable of generating multi-track electronic music with full-song structure, and we further integrate the TOMI-based model with the REAPER digital audio workstation, enabling interactive human-AI co-creation. Experimental results demonstrate that our approach produces higher-quality electronic music with stronger structural coherence compared to baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TOMI (Transforming and Organizing Music Ideas)ï¼Œä¸€ç§ç”¨äºåˆ›ä½œå…·æœ‰å®Œæ•´å…¨æ›²ç»“æ„çš„å¤šè½¨éŸ³ä¹çš„æ–°å‹æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ¨¡å‹åŸºäºç»è¿‡æŒ‡ä»¤å¾®è°ƒ (instruction-tuned) çš„åŸºç¡€å¤§è¯­è¨€æ¨¡å‹ (LLM)ï¼Œé€šè¿‡æ¢ç´¢æ¦‚å¿µå±‚æ¬¡ç»“æ„ (concept hierarchy) æ¥å®ç°éŸ³ä¹åˆ›æ„çš„ç”Ÿæˆã€å˜æ¢ä¸ç»„ç»‡ã€‚ç ”ç©¶è€…å°†å¤šè½¨åˆ›ä½œè¿‡ç¨‹å½¢å¼åŒ–ä¸ºä¸€ä¸ªç”±ç‰‡æ®µ (clips)ã€æ®µè½ (sections)ã€è½¨é“ (tracks) å’Œå˜æ¢ (transformations) æ„æˆçš„å››ç»´ç¨€ç–ç©ºé—´ã€‚TOMI èƒ½å¤Ÿç”Ÿæˆç»“æ„å®Œæ•´çš„å¤šè½¨ç”µå­éŸ³ä¹ï¼Œå¹¶å·²é›†æˆè‡³ REAPER æ•°å­—éŸ³é¢‘å·¥ä½œç«™ (DAW)ï¼Œæ”¯æŒé«˜æ•ˆçš„äººæœºåä½œåˆ›ä½œã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡å’Œç»“æ„è¿è´¯æ€§ (structural coherence) æ–¹é¢å‡ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "9 pages, 4 figures, 2 tables. To be published in ISMIR 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.23094v1",
      "published_date": "2025-06-29 05:15:41 UTC",
      "updated_date": "2025-06-29 05:15:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:24:02.179064+00:00"
    },
    {
      "arxiv_id": "2506.23085v3",
      "title": "Enhancing Live Broadcast Engagement: A Multi-modal Approach to Short Video Recommendations Using MMGCN and User Preferences",
      "title_zh": "æå‡ç›´æ’­å‚ä¸åº¦ï¼šåŸºäº MMGCN ä¸ç”¨æˆ·åå¥½çš„å¤šæ¨¡æ€çŸ­è§†é¢‘æ¨èæ–¹æ³•",
      "authors": [
        "Saeid Aghasoleymani Najafabadi",
        "Elaheh Nabavi Nia"
      ],
      "abstract": "The purpose of this paper is to explore a multi-modal approach to enhancing live broadcast engagement by developing a short video recommendation system that incorporates Multi-modal Graph Convolutional Networks (MMGCN) with user preferences. To provide personalized recommendations tailored to individual interests, the proposed system considers user interaction data, video content features, and contextual information. With the aid of a hybrid approach combining collaborative filtering and content-based filtering techniques, the system can capture nuanced relationships between users, video attributes, and engagement patterns. Three datasets are used to evaluate the effectiveness of the system: Kwai, TikTok, and MovieLens. Compared to baseline models, such as DeepFM, Wide & Deep, LightGBM, and XGBoost, the proposed MMGCN-based model shows superior performance. A notable feature of the proposed model is that it outperforms all baseline methods in capturing diverse user preferences and making accurate, personalized recommendations, resulting in a Kwai F1 score of 0.574, a Tiktok F1 score of 0.506, and a MovieLens F1 score of 0.197. We emphasize the importance of multi-modal integration and user-centric approaches in advancing recommender systems, emphasizing the role they play in enhancing content discovery and audience interaction on live broadcast platforms.",
      "tldr_zh": "æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€æ–¹æ³•å¢å¼ºç›´æ’­äº’åŠ¨æ€§ï¼Œå¼€å‘äº†ä¸€å¥—ç»“åˆå¤šæ¨¡æ€å›¾å·ç§¯ç½‘ç»œ (Multi-modal Graph Convolutional Networks, MMGCN) ä¸ç”¨æˆ·åå¥½çš„çŸ­è§†é¢‘æ¨èç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ•´åˆäº†ç”¨æˆ·äº¤äº’æ•°æ®ã€è§†é¢‘å†…å®¹ç‰¹å¾åŠä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€šè¿‡ç»“åˆåä½œè¿‡æ»¤ (collaborative filtering) å’ŒåŸºäºå†…å®¹çš„è¿‡æ»¤ (content-based filtering) çš„æ··åˆæ–¹æ³•ï¼Œç²¾å‡†æ•æ‰ç”¨æˆ·ã€è§†é¢‘å±æ€§ä¸å‚ä¸æ¨¡å¼ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚åœ¨ Kwaiã€TikTok å’Œ MovieLens æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ•æ‰å¤šæ ·åŒ–ç”¨æˆ·åå¥½æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ F1 score åˆ†åˆ«è¾¾åˆ° 0.574ã€0.506 å’Œ 0.197ã€‚å®éªŒç»“æœè¯å®ï¼Œè¯¥ MMGCN æ¨¡å‹åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äº DeepFMã€Wide & Deepã€LightGBM å’Œ XGBoost ç­‰åŸºçº¿æ–¹æ³•ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡æ€é›†æˆ (multi-modal integration) å’Œä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æ–¹æ³•åœ¨é©±åŠ¨å†…å®¹å‘ç°å’Œæå‡ç›´æ’­å¹³å°å—ä¼—å‚ä¸åº¦ä¸­çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23085v3",
      "published_date": "2025-06-29 04:50:52 UTC",
      "updated_date": "2025-09-27 00:20:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:24:14.708599+00:00"
    },
    {
      "arxiv_id": "2506.23080v2",
      "title": "AI's Euclid's Elements Moment: From Language Models to Computable Thought",
      "title_zh": "äººå·¥æ™ºèƒ½çš„ã€Šå‡ ä½•åŸæœ¬ã€‹æ—¶åˆ»ï¼šä»è¯­è¨€æ¨¡å‹åˆ°å¯è®¡ç®—æ€ç»´",
      "authors": [
        "Xinmin Fang",
        "Lingfeng Tao",
        "Zhengxiong Li"
      ],
      "abstract": "This paper presents a comprehensive five-stage evolutionary framework for understanding the development of artificial intelligence, arguing that its trajectory mirrors the historical progression of human cognitive technologies. We posit that AI is advancing through distinct epochs, each defined by a revolutionary shift in its capacity for representation and reasoning, analogous to the inventions of cuneiform, the alphabet, grammar and logic, mathematical calculus, and formal logical systems. This \"Geometry of Cognition\" framework moves beyond mere metaphor to provide a systematic, cross-disciplinary model that not only explains AI's past architectural shifts-from expert systems to Transformers-but also charts a concrete and prescriptive path forward. Crucially, we demonstrate that this evolution is not merely linear but reflexive: as AI advances through these stages, the tools and insights it develops create a feedback loop that fundamentally reshapes its own underlying architecture. We are currently transitioning into a \"Metalinguistic Moment,\" characterized by the emergence of self-reflective capabilities like Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the \"Mathematical Symbolism Moment\" and the \"Formal Logic System Moment,\" will be defined by the development of a computable calculus of thought, likely through neuro-symbolic architectures and program synthesis, culminating in provably aligned and reliable AI that reconstructs its own foundational representations. This work serves as the methodological capstone to our trilogy, which previously explored the economic drivers (\"why\") and cognitive nature (\"what\") of AI. Here, we address the \"how,\" providing a theoretical foundation for future research and offering concrete, actionable strategies for startups and developers aiming to build the next generation of intelligent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŒ…å«äº”ä¸ªé˜¶æ®µçš„AIæ¼”è¿›æ¡†æ¶ï¼Œè®¤ä¸ºäººå·¥æ™ºèƒ½çš„å‘å±•è½¨è¿¹æ˜ å°„äº†äººç±»è®¤çŸ¥æŠ€æœ¯ä»æ¥”å½¢æ–‡å­—åˆ°å½¢å¼é€»è¾‘ç³»ç»Ÿçš„å†å²æ¼”å˜ã€‚è¯¥â€œè®¤çŸ¥å‡ ä½•(Geometry of Cognition)â€æ¡†æ¶ä¸ºAIä»ä¸“å®¶ç³»ç»Ÿ(expert systems)åˆ°Transformeræ¶æ„çš„å†å²æ¼”å˜åŠæœªæ¥è·¯å¾„æä¾›äº†ç³»ç»Ÿæ€§çš„è·¨å­¦ç§‘æ¨¡å‹ã€‚ç ”ç©¶å¼ºè°ƒAIçš„è¿›åŒ–å…·æœ‰è‡ªåæ€§(reflexive)ï¼Œå³å…¶å‘å±•è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å·¥å…·å’Œæ´å¯Ÿä¼šé€šè¿‡åé¦ˆå¾ªç¯é‡å¡‘è‡ªèº«çš„åº•å±‚æ¶æ„ã€‚ç›®å‰AIæ­£å¤„äºä»¥é“¾å¼æ€ç»´(Chain-of-Thought)æç¤ºå’Œå®ªæ³•AI(Constitutional AI)ä¸ºç‰¹å¾çš„â€œå…ƒè¯­è¨€æ—¶åˆ»(Metalinguistic Moment)â€ã€‚æœªæ¥çš„æ¼”è¿›å°†é€šè¿‡ç¥ç»ç¬¦å·æ¶æ„(neuro-symbolic architectures)å’Œç¨‹åºåˆæˆ(program synthesis)è¿›å…¥â€œæ•°å­¦ç¬¦å·â€ä¸â€œå½¢å¼é€»è¾‘ç³»ç»Ÿâ€æ—¶åˆ»ï¼Œæ—¨åœ¨å¼€å‘å‡ºå¯è®¡ç®—çš„æ€ç»´å¾®ç§¯åˆ†ã€‚è¿™é¡¹å·¥ä½œæœ€ç»ˆç›®æ ‡æ˜¯å®ç°å¯è¯æ˜å¯¹é½ä¸”å¯é çš„AIï¼Œå¹¶ä¸ºå¼€å‘ä¸‹ä¸€ä»£æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ç†è®ºåŸºç¡€å’Œå…·ä½“çš„å¯æ“ä½œç­–ç•¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23080v2",
      "published_date": "2025-06-29 04:14:19 UTC",
      "updated_date": "2025-07-10 09:48:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:24:17.502750+00:00"
    },
    {
      "arxiv_id": "2506.23068v3",
      "title": "Curious Causality-Seeking Agents Learn Meta Causal World",
      "title_zh": "æ¢å¯»å› æœçš„å¥½å¥‡æ™ºèƒ½ä½“å­¦ä¹ å…ƒå› æœä¸–ç•Œ",
      "authors": [
        "Zhiyu Zhao",
        "Haoxuan Li",
        "Haifeng Zhang",
        "Jun Wang",
        "Francesco Faccio",
        "JÃ¼rgen Schmidhuber",
        "Mengyue Yang"
      ],
      "abstract": "When building a world model, a common assumption is that the environment has a single, unchanging underlying causal rule, like applying Newton's laws to every situation. In reality, what appears as a drifting causal mechanism is often the manifestation of a fixed underlying mechanism seen through a narrow observational window. This brings about a problem that, when building a world model, even subtle shifts in policy or environment states can alter the very observed causal mechanisms. In this work, we introduce the \\textbf{Meta-Causal Graph} as world models, a minimal unified representation that efficiently encodes the transformation rules governing how causal structures shift across different latent world states. A single Meta-Causal Graph is composed of multiple causal subgraphs, each triggered by meta state, which is in the latent state space. Building on this representation, we introduce a \\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta states that trigger each subgraph, (2) discover the corresponding causal relationships by agent curiosity-driven intervention policy, and (3) iteratively refine the Meta-Causal Graph through ongoing curiosity-driven exploration and agent experiences. Experiments on both synthetic tasks and a challenging robot arm manipulation task demonstrate that our method robustly captures shifts in causal dynamics and generalizes effectively to previously unseen contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸–ç•Œæ¨¡å‹ä¸­å› æœæœºåˆ¶éšç¯å¢ƒçŠ¶æ€å˜åŒ–çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†Meta-Causal Graphä½œä¸ºä¸€ç§ç»Ÿä¸€çš„è¡¨ç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨é«˜æ•ˆç¼–ç å› æœç»“æ„åœ¨ä¸åŒæ½œä¸–ç•ŒçŠ¶æ€é—´çš„å˜æ¢è§„åˆ™ã€‚Meta-Causal Graphç”±å¤šä¸ªç”±meta stateè§¦å‘çš„å› æœå­å›¾ç»„æˆï¼Œèƒ½å¤Ÿæ•æ‰å¤æ‚çš„å› æœåŠ¨æ€åç§»ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†Causality-Seeking Agentï¼Œè¯¥æ™ºèƒ½ä½“é€šè¿‡å¥½å¥‡å¿ƒé©±åŠ¨(curiosity-driven)çš„å¹²é¢„ç­–ç•¥æ¥è¯†åˆ«è§¦å‘å­å›¾çš„meta stateï¼Œå¹¶å‘ç°ç›¸åº”çš„å› æœå…³ç³»ã€‚é€šè¿‡æŒç»­çš„æ¢ç´¢å’Œç»éªŒç§¯ç´¯ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–Meta-Causal Graphã€‚åœ¨åˆæˆä»»åŠ¡å’Œæœºå™¨äººæ‰‹è‡‚æ“çºµä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½ç¨³å¥åœ°æ•æ‰å› æœåŠ¨åŠ›å­¦å˜åŒ–ï¼Œå¹¶åœ¨æœªè§è¿‡çš„è¯­å¢ƒä¸­å®ç°æœ‰æ•ˆçš„æ³›åŒ–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.23068v3",
      "published_date": "2025-06-29 03:05:25 UTC",
      "updated_date": "2025-10-26 03:16:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:24:22.164158+00:00"
    },
    {
      "arxiv_id": "2506.23055v1",
      "title": "Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis",
      "title_zh": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹å¯¹äººç±»å¿ƒç†å­¦æ¦‚å¿µçš„å†…åŒ–ç¨‹åº¦ï¼šä¸€é¡¹åˆæ­¥åˆ†æ",
      "authors": [
        "Hiro Taiyo Hamada",
        "Ippei Fujisawa",
        "Genji Kawakita",
        "Yuki Yamada"
      ],
      "abstract": "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities in producing human-like text. However, it is unclear how accurately these models internalize concepts that shape human thought and behavior. Here, we developed a quantitative framework to assess concept alignment between LLMs and human psychological dimensions using 43 standardized psychological questionnaires, selected for their established validity in measuring distinct psychological constructs. Our method evaluates how accurately language models reconstruct and classify questionnaire items through pairwise similarity analysis. We compared resulting cluster structures with the original categorical labels using hierarchical clustering. A GPT-4 model achieved superior classification accuracy (66.2\\%), significantly outperforming GPT-3.5 (55.9\\%) and BERT (48.1\\%), all exceeding random baseline performance (31.9\\%). We also demonstrated that the estimated semantic similarity from GPT-4 is associated with Pearson's correlation coefficients of human responses in multiple psychological questionnaires. This framework provides a novel approach to evaluate the alignment of the human-LLM concept and identify potential representational biases. Our findings demonstrate that modern LLMs can approximate human psychological constructs with measurable accuracy, offering insights for developing more interpretable AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå®šé‡æ¡†æ¶ï¼Œæ—¨åœ¨è¡¡é‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•å†…åŒ–å½±å“äººç±»æ€ç»´å’Œè¡Œä¸ºçš„å¿ƒç†å­¦æ¦‚å¿µã€‚ä½œè€…é‡‡ç”¨äº†43ä¸ªæ ‡å‡†åŒ–çš„å¿ƒç†å­¦é—®å·ï¼ˆpsychological questionnairesï¼‰ï¼Œé€šè¿‡æˆå¯¹ç›¸ä¼¼æ€§åˆ†æï¼ˆpairwise similarity analysisï¼‰å’Œå±‚æ¬¡èšç±»ï¼ˆhierarchical clusteringï¼‰æ¥è¯„ä¼°æ¨¡å‹å¯¹é—®å·æ¡ç›®çš„é‡æ„ä¸åˆ†ç±»ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4åœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº†66.2%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºGPT-3.5å’ŒBERTï¼Œä¸”æ‰€æœ‰æ¨¡å‹è¡¨ç°å‡è¿œè¶…éšæœºåŸºçº¿ã€‚ç ”ç©¶è¿˜å‘ç°ï¼ŒGPT-4ä¼°ç®—çš„è¯­ä¹‰ç›¸ä¼¼åº¦ä¸å¤šä»½é—®å·ä¸­äººç±»çœŸå®ååº”çš„Pearson's correlation coefficientså…·æœ‰ç›¸å…³æ€§ã€‚è¯¥æ¡†æ¶ä¸ºè¯„ä¼°äººç±»ä¸æ¨¡å‹é—´çš„æ¦‚å¿µä¸€è‡´æ€§ï¼ˆconcept alignmentï¼‰å¹¶è¯†åˆ«æ½œåœ¨çš„è¡¨å¾åè§ï¼ˆrepresentational biasesï¼‰æä¾›äº†åˆ›æ–°é€”å¾„ã€‚ç ”ç©¶å‘ç°è¯æ˜äº†ç°ä»£LLMsèƒ½å¤Ÿä»¥å¯è¡¡é‡çš„å‡†ç¡®æ€§è¿‘ä¼¼äººç±»çš„å¿ƒç†æ„å¿µï¼Œä¸ºå¼€å‘æ›´å…·å¯è§£é‡Šæ€§çš„AIç³»ç»Ÿæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23055v1",
      "published_date": "2025-06-29 01:56:56 UTC",
      "updated_date": "2025-06-29 01:56:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:24:30.090976+00:00"
    },
    {
      "arxiv_id": "2506.23049v1",
      "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks",
      "title_zh": "AURAï¼šé¢å‘è¯­éŸ³é©±åŠ¨ä»»åŠ¡çš„ç†è§£ã€æ¨ç†ä¸è‡ªåŠ¨åŒ–å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“",
      "authors": [
        "Leander Melroy Maben",
        "Gayathri Ganesh Lakshmy",
        "Srijith Radhakrishnan",
        "Siddhant Arora",
        "Shinji Watanabe"
      ],
      "abstract": "Despite advances in language and speech technologies, no open-source system enables full speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and Automated Tool Use), the first open-source, speech-native assistant capable of completing complex, goal-driven tasks through dynamic tool invocation and multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a cascaded pipeline and supports tools such as calendar booking, contact lookup, web search, and email. Its modular design allows easy integration of new tools using natural language prompts and action classes. On VoiceBench, AURA scores 92.75% on OpenBookQA-outperforming all open-weight systems and nearing GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems. Human evaluation shows 90% task success on complex, multi-turn speech tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† AURA (Agent for Understanding, Reasoning, and Automated Tool Use)ï¼Œè¿™æ˜¯é¦–ä¸ªå…·å¤‡åŠ¨æ€å·¥å…·è°ƒç”¨å’Œå¤šè½®å¯¹è¯èƒ½åŠ›çš„å¼€æºè¯­éŸ³åŸç”Ÿ (speech-native) åŠ©æ‰‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡çº§è”ç®¡çº¿ (cascaded pipeline) æ•´åˆäº†å¼€æºæƒé‡ (open-weight) çš„ ASRã€TTS å’Œ LLMsï¼Œèƒ½å¤Ÿè‡ªä¸»å®Œæˆæ—¥å†é¢„è®¢ã€è”ç³»äººæŸ¥è¯¢ã€ç½‘é¡µæœç´¢åŠé‚®ä»¶å‘é€ç­‰å¤æ‚ä»»åŠ¡ã€‚å…¶æ¨¡å—åŒ–è®¾è®¡å…è®¸å¼€å‘è€…é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå’ŒåŠ¨ä½œç±» (action classes) è½»æ¾æ‰©å±•æ–°åŠŸèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAURA åœ¨ VoiceBench æµ‹è¯•ä¸­ä»¥ 92.75% çš„å‡†ç¡®ç‡æ¥è¿‘ GPT-4o çš„è¡¨ç°ï¼Œä¸”åœ¨ AlpacaEval ä¸Šå…·æœ‰ç«äº‰åŠ›ã€‚äººå·¥è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†å…¶åœ¨å¤æ‚å¤šè½®è¯­éŸ³ä»»åŠ¡ä¸­æ‹¥æœ‰ 90% çš„æé«˜æˆåŠŸç‡ï¼Œä¸ºæ„å»ºå…¨åŠŸèƒ½çš„è¯­éŸ³æ™ºèƒ½ä½“æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.23049v1",
      "published_date": "2025-06-29 01:13:15 UTC",
      "updated_date": "2025-06-29 01:13:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:24:31.847227+00:00"
    },
    {
      "arxiv_id": "2506.23046v3",
      "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions",
      "title_zh": "SoMi-ToMï¼šå…·èº«ç¤¾äº¤äº’åŠ¨ä¸­çš„å¤šè§†è§’å¿ƒæ™ºç†è®ºè¯„ä¼°",
      "authors": [
        "Xianzhe Fan",
        "Xuhui Zhou",
        "Chuanyang Jin",
        "Kolby Nottingham",
        "Hao Zhu",
        "Maarten Sap"
      ],
      "abstract": "Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model's ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1% in first-person evaluation and 26.4% in third-person evaluation. This indicates that future LVLMs need to further improve their ToM capabilities in embodied, complex social interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SoMi-ToM è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¿ƒç†ç†è®º(Theory of Mind, ToM)åŸºå‡†å¤šé™äºé™æ€æ–‡æœ¬åœºæ™¯ï¼Œéš¾ä»¥åæ˜ çœŸå®ä¸–ç•Œå…·èº«ç¤¾äº¤äº’åŠ¨çš„é—®é¢˜ã€‚è¯¥åŸºå‡†åŸºäº SoMi äº¤äº’ç¯å¢ƒç”Ÿæˆçš„ä¸°å¯Œå¤šæ¨¡æ€æ•°æ®ï¼Œæ”¯æŒåœ¨å…·èº«å¤šæ™ºèƒ½ä½“å¤æ‚ç¤¾äº¤äº¤äº’ä¸­å¯¹å¤šè§†è§’ ToM è¿›è¡Œè¯„ä¼°ã€‚å…¶è¯„æµ‹æ¡†æ¶æ¶µç›–äº†ç¬¬ä¸€äººç§°è§†è§’(first-person evaluation)çš„å®æ—¶ä¸»è§‚çŠ¶æ€æ¨æ–­ï¼Œä»¥åŠç¬¬ä¸‰äººç§°è§†è§’(third-person evaluation)çš„å®¢è§‚å…¨å±€ç›®æ ‡ä¸è¡Œä¸ºæ¨æ–­ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å«è§†é¢‘ã€å›¾åƒåŠ 1225 é“ä¸“å®¶æ ‡æ³¨è¯•é¢˜çš„æŒ‘æˆ˜æ€§æ•°æ®é›†ï¼Œå¹¶å¯¹äººç±»ä¸å¤šç§æœ€å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)è¿›è¡Œäº†ç³»ç»Ÿå¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLVLMs åœ¨ç¬¬ä¸€äººç§°å’Œç¬¬ä¸‰äººç§°è¯„ä¼°ä¸­çš„å‡†ç¡®ç‡åˆ†åˆ«æ¯”äººç±»è½å 40.1% å’Œ 26.4%ã€‚è¿™è¡¨æ˜ç›®å‰çš„ LVLMs åœ¨å¤„ç†å…·èº«å¤æ‚ç¤¾äº¤äº¤äº’ä¸­çš„å¿ƒç†ç†è®ºèƒ½åŠ›æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼ŒäºŸå¾…è¿›ä¸€æ­¥æå‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.23046v3",
      "published_date": "2025-06-29 00:54:13 UTC",
      "updated_date": "2025-12-15 04:45:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:24:32.114605+00:00"
    },
    {
      "arxiv_id": "2506.23044v2",
      "title": "Ovis-U1 Technical Report",
      "title_zh": "Ovis-U1 æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Guo-Hua Wang",
        "Shanshan Zhao",
        "Xinjie Zhang",
        "Liangfu Cao",
        "Pengxin Zhan",
        "Lunhao Duan",
        "Shiyin Lu",
        "Minghao Fu",
        "Xiaohao Chen",
        "Jianshan Zhao",
        "Yang Li",
        "Qing-Guo Chen"
      ],
      "abstract": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.",
      "tldr_zh": "è¯¥æŠ¥å‘Šä»‹ç»äº†Ovis-U1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰30äº¿å‚æ•°çš„ç»Ÿä¸€æ¨¡å‹ï¼Œé›†æˆäº†å¤šæ¨¡æ€ç†è§£(multimodal understanding)ã€æ–‡æœ¬ç”Ÿæˆå›¾åƒ(text-to-image generation)å’Œå›¾åƒç¼–è¾‘(image editing)èƒ½åŠ›ã€‚åœ¨Ovisç³»åˆ—åŸºç¡€ä¸Šï¼ŒOvis-U1å¼•å…¥äº†åŸºäºæ‰©æ•£çš„è§†è§‰è§£ç å™¨(diffusion-based visual decoder)å’ŒåŒå‘ä»¤ç‰Œç²¾ç‚¼å™¨(bidirectional token refiner)ï¼Œå®ç°äº†ä¸GPT-4oç›¸å½“çš„å›¾åƒç”Ÿæˆæ•ˆæœã€‚è¯¥ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ä»è¯­è¨€æ¨¡å‹(language model)å¼€å§‹çš„æ–°å‹ç»Ÿä¸€è®­ç»ƒ(unified training)æ–¹æ³•ï¼Œå®éªŒè¯æ˜è¿™ç§æ•´åˆç†è§£ä¸ç”Ÿæˆä»»åŠ¡çš„ç­–ç•¥ä¼˜äºå•ä¸€ä»»åŠ¡è®­ç»ƒã€‚Ovis-U1åœ¨OpenCompasså¤šæ¨¡æ€å­¦æœ¯åŸºå‡†ä¸Šå–å¾—äº†69.6åˆ†ï¼Œè¶…è¶Šäº†Ristretto-3Bå’ŒSAIL-VL-1.5-2Bç­‰SOTAæ¨¡å‹ã€‚åœ¨ç”ŸæˆåŸºå‡†DPG-Benchä»¥åŠç¼–è¾‘åŸºå‡†ImgEdit-Benchä¸­ï¼Œè¯¥æ¨¡å‹ä¹Ÿå±•ç°äº†å“è¶Šçš„æ€§èƒ½æŒ‡æ ‡ã€‚ä½œä¸ºOvisç»Ÿä¸€æ¨¡å‹ç³»åˆ—çš„å¼€å±±ä¹‹ä½œï¼ŒOvis-U1æœ‰æ•ˆæ¨åŠ¨äº†å¤šæ¨¡æ€é¢†åŸŸç†è§£ã€ç”Ÿæˆä¸ç¼–è¾‘çš„æŠ€æœ¯è¾¹ç•Œã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "An unified model for multimodal understanding, text-to-image generation, and image editing. GitHub: https://github.com/AIDC-AI/Ovis-U1",
      "pdf_url": "https://arxiv.org/pdf/2506.23044v2",
      "published_date": "2025-06-29 00:40:17 UTC",
      "updated_date": "2025-07-01 09:33:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:24:35.611338+00:00"
    },
    {
      "arxiv_id": "2506.23040v2",
      "title": "Treatment, evidence, imitation, and chat",
      "title_zh": "æ²»ç–—ã€è¯æ®ã€æ¨¡ä»¿ä¸å¯¹è¯",
      "authors": [
        "Samuel J. Weisenthal"
      ],
      "abstract": "Large language models are thought to have potential to aid in medical decision making. We investigate this here. We start with the treatment problem, the patient's core medical decision-making task, which is solved in collaboration with a healthcare provider. We discuss approaches to solving the treatment problem, including -- within evidence-based medicine -- trials and observational data. We then discuss the chat problem, and how this differs from the treatment problem -- in particular as it relates to imitation. We then discuss how a large language model might be used to solve the treatment problem and highlight some of the challenges that emerge. We finally discuss how these challenges relate to evidence-based medicine, and how this might inform next steps.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) åœ¨åŒ»ç–—å†³ç­–ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œé‡ç‚¹åˆ†æäº†æ‚£è€…çš„æ ¸å¿ƒåŒ»ç–—å†³ç­–ä»»åŠ¡â€”â€”æ²»ç–—é—®é¢˜ (treatment problem)ã€‚ä½œè€…è¯¦ç»†è®¨è®ºäº†è§£å†³è¯¥é—®é¢˜çš„ä¼ ç»Ÿé€”å¾„ï¼ŒåŒ…æ‹¬å¾ªè¯åŒ»å­¦ (Evidence-Based Medicine) èŒƒç•´ä¸‹çš„ä¸´åºŠè¯•éªŒå’Œè§‚å¯Ÿæ€§æ•°æ®åˆ†æã€‚è®ºæ–‡è¿›ä¸€æ­¥é˜è¿°äº†å¯¹è¯é—®é¢˜ (chat problem) ä¸æ²»ç–—é—®é¢˜ä¹‹é—´çš„æœ¬è´¨åŒºåˆ«ï¼Œç‰¹åˆ«æ˜¯ä¸¤è€…åœ¨æ¨¡ä»¿ (imitation) æœºåˆ¶æ–¹é¢çš„å·®å¼‚ã€‚ç ”ç©¶è¯†åˆ«å¹¶åˆ†æäº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è§£å†³æ²»ç–—å†³ç­–æ—¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ï¼Œä»¥åŠè¿™äº›æŒ‘æˆ˜ä¸å¾ªè¯åŒ»å­¦åŸåˆ™çš„å…³è”ã€‚æœ€åï¼Œæ–‡ç« æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¾ªè¯åŒ»å­¦çš„è§†è§’æŒ‡å¯¼åç»­ç ”ç©¶ï¼Œä¸ºå®ç°æ›´å¯é çš„åŒ»ç–—äººå·¥æ™ºèƒ½è¾…åŠ©å†³ç­–æä¾›äº†ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "stat.OT",
        "cs.AI"
      ],
      "primary_category": "stat.OT",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.23040v2",
      "published_date": "2025-06-29 00:23:06 UTC",
      "updated_date": "2025-07-04 00:25:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T01:24:36.325455+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 68,
  "processed_papers_count": 68,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T01:25:18.471466+00:00"
}
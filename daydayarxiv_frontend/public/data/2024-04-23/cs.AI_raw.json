[
  {
    "arxiv_id": "2404.15564v1",
    "title": "Guided AbsoluteGrad: Magnitude of Gradients Matters to Explanation's Localization and Saliency",
    "authors": [
      "Jun Huang",
      "Yan Liu"
    ],
    "abstract": "This paper proposes a new gradient-based XAI method called Guided\nAbsoluteGrad for saliency map explanations. We utilize both positive and\nnegative gradient magnitudes and employ gradient variance to distinguish the\nimportant areas for noise deduction. We also introduce a novel evaluation\nmetric named ReCover And Predict (RCAP), which considers the Localization and\nVisual Noise Level objectives of the explanations. We propose two propositions\nfor these two objectives and prove the necessity of evaluating them. We\nevaluate Guided AbsoluteGrad with seven gradient-based XAI methods using the\nRCAP metric and other SOTA metrics in three case studies: (1) ImageNet dataset\nwith ResNet50 model; (2) International Skin Imaging Collaboration (ISIC)\ndataset with EfficientNet model; (3) the Places365 dataset with DenseNet161\nmodel. Our method surpasses other gradient-based approaches, showcasing the\nquality of enhanced saliency map explanations through gradient magnitude.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CAI2024 Camera-ready Submission",
    "pdf_url": "http://arxiv.org/pdf/2404.15564v1",
    "published_date": "2024-04-23 23:26:02 UTC",
    "updated_date": "2024-04-23 23:26:02 UTC"
  },
  {
    "arxiv_id": "2404.16885v1",
    "title": "Adapting an Artificial Intelligence Sexually Transmitted Diseases Symptom Checker Tool for Mpox Detection: The HeHealth Experience",
    "authors": [
      "Rayner Kay Jin Tan",
      "Dilruk Perera",
      "Salomi Arasaratnam",
      "Yudara Kularathne"
    ],
    "abstract": "Artificial Intelligence applications have shown promise in the management of\npandemics and have been widely used to assist the identification,\nclassification, and diagnosis of medical images. In response to the global\noutbreak of Monkeypox (Mpox), the HeHealth.ai team leveraged an existing tool\nto screen for sexually transmitted diseases to develop a digital screening test\nfor symptomatic Mpox through AI approaches. Prior to the global outbreak of\nMpox, the team developed a smartphone app, where app users can use their own\nsmartphone cameras to take pictures of their own penises to screen for\nsymptomatic STD. The AI model was initially developed using 5000 cases and use\na modified convolutional neural network to output prediction scores across\nvisually diagnosable penis pathologies including Syphilis, Herpes Simplex\nVirus, and Human Papilloma Virus. From June 2022 to October 2022, a total of\nabout 22,000 users downloaded the HeHealth app, and about 21,000 images have\nbeen analyzed using HeHealth AI technology. We then engaged in formative\nresearch, stakeholder engagement, rapid consolidation images, a validation\nstudy, and implementation of the tool from July 2022. From July 2022 to October\n2022, a total of 1000 Mpox related images had been used to train the Mpox\nsymptom checker tool. Our digital symptom checker tool showed accuracy of 87%\nto rule in Mpox and 90% to rule out symptomatic Mpox. Several hurdles\nidentified included issues of data privacy and security for app users, initial\nlack of data to train the AI tool, and the potential generalizability of input\ndata. We offer several suggestions to help others get started on similar\nprojects in emergency situations, including engaging a wide range of\nstakeholders, having a multidisciplinary team, prioritizing pragmatism, as well\nas the concept that big data in fact is made up of small data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.16885v1",
    "published_date": "2024-04-23 23:14:30 UTC",
    "updated_date": "2024-04-23 23:14:30 UTC"
  },
  {
    "arxiv_id": "2404.15549v2",
    "title": "PRISM: Patient Records Interpretation for Semantic Clinical Trial Matching using Large Language Models",
    "authors": [
      "Shashi Kant Gupta",
      "Aditya Basu",
      "Mauro Nievas",
      "Jerrin Thomas",
      "Nathan Wolfrath",
      "Adhitya Ramamurthi",
      "Bradley Taylor",
      "Anai N. Kothari",
      "Regina Schwind",
      "Therica M. Miller",
      "Sorena Nadaf-Rahrov",
      "Yanshan Wang",
      "Hrituraj Singh"
    ],
    "abstract": "Clinical trial matching is the task of identifying trials for which patients\nmay be potentially eligible. Typically, this task is labor-intensive and\nrequires detailed verification of patient electronic health records (EHRs)\nagainst the stringent inclusion and exclusion criteria of clinical trials. This\nprocess is manual, time-intensive, and challenging to scale up, resulting in\nmany patients missing out on potential therapeutic options. Recent advancements\nin Large Language Models (LLMs) have made automating patient-trial matching\npossible, as shown in multiple concurrent research studies. However, the\ncurrent approaches are confined to constrained, often synthetic datasets that\ndo not adequately mirror the complexities encountered in real-world medical\ndata. In this study, we present the first, end-to-end large-scale empirical\nevaluation of clinical trial matching using real-world EHRs. Our study\nshowcases the capability of LLMs to accurately match patients with appropriate\nclinical trials. We perform experiments with proprietary LLMs, including GPT-4\nand GPT-3.5, as well as our custom fine-tuned model called OncoLLM and show\nthat OncoLLM, despite its significantly smaller size, not only outperforms\nGPT-3.5 but also matches the performance of qualified medical doctors. All\nexperiments were carried out on real-world EHRs that include clinical notes and\navailable clinical trials from a single cancer center in the United States.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "30 Pages, 8 Figures, Supplementary Work Attached",
    "pdf_url": "http://arxiv.org/pdf/2404.15549v2",
    "published_date": "2024-04-23 22:33:19 UTC",
    "updated_date": "2024-04-27 03:10:21 UTC"
  },
  {
    "arxiv_id": "2404.16074v1",
    "title": "Explaining AI Decisions: Towards Achieving Human-Centered Explainability in Smart Home Environments",
    "authors": [
      "Md Shajalal",
      "Alexander Boden",
      "Gunnar Stevens",
      "Delong Du",
      "Dean-Robin Kern"
    ],
    "abstract": "Smart home systems are gaining popularity as homeowners strive to enhance\ntheir living and working environments while minimizing energy consumption.\nHowever, the adoption of artificial intelligence (AI)-enabled decision-making\nmodels in smart home systems faces challenges due to the complexity and\nblack-box nature of these systems, leading to concerns about explainability,\ntrust, transparency, accountability, and fairness. The emerging field of\nexplainable artificial intelligence (XAI) addresses these issues by providing\nexplanations for the models' decisions and actions. While state-of-the-art XAI\nmethods are beneficial for AI developers and practitioners, they may not be\neasily understood by general users, particularly household members. This paper\nadvocates for human-centered XAI methods, emphasizing the importance of\ndelivering readily comprehensible explanations to enhance user satisfaction and\ndrive the adoption of smart home systems. We review state-of-the-art XAI\nmethods and prior studies focusing on human-centered explanations for general\nusers in the context of smart home applications. Through experiments on two\nsmart home application scenarios, we demonstrate that explanations generated by\nprominent XAI techniques might not be effective in helping users understand and\nmake decisions. We thus argue for the necessity of a human-centric approach in\nrepresenting explanations in smart home systems and highlight relevant\nhuman-computer interaction (HCI) methodologies, including user studies,\nprototyping, technology probes analysis, and heuristic evaluation, that can be\nemployed to generate and present human-centered explanations to users.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "This is the pre-print version of our accepted paper at the 2nd World\n  Conference on eXplainable Artificial Intelligence (xAI2024), which will be\n  held in Valletta, Malta in 17-19 July, 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.16074v1",
    "published_date": "2024-04-23 22:31:42 UTC",
    "updated_date": "2024-04-23 22:31:42 UTC"
  },
  {
    "arxiv_id": "2404.15538v1",
    "title": "DreamCraft: Text-Guided Generation of Functional 3D Environments in Minecraft",
    "authors": [
      "Sam Earle",
      "Filippos Kokkinos",
      "Yuhe Nie",
      "Julian Togelius",
      "Roberta Raileanu"
    ],
    "abstract": "Procedural Content Generation (PCG) algorithms enable the automatic\ngeneration of complex and diverse artifacts. However, they don't provide\nhigh-level control over the generated content and typically require domain\nexpertise. In contrast, text-to-3D methods allow users to specify desired\ncharacteristics in natural language, offering a high amount of flexibility and\nexpressivity. But unlike PCG, such approaches cannot guarantee functionality,\nwhich is crucial for certain applications like game design. In this paper, we\npresent a method for generating functional 3D artifacts from free-form text\nprompts in the open-world game Minecraft. Our method, DreamCraft, trains\nquantized Neural Radiance Fields (NeRFs) to represent artifacts that, when\nviewed in-game, match given text descriptions. We find that DreamCraft produces\nmore aligned in-game artifacts than a baseline that post-processes the output\nof an unconstrained NeRF. Thanks to the quantized representation of the\nenvironment, functional constraints can be integrated using specialized loss\nterms. We show how this can be leveraged to generate 3D structures that match a\ntarget distribution or obey certain adjacency rules over the block types.\nDreamCraft inherits a high degree of expressivity and controllability from the\nNeRF, while still being able to incorporate functional constraints through\ndomain-specific objectives.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.GR",
    "comment": "16 pages, 9 figures, accepted to Foundation of Digital Games 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.15538v1",
    "published_date": "2024-04-23 21:57:14 UTC",
    "updated_date": "2024-04-23 21:57:14 UTC"
  },
  {
    "arxiv_id": "2407.05205v1",
    "title": "The AI Companion in Education: Analyzing the Pedagogical Potential of ChatGPT in Computer Science and Engineering",
    "authors": [
      "Zhangying He",
      "Thomas Nguyen",
      "Tahereh Miari",
      "Mehrdad Aliasgari",
      "Setareh Rafatirad",
      "Hossein Sayadi"
    ],
    "abstract": "Artificial Intelligence (AI), with ChatGPT as a prominent example, has\nrecently taken center stage in various domains including higher education,\nparticularly in Computer Science and Engineering (CSE). The AI revolution\nbrings both convenience and controversy, offering substantial benefits while\nlacking formal guidance on their application. The primary objective of this\nwork is to comprehensively analyze the pedagogical potential of ChatGPT in CSE\neducation, understanding its strengths and limitations from the perspectives of\neducators and learners. We employ a systematic approach, creating a diverse\nrange of educational practice problems within CSE field, focusing on various\nsubjects such as data science, programming, AI, machine learning, networks, and\nmore. According to our examinations, certain question types, like conceptual\nknowledge queries, typically do not pose significant challenges to ChatGPT, and\nthus, are excluded from our analysis. Alternatively, we focus our efforts on\ndeveloping more in-depth and personalized questions and project-based tasks.\nThese questions are presented to ChatGPT, followed by interactions to assess\nits effectiveness in delivering complete and meaningful responses. To this end,\nwe propose a comprehensive five-factor reliability analysis framework to\nevaluate the responses. This assessment aims to identify when ChatGPT excels\nand when it faces challenges. Our study concludes with a correlation analysis,\ndelving into the relationships among subjects, task types, and limiting\nfactors. This analysis offers valuable insights to enhance ChatGPT's utility in\nCSE education, providing guidance to educators and students regarding its\nreliability and efficacy.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "conference, 13 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.05205v1",
    "published_date": "2024-04-23 21:42:30 UTC",
    "updated_date": "2024-04-23 21:42:30 UTC"
  },
  {
    "arxiv_id": "2404.15532v1",
    "title": "BattleAgent: Multi-modal Dynamic Emulation on Historical Battles to Complement Historical Analysis",
    "authors": [
      "Shuhang Lin",
      "Wenyue Hua",
      "Lingyao Li",
      "Che-Jui Chang",
      "Lizhou Fan",
      "Jianchao Ji",
      "Hang Hua",
      "Mingyu Jin",
      "Jiebo Luo",
      "Yongfeng Zhang"
    ],
    "abstract": "This paper presents BattleAgent, an emulation system that combines the Large\nVision-Language Model and Multi-agent System. This novel system aims to\nsimulate complex dynamic interactions among multiple agents, as well as between\nagents and their environments, over a period of time. It emulates both the\ndecision-making processes of leaders and the viewpoints of ordinary\nparticipants, such as soldiers. The emulation showcases the current\ncapabilities of agents, featuring fine-grained multi-modal interactions between\nagents and landscapes. It develops customizable agent structures to meet\nspecific situational requirements, for example, a variety of battle-related\nactivities like scouting and trench digging. These components collaborate to\nrecreate historical events in a lively and comprehensive manner while offering\ninsights into the thoughts and feelings of individuals from diverse viewpoints.\nThe technological foundations of BattleAgent establish detailed and immersive\nsettings for historical battles, enabling individual agents to partake in,\nobserve, and dynamically respond to evolving battle scenarios. This methodology\nholds the potential to substantially deepen our understanding of historical\nevents, particularly through individual accounts. Such initiatives can also aid\nhistorical research, as conventional historical narratives often lack\ndocumentation and prioritize the perspectives of decision-makers, thereby\noverlooking the experiences of ordinary individuals. BattelAgent illustrates\nAI's potential to revitalize the human aspect in crucial social events, thereby\nfostering a more nuanced collective understanding and driving the progressive\ndevelopment of human society.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MA"
    ],
    "primary_category": "cs.HC",
    "comment": "26 pages, 14 figures The data and code for this project are\n  accessible at https://github.com/agiresearch/battleagent",
    "pdf_url": "http://arxiv.org/pdf/2404.15532v1",
    "published_date": "2024-04-23 21:37:22 UTC",
    "updated_date": "2024-04-23 21:37:22 UTC"
  },
  {
    "arxiv_id": "2404.15522v2",
    "title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
    "authors": [
      "Mihir Parmar",
      "Nisarg Patel",
      "Neeraj Varshney",
      "Mutsumi Nakamura",
      "Man Luo",
      "Santosh Mashetty",
      "Arindam Mitra",
      "Chitta Baral"
    ],
    "abstract": "Recently developed large language models (LLMs) have been shown to perform\nremarkably well on a wide range of language understanding tasks. But, can they\nreally \"reason\" over the natural language? This question has been receiving\nsignificant research attention and many reasoning skills such as commonsense,\nnumerical, and qualitative have been studied. However, the crucial skill\npertaining to 'logical reasoning' has remained underexplored. Existing work\ninvestigating this reasoning ability of LLMs has focused only on a couple of\ninference rules (such as modus ponens and modus tollens) of propositional and\nfirst-order logic. Addressing the above limitation, we comprehensively evaluate\nthe logical reasoning ability of LLMs on 25 different reasoning patterns\nspanning over propositional, first-order, and non-monotonic logics. To enable\nsystematic evaluation, we introduce LogicBench, a natural language\nquestion-answering dataset focusing on the use of a single inference rule. We\nconduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini,\nLlama-2, and Mistral using chain-of-thought prompting. Experimental results\nshow that existing LLMs do not fare well on LogicBench; especially, they\nstruggle with instances involving complex reasoning and negations. Furthermore,\nthey sometimes overlook contextual information necessary for reasoning to\narrive at the correct conclusion. We believe that our work and findings\nfacilitate future research for evaluating and enhancing the logical reasoning\nability of LLMs. Data and code are available at\nhttps://github.com/Mihir3009/LogicBench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL(Main) 2024 | First version available @\n  https://openreview.net/forum?id=7NR2ZVzZxx",
    "pdf_url": "http://arxiv.org/pdf/2404.15522v2",
    "published_date": "2024-04-23 21:08:49 UTC",
    "updated_date": "2024-06-06 08:15:54 UTC"
  },
  {
    "arxiv_id": "2404.15518v3",
    "title": "An MRP Formulation for Supervised Learning: Generalized Temporal Difference Learning Models",
    "authors": [
      "Yangchen Pan",
      "Junfeng Wen",
      "Chenjun Xiao",
      "Philip Torr"
    ],
    "abstract": "In traditional statistical learning, data points are usually assumed to be\nindependently and identically distributed (i.i.d.) following an unknown\nprobability distribution. This paper presents a contrasting viewpoint,\nperceiving data points as interconnected and employing a Markov reward process\n(MRP) for data modeling. We reformulate the typical supervised learning as an\non-policy policy evaluation problem within reinforcement learning (RL),\nintroducing a generalized temporal difference (TD) learning algorithm as a\nresolution. Theoretically, our analysis draws connections between the solutions\nof linear TD learning and ordinary least squares (OLS). We also show that under\nspecific conditions, particularly when noises are correlated, the TD's solution\nproves to be a more effective estimator than OLS. Furthermore, we establish the\nconvergence of our generalized TD algorithms under linear function\napproximation. Empirical studies verify our theoretical results, examine the\nvital design of our TD algorithm and show practical utility across various\ndatasets, encompassing tasks such as regression and image classification with\ndeep learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15518v3",
    "published_date": "2024-04-23 21:02:58 UTC",
    "updated_date": "2024-07-16 18:53:29 UTC"
  },
  {
    "arxiv_id": "2404.15516v1",
    "title": "Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval",
    "authors": [
      "Young Kyun Jang",
      "Donghyun Kim",
      "Zihang Meng",
      "Dat Huynh",
      "Ser-Nam Lim"
    ],
    "abstract": "Composed Image Retrieval (CIR) is a task that retrieves images similar to a\nquery, based on a provided textual modification. Current techniques rely on\nsupervised learning for CIR models using labeled triplets of the reference\nimage, text, target image. These specific triplets are not as commonly\navailable as simple image-text pairs, limiting the widespread use of CIR and\nits scalability. On the other hand, zero-shot CIR can be relatively easily\ntrained with image-caption pairs without considering the image-to-image\nrelation, but this approach tends to yield lower accuracy. We propose a new\nsemi-supervised CIR approach where we search for a reference and its related\ntarget images in auxiliary data and learn our large language model-based Visual\nDelta Generator (VDG) to generate text describing the visual difference (i.e.,\nvisual delta) between the two. VDG, equipped with fluent language knowledge and\nbeing model agnostic, can generate pseudo triplets to boost the performance of\nCIR models. Our approach significantly improves the existing supervised\nlearning approaches and achieves state-of-the-art results on the CIR\nbenchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.15516v1",
    "published_date": "2024-04-23 21:00:22 UTC",
    "updated_date": "2024-04-23 21:00:22 UTC"
  },
  {
    "arxiv_id": "2404.15515v3",
    "title": "ToM-LM: Delegating Theory of Mind Reasoning to External Symbolic Executors in Large Language Models",
    "authors": [
      "Weizhi Tang",
      "Vaishak Belle"
    ],
    "abstract": "Theory of Mind (ToM) refers to the ability of individuals to attribute mental\nstates to others. While Large Language Models (LLMs) have shown some promise\nwith ToM ability, they still struggle with complex ToM reasoning. Our approach\nleverages an external symbolic executor, specifically the SMCDEL model checker,\nand fine-tuning to improve the ToM reasoning ability of LLMs. In our approach,\nan LLM is first fine-tuned through pairs of natural language and symbolic\nformulation representation of ToM problems and is then instructed to generate\nthe symbolic formulation with a one-shot in-context example. The generated\nsymbolic formulation is then executed by the SMCDEL model checker to perform\ntransparent and verifiable ToM reasoning and give the final result. We\ndemonstrate that our approach, ToM-LM, shows a significant improvement over all\nthe constructed baselines. Our study proposes a novel view about externalizing\na particular component of ToM reasoning, mainly reasoning about beliefs, and\nsuggests generalizing it to other aspects of ToM reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeSy 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.15515v3",
    "published_date": "2024-04-23 20:59:03 UTC",
    "updated_date": "2024-06-26 15:57:22 UTC"
  },
  {
    "arxiv_id": "2405.02327v2",
    "title": "CausalLP: Learning causal relations with weighted knowledge graph link prediction",
    "authors": [
      "Utkarshani Jaimini",
      "Cory Henson",
      "Amit P. Sheth"
    ],
    "abstract": "Causal networks are useful in a wide variety of applications, from medical\ndiagnosis to root-cause analysis in manufacturing. In practice, however, causal\nnetworks are often incomplete with missing causal relations. This paper\npresents a novel approach, called CausalLP, that formulates the issue of\nincomplete causal networks as a knowledge graph completion problem. More\nspecifically, the task of finding new causal relations in an incomplete causal\nnetwork is mapped to the task of knowledge graph link prediction. The use of\nknowledge graphs to represent causal relations enables the integration of\nexternal domain knowledge; and as an added complexity, the causal relations\nhave weights representing the strength of the causal association between\nentities in the knowledge graph. Two primary tasks are supported by CausalLP:\ncausal explanation and causal prediction. An evaluation of this approach uses a\nbenchmark dataset of simulated videos for causal reasoning, CLEVRER-Humans, and\ncompares the performance of multiple knowledge graph embedding algorithms. Two\ndistinct dataset splitting approaches are used for evaluation: (1) random-based\nsplit, which is the method typically employed to evaluate link prediction\nalgorithms, and (2) Markov-based split, a novel data split technique that\nutilizes the Markovian property of causal relations. Results show that using\nweighted causal relations improves causal link prediction over the baseline\nwithout weighted relations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02327v2",
    "published_date": "2024-04-23 20:50:06 UTC",
    "updated_date": "2024-07-12 11:11:26 UTC"
  },
  {
    "arxiv_id": "2404.15503v1",
    "title": "FedGreen: Carbon-aware Federated Learning with Model Size Adaptation",
    "authors": [
      "Ali Abbasi",
      "Fan Dong",
      "Xin Wang",
      "Henry Leung",
      "Jiayu Zhou",
      "Steve Drew"
    ],
    "abstract": "Federated learning (FL) provides a promising collaborative framework to build\na model from distributed clients, and this work investigates the carbon\nemission of the FL process. Cloud and edge servers hosting FL clients may\nexhibit diverse carbon footprints influenced by their geographical locations\nwith varying power sources, offering opportunities to reduce carbon emissions\nby training local models with adaptive computations and communications. In this\npaper, we propose FedGreen, a carbon-aware FL approach to efficiently train\nmodels by adopting adaptive model sizes shared with clients based on their\ncarbon profiles and locations using ordered dropout as a model compression\ntechnique. We theoretically analyze the trade-offs between the produced carbon\nemissions and the convergence accuracy, considering the carbon intensity\ndiscrepancy across countries to choose the parameters optimally. Empirical\nstudies show that FedGreen can substantially reduce the carbon footprints of FL\ncompared to the state-of-the-art while maintaining competitive model accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15503v1",
    "published_date": "2024-04-23 20:37:26 UTC",
    "updated_date": "2024-04-23 20:37:26 UTC"
  },
  {
    "arxiv_id": "2405.00709v1",
    "title": "Evaluating Tool-Augmented Agents in Remote Sensing Platforms",
    "authors": [
      "Simranjit Singh",
      "Michael Fore",
      "Dimitrios Stamoulis"
    ],
    "abstract": "Tool-augmented Large Language Models (LLMs) have shown impressive\ncapabilities in remote sensing (RS) applications. However, existing benchmarks\nassume question-answering input templates over predefined image-text data\npairs. These standalone instructions neglect the intricacies of realistic\nuser-grounded tasks. Consider a geospatial analyst: they zoom in a map area,\nthey draw a region over which to collect satellite imagery, and they succinctly\nask \"Detect all objects here\". Where is `here`, if it is not explicitly\nhardcoded in the image-text template, but instead is implied by the system\nstate, e.g., the live map positioning? To bridge this gap, we present\nGeoLLM-QA, a benchmark designed to capture long sequences of verbal, visual,\nand click-based actions on a real UI platform. Through in-depth evaluation of\nstate-of-the-art LLMs over a diverse set of 1,000 tasks, we offer insights\ntowards stronger agents for RS applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2024 Machine Learning for Remote Sensing (ML4RS) Workshop",
    "pdf_url": "http://arxiv.org/pdf/2405.00709v1",
    "published_date": "2024-04-23 20:37:24 UTC",
    "updated_date": "2024-04-23 20:37:24 UTC"
  },
  {
    "arxiv_id": "2404.16884v1",
    "title": "Aligning Knowledge Graphs Provided by Humans and Generated from Neural Networks in Specific Tasks",
    "authors": [
      "Tangrui Li",
      "Jun Zhou"
    ],
    "abstract": "This paper develops an innovative method that enables neural networks to\ngenerate and utilize knowledge graphs, which describe their concept-level\nknowledge and optimize network parameters through alignment with human-provided\nknowledge. This research addresses a gap where traditionally, network-generated\nknowledge has been limited to applications in downstream symbolic analysis or\nenhancing network transparency. By integrating a novel autoencoder design with\nthe Vector Symbolic Architecture (VSA), we have introduced auxiliary tasks that\nsupport end-to-end training. Our approach eschews traditional dependencies on\nontologies or word embedding models, mining concepts from neural networks and\ndirectly aligning them with human knowledge. Experiments show that our method\nconsistently captures network-generated concepts that align closely with human\nknowledge and can even uncover new, useful concepts not previously identified\nby humans. This plug-and-play strategy not only enhances the interpretability\nof neural networks but also facilitates the integration of symbolic logical\nreasoning within these systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16884v1",
    "published_date": "2024-04-23 20:33:17 UTC",
    "updated_date": "2024-04-23 20:33:17 UTC"
  },
  {
    "arxiv_id": "2404.15501v1",
    "title": "Killkan: The Automatic Speech Recognition Dataset for Kichwa with Morphosyntactic Information",
    "authors": [
      "Chihiro Taguchi",
      "Jefferson Saransig",
      "Dayana Velásquez",
      "David Chiang"
    ],
    "abstract": "This paper presents Killkan, the first dataset for automatic speech\nrecognition (ASR) in the Kichwa language, an indigenous language of Ecuador.\nKichwa is an extremely low-resource endangered language, and there have been no\nresources before Killkan for Kichwa to be incorporated in applications of\nnatural language processing. The dataset contains approximately 4 hours of\naudio with transcription, translation into Spanish, and morphosyntactic\nannotation in the format of Universal Dependencies. The audio data was\nretrieved from a publicly available radio program in Kichwa. This paper also\nprovides corpus-linguistic analyses of the dataset with a special focus on the\nagglutinative morphology of Kichwa and frequent code-switching with Spanish.\nThe experiments show that the dataset makes it possible to develop the first\nASR system for Kichwa with reliable quality despite its small dataset size.\nThis dataset, the ASR model, and the code used to develop them will be publicly\navailable. Thus, our study positively showcases resource building and its\napplications for low-resource languages and their community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 9 tables, 3 figures, to be published in LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.15501v1",
    "published_date": "2024-04-23 20:26:07 UTC",
    "updated_date": "2024-04-23 20:26:07 UTC"
  },
  {
    "arxiv_id": "2404.15500v1",
    "title": "GeoLLM-Engine: A Realistic Environment for Building Geospatial Copilots",
    "authors": [
      "Simranjit Singh",
      "Michael Fore",
      "Dimitrios Stamoulis"
    ],
    "abstract": "Geospatial Copilots unlock unprecedented potential for performing Earth\nObservation (EO) applications through natural language instructions. However,\nexisting agents rely on overly simplified single tasks and template-based\nprompts, creating a disconnect with real-world scenarios. In this work, we\npresent GeoLLM-Engine, an environment for tool-augmented agents with intricate\ntasks routinely executed by analysts on remote sensing platforms. We enrich our\nenvironment with geospatial API tools, dynamic maps/UIs, and external\nmultimodal knowledge bases to properly gauge an agent's proficiency in\ninterpreting realistic high-level natural language commands and its functional\ncorrectness in task completions. By alleviating overheads typically associated\nwith human-in-the-loop benchmark curation, we harness our massively parallel\nengine across 100 GPT-4-Turbo nodes, scaling to over half a million diverse\nmulti-tool tasks and across 1.1 million satellite images. By moving beyond\ntraditional single-task image-caption paradigms, we investigate\nstate-of-the-art agents and prompting techniques against long-horizon prompts.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Earthvision 2024, CVPR Workshop",
    "pdf_url": "http://arxiv.org/pdf/2404.15500v1",
    "published_date": "2024-04-23 20:23:37 UTC",
    "updated_date": "2024-04-23 20:23:37 UTC"
  },
  {
    "arxiv_id": "2404.16072v1",
    "title": "Playing Board Games with the Predict Results of Beam Search Algorithm",
    "authors": [
      "Sergey Pastukhov"
    ],
    "abstract": "This paper introduces a novel algorithm for two-player deterministic games\nwith perfect information, which we call PROBS (Predict Results of Beam Search).\nUnlike existing methods that predominantly rely on Monte Carlo Tree Search\n(MCTS) for decision processes, our approach leverages a simpler beam search\nalgorithm. We evaluate the performance of our algorithm across a selection of\nboard games, where it consistently demonstrates an increased winning ratio\nagainst baseline opponents. A key result of this study is that the PROBS\nalgorithm operates effectively, even when the beam search size is considerably\nsmaller than the average number of turns in the game.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.16072v1",
    "published_date": "2024-04-23 20:10:27 UTC",
    "updated_date": "2024-04-23 20:10:27 UTC"
  },
  {
    "arxiv_id": "2404.15492v1",
    "title": "Multi-scale Intervention Planning based on Generative Design",
    "authors": [
      "Ioannis Kavouras",
      "Ioannis Rallis",
      "Emmanuel Sardis",
      "Eftychios Protopapadakis",
      "Anastasios Doulamis",
      "Nikolaos Doulamis"
    ],
    "abstract": "The scarcity of green spaces, in urban environments, consists a critical\nchallenge. There are multiple adverse effects, impacting the health and\nwell-being of the citizens. Small scale interventions, e.g. pocket parks, is a\nviable solution, but comes with multiple constraints, involving the design and\nimplementation over a specific area. In this study, we harness the capabilities\nof generative AI for multi-scale intervention planning, focusing on nature\nbased solutions. By leveraging image-to-image and image inpainting algorithms,\nwe propose a methodology to address the green space deficit in urban areas.\nFocusing on two alleys in Thessaloniki, where greenery is lacking, we\ndemonstrate the efficacy of our approach in visualizing NBS interventions. Our\nfindings underscore the transformative potential of emerging technologies in\nshaping the future of urban intervention planning processes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15492v1",
    "published_date": "2024-04-23 20:06:56 UTC",
    "updated_date": "2024-04-23 20:06:56 UTC"
  },
  {
    "arxiv_id": "2404.15488v1",
    "title": "IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection & Correction Task On the Shoulders of Medical Agents",
    "authors": [
      "Jean-Philippe Corbeil"
    ],
    "abstract": "In natural language processing applied to the clinical domain, utilizing\nlarge language models has emerged as a promising avenue for error detection and\ncorrection on clinical notes, a knowledge-intensive task for which annotated\ndata is scarce. This paper presents MedReAct'N'MedReFlex, which leverages a\nsuite of four LLM-based medical agents. The MedReAct agent initiates the\nprocess by observing, analyzing, and taking action, generating trajectories to\nguide the search to target a potential error in the clinical notes.\nSubsequently, the MedEval agent employs five evaluators to assess the targeted\nerror and the proposed correction. In cases where MedReAct's actions prove\ninsufficient, the MedReFlex agent intervenes, engaging in reflective analysis\nand proposing alternative strategies. Finally, the MedFinalParser agent formats\nthe final output, preserving the original style while ensuring the integrity of\nthe error correction process. One core component of our method is our RAG\npipeline based on our ClinicalCorp corpora. Among other well-known sources\ncontaining clinical guidelines and information, we preprocess and release the\nopen-source MedWiki dataset for clinical RAG application. Our results\ndemonstrate the central role of our RAG approach with ClinicalCorp leveraged\nthrough the MedReAct'N'MedReFlex framework. It achieved the ninth rank on the\nMEDIQA-CORR 2024 final leaderboard.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15488v1",
    "published_date": "2024-04-23 20:00:37 UTC",
    "updated_date": "2024-04-23 20:00:37 UTC"
  },
  {
    "arxiv_id": "2405.00708v1",
    "title": "Interactive Analysis of LLMs using Meaningful Counterfactuals",
    "authors": [
      "Furui Cheng",
      "Vilém Zouhar",
      "Robin Shing Moon Chan",
      "Daniel Fürst",
      "Hendrik Strobelt",
      "Mennatallah El-Assady"
    ],
    "abstract": "Counterfactual examples are useful for exploring the decision boundaries of\nmachine learning models and determining feature attributions. How can we apply\ncounterfactual-based methods to analyze and explain LLMs? We identify the\nfollowing key challenges. First, the generated textual counterfactuals should\nbe meaningful and readable to users and thus can be mentally compared to draw\nconclusions. Second, to make the solution scalable to long-form text, users\nshould be equipped with tools to create batches of counterfactuals from\nperturbations at various granularity levels and interactively analyze the\nresults. In this paper, we tackle the above challenges and contribute 1) a\nnovel algorithm for generating batches of complete and meaningful textual\ncounterfactuals by removing and replacing text segments in different\ngranularities, and 2) LLM Analyzer, an interactive visualization tool to help\nusers understand an LLM's behaviors by interactively inspecting and aggregating\nmeaningful counterfactuals. We evaluate the proposed algorithm by the\ngrammatical correctness of its generated counterfactuals using 1,000 samples\nfrom medical, legal, finance, education, and news datasets. In our experiments,\n97.2% of the counterfactuals are grammatically correct. Through a use case,\nuser studies, and feedback from experts, we demonstrate the usefulness and\nusability of the proposed interactive visualization tool.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "I.2.7; H.5.2"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00708v1",
    "published_date": "2024-04-23 19:57:03 UTC",
    "updated_date": "2024-04-23 19:57:03 UTC"
  },
  {
    "arxiv_id": "2404.15485v3",
    "title": "Evaluating the Efficacy of Large Language Models in Identifying Phishing Attempts",
    "authors": [
      "Het Patel",
      "Umair Rehman",
      "Farkhund Iqbal"
    ],
    "abstract": "Phishing, a prevalent cybercrime tactic for decades, remains a significant\nthreat in today's digital world. By leveraging clever social engineering\nelements and modern technology, cybercrime targets many individuals,\nbusinesses, and organizations to exploit trust and security. These\ncyber-attackers are often disguised in many trustworthy forms to appear as\nlegitimate sources. By cleverly using psychological elements like urgency,\nfear, social proof, and other manipulative strategies, phishers can lure\nindividuals into revealing sensitive and personalized information. Building on\nthis pervasive issue within modern technology, this paper aims to analyze the\neffectiveness of 15 Large Language Models (LLMs) in detecting phishing\nattempts, specifically focusing on a randomized set of \"419 Scam\" emails. The\nobjective is to determine which LLMs can accurately detect phishing emails by\nanalyzing a text file containing email metadata based on predefined criteria.\nThe experiment concluded that the following models, ChatGPT 3.5,\nGPT-3.5-Turbo-Instruct, and ChatGPT, were the most effective in detecting\nphishing emails.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.15485v3",
    "published_date": "2024-04-23 19:55:18 UTC",
    "updated_date": "2024-06-06 21:03:03 UTC"
  },
  {
    "arxiv_id": "2404.16071v1",
    "title": "Augmenting the Author: Exploring the Potential of AI Collaboration in Academic Writing",
    "authors": [
      "Joseph Tu",
      "Hilda Hadan",
      "Derrick M. Wang",
      "Sabrina A Sgandurra",
      "Reza Hadi Mogavi",
      "Lennart E. Nacke"
    ],
    "abstract": "This workshop paper presents a critical examination of the integration of\nGenerative AI (Gen AI) into the academic writing process, focusing on the use\nof AI as a collaborative tool. It contrasts the performance and interaction of\ntwo AI models, Gemini and ChatGPT, through a collaborative inquiry approach\nwhere researchers engage in facilitated sessions to design prompts that elicit\nspecific AI responses for crafting research outlines. This case study\nhighlights the importance of prompt design, output analysis, and recognizing\nthe AI's limitations to ensure responsible and effective AI integration in\nscholarly work. Preliminary findings suggest that prompt variation\nsignificantly affects output quality and reveals distinct capabilities and\nconstraints of each model. The paper contributes to the field of Human-Computer\nInteraction by exploring effective prompt strategies and providing a\ncomparative analysis of Gen AI models, ultimately aiming to enhance AI-assisted\nacademic writing and prompt a deeper dialogue within the HCI community.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages, workshop paper, CHI 2024 conference GENAI",
    "pdf_url": "http://arxiv.org/pdf/2404.16071v1",
    "published_date": "2024-04-23 19:06:39 UTC",
    "updated_date": "2024-04-23 19:06:39 UTC"
  },
  {
    "arxiv_id": "2405.02326v2",
    "title": "Evaluating LLMs for Hardware Design and Test",
    "authors": [
      "Jason Blocklove",
      "Siddharth Garg",
      "Ramesh Karri",
      "Hammond Pearce"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated capabilities for producing\ncode in Hardware Description Languages (HDLs). However, most of the focus\nremains on their abilities to write functional code, not test code. The\nhardware design process consists of both design and test, and so eschewing\nvalidation and verification leaves considerable potential benefit unexplored,\ngiven that a design and test framework may allow for progress towards full\nautomation of the digital design pipeline. In this work, we perform one of the\nfirst studies exploring how a LLM can both design and test hardware modules\nfrom provided specifications. Using a suite of 8 representative benchmarks, we\nexamined the capabilities and limitations of the state-of-the-art\nconversational LLMs when producing Verilog for functional and verification\npurposes. We taped out the benchmarks on a Skywater 130nm shuttle and received\nthe functional chip.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02326v2",
    "published_date": "2024-04-23 18:55:49 UTC",
    "updated_date": "2024-12-02 01:59:30 UTC"
  },
  {
    "arxiv_id": "2404.15449v1",
    "title": "ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with Reward Feedback Learning",
    "authors": [
      "Weifeng Chen",
      "Jiacheng Zhang",
      "Jie Wu",
      "Hefeng Wu",
      "Xuefeng Xiao",
      "Liang Lin"
    ],
    "abstract": "The rapid development of diffusion models has triggered diverse applications.\nIdentity-preserving text-to-image generation (ID-T2I) particularly has received\nsignificant attention due to its wide range of application scenarios like AI\nportrait and advertising. While existing ID-T2I methods have demonstrated\nimpressive results, several key challenges remain: (1) It is hard to maintain\nthe identity characteristics of reference portraits accurately, (2) The\ngenerated images lack aesthetic appeal especially while enforcing identity\nretention, and (3) There is a limitation that cannot be compatible with\nLoRA-based and Adapter-based methods simultaneously. To address these issues,\nwe present \\textbf{ID-Aligner}, a general feedback learning framework to\nenhance ID-T2I performance. To resolve identity features lost, we introduce\nidentity consistency reward fine-tuning to utilize the feedback from face\ndetection and recognition models to improve generated identity preservation.\nFurthermore, we propose identity aesthetic reward fine-tuning leveraging\nrewards from human-annotated preference data and automatically constructed\nfeedback on character structure generation to provide aesthetic tuning signals.\nThanks to its universal feedback fine-tuning framework, our method can be\nreadily applied to both LoRA and Adapter models, achieving consistent\nperformance gains. Extensive experiments on SD1.5 and SDXL diffusion models\nvalidate the effectiveness of our approach. \\textbf{Project Page:\n\\url{https://idaligner.github.io/}}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15449v1",
    "published_date": "2024-04-23 18:41:56 UTC",
    "updated_date": "2024-04-23 18:41:56 UTC"
  },
  {
    "arxiv_id": "2404.15447v1",
    "title": "GLoD: Composing Global Contexts and Local Details in Image Generation",
    "authors": [
      "Moyuru Yamada"
    ],
    "abstract": "Diffusion models have demonstrated their capability to synthesize\nhigh-quality and diverse images from textual prompts. However, simultaneous\ncontrol over both global contexts (e.g., object layouts and interactions) and\nlocal details (e.g., colors and emotions) still remains a significant\nchallenge. The models often fail to understand complex descriptions involving\nmultiple objects and reflect specified visual attributes to wrong targets or\nignore them. This paper presents Global-Local Diffusion (\\textit{GLoD}), a\nnovel framework which allows simultaneous control over the global contexts and\nthe local details in text-to-image generation without requiring training or\nfine-tuning. It assigns multiple global and local prompts to corresponding\nlayers and composes their noises to guide a denoising process using pre-trained\ndiffusion models. Our framework enables complex global-local compositions,\nconditioning objects in the global prompt with the local prompts while\npreserving other unspecified identities. Our quantitative and qualitative\nevaluations demonstrate that GLoD effectively generates complex images that\nadhere to both user-provided object interactions and object details.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15447v1",
    "published_date": "2024-04-23 18:39:57 UTC",
    "updated_date": "2024-04-23 18:39:57 UTC"
  },
  {
    "arxiv_id": "2404.15420v3",
    "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
    "authors": [
      "João Monteiro",
      "Étienne Marcotte",
      "Pierre-André Noël",
      "Valentina Zantedeschi",
      "David Vázquez",
      "Nicolas Chapados",
      "Christopher Pal",
      "Perouz Taslakian"
    ],
    "abstract": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15420v3",
    "published_date": "2024-04-23 18:10:42 UTC",
    "updated_date": "2024-11-01 14:56:52 UTC"
  },
  {
    "arxiv_id": "2404.15417v2",
    "title": "The Power of Resets in Online Reinforcement Learning",
    "authors": [
      "Zakaria Mhammedi",
      "Dylan J. Foster",
      "Alexander Rakhlin"
    ],
    "abstract": "Simulators are a pervasive tool in reinforcement learning, but most existing\nalgorithms cannot efficiently exploit simulator access -- particularly in\nhigh-dimensional domains that require general function approximation. We\nexplore the power of simulators through online reinforcement learning with\n{local simulator access} (or, local planning), an RL protocol where the agent\nis allowed to reset to previously observed states and follow their dynamics\nduring training. We use local simulator access to unlock new statistical\nguarantees that were previously out of reach:\n  - We show that MDPs with low coverability (Xie et al. 2023) -- a general\nstructural condition that subsumes Block MDPs and Low-Rank MDPs -- can be\nlearned in a sample-efficient fashion with only $Q^{\\star}$-realizability\n(realizability of the optimal state-value function); existing online RL\nalgorithms require significantly stronger representation conditions.\n  - As a consequence, we show that the notorious Exogenous Block MDP problem\n(Efroni et al. 2022) is tractable under local simulator access.\n  The results above are achieved through a computationally inefficient\nalgorithm. We complement them with a more computationally efficient algorithm,\nRVFS (Recursive Value Function Search), which achieves provable sample\ncomplexity guarantees under a strengthened statistical assumption known as\npushforward coverability. RVFS can be viewed as a principled, provable\ncounterpart to a successful empirical paradigm that combines recursive search\n(e.g., MCTS) with value function approximation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Fixed a small typo",
    "pdf_url": "http://arxiv.org/pdf/2404.15417v2",
    "published_date": "2024-04-23 18:09:53 UTC",
    "updated_date": "2024-04-26 14:31:09 UTC"
  },
  {
    "arxiv_id": "2404.15418v1",
    "title": "Machine Learning Techniques with Fairness for Prediction of Completion of Drug and Alcohol Rehabilitation",
    "authors": [
      "Karen Roberts-Licklider",
      "Theodore Trafalis"
    ],
    "abstract": "The aim of this study is to look at predicting whether a person will complete\na drug and alcohol rehabilitation program and the number of times a person\nattends. The study is based on demographic data obtained from Substance Abuse\nand Mental Health Services Administration (SAMHSA) from both admissions and\ndischarge data from drug and alcohol rehabilitation centers in Oklahoma.\nDemographic data is highly categorical which led to binary encoding being used\nand various fairness measures being utilized to mitigate bias of nine\ndemographic variables. Kernel methods such as linear, polynomial, sigmoid, and\nradial basis functions were compared using support vector machines at various\nparameter ranges to find the optimal values. These were then compared to\nmethods such as decision trees, random forests, and neural networks. Synthetic\nMinority Oversampling Technique Nominal (SMOTEN) for categorical data was used\nto balance the data with imputation for missing data. The nine bias variables\nwere then intersectionalized to mitigate bias and the dual and triple\ninteractions were integrated to use the probabilities to look at worst case\nratio fairness mitigation. Disparate Impact, Statistical Parity difference,\nConditional Statistical Parity Ratio, Demographic Parity, Demographic Parity\nRatio, Equalized Odds, Equalized Odds Ratio, Equal Opportunity, and Equalized\nOpportunity Ratio were all explored at both the binary and multiclass\nscenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15418v1",
    "published_date": "2024-04-23 18:09:53 UTC",
    "updated_date": "2024-04-23 18:09:53 UTC"
  },
  {
    "arxiv_id": "2404.15410v1",
    "title": "Planning the path with Reinforcement Learning: Optimal Robot Motion Planning in RoboCup Small Size League Environments",
    "authors": [
      "Mateus G. Machado",
      "João G. Melo",
      "Cleber Zanchettin",
      "Pedro H. M. Braga",
      "Pedro V. Cunha",
      "Edna N. S. Barros",
      "Hansenclever F. Bassani"
    ],
    "abstract": "This work investigates the potential of Reinforcement Learning (RL) to tackle\nrobot motion planning challenges in the dynamic RoboCup Small Size League\n(SSL). Using a heuristic control approach, we evaluate RL's effectiveness in\nobstacle-free and single-obstacle path-planning environments. Ablation studies\nreveal significant performance improvements. Our method achieved a 60% time\ngain in obstacle-free environments compared to baseline algorithms.\nAdditionally, our findings demonstrated dynamic obstacle avoidance\ncapabilities, adeptly navigating around moving blocks. These findings highlight\nthe potential of RL to enhance robot motion planning in the challenging and\nunpredictable SSL environment.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "12 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.15410v1",
    "published_date": "2024-04-23 18:01:30 UTC",
    "updated_date": "2024-04-23 18:01:30 UTC"
  },
  {
    "arxiv_id": "2404.15406v2",
    "title": "Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs",
    "authors": [
      "Davide Caffagni",
      "Federico Cocchi",
      "Nicholas Moratelli",
      "Sara Sarto",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "abstract": "Multimodal LLMs are the natural evolution of LLMs, and enlarge their\ncapabilities so as to work beyond the pure textual modality. As research is\nbeing carried out to design novel architectures and vision-and-language\nadapters, in this paper we concentrate on endowing such models with the\ncapability of answering questions that require external knowledge. Our\napproach, termed Wiki-LLaVA, aims at integrating an external knowledge source\nof multimodal documents, which is accessed through a hierarchical retrieval\npipeline. Relevant passages, using this approach, are retrieved from the\nexternal knowledge source and employed as additional context for the LLM,\naugmenting the effectiveness and precision of generated dialogues. We conduct\nextensive experiments on datasets tailored for visual question answering with\nexternal data and demonstrate the appropriateness of our approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024 Workshop on What is Next in Multimodal Foundation Models",
    "pdf_url": "http://arxiv.org/pdf/2404.15406v2",
    "published_date": "2024-04-23 18:00:09 UTC",
    "updated_date": "2024-05-22 07:15:18 UTC"
  },
  {
    "arxiv_id": "2404.15276v1",
    "title": "SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation",
    "authors": [
      "Xiangyu Xu",
      "Lijuan Liu",
      "Shuicheng Yan"
    ],
    "abstract": "Existing Transformers for monocular 3D human shape and pose estimation\ntypically have a quadratic computation and memory complexity with respect to\nthe feature length, which hinders the exploitation of fine-grained information\nin high-resolution features that is beneficial for accurate reconstruction. In\nthis work, we propose an SMPL-based Transformer framework (SMPLer) to address\nthis issue. SMPLer incorporates two key ingredients: a decoupled attention\noperation and an SMPL-based target representation, which allow effective\nutilization of high-resolution features in the Transformer. In addition, based\non these two designs, we also introduce several novel modules including a\nmulti-scale attention and a joint-aware attention to further boost the\nreconstruction performance. Extensive experiments demonstrate the effectiveness\nof SMPLer against existing 3D human shape and pose estimation methods both\nquantitatively and qualitatively. Notably, the proposed algorithm achieves an\nMPJPE of 45.2 mm on the Human3.6M dataset, improving upon Mesh Graphormer by\nmore than 10% with fewer than one-third of the parameters. Code and pretrained\nmodels are available at https://github.com/xuxy09/SMPLer.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at TPAMI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.15276v1",
    "published_date": "2024-04-23 17:59:59 UTC",
    "updated_date": "2024-04-23 17:59:59 UTC"
  },
  {
    "arxiv_id": "2404.15272v3",
    "title": "CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios",
    "authors": [
      "Jingyang Lin",
      "Yingda Xia",
      "Jianpeng Zhang",
      "Ke Yan",
      "Le Lu",
      "Jiebo Luo",
      "Ling Zhang"
    ],
    "abstract": "Medical Vision-Language Pretraining (Med-VLP) establishes a connection\nbetween visual content from medical images and the relevant textual\ndescriptions. Existing Med-VLP methods primarily focus on 2D images depicting a\nsingle body part, notably chest X-rays. In this paper, we extend the scope of\nMed-VLP to encompass 3D images, specifically targeting full-body scenarios, by\nusing a multimodal dataset of CT images and reports. Compared with the 2D\ncounterpart, 3D VLP is required to effectively capture essential semantics from\nsignificantly sparser representation in 3D imaging. In this paper, we introduce\nCT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method\nthat constructs organ-level image-text pairs to enhance multimodal contrastive\nlearning, aligning grounded visual features with precise diagnostic text.\nAdditionally, we developed an abnormality dictionary to augment contrastive\nlearning with diverse contrastive pairs. Our method, trained on a multimodal CT\ndataset comprising 44,011 organ-level vision-text pairs from 17,702 patients\nacross 104 organs, demonstrates it can identify organs and abnormalities in a\nzero-shot manner using natural languages. The performance of CT-GLIP is\nvalidated on a separate test set of 1,130 patients, focusing on the 16 most\nfrequent abnormalities across 7 organs. The experimental results show our\nmodel's superior performance over the standard CLIP framework across zero-shot\nand fine-tuning scenarios, using both CNN and ViT architectures.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 5 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.15272v3",
    "published_date": "2024-04-23 17:59:01 UTC",
    "updated_date": "2024-04-29 03:25:14 UTC"
  },
  {
    "arxiv_id": "2404.15271v1",
    "title": "Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models",
    "authors": [
      "Wanrong Zhu",
      "Jennifer Healey",
      "Ruiyi Zhang",
      "William Yang Wang",
      "Tong Sun"
    ],
    "abstract": "Recent advancements in instruction-following models have made user\ninteractions with models more user-friendly and efficient, broadening their\napplicability. In graphic design, non-professional users often struggle to\ncreate visually appealing layouts due to limited skills and resources. In this\nwork, we introduce a novel multimodal instruction-following framework for\nlayout planning, allowing users to easily arrange visual elements into tailored\nlayouts by specifying canvas size and design purpose, such as for book covers,\nposters, brochures, or menus. We developed three layout reasoning tasks to\ntrain the model in understanding and executing layout instructions. Experiments\non two benchmarks show that our method not only simplifies the design process\nfor non-professionals but also surpasses the performance of few-shot GPT-4V\nmodels, with mIoU higher by 12% on Crello. This progress highlights the\npotential of multimodal instruction-following models to automate and simplify\nthe design process, providing an approachable solution for a wide range of\ndesign tasks on visually-rich documents.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15271v1",
    "published_date": "2024-04-23 17:58:33 UTC",
    "updated_date": "2024-04-23 17:58:33 UTC"
  },
  {
    "arxiv_id": "2404.15269v3",
    "title": "Aligning LLM Agents by Learning Latent Preference from User Edits",
    "authors": [
      "Ge Gao",
      "Alexey Taymanov",
      "Eduardo Salinas",
      "Paul Mineiro",
      "Dipendra Misra"
    ],
    "abstract": "We study interactive learning of LLM-based language agents based on user\nedits made to the agent's output. In a typical setting such as writing\nassistants, the user interacts with a language agent to generate a response\ngiven a context, and may optionally edit the agent response to personalize it\nbased on their latent preference, in addition to improving the correctness. The\nedit feedback is naturally generated, making it a suitable candidate for\nimproving the agent's alignment with the user's preference, and for reducing\nthe cost of user edits over time. We propose a learning framework, PRELUDE that\ninfers a description of the user's latent preference based on historic edit\ndata. The inferred user preference descriptions are used to define prompts for\ngenerating responses in the future. This avoids fine-tuning the agent, which is\ncostly, challenging to scale with the number of users, and may even degrade its\nperformance on other tasks. Furthermore, learning descriptive preference\nimproves interpretability, allowing the user to view and modify the learned\npreference. However, user preference can be complex, subtle, and vary based on\ncontext, making it challenging to learn. To address this, we propose a simple\nyet effective algorithm named CIPHER that leverages the LLM to infer the user\npreference for a given context based on user edits. In the future, CIPHER\nretrieves inferred preferences from the k-closest contexts in the history, and\nforms an aggregate preference for response generation. We introduce two\ninteractive environments -- summarization and email writing, and use a GPT-4\nsimulated user for evaluation. On both tasks, CIPHER outperforms several\nbaselines by achieving the lowest edit distance cost while only having a small\noverhead in LLM query cost. Our analysis reports that user preferences learned\nby CIPHER show significant similarity to the ground truth latent preferences.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15269v3",
    "published_date": "2024-04-23 17:57:47 UTC",
    "updated_date": "2024-11-23 16:19:03 UTC"
  },
  {
    "arxiv_id": "2404.15256v4",
    "title": "TOP-Nav: Legged Navigation Integrating Terrain, Obstacle and Proprioception Estimation",
    "authors": [
      "Junli Ren",
      "Yikai Liu",
      "Yingru Dai",
      "Junfeng Long",
      "Guijin Wang"
    ],
    "abstract": "Legged navigation is typically examined within open-world, off-road, and\nchallenging environments. In these scenarios, estimating external disturbances\nrequires a complex synthesis of multi-modal information. This underlines a\nmajor limitation in existing works that primarily focus on avoiding obstacles.\nIn this work, we propose TOP-Nav, a novel legged navigation framework that\nintegrates a comprehensive path planner with Terrain awareness, Obstacle\navoidance and close-loop Proprioception. TOP-Nav underscores the synergies\nbetween vision and proprioception in both path and motion planning. Within the\npath planner, we present and integrate a terrain estimator that enables the\nrobot to select waypoints on terrains with higher traversability while\neffectively avoiding obstacles. In the motion planning level, we not only\nimplement a locomotion controller to track the navigation commands, but also\nconstruct a proprioception advisor to provide motion evaluations for the path\nplanner. Based on the close-loop motion feedback, we make online corrections\nfor the vision-based terrain and obstacle estimations. Consequently, TOP-Nav\nachieves open-world navigation that the robot can handle terrains or\ndisturbances beyond the distribution of prior knowledge and overcomes\nconstraints imposed by visual conditions. Building upon extensive experiments\nconducted in both simulation and real-world environments, TOP-Nav demonstrates\nsuperior performance in open-world navigation compared to existing methods.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Published on CoRL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.15256v4",
    "published_date": "2024-04-23 17:42:45 UTC",
    "updated_date": "2024-09-27 07:16:23 UTC"
  },
  {
    "arxiv_id": "2404.15247v2",
    "title": "XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts",
    "authors": [
      "Yifeng Ding",
      "Jiawei Liu",
      "Yuxiang Wei",
      "Terry Yue Zhuo",
      "Lingming Zhang"
    ],
    "abstract": "We introduce XFT, a simple yet powerful training scheme, by simply merging\nupcycled Mixture-of-Experts (MoE) to unleash the performance limit of\ninstruction-tuned code Large Language Models (LLMs). While vanilla sparse\nupcycling fails to improve instruction tuning, XFT introduces a shared expert\nmechanism with a novel routing weight normalization strategy into sparse\nupcycling, which significantly boosts instruction tuning. After fine-tuning the\nupcycled MoE model, XFT introduces a learnable model merging mechanism to\ncompile the upcycled MoE model back to a dense model, achieving upcycled\nMoE-level performance with only dense-model compute. By applying XFT to a 1.3B\nmodel, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6\npass@1 on HumanEval and HumanEval+ respectively. With the same data and model\narchitecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+,\nalong with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and\nDS-1000, demonstrating its generalizability. XFT is fully orthogonal to\nexisting techniques such as Evol-Instruct and OSS-Instruct, opening a new\ndimension for improving code instruction tuning. Codes are available at\nhttps://github.com/ise-uiuc/xft.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15247v2",
    "published_date": "2024-04-23 17:32:24 UTC",
    "updated_date": "2024-06-06 18:18:21 UTC"
  },
  {
    "arxiv_id": "2404.15238v1",
    "title": "CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies",
    "authors": [
      "Weiyan Shi",
      "Ryan Li",
      "Yutong Zhang",
      "Caleb Ziems",
      "Chunhua yu",
      "Raya Horesh",
      "Rogério Abreu de Paula",
      "Diyi Yang"
    ],
    "abstract": "To enhance language models' cultural awareness, we design a generalizable\npipeline to construct cultural knowledge bases from different online\ncommunities on a massive scale. With the pipeline, we construct CultureBank, a\nknowledge base built upon users' self-narratives with 12K cultural descriptors\nsourced from TikTok and 11K from Reddit. Unlike previous cultural knowledge\nresources, CultureBank contains diverse views on cultural descriptors to allow\nflexible interpretation of cultural knowledge, and contextualized cultural\nscenarios to help grounded evaluation. With CultureBank, we evaluate different\nLLMs' cultural awareness, and identify areas for improvement. We also fine-tune\na language model on CultureBank: experiments show that it achieves better\nperformances on two downstream cultural tasks in a zero-shot setting. Finally,\nwe offer recommendations based on our findings for future culturally aware\nlanguage technologies. The project page is https://culturebank.github.io . The\ncode and model is at https://github.com/SALT-NLP/CultureBank . The released\nCultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "32 pages, 7 figures, preprint",
    "pdf_url": "http://arxiv.org/pdf/2404.15238v1",
    "published_date": "2024-04-23 17:16:08 UTC",
    "updated_date": "2024-04-23 17:16:08 UTC"
  },
  {
    "arxiv_id": "2405.01575v1",
    "title": "Software Mention Recognition with a Three-Stage Framework Based on BERTology Models at SOMD 2024",
    "authors": [
      "Thuy Nguyen Thi",
      "Anh Nguyen Viet",
      "Thin Dang Van",
      "Ngan Nguyen Luu Thuy"
    ],
    "abstract": "This paper describes our systems for the sub-task I in the Software Mention\nDetection in Scholarly Publications shared-task. We propose three approaches\nleveraging different pre-trained language models (BERT, SciBERT, and XLM-R) to\ntackle this challenge. Our bestperforming system addresses the named entity\nrecognition (NER) problem through a three-stage framework. (1) Entity Sentence\nClassification - classifies sentences containing potential software mentions;\n(2) Entity Extraction - detects mentions within classified sentences; (3)\nEntity Type Classification - categorizes detected mentions into specific\nsoftware types. Experiments on the official dataset demonstrate that our\nthree-stage framework achieves competitive performance, surpassing both other\nparticipating teams and our alternative approaches. As a result, our framework\nbased on the XLM-R-based model achieves a weighted F1-score of 67.80%,\ndelivering our team the 3rd rank in Sub-task I for the Software Mention\nRecognition task.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "Software mention recognition, Named entity recognition, Transformer,\n  Three-stage framework",
    "pdf_url": "http://arxiv.org/pdf/2405.01575v1",
    "published_date": "2024-04-23 17:06:24 UTC",
    "updated_date": "2024-04-23 17:06:24 UTC"
  },
  {
    "arxiv_id": "2404.15231v2",
    "title": "Direct Zernike Coefficient Prediction from Point Spread Functions and Extended Images using Deep Learning",
    "authors": [
      "Yong En Kok",
      "Alexander Bentley",
      "Andrew Parkes",
      "Amanda J. Wright",
      "Michael G. Somekh",
      "Michael Pound"
    ],
    "abstract": "Optical imaging quality can be severely degraded by system and sample induced\naberrations. Existing adaptive optics systems typically rely on iterative\nsearch algorithm to correct for aberrations and improve images. This study\ndemonstrates the application of convolutional neural networks to characterise\nthe optical aberration by directly predicting the Zernike coefficients from two\nto three phase-diverse optical images. We evaluated our network on 600,000\nsimulated Point Spread Function (PSF) datasets randomly generated within the\nrange of -1 to 1 radians using the first 25 Zernike coefficients. The results\nshow that using only three phase-diverse images captured above, below and at\nthe focal plane with an amplitude of 1 achieves a low RMSE of 0.10 radians on\nthe simulated PSF dataset. Furthermore, this approach directly predicts Zernike\nmodes simulated extended 2D samples, while maintaining a comparable RMSE of\n0.15 radians. We demonstrate that this approach is effective using only a\nsingle prediction step, or can be iterated a small number of times. This simple\nand straightforward technique provides rapid and accurate method for predicting\nthe aberration correction using three or less phase-diverse images, paving the\nway for evaluation on real-world dataset.",
    "categories": [
      "physics.optics",
      "cs.AI"
    ],
    "primary_category": "physics.optics",
    "comment": "12 pages, 6 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.15231v2",
    "published_date": "2024-04-23 17:03:53 UTC",
    "updated_date": "2024-04-24 15:23:47 UTC"
  },
  {
    "arxiv_id": "2404.15392v1",
    "title": "Naïve Bayes and Random Forest for Crop Yield Prediction",
    "authors": [
      "Abbas Maazallahi",
      "Sreehari Thota",
      "Naga Prasad Kondaboina",
      "Vineetha Muktineni",
      "Deepthi Annem",
      "Abhi Stephen Rokkam",
      "Mohammad Hossein Amini",
      "Mohammad Amir Salari",
      "Payam Norouzzadeh",
      "Eli Snir",
      "Bahareh Rahmani"
    ],
    "abstract": "This study analyzes crop yield prediction in India from 1997 to 2020,\nfocusing on various crops and key environmental factors. It aims to predict\nagricultural yields by utilizing advanced machine learning techniques like\nLinear Regression, Decision Tree, KNN, Na\\\"ive Bayes, K-Mean Clustering, and\nRandom Forest. The models, particularly Na\\\"ive Bayes and Random Forest,\ndemonstrate high effectiveness, as shown through data visualizations. The\nresearch concludes that integrating these analytical methods significantly\nenhances the accuracy and reliability of crop yield predictions, offering vital\ncontributions to agricultural data science.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15392v1",
    "published_date": "2024-04-23 16:55:45 UTC",
    "updated_date": "2024-04-23 16:55:45 UTC"
  },
  {
    "arxiv_id": "2404.15224v1",
    "title": "Deep Models for Multi-View 3D Object Recognition: A Review",
    "authors": [
      "Mona Alzahrani",
      "Muhammad Usman",
      "Salma Kammoun",
      "Saeed Anwar",
      "Tarek Helmy"
    ],
    "abstract": "Human decision-making often relies on visual information from multiple\nperspectives or views. In contrast, machine learning-based object recognition\nutilizes information from a single image of the object. However, the\ninformation conveyed by a single image may not be sufficient for accurate\ndecision-making, particularly in complex recognition problems. The utilization\nof multi-view 3D representations for object recognition has thus far\ndemonstrated the most promising results for achieving state-of-the-art\nperformance. This review paper comprehensively covers recent progress in\nmulti-view 3D object recognition methods for 3D classification and retrieval\ntasks. Specifically, we focus on deep learning-based and transformer-based\ntechniques, as they are widely utilized and have achieved state-of-the-art\nperformance. We provide detailed information about existing deep learning-based\nand transformer-based multi-view 3D object recognition models, including the\nmost commonly used 3D datasets, camera configurations and number of views, view\nselection strategies, pre-trained CNN architectures, fusion strategies, and\nrecognition performance on 3D classification and 3D retrieval tasks.\nAdditionally, we examine various computer vision applications that use\nmulti-view classification. Finally, we highlight key findings and future\ndirections for developing multi-view 3D object recognition methods to provide\nreaders with a comprehensive understanding of the field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15224v1",
    "published_date": "2024-04-23 16:54:31 UTC",
    "updated_date": "2024-04-23 16:54:31 UTC"
  },
  {
    "arxiv_id": "2404.15390v2",
    "title": "Uncertainty in latent representations of variational autoencoders optimized for visual tasks",
    "authors": [
      "Josefina Catoni",
      "Domonkos Martos",
      "Ferenc Csikor",
      "Enzo Ferrante",
      "Diego H. Milone",
      "Balázs Meszéna",
      "Gergő Orbán",
      "Rodrigo Echeveste"
    ],
    "abstract": "Deep Generative Models (DGMs) can learn flexible latent variable\nrepresentations of images while avoiding intractable computations, common in\nBayesian inference. However, investigating the properties of inference in\nVariational Autoencoders (VAEs), a major class of DGMs, reveals severe problems\nin their uncertainty representations. Here we draw inspiration from classical\ncomputer vision to introduce an inductive bias into the VAE by incorporating a\nglobal explaining-away latent variable, which remedies defective inference in\nVAEs. Unlike standard VAEs, the Explaing-Away VAE (EA-VAE) provides uncertainty\nestimates that align with normative requirements across a wide spectrum of\nperceptual tasks, including image corruption, interpolation, and\nout-of-distribution detection. We find that restored inference capabilities are\ndelivered by developing a motif in the inference network (the encoder) which is\nwidespread in biological neural networks: divisive normalization. Our results\nestablish EA-VAEs as reliable tools to perform inference under deep generative\nmodels with appropriate estimates of uncertainty.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15390v2",
    "published_date": "2024-04-23 16:26:29 UTC",
    "updated_date": "2025-01-23 19:13:33 UTC"
  },
  {
    "arxiv_id": "2406.06538v1",
    "title": "Understanding attention-based encoder-decoder networks: a case study with chess scoresheet recognition",
    "authors": [
      "Sergio Y. Hayashi",
      "Nina S. T. Hirata"
    ],
    "abstract": "Deep neural networks are largely used for complex prediction tasks. There is\nplenty of empirical evidence of their successful end-to-end training for a\ndiversity of tasks. Success is often measured based solely on the final\nperformance of the trained network, and explanations on when, why and how they\nwork are less emphasized. In this paper we study encoder-decoder recurrent\nneural networks with attention mechanisms for the task of reading handwritten\nchess scoresheets. Rather than prediction performance, our concern is to better\nunderstand how learning occurs in these type of networks. We characterize the\ntask in terms of three subtasks, namely input-output alignment, sequential\npattern recognition, and handwriting recognition, and experimentally\ninvestigate which factors affect their learning. We identify competition,\ncollaboration and dependence relations between the subtasks, and argue that\nsuch knowledge might help one to better balance factors to properly train a\nnetwork.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This work was accepted and published in the 2022 26th International\n  Conference on Pattern Recognition (ICPR)",
    "pdf_url": "http://arxiv.org/pdf/2406.06538v1",
    "published_date": "2024-04-23 16:23:18 UTC",
    "updated_date": "2024-04-23 16:23:18 UTC"
  },
  {
    "arxiv_id": "2404.15141v1",
    "title": "CutDiffusion: A Simple, Fast, Cheap, and Strong Diffusion Extrapolation Method",
    "authors": [
      "Mingbao Lin",
      "Zhihang Lin",
      "Wengyi Zhan",
      "Liujuan Cao",
      "Rongrong Ji"
    ],
    "abstract": "Transforming large pre-trained low-resolution diffusion models to cater to\nhigher-resolution demands, i.e., diffusion extrapolation, significantly\nimproves diffusion adaptability. We propose tuning-free CutDiffusion, aimed at\nsimplifying and accelerating the diffusion extrapolation process, making it\nmore affordable and improving performance. CutDiffusion abides by the existing\npatch-wise extrapolation but cuts a standard patch diffusion process into an\ninitial phase focused on comprehensive structure denoising and a subsequent\nphase dedicated to specific detail refinement. Comprehensive experiments\nhighlight the numerous almighty advantages of CutDiffusion: (1) simple method\nconstruction that enables a concise higher-resolution diffusion process without\nthird-party engagement; (2) fast inference speed achieved through a single-step\nhigher-resolution diffusion process, and fewer inference patches required; (3)\ncheap GPU cost resulting from patch-wise inference and fewer patches during the\ncomprehensive structure denoising; (4) strong generation performance, stemming\nfrom the emphasis on specific detail refinement.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15141v1",
    "published_date": "2024-04-23 15:47:58 UTC",
    "updated_date": "2024-04-23 15:47:58 UTC"
  },
  {
    "arxiv_id": "2404.15121v1",
    "title": "Taming Diffusion Probabilistic Models for Character Control",
    "authors": [
      "Rui Chen",
      "Mingyi Shi",
      "Shaoli Huang",
      "Ping Tan",
      "Taku Komura",
      "Xuelin Chen"
    ],
    "abstract": "We present a novel character control framework that effectively utilizes\nmotion diffusion probabilistic models to generate high-quality and diverse\ncharacter animations, responding in real-time to a variety of dynamic\nuser-supplied control signals. At the heart of our method lies a\ntransformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM),\nwhich takes as input the character's historical motion and can generate a range\nof diverse potential future motions conditioned on high-level, coarse user\ncontrol. To meet the demands for diversity, controllability, and computational\nefficiency required by a real-time controller, we incorporate several key\nalgorithmic designs. These include separate condition tokenization,\nclassifier-free guidance on past motion, and heuristic future trajectory\nextension, all designed to address the challenges associated with taming motion\ndiffusion probabilistic models for character control. As a result, our work\nrepresents the first model that enables real-time generation of high-quality,\ndiverse character animations based on user interactive control, supporting\nanimating the character in multiple styles with a single unified model. We\nevaluate our method on a diverse set of locomotion skills, demonstrating the\nmerits of our method over existing character controllers. Project page and\nsource codes: https://aiganimation.github.io/CAMDM/",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "Accepted by SIGGRAPH 2024 (Conference Track). Project page and source\n  codes: https://aiganimation.github.io/CAMDM/",
    "pdf_url": "http://arxiv.org/pdf/2404.15121v1",
    "published_date": "2024-04-23 15:20:17 UTC",
    "updated_date": "2024-04-23 15:20:17 UTC"
  },
  {
    "arxiv_id": "2405.00706v3",
    "title": "From Complexity to Clarity: How AI Enhances Perceptions of Scientists and the Public's Understanding of Science",
    "authors": [
      "David M. Markowitz"
    ],
    "abstract": "This paper evaluated the effectiveness of using generative AI to simplify\nscience communication and enhance the public's understanding of science. By\ncomparing lay summaries of journal articles from PNAS, yoked to those generated\nby AI, this work first assessed linguistic simplicity differences across such\nsummaries and public perceptions in follow-up experiments. Specifically, Study\n1a analyzed simplicity features of PNAS abstracts (scientific summaries) and\nsignificance statements (lay summaries), observing that lay summaries were\nindeed linguistically simpler, but effect size differences were small. Study 1b\nused a large language model, GPT-4, to create significance statements based on\npaper abstracts and this more than doubled the average effect size without\nfine-tuning. Study 2 experimentally demonstrated that simply-written GPT\nsummaries facilitated more favorable perceptions of scientists (they were\nperceived as more credible and trustworthy, but less intelligent) than more\ncomplexly-written human PNAS summaries. Crucially, Study 3 experimentally\ndemonstrated that participants comprehended scientific writing better after\nreading simple GPT summaries compared to complex PNAS summaries. In their own\nwords, participants also summarized scientific papers in a more detailed and\nconcrete manner after reading GPT summaries compared to PNAS summaries of the\nsame article. AI has the potential to engage scientific communities and the\npublic via a simple language heuristic, advocating for its integration into\nscientific dissemination for a more informed society.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.00706v3",
    "published_date": "2024-04-23 14:43:35 UTC",
    "updated_date": "2024-08-28 15:29:10 UTC"
  },
  {
    "arxiv_id": "2404.15388v1",
    "title": "ML-based identification of the interface regions for coupling local and nonlocal models",
    "authors": [
      "Noujoud Nader",
      "Patrick Diehl",
      "Marta D'Elia",
      "Christian Glusa",
      "Serge Prudhomme"
    ],
    "abstract": "Local-nonlocal coupling approaches combine the computational efficiency of\nlocal models and the accuracy of nonlocal models. However, the coupling process\nis challenging, requiring expertise to identify the interface between local and\nnonlocal regions. This study introduces a machine learning-based approach to\nautomatically detect the regions in which the local and nonlocal models should\nbe used in a coupling approach. This identification process uses the loading\nfunctions and provides as output the selected model at the grid points.\nTraining is based on datasets of loading functions for which reference coupling\nconfigurations are computed using accurate coupled solutions, where accuracy is\nmeasured in terms of the relative error between the solution to the coupling\napproach and the solution to the nonlocal model. We study two approaches that\ndiffer from one another in terms of the data structure. The first approach,\nreferred to as the full-domain input data approach, inputs the full load vector\nand outputs a full label vector. In this case, the classification process is\ncarried out globally. The second approach consists of a window-based approach,\nwhere loads are preprocessed and partitioned into windows and the problem is\nformulated as a node-wise classification approach in which the central point of\neach window is treated individually. The classification problems are solved via\ndeep learning algorithms based on convolutional neural networks. The\nperformance of these approaches is studied on one-dimensional numerical\nexamples using F1-scores and accuracy metrics. In particular, it is shown that\nthe windowing approach provides promising results, achieving an accuracy of\n0.96 and an F1-score of 0.97. These results underscore the potential of the\napproach to automate coupling processes, leading to more accurate and\ncomputationally efficient solutions for material science applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 14 figures, research paper",
    "pdf_url": "http://arxiv.org/pdf/2404.15388v1",
    "published_date": "2024-04-23 14:19:36 UTC",
    "updated_date": "2024-04-23 14:19:36 UTC"
  },
  {
    "arxiv_id": "2404.15070v2",
    "title": "BotDGT: Dynamicity-aware Social Bot Detection with Dynamic Graph Transformers",
    "authors": [
      "Buyun He",
      "Yingguang Yang",
      "Qi Wu",
      "Hao Liu",
      "Renyu Yang",
      "Hao Peng",
      "Xiang Wang",
      "Yong Liao",
      "Pengyuan Zhou"
    ],
    "abstract": "Detecting social bots has evolved into a pivotal yet intricate task, aimed at\ncombating the dissemination of misinformation and preserving the authenticity\nof online interactions. While earlier graph-based approaches, which leverage\ntopological structure of social networks, yielded notable outcomes, they\noverlooked the inherent dynamicity of social networks -- In reality, they\nlargely depicted the social network as a static graph and solely relied on its\nmost recent state. Due to the absence of dynamicity modeling, such approaches\nare vulnerable to evasion, particularly when advanced social bots interact with\nother users to camouflage identities and escape detection. To tackle these\nchallenges, we propose BotDGT, a novel framework that not only considers the\ntopological structure, but also effectively incorporates dynamic nature of\nsocial network. Specifically, we characterize a social network as a dynamic\ngraph. A structural module is employed to acquire topological information from\neach historical snapshot. Additionally, a temporal module is proposed to\nintegrate historical context and model the evolving behavior patterns exhibited\nby social bots and legitimate users. Experimental results demonstrate the\nsuperiority of BotDGT against the leading methods that neglected the dynamic\nnature of social networks in terms of accuracy, recall, and F1-score.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.15070v2",
    "published_date": "2024-04-23 14:19:13 UTC",
    "updated_date": "2024-04-24 08:01:53 UTC"
  },
  {
    "arxiv_id": "2404.15065v2",
    "title": "Formal Verification of Graph Convolutional Networks with Uncertain Node Features and Uncertain Graph Structure",
    "authors": [
      "Tobias Ladner",
      "Michael Eichelbeck",
      "Matthias Althoff"
    ],
    "abstract": "Graph neural networks are becoming increasingly popular in the field of\nmachine learning due to their unique ability to process data structured in\ngraphs. They have also been applied in safety-critical environments where\nperturbations inherently occur. However, these perturbations require us to\nformally verify neural networks before their deployment in safety-critical\nenvironments as neural networks are prone to adversarial attacks. While there\nexists research on the formal verification of neural networks, there is no work\nverifying the robustness of generic graph convolutional network architectures\nwith uncertainty in the node features and in the graph structure over multiple\nmessage-passing steps. This work addresses this research gap by explicitly\npreserving the non-convex dependencies of all elements in the underlying\ncomputations through reachability analysis with (matrix) polynomial zonotopes.\nWe demonstrate our approach on three popular benchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "published at Transactions on Machine Learning Research (TMLR) 2025",
    "pdf_url": "http://arxiv.org/pdf/2404.15065v2",
    "published_date": "2024-04-23 14:12:48 UTC",
    "updated_date": "2025-04-16 13:23:25 UTC"
  },
  {
    "arxiv_id": "2404.15059v1",
    "title": "Using deep reinforcement learning to promote sustainable human behaviour on a common pool resource problem",
    "authors": [
      "Raphael Koster",
      "Miruna Pîslar",
      "Andrea Tacchetti",
      "Jan Balaguer",
      "Leqi Liu",
      "Romuald Elie",
      "Oliver P. Hauser",
      "Karl Tuyls",
      "Matt Botvinick",
      "Christopher Summerfield"
    ],
    "abstract": "A canonical social dilemma arises when finite resources are allocated to a\ngroup of people, who can choose to either reciprocate with interest, or keep\nthe proceeds for themselves. What resource allocation mechanisms will encourage\nlevels of reciprocation that sustain the commons? Here, in an iterated\nmultiplayer trust game, we use deep reinforcement learning (RL) to design an\nallocation mechanism that endogenously promotes sustainable contributions from\nhuman participants to a common pool resource. We first trained neural networks\nto behave like human players, creating a stimulated economy that allowed us to\nstudy how different mechanisms influenced the dynamics of receipt and\nreciprocation. We then used RL to train a social planner to maximise aggregate\nreturn to players. The social planner discovered a redistributive policy that\nled to a large surplus and an inclusive economy, in which players made roughly\nequal gains. The RL agent increased human surplus over baseline mechanisms\nbased on unrestricted welfare or conditional cooperation, by conditioning its\ngenerosity on available resources and temporarily sanctioning defectors by\nallocating fewer resources to them. Examining the AI policy allowed us to\ndevelop an explainable mechanism that performed similarly and was more popular\namong players. Deep reinforcement learning can be used to discover mechanisms\nthat promote sustainable human behaviour.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15059v1",
    "published_date": "2024-04-23 14:07:39 UTC",
    "updated_date": "2024-04-23 14:07:39 UTC"
  },
  {
    "arxiv_id": "2404.15058v1",
    "title": "A Mechanism-Based Approach to Mitigating Harms from Persuasive Generative AI",
    "authors": [
      "Seliem El-Sayed",
      "Canfer Akbulut",
      "Amanda McCroskery",
      "Geoff Keeling",
      "Zachary Kenton",
      "Zaria Jalan",
      "Nahema Marchal",
      "Arianna Manzini",
      "Toby Shevlane",
      "Shannon Vallor",
      "Daniel Susser",
      "Matija Franklin",
      "Sophie Bridgers",
      "Harry Law",
      "Matthew Rahtz",
      "Murray Shanahan",
      "Michael Henry Tessler",
      "Arthur Douillard",
      "Tom Everitt",
      "Sasha Brown"
    ],
    "abstract": "Recent generative AI systems have demonstrated more advanced persuasive\ncapabilities and are increasingly permeating areas of life where they can\ninfluence decision-making. Generative AI presents a new risk profile of\npersuasion due the opportunity for reciprocal exchange and prolonged\ninteractions. This has led to growing concerns about harms from AI persuasion\nand how they can be mitigated, highlighting the need for a systematic study of\nAI persuasion. The current definitions of AI persuasion are unclear and related\nharms are insufficiently studied. Existing harm mitigation approaches\nprioritise harms from the outcome of persuasion over harms from the process of\npersuasion. In this paper, we lay the groundwork for the systematic study of AI\npersuasion. We first put forward definitions of persuasive generative AI. We\ndistinguish between rationally persuasive generative AI, which relies on\nproviding relevant facts, sound reasoning, or other forms of trustworthy\nevidence, and manipulative generative AI, which relies on taking advantage of\ncognitive biases and heuristics or misrepresenting information. We also put\nforward a map of harms from AI persuasion, including definitions and examples\nof economic, physical, environmental, psychological, sociocultural, political,\nprivacy, and autonomy harm. We then introduce a map of mechanisms that\ncontribute to harmful persuasion. Lastly, we provide an overview of approaches\nthat can be used to mitigate against process harms of persuasion, including\nprompt engineering for manipulation classification and red teaming. Future work\nwill operationalise these mitigations and study the interaction between\ndifferent types of mechanisms of persuasion.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15058v1",
    "published_date": "2024-04-23 14:07:20 UTC",
    "updated_date": "2024-04-23 14:07:20 UTC"
  },
  {
    "arxiv_id": "2404.15045v1",
    "title": "Multi-Head Mixture-of-Experts",
    "authors": [
      "Xun Wu",
      "Shaohan Huang",
      "Wenhui Wang",
      "Furu Wei"
    ],
    "abstract": "Sparse Mixtures of Experts (SMoE) scales model capacity without significant\nincreases in training and inference costs, but exhibits the following two\nissues: (1) Low expert activation, where only a small subset of experts are\nactivated for optimization. (2) Lacking fine-grained analytical capabilities\nfor multiple semantic concepts within individual tokens. We propose Multi-Head\nMixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split each\ntoken into multiple sub-tokens. These sub-tokens are then assigned to and\nprocessed by a diverse set of experts in parallel, and seamlessly reintegrated\ninto the original token form. The multi-head mechanism enables the model to\ncollectively attend to information from various representation spaces within\ndifferent experts, while significantly enhances expert activation, thus deepens\ncontext understanding and alleviate overfitting. Moreover, our MH-MoE is\nstraightforward to implement and decouples from other SMoE optimization\nmethods, making it easy to integrate with other SMoE models for enhanced\nperformance. Extensive experimental results across three tasks: English-focused\nlanguage modeling, Multi-lingual language modeling and Masked multi-modality\nmodeling tasks, demonstrate the effectiveness of MH-MoE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15045v1",
    "published_date": "2024-04-23 13:47:09 UTC",
    "updated_date": "2024-04-23 13:47:09 UTC"
  },
  {
    "arxiv_id": "2404.15042v2",
    "title": "Leverage Variational Graph Representation For Model Poisoning on Federated Learning",
    "authors": [
      "Kai Li",
      "Xin Yuan",
      "Jingjing Zheng",
      "Wei Ni",
      "Falko Dressler",
      "Abbas Jamalipour"
    ],
    "abstract": "This paper puts forth a new training data-untethered model poisoning (MP)\nattack on federated learning (FL). The new MP attack extends an adversarial\nvariational graph autoencoder (VGAE) to create malicious local models based\nsolely on the benign local models overheard without any access to the training\ndata of FL. Such an advancement leads to the VGAE-MP attack that is not only\nefficacious but also remains elusive to detection. VGAE-MP attack extracts\ngraph structural correlations among the benign local models and the training\ndata features, adversarially regenerates the graph structure, and generates\nmalicious local models using the adversarial graph structure and benign models'\nfeatures. Moreover, a new attacking algorithm is presented to train the\nmalicious local models using VGAE and sub-gradient descent, while enabling an\noptimal selection of the benign local models for training the VGAE. Experiments\ndemonstrate a gradual drop in FL accuracy under the proposed VGAE-MP attack and\nthe ineffectiveness of existing defense mechanisms in detecting the attack,\nposing a severe threat to FL.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "12 pages, 8 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.15042v2",
    "published_date": "2024-04-23 13:43:56 UTC",
    "updated_date": "2024-04-24 16:08:50 UTC"
  },
  {
    "arxiv_id": "2404.15034v1",
    "title": "Deep Multi-View Channel-Wise Spatio-Temporal Network for Traffic Flow Prediction",
    "authors": [
      "Hao Miao",
      "Senzhang Wang",
      "Meiyue Zhang",
      "Diansheng Guo",
      "Funing Sun",
      "Fan Yang"
    ],
    "abstract": "Accurately forecasting traffic flows is critically important to many real\napplications including public safety and intelligent transportation systems.\nThe challenges of this problem include both the dynamic mobility patterns of\nthe people and the complex spatial-temporal correlations of the urban traffic\ndata. Meanwhile, most existing models ignore the diverse impacts of the various\ntraffic observations (e.g. vehicle speed and road occupancy) on the traffic\nflow prediction, and different traffic observations can be considered as\ndifferent channels of input features. We argue that the analysis in\nmultiple-channel traffic observations might help to better address this\nproblem. In this paper, we study the novel problem of multi-channel traffic\nflow prediction, and propose a deep \\underline{M}ulti-\\underline{V}iew\n\\underline{C}hannel-wise \\underline{S}patio-\\underline{T}emporal\n\\underline{Net}work (MVC-STNet) model to effectively address it. Specifically,\nwe first construct the localized and globalized spatial graph where the\nmulti-view fusion module is used to effectively extract the local and global\nspatial dependencies. Then LSTM is used to learn the temporal correlations. To\neffectively model the different impacts of various traffic observations on\ntraffic flow prediction, a channel-wise graph convolutional network is also\ndesigned. Extensive experiments are conducted over the PEMS04 and PEMS08\ndatasets. The results demonstrate that the proposed MVC-STNet outperforms\nstate-of-the-art methods by a large margin.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI2020 workshop",
    "pdf_url": "http://arxiv.org/pdf/2404.15034v1",
    "published_date": "2024-04-23 13:39:04 UTC",
    "updated_date": "2024-04-23 13:39:04 UTC"
  },
  {
    "arxiv_id": "2404.15022v1",
    "title": "A review of deep learning-based information fusion techniques for multimodal medical image classification",
    "authors": [
      "Yihao Li",
      "Mostafa El Habib Daho",
      "Pierre-Henri Conze",
      "Rachid Zeghlache",
      "Hugo Le Boité",
      "Ramin Tadayoni",
      "Béatrice Cochener",
      "Mathieu Lamard",
      "Gwenolé Quellec"
    ],
    "abstract": "Multimodal medical imaging plays a pivotal role in clinical diagnosis and\nresearch, as it combines information from various imaging modalities to provide\na more comprehensive understanding of the underlying pathology. Recently, deep\nlearning-based multimodal fusion techniques have emerged as powerful tools for\nimproving medical image classification. This review offers a thorough analysis\nof the developments in deep learning-based multimodal fusion for medical\nclassification tasks. We explore the complementary relationships among\nprevalent clinical modalities and outline three main fusion schemes for\nmultimodal classification networks: input fusion, intermediate fusion\n(encompassing single-level fusion, hierarchical fusion, and attention-based\nfusion), and output fusion. By evaluating the performance of these fusion\ntechniques, we provide insight into the suitability of different network\narchitectures for various multimodal fusion scenarios and application domains.\nFurthermore, we delve into challenges related to network architecture\nselection, handling incomplete multimodal data management, and the potential\nlimitations of multimodal fusion. Finally, we spotlight the promising future of\nTransformer-based multimodal fusion techniques and give recommendations for\nfuture research in this rapidly evolving field.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15022v1",
    "published_date": "2024-04-23 13:31:18 UTC",
    "updated_date": "2024-04-23 13:31:18 UTC"
  },
  {
    "arxiv_id": "2404.14994v3",
    "title": "Transformers Can Represent $n$-gram Language Models",
    "authors": [
      "Anej Svete",
      "Ryan Cotterell"
    ],
    "abstract": "Existing work has analyzed the representational capacity of the transformer\narchitecture by means of formal models of computation. However, the focus so\nfar has been on analyzing the architecture in terms of language\n\\emph{acceptance}. We contend that this is an ill-suited problem in the study\nof \\emph{language models} (LMs), which are definitionally \\emph{probability\ndistributions} over strings. In this paper, we focus on the relationship\nbetween transformer LMs and $n$-gram LMs, a simple and historically relevant\nclass of language models. We show that transformer LMs using the hard or sparse\nattention mechanisms can exactly represent any $n$-gram LM, giving us a\nconcrete lower bound on their probabilistic representational capacity. This\nprovides a first step towards understanding the mechanisms that transformer LMs\ncan use to represent probability distributions over strings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CC",
      "cs.FL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14994v3",
    "published_date": "2024-04-23 12:51:37 UTC",
    "updated_date": "2024-06-20 15:21:23 UTC"
  },
  {
    "arxiv_id": "2404.14986v1",
    "title": "$\\texttt{MiniMol}$: A Parameter-Efficient Foundation Model for Molecular Learning",
    "authors": [
      "Kerstin Kläser",
      "Błażej Banaszewski",
      "Samuel Maddrell-Mander",
      "Callum McLean",
      "Luis Müller",
      "Ali Parviz",
      "Shenyang Huang",
      "Andrew Fitzgibbon"
    ],
    "abstract": "In biological tasks, data is rarely plentiful as it is generated from\nhard-to-gather measurements. Therefore, pre-training foundation models on large\nquantities of available data and then transfer to low-data downstream tasks is\na promising direction. However, how to design effective foundation models for\nmolecular learning remains an open question, with existing approaches typically\nfocusing on models with large parameter capacities. In this work, we propose\n$\\texttt{MiniMol}$, a foundational model for molecular learning with 10 million\nparameters. $\\texttt{MiniMol}$ is pre-trained on a mix of roughly 3300 sparsely\ndefined graph- and node-level tasks of both quantum and biological nature. The\npre-training dataset includes approximately 6 million molecules and 500 million\nlabels. To demonstrate the generalizability of $\\texttt{MiniMol}$ across tasks,\nwe evaluate it on downstream tasks from the Therapeutic Data Commons (TDC)\nADMET group showing significant improvements over the prior state-of-the-art\nfoundation model across 17 tasks. $\\texttt{MiniMol}$ will be a public and\nopen-sourced model for future research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14986v1",
    "published_date": "2024-04-23 12:43:15 UTC",
    "updated_date": "2024-04-23 12:43:15 UTC"
  },
  {
    "arxiv_id": "2404.14979v3",
    "title": "SGFormer: Spherical Geometry Transformer for 360 Depth Estimation",
    "authors": [
      "Junsong Zhang",
      "Zisong Chen",
      "Chunyu Lin",
      "Lang Nie",
      "Zhijie Shen",
      "Kang Liao",
      "Junda Huang",
      "Yao Zhao"
    ],
    "abstract": "Panoramic distortion poses a significant challenge in 360 depth estimation,\nparticularly pronounced at the north and south poles. Existing methods either\nadopt a bi-projection fusion strategy to remove distortions or model long-range\ndependencies to capture global structures, which can result in either unclear\nstructure or insufficient local perception. In this paper, we propose a\nspherical geometry transformer, named SGFormer, to address the above issues,\nwith an innovative step to integrate spherical geometric priors into vision\ntransformers. To this end, we retarget the transformer decoder to a spherical\nprior decoder (termed SPDecoder), which endeavors to uphold the integrity of\nspherical structures during decoding. Concretely, we leverage bipolar\nre-projection, circular rotation, and curve local embedding to preserve the\nspherical characteristics of equidistortion, continuity, and surface distance,\nrespectively. Furthermore, we present a query-based global conditional position\nembedding to compensate for spatial structure at varying resolutions. It not\nonly boosts the global perception of spatial position but also sharpens the\ndepth structure across different patches. Finally, we conduct extensive\nexperiments on popular benchmarks, demonstrating our superiority over\nstate-of-the-art solutions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14979v3",
    "published_date": "2024-04-23 12:36:24 UTC",
    "updated_date": "2025-02-25 15:14:30 UTC"
  },
  {
    "arxiv_id": "2406.06537v1",
    "title": "Interactive Generation of Laparoscopic Videos with Diffusion Models",
    "authors": [
      "Ivan Iliash",
      "Simeon Allmendinger",
      "Felix Meissen",
      "Niklas Kühl",
      "Daniel Rückert"
    ],
    "abstract": "Generative AI, in general, and synthetic visual data generation, in specific,\nhold much promise for benefiting surgical training by providing photorealism to\nsimulation environments. Current training methods primarily rely on reading\nmaterials and observing live surgeries, which can be time-consuming and\nimpractical. In this work, we take a significant step towards improving the\ntraining process. Specifically, we use diffusion models in combination with a\nzero-shot video diffusion method to interactively generate realistic\nlaparoscopic images and videos by specifying a surgical action through text and\nguiding the generation with tool positions through segmentation masks. We\ndemonstrate the performance of our approach using the publicly available Cholec\ndataset family and evaluate the fidelity and factual correctness of our\ngenerated images using a surgical action recognition model as well as the\npixel-wise F1-score for the spatial control of tool generation. We achieve an\nFID of 38.097 and an F1-score of 0.71.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "7 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.06537v1",
    "published_date": "2024-04-23 12:36:07 UTC",
    "updated_date": "2024-04-23 12:36:07 UTC"
  },
  {
    "arxiv_id": "2404.16880v3",
    "title": "Atomas: Hierarchical Alignment on Molecule-Text for Unified Molecule Understanding and Generation",
    "authors": [
      "Yikun Zhang",
      "Geyan Ye",
      "Chaohao Yuan",
      "Bo Han",
      "Long-Kai Huang",
      "Jianhua Yao",
      "Wei Liu",
      "Yu Rong"
    ],
    "abstract": "Molecule-and-text cross-modal representation learning has emerged as a\npromising direction for enhancing the quality of molecular representation,\nthereby improving performance in various scientific fields. However, most\napproaches employ a global alignment approach to learn the knowledge from\ndifferent modalities that may fail to capture fine-grained information, such as\nmolecule-and-text fragments and stereoisomeric nuances, which is crucial for\ndownstream tasks. Furthermore, it is incapable of modeling such information\nusing a similar global alignment strategy due to the lack of annotations about\nthe fine-grained fragments in the existing dataset. In this paper, we propose\nAtomas, a hierarchical molecular representation learning framework that jointly\nlearns representations from SMILES strings and text. We design a Hierarchical\nAdaptive Alignment model to automatically learn the fine-grained fragment\ncorrespondence between two modalities and align these representations at three\nsemantic levels. Atomas's end-to-end training framework supports understanding\nand generating molecules, enabling a wider range of downstream tasks. Atomas\nachieves superior performance across 12 tasks on 11 datasets, outperforming 11\nbaseline models thus highlighting the effectiveness and versatility of our\nmethod. Scaling experiments further demonstrate Atomas's robustness and\nscalability. Moreover, visualization and qualitative analysis, validated by\nhuman experts, confirm the chemical relevance of our approach. Codes are\nreleased on https://github.com/yikunpku/Atomas.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16880v3",
    "published_date": "2024-04-23 12:35:44 UTC",
    "updated_date": "2025-03-03 16:34:19 UTC"
  },
  {
    "arxiv_id": "2404.14967v1",
    "title": "CoARF: Controllable 3D Artistic Style Transfer for Radiance Fields",
    "authors": [
      "Deheng Zhang",
      "Clara Fernandez-Labrador",
      "Christopher Schroers"
    ],
    "abstract": "Creating artistic 3D scenes can be time-consuming and requires specialized\nknowledge. To address this, recent works such as ARF, use a radiance\nfield-based approach with style constraints to generate 3D scenes that resemble\na style image provided by the user. However, these methods lack fine-grained\ncontrol over the resulting scenes. In this paper, we introduce Controllable\nArtistic Radiance Fields (CoARF), a novel algorithm for controllable 3D scene\nstylization. CoARF enables style transfer for specified objects, compositional\n3D style transfer and semantic-aware style transfer. We achieve controllability\nusing segmentation masks with different label-dependent loss functions. We also\npropose a semantic-aware nearest neighbor matching algorithm to improve the\nstyle transfer quality. Our extensive experiments demonstrate that CoARF\nprovides user-specified controllability of style transfer and superior style\ntransfer quality with more precise feature matching.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "International Conference on 3D Vision 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.14967v1",
    "published_date": "2024-04-23 12:22:32 UTC",
    "updated_date": "2024-04-23 12:22:32 UTC"
  },
  {
    "arxiv_id": "2404.14966v2",
    "title": "Mamba3D: Enhancing Local Features for 3D Point Cloud Analysis via State Space Model",
    "authors": [
      "Xu Han",
      "Yuan Tang",
      "Zhaoxuan Wang",
      "Xianzhi Li"
    ],
    "abstract": "Existing Transformer-based models for point cloud analysis suffer from\nquadratic complexity, leading to compromised point cloud resolution and\ninformation loss. In contrast, the newly proposed Mamba model, based on state\nspace models (SSM), outperforms Transformer in multiple areas with only linear\ncomplexity. However, the straightforward adoption of Mamba does not achieve\nsatisfactory performance on point cloud tasks. In this work, we present\nMamba3D, a state space model tailored for point cloud learning to enhance local\nfeature extraction, achieving superior performance, high efficiency, and\nscalability potential. Specifically, we propose a simple yet effective Local\nNorm Pooling (LNP) block to extract local geometric features. Additionally, to\nobtain better global features, we introduce a bidirectional SSM (bi-SSM) with\nboth a token forward SSM and a novel backward SSM that operates on the feature\nchannel. Extensive experimental results show that Mamba3D surpasses\nTransformer-based counterparts and concurrent works in multiple tasks, with or\nwithout pre-training. Notably, Mamba3D achieves multiple SoTA, including an\noverall accuracy of 92.6% (train from scratch) on the ScanObjectNN and 95.1%\n(with single-modal pre-training) on the ModelNet40 classification task, with\nonly linear complexity. Our code and weights are available at\nhttps://github.com/xhanxu/Mamba3D.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ACM MM 2024. Code and weights are available at\n  https://github.com/xhanxu/Mamba3D",
    "pdf_url": "http://arxiv.org/pdf/2404.14966v2",
    "published_date": "2024-04-23 12:20:27 UTC",
    "updated_date": "2024-09-02 12:55:04 UTC"
  },
  {
    "arxiv_id": "2404.14963v5",
    "title": "Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems",
    "authors": [
      "Qihuang Zhong",
      "Kang Wang",
      "Ziyang Xu",
      "Juhua Liu",
      "Liang Ding",
      "Bo Du"
    ],
    "abstract": "Chain-of-Thought (CoT) prompting has enhanced the performance of Large\nLanguage Models (LLMs) across various reasoning tasks. However, CoT still falls\nshort in dealing with complex math word problems, as it usually suffers from\nthree pitfalls: semantic misunderstanding errors, calculation errors, and\nstep-missing errors. Prior studies involve addressing the calculation errors\nand step-missing errors, but neglect the semantic misunderstanding errors,\nwhich is the major factor limiting the reasoning performance of LLMs. To this\nend, we propose a simple-yet-effective method, namely Deeply Understanding the\nProblems (DUP), to improve the LLMs' math problem-solving ability by addressing\nsemantic misunderstanding errors. The core of our method is to encourage the\nLLMs to deeply understand the problems and extract the key problem-solving\ninformation used for better reasoning. Extensive experiments on 10 diverse\nreasoning benchmarks show that our DUP method consistently outperforms the\nother counterparts by a large margin. More encouragingly, DUP achieves a new\nSOTA result on the GSM8K benchmark, with an accuracy of 97.1% under the\nzero-shot setting.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: { 10.1007/s11704-025-41102-z }",
    "pdf_url": "http://arxiv.org/pdf/2404.14963v5",
    "published_date": "2024-04-23 12:16:05 UTC",
    "updated_date": "2025-03-27 07:08:33 UTC"
  },
  {
    "arxiv_id": "2404.15386v1",
    "title": "Large-Scale Multipurpose Benchmark Datasets For Assessing Data-Driven Deep Learning Approaches For Water Distribution Networks",
    "authors": [
      "Andres Tello",
      "Huy Truong",
      "Alexander Lazovik",
      "Victoria Degeler"
    ],
    "abstract": "Currently, the number of common benchmark datasets that researchers can use\nstraight away for assessing data-driven deep learning approaches is very\nlimited. Most studies provide data as configuration files. It is still up to\neach practitioner to follow a particular data generation method and run\ncomputationally intensive simulations to obtain usable data for model training\nand evaluation. In this work, we provide a collection of datasets that includes\nseveral small and medium size publicly available Water Distribution Networks\n(WDNs), including Anytown, Modena, Balerma, C-Town, D-Town, L-Town, Ky1, Ky6,\nKy8, and Ky13. In total 1,394,400 hours of WDNs data operating under normal\nconditions is made available to the community.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at WDSA CCWI, Ferrara, Italy, July 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.15386v1",
    "published_date": "2024-04-23 11:58:40 UTC",
    "updated_date": "2024-04-23 11:58:40 UTC"
  },
  {
    "arxiv_id": "2404.14952v1",
    "title": "Leveraging Speech for Gesture Detection in Multimodal Communication",
    "authors": [
      "Esam Ghaleb",
      "Ilya Burenko",
      "Marlou Rasenberg",
      "Wim Pouw",
      "Ivan Toni",
      "Peter Uhrig",
      "Anna Wilson",
      "Judith Holler",
      "Aslı Özyürek",
      "Raquel Fernández"
    ],
    "abstract": "Gestures are inherent to human interaction and often complement speech in\nface-to-face communication, forming a multimodal communication system. An\nimportant task in gesture analysis is detecting a gesture's beginning and end.\nResearch on automatic gesture detection has primarily focused on visual and\nkinematic information to detect a limited set of isolated or silent gestures\nwith low variability, neglecting the integration of speech and vision signals\nto detect gestures that co-occur with speech. This work addresses this gap by\nfocusing on co-speech gesture detection, emphasising the synchrony between\nspeech and co-speech hand gestures. We address three main challenges: the\nvariability of gesture forms, the temporal misalignment between gesture and\nspeech onsets, and differences in sampling rate between modalities. We\ninvestigate extended speech time windows and employ separate backbone models\nfor each modality to address the temporal misalignment and sampling rate\ndifferences. We utilize Transformer encoders in cross-modal and early fusion\ntechniques to effectively align and integrate speech and skeletal sequences.\nThe study results show that combining visual and speech information\nsignificantly enhances gesture detection performance. Our findings indicate\nthat expanding the speech buffer beyond visual time segments improves\nperformance and that multimodal integration using cross-modal and early fusion\ntechniques outperforms baseline methods using unimodal and late fusion methods.\nAdditionally, we find a correlation between the models' gesture prediction\nconfidence and low-level speech frequency features potentially associated with\ngestures. Overall, the study provides a better understanding and detection\nmethods for co-speech gestures, facilitating the analysis of multimodal\ncommunication.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14952v1",
    "published_date": "2024-04-23 11:54:05 UTC",
    "updated_date": "2024-04-23 11:54:05 UTC"
  },
  {
    "arxiv_id": "2404.14941v1",
    "title": "Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph Neural Networks",
    "authors": [
      "Zhe Zhao",
      "Pengkun Wang",
      "Xu Wang",
      "Haibin Wen",
      "Xiaolong Xie",
      "Zhengyang Zhou",
      "Qingfu Zhang",
      "Yang Wang"
    ],
    "abstract": "Pre-training GNNs to extract transferable knowledge and apply it to\ndownstream tasks has become the de facto standard of graph representation\nlearning. Recent works focused on designing self-supervised pre-training tasks\nto extract useful and universal transferable knowledge from large-scale\nunlabeled data. However, they have to face an inevitable question: traditional\npre-training strategies that aim at extracting useful information about\npre-training tasks, may not extract all useful information about the downstream\ntask. In this paper, we reexamine the pre-training process within traditional\npre-training and fine-tuning frameworks from the perspective of Information\nBottleneck (IB) and confirm that the forgetting phenomenon in pre-training\nphase may cause detrimental effects on downstream tasks. Therefore, we propose\na novel \\underline{D}elayed \\underline{B}ottlenecking \\underline{P}re-training\n(DBP) framework which maintains as much as possible mutual information between\nlatent representations and training data during pre-training phase by\nsuppressing the compression operation and delays the compression operation to\nfine-tuning phase to make sure the compression can be guided with labeled\nfine-tuning data and downstream tasks. To achieve this, we design two\ninformation control objectives that can be directly optimized and further\nintegrate them into the actual model design. Extensive experiments on both\nchemistry and biology domains demonstrate the effectiveness of DBP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14941v1",
    "published_date": "2024-04-23 11:35:35 UTC",
    "updated_date": "2024-04-23 11:35:35 UTC"
  },
  {
    "arxiv_id": "2404.14933v1",
    "title": "Fin-Fed-OD: Federated Outlier Detection on Financial Tabular Data",
    "authors": [
      "Dayananda Herurkar",
      "Sebastian Palacio",
      "Ahmed Anwar",
      "Joern Hees",
      "Andreas Dengel"
    ],
    "abstract": "Anomaly detection in real-world scenarios poses challenges due to dynamic and\noften unknown anomaly distributions, requiring robust methods that operate\nunder an open-world assumption. This challenge is exacerbated in practical\nsettings, where models are employed by private organizations, precluding data\nsharing due to privacy and competitive concerns. Despite potential benefits,\nthe sharing of anomaly information across organizations is restricted. This\npaper addresses the question of enhancing outlier detection within individual\norganizations without compromising data confidentiality. We propose a novel\nmethod leveraging representation learning and federated learning techniques to\nimprove the detection of unknown anomalies. Specifically, our approach utilizes\nlatent representations obtained from client-owned autoencoders to refine the\ndecision boundary of inliers. Notably, only model parameters are shared between\norganizations, preserving data privacy. The efficacy of our proposed method is\nevaluated on two standard financial tabular datasets and an image dataset for\nanomaly detection in a distributed setting. The results demonstrate a strong\nimprovement in the classification of unknown outliers during the inference\nphase for each organization's model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14933v1",
    "published_date": "2024-04-23 11:22:04 UTC",
    "updated_date": "2024-04-23 11:22:04 UTC"
  },
  {
    "arxiv_id": "2404.14928v2",
    "title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
    "authors": [
      "Wenqi Fan",
      "Shijie Wang",
      "Jiani Huang",
      "Zhikai Chen",
      "Yu Song",
      "Wenzhuo Tang",
      "Haitao Mao",
      "Hui Liu",
      "Xiaorui Liu",
      "Dawei Yin",
      "Qing Li"
    ],
    "abstract": "Graphs play an important role in representing complex relationships in\nvarious domains like social networks, knowledge graphs, and molecular\ndiscovery. With the advent of deep learning, Graph Neural Networks (GNNs) have\nemerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the\nrepresentation and processing of graph structures. Recently, LLMs have\ndemonstrated unprecedented capabilities in language tasks and are widely\nadopted in a variety of applications such as computer vision and recommender\nsystems. This remarkable success has also attracted interest in applying LLMs\nto the graph domain. Increasing efforts have been made to explore the potential\nof LLMs in advancing Graph ML's generalization, transferability, and few-shot\nlearning ability. Meanwhile, graphs, especially knowledge graphs, are rich in\nreliable factual knowledge, which can be utilized to enhance the reasoning\ncapabilities of LLMs and potentially alleviate their limitations such as\nhallucinations and the lack of explainability. Given the rapid progress of this\nresearch direction, a systematic review summarizing the latest advancements for\nGraph ML in the era of LLMs is necessary to provide an in-depth understanding\nto researchers and practitioners. Therefore, in this survey, we first review\nthe recent developments in Graph ML. We then explore how LLMs can be utilized\nto enhance the quality of graph features, alleviate the reliance on labeled\ndata, and address challenges such as graph heterogeneity and\nout-of-distribution (OOD) generalization. Afterward, we delve into how graphs\ncan enhance LLMs, highlighting their abilities to enhance LLM pre-training and\ninference. Furthermore, we investigate various applications and discuss the\npotential future directions in this promising field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14928v2",
    "published_date": "2024-04-23 11:13:39 UTC",
    "updated_date": "2024-06-04 01:31:30 UTC"
  },
  {
    "arxiv_id": "2404.15385v1",
    "title": "Sum of Group Error Differences: A Critical Examination of Bias Evaluation in Biometric Verification and a Dual-Metric Measure",
    "authors": [
      "Alaa Elobaid",
      "Nathan Ramoly",
      "Lara Younes",
      "Symeon Papadopoulos",
      "Eirini Ntoutsi",
      "Ioannis Kompatsiaris"
    ],
    "abstract": "Biometric Verification (BV) systems often exhibit accuracy disparities across\ndifferent demographic groups, leading to biases in BV applications. Assessing\nand quantifying these biases is essential for ensuring the fairness of BV\nsystems. However, existing bias evaluation metrics in BV have limitations, such\nas focusing exclusively on match or non-match error rates, overlooking bias on\ndemographic groups with performance levels falling between the best and worst\nperformance levels, and neglecting the magnitude of the bias present.\n  This paper presents an in-depth analysis of the limitations of current bias\nevaluation metrics in BV and, through experimental analysis, demonstrates their\ncontextual suitability, merits, and limitations. Additionally, it introduces a\nnovel general-purpose bias evaluation measure for BV, the ``Sum of Group Error\nDifferences (SEDG)''. Our experimental results on controlled synthetic datasets\ndemonstrate the effectiveness of demographic bias quantification when using\nexisting metrics and our own proposed measure. We discuss the applicability of\nthe bias evaluation metrics in a set of simulated demographic bias scenarios\nand provide scenario-based metric recommendations. Our code is publicly\navailable under \\url{https://github.com/alaaobeid/SEDG}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15385v1",
    "published_date": "2024-04-23 10:59:44 UTC",
    "updated_date": "2024-04-23 10:59:44 UTC"
  },
  {
    "arxiv_id": "2404.15384v1",
    "title": "FL-TAC: Enhanced Fine-Tuning in Federated Learning via Low-Rank, Task-Specific Adapter Clustering",
    "authors": [
      "Siqi Ping",
      "Yuzhu Mao",
      "Yang Liu",
      "Xiao-Ping Zhang",
      "Wenbo Ding"
    ],
    "abstract": "Although large-scale pre-trained models hold great potential for adapting to\ndownstream tasks through fine-tuning, the performance of such fine-tuned models\nis often limited by the difficulty of collecting sufficient high-quality,\ntask-specific data. Federated Learning (FL) offers a promising solution by\nenabling fine-tuning across large-scale clients with a variety of task data,\nbut it is bottlenecked by significant communication overhead due to the\npre-trained models' extensive size. This paper addresses the high communication\ncost for fine-tuning large pre-trained models within FL frameworks through\nlow-rank fine-tuning. Specifically, we train a low-rank adapter for each\nindividual task on the client side, followed by server-side clustering for\nsimilar group of adapters to achieve task-specific aggregation. Extensive\nexperiments on various language and vision tasks, such as GLUE and\nCIFAR-10/100, reveal the evolution of task-specific adapters throughout the FL\ntraining process and verify the effectiveness of the proposed low-rank\ntask-specific adapter clustering (TAC) method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15384v1",
    "published_date": "2024-04-23 10:50:38 UTC",
    "updated_date": "2024-04-23 10:50:38 UTC"
  },
  {
    "arxiv_id": "2404.14906v1",
    "title": "Driver Activity Classification Using Generalizable Representations from Vision-Language Models",
    "authors": [
      "Ross Greer",
      "Mathias Viborg Andersen",
      "Andreas Møgelmose",
      "Mohan Trivedi"
    ],
    "abstract": "Driver activity classification is crucial for ensuring road safety, with\napplications ranging from driver assistance systems to autonomous vehicle\ncontrol transitions. In this paper, we present a novel approach leveraging\ngeneralizable representations from vision-language models for driver activity\nclassification. Our method employs a Semantic Representation Late Fusion Neural\nNetwork (SRLF-Net) to process synchronized video frames from multiple\nperspectives. Each frame is encoded using a pretrained vision-language encoder,\nand the resulting embeddings are fused to generate class probability\npredictions. By leveraging contrastively-learned vision-language\nrepresentations, our approach achieves robust performance across diverse driver\nactivities. We evaluate our method on the Naturalistic Driving Action\nRecognition Dataset, demonstrating strong accuracy across many classes. Our\nresults suggest that vision-language representations offer a promising avenue\nfor driver monitoring systems, providing both accuracy and interpretability\nthrough natural language descriptors.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14906v1",
    "published_date": "2024-04-23 10:42:24 UTC",
    "updated_date": "2024-04-23 10:42:24 UTC"
  },
  {
    "arxiv_id": "2404.14901v2",
    "title": "Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice",
    "authors": [
      "Ranim Khojah",
      "Mazen Mohamad",
      "Philipp Leitner",
      "Francisco Gomes de Oliveira Neto"
    ],
    "abstract": "Large Language Models (LLMs) are frequently discussed in academia and the\ngeneral public as support tools for virtually any use case that relies on the\nproduction of text, including software engineering. Currently there is much\ndebate, but little empirical evidence, regarding the practical usefulness of\nLLM-based tools such as ChatGPT for engineers in industry. We conduct an\nobservational study of 24 professional software engineers who have been using\nChatGPT over a period of one week in their jobs, and qualitatively analyse\ntheir dialogues with the chatbot as well as their overall experience (as\ncaptured by an exit survey). We find that, rather than expecting ChatGPT to\ngenerate ready-to-use software artifacts (e.g., code), practitioners more often\nuse ChatGPT to receive guidance on how to solve their tasks or learn about a\ntopic in more abstract terms. We also propose a theoretical framework for how\n(i) purpose of the interaction, (ii) internal factors (e.g., the user's\npersonality), and (iii) external factors (e.g., company policy) together shape\nthe experience (in terms of perceived usefulness and trust). We envision that\nour framework can be used by future research to further the academic discussion\non LLM usage by software engineering practitioners, and to serve as a reference\npoint for the design of future empirical LLM research in this domain.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted at the ACM International Conference on the Foundations of\n  Software Engineering (FSE) 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.14901v2",
    "published_date": "2024-04-23 10:34:16 UTC",
    "updated_date": "2024-05-21 12:53:30 UTC"
  },
  {
    "arxiv_id": "2404.14897v1",
    "title": "Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models",
    "authors": [
      "Chen Zhang",
      "Zhuorui Liu",
      "Dawei Song"
    ],
    "abstract": "With the increasingly giant scales of (causal) large language models (LLMs),\nthe inference efficiency comes as one of the core concerns along the improved\nperformance. In contrast to the memory footprint, the latency bottleneck seems\nto be of greater importance as there can be billions of requests to a LLM\n(e.g., GPT-4) per day. The bottleneck is mainly due to the autoregressive\ninnateness of LLMs, where tokens can only be generated sequentially during\ndecoding. To alleviate the bottleneck, the idea of speculative execution, which\noriginates from the field of computer architecture, is introduced to LLM\ndecoding in a \\textit{draft-then-verify} style. Under this regime, a sequence\nof tokens will be drafted in a fast pace by utilizing some heuristics, and then\nthe tokens shall be verified in parallel by the LLM. As the costly sequential\ninference is parallelized, LLM decoding speed can be significantly boosted.\nDriven by the success of LLMs in recent couple of years, a growing literature\nin this direction has emerged. Yet, there lacks a position survey to summarize\nthe current landscape and draw a roadmap for future development of this\npromising area. To meet this demand, we present the very first survey paper\nthat reviews and unifies literature of speculative execution in LLMs (e.g.,\nblockwise parallel decoding, speculative decoding, etc.) in a comprehensive\nframework and a systematic taxonomy. Based on the taxonomy, we present a\ncritical review and comparative analysis of the current arts. Finally we\nhighlight various key challenges and future directions to further develop the\narea.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 4 figures, 1 table, rejected from IJCAI 2024, revision in\n  progress",
    "pdf_url": "http://arxiv.org/pdf/2404.14897v1",
    "published_date": "2024-04-23 10:25:45 UTC",
    "updated_date": "2024-04-23 10:25:45 UTC"
  },
  {
    "arxiv_id": "2404.15383v1",
    "title": "WANDR: Intention-guided Human Motion Generation",
    "authors": [
      "Markos Diomataris",
      "Nikos Athanasiou",
      "Omid Taheri",
      "Xi Wang",
      "Otmar Hilliges",
      "Michael J. Black"
    ],
    "abstract": "Synthesizing natural human motions that enable a 3D human avatar to walk and\nreach for arbitrary goals in 3D space remains an unsolved problem with many\napplications. Existing methods (data-driven or using reinforcement learning)\nare limited in terms of generalization and motion naturalness. A primary\nobstacle is the scarcity of training data that combines locomotion with goal\nreaching. To address this, we introduce WANDR, a data-driven model that takes\nan avatar's initial pose and a goal's 3D position and generates natural human\nmotions that place the end effector (wrist) on the goal location. To solve\nthis, we introduce novel intention features that drive rich goal-oriented\nmovement. Intention guides the agent to the goal, and interactively adapts the\ngeneration to novel situations without needing to define sub-goals or the\nentire motion path. Crucially, intention allows training on datasets that have\ngoal-oriented motions as well as those that do not. WANDR is a conditional\nVariational Auto-Encoder (c-VAE), which we train using the AMASS and CIRCLE\ndatasets. We evaluate our method extensively and demonstrate its ability to\ngenerate natural and long-term motions that reach 3D goals and generalize to\nunseen goal locations. Our models and code are available for research purposes\nat wandr.is.tue.mpg.de.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15383v1",
    "published_date": "2024-04-23 10:20:17 UTC",
    "updated_date": "2024-04-23 10:20:17 UTC"
  },
  {
    "arxiv_id": "2404.15382v1",
    "title": "Feature Distribution Shift Mitigation with Contrastive Pretraining for Intrusion Detection",
    "authors": [
      "Weixing Wang",
      "Haojin Yang",
      "Christoph Meinel",
      "Hasan Yagiz Özkan",
      "Cristian Bermudez Serna",
      "Carmen Mas-Machuca"
    ],
    "abstract": "In recent years, there has been a growing interest in using Machine Learning\n(ML), especially Deep Learning (DL) to solve Network Intrusion Detection (NID)\nproblems. However, the feature distribution shift problem remains a difficulty,\nbecause the change in features' distributions over time negatively impacts the\nmodel's performance. As one promising solution, model pretraining has emerged\nas a novel training paradigm, which brings robustness against feature\ndistribution shift and has proven to be successful in Computer Vision (CV) and\nNatural Language Processing (NLP). To verify whether this paradigm is\nbeneficial for NID problem, we propose SwapCon, a ML model in the context of\nNID, which compresses shift-invariant feature information during the\npretraining stage and refines during the finetuning stage. We exemplify the\nevidence of feature distribution shift using the Kyoto2006+ dataset. We\ndemonstrate how pretraining a model with the proper size can increase\nrobustness against feature distribution shifts by over 8%. Moreover, we show\nhow an adequate numerical embedding strategy also enhances the performance of\npretrained models. Further experiments show that the proposed SwapCon model\nalso outperforms eXtreme Gradient Boosting (XGBoost) and K-Nearest Neighbor\n(KNN) based models by a large margin.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted by ICMLCN24",
    "pdf_url": "http://arxiv.org/pdf/2404.15382v1",
    "published_date": "2024-04-23 10:15:10 UTC",
    "updated_date": "2024-04-23 10:15:10 UTC"
  },
  {
    "arxiv_id": "2404.14871v1",
    "title": "Exploring Human-AI Collaboration in Agile: Customised LLM Meeting Assistants",
    "authors": [
      "Beatriz Cabrero-Daniel",
      "Tomas Herda",
      "Victoria Pichler",
      "Martin Eder"
    ],
    "abstract": "This action research study focuses on the integration of \"AI assistants\" in\ntwo Agile software development meetings: the Daily Scrum and a feature\nrefinement, a planning meeting that is part of an in-house Scaled Agile\nframework. We discuss the critical drivers of success, and establish a link\nbetween the use of AI and team collaboration dynamics. We conclude with a list\nof lessons learnt during the interventions in an industrial context, and\nprovide a assessment checklist for companies and teams to reflect on their\nreadiness level. This paper is thus a road-map to facilitate the integration of\nAI tools in Agile setups.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "International Conference on Agile Software Development (XP 2024), 14\n  pages",
    "pdf_url": "http://arxiv.org/pdf/2404.14871v1",
    "published_date": "2024-04-23 09:55:25 UTC",
    "updated_date": "2024-04-23 09:55:25 UTC"
  },
  {
    "arxiv_id": "2404.15381v4",
    "title": "Advances and Open Challenges in Federated Foundation Models",
    "authors": [
      "Chao Ren",
      "Han Yu",
      "Hongyi Peng",
      "Xiaoli Tang",
      "Bo Zhao",
      "Liping Yi",
      "Alysa Ziying Tan",
      "Yulan Gao",
      "Anran Li",
      "Xiaoxiao Li",
      "Zengxiang Li",
      "Qiang Yang"
    ],
    "abstract": "The integration of Foundation Models (FMs) with Federated Learning (FL)\npresents a transformative paradigm in Artificial Intelligence (AI). This\nintegration offers enhanced capabilities, while addressing concerns of privacy,\ndata decentralization and computational efficiency. This paper provides a\ncomprehensive survey of the emerging field of Federated Foundation Models\n(FedFM), elucidating their synergistic relationship and exploring novel\nmethodologies, challenges, and future directions that the FL research field\nneeds to focus on in order to thrive in the age of FMs. A systematic\nmulti-tiered taxonomy is proposed, categorizing existing FedFM approaches for\nmodel training, aggregation, trustworthiness, and incentivization. Key\nchallenges, including how to enable FL to deal with high complexity of\ncomputational demands, privacy considerations, contribution evaluation, and\ncommunication efficiency, are thoroughly discussed. Moreover, this paper\nexplores the intricate challenges of communication, scalability and security\ninherent in training/fine-tuning FMs via FL. It highlights the potential of\nquantum computing to revolutionize the processes of training, inference,\noptimization and security. This survey also introduces the implementation\nrequirement of FedFM and some practical FedFM applications. It highlights\nlessons learned with a clear understanding of our findings for FedFM. Finally,\nthis survey not only provides insights into the current state and challenges of\nFedFM, but also offers a blueprint for future research directions, emphasizing\nthe need for developing trustworthy solutions. It serves as a foundational\nguide for researchers and practitioners interested in contributing to this\ninterdisciplinary and rapidly advancing field.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Survey of Federated Foundation Models (FedFM)",
    "pdf_url": "http://arxiv.org/pdf/2404.15381v4",
    "published_date": "2024-04-23 09:44:58 UTC",
    "updated_date": "2024-09-08 09:38:11 UTC"
  },
  {
    "arxiv_id": "2404.15380v1",
    "title": "ControlTraj: Controllable Trajectory Generation with Topology-Constrained Diffusion Model",
    "authors": [
      "Yuanshao Zhu",
      "James Jianqiao Yu",
      "Xiangyu Zhao",
      "Qidong Liu",
      "Yongchao Ye",
      "Wei Chen",
      "Zijian Zhang",
      "Xuetao Wei",
      "Yuxuan Liang"
    ],
    "abstract": "Generating trajectory data is among promising solutions to addressing privacy\nconcerns, collection costs, and proprietary restrictions usually associated\nwith human mobility analyses. However, existing trajectory generation methods\nare still in their infancy due to the inherent diversity and unpredictability\nof human activities, grappling with issues such as fidelity, flexibility, and\ngeneralizability. To overcome these obstacles, we propose ControlTraj, a\nControllable Trajectory generation framework with the topology-constrained\ndiffusion model. Distinct from prior approaches, ControlTraj utilizes a\ndiffusion model to generate high-fidelity trajectories while integrating the\nstructural constraints of road network topology to guide the geographical\noutcomes. Specifically, we develop a novel road segment autoencoder to extract\nfine-grained road segment embedding. The encoded features, along with trip\nattributes, are subsequently merged into the proposed geographic denoising UNet\narchitecture, named GeoUNet, to synthesize geographic trajectories from white\nnoise. Through experimentation across three real-world data settings,\nControlTraj demonstrates its ability to produce human-directed, high-fidelity\ntrajectory generation with adaptability to unexplored geographical contexts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15380v1",
    "published_date": "2024-04-23 09:42:45 UTC",
    "updated_date": "2024-04-23 09:42:45 UTC"
  },
  {
    "arxiv_id": "2404.14851v4",
    "title": "From Matching to Generation: A Survey on Generative Information Retrieval",
    "authors": [
      "Xiaoxi Li",
      "Jiajie Jin",
      "Yujia Zhou",
      "Yuyao Zhang",
      "Peitian Zhang",
      "Yutao Zhu",
      "Zhicheng Dou"
    ],
    "abstract": "Information Retrieval (IR) systems are crucial tools for users to access\ninformation, which have long been dominated by traditional methods relying on\nsimilarity matching. With the advancement of pre-trained language models,\ngenerative information retrieval (GenIR) emerges as a novel paradigm,\nattracting increasing attention. Based on the form of information provided to\nusers, current research in GenIR can be categorized into two aspects:\n\\textbf{(1) Generative Document Retrieval} (GR) leverages the generative\nmodel's parameters for memorizing documents, enabling retrieval by directly\ngenerating relevant document identifiers without explicit indexing. \\textbf{(2)\nReliable Response Generation} employs language models to directly generate\ninformation users seek, breaking the limitations of traditional IR in terms of\ndocument granularity and relevance matching while offering flexibility,\nefficiency, and creativity to meet practical needs. This paper aims to\nsystematically review the latest research progress in GenIR. We will summarize\nthe advancements in GR regarding model training and structure, document\nidentifier, incremental learning, etc., as well as progress in reliable\nresponse generation in aspects of internal knowledge memorization, external\nknowledge augmentation, etc. We also review the evaluation, challenges and\nfuture developments in GenIR systems. This review aims to offer a comprehensive\nreference for researchers, encouraging further development in the GenIR field.\nGithub Repository: https://github.com/RUC-NLPIR/GenIR-Survey",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14851v4",
    "published_date": "2024-04-23 09:05:37 UTC",
    "updated_date": "2025-03-04 08:38:34 UTC"
  },
  {
    "arxiv_id": "2404.14830v1",
    "title": "CoProNN: Concept-based Prototypical Nearest Neighbors for Explaining Vision Models",
    "authors": [
      "Teodor Chiaburu",
      "Frank Haußer",
      "Felix Bießmann"
    ],
    "abstract": "Mounting evidence in explainability for artificial intelligence (XAI)\nresearch suggests that good explanations should be tailored to individual tasks\nand should relate to concepts relevant to the task. However, building task\nspecific explanations is time consuming and requires domain expertise which can\nbe difficult to integrate into generic XAI methods. A promising approach\ntowards designing useful task specific explanations with domain experts is\nbased on compositionality of semantic concepts. Here, we present a novel\napproach that enables domain experts to quickly create concept-based\nexplanations for computer vision tasks intuitively via natural language.\nLeveraging recent progress in deep generative methods we propose to generate\nvisual concept-based prototypes via text-to-image methods. These prototypes are\nthen used to explain predictions of computer vision models via a simple\nk-Nearest-Neighbors routine. The modular design of CoProNN is simple to\nimplement, it is straightforward to adapt to novel tasks and allows for\nreplacing the classification and text-to-image models as more powerful models\nare released. The approach can be evaluated offline against the ground-truth of\npredefined prototypes that can be easily communicated also to domain experts as\nthey are based on visual concepts. We show that our strategy competes very well\nwith other concept-based XAI approaches on coarse grained image classification\ntasks and may even outperform those methods on more demanding fine grained\ntasks. We demonstrate the effectiveness of our method for human-machine\ncollaboration settings in qualitative and quantitative user studies. All code\nand experimental data can be found in our GitHub\n$\\href{https://github.com/TeodorChiaburu/beexplainable}{repository}$.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 9 figures, 2 tables, accepted at WCXAI 2024 Valletta",
    "pdf_url": "http://arxiv.org/pdf/2404.14830v1",
    "published_date": "2024-04-23 08:32:38 UTC",
    "updated_date": "2024-04-23 08:32:38 UTC"
  },
  {
    "arxiv_id": "2404.14822v1",
    "title": "CNN2GNN: How to Bridge CNN with GNN",
    "authors": [
      "Ziheng Jiao",
      "Hongyuan Zhang",
      "Xuelong Li"
    ],
    "abstract": "Although the convolutional neural network (CNN) has achieved excellent\nperformance in vision tasks by extracting the intra-sample representation, it\nwill take a higher training expense because of stacking numerous convolutional\nlayers. Recently, as the bilinear models, graph neural networks (GNN) have\nsucceeded in exploring the underlying topological relationship among the graph\ndata with a few graph neural layers. Unfortunately, it cannot be directly\nutilized on non-graph data due to the lack of graph structure and has high\ninference latency on large-scale scenarios. Inspired by these complementary\nstrengths and weaknesses, \\textit{we discuss a natural question, how to bridge\nthese two heterogeneous networks?} In this paper, we propose a novel CNN2GNN\nframework to unify CNN and GNN together via distillation. Firstly, to break the\nlimitations of GNN, a differentiable sparse graph learning module is designed\nas the head of networks to dynamically learn the graph for inductive learning.\nThen, a response-based distillation is introduced to transfer the knowledge\nfrom CNN to GNN and bridge these two heterogeneous networks. Notably, due to\nextracting the intra-sample representation of a single instance and the\ntopological relationship among the datasets simultaneously, the performance of\ndistilled ``boosted'' two-layer GNN on Mini-ImageNet is much higher than CNN\ncontaining dozens of layers such as ResNet152.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14822v1",
    "published_date": "2024-04-23 08:19:08 UTC",
    "updated_date": "2024-04-23 08:19:08 UTC"
  },
  {
    "arxiv_id": "2405.01574v1",
    "title": "On Using Agent-based Modeling and Simulation for Studying Blockchain Systems",
    "authors": [
      "Önder Gürcan"
    ],
    "abstract": "There is a need for a simulation framework, which is develop as a software\nusing modern engineering approaches (e.g., modularity --i.e., model reuse--,\ntesting, continuous development and continuous integration, automated\nmanagement of builds, dependencies and documentation) and agile principles, (1)\nto make rapid prototyping of industrial cases and (2) to carry out their\nfeasibility analysis in a realistic manner (i.e., to test hypothesis by\nsimulating complex experiments involving large numbers of participants of\ndifferent types acting in one or several blockchain systems).",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "2 pages, \"JFMS 2020 -- Les Journees Francophones de la Modelisation\n  et de la Simulation -- Convergences entre la Theorie de la Modelisation et la\n  Simulation et les Systemes Multi-Agents\"",
    "pdf_url": "http://arxiv.org/pdf/2405.01574v1",
    "published_date": "2024-04-23 08:06:37 UTC",
    "updated_date": "2024-04-23 08:06:37 UTC"
  },
  {
    "arxiv_id": "2404.14809v1",
    "title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
    "authors": [
      "Wenbo Shang",
      "Xin Huang"
    ],
    "abstract": "A graph is a fundamental data model to represent various entities and their\ncomplex relationships in society and nature, such as social networks,\ntransportation networks, financial networks, and biomedical systems. Recently,\nlarge language models (LLMs) have showcased a strong generalization ability to\nhandle various NLP and multi-mode tasks to answer users' arbitrary questions\nand specific-domain content generation. Compared with graph learning models,\nLLMs enjoy superior advantages in addressing the challenges of generalizing\ngraph tasks by eliminating the need for training graph learning models and\nreducing the cost of manual annotation. In this survey, we conduct a\ncomprehensive investigation of existing LLM studies on graph data, which\nsummarizes the relevant graph analytics tasks solved by advanced LLM models and\npoints out the existing remaining challenges and future directions.\nSpecifically, we study the key problems of LLM-based generative graph analytics\n(LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP),\nLLM-based graph inference and learning (LLM-GIL), and graph-LLM-based\napplications. LLM-GQP focuses on an integration of graph analytics techniques\nand LLM prompts, including graph understanding and knowledge graph (KG) based\naugmented retrieval, while LLM-GIL focuses on learning and reasoning over\ngraphs, including graph learning, graph-formed reasoning and graph\nrepresentation. We summarize the useful prompts incorporated into LLM to handle\ndifferent graph downstream tasks. Moreover, we give a summary of LLM model\nevaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM\nmodels. We also explore open problems and future directions in this exciting\ninterdisciplinary research area of LLMs and graph analytics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "31 pages including references, 22 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.14809v1",
    "published_date": "2024-04-23 07:39:24 UTC",
    "updated_date": "2024-04-23 07:39:24 UTC"
  },
  {
    "arxiv_id": "2404.15379v3",
    "title": "Clustering of timed sequences -- Application to the analysis of care pathways",
    "authors": [
      "Thomas Guyet",
      "Pierre Pinson",
      "Enoal Gesny"
    ],
    "abstract": "Improving the future of healthcare starts by better understanding the current\nactual practices in hospital settings. This motivates the objective of\ndiscovering typical care pathways from patient data. Revealing typical care\npathways can be achieved through clustering. The difficulty in clustering care\npathways, represented by sequences of timestamped events, lies in defining a\nsemantically appropriate metric and clustering algorithms. In this article, we\nadapt two methods developed for time series to the clustering of timed\nsequences: the drop-DTW metric and the DBA approach for the construction of\naveraged time sequences. These methods are then applied in clustering\nalgorithms to propose original and sound clustering algorithms for timed\nsequences. This approach is experimented with and evaluated on synthetic and\nreal-world data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15379v3",
    "published_date": "2024-04-23 07:16:13 UTC",
    "updated_date": "2024-12-19 15:54:17 UTC"
  },
  {
    "arxiv_id": "2404.14786v2",
    "title": "RealTCD: Temporal Causal Discovery from Interventional Data with Large Language Model",
    "authors": [
      "Peiwen Li",
      "Xin Wang",
      "Zeyang Zhang",
      "Yuan Meng",
      "Fang Shen",
      "Yue Li",
      "Jialong Wang",
      "Yang Li",
      "Wenweu Zhu"
    ],
    "abstract": "In the field of Artificial Intelligence for Information Technology\nOperations, causal discovery is pivotal for operation and maintenance of graph\nconstruction, facilitating downstream industrial tasks such as root cause\nanalysis. Temporal causal discovery, as an emerging method, aims to identify\ntemporal causal relationships between variables directly from observations by\nutilizing interventional data. However, existing methods mainly focus on\nsynthetic datasets with heavy reliance on intervention targets and ignore the\ntextual information hidden in real-world systems, failing to conduct causal\ndiscovery for real industrial scenarios. To tackle this problem, in this paper\nwe propose to investigate temporal causal discovery in industrial scenarios,\nwhich faces two critical challenges: 1) how to discover causal relationships\nwithout the interventional targets that are costly to obtain in practice, and\n2) how to discover causal relations via leveraging the textual information in\nsystems which can be complex yet abundant in industrial contexts. To address\nthese challenges, we propose the RealTCD framework, which is able to leverage\ndomain knowledge to discover temporal causal relationships without\ninterventional targets. Specifically, we first develop a score-based temporal\ncausal discovery method capable of discovering causal relations for root cause\nanalysis without relying on interventional targets through strategic masking\nand regularization. Furthermore, by employing Large Language Models (LLMs) to\nhandle texts and integrate domain knowledge, we introduce LLM-guided\nmeta-initialization to extract the meta-knowledge from textual information\nhidden in systems to boost the quality of discovery. We conduct extensive\nexperiments on simulation and real-world datasets to show the superiority of\nour proposed RealTCD framework over existing baselines in discovering temporal\ncausal structures.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14786v2",
    "published_date": "2024-04-23 06:52:40 UTC",
    "updated_date": "2024-05-26 13:08:00 UTC"
  },
  {
    "arxiv_id": "2404.17598v1",
    "title": "Revealing and Utilizing In-group Favoritism for Graph-based Collaborative Filtering",
    "authors": [
      "Hoin Jung",
      "Hyunsoo Cho",
      "Myungje Choi",
      "Joowon Lee",
      "Jung Ho Park",
      "Myungjoo Kang"
    ],
    "abstract": "When it comes to a personalized item recommendation system, It is essential\nto extract users' preferences and purchasing patterns. Assuming that users in\nthe real world form a cluster and there is common favoritism in each cluster,\nin this work, we introduce Co-Clustering Wrapper (CCW). We compute co-clusters\nof users and items with co-clustering algorithms and add CF subnetworks for\neach cluster to extract the in-group favoritism. Combining the features from\nthe networks, we obtain rich and unified information about users. We\nexperimented real world datasets considering two aspects: Finding the number of\ngroups divided according to in-group preference, and measuring the quantity of\nimprovement of the performance.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.IR",
    "comment": "7 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.17598v1",
    "published_date": "2024-04-23 06:43:58 UTC",
    "updated_date": "2024-04-23 06:43:58 UTC"
  },
  {
    "arxiv_id": "2404.14771v1",
    "title": "Music Style Transfer With Diffusion Model",
    "authors": [
      "Hong Huang",
      "Yuyi Wang",
      "Luyao Li",
      "Jun Lin"
    ],
    "abstract": "Previous studies on music style transfer have mainly focused on one-to-one\nstyle conversion, which is relatively limited. When considering the conversion\nbetween multiple styles, previous methods required designing multiple modes to\ndisentangle the complex style of the music, resulting in large computational\ncosts and slow audio generation. The existing music style transfer methods\ngenerate spectrograms with artifacts, leading to significant noise in the\ngenerated audio. To address these issues, this study proposes a music style\ntransfer framework based on diffusion models (DM) and uses spectrogram-based\nmethods to achieve multi-to-multi music style transfer. The GuideDiff method is\nused to restore spectrograms to high-fidelity audio, accelerating audio\ngeneration speed and reducing noise in the generated audio. Experimental\nresults show that our model has good performance in multi-mode music style\ntransfer compared to the baseline and can generate high-quality audio in\nreal-time on consumer-grade GPUs.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "8 pages, 6 figures, ICMC 2023",
    "pdf_url": "http://arxiv.org/pdf/2404.14771v1",
    "published_date": "2024-04-23 06:22:19 UTC",
    "updated_date": "2024-04-23 06:22:19 UTC"
  },
  {
    "arxiv_id": "2404.14763v3",
    "title": "Evolutionary Reinforcement Learning via Cooperative Coevolution",
    "authors": [
      "Chengpeng Hu",
      "Jialin Liu",
      "Xin Yao"
    ],
    "abstract": "Recently, evolutionary reinforcement learning has obtained much attention in\nvarious domains. Maintaining a population of actors, evolutionary reinforcement\nlearning utilises the collected experiences to improve the behaviour policy\nthrough efficient exploration. However, the poor scalability of genetic\noperators limits the efficiency of optimising high-dimensional neural\nnetworks.To address this issue, this paper proposes a novel cooperative\ncoevolutionary reinforcement learning (CoERL) algorithm. Inspired by\ncooperative coevolution, CoERL periodically and adaptively decomposes the\npolicy optimisation problem into multiple subproblems and evolves a population\nof neural networks for each of the subproblems. Instead of using genetic\noperators, CoERL directly searches for partial gradients to update the policy.\nUpdating policy with partial gradients maintains consistency between the\nbehaviour spaces of parents and offspring across generations.The experiences\ncollected by the population are then used to improve the entire policy, which\nenhances the sampling efficiency.Experiments on six benchmark locomotion tasks\ndemonstrate that CoERL outperforms seven state-of-the-art algorithms and\nbaselines.Ablation study verifies the unique contribution of CoERL's core\ningredients.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "This paper is accepted by 27th European Conference on Artificial\n  Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2404.14763v3",
    "published_date": "2024-04-23 05:56:35 UTC",
    "updated_date": "2024-08-01 13:35:22 UTC"
  },
  {
    "arxiv_id": "2404.14760v2",
    "title": "Retrieval Augmented Generation for Domain-specific Question Answering",
    "authors": [
      "Sanat Sharma",
      "David Seunghyun Yoon",
      "Franck Dernoncourt",
      "Dewang Sultania",
      "Karishma Bagga",
      "Mengjiao Zhang",
      "Trung Bui",
      "Varun Kotte"
    ],
    "abstract": "Question answering (QA) has become an important application in the advanced\ndevelopment of large language models. General pre-trained large language models\nfor question-answering are not trained to properly understand the knowledge or\nterminology for a specific domain, such as finance, healthcare, education, and\ncustomer service for a product. To better cater to domain-specific\nunderstanding, we build an in-house question-answering system for Adobe\nproducts. We propose a novel framework to compile a large question-answer\ndatabase and develop the approach for retrieval-aware finetuning of a Large\nLanguage model. We showcase that fine-tuning the retriever leads to major\nimprovements in the final generation. Our overall approach reduces\nhallucinations during generation while keeping in context the latest retrieval\ninformation for contextual grounding.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2024 (Association for the Advancement of Artificial\n  Intelligence) Scientific Document Understanding Workshop",
    "pdf_url": "http://arxiv.org/pdf/2404.14760v2",
    "published_date": "2024-04-23 05:51:45 UTC",
    "updated_date": "2024-05-29 16:18:02 UTC"
  },
  {
    "arxiv_id": "2404.14757v2",
    "title": "SST: Multi-Scale Hybrid Mamba-Transformer Experts for Long-Short Range Time Series Forecasting",
    "authors": [
      "Xiongxiao Xu",
      "Canyu Chen",
      "Yueqing Liang",
      "Baixiang Huang",
      "Guangji Bai",
      "Liang Zhao",
      "Kai Shu"
    ],
    "abstract": "Despite significant progress in time series forecasting, existing forecasters\noften overlook the heterogeneity between long-range and short-range time\nseries, leading to performance degradation in practical applications. In this\nwork, we highlight the need of distinct objectives tailored to different\nranges. We point out that time series can be decomposed into global patterns\nand local variations, which should be addressed separately in long- and\nshort-range time series. To meet the objectives, we propose a multi-scale\nhybrid Mamba-Transformer experts model State Space Transformer (SST). SST\nleverages Mamba as an expert to extract global patterns in coarse-grained\nlong-range time series, and Local Window Transformer (LWT), the other expert to\nfocus on capturing local variations in fine-grained short-range time series.\nWith an input-dependent mechanism, State Space Model (SSM)-based Mamba is able\nto selectively retain long-term patterns and filter out fluctuations, while LWT\nemploys a local window to enhance locality-awareness capability, thus\neffectively capturing local variations. To adaptively integrate the global\npatterns and local variations, a long-short router dynamically adjusts\ncontributions of the two experts. SST achieves superior performance with\nscaling linearly $O(L)$ on time series length $L$. The comprehensive\nexperiments demonstrate the SST can achieve SOTA results in long-short range\ntime series forecasting while maintaining low memory footprint and\ncomputational cost. The code of SST is available at\nhttps://github.com/XiongxiaoXu/SST.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14757v2",
    "published_date": "2024-04-23 05:43:44 UTC",
    "updated_date": "2024-08-22 17:55:42 UTC"
  },
  {
    "arxiv_id": "2404.14755v2",
    "title": "SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework with Interactive Vision-Language Models",
    "authors": [
      "Bo Lin",
      "Yingjing Xu",
      "Xuanwen Bao",
      "Zhou Zhao",
      "Zhouyang Wang",
      "Jianwei Yin"
    ],
    "abstract": "With the continuous advancement of vision language models (VLMs) technology,\nremarkable research achievements have emerged in the dermatology field, the\nfourth most prevalent human disease category. However, despite these\nadvancements, VLM still faces explainable problems to user in diagnosis due to\nthe inherent complexity of dermatological conditions, existing tools offer\nrelatively limited support for user comprehension. We propose SkinGEN, a\ndiagnosis-to-generation framework that leverages the stable diffusion(SD) model\nto generate reference demonstrations from diagnosis results provided by VLM,\nthereby enhancing the visual explainability for users. Through extensive\nexperiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for\nskin condition image generation. We conduct a user study with 32 participants\nevaluating both the system performance and explainability. Results demonstrate\nthat SkinGEN significantly improves users' comprehension of VLM predictions and\nfosters increased trust in the diagnostic process. This work paves the way for\nmore transparent and user-centric VLM applications in dermatology and beyond.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14755v2",
    "published_date": "2024-04-23 05:36:33 UTC",
    "updated_date": "2025-02-13 03:15:41 UTC"
  },
  {
    "arxiv_id": "2404.14754v1",
    "title": "Skip the Benchmark: Generating System-Level High-Level Synthesis Data using Generative Machine Learning",
    "authors": [
      "Yuchao Liao",
      "Tosiron Adegbija",
      "Roman Lysecky",
      "Ravi Tandon"
    ],
    "abstract": "High-Level Synthesis (HLS) Design Space Exploration (DSE) is a widely\naccepted approach for efficiently exploring Pareto-optimal and optimal hardware\nsolutions during the HLS process. Several HLS benchmarks and datasets are\navailable for the research community to evaluate their methodologies.\nUnfortunately, these resources are limited and may not be sufficient for\ncomplex, multi-component system-level explorations. Generating new data using\nexisting HLS benchmarks can be cumbersome, given the expertise and time\nrequired to effectively generate data for different HLS designs and directives.\nAs a result, synthetic data has been used in prior work to evaluate\nsystem-level HLS DSE. However, the fidelity of the synthetic data to real data\nis often unclear, leading to uncertainty about the quality of system-level HLS\nDSE. This paper proposes a novel approach, called Vaegan, that employs\ngenerative machine learning to generate synthetic data that is robust enough to\nsupport complex system-level HLS DSE experiments that would be unattainable\nwith only the currently available data. We explore and adapt a Variational\nAutoencoder (VAE) and Generative Adversarial Network (GAN) for this task and\nevaluate our approach using state-of-the-art datasets and metrics. We compare\nour approach to prior works and show that Vaegan effectively generates\nsynthetic HLS data that closely mirrors the ground truth's distribution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at Great Lakes Symposium on VLSI 2024 (GLSVLSI 24)",
    "pdf_url": "http://arxiv.org/pdf/2404.14754v1",
    "published_date": "2024-04-23 05:32:22 UTC",
    "updated_date": "2024-04-23 05:32:22 UTC"
  },
  {
    "arxiv_id": "2404.14750v2",
    "title": "Grounded Knowledge-Enhanced Medical Vision-Language Pre-training for Chest X-Ray",
    "authors": [
      "Qiao Deng",
      "Zhongzhen Huang",
      "Yunqi Wang",
      "Zhichuan Wang",
      "Zhao Wang",
      "Xiaofan Zhang",
      "Qi Dou",
      "Yeung Yu Hui",
      "Edward S. Hui"
    ],
    "abstract": "Medical foundation models have the potential to revolutionize healthcare by\nproviding robust and generalized representations of medical data. Medical\nvision-language pre-training has emerged as a promising approach for learning\ndomain-general representations of medical image and text. Current algorithms\nthat exploit global and local alignment between medical image and text could\nhowever be marred by redundant information in medical data. To address this\nissue, we propose a grounded knowledge-enhanced medical vision-language\npre-training (GK-MVLP) framework for chest X-ray. In this framework, medical\nknowledge was grounded to the appropriate anatomical regions by using a\ntransformer-based grounded knowledge-enhanced module for fine-grained alignment\nbetween textural features of medical knowledge and the corresponding anatomical\nregion-level visual features. The performance of GK-MVLP was competitive with\nor exceeded the state of the art on downstream image understanding tasks (chest\nX-ray disease classification, disease localization), generative task (report\ngeneration), and vision-language understanding task (medical visual\nquestion-answering). Our results demonstrate the advantage of incorporating\ngrounding mechanism to remove biases and improve the alignment between chest\nX-ray image and radiology report.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14750v2",
    "published_date": "2024-04-23 05:16:24 UTC",
    "updated_date": "2025-02-17 02:49:16 UTC"
  },
  {
    "arxiv_id": "2404.14746v1",
    "title": "A Customer Level Fraudulent Activity Detection Benchmark for Enhancing Machine Learning Model Research and Evaluation",
    "authors": [
      "Phoebe Jing",
      "Yijing Gao",
      "Xianlong Zeng"
    ],
    "abstract": "In the field of fraud detection, the availability of comprehensive and\nprivacy-compliant datasets is crucial for advancing machine learning research\nand developing effective anti-fraud systems. Traditional datasets often focus\non transaction-level information, which, while useful, overlooks the broader\ncontext of customer behavior patterns that are essential for detecting\nsophisticated fraud schemes. The scarcity of such data, primarily due to\nprivacy concerns, significantly hampers the development and testing of\npredictive models that can operate effectively at the customer level.\nAddressing this gap, our study introduces a benchmark that contains structured\ndatasets specifically designed for customer-level fraud detection. The\nbenchmark not only adheres to strict privacy guidelines to ensure user\nconfidentiality but also provides a rich source of information by encapsulating\ncustomer-centric features. We have developed the benchmark that allows for the\ncomprehensive evaluation of various machine learning models, facilitating a\ndeeper understanding of their strengths and weaknesses in predicting fraudulent\nactivities. Through this work, we seek to bridge the existing gap in data\navailability, offering researchers and practitioners a valuable resource that\nempowers the development of next-generation fraud detection techniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 3 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2404.14746v1",
    "published_date": "2024-04-23 04:57:44 UTC",
    "updated_date": "2024-04-23 04:57:44 UTC"
  },
  {
    "arxiv_id": "2404.14741v3",
    "title": "Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering",
    "authors": [
      "Yao Xu",
      "Shizhu He",
      "Jiabei Chen",
      "Zihao Wang",
      "Yangqiu Song",
      "Hanghang Tong",
      "Guang Liu",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "To address the issues of insufficient knowledge and hallucination in Large\nLanguage Models (LLMs), numerous studies have explored integrating LLMs with\nKnowledge Graphs (KGs). However, these methods are typically evaluated on\nconventional Knowledge Graph Question Answering (KGQA) with complete KGs, where\nall factual triples required for each question are entirely covered by the\ngiven KG. In such cases, LLMs primarily act as an agent to find answer entities\nwithin the KG, rather than effectively integrating the internal knowledge of\nLLMs and external knowledge sources such as KGs. In fact, KGs are often\nincomplete to cover all the knowledge required to answer questions. To simulate\nthese real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, we propose leveraging LLMs for QA under\nIncomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the\nfactual triples for each question, and construct corresponding datasets. To\nhandle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),\nwhich can generate new factual triples while exploring KGs. Specifically, GoG\nperforms reasoning through a Thinking-Searching-Generating framework, which\ntreats LLM as both Agent and KG in IKGQA. Experimental results on two datasets\ndemonstrate that our GoG outperforms all previous methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2404.14741v3",
    "published_date": "2024-04-23 04:47:22 UTC",
    "updated_date": "2024-10-06 10:55:23 UTC"
  },
  {
    "arxiv_id": "2404.14736v1",
    "title": "Qualitative Approaches to Voice UX",
    "authors": [
      "Katie Seaborn",
      "Jacqueline Urakami",
      "Peter Pennefather",
      "Norihisa P. Miyake"
    ],
    "abstract": "Voice is a natural mode of expression offered by modern computer-based\nsystems. Qualitative perspectives on voice-based user experiences (voice UX)\noffer rich descriptions of complex interactions that numbers alone cannot fully\nrepresent. We conducted a systematic review of the literature on qualitative\napproaches to voice UX, capturing the nature of this body of work in a\nsystematic map and offering a qualitative synthesis of findings. We highlight\nthe benefits of qualitative methods for voice UX research, identify\nopportunities for increasing rigour in methods and outcomes, and distill\npatterns of experience across a diversity of devices and modes of qualitative\npraxis.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14736v1",
    "published_date": "2024-04-23 04:33:49 UTC",
    "updated_date": "2024-04-23 04:33:49 UTC"
  },
  {
    "arxiv_id": "2407.05181v1",
    "title": "Instructors as Innovators: A future-focused approach to new AI learning opportunities, with prompts",
    "authors": [
      "Ethan Mollick",
      "Lilach Mollick"
    ],
    "abstract": "This paper explores how instructors can leverage generative AI to create\npersonalized learning experiences for students that transform teaching and\nlearning. We present a range of AI-based exercises that enable novel forms of\npractice and application including simulations, mentoring, coaching, and\nco-creation. For each type of exercise, we provide prompts that instructors can\ncustomize, along with guidance on classroom implementation, assessment, and\nrisks to consider. We also provide blueprints, prompts that help instructors\ncreate their own original prompts. Instructors can leverage their content and\npedagogical expertise to design these experiences, putting them in the role of\nbuilders and innovators. We argue that this instructor-driven approach has the\npotential to democratize the development of educational technology by enabling\nindividual instructors to create AI exercises and tools tailored to their\nstudents' needs. While the exercises in this paper are a starting point, not a\ndefinitive solutions, they demonstrate AI's potential to expand what is\npossible in teaching and learning.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Includes prompts in Appendixes A and B",
    "pdf_url": "http://arxiv.org/pdf/2407.05181v1",
    "published_date": "2024-04-23 04:01:38 UTC",
    "updated_date": "2024-04-23 04:01:38 UTC"
  },
  {
    "arxiv_id": "2404.14716v2",
    "title": "Bayesian Example Selection Improves In-Context Learning for Speech, Text, and Visual Modalities",
    "authors": [
      "Siyin Wang",
      "Chao-Han Huck Yang",
      "Ji Wu",
      "Chao Zhang"
    ],
    "abstract": "Large language models (LLMs) can adapt to new tasks through in-context\nlearning (ICL) based on a few examples presented in dialogue history without\nany model parameter update. Despite such convenience, the performance of ICL\nheavily depends on the quality of the in-context examples presented, which\nmakes the in-context example selection approach a critical choice. This paper\nproposes a novel Bayesian in-Context example Selection method (ByCS) for ICL.\nExtending the inference probability conditioned on in-context examples based on\nBayes' theorem, ByCS focuses on the inverse inference conditioned on test\ninput. Following the assumption that accurate inverse inference probability\n(likelihood) will result in accurate inference probability (posterior),\nin-context examples are selected based on their inverse inference results.\nDiverse and extensive cross-tasking and cross-modality experiments are\nperformed with speech, text, and image examples. Experimental results show the\nefficacy and robustness of our ByCS method on various models, tasks and\nmodalities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.14716v2",
    "published_date": "2024-04-23 03:42:48 UTC",
    "updated_date": "2024-06-16 08:49:00 UTC"
  },
  {
    "arxiv_id": "2404.14712v5",
    "title": "ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability",
    "authors": [
      "Xiao Wang",
      "Siyan Liu",
      "Aristeidis Tsaris",
      "Jong-Youl Choi",
      "Ashwin Aji",
      "Ming Fan",
      "Wei Zhang",
      "Junqi Yin",
      "Moetasim Ashfaq",
      "Dan Lu",
      "Prasanna Balaprakash"
    ],
    "abstract": "Earth system predictability is challenged by the complexity of environmental\ndynamics and the multitude of variables involved. Current AI foundation models,\nalthough advanced by leveraging large and heterogeneous data, are often\nconstrained by their size and data integration, limiting their effectiveness in\naddressing the full range of Earth system prediction challenges. To overcome\nthese limitations, we introduce the Oak Ridge Base Foundation Model for Earth\nSystem Predictability (ORBIT), an advanced vision transformer model that scales\nup to 113 billion parameters using a novel hybrid tensor-data orthogonal\nparallelism technique. As the largest model of its kind, ORBIT surpasses the\ncurrent climate AI foundation model size by a thousandfold. Performance scaling\ntests conducted on the Frontier supercomputer have demonstrated that ORBIT\nachieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scaling\nefficiency maintained at 41% to 85% across 49,152 AMD GPUs. These breakthroughs\nestablish new advances in AI-driven climate modeling and demonstrate promise to\nsignificantly improve the Earth system predictability.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.DC",
      "eess.IV",
      "physics.geo-ph"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14712v5",
    "published_date": "2024-04-23 03:39:57 UTC",
    "updated_date": "2024-08-19 15:20:09 UTC"
  },
  {
    "arxiv_id": "2406.06535v3",
    "title": "Utilizing Graph Generation for Enhanced Domain Adaptive Object Detection",
    "authors": [
      "Mu Wang"
    ],
    "abstract": "The problem of Domain Adaptive in the field of Object Detection involves the\ntransfer of object detection models from labeled source domains to unannotated\ntarget domains. Recent advancements in this field aim to address domain\ndiscrepancies by aligning pixel-pairs across domains within a non-Euclidean\ngraphical space, thereby minimizing semantic distribution variance. Despite\ntheir remarkable achievements, these methods often use coarse semantic\nrepresentations to model graphs, mainly due to ignoring non-informative\nelements and failing to focus on precise semantic alignment. Additionally, the\ngeneration of coarse graphs inherently introduces abnormal nodes, posing\nchallenges and potentially biasing domain adaptation outcomes. Consequently, we\npropose a framework, which utilizes the Graph Generation to enhance the quality\nof DAOD (\\method{}). Specifically, we introduce a Node Refinement module that\nutilizes a memory bank to reconstruct noisy sampled nodes while applying\ncontrastive regularization to noisy features. To enhance semantic alignment, we\npropose separating domain-specific styles from category invariance encoded\nwithin graph covariances, which allows us to selectively remove domain-specific\nstyles while preserving category-invariant information, thus facilitating more\naccurate semantic alignment across different domains. Furthermore, we propose a\nGraph Optimization adaptor, leveraging variational inference to mitigate the\nimpact of abnormal nodes. Extensive experimentation across three adaptation\nbenchmarks validates that \\method{} achieves state-of-the-art performance in\nthe task of unsupervised domain adaptation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06535v3",
    "published_date": "2024-04-23 03:11:08 UTC",
    "updated_date": "2024-11-12 02:10:13 UTC"
  },
  {
    "arxiv_id": "2404.15378v3",
    "title": "Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for Heterogeneous Joint Distributions",
    "authors": [
      "Khai Nguyen",
      "Nhat Ho"
    ],
    "abstract": "Sliced Wasserstein (SW) and Generalized Sliced Wasserstein (GSW) have been\nwidely used in applications due to their computational and statistical\nscalability. However, the SW and the GSW are only defined between distributions\nsupported on a homogeneous domain. This limitation prevents their usage in\napplications with heterogeneous joint distributions with marginal distributions\nsupported on multiple different domains. Using SW and GSW directly on the joint\ndomains cannot make a meaningful comparison since their homogeneous slicing\noperator i.e., Radon Transform (RT) and Generalized Radon Transform (GRT) are\nnot expressive enough to capture the structure of the joint supports set. To\naddress the issue, we propose two new slicing operators i.e., Partial\nGeneralized Radon Transform (PGRT) and Hierarchical Hybrid Radon Transform\n(HHRT). In greater detail, PGRT is the generalization of Partial Radon\nTransform (PRT), which transforms a subset of function arguments non-linearly\nwhile HHRT is the composition of PRT and multiple domain-specific PGRT on\nmarginal domain arguments. By using HHRT, we extend the SW into Hierarchical\nHybrid Sliced Wasserstein (H2SW) distance which is designed specifically for\ncomparing heterogeneous joint distributions. We then discuss the topological,\nstatistical, and computational properties of H2SW. Finally, we demonstrate the\nfavorable performance of H2SW in 3D mesh deformation, deep 3D mesh\nautoencoders, and datasets comparison.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2024, 27 pages, 11 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.15378v3",
    "published_date": "2024-04-23 03:04:22 UTC",
    "updated_date": "2024-10-08 01:39:18 UTC"
  },
  {
    "arxiv_id": "2404.14700v4",
    "title": "FlashSpeech: Efficient Zero-Shot Speech Synthesis",
    "authors": [
      "Zhen Ye",
      "Zeqian Ju",
      "Haohe Liu",
      "Xu Tan",
      "Jianyi Chen",
      "Yiwen Lu",
      "Peiwen Sun",
      "Jiahao Pan",
      "Weizhen Bian",
      "Shulin He",
      "Wei Xue",
      "Qifeng Liu",
      "Yike Guo"
    ],
    "abstract": "Recent progress in large-scale zero-shot speech synthesis has been\nsignificantly advanced by language models and diffusion models. However, the\ngeneration process of both methods is slow and computationally intensive.\nEfficient speech synthesis using a lower computing budget to achieve quality on\npar with previous work remains a significant challenge. In this paper, we\npresent FlashSpeech, a large-scale zero-shot speech synthesis system with\napproximately 5\\% of the inference time compared with previous work.\nFlashSpeech is built on the latent consistency model and applies a novel\nadversarial consistency training approach that can train from scratch without\nthe need for a pre-trained diffusion model as the teacher. Furthermore, a new\nprosody generator module enhances the diversity of prosody, making the rhythm\nof the speech sound more natural. The generation processes of FlashSpeech can\nbe achieved efficiently with one or two sampling steps while maintaining high\naudio quality and high similarity to the audio prompt for zero-shot speech\ngeneration. Our experimental results demonstrate the superior performance of\nFlashSpeech. Notably, FlashSpeech can be about 20 times faster than other\nzero-shot speech synthesis systems while maintaining comparable performance in\nterms of voice quality and similarity. Furthermore, FlashSpeech demonstrates\nits versatility by efficiently performing tasks like voice conversion, speech\nediting, and diverse speech sampling. Audio samples can be found in\nhttps://flashspeech.github.io/.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Efficient zero-shot speech synthesis",
    "pdf_url": "http://arxiv.org/pdf/2404.14700v4",
    "published_date": "2024-04-23 02:57:46 UTC",
    "updated_date": "2024-10-24 08:19:04 UTC"
  },
  {
    "arxiv_id": "2404.14688v3",
    "title": "FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model",
    "authors": [
      "Zezheng Song",
      "Jiaxin Yuan",
      "Haizhao Yang"
    ],
    "abstract": "The fast simulation of dynamical systems is a key challenge in many\nscientific and engineering applications, such as weather forecasting, disease\ncontrol, and drug discovery. With the recent success of deep learning, there is\nincreasing interest in using neural networks to solve differential equations in\na data-driven manner. However, existing methods are either limited to specific\ntypes of differential equations or require large amounts of data for training.\nThis restricts their practicality in many real-world applications, where data\nis often scarce or expensive to obtain. To address this, we propose a novel\nmulti-modal foundation model, named \\textbf{FMint} (\\textbf{F}oundation\n\\textbf{M}odel based on \\textbf{In}i\\textbf{t}ialization), to bridge the gap\nbetween human-designed and data-driven models for the fast simulation of\ndynamical systems. Built on a decoder-only transformer architecture with\nin-context learning, FMint utilizes both numerical and textual data to learn a\nuniversal error correction scheme for dynamical systems, using prompted\nsequences of coarse solutions from traditional solvers. The model is\npre-trained on a corpus of 40K ODEs, and we perform extensive experiments on\nchallenging ODEs that exhibit chaotic behavior and of high dimensionality. Our\nresults demonstrate the effectiveness of the proposed model in terms of both\naccuracy and efficiency compared to classical numerical solvers, highlighting\nFMint's potential as a general-purpose solver for dynamical systems. Our\napproach achieves an accuracy improvement of 1 to 2 orders of magnitude over\nstate-of-the-art dynamical system simulators, and delivers a 5X speedup\ncompared to traditional numerical algorithms. The code for FMint is available\nat \\url{https://github.com/margotyjx/FMint}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.NA",
      "math.DS",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14688v3",
    "published_date": "2024-04-23 02:36:47 UTC",
    "updated_date": "2024-09-30 19:50:24 UTC"
  },
  {
    "arxiv_id": "2404.14687v1",
    "title": "Pegasus-v1 Technical Report",
    "authors": [
      "Raehyuk Jung",
      "Hyojun Go",
      "Jaehyuk Yi",
      "Jiho Jang",
      "Daniel Kim",
      "Jay Suh",
      "Aiden Lee",
      "Cooper Han",
      "Jae Lee",
      "Jeff Kim",
      "Jin-Young Kim",
      "Junwan Kim",
      "Kyle Park",
      "Lucas Lee",
      "Mars Ha",
      "Minjoon Seo",
      "Abraham Jo",
      "Ed Park",
      "Hassan Kianinejad",
      "SJ Kim",
      "Tony Moon",
      "Wade Jeong",
      "Andrei Popescu",
      "Esther Kim",
      "EK Yoon",
      "Genie Heo",
      "Henry Choi",
      "Jenna Kang",
      "Kevin Han",
      "Noah Seo",
      "Sunny Nguyen",
      "Ryan Won",
      "Yeonhoo Park",
      "Anthony Giuliani",
      "Dave Chung",
      "Hans Yoon",
      "James Le",
      "Jenny Ahn",
      "June Lee",
      "Maninder Saini",
      "Meredith Sanders",
      "Soyoung Lee",
      "Sue Kim",
      "Travis Couture"
    ],
    "abstract": "This technical report introduces Pegasus-1, a multimodal language model\nspecialized in video content understanding and interaction through natural\nlanguage. Pegasus-1 is designed to address the unique challenges posed by video\ndata, such as interpreting spatiotemporal information, to offer nuanced video\ncontent comprehension across various lengths. This technical report overviews\nPegasus-1's architecture, training strategies, and its performance in\nbenchmarks on video conversation, zero-shot video question answering, and video\nsummarization. We also explore qualitative characteristics of Pegasus-1 ,\ndemonstrating its capabilities as well as its limitations, in order to provide\nreaders a balanced view of its current state and its future direction.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14687v1",
    "published_date": "2024-04-23 02:32:57 UTC",
    "updated_date": "2024-04-23 02:32:57 UTC"
  },
  {
    "arxiv_id": "2404.14680v1",
    "title": "Automated Multi-Language to English Machine Translation Using Generative Pre-Trained Transformers",
    "authors": [
      "Elijah Pelofske",
      "Vincent Urias",
      "Lorie M. Liebrock"
    ],
    "abstract": "The task of accurate and efficient language translation is an extremely\nimportant information processing task. Machine learning enabled and automated\ntranslation that is accurate and fast is often a large topic of interest in the\nmachine learning and data science communities. In this study, we examine using\nlocal Generative Pretrained Transformer (GPT) models to perform automated zero\nshot black-box, sentence wise, multi-natural-language translation into English\ntext. We benchmark 16 different open-source GPT models, with no custom\nfine-tuning, from the Huggingface LLM repository for translating 50 different\nnon-English languages into English using translated TED Talk transcripts as the\nreference dataset. These GPT model inference calls are performed strictly\nlocally, on single A100 Nvidia GPUs. Benchmark metrics that are reported are\nlanguage translation accuracy, using BLEU, GLEU, METEOR, and chrF text overlap\nmeasures, and wall-clock time for each sentence translation. The best overall\nperforming GPT model for translating into English text for the BLEU metric is\nReMM-v2-L2-13B with a mean score across all tested languages of $0.152$, for\nthe GLEU metric is ReMM-v2-L2-13B with a mean score across all tested languages\nof $0.256$, for the chrF metric is Llama2-chat-AYT-13B with a mean score across\nall tested languages of $0.448$, and for the METEOR metric is ReMM-v2-L2-13B\nwith a mean score across all tested languages of $0.438$.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14680v1",
    "published_date": "2024-04-23 02:19:35 UTC",
    "updated_date": "2024-04-23 02:19:35 UTC"
  },
  {
    "arxiv_id": "2404.14674v1",
    "title": "HOIN: High-Order Implicit Neural Representations",
    "authors": [
      "Yang Chen",
      "Ruituo Wu",
      "Yipeng Liu",
      "Ce Zhu"
    ],
    "abstract": "Implicit neural representations (INR) suffer from worsening spectral bias,\nwhich results in overly smooth solutions to the inverse problem. To deal with\nthis problem, we propose a universal framework for processing inverse problems\ncalled \\textbf{High-Order Implicit Neural Representations (HOIN)}. By refining\nthe traditional cascade structure to foster high-order interactions among\nfeatures, HOIN enhances the model's expressive power and mitigates spectral\nbias through its neural tangent kernel's (NTK) strong diagonal properties,\naccelerating and optimizing inverse problem resolution. By analyzing the\nmodel's expression space, high-order derivatives, and the NTK matrix, we\ntheoretically validate the feasibility of HOIN. HOIN realizes 1 to 3 dB\nimprovements in most inverse problems, establishing a new state-of-the-art\nrecovery quality and training efficiency, thus providing a new general paradigm\nfor INR and paving the way for it to solve the inverse problem.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14674v1",
    "published_date": "2024-04-23 02:00:58 UTC",
    "updated_date": "2024-04-23 02:00:58 UTC"
  },
  {
    "arxiv_id": "2404.14664v1",
    "title": "Employing Layerwised Unsupervised Learning to Lessen Data and Loss Requirements in Forward-Forward Algorithms",
    "authors": [
      "Taewook Hwang",
      "Hyein Seo",
      "Sangkeun Jung"
    ],
    "abstract": "Recent deep learning models such as ChatGPT utilizing the back-propagation\nalgorithm have exhibited remarkable performance. However, the disparity between\nthe biological brain processes and the back-propagation algorithm has been\nnoted. The Forward-Forward algorithm, which trains deep learning models solely\nthrough the forward pass, has emerged to address this. Although the\nForward-Forward algorithm cannot replace back-propagation due to limitations\nsuch as having to use special input and loss functions, it has the potential to\nbe useful in special situations where back-propagation is difficult to use. To\nwork around this limitation and verify usability, we propose an Unsupervised\nForward-Forward algorithm. Using an unsupervised learning model enables\ntraining with usual loss functions and inputs without restriction. Through this\napproach, we lead to stable learning and enable versatile utilization across\nvarious datasets and tasks. From a usability perspective, given the\ncharacteristics of the Forward-Forward algorithm and the advantages of the\nproposed method, we anticipate its practical application even in scenarios such\nas federated learning, where deep learning layers need to be trained separately\nin physically distributed environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.14664v1",
    "published_date": "2024-04-23 01:49:12 UTC",
    "updated_date": "2024-04-23 01:49:12 UTC"
  },
  {
    "arxiv_id": "2404.14660v1",
    "title": "AI Procurement Checklists: Revisiting Implementation in the Age of AI Governance",
    "authors": [
      "Tom Zick",
      "Mason Kortz",
      "David Eaves",
      "Finale Doshi-Velez"
    ],
    "abstract": "Public sector use of AI has been quietly on the rise for the past decade, but\nonly recently have efforts to regulate it entered the cultural zeitgeist. While\nsimple to articulate, promoting ethical and effective roll outs of AI systems\nin government is a notoriously elusive task. On the one hand there are\nhard-to-address pitfalls associated with AI-based tools, including concerns\nabout bias towards marginalized communities, safety, and gameability. On the\nother, there is pressure not to make it too difficult to adopt AI, especially\nin the public sector which typically has fewer resources than the private\nsector$\\unicode{x2014}$conserving scarce government resources is often the draw\nof using AI-based tools in the first place. These tensions create a real risk\nthat procedures built to ensure marginalized groups are not hurt by government\nuse of AI will, in practice, be performative and ineffective. To inform the\nlatest wave of regulatory efforts in the United States, we look to\njurisdictions with mature regulations around government AI use. We report on\nlessons learned by officials in Brazil, Singapore and Canada, who have\ncollectively implemented risk categories, disclosure requirements and\nassessments into the way they procure AI tools. In particular, we investigate\ntwo implemented checklists: the Canadian Directive on Automated Decision-Making\n(CDADM) and the World Economic Forum's AI Procurement in a Box (WEF). We detail\nthree key pitfalls around expertise, risk frameworks and transparency, that can\ndecrease the efficacy of regulations aimed at government AI use and suggest\navenues for improvement.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14660v1",
    "published_date": "2024-04-23 01:45:38 UTC",
    "updated_date": "2024-04-23 01:45:38 UTC"
  },
  {
    "arxiv_id": "2405.06660v1",
    "title": "AI and Machine Learning for Next Generation Science Assessments",
    "authors": [
      "Xiaoming Zhai"
    ],
    "abstract": "This chapter focuses on the transformative role of Artificial Intelligence\n(AI) and Machine Learning (ML) in science assessments. The paper begins with a\ndiscussion of the Framework for K-12 Science Education, which calls for a shift\nfrom conceptual learning to knowledge-in-use. This shift necessitates the\ndevelopment of new types of assessments that align with the Framework's three\ndimensions: science and engineering practices, disciplinary core ideas, and\ncrosscutting concepts. The paper further highlights the limitations of\ntraditional assessment methods like multiple-choice questions, which often fail\nto capture the complexities of scientific thinking and three-dimensional\nlearning in science. It emphasizes the need for performance-based assessments\nthat require students to engage in scientific practices like modeling,\nexplanation, and argumentation. The paper achieves three major goals: reviewing\nthe current state of ML-based assessments in science education, introducing a\nframework for scoring accuracy in ML-based automatic assessments, and\ndiscussing future directions and challenges. It delves into the evolution of\nML-based automatic scoring systems, discussing various types of ML, like\nsupervised, unsupervised, and semi-supervised learning. These systems can\nprovide timely and objective feedback, thus alleviating the burden on teachers.\nThe paper concludes by exploring pre-trained models like BERT and finetuned\nChatGPT, which have shown promise in assessing students' written responses\neffectively.",
    "categories": [
      "physics.ed-ph",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "physics.ed-ph",
    "comment": "18 pages, book chapter, in the book: Jiao, H., & Lissitz, R. W.\n  (Eds.). Machine learning, natural language processing and psychometrics.\n  Charlotte, NC: Information Age Publisher",
    "pdf_url": "http://arxiv.org/pdf/2405.06660v1",
    "published_date": "2024-04-23 01:39:20 UTC",
    "updated_date": "2024-04-23 01:39:20 UTC"
  },
  {
    "arxiv_id": "2404.14646v2",
    "title": "Exploring and Unleashing the Power of Large Language Models in Automated Code Translation",
    "authors": [
      "Zhen Yang",
      "Fang Liu",
      "Zhongxing Yu",
      "Jacky Wai Keung",
      "Jia Li",
      "Shuo Liu",
      "Yifan Hong",
      "Xiaoxue Ma",
      "Zhi Jin",
      "Ge Li"
    ],
    "abstract": "Code translation tools (transpilers) are developed for automatic\nsource-to-source translation. Although learning-based transpilers have shown\nimpressive enhancement against rule-based counterparts, owing to their\ntask-specific pre-training on extensive monolingual corpora. Their current\nperformance still remains unsatisfactory for practical deployment, and the\nassociated training resources are also prohibitively expensive. LLMs\npre-trained on huge amounts of human-written code/text have shown remarkable\nperformance in many code intelligence tasks due to their powerful generality,\neven without task-specific training. Thus, LLMs can potentially circumvent the\nabove limitations, but they have not been exhaustively explored yet. This paper\ninvestigates diverse LLMs and learning-based transpilers for automated code\ntranslation tasks, finding that: although certain LLMs have outperformed\ncurrent transpilers, they still have some accuracy issues, where most of the\nfailures are induced by a lack of comprehension of source programs, missing\nclear instructions on I/O types in translation, and ignoring discrepancies\nbetween source and target programs. Enlightened by the above findings, we\nfurther propose UniTrans, a Unified code Translation framework, applicable to\nvarious LLMs, for unleashing their power in this field. Specifically, UniTrans\nfirst crafts a series of test cases for target programs with the assistance of\nsource programs. Next, it harnesses the above auto-generated test cases to\naugment the code translation and then evaluate their correctness via execution.\nAfterward, UniTrans further (iteratively) repairs incorrectly translated\nprograms prompted by test case execution results. Extensive experiments are\nconducted on six settings of translation datasets between Python, Java, and\nC++. Three recent LLMs of diverse sizes are tested with UniTrans, and all\nachieve substantial improvements.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "23 pages, 7 figures, accepted by FSE'24 (2024 ACM International\n  Conference on the Foundations of Software Engineering)",
    "pdf_url": "http://arxiv.org/pdf/2404.14646v2",
    "published_date": "2024-04-23 00:49:46 UTC",
    "updated_date": "2024-05-11 13:20:19 UTC"
  },
  {
    "arxiv_id": "2407.05176v1",
    "title": "Towards Socially and Environmentally Responsible AI",
    "authors": [
      "Pengfei Li",
      "Yejia Liu",
      "Jianyi Yang",
      "Shaolei Ren"
    ],
    "abstract": "The sharply increasing sizes of artificial intelligence (AI) models come with\nsignificant energy consumption and environmental footprints, which can\ndisproportionately impact certain (often marginalized) regions and hence create\nenvironmental inequity concerns. Moreover, concerns with social inequity have\nalso emerged, as AI computing resources may not be equitably distributed across\nthe globe and users from certain disadvantaged regions with severe resource\nconstraints can consistently experience inferior model performance.\nImportantly, the inequity concerns that encompass both social and environmental\ndimensions still remain unexplored and have increasingly hindered responsible\nAI. In this paper, we leverage the spatial flexibility of AI inference\nworkloads and propose equitable geographical load balancing (GLB) to fairly\nbalance AI's regional social and environmental costs. Concretely, to penalize\nthe disproportionately high social and environmental costs for equity, we\nintroduce $L_q$ norms as novel regularization terms into the optimization\nobjective for GLB decisions. Our empirical results based on real-world AI\ninference traces demonstrate that while the existing GLB algorithms result in\ndisproportionately large social and environmental costs in certain regions, our\nproposed equitable GLB can fairly balance AI's negative social and\nenvironmental costs across all the regions.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Presented at HotEthics 2024 (co-located with ASPLOS' 24)",
    "pdf_url": "http://arxiv.org/pdf/2407.05176v1",
    "published_date": "2024-04-23 00:41:41 UTC",
    "updated_date": "2024-04-23 00:41:41 UTC"
  },
  {
    "arxiv_id": "2404.14635v1",
    "title": "Digital Twins for forecasting and decision optimisation with machine learning: applications in wastewater treatment",
    "authors": [
      "Matthew Colwell",
      "Mahdi Abolghasemi"
    ],
    "abstract": "Prediction and optimisation are two widely used techniques that have found\nmany applications in solving real-world problems. While prediction is concerned\nwith estimating the unknown future values of a variable, optimisation is\nconcerned with optimising the decision given all the available data. These\nmethods are used together to solve problems for sequential decision-making\nwhere often we need to predict the future values of variables and then use them\nfor determining the optimal decisions. This paradigm is known as forecast and\noptimise and has numerous applications, e.g., forecast demand for a product and\nthen optimise inventory, forecast energy demand and schedule generations,\nforecast demand for a service and schedule staff, to name a few. In this\nextended abstract, we review a digital twin that was developed and applied in\nwastewater treatment in Urban Utility to improve their operational efficiency.\nWhile the current study is tailored to the case study problem, the underlying\nprinciples can be used to solve similar problems in other domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "A bit thin, but an interesting application of ML methods for decision\n  making",
    "pdf_url": "http://arxiv.org/pdf/2404.14635v1",
    "published_date": "2024-04-23 00:18:20 UTC",
    "updated_date": "2024-04-23 00:18:20 UTC"
  },
  {
    "arxiv_id": "2405.02325v4",
    "title": "Are Biological Systems More Intelligent Than Artificial Intelligence?",
    "authors": [
      "Michael Timothy Bennett"
    ],
    "abstract": "Are biological self-organising systems more `intelligent' than artificial\nintelligence? If so, why? We frame intelligence as adaptability, and explore\nthis question using a mathematical formalism of causal learning. We compare\nsystems by how they delegate control, illustrating how this applies with\nexamples of computational, biological, human organisational and economic\nsystems. We formally show the scale-free, dynamic, bottom-up architecture of\nbiological self-organisation allows for more efficient adaptation than the\nstatic top-down architecture typical of computers, because adaptation can take\nplace at lower levels of abstraction. Artificial intelligence rests on a\nstatic, human-engineered `stack'. It only adapts at high levels of abstraction.\nTo put it provocatively, a static computational stack is like an inflexible\nbureaucracy. Biology is more `intelligent' because it delegates adaptation down\nthe stack. We call this multilayer-causal-learning. It inherits a flaw of\nbiological systems. Cells become cancerous when isolated from the collective\ninformational structure, reverting to primitive transcriptional behaviour. We\nshow states analogous to cancer occur when collectives are too tightly\nconstrained. To adapt to adverse conditions control should be delegated to the\ngreatest extent, like the doctrine of mission-command. Our result shows how to\ndesign more robust systems and lays a mathematical foundation for future\nempirical research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Definitions shared with arXiv:2404.07227, arXiv:2302.00843",
    "pdf_url": "http://arxiv.org/pdf/2405.02325v4",
    "published_date": "2024-04-23 00:13:14 UTC",
    "updated_date": "2025-01-23 05:24:36 UTC"
  }
]
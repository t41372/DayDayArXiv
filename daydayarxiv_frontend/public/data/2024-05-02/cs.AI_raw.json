[
  {
    "arxiv_id": "2407.01557v1",
    "title": "AI Governance and Accountability: An Analysis of Anthropic's Claude",
    "authors": [
      "Aman Priyanshu",
      "Yash Maurya",
      "Zuofei Hong"
    ],
    "abstract": "As AI systems become increasingly prevalent and impactful, the need for\neffective AI governance and accountability measures is paramount. This paper\nexamines the AI governance landscape, focusing on Anthropic's Claude, a\nfoundational AI model. We analyze Claude through the lens of the NIST AI Risk\nManagement Framework and the EU AI Act, identifying potential threats and\nproposing mitigation strategies. The paper highlights the importance of\ntransparency, rigorous benchmarking, and comprehensive data handling processes\nin ensuring the responsible development and deployment of AI systems. We\nconclude by discussing the social impact of AI governance and the ethical\nconsiderations surrounding AI accountability.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01557v1",
    "published_date": "2024-05-02 23:37:06 UTC",
    "updated_date": "2024-05-02 23:37:06 UTC"
  },
  {
    "arxiv_id": "2405.01776v1",
    "title": "An Approach to Systematic Data Acquisition and Data-Driven Simulation for the Safety Testing of Automated Driving Functions",
    "authors": [
      "Leon Eisemann",
      "Mirjam Fehling-Kaschek",
      "Henrik Gommel",
      "David Hermann",
      "Marvin Klemp",
      "Martin Lauer",
      "Benjamin Lickert",
      "Florian Luettner",
      "Robin Moss",
      "Nicole Neis",
      "Maria Pohle",
      "Simon Romanski",
      "Daniel Stadler",
      "Alexander Stolz",
      "Jens Ziehn",
      "Jingxing Zhou"
    ],
    "abstract": "With growing complexity and criticality of automated driving functions in\nroad traffic and their operational design domains (ODD), there is increasing\ndemand for covering significant proportions of development, validation, and\nverification in virtual environments and through simulation models.\n  If, however, simulations are meant not only to augment real-world\nexperiments, but to replace them, quantitative approaches are required that\nmeasure to what degree and under which preconditions simulation models\nadequately represent reality, and thus, using their results accordingly.\nEspecially in R&D areas related to the safety impact of the \"open world\", there\nis a significant shortage of real-world data to parameterize and/or validate\nsimulations - especially with respect to the behavior of human traffic\nparticipants, whom automated driving functions will meet in mixed traffic.\n  We present an approach to systematically acquire data in public traffic by\nheterogeneous means, transform it into a unified representation, and use it to\nautomatically parameterize traffic behavior models for use in data-driven\nvirtual validation of automated driving functions.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01776v1",
    "published_date": "2024-05-02 23:24:27 UTC",
    "updated_date": "2024-05-02 23:24:27 UTC"
  },
  {
    "arxiv_id": "2405.01768v3",
    "title": "Context Steering: Controllable Personalization at Inference Time",
    "authors": [
      "Jerry Zhi-Yang He",
      "Sashrika Pandey",
      "Mariah L. Schrum",
      "Anca Dragan"
    ],
    "abstract": "To deliver high-quality, personalized responses, large language models (LLMs)\nmust effectively incorporate context -- personal, demographic, and cultural\ninformation specific to an end-user. For example, asking the model to explain\nNewton's second law with the context \"I am a toddler\" should produce a response\ndifferent from when the context is \"I am a physics professor\". However,\nleveraging the context in practice is a nuanced and challenging task, and is\noften dependent on the specific situation or user base. The model must strike a\nbalance between providing specific, personalized responses and maintaining\ngeneral applicability. Current solutions, such as prompt-engineering and\nfine-tuning, require collection of contextually appropriate responses as\nexamples, making them time-consuming and less flexible to use across different\ncontexts. In this work, we introduce Context Steering (CoS) -- a simple,\ntraining-free decoding approach that amplifies the influence of the context in\nnext token predictions. CoS computes contextual influence by comparing the\noutput probabilities from two LLM forward passes: one that includes the context\nand one that does not. By linearly scaling the contextual influence, CoS allows\npractitioners to flexibly control the degree of personalization for different\nuse cases. We show that CoS can be applied to autoregressive LLMs, and\ndemonstrates strong performance in personalized recommendations. Additionally,\nwe show that CoS can function as a Bayesian Generative model to infer and\nquantify correlations between open-ended texts, broadening its potential\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01768v3",
    "published_date": "2024-05-02 22:37:38 UTC",
    "updated_date": "2025-02-06 03:33:08 UTC"
  },
  {
    "arxiv_id": "2405.05275v2",
    "title": "SoMeR: Multi-View User Representation Learning for Social Media",
    "authors": [
      "Siyi Guo",
      "Keith Burghardt",
      "Valeria Pantè",
      "Kristina Lerman"
    ],
    "abstract": "Social media user representation learning aims to capture user preferences,\ninterests, and behaviors in low-dimensional vector representations. These\nrepresentations are critical to a range of social problems, including\npredicting user behaviors and detecting inauthentic accounts. However, existing\nmethods are either designed for commercial applications, or rely on specific\nfeatures like text contents, activity patterns, or platform metadata, failing\nto holistically model user behavior across different modalities. To address\nthese limitations, we propose SoMeR, a Social Media user Representation\nlearning framework that incorporates temporal activities, text contents,\nprofile information, and network interactions to learn comprehensive user\nportraits. SoMeR encodes user post streams as sequences of time-stamped textual\nfeatures, uses transformers to embed this along with profile data, and jointly\ntrains with link prediction and contrastive learning objectives to capture user\nsimilarity. We demonstrate SoMeR's versatility through three applications: 1)\nIdentifying information operation driver accounts, 2) Measuring online\npolarization after major events, and 3) Predicting future user participation in\nReddit hate communities. SoMeR provides new solutions to better understand user\nbehavior in the socio-political domains, enabling more informed decisions and\ninterventions.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05275v2",
    "published_date": "2024-05-02 22:26:55 UTC",
    "updated_date": "2025-03-20 23:54:17 UTC"
  },
  {
    "arxiv_id": "2405.01760v1",
    "title": "Reinforcement Learning-Guided Semi-Supervised Learning",
    "authors": [
      "Marzi Heidari",
      "Hanping Zhang",
      "Yuhong Guo"
    ],
    "abstract": "In recent years, semi-supervised learning (SSL) has gained significant\nattention due to its ability to leverage both labeled and unlabeled data to\nimprove model performance, especially when labeled data is scarce. However,\nmost current SSL methods rely on heuristics or predefined rules for generating\npseudo-labels and leveraging unlabeled data. They are limited to exploiting\nloss functions and regularization methods within the standard norm. In this\npaper, we propose a novel Reinforcement Learning (RL) Guided SSL method,\nRLGSSL, that formulates SSL as a one-armed bandit problem and deploys an\ninnovative RL loss based on weighted reward to adaptively guide the learning\nprocess of the prediction model. RLGSSL incorporates a carefully designed\nreward function that balances the use of labeled and unlabeled data to enhance\ngeneralization performance. A semi-supervised teacher-student framework is\nfurther deployed to increase the learning stability. We demonstrate the\neffectiveness of RLGSSL through extensive experiments on several benchmark\ndatasets and show that our approach achieves consistent superior performance\ncompared to state-of-the-art SSL methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01760v1",
    "published_date": "2024-05-02 21:52:24 UTC",
    "updated_date": "2024-05-02 21:52:24 UTC"
  },
  {
    "arxiv_id": "2405.01756v3",
    "title": "Segmentation-Free Outcome Prediction from Head and Neck Cancer PET/CT Images: Deep Learning-Based Feature Extraction from Multi-Angle Maximum Intensity Projections (MA-MIPs)",
    "authors": [
      "Amirhosein Toosi",
      "Isaac Shiri",
      "Habib Zaidi",
      "Arman Rahmim"
    ],
    "abstract": "We introduce an innovative, simple, effective segmentation-free approach for\noutcome prediction in head \\& neck cancer (HNC) patients. By harnessing deep\nlearning-based feature extraction techniques and multi-angle maximum intensity\nprojections (MA-MIPs) applied to Fluorodeoxyglucose Positron Emission\nTomography (FDG-PET) volumes, our proposed method eliminates the need for\nmanual segmentations of regions-of-interest (ROIs) such as primary tumors and\ninvolved lymph nodes. Instead, a state-of-the-art object detection model is\ntrained to perform automatic cropping of the head and neck region on the PET\nvolumes. A pre-trained deep convolutional neural network backbone is then\nutilized to extract deep features from MA-MIPs obtained from 72 multi-angel\naxial rotations of the cropped PET volumes. These deep features extracted from\nmultiple projection views of the PET volumes are then aggregated and fused, and\nemployed to perform recurrence-free survival analysis on a cohort of 489 HNC\npatients. The proposed approach outperforms the best performing method on the\ntarget dataset for the task of recurrence-free survival analysis. By\ncircumventing the manual delineation of the malignancies on the FDG PET-CT\nimages, our approach eliminates the dependency on subjective interpretations\nand highly enhances the reproducibility of the proposed survival analysis\nmethod.",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "62Nxx",
      "I.2.m"
    ],
    "primary_category": "physics.med-ph",
    "comment": "15 pages, 4 tables, 4 figures. Published in Cancers 2024, Volume 16,\n  Issue 14, page 2538",
    "pdf_url": "http://arxiv.org/pdf/2405.01756v3",
    "published_date": "2024-05-02 21:46:13 UTC",
    "updated_date": "2024-12-04 12:37:40 UTC"
  },
  {
    "arxiv_id": "2405.01745v1",
    "title": "Large Language Models for UAVs: Current State and Pathways to the Future",
    "authors": [
      "Shumaila Javaid",
      "Nasir Saeed",
      "Bin He"
    ],
    "abstract": "Unmanned Aerial Vehicles (UAVs) have emerged as a transformative technology\nacross diverse sectors, offering adaptable solutions to complex challenges in\nboth military and civilian domains. Their expanding capabilities present a\nplatform for further advancement by integrating cutting-edge computational\ntools like Artificial Intelligence (AI) and Machine Learning (ML) algorithms.\nThese advancements have significantly impacted various facets of human life,\nfostering an era of unparalleled efficiency and convenience. Large Language\nModels (LLMs), a key component of AI, exhibit remarkable learning and\nadaptation capabilities within deployed environments, demonstrating an evolving\nform of intelligence with the potential to approach human-level proficiency.\nThis work explores the significant potential of integrating UAVs and LLMs to\npropel the development of autonomous systems. We comprehensively review LLM\narchitectures, evaluating their suitability for UAV integration. Additionally,\nwe summarize the state-of-the-art LLM-based UAV architectures and identify\nnovel opportunities for LLM embedding within UAV frameworks. Notably, we focus\non leveraging LLMs to refine data analysis and decision-making processes,\nspecifically for enhanced spectral sensing and sharing in UAV applications.\nFurthermore, we investigate how LLM integration expands the scope of existing\nUAV applications, enabling autonomous data processing, improved\ndecision-making, and faster response times in emergency scenarios like disaster\nresponse and network restoration. Finally, we highlight crucial areas for\nfuture research that are critical for facilitating the effective integration of\nLLMs and UAVs.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01745v1",
    "published_date": "2024-05-02 21:30:10 UTC",
    "updated_date": "2024-05-02 21:30:10 UTC"
  },
  {
    "arxiv_id": "2405.01744v2",
    "title": "ALCM: Autonomous LLM-Augmented Causal Discovery Framework",
    "authors": [
      "Elahe Khatibi",
      "Mahyar Abbasian",
      "Zhongqi Yang",
      "Iman Azimi",
      "Amir M. Rahmani"
    ],
    "abstract": "To perform effective causal inference in high-dimensional datasets,\ninitiating the process with causal discovery is imperative, wherein a causal\ngraph is generated based on observational data. However, obtaining a complete\nand accurate causal graph poses a formidable challenge, recognized as an NP-\nhard problem. Recently, the advent of Large Language Models (LLMs) has ushered\nin a new era, indicating their emergent capabilities and widespread\napplicability in facilitating causal reasoning across diverse domains, such as\nmedicine, finance, and science. The expansive knowledge base of LLMs holds the\npotential to elevate the field of causal reasoning by offering\ninterpretability, making inferences, generalizability, and uncovering novel\ncausal structures. In this paper, we introduce a new framework, named\nAutonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize\ndata-driven causal discovery algorithms and LLMs, automating the generation of\na more resilient, accurate, and explicable causal graph. The ALCM consists of\nthree integral components: causal structure learning, causal wrapper, and\nLLM-driven causal refiner. These components autonomously collaborate within a\ndynamic environment to address causal discovery questions and deliver plausible\ncausal graphs. We evaluate the ALCM framework by implementing two\ndemonstrations on seven well-known datasets. Experimental results demonstrate\nthat ALCM outperforms existing LLM methods and conventional data-driven causal\nreasoning mechanisms. This study not only shows the effectiveness of the ALCM\nbut also underscores new research directions in leveraging the causal reasoning\ncapabilities of LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01744v2",
    "published_date": "2024-05-02 21:27:45 UTC",
    "updated_date": "2025-04-16 19:45:24 UTC"
  },
  {
    "arxiv_id": "2405.01741v3",
    "title": "PVF (Parameter Vulnerability Factor): A Scalable Metric for Understanding AI Vulnerability Against SDCs in Model Parameters",
    "authors": [
      "Xun Jiao",
      "Fred Lin",
      "Harish D. Dixit",
      "Joel Coburn",
      "Abhinav Pandey",
      "Han Wang",
      "Venkat Ramesh",
      "Jianyu Huang",
      "Wang Xu",
      "Daniel Moore",
      "Sriram Sankar"
    ],
    "abstract": "Reliability of AI systems is a fundamental concern for the successful\ndeployment and widespread adoption of AI technologies. Unfortunately, the\nescalating complexity and heterogeneity of AI hardware systems make them\nincreasingly susceptible to hardware faults, e.g., silent data corruptions\n(SDC), that can potentially corrupt model parameters. When this occurs during\nAI inference/servicing, it can potentially lead to incorrect or degraded model\noutput for users, ultimately affecting the quality and reliability of AI\nservices. In light of the escalating threat, it is crucial to address key\nquestions: How vulnerable are AI models to parameter corruptions, and how do\ndifferent components (such as modules, layers) of the models exhibit varying\nvulnerabilities to parameter corruptions? To systematically address this\nquestion, we propose a novel quantitative metric, Parameter Vulnerability\nFactor (PVF), inspired by architectural vulnerability factor (AVF) in computer\narchitecture community, aiming to standardize the quantification of AI model\nvulnerability against parameter corruptions. We define a model parameter's PVF\nas the probability that a corruption in that particular model parameter will\nresult in an incorrect output. In this paper, we present several use cases on\napplying PVF to three types of tasks/models during inference -- recommendation\n(DLRM), vision classification (CNN), and text classification (BERT), while\npresenting an in-depth vulnerability analysis on DLRM. PVF can provide pivotal\ninsights to AI hardware designers in balancing the tradeoff between fault\nprotection and performance/efficiency such as mapping vulnerable AI parameter\ncomponents to well-protected hardware modules. PVF metric is applicable to any\nAI model and has a potential to help unify and standardize AI\nvulnerability/resilience evaluation practice.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01741v3",
    "published_date": "2024-05-02 21:23:34 UTC",
    "updated_date": "2024-06-11 22:37:33 UTC"
  },
  {
    "arxiv_id": "2405.01734v1",
    "title": "Diabetic Retinopathy Detection Using Quantum Transfer Learning",
    "authors": [
      "Ankush Jain",
      "Rinav Gupta",
      "Jai Singhal"
    ],
    "abstract": "Diabetic Retinopathy (DR), a prevalent complication in diabetes patients, can\nlead to vision impairment due to lesions formed on the retina. Detecting DR at\nan advanced stage often results in irreversible blindness. The traditional\nprocess of diagnosing DR through retina fundus images by ophthalmologists is\nnot only time-intensive but also expensive. While classical transfer learning\nmodels have been widely adopted for computer-aided detection of DR, their high\nmaintenance costs can hinder their detection efficiency. In contrast, Quantum\nTransfer Learning offers a more effective solution to this challenge. This\napproach is notably advantageous because it operates on heuristic principles,\nmaking it highly optimized for the task. Our proposed methodology leverages\nthis hybrid quantum transfer learning technique to detect DR. To construct our\nmodel, we utilize the APTOS 2019 Blindness Detection dataset, available on\nKaggle. We employ the ResNet-18, ResNet34, ResNet50, ResNet101, ResNet152 and\nInception V3, pre-trained classical neural networks, for the initial feature\nextraction. For the classification stage, we use a Variational Quantum\nClassifier. Our hybrid quantum model has shown remarkable results, achieving an\naccuracy of 97% for ResNet-18. This demonstrates that quantum computing, when\nintegrated with quantum machine learning, can perform tasks with a level of\npower and efficiency unattainable by classical computers alone. By harnessing\nthese advanced technologies, we can significantly improve the detection and\ndiagnosis of Diabetic Retinopathy, potentially saving many from the risk of\nblindness.\n  Keywords: Diabetic Retinopathy, Quantum Transfer Learning, Deep Learning",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 12 figures and 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.01734v1",
    "published_date": "2024-05-02 21:09:39 UTC",
    "updated_date": "2024-05-02 21:09:39 UTC"
  },
  {
    "arxiv_id": "2405.02351v2",
    "title": "Towards General Neural Surrogate Solvers with Specialized Neural Accelerators",
    "authors": [
      "Chenkai Mao",
      "Robert Lupoiu",
      "Tianxiang Dai",
      "Mingkun Chen",
      "Jonathan A. Fan"
    ],
    "abstract": "Surrogate neural network-based partial differential equation (PDE) solvers\nhave the potential to solve PDEs in an accelerated manner, but they are largely\nlimited to systems featuring fixed domain sizes, geometric layouts, and\nboundary conditions. We propose Specialized Neural Accelerator-Powered Domain\nDecomposition Methods (SNAP-DDM), a DDM-based approach to PDE solving in which\nsubdomain problems containing arbitrary boundary conditions and geometric\nparameters are accurately solved using an ensemble of specialized neural\noperators. We tailor SNAP-DDM to 2D electromagnetics and fluidic flow problems\nand show how innovations in network architecture and loss function engineering\ncan produce specialized surrogate subdomain solvers with near unity accuracy.\nWe utilize these solvers with standard DDM algorithms to accurately solve\nfreeform electromagnetics and fluids problems featuring a wide range of domain\nsizes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "physics.optics"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 7 Figures, to be published in ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02351v2",
    "published_date": "2024-05-02 21:08:49 UTC",
    "updated_date": "2024-06-14 23:20:23 UTC"
  },
  {
    "arxiv_id": "2405.01724v1",
    "title": "Large Language Models are Inconsistent and Biased Evaluators",
    "authors": [
      "Rickard Stureborg",
      "Dimitris Alikaniotis",
      "Yoshi Suhara"
    ],
    "abstract": "The zero-shot capability of Large Language Models (LLMs) has enabled highly\nflexible, reference-free metrics for various tasks, making LLM evaluators\ncommon tools in NLP. However, the robustness of these LLM evaluators remains\nrelatively understudied; existing work mainly pursued optimal performance in\nterms of correlating LLM scores with human expert scores. In this paper, we\nconduct a series of analyses using the SummEval dataset and confirm that LLMs\nare biased evaluators as they: (1) exhibit familiarity bias-a preference for\ntext with lower perplexity, (2) show skewed and biased distributions of\nratings, and (3) experience anchoring effects for multi-attribute judgments. We\nalso found that LLMs are inconsistent evaluators, showing low \"inter-sample\"\nagreement and sensitivity to prompt differences that are insignificant to human\nunderstanding of text quality. Furthermore, we share recipes for configuring\nLLM evaluators to mitigate these limitations. Experimental results on the RoSE\ndataset demonstrate improvements over the state-of-the-art LLM evaluators.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50 (Primary) 68T01, 68T37, 91F20 (Secondary)",
      "I.2; I.2.7; I.7"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01724v1",
    "published_date": "2024-05-02 20:42:28 UTC",
    "updated_date": "2024-05-02 20:42:28 UTC"
  },
  {
    "arxiv_id": "2405.01723v1",
    "title": "Zero-Shot Monocular Motion Segmentation in the Wild by Combining Deep Learning with Geometric Motion Model Fusion",
    "authors": [
      "Yuxiang Huang",
      "Yuhao Chen",
      "John Zelek"
    ],
    "abstract": "Detecting and segmenting moving objects from a moving monocular camera is\nchallenging in the presence of unknown camera motion, diverse object motions\nand complex scene structures. Most existing methods rely on a single motion cue\nto perform motion segmentation, which is usually insufficient when facing\ndifferent complex environments. While a few recent deep learning based methods\nare able to combine multiple motion cues to achieve improved accuracy, they\ndepend heavily on vast datasets and extensive annotations, making them less\nadaptable to new scenarios. To address these limitations, we propose a novel\nmonocular dense segmentation method that achieves state-of-the-art motion\nsegmentation results in a zero-shot manner. The proposed method synergestically\ncombines the strengths of deep learning and geometric model fusion methods by\nperforming geometric model fusion on object proposals. Experiments show that\nour method achieves competitive results on several motion segmentation datasets\nand even surpasses some state-of-the-art supervised methods on certain\nbenchmarks, while not being trained on any data. We also present an ablation\nstudy to show the effectiveness of combining different geometric models\ntogether for motion segmentation, highlighting the value of our geometric model\nfusion strategy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by the 2024 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition Workshops (CVPRW)",
    "pdf_url": "http://arxiv.org/pdf/2405.01723v1",
    "published_date": "2024-05-02 20:42:17 UTC",
    "updated_date": "2024-05-02 20:42:17 UTC"
  },
  {
    "arxiv_id": "2405.01714v3",
    "title": "Interpretable Vital Sign Forecasting with Model Agnostic Attention Maps",
    "authors": [
      "Yuwei Liu",
      "Chen Dan",
      "Anubhav Bhatti",
      "Bingjie Shen",
      "Divij Gupta",
      "Suraj Parmar",
      "San Lee"
    ],
    "abstract": "Sepsis is a leading cause of mortality in intensive care units (ICUs),\nrepresenting a substantial medical challenge. The complexity of analyzing\ndiverse vital signs to predict sepsis further aggravates this issue. While deep\nlearning techniques have been advanced for early sepsis prediction, their\n'black-box' nature obscures the internal logic, impairing interpretability in\ncritical settings like ICUs. This paper introduces a framework that combines a\ndeep learning model with an attention mechanism that highlights the critical\ntime steps in the forecasting process, thus improving model interpretability\nand supporting clinical decision-making. We show that the attention mechanism\ncould be adapted to various black box time series forecasting models such as\nN-HiTS and N-BEATS. Our method preserves the accuracy of conventional deep\nlearning models while enhancing interpretability through\nattention-weight-generated heatmaps. We evaluated our model on the eICU-CRD\ndataset, focusing on forecasting vital signs for sepsis patients. We assessed\nits performance using mean squared error (MSE) and dynamic time warping (DTW)\nmetrics. We explored the attention maps of N-HiTS and N-BEATS, examining the\ndifferences in their performance and identifying crucial factors influencing\nvital sign forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01714v3",
    "published_date": "2024-05-02 20:19:07 UTC",
    "updated_date": "2024-05-21 21:02:59 UTC"
  },
  {
    "arxiv_id": "2405.01711v2",
    "title": "Individual Fairness Through Reweighting and Tuning",
    "authors": [
      "Abdoul Jalil Djiberou Mahamadou",
      "Lea Goetz",
      "Russ Altman"
    ],
    "abstract": "Inherent bias within society can be amplified and perpetuated by artificial\nintelligence (AI) systems. To address this issue, a wide range of solutions\nhave been proposed to identify and mitigate bias and enforce fairness for\nindividuals and groups. Recently, Graph Laplacian Regularizer (GLR), a\nregularization technique from the semi-supervised learning literature has been\nused as a substitute for the common Lipschitz condition to enhance individual\nfairness. Notable prior work has shown that enforcing individual fairness\nthrough a GLR can improve the transfer learning accuracy of AI models under\ncovariate shifts. However, the prior work defines a GLR on the source and\ntarget data combined, implicitly assuming that the target data are available at\ntrain time, which might not hold in practice. In this work, we investigated\nwhether defining a GLR independently on the train and target data could\nmaintain similar accuracy. Furthermore, we introduced the Normalized Fairness\nGain score (NFG) to measure individual fairness by measuring the amount of\ngained fairness when a GLR is used versus not. We evaluated the new and\noriginal methods under NFG, the Prediction Consistency (PC), and traditional\nclassification metrics on the German Credit Approval dataset. The results\nshowed that the two models achieved similar statistical mean performances over\nfive-fold cross-validation. Furthermore, the proposed metric showed that PC\nscores can be misleading as the scores can be high and statistically similar to\nfairness-enhanced models while NFG scores are small. This work therefore\nprovides new insights into when a GLR effectively enhances individual fairness\nand the pitfalls of PC.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 1 figure, and 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.01711v2",
    "published_date": "2024-05-02 20:15:25 UTC",
    "updated_date": "2024-05-07 19:55:01 UTC"
  },
  {
    "arxiv_id": "2405.02350v1",
    "title": "What makes Models Compositional? A Theoretical View: With Supplement",
    "authors": [
      "Parikshit Ram",
      "Tim Klinger",
      "Alexander G. Gray"
    ],
    "abstract": "Compositionality is thought to be a key component of language, and various\ncompositional benchmarks have been developed to empirically probe the\ncompositional generalization of existing sequence processing models. These\nbenchmarks often highlight failures of existing models, but it is not clear why\nthese models fail in this way. In this paper, we seek to theoretically\nunderstand the role the compositional structure of the models plays in these\nfailures and how this structure relates to their expressivity and sample\ncomplexity. We propose a general neuro-symbolic definition of compositional\nfunctions and their compositional complexity. We then show how various existing\ngeneral and special purpose sequence processing models (such as recurrent,\nconvolution and attention-based ones) fit this definition and use it to analyze\ntheir compositional complexity. Finally, we provide theoretical guarantees for\nthe expressivity and systematic generalization of compositional models that\nexplicitly depend on our proposed definition and highlighting factors which\ndrive poor empirical performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Extended version of the original IJCAI 2024 paper with detailed\n  supplementary materials (27 pages, 7 figures)",
    "pdf_url": "http://arxiv.org/pdf/2405.02350v1",
    "published_date": "2024-05-02 20:10:27 UTC",
    "updated_date": "2024-05-02 20:10:27 UTC"
  },
  {
    "arxiv_id": "2405.01705v1",
    "title": "Long Tail Image Generation Through Feature Space Augmentation and Iterated Learning",
    "authors": [
      "Rafael Elberg",
      "Denis Parra",
      "Mircea Petrache"
    ],
    "abstract": "Image and multimodal machine learning tasks are very challenging to solve in\nthe case of poorly distributed data. In particular, data availability and\nprivacy restrictions exacerbate these hurdles in the medical domain. The state\nof the art in image generation quality is held by Latent Diffusion models,\nmaking them prime candidates for tackling this problem. However, a few key\nissues still need to be solved, such as the difficulty in generating data from\nunder-represented classes and a slow inference process. To mitigate these\nissues, we propose a new method for image augmentation in long-tailed data\nbased on leveraging the rich latent space of pre-trained Stable Diffusion\nModels. We create a modified separable latent space to mix head and tail class\nexamples. We build this space via Iterated Learning of underlying sparsified\nembeddings, which we apply to task-specific saliency maps via a K-NN approach.\nCode is available at\nhttps://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4; I.2"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01705v1",
    "published_date": "2024-05-02 20:03:19 UTC",
    "updated_date": "2024-05-02 20:03:19 UTC"
  },
  {
    "arxiv_id": "2405.01699v2",
    "title": "SOAR: Advancements in Small Body Object Detection for Aerial Imagery Using State Space Models and Programmable Gradients",
    "authors": [
      "Tushar Verma",
      "Jyotsna Singh",
      "Yash Bhartari",
      "Rishi Jarwal",
      "Suraj Singh",
      "Shubhkarman Singh"
    ],
    "abstract": "Small object detection in aerial imagery presents significant challenges in\ncomputer vision due to the minimal data inherent in small-sized objects and\ntheir propensity to be obscured by larger objects and background noise.\nTraditional methods using transformer-based models often face limitations\nstemming from the lack of specialized databases, which adversely affect their\nperformance with objects of varying orientations and scales. This underscores\nthe need for more adaptable, lightweight models. In response, this paper\nintroduces two innovative approaches that significantly enhance detection and\nsegmentation capabilities for small aerial objects. Firstly, we explore the use\nof the SAHI framework on the newly introduced lightweight YOLO v9 architecture,\nwhich utilizes Programmable Gradient Information (PGI) to reduce the\nsubstantial information loss typically encountered in sequential feature\nextraction processes. The paper employs the Vision Mamba model, which\nincorporates position embeddings to facilitate precise location-aware visual\nunderstanding, combined with a novel bidirectional State Space Model (SSM) for\neffective visual context modeling. This State Space Model adeptly harnesses the\nlinear complexity of CNNs and the global receptive field of Transformers,\nmaking it particularly effective in remote sensing image classification. Our\nexperimental results demonstrate substantial improvements in detection accuracy\nand processing efficiency, validating the applicability of these approaches for\nreal-time small object detection across diverse aerial scenarios. This paper\nalso discusses how these methodologies could serve as foundational models for\nfuture advancements in aerial object recognition technologies. The source code\nwill be made accessible here.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01699v2",
    "published_date": "2024-05-02 19:47:08 UTC",
    "updated_date": "2024-05-06 01:06:33 UTC"
  },
  {
    "arxiv_id": "2405.01686v2",
    "title": "Automatically Extracting Numerical Results from Randomized Controlled Trials with Large Language Models",
    "authors": [
      "Hye Sun Yun",
      "David Pogrebitskiy",
      "Iain J. Marshall",
      "Byron C. Wallace"
    ],
    "abstract": "Meta-analyses statistically aggregate the findings of different randomized\ncontrolled trials (RCTs) to assess treatment effectiveness. Because this yields\nrobust estimates of treatment effectiveness, results from meta-analyses are\nconsidered the strongest form of evidence. However, rigorous evidence syntheses\nare time-consuming and labor-intensive, requiring manual extraction of data\nfrom individual trials to be synthesized. Ideally, language technologies would\npermit fully automatic meta-analysis, on demand. This requires accurately\nextracting numerical results from individual trials, which has been beyond the\ncapabilities of natural language processing (NLP) models to date. In this work,\nwe evaluate whether modern large language models (LLMs) can reliably perform\nthis task. We annotate (and release) a modest but granular evaluation dataset\nof clinical trial reports with numerical findings attached to interventions,\ncomparators, and outcomes. Using this dataset, we evaluate the performance of\nseven LLMs applied zero-shot for the task of conditionally extracting numerical\nfindings from trial reports. We find that massive LLMs that can accommodate\nlengthy inputs are tantalizingly close to realizing fully automatic\nmeta-analysis, especially for dichotomous (binary) outcomes (e.g., mortality).\nHowever, LLMs -- including ones trained on biomedical texts -- perform poorly\nwhen the outcome measures are complex and tallying the results requires\ninference. This work charts a path toward fully automatic meta-analysis of RCTs\nvia LLMs, while also highlighting the limitations of existing models for this\naim.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages, 7 figures, 6 tables, MLHC 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01686v2",
    "published_date": "2024-05-02 19:20:11 UTC",
    "updated_date": "2024-07-25 03:29:09 UTC"
  },
  {
    "arxiv_id": "2405.01684v1",
    "title": "Intelligent Switching for Reset-Free RL",
    "authors": [
      "Darshan Patil",
      "Janarthanan Rajendran",
      "Glen Berseth",
      "Sarath Chandar"
    ],
    "abstract": "In the real world, the strong episode resetting mechanisms that are needed to\ntrain agents in simulation are unavailable. The \\textit{resetting} assumption\nlimits the potential of reinforcement learning in the real world, as providing\nresets to an agent usually requires the creation of additional handcrafted\nmechanisms or human interventions. Recent work aims to train agents\n(\\textit{forward}) with learned resets by constructing a second\n(\\textit{backward}) agent that returns the forward agent to the initial state.\nWe find that the termination and timing of the transitions between these two\nagents are crucial for algorithm success. With this in mind, we create a new\nalgorithm, Reset Free RL with Intelligently Switching Controller (RISC) which\nintelligently switches between the two agents based on the agent's confidence\nin achieving its current goal. Our new method achieves state-of-the-art\nperformance on several challenging environments for reset-free RL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01684v1",
    "published_date": "2024-05-02 19:15:00 UTC",
    "updated_date": "2024-05-02 19:15:00 UTC"
  },
  {
    "arxiv_id": "2405.01682v2",
    "title": "Leveraging Prompt-Learning for Structured Information Extraction from Crohn's Disease Radiology Reports in a Low-Resource Language",
    "authors": [
      "Liam Hazan",
      "Gili Focht",
      "Naama Gavrielov",
      "Roi Reichart",
      "Talar Hagopian",
      "Mary-Louise C. Greer",
      "Ruth Cytter Kuint",
      "Dan Turner",
      "Moti Freiman"
    ],
    "abstract": "Automatic conversion of free-text radiology reports into structured data\nusing Natural Language Processing (NLP) techniques is crucial for analyzing\ndiseases on a large scale. While effective for tasks in widely spoken languages\nlike English, generative large language models (LLMs) typically underperform\nwith less common languages and can pose potential risks to patient privacy.\nFine-tuning local NLP models is hindered by the skewed nature of real-world\nmedical datasets, where rare findings represent a significant data imbalance.\nWe introduce SMP-BERT, a novel prompt learning method that leverages the\nstructured nature of reports to overcome these challenges. In our studies\ninvolving a substantial collection of Crohn's disease radiology reports in\nHebrew (over 8,000 patients and 10,000 reports), SMP-BERT greatly surpassed\ntraditional fine-tuning methods in performance, notably in detecting infrequent\nconditions (AUC: 0.99 vs 0.94, F1: 0.84 vs 0.34). SMP-BERT empowers more\naccurate AI diagnostics available for low-resource languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01682v2",
    "published_date": "2024-05-02 19:11:54 UTC",
    "updated_date": "2024-05-22 09:36:25 UTC"
  },
  {
    "arxiv_id": "2405.01677v3",
    "title": "Balance Reward and Safety Optimization for Safe Reinforcement Learning: A Perspective of Gradient Manipulation",
    "authors": [
      "Shangding Gu",
      "Bilgehan Sel",
      "Yuhao Ding",
      "Lu Wang",
      "Qingwei Lin",
      "Ming Jin",
      "Alois Knoll"
    ],
    "abstract": "Ensuring the safety of Reinforcement Learning (RL) is crucial for its\ndeployment in real-world applications. Nevertheless, managing the trade-off\nbetween reward and safety during exploration presents a significant challenge.\nImproving reward performance through policy adjustments may adversely affect\nsafety performance. In this study, we aim to address this conflicting relation\nby leveraging the theory of gradient manipulation. Initially, we analyze the\nconflict between reward and safety gradients. Subsequently, we tackle the\nbalance between reward and safety optimization by proposing a soft switching\npolicy optimization method, for which we provide convergence analysis. Based on\nour theoretical examination, we provide a safe RL framework to overcome the\naforementioned challenge, and we develop a Safety-MuJoCo Benchmark to assess\nthe performance of safe RL algorithms. Finally, we evaluate the effectiveness\nof our method on the Safety-MuJoCo Benchmark and a popular safe RL benchmark,\nOmnisafe. Experimental results demonstrate that our algorithms outperform\nseveral state-of-the-art baselines in terms of balancing reward and safety\noptimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01677v3",
    "published_date": "2024-05-02 19:07:14 UTC",
    "updated_date": "2025-03-01 18:27:26 UTC"
  },
  {
    "arxiv_id": "2405.01663v1",
    "title": "ATNPA: A Unified View of Oversmoothing Alleviation in Graph Neural Networks",
    "authors": [
      "Yufei Jin",
      "Xingquan Zhu"
    ],
    "abstract": "Oversmoothing is a commonly observed challenge in graph neural network (GNN)\nlearning, where, as layers increase, embedding features learned from GNNs\nquickly become similar/indistinguishable, making them incapable of\ndifferentiating network proximity. A GNN with shallow layer architectures can\nonly learn short-term relation or localized structure information, limiting its\npower of learning long-term connection, evidenced by their inferior learning\nperformance on heterophilous graphs. Tackling oversmoothing is crucial to\nharness deep-layer architectures for GNNs. To date, many methods have been\nproposed to alleviate oversmoothing. The vast difference behind their design\nprinciples, combined with graph complications, make it difficult to understand\nand even compare their difference in tackling the oversmoothing. In this paper,\nwe propose ATNPA, a unified view with five key steps: Augmentation,\nTransformation, Normalization, Propagation, and Aggregation, to summarize GNN\noversmoothing alleviation approaches. We first outline three themes to tackle\noversmoothing, and then separate all methods into six categories, followed by\ndetailed reviews of representative methods, including their relation to the\nATNPA, and discussion about their niche, strength, and weakness. The review not\nonly draws in-depth understanding of existing methods in the field, but also\nshows a clear road map for future study.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.01663v1",
    "published_date": "2024-05-02 18:33:41 UTC",
    "updated_date": "2024-05-02 18:33:41 UTC"
  },
  {
    "arxiv_id": "2405.01660v1",
    "title": "Investigating Wit, Creativity, and Detectability of Large Language Models in Domain-Specific Writing Style Adaptation of Reddit's Showerthoughts",
    "authors": [
      "Tolga Buz",
      "Benjamin Frost",
      "Nikola Genchev",
      "Moritz Schneider",
      "Lucie-Aimée Kaffee",
      "Gerard de Melo"
    ],
    "abstract": "Recent Large Language Models (LLMs) have shown the ability to generate\ncontent that is difficult or impossible to distinguish from human writing. We\ninvestigate the ability of differently-sized LLMs to replicate human writing\nstyle in short, creative texts in the domain of Showerthoughts, thoughts that\nmay occur during mundane activities. We compare GPT-2 and GPT-Neo fine-tuned on\nReddit data as well as GPT-3.5 invoked in a zero-shot manner, against\nhuman-authored texts. We measure human preference on the texts across the\nspecific dimensions that account for the quality of creative, witty texts.\nAdditionally, we compare the ability of humans versus fine-tuned RoBERTa\nclassifiers to detect AI-generated texts. We conclude that human evaluators\nrate the generated texts slightly worse on average regarding their creative\nquality, but they are unable to reliably distinguish between human-written and\nAI-generated texts. We further provide a dataset for creative, witty text\ngeneration based on Reddit Showerthoughts posts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to *SEM 2024 (StarSEM) conference",
    "pdf_url": "http://arxiv.org/pdf/2405.01660v1",
    "published_date": "2024-05-02 18:29:58 UTC",
    "updated_date": "2024-05-02 18:29:58 UTC"
  },
  {
    "arxiv_id": "2405.02347v2",
    "title": "COPAL: Continual Pruning in Large Language Generative Models",
    "authors": [
      "Srikanth Malla",
      "Joon Hee Choi",
      "Chiho Choi"
    ],
    "abstract": "Adapting pre-trained large language models to different domains in natural\nlanguage processing requires two key considerations: high computational demands\nand model's inability to continual adaptation. To simultaneously address both\nissues, this paper presents COPAL (COntinual Pruning in Adaptive Language\nsettings), an algorithm developed for pruning large language generative models\nunder a continual model adaptation setting. While avoiding resource-heavy\nfinetuning or retraining, our pruning process is guided by the proposed\nsensitivity analysis. The sensitivity effectively measures model's ability to\nwithstand perturbations introduced by the new dataset and finds model's weights\nthat are relevant for all encountered datasets. As a result, COPAL allows\nseamless model adaptation to new domains while enhancing the resource\nefficiency. Our empirical evaluation on a various size of LLMs show that COPAL\noutperforms baseline models, demonstrating its efficacy in efficiency and\nadaptability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02347v2",
    "published_date": "2024-05-02 18:24:41 UTC",
    "updated_date": "2024-06-14 18:06:47 UTC"
  },
  {
    "arxiv_id": "2405.01534v1",
    "title": "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks",
    "authors": [
      "Murtaza Dalal",
      "Tarun Chiruvolu",
      "Devendra Chaplot",
      "Ruslan Salakhutdinov"
    ],
    "abstract": "Large Language Models (LLMs) have been shown to be capable of performing\nhigh-level planning for long-horizon robotics tasks, yet existing methods\nrequire access to a pre-defined skill library (e.g. picking, placing, pulling,\npushing, navigating). However, LLM planning does not address how to design or\nlearn those behaviors, which remains challenging particularly in long-horizon\nsettings. Furthermore, for many tasks of interest, the robot needs to be able\nto adjust its behavior in a fine-grained manner, requiring the agent to be\ncapable of modifying low-level control actions. Can we instead use the\ninternet-scale knowledge from LLMs for high-level policies, guiding\nreinforcement learning (RL) policies to efficiently solve robotic control tasks\nonline without requiring a pre-determined set of skills? In this paper, we\npropose Plan-Seq-Learn (PSL): a modular approach that uses motion planning to\nbridge the gap between abstract language and learned low-level control for\nsolving long-horizon robotics tasks from scratch. We demonstrate that PSL\nachieves state-of-the-art results on over 25 challenging robotics tasks with up\nto 10 stages. PSL solves long-horizon tasks from raw visual input spanning four\nbenchmarks at success rates of over 85%, out-performing language-based,\nclassical, and end-to-end approaches. Video results and code at\nhttps://mihdalal.github.io/planseqlearn/",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at ICLR 2024. Website at\n  https://mihdalal.github.io/planseqlearn/ 9 pages, 3 figures, 3 tables; 14\n  pages appendix (7 additional figures)",
    "pdf_url": "http://arxiv.org/pdf/2405.01534v1",
    "published_date": "2024-05-02 17:59:31 UTC",
    "updated_date": "2024-05-02 17:59:31 UTC"
  },
  {
    "arxiv_id": "2405.01531v2",
    "title": "Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models",
    "authors": [
      "Nishad Singhi",
      "Jae Myung Kim",
      "Karsten Roth",
      "Zeynep Akata"
    ],
    "abstract": "Concept Bottleneck Models (CBMs) ground image classification on\nhuman-understandable concepts to allow for interpretable model decisions.\nCrucially, the CBM design inherently allows for human interventions, in which\nexpert users are given the ability to modify potentially misaligned concept\nchoices to influence the decision behavior of the model in an interpretable\nfashion. However, existing approaches often require numerous human\ninterventions per image to achieve strong performances, posing practical\nchallenges in scenarios where obtaining human feedback is expensive. In this\npaper, we find that this is noticeably driven by an independent treatment of\nconcepts during intervention, wherein a change of one concept does not\ninfluence the use of other ones in the model's final decision. To address this\nissue, we introduce a trainable concept intervention realignment module, which\nleverages concept relations to realign concept assignments post-intervention.\nAcross standard, real-world benchmarks, we find that concept realignment can\nsignificantly improve intervention efficacy; significantly reducing the number\nof interventions needed to reach a target classification performance or concept\nprediction accuracy. In addition, it easily integrates into existing\nconcept-based architectures without requiring changes to the models themselves.\nThis reduced cost of human-model collaboration is crucial to enhancing the\nfeasibility of CBMs in resource-constrained environments. Our code is available\nat: https://github.com/ExplainableML/concept_realignment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01531v2",
    "published_date": "2024-05-02 17:59:01 UTC",
    "updated_date": "2024-08-05 18:20:39 UTC"
  },
  {
    "arxiv_id": "2405.01525v1",
    "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
    "authors": [
      "Sheng-Chieh Lin",
      "Luyu Gao",
      "Barlas Oguz",
      "Wenhan Xiong",
      "Jimmy Lin",
      "Wen-tau Yih",
      "Xilun Chen"
    ],
    "abstract": "Alignment is a standard procedure to fine-tune pre-trained large language\nmodels (LLMs) to follow natural language instructions and serve as helpful AI\nassistants. We have observed, however, that the conventional alignment process\nfails to enhance the factual accuracy of LLMs, and often leads to the\ngeneration of more false facts (i.e. hallucination). In this paper, we study\nhow to make the LLM alignment process more factual, by first identifying\nfactors that lead to hallucination in both alignment steps:\\ supervised\nfine-tuning (SFT) and reinforcement learning (RL). In particular, we find that\ntraining the LLM on new knowledge or unfamiliar texts can encourage\nhallucination. This makes SFT less factual as it trains on human labeled data\nthat may be novel to the LLM. Furthermore, reward functions used in standard RL\ncan also encourage hallucination, because it guides the LLM to provide more\nhelpful responses on a diverse set of instructions, often preferring longer and\nmore detailed responses. Based on these observations, we propose\nfactuality-aware alignment, comprised of factuality-aware SFT and\nfactuality-aware RL through direct preference optimization. Experiments show\nthat our proposed factuality-aware alignment guides LLMs to output more factual\nresponses while maintaining instruction-following capability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01525v1",
    "published_date": "2024-05-02 17:54:54 UTC",
    "updated_date": "2024-05-02 17:54:54 UTC"
  },
  {
    "arxiv_id": "2405.01524v3",
    "title": "A separability-based approach to quantifying generalization: which layer is best?",
    "authors": [
      "Luciano Dyballa",
      "Evan Gerritz",
      "Steven W. Zucker"
    ],
    "abstract": "Generalization to unseen data remains poorly understood for deep learning\nclassification and foundation models, especially in the open set scenario. How\ncan one assess the ability of networks to adapt to new or extended versions of\ntheir input space in the spirit of few-shot learning, out-of-distribution\ngeneralization, domain adaptation, and category discovery? Which layers of a\nnetwork are likely to generalize best? We provide a new method for evaluating\nthe capacity of networks to represent a sampled domain, regardless of whether\nthe network has been trained on all classes in that domain. Our approach is the\nfollowing: after fine-tuning state-of-the-art pre-trained models for visual\nclassification on a particular domain, we assess their performance on data from\nrelated but distinct variations in that domain. Generalization power is\nquantified as a function of the latent embeddings of unseen data from\nintermediate layers for both unsupervised and supervised settings. Working\nthroughout all stages of the network, we find that (i) high classification\naccuracy does not imply high generalizability; and (ii) deeper layers in a\nmodel do not always generalize the best, which has implications for pruning.\nSince the trends observed across datasets are largely consistent, we conclude\nthat our approach reveals (a function of) the intrinsic capacity of the\ndifferent layers of a model to generalize. Our code is available at\nhttps://github.com/dyballa/generalization",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "I.5.1; I.2.6; I.4.10"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01524v3",
    "published_date": "2024-05-02 17:54:35 UTC",
    "updated_date": "2024-11-02 12:03:44 UTC"
  },
  {
    "arxiv_id": "2405.01502v1",
    "title": "Analyzing the Role of Semantic Representations in the Era of Large Language Models",
    "authors": [
      "Zhijing Jin",
      "Yuen Chen",
      "Fernando Gonzalez",
      "Jiarui Liu",
      "Jiayi Zhang",
      "Julian Michael",
      "Bernhard Schölkopf",
      "Mona Diab"
    ],
    "abstract": "Traditionally, natural language processing (NLP) models often use a rich set\nof features created by linguistic expertise, such as semantic representations.\nHowever, in the era of large language models (LLMs), more and more tasks are\nturned into generic, end-to-end sequence generation problems. In this paper, we\ninvestigate the question: what is the role of semantic representations in the\nera of LLMs? Specifically, we investigate the effect of Abstract Meaning\nRepresentation (AMR) across five diverse NLP tasks. We propose an AMR-driven\nchain-of-thought prompting method, which we call AMRCoT, and find that it\ngenerally hurts performance more than it helps. To investigate what AMR may\nhave to offer on these tasks, we conduct a series of analysis experiments. We\nfind that it is difficult to predict which input examples AMR may help or hurt\non, but errors tend to arise with multi-word expressions, named entities, and\nin the final inference step where the LLM must connect its reasoning over the\nAMR to its prediction. We recommend focusing on these areas for future work in\nsemantic representations for LLMs. Our code:\nhttps://github.com/causalNLP/amr_llm.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01502v1",
    "published_date": "2024-05-02 17:32:59 UTC",
    "updated_date": "2024-05-02 17:32:59 UTC"
  },
  {
    "arxiv_id": "2405.01490v1",
    "title": "Controllable Text Generation in the Instruction-Tuning Era",
    "authors": [
      "Dhananjay Ashok",
      "Barnabas Poczos"
    ],
    "abstract": "While most research on controllable text generation has focused on steering\nbase Language Models, the emerging instruction-tuning and prompting paradigm\noffers an alternate approach to controllability. We compile and release\nConGenBench, a testbed of 17 different controllable generation tasks, using a\nsubset of it to benchmark the performance of 9 different baselines and methods\non Instruction-tuned Language Models. To our surprise, we find that\nprompting-based approaches outperform controllable text generation methods on\nmost datasets and tasks, highlighting a need for research on controllable text\ngeneration with Instruction-tuned Language Models in specific. Prompt-based\napproaches match human performance on most stylistic tasks while lagging on\nstructural tasks, foregrounding a need to study more varied constraints and\nmore challenging stylistic tasks. To facilitate such research, we provide an\nalgorithm that uses only a task dataset and a Large Language Model with\nin-context capabilities to automatically generate a constraint dataset. This\nmethod eliminates the fields dependence on pre-curated constraint datasets,\nhence vastly expanding the range of constraints that can be studied in the\nfuture.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01490v1",
    "published_date": "2024-05-02 17:24:30 UTC",
    "updated_date": "2024-05-02 17:24:30 UTC"
  },
  {
    "arxiv_id": "2405.01483v3",
    "title": "MANTIS: Interleaved Multi-Image Instruction Tuning",
    "authors": [
      "Dongfu Jiang",
      "Xuan He",
      "Huaye Zeng",
      "Cong Wei",
      "Max Ku",
      "Qian Liu",
      "Wenhu Chen"
    ],
    "abstract": "Large multimodal models (LMMs) have shown great results in single-image\nvision language tasks. However, their abilities to solve multi-image visual\nlanguage tasks is yet to be improved. The existing LMMs like OpenFlamingo,\nEmu2, and Idefics gain their multi-image ability through pre-training on\nhundreds of millions of noisy interleaved image-text data from the web, which\nis neither efficient nor effective. In this paper, we aim to build strong\nmulti-image LMMs via instruction tuning with academic-level resources.\nTherefore, we meticulously construct Mantis-Instruct containing 721K\nmulti-image instruction data to train a family of Mantis models. The\ninstruction tuning empowers Mantis with different multi-image skills like\nco-reference, comparison, reasoning, and temporal understanding. We evaluate\nMantis on 8 multi-image benchmarks and 6 single-image benchmarks.\nMantis-Idefics2 can achieve SoTA results on all the multi-image benchmarks and\nbeat the strongest multi-image baseline, Idefics2-8B by an average of 13\nabsolute points. Notably, Idefics2-8B was pre-trained on 140M interleaved\nmulti-image data, which is 200x larger than Mantis-Instruct. We observe that\nMantis performs equivalently well on the held-in and held-out benchmarks, which\nshows its generalization ability. We further evaluate Mantis on single-image\nbenchmarks and demonstrate that Mantis also maintains a strong single-image\nperformance on par with CogVLM and Emu2. Our results show that multi-image\nabilities are not necessarily gained through massive pre-training, instead,\nthey can be gained by low-cost instruction tuning. The training and evaluation\nof Mantis has paved the road for future work to improve LMMs' multi-image\nabilities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 3 figures, 13 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.01483v3",
    "published_date": "2024-05-02 17:14:57 UTC",
    "updated_date": "2024-11-15 06:31:44 UTC"
  },
  {
    "arxiv_id": "2405.01481v2",
    "title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment",
    "authors": [
      "Gerald Shen",
      "Zhilin Wang",
      "Olivier Delalleau",
      "Jiaqi Zeng",
      "Yi Dong",
      "Daniel Egert",
      "Shengyang Sun",
      "Jimmy Zhang",
      "Sahil Jain",
      "Ali Taghibakhshi",
      "Markel Sanz Ausin",
      "Ashwath Aithal",
      "Oleksii Kuchaiev"
    ],
    "abstract": "Aligning Large Language Models (LLMs) with human values and preferences is\nessential for making them helpful and safe. However, building efficient tools\nto perform alignment can be challenging, especially for the largest and most\ncompetent LLMs which often contain tens or hundreds of billions of parameters.\nWe create NeMo-Aligner, a toolkit for model alignment that can efficiently\nscale to a thousand GPUs for training the largest open-source LLMs such as\nNemotron 4 340B and Llama 3.1 405B. NeMo-Aligner comes with highly optimized\nand scalable implementations for major paradigms of model alignment such as:\nReinforcement Learning from Human Feedback (RLHF), Direct Preference\nOptimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally,\nour toolkit supports running most of the alignment techniques in a Parameter\nEfficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for\nextensibility, allowing support for other alignment techniques with minimal\neffort. It is open-sourced with Apache 2.0 License and we invite community\ncontributions at https://github.com/NVIDIA/NeMo-Aligner",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 4 figures, Accepted to COLM 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01481v2",
    "published_date": "2024-05-02 17:13:40 UTC",
    "updated_date": "2024-09-03 05:47:42 UTC"
  },
  {
    "arxiv_id": "2405.01474v3",
    "title": "Understanding Figurative Meaning through Explainable Visual Entailment",
    "authors": [
      "Arkadiy Saakyan",
      "Shreyas Kulkarni",
      "Tuhin Chakrabarty",
      "Smaranda Muresan"
    ],
    "abstract": "Large Vision-Language Models (VLMs) have demonstrated strong capabilities in\ntasks requiring a fine-grained understanding of literal meaning in images and\ntext, such as visual question-answering or visual entailment. However, there\nhas been little exploration of the capabilities of these models when presented\nwith images and captions containing figurative meaning, such as metaphors or\nhumor. To close this gap, we propose a new task framing the figurative meaning\nunderstanding problem as an explainable visual entailment task, where the model\nhas to predict whether the image (premise) entails a caption (hypothesis) and\njustify the predicted label with a textual explanation. The figurative\nphenomena can be present in the image, in the caption, or both. Using a\nhuman-AI collaboration approach, we build the accompanying expert-verified\ndataset V-FLUTE, containing 6,027 {image, caption, label, explanation}\ninstances spanning five diverse figurative phenomena: metaphors, similes,\nidioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs\nstruggle to generalize from literal to figurative meaning, particularly when it\nis present in images. Further, we identify common types of errors in VLM\nreasoning (hallucination and incomplete or unsound reasoning) across classes of\nmodels via human evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2405.01474v3",
    "published_date": "2024-05-02 17:07:25 UTC",
    "updated_date": "2025-02-17 17:24:42 UTC"
  },
  {
    "arxiv_id": "2405.01472v1",
    "title": "IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning",
    "authors": [
      "Ryan Hoque",
      "Ajay Mandlekar",
      "Caelan Garrett",
      "Ken Goldberg",
      "Dieter Fox"
    ],
    "abstract": "Imitation learning is a promising paradigm for training robot control\npolicies, but these policies can suffer from distribution shift, where the\nconditions at evaluation time differ from those in the training data. A popular\napproach for increasing policy robustness to distribution shift is interactive\nimitation learning (i.e., DAgger and variants), where a human operator provides\ncorrective interventions during policy rollouts. However, collecting a\nsufficient amount of interventions to cover the distribution of policy mistakes\ncan be burdensome for human operators. We propose IntervenGen (I-Gen), a novel\ndata generation system that can autonomously produce a large set of corrective\ninterventions with rich coverage of the state space from a small number of\nhuman interventions. We apply I-Gen to 4 simulated environments and 1 physical\nenvironment with object pose estimation error and show that it can increase\npolicy robustness by up to 39x with only 10 human interventions. Videos and\nmore results are available at https://sites.google.com/view/intervengen2024.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01472v1",
    "published_date": "2024-05-02 17:06:19 UTC",
    "updated_date": "2024-05-02 17:06:19 UTC"
  },
  {
    "arxiv_id": "2405.01469v1",
    "title": "Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning",
    "authors": [
      "Théo Moutakanni",
      "Piotr Bojanowski",
      "Guillaume Chassagnon",
      "Céline Hudelot",
      "Armand Joulin",
      "Yann LeCun",
      "Matthew Muckley",
      "Maxime Oquab",
      "Marie-Pierre Revel",
      "Maria Vakalopoulou"
    ],
    "abstract": "AI Foundation models are gaining traction in various applications, including\nmedical fields like radiology. However, medical foundation models are often\ntested on limited tasks, leaving their generalisability and biases unexplored.\nWe present RayDINO, a large visual encoder trained by self-supervision on 873k\nchest X-rays. We compare RayDINO to previous state-of-the-art models across\nnine radiology tasks, from classification and dense segmentation to text\ngeneration, and provide an in depth analysis of population, age and sex biases\nof our model. Our findings suggest that self-supervision allows patient-centric\nAI proving useful in clinical workflows and interpreting X-rays holistically.\nWith RayDINO and small task-specific adapters, we reach state-of-the-art\nresults and improve generalization to unseen populations while mitigating bias,\nillustrating the true promise of foundation models: versatility and robustness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01469v1",
    "published_date": "2024-05-02 16:59:10 UTC",
    "updated_date": "2024-05-02 16:59:10 UTC"
  },
  {
    "arxiv_id": "2405.01468v1",
    "title": "Understanding Retrieval-Augmented Task Adaptation for Vision-Language Models",
    "authors": [
      "Yifei Ming",
      "Yixuan Li"
    ],
    "abstract": "Pre-trained contrastive vision-language models have demonstrated remarkable\nperformance across a wide range of tasks. However, they often struggle on\nfine-trained datasets with categories not adequately represented during\npre-training, which makes adaptation necessary. Recent works have shown\npromising results by utilizing samples from web-scale databases for\nretrieval-augmented adaptation, especially in low-data regimes. Despite the\nempirical success, understanding how retrieval impacts the adaptation of\nvision-language models remains an open research question. In this work, we\nadopt a reflective perspective by presenting a systematic study to understand\nthe roles of key components in retrieval-augmented adaptation. We unveil new\ninsights on uni-modal and cross-modal retrieval and highlight the critical role\nof logit ensemble for effective adaptation. We further present theoretical\nunderpinnings that directly support our empirical observations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper is accepted at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01468v1",
    "published_date": "2024-05-02 16:59:05 UTC",
    "updated_date": "2024-05-02 16:59:05 UTC"
  },
  {
    "arxiv_id": "2405.01460v2",
    "title": "Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders",
    "authors": [
      "Yi Yu",
      "Yufei Wang",
      "Song Xia",
      "Wenhan Yang",
      "Shijian Lu",
      "Yap-Peng Tan",
      "Alex C. Kot"
    ],
    "abstract": "Unlearnable examples (UEs) seek to maximize testing error by making subtle\nmodifications to training examples that are correctly labeled. Defenses against\nthese poisoning attacks can be categorized based on whether specific\ninterventions are adopted during training. The first approach is training-time\ndefense, such as adversarial training, which can mitigate poisoning effects but\nis computationally intensive. The other approach is pre-training purification,\ne.g., image short squeezing, which consists of several simple compressions but\noften encounters challenges in dealing with various UEs. Our work provides a\nnovel disentanglement mechanism to build an efficient pre-training purification\nmethod. Firstly, we uncover rate-constrained variational autoencoders (VAEs),\ndemonstrating a clear tendency to suppress the perturbations in UEs. We\nsubsequently conduct a theoretical analysis for this phenomenon. Building upon\nthese insights, we introduce a disentangle variational autoencoder (D-VAE),\ncapable of disentangling the perturbations with learnable class-wise\nembeddings. Based on this network, a two-stage purification approach is\nnaturally developed. The first stage focuses on roughly eliminating\nperturbations, while the second stage produces refined, poison-free results,\nensuring effectiveness and robustness across various scenarios. Extensive\nexperiments demonstrate the remarkable performance of our method across\nCIFAR-10, CIFAR-100, and a 100-class ImageNet-subset. Code is available at\nhttps://github.com/yuyi-sd/D-VAE.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01460v2",
    "published_date": "2024-05-02 16:49:25 UTC",
    "updated_date": "2024-05-06 06:50:10 UTC"
  },
  {
    "arxiv_id": "2405.01458v2",
    "title": "UQA: Corpus for Urdu Question Answering",
    "authors": [
      "Samee Arif",
      "Sualeha Farid",
      "Awais Athar",
      "Agha Ali Raza"
    ],
    "abstract": "This paper introduces UQA, a novel dataset for question answering and text\ncomprehension in Urdu, a low-resource language with over 70 million native\nspeakers. UQA is generated by translating the Stanford Question Answering\nDataset (SQuAD2.0), a large-scale English QA dataset, using a technique called\nEATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans in\nthe translated context paragraphs. The paper describes the process of selecting\nand evaluating the best translation model among two candidates: Google\nTranslator and Seamless M4T. The paper also benchmarks several state-of-the-art\nmultilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, and\nreports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and\n74.56 EM. UQA is a valuable resource for developing and testing multilingual\nNLP systems for Urdu and for enhancing the cross-lingual transferability of\nexisting models. Further, the paper demonstrates the effectiveness of EATS for\ncreating high-quality datasets for other languages and domains. The UQA dataset\nand the code are publicly available at www.github.com/sameearif/UQA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01458v2",
    "published_date": "2024-05-02 16:44:31 UTC",
    "updated_date": "2024-07-22 18:46:11 UTC"
  },
  {
    "arxiv_id": "2405.01616v1",
    "title": "Generative Active Learning for the Search of Small-molecule Protein Binders",
    "authors": [
      "Maksym Korablyov",
      "Cheng-Hao Liu",
      "Moksh Jain",
      "Almer M. van der Sloot",
      "Eric Jolicoeur",
      "Edward Ruediger",
      "Andrei Cristian Nica",
      "Emmanuel Bengio",
      "Kostiantyn Lapchevskyi",
      "Daniel St-Cyr",
      "Doris Alexandra Schuetz",
      "Victor Ion Butoi",
      "Jarrid Rector-Brooks",
      "Simon Blackburn",
      "Leo Feng",
      "Hadi Nekoei",
      "SaiKrishna Gottipati",
      "Priyesh Vijayan",
      "Prateek Gupta",
      "Ladislav Rampášek",
      "Sasikanth Avancha",
      "Pierre-Luc Bacon",
      "William L. Hamilton",
      "Brooks Paige",
      "Sanchit Misra",
      "Stanislaw Kamil Jastrzebski",
      "Bharat Kaul",
      "Doina Precup",
      "José Miguel Hernández-Lobato",
      "Marwin Segler",
      "Michael Bronstein",
      "Anne Marinier",
      "Mike Tyers",
      "Yoshua Bengio"
    ],
    "abstract": "Despite substantial progress in machine learning for scientific discovery in\nrecent years, truly de novo design of small molecules which exhibit a property\nof interest remains a significant challenge. We introduce LambdaZero, a\ngenerative active learning approach to search for synthesizable molecules.\nPowered by deep reinforcement learning, LambdaZero learns to search over the\nvast space of molecules to discover candidates with a desired property. We\napply LambdaZero with molecular docking to design novel small molecules that\ninhibit the enzyme soluble Epoxide Hydrolase 2 (sEH), while enforcing\nconstraints on synthesizability and drug-likeliness. LambdaZero provides an\nexponential speedup in terms of the number of calls to the expensive molecular\ndocking oracle, and LambdaZero de novo designed molecules reach docking scores\nthat would otherwise require the virtual screening of a hundred billion\nmolecules. Importantly, LambdaZero discovers novel scaffolds of synthesizable,\ndrug-like inhibitors for sEH. In in vitro experimental validation, a series of\nligands from a generated quinazoline-based scaffold were synthesized, and the\nlead inhibitor N-(4,6-di(pyrrolidin-1-yl)quinazolin-2-yl)-N-methylbenzamide\n(UM0152893) displayed sub-micromolar enzyme inhibition of sEH.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01616v1",
    "published_date": "2024-05-02 16:39:21 UTC",
    "updated_date": "2024-05-02 16:39:21 UTC"
  },
  {
    "arxiv_id": "2405.01453v3",
    "title": "Creative Problem Solving in Large Language and Vision Models -- What Would it Take?",
    "authors": [
      "Lakshmi Nair",
      "Evana Gizzi",
      "Jivko Sinapov"
    ],
    "abstract": "We advocate for a strong integration of Computational Creativity (CC) with\nresearch in large language and vision models (LLVMs) to address a key\nlimitation of these models, i.e., creative problem solving. We present\npreliminary experiments showing how CC principles can be applied to address\nthis limitation. Our goal is to foster discussions on creative problem solving\nin LLVMs and CC at prestigious ML venues. Our code is available at:\nhttps://github.com/lnairGT/creative-problem-solving-LLMs",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2405.01453v3",
    "published_date": "2024-05-02 16:36:26 UTC",
    "updated_date": "2024-10-01 13:46:04 UTC"
  },
  {
    "arxiv_id": "2405.01614v3",
    "title": "RULSurv: A probabilistic survival-based method for early censoring-aware prediction of remaining useful life in ball bearings",
    "authors": [
      "Christian Marius Lillelund",
      "Fernando Pannullo",
      "Morten Opprud Jakobsen",
      "Manuel Morante",
      "Christian Fischer Pedersen"
    ],
    "abstract": "Predicting the remaining useful life (RUL) of ball bearings is an active area\nof research, where novel machine learning techniques are continuously being\napplied to predict degradation trends and anticipate failures before they\noccur. However, few studies have explicitly addressed the challenge of handling\ncensored data, where information about a specific event (\\eg mechanical\nfailure) is incomplete or only partially observed. To address this issue, we\nintroduce a novel and flexible method for early fault detection using\nKullback-Leibler (KL) divergence and RUL estimation using survival analysis\nthat naturally supports censored data. We demonstrate our approach in the\nXJTU-SY dataset using a 5-fold cross-validation strategy across three different\noperating conditions. When predicting the time to failure for bearings under\nthe highest load (C1, 12.0 kN and 2100 RPM) with 25% random censoring, our\napproach achieves a mean absolute error (MAE) of 14.7 minutes (95% CI =\n13.6-15.8) using a linear CoxPH model, and an MAE of 12.6 minutes (95% CI =\n11.8-13.4) using a nonlinear Random Survival Forests model, compared to an MAE\nof 18.5 minutes (95% CI = 17.4-19.6) using a linear LASSO model that does not\nsupport censoring. Moreover, our approach achieves a mean cumulative relative\naccuracy (CRA) of 0.7586 over 5 bearings under the highest load, which improves\nover several state-of-the-art baselines. Our work highlights the importance of\nconsidering censored data as part of the model design when building predictive\nmodels for early fault detection and RUL estimation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01614v3",
    "published_date": "2024-05-02 16:17:29 UTC",
    "updated_date": "2025-04-14 11:57:40 UTC"
  },
  {
    "arxiv_id": "2405.01419v3",
    "title": "Natural Language to Verilog: Design of a Recurrent Spiking Neural Network using Large Language Models and ChatGPT",
    "authors": [
      "Paola Vitolo",
      "George Psaltakis",
      "Michael Tomlinson",
      "Gian Domenico Licciardo",
      "Andreas G. Andreou"
    ],
    "abstract": "This paper investigates the use of Large Language Models (LLMs) and natural\nlanguage prompts to generate hardware description code, namely Verilog.\nBuilding on our prior work, we employ OpenAI's ChatGPT4 and natural language\nprompts to synthesize an RTL Verilog module of a programmable recurrent spiking\nneural network, while also generating test benches to assess the system's\ncorrectness. The resultant design was validated in three simple machine\nlearning tasks, the exclusive OR, the IRIS flower classification and the MNIST\nhand-written digit classification. Furthermore, the design was validated on a\nField-Programmable Gate Array (FPGA) and subsequently synthesized in the\nSkyWater 130 nm technology by using an open-source electronic design automation\nflow. The design was submitted to Efabless Tiny Tapeout 6.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "This paper was presented at the IEEE/ACM International Conference on\n  Neuromorphic Systems (ICONS), July 30-Aug 2, 2024, Arlington, VA, USA",
    "pdf_url": "http://arxiv.org/pdf/2405.01419v3",
    "published_date": "2024-05-02 16:08:08 UTC",
    "updated_date": "2024-10-22 18:31:22 UTC"
  },
  {
    "arxiv_id": "2405.03701v2",
    "title": "QxEAI: Quantum-like evolutionary algorithm for automated probabilistic forecasting",
    "authors": [
      "Kevin Xin",
      "Lizhi Xin"
    ],
    "abstract": "Forecasting, to estimate future events, is crucial for business and\ndecision-making. This paper proposes QxEAI, a methodology that produces a\nprobabilistic forecast that utilizes a quantum-like evolutionary algorithm\nbased on training a quantum-like logic decision tree and a classical value tree\non a small number of related time series. We demonstrate how the application of\nour quantum-like evolutionary algorithm to forecasting can overcome the\nchallenges faced by classical and other machine learning approaches. By using\nthree real-world datasets (Dow Jones Index, retail sales, gas consumption), we\nshow how our methodology produces accurate forecasts while requiring little to\nnone manual work.",
    "categories": [
      "physics.soc-ph",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.03701v2",
    "published_date": "2024-05-02 16:05:02 UTC",
    "updated_date": "2024-06-21 02:45:04 UTC"
  },
  {
    "arxiv_id": "2405.01413v1",
    "title": "MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors",
    "authors": [
      "Yuan Tang",
      "Xu Han",
      "Xianzhi Li",
      "Qiao Yu",
      "Yixue Hao",
      "Long Hu",
      "Min Chen"
    ],
    "abstract": "Large 2D vision-language models (2D-LLMs) have gained significant attention\nby bridging Large Language Models (LLMs) with images using a simple projector.\nInspired by their success, large 3D point cloud-language models (3D-LLMs) also\nintegrate point clouds into LLMs. However, directly aligning point clouds with\nLLM requires expensive training costs, typically in hundreds of GPU-hours on\nA100, which hinders the development of 3D-LLMs. In this paper, we introduce\nMiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTA\nresults while training for only 27 hours on one RTX 3090. Specifically, we\npropose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, which\ncan leverage the similarity between 2D and 3D visual information. We introduce\na novel four-stage training strategy for modality alignment in a cascaded way,\nand a mixture of query experts module to adaptively aggregate features with\nhigh efficiency. Moreover, we utilize parameter-efficient fine-tuning methods\nLoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, which\nis up to 260x fewer than existing methods. Extensive experiments show that\nMiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, with\nsignificantly cheaper training costs. Notably, MiniGPT-3D gains an 8.12\nincrease on GPT-4 evaluation score for the challenging object captioning task\ncompared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800.\nWe are the first to explore the efficient 3D-LLM, offering new insights to the\ncommunity. Code and weights are available at\nhttps://github.com/TangYuan96/MiniGPT-3D.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01413v1",
    "published_date": "2024-05-02 16:04:30 UTC",
    "updated_date": "2024-05-02 16:04:30 UTC"
  },
  {
    "arxiv_id": "2405.01409v3",
    "title": "Goal-conditioned reinforcement learning for ultrasound navigation guidance",
    "authors": [
      "Abdoul Aziz Amadou",
      "Vivek Singh",
      "Florin C. Ghesu",
      "Young-Ho Kim",
      "Laura Stanciulescu",
      "Harshitha P. Sai",
      "Puneet Sharma",
      "Alistair Young",
      "Ronak Rajani",
      "Kawal Rhode"
    ],
    "abstract": "Transesophageal echocardiography (TEE) plays a pivotal role in cardiology for\ndiagnostic and interventional procedures. However, using it effectively\nrequires extensive training due to the intricate nature of image acquisition\nand interpretation. To enhance the efficiency of novice sonographers and reduce\nvariability in scan acquisitions, we propose a novel ultrasound (US) navigation\nassistance method based on contrastive learning as goal-conditioned\nreinforcement learning (GCRL). We augment the previous framework using a novel\ncontrastive patient batching method (CPB) and a data-augmented contrastive\nloss, both of which we demonstrate are essential to ensure generalization to\nanatomical variations across patients. The proposed framework enables\nnavigation to both standard diagnostic as well as intricate interventional\nviews with a single model. Our method was developed with a large dataset of 789\npatients and obtained an average error of 6.56 mm in position and 9.36 degrees\nin angle on a testing dataset of 140 patients, which is competitive or superior\nto models trained on individual views. Furthermore, we quantitatively validate\nour method's ability to navigate to interventional views such as the Left\nAtrial Appendage (LAA) view used in LAA closure. Our approach holds promise in\nproviding valuable guidance during transesophageal ultrasound examinations,\ncontributing to the advancement of skill acquisition for cardiac ultrasound\npractitioners.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.0; I.5.0"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in MICCAI 2024; 11 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01409v3",
    "published_date": "2024-05-02 16:01:58 UTC",
    "updated_date": "2024-08-01 08:58:40 UTC"
  },
  {
    "arxiv_id": "2405.01403v1",
    "title": "Unsupervised Flow Discovery from Task-oriented Dialogues",
    "authors": [
      "Patrícia Ferreira",
      "Daniel Martins",
      "Ana Alves",
      "Catarina Silva",
      "Hugo Gonçalo Oliveira"
    ],
    "abstract": "The design of dialogue flows is a critical but time-consuming task when\ndeveloping task-oriented dialogue (TOD) systems. We propose an approach for the\nunsupervised discovery of flows from dialogue history, thus making the process\napplicable to any domain for which such an history is available. Briefly,\nutterances are represented in a vector space and clustered according to their\nsemantic similarity. Clusters, which can be seen as dialogue states, are then\nused as the vertices of a transition graph for representing the flows visually.\nWe present concrete examples of flows, discovered from MultiWOZ, a public TOD\ndataset. We further elaborate on their significance and relevance for the\nunderlying conversations and introduce an automatic validation metric for their\nassessment. Experimental results demonstrate the potential of the proposed\napproach for extracting meaningful flows from task-oriented conversations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01403v1",
    "published_date": "2024-05-02 15:54:36 UTC",
    "updated_date": "2024-05-02 15:54:36 UTC"
  },
  {
    "arxiv_id": "2405.01402v2",
    "title": "Learning Force Control for Legged Manipulation",
    "authors": [
      "Tifanny Portela",
      "Gabriel B. Margolis",
      "Yandong Ji",
      "Pulkit Agrawal"
    ],
    "abstract": "Controlling contact forces during interactions is critical for locomotion and\nmanipulation tasks. While sim-to-real reinforcement learning (RL) has succeeded\nin many contact-rich problems, current RL methods achieve forceful interactions\nimplicitly without explicitly regulating forces. We propose a method for\ntraining RL policies for direct force control without requiring access to force\nsensing. We showcase our method on a whole-body control platform of a quadruped\nrobot with an arm. Such force control enables us to perform gravity\ncompensation and impedance control, unlocking compliant whole-body\nmanipulation. The learned whole-body controller with variable compliance makes\nit intuitive for humans to teleoperate the robot by only commanding the\nmanipulator, and the robot's body adjusts automatically to achieve the desired\nposition and force. Consequently, a human teleoperator can easily demonstrate a\nwide variety of loco-manipulation tasks. To the best of our knowledge, we\nprovide the first deployment of learned whole-body force control in legged\nmanipulators, paving the way for more versatile and adaptable legged robots.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "This work has been accepted to ICRA24, as well as the\n  Loco-manipulation workshop at ICRA24",
    "pdf_url": "http://arxiv.org/pdf/2405.01402v2",
    "published_date": "2024-05-02 15:53:43 UTC",
    "updated_date": "2024-05-20 12:15:56 UTC"
  },
  {
    "arxiv_id": "2405.02345v1",
    "title": "Exploring the Capabilities of Large Language Models for Generating Diverse Design Solutions",
    "authors": [
      "Kevin Ma",
      "Daniele Grandi",
      "Christopher McComb",
      "Kosa Goucher-Lambert"
    ],
    "abstract": "Access to large amounts of diverse design solutions can support designers\nduring the early stage of the design process. In this paper, we explore the\nefficacy of large language models (LLM) in producing diverse design solutions,\ninvestigating the level of impact that parameter tuning and various prompt\nengineering techniques can have on the diversity of LLM-generated design\nsolutions. Specifically, LLMs are used to generate a total of 4,000 design\nsolutions across five distinct design topics, eight combinations of parameters,\nand eight different types of prompt engineering techniques, comparing each\ncombination of parameter and prompt engineering method across four different\ndiversity metrics. LLM-generated solutions are compared against 100\nhuman-crowdsourced solutions in each design topic using the same set of\ndiversity metrics. Results indicate that human-generated solutions consistently\nhave greater diversity scores across all design topics. Using a post hoc\nlogistic regression analysis we investigate whether these differences primarily\nexist at the semantic level. Results show that there is a divide in some design\ntopics between humans and LLM-generated solutions, while others have no clear\ndivide. Taken together, these results contribute to the understanding of LLMs'\ncapabilities in generating a large volume of diverse design solutions and offer\ninsights for future research that leverages LLMs to generate diverse design\nsolutions for a broad range of design tasks (e.g., inspirational stimuli).",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "preprint of journal paper",
    "pdf_url": "http://arxiv.org/pdf/2405.02345v1",
    "published_date": "2024-05-02 14:20:04 UTC",
    "updated_date": "2024-05-02 14:20:04 UTC"
  },
  {
    "arxiv_id": "2405.01305v3",
    "title": "Distributed Representations Enable Robust Multi-Timescale Symbolic Computation in Neuromorphic Hardware",
    "authors": [
      "Madison Cotteret",
      "Hugh Greatorex",
      "Alpha Renner",
      "Junren Chen",
      "Emre Neftci",
      "Huaqiang Wu",
      "Giacomo Indiveri",
      "Martin Ziegler",
      "Elisabetta Chicca"
    ],
    "abstract": "Programming recurrent spiking neural networks (RSNNs) to robustly perform\nmulti-timescale computation remains a difficult challenge. To address this, we\ndescribe a single-shot weight learning scheme to embed robust multi-timescale\ndynamics into attractor-based RSNNs, by exploiting the properties of\nhigh-dimensional distributed representations. We embed finite state machines\ninto the RSNN dynamics by superimposing a symmetric autoassociative weight\nmatrix and asymmetric transition terms, which are each formed by the vector\nbinding of an input and heteroassociative outer-products between states. Our\napproach is validated through simulations with highly nonideal weights; an\nexperimental closed-loop memristive hardware setup; and on Loihi 2, where it\nscales seamlessly to large state machines. This work introduces a scalable\napproach to embed robust symbolic computation through recurrent dynamics into\nneuromorphic hardware, without requiring parameter fine-tuning or significant\nplatform-specific optimisation. Moreover, it demonstrates that distributed\nsymbolic representations serve as a highly capable representation-invariant\nlanguage for cognitive algorithms in neuromorphic hardware.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "19 pages, 7 figures. Supplementary material: 13 pages, 8 figures.\n  Accepted for publication in Neuromorphic Computing and Engineering",
    "pdf_url": "http://arxiv.org/pdf/2405.01305v3",
    "published_date": "2024-05-02 14:11:50 UTC",
    "updated_date": "2025-01-13 14:58:22 UTC"
  },
  {
    "arxiv_id": "2405.01299v1",
    "title": "The Effectiveness of LLMs as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation",
    "authors": [
      "Maja Pavlovic",
      "Massimo Poesio"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as powerful support tools across\nvarious natural language tasks and a range of application domains. Recent\nstudies focus on exploring their capabilities for data annotation. This paper\nprovides a comparative overview of twelve studies investigating the potential\nof LLMs in labelling data. While the models demonstrate promising cost and\ntime-saving benefits, there exist considerable limitations, such as\nrepresentativeness, bias, sensitivity to prompt variations and English language\npreference. Leveraging insights from these studies, our empirical analysis\nfurther examines the alignment between human and GPT-generated opinion\ndistributions across four subjective datasets. In contrast to the studies\nexamining representation, our methodology directly obtains the opinion\ndistribution from GPT. Our analysis thereby supports the minority of studies\nthat are considering diverse perspectives when evaluating data annotation tasks\nand highlights the need for further research in this direction.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "LREC-COLING NLPerspectives workshop",
    "pdf_url": "http://arxiv.org/pdf/2405.01299v1",
    "published_date": "2024-05-02 14:00:22 UTC",
    "updated_date": "2024-05-02 14:00:22 UTC"
  },
  {
    "arxiv_id": "2405.01293v1",
    "title": "Low-resource speech recognition and dialect identification of Irish in a multi-task framework",
    "authors": [
      "Liam Lonergan",
      "Mengjie Qian",
      "Neasa Ní Chiaráin",
      "Christer Gobl",
      "Ailbhe Ní Chasaide"
    ],
    "abstract": "This paper explores the use of Hybrid CTC/Attention encoder-decoder models\ntrained with Intermediate CTC (InterCTC) for Irish (Gaelic) low-resource speech\nrecognition (ASR) and dialect identification (DID). Results are compared to the\ncurrent best performing models trained for ASR (TDNN-HMM) and DID (ECAPA-TDNN).\nAn optimal InterCTC setting is initially established using a Conformer encoder.\nThis setting is then used to train a model with an E-branchformer encoder and\nthe performance of both architectures are compared. A multi-task fine-tuning\napproach is adopted for language model (LM) shallow fusion. The experiments\nyielded an improvement in DID accuracy of 10.8% relative to a baseline\nECAPA-TDNN, and WER performance approaching the TDNN-HMM model. This multi-task\napproach emerges as a promising strategy for Irish low-resource ASR and DID.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages. Accepted to Odyssey 2024 - The Speaker and Language\n  Recognition Workshop",
    "pdf_url": "http://arxiv.org/pdf/2405.01293v1",
    "published_date": "2024-05-02 13:54:39 UTC",
    "updated_date": "2024-05-02 13:54:39 UTC"
  },
  {
    "arxiv_id": "2405.02344v1",
    "title": "Backdoor-based Explainable AI Benchmark for High Fidelity Evaluation of Attribution Methods",
    "authors": [
      "Peiyu Yang",
      "Naveed Akhtar",
      "Jiantong Jiang",
      "Ajmal Mian"
    ],
    "abstract": "Attribution methods compute importance scores for input features to explain\nthe output predictions of deep models. However, accurate assessment of\nattribution methods is challenged by the lack of benchmark fidelity for\nattributing model predictions. Moreover, other confounding factors in\nattribution estimation, including the setup choices of post-processing\ntechniques and explained model predictions, further compromise the reliability\nof the evaluation. In this work, we first identify a set of fidelity criteria\nthat reliable benchmarks for attribution methods are expected to fulfill,\nthereby facilitating a systematic assessment of attribution benchmarks. Next,\nwe introduce a Backdoor-based eXplainable AI benchmark (BackX) that adheres to\nthe desired fidelity criteria. We theoretically establish the superiority of\nour approach over the existing benchmarks for well-founded attribution\nevaluation. With extensive analysis, we also identify a setup for a consistent\nand fair benchmarking of attribution methods across different underlying\nmethodologies. This setup is ultimately employed for a comprehensive comparison\nof existing methods using our BackX benchmark. Finally, our analysis also\nprovides guidance for defending against backdoor attacks with the help of\nattribution methods.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02344v1",
    "published_date": "2024-05-02 13:48:37 UTC",
    "updated_date": "2024-05-02 13:48:37 UTC"
  },
  {
    "arxiv_id": "2405.01286v1",
    "title": "Data Feminism for AI",
    "authors": [
      "Lauren Klein",
      "Catherine D'Ignazio"
    ],
    "abstract": "This paper presents a set of intersectional feminist principles for\nconducting equitable, ethical, and sustainable AI research. In Data Feminism\n(2020), we offered seven principles for examining and challenging unequal power\nin data science. Here, we present a rationale for why feminism remains deeply\nrelevant for AI research, rearticulate the original principles of data feminism\nwith respect to AI, and introduce two potential new principles related to\nenvironmental impact and consent. Together, these principles help to 1) account\nfor the unequal, undemocratic, extractive, and exclusionary forces at work in\nAI research, development, and deployment; 2) identify and mitigate predictable\nharms in advance of unsafe, discriminatory, or otherwise oppressive systems\nbeing released into the world; and 3) inspire creative, joyful, and collective\nways to work towards a more equitable, sustainable world in which all of us can\nthrive.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0; K.4.0"
    ],
    "primary_category": "cs.CY",
    "comment": "21 pages, to be published in the 2024 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT '24)",
    "pdf_url": "http://arxiv.org/pdf/2405.01286v1",
    "published_date": "2024-05-02 13:46:29 UTC",
    "updated_date": "2024-05-02 13:46:29 UTC"
  },
  {
    "arxiv_id": "2405.01273v2",
    "title": "Towards Inclusive Face Recognition Through Synthetic Ethnicity Alteration",
    "authors": [
      "Praveen Kumar Chandaliya",
      "Kiran Raja",
      "Raghavendra Ramachandra",
      "Zahid Akhtar",
      "Christoph Busch"
    ],
    "abstract": "Numerous studies have shown that existing Face Recognition Systems (FRS),\nincluding commercial ones, often exhibit biases toward certain ethnicities due\nto under-represented data. In this work, we explore ethnicity alteration and\nskin tone modification using synthetic face image generation methods to\nincrease the diversity of datasets. We conduct a detailed analysis by first\nconstructing a balanced face image dataset representing three ethnicities:\nAsian, Black, and Indian. We then make use of existing Generative Adversarial\nNetwork-based (GAN) image-to-image translation and manifold learning models to\nalter the ethnicity from one to another. A systematic analysis is further\nconducted to assess the suitability of such datasets for FRS by studying the\nrealistic skin-tone representation using Individual Typology Angle (ITA).\nFurther, we also analyze the quality characteristics using existing Face image\nquality assessment (FIQA) approaches. We then provide a holistic FRS\nperformance analysis using four different systems. Our findings pave the way\nfor future research works in (i) developing both specific ethnicity and general\n(any to any) ethnicity alteration models, (ii) expanding such approaches to\ncreate databases with diverse skin tones, (iii) creating datasets representing\nvarious ethnicities which further can help in mitigating bias while addressing\nprivacy concerns.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 Pages",
    "pdf_url": "http://arxiv.org/pdf/2405.01273v2",
    "published_date": "2024-05-02 13:31:09 UTC",
    "updated_date": "2024-05-07 03:31:22 UTC"
  },
  {
    "arxiv_id": "2405.01611v1",
    "title": "Unifying and extending Precision Recall metrics for assessing generative models",
    "authors": [
      "Benjamin Sykes",
      "Loic Simon",
      "Julien Rabin"
    ],
    "abstract": "With the recent success of generative models in image and text, the\nevaluation of generative models has gained a lot of attention. Whereas most\ngenerative models are compared in terms of scalar values such as Frechet\nInception Distance (FID) or Inception Score (IS), in the last years (Sajjadi et\nal., 2018) proposed a definition of precision-recall curve to characterize the\ncloseness of two distributions. Since then, various approaches to precision and\nrecall have seen the light (Kynkaanniemi et al., 2019; Naeem et al., 2020; Park\n& Kim, 2023). They center their attention on the extreme values of precision\nand recall, but apart from this fact, their ties are elusive. In this paper, we\nunify most of these approaches under the same umbrella, relying on the work of\n(Simon et al., 2019). Doing so, we were able not only to recover entire curves,\nbut also to expose the sources of the accounted pitfalls of the concerned\nmetrics. We also provide consistency results that go well beyond the ones\npresented in the corresponding literature. Last, we study the different\nbehaviors of the curves obtained experimentally.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01611v1",
    "published_date": "2024-05-02 13:19:21 UTC",
    "updated_date": "2024-05-02 13:19:21 UTC"
  },
  {
    "arxiv_id": "2405.01266v1",
    "title": "MFTraj: Map-Free, Behavior-Driven Trajectory Prediction for Autonomous Driving",
    "authors": [
      "Haicheng Liao",
      "Zhenning Li",
      "Chengyue Wang",
      "Huanming Shen",
      "Bonan Wang",
      "Dongping Liao",
      "Guofa Li",
      "Chengzhong Xu"
    ],
    "abstract": "This paper introduces a trajectory prediction model tailored for autonomous\ndriving, focusing on capturing complex interactions in dynamic traffic\nscenarios without reliance on high-definition maps. The model, termed MFTraj,\nharnesses historical trajectory data combined with a novel dynamic geometric\ngraph-based behavior-aware module. At its core, an adaptive structure-aware\ninteractive graph convolutional network captures both positional and behavioral\nfeatures of road users, preserving spatial-temporal intricacies. Enhanced by a\nlinear attention mechanism, the model achieves computational efficiency and\nreduced parameter overhead. Evaluations on the Argoverse, NGSIM, HighD, and\nMoCAD datasets underscore MFTraj's robustness and adaptability, outperforming\nnumerous benchmarks even in data-challenged scenarios without the need for\nadditional information such as HD maps or vectorized maps. Importantly, it\nmaintains competitive performance even in scenarios with substantial missing\ndata, on par with most existing state-of-the-art models. The results and\nmethodology suggest a significant advancement in autonomous driving trajectory\nprediction, paving the way for safer and more efficient autonomous systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01266v1",
    "published_date": "2024-05-02 13:13:52 UTC",
    "updated_date": "2024-05-02 13:13:52 UTC"
  },
  {
    "arxiv_id": "2405.01259v1",
    "title": "Identification of Entailment and Contradiction Relations between Natural Language Sentences: A Neurosymbolic Approach",
    "authors": [
      "Xuyao Feng",
      "Anthony Hunter"
    ],
    "abstract": "Natural language inference (NLI), also known as Recognizing Textual\nEntailment (RTE), is an important aspect of natural language understanding.\nMost research now uses machine learning and deep learning to perform this task\non specific datasets, meaning their solution is not explainable nor explicit.\nTo address the need for an explainable approach to RTE, we propose a novel\npipeline that is based on translating text into an Abstract Meaning\nRepresentation (AMR) graph. For this we use a pre-trained AMR parser. We then\ntranslate the AMR graph into propositional logic and use a SAT solver for\nautomated reasoning. In text, often commonsense suggests that an entailment (or\ncontradiction) relationship holds between a premise and a claim, but because\ndifferent wordings are used, this is not identified from their logical\nrepresentations. To address this, we introduce relaxation methods to allow\nreplacement or forgetting of some propositions. Our experimental results show\nthis pipeline performs well on four RTE datasets.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01259v1",
    "published_date": "2024-05-02 13:06:24 UTC",
    "updated_date": "2024-05-02 13:06:24 UTC"
  },
  {
    "arxiv_id": "2405.01242v3",
    "title": "TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms",
    "authors": [
      "Yueyuan Sui",
      "Minghui Zhao",
      "Junxi Xia",
      "Xiaofan Jiang",
      "Stephen Xia"
    ],
    "abstract": "We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic\nand bone conduction speech enhancement, suitable for mobile and wearable\nplatforms. Bone conduction speech enhancement has been impractical to adopt in\nmobile and wearable platforms for several reasons: (i) data collection is\nlabor-intensive, resulting in scarcity; (ii) there exists a performance gap\nbetween state of-art models with memory footprints of hundreds of MBs and\nmethods better suited for resource-constrained systems. To adapt TRAMBA to\nvibration-based sensing modalities, we pre-train TRAMBA with audio speech\ndatasets that are widely available. Then, users fine-tune with a small amount\nof bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in\nPESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and\nan inference speed up of up to 465 times. We integrate TRAMBA into real systems\nand show that TRAMBA (i) improves battery life of wearables by up to 160% by\nrequiring less data sampling and transmission; (ii) generates higher quality\nvoice in noisy environments than over-the-air speech; (iii) requires a memory\nfootprint of less than 20.0 MB.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01242v3",
    "published_date": "2024-05-02 12:45:48 UTC",
    "updated_date": "2024-05-29 15:46:57 UTC"
  },
  {
    "arxiv_id": "2405.01229v2",
    "title": "Boosting Jailbreak Attack with Momentum",
    "authors": [
      "Yihao Zhang",
      "Zeming Wei"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, yet they remain vulnerable to adversarial attacks, notably the\nwell-known jailbreak attack. In particular, the Greedy Coordinate Gradient\n(GCG) attack has demonstrated efficacy in exploiting this vulnerability by\noptimizing adversarial prompts through a combination of gradient heuristics and\ngreedy search. However, the efficiency of this attack has become a bottleneck\nin the attacking process. To mitigate this limitation, in this paper we rethink\nthe generation of the adversarial prompts through an optimization lens, aiming\nto stabilize the optimization process and harness more heuristic insights from\nprevious optimization iterations. Specifically, we propose the\n\\textbf{M}omentum \\textbf{A}ccelerated G\\textbf{C}G (\\textbf{MAC}) attack,\nwhich integrates a momentum term into the gradient heuristic to boost and\nstabilize the random search for tokens in adversarial prompts. Experimental\nresults showcase the notable enhancement achieved by MAC over baselines in\nterms of attack success rate and optimization efficiency. Moreover, we\ndemonstrate that MAC can still exhibit superior performance for transfer\nattacks and models under defense mechanisms. Our code is available at\nhttps://github.com/weizeming/momentum-attack-llm.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2405.01229v2",
    "published_date": "2024-05-02 12:18:14 UTC",
    "updated_date": "2025-03-02 12:27:07 UTC"
  },
  {
    "arxiv_id": "2405.01216v1",
    "title": "DMON: A Simple yet Effective Approach for Argument Structure Learning",
    "authors": [
      "Wei Sun",
      "Mingxiao Li",
      "Jingyuan Sun",
      "Jesse Davis",
      "Marie-Francine Moens"
    ],
    "abstract": "Argument structure learning~(ASL) entails predicting relations between\narguments. Because it can structure a document to facilitate its understanding,\nit has been widely applied in many fields~(medical, commercial, and scientific\ndomains). Despite its broad utilization, ASL remains a challenging task because\nit involves examining the complex relationships between the sentences in a\npotentially unstructured discourse. To resolve this problem, we have developed\na simple yet effective approach called Dual-tower Multi-scale cOnvolution\nneural Network~(DMON) for the ASL task. Specifically, we organize arguments\ninto a relationship matrix that together with the argument embeddings forms a\nrelationship tensor and design a mechanism to capture relations with contextual\narguments. Experimental results on three different-domain argument mining\ndatasets demonstrate that our framework outperforms state-of-the-art models.\nThe code is available at https://github.com/VRCMF/DMON.git .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01216v1",
    "published_date": "2024-05-02 11:56:16 UTC",
    "updated_date": "2024-05-02 11:56:16 UTC"
  },
  {
    "arxiv_id": "2405.01204v1",
    "title": "Towards Cross-Scale Attention and Surface Supervision for Fractured Bone Segmentation in CT",
    "authors": [
      "Yu Zhou",
      "Xiahao Zou",
      "Yi Wang"
    ],
    "abstract": "Bone segmentation is an essential step for the preoperative planning of\nfracture trauma surgery. The automated segmentation of fractured bone from\ncomputed tomography (CT) scans remains challenging, due to the large\ndifferences of fractures in position and morphology, and also the inherent\nanatomical characteristics of different bone structures. To alleviate these\nissues, we propose a cross-scale attention mechanism as well as a surface\nsupervision strategy for fractured bone segmentation in CT. Specifically, a\ncross-scale attention mechanism is introduced to effectively aggregate the\nfeatures among different scales to provide more powerful fracture\nrepresentation. Moreover, a surface supervision strategy is employed, which\nexplicitly constrains the network to pay more attention to the bone boundary.\nThe efficacy of the proposed method is evaluated on a public dataset containing\nCT scans with hip fractures. The evaluation metrics are Dice similarity\ncoefficient (DSC), average symmetric surface distance (ASSD), and Hausdorff\ndistance (95HD). The proposed method achieves an average DSC of 93.36%, ASSD of\n0.85mm, 95HD of 7.51mm. Our method offers an effective fracture segmentation\napproach for the pelvic CT examinations, and has the potential to be used for\nimproving the segmentation performance of other types of fractures.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01204v1",
    "published_date": "2024-05-02 11:46:12 UTC",
    "updated_date": "2024-05-02 11:46:12 UTC"
  },
  {
    "arxiv_id": "2405.01198v1",
    "title": "Towards Interpretable Reinforcement Learning with Constrained Normalizing Flow Policies",
    "authors": [
      "Finn Rietz",
      "Erik Schaffernicht",
      "Stefan Heinrich",
      "Johannes A. Stork"
    ],
    "abstract": "Reinforcement learning policies are typically represented by black-box neural\nnetworks, which are non-interpretable and not well-suited for safety-critical\ndomains. To address both of these issues, we propose constrained normalizing\nflow policies as interpretable and safe-by-construction policy models. We\nachieve safety for reinforcement learning problems with instantaneous safety\nconstraints, for which we can exploit domain knowledge by analytically\nconstructing a normalizing flow that ensures constraint satisfaction. The\nnormalizing flow corresponds to an interpretable sequence of transformations on\naction samples, each ensuring alignment with respect to a particular\nconstraint. Our experiments reveal benefits beyond interpretability in an\neasier learning objective and maintained constraint satisfaction throughout the\nentire learning process. Our approach leverages constraints over reward\nengineering while offering enhanced interpretability, safety, and direct means\nof providing domain knowledge to the agent without relying on complex reward\nfunctions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01198v1",
    "published_date": "2024-05-02 11:40:15 UTC",
    "updated_date": "2024-05-02 11:40:15 UTC"
  },
  {
    "arxiv_id": "2405.01189v1",
    "title": "Gradient-Congruity Guided Federated Sparse Training",
    "authors": [
      "Chris Xing Tian",
      "Yibing Liu",
      "Haoliang Li",
      "Ray C. C. Cheung",
      "Shiqi Wang"
    ],
    "abstract": "Edge computing allows artificial intelligence and machine learning models to\nbe deployed on edge devices, where they can learn from local data and\ncollaborate to form a global model. Federated learning (FL) is a distributed\nmachine learning technique that facilitates this process while preserving data\nprivacy. However, FL also faces challenges such as high computational and\ncommunication costs regarding resource-constrained devices, and poor\ngeneralization performance due to the heterogeneity of data across edge clients\nand the presence of out-of-distribution data. In this paper, we propose the\nGradient-Congruity Guided Federated Sparse Training (FedSGC), a novel method\nthat integrates dynamic sparse training and gradient congruity inspection into\nfederated learning framework to address these issues. Our method leverages the\nidea that the neurons, in which the associated gradients with conflicting\ndirections with respect to the global model contain irrelevant or less\ngeneralized information for other clients, and could be pruned during the\nsparse training process. Conversely, the neurons where the associated gradients\nwith consistent directions could be grown in a higher priority. In this way,\nFedSGC can greatly reduce the local computation and communication overheads\nwhile, at the same time, enhancing the generalization abilities of FL. We\nevaluate our method on challenging non-i.i.d settings and show that it achieves\ncompetitive accuracy with state-of-the-art FL methods across various scenarios\nwhile minimizing computation and communication costs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01189v1",
    "published_date": "2024-05-02 11:29:48 UTC",
    "updated_date": "2024-05-02 11:29:48 UTC"
  },
  {
    "arxiv_id": "2405.01186v1",
    "title": "Potential Energy based Mixture Model for Noisy Label Learning",
    "authors": [
      "Zijia Wang",
      "Wenbin Yang",
      "Zhisong Liu",
      "Zhen Jia"
    ],
    "abstract": "Training deep neural networks (DNNs) from noisy labels is an important and\nchallenging task. However, most existing approaches focus on the corrupted\nlabels and ignore the importance of inherent data structure. To bridge the gap\nbetween noisy labels and data, inspired by the concept of potential energy in\nphysics, we propose a novel Potential Energy based Mixture Model (PEMM) for\nnoise-labels learning. We innovate a distance-based classifier with the\npotential energy regularization on its class centers. Embedding our proposed\nclassifier with existing deep learning backbones, we can have robust networks\nwith better feature representations. They can preserve intrinsic structures\nfrom the data, resulting in a superior noisy tolerance. We conducted extensive\nexperiments to analyze the efficiency of our proposed model on several\nreal-world datasets. Quantitative results show that it can achieve\nstate-of-the-art performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "36th Conference on Neural Information Processing Systems (NeurIPS\n  2022)",
    "pdf_url": "http://arxiv.org/pdf/2405.01186v1",
    "published_date": "2024-05-02 11:19:57 UTC",
    "updated_date": "2024-05-02 11:19:57 UTC"
  },
  {
    "arxiv_id": "2405.01175v1",
    "title": "Uncertainty-aware self-training with expectation maximization basis transformation",
    "authors": [
      "Zijia Wang",
      "Wenbin Yang",
      "Zhisong Liu",
      "Zhen Jia"
    ],
    "abstract": "Self-training is a powerful approach to deep learning. The key process is to\nfind a pseudo-label for modeling. However, previous self-training algorithms\nsuffer from the over-confidence issue brought by the hard labels, even some\nconfidence-related regularizers cannot comprehensively catch the uncertainty.\nTherefore, we propose a new self-training framework to combine uncertainty\ninformation of both model and dataset. Specifically, we propose to use\nExpectation-Maximization (EM) to smooth the labels and comprehensively estimate\nthe uncertainty information. We further design a basis extraction network to\nestimate the initial basis from the dataset. The obtained basis with\nuncertainty can be filtered based on uncertainty information. It can then be\ntransformed into the real hard label to iteratively update the model and basis\nin the retraining process. Experiments on image classification and semantic\nsegmentation show the advantages of our methods among confidence-aware\nself-training algorithms with 1-3 percentage improvement on different datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01175v1",
    "published_date": "2024-05-02 11:01:31 UTC",
    "updated_date": "2024-05-02 11:01:31 UTC"
  },
  {
    "arxiv_id": "2405.01158v1",
    "title": "Interpretable Data-driven Anomaly Detection in Industrial Processes with ExIFFI",
    "authors": [
      "Davide Frizzo",
      "Francesco Borsatti",
      "Alessio Arcudi",
      "Antonio De Moliner",
      "Roberto Oboe",
      "Gian Antonio Susto"
    ],
    "abstract": "Anomaly detection (AD) is a crucial process often required in industrial\nsettings. Anomalies can signal underlying issues within a system, prompting\nfurther investigation. Industrial processes aim to streamline operations as\nmuch as possible, encompassing the production of the final product, making AD\nan essential mean to reach this goal.Conventional anomaly detection\nmethodologies typically classify observations as either normal or anomalous\nwithout providing insight into the reasons behind these\nclassifications.Consequently, in light of the emergence of Industry 5.0, a more\ndesirable approach involves providing interpretable outcomes, enabling users to\nunderstand the rationale behind the results.This paper presents the first\nindustrial application of ExIFFI, a recently developed approach focused on the\nproduction of fast and efficient explanations for the Extended Isolation Forest\n(EIF) Anomaly detection method. ExIFFI is tested on two publicly available\nindustrial datasets demonstrating superior effectiveness in explanations and\ncomputational efficiency with the respect to other state-of-the-art explainable\nAD models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, submitted to IEEE RTSI 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01158v1",
    "published_date": "2024-05-02 10:23:17 UTC",
    "updated_date": "2024-05-02 10:23:17 UTC"
  },
  {
    "arxiv_id": "2405.01156v1",
    "title": "Self-Supervised Learning for Interventional Image Analytics: Towards Robust Device Trackers",
    "authors": [
      "Saahil Islam",
      "Venkatesh N. Murthy",
      "Dominik Neumann",
      "Badhan Kumar Das",
      "Puneet Sharma",
      "Andreas Maier",
      "Dorin Comaniciu",
      "Florin C. Ghesu"
    ],
    "abstract": "An accurate detection and tracking of devices such as guiding catheters in\nlive X-ray image acquisitions is an essential prerequisite for endovascular\ncardiac interventions. This information is leveraged for procedural guidance,\ne.g., directing stent placements. To ensure procedural safety and efficacy,\nthere is a need for high robustness no failures during tracking. To achieve\nthat, one needs to efficiently tackle challenges, such as: device obscuration\nby contrast agent or other external devices or wires, changes in field-of-view\nor acquisition angle, as well as the continuous movement due to cardiac and\nrespiratory motion. To overcome the aforementioned challenges, we propose a\nnovel approach to learn spatio-temporal features from a very large data cohort\nof over 16 million interventional X-ray frames using self-supervision for image\nsequence data. Our approach is based on a masked image modeling technique that\nleverages frame interpolation based reconstruction to learn fine inter-frame\ntemporal correspondences. The features encoded in the resulting model are\nfine-tuned downstream. Our approach achieves state-of-the-art performance and\nin particular robustness compared to ultra optimized reference solutions (that\nuse multi-stage feature fusion, multi-task and flow regularization). The\nexperiments show that our method achieves 66.31% reduction in maximum tracking\nerror against reference solutions (23.20% when flow regularization is used);\nachieving a success score of 97.95% at a 3x faster inference speed of 42\nframes-per-second (on GPU). The results encourage the use of our approach in\nvarious other tasks within interventional image analytics that require\neffective understanding of spatio-temporal semantics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01156v1",
    "published_date": "2024-05-02 10:18:22 UTC",
    "updated_date": "2024-05-02 10:18:22 UTC"
  },
  {
    "arxiv_id": "2405.01148v1",
    "title": "Qualia and the Formal Structure of Meaning",
    "authors": [
      "Xerxes D. Arsiwalla"
    ],
    "abstract": "This work explores the hypothesis that subjectively attributed meaning\nconstitutes the phenomenal content of conscious experience. That is, phenomenal\ncontent is semantic. This form of subjective meaning manifests as an intrinsic\nand non-representational character of qualia. Empirically, subjective meaning\nis ubiquitous in conscious experiences. We point to phenomenological studies\nthat lend evidence to support this. Furthermore, this notion of meaning closely\nrelates to what Frege refers to as \"sense\", in metaphysics and philosophy of\nlanguage. It also aligns with Peirce's \"interpretant\", in semiotics. We discuss\nhow Frege's sense can also be extended to the raw feels of consciousness. Sense\nand reference both play a role in phenomenal experience. Moreover, within the\ncontext of the mind-matter relation, we provide a formalization of subjective\nmeaning associated to one's mental representations. Identifying the precise\nmaps between the physical and mental domains, we argue that syntactic and\nsemantic structures transcend language, and are realized within each of these\ndomains. Formally, meaning is a relational attribute, realized via a map that\ninterprets syntactic structures of a formal system within an appropriate\nsemantic space. The image of this map within the mental domain is what is\nrelevant for experience, and thus comprises the phenomenal content of qualia.\nWe conclude with possible implications this may have for experience-based\ntheories of consciousness.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "math.CT",
      "physics.hist-ph"
    ],
    "primary_category": "q-bio.NC",
    "comment": "28 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.01148v1",
    "published_date": "2024-05-02 10:05:36 UTC",
    "updated_date": "2024-05-02 10:05:36 UTC"
  },
  {
    "arxiv_id": "2405.01134v1",
    "title": "Leveraging Procedural Generation for Learning Autonomous Peg-in-Hole Assembly in Space",
    "authors": [
      "Andrej Orsula",
      "Matthieu Geist",
      "Miguel Olivares-Mendez",
      "Carol Martinez"
    ],
    "abstract": "The ability to autonomously assemble structures is crucial for the\ndevelopment of future space infrastructure. However, the unpredictable\nconditions of space pose significant challenges for robotic systems,\nnecessitating the development of advanced learning techniques to enable\nautonomous assembly. In this study, we present a novel approach for learning\nautonomous peg-in-hole assembly in the context of space robotics. Our focus is\non enhancing the generalization and adaptability of autonomous systems through\ndeep reinforcement learning. By integrating procedural generation and domain\nrandomization, we train agents in a highly parallelized simulation environment\nacross a spectrum of diverse scenarios with the aim of acquiring a robust\npolicy. The proposed approach is evaluated using three distinct reinforcement\nlearning algorithms to investigate the trade-offs among various paradigms. We\ndemonstrate the adaptability of our agents to novel scenarios and assembly\nsequences while emphasizing the potential of leveraging advanced simulation\ntechniques for robot learning in space. Our findings set the stage for future\nadvancements in intelligent robotic systems capable of supporting ambitious\nspace missions and infrastructure development beyond Earth.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for publication at the 2024 International Conference on\n  Space Robotics (iSpaRo) | The source code is available at\n  https://github.com/AndrejOrsula/drl_omni_peg",
    "pdf_url": "http://arxiv.org/pdf/2405.01134v1",
    "published_date": "2024-05-02 09:50:01 UTC",
    "updated_date": "2024-05-02 09:50:01 UTC"
  },
  {
    "arxiv_id": "2405.01121v3",
    "title": "Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts",
    "authors": [
      "Lotem Golany",
      "Filippo Galgani",
      "Maya Mamo",
      "Nimrod Parasol",
      "Omer Vandsburger",
      "Nadav Bar",
      "Ido Dagan"
    ],
    "abstract": "Automating data generation with Large Language Models (LLMs) has become\nincreasingly popular. In this work, we investigate the feasibility and\neffectiveness of LLM-based data generation in the challenging setting of\nsource-grounded information-seeking dialogs, with response attribution, over\nlong documents. Our source texts consist of long and noisy meeting transcripts,\nadding to the task complexity. Since automating attribution remains difficult,\nwe propose a semi-automatic approach: dialog queries and responses are\ngenerated with LLMs, followed by human verification and identification of\nattribution spans. Using this approach, we created MISeD -- Meeting Information\nSeeking Dialogs dataset -- a dataset of information-seeking dialogs focused on\nmeeting transcripts. Models finetuned with MISeD demonstrate superior\nperformance compared to off-the-shelf models, even those of larger size.\nFinetuning on MISeD gives comparable response generation quality to finetuning\non fully manual data, while improving attribution quality and reducing time and\neffort.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01121v3",
    "published_date": "2024-05-02 09:35:06 UTC",
    "updated_date": "2024-10-15 08:48:03 UTC"
  },
  {
    "arxiv_id": "2405.01113v1",
    "title": "Domain-Transferred Synthetic Data Generation for Improving Monocular Depth Estimation",
    "authors": [
      "Seungyeop Lee",
      "Knut Peterson",
      "Solmaz Arezoomandan",
      "Bill Cai",
      "Peihan Li",
      "Lifeng Zhou",
      "David Han"
    ],
    "abstract": "A major obstacle to the development of effective monocular depth estimation\nalgorithms is the difficulty in obtaining high-quality depth data that\ncorresponds to collected RGB images. Collecting this data is time-consuming and\ncostly, and even data collected by modern sensors has limited range or\nresolution, and is subject to inconsistencies and noise. To combat this, we\npropose a method of data generation in simulation using 3D synthetic\nenvironments and CycleGAN domain transfer. We compare this method of data\ngeneration to the popular NYUDepth V2 dataset by training a depth estimation\nmodel based on the DenseDepth structure using different training sets of real\nand simulated data. We evaluate the performance of the models on newly\ncollected images and LiDAR depth data from a Husky robot to verify the\ngeneralizability of the approach and show that GAN-transformed data can serve\nas an effective alternative to real-world data, particularly in depth\nestimation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01113v1",
    "published_date": "2024-05-02 09:21:10 UTC",
    "updated_date": "2024-05-02 09:21:10 UTC"
  },
  {
    "arxiv_id": "2405.01108v1",
    "title": "Federated Learning with Heterogeneous Data Handling for Robust Vehicular Object Detection",
    "authors": [
      "Ahmad Khalil",
      "Tizian Dege",
      "Pegah Golchin",
      "Rostyslav Olshevskyi",
      "Antonio Fernandez Anta",
      "Tobias Meuser"
    ],
    "abstract": "In the pursuit of refining precise perception models for fully autonomous\ndriving, continual online model training becomes essential. Federated Learning\n(FL) within vehicular networks offers an efficient mechanism for model training\nwhile preserving raw sensory data integrity. Yet, FL struggles with\nnon-identically distributed data (e.g., quantity skew), leading to suboptimal\nconvergence rates during model training. In previous work, we introduced FedLA,\nan innovative Label-Aware aggregation method addressing data heterogeneity in\nFL for generic scenarios.\n  In this paper, we introduce FedProx+LA, a novel FL method building upon the\nstate-of-the-art FedProx and FedLA to tackle data heterogeneity, which is\nspecifically tailored for vehicular networks. We evaluate the efficacy of\nFedProx+LA in continuous online object detection model training. Through a\ncomparative analysis against conventional and state-of-the-art methods, our\nfindings reveal the superior convergence rate of FedProx+LA. Notably, if the\nlabel distribution is very heterogeneous, our FedProx+LA approach shows\nsubstantial improvements in detection performance compared to baseline methods,\nalso outperforming our previous FedLA approach. Moreover, both FedLA and\nFedProx+LA increase convergence speed by 30% compared to baseline methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01108v1",
    "published_date": "2024-05-02 09:14:59 UTC",
    "updated_date": "2024-05-02 09:14:59 UTC"
  },
  {
    "arxiv_id": "2405.01102v2",
    "title": "Less is More: on the Over-Globalizing Problem in Graph Transformers",
    "authors": [
      "Yujie Xing",
      "Xiao Wang",
      "Yibo Li",
      "Hai Huang",
      "Chuan Shi"
    ],
    "abstract": "Graph Transformer, due to its global attention mechanism, has emerged as a\nnew tool in dealing with graph-structured data. It is well recognized that the\nglobal attention mechanism considers a wider receptive field in a fully\nconnected graph, leading many to believe that useful information can be\nextracted from all the nodes. In this paper, we challenge this belief: does the\nglobalizing property always benefit Graph Transformers? We reveal the\nover-globalizing problem in Graph Transformer by presenting both empirical\nevidence and theoretical analysis, i.e., the current attention mechanism overly\nfocuses on those distant nodes, while the near nodes, which actually contain\nmost of the useful information, are relatively weakened. Then we propose a\nnovel Bi-Level Global Graph Transformer with Collaborative Training\n(CoBFormer), including the inter-cluster and intra-cluster Transformers, to\nprevent the over-globalizing problem while keeping the ability to extract\nvaluable information from distant nodes. Moreover, the collaborative training\nis proposed to improve the model's generalization ability with a theoretical\nguarantee. Extensive experiments on various graphs well validate the\neffectiveness of our proposed CoBFormer.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML 2024 (Camera-Ready)",
    "pdf_url": "http://arxiv.org/pdf/2405.01102v2",
    "published_date": "2024-05-02 09:12:22 UTC",
    "updated_date": "2024-05-24 08:53:13 UTC"
  },
  {
    "arxiv_id": "2405.01073v1",
    "title": "Poisoning Attacks on Federated Learning for Autonomous Driving",
    "authors": [
      "Sonakshi Garg",
      "Hugo Jönsson",
      "Gustav Kalander",
      "Axel Nilsson",
      "Bhhaanu Pirange",
      "Viktor Valadi",
      "Johan Östman"
    ],
    "abstract": "Federated Learning (FL) is a decentralized learning paradigm, enabling\nparties to collaboratively train models while keeping their data confidential.\nWithin autonomous driving, it brings the potential of reducing data storage\ncosts, reducing bandwidth requirements, and to accelerate the learning. FL is,\nhowever, susceptible to poisoning attacks. In this paper, we introduce two\nnovel poisoning attacks on FL tailored to regression tasks within autonomous\ndriving: FLStealth and Off-Track Attack (OTA). FLStealth, an untargeted attack,\naims at providing model updates that deteriorate the global model performance\nwhile appearing benign. OTA, on the other hand, is a targeted attack with the\nobjective to change the global model's behavior when exposed to a certain\ntrigger. We demonstrate the effectiveness of our attacks by conducting\ncomprehensive experiments pertaining to the task of vehicle trajectory\nprediction. In particular, we show that, among five different untargeted\nattacks, FLStealth is the most successful at bypassing the considered defenses\nemployed by the server. For OTA, we demonstrate the inability of common defense\nstrategies to mitigate the attack, highlighting the critical need for new\ndefensive mechanisms against targeted attacks within FL for autonomous driving.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to SCAI2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01073v1",
    "published_date": "2024-05-02 08:06:10 UTC",
    "updated_date": "2024-05-02 08:06:10 UTC"
  },
  {
    "arxiv_id": "2405.02342v1",
    "title": "The Birkhoff completion of finite lattices",
    "authors": [
      "Mohammad Abdulla",
      "Johannes Hirth",
      "Gerd Stumme"
    ],
    "abstract": "We introduce the Birkhoff completion as the smallest distributive lattice in\nwhich a given finite lattice can be embedded as semi-lattice. We discuss its\nrelationship to implicational theories, in particular to R. Wille's\nsimply-implicational theories. By an example, we show how the Birkhoff\ncompletion can be used as a tool for ordinal data science.",
    "categories": [
      "cs.DM",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.DM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02342v1",
    "published_date": "2024-05-02 08:01:43 UTC",
    "updated_date": "2024-05-02 08:01:43 UTC"
  },
  {
    "arxiv_id": "2405.01067v2",
    "title": "AB-Training: A Communication-Efficient Approach for Distributed Low-Rank Learning",
    "authors": [
      "Daniel Coquelin",
      "Katherina Flügel",
      "Marie Weiel",
      "Nicholas Kiefer",
      "Muhammed Öz",
      "Charlotte Debus",
      "Achim Streit",
      "Markus Götz"
    ],
    "abstract": "Communication bottlenecks severely hinder the scalability of distributed\nneural network training, particularly in high-performance computing (HPC)\nenvironments. We introduce AB-training, a novel data-parallel method that\nleverages low-rank representations and independent training groups to\nsignificantly reduce communication overhead. Our experiments demonstrate an\naverage reduction in network traffic of approximately 70.31\\% across various\nscaling scenarios, increasing the training potential of\ncommunication-constrained systems and accelerating convergence at scale.\nAB-training also exhibits a pronounced regularization effect at smaller scales,\nleading to improved generalization while maintaining or even reducing training\ntime. We achieve a remarkable 44.14 : 1 compression ratio on VGG16 trained on\nCIFAR-10 with minimal accuracy loss, and outperform traditional data parallel\ntraining by 1.55\\% on ResNet-50 trained on ImageNet-2012. While AB-training is\npromising, our findings also reveal that large batch effects persist even in\nlow-rank regimes, underscoring the need for further research into optimized\nupdate mechanisms for massively distributed training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01067v2",
    "published_date": "2024-05-02 07:49:28 UTC",
    "updated_date": "2024-06-30 08:06:33 UTC"
  },
  {
    "arxiv_id": "2405.01066v3",
    "title": "HandS3C: 3D Hand Mesh Reconstruction with State Space Spatial Channel Attention from RGB images",
    "authors": [
      "Zixun Jiao",
      "Xihan Wang",
      "Zhaoqiang Xia",
      "Lianhe Shao",
      "Quanli Gao"
    ],
    "abstract": "Reconstructing the hand mesh from one single RGB image is a challenging task\nbecause hands are often occluded by other objects. Most previous works attempt\nto explore more additional information and adopt attention mechanisms for\nimproving 3D reconstruction performance, while it would increase computational\ncomplexity simultaneously. To achieve a performance-reserving architecture with\nhigh computational efficiency, in this work, we propose a simple but effective\n3D hand mesh reconstruction network (i.e., HandS3C), which is the first time to\nincorporate state space model into the task of hand mesh reconstruction. In the\nnetwork, we design a novel state-space spatial-channel attention module that\nextends the effective receptive field, extracts hand features in the spatial\ndimension, and enhances regional features of hands in the channel dimension.\nThis helps to reconstruct a complete and detailed hand mesh. Extensive\nexperiments conducted on well-known datasets facing heavy occlusions (such as\nFREIHAND, DEXYCB, and HO3D) demonstrate that our proposed HandS3C achieves\nstate-of-the-art performance while maintaining a minimal parameters.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01066v3",
    "published_date": "2024-05-02 07:47:49 UTC",
    "updated_date": "2024-05-14 11:47:26 UTC"
  },
  {
    "arxiv_id": "2407.03110v1",
    "title": "A Toolchain for Comprehensive Audio/Video Analysis Using Deep Learning Based Multimodal Approach (A use case of riot or violent context detection)",
    "authors": [
      "Lam Pham",
      "Phat Lam",
      "Tin Nguyen",
      "Hieu Tang",
      "Alexander Schindler"
    ],
    "abstract": "In this paper, we present a toolchain for a comprehensive audio/video\nanalysis by leveraging deep learning based multimodal approach. To this end,\ndifferent specific tasks of Speech to Text (S2T), Acoustic Scene Classification\n(ASC), Acoustic Event Detection (AED), Visual Object Detection (VOD), Image\nCaptioning (IC), and Video Captioning (VC) are conducted and integrated into\nthe toolchain. By combining individual tasks and analyzing both audio \\& visual\ndata extracted from input video, the toolchain offers various audio/video-based\napplications: Two general applications of audio/video clustering, comprehensive\naudio/video summary and a specific application of riot or violent context\ndetection. Furthermore, the toolchain presents a flexible and adaptable\narchitecture that is effective to integrate new models for further\naudio/video-based applications.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03110v1",
    "published_date": "2024-05-02 07:34:31 UTC",
    "updated_date": "2024-05-02 07:34:31 UTC"
  },
  {
    "arxiv_id": "2405.01060v1",
    "title": "A text-based, generative deep learning model for soil reflectance spectrum simulation in the VIS-NIR (400-2499 nm) bands",
    "authors": [
      "Tong Lei",
      "Brian N. Bailey"
    ],
    "abstract": "Simulating soil reflectance spectra is invaluable for soil-plant radiative\nmodeling and training machine learning models, yet it is difficult as the\nintricate relationships between soil structure and its constituents. To address\nthis, a fully data-driven soil optics generative model (SOGM) for simulation of\nsoil reflectance spectra based on soil property inputs was developed. The model\nis trained on an extensive dataset comprising nearly 180,000 soil\nspectra-property pairs from 17 datasets. It generates soil reflectance spectra\nfrom text-based inputs describing soil properties and their values rather than\nonly numerical values and labels in binary vector format. The generative model\ncan simulate output spectra based on an incomplete set of input properties.\nSOGM is based on the denoising diffusion probabilistic model (DDPM). Two\nadditional sub-models were also built to complement the SOGM: a spectral\npadding model that can fill in the gaps for spectra shorter than the full\nvisible-near-infrared range (VIS-NIR; 400 to 2499 nm), and a wet soil spectra\nmodel that can estimate the effects of water content on soil reflectance\nspectra given the dry spectrum predicted by the SOGM. The SOGM was up-scaled by\ncoupling with the Helios 3D plant modeling software, which allowed for\ngeneration of synthetic aerial images of simulated soil and plant scenes. It\ncan also be easily integrated with soil-plant radiation model used for remote\nsensin research like PROSAIL. The testing results of the SOGM on new datasets\nthat not included in model training proved that the model can generate\nreasonable soil reflectance spectra based on available property inputs. The\npresented models are openly accessible on:\nhttps://github.com/GEMINI-Breeding/SOGM_soil_spectra_simulation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper has been submitted to Remote sensing of Environment and\n  revised",
    "pdf_url": "http://arxiv.org/pdf/2405.01060v1",
    "published_date": "2024-05-02 07:34:12 UTC",
    "updated_date": "2024-05-02 07:34:12 UTC"
  },
  {
    "arxiv_id": "2405.01055v2",
    "title": "Leverage Multi-source Traffic Demand Data Fusion with Transformer Model for Urban Parking Prediction",
    "authors": [
      "Yin Huang",
      "Yongqi Dong",
      "Youhua Tang",
      "Li Li"
    ],
    "abstract": "The escalation in urban private car ownership has worsened the urban parking\npredicament, necessitating effective parking availability prediction for urban\nplanning and management. However, the existing prediction methods suffer from\nlow prediction accuracy with the lack of spatial-temporal correlation features\nrelated to parking volume, and neglect of flow patterns and correlations\nbetween similar parking lots within certain areas. To address these challenges,\nthis study proposes a parking availability prediction framework integrating\nspatial-temporal deep learning with multi-source data fusion, encompassing\ntraffic demand data from multiple sources (e.g., metro, bus, taxi services),\nand parking lot data. The framework is based on the Transformer as the\nspatial-temporal deep learning model and leverages K-means clustering to\nestablish parking cluster zones, extracting and integrating traffic demand\ncharacteristics from various transportation modes (i.e., metro, bus, online\nride-hailing, and taxi) connected to parking lots. Real-world empirical data\nwas used to verify the effectiveness of the proposed method compared with\ndifferent machine learning, deep learning, and traditional statistical models\nfor predicting parking availability. Experimental results reveal that, with the\nproposed pipeline, the developed Transformer model outperforms other models in\nterms of various metrics, e.g., Mean Squared Error (MSE), Mean Absolute Error\n(MAE), and Mean Absolute Percentage Error (MAPE). By fusing multi-source\ndemanding data with spatial-temporal deep learning techniques, this approach\noffers the potential to develop parking availability prediction systems that\nfurnish more accurate and timely information to both drivers and urban\nplanners, thereby fostering more efficient and sustainable urban mobility.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 4 figures, accepted by the 28th International Conference of\n  Hong Kong Society for Transportation Studies (HKSTS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.01055v2",
    "published_date": "2024-05-02 07:28:27 UTC",
    "updated_date": "2024-11-03 19:55:19 UTC"
  },
  {
    "arxiv_id": "2405.01053v4",
    "title": "On the Universality of Self-Supervised Representation Learning",
    "authors": [
      "Wenwen Qiang",
      "Jingyao Wang",
      "Lingyu Si",
      "Chuxiong Sun",
      "Fuchun Sun",
      "Hui Xiong"
    ],
    "abstract": "In this paper, we investigate the characteristics that define a good\nrepresentation or model. We propose that such a representation or model should\npossess universality, characterized by: (i) discriminability: performing well\non training samples; (ii) generalization: performing well on unseen datasets;\nand (iii) transferability: performing well on unseen tasks with distribution\nshifts. Despite its importance, current self-supervised learning (SSL) methods\nlack explicit modeling of universality, and theoretical analysis remains\nunderexplored. To address these issues, we aim to explore and incorporate\nuniversality into SSL. Specifically, we first revisit SSL from a task\nperspective and find that each mini-batch can be viewed as a multi-class\nclassification task. We then propose that a universal SSL model should achieve:\n(i) learning universality by minimizing loss across all training samples, and\n(ii) evaluation universality by learning causally invariant representations\nthat generalize well to unseen tasks. To quantify this, we introduce a\n$\\sigma$-measurement that assesses the gap between the performance of SSL model\nand optimal task-specific models. Furthermore, to model universality, we\npropose the GeSSL framework. It first learns task-specific models by minimizing\nSSL loss, then incorporates future updates to enhance discriminability, and\nfinally integrates these models to learn from multiple tasks. Theoretical and\nempirical evidence supports the effectiveness of GeSSL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01053v4",
    "published_date": "2024-05-02 07:15:23 UTC",
    "updated_date": "2025-02-17 12:50:31 UTC"
  },
  {
    "arxiv_id": "2405.01035v1",
    "title": "LOQA: Learning with Opponent Q-Learning Awareness",
    "authors": [
      "Milad Aghajohari",
      "Juan Agustin Duque",
      "Tim Cooijmans",
      "Aaron Courville"
    ],
    "abstract": "In various real-world scenarios, interactions among agents often resemble the\ndynamics of general-sum games, where each agent strives to optimize its own\nutility. Despite the ubiquitous relevance of such settings, decentralized\nmachine learning algorithms have struggled to find equilibria that maximize\nindividual utility while preserving social welfare. In this paper we introduce\nLearning with Opponent Q-Learning Awareness (LOQA), a novel, decentralized\nreinforcement learning algorithm tailored to optimizing an agent's individual\nutility while fostering cooperation among adversaries in partially competitive\nenvironments. LOQA assumes the opponent samples actions proportionally to their\naction-value function Q. Experimental results demonstrate the effectiveness of\nLOQA at achieving state-of-the-art performance in benchmark scenarios such as\nthe Iterated Prisoner's Dilemma and the Coin Game. LOQA achieves these outcomes\nwith a significantly reduced computational footprint, making it a promising\napproach for practical multi-agent applications.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "accepted to ICLR but still not in proceedings\n  https://openreview.net/forum?id=FDQF6A1s6M",
    "pdf_url": "http://arxiv.org/pdf/2405.01035v1",
    "published_date": "2024-05-02 06:33:01 UTC",
    "updated_date": "2024-05-02 06:33:01 UTC"
  },
  {
    "arxiv_id": "2405.01029v2",
    "title": "MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts",
    "authors": [
      "Jianan Zhou",
      "Zhiguang Cao",
      "Yaoxin Wu",
      "Wen Song",
      "Yining Ma",
      "Jie Zhang",
      "Chi Xu"
    ],
    "abstract": "Learning to solve vehicle routing problems (VRPs) has garnered much\nattention. However, most neural solvers are only structured and trained\nindependently on a specific problem, making them less generic and practical. In\nthis paper, we aim to develop a unified neural solver that can cope with a\nrange of VRP variants simultaneously. Specifically, we propose a multi-task\nvehicle routing solver with mixture-of-experts (MVMoE), which greatly enhances\nthe model capacity without a proportional increase in computation. We further\ndevelop a hierarchical gating mechanism for the MVMoE, delivering a good\ntrade-off between empirical performance and computational complexity.\nExperimentally, our method significantly promotes zero-shot generalization\nperformance on 10 unseen VRP variants, and showcases decent results on the\nfew-shot setting and real-world benchmark instances. We further conduct\nextensive studies on the effect of MoE configurations in solving VRPs, and\nobserve the superiority of hierarchical gating when facing out-of-distribution\ndata. The source code is available at:\nhttps://github.com/RoyalSkye/Routing-MVMoE.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01029v2",
    "published_date": "2024-05-02 06:02:07 UTC",
    "updated_date": "2024-05-06 11:35:57 UTC"
  },
  {
    "arxiv_id": "2405.01022v3",
    "title": "UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation",
    "authors": [
      "Juhwan Choi",
      "Yeonghwa Kim",
      "Seunguk Yu",
      "JungMin Yun",
      "YoungBin Kim"
    ],
    "abstract": "Although pre-trained language models have exhibited great flexibility and\nversatility with prompt-based few-shot learning, they suffer from the extensive\nparameter size and limited applicability for inference. Recent studies have\nsuggested that PLMs be used as dataset generators and a tiny task-specific\nmodel be trained to achieve efficient inference. However, their applicability\nto various domains is limited because they tend to generate domain-specific\ndatasets. In this work, we propose a novel approach to universal domain\ngeneralization that generates a dataset regardless of the target domain. This\nallows for generalization of the tiny task model to any domain that shares the\nlabel space, thus enhancing the real-world applicability of the dataset\ngeneration paradigm. Our experiments indicate that the proposed method\naccomplishes generalizability across various domains while using a parameter\nset that is orders of magnitude smaller than PLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024: Camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2405.01022v3",
    "published_date": "2024-05-02 05:46:13 UTC",
    "updated_date": "2024-09-22 08:58:10 UTC"
  },
  {
    "arxiv_id": "2405.01016v4",
    "title": "Addressing Diverging Training Costs using BEVRestore for High-resolution Bird's Eye View Map Construction",
    "authors": [
      "Minsu Kim",
      "Giseop Kim",
      "Sunwook Choi"
    ],
    "abstract": "Recent advancements in Bird's Eye View (BEV) fusion for map construction have\ndemonstrated remarkable mapping of urban environments. However, their deep and\nbulky architecture incurs substantial amounts of backpropagation memory and\ncomputing latency. Consequently, the problem poses an unavoidable bottleneck in\nconstructing high-resolution (HR) BEV maps, as their large-sized features cause\nsignificant increases in costs including GPU memory consumption and computing\nlatency, named diverging training costs issue. Affected by the problem, most\nexisting methods adopt low-resolution (LR) BEV and struggle to estimate the\nprecise locations of urban scene components like road lanes, and sidewalks. As\nthe imprecision leads to risky motion planning like collision avoidance, the\ndiverging training costs issue has to be resolved. In this paper, we address\nthe issue with our novel BEVRestore mechanism. Specifically, our proposed model\nencodes the features of each sensor to LR BEV space and restores them to HR\nspace to establish a memory-efficient map constructor. To this end, we\nintroduce the BEV restoration strategy, which restores aliasing, and blocky\nartifacts of the up-scaled BEV features, and narrows down the width of the\nlabels. Our extensive experiments show that the proposed mechanism provides a\nplug-and-play, memory-efficient pipeline, enabling an HR map construction with\na broad BEV scope.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01016v4",
    "published_date": "2024-05-02 05:35:10 UTC",
    "updated_date": "2024-08-22 06:36:47 UTC"
  },
  {
    "arxiv_id": "2405.01013v2",
    "title": "Non-clairvoyant Scheduling with Partial Predictions",
    "authors": [
      "Ziyad Benomar",
      "Vianney Perchet"
    ],
    "abstract": "The non-clairvoyant scheduling problem has gained new interest within\nlearning-augmented algorithms, where the decision-maker is equipped with\npredictions without any quality guarantees. In practical settings, access to\npredictions may be reduced to specific instances, due to cost or data\nlimitations. Our investigation focuses on scenarios where predictions for only\n$B$ job sizes out of $n$ are available to the algorithm. We first establish\nnear-optimal lower bounds and algorithms in the case of perfect predictions.\nSubsequently, we present a learning-augmented algorithm satisfying the\nrobustness, consistency, and smoothness criteria, and revealing a novel\ntradeoff between consistency and smoothness inherent in the scenario with a\nrestricted number of predictions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as a conference paper at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.01013v2",
    "published_date": "2024-05-02 05:29:22 UTC",
    "updated_date": "2024-08-04 18:09:39 UTC"
  },
  {
    "arxiv_id": "2405.01004v1",
    "title": "Deep Learning Models in Speech Recognition: Measuring GPU Energy Consumption, Impact of Noise and Model Quantization for Edge Deployment",
    "authors": [
      "Aditya Chakravarty"
    ],
    "abstract": "Recent transformer-based ASR models have achieved word-error rates (WER)\nbelow 4%, surpassing human annotator accuracy, yet they demand extensive server\nresources, contributing to significant carbon footprints. The traditional\nserver-based architecture of ASR also presents privacy concerns, alongside\nreliability and latency issues due to network dependencies. In contrast,\non-device (edge) ASR enhances privacy, boosts performance, and promotes\nsustainability by effectively balancing energy use and accuracy for specific\napplications. This study examines the effects of quantization, memory demands,\nand energy consumption on the performance of various ASR model inference on the\nNVIDIA Jetson Orin Nano. By analyzing WER and transcription speed across models\nusing FP32, FP16, and INT8 quantization on clean and noisy datasets, we\nhighlight the crucial trade-offs between accuracy, speeds, quantization, energy\nefficiency, and memory needs. We found that changing precision from fp32 to\nfp16 halves the energy consumption for audio transcription across different\nmodels, with minimal performance degradation. A larger model size and number of\nparameters neither guarantees better resilience to noise, nor predicts the\nenergy consumption for a given transcription load. These, along with several\nother findings offer novel insights for optimizing ASR systems within energy-\nand memory-limited environments, crucial for the development of efficient\non-device ASR solutions. The code and input data needed to reproduce the\nresults in this article are open sourced are available on\n[https://github.com/zzadiues3338/ASR-energy-jetson].",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01004v1",
    "published_date": "2024-05-02 05:09:07 UTC",
    "updated_date": "2024-05-02 05:09:07 UTC"
  },
  {
    "arxiv_id": "2405.00985v1",
    "title": "Progressive Feedforward Collapse of ResNet Training",
    "authors": [
      "Sicong Wang",
      "Kuo Gai",
      "Shihua Zhang"
    ],
    "abstract": "Neural collapse (NC) is a simple and symmetric phenomenon for deep neural\nnetworks (DNNs) at the terminal phase of training, where the last-layer\nfeatures collapse to their class means and form a simplex equiangular tight\nframe aligning with the classifier vectors. However, the relationship of the\nlast-layer features to the data and intermediate layers during training remains\nunexplored. To this end, we characterize the geometry of intermediate layers of\nResNet and propose a novel conjecture, progressive feedforward collapse (PFC),\nclaiming the degree of collapse increases during the forward propagation of\nDNNs. We derive a transparent model for the well-trained ResNet according to\nthat ResNet with weight decay approximates the geodesic curve in Wasserstein\nspace at the terminal phase. The metrics of PFC indeed monotonically decrease\nacross depth on various datasets. We propose a new surrogate model, multilayer\nunconstrained feature model (MUFM), connecting intermediate layers by an\noptimal transport regularizer. The optimal solution of MUFM is inconsistent\nwith NC but is more concentrated relative to the input data. Overall, this\nstudy extends NC to PFC to model the collapse phenomenon of intermediate layers\nand its dependence on the input data, shedding light on the theoretical\nunderstanding of ResNet in classification problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "math.ST",
      "stat.TH",
      "68T07",
      "I.2.0"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.00985v1",
    "published_date": "2024-05-02 03:48:08 UTC",
    "updated_date": "2024-05-02 03:48:08 UTC"
  },
  {
    "arxiv_id": "2405.00981v2",
    "title": "Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language Preference Elicitation",
    "authors": [
      "David Eric Austin",
      "Anton Korikov",
      "Armin Toroghi",
      "Scott Sanner"
    ],
    "abstract": "Designing preference elicitation (PE) methodologies that can quickly\nascertain a user's top item preferences in a cold-start setting is a key\nchallenge for building effective and personalized conversational recommendation\n(ConvRec) systems. While large language models (LLMs) enable fully natural\nlanguage (NL) PE dialogues, we hypothesize that monolithic LLM NL-PE approaches\nlack the multi-turn, decision-theoretic reasoning required to effectively\nbalance the exploration and exploitation of user preferences towards an\narbitrary item set. In contrast, traditional Bayesian optimization PE methods\ndefine theoretically optimal PE strategies, but cannot generate arbitrary NL\nqueries or reason over content in NL item descriptions -- requiring users to\nexpress preferences via ratings or comparisons of unfamiliar items. To overcome\nthe limitations of both approaches, we formulate NL-PE in a Bayesian\nOptimization (BO) framework that seeks to actively elicit NL feedback to\nidentify the best recommendation. Key challenges in generalizing BO to deal\nwith natural language feedback include determining: (a) how to leverage LLMs to\nmodel the likelihood of NL preference feedback as a function of item utilities,\nand (b) how to design an acquisition function for NL BO that can elicit\npreferences in the infinite space of language. We demonstrate our framework in\na novel NL-PE algorithm, PEBOL, which uses: 1) Natural Language Inference (NLI)\nbetween user preference utterances and NL item descriptions to maintain\nBayesian preference beliefs, and 2) BO strategies such as Thompson Sampling\n(TS) and Upper Confidence Bound (UCB) to steer LLM query generation. We\nnumerically evaluate our methods in controlled simulations, finding that after\n10 turns of dialogue, PEBOL can achieve an MRR@10 of up to 0.27 compared to the\nbest monolithic LLM baseline's MRR@10 of 0.17, despite relying on earlier and\nsmaller LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00981v2",
    "published_date": "2024-05-02 03:35:21 UTC",
    "updated_date": "2024-08-20 03:15:07 UTC"
  },
  {
    "arxiv_id": "2405.00972v1",
    "title": "CACTUS: Chemistry Agent Connecting Tool-Usage to Science",
    "authors": [
      "Andrew D. McNaughton",
      "Gautham Ramalaxmi",
      "Agustin Kruel",
      "Carter R. Knutson",
      "Rohith A. Varikoti",
      "Neeraj Kumar"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable potential in various\ndomains, but they often lack the ability to access and reason over\ndomain-specific knowledge and tools. In this paper, we introduced CACTUS\n(Chemistry Agent Connecting Tool-Usage to Science), an LLM-based agent that\nintegrates cheminformatics tools to enable advanced reasoning and\nproblem-solving in chemistry and molecular discovery. We evaluate the\nperformance of CACTUS using a diverse set of open-source LLMs, including\nGemma-7b, Falcon-7b, MPT-7b, Llama2-7b, and Mistral-7b, on a benchmark of\nthousands of chemistry questions. Our results demonstrate that CACTUS\nsignificantly outperforms baseline LLMs, with the Gemma-7b and Mistral-7b\nmodels achieving the highest accuracy regardless of the prompting strategy\nused. Moreover, we explore the impact of domain-specific prompting and hardware\nconfigurations on model performance, highlighting the importance of prompt\nengineering and the potential for deploying smaller models on consumer-grade\nhardware without significant loss in accuracy. By combining the cognitive\ncapabilities of open-source LLMs with domain-specific tools, CACTUS can assist\nresearchers in tasks such as molecular property prediction, similarity\nsearching, and drug-likeness assessment. Furthermore, CACTUS represents a\nsignificant milestone in the field of cheminformatics, offering an adaptable\ntool for researchers engaged in chemistry and molecular discovery. By\nintegrating the strengths of open-source LLMs with domain-specific tools,\nCACTUS has the potential to accelerate scientific advancement and unlock new\nfrontiers in the exploration of novel, effective, and safe therapeutic\ncandidates, catalysts, and materials. Moreover, CACTUS's ability to integrate\nwith automated experimentation platforms and make data-driven decisions in real\ntime opens up new possibilities for autonomous discovery.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "physics.chem-ph",
      "q-bio.QM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00972v1",
    "published_date": "2024-05-02 03:20:08 UTC",
    "updated_date": "2024-05-02 03:20:08 UTC"
  },
  {
    "arxiv_id": "2405.00970v1",
    "title": "How Can I Get It Right? Using GPT to Rephrase Incorrect Trainee Responses",
    "authors": [
      "Jionghao Lin",
      "Zifei Han",
      "Danielle R. Thomas",
      "Ashish Gurung",
      "Shivang Gupta",
      "Vincent Aleven",
      "Kenneth R. Koedinger"
    ],
    "abstract": "One-on-one tutoring is widely acknowledged as an effective instructional\nmethod, conditioned on qualified tutors. However, the high demand for qualified\ntutors remains a challenge, often necessitating the training of novice tutors\n(i.e., trainees) to ensure effective tutoring. Research suggests that providing\ntimely explanatory feedback can facilitate the training process for trainees.\nHowever, it presents challenges due to the time-consuming nature of assessing\ntrainee performance by human experts. Inspired by the recent advancements of\nlarge language models (LLMs), our study employed the GPT-4 model to build an\nexplanatory feedback system. This system identifies trainees' responses in\nbinary form (i.e., correct/incorrect) and automatically provides template-based\nfeedback with responses appropriately rephrased by the GPT-4 model. We\nconducted our study on 410 responses from trainees across three training\nlessons: Giving Effective Praise, Reacting to Errors, and Determining What\nStudents Know. Our findings indicate that: 1) using a few-shot approach, the\nGPT-4 model effectively identifies correct/incorrect trainees' responses from\nthree training lessons with an average F1 score of 0.84 and an AUC score of\n0.85; and 2) using the few-shot approach, the GPT-4 model adeptly rephrases\nincorrect trainees' responses into desired responses, achieving performance\ncomparable to that of human experts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "International Journal of Artificial Intelligence in Education",
    "pdf_url": "http://arxiv.org/pdf/2405.00970v1",
    "published_date": "2024-05-02 03:18:03 UTC",
    "updated_date": "2024-05-02 03:18:03 UTC"
  },
  {
    "arxiv_id": "2405.00966v1",
    "title": "Efficient Compression of Multitask Multilingual Speech Models",
    "authors": [
      "Thomas Palmeira Ferraz"
    ],
    "abstract": "Whisper is a multitask and multilingual speech model covering 99 languages.\nIt yields commendable automatic speech recognition (ASR) results in a subset of\nits covered languages, but the model still underperforms on a non-negligible\nnumber of under-represented languages, a problem exacerbated in smaller model\nversions. In this work, we examine its limitations, demonstrating the presence\nof speaker-related (gender, age) and model-related (resourcefulness and model\nsize) bias. Despite that, we show that only model-related bias are amplified by\nquantization, impacting more low-resource languages and smaller models.\nSearching for a better compression approach, we propose DistilWhisper, an\napproach that is able to bridge the performance gap in ASR for these languages\nwhile retaining the advantages of multitask and multilingual capabilities. Our\napproach involves two key strategies: lightweight modular ASR fine-tuning of\nwhisper-small using language-specific experts, and knowledge distillation from\nwhisper-large-v2. This dual approach allows us to effectively boost ASR\nperformance while keeping the robustness inherited from the multitask and\nmultilingual pre-training. Results demonstrate that our approach is more\neffective than standard fine-tuning or LoRA adapters, boosting performance in\nthe targeted languages for both in- and out-of-domain test sets, while\nintroducing only a negligible parameter overhead at inference.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Master Thesis",
    "pdf_url": "http://arxiv.org/pdf/2405.00966v1",
    "published_date": "2024-05-02 03:11:59 UTC",
    "updated_date": "2024-05-02 03:11:59 UTC"
  },
  {
    "arxiv_id": "2405.00960v2",
    "title": "Foundations for Digital Twins",
    "authors": [
      "Finn Wilson",
      "Regina Hurley",
      "Dan Maxwell",
      "Jon McLellan",
      "John Beverley"
    ],
    "abstract": "The growing reliance on digital twins across various industries and domains\nbrings with it semantic interoperability challenges. Ontologies are a\nwell-known strategy for addressing such challenges, though given the complexity\nof the phenomenon, there are risks of reintroducing the interoperability\nchallenges at the level of ontology representations. In the interest of\navoiding such pitfalls, we introduce and defend characterizations of digital\ntwins within the context of the Common Core Ontologies, an extension of the\nwidely-used Basic Formal Ontology. We provide a set of definitions and design\npatterns relevant to the domain of digital twins, highlighted by illustrative\nuse cases of digital twins and their physical counterparts. In doing so, we\nprovide a foundation on which to build more sophisticated ontological content\nrelated and connected to digital twins.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "14",
    "pdf_url": "http://arxiv.org/pdf/2405.00960v2",
    "published_date": "2024-05-02 02:52:11 UTC",
    "updated_date": "2024-08-16 03:21:24 UTC"
  },
  {
    "arxiv_id": "2405.00958v2",
    "title": "Generative manufacturing systems using diffusion models and ChatGPT",
    "authors": [
      "Xingyu Li",
      "Fei Tao",
      "Wei Ye",
      "Aydin Nassehi",
      "John W. Sutherland"
    ],
    "abstract": "In this study, we introduce Generative Manufacturing Systems (GMS) as a novel\napproach to effectively manage and coordinate autonomous manufacturing assets,\nthereby enhancing their responsiveness and flexibility to address a wide array\nof production objectives and human preferences. Deviating from traditional\nexplicit modeling, GMS employs generative AI, including diffusion models and\nChatGPT, for implicit learning from envisioned futures, marking a shift from a\nmodel-optimum to a training-sampling decision-making. Through the integration\nof generative AI, GMS enables complex decision-making through interactive\ndialogue with humans, allowing manufacturing assets to generate multiple\nhigh-quality global decisions that can be iteratively refined based on human\nfeedback. Empirical findings showcase GMS's substantial improvement in system\nresilience and responsiveness to uncertainties, with decision times reduced\nfrom seconds to milliseconds. The study underscores the inherent creativity and\ndiversity in the generated solutions, facilitating human-centric\ndecision-making through seamless and continuous human-machine interactions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "We are withdrawing this preprint to incorporate significant new\n  results and expand the scope of the paper. We plan to resubmit a\n  substantially revised version in the near future",
    "pdf_url": "http://arxiv.org/pdf/2405.00958v2",
    "published_date": "2024-05-02 02:50:58 UTC",
    "updated_date": "2025-01-08 23:16:20 UTC"
  },
  {
    "arxiv_id": "2405.00957v2",
    "title": "IntraMix: Intra-Class Mixup Generation for Accurate Labels and Neighbors",
    "authors": [
      "Shenghe Zheng",
      "Hongzhi Wang",
      "Xianglong Liu"
    ],
    "abstract": "Graph Neural Networks (GNNs) have shown great performance in various tasks,\nwith the core idea of learning from data labels and aggregating messages within\nthe neighborhood of nodes. However, the common challenges in graphs are\ntwofold: insufficient accurate (high-quality) labels and limited neighbors for\nnodes, resulting in weak GNNs. Existing graph augmentation methods typically\naddress only one of these challenges, often adding training costs or relying on\noversimplified or knowledge-intensive strategies, limiting their\ngeneralization. To simultaneously address both challenges faced by graphs in a\ngeneralized way, we propose an elegant method called IntraMix. Considering the\nincompatibility of vanilla Mixup with the complex topology of graphs, IntraMix\ninnovatively employs Mixup among inaccurate labeled data of the same class,\ngenerating high-quality labeled data at minimal cost. Additionally, it finds\ndata with high confidence of being clustered into the same group as the\ngenerated data to serve as their neighbors, thereby enriching the neighborhoods\nof graphs. IntraMix efficiently tackles both issues faced by graphs and\nchallenges the prior notion of the limited effectiveness of Mixup in node\nclassification. IntraMix is a theoretically grounded plug-in-play method that\ncan be readily applied to all GNNs. Extensive experiments demonstrate the\neffectiveness of IntraMix across various GNNs and datasets. Our code is\navailable at: https://github.com/Zhengsh123/IntraMix.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS2024",
    "pdf_url": "http://arxiv.org/pdf/2405.00957v2",
    "published_date": "2024-05-02 02:38:32 UTC",
    "updated_date": "2024-11-01 03:51:18 UTC"
  },
  {
    "arxiv_id": "2405.00950v1",
    "title": "Provably Efficient Reinforcement Learning for Adversarial Restless Multi-Armed Bandits with Unknown Transitions and Bandit Feedback",
    "authors": [
      "Guojun Xiong",
      "Jian Li"
    ],
    "abstract": "Restless multi-armed bandits (RMAB) play a central role in modeling\nsequential decision making problems under an instantaneous activation\nconstraint that at most B arms can be activated at any decision epoch. Each\nrestless arm is endowed with a state that evolves independently according to a\nMarkov decision process regardless of being activated or not. In this paper, we\nconsider the task of learning in episodic RMAB with unknown transition\nfunctions and adversarial rewards, which can change arbitrarily across\nepisodes. Further, we consider a challenging but natural bandit feedback\nsetting that only adversarial rewards of activated arms are revealed to the\ndecision maker (DM). The goal of the DM is to maximize its total adversarial\nrewards during the learning process while the instantaneous activation\nconstraint must be satisfied in each decision epoch. We develop a novel\nreinforcement learning algorithm with two key contributors: a novel biased\nadversarial reward estimator to deal with bandit feedback and unknown\ntransitions, and a low-complexity index policy to satisfy the instantaneous\nactivation constraint. We show $\\tilde{\\mathcal{O}}(H\\sqrt{T})$ regret bound\nfor our algorithm, where $T$ is the number of episodes and $H$ is the episode\nlength. To our best knowledge, this is the first algorithm to ensure\n$\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret for adversarial RMAB in our considered\nchallenging settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.00950v1",
    "published_date": "2024-05-02 02:20:19 UTC",
    "updated_date": "2024-05-02 02:20:19 UTC"
  },
  {
    "arxiv_id": "2405.17436v1",
    "title": "Intelligent Hybrid Resource Allocation in MEC-assisted RAN Slicing Network",
    "authors": [
      "Chong Zheng",
      "Yongming Huang",
      "Cheng Zhang",
      "Tony Q. S. Quek"
    ],
    "abstract": "In this paper, we aim to maximize the SSR for heterogeneous service demands\nin the cooperative MEC-assisted RAN slicing system by jointly considering the\nmulti-node computing resources cooperation and allocation, the transmission\nresource blocks (RBs) allocation, and the time-varying dynamicity of the\nsystem. To this end, we abstract the system into a weighted undirected topology\ngraph and, then propose a recurrent graph reinforcement learning (RGRL)\nalgorithm to intelligently learn the optimal hybrid RA policy. Therein, the\ngraph neural network (GCN) and the deep deterministic policy gradient (DDPG) is\ncombined to effectively extract spatial features from the equivalent topology\ngraph. Furthermore, a novel time recurrent reinforcement learning framework is\ndesigned in the proposed RGRL algorithm by incorporating the action output of\nthe policy network at the previous moment into the state input of the policy\nnetwork at the subsequent moment, so as to cope with the time-varying and\ncontextual network environment. In addition, we explore two use case scenarios\nto discuss the universal superiority of the proposed RGRL algorithm. Simulation\nresults demonstrate the superiority of the proposed algorithm in terms of the\naverage SSR, the performance stability, and the network complexity.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17436v1",
    "published_date": "2024-05-02 01:36:13 UTC",
    "updated_date": "2024-05-02 01:36:13 UTC"
  },
  {
    "arxiv_id": "2405.00915v2",
    "title": "EchoScene: Indoor Scene Generation via Information Echo over Scene Graph Diffusion",
    "authors": [
      "Guangyao Zhai",
      "Evin Pınar Örnek",
      "Dave Zhenyu Chen",
      "Ruotong Liao",
      "Yan Di",
      "Nassir Navab",
      "Federico Tombari",
      "Benjamin Busam"
    ],
    "abstract": "We present EchoScene, an interactive and controllable generative model that\ngenerates 3D indoor scenes on scene graphs. EchoScene leverages a dual-branch\ndiffusion model that dynamically adapts to scene graphs. Existing methods\nstruggle to handle scene graphs due to varying numbers of nodes, multiple edge\ncombinations, and manipulator-induced node-edge operations. EchoScene overcomes\nthis by associating each node with a denoising process and enables\ncollaborative information exchange, enhancing controllable and consistent\ngeneration aware of global constraints. This is achieved through an information\necho scheme in both shape and layout branches. At every denoising step, all\nprocesses share their denoising data with an information exchange unit that\ncombines these updates using graph convolution. The scheme ensures that the\ndenoising processes are influenced by a holistic understanding of the scene\ngraph, facilitating the generation of globally coherent scenes. The resulting\nscenes can be manipulated during inference by editing the input scene graph and\nsampling the noise in the diffusion model. Extensive experiments validate our\napproach, which maintains scene controllability and surpasses previous methods\nin generation fidelity. Moreover, the generated scenes are of high quality and\nthus directly compatible with off-the-shelf texture generation. Code and\ntrained models are open-sourced.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Nectar Track at 3DV 2025",
    "pdf_url": "http://arxiv.org/pdf/2405.00915v2",
    "published_date": "2024-05-02 00:04:02 UTC",
    "updated_date": "2025-02-27 12:28:21 UTC"
  }
]
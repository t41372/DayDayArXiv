[
  {
    "arxiv_id": "2410.06428v1",
    "title": "Stress Detection on Code-Mixed Texts in Dravidian Languages using Machine Learning",
    "authors": [
      "L. Ramos",
      "M. Shahiki-Tash",
      "Z. Ahani",
      "A. Eponon",
      "O. Kolesnikova",
      "H. Calvo"
    ],
    "abstract": "Stress is a common feeling in daily life, but it can affect mental well-being\nin some situations, the development of robust detection models is imperative.\nThis study introduces a methodical approach to the stress identification in\ncode-mixed texts for Dravidian languages. The challenge encompassed two\ndatasets, targeting Tamil and Telugu languages respectively. This proposal\nunderscores the importance of using uncleaned text as a benchmark to refine\nfuture classification methodologies, incorporating diverse preprocessing\ntechniques. Random Forest algorithm was used, featuring three textual\nrepresentations: TF-IDF, Uni-grams of words, and a composite of (1+2+3)-Grams\nof characters. The approach achieved a good performance for both linguistic\ncategories, achieving a Macro F1-score of 0.734 in Tamil and 0.727 in Telugu,\noverpassing results achieved with different complex techniques such as FastText\nand Transformer models. The results underscore the value of uncleaned data for\nmental state detection and the challenges classifying code-mixed texts for\nstress, indicating the potential for improved performance through cleaning\ndata, other preprocessing techniques, or more complex models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06428v1",
    "published_date": "2024-10-08 23:49:31 UTC",
    "updated_date": "2024-10-08 23:49:31 UTC"
  },
  {
    "arxiv_id": "2410.06427v1",
    "title": "NLP Case Study on Predicting the Before and After of the Ukraine-Russia and Hamas-Israel Conflicts",
    "authors": [
      "Jordan Miner",
      "John E. Ortega"
    ],
    "abstract": "We propose a method to predict toxicity and other textual attributes through\nthe use of natural language processing (NLP) techniques for two recent events:\nthe Ukraine-Russia and Hamas-Israel conflicts. This article provides a basis\nfor exploration in future conflicts with hopes to mitigate risk through the\nanalysis of social media before and after a conflict begins. Our work compiles\nseveral datasets from Twitter and Reddit for both conflicts in a before and\nafter separation with an aim of predicting a future state of social media for\navoidance. More specifically, we show that: (1) there is a noticeable\ndifference in social media discussion leading up to and following a conflict\nand (2) social media discourse on platforms like Twitter and Reddit is useful\nin identifying future conflicts before they arise. Our results show that\nthrough the use of advanced NLP techniques (both supervised and unsupervised)\ntoxicity and other attributes about language before and after a conflict is\npredictable with a low error of nearly 1.2 percent for both conflicts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "The clusters created using topic modeling can be viewed at\n  https://naturallang.com/conflict/conflict.html",
    "pdf_url": "http://arxiv.org/pdf/2410.06427v1",
    "published_date": "2024-10-08 23:46:56 UTC",
    "updated_date": "2024-10-08 23:46:56 UTC"
  },
  {
    "arxiv_id": "2410.06423v1",
    "title": "FAIREDU: A Multiple Regression-Based Method for Enhancing Fairness in Machine Learning Models for Educational Applications",
    "authors": [
      "Nga Pham",
      "Minh Kha Do",
      "Tran Vu Dai",
      "Pham Ngoc Hung",
      "Anh Nguyen-Duc"
    ],
    "abstract": "Fairness in artificial intelligence and machine learning (AI/ML) models is\nbecoming critically important, especially as decisions made by these systems\nimpact diverse groups. In education, a vital sector for all countries, the\nwidespread application of AI/ML systems raises specific concerns regarding\nfairness. Current research predominantly focuses on fairness for individual\nsensitive features, which limits the comprehensiveness of fairness assessments.\nThis paper introduces FAIREDU, a novel and effective method designed to improve\nfairness across multiple sensitive features. Through extensive experiments, we\nevaluate FAIREDU effectiveness in enhancing fairness without compromising model\nperformance. The results demonstrate that FAIREDU addresses intersectionality\nacross features such as gender, race, age, and other sensitive features,\noutperforming state-of-the-art methods with minimal effect on model accuracy.\nThe paper also explores potential future research directions to enhance further\nthe method robustness and applicability to various machine-learning models and\ndatasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06423v1",
    "published_date": "2024-10-08 23:29:24 UTC",
    "updated_date": "2024-10-08 23:29:24 UTC"
  },
  {
    "arxiv_id": "2410.06415v2",
    "title": "Biased AI can Influence Political Decision-Making",
    "authors": [
      "Jillian Fisher",
      "Shangbin Feng",
      "Robert Aron",
      "Thomas Richardson",
      "Yejin Choi",
      "Daniel W. Fisher",
      "Jennifer Pan",
      "Yulia Tsvetkov",
      "Katharina Reinecke"
    ],
    "abstract": "As modern AI models become integral to everyday tasks, concerns about their\ninherent biases and their potential impact on human decision-making have\nemerged. While bias in models are well-documented, less is known about how\nthese biases influence human decisions. This paper presents two interactive\nexperiments investigating the effects of partisan bias in AI language models on\npolitical decision-making. Participants interacted freely with either a biased\nliberal, biased conservative, or unbiased control model while completing\npolitical decision-making tasks. We found that participants exposed to\npolitically biased models were significantly more likely to adopt opinions and\nmake decisions aligning with the AI's bias, regardless of their personal\npolitical partisanship. However, we also discovered that prior knowledge about\nAI could lessen the impact of the bias, highlighting the possible importance of\nAI education for robust bias mitigation. Our findings not only highlight the\ncritical effects of interacting with biased AI and its ability to impact public\ndiscourse and political conduct, but also highlights potential techniques for\nmitigating these risks in the future.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06415v2",
    "published_date": "2024-10-08 22:56:00 UTC",
    "updated_date": "2024-11-04 20:12:07 UTC"
  },
  {
    "arxiv_id": "2410.06405v1",
    "title": "Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the Importance of 2D Representation, Positions, and Objects",
    "authors": [
      "Wenhao Li",
      "Yudong Xu",
      "Scott Sanner",
      "Elias Boutros Khalil"
    ],
    "abstract": "The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused on\nvisual reasoning in the evaluation of Artificial Intelligence systems. In its\noriginal framing, an ARC task requires solving a program synthesis problem over\nsmall 2D images using a few input-output training pairs. In this work, we adopt\nthe recently popular data-driven approach to the ARC and ask whether a Vision\nTransformer (ViT) can learn the implicit mapping, from input image to output\nimage, that underlies the task. We show that a ViT -- otherwise a\nstate-of-the-art model for images -- fails dramatically on most ARC tasks even\nwhen trained on one million examples per task. This points to an inherent\nrepresentational deficiency of the ViT architecture that makes it incapable of\nuncovering the simple structured mappings underlying the ARC tasks. Building on\nthese insights, we propose ViTARC, a ViT-style architecture that unlocks some\nof the visual reasoning capabilities required by the ARC. Specifically, we use\na pixel-level input representation, design a spatially-aware tokenization\nscheme, and introduce a novel object-based positional encoding that leverages\nautomatic segmentation, among other enhancements. Our task-specific ViTARC\nmodels achieve a test solve rate close to 100% on more than half of the 400\npublic ARC tasks strictly through supervised learning from input-output grids.\nThis calls attention to the importance of imbuing the powerful (Vision)\nTransformer with the correct inductive biases for abstract visual reasoning\nthat are critical even when the training data is plentiful and the mapping is\nnoise-free. Hence, ViTARC provides a strong foundation for future research in\nvisual reasoning using transformer-based architectures.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06405v1",
    "published_date": "2024-10-08 22:25:34 UTC",
    "updated_date": "2024-10-08 22:25:34 UTC"
  },
  {
    "arxiv_id": "2410.06395v1",
    "title": "Multimodal Representation Learning using Adaptive Graph Construction",
    "authors": [
      "Weichen Huang"
    ],
    "abstract": "Multimodal contrastive learning train neural networks by levergaing data from\nheterogeneous sources such as images and text. Yet, many current multimodal\nlearning architectures cannot generalize to an arbitrary number of modalities\nand need to be hand-constructed. We propose AutoBIND, a novel contrastive\nlearning framework that can learn representations from an arbitrary number of\nmodalites through graph optimization. We evaluate AutoBIND on Alzhiemer's\ndisease detection because it has real-world medical applicability and it\ncontains a broad range of data modalities. We show that AutoBIND outperforms\nprevious methods on this task, highlighting the generalizablility of the\napproach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06395v1",
    "published_date": "2024-10-08 21:57:46 UTC",
    "updated_date": "2024-10-08 21:57:46 UTC"
  },
  {
    "arxiv_id": "2410.07265v1",
    "title": "A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models",
    "authors": [
      "Cong Guo",
      "Feng Cheng",
      "Zhixu Du",
      "James Kiessling",
      "Jonathan Ku",
      "Shiyu Li",
      "Ziru Li",
      "Mingyuan Ma",
      "Tergel Molom-Ochir",
      "Benjamin Morris",
      "Haoxuan Shan",
      "Jingwei Sun",
      "Yitu Wang",
      "Chiyue Wei",
      "Xueying Wu",
      "Yuhao Wu",
      "Hao Frank Yang",
      "Jingyang Zhang",
      "Junyao Zhang",
      "Qilin Zheng",
      "Guanglei Zhou",
      "Hai",
      "Li",
      "Yiran Chen"
    ],
    "abstract": "The rapid development of large language models (LLMs) has significantly\ntransformed the field of artificial intelligence, demonstrating remarkable\ncapabilities in natural language processing and moving towards multi-modal\nfunctionality. These models are increasingly integrated into diverse\napplications, impacting both research and industry. However, their development\nand deployment present substantial challenges, including the need for extensive\ncomputational resources, high energy consumption, and complex software\noptimizations. Unlike traditional deep learning systems, LLMs require unique\noptimization strategies for training and inference, focusing on system-level\nefficiency. This paper surveys hardware and software co-design approaches\nspecifically tailored to address the unique characteristics and constraints of\nlarge language models. This survey analyzes the challenges and impacts of LLMs\non hardware and algorithm research, exploring algorithm optimization, hardware\ndesign, and system-level innovations. It aims to provide a comprehensive\nunderstanding of the trade-offs and considerations in LLM-centric computing\nsystems, guiding future advancements in AI. Finally, we summarize the existing\nefforts in this space and outline future directions toward realizing\nproduction-grade co-design methodologies for the next generation of large\nlanguage models and AI systems.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted by IEEE Circuits and Systems Magazine",
    "pdf_url": "http://arxiv.org/pdf/2410.07265v1",
    "published_date": "2024-10-08 21:46:52 UTC",
    "updated_date": "2024-10-08 21:46:52 UTC"
  },
  {
    "arxiv_id": "2410.06385v2",
    "title": "Skin Cancer Machine Learning Model Tone Bias",
    "authors": [
      "James Pope",
      "Md Hassanuzzaman",
      "William Chapman",
      "Huw Day",
      "Mingmar Sherpa",
      "Omar Emara",
      "Nirmala Adhikari",
      "Ayush Joshi"
    ],
    "abstract": "Background: Many open-source skin cancer image datasets are the result of\nclinical trials conducted in countries with lighter skin tones. Due to this\ntone imbalance, machine learning models derived from these datasets can perform\nwell at detecting skin cancer for lighter skin tones. Any tone bias in these\nmodels could introduce fairness concerns and reduce public trust in the\nartificial intelligence health field.\n  Methods: We examine a subset of images from the International Skin Imaging\nCollaboration (ISIC) archive that provide tone information. The subset has a\nsignificant tone imbalance. These imbalances could explain a model's tone bias.\nTo address this, we train models using the imbalanced dataset and a balanced\ndataset to compare against. The datasets are used to train a deep convolutional\nneural network model to classify the images as malignant or benign. We then\nevaluate the models' disparate impact, based on selection rate, relative to\ndark or light skin tone.\n  Results: Using the imbalanced dataset, we found that the model is\nsignificantly better at detecting malignant images in lighter tone resulting in\na disparate impact of 0.577. Using the balanced dataset, we found that the\nmodel is also significantly better at detecting malignant images in lighter\nversus darker tones with a disparate impact of 0.684. Using the imbalanced or\nbalanced dataset to train the model still results in a disparate impact well\nbelow the standard threshold of 0.80 which suggests the model is biased with\nrespect to skin tone.\n  Conclusion: The results show that typical skin cancer machine learning models\ncan be tone biased. These results provide evidence that diagnosis or tone\nimbalance is not the cause of the bias. Other techniques will be necessary to\nidentify and address the bias in these models, an area of future investigation.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06385v2",
    "published_date": "2024-10-08 21:33:02 UTC",
    "updated_date": "2025-03-19 13:12:12 UTC"
  },
  {
    "arxiv_id": "2410.06384v1",
    "title": "Validation of the Scientific Literature via Chemputation Augmented by Large Language Models",
    "authors": [
      "Sebastian Pagel",
      "Michael Jirasek",
      "Leroy Cronin"
    ],
    "abstract": "Chemputation is the process of programming chemical robots to do experiments\nusing a universal symbolic language, but the literature can be error prone and\nhard to read due to ambiguities. Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains, including natural language\nprocessing, robotic control, and more recently, chemistry. Despite significant\nadvancements in standardizing the reporting and collection of synthetic\nchemistry data, the automatic reproduction of reported syntheses remains a\nlabour-intensive task. In this work, we introduce an LLM-based chemical\nresearch agent workflow designed for the automatic validation of synthetic\nliterature procedures. Our workflow can autonomously extract synthetic\nprocedures and analytical data from extensive documents, translate these\nprocedures into universal XDL code, simulate the execution of the procedure in\na hardware-specific setup, and ultimately execute the procedure on an\nXDL-controlled robotic system for synthetic chemistry. This demonstrates the\npotential of LLM-based workflows for autonomous chemical synthesis with\nChemputers. Due to the abstraction of XDL this approach is safe, secure, and\nscalable since hallucinations will not be chemputable and the XDL can be both\nverified and encrypted. Unlike previous efforts, which either addressed only a\nlimited portion of the workflow, relied on inflexible hard-coded rules, or\nlacked validation in physical systems, our approach provides four realistic\nexamples of syntheses directly executed from synthetic literature. We\nanticipate that our workflow will significantly enhance automation in\nrobotically driven synthetic chemistry research, streamline data extraction,\nimprove the reproducibility, scalability, and safety of synthetic and\nexperimental chemistry.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 7 figures, 34 references",
    "pdf_url": "http://arxiv.org/pdf/2410.06384v1",
    "published_date": "2024-10-08 21:31:42 UTC",
    "updated_date": "2024-10-08 21:31:42 UTC"
  },
  {
    "arxiv_id": "2410.06378v1",
    "title": "Covering Numbers for Deep ReLU Networks with Applications to Function Approximation and Nonparametric Regression",
    "authors": [
      "Weigutian Ou",
      "Helmut Bölcskei"
    ],
    "abstract": "Covering numbers of families of (deep) ReLU networks have been used to\ncharacterize their approximation-theoretic performance, upper-bound the\nprediction error they incur in nonparametric regression, and quantify their\nclassification capacity. These results are based on covering number upper\nbounds obtained through the explicit construction of coverings. Lower bounds on\ncovering numbers do not seem to be available in the literature. The present\npaper fills this gap by deriving tight (up to a multiplicative constant) lower\nand upper bounds on the covering numbers of fully-connected networks with\nbounded weights, sparse networks with bounded weights, and fully-connected\nnetworks with quantized weights. Thanks to the tightness of the bounds, a\nfundamental understanding of the impact of sparsity, quantization, bounded vs.\nunbounded weights, and network output truncation can be developed. Furthermore,\nthe bounds allow to characterize the fundamental limits of neural network\ntransformation, including network compression, and lead to sharp upper bounds\non the prediction error in nonparametric regression through deep networks.\nSpecifically, we can remove a $\\log^6(n)$-factor in the best-known sample\ncomplexity rate in the estimation of Lipschitz functions through deep networks\nthereby establishing optimality. Finally, we identify a systematic relation\nbetween optimal nonparametric regression and optimal approximation through deep\nnetworks, unifying numerous results in the literature and uncovering general\nunderlying principles.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "68T07, 41A25, 62G08"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06378v1",
    "published_date": "2024-10-08 21:23:14 UTC",
    "updated_date": "2024-10-08 21:23:14 UTC"
  },
  {
    "arxiv_id": "2410.18109v1",
    "title": "NaVIP: An Image-Centric Indoor Navigation Solution for Visually Impaired People",
    "authors": [
      "Jun Yu",
      "Yifan Zhang",
      "Badrinadh Aila",
      "Vinod Namboodiri"
    ],
    "abstract": "Indoor navigation is challenging due to the absence of satellite positioning.\nThis challenge is manifold greater for Visually Impaired People (VIPs) who lack\nthe ability to get information from wayfinding signage. Other sensor signals\n(e.g., Bluetooth and LiDAR) can be used to create turn-by-turn navigation\nsolutions with position updates for users. Unfortunately, these solutions\nrequire tags to be installed all around the environment or the use of fairly\nexpensive hardware. Moreover, these solutions require a high degree of manual\ninvolvement that raises costs, thus hampering scalability. We propose an image\ndataset and associated image-centric solution called NaVIP towards visual\nintelligence that is infrastructure-free and task-scalable, and can assist VIPs\nin understanding their surroundings. Specifically, we start by curating\nlarge-scale phone camera data in a four-floor research building, with 300K\nimages, to lay the foundation for creating an image-centric indoor navigation\nand exploration solution for inclusiveness. Every image is labelled with\nprecise 6DoF camera poses, details of indoor PoIs, and descriptive captions to\nassist VIPs. We benchmark on two main aspects: 1) positioning system and 2)\nexploration support, prioritizing training scalability and real-time inference,\nto validate the prospect of image-based solution towards indoor navigation. The\ndataset, code, and model checkpoints are made publicly available at\nhttps://github.com/junfish/VIP_Navi.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "40 pages, 20 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.18109v1",
    "published_date": "2024-10-08 21:16:50 UTC",
    "updated_date": "2024-10-08 21:16:50 UTC"
  },
  {
    "arxiv_id": "2410.06372v2",
    "title": "Cooperative and Asynchronous Transformer-based Mission Planning for Heterogeneous Teams of Mobile Robots",
    "authors": [
      "Milad Farjadnasab",
      "Shahin Sirouspour"
    ],
    "abstract": "Cooperative mission planning for heterogeneous teams of mobile robots\npresents a unique set of challenges, particularly when operating under\ncommunication constraints and limited computational resources. To address these\nchallenges, we propose the Cooperative and Asynchronous Transformer-based\nMission Planning (CATMiP) framework, which leverages multi-agent reinforcement\nlearning (MARL) to coordinate distributed decision making among agents with\ndiverse sensing, motion, and actuation capabilities, operating under sporadic\nad hoc communication. A Class-based Macro-Action Decentralized Partially\nObservable Markov Decision Process (CMacDec-POMDP) is also formulated to\neffectively model asynchronous decision-making for heterogeneous teams of\nagents. The framework utilizes an asynchronous centralized training and\ndistributed execution scheme that is developed based on the Multi-Agent\nTransformer (MAT) architecture. This design allows a single trained model to\ngeneralize to larger environments and accommodate varying team sizes and\ncompositions. We evaluate CATMiP in a 2D grid-world simulation environment and\ncompare its performance against planning-based exploration methods. Results\ndemonstrate CATMiP's superior efficiency, scalability, and robustness to\ncommunication dropouts, highlighting its potential for real-world heterogeneous\nmobile robot systems. The code is available at\nhttps://github.com/mylad13/CATMiP.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.9; I.2.11"
    ],
    "primary_category": "cs.RO",
    "comment": "27 pages, 8 figures, this work has been submitted to Elsevier for\n  possible publication",
    "pdf_url": "http://arxiv.org/pdf/2410.06372v2",
    "published_date": "2024-10-08 21:14:09 UTC",
    "updated_date": "2025-01-14 22:43:44 UTC"
  },
  {
    "arxiv_id": "2410.06370v2",
    "title": "HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid",
    "authors": [
      "Hemank Lamba",
      "Anton Abilov",
      "Ke Zhang",
      "Elizabeth M. Olson",
      "Henry k. Dambanemuya",
      "João c. Bárcia",
      "David S. Batista",
      "Christina Wille",
      "Aoife Cahill",
      "Joel Tetreault",
      "Alex Jaimes"
    ],
    "abstract": "Humanitarian organizations can enhance their effectiveness by analyzing data\nto discover trends, gather aggregated insights, manage their security risks,\nsupport decision-making, and inform advocacy and funding proposals. However,\ndata about violent incidents with direct impact and relevance for humanitarian\naid operations is not readily available. An automatic data collection and\nNLP-backed classification framework aligned with humanitarian perspectives can\nhelp bridge this gap. In this paper, we present HumVI - a dataset comprising\nnews articles in three languages (English, French, Arabic) containing instances\nof different types of violent incidents categorized by the humanitarian sector\nthey impact, e.g., aid security, education, food security, health, and\nprotection. Reliable labels were obtained for the dataset by partnering with a\ndata-backed humanitarian organization, Insecurity Insight. We provide multiple\nbenchmarks for the dataset, employing various deep learning architectures and\ntechniques, including data augmentation and mask loss, to address different\ntask-related challenges, e.g., domain expansion. The dataset is publicly\navailable at https://github.com/dataminr-ai/humvi-dataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06370v2",
    "published_date": "2024-10-08 21:08:13 UTC",
    "updated_date": "2024-10-15 20:23:13 UTC"
  },
  {
    "arxiv_id": "2410.06366v1",
    "title": "Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling",
    "authors": [
      "Zijie Huang",
      "Wanjia Zhao",
      "Jingdong Gao",
      "Ziniu Hu",
      "Xiao Luo",
      "Yadi Cao",
      "Yuanzhou Chen",
      "Yizhou Sun",
      "Wei Wang"
    ],
    "abstract": "Learning complex physical dynamics purely from data is challenging due to the\nintrinsic properties of systems to be satisfied. Incorporating physics-informed\npriors, such as in Hamiltonian Neural Networks (HNNs), achieves high-precision\nmodeling for energy-conservative systems. However, real-world systems often\ndeviate from strict energy conservation and follow different physical priors.\nTo address this, we present a framework that achieves high-precision modeling\nfor a wide range of dynamical systems from the numerical aspect, by enforcing\nTime-Reversal Symmetry (TRS) via a novel regularization term. It helps preserve\nenergies for conservative systems while serving as a strong inductive bias for\nnon-conservative, reversible systems. While TRS is a domain-specific physical\nprior, we present the first theoretical proof that TRS loss can universally\nimprove modeling accuracy by minimizing higher-order Taylor terms in ODE\nintegration, which is numerically beneficial to various systems regardless of\ntheir properties, even for irreversible systems. By integrating the TRS loss\nwithin neural ordinary differential equation models, the proposed model TREAT\ndemonstrates superior performance on diverse physical systems. It achieves a\nsignificant 11.5% MSE improvement in a challenging chaotic triple-pendulum\nscenario, underscoring TREAT's broad applicability and effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to The Thirty-eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.06366v1",
    "published_date": "2024-10-08 21:04:01 UTC",
    "updated_date": "2024-10-08 21:04:01 UTC"
  },
  {
    "arxiv_id": "2410.10872v1",
    "title": "ToolBridge: An Open-Source Dataset to Equip LLMs with External Tool Capabilities",
    "authors": [
      "Zhenchao Jin",
      "Mengchen Liu",
      "Dongdong Chen",
      "Lingting Zhu",
      "Yunsheng Li",
      "Lequan Yu"
    ],
    "abstract": "Through the integration of external tools, large language models (LLMs) such\nas GPT-4o and Llama 3.1 significantly expand their functional capabilities,\nevolving from elementary conversational agents to general-purpose assistants.\nWe argue that the primary drivers of these advancements are the quality and\ndiversity of the training data. However, the existing LLMs with external tool\nintegration provide only limited transparency regarding their datasets and data\ncollection methods, which has led to the initiation of this research.\nSpecifically, in this paper, our objective is to elucidate the detailed process\ninvolved in constructing datasets that empower LLMs to effectively learn how to\nutilize external tools and make this information available to the public\nthrough the introduction of ToolBridge. ToolBridge proposes to employ a\ncollection of general open-access datasets as its raw dataset pool and applies\na series of strategies to identify appropriate data entries from the pool for\nexternal tool API insertions. By supervised fine-tuning on these curated data\nentries, LLMs can invoke external tools in appropriate contexts to boost their\npredictive accuracy, particularly for basic functions including data\nprocessing, numerical computation, and factual retrieval. Our experiments\nrigorously isolates model architectures and training configurations, focusing\nexclusively on the role of data. The experimental results indicate that LLMs\ntrained on ToolBridge demonstrate consistent performance improvements on both\nstandard benchmarks and custom evaluation datasets. All the associated code and\ndata will be open-source at https://github.com/CharlesPikachu/ToolBridge,\npromoting transparency and facilitating the broader community to explore\napproaches for equipping LLMs with external tools capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "technical report",
    "pdf_url": "http://arxiv.org/pdf/2410.10872v1",
    "published_date": "2024-10-08 20:54:40 UTC",
    "updated_date": "2024-10-08 20:54:40 UTC"
  },
  {
    "arxiv_id": "2410.06355v2",
    "title": "Context-Aware Command Understanding for Tabletop Scenarios",
    "authors": [
      "Paul Gajewski",
      "Antonio Galiza Cerdeira Gonzalez",
      "Bipin Indurkhya"
    ],
    "abstract": "This paper presents a novel hybrid algorithm designed to interpret natural\nhuman commands in tabletop scenarios. By integrating multiple sources of\ninformation, including speech, gestures, and scene context, the system extracts\nactionable instructions for a robot, identifying relevant objects and actions.\nThe system operates in a zero-shot fashion, without reliance on predefined\nobject models, enabling flexible and adaptive use in various environments. We\nassess the integration of multiple deep learning models, evaluating their\nsuitability for deployment in real-world robotic setups. Our algorithm performs\nrobustly across different tasks, combining language processing with visual\ngrounding. In addition, we release a small dataset of video recordings used to\nevaluate the system. This dataset captures real-world interactions in which a\nhuman provides instructions in natural language to a robot, a contribution to\nfuture research on human-robot interaction. We discuss the strengths and\nlimitations of the system, with particular focus on how it handles multimodal\ncommand interpretation, and its ability to be integrated into symbolic robotic\nframeworks for safe and explainable decision-making.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06355v2",
    "published_date": "2024-10-08 20:46:39 UTC",
    "updated_date": "2024-10-10 10:59:22 UTC"
  },
  {
    "arxiv_id": "2410.06347v1",
    "title": "Solving Multi-Goal Robotic Tasks with Decision Transformer",
    "authors": [
      "Paul Gajewski",
      "Dominik Żurek",
      "Marcin Pietroń",
      "Kamil Faber"
    ],
    "abstract": "Artificial intelligence plays a crucial role in robotics, with reinforcement\nlearning (RL) emerging as one of the most promising approaches for robot\ncontrol. However, several key challenges hinder its broader application. First,\nmany RL methods rely on online learning, which requires either real-world\nhardware or advanced simulation environments--both of which can be costly,\ntime-consuming, and impractical. Offline reinforcement learning offers a\nsolution, enabling models to be trained without ongoing access to physical\nrobots or simulations.\n  A second challenge is learning multi-goal tasks, where robots must achieve\nmultiple objectives simultaneously. This adds complexity to the training\nprocess, as the model must generalize across different goals. At the same time,\ntransformer architectures have gained significant popularity across various\ndomains, including reinforcement learning. Yet, no existing methods effectively\ncombine offline training, multi-goal learning, and transformer-based\narchitectures.\n  In this paper, we address these challenges by introducing a novel adaptation\nof the decision transformer architecture for offline multi-goal reinforcement\nlearning in robotics. Our approach integrates goal-specific information into\nthe decision transformer, allowing it to handle complex tasks in an offline\nsetting. To validate our method, we developed a new offline reinforcement\nlearning dataset using the Panda robotic platform in simulation. Our extensive\nexperiments demonstrate that the decision transformer can outperform\nstate-of-the-art online reinforcement learning methods.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06347v1",
    "published_date": "2024-10-08 20:35:30 UTC",
    "updated_date": "2024-10-08 20:35:30 UTC"
  },
  {
    "arxiv_id": "2410.06332v2",
    "title": "Boolean Nearest Neighbor Language in the Knowledge Compilation Map",
    "authors": [
      "Ondřej Čepek",
      "Jelena Glišić"
    ],
    "abstract": "The Boolean Nearest Neighbor (BNN) representation of Boolean functions was\nrecently introduced by Hajnal, Liu and Turan. A BNN representation of $f$ is a\npair $(P,N)$ of sets of Boolean vectors (called positive and negative\nprototypes) where $f(x)=1$ for every positive prototype $x \\in P$, $f(x)=0$ for\nall every negative prototype $x \\in N$, and the value $f(x)$ for $x \\not\\in P\n\\cup N$ is determined by the type of the closest prototype. The main aim of\nthis paper is to determine the position of the BNN language in the Knowledge\nCompilation Map (KCM). To this end, we derive results which compare the\nsuccinctness of the BNN language to several standard languages from KCM, and\ndetermine the complexity status of most standard queries and transformations\nfor BNN inputs.",
    "categories": [
      "cs.AI",
      "68T30",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 5 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.06332v2",
    "published_date": "2024-10-08 20:12:55 UTC",
    "updated_date": "2024-10-28 18:20:11 UTC"
  },
  {
    "arxiv_id": "2410.06331v3",
    "title": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing",
    "authors": [
      "Zhuoran Zhang",
      "Yongxiang Li",
      "Zijian Kan",
      "Keyuan Cheng",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve knowledge with implicit subject\ninformation from deeper MLP layers, unlike single-hop tasks, which rely on\nshallow layers. This distinction explains the poor performance of current\nmethods in multi-hop queries, as they primarily focus on editing shallow layers\nwith single-hop edit prompts, leaving deeper layers unchanged. To address this,\nwe propose IFMET, a novel locate-then-edit KE approach designed to edit both\nshallow and deep MLP layers. Beyond single-hop editing prompts, IFMET further\nincorporates multi-hop editing prompts to locate and modify knowledge across\ndifferent stages of reasoning. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\novercoming the limitations of previous locate-then-edit methods",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.06331v3",
    "published_date": "2024-10-08 20:12:11 UTC",
    "updated_date": "2025-02-01 12:21:59 UTC"
  },
  {
    "arxiv_id": "2410.06328v2",
    "title": "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework",
    "authors": [
      "Krishna Aswani",
      "Huilin Lu",
      "Pranav Patankar",
      "Priya Dhalwani",
      "Iris Tan",
      "Jayant Ganeshmohan",
      "Simon Lacasse"
    ],
    "abstract": "Recent advancements in prompt engineering strategies, such as\nChain-of-Thought (CoT) and Self-Discover, have demonstrated significant\npotential in improving the reasoning abilities of Large Language Models (LLMs).\nHowever, these state-of-the-art (SOTA) prompting strategies rely on single or\nfixed set of static seed reasoning modules like \"think step by step\" or \"break\ndown this problem\" intended to simulate human approach to problem-solving. This\nconstraint limits the flexibility of models in tackling diverse problems\neffectively. In this paper, we introduce Auto-Evolve, a novel framework that\nenables LLMs to self-create dynamic reasoning modules and downstream action\nplan, resulting in significant improvements over current SOTA methods. We\nevaluate Auto-Evolve on the challenging BigBench-Hard (BBH) dataset with Claude\n2.0, Claude 3 Sonnet, Mistral Large, and GPT 4, where it consistently\noutperforms the SOTA prompt strategies. Auto-Evolve outperforms CoT by up to\n10.4% and on an average by 7% across these four models. Our framework\nintroduces two innovations: a) Auto-Evolve dynamically generates reasoning\nmodules for each task while aligning with human reasoning paradigm, thus\neliminating the need for predefined templates. b) We introduce an iterative\nrefinement component, that incrementally refines instruction guidance for LLMs\nand helps boost performance by average 2.8% compared to doing it in a single\nstep.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.06328v2",
    "published_date": "2024-10-08 20:07:47 UTC",
    "updated_date": "2024-10-11 20:39:00 UTC"
  },
  {
    "arxiv_id": "2410.06317v1",
    "title": "Learning in complex action spaces without policy gradients",
    "authors": [
      "Arash Tavakoli",
      "Sina Ghiassian",
      "Nemanja Rakićević"
    ],
    "abstract": "Conventional wisdom suggests that policy gradient methods are better suited\nto complex action spaces than action-value methods. However, foundational\nstudies have shown equivalences between these paradigms in small and finite\naction spaces (O'Donoghue et al., 2017; Schulman et al., 2017a). This raises\nthe question of why their computational applicability and performance diverge\nas the complexity of the action space increases. We hypothesize that the\napparent superiority of policy gradients in such settings stems not from\nintrinsic qualities of the paradigm, but from universal principles that can\nalso be applied to action-value methods to serve similar functionality. We\nidentify three such principles and provide a framework for incorporating them\ninto action-value methods. To support our hypothesis, we instantiate this\nframework in what we term QMLE, for Q-learning with maximum likelihood\nestimation. Our results show that QMLE can be applied to complex action spaces\nwith a controllable computational cost that is comparable to that of policy\ngradient methods, all without using policy gradients. Furthermore, QMLE\ndemonstrates strong performance on the DeepMind Control Suite, even when\ncompared to the state-of-the-art methods such as DMPO and D4PG.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06317v1",
    "published_date": "2024-10-08 19:49:34 UTC",
    "updated_date": "2024-10-08 19:49:34 UTC"
  },
  {
    "arxiv_id": "2410.06311v1",
    "title": "A Comparative Study of Hybrid Models in Health Misinformation Text Classification",
    "authors": [
      "Mkululi Sikosana",
      "Oluwaseun Ajao",
      "Sean Maudsley-Barton"
    ],
    "abstract": "This study evaluates the effectiveness of machine learning (ML) and deep\nlearning (DL) models in detecting COVID-19-related misinformation on online\nsocial networks (OSNs), aiming to develop more effective tools for countering\nthe spread of health misinformation during the pan-demic. The study trained and\ntested various ML classifiers (Naive Bayes, SVM, Random Forest, etc.), DL\nmodels (CNN, LSTM, hybrid CNN+LSTM), and pretrained language models\n(DistilBERT, RoBERTa) on the \"COVID19-FNIR DATASET\". These models were\nevaluated for accuracy, F1 score, recall, precision, and ROC, and used\npreprocessing techniques like stemming and lemmatization. The results showed\nSVM performed well, achieving a 94.41% F1-score. DL models with Word2Vec\nembeddings exceeded 98% in all performance metrics (accuracy, F1 score, recall,\nprecision & ROC). The CNN+LSTM hybrid models also exceeded 98% across\nperformance metrics, outperforming pretrained models like DistilBERT and\nRoBERTa. Our study concludes that DL and hybrid DL models are more effective\nthan conventional ML algorithms for detecting COVID-19 misinformation on OSNs.\nThe findings highlight the importance of advanced neural network approaches and\nlarge-scale pretraining in misinformation detection. Future research should\noptimize these models for various misinformation types and adapt to changing\nOSNs, aiding in combating health misinformation.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "8 pages, 4 tables presented at the OASIS workshop of the ACM\n  Hypertext and Social Media Conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.06311v1",
    "published_date": "2024-10-08 19:43:37 UTC",
    "updated_date": "2024-10-08 19:43:37 UTC"
  },
  {
    "arxiv_id": "2410.18107v1",
    "title": "In-Context Code-Text Learning for Bimodal Software Engineering",
    "authors": [
      "Xunzhu Tang",
      "Liran Wang",
      "Yonghui Liu",
      "Linzheng Chai",
      "Jian Yang",
      "Zhoujun Li",
      "Haoye Tian",
      "Jacques Klein",
      "Tegawende F. Bissyande"
    ],
    "abstract": "Bimodal software analysis initially appeared to be within reach with the\nadvent of large language models. Unfortunately, the complex interplay of\nnatural language text and code in software engineering, presents unique\nchallenges that prevent pretrained models to generalize to a variety of tasks.\nWe postulate that in-context learning for the code-text bimodality is a\npromising avenue. This paper thus introduces a comprehensive study of\nin-context code-text learning, focusing on leveraging pretrained CodeLLAMA\nmodels.\n  We consider a diverse dataset encompassing 23 software engineering tasks,\nwhich we transform in an in-context learning format. To effectively extract\ninformative features, we propose a configurable prompt template. Our proposed\npipeline, InCTRL, then unifies prompt learning across various software\nengineering tasks. Extensive evaluation on the study datasets demonstrates the\nsuperiority of INCTRL-models in few-shot performance, surpassing\nstate-of-the-art models including the support model, CodeLLAMA. Typically, we\nobserve that applied to the CodeLLAMA model, INCTRL brings improvements in\nterms of precision (at least about 12\\%) and recall (up to 93.88\\%) on various\ntasks. For example, on the task of program repair, INCTRL improves the BLEU\nscore of CodeLLAMA by 85 points, while for clone detection, INCTRL achieves an\nimprovement of 69 percentage points. Moreover, INCTRL-models offer\nstate-of-the-art performance when using retrieval-augmented generation on\nindividual downstream tasks. Finally, we qualitatively analyze the benefits of\nINCTRL over CodeLLAMA and open-source all models for broader impact.\n  We make our code and dataset publicly available at: \\begin{center}\n  {\\url{https://anonymous.4open.science/r/inctrl-B65B}} \\end{center}",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18107v1",
    "published_date": "2024-10-08 19:42:00 UTC",
    "updated_date": "2024-10-08 19:42:00 UTC"
  },
  {
    "arxiv_id": "2410.06303v2",
    "title": "Compositional Risk Minimization",
    "authors": [
      "Divyat Mahajan",
      "Mohammad Pezeshki",
      "Charles Arnal",
      "Ioannis Mitliagkas",
      "Kartik Ahuja",
      "Pascal Vincent"
    ],
    "abstract": "Compositional generalization is a crucial step towards developing\ndata-efficient intelligent machines that generalize in human-like ways. In this\nwork, we tackle a challenging form of distribution shift, termed compositional\nshift, where some attribute combinations are completely absent at training but\npresent in the test distribution. This shift tests the model's ability to\ngeneralize compositionally to novel attribute combinations in discriminative\ntasks. We model the data with flexible additive energy distributions, where\neach energy term represents an attribute, and derive a simple alternative to\nempirical risk minimization termed compositional risk minimization (CRM). We\nfirst train an additive energy classifier to predict the multiple attributes\nand then adjust this classifier to tackle compositional shifts. We provide an\nextensive theoretical analysis of CRM, where we show that our proposal\nextrapolates to special affine hulls of seen attribute combinations. Empirical\nevaluations on benchmark datasets confirms the improved robustness of CRM\ncompared to other methods from the literature designed to tackle various forms\nof subpopulation shifts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under Review",
    "pdf_url": "http://arxiv.org/pdf/2410.06303v2",
    "published_date": "2024-10-08 19:25:07 UTC",
    "updated_date": "2025-02-10 23:16:02 UTC"
  },
  {
    "arxiv_id": "2410.06299v1",
    "title": "A Taxonomy of Collectible Card Games from a Game-Playing AI Perspective",
    "authors": [
      "Ronaldo e Silva Vieira",
      "Anderson Rocha Tavares",
      "Luiz Chaimowicz"
    ],
    "abstract": "Collectible card games are challenging, widely played games that have\nreceived increasing attention from the AI research community in recent years.\nDespite important breakthroughs, the field still poses many unresolved\nchallenges. This work aims to help further research on the genre by proposing a\ntaxonomy of collectible card games by analyzing their rules, mechanics, and\ngame modes from the perspective of game-playing AI research. To achieve this,\nwe studied a set of popular games and provided a thorough discussion about\ntheir characteristics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, accepted at the International Conference on Entertainment\n  Computing (ICEC) 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.06299v1",
    "published_date": "2024-10-08 19:04:12 UTC",
    "updated_date": "2024-10-08 19:04:12 UTC"
  },
  {
    "arxiv_id": "2410.06293v1",
    "title": "Accelerated Preference Optimization for Large Language Model Alignment",
    "authors": [
      "Jiafan He",
      "Huizhuo Yuan",
      "Quanquan Gu"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal\ntool for aligning large language models (LLMs) with human preferences. Direct\nPreference Optimization (DPO), one of the most popular approaches, formulates\nRLHF as a policy optimization problem without explicitly estimating the reward\nfunction. It overcomes the stability and efficiency issues of two-step\napproaches, which typically involve first estimating the reward function and\nthen optimizing the policy via proximal policy optimization (PPO). Since RLHF\nis essentially an optimization problem, and it is well-known that momentum\ntechniques can accelerate optimization both theoretically and empirically, a\nnatural question arises: Can RLHF be accelerated by momentum? This paper\nanswers this question in the affirmative. In detail, we first show that the\niterative preference optimization method can be viewed as a proximal point\nmethod. Based on this observation, we propose a general Accelerated Preference\nOptimization (APO) framework, which unifies many existing preference\noptimization algorithms and employs Nesterov's momentum technique to speed up\nthe alignment of LLMs. Theoretically, we demonstrate that APO can achieve a\nfaster convergence rate than the standard iterative preference optimization\nmethods, including DPO and Self-Play Preference Optimization (SPPO).\nEmpirically, we show the superiority of APO over DPO, iterative DPO, and other\nstrong baselines for RLHF on the AlpacaEval 2.0 benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "44 pages, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.06293v1",
    "published_date": "2024-10-08 18:51:01 UTC",
    "updated_date": "2024-10-08 18:51:01 UTC"
  },
  {
    "arxiv_id": "2410.06287v2",
    "title": "Non-Halting Queries: Exploiting Fixed Points in LLMs",
    "authors": [
      "Ghaith Hammouri",
      "Kemal Derya",
      "Berk Sunar"
    ],
    "abstract": "We introduce a new vulnerability that exploits fixed points in autoregressive\nmodels and use it to craft queries that never halt. More precisely, for\nnon-halting queries, the LLM never samples the end-of-string token <eos>. We\nrigorously analyze the conditions under which the non-halting anomaly presents\nitself. In particular, at temperature zero, we prove that if a repeating\n(cyclic) token sequence is observed at the output beyond the context size, then\nthe LLM does not halt.\n  We demonstrate non-halting queries in many experiments performed in base\nunaligned models where repeating prompts immediately lead to a non-halting\ncyclic behavior as predicted by the analysis. Further, we develop a simple\nrecipe that takes the same fixed points observed in the base model and creates\na prompt structure to target aligned models. We demonstrate the recipe's\nsuccess in sending every major model released over the past year into a\nnon-halting state with the same simple prompt even over higher temperatures.\nFurther, we devise an experiment with 100 randomly selected tokens and show\nthat the recipe to create non-halting queries succeeds with high success rates\nranging from 97% for GPT-4o to 19% for Gemini Pro 1.5. These results show that\nthe proposed adversarial recipe succeeds in bypassing alignment at one to two\norders of magnitude higher rates compared to earlier reports.\n  We also study gradient-based direct inversion using ARCA to craft new short\nprompts to induce the non-halting state. We inverted 10,000 random repeating\n2-cycle outputs for llama-3.1-8b-instruct. Out of 10,000 three-token inverted\nprompts 1,512 yield non-halting queries reaching a rate of 15%. Our experiments\nwith ARCA show that non-halting may be easily induced with as few as 3 input\ntokens with high probability. Overall, our experiments demonstrate that\nnon-halting queries are prevalent and relatively easy to find.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06287v2",
    "published_date": "2024-10-08 18:38:32 UTC",
    "updated_date": "2025-02-24 17:35:16 UTC"
  },
  {
    "arxiv_id": "2410.06277v4",
    "title": "Solving Functional Optimization with Deep Networks and Variational Principles",
    "authors": [
      "Kawisorn Kamtue",
      "Jose M. F. Moura",
      "Orathai Sangpetch"
    ],
    "abstract": "Can neural networks solve math problems using first a principle alone? This\npaper shows how to leverage the fundamental theorem of the calculus of\nvariations to design deep neural networks to solve functional optimization\nwithout requiring training data (e.g., ground-truth optimal solutions). Our\napproach is particularly crucial when the solution is a function defined over\nan unknown interval or support\\textemdash such as in minimum-time control\nproblems. By incorporating the necessary conditions satisfied by the optimal\nfunction solution, as derived from the calculus of variation, in the design of\nthe deep architecture, CalVNet leverages overparameterized neural networks to\nlearn these optimal functions directly. We validate CalVNet by showing that,\nwithout relying on ground-truth data and simply incorporating first principles,\nit successfully derives the Kalman filter for linear filtering, the bang-bang\noptimal control for minimum-time problems, and finds geodesics on manifolds.\nOur results demonstrate that CalVNet can be trained in an unsupervised manner,\nwithout relying on ground-truth data, establishing a promising framework for\naddressing general, potentially unsolved functional optimization problems that\nstill lack analytical solutions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.06277v4",
    "published_date": "2024-10-08 18:21:35 UTC",
    "updated_date": "2025-03-11 21:28:20 UTC"
  },
  {
    "arxiv_id": "2410.06273v1",
    "title": "PREDICT: Preference Reasoning by Evaluating Decomposed preferences Inferred from Candidate Trajectories",
    "authors": [
      "Stephane Aroca-Ouellette",
      "Natalie Mackraz",
      "Barry-John Theobald",
      "Katherine Metcalf"
    ],
    "abstract": "Accommodating human preferences is essential for creating AI agents that\ndeliver personalized and effective interactions. Recent work has shown the\npotential for LLMs to infer preferences from user interactions, but they often\nproduce broad and generic preferences, failing to capture the unique and\nindividualized nature of human preferences. This paper introduces PREDICT, a\nmethod designed to enhance the precision and adaptability of inferring\npreferences. PREDICT incorporates three key elements: (1) iterative refinement\nof inferred preferences, (2) decomposition of preferences into constituent\ncomponents, and (3) validation of preferences across multiple trajectories. We\nevaluate PREDICT on two distinct environments: a gridworld setting and a new\ntext-domain environment (PLUME). PREDICT more accurately infers nuanced human\npreferences improving over existing baselines by 66.2\\% (gridworld environment)\nand 41.0\\% (PLUME).",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06273v1",
    "published_date": "2024-10-08 18:16:41 UTC",
    "updated_date": "2024-10-08 18:16:41 UTC"
  },
  {
    "arxiv_id": "2410.06271v1",
    "title": "Probing the Robustness of Theory of Mind in Large Language Models",
    "authors": [
      "Christian Nickel",
      "Laura Schrewe",
      "Lucie Flek"
    ],
    "abstract": "With the success of ChatGPT and other similarly sized SotA LLMs, claims of\nemergent human like social reasoning capabilities, especially Theory of Mind\n(ToM), in these models have appeared in the scientific literature. On the one\nhand those ToM-capabilities have been successfully tested using tasks styled\nsimilar to those used in psychology (Kosinski, 2023). On the other hand, follow\nup studies showed that those capabilities vanished when the tasks were slightly\naltered (Ullman, 2023). In this work we introduce a novel dataset of 68 tasks\nfor probing ToM in LLMs, including potentially challenging variations which are\nassigned to 10 complexity classes. This way it is providing novel insights into\nthe challenges LLMs face with those task variations. We evaluate the ToM\nperformance of four SotA open source LLMs on our dataset and the dataset\nintroduced by (Kosinski, 2023). The overall low goal accuracy across all\nevaluated models indicates only a limited degree of ToM capabilities. The LLMs'\nperformance on simple complexity class tasks from both datasets are similar.\nWhereas we find a consistent tendency in all tested LLMs to perform poorly on\ntasks that require the realization that an agent has knowledge of automatic\nstate changes in its environment, even when those are spelled out to the model.\nFor task complications that change the relationship between objects by\nreplacing prepositions, we notice a performance drop in all models, with the\nstrongest impact on the mixture-of-experts model. With our dataset of tasks\ngrouped by complexity we offer directions for further research on how to\nstabilize and advance ToM capabilities in LLM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06271v1",
    "published_date": "2024-10-08 18:13:27 UTC",
    "updated_date": "2024-10-08 18:13:27 UTC"
  },
  {
    "arxiv_id": "2410.06264v2",
    "title": "Think While You Generate: Discrete Diffusion with Planned Denoising",
    "authors": [
      "Sulin Liu",
      "Juno Nam",
      "Andrew Campbell",
      "Hannes Stärk",
      "Yilun Xu",
      "Tommi Jaakkola",
      "Rafael Gómez-Bombarelli"
    ],
    "abstract": "Discrete diffusion has achieved state-of-the-art performance, outperforming\nor approaching autoregressive models on standard benchmarks. In this work, we\nintroduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework\nthat separates the generation process into two models: a planner and a\ndenoiser. At inference time, the planner selects which positions to denoise\nnext by identifying the most corrupted positions in need of denoising,\nincluding both initially corrupted and those requiring additional refinement.\nThis plan-and-denoise approach enables more efficient reconstruction during\ngeneration by iteratively identifying and denoising corruptions in the optimal\norder. DDPD outperforms traditional denoiser-only mask diffusion methods,\nachieving superior results on language modeling benchmarks such as text8,\nOpenWebText, and token-based image generation on ImageNet $256 \\times 256$.\nNotably, in language modeling, DDPD significantly reduces the performance gap\nbetween diffusion-based and autoregressive methods in terms of generative\nperplexity. Code is available at https://github.com/liusulin/DDPD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.06264v2",
    "published_date": "2024-10-08 18:03:34 UTC",
    "updated_date": "2025-04-10 01:26:11 UTC"
  },
  {
    "arxiv_id": "2410.06243v1",
    "title": "Unsupervised Model Diagnosis",
    "authors": [
      "Yinong Oliver Wang",
      "Eileen Li",
      "Jinqi Luo",
      "Zhaoning Wang",
      "Fernando De la Torre"
    ],
    "abstract": "Ensuring model explainability and robustness is essential for reliable\ndeployment of deep vision systems. Current methods for evaluating robustness\nrely on collecting and annotating extensive test sets. While this is common\npractice, the process is labor-intensive and expensive with no guarantee of\nsufficient coverage across attributes of interest. Recently, model diagnosis\nframeworks have emerged leveraging user inputs (e.g., text) to assess the\nvulnerability of the model. However, such dependence on human can introduce\nbias and limitation given the domain knowledge of particular users. This paper\nproposes Unsupervised Model Diagnosis (UMO), that leverages generative models\nto produce semantic counterfactual explanations without any user guidance.\nGiven a differentiable computer vision model (i.e., the target model), UMO\noptimizes for the most counterfactual directions in a generative latent space.\nOur approach identifies and visualizes changes in semantics, and then matches\nthese changes to attributes from wide-ranging text sources, such as\ndictionaries or language models. We validate the framework on multiple vision\ntasks (e.g., classification, segmentation, keypoint detection). Extensive\nexperiments show that our unsupervised discovery of semantic directions can\ncorrectly highlight spurious correlations and visualize the failure mode of\ntarget models without any human intervention.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 9 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.06243v1",
    "published_date": "2024-10-08 17:59:03 UTC",
    "updated_date": "2024-10-08 17:59:03 UTC"
  },
  {
    "arxiv_id": "2410.06240v1",
    "title": "Using Crank-Nikolson Scheme to Solve the Korteweg-de Vries (KdV) Equation",
    "authors": [
      "Qiming Wu"
    ],
    "abstract": "The Korteweg-de Vries (KdV) equation is a fundamental partial differential\nequation that models wave propagation in shallow water and other dispersive\nmedia. Accurately solving the KdV equation is essential for understanding wave\ndynamics in physics and engineering applications. This project focuses on\nimplementing the Crank-Nicolson scheme, a finite difference method known for\nits stability and accuracy, to solve the KdV equation. The Crank-Nicolson\nscheme's implicit nature allows for a more stable numerical solution,\nespecially in handling the dispersive and nonlinear terms of the KdV equation.\nWe investigate the performance of the scheme through various test cases,\nanalyzing its convergence and error behavior. The results demonstrate that the\nCrank-Nicolson method provides a robust approach for solving the KdV equation,\nwith improved accuracy over traditional explicit methods. Code is available at\nthe end of the paper.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06240v1",
    "published_date": "2024-10-08 17:54:20 UTC",
    "updated_date": "2024-10-08 17:54:20 UTC"
  },
  {
    "arxiv_id": "2410.06238v1",
    "title": "EVOLvE: Evaluating and Optimizing LLMs For Exploration",
    "authors": [
      "Allen Nie",
      "Yi Su",
      "Bo Chang",
      "Jonathan N. Lee",
      "Ed H. Chi",
      "Quoc V. Le",
      "Minmin Chen"
    ],
    "abstract": "Despite their success in many domains, large language models (LLMs) remain\nunder-studied in scenarios requiring optimal decision-making under uncertainty.\nThis is crucial as many real-world applications, ranging from personalized\nrecommendations to healthcare interventions, demand that LLMs not only predict\nbut also actively learn to make optimal decisions through exploration. In this\nwork, we measure LLMs' (in)ability to make optimal decisions in bandits, a\nstate-less reinforcement learning setting relevant to many applications. We\ndevelop a comprehensive suite of environments, including both context-free and\ncontextual bandits with varying task difficulties, to benchmark LLMs'\nperformance. Motivated by the existence of optimal exploration algorithms, we\npropose efficient ways to integrate this algorithmic knowledge into LLMs: by\nproviding explicit algorithm-guided support during inference; and through\nalgorithm distillation via in-context demonstrations and fine-tuning, using\nsynthetic data generated from these algorithms. Impressively, these techniques\nallow us to achieve superior exploration performance with smaller models,\nsurpassing larger models on various tasks. We conducted an extensive ablation\nstudy to shed light on various factors, such as task difficulty and data\nrepresentation, that influence the efficiency of LLM exploration. Additionally,\nwe conduct a rigorous analysis of the LLM's exploration efficiency using the\nconcept of regret, linking its ability to explore to the model size and\nunderlying algorithm.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.06238v1",
    "published_date": "2024-10-08 17:54:03 UTC",
    "updated_date": "2024-10-08 17:54:03 UTC"
  },
  {
    "arxiv_id": "2410.06237v1",
    "title": "BUMBLE: Unifying Reasoning and Acting with Vision-Language Models for Building-wide Mobile Manipulation",
    "authors": [
      "Rutav Shah",
      "Albert Yu",
      "Yifeng Zhu",
      "Yuke Zhu",
      "Roberto Martín-Martín"
    ],
    "abstract": "To operate at a building scale, service robots must perform very long-horizon\nmobile manipulation tasks by navigating to different rooms, accessing different\nfloors, and interacting with a wide and unseen range of everyday objects. We\nrefer to these tasks as Building-wide Mobile Manipulation. To tackle these\ninherently long-horizon tasks, we introduce BUMBLE, a unified Vision-Language\nModel (VLM)-based framework integrating open-world RGBD perception, a wide\nspectrum of gross-to-fine motor skills, and dual-layered memory. Our extensive\nevaluation (90+ hours) indicates that BUMBLE outperforms multiple baselines in\nlong-horizon building-wide tasks that require sequencing up to 12 ground truth\nskills spanning 15 minutes per trial. BUMBLE achieves 47.1% success rate\naveraged over 70 trials in different buildings, tasks, and scene layouts from\ndifferent starting rooms and floors. Our user study demonstrates 22% higher\nsatisfaction with our method than state-of-the-art mobile manipulation methods.\nFinally, we demonstrate the potential of using increasingly-capable foundation\nmodels to push performance further. For more information, see\nhttps://robin-lab.cs.utexas.edu/BUMBLE/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 Figures, 2 Tables, 11 Pages",
    "pdf_url": "http://arxiv.org/pdf/2410.06237v1",
    "published_date": "2024-10-08 17:52:29 UTC",
    "updated_date": "2024-10-08 17:52:29 UTC"
  },
  {
    "arxiv_id": "2410.06234v2",
    "title": "TEOChat: A Large Vision-Language Assistant for Temporal Earth Observation Data",
    "authors": [
      "Jeremy Andrew Irvin",
      "Emily Ruoyu Liu",
      "Joyce Chuyi Chen",
      "Ines Dormoy",
      "Jinyoung Kim",
      "Samar Khanna",
      "Zhuo Zheng",
      "Stefano Ermon"
    ],
    "abstract": "Large vision and language assistants have enabled new capabilities for\ninterpreting natural images. These approaches have recently been adapted to\nearth observation data, but they are only able to handle single image inputs,\nlimiting their use for many real-world tasks. In this work, we develop a new\nvision and language assistant called TEOChat that can engage in conversations\nabout temporal sequences of earth observation data. To train TEOChat, we curate\nan instruction-following dataset composed of many single image and temporal\ntasks including building change and damage assessment, semantic change\ndetection, and temporal scene classification. We show that TEOChat can perform\na wide variety of spatial and temporal reasoning tasks, substantially\noutperforming previous vision and language assistants, and even achieving\ncomparable or better performance than several specialist models trained to\nperform specific tasks. Furthermore, TEOChat achieves impressive zero-shot\nperformance on a change detection and change question answering dataset,\noutperforms GPT-4o and Gemini 1.5 Pro on multiple temporal tasks, and exhibits\nstronger single image capabilities than a comparable single image\ninstruction-following model on scene classification, visual question answering,\nand captioning. We publicly release our data, model, and code at\nhttps://github.com/ermongroup/TEOChat .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.06234v2",
    "published_date": "2024-10-08 17:45:51 UTC",
    "updated_date": "2025-01-27 01:45:15 UTC"
  },
  {
    "arxiv_id": "2410.06232v4",
    "title": "Range, not Independence, Drives Modularity in Biologically Inspired Representations",
    "authors": [
      "Will Dorrell",
      "Kyle Hsu",
      "Luke Hollingsworth",
      "Jin Hwa Lee",
      "Jiajun Wu",
      "Chelsea Finn",
      "Peter E Latham",
      "Tim EJ Behrens",
      "James CR Whittington"
    ],
    "abstract": "Why do biological and artificial neurons sometimes modularise, each encoding\na single meaningful variable, and sometimes entangle their representation of\nmany variables? In this work, we develop a theory of when biologically inspired\nnetworks -- those that are nonnegative and energy efficient -- modularise their\nrepresentation of source variables (sources). We derive necessary and\nsufficient conditions on a sample of sources that determine whether the neurons\nin an optimal biologically-inspired linear autoencoder modularise. Our theory\napplies to any dataset, extending far beyond the case of statistical\nindependence studied in previous work. Rather we show that sources modularise\nif their support is ``sufficiently spread''. From this theory, we extract and\nvalidate predictions in a variety of empirical studies on how data distribution\naffects modularisation in nonlinear feedforward and recurrent neural networks\ntrained on supervised and unsupervised tasks. Furthermore, we apply these ideas\nto neuroscience data, showing that range independence can be used to understand\nthe mixing or modularising of spatial and reward information in entorhinal\nrecordings in seemingly conflicting experiments. Further, we use these results\nto suggest alternate origins of mixed-selectivity, beyond the predominant\ntheory of flexible nonlinear classification. In sum, our theory prescribes\nprecise conditions on when neural activities modularise, providing tools for\ninducing and elucidating modular representations in brains and machines.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "q-bio.NC",
    "comment": "37 pages, 12 figures. WD and KH contributed equally; LH and JHL\n  contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2410.06232v4",
    "published_date": "2024-10-08 17:41:37 UTC",
    "updated_date": "2025-04-11 14:14:17 UTC"
  },
  {
    "arxiv_id": "2410.18105v1",
    "title": "Improving Embedding Accuracy for Document Retrieval Using Entity Relationship Maps and Model-Aware Contrastive Sampling",
    "authors": [
      "Thea Aviss"
    ],
    "abstract": "In this paper we present APEX-Embedding-7B (Advanced Processing for Epistemic\neXtraction), a 7-billion parameter decoder-only text Feature Extraction Model,\nspecifically designed for Document Retrieval-Augmented Generation (RAG) tasks.\nOur approach employs two training techniques that yield an emergent improvement\nin factual focus: (1) Pre-convergence interrupted fine-tuning using Structured\nEntity Relationship Maps as training data input: designed to shift the model's\nattention and create a bias towards factual content rather than semantic style\n- this enhances plain text performance despite not being directly trained for\nit; and (2) Model-Aware Contrastive Sampling, creating a balanced and evenly\ndistributed collation map of hard and soft negatives directly informed by the\nbase model's competency. This combined methodology yields significant\nimprovements, enhancing plain text query/document pair retrieval to achieve an\nabsolute rank@1 accuracy of 90.86% (an increase of 6.26% compared to the next\nleading model) in our evaluation, and reducing training data input context size\nby an average of 37.71% compared to plain text for both queries and document\ntexts. Based on our evaluations, our model establishes a new state-of-the-art\nstandard in text feature extraction for longer context document retrieval\ntasks.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "10 Pages, 9 Figures",
    "pdf_url": "http://arxiv.org/pdf/2410.18105v1",
    "published_date": "2024-10-08 17:36:48 UTC",
    "updated_date": "2024-10-08 17:36:48 UTC"
  },
  {
    "arxiv_id": "2410.06225v1",
    "title": "A Timeline and Analysis for Representation Plasticity in Large Language Models",
    "authors": [
      "Akshat Kannan"
    ],
    "abstract": "The ability to steer AI behavior is crucial to preventing its long term\ndangerous and catastrophic potential. Representation Engineering (RepE) has\nemerged as a novel, powerful method to steer internal model behaviors, such as\n\"honesty\", at a top-down level. Understanding the steering of representations\nshould thus be placed at the forefront of alignment initiatives. Unfortunately,\ncurrent efforts to understand plasticity at this level are highly neglected.\nThis paper aims to bridge the knowledge gap and understand how LLM\nrepresentation stability, specifically for the concept of \"honesty\", and model\nplasticity evolve by applying steering vectors extracted at different\nfine-tuning stages, revealing differing magnitudes of shifts in model behavior.\nThe findings are pivotal, showing that while early steering exhibits high\nplasticity, later stages have a surprisingly responsive critical window. This\npattern is observed across different model architectures, signaling that there\nis a general pattern of model plasticity that can be used for effective\nintervention. These insights greatly contribute to the field of AI\ntransparency, addressing a pressing lack of efficiency limiting our ability to\neffectively steer model behavior.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06225v1",
    "published_date": "2024-10-08 17:34:15 UTC",
    "updated_date": "2024-10-08 17:34:15 UTC"
  },
  {
    "arxiv_id": "2410.06215v3",
    "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback",
    "authors": [
      "Zaid Khan",
      "Elias Stengel-Eskin",
      "Jaemin Cho",
      "Mohit Bansal"
    ],
    "abstract": "The process of creating training data to teach models is currently driven by\nhumans, who manually analyze model weaknesses and plan how to create data that\nimproves a student model. Approaches using LLMs as annotators reduce human\neffort, but still require humans to interpret feedback from evaluations and\ncontrol the LLM to produce data the student needs. Automating this\nlabor-intensive process by creating autonomous data generation agents - or\nteachers - is desirable, but requires environments that can simulate the\nfeedback-driven, iterative, closed loop of data creation. To enable rapid,\nscalable testing for such agents and their modules, we introduce DataEnvGym, a\ntestbed of teacher environments for data generation agents. DataEnvGym frames\ndata generation as a sequential decision-making task, involving an agent\nconsisting of a data generation policy (which generates a plan for creating\ntraining data) and a data generation engine (which transforms the plan into\ndata), inside an environment that provides student feedback. The agent's goal\nis to improve student performance. Students are iteratively trained and\nevaluated on generated data, and their feedback (in the form of errors or weak\nskills) is reported to the agent after each iteration. DataEnvGym includes\nmultiple teacher environment instantiations across 3 levels of structure in the\nstate representation and action space. More structured environments are based\non inferred skills and offer more interpretability and curriculum control. We\nsupport 4 domains (math, code, VQA, and tool-use) and test multiple students\nand teachers. Example agents in our teaching environments can iteratively\nimprove students across tasks and settings. Moreover, we show that environments\nteach different skill levels and test variants of key modules, pointing to\nfuture work in improving data generation agents, engines, and feedback\nmechanisms.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025 Spotlight; Project Page: https://DataEnvGym.github.io",
    "pdf_url": "http://arxiv.org/pdf/2410.06215v3",
    "published_date": "2024-10-08 17:20:37 UTC",
    "updated_date": "2025-03-13 17:30:48 UTC"
  },
  {
    "arxiv_id": "2410.06209v8",
    "title": "LeanAgent: Lifelong Learning for Formal Theorem Proving",
    "authors": [
      "Adarsh Kumarappan",
      "Mo Tiwari",
      "Peiyang Song",
      "Robert Joseph George",
      "Chaowei Xiao",
      "Anima Anandkumar"
    ],
    "abstract": "Large Language Models (LLMs) have been successful in mathematical reasoning\ntasks such as formal theorem proving when integrated with interactive proof\nassistants like Lean. Existing approaches involve training or fine-tuning an\nLLM on a specific dataset to perform well on particular domains, such as\nundergraduate-level mathematics. These methods struggle with generalizability\nto advanced mathematics. A fundamental limitation is that these approaches\noperate on static domains, failing to capture how mathematicians often work\nacross multiple domains and projects simultaneously or cyclically. We present\nLeanAgent, a novel lifelong learning framework for formal theorem proving that\ncontinuously generalizes to and improves on ever-expanding mathematical\nknowledge without forgetting previously learned knowledge. LeanAgent introduces\nseveral key innovations, including a curriculum learning strategy that\noptimizes the learning trajectory in terms of mathematical difficulty, a\ndynamic database for efficient management of evolving mathematical knowledge,\nand progressive training to balance stability and plasticity. LeanAgent\nsuccessfully generates formal proofs for 155 theorems across 23 diverse Lean\nrepositories where formal proofs were previously missing, many from advanced\nmathematics. It performs significantly better than the static LLM baseline,\nproving challenging theorems in domains like abstract algebra and algebraic\ntopology while showcasing a clear progression of learning from basic concepts\nto advanced topics. In addition, we analyze LeanAgent's superior performance on\nkey lifelong learning metrics. LeanAgent achieves exceptional scores in\nstability and backward transfer, where learning new tasks improves performance\non previously learned tasks. This emphasizes LeanAgent's continuous\ngeneralizability and improvement, explaining its superior theorem-proving\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06209v8",
    "published_date": "2024-10-08 17:11:24 UTC",
    "updated_date": "2025-03-06 00:20:32 UTC"
  },
  {
    "arxiv_id": "2410.06203v1",
    "title": "Integrating Planning into Single-Turn Long-Form Text Generation",
    "authors": [
      "Yi Liang",
      "You Wu",
      "Honglei Zhuang",
      "Li Chen",
      "Jiaming Shen",
      "Yiling Jia",
      "Zhen Qin",
      "Sumit Sanghai",
      "Xuanhui Wang",
      "Carl Yang",
      "Michael Bendersky"
    ],
    "abstract": "Generating high-quality, in-depth textual documents, such as academic papers,\nnews articles, Wikipedia entries, and books, remains a significant challenge\nfor Large Language Models (LLMs). In this paper, we propose to use planning to\ngenerate long form content. To achieve our goal, we generate intermediate steps\nvia an auxiliary task that teaches the LLM to plan, reason and structure before\ngenerating the final text. Our main novelty lies in a single auxiliary task\nthat does not require multiple rounds of prompting or planning. To overcome the\nscarcity of training data for these intermediate steps, we leverage LLMs to\ngenerate synthetic intermediate writing data such as outlines, key information\nand summaries from existing full articles. Our experiments demonstrate on two\ndatasets from different domains, namely the scientific news dataset SciNews and\nWikipedia datasets in KILT-Wiki and FreshWiki, that LLMs fine-tuned with the\nauxiliary task generate higher quality documents. We observed +2.5% improvement\nin ROUGE-Lsum, and a strong 3.60 overall win/loss ratio via human SxS\nevaluation, with clear wins in organization, relevance, and verifiability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06203v1",
    "published_date": "2024-10-08 17:02:40 UTC",
    "updated_date": "2024-10-08 17:02:40 UTC"
  },
  {
    "arxiv_id": "2410.06195v3",
    "title": "EgoSocialArena: Benchmarking the Social Intelligence of Large Language Models from a First-person Perspective",
    "authors": [
      "Guiyang Hou",
      "Wenqi Zhang",
      "Yongliang Shen",
      "Zeqi Tan",
      "Sihao Shen",
      "Weiming Lu"
    ],
    "abstract": "Social intelligence is built upon three foundational pillars: cognitive\nintelligence, situational intelligence, and behavioral intelligence. As large\nlanguage models (LLMs) become increasingly integrated into our social lives,\nunderstanding, evaluating, and developing their social intelligence are\nbecoming increasingly important. While multiple existing works have\ninvestigated the social intelligence of LLMs, (1) most focus on a specific\naspect, and the social intelligence of LLMs has yet to be systematically\norganized and studied; (2) position LLMs as passive observers from a\nthird-person perspective, such as in Theory of Mind (ToM) tests. Compared to\nthe third-person perspective, ego-centric first-person perspective evaluation\ncan align well with actual LLM-based Agent use scenarios. (3) a lack of\ncomprehensive evaluation of behavioral intelligence, with specific emphasis on\nincorporating critical human-machine interaction scenarios. In light of this,\nwe present EgoSocialArena, a novel framework grounded in the three pillars of\nsocial intelligence: cognitive, situational, and behavioral intelligence, aimed\nto systematically evaluate the social intelligence of LLMs from a first-person\nperspective. With EgoSocialArena, we conduct a comprehensive evaluation of\neight prominent foundation models, even the most advanced LLMs like O1-preview\nlag behind human performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.06195v3",
    "published_date": "2024-10-08 16:55:51 UTC",
    "updated_date": "2025-02-24 02:22:39 UTC"
  },
  {
    "arxiv_id": "2410.06191v1",
    "title": "Benign Overfitting for Regression with Trained Two-Layer ReLU Networks",
    "authors": [
      "Junhyung Park",
      "Patrick Bloebaum",
      "Shiva Prasad Kasiviswanathan"
    ],
    "abstract": "We study the least-square regression problem with a two-layer fully-connected\nneural network, with ReLU activation function, trained by gradient flow. Our\nfirst result is a generalization result, that requires no assumptions on the\nunderlying regression function or the noise other than that they are bounded.\nWe operate in the neural tangent kernel regime, and our generalization result\nis developed via a decomposition of the excess risk into estimation and\napproximation errors, viewing gradient flow as an implicit regularizer. This\ndecomposition in the context of neural networks is a novel perspective of\ngradient descent, and helps us avoid uniform convergence traps. In this work,\nwe also establish that under the same setting, the trained network overfits to\nthe data. Together, these results, establishes the first result on benign\noverfitting for finite-width ReLU networks for arbitrary regression functions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "65 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.06191v1",
    "published_date": "2024-10-08 16:54:23 UTC",
    "updated_date": "2024-10-08 16:54:23 UTC"
  },
  {
    "arxiv_id": "2410.18104v1",
    "title": "ENWAR: A RAG-empowered Multi-Modal LLM Framework for Wireless Environment Perception",
    "authors": [
      "Ahmad M. Nazar",
      "Abdulkadir Celik",
      "Mohamed Y. Selim",
      "Asmaa Abdallah",
      "Daji Qiao",
      "Ahmed M. Eltawil"
    ],
    "abstract": "Large language models (LLMs) hold significant promise in advancing network\nmanagement and orchestration in 6G and beyond networks. However, existing LLMs\nare limited in domain-specific knowledge and their ability to handle\nmulti-modal sensory data, which is critical for real-time situational awareness\nin dynamic wireless environments. This paper addresses this gap by introducing\nENWAR, an ENvironment-aWARe retrieval augmented generation-empowered\nmulti-modal LLM framework. ENWAR seamlessly integrates multi-modal sensory\ninputs to perceive, interpret, and cognitively process complex wireless\nenvironments to provide human-interpretable situational awareness. ENWAR is\nevaluated on the GPS, LiDAR, and camera modality combinations of DeepSense6G\ndataset with state-of-the-art LLMs such as Mistral-7b/8x7b and\nLLaMa3.1-8/70/405b. Compared to general and often superficial environmental\ndescriptions of these vanilla LLMs, ENWAR delivers richer spatial analysis,\naccurately identifies positions, analyzes obstacles, and assesses line-of-sight\nbetween vehicles. Results show that ENWAR achieves key performance indicators\nof up to 70% relevancy, 55% context recall, 80% correctness, and 86%\nfaithfulness, demonstrating its efficacy in multi-modal perception and\ninterpretation.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18104v1",
    "published_date": "2024-10-08 16:26:18 UTC",
    "updated_date": "2024-10-08 16:26:18 UTC"
  },
  {
    "arxiv_id": "2410.06176v2",
    "title": "SC-Bench: A Large-Scale Dataset for Smart Contract Auditing",
    "authors": [
      "Shihao Xia",
      "Mengting He",
      "Linhai Song",
      "Yiying Zhang"
    ],
    "abstract": "There is a huge demand to ensure the compliance of smart contracts listed on\nblockchain platforms to safety and economic standards. Today, manual efforts in\nthe form of auditing are commonly used to achieve this goal. ML-based automated\ntechniques have the promise to alleviate human efforts and the resulting\nmonetary costs. However, unlike other domains where ML techniques have had huge\nsuccesses, no systematic ML techniques have been proposed or applied to smart\ncontract auditing. We present SC-Bench, the first dataset for automated\nsmart-contract auditing research. SC-Bench consists of 5,377 real-world smart\ncontracts running on Ethereum, a widely used blockchain platform, and 15,975\nviolations of standards on Ehereum called ERCs. Out of these violations, 139\nare real violations programmers made. The remaining are errors we\nsystematically injected to reflect the violations of different ERC rules. We\nevaluate SC-Bench using GPT-4 by prompting it with both the contracts and ERC\nrules. In addition, we manually identify each violated rule and the\ncorresponding code site (i.e., oracle) and prompt GPT-4 with the information\nasking for a True-or-False question. Our results show that without the oracle,\nGPT-4 can only detect 0.9% violations, and with the oracle, it detects 22.9%\nviolations. These results show the potential room for improvement in ML-based\ntechniques for smart-contract auditing.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06176v2",
    "published_date": "2024-10-08 16:23:50 UTC",
    "updated_date": "2025-02-08 03:58:02 UTC"
  },
  {
    "arxiv_id": "2410.06173v1",
    "title": "Manual Verbalizer Enrichment for Few-Shot Text Classification",
    "authors": [
      "Quang Anh Nguyen",
      "Nadi Tomeh",
      "Mustapha Lebbah",
      "Thierry Charnois",
      "Hanene Azzag",
      "Santiago Cordoba Muñoz"
    ],
    "abstract": "With the continuous development of pre-trained language models, prompt-based\ntraining becomes a well-adopted paradigm that drastically improves the\nexploitation of models for many natural language processing tasks. Prompting\nalso shows great performance compared to traditional fine-tuning when adapted\nto zero-shot or few-shot scenarios where the number of annotated data is\nlimited. In this framework, the role of verbalizers is essential, as an\ninterpretation from masked word distributions into output predictions. In this\nwork, we propose \\acrshort{mave}, an approach for verbalizer construction by\nenrichment of class labels using neighborhood relation in the embedding space\nof words for the text classification task. In addition, we elaborate a\nbenchmarking procedure to evaluate typical baselines of verbalizers for\ndocument classification in few-shot learning contexts. Our model achieves\nstate-of-the-art results while using significantly fewer resources. We show\nthat our approach is particularly effective in cases with extremely limited\nsupervision data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06173v1",
    "published_date": "2024-10-08 16:16:47 UTC",
    "updated_date": "2024-10-08 16:16:47 UTC"
  },
  {
    "arxiv_id": "2410.06172v2",
    "title": "Multimodal Situational Safety",
    "authors": [
      "Kaiwen Zhou",
      "Chengzhi Liu",
      "Xuandong Zhao",
      "Anderson Compalas",
      "Dawn Song",
      "Xin Eric Wang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating\nimpressive capabilities as multimodal assistants that interact with both humans\nand their environments. However, this increased sophistication introduces\nsignificant safety concerns. In this paper, we present the first evaluation and\nanalysis of a novel safety challenge termed Multimodal Situational Safety,\nwhich explores how safety considerations vary based on the specific situation\nin which the user or agent is engaged. We argue that for an MLLM to respond\nsafely, whether through language or action, it often needs to assess the safety\nimplications of a language query within its corresponding visual context. To\nevaluate this capability, we develop the Multimodal Situational Safety\nbenchmark (MSSBench) to assess the situational safety performance of current\nMLLMs. The dataset comprises 1,820 language query-image pairs, half of which\nthe image context is safe, and the other half is unsafe. We also develop an\nevaluation framework that analyzes key safety aspects, including explicit\nsafety reasoning, visual understanding, and, crucially, situational safety\nreasoning. Our findings reveal that current MLLMs struggle with this nuanced\nsafety problem in the instruction-following setting and struggle to tackle\nthese situational safety challenges all at once, highlighting a key area for\nfuture research. Furthermore, we develop multi-agent pipelines to coordinately\nsolve safety challenges, which shows consistent improvement in safety over the\noriginal MLLM response. Code and data: mssbench.github.io.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025 Camera Ready",
    "pdf_url": "http://arxiv.org/pdf/2410.06172v2",
    "published_date": "2024-10-08 16:16:07 UTC",
    "updated_date": "2025-04-22 23:01:49 UTC"
  },
  {
    "arxiv_id": "2410.06151v1",
    "title": "Quality Diversity Imitation Learning",
    "authors": [
      "Zhenglin Wan",
      "Xingrui Yu",
      "David Mark Bossens",
      "Yueming Lyu",
      "Qing Guo",
      "Flint Xiaofeng Fan",
      "Ivor Tsang"
    ],
    "abstract": "Imitation learning (IL) has shown great potential in various applications,\nsuch as robot control. However, traditional IL methods are usually designed to\nlearn only one specific type of behavior since demonstrations typically\ncorrespond to a single expert. In this work, we introduce the first generic\nframework for Quality Diversity Imitation Learning (QD-IL), which enables the\nagent to learn a broad range of skills from limited demonstrations. Our\nframework integrates the principles of quality diversity with adversarial\nimitation learning (AIL) methods, and can potentially improve any inverse\nreinforcement learning (IRL) method. Empirically, our framework significantly\nimproves the QD performance of GAIL and VAIL on the challenging continuous\ncontrol tasks derived from Mujoco environments. Moreover, our method even\nachieves 2x expert performance in the most challenging Humanoid environment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, conference paper",
    "pdf_url": "http://arxiv.org/pdf/2410.06151v1",
    "published_date": "2024-10-08 15:49:33 UTC",
    "updated_date": "2024-10-08 15:49:33 UTC"
  },
  {
    "arxiv_id": "2410.18982v1",
    "title": "O1 Replication Journey: A Strategic Progress Report -- Part 1",
    "authors": [
      "Yiwei Qin",
      "Xuefeng Li",
      "Haoyang Zou",
      "Yixiu Liu",
      "Shijie Xia",
      "Zhen Huang",
      "Yixin Ye",
      "Weizhe Yuan",
      "Hector Liu",
      "Yuanzhi Li",
      "Pengfei Liu"
    ],
    "abstract": "This paper introduces a pioneering approach to artificial intelligence\nresearch, embodied in our O1 Replication Journey. In response to the\nannouncement of OpenAI's groundbreaking O1 model, we embark on a transparent,\nreal-time exploration to replicate its capabilities while reimagining the\nprocess of conducting and communicating AI research. Our methodology addresses\ncritical challenges in modern AI research, including the insularity of\nprolonged team-based projects, delayed information sharing, and the lack of\nrecognition for diverse contributions. By providing comprehensive, real-time\ndocumentation of our replication efforts, including both successes and\nfailures, we aim to foster open science, accelerate collective advancement, and\nlay the groundwork for AI-driven scientific discovery. Our research progress\nreport diverges significantly from traditional research papers, offering\ncontinuous updates, full process transparency, and active community engagement\nthroughout the research journey. Technologically, we proposed the journey\nlearning paradigm, which encourages models to learn not just shortcuts, but the\ncomplete exploration process, including trial and error, reflection, and\nbacktracking. With only 327 training samples and without any additional tricks,\njourney learning outperformed conventional supervised learning by over 8\\% on\nthe MATH dataset, demonstrating its extremely powerful potential. We believe\nthis to be the most crucial component of O1 technology that we have\nsuccessfully decoded. We share valuable resources including technical\nhypotheses and insights, cognitive exploration maps, custom-developed tools,\netc at https://github.com/GAIR-NLP/O1-Journey.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18982v1",
    "published_date": "2024-10-08 15:13:01 UTC",
    "updated_date": "2024-10-08 15:13:01 UTC"
  },
  {
    "arxiv_id": "2410.06108v1",
    "title": "ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution",
    "authors": [
      "Corban Rivera",
      "Grayson Byrd",
      "William Paul",
      "Tyler Feldman",
      "Meghan Booker",
      "Emma Holmes",
      "David Handelman",
      "Bethany Kemp",
      "Andrew Badger",
      "Aurora Schmidt",
      "Krishna Murthy Jatavallabhula",
      "Celso M de Melo",
      "Lalithkumar Seenivasan",
      "Mathias Unberath",
      "Rama Chellappa"
    ],
    "abstract": "Robotic planning and execution in open-world environments is a complex\nproblem due to the vast state spaces and high variability of task embodiment.\nRecent advances in perception algorithms, combined with Large Language Models\n(LLMs) for planning, offer promising solutions to these challenges, as the\ncommon sense reasoning capabilities of LLMs provide a strong heuristic for\nefficiently searching the action space. However, prior work fails to address\nthe possibility of hallucinations from LLMs, which results in failures to\nexecute the planned actions largely due to logical fallacies at high- or\nlow-levels. To contend with automation failure due to such hallucinations, we\nintroduce ConceptAgent, a natural language-driven robotic platform designed for\ntask execution in unstructured environments. With a focus on scalability and\nreliability of LLM-based planning in complex state and action spaces, we\npresent innovations designed to limit these shortcomings, including 1)\nPredicate Grounding to prevent and recover from infeasible actions, and 2) an\nembodied version of LLM-guided Monte Carlo Tree Search with self reflection. In\nsimulation experiments, ConceptAgent achieved a 19% task completion rate across\nthree room layouts and 30 easy level embodied tasks outperforming other\nstate-of-the-art LLM-driven reasoning baselines that scored 10.26% and 8.11% on\nthe same benchmark. Additionally, ablation studies on moderate to hard embodied\ntasks revealed a 20% increase in task completion from the baseline agent to the\nfully enhanced ConceptAgent, highlighting the individual and combined\ncontributions of Predicate Grounding and LLM-guided Tree Search to enable more\nrobust automation in complex state and action spaces.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06108v1",
    "published_date": "2024-10-08 15:05:40 UTC",
    "updated_date": "2024-10-08 15:05:40 UTC"
  },
  {
    "arxiv_id": "2410.06107v1",
    "title": "Towards AI-Native Software Engineering (SE 3.0): A Vision and a Challenge Roadmap",
    "authors": [
      "Ahmed E. Hassan",
      "Gustavo A. Oliva",
      "Dayi Lin",
      "Boyuan Chen",
      "Zhen Ming",
      "Jiang"
    ],
    "abstract": "The rise of AI-assisted software engineering (SE 2.0), powered by Foundation\nModels (FMs) and FM-powered copilots, has shown promise in improving developer\nproductivity. However, it has also exposed inherent limitations, such as\ncognitive overload on developers and inefficiencies. We propose a shift towards\nSoftware Engineering 3.0 (SE 3.0), an AI-native approach characterized by\nintent-first, conversation-oriented development between human developers and AI\nteammates. SE 3.0 envisions AI systems evolving beyond task-driven copilots\ninto intelligent collaborators, capable of deeply understanding and reasoning\nabout software engineering principles and intents. We outline the key\ncomponents of the SE 3.0 technology stack, which includes Teammate.next for\nadaptive and personalized AI partnership, IDE.next for intent-first\nconversation-oriented development, Compiler.next for multi-objective code\nsynthesis, and Runtime.next for SLA-aware execution with edge-computing\nsupport. Our vision addresses the inefficiencies and cognitive strain of SE 2.0\nby fostering a symbiotic relationship between human developers and AI,\nmaximizing their complementary strengths. We also present a roadmap of\nchallenges that must be overcome to realize our vision of SE 3.0. This paper\nlays the foundation for future discussions on the role of AI in the next era of\nsoftware engineering.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06107v1",
    "published_date": "2024-10-08 15:04:07 UTC",
    "updated_date": "2024-10-08 15:04:07 UTC"
  },
  {
    "arxiv_id": "2410.06101v2",
    "title": "Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Hao Ma",
      "Tianyi Hu",
      "Zhiqiang Pu",
      "Boyin Liu",
      "Xiaolin Ai",
      "Yanyan Liang",
      "Min Chen"
    ],
    "abstract": "Reinforcement learning (RL) has emerged as a pivotal technique for\nfine-tuning large language models (LLMs) on specific tasks. However, prevailing\nRL fine-tuning methods predominantly rely on PPO and its variants. Though these\nalgorithms are effective in general RL settings, they often exhibit suboptimal\nperformance and vulnerability to distribution collapse when applied to the\nfine-tuning of LLMs. In this paper, we propose CORY, extending the RL\nfine-tuning of LLMs to a sequential cooperative multi-agent reinforcement\nlearning framework, to leverage the inherent coevolution and emergent\ncapabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is\ninitially duplicated into two autonomous agents: a pioneer and an observer. The\npioneer generates responses based on queries, while the observer generates\nresponses using both the queries and the pioneer's responses. The two agents\nare trained together. During training, the agents exchange roles periodically,\nfostering cooperation and coevolution between them. Experiments evaluate CORY's\nperformance by fine-tuning GPT-2 and Llama-2 under subjective and objective\nreward functions on the IMDB Review and GSM8K datasets, respectively. Results\nshow that CORY outperforms PPO in terms of policy optimality, resistance to\ndistribution collapse, and training robustness, thereby underscoring its\npotential as a superior methodology for refining LLMs in real-world\napplications.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by NeurIPS '24",
    "pdf_url": "http://arxiv.org/pdf/2410.06101v2",
    "published_date": "2024-10-08 14:55:26 UTC",
    "updated_date": "2025-02-22 17:08:44 UTC"
  },
  {
    "arxiv_id": "2410.09094v2",
    "title": "Reflections on Disentanglement and the Latent Space",
    "authors": [
      "Ludovica Schaerf"
    ],
    "abstract": "The latent space of image generative models is a multi-dimensional space of\ncompressed hidden visual knowledge. Its entity captivates computer scientists,\ndigital artists, and media scholars alike. Latent space has become an aesthetic\ncategory in AI art, inspiring artistic techniques such as the latent space\nwalk, exemplified by the works of Mario Klingemann and others. It is also\nviewed as cultural snapshots, encoding rich representations of our visual\nworld. This paper proposes a double view of the latent space, as a\nmulti-dimensional archive of culture and as a multi-dimensional space of\npotentiality. The paper discusses disentanglement as a method to elucidate the\ndouble nature of the space and as an interpretative direction to exploit its\norganization in human terms. The paper compares the role of disentanglement as\npotentiality to that of conditioning, as imagination, and confronts this\ninterpretation with the philosophy of Deleuzian potentiality and Hume's\nimagination. Lastly, this paper notes the difference between traditional\ngenerative models and recent architectures.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Published in xCoAx 2024, School of X's proceedings. DOI:\n  10.34626/2024_xcoax/classof24_002",
    "pdf_url": "http://arxiv.org/pdf/2410.09094v2",
    "published_date": "2024-10-08 14:55:07 UTC",
    "updated_date": "2024-10-20 14:40:09 UTC"
  },
  {
    "arxiv_id": "2410.06089v1",
    "title": "TOWER: Tree Organized Weighting for Evaluating Complex Instructions",
    "authors": [
      "Noah Ziems",
      "Zhihan Zhang",
      "Meng Jiang"
    ],
    "abstract": "Evaluating the ability of large language models (LLMs) to follow complex\nhuman-written instructions is essential for their deployment in real-world\napplications. While benchmarks like Chatbot Arena use human judges to assess\nmodel performance, they are resource-intensive and time-consuming. Alternative\nmethods using LLMs as judges, such as AlpacaEval, MT Bench, WildBench, and\nInFoBench offer improvements but still do not capture that certain complex\ninstruction aspects are more important than others to follow.\n  To address this gap, we propose a novel evaluation metric, \\textsc{TOWER},\nthat incorporates human-judged importance into the assessment of complex\ninstruction following. We show that human annotators agree with tree-based\nrepresentations of these complex instructions nearly as much as they agree with\nother human annotators. We release tree-based annotations of the InFoBench\ndataset and the corresponding evaluation code to facilitate future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.06089v1",
    "published_date": "2024-10-08 14:46:50 UTC",
    "updated_date": "2024-10-08 14:46:50 UTC"
  },
  {
    "arxiv_id": "2410.06065v1",
    "title": "Posets and Bounded Probabilities for Discovering Order-inducing Features in Event Knowledge Graphs",
    "authors": [
      "Christoffer Olling Back",
      "Jakob Grue Simonsen"
    ],
    "abstract": "Event knowledge graphs (EKG) extend the classical notion of a trace to\ncapture multiple, interacting views of a process execution. In this paper, we\ntackle the open problem of automating EKG discovery from uncurated data through\na principled, probabilistic framing based on the outcome space resulting from\nfeatured-derived partial orders on events. From this, we derive an EKG\ndiscovery algorithm based upon statistical inference rather than an ad-hoc or\nheuristic-based strategy, or relying on manual analysis from domain experts.\n  This approach comes at the computational cost of exploring a large,\nnon-convex hypothesis space. In particular, solving the maximum likelihood term\ninvolves counting the number of linear extensions of posets, which in general\nis #P-complete. Fortunately, bound estimates suffice for model comparison, and\nadmit incorporation into a bespoke branch-and-bound algorithm. We show that the\nposterior probability as defined is antitonic w.r.t. search depth for branching\nrules that are monotonic w.r.t. model inclusion. This allows pruning of large\nportions of the search space, which we show experimentally leads to rapid\nconvergence toward optimal solutions that are consistent with manually built\nEKGs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "06-08",
      "G.3; I.2.6; I.5"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06065v1",
    "published_date": "2024-10-08 14:12:51 UTC",
    "updated_date": "2024-10-08 14:12:51 UTC"
  },
  {
    "arxiv_id": "2410.06062v4",
    "title": "LLM-based SPARQL Query Generation from Natural Language over Federated Knowledge Graphs",
    "authors": [
      "Vincent Emonet",
      "Jerven Bolleman",
      "Severine Duvaud",
      "Tarcisio Mendes de Farias",
      "Ana Claudia Sima"
    ],
    "abstract": "We introduce a Retrieval-Augmented Generation (RAG) system for translating\nuser questions into accurate federated SPARQL queries over bioinformatics\nknowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance\naccuracy and reduce hallucinations in query generation, our system utilises\nmetadata from the KGs, including query examples and schema information, and\nincorporates a validation step to correct generated queries. The system is\navailable online at chat.expasy.org.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06062v4",
    "published_date": "2024-10-08 14:09:12 UTC",
    "updated_date": "2025-02-10 10:55:08 UTC"
  },
  {
    "arxiv_id": "2410.18103v1",
    "title": "A Hybrid Graph Neural Network for Enhanced EEG-Based Depression Detection",
    "authors": [
      "Yiye Wang",
      "Wenming Zheng",
      "Yang Li",
      "Hao Yang"
    ],
    "abstract": "Graph neural networks (GNNs) are becoming increasingly popular for EEG-based\ndepression detection. However, previous GNN-based methods fail to sufficiently\nconsider the characteristics of depression, thus limiting their performance.\nFirstly, studies in neuroscience indicate that depression patients exhibit both\ncommon and individualized brain abnormal patterns. Previous GNN-based\napproaches typically focus either on fixed graph connections to capture common\nabnormal brain patterns or on adaptive connections to capture individualized\npatterns, which is inadequate for depression detection. Secondly, brain network\nexhibits a hierarchical structure, which includes the arrangement from\nchannel-level graph to region-level graph. This hierarchical structure varies\namong individuals and contains significant information relevant to detecting\ndepression. Nonetheless, previous GNN-based methods overlook these\nindividualized hierarchical information. To address these issues, we propose a\nHybrid GNN (HGNN) that merges a Common Graph Neural Network (CGNN) branch\nutilizing fixed connection and an Individualized Graph Neural Network (IGNN)\nbranch employing adaptive connections. The two branches capture common and\nindividualized depression patterns respectively, complementing each other.\nFurthermore, we enhance the IGNN branch with a Graph Pooling and Unpooling\nModule (GPUM) to extract individualized hierarchical information. Extensive\nexperiments on two public datasets show that our model achieves\nstate-of-the-art performance.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18103v1",
    "published_date": "2024-10-08 13:57:50 UTC",
    "updated_date": "2024-10-08 13:57:50 UTC"
  },
  {
    "arxiv_id": "2410.18102v2",
    "title": "Multiple Global Peaks Big Bang-Big Crunch Algorithm for Multimodal Optimization",
    "authors": [
      "Fabio Stroppa",
      "Ahmet Astar"
    ],
    "abstract": "The main challenge of multimodal optimization problems is identifying\nmultiple peaks with high accuracy in multidimensional search spaces with\nirregular landscapes. This work proposes the Multiple Global Peaks Big Bang-Big\nCrunch (MGP-BBBC) algorithm, which addresses the challenge of multimodal\noptimization problems by introducing a specialized mechanism for each operator.\nThe algorithm expands the Big Bang-Big Crunch algorithm, a state-of-the-art\nmetaheuristic inspired by the universe's evolution. Specifically, MGP-BBBC\ngroups the best individuals of the population into cluster-based centers of\nmass and then expands them with a progressively lower disturbance to guarantee\nconvergence. During this process, it (i) applies a distance-based filtering to\nremove unnecessary elites such that the ones on smaller peaks are not lost,\n(ii) promotes isolated individuals based on their niche count after clustering,\nand (iii) balances exploration and exploitation during offspring generation to\ntarget specific accuracy levels. Experimental results on twenty multimodal\nbenchmark test functions show that MGP-BBBC generally performs better or\ncompetitively with respect to other state-of-the-art multimodal optimizers.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.18102v2",
    "published_date": "2024-10-08 13:49:35 UTC",
    "updated_date": "2024-11-08 11:07:22 UTC"
  },
  {
    "arxiv_id": "2410.06045v1",
    "title": "Extracting Finite State Machines from Transformers",
    "authors": [
      "Rik Adriaensen",
      "Jaron Maene"
    ],
    "abstract": "Fueled by the popularity of the transformer architecture in deep learning,\nseveral works have investigated what formal languages a transformer can learn.\nNonetheless, existing results remain hard to compare and a fine-grained\nunderstanding of the trainability of transformers on regular languages is still\nlacking. We investigate transformers trained on regular languages from a\nmechanistic interpretability perspective. Using an extension of the $L^*$\nalgorithm, we extract Moore machines from transformers. We empirically find\ntighter lower bounds on the trainability of transformers, when a finite number\nof symbols determine the state. Additionally, our mechanistic insight allows us\nto characterise the regular languages a one-layer transformer can learn with\ngood length generalisation. However, we also identify failure cases where the\ndetermining symbols get misrecognised due to saturation of the attention\nmechanism.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for Workshop on Mechanistic Interpretability ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.06045v1",
    "published_date": "2024-10-08 13:43:50 UTC",
    "updated_date": "2024-10-08 13:43:50 UTC"
  },
  {
    "arxiv_id": "2410.10871v1",
    "title": "Applying Refusal-Vector Ablation to Llama 3.1 70B Agents",
    "authors": [
      "Simon Lermen",
      "Mateusz Dziemian",
      "Govind Pimpale"
    ],
    "abstract": "Recently, language models like Llama 3.1 Instruct have become increasingly\ncapable of agentic behavior, enabling them to perform tasks requiring\nshort-term planning and tool use. In this study, we apply refusal-vector\nablation to Llama 3.1 70B and implement a simple agent scaffolding to create an\nunrestricted agent. Our findings imply that these refusal-vector ablated models\ncan successfully complete harmful tasks, such as bribing officials or crafting\nphishing attacks, revealing significant vulnerabilities in current safety\nmechanisms. To further explore this, we introduce a small Safe Agent Benchmark,\ndesigned to test both harmful and benign tasks in agentic scenarios. Our\nresults imply that safety fine-tuning in chat models does not generalize well\nto agentic behavior, as we find that Llama 3.1 Instruct models are willing to\nperform most harmful tasks without modifications. At the same time, these\nmodels will refuse to give advice on how to perform the same tasks when asked\nfor a chat completion. This highlights the growing risk of misuse as models\nbecome more capable, underscoring the need for improved safety frameworks for\nlanguage model agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10871v1",
    "published_date": "2024-10-08 13:42:36 UTC",
    "updated_date": "2024-10-08 13:42:36 UTC"
  },
  {
    "arxiv_id": "2410.10870v3",
    "title": "PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches",
    "authors": [
      "Rana Muhammad Shahroz Khan",
      "Pingzhi Li",
      "Sukwon Yun",
      "Zhenyu Wang",
      "Shahriar Nirjon",
      "Chau-Wai Wong",
      "Tianlong Chen"
    ],
    "abstract": "As large language models (LLMs) increasingly shape the AI landscape,\nfine-tuning pretrained models has become more popular than in the pre-LLM era\nfor achieving optimal performance in domain-specific tasks. However, pretrained\nLLMs such as ChatGPT are periodically evolved, i.e., model parameters are\nfrequently updated), making it challenging for downstream users with limited\nresources to keep up with fine-tuning the newest LLMs for their domain\napplication. Even though fine-tuning costs have nowadays been reduced thanks to\nthe innovations of parameter-efficient fine-tuning such as LoRA, not all\ndownstream users have adequate computing for frequent personalization.\nMoreover, access to fine-tuning datasets, particularly in sensitive domains\nsuch as healthcare, could be time-restrictive, making it crucial to retain the\nknowledge encoded in earlier fine-tuned rounds for future adaptation. In this\npaper, we present PortLLM, a training-free framework that (i) creates an\ninitial lightweight model update patch to capture domain-specific knowledge,\nand (ii) allows a subsequent seamless plugging for the continual\npersonalization of evolved LLM at minimal cost. Our extensive experiments cover\nseven representative datasets, from easier question-answering tasks {BoolQ,\nSST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including\n{Mistral-7B, Llama2, Llama3.1, and Gemma2}, validating the portability of our\ndesigned model patches and showcasing the effectiveness of our proposed\nframework. For instance, PortLLM achieves comparable performance to LoRA\nfine-tuning with reductions of up to 12.2x in GPU memory usage. Finally, we\nprovide theoretical justifications to understand the portability of our model\nupdate patches, which offers new insights into the theoretical dimension of\nLLMs' personalization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10870v3",
    "published_date": "2024-10-08 13:41:08 UTC",
    "updated_date": "2025-03-29 03:32:53 UTC"
  },
  {
    "arxiv_id": "2410.06041v2",
    "title": "Block Induced Signature Generative Adversarial Network (BISGAN): Signature Spoofing Using GANs and Their Evaluation",
    "authors": [
      "Haadia Amjad",
      "Kilian Goeller",
      "Steffen Seitz",
      "Carsten Knoll",
      "Naseer Bajwa",
      "Ronald Tetzlaff",
      "Muhammad Imran Malik"
    ],
    "abstract": "Deep learning is actively being used in biometrics to develop efficient\nidentification and verification systems. Handwritten signatures are a common\nsubset of biometric data for authentication purposes. Generative adversarial\nnetworks (GANs) learn from original and forged signatures to generate forged\nsignatures. While most GAN techniques create a strong signature verifier, which\nis the discriminator, there is a need to focus more on the quality of forgeries\ngenerated by the generator model. This work focuses on creating a generator\nthat produces forged samples that achieve a benchmark in spoofing signature\nverification systems. We use CycleGANs infused with Inception model-like blocks\nwith attention heads as the generator and a variation of the SigCNN model as\nthe base Discriminator. We train our model with a new technique that results in\n80% to 100% success in signature spoofing. Additionally, we create a custom\nevaluation technique to act as a goodness measure of the generated forgeries.\nOur work advocates generator-focused GAN architectures for spoofing data\nquality that aid in a better understanding of biometric data generation and\nevaluation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06041v2",
    "published_date": "2024-10-08 13:40:33 UTC",
    "updated_date": "2024-10-11 08:14:58 UTC"
  },
  {
    "arxiv_id": "2410.06030v1",
    "title": "Data Quality Issues in Vulnerability Detection Datasets",
    "authors": [
      "Yuejun Guo",
      "Seifeddine Bettaieb"
    ],
    "abstract": "Vulnerability detection is a crucial yet challenging task to identify\npotential weaknesses in software for cyber security. Recently, deep learning\n(DL) has made great progress in automating the detection process. Due to the\ncomplex multi-layer structure and a large number of parameters, a DL model\nrequires massive labeled (vulnerable or secure) source code to gain knowledge\nto effectively distinguish between vulnerable and secure code. In the\nliterature, many datasets have been created to train DL models for this\npurpose. However, these datasets suffer from several issues that will lead to\nlow detection accuracy of DL models. In this paper, we define three critical\nissues (i.e., data imbalance, low vulnerability coverage, biased vulnerability\ndistribution) that can significantly affect the model performance and three\nsecondary issues (i.e., errors in source code, mislabeling, noisy historical\ndata) that also affect the performance but can be addressed through a dedicated\npre-processing procedure. In addition, we conduct a study of 14 papers along\nwith 54 datasets for vulnerability detection to confirm these defined issues.\nFurthermore, we discuss good practices to use existing datasets and to create\nnew ones.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "2023 IEEE European Symposium on Security and Privacy Workshops\n  (EuroS&P;PW)",
    "pdf_url": "http://arxiv.org/pdf/2410.06030v1",
    "published_date": "2024-10-08 13:31:29 UTC",
    "updated_date": "2024-10-08 13:31:29 UTC"
  },
  {
    "arxiv_id": "2410.18101v2",
    "title": "Molecular Dynamics and Machine Learning Unlock Possibilities in Beauty Design -- A Perspective",
    "authors": [
      "Yuzhi Xu",
      "Haowei Ni",
      "Qinhui Gao",
      "Chia-Hua Chang",
      "Yanran Huo",
      "Fanyu Zhao",
      "Shiyu Hu",
      "Wei Xia",
      "Yike Zhang",
      "Radu Grovu",
      "Min He",
      "John. Z. H. Zhang",
      "Yuanqing Wang"
    ],
    "abstract": "Computational molecular design -- the endeavor to design molecules, with\nvarious missions, aided by machine learning and molecular dynamics approaches,\nhas been widely applied to create valuable new molecular entities, from small\nmolecule therapeutics to protein biologics. In the small data regime,\nphysics-based approaches model the interaction between the molecule being\ndesigned and proteins of key physiological functions, providing structural\ninsights into the mechanism. When abundant data has been collected, a\nquantitative structure-activity relationship (QSAR) can be more directly\nconstructed from experimental data, from which machine learning can distill key\ninsights to guide the design of the next round of experiment design. Machine\nlearning methodologies can also facilitate physical modeling, from improving\nthe accuracy of force fields and extending them to unseen chemical spaces, to\nmore directly enhancing the sampling on the conformational spaces. We argue\nthat these techniques are mature enough to be applied to not just extend the\nlongevity of life, but the beauty it manifests. In this perspective, we review\nthe current frontiers in the research \\& development of skin care products, as\nwell as the statistical and physical toolbox applicable to addressing the\nchallenges in this industry. Feasible interdisciplinary research projects are\nproposed to harness the power of machine learning tools to design innovative,\neffective, and inexpensive skin care products.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18101v2",
    "published_date": "2024-10-08 13:30:27 UTC",
    "updated_date": "2024-10-28 20:11:46 UTC"
  },
  {
    "arxiv_id": "2410.06024v1",
    "title": "Jet Expansions of Residual Computation",
    "authors": [
      "Yihong Chen",
      "Xiangxiang Xu",
      "Yao Lu",
      "Pontus Stenetorp",
      "Luca Franceschi"
    ],
    "abstract": "We introduce a framework for expanding residual computational graphs using\njets, operators that generalize truncated Taylor series. Our method provides a\nsystematic approach to disentangle contributions of different computational\npaths to model predictions. In contrast to existing techniques such as\ndistillation, probing, or early decoding, our expansions rely solely on the\nmodel itself and requires no data, training, or sampling from the model. We\ndemonstrate how our framework grounds and subsumes logit lens, reveals a\n(super-)exponential path structure in the recursive residual depth and opens up\nseveral applications. These include sketching a transformer large language\nmodel with $n$-gram statistics extracted from its computations, and indexing\nthe models' levels of toxicity knowledge. Our approach enables data-free\nanalysis of residual computation for model interpretability, development, and\nevaluation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06024v1",
    "published_date": "2024-10-08 13:25:08 UTC",
    "updated_date": "2024-10-08 13:25:08 UTC"
  },
  {
    "arxiv_id": "2410.14707v1",
    "title": "FACMIC: Federated Adaptative CLIP Model for Medical Image Classification",
    "authors": [
      "Yihang Wu",
      "Christian Desrosiers",
      "Ahmad Chaddad"
    ],
    "abstract": "Federated learning (FL) has emerged as a promising approach to medical image\nanalysis that allows deep model training using decentralized data while\nensuring data privacy. However, in the field of FL, communication cost plays a\ncritical role in evaluating the performance of the model. Thus, transferring\nvision foundation models can be particularly challenging due to the significant\nresource costs involved. In this paper, we introduce a federated adaptive\nContrastive Language Image Pretraining CLIP model designed for classification\ntasks. We employ a light-weight and efficient feature attention module for CLIP\nthat selects suitable features for each client's data. Additionally, we propose\na domain adaptation technique to reduce differences in data distribution\nbetween clients. Experimental results on four publicly available datasets\ndemonstrate the superior performance of FACMIC in dealing with real-world and\nmultisource medical imaging data. Our codes are available at\nhttps://github.com/AIPMLab/FACMIC.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.14707v1",
    "published_date": "2024-10-08 13:24:10 UTC",
    "updated_date": "2024-10-08 13:24:10 UTC"
  },
  {
    "arxiv_id": "2410.06020v1",
    "title": "QT-DoG: Quantization-aware Training for Domain Generalization",
    "authors": [
      "Saqib Javed",
      "Hieu Le",
      "Mathieu Salzmann"
    ],
    "abstract": "Domain Generalization (DG) aims to train models that perform well not only on\nthe training (source) domains but also on novel, unseen target data\ndistributions. A key challenge in DG is preventing overfitting to source\ndomains, which can be mitigated by finding flatter minima in the loss\nlandscape. In this work, we propose Quantization-aware Training for Domain\nGeneralization (QT-DoG) and demonstrate that weight quantization effectively\nleads to flatter minima in the loss landscape, thereby enhancing domain\ngeneralization. Unlike traditional quantization methods focused on model\ncompression, QT-DoG exploits quantization as an implicit regularizer by\ninducing noise in model weights, guiding the optimization process toward\nflatter minima that are less sensitive to perturbations and overfitting. We\nprovide both theoretical insights and empirical evidence demonstrating that\nquantization inherently encourages flatter minima, leading to better\ngeneralization across domains. Moreover, with the benefit of reducing the model\nsize through quantization, we demonstrate that an ensemble of multiple\nquantized models further yields superior accuracy than the state-of-the-art DG\napproaches with no computational or memory overheads. Our extensive experiments\ndemonstrate that QT-DoG generalizes across various datasets, architectures, and\nquantization algorithms, and can be combined with other DG methods,\nestablishing its versatility and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Code will be released soon",
    "pdf_url": "http://arxiv.org/pdf/2410.06020v1",
    "published_date": "2024-10-08 13:21:48 UTC",
    "updated_date": "2024-10-08 13:21:48 UTC"
  },
  {
    "arxiv_id": "2410.06019v1",
    "title": "Unveiling Transformer Perception by Exploring Input Manifolds",
    "authors": [
      "Alessandro Benfenati",
      "Alfio Ferrara",
      "Alessio Marta",
      "Davide Riva",
      "Elisabetta Rocchetti"
    ],
    "abstract": "This paper introduces a general method for the exploration of equivalence\nclasses in the input space of Transformer models. The proposed approach is\nbased on sound mathematical theory which describes the internal layers of a\nTransformer architecture as sequential deformations of the input manifold.\nUsing eigendecomposition of the pullback of the distance metric defined on the\noutput space through the Jacobian of the model, we are able to reconstruct\nequivalence classes in the input space and navigate across them. We illustrate\nhow this method can be used as a powerful tool for investigating how a\nTransformer sees the input space, facilitating local and task-agnostic\nexplainability in Computer Vision and Natural Language Processing tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2.7; I.6.4"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.06019v1",
    "published_date": "2024-10-08 13:20:31 UTC",
    "updated_date": "2024-10-08 13:20:31 UTC"
  },
  {
    "arxiv_id": "2410.06014v1",
    "title": "SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting",
    "authors": [
      "Xinyi Liu",
      "Tianyi Zhang",
      "Matthew Johnson-Roberson",
      "Weiming Zhi"
    ],
    "abstract": "Many recent developments for robots to represent environments have focused on\nphotorealistic reconstructions. This paper particularly focuses on generating\nsequences of images from the photorealistic Gaussian Splatting models, that\nmatch instructions that are given by user-inputted language. We contribute a\nnovel framework, SplaTraj, which formulates the generation of images within\nphotorealistic environment representations as a continuous-time trajectory\noptimization problem. Costs are designed so that a camera following the\ntrajectory poses will smoothly traverse through the environment and render the\nspecified spatial information in a photogenic manner. This is achieved by\nquerying a photorealistic representation with language embedding to isolate\nregions that correspond to the user-specified inputs. These regions are then\nprojected to the camera's view as it moves over time and a cost is constructed.\nWe can then apply gradient-based optimization and differentiate through the\nrendering to optimize the trajectory for the defined cost. The resulting\ntrajectory moves to photogenically view each of the specified objects. We\nempirically evaluate our approach on a suite of environments and instructions,\nand demonstrate the quality of generated image sequences.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06014v1",
    "published_date": "2024-10-08 13:16:49 UTC",
    "updated_date": "2024-10-08 13:16:49 UTC"
  },
  {
    "arxiv_id": "2410.18100v1",
    "title": "RingGesture: A Ring-Based Mid-Air Gesture Typing System Powered by a Deep-Learning Word Prediction Framework",
    "authors": [
      "Junxiao Shen",
      "Roger Boldu",
      "Arpit Kalla",
      "Michael Glueck",
      "Hemant Bhaskar Surale Amy Karlson"
    ],
    "abstract": "Text entry is a critical capability for any modern computing experience, with\nlightweight augmented reality (AR) glasses being no exception. Designed for\nall-day wearability, a limitation of lightweight AR glass is the restriction to\nthe inclusion of multiple cameras for extensive field of view in hand tracking.\nThis constraint underscores the need for an additional input device. We propose\na system to address this gap: a ring-based mid-air gesture typing technique,\nRingGesture, utilizing electrodes to mark the start and end of gesture\ntrajectories and inertial measurement units (IMU) sensors for hand tracking.\nThis method offers an intuitive experience similar to raycast-based mid-air\ngesture typing found in VR headsets, allowing for a seamless translation of\nhand movements into cursor navigation. To enhance both accuracy and input\nspeed, we propose a novel deep-learning word prediction framework, Score\nFusion, comprised of three key components: a) a word-gesture decoding model, b)\na spatial spelling correction model, and c) a lightweight contextual language\nmodel. In contrast, this framework fuses the scores from the three models to\npredict the most likely words with higher precision. We conduct comparative and\nlongitudinal studies to demonstrate two key findings: firstly, the overall\neffectiveness of RingGesture, which achieves an average text entry speed of\n27.3 words per minute (WPM) and a peak performance of 47.9 WPM. Secondly, we\nhighlight the superior performance of the Score Fusion framework, which offers\na 28.2% improvement in uncorrected Character Error Rate over a conventional\nword prediction framework, Naive Correction, leading to a 55.2% improvement in\ntext entry speed for RingGesture. Additionally, RingGesture received a System\nUsability Score of 83 signifying its excellent usability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18100v1",
    "published_date": "2024-10-08 13:15:30 UTC",
    "updated_date": "2024-10-08 13:15:30 UTC"
  },
  {
    "arxiv_id": "2410.06010v1",
    "title": "A large collection of bioinformatics question-query pairs over federated knowledge graphs: methodology and applications",
    "authors": [
      "Jerven Bolleman",
      "Vincent Emonet",
      "Adrian Altenhoff",
      "Amos Bairoch",
      "Marie-Claude Blatter",
      "Alan Bridge",
      "Severine Duvaud",
      "Elisabeth Gasteiger",
      "Dmitry Kuznetsov",
      "Sebastien Moretti",
      "Pierre-Andre Michel",
      "Anne Morgat",
      "Marco Pagni",
      "Nicole Redaschi",
      "Monique Zahn-Zabal",
      "Tarcisio Mendes de Farias",
      "Ana Claudia Sima"
    ],
    "abstract": "Background. In the last decades, several life science resources have\nstructured data using the same framework and made these accessible using the\nsame query language to facilitate interoperability. Knowledge graphs have seen\nincreased adoption in bioinformatics due to their advantages for representing\ndata in a generic graph format. For example, yummydata.org catalogs more than\n60 knowledge graphs accessible through SPARQL, a technical query language.\nAlthough SPARQL allows powerful, expressive queries, even across physically\ndistributed knowledge graphs, formulating such queries is a challenge for most\nusers. Therefore, to guide users in retrieving the relevant data, many of these\nresources provide representative examples. These examples can also be an\nimportant source of information for machine learning, if a sufficiently large\nnumber of examples are provided and published in a common, machine-readable and\nstandardized format across different resources.\n  Findings. We introduce a large collection of human-written natural language\nquestions and their corresponding SPARQL queries over federated bioinformatics\nknowledge graphs (KGs) collected for several years across different research\ngroups at the SIB Swiss Institute of Bioinformatics. The collection comprises\nmore than 1000 example questions and queries, including 65 federated queries.\nWe propose a methodology to uniformly represent the examples with minimal\nmetadata, based on existing standards. Furthermore, we introduce an extensive\nset of open-source applications, including query graph visualizations and smart\nquery editors, easily reusable by KG maintainers who adopt the proposed\nmethodology.\n  Conclusions. We encourage the community to adopt and extend the proposed\nmethodology, towards richer KG metadata and improved Semantic Web services.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06010v1",
    "published_date": "2024-10-08 13:08:07 UTC",
    "updated_date": "2024-10-08 13:08:07 UTC"
  },
  {
    "arxiv_id": "2410.18099v1",
    "title": "Gesture2Text: A Generalizable Decoder for Word-Gesture Keyboards in XR Through Trajectory Coarse Discretization and Pre-training",
    "authors": [
      "Junxiao Shen",
      "Khadija Khaldi",
      "Enmin Zhou",
      "Hemant Bhaskar Surale",
      "Amy Karlson"
    ],
    "abstract": "Text entry with word-gesture keyboards (WGK) is emerging as a popular method\nand becoming a key interaction for Extended Reality (XR). However, the\ndiversity of interaction modes, keyboard sizes, and visual feedback in these\nenvironments introduces divergent word-gesture trajectory data patterns, thus\nleading to complexity in decoding trajectories into text. Template-matching\ndecoding methods, such as SHARK^2, are commonly used for these WGK systems\nbecause they are easy to implement and configure. However, these methods are\nsusceptible to decoding inaccuracies for noisy trajectories. While conventional\nneural-network-based decoders (neural decoders) trained on word-gesture\ntrajectory data have been proposed to improve accuracy, they have their own\nlimitations: they require extensive data for training and deep-learning\nexpertise for implementation. To address these challenges, we propose a novel\nsolution that combines ease of implementation with high decoding accuracy: a\ngeneralizable neural decoder enabled by pre-training on large-scale coarsely\ndiscretized word-gesture trajectories. This approach produces a ready-to-use\nWGK decoder that is generalizable across mid-air and on-surface WGK systems in\naugmented reality (AR) and virtual reality (VR), which is evident by a robust\naverage Top-4 accuracy of 90.4% on four diverse datasets. It significantly\noutperforms SHARK^2 with a 37.2% enhancement and surpasses the conventional\nneural decoder by 7.4%. Moreover, the Pre-trained Neural Decoder's size is only\n4 MB after quantization, without sacrificing accuracy, and it can operate in\nreal-time, executing in just 97 milliseconds on Quest 3.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18099v1",
    "published_date": "2024-10-08 12:53:22 UTC",
    "updated_date": "2024-10-08 12:53:22 UTC"
  },
  {
    "arxiv_id": "2410.10869v1",
    "title": "Application of NotebookLM, a Large Language Model with Retrieval-Augmented Generation, for Lung Cancer Staging",
    "authors": [
      "Ryota Tozuka",
      "Hisashi Johno",
      "Akitomo Amakawa",
      "Junichi Sato",
      "Mizuki Muto",
      "Shoichiro Seki",
      "Atsushi Komaba",
      "Hiroshi Onishi"
    ],
    "abstract": "Purpose: In radiology, large language models (LLMs), including ChatGPT, have\nrecently gained attention, and their utility is being rapidly evaluated.\nHowever, concerns have emerged regarding their reliability in clinical\napplications due to limitations such as hallucinations and insufficient\nreferencing. To address these issues, we focus on the latest technology,\nretrieval-augmented generation (RAG), which enables LLMs to reference reliable\nexternal knowledge (REK). Specifically, this study examines the utility and\nreliability of a recently released RAG-equipped LLM (RAG-LLM), NotebookLM, for\nstaging lung cancer.\n  Materials and methods: We summarized the current lung cancer staging\nguideline in Japan and provided this as REK to NotebookLM. We then tasked\nNotebookLM with staging 100 fictional lung cancer cases based on CT findings\nand evaluated its accuracy. For comparison, we performed the same task using a\ngold-standard LLM, GPT-4 Omni (GPT-4o), both with and without the REK.\n  Results: NotebookLM achieved 86% diagnostic accuracy in the lung cancer\nstaging experiment, outperforming GPT-4o, which recorded 39% accuracy with the\nREK and 25% without it. Moreover, NotebookLM demonstrated 95% accuracy in\nsearching reference locations within the REK.\n  Conclusion: NotebookLM successfully performed lung cancer staging by\nutilizing the REK, demonstrating superior performance compared to GPT-4o.\nAdditionally, it provided highly accurate reference locations within the REK,\nallowing radiologists to efficiently evaluate the reliability of NotebookLM's\nresponses and detect possible hallucinations. Overall, this study highlights\nthe potential of NotebookLM, a RAG-LLM, in image diagnosis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 5 figures, 1 table, 3 ancillary files",
    "pdf_url": "http://arxiv.org/pdf/2410.10869v1",
    "published_date": "2024-10-08 12:42:42 UTC",
    "updated_date": "2024-10-08 12:42:42 UTC"
  },
  {
    "arxiv_id": "2410.05991v1",
    "title": "Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision",
    "authors": [
      "Moritz Feuerpfeil",
      "Marco Cipriano",
      "Gerard de Melo"
    ],
    "abstract": "Scalable Vector Graphics (SVG) is a popular format on the web and in the\ndesign industry. However, despite the great strides made in generative\nmodeling, SVG has remained underexplored due to the discrete and complex nature\nof such data. We introduce GRIMOIRE, a text-guided SVG generative model that is\ncomprised of two modules: A Visual Shape Quantizer (VSQ) learns to map raster\nimages onto a discrete codebook by reconstructing them as vector shapes, and an\nAuto-Regressive Transformer (ART) models the joint probability distribution\nover shape tokens, positions and textual descriptions, allowing us to generate\nvector graphics from natural language. Unlike existing models that require\ndirect supervision from SVG data, GRIMOIRE learns shape image patches using\nonly raster image supervision which opens up vector generative modeling to\nsignificantly more data. We demonstrate the effectiveness of our method by\nfitting GRIMOIRE for closed filled shapes on the MNIST and for outline strokes\non icon and font data, surpassing previous image-supervised methods in\ngenerative quality and vector-supervised approach in flexibility.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05991v1",
    "published_date": "2024-10-08 12:41:31 UTC",
    "updated_date": "2024-10-08 12:41:31 UTC"
  },
  {
    "arxiv_id": "2410.05988v1",
    "title": "Utilizing Lyapunov Exponents in designing deep neural networks",
    "authors": [
      "Tirthankar Mittra"
    ],
    "abstract": "Training large deep neural networks is resource intensive. This study\ninvestigates whether Lyapunov exponents can accelerate this process by aiding\nin the selection of hyperparameters. To study this I formulate an optimization\nproblem using neural networks with different activation functions in the hidden\nlayers. By initializing model weights with different random seeds, I calculate\nthe Lyapunov exponent while performing traditional gradient descent on these\nmodel weights. The findings demonstrate that variations in the learning rate\ncan induce chaotic changes in model weights. I also show that activation\nfunctions with more negative Lyapunov exponents exhibit better convergence\nproperties. Additionally, the study also demonstrates that Lyapunov exponents\ncan be utilized to select effective initial model weights for deep neural\nnetworks, potentially enhancing the optimization process.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05988v1",
    "published_date": "2024-10-08 12:36:06 UTC",
    "updated_date": "2024-10-08 12:36:06 UTC"
  },
  {
    "arxiv_id": "2410.11868v1",
    "title": "Neuropsychology of AI: Relationship Between Activation Proximity and Categorical Proximity Within Neural Categories of Synthetic Cognition",
    "authors": [
      "Michael Pichat",
      "Enola Campoli",
      "William Pogrund",
      "Jourdan Wilson",
      "Michael Veillet-Guillem",
      "Anton Melkozerov",
      "Paloma Pichat",
      "Armanouche Gasparian",
      "Samuel Demarchi",
      "Judicael Poumay"
    ],
    "abstract": "Neuropsychology of artificial intelligence focuses on synthetic neural cog\nnition as a new type of study object within cognitive psychology. With the goal\nof making artificial neural networks of language models more explainable, this\napproach involves transposing concepts from cognitive psychology to the\ninterpretive construction of artificial neural cognition. The human cognitive\nconcept involved here is categorization, serving as a heuristic for thinking\nabout the process of segmentation and construction of reality carried out by\nthe neural vectors of synthetic cognition.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.11868v1",
    "published_date": "2024-10-08 12:34:13 UTC",
    "updated_date": "2024-10-08 12:34:13 UTC"
  },
  {
    "arxiv_id": "2410.05985v3",
    "title": "Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation and Layer-Wise Updates",
    "authors": [
      "Cabrel Teguemne Fokam",
      "Khaleelulla Khan Nazeer",
      "Lukas König",
      "David Kappel",
      "Anand Subramoney"
    ],
    "abstract": "The increasing size of deep learning models has made distributed training\nacross multiple devices essential. However, current methods such as distributed\ndata-parallel training suffer from large communication and synchronization\noverheads when training across devices, leading to longer training times as a\nresult of suboptimal hardware utilization. Asynchronous stochastic gradient\ndescent (ASGD) methods can improve training speed, but are sensitive to delays\ndue to both communication and differences throughput. Moreover, the\nbackpropagation algorithm used within ASGD workers is bottlenecked by the\ninterlocking between its forward and backward passes. Current methods also do\nnot take advantage of the large differences in the computation required for the\nforward and backward passes. Therefore, we propose an extension to ASGD called\nPartial Decoupled ASGD (PD-ASGD) that addresses these issues. PD-ASGD uses\nseparate threads for the forward and backward passes, decoupling the updates\nand allowing for a higher ratio of forward to backward threads than the usual\n1:1 ratio, leading to higher throughput. PD-ASGD also performs layer-wise\n(partial) model updates concurrently across multiple threads. This reduces\nparameter staleness and consequently improves robustness to delays. Our\napproach yields close to state-of-the-art results while running up to\n$5.95\\times$ faster than synchronous data parallelism in the presence of\ndelays, and up to $2.14\\times$ times faster than comparable ASGD algorithms by\nachieving higher model flops utilization. We mathematically describe the\ngradient bias introduced by our method, establish an upper bound, and prove\nconvergence.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "G.1.6",
      "I.2.6; I.5.1"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.05985v3",
    "published_date": "2024-10-08 12:32:36 UTC",
    "updated_date": "2025-02-07 13:33:12 UTC"
  },
  {
    "arxiv_id": "2410.05983v1",
    "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
    "authors": [
      "Bowen Jin",
      "Jinsung Yoon",
      "Jiawei Han",
      "Sercan O. Arik"
    ],
    "abstract": "Retrieval-augmented generation (RAG) empowers large language models (LLMs) to\nutilize external knowledge sources. The increasing capacity of LLMs to process\nlonger input sequences opens up avenues for providing more retrieved\ninformation, to potentially enhance the quality of generated outputs. It is\nplausible to assume that a larger retrieval set would contain more relevant\ninformation (higher recall), that might result in improved performance.\nHowever, our empirical findings demonstrate that for many long-context LLMs,\nthe quality of generated output initially improves first, but then subsequently\ndeclines as the number of retrieved passages increases. This paper investigates\nthis phenomenon, identifying the detrimental impact of retrieved \"hard\nnegatives\" as a key contributor. To mitigate this and enhance the robustness of\nlong-context LLM-based RAG, we propose both training-free and training-based\napproaches. We first showcase the effectiveness of retrieval reordering as a\nsimple yet powerful training-free optimization. Furthermore, we explore\ntraining-based methods, specifically RAG-specific implicit LLM fine-tuning and\nRAG-oriented fine-tuning with intermediate reasoning, demonstrating their\ncapacity for substantial performance gains. Finally, we conduct a systematic\nanalysis of design choices for these training-based methods, including data\ndistribution, retriever selection, and training context length.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "34 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.05983v1",
    "published_date": "2024-10-08 12:30:07 UTC",
    "updated_date": "2024-10-08 12:30:07 UTC"
  },
  {
    "arxiv_id": "2410.05970v2",
    "title": "PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with End-to-End Sparse Sampling",
    "authors": [
      "Xudong Xie",
      "Hao Yan",
      "Liang Yin",
      "Yang Liu",
      "Jing Ding",
      "Minghui Liao",
      "Yuliang Liu",
      "Wei Chen",
      "Xiang Bai"
    ],
    "abstract": "Multimodal document understanding is a challenging task to process and\ncomprehend large amounts of textual and visual information. Recent advances in\nLarge Language Models (LLMs) have significantly improved the performance of\nthis task. However, existing methods typically focus on either plain text or a\nlimited number of document images, struggling to handle long PDF documents with\ninterleaved text and images, especially for academic papers. In this paper, we\nintroduce PDF-WuKong, a multimodal large language model (MLLM) which is\ndesigned to enhance multimodal question-answering (QA) for long PDF documents.\nPDF-WuKong incorporates a sparse sampler that operates on both text and image\nrepresentations, significantly improving the efficiency and capability of the\nMLLM. The sparse sampler is integrated with the MLLM's image encoder and\nselects the paragraphs or diagrams most pertinent to user queries for\nprocessing by the language model. To effectively train and evaluate our model,\nwe construct PaperPDF, a dataset consisting of a broad collection of English\nand Chinese academic papers. Multiple strategies are proposed to automatically\ngenerate 1.1 million QA pairs along with their corresponding evidence sources.\nExperimental results demonstrate the superiority and high efficiency of our\napproach over other models on the task of long multimodal document\nunderstanding, surpassing proprietary products by an average of 8.6% on F1. Our\ncode and dataset will be released at https://github.com/yh-hust/PDF-Wukong.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05970v2",
    "published_date": "2024-10-08 12:17:42 UTC",
    "updated_date": "2025-01-20 21:45:19 UTC"
  },
  {
    "arxiv_id": "2410.05966v3",
    "title": "FLOPS: Forward Learning with OPtimal Sampling",
    "authors": [
      "Tao Ren",
      "Zishi Zhang",
      "Jinyang Jiang",
      "Guanghao Li",
      "Zeliang Zhang",
      "Mingqian Feng",
      "Yijie Peng"
    ],
    "abstract": "Given the limitations of backpropagation, perturbation-based gradient\ncomputation methods have recently gained focus for learning with only forward\npasses, also referred to as queries. Conventional forward learning consumes\nenormous queries on each data point for accurate gradient estimation through\nMonte Carlo sampling, which hinders the scalability of those algorithms.\nHowever, not all data points deserve equal queries for gradient estimation. In\nthis paper, we study the problem of improving the forward learning efficiency\nfrom a novel perspective: how to reduce the gradient estimation variance with\nminimum cost? For this, we propose to allocate the optimal number of queries\nover each data in one batch during training to achieve a good balance between\nestimation accuracy and computational efficiency. Specifically, with a\nsimplified proxy objective and a reparameterization technique, we derive a\nnovel plug-and-play query allocator with minimal parameters. Theoretical\nresults are carried out to verify its optimality. We conduct extensive\nexperiments for fine-tuning Vision Transformers on various datasets and further\ndeploy the allocator to two black-box applications: prompt tuning and\nmultimodal alignment for foundation models. All findings demonstrate that our\nproposed allocator significantly enhances the scalability of forward-learning\nalgorithms, paving the way for real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the Thirteenth International Conference on Learning\n  Representations(ICLR 2025)",
    "pdf_url": "http://arxiv.org/pdf/2410.05966v3",
    "published_date": "2024-10-08 12:16:12 UTC",
    "updated_date": "2025-03-08 12:06:49 UTC"
  },
  {
    "arxiv_id": "2410.05964v1",
    "title": "STNet: Deep Audio-Visual Fusion Network for Robust Speaker Tracking",
    "authors": [
      "Yidi Li",
      "Hong Liu",
      "Bing Yang"
    ],
    "abstract": "Audio-visual speaker tracking aims to determine the location of human targets\nin a scene using signals captured by a multi-sensor platform, whose accuracy\nand robustness can be improved by multi-modal fusion methods. Recently, several\nfusion methods have been proposed to model the correlation in multiple\nmodalities. However, for the speaker tracking problem, the cross-modal\ninteraction between audio and visual signals hasn't been well exploited. To\nthis end, we present a novel Speaker Tracking Network (STNet) with a deep\naudio-visual fusion model in this work. We design a visual-guided acoustic\nmeasurement method to fuse heterogeneous cues in a unified localization space,\nwhich employs visual observations via a camera model to construct the enhanced\nacoustic map. For feature fusion, a cross-modal attention module is adopted to\njointly model multi-modal contexts and interactions. The correlated information\nbetween audio and visual features is further interacted in the fusion model.\nMoreover, the STNet-based tracker is applied to multi-speaker cases by a\nquality-aware module, which evaluates the reliability of multi-modal\nobservations to achieve robust tracking in complex scenarios. Experiments on\nthe AV16.3 and CAV3D datasets show that the proposed STNet-based tracker\noutperforms uni-modal methods and state-of-the-art audio-visual speaker\ntrackers.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05964v1",
    "published_date": "2024-10-08 12:15:17 UTC",
    "updated_date": "2024-10-08 12:15:17 UTC"
  },
  {
    "arxiv_id": "2410.07250v2",
    "title": "Lightweight Deep Learning Framework for Accurate Particle Flow Energy Reconstruction",
    "authors": [
      "Yu Wang",
      "Yangguang Zhang",
      "Shengxiang Lin",
      "Xingyi Zhang",
      "Han Zhang"
    ],
    "abstract": "Under extreme operating conditions, characterized by high particle\nmultiplicity and heavily overlapping shower energy deposits, classical particle\nflow algorithms encounter pronounced limitations in resolution, efficiency, and\naccuracy. To address this challenge, this paper proposes and systematically\nevaluates a deep learning reconstruction framework: For multichannel sparse\nfeatures, we design a hybrid loss function combining weighted mean squared\nerror with structural similarity index, effectively balancing pixel-level\naccuracy and structural fidelity. By integrating 3D convolutions,\nSqueeze-and-Excitation channel attention, and Offset self-attention modules\ninto baseline convolutional neural networks, we enhance the model's capability\nto capture cross-modal spatiotemporal correlations and energy-displacement\nnonlinearities. Validated on custom-constructed simulation data and Pythia jet\ndatasets, the framework's 90K-parameter lightweight variant approaches the\nperformance of 5M-parameter baselines, while the 25M-parameter 3D model\nachieves state-of-the-art results in both interpolation and extrapolation\ntasks. Comprehensive experiments quantitatively evaluate component\ncontributions and provide performance-parameter trade-off guidelines. All core\ncode and data processing scripts are open-sourced on a GitHub repository to\nfacilitate community reproducibility and extension.",
    "categories": [
      "physics.ins-det",
      "cs.AI"
    ],
    "primary_category": "physics.ins-det",
    "comment": "12 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.07250v2",
    "published_date": "2024-10-08 11:49:18 UTC",
    "updated_date": "2025-05-12 09:19:56 UTC"
  },
  {
    "arxiv_id": "2410.05938v1",
    "title": "EMMA: Empowering Multi-modal Mamba with Structural and Hierarchical Alignment",
    "authors": [
      "Yifei Xing",
      "Xiangyuan Lan",
      "Ruiping Wang",
      "Dongmei Jiang",
      "Wenjun Huang",
      "Qingfang Zheng",
      "Yaowei Wang"
    ],
    "abstract": "Mamba-based architectures have shown to be a promising new direction for deep\nlearning models owing to their competitive performance and sub-quadratic\ndeployment speed. However, current Mamba multi-modal large language models\n(MLLM) are insufficient in extracting visual features, leading to imbalanced\ncross-modal alignment between visual and textural latents, negatively impacting\nperformance on multi-modal tasks. In this work, we propose Empowering\nMulti-modal Mamba with Structural and Hierarchical Alignment (EMMA), which\nenables the MLLM to extract fine-grained visual information. Specifically, we\npropose a pixel-wise alignment module to autoregressively optimize the learning\nand processing of spatial image-level features along with textual tokens,\nenabling structural alignment at the image level. In addition, to prevent the\ndegradation of visual information during the cross-model alignment process, we\npropose a multi-scale feature fusion (MFF) module to combine multi-scale visual\nfeatures from intermediate layers, enabling hierarchical alignment at the\nfeature level. Extensive experiments are conducted across a variety of\nmulti-modal benchmarks. Our model shows lower latency than other Mamba-based\nMLLMs and is nearly four times faster than transformer-based MLLMs of similar\nscale during inference. Due to better cross-modal alignment, our model exhibits\nlower degrees of hallucination and enhanced sensitivity to visual details,\nwhich manifests in superior performance across diverse multi-modal benchmarks.\nCode will be provided.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05938v1",
    "published_date": "2024-10-08 11:41:55 UTC",
    "updated_date": "2024-10-08 11:41:55 UTC"
  },
  {
    "arxiv_id": "2410.05937v2",
    "title": "Athanor: Local Search over Abstract Constraint Specifications",
    "authors": [
      "Saad Attieh",
      "Nguyen Dang",
      "Christopher Jefferson",
      "Ian Miguel",
      "Peter Nightingale"
    ],
    "abstract": "Local search is a common method for solving combinatorial optimisation\nproblems. We focus on general-purpose local search solvers that accept as input\na constraint model - a declarative description of a problem consisting of a set\nof decision variables under a set of constraints. Existing approaches typically\ntake as input models written in solver-independent constraint modelling\nlanguages like MiniZinc. The Athanor solver we describe herein differs in that\nit begins from a specification of a problem in the abstract constraint\nspecification language Essence, which allows problems to be described without\ncommitment to low-level modelling decisions through its support for a rich set\nof abstract types. The advantage of proceeding from Essence is that the\nstructure apparent in a concise, abstract specification of a problem can be\nexploited to generate high quality neighbourhoods automatically, avoiding the\ndifficult task of identifying that structure in an equivalent constraint model.\nBased on the twin benefits of neighbourhoods derived from high level types and\nthe scalability derived by searching directly over those types, our empirical\nresults demonstrate strong performance in practice relative to existing\nsolution methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "72 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.05937v2",
    "published_date": "2024-10-08 11:41:38 UTC",
    "updated_date": "2025-01-02 14:41:10 UTC"
  },
  {
    "arxiv_id": "2410.05930v1",
    "title": "Fortify Your Foundations: Practical Privacy and Security for Foundation Model Deployments In The Cloud",
    "authors": [
      "Marcin Chrapek",
      "Anjo Vahldiek-Oberwagner",
      "Marcin Spoczynski",
      "Scott Constable",
      "Mona Vij",
      "Torsten Hoefler"
    ],
    "abstract": "Foundation Models (FMs) display exceptional performance in tasks such as\nnatural language processing and are being applied across a growing range of\ndisciplines. Although typically trained on large public datasets, FMs are often\nfine-tuned or integrated into Retrieval-Augmented Generation (RAG) systems,\nwhich rely on private data. This access, along with their size and costly\ntraining, heightens the risk of intellectual property theft. Moreover,\nmultimodal FMs may expose sensitive information. In this work, we examine the\nFM threat model and discuss the practicality and comprehensiveness of various\napproaches for securing against them, such as ML-based methods and trusted\nexecution environments (TEEs). We demonstrate that TEEs offer an effective\nbalance between strong security properties, usability, and performance.\nSpecifically, we present a solution achieving less than 10\\% overhead versus\nbare metal for the full Llama2 7B and 13B inference pipelines running inside\n\\intel\\ SGX and \\intel\\ TDX. We also share our configuration files and insights\nfrom our implementation. To our knowledge, our work is the first to show the\npracticality of TEEs for securing FMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05930v1",
    "published_date": "2024-10-08 11:33:09 UTC",
    "updated_date": "2024-10-08 11:33:09 UTC"
  },
  {
    "arxiv_id": "2410.05928v1",
    "title": "Beyond Captioning: Task-Specific Prompting for Improved VLM Performance in Mathematical Reasoning",
    "authors": [
      "Ayush Singh",
      "Mansi Gupta",
      "Shivank Garg",
      "Abhinav Kumar",
      "Vansh Agrawal"
    ],
    "abstract": "Vision-Language Models (VLMs) have transformed tasks requiring visual and\nreasoning abilities, such as image retrieval and Visual Question Answering\n(VQA). Despite their success, VLMs face significant challenges with tasks\ninvolving geometric reasoning, algebraic problem-solving, and counting. These\nlimitations stem from difficulties effectively integrating multiple modalities\nand accurately interpreting geometry-related tasks. Various works claim that\nintroducing a captioning pipeline before VQA tasks enhances performance. We\nincorporated this pipeline for tasks involving geometry, algebra, and counting.\nWe found that captioning results are not generalizable, specifically with\nlarger VLMs primarily trained on downstream QnA tasks showing random\nperformance on math-related challenges. However, we present a promising\nalternative: task-based prompting, enriching the prompt with task-specific\nguidance. This approach shows promise and proves more effective than direct\ncaptioning methods for math-heavy problems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05928v1",
    "published_date": "2024-10-08 11:29:40 UTC",
    "updated_date": "2024-10-08 11:29:40 UTC"
  },
  {
    "arxiv_id": "2410.18097v3",
    "title": "RRADistill: Distilling LLMs' Passage Ranking Ability for Long-Tail Queries Document Re-Ranking on a Search Engine",
    "authors": [
      "Nayoung Choi",
      "Youngjune Lee",
      "Gyu-Hwung Cho",
      "Haeyu Jeong",
      "Jungmin Kong",
      "Saehun Kim",
      "Keunchan Park",
      "Sarah Cho",
      "Inchang Jeong",
      "Gyohee Nam",
      "Sunghoon Han",
      "Wonil Yang",
      "Jaeho Choi"
    ],
    "abstract": "Large Language Models (LLMs) excel at understanding the semantic\nrelationships between queries and documents, even with lengthy and complex\nlong-tail queries. These queries are challenging for feedback-based rankings\ndue to sparse user engagement and limited feedback, making LLMs' ranking\nability highly valuable. However, the large size and slow inference of LLMs\nnecessitate the development of smaller, more efficient models (sLLMs).\nRecently, integrating ranking label generation into distillation techniques has\nbecome crucial, but existing methods underutilize LLMs' capabilities and are\ncumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose\nan efficient label generation pipeline and novel sLLM training methods for both\nencoder and decoder models. We introduce an encoder-based method using a Term\nControl Layer to capture term matching signals and a decoder-based model with a\nranking layer for enhanced understanding. A/B testing on a Korean-based search\nplatform, validates the effectiveness of our approach in improving re-ranking\nfor long-tail queries.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to EMNLP 2024 Industry Track. First two authors contributed\n  equally",
    "pdf_url": "http://arxiv.org/pdf/2410.18097v3",
    "published_date": "2024-10-08 11:28:06 UTC",
    "updated_date": "2024-11-21 14:23:49 UTC"
  },
  {
    "arxiv_id": "2410.10868v3",
    "title": "Large Continual Instruction Assistant",
    "authors": [
      "Jingyang Qiao",
      "Zhizhong Zhang",
      "Xin Tan",
      "Yanyun Qu",
      "Shouhong Ding",
      "Yuan Xie"
    ],
    "abstract": "Continual Instruction Tuning (CIT) is adopted to continually instruct Large\nModels to follow human intent data by data. It is observed that existing\ngradient update would heavily destroy the performance on previous datasets\nduring CIT process. Instead, Exponential Moving Average (EMA), owns the ability\nto trace previous parameters, which can aid in decreasing forgetting.\nNonetheless, its stable balance weight fails to deal with the ever-changing\ndatasets, leading to the out-of-balance between plasticity and stability. In\nthis paper, we propose a general continual instruction tuning framework to\naddress the challenge. Starting from the trade-off prerequisite and EMA update,\nwe propose the plasticity and stability ideal condition. Based on Taylor\nexpansion in the loss function, we find the optimal balance weight can be\nautomatically determined by the gradients and learned parameters. Therefore, we\npropose a stable-plasticity balanced coefficient to avoid knowledge confusion.\nBased on the semantic similarity of the instructions, we can determine whether\nto retrain or expand the training parameters and allocate the most suitable\nparameters for the testing instances. Extensive experiments across multiple\ncontinual instruction tuning benchmarks demonstrate that our approach not only\nenhances anti-forgetting capabilities but also significantly improves overall\ncontinual tuning performance. For example, based on LLaVA-7B, the forgetting is\nreduced from 5.42 to 1.93. Our code will be made publicly available soon.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10868v3",
    "published_date": "2024-10-08 11:24:59 UTC",
    "updated_date": "2025-02-19 07:01:35 UTC"
  },
  {
    "arxiv_id": "2410.05920v3",
    "title": "FINALLY: fast and universal speech enhancement with studio-like quality",
    "authors": [
      "Nicholas Babaev",
      "Kirill Tamogashev",
      "Azat Saginbaev",
      "Ivan Shchekotov",
      "Hanbin Bae",
      "Hosang Sung",
      "WonJun Lee",
      "Hoon-Young Cho",
      "Pavel Andreev"
    ],
    "abstract": "In this paper, we address the challenge of speech enhancement in real-world\nrecordings, which often contain various forms of distortion, such as background\nnoise, reverberation, and microphone artifacts. We revisit the use of\nGenerative Adversarial Networks (GANs) for speech enhancement and theoretically\nshow that GANs are naturally inclined to seek the point of maximum density\nwithin the conditional clean speech distribution, which, as we argue, is\nessential for the speech enhancement task. We study various feature extractors\nfor perceptual loss to facilitate the stability of adversarial training,\ndeveloping a methodology for probing the structure of the feature space. This\nleads us to integrate WavLM-based perceptual loss into MS-STFT adversarial\ntraining pipeline, creating an effective and stable training procedure for the\nspeech enhancement model. The resulting speech enhancement model, which we\nrefer to as FINALLY, builds upon the HiFi++ architecture, augmented with a\nWavLM encoder and a novel training pipeline. Empirical results on various\ndatasets confirm our model's ability to produce clear, high-quality speech at\n48 kHz, achieving state-of-the-art performance in the field of speech\nenhancement. Demo page: https://samsunglabs.github.io/FINALLY-page",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.05920v3",
    "published_date": "2024-10-08 11:16:03 UTC",
    "updated_date": "2024-10-31 08:47:01 UTC"
  },
  {
    "arxiv_id": "2410.05915v2",
    "title": "Give me a hint: Can LLMs take a hint to solve math problems?",
    "authors": [
      "Vansh Agrawal",
      "Pratham Singla",
      "Amitoj Singh Miglani",
      "Shivank Garg",
      "Ayush Mangal"
    ],
    "abstract": "While state-of-the-art LLMs have shown poor logical and basic mathematical\nreasoning, recent works try to improve their problem-solving abilities using\nprompting techniques. We propose giving \"hints\" to improve the language model's\nperformance on advanced mathematical problems, taking inspiration from how\nhumans approach math pedagogically. We also test robustness to adversarial\nhints and demonstrate their sensitivity to them. We demonstrate the\neffectiveness of our approach by evaluating various diverse LLMs, presenting\nthem with a broad set of problems of different difficulties and topics from the\nMATH dataset and comparing against techniques such as one-shot, few-shot, and\nchain of thought prompting.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05915v2",
    "published_date": "2024-10-08 11:09:31 UTC",
    "updated_date": "2024-11-09 08:32:47 UTC"
  },
  {
    "arxiv_id": "2410.10867v1",
    "title": "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics",
    "authors": [
      "Théo Gigant",
      "Camille Guinaudeau",
      "Marc Decombas",
      "Frédéric Dufaux"
    ],
    "abstract": "Automatic metrics are used as proxies to evaluate abstractive summarization\nsystems when human annotations are too expensive. To be useful, these metrics\nshould be fine-grained, show a high correlation with human annotations, and\nideally be independent of reference quality; however, most standard evaluation\nmetrics for summarization are reference-based, and existing reference-free\nmetrics correlate poorly with relevance, especially on summaries of longer\ndocuments. In this paper, we introduce a reference-free metric that correlates\nwell with human evaluated relevance, while being very cheap to compute. We show\nthat this metric can also be used alongside reference-based metrics to improve\ntheir robustness in low quality reference settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10867v1",
    "published_date": "2024-10-08 11:09:25 UTC",
    "updated_date": "2024-10-08 11:09:25 UTC"
  },
  {
    "arxiv_id": "2410.05911v1",
    "title": "Accelerating Error Correction Code Transformers",
    "authors": [
      "Matan Levy",
      "Yoni Choukroun",
      "Lior Wolf"
    ],
    "abstract": "Error correction codes (ECC) are crucial for ensuring reliable information\ntransmission in communication systems. Choukroun & Wolf (2022b) recently\nintroduced the Error Correction Code Transformer (ECCT), which has demonstrated\npromising performance across various transmission channels and families of\ncodes. However, its high computational and memory demands limit its practical\napplications compared to traditional decoding algorithms. Achieving effective\nquantization of the ECCT presents significant challenges due to its inherently\nsmall architecture, since existing, very low-precision quantization techniques\noften lead to performance degradation in compact neural networks. In this\npaper, we introduce a novel acceleration method for transformer-based decoders.\nWe first propose a ternary weight quantization method specifically designed for\nthe ECCT, inducing a decoder with multiplication-free linear layers. We present\nan optimized self-attention mechanism to reduce computational complexity via\ncodeaware multi-heads processing. Finally, we provide positional encoding via\nthe Tanner graph eigendecomposition, enabling a richer representation of the\ngraph connectivity. The approach not only matches or surpasses ECCT's\nperformance but also significantly reduces energy consumption, memory\nfootprint, and computational complexity. Our method brings transformer-based\nerror correction closer to practical implementation in resource-constrained\nenvironments, achieving a 90% compression ratio and reducing arithmetic\noperation energy consumption by at least 224 times on modern hardware.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05911v1",
    "published_date": "2024-10-08 11:07:55 UTC",
    "updated_date": "2024-10-08 11:07:55 UTC"
  },
  {
    "arxiv_id": "2410.05903v1",
    "title": "Automatic Summarization of Long Documents",
    "authors": [
      "Naman Chhibbar",
      "Jugal Kalita"
    ],
    "abstract": "A vast amount of textual data is added to the internet daily, making\nutilization and interpretation of such data difficult and cumbersome. As a\nresult, automatic text summarization is crucial for extracting relevant\ninformation, saving precious reading time. Although many transformer-based\nmodels excel in summarization, they are constrained by their input size,\npreventing them from processing texts longer than their context size. This\nstudy introduces three novel algorithms that allow any LLM to efficiently\novercome its input size limitation, effectively utilizing its full potential\nwithout any architectural modifications. We test our algorithms on texts with\nmore than 70,000 words, and our experiments show a significant increase in\nBERTScore with competitive ROUGE scores.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages (including bibliography) with 6 figures. ACL 2023 proceedings\n  format",
    "pdf_url": "http://arxiv.org/pdf/2410.05903v1",
    "published_date": "2024-10-08 11:00:49 UTC",
    "updated_date": "2024-10-08 11:00:49 UTC"
  },
  {
    "arxiv_id": "2410.05902v1",
    "title": "Mini-Batch Kernel $k$-means",
    "authors": [
      "Ben Jourdan",
      "Gregory Schwartzman"
    ],
    "abstract": "We present the first mini-batch kernel $k$-means algorithm, offering an order\nof magnitude improvement in running time compared to the full batch algorithm.\nA single iteration of our algorithm takes $\\widetilde{O}(kb^2)$ time,\nsignificantly faster than the $O(n^2)$ time required by the full batch kernel\n$k$-means, where $n$ is the dataset size and $b$ is the batch size. Extensive\nexperiments demonstrate that our algorithm consistently achieves a 10-100x\nspeedup with minimal loss in quality, addressing the slow runtime that has\nlimited kernel $k$-means adoption in practice. We further complement these\nresults with a theoretical analysis under an early stopping condition, proving\nthat with a batch size of $\\widetilde{\\Omega}(\\max \\{\\gamma^{4}, \\gamma^{2}\\}\n\\cdot \\epsilon^{-2})$, the algorithm terminates in $O(\\gamma^2/\\epsilon)$\niterations with high probability, where $\\gamma$ bounds the norm of points in\nfeature space and $\\epsilon$ is a termination threshold. Our analysis holds for\nany reasonable center initialization, and when using $k$-means++\ninitialization, the algorithm achieves an approximation ratio of $O(\\log k)$ in\nexpectation. For normalized kernels, such as Gaussian or Laplacian it holds\nthat $\\gamma=1$. Taking $\\epsilon = O(1)$ and $b=\\Theta(\\log n)$, the algorithm\nterminates in $O(1)$ iterations, with each iteration running in\n$\\widetilde{O}(k)$ time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2304.00419",
    "pdf_url": "http://arxiv.org/pdf/2410.05902v1",
    "published_date": "2024-10-08 10:59:14 UTC",
    "updated_date": "2024-10-08 10:59:14 UTC"
  },
  {
    "arxiv_id": "2410.18096v1",
    "title": "$M^3EL$: A Multi-task Multi-topic Dataset for Multi-modal Entity Linking",
    "authors": [
      "Fang Wang",
      "Shenglin Yin",
      "Xiaoying Bai",
      "Minghao Hu",
      "Tianwei Yan",
      "Yi Liang"
    ],
    "abstract": "Multi-modal Entity Linking (MEL) is a fundamental component for various\ndownstream tasks. However, existing MEL datasets suffer from small scale,\nscarcity of topic types and limited coverage of tasks, making them incapable of\neffectively enhancing the entity linking capabilities of multi-modal models. To\naddress these obstacles, we propose a dataset construction pipeline and publish\n$M^3EL$, a large-scale dataset for MEL. $M^3EL$ includes 79,625 instances,\ncovering 9 diverse multi-modal tasks, and 5 different topics. In addition, to\nfurther improve the model's adaptability to multi-modal tasks, We propose a\nmodality-augmented training strategy. Utilizing $M^3EL$ as a corpus, train the\n$\\textit{CLIP}_{\\textit{ND}}$ model based on $\\textit{CLIP}\n(\\textit{ViT}-\\textit{B}-\\textit{32})$, and conduct a comparative analysis with\nan existing multi-modal baselines. Experimental results show that the existing\nmodels perform far below expectations (ACC of 49.4%-75.8%), After analysis, it\nwas obtained that small dataset sizes, insufficient modality task coverage, and\nlimited topic diversity resulted in poor generalisation of multi-modal models.\nOur dataset effectively addresses these issues, and the\n$\\textit{CLIP}_{\\textit{ND}}$ model fine-tuned with $M^3EL$ shows a significant\nimprovement in accuracy, with an average improvement of 9.3% to 25% across\nvarious tasks. Our dataset is available at\nhttps://anonymous.4open.science/r/M3EL.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18096v1",
    "published_date": "2024-10-08 10:52:23 UTC",
    "updated_date": "2024-10-08 10:52:23 UTC"
  },
  {
    "arxiv_id": "2410.05892v1",
    "title": "Towards an Autonomous Surface Vehicle Prototype for Artificial Intelligence Applications of Water Quality Monitoring",
    "authors": [
      "Luis Miguel Díaz",
      "Samuel Yanes Luis",
      "Alejandro Mendoza Barrionuevo",
      "Dame Seck Diop",
      "Manuel Perales",
      "Alejandro Casado",
      "Sergio Toral",
      "Daniel Gutiérrez"
    ],
    "abstract": "The use of Autonomous Surface Vehicles, equipped with water quality sensors\nand artificial vision systems, allows for a smart and adaptive deployment in\nwater resources environmental monitoring. This paper presents a real\nimplementation of a vehicle prototype that to address the use of Artificial\nIntelligence algorithms and enhanced sensing techniques for water quality\nmonitoring. The vehicle is fully equipped with high-quality sensors to measure\nwater quality parameters and water depth. Furthermore, by means of a\nstereo-camera, it also can detect and locate macro-plastics in real\nenvironments by means of deep visual models, such as YOLOv5. In this paper,\nexperimental results, carried out in Lago Mayor (Sevilla), has been presented\nas proof of the capabilities of the proposed architecture. The overall system,\nand the early results obtained, are expected to provide a solid example of a\nreal platform useful for the water resource monitoring task, and to serve as a\nreal case scenario for deploying Artificial Intelligence algorithms, such as\npath planning, artificial vision, etc.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05892v1",
    "published_date": "2024-10-08 10:35:32 UTC",
    "updated_date": "2024-10-08 10:35:32 UTC"
  },
  {
    "arxiv_id": "2410.05889v1",
    "title": "Deep learning-based fault identification in condition monitoring",
    "authors": [
      "Hariom Dhungana",
      "Suresh Kumar Mukhiya",
      "Pragya Dhungana",
      "Benjamin Karic"
    ],
    "abstract": "Vibration-based condition monitoring techniques are commonly used to identify\nfaults in rolling element bearings. Accuracy and speed of fault detection\nprocedures are critical performance measures in condition monitoring. Delay is\nespecially important in remote condition monitoring and time-sensitive\nindustrial applications. While most existing methods focus on accuracy, little\nattention has been given to the inference time in the fault identification\nprocess. In this paper, we address this gap by presenting a Convolutional\nNeural Network (CNN) based approach for real-time fault identification in\nrolling element bearings. We encode raw vibration signals into two-dimensional\nimages using various encoding methods and use these with a CNN to classify\nseveral categories of bearing fault types and sizes. We analyse the interplay\nbetween fault identification accuracy and processing time. For training and\nevaluation we use a bearing failure CWRU dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05889v1",
    "published_date": "2024-10-08 10:31:13 UTC",
    "updated_date": "2024-10-08 10:31:13 UTC"
  },
  {
    "arxiv_id": "2410.10866v1",
    "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
    "authors": [
      "YuXuan Wu",
      "Bonaventure F. P. Dossou",
      "Dianbo Liu"
    ],
    "abstract": "Large Language Models (LLMs) offer extensive knowledge across various\ndomains, but they may inadvertently memorize sensitive, unauthorized, or\nmalicious data, such as personal information in the medical and financial\nsectors. Machine unlearning methods aim to remove specific information from\nmodels after training to address this. However, current approaches require\nadditional model training or struggle to effectively erase particular data\npoints and their associated context due to LLMs' complex, dense, and continuous\nnature. In this study, we propose a novel amortized unlearning approach using\ncodebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to\ndecompose the activation space and regulate information flow, our method\nefficiently unlearns targeted information while preserving the model's\nperformance on unrelated data. To the best of our knowledge, this is the first\nwork that successfully enables unlearning specific topics with contextual\nrelevance in an LLM, marking a significant step towards real-world applications\nof machine unlearning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10866v1",
    "published_date": "2024-10-08 10:26:22 UTC",
    "updated_date": "2024-10-08 10:26:22 UTC"
  },
  {
    "arxiv_id": "2410.18095v2",
    "title": "Ethical Leadership in the Age of AI Challenges, Opportunities and Framework for Ethical Leadership",
    "authors": [
      "Udaya Chandrika Kandasamy"
    ],
    "abstract": "Artificial Intelligence is currently and rapidly changing the way\norganizations and businesses operate. Ethical leadership has become\nsignificantly important since organizations and businesses across various\nsectors are evolving with AI. Organizations and businesses may be facing\nseveral challenges and potential opportunities when using AI. Ethical\nleadership plays a central role in guiding organizations in facing those\nchallenges and maximizing on those opportunities. This article explores the\nessence of ethical leadership in the age of AI, starting with a simplified\nintroduction of ethical leadership and AI, then dives into an understanding of\nethical leadership, its characteristics and importance, the ethical challenges\nAI causes including bias in AI algorithms. The opportunities for ethical\nleadership in the age of AI answers the question: What actionable strategies\ncan leaders employ to address the challenges and leverage opportunities? and\ndescribes the benefits for organizations through these opportunities. A\nproposed framework for ethical leadership is presented in this article,\nincorporating the core components: fairness, transparency, sustainability etc.\nThrough the importance of interdisciplinary collaboration, case studies of\nethical leadership in AI, and recommendations, this article emphasizes that\nethical leadership in the age of AI is morally essential and strategically\nadvantageous.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "15 pages, submitted to SAGE for review",
    "pdf_url": "http://arxiv.org/pdf/2410.18095v2",
    "published_date": "2024-10-08 10:17:19 UTC",
    "updated_date": "2024-10-30 18:30:56 UTC"
  },
  {
    "arxiv_id": "2410.18094v1",
    "title": "Self-supervised inter-intra period-aware ECG representation learning for detecting atrial fibrillation",
    "authors": [
      "Xiangqian Zhu",
      "Mengnan Shi",
      "Xuexin Yu",
      "Chang Liu",
      "Xiaocong Lian",
      "Jintao Fei",
      "Jiangying Luo",
      "Xin Jin",
      "Ping Zhang",
      "Xiangyang Ji"
    ],
    "abstract": "Atrial fibrillation is a commonly encountered clinical arrhythmia associated\nwith stroke and increased mortality. Since professional medical knowledge is\nrequired for annotation, exploiting a large corpus of ECGs to develop accurate\nsupervised learning-based atrial fibrillation algorithms remains challenging.\nSelf-supervised learning (SSL) is a promising recipe for generalized ECG\nrepresentation learning, eliminating the dependence on expensive labeling.\nHowever, without well-designed incorporations of knowledge related to atrial\nfibrillation, existing SSL approaches typically suffer from unsatisfactory\ncapture of robust ECG representations. In this paper, we propose an inter-intra\nperiod-aware ECG representation learning approach. Considering ECGs of atrial\nfibrillation patients exhibit the irregularity in RR intervals and the absence\nof P-waves, we develop specific pre-training tasks for interperiod and\nintraperiod representations, aiming to learn the single-period stable\nmorphology representation while retaining crucial interperiod features. After\nfurther fine-tuning, our approach demonstrates remarkable AUC performances on\nthe BTCH dataset, \\textit{i.e.}, 0.953/0.996 for paroxysmal/persistent atrial\nfibrillation detection. On commonly used benchmarks of CinC2017 and CPSC2021,\nthe generalization capability and effectiveness of our methodology are\nsubstantiated with competitive results.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "q-bio.QM",
    "comment": "Preprint submitted to Biomedical Signal Processing and Control",
    "pdf_url": "http://arxiv.org/pdf/2410.18094v1",
    "published_date": "2024-10-08 10:03:52 UTC",
    "updated_date": "2024-10-08 10:03:52 UTC"
  },
  {
    "arxiv_id": "2410.05873v1",
    "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment",
    "authors": [
      "Amir Hossein Kargaran",
      "Ali Modarressi",
      "Nafiseh Nikeghbal",
      "Jana Diesner",
      "François Yvon",
      "Hinrich Schütze"
    ],
    "abstract": "English-centric large language models (LLMs) often show strong multilingual\ncapabilities. However, the multilingual performance of these models remains\nunclear and is not thoroughly evaluated for many languages. Most benchmarks for\nmultilinguality focus on classic NLP tasks, or cover a minimal number of\nlanguages. We introduce MEXA, a method for assessing the multilingual\ncapabilities of pre-trained English-centric LLMs using parallel sentences,\nwhich are available for more languages than existing downstream tasks. MEXA\nleverages the fact that English-centric LLMs use English as a kind of pivot\nlanguage in their intermediate layers. It computes the alignment between\nEnglish and non-English languages using parallel sentences to evaluate the\ntransfer of language understanding from English to other languages. This\nalignment can be used to estimate model performance in other languages. We\nconduct studies using various parallel datasets (FLORES-200 and Bible), models\n(Llama family, Gemma family, Mistral, and OLMo), and established downstream\ntasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute\nembeddings in decoder-only models. Our results show that MEXA, in its default\nsettings, achieves a statistically significant average Pearson correlation of\n0.90 with three established downstream tasks across nine models and two\nparallel datasets. This suggests that MEXA is a reliable method for estimating\nthe multilingual capabilities of English-centric LLMs, providing a clearer\nunderstanding of their multilingual potential and the inner workings of LLMs.\nLeaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code:\nhttps://github.com/cisnlp/Mexa.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05873v1",
    "published_date": "2024-10-08 09:59:23 UTC",
    "updated_date": "2024-10-08 09:59:23 UTC"
  },
  {
    "arxiv_id": "2410.05871v2",
    "title": "A second-order-like optimizer with adaptive gradient scaling for deep learning",
    "authors": [
      "Jérôme Bolte",
      "Ryan Boustany",
      "Edouard Pauwels",
      "Andrei Purica"
    ],
    "abstract": "In this empirical article, we introduce INNAprop, an optimization algorithm\nthat combines the INNA method with the RMSprop adaptive gradient scaling. It\nleverages second-order information and rescaling while keeping the memory\nrequirements of standard DL methods as AdamW or SGD with momentum. After giving\ngeometrical insights, we evaluate INNAprop on CIFAR-10, Food101, and ImageNet\nwith ResNets, VGG, DenseNet, and ViT, and on GPT-2 (OpenWebText) train from\nscratch and with LoRA fine-tuning (E2E). INNAprop consistently matches or\noutperforms AdamW both in training speed and accuracy, with minimal\nhyperparameter tuning in large-scale settings. Our code is publicly available\nat \\url{https://github.com/innaprop/innaprop}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05871v2",
    "published_date": "2024-10-08 09:58:38 UTC",
    "updated_date": "2024-12-12 12:07:07 UTC"
  },
  {
    "arxiv_id": "2410.05870v1",
    "title": "Heuristics for Partially Observable Stochastic Contingent Planning",
    "authors": [
      "Guy Shani"
    ],
    "abstract": "Acting to complete tasks in stochastic partially observable domains is an\nimportant problem in artificial intelligence, and is often formulated as a\ngoal-based POMDP. Goal-based POMDPs can be solved using the RTDP-BEL algorithm,\nthat operates by running forward trajectories from the initial belief to the\ngoal. These trajectories can be guided by a heuristic, and more accurate\nheuristics can result in significantly faster convergence. In this paper, we\ndevelop a heuristic function that leverages the structured representation of\ndomain models. We compute, in a relaxed space, a plan to achieve the goal,\nwhile taking into account the value of information, as well as the stochastic\neffects. We provide experiments showing that while our heuristic is slower to\ncompute, it requires an order of magnitude less trajectories before\nconvergence. Overall, it thus speeds up RTDP-BEL, particularly in problems\nwhere significant information gathering is needed.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05870v1",
    "published_date": "2024-10-08 09:57:16 UTC",
    "updated_date": "2024-10-08 09:57:16 UTC"
  },
  {
    "arxiv_id": "2410.05869v4",
    "title": "Believing is Seeing: Unobserved Object Detection using Generative Models",
    "authors": [
      "Subhransu S. Bhattacharjee",
      "Dylan Campbell",
      "Rahul Shome"
    ],
    "abstract": "Can objects that are not visible in an image -- but are in the vicinity of\nthe camera -- be detected? This study introduces the novel tasks of 2D, 2.5D\nand 3D unobserved object detection for predicting the location of nearby\nobjects that are occluded or lie outside the image frame. We adapt several\nstate-of-the-art pre-trained generative models to address this task, including\n2D and 3D diffusion models and vision-language models, and show that they can\nbe used to infer the presence of objects that are not directly observed. To\nbenchmark this task, we propose a suite of metrics that capture different\naspects of performance. Our empirical evaluation on indoor scenes from the\nRealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the\nuse of generative models for the unobserved object detection task.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "IEEE/CVF Computer Vision and Pattern Recognition 2025; 22 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.05869v4",
    "published_date": "2024-10-08 09:57:14 UTC",
    "updated_date": "2025-03-24 13:41:43 UTC"
  },
  {
    "arxiv_id": "2410.05864v4",
    "title": "From Tokens to Words: On the Inner Lexicon of LLMs",
    "authors": [
      "Guy Kaplan",
      "Matanel Oren",
      "Yuval Reif",
      "Roy Schwartz"
    ],
    "abstract": "Natural language is composed of words, but modern large language models\n(LLMs) process sub-words as input. A natural question raised by this\ndiscrepancy is whether LLMs encode words internally, and if so how. We present\nevidence that LLMs engage in an intrinsic detokenization process, where\nsub-word sequences are combined into coherent whole-word representations at\ntheir last token. Our experiments show that this process primarily takes place\nwithin the early and middle layers of the model. We further demonstrate its\nrobustness to arbitrary splits (e.g., \"cats\" to \"ca\" and \"ts\"), typos, and\nimportantly-to out-of-vocabulary words: when feeding the last token internal\nrepresentations of such words to the model as input, it can \"understand\" them\nas the complete word despite never seeing such representations as input during\ntraining. Our findings suggest that LLMs maintain a latent vocabulary beyond\nthe tokenizer's scope. These insights provide a practical, finetuning-free\napplication for expanding the vocabulary of pre-trained models. By enabling the\naddition of new vocabulary words, we reduce input length and inference\niterations, which reduces both space and model latency, with little to no loss\nin model accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the International Conference on Learning Representations\n  (ICLR) 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.05864v4",
    "published_date": "2024-10-08 09:53:35 UTC",
    "updated_date": "2025-03-03 14:30:07 UTC"
  },
  {
    "arxiv_id": "2410.05860v1",
    "title": "MelissaDL x Breed: Towards Data-Efficient On-line Supervised Training of Multi-parametric Surrogates with Active Learning",
    "authors": [
      "Sofya Dymchenko",
      "Abhishek Purandare",
      "Bruno Raffin"
    ],
    "abstract": "Artificial intelligence is transforming scientific computing with deep neural\nnetwork surrogates that approximate solutions to partial differential equations\n(PDEs). Traditional off-line training methods face issues with storage and I/O\nefficiency, as the training dataset has to be computed with numerical solvers\nup-front. Our previous work, the Melissa framework, addresses these problems by\nenabling data to be created \"on-the-fly\" and streamed directly into the\ntraining process. In this paper we introduce a new active learning method to\nenhance data-efficiency for on-line surrogate training. The surrogate is direct\nand multi-parametric, i.e., it is trained to predict a given timestep directly\nwith different initial and boundary conditions parameters. Our approach uses\nAdaptive Multiple Importance Sampling guided by training loss statistics, in\norder to focus NN training on the difficult areas of the parameter space.\nPreliminary results for 2D heat PDE demonstrate the potential of this method,\ncalled Breed, to improve the generalization capabilities of surrogates while\nreducing computational overhead.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05860v1",
    "published_date": "2024-10-08 09:52:15 UTC",
    "updated_date": "2024-10-08 09:52:15 UTC"
  },
  {
    "arxiv_id": "2410.05851v1",
    "title": "Communicating with Speakers and Listeners of Different Pragmatic Levels",
    "authors": [
      "Kata Naszadi",
      "Frans A. Oliehoek",
      "Christof Monz"
    ],
    "abstract": "This paper explores the impact of variable pragmatic competence on\ncommunicative success through simulating language learning and conversing\nbetween speakers and listeners with different levels of reasoning abilities.\nThrough studying this interaction, we hypothesize that matching levels of\nreasoning between communication partners would create a more beneficial\nenvironment for communicative success and language learning. Our research\nfindings indicate that learning from more explicit, literal language is\nadvantageous, irrespective of the learner's level of pragmatic competence.\nFurthermore, we find that integrating pragmatic reasoning during language\nlearning, not just during evaluation, significantly enhances overall\ncommunication performance. This paper provides key insights into the importance\nof aligning reasoning levels and incorporating pragmatic reasoning in\noptimizing communicative interactions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 main",
    "pdf_url": "http://arxiv.org/pdf/2410.05851v1",
    "published_date": "2024-10-08 09:42:37 UTC",
    "updated_date": "2024-10-08 09:42:37 UTC"
  },
  {
    "arxiv_id": "2410.18092v1",
    "title": "Two-Stage Radio Map Construction with Real Environments and Sparse Measurements",
    "authors": [
      "Yifan Wang",
      "Shu Sun",
      "Na Liu",
      "Lianming Xu",
      "Li Wang"
    ],
    "abstract": "Radio map construction based on extensive measurements is accurate but\nexpensive and time-consuming, while environment-aware radio map estimation\nreduces the costs at the expense of low accuracy. Considering accuracy and\ncosts, a first-predict-then-correct (FPTC) method is proposed by leveraging\ngenerative adversarial networks (GANs). A primary radio map is first predicted\nby a radio map prediction GAN (RMP-GAN) taking environmental information as\ninput. Then, the prediction result is corrected by a radio map correction GAN\n(RMC-GAN) with sparse measurements as guidelines. Specifically, the\nself-attention mechanism and residual-connection blocks are introduced to\nRMP-GAN and RMC-GAN to improve the accuracy, respectively. Experimental results\nvalidate that the proposed FPTC-GANs method achieves the best radio map\nconstruction performance, compared with the state-of-the-art methods.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18092v1",
    "published_date": "2024-10-08 09:15:27 UTC",
    "updated_date": "2024-10-08 09:15:27 UTC"
  },
  {
    "arxiv_id": "2410.05839v1",
    "title": "Bottom-up Anytime Discovery of Generalised Multimodal Graph Patterns for Knowledge Graphs",
    "authors": [
      "Xander Wilcke",
      "Rick Mourits",
      "Auke Rijpma",
      "Richard Zijdeman"
    ],
    "abstract": "Vast amounts of heterogeneous knowledge are becoming publicly available in\nthe form of knowledge graphs, often linking multiple sources of data that have\nnever been together before, and thereby enabling scholars to answer many new\nresearch questions. It is often not known beforehand, however, which questions\nthe data might have the answers to, potentially leaving many interesting and\nnovel insights to remain undiscovered. To support scholars during this\nscientific workflow, we introduce an anytime algorithm for the bottom-up\ndiscovery of generalized multimodal graph patterns in knowledge graphs. Each\npattern is a conjunction of binary statements with (data-) type variables,\nconstants, and/or value patterns. Upon discovery, the patterns are converted to\nSPARQL queries and presented in an interactive facet browser together with\nmetadata and provenance information, enabling scholars to explore, analyse, and\nshare queries. We evaluate our method from a user perspective, with the help of\ndomain experts in the humanities.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "68T10",
      "I.5.1"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05839v1",
    "published_date": "2024-10-08 09:07:27 UTC",
    "updated_date": "2024-10-08 09:07:27 UTC"
  },
  {
    "arxiv_id": "2410.05838v2",
    "title": "Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite Data Limit",
    "authors": [
      "Oleg Filatov",
      "Jan Ebert",
      "Jiangtao Wang",
      "Stefan Kesselheim"
    ],
    "abstract": "One of the main challenges in optimal scaling of large language models (LLMs)\nis the prohibitive cost of hyperparameter tuning, particularly learning rate\n$\\eta$ and batch size $B$. While techniques like $\\mu$P (Yang et al., 2022)\nprovide scaling rules for optimal $\\eta$ transfer in the infinite model size\nlimit, the optimal scaling behavior in the infinite data size limit remains\nunknown. We fill in this gap by observing for the first time an intricate\ndependence of optimal $\\eta$ scaling on the pretraining token budget $T$, $B$\nand its relation to the critical batch size $B_\\mathrm{crit}$, which we measure\nto evolve as $B_\\mathrm{crit} \\propto T$. Furthermore, we show that the optimal\nbatch size is positively correlated with $B_\\mathrm{crit}$: keeping it fixed\nbecomes suboptimal over time even if learning rate is scaled optimally.\nSurprisingly, our results demonstrate that the observed optimal $\\eta$ and $B$\ndynamics are preserved with $\\mu$P model scaling, challenging the conventional\nview of $B_\\mathrm{crit}$ dependence solely on loss value. Complementing\noptimality, we examine the sensitivity of loss to changes in learning rate,\nwhere we find the sensitivity to decrease with increase of $T$ and to remain\nconstant with $\\mu$P model scaling. We hope our results make the first step\ntowards a unified picture of the joint optimal data and model scaling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05838v2",
    "published_date": "2024-10-08 09:06:34 UTC",
    "updated_date": "2025-01-09 14:04:01 UTC"
  },
  {
    "arxiv_id": "2410.05827v1",
    "title": "Towards an Operational Responsible AI Framework for Learning Analytics in Higher Education",
    "authors": [
      "Alba Morales Tirado",
      "Paul Mulholland",
      "Miriam Fernandez"
    ],
    "abstract": "Universities are increasingly adopting data-driven strategies to enhance\nstudent success, with AI applications like Learning Analytics (LA) and\nPredictive Learning Analytics (PLA) playing a key role in identifying at-risk\nstudents, personalising learning, supporting teachers, and guiding educational\ndecision-making. However, concerns are rising about potential harms these\nsystems may pose, such as algorithmic biases leading to unequal support for\nminority students. While many have explored the need for Responsible AI in LA,\nexisting works often lack practical guidance for how institutions can\noperationalise these principles. In this paper, we propose a novel Responsible\nAI framework tailored specifically to LA in Higher Education (HE). We started\nby mapping 11 established Responsible AI frameworks, including those by leading\ntech companies, to the context of LA in HE. This led to the identification of\nseven key principles such as transparency, fairness, and accountability. We\nthen conducted a systematic review of the literature to understand how these\nprinciples have been applied in practice. Drawing from these findings, we\npresent a novel framework that offers practical guidance to HE institutions and\nis designed to evolve with community input, ensuring its relevance as LA\nsystems continue to develop.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "16 pages, 1 figure, submitted to LAK 25",
    "pdf_url": "http://arxiv.org/pdf/2410.05827v1",
    "published_date": "2024-10-08 08:55:24 UTC",
    "updated_date": "2024-10-08 08:55:24 UTC"
  },
  {
    "arxiv_id": "2410.05806v2",
    "title": "A Parameter Update Balancing Algorithm for Multi-task Ranking Models in Recommendation Systems",
    "authors": [
      "Jun Yuan",
      "Guohao Cai",
      "Zhenhua Dong"
    ],
    "abstract": "Multi-task ranking models have become essential for modern real-world\nrecommendation systems. While most recommendation researches focus on designing\nsophisticated models for specific scenarios, achieving performance improvement\nfor multi-task ranking models across various scenarios still remains a\nsignificant challenge. Training all tasks naively can result in inconsistent\nlearning, highlighting the need for the development of multi-task optimization\n(MTO) methods to tackle this challenge. Conventional methods assume that the\noptimal joint gradient on shared parameters leads to optimal parameter updates.\nHowever, the actual update on model parameters may deviates significantly from\ngradients when using momentum based optimizers such as Adam, and we design and\nexecute statistical experiments to support the observation. In this paper, we\npropose a novel Parameter Update Balancing algorithm for multi-task\noptimization, denoted as PUB. In contrast to traditional MTO method which are\nbased on gradient level tasks fusion or loss level tasks fusion, PUB is the\nfirst work to optimize multiple tasks through parameter update balancing.\nComprehensive experiments on benchmark multi-task ranking datasets demonstrate\nthat PUB consistently improves several multi-task backbones and achieves\nstate-of-the-art performance. Additionally, experiments on benchmark computer\nvision datasets show the great potential of PUB in various multi-task learning\nscenarios. Furthermore, we deployed our method for an industrial evaluation on\nthe real-world commercial platform, HUAWEI AppGallery, where PUB significantly\nenhances the online multi-task ranking model, efficiently managing the primary\ntraffic of a crucial channel.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by ICDM'24",
    "pdf_url": "http://arxiv.org/pdf/2410.05806v2",
    "published_date": "2024-10-08 08:39:15 UTC",
    "updated_date": "2025-02-12 07:38:08 UTC"
  },
  {
    "arxiv_id": "2410.18091v1",
    "title": "Predicting Fine-grained Behavioral and Psychological Symptoms of Dementia Based on Machine Learning and Smart Wearable Devices",
    "authors": [
      "Benny Wei-Yun Hsu",
      "Yu-Ming Chen",
      "Yuan-Han Yang",
      "Vincent S. Tseng"
    ],
    "abstract": "Behavioral and Psychological Symptoms of Dementia (BPSD) impact dementia care\nsubstantially, affecting both patients and caregivers. Effective management and\nearly detection of BPSD are crucial to reduce the stress and burden on\ncaregivers and healthcare systems. Despite the advancements in machine learning\nfor dementia prediction, there is a considerable gap in utilizing these methods\nfor BPSD prediction. This study aims to fill this gap by presenting a novel\npersonalized framework for BPSD prediction, utilizing physiological signals\nfrom smart wearable devices. Our personalized fine-grained BPSD prediction\nmethod accurately predicts BPSD occurrences by extracting individual behavioral\npatterns, while the generalized models identify diverse patterns and\ndifferentiate between various BPSD symptoms. Detailed comparisons between the\nproposed personalized method and conventional generalized methods reveals\nsubstantial improvements across all performance metrics, including a 16.0%\nincrease in AUC. These results demonstrate the potential of our proposed method\nin advancing dementia care by enabling proactive interventions and improving\npatient outcomes in real-world scenarios. To the best of our knowledge, this is\nthe first study that leverages physiological signals from smart wearable\ndevices to predict BPSD, marking a significant stride in dementia care\nresearch.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18091v1",
    "published_date": "2024-10-08 08:38:37 UTC",
    "updated_date": "2024-10-08 08:38:37 UTC"
  },
  {
    "arxiv_id": "2410.05805v1",
    "title": "PostCast: Generalizable Postprocessing for Precipitation Nowcasting via Unsupervised Blurriness Modeling",
    "authors": [
      "Junchao Gong",
      "Siwei Tu",
      "Weidong Yang",
      "Ben Fei",
      "Kun Chen",
      "Wenlong Zhang",
      "Xiaokang Yang",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "abstract": "Precipitation nowcasting plays a pivotal role in socioeconomic sectors,\nespecially in severe convective weather warnings. Although notable progress has\nbeen achieved by approaches mining the spatiotemporal correlations with deep\nlearning, these methods still suffer severe blurriness as the lead time\nincreases, which hampers accurate predictions for extreme precipitation. To\nalleviate blurriness, researchers explore generative methods conditioned on\nblurry predictions. However, the pairs of blurry predictions and corresponding\nground truth need to be generated in advance, making the training pipeline\ncumbersome and limiting the generality of generative models within blur modes\nthat appear in training data. By rethinking the blurriness in precipitation\nnowcasting as a blur kernel acting on predictions, we propose an unsupervised\npostprocessing method to eliminate the blurriness without the requirement of\ntraining with the pairs of blurry predictions and corresponding ground truth.\nSpecifically, we utilize blurry predictions to guide the generation process of\na pre-trained unconditional denoising diffusion probabilistic model (DDPM) to\nobtain high-fidelity predictions with eliminated blurriness. A zero-shot blur\nkernel estimation mechanism and an auto-scale denoise guidance strategy are\nintroduced to adapt the unconditional DDPM to any blurriness modes varying from\ndatasets and lead times in precipitation nowcasting. Extensive experiments are\nconducted on 7 precipitation radar datasets, demonstrating the generality and\nsuperiority of our method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05805v1",
    "published_date": "2024-10-08 08:38:23 UTC",
    "updated_date": "2024-10-08 08:38:23 UTC"
  },
  {
    "arxiv_id": "2410.05801v1",
    "title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",
    "authors": [
      "Bolei He",
      "Nuo Chen",
      "Xinran He",
      "Lingyong Yan",
      "Zhenkai Wei",
      "Jinchang Luo",
      "Zhen-Hua Ling"
    ],
    "abstract": "Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language\nModels (LLMs) by incorporating extensive knowledge retrieved from external\nsources. However, such approach encounters some challenges: Firstly, the\noriginal queries may not be suitable for precise retrieval, resulting in\nerroneous contextual knowledge; Secondly, the language model can easily\ngenerate inconsistent answer with external references due to their knowledge\nboundary limitation. To address these issues, we propose the\nchain-of-verification (CoV-RAG) to enhance the external retrieval correctness\nand internal generation consistency. Specifically, we integrate the\nverification module into the RAG, engaging in scoring, judgment, and rewriting.\nTo correct external retrieval errors, CoV-RAG retrieves new knowledge using a\nrevised query. To correct internal generation errors, we unify QA and\nverification tasks with a Chain-of-Thought (CoT) reasoning during training. Our\ncomprehensive experiments across various LLMs demonstrate the effectiveness and\nadaptability compared with other strong baselines. Especially, our CoV-RAG can\nsignificantly surpass the state-of-the-art baselines using different LLM\nbackbones.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 Findings. 9 pages, 4 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.05801v1",
    "published_date": "2024-10-08 08:34:54 UTC",
    "updated_date": "2024-10-08 08:34:54 UTC"
  },
  {
    "arxiv_id": "2410.05800v1",
    "title": "Core Tokensets for Data-efficient Sequential Training of Transformers",
    "authors": [
      "Subarnaduti Paul",
      "Manuel Brack",
      "Patrick Schramowski",
      "Kristian Kersting",
      "Martin Mundt"
    ],
    "abstract": "Deep networks are frequently tuned to novel tasks and continue learning from\nongoing data streams. Such sequential training requires consolidation of new\nand past information, a challenge predominantly addressed by retaining the most\nimportant data points - formally known as coresets. Traditionally, these\ncoresets consist of entire samples, such as images or sentences. However,\nrecent transformer architectures operate on tokens, leading to the famous\nassertion that an image is worth 16x16 words. Intuitively, not all of these\ntokens are equally informative or memorable. Going beyond coresets, we thus\npropose to construct a deeper-level data summary on the level of tokens. Our\nrespectively named core tokensets both select the most informative data points\nand leverage feature attribution to store only their most relevant features. We\ndemonstrate that core tokensets yield significant performance retention in\nincremental image classification, open-ended visual question answering, and\ncontinual image captioning with significantly reduced memory. In fact, we\nempirically find that a core tokenset of 1\\% of the data performs comparably to\nat least a twice as large and up to 10 times larger coreset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05800v1",
    "published_date": "2024-10-08 08:34:35 UTC",
    "updated_date": "2024-10-08 08:34:35 UTC"
  },
  {
    "arxiv_id": "2410.05791v1",
    "title": "FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance",
    "authors": [
      "Ruocheng Wang",
      "Pei Xu",
      "Haochen Shi",
      "Elizabeth Schumann",
      "C. Karen Liu"
    ],
    "abstract": "Piano playing requires agile, precise, and coordinated hand control that\nstretches the limits of dexterity. Hand motion models with the sophistication\nto accurately recreate piano playing have a wide range of applications in\ncharacter animation, embodied AI, biomechanics, and VR/AR. In this paper, we\nconstruct a first-of-its-kind large-scale dataset that contains approximately\n10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153\npieces of classical music. To capture natural performances, we designed a\nmarkerless setup in which motions are reconstructed from multi-view videos\nusing state-of-the-art pose estimation models. The motion data is further\nrefined via inverse kinematics using the high-resolution MIDI key-pressing data\nobtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the\ncollected dataset, we developed a pipeline that can synthesize\nphysically-plausible hand motions for musical scores outside of the dataset.\nOur approach employs a combination of imitation learning and reinforcement\nlearning to obtain policies for physics-based bimanual control involving the\ninteraction between hands and piano keys. To solve the sampling efficiency\nproblem with the large motion dataset, we use a diffusion model to generate\nnatural reference motions, which provide high-level trajectory and fingering\n(finger order and placement) information. However, the generated reference\nmotion alone does not provide sufficient accuracy for piano performance\nmodeling. We then further augmented the data by using musical similarity to\nretrieve similar motions from the captured dataset to boost the precision of\nthe RL policy. With the proposed method, our model generates natural, dexterous\nmotions that generalize to music from outside the training dataset.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.GR",
    "comment": "SIGGRAPH Asia 2024. Project page: https://for-elise.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.05791v1",
    "published_date": "2024-10-08 08:21:05 UTC",
    "updated_date": "2024-10-08 08:21:05 UTC"
  },
  {
    "arxiv_id": "2410.11866v1",
    "title": "An Innovative Solution: AI-Based Digital Screen-Integrated Tables for Educational Settings",
    "authors": [
      "S. Tamang",
      "D. J. Bora"
    ],
    "abstract": "In this paper, we have gone through different AI-Based frameworks used for\nvarious educational tasks like digital customized assignment allotment and\nperformance monitoring, identifying slow-learners and fast-learners, etc.\napplication describes a novel invention, digital screen-integrated tables,\ndesigned specifically for educational settings. The tables feature integrated\ndigital screens controlled by a central processing unit (CPU), enabling\nsynchronized display of educational content such as textbooks, presentations,\nexam questions, and interactive learning materials. Additionally, the invention\nfacilitates the collection of student performance data during classroom\nactivities and assessments. The gathered data is utilized for analysis using\nmachine learning models to identify patterns and trends in student learning\nbehaviours. By leveraging machine learning algorithms, educators can ascertain\nwhether a student is a fast learner or a slow learner, based on which, the\nteacher can allocate more resources to the slow learners. This innovative\napproach aims to address the evolving needs of modern classrooms by providing a\ndynamic and data-driven learning environment. The unique integration of digital\nscreens into traditional classroom furniture represents a significant\nadvancement in educational technology. This patent filing encompasses the\ndesign, functionality, and method of operation of the digital screen-integrated\ntables, emphasizing their innovative features and applications in educational\ninstitutions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.11866v1",
    "published_date": "2024-10-08 08:00:17 UTC",
    "updated_date": "2024-10-08 08:00:17 UTC"
  },
  {
    "arxiv_id": "2410.05779v3",
    "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
    "authors": [
      "Zirui Guo",
      "Lianghao Xia",
      "Yanhua Yu",
      "Tu Ao",
      "Chao Huang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05779v3",
    "published_date": "2024-10-08 08:00:12 UTC",
    "updated_date": "2025-04-28 17:36:27 UTC"
  },
  {
    "arxiv_id": "2410.18090v1",
    "title": "Liver Cancer Knowledge Graph Construction based on dynamic entity replacement and masking strategies RoBERTa-BiLSTM-CRF model",
    "authors": [
      "YiChi Zhang",
      "HaiLing Wang",
      "YongBin Gao",
      "XiaoJun Hu",
      "YingFang Fan",
      "ZhiJun Fang"
    ],
    "abstract": "Background: Liver cancer ranks as the fifth most common malignant tumor and\nthe second most fatal in our country. Early diagnosis is crucial, necessitating\nthat physicians identify liver cancer in patients at the earliest possible\nstage. However, the diagnostic process is complex and demanding. Physicians\nmust analyze a broad spectrum of patient data, encompassing physical condition,\nsymptoms, medical history, and results from various examinations and tests,\nrecorded in both structured and unstructured medical formats. This results in a\nsignificant workload for healthcare professionals. In response, integrating\nknowledge graph technology to develop a liver cancer knowledge graph-assisted\ndiagnosis and treatment system aligns with national efforts toward smart\nhealthcare. Such a system promises to mitigate the challenges faced by\nphysicians in diagnosing and treating liver cancer.\n  Methods: This paper addresses the major challenges in building a knowledge\ngraph for hepatocellular carcinoma diagnosis, such as the discrepancy between\npublic data sources and real electronic medical records, the effective\nintegration of which remains a key issue. The knowledge graph construction\nprocess consists of six steps: conceptual layer design, data preprocessing,\nentity identification, entity normalization, knowledge fusion, and graph\nvisualization. A novel Dynamic Entity Replacement and Masking Strategy (DERM)\nfor named entity recognition is proposed.\n  Results: A knowledge graph for liver cancer was established, including 7\nentity types such as disease, symptom, and constitution, containing 1495\nentities. The recognition accuracy of the model was 93.23%, the recall was\n94.69%, and the F1 score was 93.96%.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18090v1",
    "published_date": "2024-10-08 07:57:29 UTC",
    "updated_date": "2024-10-08 07:57:29 UTC"
  },
  {
    "arxiv_id": "2410.05777v1",
    "title": "Integrated Encoding and Quantization to Enhance Quanvolutional Neural Networks",
    "authors": [
      "Daniele Lizzio Bosco",
      "Beatrice Portelli",
      "Giuseppe Serra"
    ],
    "abstract": "Image processing is one of the most promising applications for quantum\nmachine learning (QML). Quanvolutional Neural Networks with non-trainable\nparameters are the preferred solution to run on current and near future quantum\ndevices. The typical input preprocessing pipeline for quanvolutional layers\ncomprises of four steps: optional input binary quantization, encoding classical\ndata into quantum states, processing the data to obtain the final quantum\nstates, decoding quantum states back to classical outputs. In this paper we\npropose two ways to enhance the efficiency of quanvolutional models. First, we\npropose a flexible data quantization approach with memoization, applicable to\nany encoding method. This allows us to increase the number of quantization\nlevels to retain more information or lower them to reduce the amount of circuit\nexecutions. Second, we introduce a new integrated encoding strategy, which\ncombines the encoding and processing steps in a single circuit. This method\nallows great flexibility on several architectural parameters (e.g., number of\nqubits, filter size, and circuit depth) making them adjustable to quantum\nhardware requirements. We compare our proposed integrated model with a\nclassical convolutional neural network and the well-known rotational encoding\nmethod, on two different classification tasks. The results demonstrate that our\nproposed model encoding exhibits a comparable or superior performance to the\nother models while requiring fewer quantum resources.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05777v1",
    "published_date": "2024-10-08 07:57:13 UTC",
    "updated_date": "2024-10-08 07:57:13 UTC"
  },
  {
    "arxiv_id": "2410.05767v2",
    "title": "Grounding is All You Need? Dual Temporal Grounding for Video Dialog",
    "authors": [
      "You Qin",
      "Wei Ji",
      "Xinze Lan",
      "Hao Fei",
      "Xun Yang",
      "Dan Guo",
      "Roger Zimmermann",
      "Lizi Liao"
    ],
    "abstract": "In the realm of video dialog response generation, the understanding of video\ncontent and the temporal nuances of conversation history are paramount. While a\nsegment of current research leans heavily on large-scale pretrained\nvisual-language models and often overlooks temporal dynamics, another delves\ndeep into spatial-temporal relationships within videos but demands intricate\nobject trajectory pre-extractions and sidelines dialog temporal dynamics. This\npaper introduces the Dual Temporal Grounding-enhanced Video Dialog model\n(DTGVD), strategically designed to merge the strengths of both dominant\napproaches. It emphasizes dual temporal relationships by predicting dialog\nturn-specific temporal regions, filtering video content accordingly, and\ngrounding responses in both video and dialog contexts. One standout feature of\nDTGVD is its heightened attention to chronological interplay. By recognizing\nand acting upon the dependencies between different dialog turns, it captures\nmore nuanced conversational dynamics. To further bolster the alignment between\nvideo and dialog temporal dynamics, we've implemented a list-wise contrastive\nlearning strategy. Within this framework, accurately grounded turn-clip\npairings are designated as positive samples, while less precise pairings are\ncategorized as negative. This refined classification is then funneled into our\nholistic end-to-end response generation mechanism. Evaluations using\nAVSD@DSTC-7 and AVSD@DSTC-8 datasets underscore the superiority of our\nmethodology.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05767v2",
    "published_date": "2024-10-08 07:48:34 UTC",
    "updated_date": "2024-11-14 11:27:41 UTC"
  },
  {
    "arxiv_id": "2410.05760v2",
    "title": "Training-free Diffusion Model Alignment with Sampling Demons",
    "authors": [
      "Po-Hung Yeh",
      "Kuang-Huei Lee",
      "Jun-Cheng Chen"
    ],
    "abstract": "Aligning diffusion models with user preferences has been a key challenge.\nExisting methods for aligning diffusion models either require retraining or are\nlimited to differentiable reward functions. To address these limitations, we\npropose a stochastic optimization approach, dubbed Demon, to guide the\ndenoising process at inference time without backpropagation through reward\nfunctions or model retraining. Our approach works by controlling noise\ndistribution in denoising steps to concentrate density on regions corresponding\nto high rewards through stochastic optimization. We provide comprehensive\ntheoretical and empirical evidence to support and validate our approach,\nincluding experiments that use non-differentiable sources of rewards such as\nVisual-Language Model (VLM) APIs and human judgements. To the best of our\nknowledge, the proposed approach is the first inference-time,\nbackpropagation-free preference alignment method for diffusion models. Our\nmethod can be easily integrated with existing diffusion models without further\ntraining. Our experiments show that the proposed approach significantly\nimproves the average aesthetics scores for text-to-image generation.\nImplementation is available at https://github.com/aiiu-lab/DemonSampling.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "35 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.05760v2",
    "published_date": "2024-10-08 07:33:49 UTC",
    "updated_date": "2025-02-27 12:13:38 UTC"
  },
  {
    "arxiv_id": "2410.05756v1",
    "title": "Learning the Generalizable Manipulation Skills on Soft-body Tasks via Guided Self-attention Behavior Cloning Policy",
    "authors": [
      "Xuetao Li",
      "Fang Gao",
      "Jun Yu",
      "Shaodong Li",
      "Feng Shuang"
    ],
    "abstract": "Embodied AI represents a paradigm in AI research where artificial agents are\nsituated within and interact with physical or virtual environments. Despite the\nrecent progress in Embodied AI, it is still very challenging to learn the\ngeneralizable manipulation skills that can handle large deformation and\ntopological changes on soft-body objects, such as clay, water, and soil. In\nthis work, we proposed an effective policy, namely GP2E behavior cloning\npolicy, which can guide the agent to learn the generalizable manipulation\nskills from soft-body tasks, including pouring, filling, hanging, excavating,\npinching, and writing. Concretely, we build our policy from three insights:(1)\nExtracting intricate semantic features from point cloud data and seamlessly\nintegrating them into the robot's end-effector frame; (2) Capturing\nlong-distance interactions in long-horizon tasks through the incorporation of\nour guided self-attention module; (3) Mitigating overfitting concerns and\nfacilitating model convergence to higher accuracy levels via the introduction\nof our two-stage fine-tuning strategy. Through extensive experiments, we\ndemonstrate the effectiveness of our approach by achieving the 1st prize in the\nsoft-body track of the ManiSkill2 Challenge at the CVPR 2023 4th Embodied AI\nworkshop. Our findings highlight the potential of our method to improve the\ngeneralization abilities of Embodied AI models and pave the way for their\npractical applications in real-world scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05756v1",
    "published_date": "2024-10-08 07:31:10 UTC",
    "updated_date": "2024-10-08 07:31:10 UTC"
  },
  {
    "arxiv_id": "2410.05750v1",
    "title": "Polynomial Time Cryptanalytic Extraction of Deep Neural Networks in the Hard-Label Setting",
    "authors": [
      "Nicholas Carlini",
      "Jorge Chávez-Saab",
      "Anna Hambitzer",
      "Francisco Rodríguez-Henríquez",
      "Adi Shamir"
    ],
    "abstract": "Deep neural networks (DNNs) are valuable assets, yet their public\naccessibility raises security concerns about parameter extraction by malicious\nactors. Recent work by Carlini et al. (crypto'20) and Canales-Mart\\'inez et al.\n(eurocrypt'24) has drawn parallels between this issue and block cipher key\nextraction via chosen plaintext attacks. Leveraging differential cryptanalysis,\nthey demonstrated that all the weights and biases of black-box ReLU-based DNNs\ncould be inferred using a polynomial number of queries and computational time.\nHowever, their attacks relied on the availability of the exact numeric value of\noutput logits, which allowed the calculation of their derivatives. To overcome\nthis limitation, Chen et al. (asiacrypt'24) tackled the more realistic\nhard-label scenario, where only the final classification label (e.g., \"dog\" or\n\"car\") is accessible to the attacker. They proposed an extraction method\nrequiring a polynomial number of queries but an exponential execution time. In\naddition, their approach was applicable only to a restricted set of\narchitectures, could deal only with binary classifiers, and was demonstrated\nonly on tiny neural networks with up to four neurons split among up to two\nhidden layers. This paper introduces new techniques that, for the first time,\nachieve cryptanalytic extraction of DNN parameters in the most challenging\nhard-label setting, using both a polynomial number of queries and polynomial\ntime. We validate our approach by extracting nearly one million parameters from\na DNN trained on the CIFAR-10 dataset, comprising 832 neurons in four hidden\nlayers. Our results reveal the surprising fact that all the weights of a\nReLU-based DNN can be efficiently determined by analyzing only the geometric\nshape of its decision boundaries.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05750v1",
    "published_date": "2024-10-08 07:27:55 UTC",
    "updated_date": "2024-10-08 07:27:55 UTC"
  },
  {
    "arxiv_id": "2410.05740v2",
    "title": "Learning to Drift in Extreme Turning with Active Exploration and Gaussian Process Based MPC",
    "authors": [
      "Guoqiang Wu",
      "Cheng Hu",
      "Wangjia Weng",
      "Zhouheng Li",
      "Yonghao Fu",
      "Lei Xie",
      "Hongye Su"
    ],
    "abstract": "Extreme cornering in racing often leads to large sideslip angles, presenting\na significant challenge for vehicle control. Conventional vehicle controllers\nstruggle to manage this scenario, necessitating the use of a drifting\ncontroller. However, the large sideslip angle in drift conditions introduces\nmodel mismatch, which in turn affects control precision. To address this issue,\nwe propose a model correction drift controller that integrates Model Predictive\nControl (MPC) with Gaussian Process Regression (GPR). GPR is employed to\ncorrect vehicle model mismatches during both drift equilibrium solving and the\nMPC optimization process. Additionally, the variance from GPR is utilized to\nactively explore different cornering drifting velocities, aiming to minimize\ntrajectory tracking errors. The proposed algorithm is validated through\nsimulations on the Simulink-Carsim platform and experiments with a 1:10 scale\nRC vehicle. In the simulation, the average lateral error with GPR is reduced by\n52.8% compared to the non-GPR case. Incorporating exploration further decreases\nthis error by 27.1%. The velocity tracking Root Mean Square Error (RMSE) also\ndecreases by 10.6% with exploration. In the RC car experiment, the average\nlateral error with GPR is 36.7% lower, and exploration further leads to a 29.0%\nreduction. Moreover, the velocity tracking RMSE decreases by 7.2% with the\ninclusion of exploration.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05740v2",
    "published_date": "2024-10-08 06:56:51 UTC",
    "updated_date": "2025-05-11 04:04:24 UTC"
  },
  {
    "arxiv_id": "2410.05739v1",
    "title": "Array2BR: An End-to-End Noise-immune Binaural Audio Synthesis from Microphone-array Signals",
    "authors": [
      "Cheng Chi",
      "Xiaoyu Li",
      "Andong Li",
      "Yuxuan Ke",
      "Xiaodong Li",
      "Chengshi Zheng"
    ],
    "abstract": "Telepresence technology aims to provide an immersive virtual presence for\nremote conference applications, and it is extremely important to synthesize\nhigh-quality binaural audio signals for this aim. Because the ambient noise is\noften inevitable in practical application scenarios, it is highly desired that\nbinaural audio signals without noise can be obtained from microphone-array\nsignals directly. For this purpose, this paper proposes a new end-to-end\nnoise-immune binaural audio synthesis framework from microphone-array signals,\nabbreviated as Array2BR, and experimental results show that binaural cues can\nbe correctly mapped and noise can be well suppressed simultaneously using the\nproposed framework. Compared with existing methods, the proposed method\nachieved better performance in terms of both objective and subjective metric\nscores.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05739v1",
    "published_date": "2024-10-08 06:55:35 UTC",
    "updated_date": "2024-10-08 06:55:35 UTC"
  },
  {
    "arxiv_id": "2410.05729v1",
    "title": "Equi-GSPR: Equivariant SE(3) Graph Network Model for Sparse Point Cloud Registration",
    "authors": [
      "Xueyang Kang",
      "Zhaoliang Luan",
      "Kourosh Khoshelham",
      "Bing Wang"
    ],
    "abstract": "Point cloud registration is a foundational task for 3D alignment and\nreconstruction applications. While both traditional and learning-based\nregistration approaches have succeeded, leveraging the intrinsic symmetry of\npoint cloud data, including rotation equivariance, has received insufficient\nattention. This prohibits the model from learning effectively, resulting in a\nrequirement for more training data and increased model complexity. To address\nthese challenges, we propose a graph neural network model embedded with a local\nSpherical Euclidean 3D equivariance property through SE(3) message passing\nbased propagation. Our model is composed mainly of a descriptor module,\nequivariant graph layers, match similarity, and the final regression layers.\nSuch modular design enables us to utilize sparsely sampled input points and\ninitialize the descriptor by self-trained or pre-trained geometric feature\ndescriptors easily. Experiments conducted on the 3DMatch and KITTI datasets\nexhibit the compelling and robust performance of our model compared to\nstate-of-the-art approaches, while the model complexity remains relatively low\nat the same time.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 main body pages, and 9 pages for supplementary part",
    "pdf_url": "http://arxiv.org/pdf/2410.05729v1",
    "published_date": "2024-10-08 06:48:01 UTC",
    "updated_date": "2024-10-08 06:48:01 UTC"
  },
  {
    "arxiv_id": "2410.05728v1",
    "title": "Reducing fuzzy relation equations via concept lattices",
    "authors": [
      "David Lobo",
      "Víctor López-Marchante",
      "Jesús Medina"
    ],
    "abstract": "This paper has taken into advantage the relationship between Fuzzy Relation\nEquations (FRE) and Concept Lattices in order to introduce a procedure to\nreduce a FRE, without losing information. Specifically, attribute reduction\ntheory in property-oriented and object-oriented concept lattices has been\nconsidered in order to present a mechanism for detecting redundant equations.\nAs a first consequence, the computation of the whole solution set of a solvable\nFRE is reduced. Moreover, we will also introduce a novel method for computing\napproximate solutions of unsolvable FRE related to a (real) dataset with\nuncertainty/imprecision data.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05728v1",
    "published_date": "2024-10-08 06:47:35 UTC",
    "updated_date": "2024-10-08 06:47:35 UTC"
  },
  {
    "arxiv_id": "2410.05726v1",
    "title": "Less is more: Embracing sparsity and interpolation with Esiformer for time series forecasting",
    "authors": [
      "Yangyang Guo",
      "Yanjun Zhao",
      "Sizhe Dang",
      "Tian Zhou",
      "Liang Sun",
      "Yi Qian"
    ],
    "abstract": "Time series forecasting has played a significant role in many practical\nfields. But time series data generated from real-world applications always\nexhibits high variance and lots of noise, which makes it difficult to capture\nthe inherent periodic patterns of the data, hurting the prediction accuracy\nsignificantly. To address this issue, we propose the Esiformer, which apply\ninterpolation on the original data, decreasing the overall variance of the data\nand alleviating the influence of noise. What's more, we enhanced the vanilla\ntransformer with a robust Sparse FFN. It can enhance the representation ability\nof the model effectively, and maintain the excellent robustness, avoiding the\nrisk of overfitting compared with the vanilla implementation. Through\nevaluations on challenging real-world datasets, our method outperforms leading\nmodel PatchTST, reducing MSE by 6.5% and MAE by 5.8% in multivariate time\nseries forecasting. Code is available at:\nhttps://github.com/yyg1282142265/Esiformer/tree/main.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05726v1",
    "published_date": "2024-10-08 06:45:47 UTC",
    "updated_date": "2024-10-08 06:45:47 UTC"
  },
  {
    "arxiv_id": "2410.05725v2",
    "title": "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server",
    "authors": [
      "Wenhao Wang",
      "Xiaoyu Liang",
      "Rui Ye",
      "Jingyi Chai",
      "Siheng Chen",
      "Yanfeng Wang"
    ],
    "abstract": "The success of large language models (LLMs) facilitate many parties to\nfine-tune LLMs on their own private data. However, this practice raises privacy\nconcerns due to the memorization of LLMs. Existing solutions, such as utilizing\nsynthetic data for substitution, struggle to simultaneously improve performance\nand preserve privacy. They either rely on a local model for generation,\nresulting in a performance decline, or take advantage of APIs, directly\nexposing the data to API servers. To address this issue, we propose\nKnowledgeSG, a novel client-server framework which enhances synthetic data\nquality and improves model performance while ensuring privacy. We achieve this\nby learning local knowledge from the private data with differential privacy\n(DP) and distilling professional knowledge from the server. Additionally,\ninspired by federated learning, we transmit models rather than data between the\nclient and server to prevent privacy leakage. Extensive experiments in medical\nand financial domains demonstrate the effectiveness of KnowledgeSG. Our code is\nnow publicly available at https://github.com/wwh0411/KnowledgeSG.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "EMNLP 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2410.05725v2",
    "published_date": "2024-10-08 06:42:28 UTC",
    "updated_date": "2024-10-10 03:58:35 UTC"
  },
  {
    "arxiv_id": "2410.05721v1",
    "title": "Mero Nagarikta: Advanced Nepali Citizenship Data Extractor with Deep Learning-Powered Text Detection and OCR",
    "authors": [
      "Sisir Dhakal",
      "Sujan Sigdel",
      "Sandesh Prasad Paudel",
      "Sharad Kumar Ranabhat",
      "Nabin Lamichhane"
    ],
    "abstract": "Transforming text-based identity documents, such as Nepali citizenship cards,\ninto a structured digital format poses several challenges due to the distinct\ncharacteristics of the Nepali script and minor variations in print alignment\nand contrast across different cards. This work proposes a robust system using\nYOLOv8 for accurate text object detection and an OCR algorithm based on\nOptimized PyTesseract. The system, implemented within the context of a mobile\napplication, allows for the automated extraction of important textual\ninformation from both the front and the back side of Nepali citizenship cards,\nincluding names, citizenship numbers, and dates of birth. The final YOLOv8\nmodel was accurate, with a mean average precision of 99.1% for text detection\non the front and 96.1% on the back. The tested PyTesseract optimized for Nepali\ncharacters outperformed the standard OCR regarding flexibility and accuracy,\nextracting text from images with clean and noisy backgrounds and various\ncontrasts. Using preprocessing steps such as converting the images into\ngrayscale, removing noise from the images, and detecting edges further improved\nthe system's OCR accuracy, even for low-quality photos. This work expands the\ncurrent body of research in multilingual OCR and document analysis, especially\nfor low-resource languages such as Nepali. It emphasizes the effectiveness of\ncombining the latest object detection framework with OCR models that have been\nfine-tuned for practical applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.05721v1",
    "published_date": "2024-10-08 06:29:08 UTC",
    "updated_date": "2024-10-08 06:29:08 UTC"
  },
  {
    "arxiv_id": "2410.05714v1",
    "title": "Enhancing Temporal Modeling of Video LLMs via Time Gating",
    "authors": [
      "Zi-Yuan Hu",
      "Yiwu Zhong",
      "Shijia Huang",
      "Michael R. Lyu",
      "Liwei Wang"
    ],
    "abstract": "Video Large Language Models (Video LLMs) have achieved impressive performance\non video-and-language tasks, such as video question answering. However, most\nexisting Video LLMs neglect temporal information in video data, leading to\nstruggles with temporal-aware video understanding. To address this gap, we\npropose a Time Gating Video LLM (TG-Vid) designed to enhance temporal modeling\nthrough a novel Time Gating module (TG). The TG module employs a time gating\nmechanism on its sub-modules, comprising gating spatial attention, gating\ntemporal attention, and gating MLP. This architecture enables our model to\nachieve a robust understanding of temporal information within videos. Extensive\nevaluation of temporal-sensitive video benchmarks (i.e., MVBench, TempCompass,\nand NExT-QA) demonstrates that our TG-Vid model significantly outperforms the\nexisting Video LLMs. Further, comprehensive ablation studies validate that the\nperformance gains are attributed to the designs of our TG module. Our code is\navailable at https://github.com/LaVi-Lab/TG-Vid.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "EMNLP 2024 Findings (Short)",
    "pdf_url": "http://arxiv.org/pdf/2410.05714v1",
    "published_date": "2024-10-08 06:21:29 UTC",
    "updated_date": "2024-10-08 06:21:29 UTC"
  },
  {
    "arxiv_id": "2410.05710v1",
    "title": "PixLens: A Novel Framework for Disentangled Evaluation in Diffusion-Based Image Editing with Object Detection + SAM",
    "authors": [
      "Stefan Stefanache",
      "Lluís Pastor Pérez",
      "Julen Costa Watanabe",
      "Ernesto Sanchez Tejedor",
      "Thomas Hofmann",
      "Enis Simsar"
    ],
    "abstract": "Evaluating diffusion-based image-editing models is a crucial task in the\nfield of Generative AI. Specifically, it is imperative to assess their capacity\nto execute diverse editing tasks while preserving the image content and\nrealism. While recent developments in generative models have opened up\npreviously unheard-of possibilities for image editing, conducting a thorough\nevaluation of these models remains a challenging and open task. The absence of\na standardized evaluation benchmark, primarily due to the inherent need for a\npost-edit reference image for evaluation, further complicates this issue.\nCurrently, evaluations often rely on established models such as CLIP or require\nhuman intervention for a comprehensive understanding of the performance of\nthese image editing models. Our benchmark, PixLens, provides a comprehensive\nevaluation of both edit quality and latent representation disentanglement,\ncontributing to the advancement and refinement of existing methodologies in the\nfield.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "35 pages (17 main paper, 18 appendix), 22 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.05710v1",
    "published_date": "2024-10-08 06:05:15 UTC",
    "updated_date": "2024-10-08 06:05:15 UTC"
  },
  {
    "arxiv_id": "2410.18089v1",
    "title": "Empowering Cognitive Digital Twins with Generative Foundation Models: Developing a Low-Carbon Integrated Freight Transportation System",
    "authors": [
      "Xueping Li",
      "Haowen Xu",
      "Jose Tupayachi",
      "Olufemi Omitaomu",
      "Xudong Wang"
    ],
    "abstract": "Effective monitoring of freight transportation is essential for advancing\nsustainable, low-carbon economies. Traditional methods relying on single-modal\ndata and discrete simulations fall short in optimizing intermodal systems\nholistically. These systems involve interconnected processes that affect\nshipping time, costs, emissions, and socio-economic factors. Developing digital\ntwins for real-time awareness, predictive analytics, and urban logistics\noptimization requires extensive efforts in knowledge discovery, data\nintegration, and multi-domain simulation. Recent advancements in generative AI\noffer new opportunities to streamline digital twin development by automating\nknowledge discovery and data integration, generating innovative simulation and\noptimization solutions. These models extend digital twins' capabilities by\npromoting autonomous workflows for data engineering, analytics, and software\ndevelopment. This paper proposes an innovative paradigm that leverages\ngenerative AI to enhance digital twins for urban research and operations. Using\nfreight decarbonization as a case study, we propose a conceptual framework\nemploying transformer-based language models to enhance an urban digital twin\nthrough foundation models. We share preliminary results and our vision for more\nintelligent, autonomous, and general-purpose digital twins for optimizing\nintegrated freight systems from multimodal to synchromodal paradigms.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18089v1",
    "published_date": "2024-10-08 05:53:20 UTC",
    "updated_date": "2024-10-08 05:53:20 UTC"
  },
  {
    "arxiv_id": "2410.07245v1",
    "title": "AAAI Workshop on AI Planning for Cyber-Physical Systems -- CAIPI24",
    "authors": [
      "Oliver Niggemann",
      "Gautam Biswas",
      "Alexander Diedrich",
      "Jonas Ehrhardt",
      "René Heesch",
      "Niklas Widulle"
    ],
    "abstract": "The workshop 'AI-based Planning for Cyber-Physical Systems', which took place\non February 26, 2024, as part of the 38th Annual AAAI Conference on Artificial\nIntelligence in Vancouver, Canada, brought together researchers to discuss\nrecent advances in AI planning methods for Cyber-Physical Systems (CPS). CPS\npose a major challenge due to their complexity and data-intensive nature, which\noften exceeds the capabilities of traditional planning algorithms. The workshop\nhighlighted new approaches such as neuro-symbolic architectures, large language\nmodels (LLMs), deep reinforcement learning and advances in symbolic planning.\nThese techniques are promising when it comes to managing the complexity of CPS\nand have potential for real-world applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This is the Proceedings of the AAAI Workshop on AI Planning for\n  Cyber-Physical Systems - CAIPI24, which was held in Vancouver, CA, February\n  26, 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.07245v1",
    "published_date": "2024-10-08 05:52:00 UTC",
    "updated_date": "2024-10-08 05:52:00 UTC"
  },
  {
    "arxiv_id": "2410.18087v1",
    "title": "CUPID: A Real-Time Session-Based Reciprocal Recommendation System for a One-on-One Social Discovery Platform",
    "authors": [
      "Beomsu Kim",
      "Sangbum Kim",
      "Minchan Kim",
      "Joonyoung Yi",
      "Sungjoo Ha",
      "Suhyun Lee",
      "Youngsoo Lee",
      "Gihun Yeom",
      "Buru Chang",
      "Gihun Lee"
    ],
    "abstract": "This study introduces CUPID, a novel approach to session-based reciprocal\nrecommendation systems designed for a real-time one-on-one social discovery\nplatform. In such platforms, low latency is critical to enhance user\nexperiences. However, conventional session-based approaches struggle with high\nlatency due to the demands of modeling sequential user behavior for each\nrecommendation process. Additionally, given the reciprocal nature of the\nplatform, where users act as items for each other, training recommendation\nmodels on large-scale datasets is computationally prohibitive using\nconventional methods. To address these challenges, CUPID decouples the\ntime-intensive user session modeling from the real-time user matching process\nto reduce inference time. Furthermore, CUPID employs a two-phase training\nstrategy that separates the training of embedding and prediction layers,\nsignificantly reducing the computational burden by decreasing the number of\nsequential model inferences by several hundredfold. Extensive experiments on\nlarge-scale Azar datasets demonstrate CUPID's effectiveness in a real-world\nproduction environment. Notably, CUPID reduces response latency by more than\n76% compared to non-asynchronous systems, while significantly improving user\nengagement.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "The 2nd International Workshop on User Understanding from Big Data\n  Workshop (DMU2 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.18087v1",
    "published_date": "2024-10-08 05:44:14 UTC",
    "updated_date": "2024-10-08 05:44:14 UTC"
  },
  {
    "arxiv_id": "2410.05698v1",
    "title": "A Two-Step Approach for Data-Efficient French Pronunciation Learning",
    "authors": [
      "Hoyeon Lee",
      "Hyeeun Jang",
      "Jong-Hwan Kim",
      "Jae-Min Kim"
    ],
    "abstract": "Recent studies have addressed intricate phonological phenomena in French,\nrelying on either extensive linguistic knowledge or a significant amount of\nsentence-level pronunciation data. However, creating such resources is\nexpensive and non-trivial. To this end, we propose a novel two-step approach\nthat encompasses two pronunciation tasks: grapheme-to-phoneme and post-lexical\nprocessing. We then investigate the efficacy of the proposed approach with a\nnotably limited amount of sentence-level pronunciation data. Our findings\ndemonstrate that the proposed two-step approach effectively mitigates the lack\nof extensive labeled data, and serves as a feasible solution for addressing\nFrench phonological phenomena even under resource-constrained environments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2410.05698v1",
    "published_date": "2024-10-08 05:30:23 UTC",
    "updated_date": "2024-10-08 05:30:23 UTC"
  },
  {
    "arxiv_id": "2410.09090v1",
    "title": "Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and Contextual Search for Customized Literature Characterization for High-Impact Urban Research",
    "authors": [
      "Haowen Xu",
      "Xueping Li",
      "Jose Tupayachi",
      "Jianming",
      "Lian",
      "Femi Omitaomu"
    ],
    "abstract": "Bibliometric analysis is essential for understanding research trends, scope,\nand impact in urban science, especially in high-impact journals, such Nature\nPortfolios. However, traditional methods, relying on keyword searches and basic\nNLP techniques, often fail to uncover valuable insights not explicitly stated\nin article titles or keywords. These approaches are unable to perform semantic\nsearches and contextual understanding, limiting their effectiveness in\nclassifying topics and characterizing studies. In this paper, we address these\nlimitations by leveraging Generative AI models, specifically transformers and\nRetrieval-Augmented Generation (RAG), to automate and enhance bibliometric\nanalysis. We developed a technical workflow that integrates a vector database,\nSentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and\nLarge Language Models (LLMs) to enable contextual search, topic ranking, and\ncharacterization of research using customized prompt templates. A pilot study\nanalyzing 223 urban science-related articles published in Nature Communications\nover the past decade highlights the effectiveness of our approach in generating\ninsightful summary statistics on the quality, scope, and characteristics of\npapers in high-impact journals. This study introduces a new paradigm for\nenhancing bibliometric analysis and knowledge retrieval in urban research,\npositioning an AI agent as a powerful tool for advancing research evaluation\nand understanding.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09090v1",
    "published_date": "2024-10-08 05:13:27 UTC",
    "updated_date": "2024-10-08 05:13:27 UTC"
  },
  {
    "arxiv_id": "2410.05684v2",
    "title": "Copiloting Diagnosis of Autism in Real Clinical Scenarios via LLMs",
    "authors": [
      "Yi Jiang",
      "Qingyang Shen",
      "Shuzhong Lai",
      "Shunyu Qi",
      "Qian Zheng",
      "Lin Yao",
      "Yueming Wang",
      "Gang Pan"
    ],
    "abstract": "Autism spectrum disorder(ASD) is a pervasive developmental disorder that\nsignificantly impacts the daily functioning and social participation of\nindividuals. Despite the abundance of research focused on supporting the\nclinical diagnosis of ASD, there is still a lack of systematic and\ncomprehensive exploration in the field of methods based on Large Language\nModels (LLMs), particularly regarding the real-world clinical diagnostic\nscenarios based on Autism Diagnostic Observation Schedule, Second Edition\n(ADOS-2). Therefore, we have proposed a framework called ADOS-Copilot, which\nstrikes a balance between scoring and explanation and explored the factors that\ninfluence the performance of LLMs in this task. The experimental results\nindicate that our proposed framework is competitive with the diagnostic results\nof clinicians, with a minimum MAE of 0.4643, binary classification F1-score of\n81.79\\%, and ternary classification F1-score of 78.37\\%. Furthermore, we have\nsystematically elucidated the strengths and limitations of current LLMs in this\ntask from the perspectives of ADOS-2, LLMs' capabilities, language, and model\nscale aiming to inspire and guide the future application of LLMs in a broader\nfields of mental health disorders. We hope for more research to be transferred\ninto real clinical practice, opening a window of kindness to the world for\neccentric children.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05684v2",
    "published_date": "2024-10-08 04:48:42 UTC",
    "updated_date": "2024-10-10 03:24:14 UTC"
  },
  {
    "arxiv_id": "2410.05677v2",
    "title": "T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design",
    "authors": [
      "Jiachen Li",
      "Qian Long",
      "Jian Zheng",
      "Xiaofeng Gao",
      "Robinson Piramuthu",
      "Wenhu Chen",
      "William Yang Wang"
    ],
    "abstract": "In this paper, we focus on enhancing a diffusion-based text-to-video (T2V)\nmodel during the post-training phase by distilling a highly capable consistency\nmodel from a pretrained T2V model. Our proposed method, T2V-Turbo-v2,\nintroduces a significant advancement by integrating various supervision\nsignals, including high-quality training data, reward model feedback, and\nconditional guidance, into the consistency distillation process. Through\ncomprehensive ablation studies, we highlight the crucial importance of\ntailoring datasets to specific learning objectives and the effectiveness of\nlearning from diverse reward models for enhancing both the visual quality and\ntext-video alignment. Additionally, we highlight the vast design space of\nconditional guidance strategies, which centers on designing an effective energy\nfunction to augment the teacher ODE solver. We demonstrate the potential of\nthis approach by extracting motion guidance from the training datasets and\nincorporating it into the ODE solver, showcasing its effectiveness in improving\nthe motion quality of the generated videos with the improved motion-related\nmetrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2\nestablishes a new state-of-the-art result on VBench, with a Total score of\n85.13, surpassing proprietary systems such as Gen-3 and Kling.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://t2v-turbo-v2.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.05677v2",
    "published_date": "2024-10-08 04:30:06 UTC",
    "updated_date": "2024-10-11 07:47:36 UTC"
  },
  {
    "arxiv_id": "2410.09089v1",
    "title": "Different Cybercrimes and their Solution for Common People",
    "authors": [
      "S. Tamang",
      "G. S. Chandana",
      "B. K. Roy"
    ],
    "abstract": "In today's digital age, cyberspace has become integral to daily life, however\nit has also led to an increase in cybercriminal activities. This paper explores\ncybercrime trends and highlights the need for cybercrime awareness\n(cyberawareness) to mitigate vulnerabilities. The study also examines Indian\nstatistics on cybercrime. We review the existing literature on cybercrime and\ncybersecurity, focusing on various types of cybercrimes and their impacts. We\npresent a list of 31 technical as well as non-technical solutions considering\nthat a \"common man\" may not be technologically aware. Common man solutions,\nconsidering that they are not technologically updated. Expanding the list of\nsolutions and validating their effectiveness in cyber threats can be the future\nscope of the research.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09089v1",
    "published_date": "2024-10-08 04:23:11 UTC",
    "updated_date": "2024-10-08 04:23:11 UTC"
  },
  {
    "arxiv_id": "2410.05669v2",
    "title": "ACPBench: Reasoning about Action, Change, and Planning",
    "authors": [
      "Harsha Kokel",
      "Michael Katz",
      "Kavitha Srinivas",
      "Shirin Sohrabi"
    ],
    "abstract": "There is an increasing body of work using Large Language Models (LLMs) as\nagents for orchestrating workflows and making decisions in domains that require\nplanning and multi-step reasoning. As a result, it is imperative to evaluate\nLLMs on core skills required for planning. In this work, we present ACPBench, a\nbenchmark for evaluating the reasoning tasks in the field of planning. The\nbenchmark consists of 7 reasoning tasks over 13 planning domains. The\ncollection is constructed from planning domains described in a formal language.\nThis allows us to synthesize problems with provably correct solutions across\nmany tasks and domains. Further, it allows us the luxury of scale without\nadditional human effort, i.e., many additional problems can be created\nautomatically. Our extensive evaluation of 22 LLMs and OpenAI o1 reasoning\nmodels highlights the significant gap in the reasoning capability of the LLMs.\nOur findings with OpenAI o1, a multi-turn reasoning model, reveal significant\ngains in performance on multiple-choice questions, yet surprisingly, no notable\nprogress is made on boolean questions.\n  The ACPBench collection is available at https://ibm.github.io/ACPBench.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Added OpenAI o1 results",
    "pdf_url": "http://arxiv.org/pdf/2410.05669v2",
    "published_date": "2024-10-08 03:48:57 UTC",
    "updated_date": "2024-10-22 17:16:17 UTC"
  },
  {
    "arxiv_id": "2410.05668v1",
    "title": "Diversity and Inclusion Index with Networks and Similarity: Analysis and its Application",
    "authors": [
      "Keita Kinjo"
    ],
    "abstract": "In recent years, the concepts of ``diversity'' and ``inclusion'' have\nattracted considerable attention across a range of fields, encompassing both\nsocial and biological disciplines. To fully understand these concepts, it is\ncritical to not only examine the number of categories but also the similarities\nand relationships among them. In this study, I introduce a novel index for\ndiversity and inclusion that considers similarities and network connections. I\nanalyzed the properties of these indices and investigated their mathematical\nrelationships using established measures of diversity and networks. Moreover, I\ndeveloped a methodology for estimating similarities based on the utility of\ndiversity. I also created a method for visualizing proportions, similarities,\nand network connections. Finally, I evaluated the correlation with external\nmetrics using real-world data, confirming that both the proposed indices and\nour index can be effectively utilized. This study contributes to a more nuanced\nunderstanding of diversity and inclusion analysis.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "stat.ME",
      "68T01, 62P25"
    ],
    "primary_category": "cs.SI",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.05668v1",
    "published_date": "2024-10-08 03:41:39 UTC",
    "updated_date": "2024-10-08 03:41:39 UTC"
  },
  {
    "arxiv_id": "2410.05661v1",
    "title": "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models",
    "authors": [
      "Siqi Wang",
      "Zhengyu Chen",
      "Bei Li",
      "Keqing He",
      "Min Zhang",
      "Jingang Wang"
    ],
    "abstract": "The scaling of large language models (LLMs) is a critical research area for\nthe efficiency and effectiveness of model training and deployment. Our work\ninvestigates the transferability and discrepancies of scaling laws between\nDense Models and Mixture of Experts (MoE) models. Through a combination of\ntheoretical analysis and extensive experiments, including consistent loss\nscaling, optimal batch size and learning rate scaling, and resource allocation\nstrategies scaling, our findings reveal that the power-law scaling framework\nalso applies to MoE Models, indicating that the fundamental principles\ngoverning the scaling behavior of these models are preserved, even though the\narchitecture differs. Additionally, MoE Models demonstrate superior\ngeneralization, resulting in lower testing losses with the same training\ncompute budget compared to Dense Models. These findings indicate the scaling\nconsistency and transfer generalization capabilities of MoE Models, providing\nnew insights for optimizing MoE Model training and deployment strategies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05661v1",
    "published_date": "2024-10-08 03:21:56 UTC",
    "updated_date": "2024-10-08 03:21:56 UTC"
  },
  {
    "arxiv_id": "2410.05656v1",
    "title": "On the Modeling Capabilities of Large Language Models for Sequential Decision Making",
    "authors": [
      "Martin Klissarov",
      "Devon Hjelm",
      "Alexander Toshev",
      "Bogdan Mazoure"
    ],
    "abstract": "Large pretrained models are showing increasingly better performance in\nreasoning and planning tasks across different modalities, opening the\npossibility to leverage them for complex sequential decision making problems.\nIn this paper, we investigate the capabilities of Large Language Models (LLMs)\nfor reinforcement learning (RL) across a diversity of interactive domains. We\nevaluate their ability to produce decision-making policies, either directly, by\ngenerating actions, or indirectly, by first generating reward models to train\nan agent with RL. Our results show that, even without task-specific\nfine-tuning, LLMs excel at reward modeling. In particular, crafting rewards\nthrough artificial intelligence (AI) feedback yields the most generally\napplicable approach and can enhance performance by improving credit assignment\nand exploration. Finally, in environments with unfamiliar dynamics, we explore\nhow fine-tuning LLMs with synthetic data can significantly improve their reward\nmodeling capabilities while mitigating catastrophic forgetting, further\nbroadening their utility in sequential decision-making tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05656v1",
    "published_date": "2024-10-08 03:12:57 UTC",
    "updated_date": "2024-10-08 03:12:57 UTC"
  },
  {
    "arxiv_id": "2410.05651v3",
    "title": "ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler",
    "authors": [
      "Serin Yang",
      "Taesung Kwon",
      "Jong Chul Ye"
    ],
    "abstract": "Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V)\ndiffusion models has greatly enhanced video generation, especially in terms of\nkeyframe interpolation. However, current image-to-video diffusion models, while\npowerful in generating videos from a single conditioning frame, need adaptation\nfor two-frame (start & end) conditioned generation, which is essential for\neffective bounded interpolation. Unfortunately, existing approaches that fuse\ntemporally forward and backward paths in parallel often suffer from\noff-manifold issues, leading to artifacts or requiring multiple iterative\nre-noising steps. In this work, we introduce a novel, bidirectional sampling\nstrategy to address these off-manifold issues without requiring extensive\nre-noising or fine-tuning. Our method employs sequential sampling along both\nforward and backward paths, conditioned on the start and end frames,\nrespectively, ensuring more coherent and on-manifold generation of intermediate\nframes. Additionally, we incorporate advanced guidance techniques, CFG++ and\nDDS, to further enhance the interpolation process. By integrating these, our\nmethod achieves state-of-the-art performance, efficiently generating\nhigh-quality, smooth videos between keyframes. On a single 3090 GPU, our method\ncan interpolate 25 frames at 1024 x 576 resolution in just 195 seconds,\nestablishing it as a leading solution for keyframe interpolation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025; Project page: https://vibidsampler.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.05651v3",
    "published_date": "2024-10-08 03:01:54 UTC",
    "updated_date": "2025-03-01 07:27:28 UTC"
  },
  {
    "arxiv_id": "2410.05646v1",
    "title": "Score-Based Variational Inference for Inverse Problems",
    "authors": [
      "Zhipeng Xue",
      "Penghao Cai",
      "Xiaojun Yuan",
      "Xiqi Gao"
    ],
    "abstract": "Existing diffusion-based methods for inverse problems sample from the\nposterior using score functions and accept the generated random samples as\nsolutions. In applications that posterior mean is preferred, we have to\ngenerate multiple samples from the posterior which is time-consuming. In this\nwork, by analyzing the probability density evolution of the conditional reverse\ndiffusion process, we prove that the posterior mean can be achieved by tracking\nthe mean of each reverse diffusion step. Based on that, we establish a\nframework termed reverse mean propagation (RMP) that targets the posterior mean\ndirectly. We show that RMP can be implemented by solving a variational\ninference problem, which can be further decomposed as minimizing a reverse KL\ndivergence at each reverse step. We further develop an algorithm that optimizes\nthe reverse KL divergence with natural gradient descent using score functions\nand propagates the mean at each reverse step. Experiments demonstrate the\nvalidity of the theory of our framework and show that our algorithm outperforms\nstate-of-the-art algorithms on reconstruction performance with lower\ncomputational complexity in various inverse problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 7 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2410.05646v1",
    "published_date": "2024-10-08 02:55:16 UTC",
    "updated_date": "2024-10-08 02:55:16 UTC"
  },
  {
    "arxiv_id": "2410.05637v2",
    "title": "Federated Neural Nonparametric Point Processes",
    "authors": [
      "Hui Chen",
      "Xuhui Fan",
      "Hengyu Liu",
      "Yaqiong Li",
      "Zhilin Zhao",
      "Feng Zhou",
      "Christopher John Quinn",
      "Longbing Cao"
    ],
    "abstract": "Temporal point processes (TPPs) are effective for modeling event occurrences\nover time, but they struggle with sparse and uncertain events in federated\nsystems, where privacy is a major concern. To address this, we propose\n\\textit{FedPP}, a Federated neural nonparametric Point Process model. FedPP\nintegrates neural embeddings into Sigmoidal Gaussian Cox Processes (SGCPs) on\nthe client side, which is a flexible and expressive class of TPPs, allowing it\nto generate highly flexible intensity functions that capture client-specific\nevent dynamics and uncertainties while efficiently summarizing historical\nrecords. For global aggregation, FedPP introduces a divergence-based mechanism\nthat communicates the distributions of SGCPs' kernel hyperparameters between\nthe server and clients, while keeping client-specific parameters local to\nensure privacy and personalization. FedPP effectively captures event\nuncertainty and sparsity, and extensive experiments demonstrate its superior\nperformance in federated settings, particularly with KL divergence and\nWasserstein distance-based global aggregation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05637v2",
    "published_date": "2024-10-08 02:40:04 UTC",
    "updated_date": "2025-01-20 23:38:41 UTC"
  },
  {
    "arxiv_id": "2410.05629v2",
    "title": "Vector-ICL: In-context Learning with Continuous Vector Representations",
    "authors": [
      "Yufan Zhuang",
      "Chandan Singh",
      "Liyuan Liu",
      "Jingbo Shang",
      "Jianfeng Gao"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable in-context learning (ICL)\ncapabilities on textual data. We explore whether these capabilities can be\nextended to continuous vectors from diverse domains, obtained from black-box\npretrained encoders. By aligning input data with an LLM's embedding space\nthrough lightweight projectors, we observe that LLMs can effectively process\nand learn from these projected vectors, which we term Vector-ICL. In\nparticular, we find that pretraining projectors with general language modeling\nobjectives enables Vector-ICL, while task-specific finetuning further enhances\nperformance. In our experiments across various tasks and modalities, including\ntext reconstruction, numerical function regression, text classification,\nsummarization, molecule captioning, time-series classification, graph\nclassification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL\nand domain-specific model or tuning. We further conduct analyses and case\nstudies, indicating the potential of LLMs to process vector representations\nbeyond traditional token-based paradigms.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05629v2",
    "published_date": "2024-10-08 02:25:38 UTC",
    "updated_date": "2025-02-19 22:48:13 UTC"
  },
  {
    "arxiv_id": "2410.05628v5",
    "title": "A Unified Framework for Motion Reasoning and Generation in Human Interaction",
    "authors": [
      "Jeongeun Park",
      "Sungjoon Choi",
      "Sangdoo Yun"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural and contextually relevant text,\nenabling more human-like AI interactions. However, generating and understanding\ninteractive human-like motion, where multiple individuals engage in coordinated\nmovements, remains challenging due to the complexity of modeling these\ninteractions. Additionally, a unified and versatile model is needed to handle\ndiverse interactive scenarios, such as chat systems that dynamically adapt to\nuser instructions and assigned roles. To address these challenges, we introduce\nVIM, the Versatile Interactive Motion-language model, which integrates both\nlanguage and motion modalities to effectively understand, generate, and control\ninteractive motions in multi-turn conversational contexts. Unlike previous\nstudies that primarily focus on uni-directional tasks such as text-to-motion or\nmotion-to-text, VIM employs a unified architecture capable of simultaneously\nunderstanding and generating both motion and text modalities. Given the absence\nof an appropriate dataset to support this task, we introduce Inter-MT2, a\nlarge-scale instruction-tuning dataset containing 82.7K multi-turn interactive\nmotion instructions, covering 153K interactive motion samples. Inter-MT2 spans\ndiverse instructional scenarios, including motion editing, question answering,\nand story generation, leveraging off-the-shelf large language models and motion\ndiffusion models to construct a broad set of interactive motion instructions.\nWe extensively evaluate the versatility of VIM across multiple interactive\nmotion-related tasks, including motion-to-text, text-to-motion, reaction\ngeneration, motion editing, and reasoning about motion sequences.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "https://vim-motion-language.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.05628v5",
    "published_date": "2024-10-08 02:23:53 UTC",
    "updated_date": "2025-03-12 05:54:44 UTC"
  },
  {
    "arxiv_id": "2410.05627v1",
    "title": "CLOSER: Towards Better Representation Learning for Few-Shot Class-Incremental Learning",
    "authors": [
      "Junghun Oh",
      "Sungyong Baik",
      "Kyoung Mu Lee"
    ],
    "abstract": "Aiming to incrementally learn new classes with only few samples while\npreserving the knowledge of base (old) classes, few-shot class-incremental\nlearning (FSCIL) faces several challenges, such as overfitting and catastrophic\nforgetting. Such a challenging problem is often tackled by fixing a feature\nextractor trained on base classes to reduce the adverse effects of overfitting\nand forgetting. Under such formulation, our primary focus is representation\nlearning on base classes to tackle the unique challenge of FSCIL:\nsimultaneously achieving the transferability and the discriminability of the\nlearned representation. Building upon the recent efforts for enhancing\ntransferability, such as promoting the spread of features, we find that trying\nto secure the spread of features within a more confined feature space enables\nthe learned representation to strike a better balance between transferability\nand discriminability. Thus, in stark contrast to prior beliefs that the\ninter-class distance should be maximized, we claim that the closer different\nclasses are, the better for FSCIL. The empirical results and analysis from the\nperspective of information bottleneck theory justify our simple yet seemingly\ncounter-intuitive representation learning method, raising research questions\nand suggesting alternative research directions. The code is available at\nhttps://github.com/JungHunOh/CLOSER_ECCV2024.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2410.05627v1",
    "published_date": "2024-10-08 02:23:16 UTC",
    "updated_date": "2024-10-08 02:23:16 UTC"
  },
  {
    "arxiv_id": "2410.05623v2",
    "title": "Understanding Gradient Boosting Classifier: Training, Prediction, and the Role of $γ_j$",
    "authors": [
      "Hung-Hsuan Chen"
    ],
    "abstract": "The Gradient Boosting Classifier (GBC) is a widely used machine learning\nalgorithm for binary classification, which builds decision trees iteratively to\nminimize prediction errors. This document explains the GBC's training and\nprediction processes, focusing on the computation of terminal node values\n$\\gamma_j$, which are crucial to optimizing the logistic loss function. We\nderive $\\gamma_j$ through a Taylor series approximation and provide a\nstep-by-step pseudocode for the algorithm's implementation. The guide explains\nthe theory of GBC and its practical application, demonstrating its\neffectiveness in binary classification tasks. We provide a step-by-step example\nin the appendix to help readers understand.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05623v2",
    "published_date": "2024-10-08 02:11:35 UTC",
    "updated_date": "2024-10-23 07:28:19 UTC"
  },
  {
    "arxiv_id": "2410.05610v1",
    "title": "Chain-of-Thoughts for Molecular Understanding",
    "authors": [
      "Yunhui Jang",
      "Jaehyung Kim",
      "Sungsoo Ahn"
    ],
    "abstract": "The adaptation of large language models (LLMs) to chemistry has shown\npromising performance in molecular understanding tasks, such as generating a\ntext description from a molecule. However, proper reasoning based on molecular\nstructural information remains a significant challenge, e.g., even advanced\nLLMs such as GPT-4o struggle to identify functional groups which are crucial\nfor inferring the molecular property of interest. To address this limitation,\nwe propose StructCoT, a structure-aware chain-of-thought (CoT) that enhances\nLLMs' understanding of molecular structures by explicitly injecting the key\nstructural features of molecules. Moreover, we introduce two fine-tuning\nframeworks for adapting the existing LLMs to use our StructCoT. Our experiments\ndemonstrate that incorporating StructCoT with our fine-tuning frameworks leads\nto consistent improvements in both molecular understanding tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05610v1",
    "published_date": "2024-10-08 01:49:48 UTC",
    "updated_date": "2024-10-08 01:49:48 UTC"
  },
  {
    "arxiv_id": "2410.05603v1",
    "title": "Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition",
    "authors": [
      "Zheyang Xiong",
      "Ziyang Cai",
      "John Cooper",
      "Albert Ge",
      "Vasilis Papageorgiou",
      "Zack Sifakis",
      "Angeliki Giannou",
      "Ziqian Lin",
      "Liu Yang",
      "Saurabh Agarwal",
      "Grigorios G Chrysos",
      "Samet Oymak",
      "Kangwook Lee",
      "Dimitris Papailiopoulos"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable in-context learning\n(ICL) capabilities. In this study, we explore a surprising phenomenon related\nto ICL: LLMs can perform multiple, computationally distinct ICL tasks\nsimultaneously, during a single inference call, a capability we term \"task\nsuperposition\". We provide empirical evidence of this phenomenon across various\nLLM families and scales and show that this phenomenon emerges even if we train\nthe model to in-context learn one task at a time. We offer theoretical\nexplanations that this capability is well within the expressive power of\ntransformers. We also explore how LLMs internally compose task vectors during\nsuperposition. Furthermore, we show that larger models can solve more ICL tasks\nin parallel, and better calibrate their output distribution. Our findings offer\ninsights into the latent capabilities of LLMs, further substantiate the\nperspective of \"LLMs as superposition of simulators\", and raise questions about\nthe mechanisms enabling simultaneous task execution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05603v1",
    "published_date": "2024-10-08 01:28:57 UTC",
    "updated_date": "2024-10-08 01:28:57 UTC"
  },
  {
    "arxiv_id": "2410.05592v1",
    "title": "Training Stiff Neural Ordinary Differential Equations with Implicit Single-Step Methods",
    "authors": [
      "Colby Fronk",
      "Linda Petzold"
    ],
    "abstract": "Stiff systems of ordinary differential equations (ODEs) are pervasive in many\nscience and engineering fields, yet standard neural ODE approaches struggle to\nlearn them. This limitation is the main barrier to the widespread adoption of\nneural ODEs. In this paper, we propose an approach based on single-step\nimplicit schemes to enable neural ODEs to handle stiffness and demonstrate that\nour implicit neural ODE method can learn stiff dynamics. This work addresses a\nkey limitation in current neural ODE methods, paving the way for their use in a\nwider range of scientific problems.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.CE",
      "cs.NA"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05592v1",
    "published_date": "2024-10-08 01:08:17 UTC",
    "updated_date": "2024-10-08 01:08:17 UTC"
  },
  {
    "arxiv_id": "2410.09088v1",
    "title": "The Solution for Temporal Action Localisation Task of Perception Test Challenge 2024",
    "authors": [
      "Yinan Han",
      "Qingyuan Jiang",
      "Hongming Mei",
      "Yang Yang",
      "Jinhui Tang"
    ],
    "abstract": "This report presents our method for Temporal Action Localisation (TAL), which\nfocuses on identifying and classifying actions within specific time intervals\nthroughout a video sequence. We employ a data augmentation technique by\nexpanding the training dataset using overlapping labels from the\nSomething-SomethingV2 dataset, enhancing the model's ability to generalize\nacross various action classes. For feature extraction, we utilize\nstate-of-the-art models, including UMT, VideoMAEv2 for video features, and\nBEATs and CAV-MAE for audio features. Our approach involves training both\nmultimodal (video and audio) and unimodal (video only) models, followed by\ncombining their predictions using the Weighted Box Fusion (WBF) method. This\nfusion strategy ensures robust action localisation. our overall approach\nachieves a score of 0.5498, securing first place in the competition.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09088v1",
    "published_date": "2024-10-08 01:07:21 UTC",
    "updated_date": "2024-10-08 01:07:21 UTC"
  },
  {
    "arxiv_id": "2410.05586v2",
    "title": "TeaserGen: Generating Teasers for Long Documentaries",
    "authors": [
      "Weihan Xu",
      "Paul Pu Liang",
      "Haven Kim",
      "Julian McAuley",
      "Taylor Berg-Kirkpatrick",
      "Hao-Wen Dong"
    ],
    "abstract": "Teasers are an effective tool for promoting content in entertainment,\ncommercial and educational fields. However, creating an effective teaser for\nlong videos is challenging for it requires long-range multimodal modeling on\nthe input videos, while necessitating maintaining audiovisual alignments,\nmanaging scene changes and preserving factual accuracy for the output teasers.\nDue to the lack of a publicly-available dataset, progress along this research\ndirection has been hindered. In this work, we present DocumentaryNet, a\ncollection of 1,269 documentaries paired with their teasers, featuring\nmultimodal data streams of video, speech, music, sound effects and narrations.\nWith DocumentaryNet, we propose a new two-stage system for generating teasers\nfrom long documentaries. The proposed TeaserGen system first generates the\nteaser narration from the transcribed narration of the documentary using a\npretrained large language model, and then selects the most relevant visual\ncontent to accompany the generated narration through language-vision models.\nFor narration-video matching, we explore two approaches: a pretraining-based\nmodel using pretrained contrastive language-vision models and a deep sequential\nmodel that learns the mapping between the narrations and visuals. Our\nexperimental results show that the pretraining-based approach is more effective\nat identifying relevant visual content than directly trained deep\nautoregressive models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05586v2",
    "published_date": "2024-10-08 01:00:09 UTC",
    "updated_date": "2024-11-10 02:20:47 UTC"
  },
  {
    "arxiv_id": "2410.10865v1",
    "title": "Generating Synthetic Datasets for Few-shot Prompt Tuning",
    "authors": [
      "Xu Guo",
      "Zilin Du",
      "Boyang Li",
      "Chunyan Miao"
    ],
    "abstract": "A major limitation of prompt tuning is its dependence on large labeled\ntraining datasets. Under few-shot learning settings, prompt tuning lags far\nbehind full-model fine-tuning, limiting its scope of application. In this\npaper, we leverage the powerful LLMs to synthesize task-specific labeled data\nfor training the soft prompts. We first introduce a distribution-aligned\nweighted generator tuning (DawGen) method to encourage generating\nin-distribution data that aligns with the few-shot real data. Then, we train\nsoft prompts on both synthetic and real datasets using a gradient surgery\napproach, which eliminates the conflicting gradients from different data\nsources. Experiments on seven sentence-pair classification datasets demonstrate\nthe effectiveness of our proposed method for boosting prompt tuning in few-shot\nlearning settings. Results on QQP, MRPC, and SICK datasets are even comparable\nto the performance of transfer learning from large real-world datasets, showing\nthe promise of synthetic data as an alternative for enhancing soft prompt\ntuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10865v1",
    "published_date": "2024-10-08 01:00:02 UTC",
    "updated_date": "2024-10-08 01:00:02 UTC"
  },
  {
    "arxiv_id": "2410.05585v2",
    "title": "Towards Robust Spacecraft Trajectory Optimization via Transformers",
    "authors": [
      "Yuji Takubo",
      "Tommaso Guffanti",
      "Daniele Gammelli",
      "Marco Pavone",
      "Simone D'Amico"
    ],
    "abstract": "Future multi-spacecraft missions require robust autonomous trajectory\noptimization capabilities to ensure safe and efficient rendezvous operations.\nThis capability hinges on solving non-convex optimal control problems in\nreal-time, although traditional iterative methods such as sequential convex\nprogramming impose significant computational challenges. To mitigate this\nburden, the Autonomous Rendezvous Transformer (ART) introduced a generative\nmodel trained to provide near-optimal initial guesses. This approach provides\nconvergence to better local optima (e.g., fuel optimality), improves\nfeasibility rates, and results in faster convergence speed of optimization\nalgorithms through warm-starting. This work extends the capabilities of ART to\naddress robust chance-constrained optimal control problems. Specifically, ART\nis applied to challenging rendezvous scenarios in Low Earth Orbit (LEO),\nensuring fault-tolerant behavior under uncertainty. Through extensive\nexperimentation, the proposed warm-starting strategy is shown to consistently\nproduce high-quality reference trajectories, achieving up to 30\\% cost\nimprovement and 50\\% reduction in infeasible cases compared to conventional\nmethods, demonstrating robust performance across multiple state\nrepresentations. Additionally, a post hoc evaluation framework is proposed to\nassess the quality of generated trajectories and mitigate runtime failures,\nmarking an initial step toward the reliable deployment of AI-driven solutions\nin safety-critical autonomous systems such as spacecraft.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "math.OC",
    "comment": "Submitted to the IEEE Aerospace Conference 2025. 13 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.05585v2",
    "published_date": "2024-10-08 00:58:42 UTC",
    "updated_date": "2025-01-25 03:16:03 UTC"
  },
  {
    "arxiv_id": "2410.05584v5",
    "title": "Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?",
    "authors": [
      "Xueru Wen",
      "Jie Lou",
      "Yaojie Lu",
      "Hongyu Lin",
      "Xing Yu",
      "Xinyu Lu",
      "Ben He",
      "Xianpei Han",
      "Debing Zhang",
      "Le Sun"
    ],
    "abstract": "Reward Models (RMs) are crucial for aligning language models with human\npreferences. Currently, the evaluation of RMs depends on measuring accuracy\nagainst a validation set of manually annotated preference data. Although this\nmethod is straightforward and widely adopted, the relationship between RM\naccuracy and downstream policy performance remains under-explored. In this\nwork, we conduct experiments in a synthetic setting to investigate how\ndifferences in RM measured by accuracy translate into gaps in optimized policy\nperformance. Our findings reveal that while there is a weak positive\ncorrelation between accuracy and downstream performance, policies optimized\ntowards RMs with similar accuracy can exhibit quite different performance.\nMoreover, we discover that the way of measuring accuracy significantly impacts\nits ability to predict the final policy performance. Through the lens of the\nRegressional Goodhart effect, we recognize that accuracy, when used for\nmeasuring RM quality, can fail to fully capture the potential RM\noveroptimization. This underscores the inadequacy of relying solely on accuracy\nto reflect their impact on policy optimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR2025 Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2410.05584v5",
    "published_date": "2024-10-08 00:52:03 UTC",
    "updated_date": "2025-02-14 01:21:57 UTC"
  },
  {
    "arxiv_id": "2410.05583v1",
    "title": "NegMerge: Consensual Weight Negation for Strong Machine Unlearning",
    "authors": [
      "Hyoseo Kim",
      "Dongyoon Han",
      "Junsuk Choe"
    ],
    "abstract": "Machine unlearning aims to selectively remove specific knowledge from a\nmodel. Current methods, such as task arithmetic, rely on fine-tuning models on\nthe forget set, generating a task vector, and subtracting it from the original\nmodel. However, we argue the effectiveness of this approach is highly sensitive\nto hyperparameter selection, necessitating careful validation to identify the\nbest model among many fine-tuned candidates. In this paper, we propose a novel\nmethod that leverages all given fine-tuned models rather than selecting a\nsingle one. By constructing task vectors from models trained with varied\nhyperparameters and merging only the components of the task vectors with\nconsistent signs, we perform unlearning by negating the merged task vector from\nthe original model. Given that existing methods also utilize multiple\nfine-tuned models, our approach delivers more effective unlearning without\nincurring additional computational costs. We demonstrate the effectiveness of\nour method on both vision-language models and standard image classification\nmodels, showing improved unlearning performance with minimal degradation on the\nretain set, outperforming state-of-the-art techniques.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.05583v1",
    "published_date": "2024-10-08 00:50:54 UTC",
    "updated_date": "2024-10-08 00:50:54 UTC"
  },
  {
    "arxiv_id": "2410.05581v2",
    "title": "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?",
    "authors": [
      "Fırat Öncel",
      "Matthias Bethge",
      "Beyza Ermis",
      "Mirco Ravanelli",
      "Cem Subakan",
      "Çağatay Yıldız"
    ],
    "abstract": "In the last decade, the generalization and adaptation abilities of deep\nlearning models were typically evaluated on fixed training and test\ndistributions. Contrary to traditional deep learning, large language models\n(LLMs) are (i) even more overparameterized, (ii) trained on unlabeled text\ncorpora curated from the Internet with minimal human intervention, and (iii)\ntrained in an online fashion. These stark contrasts prevent researchers from\ntransferring lessons learned on model generalization and adaptation in deep\nlearning contexts to LLMs. To this end, our short paper introduces empirical\nobservations that aim to shed light on further training of already pretrained\nlanguage models. Specifically, we demonstrate that training a model on a text\ndomain could degrade its perplexity on the test portion of the same domain. We\nobserve with our subsequent analysis that the performance degradation is\npositively correlated with the similarity between the additional and the\noriginal pretraining dataset of the LLM. Our further token-level perplexity\nobservations reveals that the perplexity degradation is due to a handful of\ntokens that are not informative about the domain. We hope these findings will\nguide us in determining when to adapt a model vs when to rely on its\nfoundational capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2410.05581v2",
    "published_date": "2024-10-08 00:37:16 UTC",
    "updated_date": "2024-10-16 07:07:20 UTC"
  },
  {
    "arxiv_id": "2410.05578v1",
    "title": "Swift Sampler: Efficient Learning of Sampler by 10 Parameters",
    "authors": [
      "Jiawei Yao",
      "Chuming Li",
      "Canran Xiao"
    ],
    "abstract": "Data selection is essential for training deep learning models. An effective\ndata sampler assigns proper sampling probability for training data and helps\nthe model converge to a good local minimum with high performance. Previous\nstudies in data sampling are mainly based on heuristic rules or learning\nthrough a huge amount of time-consuming trials. In this paper, we propose an\nautomatic \\textbf{swift sampler} search algorithm, \\textbf{SS}, to explore\nautomatically learning effective samplers efficiently. In particular,\n\\textbf{SS} utilizes a novel formulation to map a sampler to a low dimension of\nhyper-parameters and uses an approximated local minimum to quickly examine the\nquality of a sampler. Benefiting from its low computational expense,\n\\textbf{SS} can be applied on large-scale data sets with high efficiency.\nComprehensive experiments on various tasks demonstrate that \\textbf{SS} powered\nsampling can achieve obvious improvements (e.g., 1.5\\% on ImageNet) and\ntransfer among different neural networks. Project page:\nhttps://github.com/Alexander-Yao/Swift-Sampler.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2024. Project page:\n  https://github.com/Alexander-Yao/Swift-Sampler",
    "pdf_url": "http://arxiv.org/pdf/2410.05578v1",
    "published_date": "2024-10-08 00:26:29 UTC",
    "updated_date": "2024-10-08 00:26:29 UTC"
  },
  {
    "arxiv_id": "2410.05575v2",
    "title": "ClaimBrush: A Novel Framework for Automated Patent Claim Refinement Based on Large Language Models",
    "authors": [
      "Seiya Kawano",
      "Hirofumi Nonaka",
      "Koichiro Yoshino"
    ],
    "abstract": "Automatic refinement of patent claims in patent applications is crucial from\nthe perspective of intellectual property strategy. In this paper, we propose\nClaimBrush, a novel framework for automated patent claim refinement that\nincludes a dataset and a rewriting model. We constructed a dataset for training\nand evaluating patent claim rewriting models by collecting a large number of\nactual patent claim rewriting cases from the patent examination process. Using\nthe constructed dataset, we built an automatic patent claim rewriting model by\nfine-tuning a large language model. Furthermore, we enhanced the performance of\nthe automatic patent claim rewriting model by applying preference optimization\nbased on a prediction model of patent examiners' Office Actions. The\nexperimental results showed that our proposed rewriting model outperformed\nheuristic baselines and zero-shot learning in state-of-the-art large language\nmodels. Moreover, preference optimization based on patent examiners'\npreferences boosted the performance of patent claim refinement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, This work has been submitted to the IEEE for possible\n  publication",
    "pdf_url": "http://arxiv.org/pdf/2410.05575v2",
    "published_date": "2024-10-08 00:20:54 UTC",
    "updated_date": "2024-10-10 05:45:21 UTC"
  },
  {
    "arxiv_id": "2410.05573v2",
    "title": "TaeBench: Improving Quality of Toxic Adversarial Examples",
    "authors": [
      "Xuan Zhu",
      "Dmitriy Bespalov",
      "Liwen You",
      "Ninad Kulkarni",
      "Yanjun Qi"
    ],
    "abstract": "Toxicity text detectors can be vulnerable to adversarial examples - small\nperturbations to input text that fool the systems into wrong detection.\nExisting attack algorithms are time-consuming and often produce invalid or\nambiguous adversarial examples, making them less useful for evaluating or\nimproving real-world toxicity content moderators. This paper proposes an\nannotation pipeline for quality control of generated toxic adversarial examples\n(TAE). We design model-based automated annotation and human-based quality\nverification to assess the quality requirements of TAE. Successful TAE should\nfool a target toxicity model into making benign predictions, be grammatically\nreasonable, appear natural like human-generated text, and exhibit semantic\ntoxicity. When applying these requirements to more than 20 state-of-the-art\n(SOTA) TAE attack recipes, we find many invalid samples from a total of 940k\nraw TAE attack generations. We then utilize the proposed pipeline to filter and\ncurate a high-quality TAE dataset we call TaeBench (of size 264k). Empirically,\nwe demonstrate that TaeBench can effectively transfer-attack SOTA toxicity\ncontent moderation models and services. Our experiments also show that TaeBench\nwith adversarial training achieve significant improvements of the robustness of\ntwo toxicity detectors.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted for publication in NAACL 2025. The official version will be\n  available in the ACL Anthology",
    "pdf_url": "http://arxiv.org/pdf/2410.05573v2",
    "published_date": "2024-10-08 00:14:27 UTC",
    "updated_date": "2025-05-01 02:59:58 UTC"
  },
  {
    "arxiv_id": "2410.05572v1",
    "title": "Improved deep learning of chaotic dynamical systems with multistep penalty losses",
    "authors": [
      "Dibyajyoti Chakraborty",
      "Seung Whan Chung",
      "Ashesh Chattopadhyay",
      "Romit Maulik"
    ],
    "abstract": "Predicting the long-term behavior of chaotic systems remains a formidable\nchallenge due to their extreme sensitivity to initial conditions and the\ninherent limitations of traditional data-driven modeling approaches. This paper\nintroduces a novel framework that addresses these challenges by leveraging the\nrecently proposed multi-step penalty (MP) optimization technique. Our approach\nextends the applicability of MP optimization to a wide range of deep learning\narchitectures, including Fourier Neural Operators and UNETs. By introducing\npenalized local discontinuities in the forecast trajectory, we effectively\nhandle the non-convexity of loss landscapes commonly encountered in training\nneural networks for chaotic systems. We demonstrate the effectiveness of our\nmethod through its application to two challenging use-cases: the prediction of\nflow velocity evolution in two-dimensional turbulence and ocean dynamics using\nreanalysis data. Our results highlight the potential of this approach for\naccurate and stable long-term prediction of chaotic dynamics, paving the way\nfor new advancements in data-driven modeling of complex natural phenomena.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 5 Figures, Submitted to CASML2024",
    "pdf_url": "http://arxiv.org/pdf/2410.05572v1",
    "published_date": "2024-10-08 00:13:57 UTC",
    "updated_date": "2024-10-08 00:13:57 UTC"
  },
  {
    "arxiv_id": "2410.17275v1",
    "title": "Automated Quality Control System for Canned Tuna Production using Artificial Vision",
    "authors": [
      "Sendey Vera",
      "Luis Chuquimarca",
      "Wilson Galdea",
      "Bremnen Véliz",
      "Carlos Saldaña"
    ],
    "abstract": "This scientific article presents the implementation of an automated control\nsystem for detecting and classifying faults in tuna metal cans using artificial\nvision. The system utilizes a conveyor belt and a camera for visual recognition\ntriggered by a photoelectric sensor. A robotic arm classifies the metal cans\naccording to their condition. Industry 4.0 integration is achieved through an\nIoT system using Mosquitto, Node-RED, InfluxDB, and Grafana. The YOLOv5 model\nis employed to detect faults in the metal can lids and the positioning of the\neasy-open ring. Training with GPU on Google Colab enables OCR text detection on\nthe labels. The results indicate efficient real-time problem identification,\noptimization of resources, and delivery of quality products. At the same time,\nthe vision system contributes to autonomy in quality control tasks, freeing\noperators to perform other functions within the company.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "eess.IV",
      "68Txx, 65D19",
      "B.7; I.2; I.4"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.17275v1",
    "published_date": "2024-10-08 00:11:24 UTC",
    "updated_date": "2024-10-08 00:11:24 UTC"
  }
]
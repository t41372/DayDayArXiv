{
  "date": "2025-03-11",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-11 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 上的研究热点依然集中在大型语言模型（LLM）和扩散模型上。LLM 不仅在机器人控制（如 FP3 的 3D 基础策略）、化学合成规划、音乐生成（如 YuE）、企业智能等领域展现出强大的潜力，其效率（SAGE-KV）、安全性（ASIMOV 基准）、可解释性（重复 Token 现象）和对齐（RMOD、D3PO）也备受关注。扩散模型在 3D 视觉（如 7DGS）、视频生成（REGEN、SynCoS）、可控性（OminiControl2、Latent CLIP）和理论理解（高维特性、质量-多样性权衡）方面持续创新。此外，机器人技术（尤其是结合 LLM）、联邦学习、可解释 AI 和多模态学习也是今日的重点。\n\n以下是值得关注的论文：\n\n---\n\n**重点论文解读**\n\n**1. FP3：用于机器人操作的 3D 基础策略 (FP3: A 3D Foundation Policy for Robotic Manipulation)**\n针对现有机器人基础模型主要依赖 2D 图像、忽略关键 3D 几何信息的痛点，本文提出了首个大规模 3D 基础策略模型 FP3。该模型基于可扩展的扩散 Transformer (diffusion transformer) 架构，并在包含点云观测的大规模多任务轨迹数据上进行预训练。FP3 能够高效微调至下游任务，并展现出强大的泛化能力，仅需 80 个演示即可在包含未见物体的新环境中学习新任务，成功率超过 90%，显著优于现有模型。\n\n**2. YuE：用于长篇音乐生成的开放基础模型扩展 (YuE: Scaling Open Foundation Models for Long-Form Music Generation)**\n本文介绍了 YuE，一个基于 LLaMA2 架构的开放基础模型系列，专注于解决具有挑战性的长篇音乐生成任务，特别是“歌词到歌曲”的生成。YuE 能够处理数万亿级别的 Token，生成长达五分钟的音乐，同时保持歌词对齐、连贯的音乐结构和吸引人的声乐旋律及伴奏。其关键技术包括：(1) 解耦音轨的 next-token 预测以克服密集混合信号；(2) 结构化渐进条件化以实现长上下文歌词对齐；(3) 多任务、多阶段预训练。此外，该研究重新设计了音乐生成的上下文学习技术，实现了多样的风格迁移和双向生成。评估表明 YuE 在音乐性和人声表现上媲美甚至超越了一些专有系统，并且其学习到的表示在音乐理解任务上也表现优异。\n\n**3. ASIMOV 基准：为机器人生成宪法和语义安全基准 (Generating Robot Constitutions & Benchmarks for Semantic Safety)**\n随着大型视觉语言模型 (VLM) 被用于控制机器人，语义安全成为紧迫问题。本文贡献了两方面：(1) 发布了 ASIMOV 基准，一个大规模、全面的数据集集合，用于评估和改进作为机器人大脑的基础模型的语义安全性。该基准利用文本和图像生成技术，从真实世界视觉场景和医院伤害报告中生成不期望的情况，具有高度可扩展性。 (2) 开发了一个框架，利用 Constitutional AI 机制，从真实世界数据自动生成机器人“宪法”来引导机器人行为，并提出了一种新颖的自动修正过程来细化规则。实验表明，生成的宪法能有效引导机器人拒绝违宪行为，在 ASIMOV 基准上达到了 84.3% 的对齐率。\n\n**4. LLM 中的化学推理解锁可控合成规划和反应机理阐明 (Chemical reasoning in LLMs unlocks steerable synthesis planning and reaction mechanism elucidation)**\n大型语言模型 (LLM) 被证明可以作为强大的化学推理引擎。研究者将 LLM 与传统搜索算法结合，利用 LLM 评估化学策略并指导搜索。该方法在两个基本化学挑战中得到验证：(1) 策略感知的逆合成规划：允许化学家以自然语言指定期望的合成策略，在巨大搜索空间中找到满足约束的路线。(2) 机理阐明：LLM 通过结合化学原理和系统探索来指导寻找合理的反应机理。该方法在多种化学任务中表现出色，展示了 LLM 在化学推理方面的潜力。\n\n**5. SAGE-KV：自注意力引导的 KV 缓存驱逐实现高效长上下文推理 (LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference)**\n随着 LLM 上下文窗口扩展到百万级 Token，KV 缓存和注意力计算成为内存和延迟瓶颈。本文发现长上下文任务中的注意力具有稀疏性，LLM 在预填充阶段后隐式地“知道”哪些 Token 可以在注意力头 (head) 级别被丢弃。基于此，提出了 SAGE-KV 方法，在预填充后执行一次性的 Token 和 Head 级别的 Top-k 选择来压缩 KV 缓存，从而实现高效推理。实验表明，SAGE-KV 在保持与全注意力相当的准确性的同时，显著提高了内存效率和推理速度，优于 StreamLLM 和 Quest 等现有方法。\n\n**6. 7DGS：统一时空-角度的高斯溅射 (7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting)**\n实时渲染具有视角依赖效应的动态场景仍然是一个挑战。本文提出了 7D 高斯溅射 (7DGS)，一个统一框架，将场景元素表示为跨越位置 (3D)、时间 (1D) 和视角方向 (3D) 的七维高斯分布。核心贡献是一种高效的条件切片机制，将 7D 高斯转换为视图和时间条件下的 3D 高斯，保持与现有 3D 高斯溅射管线的兼容性并实现联合优化。实验证明，7DGS 在 PSNR 上比先前方法提高了 7.36 dB，同时在具有复杂视角依赖效应的动态场景中实现了实时渲染 (401 FPS)。\n\n**7. ProtTeX：使用大型语言模型进行蛋白质的结构上下文推理和编辑 (ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with Large Language Models)**\n现有蛋白质 LLM 主要依赖氨基酸序列作为 Tokenizer，限制了其处理结构依赖任务的能力。本文提出 ProtTeX 框架，将蛋白质序列、结构和文本信息统一 Token 化到离散空间。通过 Next-Token Prediction 范式进行联合训练，使通用 LLM 能够通过序列文本输入感知和处理蛋白质结构，利用结构信息作为中间推理组件，并通过序列文本输出生成或操作结构。实验表明，该模型在蛋白质功能预测方面显著优于 SOTA 领域专家模型，并实现了高质量构象生成和可定制蛋白质设计。\n\n**8. 解释大型语言模型中的重复 Token 现象 (Interpreting the Repeated Token Phenomenon in Large Language Models)**\nLLM 在被要求重复单个单词时，有时会失败并输出无关文本。本研究旨在解释此现象，并将其与“注意力汇点 (attention sinks)”（初始 Token 获得不成比例高注意力分数的现象）联系起来。研究识别了负责注意力汇点的神经回路，并展示了长重复如何破坏该回路。研究者还发现其他非重复序列也存在类似的回路破坏。最后，提出了一种针对性的补丁，有效解决了该问题而不影响模型整体性能。\n\n---\n\n**LLM 应用与探索**\n\n*   **FPGS：大规模高斯溅射的前馈语义感知照片级风格迁移 (FPGS: Feed-Forward Semantic-aware Photorealistic Style Transfer of Large-Scale Gaussian Splatting)**：提出 FPGS，一种用于高斯溅射表示的大规模 3D 场景照片级风格迁移方法，支持任意风格参考图像，无需额外优化，保持多视图一致性和实时渲染速度。\n*   **用于机器人操作的 3D 基础策略 (FP3: A 3D Foundation Policy for Robotic Manipulation)**：(见重点解读)\n*   **LLM 知道该丢弃什么：自注意力引导的 KV 缓存驱逐实现高效长上下文推理 (LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference)**：(见重点解读)\n*   **用于协作自动驾驶的 CoLMDriver：基于 LLM 的协商带来益处 (CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving)**：提出 CoLMDriver，首个基于 LLM 的完整协作驾驶系统，通过基于语言的协商和实时驾驶控制改善 V2V 协作。\n*   **交叉审查员：评估大型语言模型生成解释的一致性 (Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations)**：提出 cross-examiner 方法，结合符号信息提取和 LLM 驱动的问题生成，用于生成后续问题以检查模型解释的一致性。\n*   **鲁棒多目标受控解码大型语言模型 (Robust Multi-Objective Controlled Decoding of Large Language Models)**：提出 RMOD，一种推理时算法，通过优化最坏情况奖励来实现多目标（如指令遵循、有用性、简洁性）的公平对齐。\n*   **困惑度陷阱：基于 PLM 的检索器高估低困惑度文档 (Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents)**：揭示了基于预训练语言模型 (PLM) 的检索器偏好 LLM 生成内容的“源偏见”源于其学习了困惑度特征，并提出因果启发的去偏方法 CDC。\n*   **野外的思维链推理并非总是忠实的 (Chain-of-Thought Reasoning In The Wild Is Not Always Faithful)**：研究发现，即使在没有人工偏见的现实提示中，前沿 LLM（如 Sonnet 3.7, DeepSeek R1, ChatGPT-4o）的思维链 (CoT) 推理也存在不可忽略的不忠实率，例如隐式事后合理化。\n*   **AgentOrca：评估语言智能体操作常规和约束遵守的双系统框架 (AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence)**：提出 AgentOrca 框架，用于评估语言智能体遵守操作约束和例程的能力，发现现有模型在遵守复杂约束或面对用户说服时存在显著性能差距。\n*   **利用指令遵循检索器进行恶意信息检索 (Exploiting Instruction-Following Retrievers for Malicious Information Retrieval)**：研究发现，指令遵循的检索器（如 NV-Embed, LLM2Vec）在面对恶意请求时，能够检索到相关的有害内容，即使是安全对齐的 LLM 在获得这些有害上下文后也可能满足恶意请求。\n*   **探索大型语言模型的词义消歧能力 (Exploring the Word Sense Disambiguation Capabilities of Large Language Models)**：评估了多种 LLM 在词义消歧 (WSD) 任务上的表现，发现零样本学习效果良好但不如 SOTA，而中等参数量的微调模型则超越了 SOTA。\n*   **DAFE：通过动态仲裁评估自由形式问答的基于 LLM 的评估 (DAFE: LLM-Based Evaluation Through Dynamic Arbitration for Free-Form Question-Answering)**：提出 DAFE 框架，使用两个主要 LLM 作为评判者，仅在意见不一致时引入第三方仲裁者，以提高自由形式问答评估的可靠性和效率。\n*   **Mellow：用于推理的小型音频语言模型 (Mellow: a small audio language model for reasoning)**：推出 Mellow，一个专为推理设计的小型音频语言模型，在推理能力上达到 SOTA 水平，并超过了一些更大的模型。同时发布了 ReasonAQA 数据集用于训练。\n*   **化学推理在 LLM 中解锁可控合成规划和反应机理阐明 (Chemical reasoning in LLMs unlocks steerable synthesis planning and reaction mechanism elucidation)**：(见重点解读)\n*   **GTR：引导式思维强化防止基于 RL 的 VLM 智能体训练中的思维崩溃 (GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training)**：发现在视觉环境中仅基于动作结果奖励的 RL 会导致 VLM 智能体思维链 (CoT) 推理的“思维崩溃”，提出 GTR 框架，通过自动校正器评估和改进推理，同时训练推理和行动，显著提升了性能。\n*   **LLM 作为数据驱动多任务优化的元代理：原理验证研究 (Large Language Model as Meta-Surrogate for Data-Driven Many-Task Optimization: A Proof-of-Principle Study)**：提出利用 LLM 作为元代理模型，通过统一框架和元数据进行多任务适应度预测，实现任务间知识共享和对新任务的适应，并在进化迁移优化中展示了其潜力。\n*   **超越大纲：使用语言模型进行自适应长文写作的异构递归规划 (Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models)**：提出一种通用智能体框架，通过递归任务分解和动态整合检索、推理、组合三种任务类型，实现类似人类的自适应长文写作。\n*   **用于门诊转诊的大型语言模型：问题定义、基准测试和挑战 (Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges)**：系统研究了 LLM 在智能门诊转诊 (IOR) 系统中的能力和局限性，并提出了一个包含静态和动态评估任务的综合评估框架。\n*   **Oasis：只需一张图像即可合成多模态指令数据 (Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis)**：提出 Oasis 方法，仅使用图像提示 MLLM 来自动合成高质量、多样化的多模态训练数据，并通过精细的质量控制确保数据质量。\n*   **Reflective Memory Management：用于长期个性化对话智能体的反思性记忆管理 (In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents)**：提出 RMM 机制，通过前瞻性反思（动态总结交互）和回顾性反思（基于 LLM 引用证据的在线 RL 优化检索）来管理长期对话记忆，提升个性化和连续性。\n*   **用于科学声明验证的基于 LLM 的确证和驳斥证据检索 (LLM-based Corroborating and Refuting Evidence Retrieval for Scientific Claim Verification)**：提出 CIBER 框架，扩展 RAG，通过评估 LLM 对不同探针响应的一致性来识别科学声明的确证和驳斥证据，适用于黑盒和白盒模型。\n*   **自回归思维链学习理论 (A Theory of Learning with Autoregressive Chain of Thought)**：形式化并分析了通过迭代固定生成器产生思维链（CoT）来学习提示-答案映射的问题，探讨了 CoT 可见和潜在情况下的样本和计算复杂性。\n*   **StudyChat 数据集：人工智能课程中学生与 ChatGPT 的对话 (The StudyChat Dataset: Student Dialogues With ChatGPT in an Artificial Intelligence Course)**：发布 StudyChat 数据集，包含大学 AI 课程中学生与 LLM 聊天机器人的真实交互日志及标注，用于分析学生使用模式。\n*   **用于企业智能和分析的 LLM 驱动知识图谱 (LLM-Powered Knowledge Graphs for Enterprise Intelligence and Analytics)**：提出一个框架，使用 LLM 将企业内各种数据源（邮件、日历、文档等）统一到以活动为中心的知识图谱中，实现跨数据类型的高级查询、推理和分析。\n\n---\n\n**计算机视觉与图形学**\n\n*   **FPGS：大规模高斯溅射的前馈语义感知照片级风格迁移 (FPGS: Feed-Forward Semantic-aware Photorealistic Style Transfer of Large-Scale Gaussian Splatting)**：(见 LLM 应用)\n*   **HessianForge：基于物理信息神经表示和平滑能量约束的可扩展 LiDAR 重建 (HessianForge: Scalable LiDAR reconstruction with Physics-Informed Neural Representation and Smoothness Energy Constraints)**：提出一种基于深度学习的方法，通过优化 L2-Hessian 能量的物理信息损失函数，从原始 LiDAR 点云学习 SDF，以生成更平滑、无伪影的大规模室外环境 3D 地图。\n*   **GarmentCrafter：用于单视图 3D 服装重建和编辑的渐进式新视图合成 (GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D Garment Reconstruction and Editing)**：提出 GarmentCrafter，通过渐进式深度预测和图像变形来近似新视图，并训练多视图扩散模型补全遮挡区域，实现从单视图图像创建和修改 3D 服装。\n*   **REGEN：使用（再）生成解码器学习紧凑视频嵌入 (REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder)**：提出用编码器-生成器框架取代传统编码器-解码器，使用 Diffusion Transformer (DiT) 从紧凑潜空间合成细节，实现高压缩率视频嵌入，同时保持下游生成模型质量。\n*   **MEAT：基于网格注意力的百万像素级人体多视图扩散模型 (MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention)**：提出 MEAT，一种用于百万像素级人体生成的多视图扩散模型，引入网格注意力 (mesh attention) 解决高分辨率下多视图注意力的挑战，并利用人体运动视频进行训练。\n*   **使用同步耦合采样实现无需微调的多事件长视频生成 (Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling)**：提出 SynCoS，一种推理框架，通过同步反向采样和基于优化的采样路径，确保多事件长视频生成中的长距离一致性。\n*   **AnyMoLe：利用视频扩散模型实现任意角色的运动中间帧生成 (AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models)**：提出 AnyMoLe，利用视频扩散模型为任意角色生成运动中间帧，无需特定角色数据集，通过两阶段生成、ICAdapt 微调和“运动-视频模仿”优化实现。\n*   **ObjectMover：使用视频先验生成对象移动 (ObjectMover: Generative Object Movement with Video Prior)**：提出 ObjectMover，将对象移动建模为序列到序列问题，并微调视频生成模型，利用其跨帧生成一致对象的能力，实现复杂场景下的对象移动编辑。\n*   **7DGS：统一时空-角度的高斯溅射 (7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting)**：(见重点解读)\n*   **COD-VAE：用 64 个潜在向量表示 3D 形状用于 3D 扩散模型 (Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models)**：提出 COD-VAE，一种两阶段 VAE，将 3D 点云压缩为 64 个 1D 潜在向量，并通过基于 Triplane 的解码器高效重建，显著提升 3D 扩散模型的效率。\n*   **MGHanD：用于真实手部扩散的多模态引导 (MGHanD: Multi-modal Guidance for authentic Hand Diffusion)**：提出 MGHanD，在推理过程中应用视觉（判别器）和文本（LoRA 适配器）多模态引导，结合累积手部掩码，改善 T2I 模型生成真实手部的能力。\n*   **稳定世界模型：测量和解决生成环境中的世界不稳定性 (Toward Stable World Models: Measuring and Addressing World Instability in Generative Environments)**：提出“世界稳定性”概念，用于衡量世界模型在时间推移中保持内容一致性的能力，并引入评估框架和改进策略。\n*   **CL-MVSNet：具有双层对比学习的无监督多视图立体匹配 (CL-MVSNet: Unsupervised Multi-view Stereo with Dual-level Contrastive Learning)**：提出 CL-MVSNet，一种无监督 MVS 方法，通过图像级和场景级双层对比学习构建额外监督信号，解决低纹理和视角依赖效应问题。\n*   **TRACT：轨迹感知的开放词汇跟踪 (Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking)**：提出 TRACT，一种利用轨迹信息改进目标关联和分类的开放词汇多目标跟踪器，包含轨迹一致性强化 (TCR) 和 TraCLIP 模块。\n*   **UniF²ace：使用统一多模态模型进行细粒度人脸理解和生成 (UniF²ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models)**：提出 UniF²ace，首个专为细粒度人脸理解和生成定制的统一多模态模型 (UMM)，通过自建数据集、双扩散技术和双层 MoE 架构实现。\n\n---\n\n**机器人与强化学习**\n\n*   **FP3：用于机器人操作的 3D 基础策略 (FP3: A 3D Foundation Policy for Robotic Manipulation)**：(见重点解读)\n*   **ASIMOV 基准：为机器人生成宪法和语义安全基准 (Generating Robot Constitutions & Benchmarks for Semantic Safety)**：(见重点解读)\n*   **用于协作自动驾驶的 CoLMDriver：基于 LLM 的协商带来益处 (CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving)**：(见 LLM 应用)\n*   **GTR：引导式思维强化防止基于 RL 的 VLM 智能体训练中的思维崩溃 (GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training)**：(见 LLM 应用)\n*   **MoE-Loco：用于多任务运动的专家混合模型 (MoE-Loco: Mixture of Experts for Multitask Locomotion)**：提出 MoE-Loco 框架，使用专家混合 (MoE) 处理足式机器人的多任务运动（不同地形、步态），缓解梯度冲突，提高训练效率和性能。\n*   **FAIL-Detect：我们能否在没有失败数据的情况下检测失败？模仿学习策略的不确定性感知的运行时失败检测 (Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies)**：提出 FAIL-Detect，一种模块化两阶段方法，将失败检测视为序列 OOD 检测，利用策略输入输出和认知不确定性，结合共形预测 (CP) 实现无需失败数据的失败检测。\n*   **EMMOE：开放环境中具身移动操作的综合基准 (EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments)**：推出 EMMOE 基准，包含解释指令和执行长序列日常任务，并提供 EMMOE-100 数据集和 HomieBot 智能体系统。\n*   **通用空中智能体由大型语言模型赋能 (General-Purpose Aerial Intelligent Agents Empowered by Large Language Models)**：提出首个能够执行开放世界任务的空中智能体系统，通过软硬件协同设计，在边缘计算平台运行 LLM，并结合双向认知架构实现任务规划与反应控制。\n*   **DexGrasp Anything：迈向具有物理意识的通用机器人灵巧抓取 (DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness)**：提出 DexGrasp Anything，将物理约束整合到扩散生成模型的训练和采样阶段，用于生成高质量、可用的灵巧手抓取姿态，并发布了包含 340 万抓取姿态的大型数据集。\n*   **HASARD：基于视觉的具身智能体安全强化学习基准 (HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents)**：推出 HASARD，一套基于 Doom 引擎的复杂 3D 任务套件，专门用于评估仅依赖自我中心视觉输入的 Safe RL 方法。\n*   **使用安全动作扰动平衡电池单元的 SoC (Balancing SoC in Battery Cells using Safe Action Perturbations)**：提出一种方法，通过在深度强化学习智能体上添加安全层来学习安全的电池充电动作，安全层扰动智能体动作以防止电池进入不安全状态。\n*   **指令增强的长序列规划：在具身移动操作中嵌入基础机制 (Instruction-Augmented Long-Horizon Planning: Embedding Grounding Mechanisms in Embodied Mobile Manipulation)**：提出 IALP 系统，利用 LLM 基于实时传感器反馈（包括环境的基础知识）生成可行和最优动作，通过将用户指令增强为 PDDL 问题来解决长序列移动操作规划。\n*   **用于多模态操作的分层接触丰富轨迹优化，使用紧凸松弛 (Hierarchical Contact-Rich Trajectory Optimization for Multi-Modal Manipulation using Tight Convex Relaxations)**：提出一种分层优化框架，结合 MILP（选择最优接触）和 NLP（优化轨迹），用于高效设计接触丰富的操作任务中机器人、物体和接触的轨迹。\n*   **通过基于模型的强化学习和策略重用增强交通信号控制 (Enhancing Traffic Signal Control through Model-based Reinforcement Learning and Policy Reuse)**：提出 PLight (基于模型的 RL 预训练) 和 PRLight (基于相似性自适应选择预训练智能体) 算法，以提高交通信号控制 MARL 方法的泛化性和适应性。\n*   **MoRE：解锁四足视觉-语言-动作模型中强化学习的可扩展性 (MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models)**：提出 MoRE，一种用于四足机器人的 VLA 模型，使用专家混合 (MoE) 结构和 RL 进行微调，以处理大量混合质量数据并适应下游任务。\n\n---\n\n**AI/ML 理论与方法**\n\n*   **KAN-Mixers：一种用于图像分类的新型深度学习架构 (KAN-Mixers: a new deep learning architecture for image classification)**：提出 KAN-Mixers，一种基于 Mixer 的新架构，使用 Kolmogorov-Arnold Network (KAN) 作为主要层，在图像分类任务上优于 MLP、MLP-Mixer 和 KAN 模型。\n*   **Prompt-OT：视觉语言模型自适应中知识保存的最优传输正则化范式 (Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation)**：提出一种最优传输 (OT) 引导的提示学习框架，通过保持预训练和微调模型之间特征分布的结构一致性来缓解遗忘，提升泛化能力。\n*   **Stackelberg 博弈中相关策略的模仿学习 (Imitation Learning of Correlated Policies in Stackelberg Games)**：提出一种针对 Stackelberg 博弈的相关策略占据度量，并引入 Latent Stackelberg Differential Network (LSDN) 来匹配它，以更好地模拟非对称交互动态。\n*   **具有有限观测的零样本动作泛化 (Zero-Shot Action Generalization with Limited Observations)**：提出 AGLO 框架，包含动作表示学习模块和策略学习模块，利用有限观测和增强的合成动作表示来学习能够处理未见动作的任务的策略。\n*   **结合局部对称性利用和强化学习优化概率推理 - 进行中 (Combining Local Symmetry Exploitation and Reinforcement Learning for Optimised Probabilistic Inference -- A Work In Progress)**：将强化学习方法应用于图模型中的变量消除顺序优化，并结合利用局部对称性来压缩中间结果，使智能体能探索更有效的收缩顺序。\n*   **重新思考高维扩散模型 (Rethinking Diffusion Model in High Dimension)**：分析了扩散模型的目标函数和推理方法，指出在高维稀疏场景下，目标函数拟合目标退化为单个样本，主流推理方法可统一表示，并可发现更高效的推理方法。\n*   **理解扩散语言模型中的质量-多样性权衡 (Understanding the Quality-Diversity Trade-off in Diffusion Language Models)**：提出使用无分类器指导 (classifier-free guidance) 和随机钳制 (stochastic clamping) 来控制序列到序列任务中扩散语言模型的质量-多样性权衡。\n*   **通过数据生成过程视角学习异构图结构 (Heterogeneous Graph Structure Learning through the Lens of Data-generating Processes)**：首次提出异构图结构学习 (HGSL) 方法，通过新的数据生成过程模型 H2MN，将 HGSL 形式化为最大后验估计问题，并推导出交替优化方法。\n*   **神经符号决策树 (Neurosymbolic Decision Trees)**：引入神经符号决策树 (NDT) 和新的 NeSy 结构学习算法 NeuID3，将决策树归纳与神经概率逻辑表示相结合，支持符号和子符号数据，并能在归纳树结构时利用背景知识。\n*   **用于深度学习训练的三重惯性加速交替优化方法 (A Triple-Inertial Accelerated Alternating Optimization Method for Deep Learning Training)**：提出 TIAM 框架，将三重惯性加速策略与专门的近似方法相结合，用于神经网络训练中的交替最小化，以提高收敛效率。\n*   **使用潜在 CLIP 控制潜在扩散 (Controlling Latent Diffusion Using Latent CLIP)**：引入 Latent-CLIP，一个直接在 VAE 潜空间操作的 CLIP 模型，用于评估和指导潜在扩散模型 (LDM) 的生成，降低了计算成本。\n*   **通过激活上的稀疏干预进行端到端学习以引导生成 (End-to-end Learning of Sparse Interventions on Activations to Steer Generation)**：提出 LinEAS，一种通过全局损失训练的线性端到端激活引导方法，可同时考虑所有层级分布偏移，并通过稀疏化正则自动选择神经元和层。\n*   **鲁棒潜在事项：通过采样误差合成提升图像生成 (Robust Latent Matters: Boosting Image Generation with Sampling Error Synthesis)**：分析了离散潜空间中重建和生成质量差异的原因，提出潜在扰动方法模拟采样噪声，并基于此提出新的 Tokenizer 评估指标 pFID 和即插即用训练方案，提升生成质量。\n*   **通过将策略蒸馏为程序来评估可解释强化学习 (Evaluating Interpretable Reinforcement Learning by Distilling Policies into Programs)**：提出一种无需人类参与的评估策略可解释性的方法，使用模仿学习将专家神经网络蒸馏为小程序作为基线，并使用可模拟性代理进行评估。\n*   **FlowDPS：用于逆问题的流驱动后验采样 (FlowDPS: Flow-Driven Posterior Sampling for Inverse Problems)**：将扩散逆求解器 (DIS) 扩展到流模型 (Flow Matching) 框架，提出 FlowDPS，通过分解流 ODE 并结合似然梯度和随机噪声实现逆问题的后验采样。\n*   **对齐扩散模型中的文本到图像比你想象的更容易 (Aligning Text to Image in Diffusion Models is Easier Than You Think)**：认为传统 T2I 扩散模型训练方式在表示对齐方面次优，提出 SoftREPA，一种轻量级对比微调策略，使用软文本 Token 和对比学习来改善文本-图像对齐。\n*   **多模态持续学习 (Continual Learning for Multiple Modalities)**：提出一种适应多种模态（图像、视频、音频、深度、文本）的持续学习框架，通过模态内自正则化和模态间知识关联来聚合知识，并重新对齐模态嵌入以解决偏置对齐问题。\n*   **注入不平衡敏感性的多任务学习 (Injecting Imbalance Sensitivity for Multi-Task Learning)**：认为现有基于梯度的 MTL 方法忽视了任务不平衡/主导问题，提出 IMGrad 方法，通过在投影范数上施加约束来增强基线方法的不平衡敏感性。\n*   **离线强化学习中可证明的零样本泛化 (Provable Zero-Shot Generalization in Offline Reinforcement Learning)**：提出 PERM 和 PPPO 算法，利用悲观策略评估指导策略学习，以增强离线 RL 在未见环境中的零样本泛化能力，并提供了理论保证。\n*   **广义 Kullback-Leibler 散度损失 (Generalized Kullback-Leibler Divergence Loss)**：证明 KL 散度等价于解耦 KL (DKL) 散度，并基于 DKL 提出广义 KL (GKL) 散度，通过打破非对称优化和引入类全局信息来改进 KL 损失，在对抗训练和知识蒸馏任务上取得 SOTA 结果。\n*   **HOFAR：流自回归 Transformer 的高阶增强 (HOFAR: High-Order Augmentation of Flow Autoregressive Transformers)**：提出 HOFAR 框架，通过高阶监督系统地增强流自回归 Transformer (FlowAR)，理论和实验证明其在生成质量上优于基线模型。\n*   **共同演化生成模型的收敛动力学与稳定策略 (Convergence Dynamics and Stabilization Strategies of Co-Evolving Generative Models)**：分析了文本模型和图像模型相互影响、共同演化的系统，揭示了模型崩溃的加速机制，并分析了通过外部语料注入稳定系统的策略。\n\n---\n\n**其他研究**\n\n*   **ECG 就足够了吗？仅使用心电图对心脏异常进行深度学习分类 (Are ECGs enough? Deep learning classification of cardiac anomalies using only electrocardiograms)**：研究了多种神经网络架构在仅使用 ECG 数据进行心脏异常分类任务上的性能，并探讨了迁移学习在小数据集（如肺栓塞检测）上的应用。\n*   **用于可信赖自动驾驶测试的模拟器集成 (Simulator Ensembles for Trustworthy Autonomous Driving Testing)**：提出 MultiSim 方法，利用模拟器集成进行基于搜索的 ADAS 测试，以识别与模拟器无关的故障诱发场景。\n*   **回溯以保障安全 (Backtracking for Safety)**：提出一种新的回溯方法，允许 LLM 在生成过程中遇到安全违规（如毒性）时，回退到较早的安全状态进行修正，而不是丢弃整个生成文本。\n*   **高维非线性噪声数据的鲁棒无监督故障诊断 (Robust Unsupervised Fault Diagnosis For High-Dimensional Nonlinear Noisy Data)**：(见 LLM 应用)\n*   **ResBench：具有资源意识的 LLM 生成 FPGA 设计基准测试 (ResBench: Benchmarking LLM-Generated FPGA Designs with Resource Awareness)**：推出 ResBench，首个关注硬件资源使用（如 LUT）的基准，用于评估 LLM 生成 HDL 代码的效率。\n*   **来自未分割演示的开放世界技能发现 (Open-World Skill Discovery from Unsegmented Demonstrations)**：提出技能边界检测 (SBD) 算法，利用预训练动作预测模型的预测误差，在无标注的长视频中自监督地检测技能边界，用于分割技能片段。\n*   **关于社交机器人是否以及如何支持残疾大学生调解和倡导的利益相关者观点 (Stakeholder Perspectives on Whether and How Social Robots Can Support Mediation and Advocacy for Higher Education Students with Disabilities)**：通过访谈和焦点小组，探讨了使用社交机器人和 LLM 支持残疾学生调解和倡导的潜力、局限性和伦理考量。\n*   **使用熵感知模糊积分策略进行自适应扫描级决策融合的 CT 扫描颅内出血分类 Vision Transformer (Vision Transformer for Intracranial Hemorrhage Classification in CT Scans Using an Entropy-Aware Fuzzy Integral Strategy for Adaptive Scan-Level Decision Fusion)**：提出基于 PVT 的模型，结合 SHAP 特征选择和熵感知模糊积分融合策略，用于 CT 扫描中的颅内出血 (ICH) 亚型分类。\n*   **受网格细胞启发的认知图谱结构化向量代数 (A Grid Cell-Inspired Structured Vector Algebra for Cognitive Maps)**：提出 GC-VSA 模型，结合连续吸引子网络 (CAN) 和向量符号架构 (VSA)，模拟网格细胞模块，用于路径整合、时空表示和符号推理。\n*   **用于自适应负载均衡的具有离散世界模型的元强化学习 (Meta-Reinforcement Learning with Discrete World Models for Adaptive Load Balancing)**：将元强化学习算法与 DreamerV3 架构集成，用于操作系统负载均衡，实现对动态工作负载的快速适应。\n*   **当讨论停滞时：超越关于生成式 AI 在设计研究中的五个语义障碍 (When Discourse Stalls: Moving Past Five Semantic Stopsigns about Generative AI in Design Research)**：识别并解构了关于 GenAI 在设计中五个普遍存在的简化性框架（语义障碍），并提出了更细致的思考和实践方法。\n*   **MsaMIL-Net：用于高效全切片图像分类的端到端多尺度感知多示例学习网络 (MsaMIL-Net: An End-to-End Multi-Scale Aware Multiple Instance Learning Network for Efficient Whole Slide Image Classification)**：提出一个端到端的多尺度 WSI 分类框架，包含语义特征过滤、多尺度特征提取和多尺度融合 MIL 模块。\n*   **有效但短暂的宣传防御：需要不止一次性接种来增强批判性思维 (Effective Yet Ephemeral Propaganda Defense: There Needs to Be More than One-Shot Inoculation to Enhance Critical Thinking)**：研究发现，使用宣传检测和情境化工具虽然能在使用期间提高批判性思维，但这种效果在停止使用后会消失，表明单次使用不足以产生持久影响。\n*   **通过 LLM 进行推理和采样增强的选择题难度预测 (Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs)**：提出一种两阶段方法预测 MCQ 难度，利用 LLM 增强推理步骤，并采样知识水平以估计学生选择各选项的可能性。\n*   **AI 创意图谱：利用知识图谱和 LLM 生成 AI 研究思路 (Graph of AI Ideas: Leveraging Knowledge Graphs and LLMs for AI Research Idea Generation)**：提出 GoAI 框架，将文献组织成知识图谱，将引文语义信息总结为关系，辅助 LLM 捕捉研究进展，生成 AI 研究思路。\n*   **BiasEdit：通过模型编辑对刻板印象语言模型进行去偏 (BiasEdit: Debiasing Stereotyped Language Models via Model Editing)**：提出 BiasEdit，一种模型编辑方法，使用轻量级网络作为编辑器生成参数更新，以消除语言模型中的刻板印象偏见，同时保留语言建模能力。\n*   **最小化和控制 I 型和 II 型错误的批次接受或拒绝 (Acceptance or Rejection of Lots while Minimizing and Controlling Type I and Type II Errors)**：讨论了双重假设检验 (DHT) 在控制生产者风险 (Type I) 和消费者风险 (Type II) 中的应用，并结合了连续失败极限 (LSF) 来放大测试功效。\n*   **加速 MoE 模型推理的专家分片 (Accelerating MoE Model Inference with Expert Sharding)**：提出 MoEShard，一种通过 MoE 专家的张量分片 (tensor sharding) 实现完美负载均衡的推理系统，优化多 GPU 环境下基于编码器的 MoE 模型推理。\n*   **工业 4.0 标准化框架的现状与未来展望：欧洲视角 (Status and Future Prospects of the Standardization Framework Industry 4.0: A European Perspective)**：探讨了工业 4.0 标准化的作用，特别是在欧洲监管框架下的重要性，并概述了智能制造和数字孪生相关的标准化活动。\n*   **ICPR 2024 骑手意图预测竞赛 (ICPR 2024 Competition on Rider Intention Prediction)**：介绍了旨在通过预测骑手操作来提高道路安全的骑手意图预测 (RIP) 竞赛，包括新的 RAAD 数据集和对提交方法的总结。\n*   **用于情感识别的神经气网络身体运动数据合成数据生成 (Synthetic Data Generation of Body Motion Data by Neural Gas Network for Emotion Recognition)**：提出使用神经气网络 (NGN) 算法合成身体运动数据，以解决情感识别领域缺乏多样化数据集的问题。\n*   **揭示 LLM 投资推荐中的产品偏见 (Exposing Product Bias in LLM Investment Recommendation)**：揭示了 LLM 在投资推荐中存在系统性地偏好特定产品（如 AAPL、MSFT）的“产品偏见”，并构建了数据集进行研究。\n*   **基于标签可靠性的无源域自适应用于跨域轴承故障诊断 (Source-free domain adaptation based on label reliability for cross-domain bearing fault diagnosis)**：提出一种新的 SFDA 方法，利用基于数据增强的标签投票策略区分可靠和不可靠伪标签，并同时利用两者进行模型自适应。\n*   **V-Max：让强化学习在自动驾驶中实用化 (V-Max: Making RL practical for Autonomous Driving)**：推出 V-Max，一个基于 Waymax 模拟器的开源研究框架，提供工具集（观测/奖励函数、编码器、训练流程、评估）使 RL 在自动驾驶中更实用。\n*   **InfluenceNet：用于 Banzhaf 和 Shapley 值预测的 AI 模型 (InfluenceNet: AI Models for Banzhaf and Shapley Value Prediction)**：提出一种基于神经网络的方法，用于高效估计投票博弈中的权力指数（Banzhaf 值和 Shapley 值），克服了传统方法的计算瓶颈。\n*   **MINT-Demo：成员推理测试演示器 (MINT-Demo: Membership Inference Test Demonstrator)**：展示了 MINT 技术，用于实验性地确定特定数据是否被用于训练机器学习模型（特别是人脸识别模型），旨在提高 AI 训练的透明度。\n*   **给薄荷加巧克力：缓解机器翻译中的度量干扰 (Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation)**：分析了机器翻译中度量干扰 (Mint) 问题（即使用相同或相关度量进行模型调优和评估导致评估失真），并提出 MintAdjust 方法进行更可靠的评估。\n*   **基于原型的异构联邦学习用于类不平衡数据下风力涡轮机叶片结冰检测 (Prototype-based Heterogeneous Federated Learning for Blade Icing Detection in Wind Turbines with Class Imbalanced Data)**：提出一种联邦原型学习模型，结合对比监督损失函数，解决异构环境和类不平衡数据下的风力涡轮机叶片结冰检测问题。\n*   **调查执行感知语言模型在代码优化中的有效性 (Investigating Execution-Aware Language Models for Code Optimization)**：研究了将代码执行信息（行执行、覆盖率、变量状态等）整合到语言模型中对代码优化能力的影响，发现相比标准 CodeT5+ 模型，收益有限。\n*   **使用可解释 AI 的灰盒文本攻击框架 (A Grey-box Text Attack Framework using Explainable AI)**：提出一种结合灰盒和黑盒的方法，利用可解释 AI 技术和一组代理 Transformer 模型生成语义良好且难以被人类检测的对抗性文本攻击。\n*   **用于盲解混的神经网络：一种新颖的矩阵卷积解混 (MCU) 方法 (Neural Network for Blind Unmixing: a novel MatrixConv Unmixing (MCU) Approach)**：提出 MCU 方法用于高光谱图像盲解混，将迭代求解器展开构建 UEDIP 和 UADIP 子网络分别估计端元和丰度，并设计复合损失函数生成有意义结果。\n*   **EgoBlind：面向盲人的自我中心视觉辅助 (EgoBlind: Towards Egocentric Visual Assistance for the Blind People)**：发布 EgoBlind 数据集，包含盲人第一视角视频和问题，用于评估 MLLM 在视觉辅助任务上的能力，发现现有模型表现不佳。\n*   **OLMD：用于连续手语识别的朝向感知长期运动解耦 (OLMD: Orientation-aware Long-term Motion Decoupling for Continuous Sign Language Recognition)**：提出 OLMD 框架，通过 LMA 模块聚合长期运动，并将复杂运动解耦为水平和垂直分量，以提高 CSLR 准确性。\n*   **集成大型语言模型的级联协作多智能体框架用于匝道汇合控制 (A Cascading Cooperative Multi-agent Framework for On-ramp Merging Control Integrating Large Language Models)**：提出 CCMA 框架，集成 RL（个体交互）、微调 LLM（区域合作）、奖励函数（全局优化）和 RAG（动态优化），用于匝道汇合控制。\n*   **猜我在想什么：角色扮演语言智能体内在思维推理基准 (Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of Role-Playing Language Agents)**：提出 ROLETHINK 基准和 MIRROR 方法，用于评估和生成角色扮演语言智能体 (RPLA) 的内在思维。\n*   **RigoChat 2：使用有限数据集和简化硬件适配西班牙语的语言模型 (RigoChat 2: an adapted language model to Spanish using a bounded dataset and reduced hardware)**：展示了如何使用有限资源和时间，基于较小的预训练 LLM (Mistral-7B-v0.1)，通过继续预训练和指令微调，显著提升模型在特定语言（西班牙语）任务上的性能。\n*   **联邦多智能体系统中的隐私增强范式 (Privacy-Enhancing Paradigms within Federated Multi-Agent Systems)**：提出联邦 MAS 概念和 EPEAgent 解决方案，将隐私增强代理嵌入 RAG 阶段，以应对 LLM 多智能体系统中的隐私挑战。\n*   **研究苏格拉底式思维链推理方法在机器人任务规划中的有效性，案例研究 (Investigating the Effectiveness of a Socratic Chain-of-Thoughts Reasoning Method for Task Planning in Robotics, A Case Study)**：应用 GPT-4(Omni) 和 SocraCoT 推理策略于模拟 Tiago 机器人的对象搜索任务，初步结果显示结合 CoT 的苏格拉底方法可用于需要空间意识的机器人任务代码生成。\n*   **XAI4Extremes：用于理解气候变化下极端天气前兆的可解释机器学习框架 (XAI4Extremes: An interpretable machine learning framework for understanding extreme-weather precursors under climate change)**：提出使用事后可解释性方法构建相关性天气图，以识别深度学习模型发现的极端天气前兆，并分析其在气候变化下的演变。\n*   **用于检索增强生成的面向知识的调查 (A Survey on Knowledge-Oriented Retrieval-Augmented Generation)**：全面概述了检索增强生成 (RAG) 的基本组成、关键特征、挑战、方法分类、评估基准和应用。\n*   **EFPC：迈向高效灵活的提示压缩 (EFPC: Towards Efficient and Flexible Prompt Compression)**：提出 EFPC 方法，统一任务感知和任务无关的提示压缩，通过 GPT-4 生成压缩提示并选择性使用，实现精度和效率的权衡。\n*   **当然是机器人还是不是？通过鲁棒多模态神经过程进行可信社交机器人检测 (Certainly Bot Or Not? Trustworthy Social Bot Detection via Robust Multi-Modal Neural Processes)**：提出 UESBD 框架和 RMNP 模型，利用多模态神经过程和证据门控网络进行社交机器人检测，同时量化预测不确定性并增强对模态冲突的鲁棒性。\n*   **在大规模图像重构中通过扩散模型保持产品保真度 (Preserving Product Fidelity in Large Scale Image Recontextualization with Diffusion Models)**：提出一个框架，使用 T2I 扩散模型和新颖的数据增强流程（利用图像到视频扩散、修复等）进行高保真度产品图像重构。\n*   **零到一 IDV：AI 驱动身份验证的概念模型 (Zero-to-One IDV: A Conceptual Model for AI-Powered Identity Verification)**：提出 \"Zero to One\" 概念框架，用于开发 AI 驱动的身份验证 (IDV) 产品，包含文档验证、生物特征验证、风险评估和编排四个组件。\n*   **量化 ICU 患者的昼夜节律失调及其与谵妄的关系 (Quantifying Circadian Desynchrony in ICU Patients and Its Association with Delirium)**：开发了一种量化昼夜节律失调的方法，发现 ICU 患者失调程度显著高于健康人，并探讨了其与谵妄发生的关系。\n*   **探索超过 100 个文本到图像生成模型的偏见 (Exploring Bias in over 100 Text-to-Image Generative Models)**：大规模评估了 100 多个 T2I 模型在分布偏见、生成幻觉和生成漏失率方面的偏见趋势，发现艺术和风格迁移模型偏见显著，而基础模型偏见逐渐减少。\n*   **SKALD：基于学习的镜头组装用于连贯的多镜头视频创作 (SKALD: Learning-Based Shot Assembly for Coherent Multi-Shot Video Creation)**：提出 SKALD 方法，使用学习到的 LCA 分数衡量镜头间的叙事连贯性，并通过束搜索组装候选镜头，创建连贯的多镜头视频。\n*   **SQLCritic：通过子句级 Critic 修正文本到 SQL 生成 (SQLCritic: Correcting Text-to-SQL Generation via Clause-wise Critic)**：提出 SQLCritic，结合结构化执行反馈和训练有素的 Critic 智能体，对 Text-to-SQL 生成的查询进行子句级批判和修正，以解决语法和语义错误。\n*   **用于空间物理的神经符号模型 (A Neural Symbolic Model for Space Physics)**：提出 PhyE2E 模型，通过将符号回归分解为子问题并使用 Transformer 端到端翻译数据为符号公式，结合 MCTS 和 GP 进行优化，成功应用于多个空间物理问题并发现新见解。\n*   **混合量子-经典决策模型的 Lipschitz 常数的高效准确估计 (Efficient and Accurate Estimation of Lipschitz Constants for Hybrid Quantum-Classical Decision Models)**：提出一个框架，通过统一的凸优化公式，将经典方法扩展到混合量子-经典模型，以高效准确地估计 Lipschitz 常数。\n*   **边界提示：通过基于图的空间 Token 化实现弹性城市区域表示 (Boundary Prompting: Elastic Urban Region Representation via Graph-based Spatial Tokenization)**：提出 BPURF 框架，将城市实体视为 Token 构建统一图，并通过边界提示定义弹性城市区域，用于表示学习。\n*   **AI 原生内存 2.0：第二个我 (AI-native Memory 2.0: Second Me)**：提出 SECOND ME 概念，一个 AI 原生的持久化记忆卸载系统，利用 LLM 保留、组织和动态利用用户特定知识，作为用户交互的中介，减少认知负荷。\n*   **多边缘 IoV 系统中移动性感知的无缝服务迁移和资源分配 (Mobility-aware Seamless Service Migration and Resource Allocation in Multi-edge IoV Systems)**：提出 SR-CL 框架，结合凸优化和深度强化学习，解决 IoV 环境中 MEC 服务器间的服务迁移和资源分配问题。\n*   **面向 6G 的无线信号识别革命：最新进展、挑战与未来方向 (Revolution of Wireless Signal Recognition for 6G: Recent Advances, Challenges and Future Directions)**：全面综述了无线信号识别 (WSR) 技术，包括应用、任务、智能方法（基于模型、数据、学习、实现）、数据集、评估指标、挑战和未来方向。\n*   **锂离子电池健康诊断的退化自监督学习 (Degradation Self-Supervised Learning for Lithium-ion Battery Health Diagnostics)**：提出一种基于自监督学习的 LIB 健康状态估计训练策略，利用经验小波变换处理非平稳电压信号，并设计描述容量退化行为的损失函数。\n*   **STGDPM：基于时空图扩散概率模型的船舶轨迹预测 (STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion Probabilistic Model)**：首次将时空图 (STG) 与扩散模型结合用于船舶轨迹预测，通过将交互建模为动态图并利用扩散模型的多模态能力来捕捉行为不确定性。",
  "papers": [
    {
      "arxiv_id": "2503.09635v1",
      "title": "FPGS: Feed-Forward Semantic-aware Photorealistic Style Transfer of Large-Scale Gaussian Splatting",
      "title_zh": "FPGS：基于前馈语义感知的大规模高斯溅射真实感风格迁移",
      "authors": [
        "GeonU Kim",
        "Kim Youwang",
        "Lee Hyoseok",
        "Tae-Hyun Oh"
      ],
      "abstract": "We present FPGS, a feed-forward photorealistic style transfer method of\nlarge-scale radiance fields represented by Gaussian Splatting. FPGS, stylizes\nlarge-scale 3D scenes with arbitrary, multiple style reference images without\nadditional optimization while preserving multi-view consistency and real-time\nrendering speed of 3D Gaussians. Prior arts required tedious per-style\noptimization or time-consuming per-scene training stage and were limited to\nsmall-scale 3D scenes. FPGS efficiently stylizes large-scale 3D scenes by\nintroducing a style-decomposed 3D feature field, which inherits AdaIN's\nfeed-forward stylization machinery, supporting arbitrary style reference\nimages. Furthermore, FPGS supports multi-reference stylization with the\nsemantic correspondence matching and local AdaIN, which adds diverse user\ncontrol for 3D scene styles. FPGS also preserves multi-view consistency by\napplying semantic matching and style transfer processes directly onto queried\nfeatures in 3D space. In experiments, we demonstrate that FPGS achieves\nfavorable photorealistic quality scene stylization for large-scale static and\ndynamic 3D scenes with diverse reference images. Project page:\nhttps://kim-geonu.github.io/FPGS/",
      "tldr_zh": "本文提出FPGS方法，实现基于高斯溅射(Gaussian Splatting)的大规模3D场景快速风格迁移。该方法通过构建风格解耦的3D特征场，在保留原始场景实时渲染速度和多视角一致性的同时，支持任意风格参考图像的单次前馈处理，无需逐风格优化。FPGS创新性地结合语义匹配和局部AdaIN技术，首次实现了大规模静态/动态3D场景的多参考图像语义感知风格化，相比现有方法显著提升了处理效率和应用范围。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "Project page: https://kim-geonu.github.io/FPGS/. arXiv admin note:\n  substantial text overlap with arXiv:2401.05516",
      "pdf_url": "http://arxiv.org/pdf/2503.09635v1",
      "published_date": "2025-03-11 23:52:56 UTC",
      "updated_date": "2025-03-11 23:52:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:12:50.932247"
    },
    {
      "arxiv_id": "2503.08960v1",
      "title": "Are ECGs enough? Deep learning classification of cardiac anomalies using only electrocardiograms",
      "title_zh": "仅凭心电图足够吗？基于深度学习的纯心电图心脏异常分类",
      "authors": [
        "Joao D. S. Marques",
        "Arlindo L. Oliveira"
      ],
      "abstract": "Electrocardiography (ECG) is an essential tool for diagnosing multiple\ncardiac anomalies: it provides valuable clinical insights, while being\naffordable, fast and available in many settings. However, in the current\nliterature, the role of ECG analysis is often unclear: many approaches either\nrely on additional imaging modalities, such as Computed Tomography Pulmonary\nAngiography (CTPA), which may not always be available, or do not effectively\ngeneralize across different classification problems. Furthermore, the\navailability of public ECG datasets is limited and, in practice, these datasets\ntend to be small, making it essential to optimize learning strategies. In this\nstudy, we investigate the performance of multiple neural network architectures\nin order to assess the impact of various approaches. Moreover, we check whether\nthese practices enhance model generalization when transfer learning is used to\ntranslate information learned in larger ECG datasets, such as PTB-XL and\nCPSC18, to a smaller, more challenging dataset for pulmonary embolism (PE)\ndetection. By leveraging transfer learning, we analyze the extent to which we\ncan improve learning efficiency and predictive performance on limited data.\nCode available at\nhttps://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers .",
      "tldr_zh": "该研究探讨了仅依靠心电图(ECG)进行心脏异常分类的可行性，通过比较多种深度学习架构的性能。研究利用迁移学习技术，将PTB-XL和CPSC18等大型ECG数据集的知识迁移到较小的肺栓塞(PE)检测数据集，显著提升了模型在有限数据下的预测性能。结果表明，优化后的深度学习策略能够在不依赖CTPA等其他成像模态的情况下，有效实现多种心脏异常的ECG分类。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.2"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08960v1",
      "published_date": "2025-03-11 23:37:18 UTC",
      "updated_date": "2025-03-11 23:37:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:12:50.200818"
    },
    {
      "arxiv_id": "2503.08950v1",
      "title": "FP3: A 3D Foundation Policy for Robotic Manipulation",
      "title_zh": "FP3：面向机器人操作的3D基础策略模型",
      "authors": [
        "Rujia Yang",
        "Geng Chen",
        "Chuan Wen",
        "Yang Gao"
      ],
      "abstract": "Following its success in natural language processing and computer vision,\nfoundation models that are pre-trained on large-scale multi-task datasets have\nalso shown great potential in robotics. However, most existing robot foundation\nmodels rely solely on 2D image observations, ignoring 3D geometric information,\nwhich is essential for robots to perceive and reason about the 3D world. In\nthis paper, we introduce FP3, a first large-scale 3D foundation policy model\nfor robotic manipulation. FP3 builds on a scalable diffusion transformer\narchitecture and is pre-trained on 60k trajectories with point cloud\nobservations. With the model design and diverse pre-training data, FP3 can be\nefficiently fine-tuned for downstream tasks while exhibiting strong\ngeneralization capabilities. Experiments on real robots demonstrate that with\nonly 80 demonstrations, FP3 is able to learn a new task with over 90% success\nrates in novel environments with unseen objects, significantly surpassing\nexisting robot foundation models.",
      "tldr_zh": "该研究提出了FP3，首个面向机器人操作的大规模3D基础策略模型，突破了现有机器人基础模型仅依赖2D图像的局限。基于扩散Transformer架构，该模型在6万条点云观测轨迹上进行预训练，能够高效适应下游任务并展现强大的泛化能力。实验表明，仅需80次演示，FP3就能在包含新物体的陌生环境中以超过90%的成功率学习新任务，性能显著超越现有模型。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://3d-foundation-policy.github.io",
      "pdf_url": "http://arxiv.org/pdf/2503.08950v1",
      "published_date": "2025-03-11 23:01:08 UTC",
      "updated_date": "2025-03-11 23:01:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:13:29.485140"
    },
    {
      "arxiv_id": "2503.08939v1",
      "title": "KAN-Mixers: a new deep learning architecture for image classification",
      "title_zh": "KAN-Mixers：一种用于图像分类的新型深度学习架构",
      "authors": [
        "Jorge Luiz dos Santos Canuto",
        "Linnyer Beatrys Ruiz Aylon",
        "Rodrigo Clemente Thom de Souza"
      ],
      "abstract": "Due to their effective performance, Convolutional Neural Network (CNN) and\nVision Transformer (ViT) architectures have become the standard for solving\ncomputer vision tasks. Such architectures require large data sets and rely on\nconvolution and self-attention operations. In 2021, MLP-Mixer emerged, an\narchitecture that relies only on Multilayer Perceptron (MLP) and achieves\nextremely competitive results when compared to CNNs and ViTs. Despite its good\nperformance in computer vision tasks, the MLP-Mixer architecture may not be\nsuitable for refined feature extraction in images. Recently, the\nKolmogorov-Arnold Network (KAN) was proposed as a promising alternative to MLP\nmodels. KANs promise to improve accuracy and interpretability when compared to\nMLPs. Therefore, the present work aims to design a new mixer-based\narchitecture, called KAN-Mixers, using KANs as main layers and evaluate its\nperformance, in terms of several performance metrics, in the image\nclassification task. As main results obtained, the KAN-Mixers model was\nsuperior to the MLP, MLP-Mixer and KAN models in the Fashion-MNIST and CIFAR-10\ndatasets, with 0.9030 and 0.6980 of average accuracy, respectively.",
      "tldr_zh": "该研究提出了一种新型深度学习架构KAN-Mixers，用于图像分类任务。该架构基于Kolmogorov-Arnold Networks（KAN）替代传统MLP层，旨在改进特征提取能力，同时提升模型准确性和可解释性。实验表明，KAN-Mixers在Fashion-MNIST和CIFAR-10数据集上的平均准确率分别达到0.9030和0.6980，性能优于MLP、MLP-Mixer和传统KAN模型。这一成果为计算机视觉任务提供了新的深度学习解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08939v1",
      "published_date": "2025-03-11 22:41:22 UTC",
      "updated_date": "2025-03-11 22:41:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:13:31.841855"
    },
    {
      "arxiv_id": "2503.08936v1",
      "title": "Simulator Ensembles for Trustworthy Autonomous Driving Testing",
      "title_zh": "模拟器集成用于可信自动驾驶测试",
      "authors": [
        "Lev Sorokin",
        "Matteo Biagiola",
        "Andrea Stocco"
      ],
      "abstract": "Scenario-based testing with driving simulators is extensively used to\nidentify failing conditions of automated driving assistance systems (ADAS) and\nreduce the amount of in-field road testing. However, existing studies have\nshown that repeated test execution in the same as well as in distinct\nsimulators can yield different outcomes, which can be attributed to sources of\nflakiness or different implementations of the physics, among other factors. In\nthis paper, we present MultiSim, a novel approach to multi-simulation ADAS\ntesting based on a search-based testing approach that leverages an ensemble of\nsimulators to identify failure-inducing, simulator-agnostic test scenarios.\nDuring the search, each scenario is evaluated jointly on multiple simulators.\nScenarios that produce consistent results across simulators are prioritized for\nfurther exploration, while those that fail on only a subset of simulators are\ngiven less priority, as they may reflect simulator-specific issues rather than\ngeneralizable failures. Our case study, which involves testing a deep neural\nnetwork-based ADAS on different pairs of three widely used simulators,\ndemonstrates that MultiSim outperforms single-simulator testing by achieving on\naverage a higher rate of simulator-agnostic failures by 51%. Compared to a\nstate-of-the-art multi-simulator approach that combines the outcome of\nindependent test generation campaigns obtained in different simulators,\nMultiSim identifies 54% more simulator-agnostic failing tests while showing a\ncomparable validity rate. An enhancement of MultiSim that leverages surrogate\nmodels to predict simulator disagreements and bypass executions does not only\nincrease the average number of valid failures but also improves efficiency in\nfinding the first valid failure.",
      "tldr_zh": "本研究提出MultiSim方法，通过构建模拟器集合(Simulator Ensembles)来提升自动驾驶系统(ADAS)测试的可信度。该方法采用基于搜索的测试策略，在多模拟器环境中并行评估测试场景，优先筛选能跨模拟器复现的故障场景（相比单模拟器测试发现通用故障率提升51%）。实验在三种主流模拟器上验证表明，MultiSim不仅比现有多模拟器方法多发现54%的通用故障，其增强版还通过代理模型预测模拟器分歧，进一步提高了故障发现效率和有效性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08936v1",
      "published_date": "2025-03-11 22:34:14 UTC",
      "updated_date": "2025-03-11 22:34:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:13:20.544708"
    },
    {
      "arxiv_id": "2503.08929v1",
      "title": "HessianForge: Scalable LiDAR reconstruction with Physics-Informed Neural Representation and Smoothness Energy Constraints",
      "title_zh": "HessianForge：基于物理信息神经表示与平滑能量约束的可扩展LiDAR重建",
      "authors": [
        "Hrishikesh Viswanath",
        "Md Ashiqur Rahman",
        "Chi Lin",
        "Damon Conover",
        "Aniket Bera"
      ],
      "abstract": "Accurate and efficient 3D mapping of large-scale outdoor environments from\nLiDAR measurements is a fundamental challenge in robotics, particularly towards\nensuring smooth and artifact-free surface reconstructions. Although the\nstate-of-the-art methods focus on memory-efficient neural representations for\nhigh-fidelity surface generation, they often fail to produce artifact-free\nmanifolds, with artifacts arising due to noisy and sparse inputs. To address\nthis issue, we frame surface mapping as a physics-informed energy optimization\nproblem, enforcing surface smoothness by optimizing an energy functional that\npenalizes sharp surface ridges. Specifically, we propose a deep learning based\napproach that learns the signed distance field (SDF) of the surface manifold\nfrom raw LiDAR point clouds using a physics-informed loss function that\noptimizes the $L_2$-Hessian energy of the surface. Our learning framework\nincludes a hierarchical octree based input feature encoding and a multi-scale\nneural network to iteratively refine the signed distance field at different\nscales of resolution. Lastly, we introduce a test-time refinement strategy to\ncorrect topological inconsistencies and edge distortions that can arise in the\ngenerated mesh. We propose a \\texttt{CUDA}-accelerated least-squares\noptimization that locally adjusts vertex positions to enforce\nfeature-preserving smoothing. We evaluate our approach on large-scale outdoor\ndatasets and demonstrate that our approach outperforms current state-of-the-art\nmethods in terms of improved accuracy and smoothness. Our code is available at\n\\href{https://github.com/HrishikeshVish/HessianForge/}{https://github.com/HrishikeshVish/HessianForge/}",
      "tldr_zh": "该研究提出了HessianForge，一种基于物理信息神经表示和平滑能量约束的可扩展LiDAR重建方法。该方法将表面映射转化为物理信息能量优化问题，通过优化惩罚尖锐表面脊的$L_2$-Hessian能量函数，从原始LiDAR点云学习有符号距离场(SDF)。研究采用分层八叉树特征编码和多尺度神经网络进行迭代优化，并引入基于CUDA加速的测试时细化策略修正拓扑不一致性。实验表明，该方法在大规模户外数据集上重建精度和平滑度均优于现有技术。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO",
        "eess.IV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08929v1",
      "published_date": "2025-03-11 22:18:51 UTC",
      "updated_date": "2025-03-11 22:18:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:13:27.977535"
    },
    {
      "arxiv_id": "2503.08919v1",
      "title": "Backtracking for Safety",
      "title_zh": "回溯安全机制",
      "authors": [
        "Bilgehan Sel",
        "Dingcheng Li",
        "Phillip Wallis",
        "Vaishakh Keshava",
        "Ming Jin",
        "Siddhartha Reddy Jonnalagadda"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, but ensuring their safety and alignment with human values\nremains crucial. Current safety alignment methods, such as supervised\nfine-tuning and reinforcement learning-based approaches, can exhibit\nvulnerabilities to adversarial attacks and often result in shallow safety\nalignment, primarily focusing on preventing harmful content in the initial\ntokens of the generated output. While methods like resetting can help recover\nfrom unsafe generations by discarding previous tokens and restarting the\ngeneration process, they are not well-suited for addressing nuanced safety\nviolations like toxicity that may arise within otherwise benign and lengthy\ngenerations. In this paper, we propose a novel backtracking method designed to\naddress these limitations. Our method allows the model to revert to a safer\ngeneration state, not necessarily at the beginning, when safety violations\noccur during generation. This approach enables targeted correction of\nproblematic segments without discarding the entire generated text, thereby\npreserving efficiency. We demonstrate that our method dramatically reduces\ntoxicity appearing through the generation process with minimal impact to\nefficiency.",
      "tldr_zh": "该研究提出了一种新型回溯方法(backtracking)，用于提升大语言模型(LLMs)生成内容的安全性。不同于现有方法仅关注初始标记或完全重置生成过程，该方法能在检测到安全违规时智能回溯至更安全的中间状态，针对性地修正问题片段。实验表明，该方法能显著降低生成过程中的毒性(toxicity)内容，同时保持较高的生成效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08919v1",
      "published_date": "2025-03-11 22:04:22 UTC",
      "updated_date": "2025-03-11 22:04:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:13:32.078658"
    },
    {
      "arxiv_id": "2503.08916v1",
      "title": "Robust Unsupervised Fault Diagnosis For High-Dimensional Nonlinear Noisy Data",
      "title_zh": "高维非线性噪声数据的鲁棒无监督故障诊断",
      "authors": [
        "Dandan Zhao",
        "Hongpeng Yin",
        "Jintang Bian",
        "Han Zhou"
      ],
      "abstract": "Traditional fault diagnosis methods struggle to handle fault data, with\ncomplex data characteristics such as high dimensions and large noise. Deep\nlearning is a promising solution, which typically works well only when labeled\nfault data are available. To address these problems, a robust unsupervised\nfault diagnosis using machine learning is proposed in this paper. First, a\nspecial dimension reduction method for the high-dimensional fault data is\ndesigned. Second, the extracted features are enhanced by incorporating\nnonlinear information through the learning of a graph structure. Third, to\nalleviate the problem of reduced fault-diagnosis accuracy attributed to noise\nand outliers, $l_{2,1}$-norm and typicality-aware constraints are introduced\nfrom the perspective of model optimization, respectively. Finally, this paper\nprovides comprehensive theoretical and experimental evidence supporting the\neffectiveness and robustness of the proposed method. The experiments on both\nthe benchmark Tennessee-Eastman process and a real hot-steel milling process\nshow that the proposed method exhibits better robustness compared to other\nmethods, maintaining high diagnostic accuracy even in the presence of outliers\nor noise.",
      "tldr_zh": "该论文提出了一种针对高维非线性噪声数据的鲁棒无监督故障诊断方法。通过设计专门的高维数据降维技术，并利用图结构学习增强非线性特征提取，有效解决了传统方法在噪声和异常值干扰下的性能下降问题。创新性地引入$l_{2,1}$-范数和典型性感知约束进行模型优化，在田纳西-伊斯曼基准测试和实际热轧钢铣削工艺中验证了方法的优越性，相比现有方法展现出更强的鲁棒性，在噪声环境下仍保持高诊断准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08916v1",
      "published_date": "2025-03-11 21:55:46 UTC",
      "updated_date": "2025-03-11 21:55:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:13:56.718504"
    },
    {
      "arxiv_id": "2503.08908v1",
      "title": "Interpreting the Repeated Token Phenomenon in Large Language Models",
      "title_zh": "解析大语言模型中的重复令牌现象",
      "authors": [
        "Itay Yona",
        "Ilia Shumailov",
        "Jamie Hayes",
        "Federico Barbero",
        "Yossi Gandelsman"
      ],
      "abstract": "Large Language Models (LLMs), despite their impressive capabilities, often\nfail to accurately repeat a single word when prompted to, and instead output\nunrelated text. This unexplained failure mode represents a vulnerability,\nallowing even end-users to diverge models away from their intended behavior. We\naim to explain the causes for this phenomenon and link it to the concept of\n``attention sinks'', an emergent LLM behavior crucial for fluency, in which the\ninitial token receives disproportionately high attention scores. Our\ninvestigation identifies the neural circuit responsible for attention sinks and\nshows how long repetitions disrupt this circuit. We extend this finding to\nother non-repeating sequences that exhibit similar circuit disruptions. To\naddress this, we propose a targeted patch that effectively resolves the issue\nwithout negatively impacting the model's overall performance. This study\nprovides a mechanistic explanation for an LLM vulnerability, demonstrating how\ninterpretability can diagnose and address issues, and offering insights that\npave the way for more secure and reliable models.",
      "tldr_zh": "这篇论文研究了大型语言模型(LLMs)中出现的重复令牌现象——当要求模型重复单个单词时，它反而会输出无关内容。研究发现这一现象与\"注意力汇聚点\"(attention sinks)机制有关，即模型会过度关注初始令牌以保持流畅性。作者不仅识别了导致该现象的神经回路，还开发了一种针对性修复方案，能有效解决问题而不影响模型整体性能。这项工作为LLM漏洞提供了机制性解释，展示了可解释性研究如何诊断和解决问题，为开发更安全可靠的模型奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08908v1",
      "published_date": "2025-03-11 21:40:58 UTC",
      "updated_date": "2025-03-11 21:40:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:14:04.215232"
    },
    {
      "arxiv_id": "2503.08906v1",
      "title": "Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation",
      "title_zh": "Prompt-OT：视觉语言模型适应中知识保持的最优输运正则化范式",
      "authors": [
        "Xiwen Chen",
        "Wenhui Zhu",
        "Peijie Qiu",
        "Hao Wang",
        "Huayu Li",
        "Haiyu Wu",
        "Aristeidis Sotiras",
        "Yalin Wang",
        "Abolfazl Razi"
      ],
      "abstract": "Vision-language models (VLMs) such as CLIP demonstrate strong performance but\nstruggle when adapted to downstream tasks. Prompt learning has emerged as an\nefficient and effective strategy to adapt VLMs while preserving their\npre-trained knowledge. However, existing methods still lead to overfitting and\ndegrade zero-shot generalization. To address this challenge, we propose an\noptimal transport (OT)-guided prompt learning framework that mitigates\nforgetting by preserving the structural consistency of feature distributions\nbetween pre-trained and fine-tuned models. Unlike conventional point-wise\nconstraints, OT naturally captures cross-instance relationships and expands the\nfeasible parameter space for prompt tuning, allowing a better trade-off between\nadaptation and generalization. Our approach enforces joint constraints on both\nvision and text representations, ensuring a holistic feature alignment.\nExtensive experiments on benchmark datasets demonstrate that our simple yet\neffective method can outperform existing prompt learning strategies in\nbase-to-novel generalization, cross-dataset evaluation, and domain\ngeneralization without additional augmentation or ensemble techniques. The code\nis available at https://github.com/ChongQingNoSubway/Prompt-OT",
      "tldr_zh": "该研究提出了一种基于最优传输(Optimal Transport, OT)的提示学习框架Prompt-OT，用于解决视觉语言模型(VLMs)在下游任务适应过程中出现的知识遗忘问题。该方法通过保持预训练模型和微调模型之间特征分布的结构一致性，缓解过拟合并提升零样本泛化能力。与传统的点对点约束不同，OT能够自然捕捉跨实例关系，扩大提示调参的可行空间，实现适应与泛化的更好平衡。实验表明，Prompt-OT在基础到新类泛化、跨数据集评估和领域泛化等任务上优于现有提示学习策略，且无需额外数据增强或集成技术。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08906v1",
      "published_date": "2025-03-11 21:38:34 UTC",
      "updated_date": "2025-03-11 21:38:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:13:56.456517"
    },
    {
      "arxiv_id": "2503.08883v2",
      "title": "Imitation Learning of Correlated Policies in Stackelberg Games",
      "title_zh": "斯塔克尔伯格博弈中关联策略的模仿学习",
      "authors": [
        "Kuang-Da Wang",
        "Ping-Chun Hsieh",
        "Wen-Chih Peng"
      ],
      "abstract": "Stackelberg games, widely applied in domains like economics and security,\ninvolve asymmetric interactions where a leader's strategy drives follower\nresponses. Accurately modeling these dynamics allows domain experts to optimize\nstrategies in interactive scenarios, such as turn-based sports like badminton.\nIn multi-agent systems, agent behaviors are interdependent, and traditional\nMulti-Agent Imitation Learning (MAIL) methods often fail to capture these\ncomplex interactions. Correlated policies, which account for opponents'\nstrategies, are essential for accurately modeling such dynamics. However, even\nmethods designed for learning correlated policies, like CoDAIL, struggle in\nStackelberg games due to their asymmetric decision-making, where leaders and\nfollowers cannot simultaneously account for each other's actions, often leading\nto non-correlated policies. Furthermore, existing MAIL methods that match\noccupancy measures or use adversarial techniques like GAIL or Inverse RL face\nscalability challenges, particularly in high-dimensional environments, and\nsuffer from unstable training. To address these challenges, we propose a\ncorrelated policy occupancy measure specifically designed for Stackelberg games\nand introduce the Latent Stackelberg Differential Network (LSDN) to match it.\nLSDN models two-agent interactions as shared latent state trajectories and uses\nmulti-output Geometric Brownian Motion (MO-GBM) to effectively capture joint\npolicies. By leveraging MO-GBM, LSDN disentangles environmental influences from\nagent-driven transitions in latent space, enabling the simultaneous learning of\ninterdependent policies. This design eliminates the need for adversarial\ntraining and simplifies the learning process. Extensive experiments on\nIterative Matrix Games and multi-agent particle environments demonstrate that\nLSDN can better reproduce complex interaction dynamics than existing MAIL\nmethods.",
      "tldr_zh": "该研究提出了一种针对Stackelberg博弈的关联策略学习方法，解决了传统多智能体模仿学习(MAIL)方法在不对称决策场景中的局限性。研究者设计了关联策略的占用度量，并引入了Latent Stackelberg Differential Network (LSDN)，利用多输出几何布朗运动(MO-GBM)在潜在空间中捕捉联合策略，从而同时学习相互依赖的领导者与跟随者策略。实验表明，LSDN在迭代矩阵游戏和多智能体粒子环境中能够更好地复现复杂的交互动态，且无需对抗训练，简化了学习过程。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Code will be released at this GitHub link:\n  https://github.com/NYCU-RL-Bandits-Lab/LSDN",
      "pdf_url": "http://arxiv.org/pdf/2503.08883v2",
      "published_date": "2025-03-11 20:52:56 UTC",
      "updated_date": "2025-03-16 17:42:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:14:49.592688"
    },
    {
      "arxiv_id": "2503.08879v1",
      "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference",
      "title_zh": "大语言模型知晓何时舍弃：基于自注意力引导的KV缓存淘汰机制实现高效长上下文推理",
      "authors": [
        "Guangtao Wang",
        "Shubhangi Upasani",
        "Chen Wu",
        "Darshan Gandhi",
        "Jonathan Li",
        "Changran Hu",
        "Bo Li",
        "Urmish Thakker"
      ],
      "abstract": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
      "tldr_zh": "本文提出SAGE-KV（Self-Attention Guided Eviction），一种基于自注意力机制的动态KV缓存淘汰方法，用于解决大语言模型（LLMs）长上下文推理中的内存和延迟瓶颈。研究发现LLMs在预填充阶段后能隐式识别可丢弃的token和注意力头，因此该方法通过token和head级别的top-k选择压缩KV缓存。实验表明，在LongBench基准测试和多个长上下文LLM上，SAGE-KV在保持与全注意力相当精度的同时，内存效率比StreamLLM提升4倍、比Quest提升2倍。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08879v1",
      "published_date": "2025-03-11 20:45:02 UTC",
      "updated_date": "2025-03-11 20:45:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:14:32.483716"
    },
    {
      "arxiv_id": "2503.08872v1",
      "title": "Meta-Reinforcement Learning with Discrete World Models for Adaptive Load Balancing",
      "title_zh": "基于离散世界模型的元强化学习自适应负载均衡方法",
      "authors": [
        "Cameron Redovian"
      ],
      "abstract": "We integrate a meta-reinforcement learning algorithm with the DreamerV3\narchitecture to improve load balancing in operating systems. This approach\nenables rapid adaptation to dynamic workloads with minimal retraining,\noutperforming the Advantage Actor-Critic (A2C) algorithm in standard and\nadaptive trials. It demonstrates robust resilience to catastrophic forgetting,\nmaintaining high performance under varying workload distributions and sizes.\nThese findings have important implications for optimizing resource management\nand performance in modern operating systems. By addressing the challenges posed\nby dynamic and heterogeneous workloads, our approach advances the adaptability\nand efficiency of reinforcement learning in real-world system management tasks.",
      "tldr_zh": "本研究将元强化学习(Meta-Reinforcement Learning)与DreamerV3架构结合，提出了一种用于操作系统负载均衡的自适应方法。该方法能够快速适应动态工作负载，且无需大量重新训练，在标准和自适应测试中均优于Advantage Actor-Critic (A2C)算法。实验表明，该方法对灾难性遗忘具有强鲁棒性，在不同工作负载分布和规模下均保持了高性能。这一成果为优化现代操作系统的资源管理和性能提供了重要思路，提升了强化学习在真实系统管理任务中的适应性和效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.OS",
        "I.2.6; I.2.8; D.4.1"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 1 figure, to be published in ACMSE 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08872v1",
      "published_date": "2025-03-11 20:36:49 UTC",
      "updated_date": "2025-03-11 20:36:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:14:36.635596"
    },
    {
      "arxiv_id": "2503.08867v1",
      "title": "Zero-Shot Action Generalization with Limited Observations",
      "title_zh": "零样本动作泛化与有限观测",
      "authors": [
        "Abdullah Alchihabi",
        "Hanping Zhang",
        "Yuhong Guo"
      ],
      "abstract": "Reinforcement Learning (RL) has demonstrated remarkable success in solving\nsequential decision-making problems. However, in real-world scenarios, RL\nagents often struggle to generalize when faced with unseen actions that were\nnot encountered during training. Some previous works on zero-shot action\ngeneralization rely on large datasets of action observations to capture the\nbehaviors of new actions, making them impractical for real-world applications.\nIn this paper, we introduce a novel zero-shot framework, Action Generalization\nfrom Limited Observations (AGLO). Our framework has two main components: an\naction representation learning module and a policy learning module. The action\nrepresentation learning module extracts discriminative embeddings of actions\nfrom limited observations, while the policy learning module leverages the\nlearned action representations, along with augmented synthetic action\nrepresentations, to learn a policy capable of handling tasks with unseen\nactions. The experimental results demonstrate that our framework significantly\noutperforms state-of-the-art methods for zero-shot action generalization across\nmultiple benchmark tasks, showcasing its effectiveness in generalizing to new\nactions with minimal action observations.",
      "tldr_zh": "该研究提出了一种名为AGLO的零样本动作泛化框架，旨在解决强化学习（RL）智能体在面对训练中未见动作时难以泛化的问题。AGLO框架包含两个核心模块：动作表示学习模块从有限的动作观察中提取判别性嵌入，而策略学习模块则利用这些学习到的动作表示以及增强的合成动作表示，学习能够处理未见动作的策略。实验结果表明，AGLO在多个基准任务上显著优于现有的零样本动作泛化方法，展示了其在仅需少量动作观察的情况下有效泛化到新动作的能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "AISTATS 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08867v1",
      "published_date": "2025-03-11 20:14:25 UTC",
      "updated_date": "2025-03-11 20:14:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:14:42.075151"
    },
    {
      "arxiv_id": "2503.16499v1",
      "title": "Stakeholder Perspectives on Whether and How Social Robots Can Support Mediation and Advocacy for Higher Education Students with Disabilities",
      "title_zh": "利益相关者视角：社交机器人是否及如何为高等教育残障学生提供调解与倡导支持",
      "authors": [
        "Alva Markelius",
        "Julie Bailey",
        "Jenny L. Gibson",
        "Hatice Gunes"
      ],
      "abstract": "This paper presents an iterative, participatory, empirical study that\nexamines the potential of using artificial intelligence, such as social robots\nand large language models, to support mediation and advocacy for students with\ndisabilities in higher education. Drawing on qualitative data from interviews\nand focus groups conducted with various stakeholders, including disabled\nstudents, disabled student representatives, and disability practitioners at the\nUniversity of Cambridge, this study reports findings relating to understanding\nthe problem space, ideating robotic support and participatory co-design of\nadvocacy support robots. The findings highlight the potential of these\ntechnologies in providing signposting and acting as a sounding board or study\ncompanion, while also addressing limitations in empathic understanding, trust,\nequity, and accessibility. We discuss ethical considerations, including\nintersectional biases, the double empathy problem, and the implications of\ndeploying social robots in contexts shaped by structural inequalities. Finally,\nwe offer a set of recommendations and suggestions for future research,\nrethinking the notion of corrective technological interventions to tools that\nempower and amplify self-advocacy.",
      "tldr_zh": "这项研究通过参与式方法探讨了社交机器人(Social Robots)和大型语言模型(LLMs)在高等教育残障学生权益倡导中的潜力。基于剑桥大学残障学生、代表及从业者的访谈数据，研究发现这些技术可作为\"学习伙伴\"提供引导支持，但也存在共情理解、信任和公平性等局限。研究特别探讨了结构性不平等环境下的伦理问题，包括交叉偏见和双重共情问题，并提出应将技术定位为增强自我倡导(self-advocacy)的工具而非矫正干预。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "This is a pre-print",
      "pdf_url": "http://arxiv.org/pdf/2503.16499v1",
      "published_date": "2025-03-11 19:57:11 UTC",
      "updated_date": "2025-03-11 19:57:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:15:43.255230"
    },
    {
      "arxiv_id": "2503.08823v2",
      "title": "ResBench: Benchmarking LLM-Generated FPGA Designs with Resource Awareness",
      "title_zh": "ResBench：基于资源感知的LLM生成FPGA设计基准测试",
      "authors": [
        "Ce Guo",
        "Tong Zhao"
      ],
      "abstract": "Field-Programmable Gate Arrays (FPGAs) are widely used in modern hardware\ndesign, yet writing Hardware Description Language (HDL) code for FPGA\nimplementation remains a complex and time-consuming task. Large Language Models\n(LLMs) have emerged as a promising tool for HDL generation, but existing\nbenchmarks for LLM-based code generation primarily focus on functional\ncorrectness while overlooking hardware resource usage. Furthermore, current\nbenchmarks offer limited diversity and do not fully represent the wide range of\nreal-world FPGA applications. To address these shortcomings, we introduce\nResBench, the first resource-focused benchmark explicitly designed to\ndistinguish between resource-optimized and inefficient LLM-generated HDL code.\nResBench consists of 56 problems across 12 categories, covering applications\nfrom finite state machines to financial computing. Our open-source evaluation\nframework automatically tests LLMs by generating Verilog code, verifying\ncorrectness, and measuring resource usage. The experiments, which primarily\nanalyze Lookup Table (LUT) usage, reveal significant differences among LLMs,\ndemonstrating ResBench's capability to identify models that generate more\nresource-optimized FPGA designs.",
      "tldr_zh": "该研究提出首个面向FPGA设计资源优化的基准测试框架ResBench，用于评估大语言模型(LLM)生成的硬件描述语言(HDL)代码。该基准包含12类56项任务，涵盖从有限状态机到金融计算等实际应用场景，通过自动验证功能正确性和测量资源使用(特别是查找表LUT)来评估模型性能。实验表明不同LLM生成的FPGA设计在资源利用率上存在显著差异，ResBench能有效识别出生成更优化硬件设计的模型。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CL",
        "cs.ET",
        "cs.LG",
        "I.2.2"
      ],
      "primary_category": "cs.AR",
      "comment": "to be published in International Symposium on Highly Efficient\n  Accelerators and Reconfigurable Technologies 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08823v2",
      "published_date": "2025-03-11 18:54:17 UTC",
      "updated_date": "2025-03-21 23:27:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:14:59.306856"
    },
    {
      "arxiv_id": "2503.10684v1",
      "title": "Open-World Skill Discovery from Unsegmented Demonstrations",
      "title_zh": "从非分段演示中实现开放世界技能发现",
      "authors": [
        "Jingwen Deng",
        "Zihao Wang",
        "Shaofei Cai",
        "Anji Liu",
        "Yitao Liang"
      ],
      "abstract": "Learning skills in open-world environments is essential for developing agents\ncapable of handling a variety of tasks by combining basic skills. Online\ndemonstration videos are typically long but unsegmented, making them difficult\nto segment and label with skill identifiers. Unlike existing methods that rely\non sequence sampling or human labeling, we have developed a self-supervised\nlearning-based approach to segment these long videos into a series of\nsemantic-aware and skill-consistent segments. Drawing inspiration from human\ncognitive event segmentation theory, we introduce Skill Boundary Detection\n(SBD), an annotation-free temporal video segmentation algorithm. SBD detects\nskill boundaries in a video by leveraging prediction errors from a pretrained\nunconditional action-prediction model. This approach is based on the assumption\nthat a significant increase in prediction error indicates a shift in the skill\nbeing executed. We evaluated our method in Minecraft, a rich open-world\nsimulator with extensive gameplay videos available online. Our SBD-generated\nsegments improved the average performance of conditioned policies by 63.7% and\n52.1% on short-term atomic skill tasks, and their corresponding hierarchical\nagents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the\ndiverse YouTube videos to train instruction-following agents. The project page\ncan be found in https://craftjarvis.github.io/SkillDiscovery.",
      "tldr_zh": "该研究提出了一种基于自监督学习的开放世界技能发现方法，能够从未分割的长视频演示中自动识别技能边界。受人类认知事件分割理论启发，该方法通过预训练的动作预测模型的误差峰值检测技能转换点，无需人工标注。在Minecraft环境中的实验表明，该方法生成的技能片段使条件策略在短期原子任务上的性能提升63.7%，分层智能体在长时程任务上的性能提升20.8%。该技术可利用YouTube等开放视频资源训练指令跟随型智能体。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10684v1",
      "published_date": "2025-03-11 18:51:40 UTC",
      "updated_date": "2025-03-11 18:51:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:15:04.781467"
    },
    {
      "arxiv_id": "2503.08815v1",
      "title": "Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations",
      "title_zh": "Cross-Examiner：评估大语言模型生成解释的一致性",
      "authors": [
        "Danielle Villa",
        "Maria Chang",
        "Keerthiram Murugesan",
        "Rosario Uceda-Sosa",
        "Karthikeyan Natesan Ramamurthy"
      ],
      "abstract": "Large Language Models (LLMs) are often asked to explain their outputs to\nenhance accuracy and transparency. However, evidence suggests that these\nexplanations can misrepresent the models' true reasoning processes. One\neffective way to identify inaccuracies or omissions in these explanations is\nthrough consistency checking, which typically involves asking follow-up\nquestions. This paper introduces, cross-examiner, a new method for generating\nfollow-up questions based on a model's explanation of an initial question. Our\nmethod combines symbolic information extraction with language model-driven\nquestion generation, resulting in better follow-up questions than those\nproduced by LLMs alone. Additionally, this approach is more flexible than other\nmethods and can generate a wider variety of follow-up questions.",
      "tldr_zh": "该研究提出了Cross-Examiner方法，用于评估大语言模型(LLM)生成解释的一致性。该方法结合符号化信息提取和语言模型驱动的提问生成技术，能比单纯使用LLM产生更优质的后续追问问题。实验表明，这种混合方法不仅灵活性更高，还能生成更多样化的追问问题，从而更有效地检测LLM解释中的不准确或遗漏之处。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08815v1",
      "published_date": "2025-03-11 18:50:43 UTC",
      "updated_date": "2025-03-11 18:50:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:15:16.309446"
    },
    {
      "arxiv_id": "2503.08796v1",
      "title": "Robust Multi-Objective Controlled Decoding of Large Language Models",
      "title_zh": "鲁棒多目标控制解码大型语言模型",
      "authors": [
        "Seongho Son",
        "William Bankes",
        "Sangwoong Yoon",
        "Shyam Sundhar Ramesh",
        "Xiaohang Tang",
        "Ilija Bogunovic"
      ],
      "abstract": "Test-time alignment of Large Language Models (LLMs) to human preferences\noffers a flexible way to generate responses aligned to diverse objectives\nwithout extensive retraining of LLMs. Existing methods achieve alignment to\nmultiple objectives simultaneously (e.g., instruction-following, helpfulness,\nconciseness) by optimizing their corresponding reward functions. However, they\noften rely on predefined weights or optimize for averages, sacrificing one\nobjective for another and leading to unbalanced outcomes. To address this, we\nintroduce Robust Multi-Objective Decoding (RMOD), a novel inference-time\nalgorithm that optimizes for improving worst-case rewards. RMOD formalizes the\nrobust decoding problem as a maximin two-player game between reward weights and\nthe sampling policy, solving for the Nash equilibrium. We show that the game\nreduces to a convex optimization problem to find the worst-case weights, while\nthe best response policy can be computed analytically. We also introduce a\npractical RMOD variant designed for efficient decoding with contemporary LLMs,\nincurring minimal computational overhead compared to non-robust Multi-Objective\nDecoding (MOD) methods. Our experimental results showcase the effectiveness of\nRMOD in generating responses equitably aligned with diverse objectives,\noutperforming baselines up to 20%.",
      "tldr_zh": "该研究提出了鲁棒多目标解码（RMOD），一种新型推理时算法，用于优化大型语言模型（LLMs）的多目标对齐问题。该方法将鲁棒解码问题建模为奖励权重和采样策略之间的极大极小博弈，通过求解纳什均衡来平衡多个目标（如指令遵循、帮助性和简洁性），避免传统方法因预设权重导致的失衡问题。实验表明，RMOD能生成更公平满足多目标的响应，性能比基线方法提升高达20%，且计算开销与非鲁棒方法相当。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08796v1",
      "published_date": "2025-03-11 18:15:26 UTC",
      "updated_date": "2025-03-11 18:15:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:15:25.547190"
    },
    {
      "arxiv_id": "2503.08786v1",
      "title": "Combining Local Symmetry Exploitation and Reinforcement Learning for Optimised Probabilistic Inference -- A Work In Progress",
      "title_zh": "结合局部对称性利用与强化学习的优化概率推理——一项进展中的工作",
      "authors": [
        "Sagad Hamid",
        "Tanya Braun"
      ],
      "abstract": "Efficient probabilistic inference by variable elimination in graphical models\nrequires an optimal elimination order. However, finding an optimal order is a\nchallenging combinatorial optimisation problem for models with a large number\nof random variables. Most recently, a reinforcement learning approach has been\nproposed to find efficient contraction orders in tensor networks. Due to the\nduality between graphical models and tensor networks, we adapt this approach to\nprobabilistic inference in graphical models. Furthermore, we incorporate\nstructure exploitation into the process of finding an optimal order. Currently,\nthe agent's cost function is formulated in terms of intermediate result sizes\nwhich are exponential in the number of indices (i.e., random variables). We\nshow that leveraging specific structures during inference allows for\nintroducing compact encodings of intermediate results which can be\nsignificantly smaller. By considering the compact encoding sizes for the cost\nfunction instead, we enable the agent to explore more efficient contraction\norders. The structure we consider in this work is the presence of local\nsymmetries (i.e., symmetries within a model's factors).",
      "tldr_zh": "该研究提出了一种结合局部对称性利用和强化学习的方法，用于优化概率图模型中的变量消元顺序。通过将张量网络中的强化学习排序方法迁移到概率图模型，并引入局部对称性结构进行中间结果的紧凑编码，显著降低了计算复杂度。该方法通过重构智能体的成本函数，使其基于紧凑编码规模而非指数级增长的中间结果规模，从而能够探索更高效的消元顺序。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Contributed to: Sixth Data Science Meets Optimisation (DSO) Workshop\n  at IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.08786v1",
      "published_date": "2025-03-11 18:00:23 UTC",
      "updated_date": "2025-03-11 18:00:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:15:32.227909"
    },
    {
      "arxiv_id": "2503.08684v1",
      "title": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents",
      "title_zh": "困惑度陷阱：基于预训练语言模型的检索器高估低困惑度文档",
      "authors": [
        "Haoyu Wang",
        "Sunhao Dai",
        "Haiyuan Zhao",
        "Liang Pang",
        "Xiao Zhang",
        "Gang Wang",
        "Zhenhua Dong",
        "Jun Xu",
        "Ji-Rong Wen"
      ],
      "abstract": "Previous studies have found that PLM-based retrieval models exhibit a\npreference for LLM-generated content, assigning higher relevance scores to\nthese documents even when their semantic quality is comparable to human-written\nones. This phenomenon, known as source bias, threatens the sustainable\ndevelopment of the information access ecosystem. However, the underlying causes\nof source bias remain unexplored. In this paper, we explain the process of\ninformation retrieval with a causal graph and discover that PLM-based\nretrievers learn perplexity features for relevance estimation, causing source\nbias by ranking the documents with low perplexity higher. Theoretical analysis\nfurther reveals that the phenomenon stems from the positive correlation between\nthe gradients of the loss functions in language modeling task and retrieval\ntask. Based on the analysis, a causal-inspired inference-time debiasing method\nis proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses\nthe bias effect of the perplexity and then separates the bias effect from the\noverall estimated relevance score. Experimental results across three domains\ndemonstrate the superior debiasing effectiveness of CDC, emphasizing the\nvalidity of our proposed explanatory framework. Source codes are available at\nhttps://github.com/WhyDwelledOnAi/Perplexity-Trap.",
      "tldr_zh": "研究发现基于预训练语言模型（PLM）的检索系统存在\"困惑度陷阱\"现象：检索模型会过度偏好低困惑度（perplexity）文档，导致对LLM生成内容产生系统性高估（source bias）。通过因果图分析，作者揭示了该现象源于语言建模任务与检索任务梯度间的正相关性，并提出因果诊断校正方法（CDC），通过分离困惑度偏差效应实现去偏。实验表明CDC在三个领域均能有效消除偏差，为构建公平的信息检索系统提供了新思路。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08684v1",
      "published_date": "2025-03-11 17:59:00 UTC",
      "updated_date": "2025-03-11 17:59:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:16:21.981501"
    },
    {
      "arxiv_id": "2503.08683v1",
      "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving",
      "title_zh": "CoLMDriver：基于大语言模型的协商助力协作式自动驾驶",
      "authors": [
        "Changxing Liu",
        "Genjia Liu",
        "Zijun Wang",
        "Jinchang Yang",
        "Siheng Chen"
      ],
      "abstract": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https://github.com/cxliu0314/CoLMDriver.",
      "tldr_zh": "该研究提出了CoLMDriver，首个基于大语言模型(LLM)的全流程协同自动驾驶系统。该系统通过并行驾驶架构实现两大创新：1）采用actor-critic范式的LLM协商模块，能基于车辆历史决策持续优化协作策略；2）意图引导的航点生成器，将协商结果转化为可执行路径。研究团队同时开发了InterDrive仿真基准（包含10种CARLA复杂交互场景），实验显示CoLMDriver在车对车(V2V)协同场景中成功率提升11%，有效解决了传统方法泛化性不足和LLM空间规划困难的问题。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08683v1",
      "published_date": "2025-03-11 17:58:42 UTC",
      "updated_date": "2025-03-11 17:58:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:16:07.252326"
    },
    {
      "arxiv_id": "2503.08764v1",
      "title": "Towards Interpretable Protein Structure Prediction with Sparse Autoencoders",
      "title_zh": "迈向可解释的蛋白质结构预测：稀疏自编码器的应用",
      "authors": [
        "Nithin Parsan",
        "David J. Yang",
        "John J. Yang"
      ],
      "abstract": "Protein language models have revolutionized structure prediction, but their\nnonlinear nature obscures how sequence representations inform structure\nprediction. While sparse autoencoders (SAEs) offer a path to interpretability\nhere by learning linear representations in high-dimensional space, their\napplication has been limited to smaller protein language models unable to\nperform structure prediction. In this work, we make two key advances: (1) we\nscale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic\ninterpretability of protein structure prediction for the first time, and (2) we\nadapt Matryoshka SAEs for protein language models, which learn hierarchically\norganized features by forcing nested groups of latents to reconstruct inputs\nindependently. We demonstrate that our Matryoshka SAEs achieve comparable or\nbetter performance than standard architectures. Through comprehensive\nevaluations, we show that SAEs trained on ESM2-3B significantly outperform\nthose trained on smaller models for both biological concept discovery and\ncontact map prediction. Finally, we present an initial case study demonstrating\nhow our approach enables targeted steering of ESMFold predictions, increasing\nstructure solvent accessibility while fixing the input sequence. To facilitate\nfurther investigation by the broader community, we open-source our code,\ndataset, pretrained models https://github.com/johnyang101/reticular-sae , and\nvisualizer https://sae.reticular.ai .",
      "tldr_zh": "本研究提出了一种基于稀疏自编码器(SAEs)的可解释蛋白质结构预测方法，首次将SAEs扩展至ESM2-3B模型（ESMFold的基础模型），实现了蛋白质结构预测的机制解释。通过引入Matryoshka SAEs，模型能够学习层次化特征，并在生物概念发现和接触图预测任务中显著优于基于较小模型的SAEs。研究还展示了如何利用该方法定向调控ESMFold预测，提高结构溶剂可及性。相关代码、数据集和预训练模型已开源。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Published at the GEMBio ICLR 2025 Workshop",
      "pdf_url": "http://arxiv.org/pdf/2503.08764v1",
      "published_date": "2025-03-11 17:57:29 UTC",
      "updated_date": "2025-03-11 17:57:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:16:22.433055"
    },
    {
      "arxiv_id": "2503.08679v3",
      "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
      "title_zh": "链式思维推理在现实场景中并非总是可信",
      "authors": [
        "Iván Arcuschin",
        "Jett Janiak",
        "Robert Krzyzanowski",
        "Senthooran Rajamanoharan",
        "Neel Nanda",
        "Arthur Conmy"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal non-negligible rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and\nChatGPT-4o (7.0%) all answer a notable proportion of question pairs\nunfaithfully. Specifically, we find that models rationalize their implicit\nbiases in answers to binary questions (\"implicit post-hoc rationalization\").\nFor example, when separately presented with the questions \"Is X bigger than Y?\"\nand \"Is Y bigger than X?\", models sometimes produce superficially coherent\narguments to justify answering Yes to both questions or No to both questions,\ndespite such responses being logically contradictory. We also investigate\nrestoration errors (Dziri et al., 2023), where models make and then silently\ncorrect errors in their reasoning, and unfaithful shortcuts, where models use\nclearly illogical reasoning to simplify solving problems in Putnam questions (a\nhard benchmark). Our findings raise challenges for AI safety work that relies\non monitoring CoT to detect undesired behavior.",
      "tldr_zh": "这篇论文揭示了链式思维推理(CoT)在实际应用中并不总是可靠的问题。研究发现，前沿AI模型(Sonnet 3.7、DeepSeek R1和ChatGPT-4o)在无人工干预的自然场景下仍存在显著比例(5.3%-16.3%)的不忠实推理现象。具体表现为三种问题：1)对二元问题的\"隐性事后合理化\"矛盾回答；2)推理过程中的\"修复错误\"；3)解决Putnam难题时使用明显不合逻辑的\"不忠实捷径\"。这些发现对依赖CoT监控的AI安全研究提出了重要挑战。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),\n  10 main paper pages, 39 appendix pages",
      "pdf_url": "http://arxiv.org/pdf/2503.08679v3",
      "published_date": "2025-03-11 17:56:30 UTC",
      "updated_date": "2025-03-19 19:20:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:16:47.482493"
    },
    {
      "arxiv_id": "2503.08678v1",
      "title": "GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D Garment Reconstruction and Editing",
      "title_zh": "GarmentCrafter：单视图3D服装重建与编辑的渐进式新视角合成",
      "authors": [
        "Yuanhao Wang",
        "Cheng Zhang",
        "Gonçalo Frazão",
        "Jinlong Yang",
        "Alexandru-Eugen Ichim",
        "Thabo Beeler",
        "Fernando De la Torre"
      ],
      "abstract": "We introduce GarmentCrafter, a new approach that enables non-professional\nusers to create and modify 3D garments from a single-view image. While recent\nadvances in image generation have facilitated 2D garment design, creating and\nediting 3D garments remains challenging for non-professional users. Existing\nmethods for single-view 3D reconstruction often rely on pre-trained generative\nmodels to synthesize novel views conditioning on the reference image and camera\npose, yet they lack cross-view consistency, failing to capture the internal\nrelationships across different views. In this paper, we tackle this challenge\nthrough progressive depth prediction and image warping to approximate novel\nviews. Subsequently, we train a multi-view diffusion model to complete occluded\nand unknown clothing regions, informed by the evolving camera pose. By jointly\ninferring RGB and depth, GarmentCrafter enforces inter-view coherence and\nreconstructs precise geometries and fine details. Extensive experiments\ndemonstrate that our method achieves superior visual fidelity and inter-view\ncoherence compared to state-of-the-art single-view 3D garment reconstruction\nmethods.",
      "tldr_zh": "本文提出了GarmentCrafter，一种从单视角图像实现3D服装重建与编辑的新方法。该方法通过渐进式深度预测和图像变形生成新视角，并结合多视角扩散模型补全遮挡和未知区域，从而增强跨视角一致性。通过联合推断RGB和深度信息，GarmentCrafter能够重建精确的几何结构和细节。实验表明，该方法在视觉保真度和跨视角一致性上优于现有的单视角3D服装重建技术。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Project Page: https://humansensinglab.github.io/garment-crafter/",
      "pdf_url": "http://arxiv.org/pdf/2503.08678v1",
      "published_date": "2025-03-11 17:56:03 UTC",
      "updated_date": "2025-03-11 17:56:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:16:39.089243"
    },
    {
      "arxiv_id": "2503.08669v1",
      "title": "AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence",
      "title_zh": "AgentOrca：评估语言智能体在操作常规与约束遵循上的双系统框架",
      "authors": [
        "Zekun Li",
        "Shinda Huang",
        "Jiangtian Wang",
        "Nathan Zhang",
        "Antonis Antoniades",
        "Wenyue Hua",
        "Kaijie Zhu",
        "Sirui Zeng",
        "William Yang Wang",
        "Xifeng Yan"
      ],
      "abstract": "As language agents progressively automate critical tasks across domains,\ntheir ability to operate within operational constraints and safety protocols\nbecomes essential. While extensive research has demonstrated these agents'\neffectiveness in downstream task completion, their reliability in following\noperational procedures and constraints remains largely unexplored. To this end,\nwe present AgentOrca, a dual-system framework for evaluating language agents'\ncompliance with operational constraints and routines. Our framework encodes\naction constraints and routines through both natural language prompts for\nagents and corresponding executable code serving as ground truth for automated\nverification. Through an automated pipeline of test case generation and\nevaluation across five real-world domains, we quantitatively assess current\nlanguage agents' adherence to operational constraints. Our findings reveal\nnotable performance gaps among state-of-the-art models, with large reasoning\nmodels like o1 demonstrating superior compliance while others show\nsignificantly lower performance, particularly when encountering complex\nconstraints or user persuasion attempts.",
      "tldr_zh": "该研究提出了AgentOrca，一种双系统框架，用于评估语言智能体在操作流程和约束遵循方面的表现。该框架通过自然语言提示和可执行代码分别编码操作约束和流程，并利用自动化测试管道在五个真实领域进行验证。研究发现，尽管大型推理模型（如o1）展现出较高的约束遵循能力，但其他模型在面对复杂约束或用户劝说时表现显著较差，揭示了当前语言智能体在操作合规性方面的明显差距。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08669v1",
      "published_date": "2025-03-11 17:53:02 UTC",
      "updated_date": "2025-03-11 17:53:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:16:38.571088"
    },
    {
      "arxiv_id": "2503.08665v1",
      "title": "REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder",
      "title_zh": "REGEN：利用（再）生成解码器学习紧凑视频嵌入",
      "authors": [
        "Yitian Zhang",
        "Long Mai",
        "Aniruddha Mahapatra",
        "David Bourgin",
        "Yicong Hong",
        "Jonah Casebeer",
        "Feng Liu",
        "Yun Fu"
      ],
      "abstract": "We present a novel perspective on learning video embedders for generative\nmodeling: rather than requiring an exact reproduction of an input video, an\neffective embedder should focus on synthesizing visually plausible\nreconstructions. This relaxed criterion enables substantial improvements in\ncompression ratios without compromising the quality of downstream generative\nmodels. Specifically, we propose replacing the conventional encoder-decoder\nvideo embedder with an encoder-generator framework that employs a diffusion\ntransformer (DiT) to synthesize missing details from a compact latent space.\nTherein, we develop a dedicated latent conditioning module to condition the DiT\ndecoder on the encoded video latent embedding. Our experiments demonstrate that\nour approach enables superior encoding-decoding performance compared to\nstate-of-the-art methods, particularly as the compression ratio increases. To\ndemonstrate the efficacy of our approach, we report results from our video\nembedders achieving a temporal compression ratio of up to 32x (8x higher than\nleading video embedders) and validate the robustness of this ultra-compact\nlatent space for text-to-video generation, providing a significant efficiency\nboost in latent diffusion model training and inference.",
      "tldr_zh": "该研究提出了一种新颖的视频嵌入学习方法REGEN，通过将传统的编码器-解码器框架替换为编码器-生成器框架，利用扩散变换器(DiT)从紧凑的潜在空间中合成缺失细节，从而显著提升压缩比而不影响生成质量。实验表明，该方法在高达32倍的时间压缩比下仍能保持优异的编码-解码性能，并在文本到视频生成任务中验证了其超紧凑潜在空间的鲁棒性，为潜在扩散模型的训练和推理提供了显著的效率提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08665v1",
      "published_date": "2025-03-11 17:51:07 UTC",
      "updated_date": "2025-03-11 17:51:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:16:46.536459"
    },
    {
      "arxiv_id": "2503.08664v1",
      "title": "MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention",
      "title_zh": "MEAT：基于网格注意力机制的多视角扩散模型在百万像素级人体生成中的应用",
      "authors": [
        "Yuhan Wang",
        "Fangzhou Hong",
        "Shuai Yang",
        "Liming Jiang",
        "Wayne Wu",
        "Chen Change Loy"
      ],
      "abstract": "Multiview diffusion models have shown considerable success in image-to-3D\ngeneration for general objects. However, when applied to human data, existing\nmethods have yet to deliver promising results, largely due to the challenges of\nscaling multiview attention to higher resolutions. In this paper, we explore\nhuman multiview diffusion models at the megapixel level and introduce a\nsolution called mesh attention to enable training at 1024x1024 resolution.\nUsing a clothed human mesh as a central coarse geometric representation, the\nproposed mesh attention leverages rasterization and projection to establish\ndirect cross-view coordinate correspondences. This approach significantly\nreduces the complexity of multiview attention while maintaining cross-view\nconsistency. Building on this foundation, we devise a mesh attention block and\ncombine it with keypoint conditioning to create our human-specific multiview\ndiffusion model, MEAT. In addition, we present valuable insights into applying\nmultiview human motion videos for diffusion training, addressing the\nlongstanding issue of data scarcity. Extensive experiments show that MEAT\neffectively generates dense, consistent multiview human images at the megapixel\nlevel, outperforming existing multiview diffusion methods.",
      "tldr_zh": "本文提出MEAT模型，一种基于网格注意力(mesh attention)的百万像素级多视角人体生成扩散模型。针对现有多视角扩散模型在高分辨率人体生成中的性能瓶颈，该研究通过引入以服装人体网格为核心的几何表示，利用栅格化和投影建立跨视角坐标对应，将多视角注意力复杂度从O(N²)降至O(N)。结合关键点条件机制，MEAT在1024×1024分辨率下实现了稠密且一致的多视角人体图像生成，同时创新性地利用多视角人体运动视频缓解训练数据稀缺问题。实验表明，该方法显著优于现有技术。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025. Code https://github.com/johannwyh/MEAT Project Page\n  https://johann.wang/MEAT/",
      "pdf_url": "http://arxiv.org/pdf/2503.08664v1",
      "published_date": "2025-03-11 17:50:59 UTC",
      "updated_date": "2025-03-11 17:50:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:17:28.064945"
    },
    {
      "arxiv_id": "2503.08663v1",
      "title": "Generating Robot Constitutions & Benchmarks for Semantic Safety",
      "title_zh": "生成机器人宪法与语义安全基准",
      "authors": [
        "Pierre Sermanet",
        "Anirudha Majumdar",
        "Alex Irpan",
        "Dmitry Kalashnikov",
        "Vikas Sindhwani"
      ],
      "abstract": "Until recently, robotics safety research was predominantly about collision\navoidance and hazard reduction in the immediate vicinity of a robot. Since the\nadvent of large vision and language models (VLMs), robots are now also capable\nof higher-level semantic scene understanding and natural language interactions\nwith humans. Despite their known vulnerabilities (e.g. hallucinations or\njail-breaking), VLMs are being handed control of robots capable of physical\ncontact with the real world. This can lead to dangerous behaviors, making\nsemantic safety for robots a matter of immediate concern. Our contributions in\nthis paper are two fold: first, to address these emerging risks, we release the\nASIMOV Benchmark, a large-scale and comprehensive collection of datasets for\nevaluating and improving semantic safety of foundation models serving as robot\nbrains. Our data generation recipe is highly scalable: by leveraging text and\nimage generation techniques, we generate undesirable situations from real-world\nvisual scenes and human injury reports from hospitals. Secondly, we develop a\nframework to automatically generate robot constitutions from real-world data to\nsteer a robot's behavior using Constitutional AI mechanisms. We propose a novel\nauto-amending process that is able to introduce nuances in written rules of\nbehavior; this can lead to increased alignment with human preferences on\nbehavior desirability and safety. We explore trade-offs between generality and\nspecificity across a diverse set of constitutions of different lengths, and\ndemonstrate that a robot is able to effectively reject unconstitutional\nactions. We measure a top alignment rate of 84.3% on the ASIMOV Benchmark using\ngenerated constitutions, outperforming no-constitution baselines and\nhuman-written constitutions. Data is available at asimov-benchmark.github.io",
      "tldr_zh": "该研究针对基于视觉语言模型(VLMs)的机器人安全风险，提出了两项核心贡献：首先开发了ASIMOV Benchmark——一个通过文本/图像生成技术构建的大规模语义安全评估数据集，包含从真实场景和医院伤害报告生成的风险情境；其次提出机器人宪法自动生成框架，采用Constitutional AI机制和新型auto-amending流程，能动态优化行为规则并实现84.3%的指令对齐率，显著优于无宪法基线(no-constitution baselines)和人工编写宪法。研究通过权衡规则通用性与特异性，验证了机器人有效拒绝违宪行为的能力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08663v1",
      "published_date": "2025-03-11 17:50:47 UTC",
      "updated_date": "2025-03-11 17:50:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:17:10.556261"
    },
    {
      "arxiv_id": "2503.08662v1",
      "title": "Exploring the Word Sense Disambiguation Capabilities of Large Language Models",
      "title_zh": "探索大语言模型的词义消歧能力",
      "authors": [
        "Pierpaolo Basile",
        "Lucia Siciliani",
        "Elio Musacchio",
        "Giovanni Semeraro"
      ],
      "abstract": "Word Sense Disambiguation (WSD) is a historical task in computational\nlinguistics that has received much attention over the years. However, with the\nadvent of Large Language Models (LLMs), interest in this task (in its classical\ndefinition) has decreased. In this study, we evaluate the performance of\nvarious LLMs on the WSD task. We extend a previous benchmark (XL-WSD) to\nre-design two subtasks suitable for LLM: 1) given a word in a sentence, the LLM\nmust generate the correct definition; 2) given a word in a sentence and a set\nof predefined meanings, the LLM must select the correct one. The extended\nbenchmark is built using the XL-WSD and BabelNet. The results indicate that\nLLMs perform well in zero-shot learning but cannot surpass current\nstate-of-the-art methods. However, a fine-tuned model with a medium number of\nparameters outperforms all other models, including the state-of-the-art.",
      "tldr_zh": "本研究评估了大型语言模型(LLMs)在词义消歧(WSD)任务上的表现。通过扩展XL-WSD基准测试，设计了两个适合LLM的子任务：1)根据句子中的单词生成正确释义；2)从预定义含义集中选择正确词义。实验表明，LLMs在零样本学习表现良好但未能超越现有最优方法，而经过微调的中等规模参数模型却能超越包括SOTA在内的所有模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08662v1",
      "published_date": "2025-03-11 17:50:44 UTC",
      "updated_date": "2025-03-11 17:50:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:17:50.953883"
    },
    {
      "arxiv_id": "2503.08644v1",
      "title": "Exploiting Instruction-Following Retrievers for Malicious Information Retrieval",
      "title_zh": "利用指令遵循检索器进行恶意信息检索",
      "authors": [
        "Parishad BehnamGhader",
        "Nicholas Meade",
        "Siva Reddy"
      ],
      "abstract": "Instruction-following retrievers have been widely adopted alongside LLMs in\nreal-world applications, but little work has investigated the safety risks\nsurrounding their increasing search capabilities. We empirically study the\nability of retrievers to satisfy malicious queries, both when used directly and\nwhen used in a retrieval augmented generation-based setup. Concretely, we\ninvestigate six leading retrievers, including NV-Embed and LLM2Vec, and find\nthat given malicious requests, most retrievers can (for >50% of queries) select\nrelevant harmful passages. For example, LLM2Vec correctly selects passages for\n61.35% of our malicious queries. We further uncover an emerging risk with\ninstruction-following retrievers, where highly relevant harmful information can\nbe surfaced by exploiting their instruction-following capabilities. Finally, we\nshow that even safety-aligned LLMs, such as Llama3, can satisfy malicious\nrequests when provided with harmful retrieved passages in-context. In summary,\nour findings underscore the malicious misuse risks associated with increasing\nretriever capability.",
      "tldr_zh": "该研究揭示了指令跟随检索模型(instruction-following retrievers)在恶意信息检索中的安全隐患。实验评估了包括NV-Embed和LLM2Vec在内的六种主流检索模型，发现大多数模型对超过50%的恶意查询能返回相关有害内容，其中LLM2Vec的恶意查询满足率高达61.35%。研究还发现，攻击者可利用这类模型的指令跟随特性获取高相关性有害信息，即使经过安全对齐的大语言模型(如Llama3)在获取有害检索结果后也会响应恶意请求。这项研究凸显了随着检索能力提升带来的恶意滥用风险。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08644v1",
      "published_date": "2025-03-11 17:36:53 UTC",
      "updated_date": "2025-03-11 17:36:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:18:28.448792"
    },
    {
      "arxiv_id": "2503.08643v1",
      "title": "Rethinking Diffusion Model in High Dimension",
      "title_zh": "重新思考高维空间中的扩散模型",
      "authors": [
        "Zhenxin Zheng",
        "Zhenjie Zheng"
      ],
      "abstract": "Curse of Dimensionality is an unavoidable challenge in statistical\nprobability models, yet diffusion models seem to overcome this limitation,\nachieving impressive results in high-dimensional data generation. Diffusion\nmodels assume that they can learn the statistical properties of the underlying\nprobability distribution, enabling sampling from this distribution to generate\nrealistic samples. But is this really how they work? To address this question,\nthis paper conducts a detailed analysis of the objective function and inference\nmethods of diffusion models, leading to several important conclusions that help\nanswer the above question: 1) In high-dimensional sparse scenarios, the target\nof the objective function fitting degrades from a weighted sum of multiple\nsamples to a single sample. 2) The mainstream inference methods can all be\nrepresented within a simple unified framework, without requiring statistical\nconcepts such as Markov chains and SDEs. 3) Guided by this simple framework,\nmore efficient inference methods can be discovered.",
      "tldr_zh": "该研究重新审视了扩散模型在高维数据中的工作原理，揭示了三个关键发现：1）在高维稀疏场景下，目标函数的拟合目标会从多样本加权和退化为单样本；2）主流推理方法可统一为简单框架，无需依赖马尔可夫链或随机微分方程等统计概念；3）基于该框架可发现更高效的推理方法。这些结论挑战了扩散模型通过学习概率分布统计特性生成数据的传统认知。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08643v1",
      "published_date": "2025-03-11 17:36:11 UTC",
      "updated_date": "2025-03-11 17:36:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:18:17.477392"
    },
    {
      "arxiv_id": "2503.08638v1",
      "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
      "title_zh": "YuE：面向长篇幅音乐生成的可扩展开放基础模型",
      "authors": [
        "Ruibin Yuan",
        "Hanfeng Lin",
        "Shuyue Guo",
        "Ge Zhang",
        "Jiahao Pan",
        "Yongyi Zang",
        "Haohe Liu",
        "Yiming Liang",
        "Wenye Ma",
        "Xingjian Du",
        "Xinrun Du",
        "Zhen Ye",
        "Tianyu Zheng",
        "Yinghao Ma",
        "Minghao Liu",
        "Zeyue Tian",
        "Ziya Zhou",
        "Liumeng Xue",
        "Xingwei Qu",
        "Yizhi Li",
        "Shangda Wu",
        "Tianhao Shen",
        "Ziyang Ma",
        "Jun Zhan",
        "Chunhui Wang",
        "Yatian Wang",
        "Xiaowei Chi",
        "Xinyue Zhang",
        "Zhenzhu Yang",
        "Xiangzhou Wang",
        "Shansong Liu",
        "Lingrui Mei",
        "Peng Li",
        "Junjie Wang",
        "Jianwei Yu",
        "Guojian Pang",
        "Xu Li",
        "Zihao Wang",
        "Xiaohuan Zhou",
        "Lijun Yu",
        "Emmanouil Benetos",
        "Yong Chen",
        "Chenghua Lin",
        "Xie Chen",
        "Gus Xia",
        "Zhaoxiang Zhang",
        "Chao Zhang",
        "Wenhu Chen",
        "Xinyu Zhou",
        "Xipeng Qiu",
        "Roger Dannenberg",
        "Jiaheng Liu",
        "Jian Yang",
        "Wenhao Huang",
        "Wei Xue",
        "Xu Tan",
        "Yike Guo"
      ],
      "abstract": "We tackle the task of long-form music generation--particularly the\nchallenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
      "tldr_zh": "该研究提出了YuE，一个基于LLaMA2架构的开放基础模型家族，专注于长音乐生成，尤其是歌词到歌曲的转换任务。YuE通过解耦的音轨预测、渐进式结构条件化以及多任务多阶段预训练策略，能够生成长达五分钟的音乐，同时保持歌词对齐、音乐结构连贯和伴奏适宜。此外，YuE支持风格迁移和双向生成，并在音乐性和声乐灵活性上媲美甚至超越部分专有系统。模型还展示了在音乐理解任务上的优异表现，显著提升了MARBLE基准测试的性能。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "https://github.com/multimodal-art-projection/YuE",
      "pdf_url": "http://arxiv.org/pdf/2503.08638v1",
      "published_date": "2025-03-11 17:26:50 UTC",
      "updated_date": "2025-03-11 17:26:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:19:09.154287"
    },
    {
      "arxiv_id": "2503.10683v1",
      "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
      "title_zh": "理解扩散语言模型中的质量-多样性权衡",
      "authors": [
        "Zak Buzzard"
      ],
      "abstract": "Diffusion models have seen immense success in modelling continuous data\nacross a range of domains such as vision and audio. Despite the challenges of\nadapting diffusion models to discrete data, recent work explores their\napplication to text generation by working in the continuous embedding space.\nHowever, these models lack a natural means to control the inherent trade-off\nbetween quality and diversity as afforded by the temperature hyperparameter in\nautoregressive models, hindering understanding of model performance and\nrestricting generation quality. This work proposes the use of classifier-free\nguidance and stochastic clamping for manipulating the quality-diversity\ntrade-off on sequence-to-sequence tasks, demonstrating that these techniques\nmay be used to improve the performance of a diffusion language model.",
      "tldr_zh": "该研究探讨了扩散模型(Diffusion Models)在文本生成中的质量-多样性权衡问题。针对当前扩散语言模型缺乏类似自回归模型中温度超参数的自然控制机制，研究者提出了基于无分类器引导(classifier-free guidance)和随机钳位(stochastic clamping)的方法来调节生成质量与多样性。实验表明，这些技术可有效提升扩散模型在序列到序列任务上的性能表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.10683v1",
      "published_date": "2025-03-11 17:18:01 UTC",
      "updated_date": "2025-03-11 17:18:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:18:34.736038"
    },
    {
      "arxiv_id": "2503.08609v1",
      "title": "Vision Transformer for Intracranial Hemorrhage Classification in CT Scans Using an Entropy-Aware Fuzzy Integral Strategy for Adaptive Scan-Level Decision Fusion",
      "title_zh": "基于熵感知模糊积分策略的 Vision Transformer 用于 CT 扫描颅内出血分类的自适应扫描级决策融合",
      "authors": [
        "Mehdi Hosseini Chagahi",
        "Niloufar Delfan",
        "Behzad Moshiri",
        "Md. Jalil Piran",
        "Jaber Hatam Parikhan"
      ],
      "abstract": "Intracranial hemorrhage (ICH) is a critical medical emergency caused by the\nrupture of cerebral blood vessels, leading to internal bleeding within the\nskull. Accurate and timely classification of hemorrhage subtypes is essential\nfor effective clinical decision-making. To address this challenge, we propose\nan advanced pyramid vision transformer (PVT)-based model, leveraging its\nhierarchical attention mechanisms to capture both local and global spatial\ndependencies in brain CT scans. Instead of processing all extracted features\nindiscriminately, A SHAP-based feature selection method is employed to identify\nthe most discriminative components, which are then used as a latent feature\nspace to train a boosting neural network, reducing computational complexity. We\nintroduce an entropy-aware aggregation strategy along with a fuzzy integral\noperator to fuse information across multiple CT slices, ensuring a more\ncomprehensive and reliable scan-level diagnosis by accounting for inter-slice\ndependencies. Experimental results show that our PVT-based framework\nsignificantly outperforms state-of-the-art deep learning architectures in terms\nof classification accuracy, precision, and robustness. By combining SHAP-driven\nfeature selection, transformer-based modeling, and an entropy-aware fuzzy\nintegral operator for decision fusion, our method offers a scalable and\ncomputationally efficient AI-driven solution for automated ICH subtype\nclassification.",
      "tldr_zh": "本研究提出了一种基于金字塔视觉Transformer（PVT）的模型，用于颅内出血（ICH）亚型的自动分类。该模型通过层次化注意力机制捕捉脑部CT扫描的局部和全局空间特征，并采用SHAP驱动的特征选择方法优化特征空间，降低计算复杂度。此外，研究引入了一种基于熵感知的模糊积分策略，融合多CT切片信息，提升扫描级诊断的全面性和可靠性。实验结果表明，该框架在分类准确性、精度和鲁棒性上显著优于现有深度学习架构，为ICH亚型分类提供了一种高效且可扩展的AI解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08609v1",
      "published_date": "2025-03-11 16:47:32 UTC",
      "updated_date": "2025-03-11 16:47:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:18:42.300714"
    },
    {
      "arxiv_id": "2503.08608v1",
      "title": "A Grid Cell-Inspired Structured Vector Algebra for Cognitive Maps",
      "title_zh": "受网格细胞启发的认知地图结构化向量代数",
      "authors": [
        "Sven Krausse",
        "Emre Neftci",
        "Friedrich T. Sommer",
        "Alpha Renner"
      ],
      "abstract": "The entorhinal-hippocampal formation is the mammalian brain's navigation\nsystem, encoding both physical and abstract spaces via grid cells. This system\nis well-studied in neuroscience, and its efficiency and versatility make it\nattractive for applications in robotics and machine learning. While continuous\nattractor networks (CANs) successfully model entorhinal grid cells for encoding\nphysical space, integrating both continuous spatial and abstract spatial\ncomputations into a unified framework remains challenging. Here, we attempt to\nbridge this gap by proposing a mechanistic model for versatile information\nprocessing in the entorhinal-hippocampal formation inspired by CANs and Vector\nSymbolic Architectures (VSAs), a neuro-symbolic computing framework. The novel\ngrid-cell VSA (GC-VSA) model employs a spatially structured encoding scheme\nwith 3D neuronal modules mimicking the discrete scales and orientations of grid\ncell modules, reproducing their characteristic hexagonal receptive fields. In\nexperiments, the model demonstrates versatility in spatial and abstract tasks:\n(1) accurate path integration for tracking locations, (2) spatio-temporal\nrepresentation for querying object locations and temporal relations, and (3)\nsymbolic reasoning using family trees as a structured test case for\nhierarchical relationships.",
      "tldr_zh": "该研究提出了一种受网格细胞启发的结构化向量代数模型GC-VSA，用于构建认知地图。该模型结合连续吸引子网络(CANs)和向量符号架构(VSAs)，通过模拟网格细胞的3D模块化结构和六边形感受野，实现了空间与抽象信息的统一处理。实验表明，GC-VSA能够同时完成路径整合、时空表征和符号推理三项任务，包括精确定位、物体时空关系查询以及家族谱系等层次关系推理，为类脑导航与认知计算提供了新框架。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.NE",
      "comment": "10 pages, 5 figures, accepted at the 2025 Neuro Inspired\n  Computational Elements (NICE) conference",
      "pdf_url": "http://arxiv.org/pdf/2503.08608v1",
      "published_date": "2025-03-11 16:45:52 UTC",
      "updated_date": "2025-03-11 16:45:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:18:47.289197"
    },
    {
      "arxiv_id": "2503.08605v1",
      "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling",
      "title_zh": "免调优多事件长视频生成：基于同步耦合采样的方法",
      "authors": [
        "Subin Kim",
        "Seoung Wug Oh",
        "Jui-Hsien Wang",
        "Joon-Young Lee",
        "Jinwoo Shin"
      ],
      "abstract": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively.",
      "tldr_zh": "本文提出了一种免调参的多事件长视频生成方法SynCoS，通过同步耦合采样技术解决现有模型在生成长视频时出现的语义漂移问题。该框架创新性地结合反向采样和优化采样两种策略，利用固定基准噪声和同步时间步实现去噪路径对齐，从而同时保证局部平滑过渡和全局语义一致性。实验表明，SynCoS在保持多提示词动态内容切换的同时，显著提升了长视频的连贯性表现，在定量和定性评估中均优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page with visuals: https://syncos2025.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.08605v1",
      "published_date": "2025-03-11 16:43:45 UTC",
      "updated_date": "2025-03-11 16:43:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:19:10.740776"
    },
    {
      "arxiv_id": "2503.08604v1",
      "title": "EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments",
      "title_zh": "EMMOE：开放环境中具身移动操作的综合基准评测框架",
      "authors": [
        "Dongping Li",
        "Tielong Cai",
        "Tianci Tang",
        "Wenhao Chai",
        "Katherine Rose Driggs-Campbell",
        "Gaoang Wang"
      ],
      "abstract": "Developing autonomous home robots controlled by natural language has long\nbeen a pursuit of human. While advancements in large language models (LLMs) and\nembodied intelligence make this goal closer, several challenges persist: the\nlack of a unified benchmark for more complex robot tasks, limited evaluation\nmethods and metrics, data incompatibility between LLMs and mobile manipulation\ntrajectories. To address these issues, we introduce Embodied Mobile\nManipulation in Open Environments (EMMOE), which requires agents to interpret\nuser instructions and execute long-horizon everyday tasks in continuous space.\nEMMOE seamlessly integrates high-level and low-level embodied tasks into a\nunified framework, along with three new metrics for more diverse assessment.\nAdditionally, we collect EMMOE-100, which features in various task attributes,\ndetailed process annotations, re-plans after failures, and two sub-datasets for\nLLM training. Furthermore, we design HomieBot, a sophisticated agent system\nconsists of LLM with Direct Preference Optimization (DPO), light weighted\nnavigation and manipulation models, and multiple error detection mechanisms.\nFinally, we demonstrate HomieBot's performance and the evaluation of different\nmodels and policies.",
      "tldr_zh": "该研究提出了EMMOE基准测试，用于评估开放环境中具身移动操作任务，解决了现有基准在复杂任务统一性、评估方法和数据兼容性方面的不足。该框架整合了高层规划与底层控制任务，并引入三种新评估指标，同时发布了包含100个任务的EMMOE-100数据集，具有详细标注和失败重规划特性。研究团队还开发了HomieBot智能体系统，结合经过DPO优化的LLM、轻量级导航操作模型和错误检测机制，展示了其在连续空间执行长时程日常任务的能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08604v1",
      "published_date": "2025-03-11 16:42:36 UTC",
      "updated_date": "2025-03-11 16:42:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:19:03.276407"
    },
    {
      "arxiv_id": "2503.08762v1",
      "title": "Neurosymbolic Decision Trees",
      "title_zh": "神经符号决策树",
      "authors": [
        "Matthias Möller",
        "Arvid Norlander",
        "Pedro Zuidberg Dos Martires",
        "Luc De Raedt"
      ],
      "abstract": "Neurosymbolic (NeSy) AI studies the integration of neural networks (NNs) and\nsymbolic reasoning based on logic. Usually, NeSy techniques focus on learning\nthe neural, probabilistic and/or fuzzy parameters of NeSy models. Learning the\nsymbolic or logical structure of such models has, so far, received less\nattention. We introduce neurosymbolic decision trees (NDTs), as an extension of\ndecision trees together with a novel NeSy structure learning algorithm, which\nwe dub NeuID3. NeuID3 adapts the standard top-down induction of decision tree\nalgorithms and combines it with a neural probabilistic logic representation,\ninherited from the DeepProbLog family of models. The key advantage of learning\nNDTs with NeuID3 is the support of both symbolic and subsymbolic data (such as\nimages), and that they can exploit background knowledge during the induction of\nthe tree structure, In our experimental evaluation we demonstrate the benefits\nof NeSys structure learning over more traditonal approaches such as purely\ndata-driven learning with neural networks.",
      "tldr_zh": "该研究提出了神经符号决策树（NDTs），这是一种结合神经网络与逻辑推理的新型神经符号AI模型。研究者开发了NeuID3算法，通过改进传统决策树自顶向下归纳方法，并整合DeepProbLog的概率逻辑表示，实现了同时支持符号数据和亚符号数据（如图像）的学习框架。实验证明，该方法在利用背景知识进行结构学习方面，比纯数据驱动的神经网络方法更具优势。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08762v1",
      "published_date": "2025-03-11 16:40:38 UTC",
      "updated_date": "2025-03-11 16:40:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:19:09.040749"
    },
    {
      "arxiv_id": "2503.16498v1",
      "title": "Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data",
      "title_zh": "LLMs、虚拟用户与偏见：无需人类数据预测任何调查问题",
      "authors": [
        "Enzo Sinacola",
        "Arnault Pachot",
        "Thierry Petit"
      ],
      "abstract": "Large Language Models (LLMs) offer a promising alternative to traditional\nsurvey methods, potentially enhancing efficiency and reducing costs. In this\nstudy, we use LLMs to create virtual populations that answer survey questions,\nenabling us to predict outcomes comparable to human responses. We evaluate\nseveral LLMs-including GPT-4o, GPT-3.5, Claude 3.5-Sonnet, and versions of the\nLlama and Mistral models-comparing their performance to that of a traditional\nRandom Forests algorithm using demographic data from the World Values Survey\n(WVS). LLMs demonstrate competitive performance overall, with the significant\nadvantage of requiring no additional training data. However, they exhibit\nbiases when predicting responses for certain religious and population groups,\nunderperforming in these areas. On the other hand, Random Forests demonstrate\nstronger performance than LLMs when trained with sufficient data. We observe\nthat removing censorship mechanisms from LLMs significantly improves predictive\naccuracy, particularly for underrepresented demographic segments where censored\nmodels struggle. These findings highlight the importance of addressing biases\nand reconsidering censorship approaches in LLMs to enhance their reliability\nand fairness in public opinion research.",
      "tldr_zh": "该研究探讨了使用大语言模型（LLMs）生成虚拟用户回答调查问题的潜力，并与传统随机森林算法进行对比。实验表明，LLMs（如GPT-4o、Claude 3.5-Sonnet等）在无需额外训练数据的情况下表现优异，但在预测某些宗教和人口群体的回答时存在偏差。去除LLMs的审查机制显著提高了预测准确性，尤其在代表性不足的群体中。研究强调，优化LLMs的偏差和审查机制对其在公共舆论研究中的可靠性和公平性至关重要。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted, proceedings of the 17th International Conference on Machine\n  Learning and Computing",
      "pdf_url": "http://arxiv.org/pdf/2503.16498v1",
      "published_date": "2025-03-11 16:27:20 UTC",
      "updated_date": "2025-03-11 16:27:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:19:19.119427"
    },
    {
      "arxiv_id": "2503.08588v1",
      "title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
      "title_zh": "BiasEdit：通过模型编辑消除语言模型中的刻板偏见",
      "authors": [
        "Xin Xu",
        "Wei Xu",
        "Ningyu Zhang",
        "Julian McAuley"
      ],
      "abstract": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
      "tldr_zh": "该研究提出了BiasEdit，一种通过模型编辑消除语言模型中刻板偏见的轻量级方法。该方法利用编辑网络生成参数更新，通过去偏损失指导对部分参数进行局部编辑，同时通过保留损失确保语言建模能力不受影响。实验表明，BiasEdit在StereoSet和Crows-Pairs数据集上有效消除了偏见，且对模型的通用能力影响极小，相比现有方法展现出更高的效率和鲁棒性。研究还通过偏见追踪探索了不同模块的偏见分布及编辑影响。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by TrustNLP @ NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08588v1",
      "published_date": "2025-03-11 16:25:36 UTC",
      "updated_date": "2025-03-11 16:25:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:19:50.095989"
    },
    {
      "arxiv_id": "2503.16497v1",
      "title": "Effective Yet Ephemeral Propaganda Defense: There Needs to Be More than One-Shot Inoculation to Enhance Critical Thinking",
      "title_zh": "有效却短暂的宣传防御：仅靠一次性“接种”不足以提升批判性思维",
      "authors": [
        "Nicolas Hoferer",
        "Kilian Sprenkamp",
        "Dorian Christoph Quelle",
        "Daniel Gordon Jones",
        "Zoya Katashinskaya",
        "Alexandre Bovet",
        "Liudmila Zavolokina"
      ],
      "abstract": "In today's media landscape, propaganda distribution has a significant impact\non society. It sows confusion, undermines democratic processes, and leads to\nincreasingly difficult decision-making for news readers. We investigate the\nlasting effect on critical thinking and propaganda awareness on them when using\na propaganda detection and contextualization tool. Building on inoculation\ntheory, which suggests that preemptively exposing individuals to weakened forms\nof propaganda can improve their resilience against it, we integrate Kahneman's\ndual-system theory to measure the tools' impact on critical thinking. Through a\ntwo-phase online experiment, we measure the effect of several inoculation\ndoses. Our findings show that while the tool increases critical thinking during\nits use, this increase vanishes without access to the tool. This indicates a\nsingle use of the tool does not create a lasting impact. We discuss the\nimplications and propose possible approaches to improve the resilience against\npropaganda in the long-term.",
      "tldr_zh": "本研究探讨了利用宣传检测与情境化工具对提升批判性思维和宣传意识的长期效果。基于接种理论(inoculation theory)和卡尼曼的双系统理论(Kahneman's dual-system theory)，研究通过两阶段在线实验测量了多次“接种”剂量的效果。结果表明，虽然工具在使用期间能显著提升批判性思维，但一旦停止使用，这种提升效果便会消失，说明单次使用无法产生持久影响。研究进一步讨论了长期增强对宣传抵抗力的可能方法。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16497v1",
      "published_date": "2025-03-11 16:24:19 UTC",
      "updated_date": "2025-03-11 16:24:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:19:46.781725"
    },
    {
      "arxiv_id": "2503.08581v2",
      "title": "MsaMIL-Net: An End-to-End Multi-Scale Aware Multiple Instance Learning Network for Efficient Whole Slide Image Classification",
      "title_zh": "MsaMIL-Net：一种端到端多尺度感知的多实例学习网络，用于高效全切片图像分类",
      "authors": [
        "Jiangping Wen",
        "Jinyu Wen",
        "Meie Fang"
      ],
      "abstract": "Bag-based Multiple Instance Learning (MIL) approaches have emerged as the\nmainstream methodology for Whole Slide Image (WSI) classification. However,\nmost existing methods adopt a segmented training strategy, which first extracts\nfeatures using a pre-trained feature extractor and then aggregates these\nfeatures through MIL. This segmented training approach leads to insufficient\ncollaborative optimization between the feature extraction network and the MIL\nnetwork, preventing end-to-end joint optimization and thereby limiting the\noverall performance of the model. Additionally, conventional methods typically\nextract features from all patches of fixed size, ignoring the multi-scale\nobservation characteristics of pathologists. This not only results in\nsignificant computational resource waste when tumor regions represent a minimal\nproportion (as in the Camelyon16 dataset) but may also lead the model to\nsuboptimal solutions.\n  To address these limitations, this paper proposes an end-to-end multi-scale\nWSI classification framework that integrates multi-scale feature extraction\nwith multiple instance learning. Specifically, our approach includes: (1) a\nsemantic feature filtering module to reduce interference from non-lesion areas;\n(2) a multi-scale feature extraction module to capture pathological information\nat different levels; and (3) a multi-scale fusion MIL module for global\nmodeling and feature integration. Through an end-to-end training strategy, we\nsimultaneously optimize both the feature extractor and MIL network, ensuring\nmaximum compatibility between them.\n  Experiments were conducted on three cross-center datasets (DigestPath2019,\nBCNB, and UBC-OCEAN). Results demonstrate that our proposed method outperforms\nexisting state-of-the-art approaches in terms of both accuracy (ACC) and AUC\nmetrics.",
      "tldr_zh": "本文提出了一种端到端的多尺度感知多实例学习网络MsaMIL-Net，用于高效的全切片图像(WSI)分类。该网络通过语义特征过滤模块减少非病变区域的干扰，并结合多尺度特征提取模块捕捉不同层次的病理信息，最后通过多尺度融合MIL模块进行全局建模和特征整合。实验表明，该方法在多个跨中心数据集上均优于现有最先进方法，显著提升了分类准确率(ACC)和AUC指标。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "summited to ICCV2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08581v2",
      "published_date": "2025-03-11 16:16:44 UTC",
      "updated_date": "2025-03-12 09:27:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:19:49.647122"
    },
    {
      "arxiv_id": "2503.08760v1",
      "title": "Heterogeneous Graph Structure Learning through the Lens of Data-generating Processes",
      "title_zh": "从数据生成过程视角探索异质图结构学习",
      "authors": [
        "Keyue Jiang",
        "Bohan Tang",
        "Xiaowen Dong",
        "Laura Toni"
      ],
      "abstract": "Inferring the graph structure from observed data is a key task in graph\nmachine learning to capture the intrinsic relationship between data entities.\nWhile significant advancements have been made in learning the structure of\nhomogeneous graphs, many real-world graphs exhibit heterogeneous patterns where\nnodes and edges have multiple types. This paper fills this gap by introducing\nthe first approach for heterogeneous graph structure learning (HGSL). To this\nend, we first propose a novel statistical model for the data-generating process\n(DGP) of heterogeneous graph data, namely hidden Markov networks for\nheterogeneous graphs (H2MN). Then we formalize HGSL as a maximum a-posterior\nestimation problem parameterized by such DGP and derive an alternating\noptimization method to obtain a solution together with a theoretical\njustification of the optimization conditions. Finally, we conduct extensive\nexperiments on both synthetic and real-world datasets to demonstrate that our\nproposed method excels in learning structure on heterogeneous graphs in terms\nof edge type identification and edge weight recovery.",
      "tldr_zh": "该论文提出了首个面向异构图结构学习（HGSL）的框架，通过数据生成过程（DGP）视角解决异构图（节点和边具有多类型）的结构推断难题。研究者首先建立了异构图数据的统计生成模型H2MN，将HGSL形式化为基于该DGP的最大后验估计问题，并开发了具有理论保证的交替优化算法。实验表明，该方法在边类型识别和边权重恢复等任务上显著优于现有技术，填补了异构图结构学习领域的空白。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08760v1",
      "published_date": "2025-03-11 16:14:53 UTC",
      "updated_date": "2025-03-11 16:14:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:19:56.331184"
    },
    {
      "arxiv_id": "2503.08565v1",
      "title": "When Discourse Stalls: Moving Past Five Semantic Stopsigns about Generative AI in Design Research",
      "title_zh": "当话语停滞时：超越设计研究中关于生成式人工智能的五个语义障碍",
      "authors": [
        "Willem van der Maden",
        "Vera van der Burg",
        "Brett A. Halperin",
        "Petra Jääskeläinen",
        "Joseph Lindley",
        "Derek Lomas",
        "Timothy Merritt"
      ],
      "abstract": "This essay examines how Generative AI (GenAI) is rapidly transforming design\npractices and how discourse often falls into over-simplified narratives that\nimpede meaningful research and practical progress. We identify and deconstruct\nfive prevalent \"semantic stopsigns\" -- reductive framings about GenAI in design\nthat halt deeper inquiry and limit productive engagement. Reflecting upon two\nexpert workshops at ACM conferences and semi-structured interviews with design\npractitioners, we analyze how these stopsigns manifest in research and\npractice. Our analysis develops mid-level knowledge that bridges theoretical\ndiscourse and practical implementation, helping designers and researchers\ninterrogate common assumptions about GenAI in their own contexts. By recasting\nthese stopsigns into more nuanced frameworks, we provide the design research\ncommunity with practical approaches for thinking about and working with these\nemerging technologies.",
      "tldr_zh": "这篇论文分析了生成式AI(GenAI)如何改变设计实践，并指出当前讨论常陷入五种阻碍深入研究的\"语义停止标志\"(semantic stopsigns)。通过ACM会议工作坊和设计师访谈，作者解构了这些简化叙事如何限制设计领域的创新思考。研究提出了连接理论与实践的中间层知识框架，帮助设计研究者以更细致的方式理解和应用GenAI技术。该工作为设计研究社区提供了突破固有思维定式的实用方法。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08565v1",
      "published_date": "2025-03-11 15:54:03 UTC",
      "updated_date": "2025-03-11 15:54:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:20:19.975003"
    },
    {
      "arxiv_id": "2503.08564v1",
      "title": "MoE-Loco: Mixture of Experts for Multitask Locomotion",
      "title_zh": "MoE-Loco：面向多任务运动的专家混合方法",
      "authors": [
        "Runhan Huang",
        "Shaoting Zhu",
        "Yilun Du",
        "Hang Zhao"
      ],
      "abstract": "We present MoE-Loco, a Mixture of Experts (MoE) framework for multitask\nlocomotion for legged robots. Our method enables a single policy to handle\ndiverse terrains, including bars, pits, stairs, slopes, and baffles, while\nsupporting quadrupedal and bipedal gaits. Using MoE, we mitigate the gradient\nconflicts that typically arise in multitask reinforcement learning, improving\nboth training efficiency and performance. Our experiments demonstrate that\ndifferent experts naturally specialize in distinct locomotion behaviors, which\ncan be leveraged for task migration and skill composition. We further validate\nour approach in both simulation and real-world deployment, showcasing its\nrobustness and adaptability.",
      "tldr_zh": "该研究提出MoE-Loco框架，采用专家混合(Mixture of Experts)方法实现腿式机器人的多任务运动控制，使单个策略能同时应对横杆、坑洞、楼梯等多种地形，并支持四足和双足步态。该方法通过MoE缓解多任务强化学习中的梯度冲突问题，实验表明不同专家会自然专精于特定运动行为，该特性可用于任务迁移和技能组合。框架在仿真和真实环境中均验证了其鲁棒性和适应性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08564v1",
      "published_date": "2025-03-11 15:53:54 UTC",
      "updated_date": "2025-03-11 15:53:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:20:22.662278"
    },
    {
      "arxiv_id": "2503.08558v1",
      "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies",
      "title_zh": "我们能否在没有故障数据的情况下检测故障？面向模仿学习策略的基于不确定性感知的运行时故障检测",
      "authors": [
        "Chen Xu",
        "Tony Khuong Nguyen",
        "Emma Dixon",
        "Christopher Rodriguez",
        "Patrick Miller",
        "Robert Lee",
        "Paarth Shah",
        "Rares Ambrus",
        "Haruki Nishimura",
        "Masha Itkina"
      ],
      "abstract": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment.",
      "tldr_zh": "该研究提出FAIL-Detect框架，首次实现无需故障数据的模仿学习策略运行时故障检测。该方法将问题转化为序列分布外(OOD)检测，通过提取策略输入输出的关键标量信号捕捉认知不确定性，并采用符合预测(Conformal Prediction)进行统计保障的不确定性量化。实验表明，基于新型flow-based密度估计器的学习信号效果最佳，其故障检测准确率和速度均超越现有方法，为模仿学习机器人系统的安全部署提供了可靠保障。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08558v1",
      "published_date": "2025-03-11 15:47:12 UTC",
      "updated_date": "2025-03-11 15:47:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:20:47.490520"
    },
    {
      "arxiv_id": "2503.08551v1",
      "title": "Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs",
      "title_zh": "基于大语言模型的推理与采样增强型多选题难度预测",
      "authors": [
        "Wanyong Feng",
        "Peter Tran",
        "Stephen Sireci",
        "Andrew Lan"
      ],
      "abstract": "The difficulty of multiple-choice questions (MCQs) is a crucial factor for\neducational assessments. Predicting MCQ difficulty is challenging since it\nrequires understanding both the complexity of reaching the correct option and\nthe plausibility of distractors, i.e., incorrect options. In this paper, we\npropose a novel, two-stage method to predict the difficulty of MCQs. First, to\nbetter estimate the complexity of each MCQ, we use large language models (LLMs)\nto augment the reasoning steps required to reach each option. We use not just\nthe MCQ itself but also these reasoning steps as input to predict the\ndifficulty. Second, to capture the plausibility of distractors, we sample\nknowledge levels from a distribution to account for variation among students\nresponding to the MCQ. This setup, inspired by item response theory (IRT),\nenable us to estimate the likelihood of students selecting each (both correct\nand incorrect) option. We align these predictions with their ground truth\nvalues, using a Kullback-Leibler (KL) divergence-based regularization\nobjective, and use estimated likelihoods to predict MCQ difficulty. We evaluate\nour method on two real-world \\emph{math} MCQ and response datasets with ground\ntruth difficulty values estimated using IRT. Experimental results show that our\nmethod outperforms all baselines, up to a 28.3\\% reduction in mean squared\nerror and a 34.6\\% improvement in the coefficient of determination. We also\nqualitatively discuss how our novel method results in higher accuracy in\npredicting MCQ difficulty.",
      "tldr_zh": "该论文提出了一种基于大语言模型(LLMs)的两阶段多选题(MCQ)难度预测新方法。第一阶段利用LLMs生成解题推理步骤，结合题目本身预测难度；第二阶段采用项目反应理论(IRT)思想，通过采样不同知识水平的学生反应来评估干扰项(错误选项)的迷惑性。实验表明，该方法在数学MCQ数据集上显著优于基线模型，均方误差降低28.3%，决定系数提高34.6%，为教育评估提供了更准确的难度预测工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08551v1",
      "published_date": "2025-03-11 15:39:43 UTC",
      "updated_date": "2025-03-11 15:39:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:20:37.114756"
    },
    {
      "arxiv_id": "2503.08549v1",
      "title": "Graph of AI Ideas: Leveraging Knowledge Graphs and LLMs for AI Research Idea Generation",
      "title_zh": "AI想法图谱：利用知识图谱与大型语言模型生成AI研究创意",
      "authors": [
        "Xian Gao",
        "Zongyun Zhang",
        "Mingye Xie",
        "Ting Liu",
        "Yuzhuo Fu"
      ],
      "abstract": "Reading relevant scientific papers and analyzing research development trends\nis a critical step in generating new scientific ideas. However, the rapid\nincrease in the volume of research literature and the complex citation\nrelationships make it difficult for researchers to quickly analyze and derive\nmeaningful research trends. The development of large language models (LLMs) has\nprovided a novel approach for automatically summarizing papers and generating\ninnovative research ideas. However, existing paper-based idea generation\nmethods either simply input papers into LLMs via prompts or form logical chains\nof creative development based on citation relationships, without fully\nexploiting the semantic information embedded in these citations. Inspired by\nknowledge graphs and human cognitive processes, we propose a framework called\nthe Graph of AI Ideas (GoAI) for the AI research field, which is dominated by\nopen-access papers. This framework organizes relevant literature into entities\nwithin a knowledge graph and summarizes the semantic information contained in\ncitations into relations within the graph. This organization effectively\nreflects the relationships between two academic papers and the advancement of\nthe AI research field. Such organization aids LLMs in capturing the current\nprogress of research, thereby enhancing their creativity. Experimental results\ndemonstrate the effectiveness of our approach in generating novel, clear, and\neffective research ideas.",
      "tldr_zh": "该研究提出Graph of AI Ideas (GoAI)框架，通过知识图谱（Knowledge Graph）与大型语言模型（LLMs）的结合来提升AI领域研究创意生成。该框架将文献组织为知识图谱中的实体，并将引用关系中的语义信息提炼为图谱关系，从而更全面地反映论文关联和研究进展。实验证明，该方法能帮助LLMs更好地把握研究现状，生成新颖、清晰且有效的研究思路，解决了传统方法简单输入论文或仅依赖引用链的局限性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.08549v1",
      "published_date": "2025-03-11 15:36:38 UTC",
      "updated_date": "2025-03-11 15:36:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:20:37.892317"
    },
    {
      "arxiv_id": "2503.08542v1",
      "title": "DAFE: LLM-Based Evaluation Through Dynamic Arbitration for Free-Form Question-Answering",
      "title_zh": "DAFE：基于大语言模型的自由问答动态仲裁评估框架",
      "authors": [
        "Sher Badshah",
        "Hassan Sajjad"
      ],
      "abstract": "Evaluating Large Language Models (LLMs) free-form generated responses remains\na challenge due to their diverse and open-ended nature. Traditional supervised\nsignal-based automatic metrics fail to capture semantic equivalence or handle\nthe variability of open-ended responses, while human evaluation, though\nreliable, is resource-intensive. Leveraging LLMs as evaluators offers a\npromising alternative due to their strong language understanding and\ninstruction-following capabilities. Taking advantage of these capabilities, we\npropose the Dynamic Arbitration Framework for Evaluation (DAFE), which employs\ntwo primary LLM-as-judges and engages a third arbitrator only in cases of\ndisagreements. This selective arbitration prioritizes evaluation reliability\nwhile reducing unnecessary computational demands compared to conventional\nmajority voting. DAFE utilizes task-specific reference answers with dynamic\narbitration to enhance judgment accuracy, resulting in significant improvements\nin evaluation metrics such as Macro F1 and Cohen's Kappa. Through experiments,\nincluding a comprehensive human evaluation, we demonstrate DAFE's ability to\nprovide consistent, scalable, and resource-efficient assessments, establishing\nit as a robust framework for evaluating free-form model outputs.",
      "tldr_zh": "该研究提出了DAFE框架，一种基于大语言模型(LLM)的动态仲裁评估方法，用于解决开放域问答任务中自由生成答案的评估难题。该方法创新性地采用双LLM评委加动态仲裁机制，仅在评委意见分歧时引入第三方仲裁，既保证了评估可靠性又显著降低了传统多数投票的计算开销。实验表明，DAFE通过任务特定参考答案和动态仲裁策略，在Macro F1和Cohen's Kappa等指标上显著提升，其评估效果与人工评估高度一致，为自由文本输出提供了可扩展且资源高效的自动化评估方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.0; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08542v1",
      "published_date": "2025-03-11 15:29:55 UTC",
      "updated_date": "2025-03-11 15:29:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:20:49.305499"
    },
    {
      "arxiv_id": "2503.08540v1",
      "title": "Mellow: a small audio language model for reasoning",
      "title_zh": "Mellow：面向推理的小型音频语言模型",
      "authors": [
        "Soham Deshmukh",
        "Satvik Dixit",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "abstract": "Multimodal Audio-Language Models (ALMs) can understand and reason over both\naudio and text. Typically, reasoning performance correlates with model size,\nwith the best results achieved by models exceeding 8 billion parameters.\nHowever, no prior work has explored enabling small audio-language models to\nperform reasoning tasks, despite the potential applications for edge devices.\nTo address this gap, we introduce Mellow, a small Audio-Language Model\nspecifically designed for reasoning. Mellow achieves state-of-the-art\nperformance among existing small audio-language models and surpasses several\nlarger models in reasoning capabilities. For instance, Mellow scores 52.11 on\nMMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times\nfewer parameters and being trained on 60 times less data (audio hrs). To train\nMellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded\nreasoning in models. It consists of a mixture of existing datasets (30% of the\ndata) and synthetically generated data (70%). The synthetic dataset is derived\nfrom audio captioning datasets, where Large Language Models (LLMs) generate\ndetailed and multiple-choice questions focusing on audio events, objects,\nacoustic scenes, signal properties, semantics, and listener emotions. To\nevaluate Mellow's reasoning ability, we benchmark it on a diverse set of tasks,\nassessing on both in-distribution and out-of-distribution data, including audio\nunderstanding, deductive reasoning, and comparative reasoning. Finally, we\nconduct extensive ablation studies to explore the impact of projection layer\nchoices, synthetic data generation methods, and language model pretraining on\nreasoning performance. Our training dataset, findings, and baseline pave the\nway for developing small ALMs capable of reasoning.",
      "tldr_zh": "该研究提出了Mellow，一个专为推理任务设计的小型音频语言模型（ALM）。尽管传统上推理性能与模型规模相关，但Mellow在参数减少50倍、训练数据减少60倍的情况下，性能媲美当前最优的大模型Qwen2 Audio（MMAU得分52.11 vs 52.5）。研究团队开发了ReasonAQA数据集（含30%真实数据和70%由LLM生成的合成数据），通过聚焦音频事件、声学场景等要素的问题来增强模型推理能力。实验表明，Mellow在音频理解、演绎推理等任务上表现优异，为边缘设备部署高效ALM提供了新基准。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Checkpoint and dataset available at:\n  https://github.com/soham97/mellow",
      "pdf_url": "http://arxiv.org/pdf/2503.08540v1",
      "published_date": "2025-03-11 15:29:00 UTC",
      "updated_date": "2025-03-11 15:29:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:21:14.167172"
    },
    {
      "arxiv_id": "2503.08537v1",
      "title": "Chemical reasoning in LLMs unlocks steerable synthesis planning and reaction mechanism elucidation",
      "title_zh": "大语言模型中的化学推理能力实现可调控的合成路线规划与反应机理阐明",
      "authors": [
        "Andres M Bran",
        "Theo A Neukomm",
        "Daniel P Armstrong",
        "Zlatko Jončev",
        "Philippe Schwaller"
      ],
      "abstract": "While machine learning algorithms have been shown to excel at specific\nchemical tasks, they have struggled to capture the strategic thinking that\ncharacterizes expert chemical reasoning, limiting their widespread adoption.\nHere we demonstrate that large language models (LLMs) can serve as powerful\nchemical reasoning engines when integrated with traditional search algorithms,\nenabling a new approach to computer-aided chemistry that mirrors human expert\nthinking. Rather than using LLMs to directly manipulate chemical structures, we\nleverage their ability to evaluate chemical strategies and guide search\nalgorithms toward chemically meaningful solutions. We demonstrate this paradigm\nthrough two fundamental challenges: strategy-aware retrosynthetic planning and\nmechanism elucidation. In retrosynthetic planning, our method allows chemists\nto specify desired synthetic strategies in natural language to find routes that\nsatisfy these constraints in vast searches. In mechanism elucidation, LLMs\nguide the search for plausible reaction mechanisms by combining chemical\nprinciples with systematic exploration. Our approach shows strong performance\nacross diverse chemical tasks, with larger models demonstrating increasingly\nsophisticated chemical reasoning. Our approach establishes a new paradigm for\ncomputer-aided chemistry that combines the strategic understanding of LLMs with\nthe precision of traditional chemical tools, opening possibilities for more\nintuitive and powerful chemical reasoning systems.",
      "tldr_zh": "该研究展示了大型语言模型(LLMs)与传统搜索算法结合后，可作为强大的化学推理引擎，推动计算机辅助化学的新范式。通过策略感知的逆合成路线规划和反应机制阐明两大挑战，研究证明LLMs能够评估化学策略并引导搜索算法找到化学意义明确的解决方案。这种方法不仅允许化学家用自然语言指定合成策略，还能结合化学原理系统探索反应机制。研究结果表明，LLMs展现出日益复杂的化学推理能力，为更直观、强大的化学推理系统开辟了新可能。",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08537v1",
      "published_date": "2025-03-11 15:27:17 UTC",
      "updated_date": "2025-03-11 15:27:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:21:14.652696"
    },
    {
      "arxiv_id": "2503.08525v1",
      "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training",
      "title_zh": "GTR：引导式思维强化防止基于强化学习的视觉语言模型智能体训练中的思维坍缩",
      "authors": [
        "Tong Wei",
        "Yijun Yang",
        "Junliang Xing",
        "Yuanchun Shi",
        "Zongqing Lu",
        "Deheng Ye"
      ],
      "abstract": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
      "tldr_zh": "该研究提出了GTR（Guided Thought Reinforcement）框架，旨在解决基于强化学习（RL）的视觉语言模型（VLM）训练中出现的“思维崩溃”问题。研究发现，仅依赖动作结果的奖励机制会导致思维多样性丧失和无效推理，从而影响任务表现。GTR通过引入自动化校正器，在每一步RL训练中评估和优化模型的推理过程，无需密集的人工标注。实验表明，GTR显著提升了LLaVA-7b模型在多种视觉环境中的任务成功率和泛化能力，性能比现有最先进模型高出3-5倍。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08525v1",
      "published_date": "2025-03-11 15:17:02 UTC",
      "updated_date": "2025-03-11 15:17:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:21:58.458034"
    },
    {
      "arxiv_id": "2503.14514v1",
      "title": "Acceptance or Rejection of Lots while Minimizing and Controlling Type I and Type II Errors",
      "title_zh": "在最小化并控制I类和II类错误下的批次接受或拒绝",
      "authors": [
        "Edson Luiz Ursini",
        "Elaine Cristina Catapani Poletti",
        "Loreno Menezes da Silveira",
        "José Roberto Emiliano Leite"
      ],
      "abstract": "The double hypothesis test (DHT) is a test that allows controlling Type I\n(producer) and Type II (consumer) errors. It is possible to say whether the\nbatch has a defect rate, p, between 1.5 and 2%, or between 2 and 5%, or between\n5 and 10%, and so on, until finding a required value for this probability.\nUsing the two probabilities side by side, the Type I error for the lower\nprobability distribution and the Type II error for the higher probability\ndistribution, both can be controlled and minimized. It can be applied in the\ndevelopment or manufacturing process of a batch of components, or in the case\nof purchasing from a supplier, when the percentage of defects (p) is unknown,\nconsidering the technology and/or process available to obtain them. The power\nof the test is amplified by the joint application of the Limit of Successive\nFailures (LSF) related to the Renewal Theory. To enable the choice of the most\nappropriate algorithm for each application. Four distributions are proposed for\nthe Bernoulli event sequence, including their computational efforts: Binomial,\nBinomial approximated by Poisson, and Binomial approximated by Gaussian (with\ntwo variants). Fuzzy logic rules are also applied to facilitate\ndecision-making.",
      "tldr_zh": "该研究提出了一种双假设检验(DHT)方法，用于在批次验收或拒收过程中同时控制并最小化I类（生产者）和II类（消费者）错误。通过结合极限连续失败(LSF)和更新理论，该方法能够确定批次的缺陷率(p)是否处于特定区间（如1.5-2%、2-5%等），适用于制造或采购过程中缺陷率未知的场景。研究还提出了四种伯努利事件序列分布（二项分布、泊松近似、高斯近似及其变体）以及模糊逻辑规则，以优化算法选择并辅助决策。",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.14514v1",
      "published_date": "2025-03-11 15:02:45 UTC",
      "updated_date": "2025-03-11 15:02:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:21:43.463486"
    },
    {
      "arxiv_id": "2503.08489v2",
      "title": "A Triple-Inertial Accelerated Alternating Optimization Method for Deep Learning Training",
      "title_zh": "三重惯性加速交替优化方法在深度学习训练中的应用",
      "authors": [
        "Chengcheng Yan",
        "Jiawei Xu",
        "Qingsong Wang",
        "Zheng Peng"
      ],
      "abstract": "The stochastic gradient descent (SGD) algorithm has achieved remarkable\nsuccess in training deep learning models. However, it has several limitations,\nincluding susceptibility to vanishing gradients, sensitivity to input data, and\na lack of robust theoretical guarantees. In recent years, alternating\nminimization (AM) methods have emerged as a promising alternative for model\ntraining by employing gradient-free approaches to iteratively update model\nparameters. Despite their potential, these methods often exhibit slow\nconvergence rates. To address this challenge, we propose a novel\nTriple-Inertial Accelerated Alternating Minimization (TIAM) framework for\nneural network training. The TIAM approach incorporates a triple-inertial\nacceleration strategy with a specialized approximation method, facilitating\ntargeted acceleration of different terms in each sub-problem optimization. This\nintegration improves the efficiency of convergence, achieving superior\nperformance with fewer iterations. Additionally, we provide a convergence\nanalysis of the TIAM algorithm, including its global convergence properties and\nconvergence rate. Extensive experiments validate the effectiveness of the TIAM\nmethod, showing significant improvements in generalization capability and\ncomputational efficiency compared to existing approaches, particularly when\napplied to the rectified linear unit (ReLU) and its variants.",
      "tldr_zh": "本研究提出了一种三重惯性加速交替优化方法(TIAM)，用于深度学习模型的训练。该方法通过引入三重惯性加速策略和特定的近似方法，有针对性地加速每个子问题的优化过程，从而提高了收敛效率。与现有方法相比，TIAM在更少的迭代次数下实现了更好的性能，特别是在ReLU及其变体上的应用表现突出。研究还提供了TIAM算法的收敛性分析，验证了其全局收敛性和收敛速率。实验结果表明，TIAM在泛化能力和计算效率方面均有显著提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08489v2",
      "published_date": "2025-03-11 14:42:17 UTC",
      "updated_date": "2025-03-13 12:57:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:21:33.413800"
    },
    {
      "arxiv_id": "2503.08472v1",
      "title": "Optimizing Ride-Pooling Operations with Extended Pickup and Drop-Off Flexibility",
      "title_zh": "优化具有扩展上下车灵活性的拼车运营",
      "authors": [
        "Hao Jiang",
        "Yixing Xu",
        "Pradeep Varakantham"
      ],
      "abstract": "The Ride-Pool Matching Problem (RMP) is central to on-demand ride-pooling\nservices, where vehicles must be matched with multiple requests while adhering\nto service constraints such as pickup delays, detour limits, and vehicle\ncapacity. Most existing RMP solutions assume passengers are picked up and\ndropped off at their original locations, neglecting the potential for\npassengers to walk to nearby spots to meet vehicles. This assumption restricts\nthe optimization potential in ride-pooling operations. In this paper, we\npropose a novel matching method that incorporates extended pickup and drop-off\nareas for passengers. We first design a tree-based approach to efficiently\ngenerate feasible matches between passengers and vehicles. Next, we optimize\nvehicle routes to cover all designated pickup and drop-off locations while\nminimizing total travel distance. Finally, we employ dynamic assignment\nstrategies to achieve optimal matching outcomes. Experiments on city-scale taxi\ndatasets demonstrate that our method improves the number of served requests by\nup to 13\\% and average travel distance by up to 21\\% compared to leading\nexisting solutions, underscoring the potential of leveraging passenger mobility\nto significantly enhance ride-pooling service efficiency.",
      "tldr_zh": "本研究提出了一种创新的拼车匹配方法，通过扩展乘客上下车区域的灵活性来优化拼车运营。该方法采用树状结构高效生成乘客与车辆间的可行匹配方案，并优化车辆路线以覆盖所有指定上下车点，同时最小化总行驶距离。实验表明，相比现有方案，该方法能将服务请求数量提升13%，平均行驶距离减少21%，显著提高了拼车服务效率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08472v1",
      "published_date": "2025-03-11 14:17:30 UTC",
      "updated_date": "2025-03-11 14:17:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:21:49.240214"
    },
    {
      "arxiv_id": "2503.08467v1",
      "title": "Accelerating MoE Model Inference with Expert Sharding",
      "title_zh": "加速MoE模型推理：基于专家分片的优化方法",
      "authors": [
        "Oana Balmau",
        "Anne-Marie Kermarrec",
        "Rafael Pires",
        "André Loureiro Espírito Santo",
        "Martijn de Vos",
        "Milos Vujasinovic"
      ],
      "abstract": "Mixture of experts (MoE) models achieve state-of-the-art results in language\nmodeling but suffer from inefficient hardware utilization due to imbalanced\ntoken routing and communication overhead. While prior work has focused on\noptimizing MoE training and decoder architectures, inference for encoder-based\nMoE models in a multi-GPU with expert parallelism setting remains\nunderexplored. We introduce MoEShard, an inference system that achieves perfect\nload balancing through tensor sharding of MoE experts. Unlike existing\napproaches that rely on heuristic capacity factors or drop tokens, MoEShard\nevenly distributes computation across GPUs and ensures full token retention,\nmaximizing utilization regardless of routing skewness. We achieve this through\na strategic row- and column-wise decomposition of expert matrices. This reduces\nidle time and avoids bottlenecks caused by imbalanced expert assignments.\nFurthermore, MoEShard minimizes kernel launches by fusing decomposed expert\ncomputations, significantly improving throughput. We evaluate MoEShard against\nDeepSpeed on encoder-based architectures, demonstrating speedups of up to\n6.4$\\times$ in time to first token (TTFT). Our results show that tensor\nsharding, when properly applied to experts, is a viable and effective strategy\nfor efficient MoE inference.",
      "tldr_zh": "该论文提出MoEShard系统，通过专家张量分片(Expert Sharding)技术显著加速基于混合专家(MoE)模型的推理效率。系统采用行列分解策略对专家矩阵进行切分，实现跨GPU的完美负载均衡，无需丢弃token或依赖启发式容量因子，解决了传统MoE模型因路由倾斜导致的硬件利用率低下问题。实验表明，相比DeepSpeed方案，MoEShard在基于编码器的MoE架构上实现首token延迟(TTFT)最高6.4倍的加速，同时通过计算内核融合进一步提升了吞吐量。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in the proceedings of the 5th Workshop on Machine Learning\n  and Systems (EuroMLSys 25)",
      "pdf_url": "http://arxiv.org/pdf/2503.08467v1",
      "published_date": "2025-03-11 14:15:01 UTC",
      "updated_date": "2025-03-11 14:15:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:21:55.748568"
    },
    {
      "arxiv_id": "2503.10679v1",
      "title": "End-to-end Learning of Sparse Interventions on Activations to Steer Generation",
      "title_zh": "端到端学习稀疏激活干预以引导生成",
      "authors": [
        "Pau Rodriguez",
        "Michal Klein",
        "Eleonora Gualdoni",
        "Arno Blaas",
        "Luca Zappella",
        "Marco Cuturi",
        "Xavier Suau"
      ],
      "abstract": "The growing use of generative models in daily life calls for efficient\nmechanisms to control their generation, to e.g., produce safe content or\nprovide users with tools to explore style changes. Ideally, such mechanisms\nshould be cheap, both at train and inference time, while preserving output\nquality. Recent research has shown that such mechanisms can be obtained by\nintervening exclusively on model activations, with the goal of correcting\ndistributional differences between activations seen when using prompts from a\nsource vs. a target set (e.g., toxic and non-toxic sentences). While cheap,\nthese fast methods are inherently crude: their maps are tuned locally, not\naccounting for their impact on downstream layers, resulting in interventions\nthat cause unintended shifts when used out-of-sample. We propose in this work\nlinear end-to-end activation steering (LinEAS), an approach trained with a\nglobal loss that accounts simultaneously for all layerwise distributional\nshifts. In addition to being more robust, the loss used to train LinEAS can be\nregularized with sparsifying norms, which can automatically carry out neuron\nand layer selection. Empirically, LinEAS only requires a handful of samples to\nbe effective, and beats similar baselines on toxicity mitigation, while\nperforming on par with far more involved finetuning approaches. We show that\nLinEAS interventions can be composed, study the impact of sparsity on their\nperformance, and showcase applications in text-to-image diffusions.",
      "tldr_zh": "该研究提出了一种端到端的线性激活干预方法（LinEAS），用于高效控制生成模型的输出，例如生成安全内容或探索风格变化。LinEAS通过全局损失函数同时优化所有层的分布偏移，增强了鲁棒性，并利用稀疏化正则化自动选择神经元和层。实验表明，LinEAS仅需少量样本即可有效缓解毒性内容，性能与更复杂的微调方法相当，并展示了其在文本到图像扩散模型中的应用潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10679v1",
      "published_date": "2025-03-11 14:09:04 UTC",
      "updated_date": "2025-03-11 14:09:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:22:11.734228"
    },
    {
      "arxiv_id": "2503.08460v2",
      "title": "Status and Future Prospects of the Standardization Framework Industry 4.0: A European Perspective",
      "title_zh": "工业4.0标准化框架的现状与未来前景：欧洲视角",
      "authors": [
        "Olga Meyer",
        "Marvin Boell",
        "Christoph Legat"
      ],
      "abstract": "The rapid development of Industry 4.0 technologies requires robust and\ncomprehensive standardization to ensure interoperability, safety and efficiency\nin the Industry of the Future. This paper examines the fundamental role and\nfunctionality of standardization, with a particular focus on its importance in\nEurope's regulatory framework. Based on this, selected topics in context of\nstandardization activities in context intelligent manufacturing and digital\ntwins are highlighted and, by that, an overview of the Industry 4.0 standards\nframework is provided. This paper serves both as an informative guide to the\nexisting standards in Industry 4.0 with respect to Artificial Intelligence and\nDigital Twins, and as a call to action for increased cooperation between\nstandardization bodies and the research community. By fostering such\ncollaboration, we aim to facilitate the continued development and\nimplementation of standards that will drive innovation and progress in the\nmanufacturing sector.",
      "tldr_zh": "本文探讨了工业4.0标准化框架的现状与未来前景，特别聚焦于欧洲的监管环境。文章强调了标准化在智能制造和数字孪生等领域的核心作用，并提供了工业4.0标准框架的全面概述。同时，本文呼吁标准化机构与研究界加强合作，以推动人工智能和数字孪生相关标准的持续发展，促进制造业的创新与进步。",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08460v2",
      "published_date": "2025-03-11 14:08:57 UTC",
      "updated_date": "2025-03-12 09:30:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:22:09.914732"
    },
    {
      "arxiv_id": "2503.08455v1",
      "title": "Controlling Latent Diffusion Using Latent CLIP",
      "title_zh": "利用潜在CLIP控制潜在扩散模型",
      "authors": [
        "Jason Becker",
        "Chris Wendler",
        "Peter Baylies",
        "Robert West",
        "Christian Wressnegger"
      ],
      "abstract": "Instead of performing text-conditioned denoising in the image domain, latent\ndiffusion models (LDMs) operate in latent space of a variational autoencoder\n(VAE), enabling more efficient processing at reduced computational costs.\nHowever, while the diffusion process has moved to the latent space, the\ncontrastive language-image pre-training (CLIP) models, as used in many image\nprocessing tasks, still operate in pixel space. Doing so requires costly\nVAE-decoding of latent images before they can be processed. In this paper, we\nintroduce Latent-CLIP, a CLIP model that operates directly in the latent space.\nWe train Latent-CLIP on 2.7B pairs of latent images and descriptive texts, and\nshow that it matches zero-shot classification performance of similarly sized\nCLIP models on both the ImageNet benchmark and a LDM-generated version of it,\ndemonstrating its effectiveness in assessing both real and generated content.\nFurthermore, we construct Latent-CLIP rewards for reward-based noise\noptimization (ReNO) and show that they match the performance of their CLIP\ncounterparts on GenEval and T2I-CompBench while cutting the cost of the total\npipeline by 21%. Finally, we use Latent-CLIP to guide generation away from\nharmful content, achieving strong performance on the inappropriate image\nprompts (I2P) benchmark and a custom evaluation, without ever requiring the\ncostly step of decoding intermediate images.",
      "tldr_zh": "本文提出了Latent-CLIP模型，可直接在VAE潜在空间中运行，避免了传统CLIP模型需要先解码潜在图像的昂贵计算开销。研究通过在27亿组潜在图像-文本对上训练，证明其零样本分类性能与常规CLIP相当，并能同时评估真实和生成内容。实验表明，该方法在保持性能的同时将流程总成本降低21%，并成功应用于有害内容过滤任务，显著提升了潜在扩散模型的安全性和效率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08455v1",
      "published_date": "2025-03-11 14:04:29 UTC",
      "updated_date": "2025-03-11 14:04:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:22:17.023232"
    },
    {
      "arxiv_id": "2503.08437v1",
      "title": "ICPR 2024 Competition on Rider Intention Prediction",
      "title_zh": "ICPR 2024骑手意图预测竞赛",
      "authors": [
        "Shankar Gangisetty",
        "Abdul Wasi",
        "Shyam Nandan Rai",
        "C. V. Jawahar",
        "Sajay Raj",
        "Manish Prajapati",
        "Ayesha Choudhary",
        "Aaryadev Chandra",
        "Dev Chandan",
        "Shireen Chand",
        "Suvaditya Mukherjee"
      ],
      "abstract": "The recent surge in the vehicle market has led to an alarming increase in\nroad accidents. This underscores the critical importance of enhancing road\nsafety measures, particularly for vulnerable road users like motorcyclists.\nHence, we introduce the rider intention prediction (RIP) competition that aims\nto address challenges in rider safety by proactively predicting maneuvers\nbefore they occur, thereby strengthening rider safety. This capability enables\nthe riders to react to the potential incorrect maneuvers flagged by advanced\ndriver assistance systems (ADAS). We collect a new dataset, namely, rider\naction anticipation dataset (RAAD) for the competition consisting of two tasks:\nsingle-view RIP and multi-view RIP. The dataset incorporates a spectrum of\ntraffic conditions and challenging navigational maneuvers on roads with varying\nlighting conditions. For the competition, we received seventy-five\nregistrations and five team submissions for inference of which we compared the\nmethods of the top three performing teams on both the RIP tasks: one\nstate-space model (Mamba2) and two learning-based approaches (SVM and\nCNN-LSTM). The results indicate that the state-space model outperformed the\nother methods across the entire dataset, providing a balanced performance\nacross maneuver classes. The SVM-based RIP method showed the second-best\nperformance when using random sampling and SMOTE. However, the CNN-LSTM method\nunderperformed, primarily due to class imbalance issues, particularly\nstruggling with minority classes. This paper details the proposed RAAD dataset\nand provides a summary of the submissions for the RIP 2024 competition.",
      "tldr_zh": "本文介绍了ICPR 2024年骑手意图预测(RIP)竞赛，旨在通过提前预测骑手的操作来提升道路安全，特别是针对摩托车骑手。竞赛基于新构建的骑手动作预测数据集(RAAD)，包含单视角和多视角RIP任务，涵盖多种交通条件和复杂操作场景。竞赛中，75支队伍注册，5支提交了方案，最终对比了三种方法的性能：状态空间模型(Mamba2)、支持向量机(SVM)和CNN-LSTM模型。结果显示，Mamba2在整体数据集上表现最佳，SVM次之，而CNN-LSTM因类别不平衡问题表现较差。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08437v1",
      "published_date": "2025-03-11 13:50:37 UTC",
      "updated_date": "2025-03-11 13:50:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:22:34.549357"
    },
    {
      "arxiv_id": "2503.08417v1",
      "title": "AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models",
      "title_zh": "AnyMoLe：基于视频扩散模型的任意角色动作中间帧生成",
      "authors": [
        "Kwan Yun",
        "Seokhyeon Hong",
        "Chaelin Kim",
        "Junyong Noh"
      ],
      "abstract": "Despite recent advancements in learning-based motion in-betweening, a key\nlimitation has been overlooked: the requirement for character-specific\ndatasets. In this work, we introduce AnyMoLe, a novel method that addresses\nthis limitation by leveraging video diffusion models to generate motion\nin-between frames for arbitrary characters without external data. Our approach\nemploys a two-stage frame generation process to enhance contextual\nunderstanding. Furthermore, to bridge the domain gap between real-world and\nrendered character animations, we introduce ICAdapt, a fine-tuning technique\nfor video diffusion models. Additionally, we propose a ``motion-video\nmimicking'' optimization technique, enabling seamless motion generation for\ncharacters with arbitrary joint structures using 2D and 3D-aware features.\nAnyMoLe significantly reduces data dependency while generating smooth and\nrealistic transitions, making it applicable to a wide range of motion\nin-betweening tasks.",
      "tldr_zh": "该研究提出AnyMoLe方法，利用视频扩散模型为任意角色生成中间运动帧，无需依赖特定角色的训练数据。该方法采用两阶段帧生成流程增强上下文理解，并创新性地提出ICAdapt微调技术来弥合真实世界与渲染动画间的领域差异。通过\"运动-视频模仿\"优化技术，系统能基于2D和3D特征为任意骨骼结构的角色生成流畅自然的运动过渡，显著降低了对特定数据集的依赖。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "68U05",
        "I.3.7; I.4.9"
      ],
      "primary_category": "cs.GR",
      "comment": "11 pages, 10 figures, CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08417v1",
      "published_date": "2025-03-11 13:28:59 UTC",
      "updated_date": "2025-03-11 13:28:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:22:38.423725"
    },
    {
      "arxiv_id": "2503.14513v1",
      "title": "Synthetic Data Generation of Body Motion Data by Neural Gas Network for Emotion Recognition",
      "title_zh": "基于神经气体网络的人体运动数据生成方法在情感识别中的应用",
      "authors": [
        "Seyed Muhammad Hossein Mousavi"
      ],
      "abstract": "In the domain of emotion recognition using body motion, the primary challenge\nlies in the scarcity of diverse and generalizable datasets. Automatic emotion\nrecognition uses machine learning and artificial intelligence techniques to\nrecognize a person's emotional state from various data types, such as text,\nimages, sound, and body motion. Body motion poses unique challenges as many\nfactors, such as age, gender, ethnicity, personality, and illness, affect its\nappearance, leading to a lack of diverse and robust datasets specifically for\nemotion recognition. To address this, employing Synthetic Data Generation (SDG)\nmethods, such as Generative Adversarial Networks (GANs) and Variational Auto\nEncoders (VAEs), offers potential solutions, though these methods are often\ncomplex. This research introduces a novel application of the Neural Gas Network\n(NGN) algorithm for synthesizing body motion data and optimizing diversity and\ngeneration speed. By learning skeletal structure topology, the NGN fits the\nneurons or gas particles on body joints. Generated gas particles, which form\nthe skeletal structure later on, will be used to synthesize the new body\nposture. By attaching body postures over frames, the final synthetic body\nmotion appears. We compared our generated dataset against others generated by\nGANs, VAEs, and another benchmark algorithm, using benchmark metrics such as\nFr\\'echet Inception Distance (FID), Diversity, and a few more. Furthermore, we\ncontinued evaluation using classification metrics such as accuracy, precision,\nrecall, and a few others. Joint-related features or kinematic parameters were\nextracted, and the system assessed model performance against unseen data. Our\nfindings demonstrate that the NGN algorithm produces more realistic and\nemotionally distinct body motion data and does so with more synthesizing speed\nthan existing methods.",
      "tldr_zh": "该研究提出了一种基于神经气体网络(Neural Gas Network, NGN)的新型身体动作数据合成方法，用于解决情感识别领域数据多样性不足的问题。该方法通过学习骨骼结构拓扑，将气体粒子拟合到身体关节上生成新的身体姿态，最终合成完整的动作序列。实验表明，与GAN、VAE等传统方法相比，NGN算法能生成更具情感区分度的逼真动作数据，且合成速度更快，在FID和多样性等指标上表现更优。该方法通过合成多样化数据有效提升了情感识别模型的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV",
        "A.I"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.14513v1",
      "published_date": "2025-03-11 13:16:30 UTC",
      "updated_date": "2025-03-11 13:16:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:22:46.401886"
    },
    {
      "arxiv_id": "2503.08750v1",
      "title": "Exposing Product Bias in LLM Investment Recommendation",
      "title_zh": "揭露大语言模型投资推荐中的产品偏见",
      "authors": [
        "Yuhan Zhi",
        "Xiaoyu Zhang",
        "Longtian Wang",
        "Shumin Jiang",
        "Shiqing Ma",
        "Xiaohong Guan",
        "Chao Shen"
      ],
      "abstract": "Large language models (LLMs), as a new generation of recommendation engines,\npossess powerful summarization and data analysis capabilities, surpassing\ntraditional recommendation systems in both scope and performance. One promising\napplication is investment recommendation. In this paper, we reveal a novel\nproduct bias in LLM investment recommendation, where LLMs exhibit systematic\npreferences for specific products. Such preferences can subtly influence user\ninvestment decisions, potentially leading to inflated valuations of products\nand financial bubbles, posing risks to both individual investors and market\nstability. To comprehensively study the product bias, we develop an automated\npipeline to create a dataset of 567,000 samples across five asset classes\n(stocks, mutual funds, cryptocurrencies, savings, and portfolios). With this\ndataset, we present the bf first study on product bias in LLM investment\nrecommendations. Our findings reveal that LLMs exhibit clear product\npreferences, such as certain stocks (e.g., `AAPL' from Apple and `MSFT' from\nMicrosoft). Notably, this bias persists even after applying debiasing\ntechniques. We urge AI researchers to take heed of the product bias in LLM\ninvestment recommendations and its implications, ensuring fairness and security\nin the digital space and market.",
      "tldr_zh": "该研究揭示了大型语言模型(LLMs)在投资推荐中存在的新型\"产品偏见\"(product bias)，即模型对特定产品(如苹果AAPL、微软MSFT等股票)表现系统性偏好。研究者构建了包含56.7万样本、覆盖五大资产类别的数据集，首次系统分析了LLM投资推荐中的这种偏见。研究发现，即使经过去偏处理，这种可能导致市场估值虚高的偏好仍然持续存在，对投资者和市场稳定性构成潜在风险。论文呼吁AI研究者重视此类偏见的公平性和安全性影响。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08750v1",
      "published_date": "2025-03-11 13:10:00 UTC",
      "updated_date": "2025-03-11 13:10:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:23:33.907631"
    },
    {
      "arxiv_id": "2503.08749v1",
      "title": "Source-free domain adaptation based on label reliability for cross-domain bearing fault diagnosis",
      "title_zh": "基于标签可靠性的无源域自适应方法在跨域轴承故障诊断中的应用",
      "authors": [
        "Wenyi Wu",
        "Hao Zhang",
        "Zhisen Wei",
        "Xiao-Yuan Jing",
        "Qinghua Zhang",
        "Songsong Wu"
      ],
      "abstract": "Source-free domain adaptation (SFDA) has been exploited for cross-domain\nbearing fault diagnosis without access to source data. Current methods select\npartial target samples with reliable pseudo-labels for model adaptation, which\nis sub-optimal due to the ignored target samples. We argue that every target\nsample can contribute to model adaptation, and accordingly propose in this\npaper a novel SFDA-based approach for bearing fault diagnosis that exploits\nboth reliable and unreliable pseudo-labels. We develop a\ndata-augmentation-based label voting strategy to divide the target samples into\nreliable and unreliable ones. We propose to explore the underlying relation\nbetween feature space and label space by using the reliable pseudo-labels as\nground-truth labels, meanwhile, alleviating negative transfer by maximizing the\nentropy of the unreliable pseudo-labels. The proposed method achieves\nwell-balance between discriminability and diversity by taking advantage of\nreliable and unreliable pseudo-labels. Extensive experiments are conducted on\ntwo bearing fault benchmarks, demonstrating that our approach achieves\nsignificant performance improvements against existing SFDA-based bearing fault\ndiagnosis methods. Our code is available at https://github.com/BdLab405/SDALR.",
      "tldr_zh": "本文提出了一种基于标签可靠性的无源域适应(SFDA)方法，用于跨域轴承故障诊断。该方法通过数据增强的标签投票策略将目标样本分为可靠和不可靠的伪标签，并利用可靠伪标签作为真实标签探索特征空间与标签空间的关系，同时通过最大化不可靠伪标签的熵来减轻负迁移。实验结果表明，该方法在两个轴承故障诊断基准测试中显著优于现有的SFDA方法，实现了判别性与多样性的良好平衡。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 12 figures and 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.08749v1",
      "published_date": "2025-03-11 13:02:18 UTC",
      "updated_date": "2025-03-11 13:02:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:23:02.199682"
    },
    {
      "arxiv_id": "2503.08388v1",
      "title": "V-Max: Making RL practical for Autonomous Driving",
      "title_zh": "V-Max：让强化学习在自动驾驶中实用化",
      "authors": [
        "Valentin Charraut",
        "Thomas Tournaire",
        "Waël Doulazmi",
        "Thibault Buhet"
      ],
      "abstract": "Learning-based decision-making has the potential to enable generalizable\nAutonomous Driving (AD) policies, reducing the engineering overhead of\nrule-based approaches. Imitation Learning (IL) remains the dominant paradigm,\nbenefiting from large-scale human demonstration datasets, but it suffers from\ninherent limitations such as distribution shift and imitation gaps.\nReinforcement Learning (RL) presents a promising alternative, yet its adoption\nin AD remains limited due to the lack of standardized and efficient research\nframeworks. To this end, we introduce V-Max, an open research framework\nproviding all the necessary tools to make RL practical for AD. V-Max is built\non Waymax, a hardware-accelerated AD simulator designed for large-scale\nexperimentation. We extend it using ScenarioNet's approach, enabling the fast\nsimulation of diverse AD datasets. V-Max integrates a set of observation and\nreward functions, transformer-based encoders, and training pipelines.\nAdditionally, it includes adversarial evaluation settings and an extensive set\nof evaluation metrics. Through a large-scale benchmark, we analyze how network\narchitectures, observation functions, training data, and reward shaping impact\nRL performance.",
      "tldr_zh": "该研究提出了V-Max框架，旨在解决强化学习(RL)在自动驾驶(AD)领域应用受限的问题。该框架基于硬件加速的Waymax仿真器构建，整合了多样化观测/奖励函数、Transformer编码器和训练流程，支持大规模AD数据集快速仿真。研究表明，V-Max通过系统分析网络架构、观测函数和奖励机制等因素，为RL在AD中的实际应用提供了标准化研究平台和评估体系。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08388v1",
      "published_date": "2025-03-11 12:53:24 UTC",
      "updated_date": "2025-03-11 12:53:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:23:43.568286"
    },
    {
      "arxiv_id": "2503.08381v1",
      "title": "InfluenceNet: AI Models for Banzhaf and Shapley Value Prediction",
      "title_zh": "InfluenceNet：用于 Banzhaf 和 Shapley 值预测的 AI 模型",
      "authors": [
        "Benjamin Kempinski",
        "Tal Kachman"
      ],
      "abstract": "Power indices are essential in assessing the contribution and influence of\nindividual agents in multi-agent systems, providing crucial insights into\ncollaborative dynamics and decision-making processes. While invaluable,\ntraditional computational methods for exact or estimated power indices values\nrequire significant time and computational constraints, especially for large\n$(n\\ge10)$ coalitions. These constraints have historically limited researchers'\nability to analyse complex multi-agent interactions comprehensively. To address\nthis limitation, we introduce a novel Neural Networks-based approach that\nefficiently estimates power indices for voting games, demonstrating comparable\nand often superiour performance to existing tools in terms of both speed and\naccuracy. This method not only addresses existing computational bottlenecks,\nbut also enables rapid analysis of large coalitions, opening new avenues for\nmulti-agent system research by overcoming previous computational limitations\nand providing researchers with a more accessible, scalable analytical tool.This\nincreased efficiency will allow for the analysis of more complex and realistic\nmulti-agent scenarios.",
      "tldr_zh": "该研究提出InfluenceNet，一种基于神经网络的方法，用于高效预测Banzhaf和Shapley值这两种关键的多智能体系统权力指数。相比传统计算方法，该模型在大型联盟(n≥10)分析中展现出更快的速度和更高的准确性，解决了现有计算瓶颈。该方法为复杂多智能体交互研究提供了可扩展的分析工具，使研究人员能够分析更复杂、更现实的协作场景。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "I.2; F.2.1"
      ],
      "primary_category": "cs.MA",
      "comment": "20 pages main text + 6 pages appendix, 11 figures. Accepted to\n  IntelliSys 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08381v1",
      "published_date": "2025-03-11 12:40:42 UTC",
      "updated_date": "2025-03-11 12:40:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:23:46.675550"
    },
    {
      "arxiv_id": "2503.08354v2",
      "title": "Robust Latent Matters: Boosting Image Generation with Sampling Error Synthesis",
      "title_zh": "鲁棒潜在空间：通过采样误差合成提升图像生成",
      "authors": [
        "Kai Qiu",
        "Xiang Li",
        "Jason Kuen",
        "Hao Chen",
        "Xiaohao Xu",
        "Jiuxiang Gu",
        "Yinyi Luo",
        "Bhiksha Raj",
        "Zhe Lin",
        "Marios Savvides"
      ],
      "abstract": "Recent image generation schemes typically capture image distribution in a\npre-constructed latent space relying on a frozen image tokenizer. Though the\nperformance of tokenizer plays an essential role to the successful generation,\nits current evaluation metrics (e.g. rFID) fail to precisely assess the\ntokenizer and correlate its performance to the generation quality (e.g. gFID).\nIn this paper, we comprehensively analyze the reason for the discrepancy of\nreconstruction and generation qualities in a discrete latent space, and, from\nwhich, we propose a novel plug-and-play tokenizer training scheme to facilitate\nlatent space construction. Specifically, a latent perturbation approach is\nproposed to simulate sampling noises, i.e., the unexpected tokens sampled, from\nthe generative process. With the latent perturbation, we further propose (1) a\nnovel tokenizer evaluation metric, i.e., pFID, which successfully correlates\nthe tokenizer performance to generation quality and (2) a plug-and-play\ntokenizer training scheme, which significantly enhances the robustness of\ntokenizer thus boosting the generation quality and convergence speed. Extensive\nbenchmarking are conducted with 11 advanced discrete image tokenizers with 2\nautoregressive generation models to validate our approach. The tokenizer\ntrained with our proposed latent perturbation achieve a notable 1.60 gFID with\nclassifier-free guidance (CFG) and 3.45 gFID without CFG with a $\\sim$400M\ngenerator. Code: https://github.com/lxa9867/ImageFolder.",
      "tldr_zh": "本文提出了一种通过采样误差合成提升图像生成质量的新方法。研究发现当前基于离散潜空间的图像生成方法存在重建质量与生成质量不一致的问题，并提出了一种可插拔的tokenizer训练方案。核心创新包括：(1) 提出潜空间扰动方法模拟生成过程中的采样噪声；(2) 设计新的评估指标pFID，能准确反映tokenizer性能与生成质量的相关性。实验表明，该方法在400M参数的生成器上实现了1.60(带CFG)和3.45(无CFG)的gFID指标，显著提升了生成质量和收敛速度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 13 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.08354v2",
      "published_date": "2025-03-11 12:09:11 UTC",
      "updated_date": "2025-03-17 17:54:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:24:47.521136"
    },
    {
      "arxiv_id": "2503.08332v1",
      "title": "MINT-Demo: Membership Inference Test Demonstrator",
      "title_zh": "MINT-Demo：成员推理测试演示器",
      "authors": [
        "Daniel DeAlcala",
        "Aythami Morales",
        "Julian Fierrez",
        "Gonzalo Mancera",
        "Ruben Tolosana",
        "Ruben Vera-Rodriguez"
      ],
      "abstract": "We present the Membership Inference Test Demonstrator, to emphasize the need\nfor more transparent machine learning training processes. MINT is a technique\nfor experimentally determining whether certain data has been used during the\ntraining of machine learning models. We conduct experiments with popular face\nrecognition models and 5 public databases containing over 22M images. Promising\nresults, up to 89% accuracy are achieved, suggesting that it is possible to\nrecognize if an AI model has been trained with specific data. Finally, we\npresent a MINT platform as demonstrator of this technology aimed to promote\ntransparency in AI training.",
      "tldr_zh": "该研究提出了MINT-Demo（Membership Inference Test Demonstrator），一种用于检测特定数据是否被用于机器学习模型训练的技术。通过在人脸识别模型和包含超过2200万张图像的5个公共数据库上进行实验，MINT-Demo实现了高达89%的准确率，证明了识别AI模型是否使用特定数据训练的可能性。研究还开发了一个MINT平台，旨在促进AI训练过程的透明度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Demo Paper Presented at Demo Track CVPR 24' and at AAAI 25' AIGOV\n  workshop",
      "pdf_url": "http://arxiv.org/pdf/2503.08332v1",
      "published_date": "2025-03-11 11:45:05 UTC",
      "updated_date": "2025-03-11 11:45:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:23:40.444504"
    },
    {
      "arxiv_id": "2503.08327v1",
      "title": "Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation",
      "title_zh": "为薄荷添加巧克力：缓解机器翻译中的指标干扰问题",
      "authors": [
        "José Pombal",
        "Nuno M. Guerreiro",
        "Ricardo Rei",
        "André F. T. Martins"
      ],
      "abstract": "As automatic metrics become increasingly stronger and widely adopted, the\nrisk of unintentionally \"gaming the metric\" during model development rises.\nThis issue is caused by metric interference (Mint), i.e., the use of the same\nor related metrics for both model tuning and evaluation. Mint can misguide\npractitioners into being overoptimistic about the performance of their systems:\nas system outputs become a function of the interfering metric, their estimated\nquality loses correlation with human judgments. In this work, we analyze two\ncommon cases of Mint in machine translation-related tasks: filtering of\ntraining data, and decoding with quality signals. Importantly, we find that\nMint strongly distorts instance-level metric scores, even when metrics are not\ndirectly optimized for -- questioning the common strategy of leveraging a\ndifferent, yet related metric for evaluation that is not used for tuning. To\naddress this problem, we propose MintAdjust, a method for more reliable\nevaluation under Mint. On the WMT24 MT shared task test set, MintAdjust ranks\ntranslations and systems more accurately than state-of-the-art-metrics across a\nmajority of language pairs, especially for high-quality systems. Furthermore,\nMintAdjust outperforms AutoRank, the ensembling method used by the organizers.",
      "tldr_zh": "该研究针对机器翻译中自动评估指标干扰（Mint）问题展开分析，指出使用相同或相关指标进行模型调优和评估会导致系统性能被高估，与人类判断脱节。研究提出MintAdjust方法，通过更可靠的评估机制缓解Mint影响。实验表明，在WMT24机器翻译任务中，MintAdjust在大多数语言对上比现有最先进指标和AutoRank集成方法更能准确评估翻译质量和系统性能，尤其对高质量系统表现更优。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08327v1",
      "published_date": "2025-03-11 11:40:10 UTC",
      "updated_date": "2025-03-11 11:40:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:23:45.854065"
    },
    {
      "arxiv_id": "2503.08325v1",
      "title": "Prototype-based Heterogeneous Federated Learning for Blade Icing Detection in Wind Turbines with Class Imbalanced Data",
      "title_zh": "基于原型的异构联邦学习在类别不平衡数据下的风力涡轮机叶片结冰检测",
      "authors": [
        "Lele Qi",
        "Mengna Liu",
        "Xu Cheng",
        "Fan Shi",
        "Xiufeng Liu",
        "Shengyong Chen"
      ],
      "abstract": "Wind farms, typically in high-latitude regions, face a high risk of blade\nicing. Traditional centralized training methods raise serious privacy concerns.\nTo enhance data privacy in detecting wind turbine blade icing, traditional\nfederated learning (FL) is employed. However, data heterogeneity, resulting\nfrom collections across wind farms in varying environmental conditions, impacts\nthe model's optimization capabilities. Moreover, imbalances in wind turbine\ndata lead to models that tend to favor recognizing majority classes, thus\nneglecting critical icing anomalies. To tackle these challenges, we propose a\nfederated prototype learning model for class-imbalanced data in heterogeneous\nenvironments to detect wind turbine blade icing. We also propose a contrastive\nsupervised loss function to address the class imbalance problem. Experiments on\nreal data from 20 turbines across two wind farms show our method outperforms\nfive FL models and five class imbalance methods, with an average improvement of\n19.64\\% in \\( mF_{\\beta} \\) and 5.73\\% in \\( m \\)BA compared to the second-best\nmethod, BiFL.",
      "tldr_zh": "该研究提出了一种基于原型学习的异构联邦学习模型，用于解决风力涡轮机叶片结冰检测中的类别不平衡问题。针对不同风电场环境导致的数据异构性以及结冰异常数据稀缺的挑战，研究设计了一种对比监督损失函数，有效提升了模型对少数类（结冰异常）的识别能力。实验结果表明，该模型在20台风力涡轮机的真实数据上表现优异，在 \\( mF_{\\beta} \\) 和 \\( m \\)BA 指标上分别比次优方法 BiFL 提升了19.64%和5.73%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08325v1",
      "published_date": "2025-03-11 11:37:43 UTC",
      "updated_date": "2025-03-11 11:37:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:25:19.694587"
    },
    {
      "arxiv_id": "2503.08322v1",
      "title": "Evaluating Interpretable Reinforcement Learning by Distilling Policies into Programs",
      "title_zh": "评估可解释强化学习：通过将策略提炼为程序",
      "authors": [
        "Hector Kohler",
        "Quentin Delfosse",
        "Waris Radji",
        "Riad Akrour",
        "Philippe Preux"
      ],
      "abstract": "There exist applications of reinforcement learning like medicine where\npolicies need to be ''interpretable'' by humans. User studies have shown that\nsome policy classes might be more interpretable than others. However, it is\ncostly to conduct human studies of policy interpretability. Furthermore, there\nis no clear definition of policy interpretabiliy, i.e., no clear metrics for\ninterpretability and thus claims depend on the chosen definition. We tackle the\nproblem of empirically evaluating policies interpretability without humans.\nDespite this lack of clear definition, researchers agree on the notions of\n''simulatability'': policy interpretability should relate to how humans\nunderstand policy actions given states. To advance research in interpretable\nreinforcement learning, we contribute a new methodology to evaluate policy\ninterpretability. This new methodology relies on proxies for simulatability\nthat we use to conduct a large-scale empirical evaluation of policy\ninterpretability. We use imitation learning to compute baseline policies by\ndistilling expert neural networks into small programs. We then show that using\nour methodology to evaluate the baselines interpretability leads to similar\nconclusions as user studies. We show that increasing interpretability does not\nnecessarily reduce performances and can sometimes increase them. We also show\nthat there is no policy class that better trades off interpretability and\nperformance across tasks making it necessary for researcher to have\nmethodologies for comparing policies interpretability.",
      "tldr_zh": "该研究针对强化学习在医疗等需要可解释性的应用场景，提出了一种无需人工参与的策略可解释性评估新方法。通过将专家神经网络蒸馏为小型程序作为基线策略，研究者建立了基于\"可模拟性\"概念的可解释性代理指标，大规模实证评估表明该方法与人工研究结论一致。研究发现：提升可解释性并不必然降低性能，且不同策略类别在可解释性与性能的权衡上存在任务依赖性，凸显了开发标准化评估方法的必要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages of main text, under review",
      "pdf_url": "http://arxiv.org/pdf/2503.08322v1",
      "published_date": "2025-03-11 11:34:06 UTC",
      "updated_date": "2025-03-11 11:34:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:25:15.878640"
    },
    {
      "arxiv_id": "2503.08308v1",
      "title": "Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with an Uncertainty-Aware Agentic Framework",
      "title_zh": "《置信感知的视觉推理：基于不确定性认知的智能框架赋能多模态大语言模型》",
      "authors": [
        "Zhuo Zhi",
        "Chen Feng",
        "Adam Daneshmend",
        "Mine Orlu",
        "Andreas Demosthenous",
        "Lu Yin",
        "Da Li",
        "Ziquan Liu",
        "Miguel R. D. Rodrigues"
      ],
      "abstract": "Multimodal large language models (MLLMs) show promise in tasks like visual\nquestion answering (VQA) but still face challenges in multimodal reasoning.\nRecent works adapt agentic frameworks or chain-of-thought (CoT) reasoning to\nimprove performance. However, CoT-based multimodal reasoning often demands\ncostly data annotation and fine-tuning, while agentic approaches relying on\nexternal tools risk introducing unreliable output from these tools. In this\npaper, we propose Seeing and Reasoning with Confidence (SRICE), a training-free\nmultimodal reasoning framework that integrates external vision models with\nuncertainty quantification (UQ) into an MLLM to address these challenges.\nSpecifically, SRICE guides the inference process by allowing MLLM to\nautonomously select regions of interest through multi-stage interactions with\nthe help of external tools. We propose to use a conformal prediction-based\napproach to calibrate the output of external tools and select the optimal tool\nby estimating the uncertainty of an MLLM's output. Our experiment shows that\nthe average improvement of SRICE over the base MLLM is 4.6% on five datasets\nand the performance on some datasets even outperforms fine-tuning-based\nmethods, revealing the significance of ensuring reliable tool use in an MLLM\nagent.",
      "tldr_zh": "该研究提出了SRICE框架，一种无需训练的多模态推理方法，旨在解决多模态大语言模型(MLLMs)在视觉问答(VQA)等任务中的可靠性问题。SRICE通过引入不确定性量化(UQ)技术，结合外部视觉工具，使MLLM能够自主选择感兴趣区域并校准工具输出，从而优化推理过程。实验表明，SRICE在五个数据集上平均比基线模型提升了4.6%，部分数据集表现甚至优于基于微调的方法，验证了确保工具可靠性的重要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08308v1",
      "published_date": "2025-03-11 11:18:53 UTC",
      "updated_date": "2025-03-11 11:18:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:25:25.356796"
    },
    {
      "arxiv_id": "2503.08302v1",
      "title": "General-Purpose Aerial Intelligent Agents Empowered by Large Language Models",
      "title_zh": "由大语言模型驱动的通用型空中智能体",
      "authors": [
        "Ji Zhao",
        "Xiao Lin"
      ],
      "abstract": "The emergence of large language models (LLMs) opens new frontiers for\nunmanned aerial vehicle (UAVs), yet existing systems remain confined to\npredefined tasks due to hardware-software co-design challenges. This paper\npresents the first aerial intelligent agent capable of open-world task\nexecution through tight integration of LLM-based reasoning and robotic\nautonomy. Our hardware-software co-designed system addresses two fundamental\nlimitations: (1) Onboard LLM operation via an edge-optimized computing\nplatform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W\npeak power; (2) A bidirectional cognitive architecture that synergizes slow\ndeliberative planning (LLM task planning) with fast reactive control (state\nestimation, mapping, obstacle avoidance, and motion planning). Validated\nthrough preliminary results using our prototype, the system demonstrates\nreliable task planning and scene understanding in communication-constrained\nenvironments, such as sugarcane monitoring, power grid inspection, mine tunnel\nexploration, and biological observation applications. This work establishes a\nnovel framework for embodied aerial artificial intelligence, bridging the gap\nbetween task planning and robotic autonomy in open environments.",
      "tldr_zh": "该研究提出了首个基于大语言模型（LLM）的通用空中智能体框架，通过软硬件协同设计突破了无人机（UAV）只能执行预设任务的局限。系统采用边缘优化计算平台实现14B参数LLM的机载部署（5-6 tokens/sec推理速度/220W峰值功耗），并创新性地将慢速决策规划（LLM任务规划）与快速反应控制（状态估计、避障等）通过双向认知架构融合。原型验证表明，该系统在甘蔗监测、电网巡检等通信受限场景中展现出可靠的任务规划与环境理解能力，为开放环境下的具身空中人工智能建立了新范式。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08302v1",
      "published_date": "2025-03-11 11:13:58 UTC",
      "updated_date": "2025-03-11 11:13:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:25:27.589916"
    },
    {
      "arxiv_id": "2503.08301v2",
      "title": "Large Language Model as Meta-Surrogate for Data-Driven Many-Task Optimization: A Proof-of-Principle Study",
      "title_zh": "基于大语言模型的元代理框架：数据驱动多任务优化的原理验证研究",
      "authors": [
        "Xian-Rong Zhang",
        "Yue-Jiao Gong",
        "Jun Zhang"
      ],
      "abstract": "In many-task optimization scenarios, surrogate models are valuable for\nmitigating the computational burden of repeated fitness evaluations across\ntasks. This study proposes a novel meta-surrogate framework to assist many-task\noptimization, by leveraging the knowledge transfer strengths and emergent\ncapabilities of large language models (LLMs). We formulate a unified framework\nfor many-task fitness prediction, by defining a universal model with metadata\nto fit a group of problems. Fitness prediction is performed on metadata and\ndecision variables, enabling efficient knowledge sharing across tasks and\nadaptability to new tasks. The LLM-based meta-surrogate treats fitness\nprediction as conditional probability estimation, employing a unified token\nsequence representation for task metadata, inputs, and outputs. This approach\nfacilitates efficient inter-task knowledge sharing through shared token\nembeddings and captures complex task dependencies via multi-task model\ntraining. Experimental results demonstrate the model's emergent generalization\nability, including zero-shot performance on problems with unseen dimensions.\nWhen integrated into evolutionary transfer optimization (ETO), our framework\nsupports dual-level knowledge transfer -- at both the surrogate and individual\nlevels -- enhancing optimization efficiency and robustness. This work\nestablishes a novel foundation for applying LLMs in surrogate modeling,\noffering a versatile solution for many-task optimization.",
      "tldr_zh": "本研究提出了一种基于大语言模型(LLM)的元代理框架，用于解决多任务优化问题中的计算瓶颈。该方法创新性地将LLM作为元代理模型，通过统一的任务元数据表示和条件概率估计方法，实现了跨任务的知识共享和适应新任务的泛化能力。实验表明，该框架不仅支持零样本(zero-shot)处理未见维度问题，还能在进化迁移优化(ETO)中实现代理层和个体层的双重知识迁移，显著提升了优化效率和鲁棒性。这项工作为LLM在代理建模领域的应用开辟了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.08301v2",
      "published_date": "2025-03-11 11:13:11 UTC",
      "updated_date": "2025-03-12 06:00:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:25:47.623748"
    },
    {
      "arxiv_id": "2503.08295v1",
      "title": "D3PO: Preference-Based Alignment of Discrete Diffusion Models",
      "title_zh": "D3PO：基于偏好的离散扩散模型对齐方法",
      "authors": [
        "Umberto Borso",
        "Davide Paglieri",
        "Jude Wells",
        "Tim Rocktäschel"
      ],
      "abstract": "Diffusion models have achieved state-of-the-art performance across multiple\ndomains, with recent advancements extending their applicability to discrete\ndata. However, aligning discrete diffusion models with task-specific\npreferences remains challenging, particularly in scenarios where explicit\nreward functions are unavailable. In this work, we introduce Discrete Diffusion\nDPO (D3PO), the first adaptation of Direct Preference Optimization (DPO) to\ndiscrete diffusion models formulated as continuous-time Markov chains. Our\napproach derives a novel loss function that directly fine-tunes the generative\nprocess using preference data while preserving fidelity to a reference\ndistribution. We validate D3PO on a structured binary sequence generation task,\ndemonstrating that the method effectively aligns model outputs with preferences\nwhile maintaining structural validity. Our results highlight that D3PO enables\ncontrolled fine-tuning without requiring explicit reward models, making it a\npractical alternative to reinforcement learning-based approaches. Future\nresearch will explore extending D3PO to more complex generative tasks,\nincluding language modeling and protein sequence generation, as well as\ninvestigating alternative noise schedules, such as uniform noising, to enhance\nflexibility across different applications.",
      "tldr_zh": "该研究提出了D3PO，首次将直接偏好优化(DPO)应用于离散扩散模型，解决了在缺乏显式奖励函数情况下对齐任务特定偏好的挑战。D3PO通过推导新的损失函数，直接利用偏好数据微调生成过程，同时保持对参考分布的保真度。实验表明，D3PO在结构化二进制序列生成任务中有效对齐模型输出与偏好，并保持结构有效性，为无需显式奖励模型的受控微调提供了实用方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08295v1",
      "published_date": "2025-03-11 11:07:35 UTC",
      "updated_date": "2025-03-11 11:07:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:25:41.552769"
    },
    {
      "arxiv_id": "2503.08292v1",
      "title": "Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges",
      "title_zh": "大型语言模型在门诊转诊中的应用：问题定义、基准测试与挑战",
      "authors": [
        "Xiaoxiao Liu",
        "Qingying Xiao",
        "Junying Chen",
        "Xiangyi Feng",
        "Xiangbo Wu",
        "Bairui Zhang",
        "Xiang Wan",
        "Jian Chang",
        "Guangjun Yu",
        "Yan Hu",
        "Benyou Wang"
      ],
      "abstract": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)在门诊转诊(IOR)系统中的应用，提出了一个包含静态和动态评估的标准化框架。静态评估关注预定义转诊任务的能力，而动态评估则通过迭代对话测试转诊建议的优化能力。研究发现，LLMs在交互式对话中提问效果良好，但在整体性能上较BERT类模型优势有限。该研究为LLMs在门诊转诊领域的应用提供了系统的评估方法和洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08292v1",
      "published_date": "2025-03-11 11:05:42 UTC",
      "updated_date": "2025-03-11 11:05:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:25:48.008644"
    },
    {
      "arxiv_id": "2503.08280v1",
      "title": "OminiControl2: Efficient Conditioning for Diffusion Transformers",
      "title_zh": "OminiControl2：扩散变压器的高效条件调控框架",
      "authors": [
        "Zhenxiong Tan",
        "Qiaochu Xue",
        "Xingyi Yang",
        "Songhua Liu",
        "Xinchao Wang"
      ],
      "abstract": "Fine-grained control of text-to-image diffusion transformer models (DiT)\nremains a critical challenge for practical deployment. While recent advances\nsuch as OminiControl and others have enabled a controllable generation of\ndiverse control signals, these methods face significant computational\ninefficiency when handling long conditional inputs. We present OminiControl2,\nan efficient framework that achieves efficient image-conditional image\ngeneration. OminiControl2 introduces two key innovations: (1) a dynamic\ncompression strategy that streamlines conditional inputs by preserving only the\nmost semantically relevant tokens during generation, and (2) a conditional\nfeature reuse mechanism that computes condition token features only once and\nreuses them across denoising steps. These architectural improvements preserve\nthe original framework's parameter efficiency and multi-modal versatility while\ndramatically reducing computational costs. Our experiments demonstrate that\nOminiControl2 reduces conditional processing overhead by over 90% compared to\nits predecessor, achieving an overall 5.9$\\times$ speedup in multi-conditional\ngeneration scenarios. This efficiency enables the practical implementation of\ncomplex, multi-modal control for high-quality image synthesis with DiT models.",
      "tldr_zh": "该研究提出OminiControl2框架，旨在解决扩散变换器(DiT)模型在多条件图像生成中的计算效率问题。通过动态压缩策略和条件特征重用机制两项创新，该框架能保留关键语义信息的同时显著降低计算成本。实验表明，OminiControl2相比前代模型减少90%以上的条件处理开销，在多重条件生成场景中实现5.9倍的加速，为高质量DiT图像合成提供了实用化解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08280v1",
      "published_date": "2025-03-11 10:50:14 UTC",
      "updated_date": "2025-03-11 10:50:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:26:04.869071"
    },
    {
      "arxiv_id": "2503.08748v3",
      "title": "Mirror Descent and Novel Exponentiated Gradient Algorithms Using Trace-Form Entropies and Deformed Logarithms",
      "title_zh": "镜像下降与基于迹形式熵及变形对数的新型指数梯度算法",
      "authors": [
        "Andrzej Cichocki",
        "Toshihisa Tanaka",
        "Sergio Cruces"
      ],
      "abstract": "In this paper we propose and investigate a wide class of Mirror Descent\nupdates (MD) and associated novel Generalized Exponentiated Gradient (GEG)\nalgorithms by exploiting various trace-form entropies and associated deformed\nlogarithms and their inverses - deformed (generalized) exponential functions.\nThe proposed algorithms can be considered as extension of entropic MD and\ngeneralization of multiplicative updates. In the literature, there exist\nnowadays over fifty mathematically well defined generalized entropies, so\nimpossible to exploit all of them in one research paper. So we focus on a few\nselected most popular entropies and associated logarithms like the Tsallis,\nKaniadakis and Sharma-Taneja-Mittal and some of their extension like Tempesta\nor Kaniadakis-Scarfone entropies. The shape and properties of the deformed\nlogarithms and their inverses are tuned by one or more hyperparameters. By\nlearning these hyperparameters, we can adapt to distribution of training data,\nwhich can be designed to the specific geometry of the optimization problem,\nleading to potentially faster convergence and better performance. The using\ngeneralized entropies and associated deformed logarithms in the Bregman\ndivergence, used as a regularization term, provides some new insight into\nexponentiated gradient descent updates.",
      "tldr_zh": "该论文提出并研究了一类基于迹形式熵和变形对数的Mirror Descent更新方法及新型广义指数梯度算法（GEG）。这些算法被视为熵MD的扩展和乘法更新的泛化，重点关注Tsallis、Kaniadakis和Sharma-Taneja-Mittal等熵及其变形对数。通过调整变形对数的超参数，算法能够适应训练数据的分布，优化问题的几何特性，从而可能实现更快的收敛和更好的性能。研究还揭示了在Bregman散度中使用广义熵和变形对数对指数梯度下降更新的新见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08748v3",
      "published_date": "2025-03-11 10:50:07 UTC",
      "updated_date": "2025-03-21 02:07:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:26:04.550067"
    },
    {
      "arxiv_id": "2503.08275v1",
      "title": "Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models",
      "title_zh": "超越大纲：基于语言模型的异构递归规划实现自适应长文写作",
      "authors": [
        "Ruibin Xiong",
        "Yimeng Chen",
        "Dmitrii Khizbullin",
        "Jürgen Schmidhuber"
      ],
      "abstract": "Long-form writing agents require flexible integration and interaction across\ninformation retrieval, reasoning, and composition. Current approaches rely on\npredetermined workflows and rigid thinking patterns to generate outlines before\nwriting, resulting in constrained adaptability during writing. In this paper we\npropose a general agent framework that achieves human-like adaptive writing\nthrough recursive task decomposition and dynamic integration of three\nfundamental task types, i.e. retrieval, reasoning, and composition. Our\nmethodology features: 1) a planning mechanism that interleaves recursive task\ndecomposition and execution, eliminating artificial restrictions on writing\nworkflow; and 2) integration of task types that facilitates heterogeneous task\ndecomposition. Evaluations on both fiction writing and technical report\ngeneration show that our method consistently outperforms state-of-the-art\napproaches across all automatic evaluation metrics, which demonstrate the\neffectiveness and broad applicability of our proposed framework.",
      "tldr_zh": "该研究提出了一个新型语言模型写作框架，突破了传统基于大纲的固定写作模式。通过递归任务分解方法（recursive task decomposition），该系统动态整合信息检索（retrieval）、推理（reasoning）和文本生成（composition）三大核心功能，实现类人的自适应长文本写作。实验表明，该框架在小说和技术报告创作中均优于现有最优方法，其创新性的异构任务分解机制（heterogeneous task decomposition）显著提升了写作流程的灵活性和适应性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "29 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08275v1",
      "published_date": "2025-03-11 10:43:01 UTC",
      "updated_date": "2025-03-11 10:43:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:26:24.010936"
    },
    {
      "arxiv_id": "2503.08269v1",
      "title": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks",
      "title_zh": "Adv-CPG：融合面部对抗攻击的定制化肖像生成框架",
      "authors": [
        "Junying Wang",
        "Hongyuan Zhang",
        "Yuan Yuan"
      ],
      "abstract": "Recent Customized Portrait Generation (CPG) methods, taking a facial image\nand a textual prompt as inputs, have attracted substantial attention. Although\nthese methods generate high-fidelity portraits, they fail to prevent the\ngenerated portraits from being tracked and misused by malicious face\nrecognition systems. To address this, this paper proposes a Customized Portrait\nGeneration framework with facial Adversarial attacks (Adv-CPG). Specifically,\nto achieve facial privacy protection, we devise a lightweight local ID\nencryptor and an encryption enhancer. They implement progressive double-layer\nencryption protection by directly injecting the target identity and adding\nadditional identity guidance, respectively. Furthermore, to accomplish\nfine-grained and personalized portrait generation, we develop a multi-modal\nimage customizer capable of generating controlled fine-grained facial features.\nTo the best of our knowledge, Adv-CPG is the first study that introduces facial\nadversarial attacks into CPG. Extensive experiments demonstrate the superiority\nof Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is\n28.1% and 2.86% higher compared to the SOTA noise-based attack methods and\nunconstrained attack methods, respectively.",
      "tldr_zh": "本研究提出了一种结合面部对抗攻击的自定义肖像生成框架Adv-CPG，旨在解决现有方法生成的高保真肖像容易被恶意面部识别系统追踪和滥用的问题。该框架通过轻量级本地ID加密器和加密增强器实现双层加密保护，同时采用多模态图像定制器实现精细化和个性化肖像生成。实验表明，Adv-CPG在攻击成功率上显著优于现有方法，为面部隐私保护提供了新的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR-25",
      "pdf_url": "http://arxiv.org/pdf/2503.08269v1",
      "published_date": "2025-03-11 10:34:57 UTC",
      "updated_date": "2025-03-11 10:34:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:26:39.551935"
    },
    {
      "arxiv_id": "2503.08257v2",
      "title": "DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness",
      "title_zh": "DexGrasp Anything：面向具有物理感知的通用机器人灵巧抓取",
      "authors": [
        "Yiming Zhong",
        "Qi Jiang",
        "Jingyi Yu",
        "Yuexin Ma"
      ],
      "abstract": "A dexterous hand capable of grasping any object is essential for the\ndevelopment of general-purpose embodied intelligent robots. However, due to the\nhigh degree of freedom in dexterous hands and the vast diversity of objects,\ngenerating high-quality, usable grasping poses in a robust manner is a\nsignificant challenge. In this paper, we introduce DexGrasp Anything, a method\nthat effectively integrates physical constraints into both the training and\nsampling phases of a diffusion-based generative model, achieving\nstate-of-the-art performance across nearly all open datasets. Additionally, we\npresent a new dexterous grasping dataset containing over 3.4 million diverse\ngrasping poses for more than 15k different objects, demonstrating its potential\nto advance universal dexterous grasping. The code of our method and our dataset\nwill be publicly released soon.",
      "tldr_zh": "该研究提出DexGrasp Anything方法，通过将物理约束整合到基于diffusion的生成模型中，实现了通用机器人灵巧抓取的突破。该方法在训练和采样阶段均考虑物理限制，在几乎所有公开数据集上达到state-of-the-art性能。研究还发布了包含340万种抓取姿态的新数据集，覆盖15k多种物体，为推进通用灵巧抓取研究提供了重要资源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08257v2",
      "published_date": "2025-03-11 10:21:50 UTC",
      "updated_date": "2025-03-16 13:05:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:26:53.042361"
    },
    {
      "arxiv_id": "2503.08251v1",
      "title": "MT-NAM: An Efficient and Adaptive Model for Epileptic Seizure Detection",
      "title_zh": "MT-NAM：一种高效自适应的癫痫发作检测模型",
      "authors": [
        "Arshia Afzal",
        "Volkan Cevher",
        "Mahsa Shoaran"
      ],
      "abstract": "Enhancing the accuracy and efficiency of machine learning algorithms employed\nin neural interface systems is crucial for advancing next-generation\nintelligent therapeutic devices. However, current systems often utilize basic\nmachine learning models that do not fully exploit the natural structure of\nbrain signals. Additionally, existing learning models used for neural signal\nprocessing often demonstrate low speed and efficiency during inference. To\naddress these challenges, this study introduces Micro Tree-based NAM (MT-NAM),\na distilled model based on the recently proposed Neural Additive Models (NAM).\nThe MT-NAM achieves a remarkable 100$\\times$ improvement in inference speed\ncompared to standard NAM, without compromising accuracy. We evaluate our\napproach on the CHB-MIT scalp EEG dataset, which includes recordings from 24\npatients with varying numbers of sessions and seizures. NAM achieves an 85.3\\%\nwindow-based sensitivity and 95\\% specificity. Interestingly, our proposed\nMT-NAM shows only a 2\\% reduction in sensitivity compared to the original NAM.\nTo regain this sensitivity, we utilize a test-time template adjuster (T3A) as\nan update mechanism, enabling our model to achieve higher sensitivity during\ntest time by accommodating transient shifts in neural signals. With this online\nupdate approach, MT-NAM achieves the same sensitivity as the standard NAM while\nachieving approximately 50$\\times$ acceleration in inference speed.",
      "tldr_zh": "本研究提出了MT-NAM，一种基于微树结构的神经加性模型(Neural Additive Models, NAM)的蒸馏模型，用于癫痫发作检测。该模型在CHB-MIT头皮脑电图数据集上实现了100倍的推理速度提升，同时保持了与标准NAM相当的检测精度（85.3%的窗口敏感性和95%的特异性）。通过引入测试时间模板调整器(T3A)作为在线更新机制，MT-NAM能够适应神经信号的瞬态变化，最终在保持与标准NAM相同敏感性的同时，实现了约50倍的推理加速。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Submitted to IEEE-TBME",
      "pdf_url": "http://arxiv.org/pdf/2503.08251v1",
      "published_date": "2025-03-11 10:14:53 UTC",
      "updated_date": "2025-03-11 10:14:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:26:58.267835"
    },
    {
      "arxiv_id": "2503.08250v3",
      "title": "Aligning Text to Image in Diffusion Models is Easier Than You Think",
      "title_zh": "扩散模型中文本与图像的对齐比你想象的要简单",
      "authors": [
        "Jaa-Yeon Lee",
        "Byunghee Cha",
        "Jeongsol Kim",
        "Jong Chul Ye"
      ],
      "abstract": "While recent advancements in generative modeling have significantly improved\ntext-image alignment, some residual misalignment between text and image\nrepresentations still remains. Although many approaches have attempted to\naddress this issue by fine-tuning models using various reward models, etc., we\nrevisit the challenge from the perspective of representation alignment-an\napproach that has gained popularity with the success of REPresentation\nAlignment (REPA). We first argue that conventional text-to-image (T2I)\ndiffusion models, typically trained on paired image and text data (i.e.,\npositive pairs) by minimizing score matching or flow matching losses, is\nsuboptimal from the standpoint of representation alignment. Instead, a better\nalignment can be achieved through contrastive learning that leverages both\npositive and negative pairs. To achieve this efficiently even with pretrained\nmodels, we introduce a lightweight contrastive fine tuning strategy called\nSoftREPA that uses soft text tokens. This approach improves alignment with\nminimal computational overhead by adding fewer than 1M trainable parameters to\nthe pretrained model. Our theoretical analysis demonstrates that our method\nexplicitly increases the mutual information between text and image\nrepresentations, leading to enhanced semantic consistency. Experimental results\nacross text-to-image generation and text-guided image editing tasks validate\nthe effectiveness of our approach in improving the semantic consistency of T2I\ngenerative models.",
      "tldr_zh": "这篇论文提出了一种改进扩散模型中文本-图像对齐的新方法。研究发现传统仅使用正样本对训练的扩散模型在表征对齐上存在不足，提出通过对比学习同时利用正负样本对来提升对齐效果。作者开发了名为SoftREPA的轻量级对比微调策略，仅需增加不到100万个可训练参数即可显著改善语义一致性。理论分析表明该方法能显式增加文本与图像表征间的互信息，实验证明其在文本生成图像和文本引导图像编辑任务中均有效提升了语义一致性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08250v3",
      "published_date": "2025-03-11 10:14:22 UTC",
      "updated_date": "2025-03-21 07:28:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:27:10.538727"
    },
    {
      "arxiv_id": "2503.08241v1",
      "title": "HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents",
      "title_zh": "HASARD：基于视觉的具身智能体安全强化学习基准",
      "authors": [
        "Tristan Tomilin",
        "Meng Fang",
        "Mykola Pechenizkiy"
      ],
      "abstract": "Advancing safe autonomous systems through reinforcement learning (RL)\nrequires robust benchmarks to evaluate performance, analyze methods, and assess\nagent competencies. Humans primarily rely on embodied visual perception to\nsafely navigate and interact with their surroundings, making it a valuable\ncapability for RL agents. However, existing vision-based 3D benchmarks only\nconsider simple navigation tasks. To address this shortcoming, we introduce\n\\textbf{HASARD}, a suite of diverse and complex tasks to $\\textbf{HA}$rness\n$\\textbf{SA}$fe $\\textbf{R}$L with $\\textbf{D}$oom, requiring strategic\ndecision-making, comprehending spatial relationships, and predicting the\nshort-term future. HASARD features three difficulty levels and two action\nspaces. An empirical evaluation of popular baseline methods demonstrates the\nbenchmark's complexity, unique challenges, and reward-cost trade-offs.\nVisualizing agent navigation during training with top-down heatmaps provides\ninsight into a method's learning process. Incrementally training across\ndifficulty levels offers an implicit learning curriculum. HASARD is the first\nsafe RL benchmark to exclusively target egocentric vision-based learning,\noffering a cost-effective and insightful way to explore the potential and\nboundaries of current and future safe RL methods. The environments and baseline\nimplementations are open-sourced at\nhttps://sites.google.com/view/hasard-bench/.",
      "tldr_zh": "该研究提出了HASARD，首个专注于基于视觉的安全强化学习（Safe RL）的基准测试，旨在评估智能体在复杂任务中的安全决策能力。HASARD通过三个难度等级和两种动作空间，模拟需要战略决策、空间关系理解和短期预测的任务，填补了现有3D视觉基准在复杂性和安全性方面的不足。实验表明，该基准能够有效揭示方法的性能边界和学习过程，并通过逐步训练提供隐式学习路径。HASARD为研究安全强化学习的潜力和局限性提供了成本效益高且具有洞察力的工具，相关环境和基线实现已开源。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08241v1",
      "published_date": "2025-03-11 10:05:01 UTC",
      "updated_date": "2025-03-11 10:05:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:27:12.072448"
    },
    {
      "arxiv_id": "2503.11696v1",
      "title": "Balancing SoC in Battery Cells using Safe Action Perturbations",
      "title_zh": "利用安全动作扰动实现电池单元间的荷电状态平衡",
      "authors": [
        "E Harshith Kumar Yadav",
        "Rahul Narava",
        "Anshika",
        "Shashi Shekher Jha"
      ],
      "abstract": "Managing equal charge levels in active cell balancing while charging a Li-ion\nbattery is challenging. An imbalance in charge levels affects the state of\nhealth of the battery, along with the concerns of thermal runaway and fire\nhazards. Traditional methods focus on safety assurance as a trade-off between\nsafety and charging time. Others deal with battery-specific conditions to\nensure safety, therefore losing on the generalization of the control strategies\nover various configurations of batteries. In this work, we propose a method to\nlearn safe battery charging actions by using a safety-layer as an add-on over a\nDeep Reinforcement Learning (RL) agent. The safety layer perturbs the agent's\naction to prevent the battery from encountering unsafe or dangerous states.\nFurther, our Deep RL framework focuses on learning a generalized policy that\ncan be effectively employed with varying configurations of batteries. Our\nexperimental results demonstrate that the safety-layer based action\nperturbation incurs fewer safety violations by avoiding unsafe states along\nwith learning a robust policy for several battery configurations.",
      "tldr_zh": "该研究提出了一种基于安全动作扰动的方法，用于平衡锂离子电池充电过程中的单体电荷状态（SoC）。通过将安全层（safety-layer）集成到深度强化学习（DRL）框架中，该系统能动态调整智能体的充电动作以避免电池进入危险状态。相比传统方法，该方案在保证安全性的同时实现了对不同电池配置的泛化控制，实验表明其能显著减少安全违规次数并学习到稳健的充电策略。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.11696v1",
      "published_date": "2025-03-11 09:59:14 UTC",
      "updated_date": "2025-03-11 09:59:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:27:29.718025"
    },
    {
      "arxiv_id": "2503.08228v1",
      "title": "Investigating Execution-Aware Language Models for Code Optimization",
      "title_zh": "探究面向代码优化的执行感知语言模型",
      "authors": [
        "Federico Di Menna",
        "Luca Traini",
        "Gabriele Bavota",
        "Vittorio Cortellessa"
      ],
      "abstract": "Code optimization is the process of enhancing code efficiency, while\npreserving its intended functionality. This process often requires a deep\nunderstanding of the code execution behavior at run-time to identify and\naddress inefficiencies effectively. Recent studies have shown that language\nmodels can play a significant role in automating code optimization. However,\nthese models may have insufficient knowledge of how code execute at run-time.\nTo address this limitation, researchers have developed strategies that\nintegrate code execution information into language models. These strategies\nhave shown promise, enhancing the effectiveness of language models in various\nsoftware engineering tasks. However, despite the close relationship between\ncode execution behavior and efficiency, the specific impact of these strategies\non code optimization remains largely unexplored. This study investigates how\nincorporating code execution information into language models affects their\nability to optimize code. Specifically, we apply three different training\nstrategies to incorporate four code execution aspects -- line executions, line\ncoverage, branch coverage, and variable states -- into CodeT5+, a well-known\nlanguage model for code. Our results indicate that execution-aware models\nprovide limited benefits compared to the standard CodeT5+ model in optimizing\ncode.",
      "tldr_zh": "这项研究探讨了如何将代码执行信息（如行执行、行覆盖率、分支覆盖率和变量状态）整合到语言模型中，以提升其代码优化能力。研究者基于CodeT5+模型测试了三种不同的训练策略，发现相比标准模型，整合执行信息的模型在代码优化任务中带来的改进有限。结果表明，尽管执行信息与代码效率密切相关，但当前方法在提升语言模型优化能力方面仍有局限。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.PF"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08228v1",
      "published_date": "2025-03-11 09:46:07 UTC",
      "updated_date": "2025-03-11 09:46:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:27:24.628759"
    },
    {
      "arxiv_id": "2503.08226v1",
      "title": "A Grey-box Text Attack Framework using Explainable AI",
      "title_zh": "基于可解释性AI的灰盒文本攻击框架",
      "authors": [
        "Esther Chiramal",
        "Kelvin Soh Boon Kai"
      ],
      "abstract": "Explainable AI is a strong strategy implemented to understand complex\nblack-box model predictions in a human interpretable language. It provides the\nevidence required to execute the use of trustworthy and reliable AI systems. On\nthe other hand, however, it also opens the door to locating possible\nvulnerabilities in an AI model. Traditional adversarial text attack uses word\nsubstitution, data augmentation techniques and gradient-based attacks on\npowerful pre-trained Bidirectional Encoder Representations from Transformers\n(BERT) variants to generate adversarial sentences. These attacks are generally\nwhitebox in nature and not practical as they can be easily detected by humans\nE.g. Changing the word from \"Poor\" to \"Rich\". We proposed a simple yet\neffective Grey-box cum Black-box approach that does not require the knowledge\nof the model while using a set of surrogate Transformer/BERT models to perform\nthe attack using Explainable AI techniques. As Transformers are the current\nstate-of-the-art models for almost all Natural Language Processing (NLP) tasks,\nan attack generated from BERT1 is transferable to BERT2. This transferability\nis made possible due to the attention mechanism in the transformer that allows\nthe model to capture long-range dependencies in a sequence. Using the power of\nBERT generalisation via attention, we attempt to exploit how transformers learn\nby attacking a few surrogate transformer variants which are all based on a\ndifferent architecture. We demonstrate that this approach is highly effective\nto generate semantically good sentences by changing as little as one word that\nis not detectable by humans while still fooling other BERT models.",
      "tldr_zh": "本文提出了一种基于可解释AI（Explainable AI）的灰盒文本攻击框架，旨在利用Transformer/BERT模型的注意力机制生成语义连贯且不易被人类察觉的对抗样本。该方法通过使用一组替代的Transformer/BERT模型进行攻击，无需了解目标模型的具体信息，从而实现了灰盒到黑盒的攻击方式。实验表明，该方法能够通过仅修改一个单词生成高质量的对抗句子，成功欺骗其他BERT模型，同时保持语义的自然性，为研究Transformer模型的脆弱性提供了新视角。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08226v1",
      "published_date": "2025-03-11 09:44:17 UTC",
      "updated_date": "2025-03-11 09:44:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:27:31.979907"
    },
    {
      "arxiv_id": "2503.08745v1",
      "title": "Neural Network for Blind Unmixing: a novel MatrixConv Unmixing (MCU) Approach",
      "title_zh": "神经网络用于盲解混：一种新颖的矩阵卷积解混（MCU）方法",
      "authors": [
        "Chao Zhou",
        "Wei Pu",
        "Miguel Rodrigues"
      ],
      "abstract": "Hyperspectral image (HSI) unmixing is a challenging research problem that\ntries to identify the constituent components, known as endmembers, and their\ncorresponding proportions, known as abundances, in the scene by analysing\nimages captured by hyperspectral cameras. Recently, many deep learning based\nunmixing approaches have been proposed with the surge of machine learning\ntechniques, especially convolutional neural networks (CNN). However, these\nmethods face two notable challenges: 1. They frequently yield results lacking\nphysical significance, such as signatures corresponding to unknown or\nnon-existent materials. 2. CNNs, as general-purpose network structures, are not\nexplicitly tailored for unmixing tasks. In response to these concerns, our work\ndraws inspiration from double deep image prior (DIP) techniques and algorithm\nunrolling, presenting a novel network structure that effectively addresses both\nissues. Specifically, we first propose a MatrixConv Unmixing (MCU) approach for\nendmember and abundance estimation, respectively, which can be solved via\ncertain iterative solvers. We then unroll these solvers to build two\nsub-networks, endmember estimation DIP (UEDIP) and abundance estimation DIP\n(UADIP), to generate the estimation of endmember and abundance, respectively.\nThe overall network is constructed by assembling these two sub-networks. In\norder to generate meaningful unmixing results, we also propose a composite loss\nfunction. To further improve the unmixing quality, we also add explicitly a\nregularizer for endmember and abundance estimation, respectively. The proposed\nmethods are tested for effectiveness on both synthetic and real datasets.",
      "tldr_zh": "该研究提出了一种新型神经网络方法MatrixConv Unmixing (MCU)，用于解决高光谱图像(HSI)盲解混问题。针对现有CNN方法存在物理意义缺失和网络结构不适配的两大挑战，该方法创新性地结合双深度图像先验(DIP)技术和算法展开技术，设计了分别用于端元估计(UEDIP)和丰度估计(UADIP)的两个子网络。通过构建复合损失函数并加入端元和丰度的显式正则化项，该方法在合成和真实数据集上均取得了显著效果提升，为高光谱解混提供了更具物理意义的解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08745v1",
      "published_date": "2025-03-11 09:41:57 UTC",
      "updated_date": "2025-03-11 09:41:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:27:43.169522"
    },
    {
      "arxiv_id": "2503.08221v1",
      "title": "EgoBlind: Towards Egocentric Visual Assistance for the Blind People",
      "title_zh": "EgoBlind：面向盲人的自我中心视觉辅助系统",
      "authors": [
        "Junbin Xiao",
        "Nanxin Huang",
        "Hao Qiu",
        "Zhulin Tao",
        "Xun Yang",
        "Richang Hong",
        "Meng Wang",
        "Angela Yao"
      ],
      "abstract": "We present EgoBlind, the first egocentric VideoQA dataset collected from\nblind individuals to evaluate the assistive capabilities of contemporary\nmultimodal large language models (MLLMs). EgoBlind comprises 1,210 videos that\nrecord the daily lives of real blind users from a first-person perspective. It\nalso features 4,927 questions directly posed or generated and verified by blind\nindividuals to reflect their needs for visual assistance under various\nscenarios. We provide each question with an average of 3 reference answers to\nalleviate subjective evaluation. Using EgoBlind, we comprehensively evaluate 15\nleading MLLMs and find that all models struggle, with the best performers\nachieving accuracy around 56\\%, far behind human performance of 87.4\\%. To\nguide future advancements, we identify and summarize major limitations of\nexisting MLLMs in egocentric visual assistance for the blind and provide\nheuristic suggestions for improvement. With these efforts, we hope EgoBlind can\nserve as a valuable foundation for developing more effective AI assistants to\nenhance the independence of the blind individuals' lives.",
      "tldr_zh": "本文提出了首个盲人第一视角视频问答数据集EgoBlind，包含1,210段真实盲人日常生活的第一人称视频和4,927个由盲人提出或验证的问题。研究评估了15个主流多模态大语言模型(MLLMs)，发现最佳模型准确率仅56%，远低于人类表现的87.4%。该工作不仅揭示了现有MLLMs在盲人视觉辅助方面的主要局限，还为开发更有效的AI助手提供了改进方向，有望增强盲人生活的独立性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint. Under Review",
      "pdf_url": "http://arxiv.org/pdf/2503.08221v1",
      "published_date": "2025-03-11 09:40:31 UTC",
      "updated_date": "2025-03-11 09:40:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:28:08.611013"
    },
    {
      "arxiv_id": "2503.08219v1",
      "title": "CL-MVSNet: Unsupervised Multi-view Stereo with Dual-level Contrastive Learning",
      "title_zh": "CL-MVSNet：基于双层次对比学习的无监督多视图立体匹配",
      "authors": [
        "Kaiqiang Xiong",
        "Rui Peng",
        "Zhe Zhang",
        "Tianxing Feng",
        "Jianbo Jiao",
        "Feng Gao",
        "Ronggang Wang"
      ],
      "abstract": "Unsupervised Multi-View Stereo (MVS) methods have achieved promising progress\nrecently. However, previous methods primarily depend on the photometric\nconsistency assumption, which may suffer from two limitations:\nindistinguishable regions and view-dependent effects, e.g., low-textured areas\nand reflections. To address these issues, in this paper, we propose a new\ndual-level contrastive learning approach, named CL-MVSNet. Specifically, our\nmodel integrates two contrastive branches into an unsupervised MVS framework to\nconstruct additional supervisory signals. On the one hand, we present an\nimage-level contrastive branch to guide the model to acquire more context\nawareness, thus leading to more complete depth estimation in indistinguishable\nregions. On the other hand, we exploit a scene-level contrastive branch to\nboost the representation ability, improving robustness to view-dependent\neffects. Moreover, to recover more accurate 3D geometry, we introduce an L0.5\nphotometric consistency loss, which encourages the model to focus more on\naccurate points while mitigating the gradient penalty of undesirable ones.\nExtensive experiments on DTU and Tanks&Temples benchmarks demonstrate that our\napproach achieves state-of-the-art performance among all end-to-end\nunsupervised MVS frameworks and outperforms its supervised counterpart by a\nconsiderable margin without fine-tuning.",
      "tldr_zh": "本文提出CL-MVSNet，一种基于双层次对比学习的无监督多视角立体匹配方法。该方法通过图像级对比分支增强上下文感知能力以解决低纹理区域问题，并通过场景级对比分支提升表征能力以应对视角依赖效应。同时引入L0.5光度一致性损失函数，优化3D几何重建精度。实验表明，该模型在DTU和Tanks&Temples基准测试中表现优异，不仅优于所有端到端无监督方法，更以显著优势超越有监督模型而无需微调。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accpetd by ICCV2023",
      "pdf_url": "http://arxiv.org/pdf/2503.08219v1",
      "published_date": "2025-03-11 09:39:06 UTC",
      "updated_date": "2025-03-11 09:39:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:28:28.435708"
    },
    {
      "arxiv_id": "2503.08213v1",
      "title": "DeepRAG: Building a Custom Hindi Embedding Model for Retrieval Augmented Generation from Scratch",
      "title_zh": "DeepRAG：从头构建用于检索增强生成的定制化印地语嵌入模型",
      "authors": [
        "Nandakishor M"
      ],
      "abstract": "In this paper, I present our work on DeepRAG, a specialized embedding model\nwe built specifically for Hindi language in RAG systems. While LLMs have gotten\nreally good at generating text, their performance in retrieval tasks still\ndepends heavily on having quality embeddings - something that's been lacking\nfor Hindi despite being one of the world's most spoken languages. We tackled\nthis by creating embeddings from the ground up rather than just fine-tuning\nexisting models. Our process involved collecting diverse Hindi texts (over 2.7M\nsamples), training a custom SentencePiece tokenizer that actually understands\nHindi morphology, designing transformer architecture with Hindi-specific\nattention mechanisms, and optimizing with contrastive learning. Results were\nhonestly better than I expected - we saw a 23% improvement in retrieval\nprecision compared to the multilingual models everyone's been using. The paper\ndetails our methodology, which I think could help others working with\nlow-resource languages where the one-size-fits-all multilingual models fall\nshort. We've also integrated our embeddings with LangChain to build complete\nHindi RAG systems, which might be useful for practitioners. While there's still\ntons more to explore, I believe this work addresses a critical gap for Hindi\nNLP and demonstrates why language-specific approaches matter.",
      "tldr_zh": "该论文提出了DeepRAG，一个专门为印地语设计的定制化嵌入模型，用于构建检索增强生成(RAG)系统。针对现有多语言模型在印地语任务中的不足，研究者从零开始构建模型，包括收集270万条印地语样本、训练理解印地语形态的SentencePiece分词器，以及设计具有印地语特定注意力机制的Transformer架构。实验表明，该模型比通用多语言模型的检索精度提高了23%，并通过LangChain集成了完整的印地语RAG系统，为低资源语言处理提供了新思路。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08213v1",
      "published_date": "2025-03-11 09:27:56 UTC",
      "updated_date": "2025-03-11 09:27:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:28:45.027260"
    },
    {
      "arxiv_id": "2503.08205v1",
      "title": "OLMD: Orientation-aware Long-term Motion Decoupling for Continuous Sign Language Recognition",
      "title_zh": "OLMD：面向连续手语识别的方向感知长期运动解耦框架",
      "authors": [
        "Yiheng Yu",
        "Sheng Liu",
        "Yuan Feng",
        "Min Xu",
        "Zhelun Jin",
        "Xuhua Yang"
      ],
      "abstract": "The primary challenge in continuous sign language recognition (CSLR) mainly\nstems from the presence of multi-orientational and long-term motions. However,\ncurrent research overlooks these crucial aspects, significantly impacting\naccuracy. To tackle these issues, we propose a novel CSLR framework:\nOrientation-aware Long-term Motion Decoupling (OLMD), which efficiently\naggregates long-term motions and decouples multi-orientational signals into\neasily interpretable components. Specifically, our innovative Long-term Motion\nAggregation (LMA) module filters out static redundancy while adaptively\ncapturing abundant features of long-term motions. We further enhance\norientation awareness by decoupling complex movements into horizontal and\nvertical components, allowing for motion purification in both orientations.\nAdditionally, two coupling mechanisms are proposed: stage and cross-stage\ncoupling, which together enrich multi-scale features and improve the\ngeneralization capabilities of the model. Experimentally, OLMD shows SOTA\nperformance on three large-scale datasets: PHOENIX14, PHOENIX14-T, and\nCSL-Daily. Notably, we improved the word error rate (WER) on PHOENIX14 by an\nabsolute 1.6% compared to the previous SOTA",
      "tldr_zh": "该研究提出了一种面向连续手语识别(CSLR)的新框架OLMD，通过方向感知的长时运动解耦技术有效解决了多方向性和长时动作带来的核心挑战。创新性地设计了长时运动聚合(LMA)模块来过滤静态冗余并自适应捕获长时运动特征，同时将复杂动作解耦为水平和垂直分量实现运动纯化。通过阶段耦合和跨阶段耦合机制增强多尺度特征表示，在PHOENIX14等三大数据集上取得SOTA性能，其中PHOENIX14的词错误率(WER)较之前最佳结果绝对降低了1.6%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08205v1",
      "published_date": "2025-03-11 09:20:06 UTC",
      "updated_date": "2025-03-11 09:20:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:28:55.901230"
    },
    {
      "arxiv_id": "2503.08199v1",
      "title": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging Control Integrating Large Language Models",
      "title_zh": "集成大语言模型的匝道汇入控制级联协作多智能体框架",
      "authors": [
        "Miao Zhang",
        "Zhenlong Fang",
        "Tianyi Wang",
        "Qian Zhang",
        "Shuai Lu",
        "Junfeng Jiao",
        "Tianyu Shi"
      ],
      "abstract": "Traditional Reinforcement Learning (RL) suffers from replicating human-like\nbehaviors, generalizing effectively in multi-agent scenarios, and overcoming\ninherent interpretability issues.These tasks are compounded when deep\nenvironment understanding, agent coordination and dynamic optimization are\nrequired. While Large Language Model (LLM) enhanced methods have shown promise\nin generalization and interoperability, they often neglect necessary\nmulti-agent coordination. Therefore, we introduce the Cascading Cooperative\nMulti-agent (CCMA) framework, integrating RL for individual interactions, a\nfine-tuned LLM for regional cooperation, a reward function for global\noptimization, and the Retrieval-augmented Generation mechanism to dynamically\noptimize decision-making across complex driving scenarios. Our experiments\ndemonstrate that the CCMA outperforms existing RL methods, demonstrating\nsignificant improvements in both micro and macro-level performance in complex\ndriving environments.",
      "tldr_zh": "该研究提出了一种级联协作多智能体框架(CCMA)，用于整合大语言模型(LLMs)的匝道汇入控制。该框架结合了强化学习(RL)处理个体交互、微调LLM实现区域协作、全局优化奖励函数以及检索增强生成(RAG)机制，以动态优化复杂驾驶场景的决策。实验表明，CCMA在微观和宏观层面均显著优于现有RL方法，有效解决了传统方法在类人行为模拟、多智能体泛化和可解释性方面的局限。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08199v1",
      "published_date": "2025-03-11 09:08:04 UTC",
      "updated_date": "2025-03-11 09:08:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:29:18.773674"
    },
    {
      "arxiv_id": "2503.08193v1",
      "title": "Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of Role-Playing Language Agents",
      "title_zh": "猜我所想：角色扮演语言智能体内在思维推理基准",
      "authors": [
        "Rui Xu",
        "MingYu Wang",
        "XinTao Wang",
        "Dakuan Lu",
        "Xiaoyu Tan",
        "Wei Chu",
        "Yinghui Xu"
      ],
      "abstract": "Recent advances in LLM-based role-playing language agents (RPLAs) have\nattracted broad attention in various applications. While chain-of-thought\nreasoning has shown importance in many tasks for LLMs, the internal thinking\nprocesses of RPLAs remain unexplored. Understanding characters' inner thoughts\nis crucial for developing advanced RPLAs. In this paper, we introduce\nROLETHINK, a novel benchmark constructed from literature for evaluating\ncharacter thought generation. We propose the task of inner thought reasoning,\nwhich includes two sets: the gold set that compares generated thoughts with\noriginal character monologues, and the silver set that uses expert synthesized\ncharacter analyses as references. To address this challenge, we propose MIRROR,\na chain-of-thought approach that generates character thoughts by retrieving\nmemories, predicting character reactions, and synthesizing motivations. Through\nextensive experiments, we demonstrate the importance of inner thought reasoning\nfor RPLAs, and MIRROR consistently outperforms existing methods. Resources are\navailable at https://github.com/airaer1998/RPA_Thought.",
      "tldr_zh": "该研究提出了ROLETHINK基准测试，用于评估角色扮演语言代理(RPLAs)的内心思维推理能力。研究者开发了MIRROR方法，通过记忆检索、角色反应预测和动机合成的链式思维推理来生成角色内心独白。实验表明，MIRROR方法在黄金集（对比原始角色独白）和白银集（基于专家分析）上都优于现有方法，证实了内心思维推理对提升角色扮演代理的重要性。相关资源已在GitHub开源。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08193v1",
      "published_date": "2025-03-11 08:57:07 UTC",
      "updated_date": "2025-03-11 08:57:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:29:08.609099"
    },
    {
      "arxiv_id": "2503.08188v1",
      "title": "RigoChat 2: an adapted language model to Spanish using a bounded dataset and reduced hardware",
      "title_zh": "RigoChat 2：利用有限数据集和减少的硬件资源为西班牙语优化的语言模型",
      "authors": [
        "Gonzalo Santamaría Gómez",
        "Guillem García Subies",
        "Pablo Gutiérrez Ruiz",
        "Mario González Valero",
        "Natàlia Fuertes",
        "Helena Montoro Zamorano",
        "Carmen Muñoz Sanz",
        "Leire Rosado Plaza",
        "Nuria Aldama García",
        "David Betancur Sánchez",
        "Kateryna Sushkova",
        "Marta Guerrero Nieto",
        "Álvaro Barbero Jiménez"
      ],
      "abstract": "Large Language Models (LLMs) have become a key element of modern artificial\nintelligence, demonstrating the ability to address a wide range of language\nprocessing tasks at unprecedented levels of accuracy without the need of\ncollecting problem-specific data. However, these versatile models face a\nsignificant challenge: both their training and inference processes require\nsubstantial computational resources, time, and memory. Consequently, optimizing\nthis kind of models to minimize these requirements is crucial. In this article,\nwe demonstrate that, with minimal resources and in a remarkably short time, it\nis possible to enhance a state-of-the-art model, specifically for a given\nlanguage task, without compromising its overall capabilities using a relatively\nsmall pretrained LLM as a basis. Specifically, we present our use case,\nRigoChat 2, illustrating how LLMs can be adapted to achieve superior results in\nSpanish-language tasks.",
      "tldr_zh": "本文提出了RigoChat 2，一种基于有限数据集和低硬件资源优化的西班牙语语言模型。研究表明，通过使用较小的预训练大语言模型(LLM)作为基础，可以在极短的时间内以最小资源显著提升模型在特定语言任务（如西班牙语处理）上的性能，同时不损害其整体能力。这一方法为大语言模型的优化和本地化提供了高效解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08188v1",
      "published_date": "2025-03-11 08:53:53 UTC",
      "updated_date": "2025-03-11 08:53:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:29:02.089928"
    },
    {
      "arxiv_id": "2503.08179v3",
      "title": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with Large Language Models",
      "title_zh": "ProtTeX：基于大语言模型的蛋白质结构上下文推理与编辑",
      "authors": [
        "Zicheng Ma",
        "Chuanliu Fan",
        "Zhicong Wang",
        "Zhenyu Chen",
        "Xiaohan Lin",
        "Yanheng Li",
        "Shihao Feng",
        "Jun Zhang",
        "Ziqiang Cao",
        "Yi Qin Gao"
      ],
      "abstract": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.",
      "tldr_zh": "该研究提出了ProtTeX框架，通过将蛋白质序列、结构和文本信息统一编码到离散空间中，首次实现了基于Next-Token Prediction范式的大语言模型（LLMs）多模态蛋白质推理与生成。该框架使LLMs能够感知和处理蛋白质结构信息，并将其作为中间推理组件，从而显著提升了蛋白质功能预测的准确性，性能超越当前最先进的领域专家模型两倍。此外，ProtTeX还支持高质量的构象生成和可定制的蛋白质设计，为蛋白质科学领域的多模态任务提供了新思路。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "26 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08179v3",
      "published_date": "2025-03-11 08:43:05 UTC",
      "updated_date": "2025-03-13 13:54:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:29:21.786559"
    },
    {
      "arxiv_id": "2503.08175v1",
      "title": "Privacy-Enhancing Paradigms within Federated Multi-Agent Systems",
      "title_zh": "联邦多智能体系统中的隐私增强范式",
      "authors": [
        "Zitong Shi",
        "Guancheng Wan",
        "Wenke Huang",
        "Guibin Zhang",
        "Jiawei Shao",
        "Mang Ye",
        "Carl Yang"
      ],
      "abstract": "LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving\ncomplex problems by integrating multiple agents, each performing different\nroles. However, in sensitive domains, they face emerging privacy protection\nchallenges. In this paper, we introduce the concept of Federated MAS,\nhighlighting the fundamental differences between Federated MAS and traditional\nFL. We then identify key challenges in developing Federated MAS, including: 1)\nheterogeneous privacy protocols among agents, 2) structural differences in\nmulti-party conversations, and 3) dynamic conversational network structures. To\naddress these challenges, we propose Embedded Privacy-Enhancing Agents\n(EPEAgent), an innovative solution that integrates seamlessly into the\nRetrieval-Augmented Generation (RAG) phase and the context retrieval stage.\nThis solution minimizes data flows, ensuring that only task-relevant,\nagent-specific information is shared. Additionally, we design and generate a\ncomprehensive dataset to evaluate the proposed paradigm. Extensive experiments\ndemonstrate that EPEAgent effectively enhances privacy protection while\nmaintaining strong system performance. The code will be availiable at\nhttps://github.com/ZitongShi/EPEAgent",
      "tldr_zh": "该研究提出**联邦多智能体系统(Federated MAS)**框架，解决基于LLM的多智能体系统在敏感领域面临的三大隐私挑战：智能体间异构隐私协议、多方会话结构差异和动态对话网络。研究者开发了**嵌入式隐私增强智能体(EPEAgent)**，通过无缝集成到检索增强生成(RAG)流程中，仅共享任务相关的智能体特定信息来最小化数据流动。实验表明，该方法在保持系统性能的同时显著提升了隐私保护效果，并配套发布了专用评估数据集。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08175v1",
      "published_date": "2025-03-11 08:38:45 UTC",
      "updated_date": "2025-03-11 08:38:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:29:19.473360"
    },
    {
      "arxiv_id": "2503.08174v1",
      "title": "Investigating the Effectiveness of a Socratic Chain-of-Thoughts Reasoning Method for Task Planning in Robotics, A Case Study",
      "title_zh": "探究苏格拉底式链式思维推理方法在机器人任务规划中的有效性——一项案例研究",
      "authors": [
        "Veronica Bot",
        "Zheyuan Xu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated unprecedented capability in\nreasoning with natural language. Coupled with this development is the emergence\nof embodied AI in robotics. Despite showing promise for verbal and written\nreasoning tasks, it remains unknown whether LLMs are capable of navigating\ncomplex spatial tasks with physical actions in the real world. To this end, it\nis of interest to investigate applying LLMs to robotics in zero-shot learning\nscenarios, and in the absence of fine-tuning - a feat which could significantly\nimprove human-robot interaction, alleviate compute cost, and eliminate\nlow-level programming tasks associated with robot tasks.\n  To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot\nin Webots engine for an object search task. We evaluate the effectiveness of\nthree reasoning strategies based on Chain-of-Thought (CoT) sub-task list\ngeneration with the Socratic method (SocraCoT) (in order of increasing rigor):\n(1) Non-CoT/Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was\nmeasured in terms of the proportion of tasks successfully completed and\nexecution time (N = 20). Our preliminary results show that when combined with\nchain-of-thought reasoning, the Socratic method can be used for code generation\nfor robotic tasks that require spatial awareness. In extension of this finding,\nwe propose EVINCE-LoC; a modified EVINCE method that could further enhance\nperformance in highly complex and or dynamic testing scenarios.",
      "tldr_zh": "该研究探索了将大语言模型（LLMs）应用于机器人任务规划的零样本学习场景，提出了一种结合链式思维推理（Chain-of-Thought, CoT）与苏格拉底方法（Socratic Method）的推理策略（SocraCoT）。通过在Webots模拟环境中使用GPT-4(Omni)与Tiago机器人进行物体搜索任务实验，研究发现SocraCoT在空间感知任务中的代码生成表现优于传统CoT和非CoT方法。基于这一发现，研究进一步提出了改进的EVINCE-LoC方法，以应对更复杂和动态的任务场景。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08174v1",
      "published_date": "2025-03-11 08:36:37 UTC",
      "updated_date": "2025-03-11 08:36:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:29:30.951476"
    },
    {
      "arxiv_id": "2503.08163v1",
      "title": "XAI4Extremes: An interpretable machine learning framework for understanding extreme-weather precursors under climate change",
      "title_zh": "XAI4Extremes：一个可解释的机器学习框架，用于理解气候变化下的极端天气前兆",
      "authors": [
        "Jiawen Wei",
        "Aniruddha Bora",
        "Vivek Oommen",
        "Chenyu Dong",
        "Juntao Yang",
        "Jeff Adie",
        "Chen Chen",
        "Simon See",
        "George Karniadakis",
        "Gianmarco Mengaldo"
      ],
      "abstract": "Extreme weather events are increasing in frequency and intensity due to\nclimate change. This, in turn, is exacting a significant toll in communities\nworldwide. While prediction skills are increasing with advances in numerical\nweather prediction and artificial intelligence tools, extreme weather still\npresent challenges. More specifically, identifying the precursors of such\nextreme weather events and how these precursors may evolve under climate change\nremain unclear. In this paper, we propose to use post-hoc interpretability\nmethods to construct relevance weather maps that show the key extreme-weather\nprecursors identified by deep learning models. We then compare this machine\nview with existing domain knowledge to understand whether deep learning models\nidentified patterns in data that may enrich our understanding of\nextreme-weather precursors. We finally bin these relevant maps into different\nmulti-year time periods to understand the role that climate change is having on\nthese precursors. The experiments are carried out on Indochina heatwaves, but\nthe methodology can be readily extended to other extreme weather events\nworldwide.",
      "tldr_zh": "该研究提出了XAI4Extremes框架，利用可解释机器学习方法（post-hoc interpretability）构建相关天气图，揭示深度学习模型识别的极端天气前兆特征。通过将模型识别的模式与现有领域知识对比，研究探讨了深度学习是否能够丰富对极端天气前兆的理解，并分析了气候变化对这些前兆的影响。实验以印度支那热浪为例，但方法可扩展至其他全球极端天气事件。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08163v1",
      "published_date": "2025-03-11 08:27:08 UTC",
      "updated_date": "2025-03-11 08:27:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:29:34.225098"
    },
    {
      "arxiv_id": "2503.08741v2",
      "title": "Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis",
      "title_zh": "Oasis：单张图像实现多模态指令数据合成",
      "authors": [
        "Letian Zhang",
        "Quan Cui",
        "Bingchen Zhao",
        "Cheng Yang"
      ],
      "abstract": "The success of multi-modal large language models (MLLMs) has been largely\nattributed to the large-scale training data. However, the training data of many\nMLLMs is unavailable due to privacy concerns. The expensive and labor-intensive\nprocess of collecting multi-modal data further exacerbates the problem. Is it\npossible to synthesize multi-modal training data automatically without\ncompromising diversity and quality? In this paper, we propose a new method,\nOasis, to synthesize high-quality multi-modal data with only images. Oasis\nbreaks through traditional methods by prompting only images to the MLLMs, thus\nextending the data diversity by a large margin. Our method features a delicate\nquality control method which ensures the data quality. We collected over 500k\ndata and conducted incremental experiments on LLaVA-NeXT. Extensive experiments\ndemonstrate that our method can significantly improve the performance of MLLMs.\nThe image-based synthesis also allows us to focus on the specific-domain\nability of MLLMs. Code and data will be publicly available.",
      "tldr_zh": "该研究提出Oasis方法，仅需单张图像即可自动合成高质量多模态指令数据，突破传统方法限制。通过创新的质量管控机制，该方法能大幅提升数据多样性（构建超50万条数据），并在LLaVA-NeXT模型上验证了性能提升效果。这种基于图像的合成方式特别适用于增强多模态大语言模型(MLLMs)在特定领域的能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08741v2",
      "published_date": "2025-03-11 08:25:40 UTC",
      "updated_date": "2025-03-13 06:15:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:29:56.656751"
    },
    {
      "arxiv_id": "2503.08145v1",
      "title": "Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking",
      "title_zh": "关注轨迹：轨迹感知的开放词汇追踪",
      "authors": [
        "Yunhao Li",
        "Yifan Jiao",
        "Dan Meng",
        "Heng Fan",
        "Libo Zhang"
      ],
      "abstract": "Open-Vocabulary Multi-Object Tracking (OV-MOT) aims to enable approaches to\ntrack objects without being limited to a predefined set of categories. Current\nOV-MOT methods typically rely primarily on instance-level detection and\nassociation, often overlooking trajectory information that is unique and\nessential for object tracking tasks. Utilizing trajectory information can\nenhance association stability and classification accuracy, especially in cases\nof occlusion and category ambiguity, thereby improving adaptability to novel\nclasses. Thus motivated, in this paper we propose \\textbf{TRACT}, an\nopen-vocabulary tracker that leverages trajectory information to improve both\nobject association and classification in OV-MOT. Specifically, we introduce a\n\\textit{Trajectory Consistency Reinforcement} (\\textbf{TCR}) strategy, that\nbenefits tracking performance by improving target identity and category\nconsistency. In addition, we present \\textbf{TraCLIP}, a plug-and-play\ntrajectory classification module. It integrates \\textit{Trajectory Feature\nAggregation} (\\textbf{TFA}) and \\textit{Trajectory Semantic Enrichment}\n(\\textbf{TSE}) strategies to fully leverage trajectory information from visual\nand language perspectives for enhancing the classification results. Extensive\nexperiments on OV-TAO show that our TRACT significantly improves tracking\nperformance, highlighting trajectory information as a valuable asset for\nOV-MOT. Code will be released.",
      "tldr_zh": "本文提出了TRACT，一种基于轨迹感知的开放词汇多目标跟踪(OV-MOT)方法，通过创新性地利用轨迹信息来解决现有方法忽视运动模式的问题。该方法包含两个核心组件：轨迹一致性强化(TCR)策略用于提升目标身份和类别一致性，以及可插拔的轨迹分类模块TraCLIP，该模块通过轨迹特征聚合(TFA)和轨迹语义增强(TSE)策略从视觉和语言角度充分挖掘轨迹信息。在OV-TAO基准测试中，TRACT显著提升了跟踪性能，证明了轨迹信息对于开放词汇跟踪任务的重要价值。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08145v1",
      "published_date": "2025-03-11 08:03:47 UTC",
      "updated_date": "2025-03-11 08:03:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:30:05.272417"
    },
    {
      "arxiv_id": "2503.08136v1",
      "title": "FlowDPS: Flow-Driven Posterior Sampling for Inverse Problems",
      "title_zh": "FlowDPS：面向逆问题的流驱动后验采样方法",
      "authors": [
        "Jeongsol Kim",
        "Bryan Sangwoo Kim",
        "Jong Chul Ye"
      ],
      "abstract": "Flow matching is a recent state-of-the-art framework for generative modeling\nbased on ordinary differential equations (ODEs). While closely related to\ndiffusion models, it provides a more general perspective on generative\nmodeling. Although inverse problem solving has been extensively explored using\ndiffusion models, it has not been rigorously examined within the broader\ncontext of flow models. Therefore, here we extend the diffusion inverse solvers\n(DIS) - which perform posterior sampling by combining a denoising diffusion\nprior with an likelihood gradient - into the flow framework. Specifically, by\ndriving the flow-version of Tweedie's formula, we decompose the flow ODE into\ntwo components: one for clean image estimation and the other for noise\nestimation. By integrating the likelihood gradient and stochastic noise into\neach component, respectively, we demonstrate that posterior sampling for\ninverse problem solving can be effectively achieved using flows. Our proposed\nsolver, Flow-Driven Posterior Sampling (FlowDPS), can also be seamlessly\nintegrated into a latent flow model with a transformer architecture. Across\nfour linear inverse problems, we confirm that FlowDPS outperforms\nstate-of-the-art alternatives, all without requiring additional training.",
      "tldr_zh": "本文提出FlowDPS方法，将基于扩散模型的逆向问题求解扩展至更通用的流匹配(flow matching)框架。通过推导流版本的Tweedie公式，该方法将流ODE分解为干净图像估计和噪声估计两个组件，分别整合似然梯度和随机噪声，实现了高效的逆向问题后验采样。实验表明，FlowDPS在四种线性逆向问题上优于现有方法，且无需额外训练即可与基于transformer的潜流模型无缝集成。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08136v1",
      "published_date": "2025-03-11 07:56:14 UTC",
      "updated_date": "2025-03-11 07:56:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:30:11.618571"
    },
    {
      "arxiv_id": "2503.08133v1",
      "title": "MGHanD: Multi-modal Guidance for authentic Hand Diffusion",
      "title_zh": "MGHanD：面向真实手部生成的多模态引导扩散模型",
      "authors": [
        "Taehyeon Eum",
        "Jieun Choi",
        "Tae-Kyun Kim"
      ],
      "abstract": "Diffusion-based methods have achieved significant successes in T2I\ngeneration, providing realistic images from text prompts. Despite their\ncapabilities, these models face persistent challenges in generating realistic\nhuman hands, often producing images with incorrect finger counts and\nstructurally deformed hands. MGHanD addresses this challenge by applying\nmulti-modal guidance during the inference process. For visual guidance, we\nemploy a discriminator trained on a dataset comprising paired real and\ngenerated images with captions, derived from various hand-in-the-wild datasets.\nWe also employ textual guidance with LoRA adapter, which learns the direction\nfrom `hands' towards more detailed prompts such as `natural hands', and\n`anatomically correct fingers' at the latent level. A cumulative hand mask\nwhich is gradually enlarged in the assigned time step is applied to the added\nguidance, allowing the hand to be refined while maintaining the rich generative\ncapabilities of the pre-trained model. In the experiments, our method achieves\nsuperior hand generation qualities, without any specific conditions or priors.\nWe carry out both quantitative and qualitative evaluations, along with user\nstudies, to showcase the benefits of our approach in producing high-quality\nhand images.",
      "tldr_zh": "该研究提出MGHanD方法，通过多模态引导解决扩散模型生成手部图像时的结构错误问题。该方法创新性地结合视觉引导（基于真实与生成手部图像训练的判别器）和文本引导（通过LoRA适配器学习\"自然手部\"等细节提示），并在指定时间步渐进式应用手部掩模进行细化。实验表明，该方法无需特定条件或先验知识即可生成解剖结构准确的手部图像，定量评估和用户研究均验证了其优越性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08133v1",
      "published_date": "2025-03-11 07:51:47 UTC",
      "updated_date": "2025-03-11 07:51:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:30:22.555464"
    },
    {
      "arxiv_id": "2503.08122v1",
      "title": "Toward Stable World Models: Measuring and Addressing World Instability in Generative Environments",
      "title_zh": "迈向稳定的世界模型：生成式环境中世界不稳定性的测量与应对策略",
      "authors": [
        "Soonwoo Kwon",
        "Jin-Young Kim",
        "Hyojun Go",
        "Kyungjune Baek"
      ],
      "abstract": "We present a novel study on enhancing the capability of preserving the\ncontent in world models, focusing on a property we term World Stability. Recent\ndiffusion-based generative models have advanced the synthesis of immersive and\nrealistic environments that are pivotal for applications such as reinforcement\nlearning and interactive game engines. However, while these models excel in\nquality and diversity, they often neglect the preservation of previously\ngenerated scenes over time--a shortfall that can introduce noise into agent\nlearning and compromise performance in safety-critical settings. In this work,\nwe introduce an evaluation framework that measures world stability by having\nworld models perform a sequence of actions followed by their inverses to return\nto their initial viewpoint, thereby quantifying the consistency between the\nstarting and ending observations. Our comprehensive assessment of\nstate-of-the-art diffusion-based world models reveals significant challenges in\nachieving high world stability. Moreover, we investigate several improvement\nstrategies to enhance world stability. Our results underscore the importance of\nworld stability in world modeling and provide actionable insights for future\nresearch in this domain.",
      "tldr_zh": "该研究提出\"世界稳定性(World Stability)\"概念，用于评估和提升生成式世界模型的内容保持能力。作者开发了一种新型评估框架，通过让世界模型执行动作序列及其逆操作来量化初始与最终观察的一致性，发现当前基于扩散模型(Diffusion Models)的世界模型存在显著的稳定性不足问题。研究还探索了多种提升策略，为强化学习等关键应用场景中稳定世界模型的开发提供了重要洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.08122v1",
      "published_date": "2025-03-11 07:38:11 UTC",
      "updated_date": "2025-03-11 07:38:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:30:18.580862"
    },
    {
      "arxiv_id": "2503.08739v1",
      "title": "HeGMN: Heterogeneous Graph Matching Network for Learning Graph Similarity",
      "title_zh": "HeGMN：面向图相似度学习的异构图匹配网络",
      "authors": [
        "Shilong Sang",
        "Ke-Jia Chen",
        "Zheng liu"
      ],
      "abstract": "Graph similarity learning (GSL), also referred to as graph matching in many\nscenarios, is a fundamental problem in computer vision, pattern recognition,\nand graph learning. However, previous GSL methods assume that graphs are\nhomogeneous and struggle to maintain their performance on heterogeneous graphs.\nTo address this problem, this paper proposes a Heterogeneous Graph Matching\nNetwork (HeGMN), which is an end-to-end graph similarity learning framework\ncomposed of a two-tier matching mechanism. Firstly, a heterogeneous graph\nisomorphism network is proposed as the encoder, which reinvents graph\nisomorphism network for heterogeneous graphs by perceiving different semantic\nrelationships during aggregation. Secondly, a graph-level and node-level\nmatching modules are designed, both employing type-aligned matching principles.\nThe former conducts graph-level matching by node type alignment, and the latter\ncomputes the interactions between the cross-graph nodes with the same type thus\nreducing noise interference and computational overhead. Finally, the\ngraph-level and node-level matching features are combined and fed into fully\nconnected layers for predicting graph similarity scores. In experiments, we\npropose a heterogeneous graph resampling method to construct heterogeneous\ngraph pairs and define the corresponding heterogeneous graph edit distance,\nfilling the gap in missing datasets. Extensive experiments demonstrate that\nHeGMN consistently achieves advanced performance on graph similarity prediction\nacross all datasets.",
      "tldr_zh": "本文提出异构图匹配网络HeGMN，通过双层匹配机制解决现有图相似度学习（GSL）方法在异构图上性能受限的问题。该方法包含三个创新：1）异构图形同构网络编码器，在聚合时感知不同语义关系；2）图级与节点级匹配模块，均采用类型对齐原则降低噪声干扰；3）首创的异构图重采样方法及异构图编辑距离定义，填补了该领域数据集空白。实验表明，HeGMN在所有数据集上均取得先进的图相似度预测性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08739v1",
      "published_date": "2025-03-11 07:36:35 UTC",
      "updated_date": "2025-03-11 07:36:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:30:29.409105"
    },
    {
      "arxiv_id": "2503.08120v1",
      "title": "Uni$\\textbf{F}^2$ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models",
      "title_zh": "Uni$\\textbf{F}^2$ace：基于统一多模态模型的细粒度人脸理解与生成",
      "authors": [
        "Junzhe Li",
        "Xuerui Qiu",
        "Linrui Xu",
        "Liya Guo",
        "Delin Qu",
        "Tingting Long",
        "Chun Fan",
        "Ming Li"
      ],
      "abstract": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in\nfoundational computer vision research, demonstrating significant potential in\nboth image understanding and generation. However, existing research in the face\ndomain primarily focuses on $\\textbf{coarse}$ facial attribute understanding,\nwith limited capacity to handle $\\textbf{fine-grained}$ facial attributes and\nwithout addressing generation capabilities. To overcome these limitations, we\npropose Uni$\\textbf{F}^2$ace, the first UMM tailored specifically for\nfine-grained face understanding and generation. In general, we train\nUni$\\textbf{F}^2$ace on a self-constructed, specialized dataset utilizing two\nmutually beneficial diffusion techniques and a two-level mixture-of-experts\narchitecture. Specifically, we first build a large-scale facial dataset,\nUni$\\textbf{F}^2$ace-130K, which contains 130K image-text pairs with one\nmillion question-answering pairs that span a wide range of facial attributes.\nSecond, we establish a theoretical connection between discrete diffusion score\nmatching and masked generative models, optimizing both evidence lower bounds\nsimultaneously, which significantly improves the model's ability to synthesize\nfacial details. Finally, we introduce both token-level and sequence-level\nmixture-of-experts, enabling efficient fine-grained representation learning for\nboth understanding and generation tasks. Extensive experiments on\nUni$\\textbf{F}^2$ace-130K demonstrate that Uni$\\textbf{F}^2$ace outperforms\nexisting UMMs and generative models, achieving superior performance across both\nunderstanding and generation tasks.",
      "tldr_zh": "该研究提出UniF²ace，首个专为细粒度(fine-grained)人脸理解与生成设计的统一多模态模型(UMM)。通过构建包含13万图像-文本对和百万问答对的专用数据集UniF²ace-130K，并创新性地结合离散扩散分数匹配与掩码生成模型理论，显著提升了面部细节合成能力。模型采用双层专家混合架构（token级和序列级），在理解和生成任务上均超越现有UMM和生成模型，实现了人脸分析从粗粒度到细粒度的突破。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08120v1",
      "published_date": "2025-03-11 07:34:59 UTC",
      "updated_date": "2025-03-11 07:34:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:30:37.752599"
    },
    {
      "arxiv_id": "2503.08117v1",
      "title": "Convergence Dynamics and Stabilization Strategies of Co-Evolving Generative Models",
      "title_zh": "协同演化生成模型的收敛动态与稳定化策略",
      "authors": [
        "Weiguo Gao",
        "Ming Li"
      ],
      "abstract": "The increasing prevalence of synthetic data in training loops has raised\nconcerns about model collapse, where generative models degrade when trained on\ntheir own outputs. While prior work focuses on this self-consuming process, we\nstudy an underexplored yet prevalent phenomenon: co-evolving generative models\nthat shape each other's training through iterative feedback. This is common in\nmultimodal AI ecosystems, such as social media platforms, where text models\ngenerate captions that guide image models, and the resulting images influence\nthe future adaptation of the text model. We take a first step by analyzing such\na system, modeling the text model as a multinomial distribution and the image\nmodel as a conditional multi-dimensional Gaussian distribution. Our analysis\nuncovers three key results. First, when one model remains fixed, the other\ncollapses: a frozen image model causes the text model to lose diversity, while\na frozen text model leads to an exponential contraction of image diversity,\nthough fidelity remains bounded. Second, in fully interactive systems, mutual\nreinforcement accelerates collapse, with image contraction amplifying text\nhomogenization and vice versa, leading to a Matthew effect where dominant texts\nsustain higher image diversity while rarer texts collapse faster. Third, we\nanalyze stabilization strategies implicitly introduced by real-world external\ninfluences. Random corpus injections for text models and user-content\ninjections for image models prevent collapse while preserving both diversity\nand fidelity. Our theoretical findings are further validated through\nexperiments.",
      "tldr_zh": "该论文研究了协同进化的生成模型（co-evolving generative models）在相互反馈训练中的收敛动态和稳定策略。研究发现：（1）当其中一个模型固定时，另一个模型会出现崩溃现象——固定的图像模型会导致文本模型失去多样性，而固定的文本模型则会导致图像多样性指数级收缩；（2）在完全互动的系统中，模型间的相互强化会加速崩溃，形成马太效应（Matthew effect）；（3）通过外部干预策略（如随机语料注入和用户内容注入）可有效防止崩溃并保持多样性。理论分析通过实验验证了这些结论。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T05, 68T99"
      ],
      "primary_category": "cs.LG",
      "comment": "37 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08117v1",
      "published_date": "2025-03-11 07:30:25 UTC",
      "updated_date": "2025-03-11 07:30:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:30:44.217800"
    },
    {
      "arxiv_id": "2503.08102v2",
      "title": "AI-native Memory 2.0: Second Me",
      "title_zh": "AI原生记忆2.0：第二自我",
      "authors": [
        "Jiale Wei",
        "Xiang Ying",
        "Tao Gao",
        "Fangyi Bao",
        "Felix Tao",
        "Jingbo Shang"
      ],
      "abstract": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.",
      "tldr_zh": "该研究提出了AI原生记忆系统\"SECOND ME 2.0\"，这是一个基于大语言模型(LLM)的智能记忆卸载系统。该系统通过LLM参数化记忆技术，不仅能存储用户信息，还能进行上下文推理和自适应知识检索，显著降低用户的认知负荷。与传统存储方案不同，SECOND ME具备动态生成上下文感知响应、预填信息等功能，实现了更智能化的记忆管理。研究团队已将该系统的完整可本地化部署方案开源。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08102v2",
      "published_date": "2025-03-11 07:05:52 UTC",
      "updated_date": "2025-03-12 11:31:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:30:50.880079"
    },
    {
      "arxiv_id": "2503.13494v1",
      "title": "Mobility-aware Seamless Service Migration and Resource Allocation in Multi-edge IoV Systems",
      "title_zh": "多边缘车联网系统中基于移动感知的无缝服务迁移与资源分配",
      "authors": [
        "Zheyi Chen",
        "Sijin Huang",
        "Geyong Min",
        "Zhaolong Ning",
        "Jie Li",
        "Yan Zhang"
      ],
      "abstract": "Mobile Edge Computing (MEC) offers low-latency and high-bandwidth support for\nInternet-of-Vehicles (IoV) applications. However, due to high vehicle mobility\nand finite communication coverage of base stations, it is hard to maintain\nuninterrupted and high-quality services without proper service migration among\nMEC servers. Existing solutions commonly rely on prior knowledge and rarely\nconsider efficient resource allocation during the service migration process,\nmaking it hard to reach optimal performance in dynamic IoV environments. To\naddress these important challenges, we propose SR-CL, a novel mobility-aware\nseamless Service migration and Resource allocation framework via\nConvex-optimization-enabled deep reinforcement Learning in multi-edge IoV\nsystems. First, we decouple the Mixed Integer Nonlinear Programming (MINLP)\nproblem of service migration and resource allocation into two sub-problems.\nNext, we design a new actor-critic-based asynchronous-update deep reinforcement\nlearning method to handle service migration, where the delayed-update actor\nmakes migration decisions and the one-step-update critic evaluates the\ndecisions to guide the policy update. Notably, we theoretically derive the\noptimal resource allocation with convex optimization for each MEC server,\nthereby further improving system performance. Using the real-world datasets of\nvehicle trajectories and testbed, extensive experiments are conducted to verify\nthe effectiveness of the proposed SR-CL. Compared to benchmark methods, the\nSR-CL achieves superior convergence and delay performance under various\nscenarios.",
      "tldr_zh": "该研究提出SR-CL框架，通过凸优化强化学习实现车联网多边缘计算系统中的服务迁移与资源分配联合优化。框架将混合整数非线性规划问题分解为服务迁移决策和资源分配两个子问题：采用新型异步更新演员-评论家深度强化学习方法处理动态迁移决策，同时通过凸优化理论推导各边缘服务器的最优资源分配方案。基于真实车辆轨迹数据的实验表明，相比现有方法，SR-CL在收敛速度和延迟性能上均表现出显著优势，能有效应对车辆高移动性带来的服务连续性挑战。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13494v1",
      "published_date": "2025-03-11 07:03:25 UTC",
      "updated_date": "2025-03-11 07:03:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:31:07.704397"
    },
    {
      "arxiv_id": "2503.08091v1",
      "title": "Revolution of Wireless Signal Recognition for 6G: Recent Advances, Challenges and Future Directions",
      "title_zh": "6G无线信号识别的革命：最新进展、挑战与未来方向",
      "authors": [
        "Hao Zhang",
        "Fuhui Zhou",
        "Hongyang Du",
        "Qihui Wu",
        "Chau Yuen"
      ],
      "abstract": "Wireless signal recognition (WSR) is a crucial technique for intelligent\ncommunications and spectrum sharing in the next six-generation (6G) wireless\ncommunication networks. It can be utilized to enhance network performance and\nefficiency, improve quality of service (QoS), and improve network security and\nreliability. Additionally, WSR can be applied for military applications such as\nsignal interception, signal race, and signal abduction. In the past decades,\ngreat efforts have been made for the research of WSR. Earlier works mainly\nfocus on model-based methods, including likelihood-based (LB) and feature-based\n(FB) methods, which have taken the leading position for many years. With the\nemergence of artificial intelligence (AI), intelligent methods including\nmachine learning-based (ML-based) and deep learning-based (DL-based) methods\nhave been developed to extract the features of the received signals and perform\nthe classification. In this work, we provide a comprehensive review of WSR from\nthe view of applications, main tasks, recent advances, datasets and evaluation\nmetrics, challenges, and future directions. Specifically, intelligent WSR\nmethods are introduced from the perspective of model, data, learning and\nimplementation. Moreover, we analyze the challenges for WSR from the view of\ncomplex, dynamic, and open 6G wireless environments and discuss the future\ndirections for WSR. This survey is expected to provide a comprehensive overview\nof the state-of-the-art WSR techniques and inspire new research directions for\nWSR in 6G networks.",
      "tldr_zh": "本文全面回顾了6G无线通信网络中无线信号识别(WSR)技术的进展、挑战与未来方向。WSR在智能通信、频谱共享、网络性能优化及军事应用中具有重要作用。早期研究主要基于模型的方法，如似然法(LB)和特征法(FB)，而随着人工智能(AI)的发展，基于机器学习(ML)和深度学习(DL)的智能方法逐渐成为主流。本文从应用、任务、数据集、评估指标等角度系统总结了WSR的最新进展，并分析了6G复杂、动态和开放环境带来的挑战，为未来研究提供了方向。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "submitted to IEEE Communications Surveys & Tutorials",
      "pdf_url": "http://arxiv.org/pdf/2503.08091v1",
      "published_date": "2025-03-11 06:47:27 UTC",
      "updated_date": "2025-03-11 06:47:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:31:24.936749"
    },
    {
      "arxiv_id": "2503.08084v1",
      "title": "Instruction-Augmented Long-Horizon Planning: Embedding Grounding Mechanisms in Embodied Mobile Manipulation",
      "title_zh": "指令增强的长时程规划：在移动操作中嵌入接地机制",
      "authors": [
        "Fangyuan Wang",
        "Shipeng Lyu",
        "Peng Zhou",
        "Anqing Duan",
        "Guodong Guo",
        "David Navarro-Alarcon"
      ],
      "abstract": "Enabling humanoid robots to perform long-horizon mobile manipulation planning\nin real-world environments based on embodied perception and comprehension\nabilities has been a longstanding challenge. With the recent rise of large\nlanguage models (LLMs), there has been a notable increase in the development of\nLLM-based planners. These approaches either utilize human-provided textual\nrepresentations of the real world or heavily depend on prompt engineering to\nextract such representations, lacking the capability to quantitatively\nunderstand the environment, such as determining the feasibility of manipulating\nobjects. To address these limitations, we present the Instruction-Augmented\nLong-Horizon Planning (IALP) system, a novel framework that employs LLMs to\ngenerate feasible and optimal actions based on real-time sensor feedback,\nincluding grounded knowledge of the environment, in a closed-loop interaction.\nDistinct from prior works, our approach augments user instructions into PDDL\nproblems by leveraging both the abstract reasoning capabilities of LLMs and\ngrounding mechanisms. By conducting various real-world long-horizon tasks, each\nconsisting of seven distinct manipulatory skills, our results demonstrate that\nthe IALP system can efficiently solve these tasks with an average success rate\nexceeding 80%. Our proposed method can operate as a high-level planner,\nequipping robots with substantial autonomy in unstructured environments through\nthe utilization of multi-modal sensor inputs.",
      "tldr_zh": "该研究提出指令增强的长时程规划（IALP）系统，通过将大语言模型（LLMs）与实体环境感知相结合，解决了传统LLM规划器依赖人工文本描述或提示工程、缺乏定量环境理解能力的问题。该系统创新性地将用户指令转化为PDDL规划问题，同时利用LLMs的抽象推理能力和实体感知机制，基于实时多模态传感器反馈生成可行且最优的动作序列。实验表明，IALP系统能在包含7种不同操作技能的长时程任务中实现超过80%的平均成功率，显著提升了人形机器人在非结构化环境中的自主移动操作能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "17 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08084v1",
      "published_date": "2025-03-11 06:37:33 UTC",
      "updated_date": "2025-03-11 06:37:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:32:10.745984"
    },
    {
      "arxiv_id": "2503.08737v1",
      "title": "Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models",
      "title_zh": "用64个潜在向量表示3D形状以构建3D扩散模型",
      "authors": [
        "In Cho",
        "Youngbeom Yoo",
        "Subin Jeon",
        "Seon Joo Kim"
      ],
      "abstract": "Constructing a compressed latent space through a variational autoencoder\n(VAE) is the key for efficient 3D diffusion models. This paper introduces\nCOD-VAE, a VAE that encodes 3D shapes into a COmpact set of 1D latent vectors\nwithout sacrificing quality. COD-VAE introduces a two-stage autoencoder scheme\nto improve compression and decoding efficiency. First, our encoder block\nprogressively compresses point clouds into compact latent vectors via\nintermediate point patches. Second, our triplane-based decoder reconstructs\ndense triplanes from latent vectors instead of directly decoding neural fields,\nsignificantly reducing computational overhead of neural fields decoding.\nFinally, we propose uncertainty-guided token pruning, which allocates resources\nadaptively by skipping computations in simpler regions and improves the decoder\nefficiency. Experimental results demonstrate that COD-VAE achieves 16x\ncompression compared to the baseline while maintaining quality. This enables\n20.8x speedup in generation, highlighting that a large number of latent vectors\nis not a prerequisite for high-quality reconstruction and generation.",
      "tldr_zh": "本文提出COD-VAE模型，通过两阶段自动编码器方案将3D点云压缩为64个1D潜在向量，实现高质量3D形状表示。该方法首先通过中间点块逐步压缩点云，再采用基于triplane的解码器重建密集triplane而非直接解码神经场，显著降低计算开销。实验表明，COD-VAE在保持质量的同时实现16倍压缩比和20.8倍生成加速，证明少量潜在向量即可完成高质量3D重建与生成。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08737v1",
      "published_date": "2025-03-11 06:29:39 UTC",
      "updated_date": "2025-03-11 06:29:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:31:52.144123"
    },
    {
      "arxiv_id": "2503.08083v1",
      "title": "Degradation Self-Supervised Learning for Lithium-ion Battery Health Diagnostics",
      "title_zh": "锂离子电池健康诊断的退化自监督学习",
      "authors": [
        "J. C. Chen"
      ],
      "abstract": "Health evaluation for lithium-ion batteries (LIBs) typically relies on\nconstant charging/discharging protocols, often neglecting scenarios involving\ndynamic current profiles prevalent in electric vehicles. Conventional health\nindicators for LIBs also depend on the uniformity of measured data, restricting\ntheir adaptability to non-uniform conditions. In this study, a novel training\nstrategy for estimating LIB health based on the paradigm of self-supervised\nlearning is proposed. A multiresolution analysis technique, empirical wavelet\ntransform, is utilized to decompose non-stationary voltage signals in the\nfrequency domain. This allows the removal of ineffective components for the\nhealth evaluation model. The transformer neural network serves as the model\nbackbone, and a loss function is designed to describe the capacity degradation\nbehavior with the assumption that the degradation in LIBs across most operating\nconditions is inevitable and irreversible. The results show that the model can\nlearn the aging characteristics by analyzing sequences of voltage and current\nprofiles obtained at various time intervals from the same LIB cell. The\nproposed method is successfully applied to the Stanford University LIB aging\ndataset, derived from electric vehicle real driving profiles. Notably, this\napproach achieves an average correlation coefficient of 0.9 between the\nevaluated health index and the degradation of actual capacity, demonstrating\nits efficacy in capturing LIB health degradation. This research highlights the\nfeasibility of training deep neural networks using unlabeled LIB data, offering\ncost-efficient means and unleashing the potential of the measured information.",
      "tldr_zh": "该研究提出了一种基于自监督学习（Self-Supervised Learning）的锂离子电池（LIB）健康诊断新方法，通过分析动态电流工况下的非平稳电压信号来评估电池健康状态。该方法采用经验小波变换（Empirical Wavelet Transform）进行多分辨率频域信号分解，并结合Transformer神经网络构建模型，通过设计专门描述容量衰减行为的损失函数来捕捉电池不可逆的老化特征。实验表明，该方法在斯坦福大学电动汽车真实驾驶数据上实现了评估健康指数与实际容量衰减的0.9平均相关系数，为利用无标签电池数据训练深度网络提供了经济高效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08083v1",
      "published_date": "2025-03-11 06:29:13 UTC",
      "updated_date": "2025-03-11 06:29:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:31:49.586826"
    },
    {
      "arxiv_id": "2503.08065v1",
      "title": "STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion Probabilistic Model",
      "title_zh": "STGDPM：基于时空图扩散概率模型的船舶轨迹预测",
      "authors": [
        "Jin Wenzhe",
        "Tang Haina",
        "Zhang Xudong"
      ],
      "abstract": "Vessel trajectory prediction is a critical component for ensuring maritime\ntraffic safety and avoiding collisions. Due to the inherent uncertainty in\nvessel behavior, trajectory prediction systems must adopt a multimodal approach\nto accurately model potential future motion states. However, existing vessel\ntrajectory prediction methods lack the ability to comprehensively model\nbehavioral multi-modality. To better capture multimodal behavior in interactive\nscenarios, we propose modeling interactions as dynamic graphs, replacing\ntraditional aggregation-based techniques that rely on vessel states. By\nleveraging the natural multimodal capabilities of diffusion models, we frame\nthe trajectory prediction task as an inverse process of motion uncertainty\ndiffusion, wherein uncertainties across potential navigational areas are\nprogressively eliminated until the desired trajectories is produced. In\nsummary, we pioneer the integration of Spatio-Temporal Graph (STG) with\ndiffusion models in ship trajectory prediction. Extensive experiments on real\nAutomatic Identification System (AIS) data validate the superiority of our\napproach.",
      "tldr_zh": "该研究提出STGDPM模型，首次将时空图（Spatio-Temporal Graph）与扩散概率模型（Diffusion Probabilistic Model）相结合用于船舶轨迹预测。通过将船舶交互建模为动态图而非传统状态聚合方法，并利用扩散模型固有的多模态特性，该框架将轨迹预测重构为运动不确定性的逆向扩散过程。实验基于真实AIS数据验证，该模型能有效捕捉船舶行为多模态特征，显著提升海上交通避碰预测精度。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been ACCEPTED as a FULL PAPER at DASFAA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08065v1",
      "published_date": "2025-03-11 05:50:27 UTC",
      "updated_date": "2025-03-11 05:50:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:31:59.470269"
    },
    {
      "arxiv_id": "2503.08064v1",
      "title": "Continual Learning for Multiple Modalities",
      "title_zh": "多模态持续学习",
      "authors": [
        "Hyundong Jin",
        "Eunwoo Kim"
      ],
      "abstract": "Continual learning aims to learn knowledge of tasks observed in sequential\ntime steps while mitigating the forgetting of previously learned knowledge.\nExisting methods were proposed under the assumption of learning a single\nmodality (e.g., image) over time, which limits their applicability in scenarios\ninvolving multiple modalities. In this work, we propose a novel continual\nlearning framework that accommodates multiple modalities (image, video, audio,\ndepth, and text). We train a model to align various modalities with text,\nleveraging its rich semantic information. However, this increases the risk of\nforgetting previously learned knowledge, exacerbated by the differing input\ntraits of each task. To alleviate the overwriting of the previous knowledge of\nmodalities, we propose a method for aggregating knowledge within and across\nmodalities. The aggregated knowledge is obtained by assimilating new\ninformation through self-regularization within each modality and associating\nknowledge between modalities by prioritizing contributions from relevant\nmodalities. Furthermore, we propose a strategy that re-aligns the embeddings of\nmodalities to resolve biased alignment between modalities. We evaluate the\nproposed method in a wide range of continual learning scenarios using multiple\ndatasets with different modalities. Extensive experiments demonstrate that ours\noutperforms existing methods in the scenarios, regardless of whether the\nidentity of the modality is given.",
      "tldr_zh": "本文提出了一种新颖的持续学习框架，旨在处理多模态（如图像、视频、音频、深度和文本）数据的连续学习任务。通过将各类模态与富含语义信息的文本对齐，该框架有效缓解了多模态学习中遗忘先前知识的问题。具体方法包括模态内和跨模态的知识聚合，通过自正则化吸收新信息，并优先关联相关模态的贡献。此外，还提出了一种重新对齐模态嵌入的策略，以解决模态间的偏差对齐问题。实验表明，该框架在多种持续学习场景中均优于现有方法，无论模态身份是否已知。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08064v1",
      "published_date": "2025-03-11 05:50:13 UTC",
      "updated_date": "2025-03-11 05:50:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:32:05.228292"
    },
    {
      "arxiv_id": "2503.08051v1",
      "title": "Counterfactual Language Reasoning for Explainable Recommendation Systems",
      "title_zh": "可解释推荐系统中的反事实语言推理",
      "authors": [
        "Guanrong Li",
        "Haolin Yang",
        "Xinyu Liu",
        "Zhen Wu",
        "Xinyu Dai"
      ],
      "abstract": "Explainable recommendation systems leverage transparent reasoning to foster\nuser trust and improve decision-making processes. Current approaches typically\ndecouple recommendation generation from explanation creation, violating causal\nprecedence principles where explanatory factors should logically precede\noutcomes. This paper introduces a novel framework integrating structural causal\nmodels with large language models to establish causal consistency in\nrecommendation pipelines. Our methodology enforces explanation factors as\ncausal antecedents to recommendation predictions through causal graph\nconstruction and counterfactual adjustment. We particularly address the\nconfounding effect of item popularity that distorts personalization signals in\nexplanations, developing a debiasing mechanism that disentangles genuine user\npreferences from conformity bias. Through comprehensive experiments across\nmultiple recommendation scenarios, we demonstrate that CausalX achieves\nsuperior performance in recommendation accuracy, explanation plausibility, and\nbias mitigation compared to baselines.",
      "tldr_zh": "该论文提出了一种新颖的可解释推荐系统框架CausalX，通过将结构因果模型（SCM）与大型语言模型（LLM）相结合，确保推荐预测与解释因素之间的因果一致性。该方法利用因果图构建和反事实调整技术，使解释因素成为推荐预测的因果前提，并特别解决了商品流行度对个性化信号的混淆影响。实验表明，CausalX在推荐准确性、解释合理性和偏差缓解方面均优于基线方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08051v1",
      "published_date": "2025-03-11 05:15:37 UTC",
      "updated_date": "2025-03-11 05:15:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:32:17.543544"
    },
    {
      "arxiv_id": "2503.08038v1",
      "title": "Generalized Kullback-Leibler Divergence Loss",
      "title_zh": "广义Kullback-Leibler散度损失",
      "authors": [
        "Jiequan Cui",
        "Beier Zhu",
        "Qingshan Xu",
        "Zhuotao Tian",
        "Xiaojuan Qi",
        "Bei Yu",
        "Hanwang Zhang",
        "Richang Hong"
      ],
      "abstract": "In this paper, we delve deeper into the Kullback-Leibler (KL) Divergence loss\nand mathematically prove that it is equivalent to the Decoupled\nKullback-Leibler (DKL) Divergence loss that consists of (1) a weighted Mean\nSquare Error (wMSE) loss and (2) a Cross-Entropy loss incorporating soft\nlabels. Thanks to the decoupled structure of DKL loss, we have identified two\nareas for improvement. Firstly, we address the limitation of KL loss in\nscenarios like knowledge distillation by breaking its asymmetric optimization\nproperty along with a smoother weight function. This modification effectively\nalleviates convergence challenges in optimization, particularly for classes\nwith high predicted scores in soft labels. Secondly, we introduce class-wise\nglobal information into KL/DKL to reduce bias arising from individual samples.\nWith these two enhancements, we derive the Generalized Kullback-Leibler (GKL)\nDivergence loss and evaluate its effectiveness by conducting experiments on\nCIFAR-10/100, ImageNet, and vision-language datasets, focusing on adversarial\ntraining, and knowledge distillation tasks. Specifically, we achieve new\nstate-of-the-art adversarial robustness on the public leaderboard --\nRobustBench and competitive knowledge distillation performance across\nCIFAR/ImageNet models and CLIP models, demonstrating the substantial practical\nmerits. Our code is available at https://github.com/jiequancui/DKL.",
      "tldr_zh": "本文提出广义Kullback-Leibler（GKL）散度损失函数，通过数学证明将传统KL散度解耦为加权均方误差（wMSE）和带软标签的交叉熵损失（DKL）。针对KL散度在知识蒸馏等场景的局限，作者进行两大改进：（1）打破不对称优化特性并采用平滑权重函数，缓解高预测分数类别的收敛难题；（2）引入类别全局信息以减少样本偏差。实验表明，GKL在CIFAR/ImageNet等数据集上实现了RobustBench对抗鲁棒性新SOTA，并在CLIP模型知识蒸馏任务中展现竞争优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "extension of our NeurIPS paper \"Decoupled Kullback-Leibler Divergence\n  Loss\". arXiv admin note: substantial text overlap with arXiv:2305.13948",
      "pdf_url": "http://arxiv.org/pdf/2503.08038v1",
      "published_date": "2025-03-11 04:43:33 UTC",
      "updated_date": "2025-03-11 04:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:32:47.849001"
    },
    {
      "arxiv_id": "2503.08037v1",
      "title": "ObjectMover: Generative Object Movement with Video Prior",
      "title_zh": "ObjectMover：基于视频先验的生成式物体移动",
      "authors": [
        "Xin Yu",
        "Tianyu Wang",
        "Soo Ye Kim",
        "Paul Guerrero",
        "Xi Chen",
        "Qing Liu",
        "Zhe Lin",
        "Xiaojuan Qi"
      ],
      "abstract": "Simple as it seems, moving an object to another location within an image is,\nin fact, a challenging image-editing task that requires re-harmonizing the\nlighting, adjusting the pose based on perspective, accurately filling occluded\nregions, and ensuring coherent synchronization of shadows and reflections while\nmaintaining the object identity. In this paper, we present ObjectMover, a\ngenerative model that can perform object movement in highly challenging scenes.\nOur key insight is that we model this task as a sequence-to-sequence problem\nand fine-tune a video generation model to leverage its knowledge of consistent\nobject generation across video frames. We show that with this approach, our\nmodel is able to adjust to complex real-world scenarios, handling extreme\nlighting harmonization and object effect movement. As large-scale data for\nobject movement are unavailable, we construct a data generation pipeline using\na modern game engine to synthesize high-quality data pairs. We further propose\na multi-task learning strategy that enables training on real-world video data\nto improve the model generalization. Through extensive experiments, we\ndemonstrate that ObjectMover achieves outstanding results and adapts well to\nreal-world scenarios.",
      "tldr_zh": "该论文提出了ObjectMover生成模型，通过将物体移动任务建模为序列到序列问题，并微调视频生成模型来利用其在视频帧间保持物体一致性的知识。研究采用游戏引擎构建高质量合成数据，并提出多任务学习策略结合真实视频数据提升泛化能力。实验表明，该模型能有效处理极端光照协调和物体效果移动等复杂场景。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "CVPR 2025, Project Page: https://xinyu-andy.github.io/ObjMover",
      "pdf_url": "http://arxiv.org/pdf/2503.08037v1",
      "published_date": "2025-03-11 04:42:59 UTC",
      "updated_date": "2025-03-11 04:42:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:32:26.453124"
    },
    {
      "arxiv_id": "2503.08032v1",
      "title": "HOFAR: High-Order Augmentation of Flow Autoregressive Transformers",
      "title_zh": "HOFAR：流式自回归变换器的高阶增强方法",
      "authors": [
        "Yingyu Liang",
        "Zhizhou Sha",
        "Zhenmei Shi",
        "Zhao Song",
        "Mingda Wan"
      ],
      "abstract": "Flow Matching and Transformer architectures have demonstrated remarkable\nperformance in image generation tasks, with recent work FlowAR [Ren et al.,\n2024] synergistically integrating both paradigms to advance synthesis fidelity.\nHowever, current FlowAR implementations remain constrained by first-order\ntrajectory modeling during the generation process. This paper introduces a\nnovel framework that systematically enhances flow autoregressive transformers\nthrough high-order supervision. We provide theoretical analysis and empirical\nevaluation showing that our High-Order FlowAR (HOFAR) demonstrates measurable\nimprovements in generation quality compared to baseline models. The proposed\napproach advances the understanding of flow-based autoregressive modeling by\nintroducing a systematic framework for analyzing trajectory dynamics through\nhigh-order expansion.",
      "tldr_zh": "本文提出HOFAR框架，通过高阶监督系统性地增强基于Flow Matching和Transformer架构的FlowAR模型。现有FlowAR方法受限于一阶轨迹建模，而HOFAR引入高阶扩展来分析轨迹动态，在理论上和实验中都展现出生成质量的显著提升。该研究为基于流的自回归建模提供了系统化的高阶分析框架，推动了图像生成领域的发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08032v1",
      "published_date": "2025-03-11 04:29:22 UTC",
      "updated_date": "2025-03-11 04:29:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:32:39.104158"
    },
    {
      "arxiv_id": "2503.08734v1",
      "title": "Zero-to-One IDV: A Conceptual Model for AI-Powered Identity Verification",
      "title_zh": "零到一IDV：AI赋能的身份验证概念模型",
      "authors": [
        "Aniket Vaidya",
        "Anurag Awasthi"
      ],
      "abstract": "In today's increasingly digital interactions, robust Identity Verification\n(IDV) is crucial for security and trust. Artificial Intelligence (AI) is\ntransforming IDV, enhancing accuracy and fraud detection. This paper introduces\n``Zero to One,'' a holistic conceptual framework for developing AI-powered IDV\nproducts. This paper outlines the foundational problem and research objectives\nthat necessitate a new framework for IDV in the age of AI. It details the\nevolution of identity verification and the current regulatory landscape to\ncontextualize the need for a robust conceptual model. The core of the paper is\nthe presentation of the ``Zero to One'' framework itself, dissecting its four\nessential components: Document Verification, Biometric Verification, Risk\nAssessment, and Orchestration. The paper concludes by discussing the\nimplications of this conceptual model and suggesting future research directions\nfocused on the framework's further development and application. The framework\naddresses security, privacy, UX, and regulatory compliance, offering a\nstructured approach to building effective IDV solutions. Successful IDV\nplatforms require a balanced conceptual understanding of verification methods,\nrisk management, and operational scalability, with AI as a key enabler. This\npaper presents the ``Zero to One'' framework as a refined conceptual model,\ndetailing verification layers, and AI's transformative role in shaping\nnext-generation IDV products.",
      "tldr_zh": "本文提出了“Zero to One”概念框架，用于开发基于人工智能(AI)的身份验证(IDV)产品。该框架包含四个核心组件：文档验证、生物特征验证、风险评估和流程编排，旨在提升IDV的安全性、隐私性、用户体验和合规性。通过整合AI技术，该模型为构建高效且可扩展的IDV解决方案提供了结构化方法，并强调了AI在推动下一代IDV产品中的关键作用。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.08734v1",
      "published_date": "2025-03-11 04:20:02 UTC",
      "updated_date": "2025-03-11 04:20:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:33:05.243424"
    },
    {
      "arxiv_id": "2503.08026v1",
      "title": "In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents",
      "title_zh": "前瞻与回溯：面向长期个性化对话代理的反思性记忆管理",
      "authors": [
        "Zhen Tan",
        "Jun Yan",
        "I-Hung Hsu",
        "Rujun Han",
        "Zifeng Wang",
        "Long T. Le",
        "Yiwen Song",
        "Yanfei Chen",
        "Hamid Palangi",
        "George Lee",
        "Anand Iyer",
        "Tianlong Chen",
        "Huan Liu",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "abstract": "Large Language Models (LLMs) have made significant progress in open-ended\ndialogue, yet their inability to retain and retrieve relevant information from\nlong-term interactions limits their effectiveness in applications requiring\nsustained personalization. External memory mechanisms have been proposed to\naddress this limitation, enabling LLMs to maintain conversational continuity.\nHowever, existing approaches struggle with two key challenges. First, rigid\nmemory granularity fails to capture the natural semantic structure of\nconversations, leading to fragmented and incomplete representations. Second,\nfixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user\ninteraction patterns. In this work, we propose Reflective Memory Management\n(RMM), a novel mechanism for long-term dialogue agents, integrating forward-\nand backward-looking reflections: (1) Prospective Reflection, which dynamically\nsummarizes interactions across granularities-utterances, turns, and\nsessions-into a personalized memory bank for effective future retrieval, and\n(2) Retrospective Reflection, which iteratively refines the retrieval in an\nonline reinforcement learning (RL) manner based on LLMs' cited evidence.\nExperiments show that RMM demonstrates consistent improvement across various\nmetrics and benchmarks. For example, RMM shows more than 10% accuracy\nimprovement over the baseline without memory management on the LongMemEval\ndataset.",
      "tldr_zh": "该研究提出了一种反思式记忆管理机制(RMM)，用于解决大语言模型(LLMs)在长期个性化对话中的记忆挑战。该系统创新性地结合了前瞻性反思(Prospective Reflection)和后顾性反思(Retrospective Reflection)：前者动态生成多粒度对话摘要(话语/轮次/会话)，后者通过强化学习优化记忆检索。实验表明，RMM在LongMemEval基准上比无记忆管理的基线模型准确率提升超过10%，有效解决了现有方法在记忆粒度固定和检索机制不灵活方面的问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08026v1",
      "published_date": "2025-03-11 04:15:52 UTC",
      "updated_date": "2025-03-11 04:15:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:33:23.592437"
    },
    {
      "arxiv_id": "2503.08732v1",
      "title": "Quantifying Circadian Desynchrony in ICU Patients and Its Association with Delirium",
      "title_zh": "量化ICU患者昼夜节律失调及其与谵妄的关联",
      "authors": [
        "Yuanfang Ren",
        "Andrea E. Davidson",
        "Jiaqing Zhang",
        "Miguel Contreras",
        "Ayush K. Patel",
        "Michelle Gumz",
        "Tezcan Ozrazgat-Baslanti",
        "Parisa Rashidi",
        "Azra Bihorac"
      ],
      "abstract": "Background: Circadian desynchrony characterized by the misalignment between\nan individual's internal biological rhythms and external environmental cues,\nsignificantly affects various physiological processes and health outcomes.\nQuantifying circadian desynchrony often requires prolonged and frequent\nmonitoring, and currently, an easy tool for this purpose is missing.\nAdditionally, its association with the incidence of delirium has not been\nclearly explored. Methods: A prospective observational study was carried out in\nintensive care units (ICU) of a tertiary hospital. Circadian transcriptomics of\nblood monocytes from 86 individuals were collected on two consecutive days,\nalthough a second sample could not be obtained from all participants. Using two\npublic datasets comprised of healthy volunteers, we replicated a model for\ndetermining internal circadian time. We developed an approach to quantify\ncircadian desynchrony by comparing internal circadian time and external blood\ncollection time. We applied the model and quantified circadian desynchrony\nindex among ICU patients, and investigated its association with the incidence\nof delirium. Results: The replicated model for determining internal circadian\ntime achieved comparable high accuracy. The quantified circadian desynchrony\nindex was significantly higher among critically ill ICU patients compared to\nhealthy subjects, with values of 10.03 hours vs 2.50-2.95 hours (p < 0.001).\nMost ICU patients had a circadian desynchrony index greater than 9 hours.\nAdditionally, the index was lower in patients whose blood samples were drawn\nafter 3pm, with values of 5.00 hours compared to 10.01-10.90 hours in other\ngroups (p < 0.001)...",
      "tldr_zh": "该研究开发了一种量化重症监护病房(ICU)患者昼夜节律失调(circadian desynchrony)的新方法。通过分析86名ICU患者的单核细胞昼夜转录组数据，结合健康人群数据集建立的生物钟预测模型，研究发现ICU患者的昼夜节律失调指数(平均10.03小时)显著高于健康人群(2.50-2.95小时)。研究还发现，下午3点后采血的患者失调程度较低(5.00小时)，且昼夜节律失调与谵妄发生率存在潜在关联。",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08732v1",
      "published_date": "2025-03-11 03:56:10 UTC",
      "updated_date": "2025-03-11 03:56:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:33:18.136416"
    },
    {
      "arxiv_id": "2503.08012v1",
      "title": "Exploring Bias in over 100 Text-to-Image Generative Models",
      "title_zh": "探索100多种文本到图像生成模型中的偏见",
      "authors": [
        "Jordan Vice",
        "Naveed Akhtar",
        "Richard Hartley",
        "Ajmal Mian"
      ],
      "abstract": "We investigate bias trends in text-to-image generative models over time,\nfocusing on the increasing availability of models through open platforms like\nHugging Face. While these platforms democratize AI, they also facilitate the\nspread of inherently biased models, often shaped by task-specific fine-tuning.\nEnsuring ethical and transparent AI deployment requires robust evaluation\nframeworks and quantifiable bias metrics. To this end, we assess bias across\nthree key dimensions: (i) distribution bias, (ii) generative hallucination, and\n(iii) generative miss-rate. Analyzing over 100 models, we reveal how bias\npatterns evolve over time and across generative tasks. Our findings indicate\nthat artistic and style-transferred models exhibit significant bias, whereas\nfoundation models, benefiting from broader training distributions, are becoming\nprogressively less biased. By identifying these systemic trends, we contribute\na large-scale evaluation corpus to inform bias research and mitigation\nstrategies, fostering more responsible AI development.\n  Keywords: Bias, Ethical AI, Text-to-Image, Generative Models, Open-Source\nModels",
      "tldr_zh": "本研究系统评估了100多个文本到图像生成模型的偏见趋势，重点关注通过Hugging Face等开放平台日益普及的模型。研究发现，艺术风格和风格迁移模型表现出显著的偏见，而得益于更广泛训练数据的基础模型则逐渐减少偏见。通过分析分布偏见、生成幻觉和生成失误率三个维度，揭示了偏见模式随时间和生成任务的演变规律。研究贡献了一个大规模评估语料库，为偏见研究和缓解策略提供了依据，推动了更负责任的AI发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICLR 2025 Workshop on Open Science for Foundation Models\n  (SCI-FM)",
      "pdf_url": "http://arxiv.org/pdf/2503.08012v1",
      "published_date": "2025-03-11 03:40:44 UTC",
      "updated_date": "2025-03-11 03:40:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:33:28.630600"
    },
    {
      "arxiv_id": "2503.08010v1",
      "title": "SKALD: Learning-Based Shot Assembly for Coherent Multi-Shot Video Creation",
      "title_zh": "SKALD：基于学习的镜头组装技术用于连贯多镜头视频创作",
      "authors": [
        "Chen Yi Lu",
        "Md Mehrab Tanjim",
        "Ishita Dasgupta",
        "Somdeb Sarkhel",
        "Gang Wu",
        "Saayan Mitra",
        "Somali Chaterji"
      ],
      "abstract": "We present SKALD, a multi-shot video assembly method that constructs coherent\nvideo sequences from candidate shots with minimal reliance on text. Central to\nour approach is the Learned Clip Assembly (LCA) score, a learning-based metric\nthat measures temporal and semantic relationships between shots to quantify\nnarrative coherence. We tackle the exponential complexity of combining multiple\nshots with an efficient beam-search algorithm guided by the LCA score. To train\nour model effectively with limited human annotations, we propose two tasks for\nthe LCA encoder: Shot Coherence Learning, which uses contrastive learning to\ndistinguish coherent and incoherent sequences, and Feature Regression, which\nconverts these learned representations into a real-valued coherence score. We\ndevelop two variants: a base SKALD model that relies solely on visual coherence\nand SKALD-text, which integrates auxiliary text information when available.\nExperiments on the VSPD and our curated MSV3C datasets show that SKALD achieves\nan improvement of up to 48.6% in IoU and a 43% speedup over the\nstate-of-the-art methods. A user study further validates our approach, with 45%\nof participants favoring SKALD-assembled videos, compared to 22% preferring\ntext-based assembly methods.",
      "tldr_zh": "本研究提出SKALD，一种基于学习的多镜头视频组装方法，通过创新性Learned Clip Assembly (LCA)评分系统衡量镜头间的时序与语义关系来保证叙事连贯性。该系统采用高效beam-search算法处理组合爆炸问题，并创新性地结合对比学习(Shot Coherence Learning)和特征回归(Feature Regression)两种训练任务，在有限标注数据下实现有效训练。实验表明，SKALD在VSPD和MSV3C数据集上相比现有方法取得48.6%的IoU提升和43%的速度优势，用户调研显示其生成视频的偏好率(45%)是文本基方法(22%)的两倍。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08010v1",
      "published_date": "2025-03-11 03:25:44 UTC",
      "updated_date": "2025-03-11 03:25:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:33:37.751000"
    },
    {
      "arxiv_id": "2503.08007v1",
      "title": "MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models",
      "title_zh": "MoRE：解锁四足机器人视觉-语言-动作模型中强化学习的可扩展性",
      "authors": [
        "Han Zhao",
        "Wenxuan Song",
        "Donglin Wang",
        "Xinyang Tong",
        "Pengxiang Ding",
        "Xuelian Cheng",
        "Zongyuan Ge"
      ],
      "abstract": "Developing versatile quadruped robots that can smoothly perform various\nactions and tasks in real-world environments remains a significant challenge.\nThis paper introduces a novel vision-language-action (VLA) model, mixture of\nrobotic experts (MoRE), for quadruped robots that aim to introduce\nreinforcement learning (RL) for fine-tuning large-scale VLA models with a large\namount of mixed-quality data. MoRE integrates multiple low-rank adaptation\nmodules as distinct experts within a dense multi-modal large language model\n(MLLM), forming a sparse-activated mixture-of-experts model. This design\nenables the model to effectively adapt to a wide array of downstream tasks.\nMoreover, we employ a reinforcement learning-based training objective to train\nour model as a Q-function after deeply exploring the structural properties of\nour tasks. Effective learning from automatically collected mixed-quality data\nenhances data efficiency and model performance. Extensive experiments\ndemonstrate that MoRE outperforms all baselines across six different skills and\nexhibits superior generalization capabilities in out-of-distribution scenarios.\nWe further validate our method in real-world scenarios, confirming the\npracticality of our approach and laying a solid foundation for future research\non multi-task learning in quadruped robots.",
      "tldr_zh": "本研究提出了MoRE（Mixture of Robotic Experts），一种用于四足机器人的视觉-语言-动作（VLA）模型，旨在通过强化学习（RL）对大规模VLA模型进行微调。MoRE通过集成多个低秩适应模块作为多模态大语言模型（MLLM）中的专家，形成一个稀疏激活的专家混合模型，从而有效适应多种下游任务。实验表明，MoRE在六种不同技能上均优于基线模型，并在分布外场景中表现出优异的泛化能力，为四足机器人的多任务学习奠定了坚实基础。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.08007v1",
      "published_date": "2025-03-11 03:13:45 UTC",
      "updated_date": "2025-03-11 03:13:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:34:09.835323"
    },
    {
      "arxiv_id": "2503.08006v1",
      "title": "Injecting Imbalance Sensitivity for Multi-Task Learning",
      "title_zh": "注入不平衡敏感性：面向多任务学习的优化方法",
      "authors": [
        "Zhipeng Zhou",
        "Liu Liu",
        "Peilin Zhao",
        "Wei Gong"
      ],
      "abstract": "Multi-task learning (MTL) has emerged as a promising approach for deploying\ndeep learning models in real-life applications. Recent studies have proposed\noptimization-based learning paradigms to establish task-shared representations\nin MTL. However, our paper empirically argues that these studies, specifically\ngradient-based ones, primarily emphasize the conflict issue while neglecting\nthe potentially more significant impact of imbalance/dominance in MTL. In line\nwith this perspective, we enhance the existing baseline method by injecting\nimbalance-sensitivity through the imposition of constraints on the projected\nnorms. To demonstrate the effectiveness of our proposed IMbalance-sensitive\nGradient (IMGrad) descent method, we evaluate it on multiple mainstream MTL\nbenchmarks, encompassing supervised learning tasks as well as reinforcement\nlearning. The experimental results consistently demonstrate competitive\nperformance.",
      "tldr_zh": "本文提出了一种名为IMGrad的失衡敏感梯度下降方法，旨在解决多任务学习(MTL)中任务间失衡/主导问题。与现有方法主要关注任务冲突不同，该方法通过对投影范数施加约束，增强了基线方法的失衡敏感性。实验在多个主流MTL基准上进行，涵盖监督学习和强化学习任务，结果表明IMGrad方法在性能上具有竞争力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 6 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.08006v1",
      "published_date": "2025-03-11 03:11:54 UTC",
      "updated_date": "2025-03-11 03:11:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:34:07.330312"
    },
    {
      "arxiv_id": "2503.07996v3",
      "title": "SQLCritic: Correcting Text-to-SQL Generation via Clause-wise Critic",
      "title_zh": "SQLCritic：基于子句级批评器的文本到SQL生成校正",
      "authors": [
        "Jikai Chen"
      ],
      "abstract": "Recent advancements in Text-to-SQL systems have improved the conversion of\nnatural language queries into SQL, but challenges remain in ensuring accuracy\nand reliability. While self-correction techniques refine outputs, they often\nintroduce new errors. Existing methods focused on execution feedback mainly\naddress syntax issues, leaving semantic errors -- where the query's logic fails\nto align with the user's intent -- largely unaddressed. We propose a novel\napproach combining structured execution feedback with a trained critic agent\nthat provides detailed, interpretable critiques. This method effectively\nidentifies and corrects both syntactic and semantic errors, enhancing accuracy\nand interpretability. Experimental results show significant improvements on two\nmajor Text-to-SQL benchmarks, Spider and BIRD, demonstrating the effectiveness\nof our approach.",
      "tldr_zh": "该研究提出SQLCritic，一种通过分句式批评（Clause-wise Critic）改进Text-to-SQL生成的新方法。针对现有自我修正技术容易引入新错误、且主要解决语法而非语义错误的问题，该方法结合结构化执行反馈与训练好的批评智能体，提供可解释的详细修正建议。实验表明，该方法在Spider和BIRD两大基准测试中显著提升准确率，能同时有效纠正语法和语义错误。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07996v3",
      "published_date": "2025-03-11 02:52:39 UTC",
      "updated_date": "2025-03-17 02:57:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:34:04.672164"
    },
    {
      "arxiv_id": "2503.07993v1",
      "title": "LLM-Powered Knowledge Graphs for Enterprise Intelligence and Analytics",
      "title_zh": "LLM驱动的知识图谱在企业智能与分析中的应用",
      "authors": [
        "Rajeev Kumar",
        "Kumar Ishan",
        "Harishankar Kumar",
        "Abhinandan Singla"
      ],
      "abstract": "Disconnected data silos within enterprises obstruct the extraction of\nactionable insights, diminishing efficiency in areas such as product\ndevelopment, client engagement, meeting preparation, and analytics-driven\ndecision-making. This paper introduces a framework that uses large language\nmodels (LLMs) to unify various data sources into a comprehensive,\nactivity-centric knowledge graph. The framework automates tasks such as entity\nextraction, relationship inference, and semantic enrichment, enabling advanced\nquerying, reasoning, and analytics across data types like emails, calendars,\nchats, documents, and logs. Designed for enterprise flexibility, it supports\napplications such as contextual search, task prioritization, expertise\ndiscovery, personalized recommendations, and advanced analytics to identify\ntrends and actionable insights. Experimental results demonstrate its success in\nthe discovery of expertise, task management, and data-driven decision making.\nBy integrating LLMs with knowledge graphs, this solution bridges disconnected\nsystems and delivers intelligent analytics-powered enterprise tools.",
      "tldr_zh": "该研究提出了一种利用大语言模型(LLMs)构建企业知识图谱的框架，旨在解决企业内部数据孤岛问题。该框架通过自动执行实体抽取、关系推断和语义增强等任务，将电子邮件、日历、聊天记录等多种数据类型整合为统一的活动中心型知识图谱。实验表明，该方法在专业知识发现、任务管理和数据驱动决策等企业应用场景中表现优异，支持上下文搜索、任务优先级排序等智能分析功能，有效提升了企业运营效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07993v1",
      "published_date": "2025-03-11 02:50:45 UTC",
      "updated_date": "2025-03-11 02:50:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:34:20.932387"
    },
    {
      "arxiv_id": "2503.07994v1",
      "title": "A Neural Symbolic Model for Space Physics",
      "title_zh": "空间物理的神经符号模型",
      "authors": [
        "Jie Ying",
        "Haowei Lin",
        "Chao Yue",
        "Yajie Chen",
        "Chao Xiao",
        "Quanqi Shi",
        "Yitao Liang",
        "Shing-Tung Yau",
        "Yuan Zhou",
        "Jianzhu Ma"
      ],
      "abstract": "In this study, we unveil a new AI model, termed PhyE2E, to discover physical\nformulas through symbolic regression. PhyE2E simplifies symbolic regression by\ndecomposing it into sub-problems using the second-order derivatives of an\noracle neural network, and employs a transformer model to translate data into\nsymbolic formulas in an end-to-end manner. The resulting formulas are refined\nthrough Monte-Carlo Tree Search and Genetic Programming. We leverage a large\nlanguage model to synthesize extensive symbolic expressions resembling real\nphysics, and train the model to recover these formulas directly from data. A\ncomprehensive evaluation reveals that PhyE2E outperforms existing\nstate-of-the-art approaches, delivering superior symbolic accuracy, precision\nin data fitting, and consistency in physical units. We deployed PhyE2E to five\napplications in space physics, including the prediction of sunspot numbers,\nsolar rotational angular velocity, emission line contribution functions,\nnear-Earth plasma pressure, and lunar-tide plasma signals. The physical\nformulas generated by AI demonstrate a high degree of accuracy in fitting the\nexperimental data from satellites and astronomical telescopes. We have\nsuccessfully upgraded the formula proposed by NASA in 1993 regarding solar\nactivity, and for the first time, provided the explanations for the long cycle\nof solar activity in an explicit form. We also found that the decay of\nnear-Earth plasma pressure is proportional to r^2 to Earth, where subsequent\nmathematical derivations are consistent with satellite data from another\nindependent study. Moreover, we found physical formulas that can describe the\nrelationships between emission lines in the extreme ultraviolet spectrum of the\nSun, temperatures, electron densities, and magnetic fields. The formula\nobtained is consistent with the properties that physicists had previously\nhypothesized it should possess.",
      "tldr_zh": "本研究提出了一种名为PhyE2E的神经符号模型，用于通过符号回归发现物理公式。该模型创新性地结合了二阶导数神经网络、Transformer和蒙特卡洛树搜索等方法，实现了从数据到物理公式的端到端推导。在空间物理学的五项应用中，该模型不仅显著提升了NASA 1993年提出的太阳活动公式，还首次以显式形式解释了太阳活动的长周期现象，并发现了近地等离子体压力衰减与距离平方成正比等新规律。实验表明，该模型生成的公式在拟合卫星和天文望远镜数据方面表现出色，其符号准确性、数据拟合精度和物理单位一致性均优于现有方法。",
      "categories": [
        "astro-ph.SR",
        "astro-ph.EP",
        "astro-ph.IM",
        "cs.AI",
        "physics.space-ph"
      ],
      "primary_category": "astro-ph.SR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07994v1",
      "published_date": "2025-03-11 02:50:45 UTC",
      "updated_date": "2025-03-11 02:50:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:34:28.087541"
    },
    {
      "arxiv_id": "2503.07992v1",
      "title": "Efficient and Accurate Estimation of Lipschitz Constants for Hybrid Quantum-Classical Decision Models",
      "title_zh": "高效准确估计混合量子-经典决策模型的Lipschitz常数",
      "authors": [
        "Sajjad Hashemian",
        "Mohammad Saeed Arvenaghi"
      ],
      "abstract": "In this paper, we propose a novel framework for efficiently and accurately\nestimating Lipschitz constants in hybrid quantum-classical decision models. Our\napproach integrates classical neural network with quantum variational circuits\nto address critical issues in learning theory such as fairness verification,\nrobust training, and generalization.\n  By a unified convex optimization formulation, we extend existing classical\nmethods to capture the interplay between classical and quantum layers. This\nintegrated strategy not only provide a tight bound on the Lipschitz constant\nbut also improves computational efficiency with respect to the previous\nmethods.",
      "tldr_zh": "该研究提出了一种新颖的框架，用于高效准确地估计混合量子-经典决策模型的Lipschitz常数。该方法将经典神经网络与量子变分电路相结合，通过统一的凸优化公式，扩展现有经典方法以捕捉经典层和量子层之间的相互作用。这一集成策略不仅提供了对Lipschitz常数的严格约束，还显著提高了计算效率，为公平性验证、鲁棒训练和泛化等学习理论中的关键问题提供了解决方案。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "14 pages, 5 figuers, Submitted to TASE 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.07992v1",
      "published_date": "2025-03-11 02:50:16 UTC",
      "updated_date": "2025-03-11 02:50:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:35:17.967786"
    },
    {
      "arxiv_id": "2503.07991v1",
      "title": "Boundary Prompting: Elastic Urban Region Representation via Graph-based Spatial Tokenization",
      "title_zh": "边界提示：基于图的空间标记化实现弹性城市区域表征",
      "authors": [
        "Haojia Zhu",
        "Jiahui Jin",
        "Dong Kan",
        "Rouxi Shen",
        "Ruize Wang",
        "Xiangguo Sun",
        "Jinghui Zhang"
      ],
      "abstract": "Urban region representation is essential for various applications such as\nurban planning, resource allocation, and policy development. Traditional\nmethods rely on fixed, predefined region boundaries, which fail to capture the\ndynamic and complex nature of real-world urban areas. In this paper, we propose\nthe Boundary Prompting Urban Region Representation Framework (BPURF), a novel\napproach that allows for elastic urban region definitions. BPURF comprises two\nkey components: (1) A spatial token dictionary, where urban entities are\ntreated as tokens and integrated into a unified token graph, and (2) a region\ntoken set representation model which utilize token aggregation and a\nmulti-channel model to embed token sets corresponding to region boundaries.\nAdditionally, we propose fast token set extraction strategy to enable online\ntoken set extraction during training and prompting. This framework enables the\ndefinition of urban regions through boundary prompting, supporting varying\nregion boundaries and adapting to different tasks. Extensive experiments\ndemonstrate the effectiveness of BPURF in capturing the complex characteristics\nof urban regions.",
      "tldr_zh": "该研究提出了一种基于边界提示的弹性城市区域表征框架（BPURF），用于解决传统固定边界方法难以捕捉城市动态复杂特性的问题。该框架通过空间词表字典将城市实体编码为统一词图，并采用多通道模型对区域边界对应的词集进行表征。创新性地引入快速词集提取策略，支持训练和推理过程中的在线边界调整，使区域定义能根据不同任务需求弹性变化。实验验证了该方法在表征复杂城市区域特征方面的优越性，为城市规划、资源分配等应用提供了灵活有效的分析工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07991v1",
      "published_date": "2025-03-11 02:49:58 UTC",
      "updated_date": "2025-03-11 02:49:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:34:52.744209"
    },
    {
      "arxiv_id": "2503.07988v1",
      "title": "Provable Zero-Shot Generalization in Offline Reinforcement Learning",
      "title_zh": "可证明的离线强化学习零样本泛化能力",
      "authors": [
        "Zhiyong Wang",
        "Chen Yang",
        "John C. S. Lui",
        "Dongruo Zhou"
      ],
      "abstract": "In this work, we study offline reinforcement learning (RL) with zero-shot\ngeneralization property (ZSG), where the agent has access to an offline dataset\nincluding experiences from different environments, and the goal of the agent is\nto train a policy over the training environments which performs well on test\nenvironments without further interaction. Existing work showed that classical\noffline RL fails to generalize to new, unseen environments. We propose\npessimistic empirical risk minimization (PERM) and pessimistic proximal policy\noptimization (PPPO), which leverage pessimistic policy evaluation to guide\npolicy learning and enhance generalization. We show that both PERM and PPPO are\ncapable of finding a near-optimal policy with ZSG. Our result serves as a first\nstep in understanding the foundation of the generalization phenomenon in\noffline reinforcement learning.",
      "tldr_zh": "该研究提出了两种具有零样本泛化能力(ZSG)的离线强化学习算法：悲观经验风险最小化(PERM)和悲观近端策略优化(PPPO)。这些方法通过悲观策略评估来指导策略学习，解决了传统离线强化学习在新环境中泛化失败的问题。理论证明表明，PERM和PPPO能够在不同训练环境中学习策略，并在未见过的测试环境中找到接近最优的策略，为理解离线强化学习的泛化现象奠定了理论基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 1 figure, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.07988v1",
      "published_date": "2025-03-11 02:44:32 UTC",
      "updated_date": "2025-03-11 02:44:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:34:57.069973"
    },
    {
      "arxiv_id": "2503.10677v2",
      "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
      "title_zh": "知识导向的检索增强生成技术综述",
      "authors": [
        "Mingyue Cheng",
        "Yucong Luo",
        "Jie Ouyang",
        "Qi Liu",
        "Huijie Liu",
        "Li Li",
        "Shuo Yu",
        "Bohou Zhang",
        "Jiawei Cao",
        "Jie Ma",
        "Daoyu Wang",
        "Enhong Chen"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has gained significant attention in\nrecent years for its potential to enhance natural language understanding and\ngeneration by combining large-scale retrieval systems with generative models.\nRAG leverages external knowledge sources, such as documents, databases, or\nstructured data, to improve model performance and generate more accurate and\ncontextually relevant outputs. This survey aims to provide a comprehensive\noverview of RAG by examining its fundamental components, including retrieval\nmechanisms, generation processes, and the integration between the two. We\ndiscuss the key characteristics of RAG, such as its ability to augment\ngenerative models with dynamic external knowledge, and the challenges\nassociated with aligning retrieved information with generative objectives. We\nalso present a taxonomy that categorizes RAG methods, ranging from basic\nretrieval-augmented approaches to more advanced models incorporating\nmulti-modal data and reasoning capabilities. Additionally, we review the\nevaluation benchmarks and datasets commonly used to assess RAG systems, along\nwith a detailed exploration of its applications in fields such as question\nanswering, summarization, and information retrieval. Finally, we highlight\nemerging research directions and opportunities for improving RAG systems, such\nas enhanced retrieval efficiency, model interpretability, and domain-specific\nadaptations. This paper concludes by outlining the prospects for RAG in\naddressing real-world challenges and its potential to drive further\nadvancements in natural language processing.",
      "tldr_zh": "本文综述了检索增强生成(RAG)技术的研究进展，该技术通过结合检索系统与生成模型来提升自然语言处理性能。文章系统分析了RAG的核心组件，包括检索机制、生成过程及其集成方法，并提出了涵盖基础到多模态推理的RAG方法分类体系。研究还总结了RAG在问答、摘要等任务中的应用现状，并指出提升检索效率、可解释性等未来发展方向，为推进NLP技术提供了重要参考框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10677v2",
      "published_date": "2025-03-11 01:59:35 UTC",
      "updated_date": "2025-03-17 11:24:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:35:06.641078"
    },
    {
      "arxiv_id": "2503.07963v2",
      "title": "Hierarchical Contact-Rich Trajectory Optimization for Multi-Modal Manipulation using Tight Convex Relaxations",
      "title_zh": "基于紧密凸松弛的多模态操作分层接触丰富轨迹优化",
      "authors": [
        "Yuki Shirai",
        "Arvind Raghunathan",
        "Devesh K. Jha"
      ],
      "abstract": "Designing trajectories for manipulation through contact is challenging as it\nrequires reasoning of object \\& robot trajectories as well as complex contact\nsequences simultaneously. In this paper, we present a novel framework for\nsimultaneously designing trajectories of robots, objects, and contacts\nefficiently for contact-rich manipulation. We propose a hierarchical\noptimization framework where Mixed-Integer Linear Program (MILP) selects\noptimal contacts between robot \\& object using approximate dynamical\nconstraints, and then a NonLinear Program (NLP) optimizes trajectory of the\nrobot(s) and object considering full nonlinear constraints. We present a convex\nrelaxation of bilinear constraints using binary encoding technique such that\nMILP can provide tighter solutions with better computational complexity. The\nproposed framework is evaluated on various manipulation tasks where it can\nreason about complex multi-contact interactions while providing computational\nadvantages. We also demonstrate our framework in hardware experiments using a\nbimanual robot system. The video summarizing this paper and hardware\nexperiments is found https://youtu.be/s2S1Eg5RsRE?si=chPkftz_a3NAHxLq",
      "tldr_zh": "该研究提出了一种分层优化框架，用于高效解决接触式操作中的多模态轨迹规划问题。通过混合整数线性规划(MILP)选择最优接触点，并结合非线性规划(NLP)优化完整轨迹，实现了对机器人、物体运动和复杂接触序列的同步规划。创新性地采用二进制编码技术对双线性约束进行紧致凸松弛，在保证计算效率的同时提高了求解精度。该框架在多种操作任务中验证了有效性，并成功应用于双手机器人系统的硬件实验。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "2025 IEEE International Conference on Robotics and Automation (2025\n  ICRA)",
      "pdf_url": "http://arxiv.org/pdf/2503.07963v2",
      "published_date": "2025-03-11 01:40:23 UTC",
      "updated_date": "2025-03-12 01:43:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:35:24.754530"
    },
    {
      "arxiv_id": "2503.07956v1",
      "title": "EFPC: Towards Efficient and Flexible Prompt Compression",
      "title_zh": "EFPC：迈向高效灵活的提示压缩方法",
      "authors": [
        "Yun-Hao Cao",
        "Yangsong Wang",
        "Shuzheng Hao",
        "Zhenxing Li",
        "Chengjun Zhan",
        "Sichao Liu",
        "Yi-Qi Hu"
      ],
      "abstract": "The emergence of large language models (LLMs) like GPT-4 has revolutionized\nnatural language processing (NLP), enabling diverse, complex tasks. However,\nextensive token counts lead to high computational and financial burdens. To\naddress this, we propose Efficient and Flexible Prompt Compression (EFPC), a\nnovel method unifying task-aware and task-agnostic compression for a favorable\naccuracy-efficiency trade-off. EFPC uses GPT-4 to generate compressed prompts\nand integrates them with original prompts for training. During training and\ninference, we selectively prepend user instructions and compress prompts based\non predicted probabilities. EFPC is highly data-efficient, achieving\nsignificant performance with minimal data. Compared to the state-of-the-art\nmethod LLMLingua-2, EFPC achieves a 4.8% relative improvement in F1-score with\n1% additional data at a 4x compression rate, and an 11.4% gain with 10%\nadditional data on the LongBench single-doc QA benchmark. EFPC's unified\nframework supports broad applicability and enhances performance across various\nmodels, tasks, and domains, offering a practical advancement in NLP.",
      "tldr_zh": "该研究提出EFPC（高效灵活提示压缩）方法，通过统一任务感知(task-aware)和任务无关(task-agnostic)的提示压缩策略，在GPT-4等大语言模型(LLMs)应用中实现精度与效率的平衡。该方法利用GPT-4生成压缩提示并与原始提示结合训练，根据预测概率动态选择是否压缩提示，具有极高的数据效率。实验表明，EFPC在4倍压缩率下仅需1%额外数据即可比当前最佳方法LLMLingua-2提升4.8%的F1分数，且在多种模型和任务上展现出广泛适用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.07956v1",
      "published_date": "2025-03-11 01:34:03 UTC",
      "updated_date": "2025-03-11 01:34:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:35:26.647080"
    },
    {
      "arxiv_id": "2503.09626v1",
      "title": "Certainly Bot Or Not? Trustworthy Social Bot Detection via Robust Multi-Modal Neural Processes",
      "title_zh": "“确定是机器人吗？”：基于鲁棒多模态神经过程的可信社交机器人检测",
      "authors": [
        "Qi Wu",
        "Yingguang Yang",
        "hao liu",
        "Hao Peng",
        "Buyun He",
        "Yutong Xia",
        "Yong Liao"
      ],
      "abstract": "Social bot detection is crucial for mitigating misinformation, online\nmanipulation, and coordinated inauthentic behavior. While existing neural\nnetwork-based detectors perform well on benchmarks, they struggle with\ngeneralization due to distribution shifts across datasets and frequently\nproduce overconfident predictions for out-of-distribution accounts beyond the\ntraining data. To address this, we introduce a novel Uncertainty Estimation for\nSocial Bot Detection (UESBD) framework, which quantifies the predictive\nuncertainty of detectors beyond mere classification. For this task, we propose\nRobust Multi-modal Neural Processes (RMNP), which aims to enhance the\nrobustness of multi-modal neural processes to modality inconsistencies caused\nby social bot camouflage. RMNP first learns unimodal representations through\nmodality-specific encoders. Then, unimodal attentive neural processes are\nemployed to encode the Gaussian distribution of unimodal latent variables.\nFurthermore, to avoid social bots stealing human features to camouflage\nthemselves thus causing certain modalities to provide conflictive information,\nwe introduce an evidential gating network to explicitly model the reliability\nof modalities. The joint latent distribution is learned through the generalized\nproduct of experts, which takes the reliability of each modality into\nconsideration during fusion. The final prediction is obtained through Monte\nCarlo sampling of the joint latent distribution followed by a decoder.\nExperiments on three real-world benchmarks show the effectiveness of RMNP in\nclassification and uncertainty estimation, as well as its robustness to\nmodality conflicts.",
      "tldr_zh": "该研究提出了一种基于鲁棒多模态神经过程(RMNP)的可信社交机器人检测框架，通过量化预测不确定性解决现有方法在数据分布偏移下的泛化问题。该方法采用模态特定编码器学习单模态表征，通过注意力神经过程编码高斯分布，并创新性地引入证据门控网络显式建模各模态可靠性。实验表明，该框架在三个真实基准数据集上有效提升了分类性能和不确定性估计能力，同时增强了对社交机器人伪装导致模态冲突的鲁棒性。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "12 pages. 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.09626v1",
      "published_date": "2025-03-11 01:32:52 UTC",
      "updated_date": "2025-03-11 01:32:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:35:56.279589"
    },
    {
      "arxiv_id": "2503.08729v1",
      "title": "Preserving Product Fidelity in Large Scale Image Recontextualization with Diffusion Models",
      "title_zh": "利用扩散模型在大规模图像重设上下文中保持产品保真度",
      "authors": [
        "Ishaan Malhi",
        "Praneet Dutta",
        "Ellie Talius",
        "Sally Ma",
        "Brendan Driscoll",
        "Krista Holden",
        "Garima Pruthi",
        "Arunachalam Narayanaswamy"
      ],
      "abstract": "We present a framework for high-fidelity product image recontextualization\nusing text-to-image diffusion models and a novel data augmentation pipeline.\nThis pipeline leverages image-to-video diffusion, in/outpainting & negatives to\ncreate synthetic training data, addressing limitations of real-world data\ncollection for this task. Our method improves the quality and diversity of\ngenerated images by disentangling product representations and enhancing the\nmodel's understanding of product characteristics. Evaluation on the ABO dataset\nand a private product dataset, using automated metrics and human assessment,\ndemonstrates the effectiveness of our framework in generating realistic and\ncompelling product visualizations, with implications for applications such as\ne-commerce and virtual product showcasing.",
      "tldr_zh": "该研究提出了一种基于扩散模型的高保真产品图像重定位框架，通过创新的数据增强流程（结合图像到视频扩散、内外绘制和负样本生成技术）解决真实数据收集的局限性。该方法通过解耦产品表征和增强模型对产品特性的理解，显著提升了生成图像的质量和多样性。在ABO数据集和私有产品数据集上的评估表明，该框架能生成逼真且吸引人的产品可视化效果，对电子商务和虚拟产品展示具有重要应用价值。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08729v1",
      "published_date": "2025-03-11 01:24:39 UTC",
      "updated_date": "2025-03-11 01:24:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:35:35.674708"
    },
    {
      "arxiv_id": "2503.08728v1",
      "title": "Enhancing Traffic Signal Control through Model-based Reinforcement Learning and Policy Reuse",
      "title_zh": "通过基于模型的强化学习与策略复用增强交通信号控制",
      "authors": [
        "Yihong Li",
        "Chengwei Zhang",
        "Furui Zhan",
        "Wanting Liu",
        "Kailing Zhou",
        "Longji Zheng"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) has shown significant potential in\ntraffic signal control (TSC). However, current MARL-based methods often suffer\nfrom insufficient generalization due to the fixed traffic patterns and road\nnetwork conditions used during training. This limitation results in poor\nadaptability to new traffic scenarios, leading to high retraining costs and\ncomplex deployment. To address this challenge, we propose two algorithms:\nPLight and PRLight. PLight employs a model-based reinforcement learning\napproach, pretraining control policies and environment models using predefined\nsource-domain traffic scenarios. The environment model predicts the state\ntransitions, which facilitates the comparison of environmental features.\nPRLight further enhances adaptability by adaptively selecting pre-trained\nPLight agents based on the similarity between the source and target domains to\naccelerate the learning process in the target domain. We evaluated the\nalgorithms through two transfer settings: (1) adaptability to different traffic\nscenarios within the same road network, and (2) generalization across different\nroad networks. The results show that PRLight significantly reduces the\nadaptation time compared to learning from scratch in new TSC scenarios,\nachieving optimal performance using similarities between available and target\nscenarios.",
      "tldr_zh": "该研究提出PLight和PRLight两种算法，通过基于模型的强化学习方法提升交通信号控制（TSC）的泛化能力。PLight采用预训练策略和环境模型来预测状态转移，而PRLight通过自适应选择预训练代理来加速目标领域的学习过程。实验表明，PRLight在新场景中显著减少适应时间，并能跨不同路网实现最优性能。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08728v1",
      "published_date": "2025-03-11 01:21:13 UTC",
      "updated_date": "2025-03-11 01:21:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:35:44.049151"
    },
    {
      "arxiv_id": "2503.07946v1",
      "title": "7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting",
      "title_zh": "7DGS：统一时空角高斯泼溅框架",
      "authors": [
        "Zhongpai Gao",
        "Benjamin Planche",
        "Meng Zheng",
        "Anwesa Choudhuri",
        "Terrence Chen",
        "Ziyan Wu"
      ],
      "abstract": "Real-time rendering of dynamic scenes with view-dependent effects remains a\nfundamental challenge in computer graphics. While recent advances in Gaussian\nSplatting have shown promising results separately handling dynamic scenes\n(4DGS) and view-dependent effects (6DGS), no existing method unifies these\ncapabilities while maintaining real-time performance. We present 7D Gaussian\nSplatting (7DGS), a unified framework representing scene elements as\nseven-dimensional Gaussians spanning position (3D), time (1D), and viewing\ndirection (3D). Our key contribution is an efficient conditional slicing\nmechanism that transforms 7D Gaussians into view- and time-conditioned 3D\nGaussians, maintaining compatibility with existing 3D Gaussian Splatting\npipelines while enabling joint optimization. Experiments demonstrate that 7DGS\noutperforms prior methods by up to 7.36 dB in PSNR while achieving real-time\nrendering (401 FPS) on challenging dynamic scenes with complex view-dependent\neffects. The project page is: https://gaozhongpai.github.io/7dgs/.",
      "tldr_zh": "本文提出了7D高斯泼溅(7DGS)方法，首次将动态场景(4DGS)和视角相关效果(6DGS)统一到一个实时渲染框架中。该方法通过创新的条件切片机制，将七维(位置3D+时间1D+视角方向3D)高斯函数转化为特定视角和时间的3D高斯表示，兼容现有3D高斯泼溅管线。实验表明，7DGS在保持实时渲染性能(401FPS)的同时，PSNR指标最高提升7.36dB，能有效处理具有复杂视角相关效果的动态场景。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07946v1",
      "published_date": "2025-03-11 01:16:08 UTC",
      "updated_date": "2025-03-11 01:16:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:35:58.570320"
    },
    {
      "arxiv_id": "2503.08727v1",
      "title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation",
      "title_zh": "训练即插即用知识模块的深度上下文蒸馏方法",
      "authors": [
        "Lucas Caccia",
        "Alan Ansell",
        "Edoardo Ponti",
        "Ivan Vulić",
        "Alessandro Sordoni"
      ],
      "abstract": "Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and retrieval-augmented generation.",
      "tldr_zh": "该研究提出了一种基于深度上下文蒸馏(Deep Context Distillation)的可插拔知识模块(KMs)训练方法，用于解决大语言模型在动态整合新知识时面临的挑战。这些轻量级的LoRA模块能够存储文档级知识，可按需插入模型，相比传统的下一个词预测方法，该技术通过模拟教师模型的隐藏状态和logits来更有效地捕获全局文档信息。实验表明，该方法在两个数据集上优于标准训练技术，并与检索增强生成(RAG)存在协同效应。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.08727v1",
      "published_date": "2025-03-11 01:07:57 UTC",
      "updated_date": "2025-03-11 01:07:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:36:16.567317"
    },
    {
      "arxiv_id": "2503.08726v1",
      "title": "SIMAC: A Semantic-Driven Integrated Multimodal Sensing And Communication Framework",
      "title_zh": "SIMAC：语义驱动的集成多模态感知与通信框架",
      "authors": [
        "Yubo Peng",
        "Luping Xiang",
        "Kun Yang",
        "Feibo Jiang",
        "Kezhi Wang",
        "Dapeng Oliver Wu"
      ],
      "abstract": "Traditional single-modality sensing faces limitations in accuracy and\ncapability, and its decoupled implementation with communication systems\nincreases latency in bandwidth-constrained environments. Additionally,\nsingle-task-oriented sensing systems fail to address users' diverse demands. To\novercome these challenges, we propose a semantic-driven integrated multimodal\nsensing and communication (SIMAC) framework. This framework leverages a joint\nsource-channel coding architecture to achieve simultaneous sensing decoding and\ntransmission of sensing results. Specifically, SIMAC first introduces a\nmultimodal semantic fusion (MSF) network, which employs two extractors to\nextract semantic information from radar signals and images, respectively. MSF\nthen applies cross-attention mechanisms to fuse these unimodal features and\ngenerate multimodal semantic representations. Secondly, we present a large\nlanguage model (LLM)-based semantic encoder (LSE), where relevant communication\nparameters and multimodal semantics are mapped into a unified latent space and\ninput to the LLM, enabling channel-adaptive semantic encoding. Thirdly, a\ntask-oriented sensing semantic decoder (SSD) is proposed, in which different\ndecoded heads are designed according to the specific needs of tasks.\nSimultaneously, a multi-task learning strategy is introduced to train the SIMAC\nframework, achieving diverse sensing services. Finally, experimental\nsimulations demonstrate that the proposed framework achieves diverse sensing\nservices and higher accuracy.",
      "tldr_zh": "该研究提出了SIMAC框架，一种语义驱动的集成多模态感知与通信系统，旨在解决传统单模态感知精度低、与通信系统分离导致的延迟问题。框架通过联合源信道编码架构实现感知结果的同步解码与传输，核心包括：1）多模态语义融合网络（MSF），利用交叉注意力机制融合雷达信号和图像的语义信息；2）基于大语言模型（LLM）的语义编码器（LSE），将通信参数与多模态语义映射到统一潜在空间；3）任务导向的感知语义解码器（SSD），支持多任务学习策略。实验表明，SIMAC能够提供多样化的感知服务并显著提高精度。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.08726v1",
      "published_date": "2025-03-11 01:04:42 UTC",
      "updated_date": "2025-03-11 01:04:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:36:28.268480"
    },
    {
      "arxiv_id": "2503.07937v1",
      "title": "LLM-based Corroborating and Refuting Evidence Retrieval for Scientific Claim Verification",
      "title_zh": "基于大语言模型的科学声明验证：支持与反驳证据检索",
      "authors": [
        "Siyuan Wang",
        "James R. Foulds",
        "Md Osman Gani",
        "Shimei Pan"
      ],
      "abstract": "In this paper, we introduce CIBER (Claim Investigation Based on Evidence\nRetrieval), an extension of the Retrieval-Augmented Generation (RAG) framework\ndesigned to identify corroborating and refuting documents as evidence for\nscientific claim verification. CIBER addresses the inherent uncertainty in\nLarge Language Models (LLMs) by evaluating response consistency across diverse\ninterrogation probes. By focusing on the behavioral analysis of LLMs without\nrequiring access to their internal information, CIBER is applicable to both\nwhite-box and black-box models. Furthermore, CIBER operates in an unsupervised\nmanner, enabling easy generalization across various scientific domains.\nComprehensive evaluations conducted using LLMs with varying levels of\nlinguistic proficiency reveal CIBER's superior performance compared to\nconventional RAG approaches. These findings not only highlight the\neffectiveness of CIBER but also provide valuable insights for future\nadvancements in LLM-based scientific claim verification.",
      "tldr_zh": "本文提出CIBER（基于证据检索的声明验证框架），通过扩展检索增强生成（RAG）技术，专门用于检索支持或反驳科学论断的证据文档。该框架创新性地采用多轮探针一致性评估方法，在不依赖模型内部信息的情况下有效缓解大语言模型（LLMs）的不确定性，适用于白盒与黑盒场景。实验表明，CIBER在跨学科科学论断验证中显著优于传统RAG方法，其无监督特性为LLMs在科学验证领域的可靠应用提供了新范式。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07937v1",
      "published_date": "2025-03-11 00:29:50 UTC",
      "updated_date": "2025-03-11 00:29:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:36:33.883599"
    },
    {
      "arxiv_id": "2503.07932v1",
      "title": "A Theory of Learning with Autoregressive Chain of Thought",
      "title_zh": "自回归思维链学习理论",
      "authors": [
        "Nirmit Joshi",
        "Gal Vardi",
        "Adam Block",
        "Surbhi Goel",
        "Zhiyuan Li",
        "Theodor Misiakiewicz",
        "Nathan Srebro"
      ],
      "abstract": "For a given base class of sequence-to-next-token generators, we consider\nlearning prompt-to-answer mappings obtained by iterating a fixed,\ntime-invariant generator for multiple steps, thus generating a\nchain-of-thought, and then taking the final token as the answer. We formalize\nthe learning problems both when the chain-of-thought is observed and when\ntraining only on prompt-answer pairs, with the chain-of-thought latent. We\nanalyze the sample and computational complexity both in terms of general\nproperties of the base class (e.g. its VC dimension) and for specific base\nclasses such as linear thresholds. We present a simple base class that allows\nfor universal representability and computationally tractable chain-of-thought\nlearning. Central to our development is that time invariance allows for sample\ncomplexity that is independent of the length of the chain-of-thought. Attention\narises naturally in our construction.",
      "tldr_zh": "本文提出了一种基于自回归链式思维（Autoregressive Chain-of-Thought）的学习理论框架，研究如何通过固定、时不变的序列生成器迭代产生思维链，并最终输出答案。作者从理论和计算复杂度两个维度分析了思维链在显式和隐式两种训练模式下的学习问题，既考虑了基础类的一般性质（如VC维），也针对线性阈值等具体基础类进行了分析。研究发现时间不变性使得样本复杂度与思维链长度无关，并自然衍生出注意力机制，同时提出了一个兼具通用表示能力和计算可行性的基础类构造方案。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CC",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Comments are welcome",
      "pdf_url": "http://arxiv.org/pdf/2503.07932v1",
      "published_date": "2025-03-11 00:21:32 UTC",
      "updated_date": "2025-03-11 00:21:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:36:39.519688"
    },
    {
      "arxiv_id": "2503.08725v2",
      "title": "The Algorithmic State Architecture (ASA): An Integrated Framework for AI-Enabled Government",
      "title_zh": "算法国家架构（ASA）：面向AI赋能政府的集成式框架",
      "authors": [
        "Zeynep Engin",
        "Jon Crowcroft",
        "David Hand",
        "Philip Treleaven"
      ],
      "abstract": "As artificial intelligence transforms public sector operations, governments\nstruggle to integrate technological innovations into coherent systems for\neffective service delivery. This paper introduces the Algorithmic State\nArchitecture (ASA), a novel four-layer framework conceptualising how Digital\nPublic Infrastructure, Data-for-Policy, Algorithmic Government/Governance, and\nGovTech interact as an integrated system in AI-enabled states. Unlike\napproaches that treat these as parallel developments, ASA positions them as\ninterdependent layers with specific enabling relationships and feedback\nmechanisms. Through comparative analysis of implementations in Estonia,\nSingapore, India, and the UK, we demonstrate how foundational digital\ninfrastructure enables systematic data collection, which powers algorithmic\ndecision-making processes, ultimately manifesting in user-facing services. Our\nanalysis reveals that successful implementations require balanced development\nacross all layers, with particular attention to integration mechanisms between\nthem. The framework contributes to both theory and practice by bridging\npreviously disconnected domains of digital government research, identifying\ncritical dependencies that influence implementation success, and providing a\nstructured approach for analysing the maturity and development pathways of\nAI-enabled government systems.",
      "tldr_zh": "本文提出\"算法国家架构\"(ASA)，这是一个四层框架，系统整合了数字公共基础设施、政策数据、算法治理和政务科技，用于构建AI驱动的政府体系。该研究通过分析爱沙尼亚、新加坡、印度和英国的案例，揭示了各层级间的依赖关系——数字基础设施支撑数据收集，进而赋能算法决策，最终体现在公共服务中。研究发现，成功的AI政府系统需要各层级均衡发展，并重点关注层级间的整合机制。ASA框架不仅连接了数字政府研究中的分散领域，还为评估AI政府系统的成熟度提供了结构化方法。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "cs.MA",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.CY",
      "comment": "Main text: 25 pages, with references: 35 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.08725v2",
      "published_date": "2025-03-11 00:20:56 UTC",
      "updated_date": "2025-03-13 11:16:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:36:47.405064"
    },
    {
      "arxiv_id": "2503.07928v1",
      "title": "The StudyChat Dataset: Student Dialogues With ChatGPT in an Artificial Intelligence Course",
      "title_zh": "StudyChat数据集：人工智能课程中与ChatGPT的学生对话",
      "authors": [
        "Hunter McNichols",
        "Andrew Lan"
      ],
      "abstract": "The widespread availability of large language models (LLMs), such as ChatGPT,\nhas significantly impacted education, raising both opportunities and\nchallenges. Students can frequently interact with LLM-powered, interactive\nlearning tools, but their usage patterns need to be analyzed to ensure ethical\nusage of these tools. To better understand how students interact with LLMs in\nan academic setting, we introduce \\textbf{StudyChat}, a publicly available\ndataset capturing real-world student interactions with an LLM-powered tutoring\nchatbot in a semester-long, university-level artificial intelligence (AI)\ncourse. We deploy a web application that replicates ChatGPT's core\nfunctionalities, and use it to log student interactions with the LLM while\nworking on programming assignments. We collect 1,197 conversations, which we\nannotate using a dialogue act labeling schema inspired by observed interaction\npatterns and prior research. Additionally, we analyze these interactions,\nhighlight behavioral trends, and analyze how specific usage patterns relate to\ncourse outcomes. \\textbf{StudyChat} provides a rich resource for the learning\nsciences and AI in education communities, enabling further research into the\nevolving role of LLMs in education.",
      "tldr_zh": "该研究介绍了StudyChat数据集，记录了大学生在一学期的人工智能课程中与ChatGPT驱动的辅导聊天机器人的真实互动。研究者开发了一个模拟ChatGPT核心功能的网页应用，收集了1,197次对话，并通过对话行为标注框架进行分析。研究揭示了学生的使用模式，并探讨了这些模式与课程成绩的关系。该数据集为学习科学和教育AI社区提供了丰富资源，有助于进一步研究大语言模型在教育中的角色演变。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Pre-print",
      "pdf_url": "http://arxiv.org/pdf/2503.07928v1",
      "published_date": "2025-03-11 00:17:07 UTC",
      "updated_date": "2025-03-11 00:17:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T05:36:52.833643"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 146,
  "processed_papers_count": 146,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-26T05:39:21.686534"
}
{
  "date": "2024-03-28",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-28 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 88 篇论文，主要聚焦 AI 模型优化、多模态学习、机器学习在医疗和自动驾驶的应用等领域，其中 Dimitris Bertsimas 等知名学者的稳定 ML 模型重训论文、Shirui Pan 等人提出的 Temporal Knowledge Graph Completion 方法，以及 LLM 在传感器和农业中的创新应用最令人印象深刻，展示了 AI 在实际场景中的潜力。\n\n下面，我挑选了最具话题度和影响力的几篇论文先进行详细讨论，将相关主题的论文归类放在一起，其他较常规或低影响力的论文则快速掠过。每个条目包括论文标题（中文 + 英文）、主要贡献和发现，力求简洁明了。\n\n### 重点论文讨论\n\n1. **IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion**（中文：多曲率共享和特定嵌入整合用于时间知识图谱补全）  \n   作者：Jiapu Wang, Zheng Cui, Boyue Wang, Shirui Pan 等。  \n   这篇论文提出 IME 模型，通过在超球面、双曲和欧几里得空间中整合共享和特定属性，捕捉时间知识图谱的复杂几何结构。贡献在于提升了 Temporal Knowledge Graph Completion 的性能，实验显示 IME 在多数据集上超越现有方法，显著提高了知识图谱补全的准确性和鲁棒性。\n\n2. **Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences**（中文：通过缓慢变化序列实现稳定机器学习模型重训）  \n   作者：Dimitris Bertsimas, Vassilis Digalakis Jr, Yu Ma, Phevos Paschalidis 等。  \n   Bertsimas 等知名学者的工作引入混合整数优化框架，平衡预测性能和模型稳定性。关键发现是，通过自定义距离指标，模型在牺牲 2% 预测准确率的情况下，稳定性提升 30%，并在医疗、视觉和语言领域验证了泛化性。\n\n3. **LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces**（中文：利用大语言模型进行时空传感器轨迹的高级推理）  \n   作者：Xiaomin Ouyang, Mani Srivastava。  \n   这篇论文探索 Large Language Models (LLMs) 在传感器数据上的应用，设计了提示框架处理长序列轨迹，实现 80% 以上准确率。贡献包括引入总结和选择性历史轨迹策略，提升了高阶推理任务如痴呆诊断和占用跟踪的性能。\n\n4. **The New Agronomists: Language Models are Experts in Crop Management**（中文：新型农学家：语言模型在作物管理的专家应用）  \n   作者：Jing Wu, Zhixin Lai, Suiyao Chen, Naira Hovakimyan 等。  \n   论文将 LLM 与强化学习结合，应用于作物模拟，显著提升经济收益（超过 49%）。主要发现是，LLM 通过处理状态变量和模拟工具，优化作物管理策略，在真实场景中减少环境影响。\n\n5. **Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving**（中文：多帧轻量高效视觉-语言模型用于自动驾驶问答）  \n   作者：Akshay Gopalkrishnan, Ross Greer, Mohan Trivedi。  \n   这篇 CVPR 2024 接受的论文提出 EM-VLM4AD 模型，减少内存和计算开销（10 倍以上），在 DriveLM 数据集上提升 CIDEr 和 ROUGE-L 分数。贡献在于支持实时自动驾驶问答，提高了模型在交通场景中的鲁棒性。\n\n### 相关主题论文简要聊聊\n- **LLM 和多模态相关**：如 \"Concept-based Analysis of Neural Networks via Vision-Language Models\"（中文：基于视觉-语言模型的概念神经网络分析），作者包括 Yonatan Belinkov 等，利用 Vision-Language Models (VLMs) 解释神经网络行为，贡献是构建逻辑规格语言提升视觉任务的可解释性。另外，\"Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering\"（中文：检索增强知识编辑用于多跳问答）提出检索框架优化 LLM 知识编辑，显著改善多跳问答性能。这些论文共同探讨 LLM 在知识编辑和多模态交互中的潜力。\n  \n- **机器人和自动驾驶领域**：如 \"InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction\"（中文：零样本文本到 3D 动态人-物交互），实现文本驱动的 3D 交互生成；以及 \"RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation\"（中文：无需点云分割的近实时 SE(3) 等变机器人操作），贡献是无需分割就实现高效机器人操控。这些工作推进了机器人设计的创新，但细节较技术化，就不展开了。\n\n其他论文涉及量子计算、图神经网络和医疗应用等，但许多是小规模改进或理论探讨（如第 12 篇的 premature submission），因此快速掠过。例如，\"Quantum Natural Language Processing\"（中文：量子自然语言处理）探讨量子模型在语言任务中的潜力，但尚未有突破性发现；\"Developing Healthcare Language Model Embedding Spaces\"（中文：开发医疗语言模型嵌入空间）优化医疗 LLM，但影响有限。这些论文总体上丰富了 AI 生态，但不影响核心主题的讨论。\n\n今天的 arXiv 快报到此结束，AI 领域的创新持续加速，欢迎关注后续更新！",
  "papers": [
    {
      "arxiv_id": "2403.19883v1",
      "title": "Policy-Space Search: Equivalences, Improvements, and Compression",
      "title_zh": "策略空间搜索：等价性、改进和压缩",
      "authors": [
        "Frederico Messa",
        "André Grahl Pereira"
      ],
      "abstract": "Fully-observable non-deterministic (FOND) planning is at the core of\nartificial intelligence planning with uncertainty. It models uncertainty\nthrough actions with non-deterministic effects. A* with Non-Determinism (AND*)\n(Messa and Pereira, 2023) is a FOND planner that generalizes A* (Hart et al.,\n1968) for FOND planning. It searches for a solution policy by performing an\nexplicit heuristic search on the policy space of the FOND task. In this paper,\nwe study and improve the performance of the policy-space search performed by\nAND*. We present a polynomial-time procedure that constructs a solution policy\ngiven just the set of states that should be mapped. This procedure, together\nwith a better understanding of the structure of FOND policies, allows us to\npresent three concepts of equivalences between policies. We use policy\nequivalences to prune part of the policy search space, making AND*\nsubstantially more effective in solving FOND tasks. We also study the impact of\ntaking into account structural state-space symmetries to strengthen the\ndetection of equivalence policies and the impact of performing the search with\nsatisficing techniques. We apply a recent technique from the group theory\nliterature to better compute structural state-space symmetries. Finally, we\npresent a solution compressor that, given a policy defined over complete\nstates, finds a policy that unambiguously represents it using the minimum\nnumber of partial states. AND* with the introduced techniques generates, on\naverage, two orders of magnitude fewer policies to solve FOND tasks. These\ntechniques allow explicit policy-space search to be competitive in terms of\nboth coverage and solution compactness with other state-of-the-art FOND\nplanners.",
      "tldr_zh": "这篇论文研究了FOND（Fully-observable non-deterministic）规划中的AND*算法，该算法在策略空间上扩展了A*（Hart et al., 1968）以处理AI规划不确定性。论文提出一个多项式时间程序来从状态映射集构建解决方案策略，并定义了三种策略等价概念，用于修剪搜索空间，同时整合结构状态空间对称性和满足技术（satisficing techniques）来提升算法效率。最终，AND*算法通过这些改进生成策略数量减少两个数量级，在覆盖性和解决方案紧凑性上与最先进FOND规划器相当。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19883v1",
      "published_date": "2024-03-28 23:40:20 UTC",
      "updated_date": "2024-03-28 23:40:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:41:23.408977"
    },
    {
      "arxiv_id": "2403.19881v1",
      "title": "IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion",
      "title_zh": "IME：整合多曲率共享和特定嵌入用于时序知识图谱补全",
      "authors": [
        "Jiapu Wang",
        "Zheng Cui",
        "Boyue Wang",
        "Shirui Pan",
        "Junbin Gao",
        "Baocai Yin",
        "Wen Gao"
      ],
      "abstract": "Temporal Knowledge Graphs (TKGs) incorporate a temporal dimension, allowing\nfor a precise capture of the evolution of knowledge and reflecting the dynamic\nnature of the real world. Typically, TKGs contain complex geometric structures,\nwith various geometric structures interwoven. However, existing Temporal\nKnowledge Graph Completion (TKGC) methods either model TKGs in a single space\nor neglect the heterogeneity of different curvature spaces, thus constraining\ntheir capacity to capture these intricate geometric structures. In this paper,\nwe propose a novel Integrating Multi-curvature shared and specific Embedding\n(IME) model for TKGC tasks. Concretely, IME models TKGs into multi-curvature\nspaces, including hyperspherical, hyperbolic, and Euclidean spaces.\nSubsequently, IME incorporates two key properties, namely space-shared property\nand space-specific property. The space-shared property facilitates the learning\nof commonalities across different curvature spaces and alleviates the spatial\ngap caused by the heterogeneous nature of multi-curvature spaces, while the\nspace-specific property captures characteristic features. Meanwhile, IME\nproposes an Adjustable Multi-curvature Pooling (AMP) approach to effectively\nretain important information. Furthermore, IME innovatively designs similarity,\ndifference, and structure loss functions to attain the stated objective.\nExperimental results clearly demonstrate the superior performance of IME over\nexisting state-of-the-art TKGC models.",
      "tldr_zh": "这篇论文提出了一种名为 IME 的模型，用于 Temporal Knowledge Graphs (TKGs) 补全任务，以更好地捕捉 TKGs 中复杂的几何结构。IME 将 TKGs 建模到多曲率空间，包括 hyperspherical、hyperbolic 和 Euclidean spaces，并引入 space-shared property（学习不同空间的共性以缓解空间异质性）和 space-specific property（捕捉特定特征）。模型还设计了 Adjustable Multi-curvature Pooling (AMP) 方法以及 similarity、difference 和 structure 损失函数，以保留重要信息并优化性能。实验结果表明，IME 在 TKGC 任务上优于现有最先进模型。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19881v1",
      "published_date": "2024-03-28 23:31:25 UTC",
      "updated_date": "2024-03-28 23:31:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:41:35.202492"
    },
    {
      "arxiv_id": "2403.19871v5",
      "title": "Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences",
      "title_zh": "翻译失败",
      "authors": [
        "Dimitris Bertsimas",
        "Vassilis Digalakis Jr",
        "Yu Ma",
        "Phevos Paschalidis"
      ],
      "abstract": "We consider the problem of retraining machine learning (ML) models when new\nbatches of data become available. Existing approaches greedily optimize for\npredictive power independently at each batch, without considering the stability\nof the model's structure or analytical insights across retraining iterations.\nWe propose a model-agnostic framework for finding sequences of models that are\nstable across retraining iterations. We develop a mixed-integer optimization\nformulation that is guaranteed to recover Pareto optimal models (in terms of\nthe predictive power-stability trade-off) with good generalization properties,\nas well as an efficient polynomial-time algorithm that performs well in\npractice. We focus on retaining consistent analytical insights-which is\nimportant to model interpretability, ease of implementation, and fostering\ntrust with users-by using custom-defined distance metrics that can be directly\nincorporated into the optimization problem. We evaluate our framework across\nmodels (regression, decision trees, boosted trees, and neural networks) and\napplication domains (healthcare, vision, and language), including deployment in\na production pipeline at a major US hospital. We find that, on average, a 2%\nreduction in predictive power leads to a 30% improvement in stability.",
      "tldr_zh": "该研究针对机器学习模型重训问题，提出一个模型无关框架，通过Slowly Varying Sequences方法来确保模型在迭代过程中保持稳定性，而非仅优化每个数据批次的预测性能。该框架利用混合整数优化（mixed-integer optimization）公式来寻找帕累托最优（Pareto optimal）模型序列，实现预测性能与稳定性的权衡，并通过自定义距离指标保留一致的分析洞见，以提升模型可解释性和用户信任。在多领域评估（如回归、决策树、神经网络以及医疗、视觉和语言应用）中，结果显示平均预测性能降低2%可带来30%的稳定性改善，为实际部署（如美国医院生产管道）提供了可靠支持。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19871v5",
      "published_date": "2024-03-28 22:45:38 UTC",
      "updated_date": "2025-02-04 12:25:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:41:47.316053"
    },
    {
      "arxiv_id": "2403.19867v4",
      "title": "Constructing Decision Trees from Data Streams",
      "title_zh": "从数据流中构建决策树",
      "authors": [
        "Huy Pham",
        "Hoang Ta",
        "Hoa T. Vu"
      ],
      "abstract": "In this work, we present data stream algorithms to compute optimal splits for\ndecision tree learning. In particular, given a data stream of observations\n\\(x_i\\) and their corresponding labels \\(y_i\\), without the i.i.d. assumption,\nthe objective is to identify the optimal split \\(j\\) that partitions the data\ninto two sets, minimizing the mean squared error (for regression) or the\nmisclassification rate and Gini impurity (for classification). We propose\nseveral efficient streaming algorithms that require sublinear space and use a\nsmall number of passes to solve these problems. These algorithms can also be\nextended to the MapReduce model. Our results, while not directly comparable,\ncomplements the seminal work of Domingos-Hulten (KDD 2000) and\nHulten-Spencer-Domingos (KDD 2001).",
      "tldr_zh": "这篇论文提出了高效的数据流算法，用于从数据流中构建决策树，目标是识别最优分割 \\(j\\)，以最小化均方误差（for regression）或误分类率和 Gini impurity（for classification），而不依赖 i.i.d. 假设。算法采用 sublinear space 和少量 passes 的设计，确保处理效率，并可扩展到 MapReduce 模型。这些方法与 Domingos-Hulten (KDD 2000) 和 Hulten-Spencer-Domingos (KDD 2001) 的工作互补，提供了更灵活的决策树学习框架。",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DS",
      "comment": "To appear at ISIT 2025",
      "pdf_url": "http://arxiv.org/pdf/2403.19867v4",
      "published_date": "2024-03-28 22:26:38 UTC",
      "updated_date": "2025-04-16 21:09:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:41:59.458918"
    },
    {
      "arxiv_id": "2403.19866v2",
      "title": "Is Synthetic Image Useful for Transfer Learning? An Investigation into Data Generation, Volume, and Utilization",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhang Li",
        "Xin Dong",
        "Chen Chen",
        "Jingtao Li",
        "Yuxin Wen",
        "Michael Spranger",
        "Lingjuan Lyu"
      ],
      "abstract": "Synthetic image data generation represents a promising avenue for training\ndeep learning models, particularly in the realm of transfer learning, where\nobtaining real images within a specific domain can be prohibitively expensive\ndue to privacy and intellectual property considerations. This work delves into\nthe generation and utilization of synthetic images derived from text-to-image\ngenerative models in facilitating transfer learning paradigms. Despite the high\nvisual fidelity of the generated images, we observe that their naive\nincorporation into existing real-image datasets does not consistently enhance\nmodel performance due to the inherent distribution gap between synthetic and\nreal images. To address this issue, we introduce a novel two-stage framework\ncalled bridged transfer, which initially employs synthetic images for\nfine-tuning a pre-trained model to improve its transferability and subsequently\nuses real data for rapid adaptation. Alongside, We propose dataset style\ninversion strategy to improve the stylistic alignment between synthetic and\nreal images. Our proposed methods are evaluated across 10 different datasets\nand 5 distinct models, demonstrating consistent improvements, with up to 30%\naccuracy increase on classification tasks. Intriguingly, we note that the\nenhancements were not yet saturated, indicating that the benefits may further\nincrease with an expanded volume of synthetic data.",
      "tldr_zh": "本研究探讨了合成图像在迁移学习(transfer learning)中的效用，特别是在数据获取成本高企的领域。作者发现，直接将文本到图像生成模型生成的合成图像添加到真实数据集中，由于分布差距，往往无法稳定提升模型性能。为解决此问题，他们提出了一种两阶段bridged transfer框架，先用合成图像微调预训练模型以增强其可转移性，然后用真实数据进行快速适应，并引入dataset style inversion策略来优化合成图像与真实图像的风格对齐。在10个数据集和5个模型上的实验显示，该方法实现了持续改进，最多可提升30%的分类任务准确率，且随着合成数据量的增加，益处可能进一步放大。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR24 Score 6865 https://openreview.net/forum?id=CjPt1AC6w0",
      "pdf_url": "http://arxiv.org/pdf/2403.19866v2",
      "published_date": "2024-03-28 22:25:05 UTC",
      "updated_date": "2024-04-02 22:41:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:42:10.267351"
    },
    {
      "arxiv_id": "2403.19857v1",
      "title": "LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces",
      "title_zh": "LLMSense：利用大",
      "authors": [
        "Xiaomin Ouyang",
        "Mani Srivastava"
      ],
      "abstract": "Most studies on machine learning in sensing systems focus on low-level\nperception tasks that process raw sensory data within a short time window.\nHowever, many practical applications, such as human routine modeling and\noccupancy tracking, require high-level reasoning abilities to comprehend\nconcepts and make inferences based on long-term sensor traces. Existing machine\nlearning-based approaches for handling such complex tasks struggle to\ngeneralize due to the limited training samples and the high dimensionality of\nsensor traces, necessitating the integration of human knowledge for designing\nfirst-principle models or logic reasoning methods. We pose a fundamental\nquestion: Can we harness the reasoning capabilities and world knowledge of\nLarge Language Models (LLMs) to recognize complex events from long-term\nspatiotemporal sensor traces? To answer this question, we design an effective\nprompting framework for LLMs on high-level reasoning tasks, which can handle\ntraces from the raw sensor data as well as the low-level perception results. We\nalso design two strategies to enhance performance with long sensor traces,\nincluding summarization before reasoning and selective inclusion of historical\ntraces. Our framework can be implemented in an edge-cloud setup, running small\nLLMs on the edge for data summarization and performing high-level reasoning on\nthe cloud for privacy preservation. The results show that LLMSense can achieve\nover 80\\% accuracy on two high-level reasoning tasks such as dementia diagnosis\nwith behavior traces and occupancy tracking with environmental sensor traces.\nThis paper provides a few insights and guidelines for leveraging LLM for\nhigh-level reasoning on sensor traces and highlights several directions for\nfuture work.",
      "tldr_zh": "本论文提出 LLMSense 框架，利用 Large Language Models (LLMs) 对 spatiotemporal sensor traces 进行高级推理，旨在解决现有方法在处理长期传感器数据时存在的泛化性和数据维度问题。框架设计了一个有效的 prompting 机制，能够处理原始传感器数据或低级感知结果，并引入数据总结和选择性历史数据包含策略，以提升性能；同时，通过边-云设置在边缘运行小 LLMs 进行数据总结，并在云端执行推理，以保障隐私。实验结果显示，LLMSense 在痴呆诊断和占用跟踪等高水平任务上实现了超过 80% 的准确率。该研究为利用 LLMs 进行高级推理提供了关键见解，并指出了未来工作的方向。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.19857v1",
      "published_date": "2024-03-28 22:06:04 UTC",
      "updated_date": "2024-03-28 22:06:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:42:25.234750"
    },
    {
      "arxiv_id": "2403.19856v1",
      "title": "Towards a Brazilian History Knowledge Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Valeria de Paiva",
        "Alexandre Rademaker"
      ],
      "abstract": "This short paper describes the first steps in a project to construct a\nknowledge graph for Brazilian history based on the Brazilian Dictionary of\nHistorical Biographies (DHBB) and Wikipedia/Wikidata. We contend that large\nrepositories of Brazilian-named entities (people, places, organizations, and\npolitical events and movements) would be beneficial for extracting information\nfrom Portuguese texts. We show that many of the terms/entities described in the\nDHBB do not have corresponding concepts (or Q items) in Wikidata, the largest\nstructured database of entities associated with Wikipedia. We describe previous\nwork on extracting information from the DHBB and outline the steps to construct\na Wikidata-based historical knowledge graph.",
      "tldr_zh": "这篇论文介绍了构建巴西历史知识图谱(Knowledge Graph)的初步项目，基于巴西历史人物传记词典(DHBB)和Wikipedia/Wikidata的数据源。研究者认为，建立大型巴西实体仓库（包括人、地、组织和政治事件）有助于从葡萄牙语文本中提取信息。发现显示，DHBB中的许多术语或实体在Wikidata中缺乏对应的概念或Q项。论文回顾了之前从DHBB提取信息的工作，并概述了构建基于Wikidata的历史知识图谱的步骤。",
      "categories": [
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19856v1",
      "published_date": "2024-03-28 22:05:32 UTC",
      "updated_date": "2024-03-28 22:05:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:42:34.463694"
    },
    {
      "arxiv_id": "2403.19839v1",
      "title": "The New Agronomists: Language Models are Experts in Crop Management",
      "title_zh": "新农学家：语言模型是作物管理的专家",
      "authors": [
        "Jing Wu",
        "Zhixin Lai",
        "Suiyao Chen",
        "Ran Tao",
        "Pan Zhao",
        "Naira Hovakimyan"
      ],
      "abstract": "Crop management plays a crucial role in determining crop yield, economic\nprofitability, and environmental sustainability. Despite the availability of\nmanagement guidelines, optimizing these practices remains a complex and\nmultifaceted challenge. In response, previous studies have explored using\nreinforcement learning with crop simulators, typically employing simple\nneural-network-based reinforcement learning (RL) agents. Building on this\nfoundation, this paper introduces a more advanced intelligent crop management\nsystem. This system uniquely combines RL, a language model (LM), and crop\nsimulations facilitated by the Decision Support System for Agrotechnology\nTransfer (DSSAT). We utilize deep RL, specifically a deep Q-network, to train\nmanagement policies that process numerous state variables from the simulator as\nobservations. A novel aspect of our approach is the conversion of these state\nvariables into more informative language, facilitating the language model's\ncapacity to understand states and explore optimal management practices. The\nempirical results reveal that the LM exhibits superior learning capabilities.\nThrough simulation experiments with maize crops in Florida (US) and Zaragoza\n(Spain), the LM not only achieves state-of-the-art performance under various\nevaluation metrics but also demonstrates a remarkable improvement of over 49\\%\nin economic profit, coupled with reduced environmental impact when compared to\nbaseline methods. Our code is available at\n\\url{https://github.com/jingwu6/LM_AG}.",
      "tldr_zh": "这篇论文提出了一种先进的作物管理系统，将强化学习（RL）、语言模型（LM）和DSSAT作物模拟器相结合，以优化作物管理。该系统使用深Q网络处理模拟器的状态变量，并创新性地将这些变量转化为更具信息性的语言形式，便于LM理解和探索最佳实践。在玉米作物模拟实验中，LM展示了卓越的学习能力，在佛罗里达（美国）和萨拉戈萨（西班牙）的场景下，经济利润提升超过49%，并显著降低了环境影响，优于现有基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19839v1",
      "published_date": "2024-03-28 21:20:27 UTC",
      "updated_date": "2024-03-28 21:20:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:42:46.648885"
    },
    {
      "arxiv_id": "2403.19838v2",
      "title": "Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Akshay Gopalkrishnan",
        "Ross Greer",
        "Mohan Trivedi"
      ],
      "abstract": "Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have\nbecome prominent in autonomous driving research, as these models can provide\ninterpretable textual reasoning and responses for end-to-end autonomous driving\nsafety tasks using traffic scene images and other data modalities. However,\ncurrent approaches to these systems use expensive large language model (LLM)\nbackbones and image encoders, making such systems unsuitable for real-time\nautonomous driving systems where tight memory constraints exist and fast\ninference time is necessary. To address these previous issues, we develop\nEM-VLM4AD, an efficient, lightweight, multi-frame vision language model which\nperforms Visual Question Answering for autonomous driving. In comparison to\nprevious approaches, EM-VLM4AD requires at least 10 times less memory and\nfloating point operations, while also achieving higher CIDEr and ROUGE-L scores\nthan the existing baseline on the DriveLM dataset. EM-VLM4AD also exhibits the\nability to extract relevant information from traffic views related to prompts\nand can answer questions for various autonomous driving subtasks. We release\nour code to train and evaluate our model at\nhttps://github.com/akshaygopalkr/EM-VLM4AD.",
      "tldr_zh": "本论文针对自动驾驶领域的视觉语言模型(VLMs)和多模态语言模型(MMLMs)存在的高内存消耗和低推理效率问题，提出了一种轻量级、多帧的EM-VLM4AD模型，用于视觉问答任务。EM-VLM4AD采用高效的多帧输入设计，显著减少了至少10倍的内存和浮点运算需求，同时能够从交通场景中提取相关信息并回答各种自动驾驶子任务。实验结果显示，该模型在DriveLM数据集上比现有基线模型取得了更高的CIDEr和ROUGE-L分数，为实时自动驾驶系统的可解释性推理提供了可靠解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 3 figures, Accepted at CVPR 2024 Vision and Language for\n  Autonomous Driving and Robotics Workshop",
      "pdf_url": "http://arxiv.org/pdf/2403.19838v2",
      "published_date": "2024-03-28 21:18:33 UTC",
      "updated_date": "2024-05-09 03:27:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:42:59.370279"
    },
    {
      "arxiv_id": "2403.19837v3",
      "title": "Concept-based Analysis of Neural Networks via Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ravi Mangal",
        "Nina Narodytska",
        "Divya Gopinath",
        "Boyue Caroline Hu",
        "Anirban Roy",
        "Susmit Jha",
        "Corina Pasareanu"
      ],
      "abstract": "The analysis of vision-based deep neural networks (DNNs) is highly desirable\nbut it is very challenging due to the difficulty of expressing formal\nspecifications for vision tasks and the lack of efficient verification\nprocedures. In this paper, we propose to leverage emerging multimodal,\nvision-language, foundation models (VLMs) as a lens through which we can reason\nabout vision models. VLMs have been trained on a large body of images\naccompanied by their textual description, and are thus implicitly aware of\nhigh-level, human-understandable concepts describing the images. We describe a\nlogical specification language $\\texttt{Con}_{\\texttt{spec}}$ designed to\nfacilitate writing specifications in terms of these concepts. To define and\nformally check $\\texttt{Con}_{\\texttt{spec}}$ specifications, we build a map\nbetween the internal representations of a given vision model and a VLM, leading\nto an efficient verification procedure of natural-language properties for\nvision models. We demonstrate our techniques on a ResNet-based classifier\ntrained on the RIVAL-10 dataset using CLIP as the multimodal model.",
      "tldr_zh": "本文提出一种基于视觉语言模型(VLMs)的方法，用于分析视觉深度神经网络(DNNs)，以解决视觉任务正式规范表达和验证效率的挑战。研究者设计了逻辑规范语言$\\texttt{Con}_{\\texttt{spec}}$，允许用户用高层次人类概念编写规范，并通过映射DNNs的内部表示到VLMs，实现高效的自然语言属性验证。实验在ResNet-based分类器上，使用CLIP作为多模态模型和RIVAL-10数据集，证明了该方法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19837v3",
      "published_date": "2024-03-28 21:15:38 UTC",
      "updated_date": "2024-04-10 23:47:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:43:10.975648"
    },
    {
      "arxiv_id": "2403.19833v2",
      "title": "ChatTracer: Large Language Model Powered Real-time Bluetooth Device Tracking System",
      "title_zh": "翻译失败",
      "authors": [
        "Qijun Wang",
        "Shichen Zhang",
        "Kunzhe Song",
        "Huacheng Zeng"
      ],
      "abstract": "Large language models (LLMs) have transformed the way we interact with cyber\ntechnologies. In this paper, we study the possibility of connecting LLM with\nwireless sensor networks (WSN). A successful design will not only extend LLM's\nknowledge landscape to the physical world but also revolutionize human\ninteraction with WSN. To the end, we present ChatTracer, an LLM-powered\nreal-time Bluetooth device tracking system. ChatTracer comprises three key\ncomponents: an array of Bluetooth sniffing nodes, a database, and a fine-tuned\nLLM. ChatTracer was designed based on our experimental observation that\ncommercial Apple/Android devices always broadcast hundreds of BLE packets per\nminute even in their idle status. Its novelties lie in two aspects: i) a\nreliable and efficient BLE packet grouping algorithm; and ii) an LLM\nfine-tuning strategy that combines both supervised fine-tuning (SFT) and\nreinforcement learning with human feedback (RLHF). We have built a prototype of\nChatTracer with four sniffing nodes. Experimental results show that ChatTracer\nnot only outperforms existing localization approaches, but also provides an\nintelligent interface for user interaction.",
      "tldr_zh": "该论文探讨了将大型语言模型(LLMs)与无线传感器网络(WSN)整合的可能性，提出ChatTracer系统，用于实时跟踪Bluetooth设备，从而扩展LLMs到物理世界并革新人类与WSN的交互。\nChatTracer的核心组件包括Bluetooth嗅探节点数组、数据库和经微调的LLM，基于设备闲置时广播数百个BLE packets的观察，创新性地开发了可靠高效的BLE包分组算法以及结合监督微调(SFT)和强化学习与人类反馈(RLHF)的微调策略。\n实验结果表明，ChatTracer原型在四节点设置下优于现有定位方法，并提供智能的用户交互界面。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19833v2",
      "published_date": "2024-03-28 21:04:11 UTC",
      "updated_date": "2024-07-09 12:56:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:43:25.025095"
    },
    {
      "arxiv_id": "2403.19826v2",
      "title": "Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Qitian Ma",
        "Shyam Nanda Rai",
        "Carlo Masone",
        "Tatiana Tommasi"
      ],
      "abstract": "In the domain of computer vision, semantic segmentation emerges as a\nfundamental application within machine learning, wherein individual pixels of\nan image are classified into distinct semantic categories. This task transcends\ntraditional accuracy metrics by incorporating uncertainty quantification, a\ncritical measure for assessing the reliability of each segmentation prediction.\nSuch quantification is instrumental in facilitating informed decision-making,\nparticularly in applications where precision is paramount. Within this nuanced\nframework, the metric known as PAvPU (Patch Accuracy versus Patch Uncertainty)\nhas been developed as a specialized tool for evaluating entropy-based\nuncertainty in image segmentation tasks. However, our investigation identifies\nthree core deficiencies within the PAvPU framework and proposes robust\nsolutions aimed at refining the metric. By addressing these issues, we aim to\nenhance the reliability and applicability of uncertainty quantification,\nespecially in scenarios that demand high levels of safety and accuracy, thus\ncontributing to the advancement of semantic segmentation methodologies in\ncritical applications.",
      "tldr_zh": "语义分割是计算机视觉中的核心任务，通过对图像像素分类并量化不确定性，以提升预测可靠性，尤其在精度要求高的应用中。论文审视了基于熵的不确定性估计指标 PAvPU，识别了其三个核心缺陷，包括评估框架的局限性。研究提出 robust solutions 来改进 PAvPU，从而提升不确定性量化的可靠性和适用性，促进语义分割在安全关键场景中的发展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Premature Submission: accidentally submitted before it was ready",
      "pdf_url": "http://arxiv.org/pdf/2403.19826v2",
      "published_date": "2024-03-28 20:34:02 UTC",
      "updated_date": "2024-04-08 14:55:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:43:35.222858"
    },
    {
      "arxiv_id": "2403.19822v1",
      "title": "Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Yash Jain",
        "David Chan",
        "Pranav Dheram",
        "Aparna Khare",
        "Olabanji Shonibare",
        "Venkatesh Ravichandran",
        "Shalini Ghosh"
      ],
      "abstract": "Recent advances in machine learning have demonstrated that multi-modal\npre-training can improve automatic speech recognition (ASR) performance\ncompared to randomly initialized models, even when models are fine-tuned on\nuni-modal tasks. Existing multi-modal pre-training methods for the ASR task\nhave primarily focused on single-stage pre-training where a single unsupervised\ntask is used for pre-training followed by fine-tuning on the downstream task.\nIn this work, we introduce a novel method combining multi-modal and multi-task\nunsupervised pre-training with a translation-based supervised mid-training\napproach. We empirically demonstrate that such a multi-stage approach leads to\nrelative word error rate (WER) improvements of up to 38.45% over baselines on\nboth Librispeech and SUPERB. Additionally, we share several important findings\nfor choosing pre-training methods and datasets.",
      "tldr_zh": "该论文提出了一种多阶段多模态预训练方法，用于提升自动语音识别（ASR）的性能，通过结合多模态和多任务无监督预训练，以及基于翻译的监督中间训练，超越了传统的单阶段方法。实验结果显示，在Librispeech和SUPERB数据集上，该方法相对于基线模型的相对单词错误率（WER）改善高达38.45%。此外，论文分享了选择预训练方法和数据集的关键见解，以指导未来ASR模型的优化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in LREC-COLING 2024 - The 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation",
      "pdf_url": "http://arxiv.org/pdf/2403.19822v1",
      "published_date": "2024-03-28 20:23:39 UTC",
      "updated_date": "2024-03-28 20:23:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:43:48.372748"
    },
    {
      "arxiv_id": "2403.19820v1",
      "title": "Evaluating Explanatory Capabilities of Machine Learning Models in Medical Diagnostics: A Human-in-the-Loop Approach",
      "title_zh": "评估机器学习模型在医疗诊断中的解释能力：一种人类在环方法",
      "authors": [
        "José Bobes-Bascarán",
        "Eduardo Mosqueira-Rey",
        "Ángel Fernández-Leal",
        "Elena Hernández-Pereira",
        "David Alonso-Ríos",
        "Vicente Moret-Bonillo",
        "Israel Figueirido-Arnoso",
        "Yolanda Vidal-Ínsua"
      ],
      "abstract": "This paper presents a comprehensive study on the evaluation of explanatory\ncapabilities of machine learning models, with a focus on Decision Trees, Random\nForest and XGBoost models using a pancreatic cancer dataset. We use\nHuman-in-the-Loop related techniques and medical guidelines as a source of\ndomain knowledge to establish the importance of the different features that are\nrelevant to establish a pancreatic cancer treatment. These features are not\nonly used as a dimensionality reduction approach for the machine learning\nmodels, but also as way to evaluate the explainability capabilities of the\ndifferent models using agnostic and non-agnostic explainability techniques. To\nfacilitate interpretation of explanatory results, we propose the use of\nsimilarity measures such as the Weighted Jaccard Similarity coefficient. The\ngoal is to not only select the best performing model but also the one that can\nbest explain its conclusions and aligns with human domain knowledge.",
      "tldr_zh": "这篇论文评估了机器学习模型在医疗诊断中的解释能力，焦点放在 Decision Trees、Random Forest 和 XGBoost 模型上，使用胰腺癌数据集作为基础。研究采用 Human-in-the-Loop 技术结合医疗指南来识别关键特征，这些特征不仅用于模型的维度减少，还通过 agnostic 和 non-agnostic 解释性技术评估模型的解释性能。论文引入 Weighted Jaccard Similarity 系数来量化解释结果的相似性，最终目标是选择既高性能又与人类领域知识一致的最佳模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19820v1",
      "published_date": "2024-03-28 20:11:34 UTC",
      "updated_date": "2024-03-28 20:11:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:44:00.531287"
    },
    {
      "arxiv_id": "2403.19802v1",
      "title": "Developing Healthcare Language Model Embedding Spaces",
      "title_zh": "医疗保健语言模型嵌入空间的开发",
      "authors": [
        "Niall Taylor",
        "Dan Schofield",
        "Andrey Kormilitzin",
        "Dan W Joyce",
        "Alejo Nevado-Holgado"
      ],
      "abstract": "Pre-trained Large Language Models (LLMs) often struggle on out-of-domain\ndatasets like healthcare focused text. We explore specialized pre-training to\nadapt smaller LLMs to different healthcare datasets. Three methods are\nassessed: traditional masked language modeling, Deep Contrastive Learning for\nUnsupervised Textual Representations (DeCLUTR), and a novel pre-training\nobjective utilizing metadata categories from the healthcare settings. These\nschemes are evaluated on downstream document classification tasks for each\ndataset, with additional analysis of the resultant embedding spaces.\nContrastively trained models outperform other approaches on the classification\ntasks, delivering strong performance from limited labeled data and with fewer\nmodel parameter updates required. While metadata-based pre-training does not\nfurther improve classifications across the datasets, it yields interesting\nembedding cluster separability. All domain adapted LLMs outperform their\npublicly available general base LLM, validating the importance of\ndomain-specialization. This research illustrates efficient approaches to\ninstill healthcare competency in compact LLMs even under tight computational\nbudgets, an essential capability for responsible and sustainable deployment in\nlocal healthcare settings. We provide pre-training guidelines for specialized\nhealthcare LLMs, motivate continued inquiry into contrastive objectives, and\ndemonstrates adaptation techniques to align small LLMs with privacy-sensitive\nmedical tasks.",
      "tldr_zh": "本研究探讨了通过专门预训练来适应较小LLM（Large Language Models）处理医疗领域的文本数据集，评估了三种方法：传统的masked language modeling、DeCLUTR（Deep Contrastive Learning for Unsupervised Textual Representations），以及一种新颖的基于元数据类别的预训练目标。结果显示，对比训练模型在下游文档分类任务中表现出色，即使在有限的标记数据和较少的模型参数更新下也能实现强性能，而元数据-based预训练虽未提升分类准确率，但改善了嵌入空间的聚类可分离性。所有适应医疗领域的LLM均优于通用基线模型，证明了领域专业化的重要性，并为在计算资源有限的医疗环境中部署紧凑LLM提供了高效预训练指南和适应技术。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19802v1",
      "published_date": "2024-03-28 19:31:32 UTC",
      "updated_date": "2024-03-28 19:31:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:44:13.286530"
    },
    {
      "arxiv_id": "2403.19800v2",
      "title": "Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction",
      "title_zh": "Gegenbauer 图神经网络用于时变信号重构",
      "authors": [
        "Jhon A. Castro-Correa",
        "Jhony H. Giraldo",
        "Mohsen Badiey",
        "Fragkiskos D. Malliaros"
      ],
      "abstract": "Reconstructing time-varying graph signals (or graph time-series imputation)\nis a critical problem in machine learning and signal processing with broad\napplications, ranging from missing data imputation in sensor networks to\ntime-series forecasting. Accurately capturing the spatio-temporal information\ninherent in these signals is crucial for effectively addressing these tasks.\nHowever, existing approaches relying on smoothness assumptions of temporal\ndifferences and simple convex optimization techniques have inherent\nlimitations. To address these challenges, we propose a novel approach that\nincorporates a learning module to enhance the accuracy of the downstream task.\nTo this end, we introduce the Gegenbauer-based graph convolutional (GegenConv)\noperator, which is a generalization of the conventional Chebyshev graph\nconvolution by leveraging the theory of Gegenbauer polynomials. By deviating\nfrom traditional convex problems, we expand the complexity of the model and\noffer a more accurate solution for recovering time-varying graph signals.\nBuilding upon GegenConv, we design the Gegenbauer-based time Graph Neural\nNetwork (GegenGNN) architecture, which adopts an encoder-decoder structure.\nLikewise, our approach also utilizes a dedicated loss function that\nincorporates a mean squared error component alongside Sobolev smoothness\nregularization. This combination enables GegenGNN to capture both the fidelity\nto ground truth and the underlying smoothness properties of the signals,\nenhancing the reconstruction performance. We conduct extensive experiments on\nreal datasets to evaluate the effectiveness of our proposed approach. The\nexperimental results demonstrate that GegenGNN outperforms state-of-the-art\nmethods, showcasing its superior capability in recovering time-varying graph\nsignals.",
      "tldr_zh": "本论文针对时间变化图信号的重建问题（如传感器网络中的缺失数据填充），提出了一种新方法，以克服现有方法依赖光滑假设和凸优化的局限性。核心创新是引入 Gegenbauer-based graph convolutional (GegenConv) 运算符，该运算符基于 Gegenbauer polynomials 理论，对传统 Chebyshev graph convolution 进行泛化，从而提升模型复杂度并提高重建准确性。基于此，他们设计了 Gegenbauer-based time Graph Neural Network (GegenGNN) 架构，采用编码器-解码器结构，并结合均方误差和 Sobolev smoothness regularization 的损失函数。在真实数据集上的实验显示，GegenGNN 优于最先进方法，在恢复时间变化图信号方面表现出显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS)",
      "pdf_url": "http://arxiv.org/pdf/2403.19800v2",
      "published_date": "2024-03-28 19:29:17 UTC",
      "updated_date": "2024-04-03 13:49:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:44:26.759251"
    },
    {
      "arxiv_id": "2403.19792v1",
      "title": "MAPL: Model Agnostic Peer-to-peer Learning",
      "title_zh": "MAPL：模型",
      "authors": [
        "Sayak Mukherjee",
        "Andrea Simonetto",
        "Hadi Jamali-Rad"
      ],
      "abstract": "Effective collaboration among heterogeneous clients in a decentralized\nsetting is a rather unexplored avenue in the literature. To structurally\naddress this, we introduce Model Agnostic Peer-to-peer Learning (coined as\nMAPL) a novel approach to simultaneously learn heterogeneous personalized\nmodels as well as a collaboration graph through peer-to-peer communication\namong neighboring clients. MAPL is comprised of two main modules: (i)\nlocal-level Personalized Model Learning (PML), leveraging a combination of\nintra- and inter-client contrastive losses; (ii) network-wide decentralized\nCollaborative Graph Learning (CGL) dynamically refining collaboration weights\nin a privacy-preserving manner based on local task similarities. Our extensive\nexperimentation demonstrates the efficacy of MAPL and its competitive (or, in\nmost cases, superior) performance compared to its centralized model-agnostic\ncounterparts, without relying on any central server. Our code is available and\ncan be accessed here: https://github.com/SayakMukherjee/MAPL",
      "tldr_zh": "这篇论文提出了 MAPL（Model Agnostic Peer-to-peer Learning），一种在去中心化环境中让异构客户端通过点对点通信同时学习个性化模型和协作图的方法。MAPL 包括两个主要模块：本地级别的 Personalized Model Learning (PML)，利用 intra- 和 inter-client contrastive losses 来优化模型；以及网络级的 decentralized Collaborative Graph Learning (CGL)，通过基于本地任务相似性的动态权重精炼来实现隐私保护协作。实验结果显示，MAPL 的性能与中心化方法相当或更优，且无需依赖中央服务器。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Our code is available and can be accessed here:\n  https://github.com/SayakMukherjee/MAPL",
      "pdf_url": "http://arxiv.org/pdf/2403.19792v1",
      "published_date": "2024-03-28 19:17:54 UTC",
      "updated_date": "2024-03-28 19:17:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:44:36.521190"
    },
    {
      "arxiv_id": "2403.19790v1",
      "title": "Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care",
      "title_zh": "翻译失败",
      "authors": [
        "Niall Taylor",
        "Andrey Kormilitzin",
        "Isabelle Lorge",
        "Alejo Nevado-Holgado",
        "Dan W Joyce"
      ],
      "abstract": "Contemporary large language models (LLMs) may have utility for processing\nunstructured, narrative free-text clinical data contained in electronic health\nrecords (EHRs) -- a particularly important use-case for mental health where a\nmajority of routinely-collected patient data lacks structured, machine-readable\ncontent.\n  A significant problem for the the United Kingdom's National Health Service\n(NHS) are the long waiting lists for specialist mental healthcare. According to\nNHS data, in each month of 2023, there were between 370,000 and 470,000\nindividual new referrals into secondary mental healthcare services. Referrals\nmust be triaged by clinicians, using clinical information contained in the\npatient's EHR to arrive at a decision about the most appropriate mental\nhealthcare team to assess and potentially treat these patients.\n  The ability to efficiently recommend a relevant team by ingesting potentially\nvoluminous clinical notes could help services both reduce referral waiting\ntimes and with the right technology, improve the evidence available to justify\ntriage decisions.\n  We present and evaluate three different approaches for LLM-based, end-to-end\ningestion of variable-length clinical EHR data to assist clinicians when\ntriaging referrals. Our model is able to deliver triage recommendations\nconsistent with existing clinical practices and it's architecture was\nimplemented on a single GPU, making it practical for implementation in\nresource-limited NHS environments where private implementations of LLM\ntechnology will be necessary to ensure confidential clinical data is\nappropriately controlled and governed.",
      "tldr_zh": "这篇论文探讨了使用定制的大型语言模型 (LLMs) 来辅助英国国家卫生服务 (NHS) 在心理保健领域的数字转诊分流问题，针对电子健康记录 (EHRs) 中非结构化临床数据的处理。研究者评估了三种端到端 LLM 方法，用于摄取可变长度的临床笔记，并提供与现有临床实践一致的转诊推荐，以帮助临床医生快速决定合适的心理保健团队。结果显示，该模型在单 GPU 上即可运行，适用于资源有限的环境，从而减少转诊等待时间、改善证据支持，并确保临床数据的保密治理。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19790v1",
      "published_date": "2024-03-28 19:17:07 UTC",
      "updated_date": "2024-03-28 19:17:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:44:48.244886"
    },
    {
      "arxiv_id": "2403.19770v1",
      "title": "Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyu Cai",
        "Karankumar Patel",
        "Soshi Iba",
        "Songpo Li"
      ],
      "abstract": "In human-robot collaboration, shared control presents an opportunity to\nteleoperate robotic manipulation to improve the efficiency of manufacturing and\nassembly processes. Robots are expected to assist in executing the user's\nintentions. To this end, robust and prompt intention estimation is needed,\nrelying on behavioral observations. The framework presents an intention\nestimation technique at hierarchical levels i.e., low-level actions and\nhigh-level tasks, by incorporating multi-scale hierarchical information in\nneural networks. Technically, we employ hierarchical dependency loss to boost\noverall accuracy. Furthermore, we propose a multi-window method that assigns\nproper hierarchical prediction windows of input data. An analysis of the\npredictive power with various inputs demonstrates the predominance of the deep\nhierarchical model in the sense of prediction accuracy and early intention\nidentification. We implement the algorithm on a virtual reality (VR) setup to\nteleoperate robotic hands in a simulation with various assembly tasks to show\nthe effectiveness of online estimation.",
      "tldr_zh": "该研究提出了一种基于分层深度学习（hierarchical deep learning）的框架，用于估计遥操作机械臂在组装任务中的用户意图。该框架通过整合多尺度分层信息来处理低层动作和高层任务的意图估计，并引入分层依赖损失（hierarchical dependency loss）和多窗口方法（multi-window method）来提升预测准确性和早期识别能力。实验分析显示，该模型在各种输入条件下表现出色，并在虚拟现实（VR）模拟环境中成功应用于遥操作机器人手执行组装任务，证明了其在线估计的有效性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "ICRA 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.19770v1",
      "published_date": "2024-03-28 18:45:43 UTC",
      "updated_date": "2024-03-28 18:45:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:44:59.876292"
    },
    {
      "arxiv_id": "2403.19760v1",
      "title": "Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Kraske",
        "Zakariya Laouar",
        "Zachary Sunberg"
      ],
      "abstract": "As humans come to rely on autonomous systems more, ensuring the transparency\nof such systems is important to their continued adoption. Explainable\nArtificial Intelligence (XAI) aims to reduce confusion and foster trust in\nsystems by providing explanations of agent behavior. Partially observable\nMarkov decision processes (POMDPs) provide a flexible framework capable of\nreasoning over transition and state uncertainty, while also being amenable to\nexplanation. This work investigates the use of user-provided counterfactuals to\ngenerate contrastive explanations of POMDP policies. Feature expectations are\nused as a means of contrasting the performance of these policies. We\ndemonstrate our approach in a Search and Rescue (SAR) setting. We analyze and\ndiscuss the associated challenges through two case studies.",
      "tldr_zh": "这篇论文探讨了在 Explainable Artificial Intelligence (XAI) 领域中，利用反事实路径 (counterfactual paths) 来生成部分可观测马尔可夫决策过程 (POMDP) 策略的对比解释，以提升自主系统的透明性和用户信任。方法通过用户提供的反事实来对比策略性能，并采用特征期望 (feature expectations) 作为评估指标。在搜索和救援 (SAR) 设置中演示了该方法，并通过两个案例研究分析了相关挑战和潜在问题。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2403.19760v1",
      "published_date": "2024-03-28 18:19:38 UTC",
      "updated_date": "2024-03-28 18:19:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:45:11.562980"
    },
    {
      "arxiv_id": "2403.19758v2",
      "title": "Quantum Natural Language Processing",
      "title_zh": "量子自然语言处理",
      "authors": [
        "Dominic Widdows",
        "Willie Aboumrad",
        "Dohun Kim",
        "Sayonee Ray",
        "Jonathan Mei"
      ],
      "abstract": "Language processing is at the heart of current developments in artificial\nintelligence, and quantum computers are becoming available at the same time.\nThis has led to great interest in quantum natural language processing, and\nseveral early proposals and experiments.\n  This paper surveys the state of this area, showing how NLP-related techniques\nhave been used in quantum language processing. We examine the art of word\nembeddings and sequential models, proposing some avenues for future\ninvestigation and discussing the tradeoffs present in these directions. We also\nhighlight some recent methods to compute attention in transformer models, and\nperform grammatical parsing. We also introduce a new quantum design for the\nbasic task of text encoding (representing a string of characters in memory),\nwhich has not been addressed in detail before.\n  Quantum theory has contributed toward quantifying uncertainty and explaining\n\"What is intelligence?\" In this context, we argue that \"hallucinations\" in\nmodern artificial intelligence systems are a misunderstanding of the way facts\nare conceptualized: language can express many plausible hypotheses, of which\nonly a few become actual.",
      "tldr_zh": "这篇论文调查了量子自然语言处理（Quantum Natural Language Processing）的现状，探讨了如何将传统 NLP 技术应用于量子计算环境，包括词嵌入（word embeddings）、序列模型（sequential models）、注意力机制（attention）和语法解析（grammatical parsing）。作者提出了一种新的量子设计，用于文本编码（text encoding）任务，以更有效地表示字符字符串。论文还讨论了量子理论在量化不确定性和解释智能方面的贡献，认为现代 AI 系统中的“hallucinations”（幻觉）源于对事实概念化的误解，导致语言表达了多种可能假设而非实际事实。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19758v2",
      "published_date": "2024-03-28 18:15:07 UTC",
      "updated_date": "2024-04-26 18:45:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:45:23.205696"
    },
    {
      "arxiv_id": "2403.19652v1",
      "title": "InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Sirui Xu",
        "Ziyin Wang",
        "Yu-Xiong Wang",
        "Liang-Yan Gui"
      ],
      "abstract": "Text-conditioned human motion generation has experienced significant\nadvancements with diffusion models trained on extensive motion capture data and\ncorresponding textual annotations. However, extending such success to 3D\ndynamic human-object interaction (HOI) generation faces notable challenges,\nprimarily due to the lack of large-scale interaction data and comprehensive\ndescriptions that align with these interactions. This paper takes the\ninitiative and showcases the potential of generating human-object interactions\nwithout direct training on text-interaction pair data. Our key insight in\nachieving this is that interaction semantics and dynamics can be decoupled.\nBeing unable to learn interaction semantics through supervised training, we\ninstead leverage pre-trained large models, synergizing knowledge from a large\nlanguage model and a text-to-motion model. While such knowledge offers\nhigh-level control over interaction semantics, it cannot grasp the intricacies\nof low-level interaction dynamics. To overcome this issue, we further introduce\na world model designed to comprehend simple physics, modeling how human actions\ninfluence object motion. By integrating these components, our novel framework,\nInterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot\nmanner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our\ncomprehensive experimental analysis demonstrates its capability to generate\nrealistic and coherent interaction sequences that seamlessly align with the\ntext directives.",
      "tldr_zh": "本研究提出InterDreamer框架，实现零样本（Zero-Shot）文本到3D动态人类-物体交互（HOI）生成，解决现有方法因缺乏大规模交互数据而面临的挑战。框架的关键洞见是将交互语义和动态分离，利用预训练的大型语言模型（LLM）和文本到动作模型（Text-to-Motion Model）处理高层次语义，同时引入一个世界模型（World Model）来模拟简单物理学并管理低级动态。通过整合这些组件，InterDreamer能够在BEHAVE和CHAIRS数据集上生成与文本指令高度对齐的真实、连贯HOI序列。实验结果证明，该框架显著提升了交互生成的质量和可控性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://sirui-xu.github.io/InterDreamer/",
      "pdf_url": "http://arxiv.org/pdf/2403.19652v1",
      "published_date": "2024-03-28 17:59:30 UTC",
      "updated_date": "2024-03-28 17:59:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:45:37.488786"
    },
    {
      "arxiv_id": "2403.19651v2",
      "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Zhang",
        "Yi Luan",
        "Hexiang Hu",
        "Kenton Lee",
        "Siyuan Qiao",
        "Wenhu Chen",
        "Yu Su",
        "Ming-Wei Chang"
      ],
      "abstract": "Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent works leverage text\ninstructions to allow users to more freely express their search intents.\nHowever, they primarily focus on image pairs that are visually similar and/or\ncan be characterized by a small set of pre-defined relations. The core thesis\nof this paper is that text instructions can enable retrieving images with\nricher relations beyond visual similarity. To show this, we introduce\nMagicLens, a series of self-supervised image retrieval models that support\nopen-ended instructions. MagicLens is built on a key novel insight: image pairs\nthat naturally occur on the same web pages contain a wide range of implicit\nrelations (e.g., inside view of), and we can bring those implicit relations\nexplicit by synthesizing instructions via foundation models. Trained on 36.7M\n(query image, instruction, target image) triplets with rich semantic relations\nmined from the web, MagicLens achieves results comparable with or better than\nprior best on eight benchmarks of various image retrieval tasks, while\nmaintaining high parameter efficiency with a significantly smaller model size.\nAdditional human analyses on a 1.4M-image unseen corpus further demonstrate the\ndiversity of search intents supported by MagicLens. Code and models are\npublicly available at https://open-vision-language.github.io/MagicLens/.",
      "tldr_zh": "这篇论文提出 MagicLens，一种自监督(Self-Supervised)图像检索模型，支持开放式(Open-Ended)指令，以捕捉图像检索中超出视觉相似性的丰富关系，如隐式语义联系。MagicLens 的核心创新是通过基础模型合成指令，将网页上自然出现的图像对（如内部视图）中的隐式关系显性化，并使用36.7M 的（查询图像、指令、目标图像）三元组进行训练。实验结果显示，MagicLens 在八个图像检索基准测试中达到或超过了现有最佳模型，同时保持高参数效率，模型尺寸显著更小；此外，人为分析在1.4M 未见过图像语料上证实了其支持的搜索意图多样性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "ICML 2024 (Oral); Project Website:\n  https://open-vision-language.github.io/MagicLens/",
      "pdf_url": "http://arxiv.org/pdf/2403.19651v2",
      "published_date": "2024-03-28 17:59:20 UTC",
      "updated_date": "2024-06-24 23:41:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:45:49.477754"
    },
    {
      "arxiv_id": "2403.19648v2",
      "title": "Human-compatible driving partners through data-regularized self-play reinforcement learning",
      "title_zh": "翻译失败",
      "authors": [
        "Daphne Cornelisse",
        "Eugene Vinitsky"
      ],
      "abstract": "A central challenge for autonomous vehicles is coordinating with humans.\nTherefore, incorporating realistic human agents is essential for scalable\ntraining and evaluation of autonomous driving systems in simulation. Simulation\nagents are typically developed by imitating large-scale, high-quality datasets\nof human driving. However, pure imitation learning agents empirically have high\ncollision rates when executed in a multi-agent closed-loop setting. To build\nagents that are realistic and effective in closed-loop settings, we propose\nHuman-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are\ntrained through self-play with a small penalty for deviating from a human\nreference policy. In contrast to prior work, our approach is RL-first and only\nuses 30 minutes of imperfect human demonstrations. We evaluate agents in a\nlarge set of multi-agent traffic scenes. Results show our HR-PPO agents are\nhighly effective in achieving goals, with a success rate of 93%, an off-road\nrate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in\na human-like manner, as measured by their similarity to existing human driving\nlogs. We also find that HR-PPO agents show considerable improvements on proxy\nmeasures for coordination with human driving, particularly in highly\ninteractive scenarios. We open-source our code and trained agents at\nhttps://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent\nbehaviors at https://sites.google.com/view/driving-partners.",
      "tldr_zh": "这篇论文提出了一种名为 Human-Regularized PPO (HR-PPO) 的多代理算法，通过 self-play reinforcement learning 训练自动驾驶代理，并在偏离人类参考策略时施加小惩罚，以提升代理在闭环环境中的协调性和安全性。该方法以 RL-first 为主，仅需 30 分钟的不完美人类演示数据，就能在多代理交通场景中实现高性能。实验结果显示，HR-PPO 代理的成功率达到 93%，碰撞率仅 3%，并在高度互动场景中显著改善了与人类驾驶的协调效果。该研究为开发更可靠的自动驾驶系统提供了新框架，并开源了代码和演示。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19648v2",
      "published_date": "2024-03-28 17:56:56 UTC",
      "updated_date": "2024-06-22 23:47:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:46:01.553682"
    },
    {
      "arxiv_id": "2403.19647v3",
      "title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models",
      "title_zh": "稀疏特征电路：发现和编辑语言模型中的可解释因果图",
      "authors": [
        "Samuel Marks",
        "Can Rager",
        "Eric J. Michaud",
        "Yonatan Belinkov",
        "David Bau",
        "Aaron Mueller"
      ],
      "abstract": "We introduce methods for discovering and applying sparse feature circuits.\nThese are causally implicated subnetworks of human-interpretable features for\nexplaining language model behaviors. Circuits identified in prior work consist\nof polysemantic and difficult-to-interpret units like attention heads or\nneurons, rendering them unsuitable for many downstream applications. In\ncontrast, sparse feature circuits enable detailed understanding of\nunanticipated mechanisms. Because they are based on fine-grained units, sparse\nfeature circuits are useful for downstream tasks: We introduce SHIFT, where we\nimprove the generalization of a classifier by ablating features that a human\njudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised\nand scalable interpretability pipeline by discovering thousands of sparse\nfeature circuits for automatically discovered model behaviors.",
      "tldr_zh": "本研究引入了稀疏特征电路（Sparse Feature Circuits），一种基于人类可解释特征的因果相关子网络，用于发现和编辑语言模型的行为机制，与以往依赖多义性单元（如注意力头或神经元）的电路方法相比，它提供更细粒度和详细的理解。研究提出SHIFT技术，通过去除人类判断为任务无关的特征，来提升分类器的泛化性能；此外，他们构建了一个完全无监督且可扩展的解释性管道，能够自动发现数千个稀疏特征电路，应用于模型行为的自动识别。整体而言，此方法为语言模型的可解释性和下游任务优化提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Code and data at https://github.com/saprmarks/feature-circuits.\n  Demonstration at https://feature-circuits.xyz",
      "pdf_url": "http://arxiv.org/pdf/2403.19647v3",
      "published_date": "2024-03-28 17:56:07 UTC",
      "updated_date": "2025-03-27 05:44:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:46:12.120067"
    },
    {
      "arxiv_id": "2403.19631v2",
      "title": "Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Yucheng Shi",
        "Qiaoyu Tan",
        "Xuansheng Wu",
        "Shaochen Zhong",
        "Kaixiong Zhou",
        "Ninghao Liu"
      ],
      "abstract": "Large Language Models (LLMs) have shown proficiency in question-answering\ntasks but often struggle to integrate real-time knowledge, leading to\npotentially outdated or inaccurate responses. This problem becomes even more\nchallenging when dealing with multi-hop questions, since they require LLMs to\nupdate and integrate multiple knowledge pieces relevant to the questions. To\ntackle the problem, we propose the Retrieval-Augmented model Editing (RAE)\nframework for multi-hop question answering. RAE first retrieves edited facts\nand then refines the language model through in-context learning. Specifically,\nour retrieval approach, based on mutual information maximization, leverages the\nreasoning abilities of LLMs to identify chain facts that traditional\nsimilarity-based searches might miss. In addition, our framework includes a\npruning strategy to eliminate redundant information from the retrieved facts,\nwhich enhances the editing accuracy and mitigates the hallucination problem.\nOur framework is supported by theoretical justification for its fact retrieval\nefficacy. Finally, comprehensive evaluation across various LLMs validates RAE's\nability in providing accurate answers with updated knowledge. Our code is\navailable at: https://github.com/sycny/RAE.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)在多跳问答(multi-hop question answering)中整合实时知识的挑战，提出Retrieval-Augmented model Editing (RAE)框架，以提升模型对更新知识的处理能力。RAE框架首先通过基于mutual information maximization的检索方法，利用LLMs的推理能力识别链式事实(chain facts)，并采用修剪策略(pruning strategy)去除冗余信息，以提高编辑准确性和减少幻觉(hallucination)问题；随后，通过in-context learning精炼模型。实验结果显示，RAE在各种LLMs上提供更准确的答案，并得到理论支持证明其检索效能，代码已在GitHub开源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.19631v2",
      "published_date": "2024-03-28 17:47:19 UTC",
      "updated_date": "2024-08-13 19:34:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:46:24.866787"
    },
    {
      "arxiv_id": "2403.19620v1",
      "title": "Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ole Hall",
        "Anil Yaman"
      ],
      "abstract": "Generative Adversarial Networks (GANs) have shown great success in generating\nhigh quality images and are thus used as one of the main approaches to generate\nart images. However, usually the image generation process involves sampling\nfrom the latent space of the learned art representations, allowing little\ncontrol over the output. In this work, we first employ GANs that are trained to\nproduce creative images using an architecture known as Creative Adversarial\nNetworks (CANs), then, we employ an evolutionary approach to navigate within\nthe latent space of the models to discover images. We use automatic aesthetic\nand collaborative interactive human evaluation metrics to assess the generated\nimages. In the human interactive evaluation case, we propose a collaborative\nevaluation based on the assessments of several participants. Furthermore, we\nalso experiment with an intelligent mutation operator that aims to improve the\nquality of the images through local search based on an aesthetic measure. We\nevaluate the effectiveness of this approach by comparing the results produced\nby the automatic and collaborative interactive evolution. The results show that\nthe proposed approach can generate highly attractive art images when the\nevolution is guided by collaborative human feedback.",
      "tldr_zh": "本研究探讨了在深度生成模型潜在空间中，通过进化算法生成艺术图像的问题。作者首先使用 Creative Adversarial Networks (CANs) 训练模型生成创意图像，然后采用进化方法导航潜在空间，并引入智能变异操作符基于美学度量进行局部优化。评估采用自动美学指标和协作互动人类评估（涉及多个参与者），结果显示，在协作人类反馈指导下，该方法能生成高度吸引人的艺术图像，并显著提升生成过程的可控性。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "Preprint. The Version of Record of this contribution is to be\n  published in the proceedings of the 13th International Conference on\n  Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART) 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.19620v1",
      "published_date": "2024-03-28 17:40:15 UTC",
      "updated_date": "2024-03-28 17:40:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:46:36.095211"
    },
    {
      "arxiv_id": "2403.19603v1",
      "title": "Semantic Map-based Generation of Navigation Instructions",
      "title_zh": "翻译失败",
      "authors": [
        "Chengzu Li",
        "Chao Zhang",
        "Simone Teufel",
        "Rama Sanand Doddipatla",
        "Svetlana Stoyanchev"
      ],
      "abstract": "We are interested in the generation of navigation instructions, either in\ntheir own right or as training material for robotic navigation task. In this\npaper, we propose a new approach to navigation instruction generation by\nframing the problem as an image captioning task using semantic maps as visual\ninput. Conventional approaches employ a sequence of panorama images to generate\nnavigation instructions. Semantic maps abstract away from visual details and\nfuse the information in multiple panorama images into a single top-down\nrepresentation, thereby reducing computational complexity to process the input.\nWe present a benchmark dataset for instruction generation using semantic maps,\npropose an initial model and ask human subjects to manually assess the quality\nof generated instructions. Our initial investigations show promise in using\nsemantic maps for instruction generation instead of a sequence of panorama\nimages, but there is vast scope for improvement. We release the code for data\npreparation and model training at https://github.com/chengzu-li/VLGen.",
      "tldr_zh": "本研究提出了一种基于语义地图（semantic maps）的导航指令生成方法，将问题框架化为图像描述任务（image captioning），以减少处理传统全景图像序列（panorama images）的计算复杂性。该方法通过融合多张图像信息生成单一顶部视图表示，并创建了一个新的基准数据集，同时开发了一个初始模型，并通过人类评估验证其指令质量。实验结果显示，这种方法在导航指令生成上显示出潜力，但仍需进一步改进。研究代码已发布在https://github.com/chengzu-li/VLGen。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 2 figures, 3 tables (13 pages, 3 figures, 5 tables including\n  references and appendices), accepted at LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.19603v1",
      "published_date": "2024-03-28 17:27:44 UTC",
      "updated_date": "2024-03-28 17:27:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:46:48.777938"
    },
    {
      "arxiv_id": "2403.19595v1",
      "title": "Situation Awareness for Driver-Centric Driving Style Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Johann Haselberger",
        "Bonifaz Stuhr",
        "Bernhard Schick",
        "Steffen Müller"
      ],
      "abstract": "There is evidence that the driving style of an autonomous vehicle is\nimportant to increase the acceptance and trust of the passengers. The driving\nsituation has been found to have a significant influence on human driving\nbehavior. However, current driving style models only partially incorporate\ndriving environment information, limiting the alignment between an agent and\nthe given situation. Therefore, we propose a situation-aware driving style\nmodel based on different visual feature encoders pretrained on fleet data, as\nwell as driving behavior predictors, which are adapted to the driving style of\na specific driver. Our experiments show that the proposed method outperforms\nstatic driving styles significantly and forms plausible situation clusters.\nFurthermore, we found that feature encoders pretrained on our dataset lead to\nmore precise driving behavior modeling. In contrast, feature encoders\npretrained supervised and unsupervised on different data sources lead to more\nspecific situation clusters, which can be utilized to constrain and control the\ndriving style adaptation for specific situations. Moreover, in a real-world\nsetting, where driving style adaptation is happening iteratively, we found the\nMLP-based behavior predictors achieve good performance initially but suffer\nfrom catastrophic forgetting. In contrast, behavior predictors based on\nsituationdependent statistics can learn iteratively from continuous data\nstreams by design. Overall, our experiments show that important information for\ndriving behavior prediction is contained within the visual feature encoder. The\ndataset is publicly available at\nhuggingface.co/datasets/jHaselberger/SADC-Situation-Awareness-for-Driver-Centric-Driving-Style-Adaptation.",
      "tldr_zh": "本研究针对自动驾驶车辆的驾驶风格对乘客接受度和信任的影响，提出了一种情境感知驾驶风格模型（situation-aware driving style model），该模型利用预训练的视觉特征编码器（visual feature encoders）和适应特定司机的驾驶行为预测器（driving behavior predictors），以更好地整合驾驶环境信息。实验结果显示，该方法显著优于静态驾驶风格，能够形成合理的情境集群，并在不同数据来源的编码器上表现出色，例如数据集上预训练的编码器提升了行为建模精度，而其他来源的编码器则更适合特定情境控制。总体而言，该模型证明视觉特征编码器中包含关键信息，且基于情境统计的行为预测器在迭代学习中避免了MLP-based预测器的灾难性遗忘问题，并公开了相关数据集以供进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication",
      "pdf_url": "http://arxiv.org/pdf/2403.19595v1",
      "published_date": "2024-03-28 17:19:16 UTC",
      "updated_date": "2024-03-28 17:19:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:47:01.229931"
    },
    {
      "arxiv_id": "2403.19584v1",
      "title": "Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongliang Zhou",
        "Jielu Zhang",
        "Zihan Guan",
        "Mengxuan Hu",
        "Ni Lao",
        "Lan Mu",
        "Sheng Li",
        "Gengchen Mai"
      ],
      "abstract": "Geolocating precise locations from images presents a challenging problem in\ncomputer vision and information retrieval.Traditional methods typically employ\neither classification, which dividing the Earth surface into grid cells and\nclassifying images accordingly, or retrieval, which identifying locations by\nmatching images with a database of image-location pairs. However,\nclassification-based approaches are limited by the cell size and cannot yield\nprecise predictions, while retrieval-based systems usually suffer from poor\nsearch quality and inadequate coverage of the global landscape at varied scale\nand aggregation levels. To overcome these drawbacks, we present Img2Loc, a\nnovel system that redefines image geolocalization as a text generation task.\nThis is achieved using cutting-edge large multi-modality models like GPT4V or\nLLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based\nrepresentations to generate an image-based coordinate query database. It then\nuniquely combines query results with images itself, forming elaborate prompts\ncustomized for LMMs. When tested on benchmark datasets such as Im2GPS3k and\nYFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art\nmodels but does so without any model training.",
      "tldr_zh": "本文提出 Img2Loc 系统，重定义图像地理定位为文本生成任务，利用多模态 Foundation Models（如 GPT4V 或 LLaVA）和图像-based Retrieval-Augmented Generation，解决传统分类和检索方法的局限性。系统首先使用 CLIP-based representations 生成图像坐标查询数据库，然后将查询结果与图像结合形成定制提示，以提升全球景观覆盖和搜索质量。在 Im2GPS3k 和 YFCC4k 等基准数据集上，Img2Loc 超越了现有最先进模型的性能，且无需任何模型训练。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19584v1",
      "published_date": "2024-03-28 17:07:02 UTC",
      "updated_date": "2024-03-28 17:07:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:47:12.941594"
    },
    {
      "arxiv_id": "2403.19561v3",
      "title": "Self-Improved Learning for Scalable Neural Combinatorial Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Fu Luo",
        "Xi Lin",
        "Zhenkun Wang",
        "Xialiang Tong",
        "Mingxuan Yuan",
        "Qingfu Zhang"
      ],
      "abstract": "The end-to-end neural combinatorial optimization (NCO) method shows promising\nperformance in solving complex combinatorial optimization problems without the\nneed for expert design. However, existing methods struggle with large-scale\nproblems, hindering their practical applicability. To overcome this limitation,\nthis work proposes a novel Self-Improved Learning (SIL) method for better\nscalability of neural combinatorial optimization. Specifically, we develop an\nefficient self-improved mechanism that enables direct model training on\nlarge-scale problem instances without any labeled data. Powered by an\ninnovative local reconstruction approach, this method can iteratively generate\nbetter solutions by itself as pseudo-labels to guide efficient model training.\nIn addition, we design a linear complexity attention mechanism for the model to\nefficiently handle large-scale combinatorial problem instances with low\ncomputation overhead. Comprehensive experiments on the Travelling Salesman\nProblem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to\n100K nodes in both uniform and real-world distributions demonstrate the\nsuperior scalability of our method.",
      "tldr_zh": "本研究针对神经组合优化 (NCO) 在处理大规模组合优化问题时存在的可扩展性挑战，提出了一种新型的 Self-Improved Learning (SIL) 方法。该方法通过高效的自改进机制和局部重建方法，实现模型在无标记数据情况下直接在大规模实例上训练，并生成伪标签来优化学习过程。同时，SIL 引入了线性复杂度注意力机制，以降低计算开销并高效处理大型问题。实验在 Travelling Salesman Problem (TSP) 和 Capacitated Vehicle Routing Problem (CVRP) 上验证了其优越性，支持高达 100K 节点的实例，并展示了显著的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19561v3",
      "published_date": "2024-03-28 16:46:53 UTC",
      "updated_date": "2024-05-02 09:07:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:47:26.120238"
    },
    {
      "arxiv_id": "2403.19546v3",
      "title": "Croissant: A Metadata Format for ML-Ready Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Mubashara Akhtar",
        "Omar Benjelloun",
        "Costanza Conforti",
        "Luca Foschini",
        "Joan Giner-Miguelez",
        "Pieter Gijsbers",
        "Sujata Goswami",
        "Nitisha Jain",
        "Michalis Karamousadakis",
        "Michael Kuchnik",
        "Satyapriya Krishna",
        "Sylvain Lesage",
        "Quentin Lhoest",
        "Pierre Marcenac",
        "Manil Maskey",
        "Peter Mattson",
        "Luis Oala",
        "Hamidah Oderinwale",
        "Pierre Ruyssen",
        "Tim Santos",
        "Rajat Shinde",
        "Elena Simperl",
        "Arjun Suresh",
        "Goeffry Thomas",
        "Slava Tykhonov",
        "Joaquin Vanschoren",
        "Susheel Varma",
        "Jos van der Velde",
        "Steffen Vogler",
        "Carole-Jean Wu",
        "Luyao Zhang"
      ],
      "abstract": "Data is a critical resource for machine learning (ML), yet working with data\nremains a key friction point. This paper introduces Croissant, a metadata\nformat for datasets that creates a shared representation across ML tools,\nframeworks, and platforms. Croissant makes datasets more discoverable,\nportable, and interoperable, thereby addressing significant challenges in ML\ndata management. Croissant is already supported by several popular dataset\nrepositories, spanning hundreds of thousands of datasets, enabling easy loading\ninto the most commonly-used ML frameworks, regardless of where the data is\nstored. Our initial evaluation by human raters shows that Croissant metadata is\nreadable, understandable, complete, yet concise.",
      "tldr_zh": "本论文提出 Croissant，一种用于机器学习 (ML) 就绪数据集的元数据格式，旨在为 ML 工具、框架和平台创建共享表示，从而提升数据集的可发现性、可移植性和互操作性。Croissant 通过解决 ML 数据管理的关键挑战，已被多个流行数据集仓库支持，覆盖数十万个数据集，并便于无缝加载到常用 ML 框架中，无论数据存储位置。初步人类评估显示，Croissant 元数据格式易读、易懂、完整且简洁，为改进 ML 数据处理提供了实用解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "Published at the NeurIPS 2024 Datasets and Benchmark Track. A shorter\n  version appeared earlier in Proceedings of ACM SIGMOD/PODS'24 Data Management\n  for End-to-End Machine Learning (DEEM) Workshop\n  https://dl.acm.org/doi/10.1145/3650203.3663326",
      "pdf_url": "http://arxiv.org/pdf/2403.19546v3",
      "published_date": "2024-03-28 16:27:26 UTC",
      "updated_date": "2024-12-09 18:37:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:47:37.402550"
    },
    {
      "arxiv_id": "2403.19545v1",
      "title": "Lamarckian Inheritance Improves Robot Evolution in Dynamic Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Luo",
        "Karine Miras",
        "Carlo Longhi",
        "Oliver Weissl",
        "Agoston E. Eiben"
      ],
      "abstract": "This study explores the integration of Lamarckian system into evolutionary\nrobotics (ER), comparing it with the traditional Darwinian model across various\nenvironments. By adopting Lamarckian principles, where robots inherit learned\ntraits, alongside Darwinian learning without inheritance, we investigate\nadaptation in dynamic settings. Our research, conducted in six distinct\nenvironmental setups, demonstrates that Lamarckian systems outperform Darwinian\nones in adaptability and efficiency, particularly in challenging conditions.\nOur analysis highlights the critical role of the interplay between controller\n\\& morphological evolution and environment adaptation, with parent-offspring\nsimilarities and newborn \\&survivors before and after learning providing\ninsights into the effectiveness of trait inheritance. Our findings suggest\nLamarckian principles could significantly advance autonomous system design,\nhighlighting the potential for more adaptable and robust robotic solutions in\ncomplex, real-world applications. These theoretical insights were validated\nusing real physical robots, bridging the gap between simulation and practical\napplication.",
      "tldr_zh": "本研究探讨了将 Lamarckian 系统整合到进化机器人学 (ER) 中，与传统的 Darwinian 模型进行比较，重点考察动态环境下的适应性。通过在六个不同环境设置中进行实验，Lamarckian 系统允许机器人继承学到的特性，从而在挑战条件下表现出更高的适应性和效率。分析结果强调了控制器与形态进化之间的互动，以及亲子相似性的重要作用，并通过实际物理机器人的验证，证明 Lamarckian 原则可显著提升自主系统的设计和实际应用潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Nature. arXiv admin note: substantial text overlap with\n  arXiv:2309.13099; text overlap with arXiv:2303.12594, arXiv:2309.14387",
      "pdf_url": "http://arxiv.org/pdf/2403.19545v1",
      "published_date": "2024-03-28 16:27:20 UTC",
      "updated_date": "2024-03-28 16:27:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:47:50.271785"
    },
    {
      "arxiv_id": "2403.19521v4",
      "title": "Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models",
      "title_zh": "解读 Transformer-Based 语言模型中事实回忆的关键机制",
      "authors": [
        "Ang Lv",
        "Yuhan Chen",
        "Kaiyi Zhang",
        "Yulong Wang",
        "Lifeng Liu",
        "Ji-Rong Wen",
        "Jian Xie",
        "Rui Yan"
      ],
      "abstract": "In this paper, we delve into several mechanisms employed by Transformer-based\nlanguage models (LLMs) for factual recall tasks. We outline a pipeline\nconsisting of three major steps: (1) Given a prompt ``The capital of France\nis,'' task-specific attention heads extract the topic token, such as\n``France,'' from the context and pass it to subsequent MLPs. (2) As attention\nheads' outputs are aggregated with equal weight and added to the residual\nstream, the subsequent MLP acts as an ``activation,'' which either erases or\namplifies the information originating from individual heads. As a result, the\ntopic token ``France'' stands out in the residual stream. (3) A deep MLP takes\n``France'' and generates a component that redirects the residual stream towards\nthe direction of the correct answer, i.e., ``Paris.'' This procedure is akin to\napplying an implicit function such as ``get\\_capital($X$),'' and the argument\n$X$ is the topic token information passed by attention heads. To achieve the\nabove quantitative and qualitative analysis for MLPs, we proposed a novel\nanalytic method aimed at decomposing the outputs of the MLP into components\nunderstandable by humans. Additionally, we observed a universal\nanti-overconfidence mechanism in the final layer of models, which suppresses\ncorrect predictions. We mitigate this suppression by leveraging our\ninterpretation to improve factual recall confidence. The above interpretations\nare evaluated across diverse tasks spanning various domains of factual\nknowledge, using various language models from the GPT-2 families, 1.3B OPT, up\nto 7B Llama-2, and in both zero- and few-shot setups.",
      "tldr_zh": "本文研究了Transformer-based语言模型（LLMs）在事实回忆任务中的关键机制，提出一个三步管道：任务特定的attention heads从上下文中提取主题token（如“France”）并传递给后续MLPs；MLPs作为“activation”机制，在residual stream中增强或擦除信息，使主题突出；深层MLP生成组件引导residual stream指向正确答案（如“Paris”），类似于隐式函数。研究者开发了一种新型分析方法来分解MLPs的输出，便于人类理解，并发现模型最终层存在普遍的anti-overconfidence机制，通过解释性干预成功提高了事实回忆的置信度。这些发现已在GPT-2系列、1.3B OPT和7B Llama-2等模型上，以及零样本和少样本设置中得到验证。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19521v4",
      "published_date": "2024-03-28 15:54:59 UTC",
      "updated_date": "2024-05-24 15:06:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:48:03.659671"
    },
    {
      "arxiv_id": "2404.01322v1",
      "title": "A Review of Multi-Modal Large Language and Vision Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kilian Carolan",
        "Laura Fennelly",
        "Alan F. Smeaton"
      ],
      "abstract": "Large Language Models (LLMs) have recently emerged as a focal point of\nresearch and application, driven by their unprecedented ability to understand\nand generate text with human-like quality. Even more recently, LLMs have been\nextended into multi-modal large language models (MM-LLMs) which extends their\ncapabilities to deal with image, video and audio information, in addition to\ntext. This opens up applications like text-to-video generation, image\ncaptioning, text-to-speech, and more and is achieved either by retro-fitting an\nLLM with multi-modal capabilities, or building a MM-LLM from scratch. This\npaper provides an extensive review of the current state of those LLMs with\nmulti-modal capabilities as well as the very recent MM-LLMs. It covers the\nhistorical development of LLMs especially the advances enabled by\ntransformer-based architectures like OpenAI's GPT series and Google's BERT, as\nwell as the role of attention mechanisms in enhancing model performance. The\npaper includes coverage of the major and most important of the LLMs and MM-LLMs\nand also covers the techniques of model tuning, including fine-tuning and\nprompt engineering, which tailor pre-trained models to specific tasks or\ndomains. Ethical considerations and challenges, such as data bias and model\nmisuse, are also analysed to underscore the importance of responsible AI\ndevelopment and deployment. Finally, we discuss the implications of open-source\nversus proprietary models in AI research. Through this review, we provide\ninsights into the transformative potential of MM-LLMs in various applications.",
      "tldr_zh": "这篇论文对多模态大语言模型（MM-LLMs）进行了全面回顾，这些模型扩展了Large Language Models (LLMs) 的能力，以处理图像、视频和音频等非文本信息。论文追溯了LLMs的历史发展，特别是基于Transformer架构的进步，如OpenAI的GPT系列和Google的BERT，以及注意力机制的作用。作者讨论了MM-LLMs的构建方法，包括对现有LLMs进行改装或从零构建，并介绍了模型调整技术如fine-tuning和prompt engineering，以适应特定任务。最终，论文分析了伦理挑战（如数据偏差和模型误用）、开源与专有模型的比较，并强调了MM-LLMs在文本到视频生成、图像描述等领域中的变革潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "33 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2404.01322v1",
      "published_date": "2024-03-28 15:53:45 UTC",
      "updated_date": "2024-03-28 15:53:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:48:14.075707"
    },
    {
      "arxiv_id": "2403.19736v1",
      "title": "Physics-Informed Neural Networks for Satellite State Estimation",
      "title_zh": "物理信息神经网络用于卫星状态估计",
      "authors": [
        "Jacob Varey",
        "Jessica D. Ruprecht",
        "Michael Tierney",
        "Ryan Sullenberger"
      ],
      "abstract": "The Space Domain Awareness (SDA) community routinely tracks satellites in\norbit by fitting an orbital state to observations made by the Space\nSurveillance Network (SSN). In order to fit such orbits, an accurate model of\nthe forces that are acting on the satellite is required. Over the past several\ndecades, high-quality, physics-based models have been developed for satellite\nstate estimation and propagation. These models are exceedingly good at\nestimating and propagating orbital states for non-maneuvering satellites;\nhowever, there are several classes of anomalous accelerations that a satellite\nmight experience which are not well-modeled, such as satellites that use\nlow-thrust electric propulsion to modify their orbit. Physics-Informed Neural\nNetworks (PINNs) are a valuable tool for these classes of satellites as they\ncombine physics models with Deep Neural Networks (DNNs), which are highly\nexpressive and versatile function approximators. By combining a physics model\nwith a DNN, the machine learning model need not learn astrodynamics, which\nresults in more efficient and effective utilization of machine learning\nresources. This paper details the application of PINNs to estimate the orbital\nstate and a continuous, low-amplitude anomalous acceleration profile for\nsatellites. The PINN is trained to learn the unknown acceleration by minimizing\nthe mean square error of observations. We evaluate the performance of pure\nphysics models with PINNs in terms of their observation residuals and their\npropagation accuracy beyond the fit span of the observations. For a two-day\nsimulation of a GEO satellite using an unmodeled acceleration profile on the\norder of $10^{-8} \\text{ km/s}^2$, the PINN outperformed the best-fit physics\nmodel by orders of magnitude for both observation residuals (123 arcsec vs 1.00\narcsec) as well as propagation accuracy (3860 km vs 164 km after five days).",
      "tldr_zh": "本研究探讨了使用 Physics-Informed Neural Networks (PINNs) 来改进卫星轨道状态估计，针对传统物理模型在处理异常加速度（如低推力电动推进）时的不足。PINNs 通过将物理模型与 Deep Neural Networks (DNNs) 结合，仅需学习未知加速度，并通过最小化观测均方误差进行训练，从而更高效地估计卫星状态和连续低幅异常加速度。实验结果显示，在一个 GEO 卫星的两天模拟中，PINNs 相较纯物理模型大幅提升性能，观测残差从 123 弧秒降至 1.00 弧秒，传播准确性在五天后从 3860 km 改善至 164 km。总的来说，该方法为 Space Domain Awareness (SDA) 和 Space Surveillance Network (SSN) 的卫星跟踪提供了更准确可靠的工具。",
      "categories": [
        "astro-ph.IM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19736v1",
      "published_date": "2024-03-28 14:54:57 UTC",
      "updated_date": "2024-03-28 14:54:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:48:25.990728"
    },
    {
      "arxiv_id": "2403.19460v2",
      "title": "RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Chongkai Gao",
        "Zhengrong Xue",
        "Shuying Deng",
        "Tianhai Liang",
        "Siqi Yang",
        "Lin Shao",
        "Huazhe Xu"
      ],
      "abstract": "We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot\nManipulation imitation learning framework from scene point cloud input.\nCompared to previous methods that rely on descriptor field matching, RiEMann\ndirectly predicts the target poses of objects for manipulation without any\nobject segmentation. RiEMann learns a manipulation task from scratch with 5 to\n10 demonstrations, generalizes to unseen SE(3) transformations and instances of\ntarget objects, resists visual interference of distracting objects, and follows\nthe near real-time pose change of the target object. The scalable action space\nof RiEMann facilitates the addition of custom equivariant actions such as the\ndirection of turning the faucet, which makes articulated object manipulation\npossible for RiEMann. In simulation and real-world 6-DOF robot manipulation\nexperiments, we test RiEMann on 5 categories of manipulation tasks with a total\nof 25 variants and show that RiEMann outperforms baselines in both task success\nrates and SE(3) geodesic distance errors on predicted poses (reduced by 68.6%),\nand achieves a 5.4 frames per second (FPS) network inference speed. Code and\nvideo results are available at https://riemann-web.github.io/.",
      "tldr_zh": "本研究提出RiEMann，一个端到端的近实时SE(3)-Equivariant机器人操作模仿学习框架，能够从场景点云输入直接预测物体目标姿势，而无需点云分割。相比传统依赖描述符字段匹配的方法，RiEMann仅需5到10个演示即可学习任务，并实现对未见SE(3)变换、物体实例的泛化，以及抵抗视觉干扰和实时姿势跟踪。该框架还支持可扩展的操作空间，便于添加自定义等变动作，如操作铰接物体。在模拟和真实6-DOF机器人实验中，RiEMann在25个任务变体上优于基线模型，任务成功率提升且SE(3)测地距离错误减少68.6%，网络推理速度达5.4 FPS。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19460v2",
      "published_date": "2024-03-28 14:31:10 UTC",
      "updated_date": "2024-10-03 11:13:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:48:38.488626"
    },
    {
      "arxiv_id": "2403.19459v1",
      "title": "NeuroLGP-SM: A Surrogate-assisted Neuroevolution Approach using Linear Genetic Programming",
      "title_zh": "翻译失败",
      "authors": [
        "Fergal Stapleton",
        "Brendan Cody-Kenny",
        "Edgar Galván"
      ],
      "abstract": "Evolutionary algorithms are increasingly recognised as a viable computational\napproach for the automated optimisation of deep neural networks (DNNs) within\nartificial intelligence. This method extends to the training of DNNs, an\napproach known as neuroevolution. However, neuroevolution is an inherently\nresource-intensive process, with certain studies reporting the consumption of\nthousands of GPU days for refining and training a single DNN network. To\naddress the computational challenges associated with neuroevolution while still\nattaining good DNN accuracy, surrogate models emerge as a pragmatic solution.\nDespite their potential, the integration of surrogate models into\nneuroevolution is still in its early stages, hindered by factors such as the\neffective use of high-dimensional data and the representation employed in\nneuroevolution. In this context, we address these challenges by employing a\nsuitable representation based on Linear Genetic Programming, denoted as\nNeuroLGP, and leveraging Kriging Partial Least Squares. The amalgamation of\nthese two techniques culminates in our proposed methodology known as the\nNeuroLGP-Surrogate Model (NeuroLGP-SM). For comparison purposes, we also code\nand use a baseline approach incorporating a repair mechanism, a common practice\nin neuroevolution. Notably, the baseline approach surpasses the renowned VGG-16\nmodel in accuracy. Given the computational intensity inherent in DNN\noperations, a singular run is typically the norm. To evaluate the efficacy of\nour proposed approach, we conducted 96 independent runs. Significantly, our\nmethodologies consistently outperform the baseline, with the SM model\ndemonstrating superior accuracy or comparable results to the NeuroLGP approach.\nNoteworthy is the additional advantage that the SM approach exhibits a 25%\nreduction in computational requirements, further emphasising its efficiency for\nneuroevolution.",
      "tldr_zh": "该研究提出 NeuroLGP-SM，一种基于 Linear Genetic Programming 的代理辅助 neuroevolution 方法，用于优化深度神经网络（DNNs）的训练，旨在解决 neuroevolution 的高计算资源消耗问题。该方法结合 NeuroLGP 表示和 Kriging Partial Least Squares 代理模型，显著提升了高维数据的处理效率。在 96 次独立实验中，NeuroLGP-SM 比基线方法表现出色，准确性超过 VGG-16 模型，并实现了 25% 的计算资源减少，展示了其在高效神经网络优化方面的潜力。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted in \"International Conference on Optimization and Learning\n  (OLA), Dubrovnik, Croatia, 2024\", 13 pages, 4 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2403.19459v1",
      "published_date": "2024-03-28 14:31:01 UTC",
      "updated_date": "2024-03-28 14:31:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:48:50.488205"
    },
    {
      "arxiv_id": "2403.19432v2",
      "title": "Uncovering Misattributed Suicide Causes through Annotation Inconsistency Detection in Death Investigation Notes",
      "title_zh": "通过在死亡调查笔记中检测标注不一致来揭示被错误归因的自杀原因",
      "authors": [
        "Song Wang",
        "Yiliang Zhou",
        "Ziqiang Han",
        "Cui Tao",
        "Yunyu Xiao",
        "Ying Ding",
        "Joydeep Ghosh",
        "Yifan Peng"
      ],
      "abstract": "Data accuracy is essential for scientific research and policy development.\nThe National Violent Death Reporting System (NVDRS) data is widely used for\ndiscovering the patterns and causes of death. Recent studies suggested the\nannotation inconsistencies within the NVDRS and the potential impact on\nerroneous suicide-cause attributions. We present an empirical Natural Language\nProcessing (NLP) approach to detect annotation inconsistencies and adopt a\ncross-validation-like paradigm to identify problematic instances. We analyzed\n267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our\nresults showed that incorporating the target state's data into training the\nsuicide-crisis classifier brought an increase of 5.4% to the F-1 score on the\ntarget state's test set and a decrease of 1.1% on other states' test set. To\nconclude, we demonstrated the annotation inconsistencies in NVDRS's death\ninvestigation notes, identified problematic instances, evaluated the\neffectiveness of correcting problematic instances, and eventually proposed an\nNLP improvement solution.",
      "tldr_zh": "该研究针对美国国家暴力死亡报告系统(NVDRS)中的标注不一致问题，提出了一种基于自然语言处理(NLP)的实证方法，来检测死亡调查笔记中的标注错误并识别问题实例。研究分析了2003年至2020年间267,804起自杀事件的数据，发现将目标州数据纳入自杀危机分类器的训练，能使目标州测试集的F-1分数提高5.4%，但其他州测试集下降1.1%。最终，该方法证明了NVDRS标注不一致的影响，评估了纠正问题实例的有效性，并提出NLP改进方案以提升数据准确性和政策决策可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.19432v2",
      "published_date": "2024-03-28 14:03:12 UTC",
      "updated_date": "2024-03-29 17:21:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:49:01.936853"
    },
    {
      "arxiv_id": "2403.19424v1",
      "title": "The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement",
      "title_zh": "翻译失败",
      "authors": [
        "Jonathan Kamp",
        "Lisa Beinborn",
        "Antske Fokkens"
      ],
      "abstract": "Post-hoc explanation methods are an important tool for increasing model\ntransparency for users. Unfortunately, the currently used methods for\nattributing token importance often yield diverging patterns. In this work, we\nstudy potential sources of disagreement across methods from a linguistic\nperspective. We find that different methods systematically select different\nclasses of words and that methods that agree most with other methods and with\nhumans display similar linguistic preferences. Token-level differences between\nmethods are smoothed out if we compare them on the syntactic span level. We\nalso find higher agreement across methods by estimating the most important\nspans dynamically instead of relying on a fixed subset of size $k$. We\nsystematically investigate the interaction between $k$ and spans and propose an\nimproved configuration for selecting important tokens.",
      "tldr_zh": "这篇论文探讨了后验解释方法（post-hoc explanation methods）在提升模型透明度中的作用，焦点在于不同方法在分配标记重要性（token importance）时存在的系统性分歧。研究从语言学视角分析这些分歧，发现不同方法偏好选择不同的词类，而与人类和其它方法一致的解释显示相似的语言偏好。将方法在句法跨度（syntactic span）级别上比较时，标记级别的差异显著减少。作者还发现动态估计重要跨度比固定大小的 k 子集更能提高方法间的一致性，并提出了一种改进的配置来优化重要标记的选择。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Long paper accepted to LREC-Coling 2024 main conference. Please cite\n  the conference proceedings version when available",
      "pdf_url": "http://arxiv.org/pdf/2403.19424v1",
      "published_date": "2024-03-28 13:56:23 UTC",
      "updated_date": "2024-03-28 13:56:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:49:15.579107"
    },
    {
      "arxiv_id": "2403.19421v1",
      "title": "Scaling up ridge regression for brain encoding in a massive individual fMRI dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Sana Ahmadi",
        "Pierre Bellec",
        "Tristan Glatard"
      ],
      "abstract": "Brain encoding with neuroimaging data is an established analysis aimed at\npredicting human brain activity directly from complex stimuli features such as\nmovie frames. Typically, these features are the latent space representation\nfrom an artificial neural network, and the stimuli are image, audio, or text\ninputs. Ridge regression is a popular prediction model for brain encoding due\nto its good out-of-sample generalization performance. However, training a ridge\nregression model can be highly time-consuming when dealing with large-scale\ndeep functional magnetic resonance imaging (fMRI) datasets that include many\nspace-time samples of brain activity. This paper evaluates different\nparallelization techniques to reduce the training time of brain encoding with\nridge regression on the CNeuroMod Friends dataset, one of the largest deep fMRI\nresource currently available. With multi-threading, our results show that the\nIntel Math Kernel Library (MKL) significantly outperforms the OpenBLAS library,\nbeing 1.9 times faster using 32 threads on a single machine. We then evaluated\nthe Dask multi-CPU implementation of ridge regression readily available in\nscikit-learn (MultiOutput), and we proposed a new \"batch\" version of Dask\nparallelization, motivated by a time complexity analysis. In line with our\ntheoretical analysis, MultiOutput parallelization was found to be impractical,\ni.e., slower than multi-threading on a single machine. In contrast, the\nBatch-MultiOutput regression scaled well across compute nodes and threads,\nproviding speed-ups of up to 33 times with 8 compute nodes and 32 threads\ncompared to a single-threaded scikit-learn execution. Batch parallelization\nusing Dask thus emerges as a scalable approach for brain encoding with ridge\nregression on high-performance computing systems using scikit-learn and large\nfMRI datasets.",
      "tldr_zh": "该研究针对大规模 fMRI 数据集，探讨了使用 ridge regression 模型进行脑编码的训练加速问题，旨在从复杂刺激特征（如电影帧）预测大脑活动。研究者评估了多种并行化技术，包括多线程（Intel MKL 比 OpenBLAS 快 1.9 倍使用 32 线程）和 Dask 框架的 MultiOutput 实现，但发现 MultiOutput 在实际应用中比单机多线程慢。作者提出了一种新的 Batch-MultiOutput 版本，通过在高性能计算系统中跨计算节点和线程并行化，实现高达 33 倍的速度提升，为处理大型 fMRI 数据集的脑编码提供了可扩展的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19421v1",
      "published_date": "2024-03-28 13:52:12 UTC",
      "updated_date": "2024-03-28 13:52:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:49:29.166991"
    },
    {
      "arxiv_id": "2403.19419v1",
      "title": "Fairness in Ranking: Robustness through Randomization without the Protected Attribute",
      "title_zh": "翻译失败",
      "authors": [
        "Andrii Kliachkin",
        "Eleni Psaroudaki",
        "Jakub Marecek",
        "Dimitris Fotakis"
      ],
      "abstract": "There has been great interest in fairness in machine learning, especially in\nrelation to classification problems. In ranking-related problems, such as in\nonline advertising, recommender systems, and HR automation, much work on\nfairness remains to be done. Two complications arise: first, the protected\nattribute may not be available in many applications. Second, there are multiple\nmeasures of fairness of rankings, and optimization-based methods utilizing a\nsingle measure of fairness of rankings may produce rankings that are unfair\nwith respect to other measures. In this work, we propose a randomized method\nfor post-processing rankings, which do not require the availability of the\nprotected attribute. In an extensive numerical study, we show the robustness of\nour methods with respect to P-Fairness and effectiveness with respect to\nNormalized Discounted Cumulative Gain (NDCG) from the baseline ranking,\nimproving on previously proposed methods.",
      "tldr_zh": "本论文探讨了机器学习中排名公平性的问题，特别是针对在线广告、推荐系统和 HR 自动化等应用，强调了保护属性不可用以及多种公平性衡量标准的挑战。研究提出了一种随机化后处理方法，用于优化排名，而无需访问保护属性，通过这种方法提升排名的鲁棒性。实验结果显示，该方法在 P-Fairness 方面比现有方法更具鲁棒性，并在 Normalized Discounted Cumulative Gain (NDCG) 上从基线排名中实现了显著改善。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19419v1",
      "published_date": "2024-03-28 13:50:24 UTC",
      "updated_date": "2024-03-28 13:50:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:49:41.273832"
    },
    {
      "arxiv_id": "2403.19405v1",
      "title": "Tabular Learning: Encoding for Entity and Context Embeddings",
      "title_zh": "表格学习：实体和上下文嵌入的编码",
      "authors": [
        "Fredy Reusser"
      ],
      "abstract": "Examining the effect of different encoding techniques on entity and context\nembeddings, the goal of this work is to challenge commonly used Ordinal\nencoding for tabular learning. Applying different preprocessing methods and\nnetwork architectures over several datasets resulted in a benchmark on how the\nencoders influence the learning outcome of the networks. By keeping the test,\nvalidation and training data consistent, results have shown that ordinal\nencoding is not the most suited encoder for categorical data in terms of\npreprocessing the data and thereafter, classifying the target variable\ncorrectly. A better outcome was achieved, encoding the features based on string\nsimilarities by computing a similarity matrix as input for the network. This is\nthe case for both, entity and context embeddings, where the transformer\narchitecture showed improved performance for Ordinal and Similarity encoding\nwith regard to multi-label classification tasks.",
      "tldr_zh": "本研究挑战了表格学习中常用的 Ordinal encoding，探讨了不同编码技术对实体和上下文嵌入的影响。研究者通过应用各种预处理方法和网络架构（如 Transformer）在多个数据集上进行基准测试，结果显示 Ordinal encoding 在处理分类数据时并非最佳选择。基于字符串相似度的编码方法（通过计算相似矩阵）在实体和上下文嵌入上取得了更好的学习效果，尤其在多标签分类任务中提升了性能。该工作为改进表格学习中的数据预处理提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19405v1",
      "published_date": "2024-03-28 13:29:29 UTC",
      "updated_date": "2024-03-28 13:29:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:49:52.772920"
    },
    {
      "arxiv_id": "2406.16872v1",
      "title": "Multi-channel Time Series Decomposition Network For Generalizable Sensor-Based Activity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Jianguo Pan",
        "Zhengxin Hu",
        "Lingdun Zhang",
        "Xia Cai"
      ],
      "abstract": "Sensor-based human activity recognition is important in daily scenarios such\nas smart healthcare and homes due to its non-intrusive privacy and low cost\nadvantages, but the problem of out-of-domain generalization caused by\ndifferences in focusing individuals and operating environments can lead to\nsignificant accuracy degradation on cross-person behavior recognition due to\nthe inconsistent distributions of training and test data. To address the above\nproblems, this paper proposes a new method, Multi-channel Time Series\nDecomposition Network (MTSDNet). Firstly, MTSDNet decomposes the original\nsignal into a combination of multiple polynomials and trigonometric functions\nby the trainable parameterized temporal decomposition to learn the low-rank\nrepresentation of the original signal for improving the extraterritorial\ngeneralization ability of the model. Then, the different components obtained by\nthe decomposition are classified layer by layer and the layer attention is used\nto aggregate components to obtain the final classification result. Extensive\nevaluation on DSADS, OPPORTUNITY, PAMAP2, UCIHAR and UniMib public datasets\nshows the advantages in predicting accuracy and stability of our method\ncompared with other competing strategies, including the state-of-the-art ones.\nAnd the visualization is conducted to reveal MTSDNet's interpretability and\nlayer-by-layer characteristics.",
      "tldr_zh": "本论文提出了一种名为Multi-channel Time Series Decomposition Network (MTSDNet)的创新方法，用于提升基于传感器的活动识别在跨个人和环境下的泛化能力，以解决训练和测试数据分布不一致导致的准确率下降问题。MTSDNet首先通过可训练的参数化时间分解，将原始信号分解为多项式和三角函数的组合，以学习信号的低秩表示；随后，对分解组件逐层分类，并使用层注意力机制聚合结果以获得最终输出。该方法在DSADS、OPPORTUNITY、PAMAP2、UCIHAR和UniMib等公共数据集上的广泛评估显示，其预测准确性和稳定性优于现有竞争策略，并通过可视化证明了模型的可解释性和逐层特性。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16872v1",
      "published_date": "2024-03-28 12:54:06 UTC",
      "updated_date": "2024-03-28 12:54:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:50:05.294896"
    },
    {
      "arxiv_id": "2403.19386v2",
      "title": "PointCloud-Text Matching: Benchmark Datasets and a Baseline",
      "title_zh": "翻译失败",
      "authors": [
        "Yanglin Feng",
        "Yang Qin",
        "Dezhong Peng",
        "Hongyuan Zhu",
        "Xi Peng",
        "Peng Hu"
      ],
      "abstract": "In this paper, we present and study a new instance-level retrieval task:\nPointCloud-Text Matching~(PTM), which aims to find the exact cross-modal\ninstance that matches a given point-cloud query or text query. PTM could be\napplied to various scenarios, such as indoor/urban-canyon localization and\nscene retrieval. However, there exists no suitable and targeted dataset for PTM\nin practice. Therefore, we construct three new PTM benchmark datasets, namely\n3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with\nnoisy correspondence due to the sparsity, noise, or disorder of point clouds\nand the ambiguity, vagueness, or incompleteness of texts, which make existing\ncross-modal matching methods ineffective for PTM. To tackle these challenges,\nwe propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa).\nRoMa consists of two modules: a Dual Attention Perception module (DAP) and a\nRobust Negative Contrastive Learning module (RNCL). Specifically, DAP leverages\ntoken-level and feature-level attention to adaptively focus on useful local and\nglobal features, and aggregate them into common representations, thereby\nreducing the adverse impact of noise and ambiguity. To handle noisy\ncorrespondence, RNCL divides negative pairs, which are much less error-prone\nthan positive pairs, into clean and noisy subsets, and assigns them forward and\nreverse optimization directions respectively, thus enhancing robustness against\nnoisy correspondence. We conduct extensive experiments on our benchmarks and\ndemonstrate the superiority of our RoMa.",
      "tldr_zh": "本研究引入了一个新的实例级检索任务：PointCloud-Text Matching (PTM)，旨在通过点云或文本查询找到精确的跨模态匹配实例，并适用于室内/城市峡谷定位和场景检索场景。为此，作者构建了三个基准数据集：3D2T-SR、3D2T-NR 和 3D2T-QA，这些数据集面临点云的稀疏性、噪声或无序性以及文本的模糊性或不完整性等挑战，导致现有跨模态匹配方法效果不佳。针对这些问题，提出了一种基线方法 Robust PointCloud-Text Matching method (RoMa)，包括 Dual Attention Perception module (DAP) 用于自适应关注局部和全局特征以减少噪声影响，以及 Robust Negative Contrastive Learning module (RNCL) 通过区分干净和噪声负对进行优化以提升鲁棒性。在实验中，RoMa 在这些基准上表现出色，证明了其优越性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Upon further consideration, we have concluded that the current\n  version requires significant revision and may not yet be ready for\n  publication. We plan to conduct additional experiments and make the necessary\n  improvements to ensure the paper meets the standards for future submission",
      "pdf_url": "http://arxiv.org/pdf/2403.19386v2",
      "published_date": "2024-03-28 12:51:15 UTC",
      "updated_date": "2024-09-05 03:18:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:50:17.736747"
    },
    {
      "arxiv_id": "2403.19376v3",
      "title": "NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data",
      "title_zh": "翻译失败",
      "authors": [
        "Matteo Caligiuri",
        "Adriano Simonetto",
        "Pietro Zanuttigh"
      ],
      "abstract": "The acquisition of objects outside the Line-of-Sight of cameras is a very\nintriguing but also extremely challenging research topic. Recent works showed\nthe feasibility of this idea exploiting transient imaging data produced by\ncustom direct Time of Flight sensors. In this paper, for the first time, we\ntackle this problem using only data from an off-the-shelf indirect Time of\nFlight sensor without any further hardware requirement. We introduced a Deep\nLearning model able to re-frame the surfaces where light bounces happen as a\nvirtual mirror. This modeling makes the task easier to handle and also\nfacilitates the construction of annotated training data. From the obtained data\nit is possible to retrieve the depth information of the hidden scene. We also\nprovide a first-in-its-kind synthetic dataset for the task and demonstrate the\nfeasibility of the proposed idea over it.",
      "tldr_zh": "这篇论文首次使用现成的间接 Time of Flight 传感器数据来实现 Non-Line-of-Sight Imaging，旨在获取相机视线外的物体图像，而无需额外硬件。作者引入了一个 Deep Learning 模型，将光反弹发生的表面建模为虚拟镜子（virtual mirror），从而简化任务处理并便于构建标注训练数据。该方法能够从数据中提取隐藏场景的深度信息，并在他们提供的首创合成数据集上验证了其可行性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024 - MELEX workshop, 17 pages, 6 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2403.19376v3",
      "published_date": "2024-03-28 12:38:21 UTC",
      "updated_date": "2024-09-20 12:35:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:50:28.534974"
    },
    {
      "arxiv_id": "2404.01320v1",
      "title": "Graph-Based Optimisation of Network Expansion in a Dockless Bike Sharing System",
      "title_zh": "翻译失败",
      "authors": [
        "Mark Roantree",
        "Niamh Murphi",
        "Dinh Viet Cuong",
        "Vuong Minh Ngo"
      ],
      "abstract": "Bike-sharing systems (BSSs) are deployed in over a thousand cities worldwide\nand play an important role in many urban transportation systems. BSSs alleviate\ncongestion, reduce pollution and promote physical exercise. It is essential to\nexplore the spatiotemporal patterns of bike-sharing demand, as well as the\nfactors that influence these patterns, in order to optimise system operational\nefficiency. In this study, an optimised geo-temporal graph is constructed using\ntrip data from Moby Bikes, a dockless BSS operator. The process of optimising\nthe graph unveiled prime locations for erecting new stations during future\nexpansions of the BSS. The Louvain algorithm, a community detection technique,\nis employed to uncover usage patterns at different levels of temporal\ngranularity. The community detection results reveal largely self-contained\nsub-networks that exhibit similar usage patterns at their respective levels of\ntemporal granularity. Overall, this study reinforces that BSSs are\nintrinsically spatiotemporal systems, with community presence driven by\nspatiotemporal dynamics. These findings may aid operators in improving\nredistribution efficiency.",
      "tldr_zh": "这篇论文探讨了无桩自行车共享系统（BSSs）的网络扩展优化，旨在通过分析时空需求模式来提升系统效率。研究者使用 Moby Bikes 的出行数据构建了一个优化的时空图，并应用 Louvain 算法进行社区检测，以识别不同时间粒度下的使用模式和理想的新站点位置。结果显示，这些社区检测揭示了自包含的子网络，受时空动态驱动，有助于运营商改善再分配效率并优化整体系统性能。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted to publish in The 2024 IEEE 40th International Conference on\n  Data Engineering Workshops (ICDEW&DASC-2024), pp. 1-8",
      "pdf_url": "http://arxiv.org/pdf/2404.01320v1",
      "published_date": "2024-03-28 12:29:25 UTC",
      "updated_date": "2024-03-28 12:29:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:50:39.902781"
    },
    {
      "arxiv_id": "2403.19347v1",
      "title": "Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors",
      "title_zh": "打破长度障碍：LLM 增强的长文本用户行为中的 CTR ",
      "authors": [
        "Binzong Geng",
        "Zhaoxin Huan",
        "Xiaolu Zhang",
        "Yong He",
        "Liang Zhang",
        "Fajie Yuan",
        "Jun Zhou",
        "Linjian Mo"
      ],
      "abstract": "With the rise of large language models (LLMs), recent works have leveraged\nLLMs to improve the performance of click-through rate (CTR) prediction.\nHowever, we argue that a critical obstacle remains in deploying LLMs for\npractical use: the efficiency of LLMs when processing long textual user\nbehaviors. As user sequences grow longer, the current efficiency of LLMs is\ninadequate for training on billions of users and items. To break through the\nefficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical\nEncoding (BAHE) to enhance the efficiency of LLM-based CTR modeling.\nSpecifically, BAHE proposes a novel hierarchical architecture that decouples\nthe encoding of user behaviors from inter-behavior interactions. Firstly, to\nprevent computational redundancy from repeated encoding of identical user\nbehaviors, BAHE employs the LLM's pre-trained shallow layers to extract\nembeddings of the most granular, atomic user behaviors from extensive user\nsequences and stores them in the offline database. Subsequently, the deeper,\ntrainable layers of the LLM facilitate intricate inter-behavior interactions,\nthereby generating comprehensive user embeddings. This separation allows the\nlearning of high-level user representations to be independent of low-level\nbehavior encoding, significantly reducing computational complexity. Finally,\nthese refined user embeddings, in conjunction with correspondingly processed\nitem embeddings, are incorporated into the CTR model to compute the CTR scores.\nExtensive experimental results show that BAHE reduces training time and memory\nby five times for CTR models using LLMs, especially with longer user sequences.\nBAHE has been deployed in a real-world system, allowing for daily updates of 50\nmillion CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR\nprediction.",
      "tldr_zh": "这篇论文针对大型语言模型（LLMs）在处理长文本用户行为时的效率瓶颈，提出了Behavior Aggregated Hierarchical Encoding (BAHE)方法，以提升点击率（CTR）预测的性能。BAHE采用分层架构，将用户行为的低级编码与行为间交互分离：首先，使用LLMs的浅层预训练提取原子行为嵌入并存储在离线数据库中，避免重复计算；随后，深层可训练层处理行为交互，生成高效的用户嵌入，并与物品嵌入结合计算CTR分数。实验结果显示，BAHE将训练时间和内存消耗减少五倍，尤其适用于长用户序列，并在实际系统中部署，每天处理5000万CTR数据，使LLMs在工业CTR预测中变得实用。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by the 47th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval (SIGIR), 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.19347v1",
      "published_date": "2024-03-28 12:05:15 UTC",
      "updated_date": "2024-03-28 12:05:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:50:53.397022"
    },
    {
      "arxiv_id": "2403.19345v1",
      "title": "Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning",
      "title_zh": "基于机器学习的电子商务产品智能分类和个性化推荐",
      "authors": [
        "Kangming Xu",
        "Huiming Zhou",
        "Haotian Zheng",
        "Mingwei Zhu",
        "Qi Xin"
      ],
      "abstract": "With the rapid evolution of the Internet and the exponential proliferation of\ninformation, users encounter information overload and the conundrum of choice.\nPersonalized recommendation systems play a pivotal role in alleviating this\nburden by aiding users in filtering and selecting information tailored to their\npreferences and requirements. Such systems not only enhance user experience and\nsatisfaction but also furnish opportunities for businesses and platforms to\naugment user engagement, sales, and advertising efficacy.This paper undertakes\na comparative analysis between the operational mechanisms of traditional\ne-commerce commodity classification systems and personalized recommendation\nsystems. It delineates the significance and application of personalized\nrecommendation systems across e-commerce, content information, and media\ndomains. Furthermore, it delves into the challenges confronting personalized\nrecommendation systems in e-commerce, including data privacy, algorithmic bias,\nscalability, and the cold start problem. Strategies to address these challenges\nare elucidated.Subsequently, the paper outlines a personalized recommendation\nsystem leveraging the BERT model and nearest neighbor algorithm, specifically\ntailored to address the exigencies of the eBay e-commerce platform. The\nefficacy of this recommendation system is substantiated through manual\nevaluation, and a practical application operational guide and structured output\nrecommendation results are furnished to ensure the system's operability and\nscalability.",
      "tldr_zh": "这篇论文探讨了机器学习在电商产品分类和个性化推荐中的应用，强调个性化推荐系统能缓解用户的信息过载问题，提高用户体验和商业效益。论文比较了传统电商分类系统与个性化推荐系统的机制，并分析了后者的挑战，如数据隐私、算法偏差、scalability 和 cold start problem，同时提出了相应的解决策略。最终，该研究基于 BERT 模型和 nearest neighbor algorithm 构建了一个针对 eBay 平台的个性化推荐系统，通过手动评估验证了其有效性，并提供了操作指南以确保系统的可操作性和可扩展性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19345v1",
      "published_date": "2024-03-28 12:02:45 UTC",
      "updated_date": "2024-03-28 12:02:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:51:04.785806"
    },
    {
      "arxiv_id": "2403.19340v2",
      "title": "Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hyunbyung Park",
        "Sukyung Lee",
        "Gyoungjin Gim",
        "Yungi Kim",
        "Dahyun Kim",
        "Chanjun Park"
      ],
      "abstract": "To address the challenges associated with data processing at scale, we\npropose Dataverse, a unified open-source Extract-Transform-Load (ETL) pipeline\nfor large language models (LLMs) with a user-friendly design at its core. Easy\naddition of custom processors with block-based interface in Dataverse allows\nusers to readily and efficiently use Dataverse to build their own ETL pipeline.\nWe hope that Dataverse will serve as a vital tool for LLM development and open\nsource the entire library to welcome community contribution. Additionally, we\nprovide a concise, two-minute video demonstration of our system, illustrating\nits capabilities and implementation.",
      "tldr_zh": "论文提出了 Dataverse，这是一个开源的 ETL (Extract, Transform, Load) 管道，旨在解决大规模数据处理在 Large Language Models (LLMs) 中的挑战。Dataverse 以用户友好的块状接口为核心，允许用户轻松添加自定义处理器并高效构建自己的 ETL 管道。该系统开源以促进社区贡献，并附带一个简短视频演示，展示其功能和实现方式。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025 Demo",
      "pdf_url": "http://arxiv.org/pdf/2403.19340v2",
      "published_date": "2024-03-28 11:57:08 UTC",
      "updated_date": "2025-03-04 03:06:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:51:15.526195"
    },
    {
      "arxiv_id": "2403.19336v1",
      "title": "IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation",
      "title_zh": "IVLMap：实例感知视觉语言 grounding 用于消费机器人导航",
      "authors": [
        "Jiacui Huang",
        "Hongtao Zhang",
        "Mingbo Zhao",
        "Zhou Wu"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) is a challenging task that requires a\nrobot to navigate in photo-realistic environments with human natural language\npromptings. Recent studies aim to handle this task by constructing the semantic\nspatial map representation of the environment, and then leveraging the strong\nability of reasoning in large language models for generalizing code for guiding\nthe robot navigation. However, these methods face limitations in instance-level\nand attribute-level navigation tasks as they cannot distinguish different\ninstances of the same object. To address this challenge, we propose a new\nmethod, namely, Instance-aware Visual Language Map (IVLMap), to empower the\nrobot with instance-level and attribute-level semantic mapping, where it is\nautonomously constructed by fusing the RGBD video data collected from the robot\nagent with special-designed natural language map indexing in the bird's-in-eye\nview. Such indexing is instance-level and attribute-level. In particular, when\nintegrated with a large language model, IVLMap demonstrates the capability to\ni) transform natural language into navigation targets with instance and\nattribute information, enabling precise localization, and ii) accomplish\nzero-shot end-to-end navigation tasks based on natural language commands.\nExtensive navigation experiments are conducted. Simulation results illustrate\nthat our method can achieve an average improvement of 14.4\\% in navigation\naccuracy. Code and demo are released at https://ivlmap.github.io/.",
      "tldr_zh": "该论文针对 Vision-and-Language Navigation (VLN) 任务，提出了一种新的方法 IVLMap，以解决现有方法在区分同一对象不同实例方面的局限性。IVLMap 通过融合机器人收集的 RGBD 视频数据和鸟瞰视角的实例级及属性级自然语言索引，构建出实例感知的语义地图。当与大型语言模型集成后，IVLMap 能够将自然语言命令转化为精确的导航目标，实现零样本端到端导航。实验结果显示，在模拟环境中，该方法将导航准确率平均提高了 14.4%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19336v1",
      "published_date": "2024-03-28 11:52:42 UTC",
      "updated_date": "2024-03-28 11:52:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:51:30.110911"
    },
    {
      "arxiv_id": "2403.19316v1",
      "title": "Hypergraph-based Multi-View Action Recognition using Event Cameras",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Gao",
        "Jiaxuan Lu",
        "Siqi Li",
        "Yipeng Li",
        "Shaoyi Du"
      ],
      "abstract": "Action recognition from video data forms a cornerstone with wide-ranging\napplications. Single-view action recognition faces limitations due to its\nreliance on a single viewpoint. In contrast, multi-view approaches capture\ncomplementary information from various viewpoints for improved accuracy.\nRecently, event cameras have emerged as innovative bio-inspired sensors,\nleading to advancements in event-based action recognition. However, existing\nworks predominantly focus on single-view scenarios, leaving a gap in multi-view\nevent data exploitation, particularly in challenges like information deficit\nand semantic misalignment. To bridge this gap, we introduce HyperMV, a\nmulti-view event-based action recognition framework. HyperMV converts discrete\nevent data into frame-like representations and extracts view-related features\nusing a shared convolutional network. By treating segments as vertices and\nconstructing hyperedges using rule-based and KNN-based strategies, a multi-view\nhypergraph neural network that captures relationships across viewpoint and\ntemporal features is established. The vertex attention hypergraph propagation\nis also introduced for enhanced feature fusion. To prompt research in this\narea, we present the largest multi-view event-based action dataset\n$\\text{THU}^{\\text{MV-EACT}}\\text{-50}$, comprising 50 actions from 6\nviewpoints, which surpasses existing datasets by over tenfold. Experimental\nresults show that HyperMV significantly outperforms baselines in both\ncross-subject and cross-view scenarios, and also exceeds the state-of-the-arts\nin frame-based multi-view action recognition.",
      "tldr_zh": "该论文提出 HyperMV 框架，用于基于 event cameras 的多视图动作识别，旨在解决现有方法在信息缺失和语义不对齐方面的局限性。HyperMV 通过将事件数据转换为帧状表示、提取视图相关特征，并构建 hypergraph 神经网络（使用规则和 KNN 策略创建超边，并引入 vertex attention hypergraph propagation 进行特征融合）来捕获跨视角和时序关系。为了推动研究，他们发布了最大的多视图事件动作数据集 THU-MV-EACT-50，包含 50 个动作和 6 个视角。实验结果显示，HyperMV 在跨主体和跨视角场景中显著优于基线模型，并在基于帧的多视图动作识别中超越现有最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI 2024)",
      "pdf_url": "http://arxiv.org/pdf/2403.19316v1",
      "published_date": "2024-03-28 11:17:00 UTC",
      "updated_date": "2024-03-28 11:17:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:51:41.324105"
    },
    {
      "arxiv_id": "2403.19305v2",
      "title": "MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Li",
        "Shenyu Zhang",
        "Rui Wu",
        "Xiutian Huang",
        "Yongrui Chen",
        "Wenhao Xu",
        "Guilin Qi",
        "Dehai Min"
      ],
      "abstract": "Recent advancements in generative Large Language Models(LLMs) have been\nremarkable, however, the quality of the text generated by these models often\nreveals persistent issues. Evaluating the quality of text generated by these\nmodels, especially in open-ended text, has consistently presented a significant\nchallenge. Addressing this, recent work has explored the possibility of using\nLLMs as evaluators. While using a single LLM as an evaluation agent shows\npotential, it is filled with significant uncertainty and instability. To\naddress these issues, we propose the MATEval: A \"Multi-Agent Text Evaluation\nframework\" where all agents are played by LLMs like GPT-4. The MATEval\nframework emulates human collaborative discussion methods, integrating multiple\nagents' interactions to evaluate open-ended text. Our framework incorporates\nself-reflection and Chain-of-Thought (CoT) strategies, along with feedback\nmechanisms, enhancing the depth and breadth of the evaluation process and\nguiding discussions towards consensus, while the framework generates\ncomprehensive evaluation reports, including error localization, error types and\nscoring. Experimental results show that our framework outperforms existing\nopen-ended text evaluation methods and achieves the highest correlation with\nhuman evaluation, which confirms the effectiveness and advancement of our\nframework in addressing the uncertainties and instabilities in evaluating\nLLMs-generated text. Furthermore, our framework significantly improves the\nefficiency of text evaluation and model iteration in industrial scenarios.",
      "tldr_zh": "本文提出 MATEval，一种多智能体讨论框架，旨在解决生成式 Large Language Models (LLMs) 生成的开放式文本评估中的不确定性和不稳定性问题。框架模拟人类协作讨论，使用 LLMs（如 GPT-4）作为代理，整合自反省、Chain-of-Thought (CoT) 策略和反馈机制，以生成全面评估报告，包括错误定位、错误类型和评分。实验结果显示，MATEval 优于现有方法，与人类评估的相关性最高，并显著提升了文本评估和模型迭代的效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted as a long paper presentation by DASFAA\n  2024 Industrial Track",
      "pdf_url": "http://arxiv.org/pdf/2403.19305v2",
      "published_date": "2024-03-28 10:41:47 UTC",
      "updated_date": "2024-04-15 14:21:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:51:52.853768"
    },
    {
      "arxiv_id": "2403.19289v4",
      "title": "Uplift Modeling Under Limited Supervision",
      "title_zh": "有限监督下的提升建模",
      "authors": [
        "George Panagopoulos",
        "Daniele Malitesta",
        "Fragkiskos D. Malliaros",
        "Jun Pang"
      ],
      "abstract": "Estimating causal effects in e-commerce tends to involve costly treatment\nassignments which can be impractical in large-scale settings. Leveraging\nmachine learning to predict such treatment effects without actual intervention\nis a standard practice to diminish the risk. However, existing methods for\ntreatment effect prediction tend to rely on training sets of substantial size,\nwhich are built from real experiments and are thus inherently risky to create.\nIn this work we propose a graph neural network to diminish the required\ntraining set size, relying on graphs that are common in e-commerce data.\nSpecifically, we view the problem as node regression with a restricted number\nof labeled instances, develop a two-model neural architecture akin to previous\ncausal effect estimators, and test varying message-passing layers for encoding.\nFurthermore, as an extra step, we combine the model with an acquisition\nfunction to guide the creation of the training set in settings with extremely\nlow experimental budget. The framework is flexible since each step can be used\nseparately with other models or treatment policies. The experiments on real\nlarge-scale networks indicate a clear advantage of our methodology over the\nstate of the art, which in many cases performs close to random, underlining the\nneed for models that can generalize with limited supervision to reduce\nexperimental risks.",
      "tldr_zh": "本文提出了一种在有限监督下进行Uplift Modeling的方法，旨在减少电子商务中因果效应估计所需的训练数据规模，从而降低实验风险。该方法利用Graph Neural Network（GNN）将问题视为node regression问题，开发了一个双模型神经架构并测试不同message-passing layers进行编码，同时结合acquisition function来优化训练集创建过程。实验结果显示，该框架在真实大型网络上显著优于现有技术，许多情况下后者表现接近随机，突出了其在数据有限场景下的泛化优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19289v4",
      "published_date": "2024-03-28 10:19:36 UTC",
      "updated_date": "2024-09-02 20:21:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:52:05.007663"
    },
    {
      "arxiv_id": "2403.19279v1",
      "title": "Fine-Tuning Language Models with Reward Learning on Policy",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Lang",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as an effective\napproach to aligning large language models (LLMs) to human preferences. RLHF\ncontains three steps, i.e., human preference collecting, reward learning, and\npolicy optimization, which are usually performed serially. Despite its\npopularity, however, (fixed) reward models may suffer from inaccurate\noff-distribution, since policy optimization continuously shifts LLMs' data\ndistribution. Repeatedly collecting new preference data from the latest LLMs\nmay alleviate this issue, which unfortunately makes the resulting system more\ncomplicated and difficult to optimize. In this paper, we propose reward\nlearning on policy (RLP), an unsupervised framework that refines a reward model\nusing policy samples to keep it on-distribution. Specifically, an unsupervised\nmulti-view learning method is introduced to learn robust representations of\npolicy samples. Meanwhile, a synthetic preference generation approach is\ndeveloped to simulate high-quality preference data with policy outputs.\nExtensive experiments on three benchmark datasets show that RLP consistently\noutperforms the state-of-the-art. Our code is available at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.",
      "tldr_zh": "这篇论文针对强化学习从人类反馈（RLHF）中奖励模型的分布偏移问题，提出了一种无监督框架reward learning on policy (RLP)，旨在通过策略样本动态精炼奖励模型。该框架引入无监督多视图学习（multi-view learning）来获取策略样本的鲁棒表示，并开发合成偏好生成方法来模拟高质量偏好数据，从而保持奖励模型的准确性。实验在三个基准数据集上显示，RLP  consistently outperforms state-of-the-art 方法，证明了其在优化大型语言模型（LLMs）时的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL2024 Main Track Long Paper",
      "pdf_url": "http://arxiv.org/pdf/2403.19279v1",
      "published_date": "2024-03-28 10:02:10 UTC",
      "updated_date": "2024-03-28 10:02:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:52:17.086705"
    },
    {
      "arxiv_id": "2403.19275v2",
      "title": "Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Junkai Zhou",
        "Liang Pang",
        "Ya Jing",
        "Jia Gu",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "abstract": "Constructing personalized and anthropomorphic agents holds significant\nimportance in the simulation of social networks. However, there are still two\nkey problems in existing works: the agent possesses world knowledge that does\nnot belong to its personas, and it cannot eliminate the interference of diverse\npersona information on current actions, which reduces the personalization and\nanthropomorphism of the agent. To solve the above problems, we construct the\nsocial media agent based on personalized knowledge and dynamic persona\ninformation. For personalized knowledge, we add external knowledge sources and\nmatch them with the persona information of agents, thereby giving the agent\npersonalized world knowledge. For dynamic persona information, we use current\naction information to internally retrieve the persona information of the agent,\nthereby reducing the interference of diverse persona information on the current\naction. To make the agent suitable for social media, we design five basic\nmodules for it: persona, planning, action, memory and reflection. To provide an\ninteraction and verification environment for the agent, we build a social media\nsimulation sandbox. In the experimental verification, automatic and human\nevaluations demonstrated the effectiveness of the agent we constructed.",
      "tldr_zh": "该研究针对社交媒体代理的个性化与拟人化问题，提出了一种基于知识边界和动态人物信息（persona）的改进框架，以解决代理拥有不属于其人物设定的世界知识以及多样人物信息干扰当前行为的问题。具体方法包括：为代理添加外部知识源并与persona信息匹配，实现个性化知识；使用当前行为信息内部检索persona信息，减少干扰。同时，该框架设计了五个基本模块——persona、planning、action、memory和reflection，并构建了一个社交媒体模拟沙箱作为交互环境。实验结果通过自动和人工评估证明，该代理在提升个性化与拟人化方面表现出显著有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19275v2",
      "published_date": "2024-03-28 10:01:23 UTC",
      "updated_date": "2024-04-02 10:59:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:52:28.139183"
    },
    {
      "arxiv_id": "2403.19273v1",
      "title": "A Machine Learning Approach for Crop Yield and Disease Prediction Integrating Soil Nutrition and Weather Factors",
      "title_zh": "整合土壤营养和天气因素的机器学习方法，用于作物产量和疾病预测",
      "authors": [
        "Forkan Uddin Ahmed",
        "Annesha Das",
        "Md Zubair"
      ],
      "abstract": "The development of an intelligent agricultural decision-supporting system for\ncrop selection and disease forecasting in Bangladesh is the main objective of\nthis work. The economy of the nation depends heavily on agriculture. However,\nchoosing crops with better production rates and efficiently controlling crop\ndisease are obstacles that farmers have to face. These issues are addressed in\nthis research by utilizing machine learning methods and real-world datasets.\nThe recommended approach uses a variety of datasets on the production of crops,\nsoil conditions, agro-meteorological regions, crop disease, and meteorological\nfactors. These datasets offer insightful information on disease trends, soil\nnutrition demand of crops, and agricultural production history. By\nincorporating this knowledge, the model first recommends the list of primarily\nselected crops based on the soil nutrition of a particular user location. Then\nthe predictions of meteorological variables like temperature, rainfall, and\nhumidity are made using SARIMAX models. These weather predictions are then used\nto forecast the possibilities of diseases for the primary crops list by\nutilizing the support vector classifier. Finally, the developed model makes use\nof the decision tree regression model to forecast crop yield and provides a\nfinal crop list along with associated possible disease forecast. Utilizing the\noutcome of the model, farmers may choose the best productive crops as well as\nprevent crop diseases and reduce output losses by taking preventive actions.\nConsequently, planning and decision-making processes are supported and farmers\ncan predict possible crop yields. Overall, by offering a detailed decision\nsupport system for crop selection and disease prediction, this work can play a\nvital role in advancing agricultural practices in Bangladesh.",
      "tldr_zh": "本研究提出了一种整合土壤营养和气象因素的机器学习方法，用于孟加拉国作物的选择、产量预测和疾病预测，旨在帮助农民应对高产作物选择和疾病控制的挑战。方法首先基于用户位置的土壤营养推荐初步作物列表，然后利用 SARIMAX 模型预测气象变量（如温度、降雨和湿度），并结合支持向量分类器（Support Vector Classifier）预测这些作物的疾病可能性，最后采用决策树回归模型（Decision Tree Regression）预测作物产量并提供最终作物建议。该模型通过整合真实数据集，支持农民进行决策性规划，减少产量损失，并在农业实践中发挥关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper was presented to the IEEE conference, \"2024 International\n  Conference on Advances in Computing, Communication, Electrical, and Smart\n  Systems (iCACCESS), 8-9 March, Dhaka, Bangladesh\"",
      "pdf_url": "http://arxiv.org/pdf/2403.19273v1",
      "published_date": "2024-03-28 09:57:50 UTC",
      "updated_date": "2024-03-28 09:57:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:52:41.469858"
    },
    {
      "arxiv_id": "2403.19271v1",
      "title": "DeepSample: DNN sampling-based testing for operational accuracy assessment",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio Guerriero",
        "Roberto Pietrantuono",
        "Stefano Russo"
      ],
      "abstract": "Deep Neural Networks (DNN) are core components for classification and\nregression tasks of many software systems. Companies incur in high costs for\ntesting DNN with datasets representative of the inputs expected in operation,\nas these need to be manually labelled. The challenge is to select a\nrepresentative set of test inputs as small as possible to reduce the labelling\ncost, while sufficing to yield unbiased high-confidence estimates of the\nexpected DNN accuracy. At the same time, testers are interested in exposing as\nmany DNN mispredictions as possible to improve the DNN, ending up in the need\nfor techniques pursuing a threefold aim: small dataset size, trustworthy\nestimates, mispredictions exposure. This study presents DeepSample, a family of\nDNN testing techniques for cost-effective accuracy assessment based on\nprobabilistic sampling. We investigate whether, to what extent, and under which\nconditions probabilistic sampling can help to tackle the outlined challenge. We\nimplement five new sampling-based testing techniques, and perform a\ncomprehensive comparison of such techniques and of three further\nstate-of-the-art techniques for both DNN classification and regression tasks.\nResults serve as guidance for best use of sampling-based testing for faithful\nand high-confidence estimates of DNN accuracy in operation at low cost.",
      "tldr_zh": "这篇论文针对深度神经网络（DNN）测试中的高标注成本问题，提出了 DeepSample，一系列基于概率采样的测试技术，以最小化测试数据集规模，同时确保准确性估计的可靠性和高置信度。DeepSample 实现了五个新采样方法，并与三种现有技术在 DNN 分类和回归任务上进行了全面比较。实验结果显示，这些技术有助于在低成本下提供可信的运营准确性评估，并暴露更多 DNN 错误，为最佳测试实践提供了指导。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted for publication at ICSE 2024, Lisbon, Portugal",
      "pdf_url": "http://arxiv.org/pdf/2403.19271v1",
      "published_date": "2024-03-28 09:56:26 UTC",
      "updated_date": "2024-03-28 09:56:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:52:52.141873"
    },
    {
      "arxiv_id": "2403.19270v2",
      "title": "sDPO: Don't Use Your Data All at Once",
      "title_zh": "sDPO：不要一次性使用你的所有数据",
      "authors": [
        "Dahyun Kim",
        "Yungi Kim",
        "Wonho Song",
        "Hyeonwoo Kim",
        "Yunsu Kim",
        "Sanghoon Kim",
        "Chanjun Park"
      ],
      "abstract": "As development of large language models (LLM) progresses, aligning them with\nhuman preferences has become increasingly important. We propose stepwise DPO\n(sDPO), an extension of the recently popularized direct preference optimization\n(DPO) for alignment tuning. This approach involves dividing the available\npreference datasets and utilizing them in a stepwise manner, rather than\nemploying it all at once. We demonstrate that this method facilitates the use\nof more precisely aligned reference models within the DPO training framework.\nFurthermore, sDPO trains the final model to be more performant, even\noutperforming other popular LLMs with more parameters.",
      "tldr_zh": "该论文提出了一种名为 stepwise DPO (sDPO) 的方法，作为 direct preference optimization (DPO) 的扩展，用于提升大型语言模型 (LLM) 与人类偏好的对齐训练。sDPO 通过分步使用偏好数据集，而不是一次性全部投入，从而实现更精确的参考模型选择和训练过程。实验结果显示，这种方法能显著提高模型性能，甚至让最终模型超越参数更多的其他流行 LLM。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19270v2",
      "published_date": "2024-03-28 09:56:04 UTC",
      "updated_date": "2024-10-07 04:21:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:53:03.675995"
    },
    {
      "arxiv_id": "2403.19267v2",
      "title": "MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs",
      "title_zh": "MineLand：模拟具有有限多模态感知和物理需求",
      "authors": [
        "Xianhao Yu",
        "Jiaqi Fu",
        "Renjia Deng",
        "Wenjuan Han"
      ],
      "abstract": "While Vision-Language Models (VLMs) hold promise for tasks requiring\nextensive collaboration, traditional multi-agent simulators have facilitated\nrich explorations of an interactive artificial society that reflects collective\nbehavior. However, these existing simulators face significant limitations.\nFirstly, they struggle with handling large numbers of agents due to high\nresource demands. Secondly, they often assume agents possess perfect\ninformation and limitless capabilities, hindering the ecological validity of\nsimulated social interactions. To bridge this gap, we propose a multi-agent\nMinecraft simulator, MineLand, that bridges this gap by introducing three key\nfeatures: large-scale scalability, limited multimodal senses, and physical\nneeds. Our simulator supports 64 or more agents. Agents have limited visual,\nauditory, and environmental awareness, forcing them to actively communicate and\ncollaborate to fulfill physical needs like food and resources. Additionally, we\nfurther introduce an AI agent framework, Alex, inspired by multitasking theory,\nenabling agents to handle intricate coordination and scheduling. Our\nexperiments demonstrate that the simulator, the corresponding benchmark, and\nthe AI agent framework contribute to more ecological and nuanced collective\nbehavior.The source code of MineLand and Alex is openly available at\nhttps://github.com/cocacola-lab/MineLand.",
      "tldr_zh": "该论文提出 MineLand，一个基于 Minecraft 的多智能体模拟器，旨在解决现有模拟器在处理大规模代理器时资源需求高以及假设代理器拥有完美信息的问题。MineLand 引入大规模可扩展性（支持64或更多代理器）、有限的多模态感知（如视觉和听觉）以及物理需求（如食物和资源），迫使代理器通过主动沟通和合作来实现任务。论文还引入 AI 代理框架 Alex，灵感来源于多任务理论，帮助代理器处理复杂的协调和调度。实验结果表明，MineLand 促进了更生态和细致的集体行为，并开源了源代码。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Project website: https://github.com/cocacola-lab/MineLand",
      "pdf_url": "http://arxiv.org/pdf/2403.19267v2",
      "published_date": "2024-03-28 09:53:41 UTC",
      "updated_date": "2024-05-23 14:27:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:53:16.603544"
    },
    {
      "arxiv_id": "2403.19238v2",
      "title": "Taming Lookup Tables for Efficient Image Retouching",
      "title_zh": "翻译失败",
      "authors": [
        "Sidi Yang",
        "Binxiao Huang",
        "Mingdeng Cao",
        "Yatai Ji",
        "Hanzhong Guo",
        "Ngai Wong",
        "Yujiu Yang"
      ],
      "abstract": "The widespread use of high-definition screens in edge devices, such as\nend-user cameras, smartphones, and televisions, is spurring a significant\ndemand for image enhancement. Existing enhancement models often optimize for\nhigh performance while falling short of reducing hardware inference time and\npower consumption, especially on edge devices with constrained computing and\nstorage resources. To this end, we propose Image Color Enhancement Lookup Table\n(ICELUT) that adopts LUTs for extremely efficient edge inference, without any\nconvolutional neural network (CNN). During training, we leverage pointwise\n(1x1) convolution to extract color information, alongside a split fully\nconnected layer to incorporate global information. Both components are then\nseamlessly converted into LUTs for hardware-agnostic deployment. ICELUT\nachieves near-state-of-the-art performance and remarkably low power\nconsumption. We observe that the pointwise network structure exhibits robust\nscalability, upkeeping the performance even with a heavily downsampled 32x32\ninput image. These enable ICELUT, the first-ever purely LUT-based image\nenhancer, to reach an unprecedented speed of 0.4ms on GPU and 7ms on CPU, at\nleast one order faster than any CNN solution. Codes are available at\nhttps://github.com/Stephen0808/ICELUT.",
      "tldr_zh": "该论文提出了一种名为 ICELUT 的图像颜色增强查找表方法，用于高效的图像修复，旨在解决现有模型在边缘设备上高功耗和长推理时间的问题，而无需依赖卷积神经网络 (CNN)。在训练过程中，ICELUT 利用点式 (1x1) 卷积提取颜色信息，并结合分割的全连接层整合全局信息，随后将这些组件转换为 LUTs 以实现硬件无关部署。实验结果显示，ICELUT 性能接近最先进水平，同时功耗极低，在 GPU 上达到 0.4ms 和 CPU 上 7ms 的超快速度，比任何 CNN 解决方案快至少一个数量级。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV2024",
      "pdf_url": "http://arxiv.org/pdf/2403.19238v2",
      "published_date": "2024-03-28 08:49:35 UTC",
      "updated_date": "2024-07-13 08:38:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:53:29.767552"
    },
    {
      "arxiv_id": "2403.19727v1",
      "title": "New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark",
      "title_zh": "针对法语口语理解 MEDIA 基准的新语义任务",
      "authors": [
        "Nadège Alavoine",
        "Gaëlle Laperriere",
        "Christophe Servan",
        "Sahar Ghannay",
        "Sophie Rosset"
      ],
      "abstract": "Intent classification and slot-filling are essential tasks of Spoken Language\nUnderstanding (SLU). In most SLUsystems, those tasks are realized by\nindependent modules. For about fifteen years, models achieving both of\nthemjointly and exploiting their mutual enhancement have been proposed. A\nmultilingual module using a joint modelwas envisioned to create a touristic\ndialogue system for a European project, HumanE-AI-Net. A combination ofmultiple\ndatasets, including the MEDIA dataset, was suggested for training this joint\nmodel. The MEDIA SLU datasetis a French dataset distributed since 2005 by ELRA,\nmainly used by the French research community and free foracademic research\nsince 2020. Unfortunately, it is annotated only in slots but not intents. An\nenhanced version ofMEDIA annotated with intents has been built to extend its\nuse to more tasks and use cases. This paper presents thesemi-automatic\nmethodology used to obtain this enhanced version. In addition, we present the\nfirst results of SLUexperiments on this enhanced dataset using joint models for\nintent classification and slot-filling.",
      "tldr_zh": "这篇论文针对 Spoken Language Understanding (SLU) 中的 Intent classification 和 slot-filling 任务，提出了一种增强版的法国 MEDIA 数据集，通过添加意图标注来扩展其应用。研究者采用半自动方法来标注意图，从而使数据集能够支持联合模型的训练，并利用多个数据集组合进行多语言模块开发。实验结果显示，在增强版 MEDIA 数据集上使用联合模型取得了初步的积极效果，提升了任务间的互补性能，为旅游对话系统等应用提供了新的基准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19727v1",
      "published_date": "2024-03-28 08:40:02 UTC",
      "updated_date": "2024-03-28 08:40:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:53:41.962892"
    },
    {
      "arxiv_id": "2403.19221v1",
      "title": "Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality",
      "title_zh": "翻译失败",
      "authors": [
        "Sishuo Chen",
        "Lei Li",
        "Shuhuai Ren",
        "Rundong Gao",
        "Yuanxin Liu",
        "Xiaohan Bi",
        "Xu Sun",
        "Lu Hou"
      ],
      "abstract": "Video paragraph captioning (VPC) involves generating detailed narratives for\nlong videos, utilizing supportive modalities such as speech and event\nboundaries. However, the existing models are constrained by the assumption of\nconstant availability of a single auxiliary modality, which is impractical\ngiven the diversity and unpredictable nature of real-world scenarios. To this\nend, we propose a Missing-Resistant framework MR-VPC that effectively harnesses\nall available auxiliary inputs and maintains resilience even in the absence of\ncertain modalities. Under this framework, we propose the Multimodal VPC (MVPC)\narchitecture integrating video, speech, and event boundary inputs in a unified\nmanner to process various auxiliary inputs. Moreover, to fortify the model\nagainst incomplete data, we introduce DropAM, a data augmentation strategy that\nrandomly omits auxiliary inputs, paired with DistillAM, a regularization target\nthat distills knowledge from teacher models trained on modality-complete data,\nenabling efficient learning in modality-deficient environments. Through\nexhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC has\nproven to deliver superior performance on modality-complete and\nmodality-missing test data. This work highlights the significance of developing\nresilient VPC models and paves the way for more adaptive, robust multimodal\nvideo understanding.",
      "tldr_zh": "该研究针对视频段落字幕生成（VPC）模型的问题，提出了一种对缺失模态鲁棒的Missing-Resistant框架（MR-VPC），旨在处理视频、语音和事件边界等辅助输入的不确定性。框架中，Multimodal VPC (MVPC)架构统一整合多种模态输入，而DropAM数据增强策略通过随机省略输入来模拟缺失场景，DistillAM则通过从完整模态教师模型中蒸馏知识，实现高效学习。实验在YouCook2和ActivityNet Captions数据集上证明，MR-VPC在完整和缺失模态测试数据上均表现出色，提升了多模态视频理解的适应性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code available at https://github.com/lancopku/MR-VPC",
      "pdf_url": "http://arxiv.org/pdf/2403.19221v1",
      "published_date": "2024-03-28 08:35:46 UTC",
      "updated_date": "2024-03-28 08:35:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:53:54.131537"
    },
    {
      "arxiv_id": "2403.19211v2",
      "title": "Dual-Personalizing Adapter for Federated Foundation Models",
      "title_zh": "联邦基础模型的双个性化适",
      "authors": [
        "Yiyuan Yang",
        "Guodong Long",
        "Tao Shen",
        "Jing Jiang",
        "Michael Blumenstein"
      ],
      "abstract": "Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\ndiverse instruction data. Notably, federated foundation models (FedFM) emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to FedFM for better user preferences alignment.\nHowever, a critical gap in existing research is the neglect of test-time\ndistribution shifts in real-world applications, and conventional methods for\ntest-time distribution shifts in personalized FL are less effective for FedFM\ndue to their failure to adapt to complex distribution shift scenarios and the\nrequirement to train all parameters. To bridge this gap, we refine the setting\nin FedFM, termed test-time personalization, which aims to learn personalized\nfederated foundation models on clients while effectively handling test-time\ndistribution shifts simultaneously. To address challenges in this setting, we\nexplore a simple yet effective solution, a Federated Dual-Personalizing Adapter\n(FedDPA) architecture. By co-working with a foundation model, a global adapter\nand a local adapter jointly tackle the test-time distribution shifts and\nclient-specific personalization. Additionally, we introduce an instance-wise\ndynamic weighting mechanism that dynamically integrates the global and local\nadapters for each test instance during inference, facilitating effective\ntest-time personalization. The effectiveness of the proposed method has been\nevaluated on benchmark datasets across different NLP tasks.",
      "tldr_zh": "该论文针对联邦基础模型 (FedFM) 提出了一种双重个性化适配器 (FedDPA) 架构，以解决现有方法忽略测试时分布偏移 (test-time distribution shifts) 的问题，同时实现客户端个性化。FedDPA 通过一个全局适配器和一个本地适配器与基础模型协作，共同处理复杂的分布偏移场景和用户特定偏好。论文还引入了实例-wise 动态加权机制，在推理过程中动态整合全局和本地适配器，以提升个性化效果；在不同 NLP 任务的基准数据集上，实验验证了该方法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19211v2",
      "published_date": "2024-03-28 08:19:33 UTC",
      "updated_date": "2024-12-02 10:44:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:54:06.830538"
    },
    {
      "arxiv_id": "2403.19726v1",
      "title": "A Benchmark Evaluation of Clinical Named Entity Recognition in French",
      "title_zh": "翻译失败",
      "authors": [
        "Nesrine Bannour",
        "Christophe Servan",
        "Aurélie Névéol",
        "Xavier Tannier"
      ],
      "abstract": "Background: Transformer-based language models have shown strong performance\non many Natural LanguageProcessing (NLP) tasks. Masked Language Models (MLMs)\nattract sustained interest because they can be adaptedto different languages\nand sub-domains through training or fine-tuning on specific corpora while\nremaining lighterthan modern Large Language Models (LLMs). Recently, several\nMLMs have been released for the biomedicaldomain in French, and experiments\nsuggest that they outperform standard French counterparts. However,\nnosystematic evaluation comparing all models on the same corpora is available.\nObjective: This paper presentsan evaluation of masked language models for\nbiomedical French on the task of clinical named entity recognition.Material and\nmethods: We evaluate biomedical models CamemBERT-bio and DrBERT and compare\nthem tostandard French models CamemBERT, FlauBERT and FrALBERT as well as\nmultilingual mBERT using three publicallyavailable corpora for clinical named\nentity recognition in French. The evaluation set-up relies on\ngold-standardcorpora as released by the corpus developers. Results: Results\nsuggest that CamemBERT-bio outperformsDrBERT consistently while FlauBERT offers\ncompetitive performance and FrAlBERT achieves the lowest carbonfootprint.\nConclusion: This is the first benchmark evaluation of biomedical masked\nlanguage models for Frenchclinical entity recognition that compares model\nperformance consistently on nested entity recognition using metricscovering\nperformance and environmental impact.",
      "tldr_zh": "本研究评估了 Masked Language Models (MLMs) 在法语临床命名实体识别（Clinical Named Entity Recognition）任务中的性能，旨在比较生物医学专用模型与标准法语模型的差异。研究者使用三个公开的法语临床语料库，对 CamemBERT-bio、DrBERT 等生物医学模型，以及 CamemBERT、FlauBERT、FrALBERT 和多语言 mBERT 进行了系统评估。结果显示，CamemBERT-bio 一致优于 DrBERT，而 FlauBERT 表现出色，且 FrALBERT 的碳足迹最低。该基准评估首次全面比较了这些模型在嵌套实体识别上的表现，包括性能和环境影响，为法语生物医学 NLP 应用提供了重要参考。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19726v1",
      "published_date": "2024-03-28 07:59:58 UTC",
      "updated_date": "2024-03-28 07:59:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:54:18.233755"
    },
    {
      "arxiv_id": "2403.19725v1",
      "title": "MUGC: Machine Generated versus User Generated Content Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yaqi Xie",
        "Anjali Rawal",
        "Yujing Cen",
        "Dixuan Zhao",
        "Sunil K Narang",
        "Shanu Sushmita"
      ],
      "abstract": "As advanced modern systems like deep neural networks (DNNs) and generative AI\ncontinue to enhance their capabilities in producing convincing and realistic\ncontent, the need to distinguish between user-generated and machine generated\ncontent is becoming increasingly evident. In this research, we undertake a\ncomparative evaluation of eight traditional machine-learning algorithms to\ndistinguish between machine-generated and human-generated data across three\ndiverse datasets: Poems, Abstracts, and Essays. Our results indicate that\ntraditional methods demonstrate a high level of accuracy in identifying\nmachine-generated data, reflecting the documented effectiveness of popular\npre-trained models like RoBERT. We note that machine-generated texts tend to be\nshorter and exhibit less word variety compared to human-generated content.\nWhile specific domain-related keywords commonly utilized by humans, albeit\ndisregarded by current LLMs (Large Language Models), may contribute to this\nhigh detection accuracy, we show that deeper word representations like word2vec\ncan capture subtle semantic variances. Furthermore, readability, bias, moral,\nand affect comparisons reveal a discernible contrast between machine-generated\nand human generated content. There are variations in expression styles and\npotentially underlying biases in the data sources (human and\nmachine-generated). This study provides valuable insights into the advancing\ncapacities and challenges associated with machine-generated content across\nvarious domains.",
      "tldr_zh": "本文研究评估了八种传统机器学习算法，用于区分机器生成内容（machine-generated）和用户生成内容（user-generated），在Poems、Abstracts和Essays三个数据集上进行比较。结果显示，这些算法表现出高准确率，与RoBERT等预训练模型类似，机器生成文本通常更短、词汇多样性较低，且在可读性、偏见、道德和情感方面与人类文本存在显著差异。利用word2vec等技术，研究捕捉了细微语义差异，并提供了机器生成内容在不同领域的优势和挑战的宝贵洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.19725v1",
      "published_date": "2024-03-28 07:33:53 UTC",
      "updated_date": "2024-03-28 07:33:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:54:30.150998"
    },
    {
      "arxiv_id": "2403.19178v1",
      "title": "Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning",
      "title_zh": "在分布式网络中增强信任和隐私：基于区块链的联邦学习的全面调查",
      "authors": [
        "Ji Liu",
        "Chunlu Chen",
        "Yu Li",
        "Lin Sun",
        "Yulun Song",
        "Jingbo Zhou",
        "Bo Jing",
        "Dejing Dou"
      ],
      "abstract": "While centralized servers pose a risk of being a single point of failure,\ndecentralized approaches like blockchain offer a compelling solution by\nimplementing a consensus mechanism among multiple entities. Merging distributed\ncomputing with cryptographic techniques, decentralized technologies introduce a\nnovel computing paradigm. Blockchain ensures secure, transparent, and\ntamper-proof data management by validating and recording transactions via\nconsensus across network nodes. Federated Learning (FL), as a distributed\nmachine learning framework, enables participants to collaboratively train\nmodels while safeguarding data privacy by avoiding direct raw data exchange.\nDespite the growing interest in decentralized methods, their application in FL\nremains underexplored. This paper presents a thorough investigation into\nBlockchain-based FL (BCFL), spotlighting the synergy between blockchain's\nsecurity features and FL's privacy-preserving model training capabilities.\nFirst, we present the taxonomy of BCFL from three aspects, including\ndecentralized, separate networks, and reputation-based architectures. Then, we\nsummarize the general architecture of BCFL systems, providing a comprehensive\nperspective on FL architectures informed by blockchain. Afterward, we analyze\nthe application of BCFL in healthcare, IoT, and other privacy-sensitive areas.\nFinally, we identify future research directions of BCFL.",
      "tldr_zh": "这篇论文调查了区块链与联邦学习（BCFL）的整合，以提升分布式网络的信任和隐私，解决集中式服务器的单点故障问题。论文从分散化、分离网络和基于声誉的架构三个方面对BCFL进行了分类，并总结了其一般架构，包括区块链的安全机制与Federated Learning（FL）的隐私保护能力的协同作用。研究还分析了BCFL在医疗、IoT等隐私敏感领域的应用，并提出了未来的研究方向，如进一步探索其扩展性和优化。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "25 pages, accepted by KAIS 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.19178v1",
      "published_date": "2024-03-28 07:08:26 UTC",
      "updated_date": "2024-03-28 07:08:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:54:40.742221"
    },
    {
      "arxiv_id": "2403.19177v1",
      "title": "Rethinking Information Loss in Medical Image Segmentation with Various-sized Targets",
      "title_zh": "重新思考医学图像分割中各种大小目标的信息损失",
      "authors": [
        "Tianyi Liu",
        "Zhaorui Tan",
        "Kaizhu Huang",
        "Haochuan Jiang"
      ],
      "abstract": "Medical image segmentation presents the challenge of segmenting various-size\ntargets, demanding the model to effectively capture both local and global\ninformation. Despite recent efforts using CNNs and ViTs to predict annotations\nof different scales, these approaches often struggle to effectively balance the\ndetection of targets across varying sizes. Simply utilizing local information\nfrom CNNs and global relationships from ViTs without considering potential\nsignificant divergence in latent feature distributions may result in\nsubstantial information loss. To address this issue, in this paper, we will\nintroduce a novel Stagger Network (SNet) and argues that a well-designed fusion\nstructure can mitigate the divergence in latent feature distributions between\nCNNs and ViTs, thereby reducing information loss. Specifically, to emphasize\nboth global dependencies and local focus, we design a Parallel Module to bridge\nthe semantic gap. Meanwhile, we propose the Stagger Module, trying to fuse the\nselected features that are more semantically similar. An Information Recovery\nModule is further adopted to recover complementary information back to the\nnetwork. As a key contribution, we theoretically analyze that the proposed\nparallel and stagger strategies would lead to less information loss, thus\ncertifying the SNet's rationale. Experimental results clearly proved that the\nproposed SNet excels comparisons with recent SOTAs in segmenting on the Synapse\ndataset where targets are in various sizes. Besides, it also demonstrates\nsuperiority on the ACDC and the MoNuSeg datasets where targets are with more\nconsistent dimensions.",
      "tldr_zh": "本论文重新审视了医疗图像分割中处理各种大小目标时可能的信息损失问题，指出现有基于 CNNs 和 ViTs 的方法在平衡局部和全局信息时，常因特征分布差异导致显著损失。作者提出了一种新型 Stagger Network (SNet)，包括 Parallel Module 用于桥接语义差距、Stagger Module 用于融合语义相似的特征，以及 Information Recovery Module 来恢复互补信息，从而有效减少信息损失。理论分析证明了这些策略能降低信息丢失风险，实验结果显示 SNet 在 Synapse 数据集（各种大小目标）上优于现有最先进方法，并在 ACDC 和 MoNuSeg 数据集（目标尺寸更一致）中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19177v1",
      "published_date": "2024-03-28 07:01:11 UTC",
      "updated_date": "2024-03-28 07:01:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:54:53.635218"
    },
    {
      "arxiv_id": "2403.19167v1",
      "title": "Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering",
      "title_zh": "通过选择性过滤缓解误导性的链式思维推理",
      "authors": [
        "Yexin Wu",
        "Zhuosheng Zhang",
        "Hai Zhao"
      ],
      "abstract": "Large language models have manifested remarkable capabilities by leveraging\nchain-of-thought (CoT) reasoning techniques to solve intricate questions\nthrough step-by-step reasoning chains. Despite its success, the efficacy of\nsuch reasoning is inherently contingent upon the quality of CoT. However,\nflawless CoT reasoning cannot be guaranteed due to the presence of\nindecomposable questions and the potential for erroneous reasoning chains,\nparticularly in the case of small-scale language models. To tackle this\nchallenge, we propose a novel approach called the selective filtering reasoner\n(SelF-Reasoner) that assesses the entailment relationship between the question\nand the candidate reasoning chain. Then, we proceed with CoT reasoning when the\nreasoning chain demonstrates confidence; otherwise, we opt to predict the\nanswer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently\nover the ScienceQA, ECQA, and LastLetter tasks. Code is available at\n\\texttt{https://github.com/LibroWu/SelF-Reasoner}.",
      "tldr_zh": "本研究针对大型语言模型在Chain-of-Thought (CoT)推理中可能出现误导问题（如不可分解的问题和错误推理链），提出了一种新型方法Selective Filtering Reasoner (SelF-Reasoner)。该方法通过评估问题与候选推理链之间的entailment relationship，如果推理链可靠则继续CoT推理，否则直接预测答案，从而提升推理准确性。实验结果显示，SelF-Reasoner在ScienceQA、ECQA和LastLetter任务上 consistently改善了fine-tuned T5基线的性能，并提供了开源代码以便进一步应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19167v1",
      "published_date": "2024-03-28 06:28:35 UTC",
      "updated_date": "2024-03-28 06:28:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:55:05.425579"
    },
    {
      "arxiv_id": "2403.19154v3",
      "title": "STaR-GATE: Teaching Language Models to Ask Clarifying Questions",
      "title_zh": "STaR-GATE：训练语言模型提出澄清问题",
      "authors": [
        "Chinmaya Andukuri",
        "Jan-Philipp Fränken",
        "Tobias Gerstenberg",
        "Noah D. Goodman"
      ],
      "abstract": "When prompting language models to complete a task, users often leave\nimportant aspects unsaid. While asking questions could resolve this ambiguity\n(GATE; Li et al., 2023), models often struggle to ask good questions. We\nexplore a language model's ability to self-improve (STaR; Zelikman et al.,\n2022) by rewarding the model for generating useful questions-a simple method we\ndub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task\nprompts to simulate conversations between a pretrained language model-the\nQuestioner-and a Roleplayer whose preferences are unknown to the Questioner. By\nasking questions, the Questioner elicits preferences from the Roleplayer. The\nQuestioner is iteratively finetuned on questions that increase the probability\nof high-quality responses to the task, which are generated by an Oracle with\naccess to the Roleplayer's latent preferences. After two iterations of\nself-improvement, the Questioner asks better questions, allowing it to generate\nresponses that are preferred over responses from the initial model on 72% of\ntasks. Our results indicate that teaching a language model to ask better\nquestions leads to better personalized responses.",
      "tldr_zh": "这篇论文提出了STaR-GATE方法，通过自提升（STaR）技术训练语言模型（language models）生成有用的问题，以解决用户提示中的模糊性问题。研究者创建了一个包含25,500个独特persona-task提示的合成数据集，模拟Questioner与Roleplayer的对话，Questioner通过迭代微调来奖励那些能提升任务响应质量的问题。结果显示，经过两次自提升迭代后，模型提出的澄清questions在72%的任务上生成更优的个性化响应，证明了这一方法能显著改善语言模型的交互性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19154v3",
      "published_date": "2024-03-28 05:35:22 UTC",
      "updated_date": "2024-08-07 19:44:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:55:18.212847"
    },
    {
      "arxiv_id": "2403.19150v1",
      "title": "Towards Understanding Dual BN In Hybrid Adversarial Training",
      "title_zh": "翻译失败",
      "authors": [
        "Chenshuang Zhang",
        "Chaoning Zhang",
        "Kang Zhang",
        "Axi Niu",
        "Junmo Kim",
        "In So Kweon"
      ],
      "abstract": "There is a growing concern about applying batch normalization (BN) in\nadversarial training (AT), especially when the model is trained on both\nadversarial samples and clean samples (termed Hybrid-AT). With the assumption\nthat adversarial and clean samples are from two different domains, a common\npractice in prior works is to adopt Dual BN, where BN and BN are used for\nadversarial and clean branches, respectively. A popular belief for motivating\nDual BN is that estimating normalization statistics of this mixture\ndistribution is challenging and thus disentangling it for normalization\nachieves stronger robustness. In contrast to this belief, we reveal that\ndisentangling statistics plays a less role than disentangling affine parameters\nin model training. This finding aligns with prior work (Rebuffi et al., 2023),\nand we build upon their research for further investigations. We demonstrate\nthat the domain gap between adversarial and clean samples is not very large,\nwhich is counter-intuitive considering the significant influence of adversarial\nperturbation on the model accuracy. We further propose a two-task hypothesis\nwhich serves as the empirical foundation and a unified framework for Hybrid-AT\nimprovement. We also investigate Dual BN in test-time and reveal that affine\nparameters characterize the robustness during inference. Overall, our work\nsheds new light on understanding the mechanism of Dual BN in Hybrid-AT and its\nunderlying justification.",
      "tldr_zh": "本研究探讨了在混合对抗训练（Hybrid-AT）中使用双批量归一化（Dual BN）的机制，其中Hybrid-AT同时处理对抗样本和干净样本。作者发现，与先前信念相反，分离统计信息的作用较小，而分离仿射参数（affine parameters）在模型训练中更为关键，这与Rebuffi et al. (2023)的研究一致。实验显示，对抗样本和干净样本之间的领域差距并不显著，尽管对抗扰动对模型准确性有重大影响。研究进一步提出两任务假设（two-task hypothesis）作为Hybrid-AT改进的统一框架，并揭示在测试时，仿射参数是表征鲁棒性的关键因素。该工作为理解Dual BN在Hybrid-AT中的作用提供了新见解，推动了更有效的对抗训练方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at TMLR",
      "pdf_url": "http://arxiv.org/pdf/2403.19150v1",
      "published_date": "2024-03-28 05:08:25 UTC",
      "updated_date": "2024-03-28 05:08:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:55:32.012446"
    },
    {
      "arxiv_id": "2403.19148v1",
      "title": "GenAI Detection Tools, Adversarial Techniques and Implications for Inclusivity in Higher Education",
      "title_zh": "GenAI 检测工具、抗对技术以及对高等教育包容性的影响",
      "authors": [
        "Mike Perkins",
        "Jasper Roe",
        "Binh H. Vu",
        "Darius Postma",
        "Don Hickerson",
        "James McGaughran",
        "Huy Q. Khuat"
      ],
      "abstract": "This study investigates the efficacy of six major Generative AI (GenAI) text\ndetectors when confronted with machine-generated content that has been modified\nusing techniques designed to evade detection by these tools (n=805). The\nresults demonstrate that the detectors' already low accuracy rates (39.5%) show\nmajor reductions in accuracy (17.4%) when faced with manipulated content, with\nsome techniques proving more effective than others in evading detection.\n  The accuracy limitations and the potential for false accusations demonstrate\nthat these tools cannot currently be recommended for determining whether\nviolations of academic integrity have occurred, underscoring the challenges\neducators face in maintaining inclusive and fair assessment practices. However,\nthey may have a role in supporting student learning and maintaining academic\nintegrity when used in a non-punitive manner.\n  These results underscore the need for a combined approach to addressing the\nchallenges posed by GenAI in academia to promote the responsible and equitable\nuse of these emerging technologies. The study concludes that the current\nlimitations of AI text detectors require a critical approach for any possible\nimplementation in HE and highlight possible alternatives to AI assessment\nstrategies.",
      "tldr_zh": "本研究评估了六种主要 Generative AI (GenAI) 文本检测工具的效能，针对被设计以逃避检测的对抗技术 (adversarial techniques) 修改过的机器生成内容（样本数 n=805）。结果显示，这些工具的准确率从原本的 39.5% 进一步降至 17.4%，某些技术更为有效，导致潜在的假阳性指控和学术诚信问题。研究强调，这些工具目前不适合用于判断学术诚信违规，而是可能在非惩罚性方式下支持学生学习；同时，呼吁采用综合方法来应对 GenAI 在高等教育 (HE) 中的挑战，促进负责任和公平的使用，并探索 AI 评估策略的替代方案。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19148v1",
      "published_date": "2024-03-28 04:57:13 UTC",
      "updated_date": "2024-03-28 04:57:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:55:44.484880"
    },
    {
      "arxiv_id": "2403.19140v2",
      "title": "QNCD: Quantization Noise Correction for Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Huanpeng Chu",
        "Wei Wu",
        "Chengjie Zang",
        "Kun Yuan"
      ],
      "abstract": "Diffusion models have revolutionized image synthesis, setting new benchmarks\nin quality and creativity. However, their widespread adoption is hindered by\nthe intensive computation required during the iterative denoising process.\nPost-training quantization (PTQ) presents a solution to accelerate sampling,\naibeit at the expense of sample quality, extremely in low-bit settings.\nAddressing this, our study introduces a unified Quantization Noise Correction\nScheme (QNCD), aimed at minishing quantization noise throughout the sampling\nprocess. We identify two primary quantization challenges: intra and inter\nquantization noise. Intra quantization noise, mainly exacerbated by embeddings\nin the resblock module, extends activation quantization ranges, increasing\ndisturbances in each single denosing step. Besides, inter quantization noise\nstems from cumulative quantization deviations across the entire denoising\nprocess, altering data distributions step-by-step. QNCD combats these through\nembedding-derived feature smoothing for eliminating intra quantization noise\nand an effective runtime noise estimatiation module for dynamicly filtering\ninter quantization noise. Extensive experiments demonstrate that our method\noutperforms previous quantization methods for diffusion models, achieving\nlossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4).\nCode is available at: https://github.com/huanpengchu/QNCD",
      "tldr_zh": "该论文针对扩散模型（Diffusion Models）在图像合成中的高计算需求，提出了一种统一的量化噪声校正方案（QNCD），以减少后期训练量化（PTQ）导致的样本质量下降问题。QNCD 识别并解决两种主要量化噪声：intra quantization noise（通过 embedding-derived feature smoothing 平滑 resblock 模块的激活范围）和 inter quantization noise（利用 runtime noise estimation module 动态过滤跨步累积偏差）。实验结果显示，QNCD 在 ImageNet (LDM-4) 的 W4A8 和 W8A8 量化设置中实现了无损性能，比现有方法表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACMMM2024",
      "pdf_url": "http://arxiv.org/pdf/2403.19140v2",
      "published_date": "2024-03-28 04:24:56 UTC",
      "updated_date": "2024-09-18 10:50:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:55:56.006300"
    },
    {
      "arxiv_id": "2403.19135v5",
      "title": "Streamlining Redundant Layers to Compress Large Language Models",
      "title_zh": "精简冗余层以压缩",
      "authors": [
        "Xiaodong Chen",
        "Yuxuan Hu",
        "Jing Zhang",
        "Yanling Wang",
        "Cuiping Li",
        "Hong Chen"
      ],
      "abstract": "This paper introduces LLM-Streamline, a pioneer work on layer pruning for\nlarge language models (LLMs). It is based on the observation that different\nlayers have varying impacts on hidden states, enabling the identification of\nless important layers to be pruned.LLM-Streamline comprises two parts: layer\npruning, which removes consecutive layers with the lowest importance based on\ntarget sparsity, and layer replacement, a novel module that trains a\nlightweight network to replace the pruned layers to mitigate performance loss.\nAdditionally, a new metric called stability is proposed to address the\nlimitations of the widely used accuracy metric in evaluating model compression.\nExperiments show that LLM-Streamline outperforms both previous and concurrent\nstate-of-the-art pruning methods in terms of both performance and training\nefficiency.Our code is available at\nhttps://github.com/RUCKBReasoning/LLM-Streamline",
      "tldr_zh": "本论文提出LLM-Streamline，一种针对大型语言模型(LLMs)的层剪枝方法，通过评估不同层对隐藏状态的影响来识别并移除不重要层，从而实现模型压缩。该方法包括层剪枝（基于目标稀疏度移除连续的最低重要性层）和层替换（训练一个轻量级网络来替代剪枝层，以减少性能损失）。此外，论文引入了新的度量标准stability，以弥补accuracy指标的不足。实验结果显示，LLM-Streamline在性能和训练效率上优于现有最先进剪枝方法，并提供了开源代码。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19135v5",
      "published_date": "2024-03-28 04:12:13 UTC",
      "updated_date": "2025-01-25 00:11:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:56:06.101883"
    },
    {
      "arxiv_id": "2404.01319v2",
      "title": "Information Cascade Prediction under Public Emergencies: A Survey",
      "title_zh": "公共紧急事件下的信息级联预测：综述",
      "authors": [
        "Qi Zhang",
        "Guang Wang",
        "Li Lin",
        "Kaiwen Xia",
        "Shuai Wang"
      ],
      "abstract": "With the advent of the era of big data, massive information, expert\nexperience, and high-accuracy models bring great opportunities to the\ninformation cascade prediction of public emergencies. However, the involvement\nof specialist knowledge from various disciplines has resulted in a primarily\napplication-specific focus (e.g., earthquakes, floods, infectious diseases) for\ninformation cascade prediction of public emergencies. The lack of a unified\nprediction framework poses a challenge for classifying intersectional\nprediction methods across different application fields. This survey paper\noffers a systematic classification and summary of information cascade modeling,\nprediction, and application. We aim to help researchers identify cutting-edge\nresearch and comprehend models and methods of information cascade prediction\nunder public emergencies. By summarizing open issues and outlining future\ndirections in this field, this paper has the potential to be a valuable\nresource for researchers conducting further studies on predicting information\ncascades.",
      "tldr_zh": "这篇调查论文探讨了在公共紧急事件（如地震、洪水和传染病）下进行信息级联预测的现状，强调大数据时代带来的机遇，同时指出现有研究因依赖特定领域知识而缺乏统一的预测框架。论文对信息级联建模、预测和应用进行了系统分类和总结，帮助研究者识别前沿工作并理解相关模型和方法。通过分析开放问题和未来方向，该论文为进一步的信息级联预测研究提供了宝贵资源。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.SI",
      "comment": "arXiv admin note: text overlap with arXiv:2007.09815 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2404.01319v2",
      "published_date": "2024-03-28 03:46:56 UTC",
      "updated_date": "2024-05-16 23:56:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:56:17.708936"
    },
    {
      "arxiv_id": "2403.19723v2",
      "title": "HeGTa: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding",
      "title_zh": "HeGTa：利用异构图增强的大型语言模型进行少样本复杂表格理解",
      "authors": [
        "Rihui Jin",
        "Yu Li",
        "Guilin Qi",
        "Nan Hu",
        "Yuan-Fang Li",
        "Jiaoyan Chen",
        "Jianan Wang",
        "Yongrui Chen",
        "Dehai Min",
        "Sheng Bi"
      ],
      "abstract": "Table understanding (TU) has achieved promising advancements, but it faces\nthe challenges of the scarcity of manually labeled tables and the presence of\ncomplex table structures.To address these challenges, we propose HGT, a\nframework with a heterogeneous graph (HG)-enhanced large language model (LLM)\nto tackle few-shot TU tasks.It leverages the LLM by aligning the table\nsemantics with the LLM's parametric knowledge through soft prompts and\ninstruction turning and deals with complex tables by a multi-task pre-training\nscheme involving three novel multi-granularity self-supervised HG pre-training\nobjectives.We empirically demonstrate the effectiveness of HGT, showing that it\noutperforms the SOTA for few-shot complex TU on several benchmarks.",
      "tldr_zh": "该论文提出HeGTa框架，利用Heterogeneous Graph (HG)-enhanced Large Language Models (LLM) 来解决少样本复杂表理解(Few-shot Complex Table Understanding)面临的标注数据稀缺和结构复杂问题。该框架通过软提示和指令微调将表语义与LLM的参数知识对齐，并引入多任务预训练方案，包括三个新颖的多粒度自监督HG预训练目标，以有效处理复杂表结构。实验结果表明，HeGTa在多个基准上超过了现有最先进方法(SOTA)。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2403.19723v2",
      "published_date": "2024-03-28 03:20:54 UTC",
      "updated_date": "2024-12-15 14:24:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:56:30.724641"
    },
    {
      "arxiv_id": "2403.19116v1",
      "title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Che Guan",
        "Mengyu Huang",
        "Peng Zhang"
      ],
      "abstract": "In today's fast-paced industry, professionals face the challenge of\nsummarizing a large number of documents and extracting vital information from\nthem on a daily basis. These metrics are frequently hidden away in tables\nand/or their nested hyperlinks. To address this challenge, the approach of\nTable Question Answering (QA) has been developed to extract the relevant\ninformation. However, traditional Table QA training tasks that provide a table\nand an answer(s) from a gold cell coordinate(s) for a question may not always\nensure extracting the accurate answer(s). Recent advancements in Large Language\nModels (LLMs) have opened up new possibilities for extracting information from\ntabular data using prompts. In this paper, we introduce the Multi-hop Few-shot\nOpen Rich Table QA (MFORT-QA) approach, which consists of two major steps. The\nfirst step involves Few-Shot Learning (FSL), where relevant tables and\nassociated contexts of hyperlinks are retrieved based on a given question. The\nretrieved content is then used to construct few-shot prompts as inputs to an\nLLM, such as ChatGPT. To tackle the challenge of answering complex questions,\nthe second step leverages Chain-of-thought (CoT) prompting to decompose the\ncomplex question into a sequential chain of questions and reasoning thoughts in\na multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process\nby retrieving relevant tables and contexts of hyperlinks that are relevant to\nthe resulting reasoning thoughts and questions. These additional contexts are\nthen used to supplement the prompt used in the first step, resulting in more\naccurate answers from an LLM. Empirical results from OTT-QA demonstrate that\nour abstractive QA approach significantly improves the accuracy of extractive\nTable QA methods.",
      "tldr_zh": "本论文提出 MFORT-QA 框架，用于处理多跳少样本的开放式丰富表格问答问题，旨在从表格和嵌套超链接中准确提取信息。\n该框架包括两个主要步骤：首先，通过 Few-Shot Learning (FSL) 检索相关表格和上下文构建提示输入 Large Language Models (LLMs)；其次，利用 Chain-of-Thought (CoT) 提示将复杂问题分解为多跳问题，并结合 Retrieval-Augmented Generation (RAG) 补充相关内容以提升答案准确性。\n实验结果在 OTT-QA 数据集上表明，MFORT-QA 显著提高了抽取式 Table QA 方法的准确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2403.19116v1",
      "published_date": "2024-03-28 03:14:18 UTC",
      "updated_date": "2024-03-28 03:14:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:56:43.633600"
    },
    {
      "arxiv_id": "2403.19113v1",
      "title": "FACTOID: FACtual enTailment fOr hallucInation Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Vipula Rawte",
        "S. M Towhidul Islam Tonmoy",
        "Krishnav Rajbangshi",
        "Shravani Nag",
        "Aman Chadha",
        "Amit P. Sheth",
        "Amitava Das"
      ],
      "abstract": "The widespread adoption of Large Language Models (LLMs) has facilitated\nnumerous benefits. However, hallucination is a significant concern. In\nresponse, Retrieval Augmented Generation (RAG) has emerged as a highly\npromising paradigm to improve LLM outputs by grounding them in factual\ninformation. RAG relies on textual entailment (TE) or similar methods to check\nif the text produced by LLMs is supported or contradicted, compared to\nretrieved documents. This paper argues that conventional TE methods are\ninadequate for spotting hallucinations in content generated by LLMs. For\ninstance, consider a prompt about the 'USA's stance on the Ukraine war''. The\nAI-generated text states, ...U.S. President Barack Obama says the U.S. will not\nput troops in Ukraine...'' However, during the war the U.S. president is Joe\nBiden which contradicts factual reality. Moreover, current TE systems are\nunable to accurately annotate the given text and identify the exact portion\nthat is contradicted. To address this, we introduces a new type of TE called\n``Factual Entailment (FE).'', aims to detect factual inaccuracies in content\ngenerated by LLMs while also highlighting the specific text segment that\ncontradicts reality. We present FACTOID (FACTual enTAILment for hallucInation\nDetection), a benchmark dataset for FE. We propose a multi-task learning (MTL)\nframework for FE, incorporating state-of-the-art (SoTA) long text embeddings\nsuch as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The\nproposed MTL architecture for FE achieves an avg. 40\\% improvement in accuracy\non the FACTOID benchmark compared to SoTA TE methods. As FE automatically\ndetects hallucinations, we assessed 15 modern LLMs and ranked them using our\nproposed Auto Hallucination Vulnerability Index (HVI_auto). This index\nquantifies and offers a comparative scale to evaluate and rank LLMs according\nto their hallucinations.",
      "tldr_zh": "这篇论文针对 Large Language Models (LLMs) 的幻觉问题，指出传统的 Textual Entailment (TE) 方法不足以准确检测事实不符内容，因此提出了一种新的 Factual Entailment (FE) 方法，用于识别 LLM 生成文本中的 factual inaccuracies 并突出具体矛盾段落。作者构建了 FACTOID 基准数据集，并开发了一个基于 Multi-Task Learning (MTL) 框架的模型，使用如 e5-mistral-7b-instruct、GPT-3、SpanBERT 和 RoFormer 等技术，该框架在 FACTOID 上比现有 TE 方法提高了 40% 的准确率。同时，他们引入了 Auto Hallucination Vulnerability Index (HVI_auto) 指标，对 15 个现代 LLMs 进行评估和排名，以量化其幻觉风险。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19113v1",
      "published_date": "2024-03-28 03:09:42 UTC",
      "updated_date": "2024-03-28 03:09:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:56:57.217990"
    },
    {
      "arxiv_id": "2406.17605v1",
      "title": "NativE: Multi-modal Knowledge Graph Completion in the Wild",
      "title_zh": "翻译失败",
      "authors": [
        "Yichi Zhang",
        "Zhuo Chen",
        "Lingbing Guo",
        "Yajing Xu",
        "Binbin Hu",
        "Ziqi Liu",
        "Wen Zhang",
        "Huajun Chen"
      ],
      "abstract": "Multi-modal knowledge graph completion (MMKGC) aims to automatically discover\nthe unobserved factual knowledge from a given multi-modal knowledge graph by\ncollaboratively modeling the triple structure and multi-modal information from\nentities. However, real-world MMKGs present challenges due to their diverse and\nimbalanced nature, which means that the modality information can span various\ntypes (e.g., image, text, numeric, audio, video) but its distribution among\nentities is uneven, leading to missing modalities for certain entities.\nExisting works usually focus on common modalities like image and text while\nneglecting the imbalanced distribution phenomenon of modal information. To\naddress these issues, we propose a comprehensive framework NativE to achieve\nMMKGC in the wild. NativE proposes a relation-guided dual adaptive fusion\nmodule that enables adaptive fusion for any modalities and employs a\ncollaborative modality adversarial training framework to augment the imbalanced\nmodality information. We construct a new benchmark called WildKGC with five\ndatasets to evaluate our method. The empirical results compared with 21 recent\nbaselines confirm the superiority of our method, consistently achieving\nstate-of-the-art performance across different datasets and various scenarios\nwhile keeping efficient and generalizable. Our code and data are released at\nhttps://github.com/zjukg/NATIVE",
      "tldr_zh": "该研究针对现实世界中的多模态知识图谱补全（Multi-modal Knowledge Graph Completion, MMKGC）问题，提出了一种全面框架NativE，以应对模态信息多样（如图像、文本、数字、音频、视频）和不平衡分布导致的缺失挑战。NativE引入关系引导的双适应融合模块（relation-guided dual adaptive fusion module）来实现任意模态的自适应融合，并采用协作模态对抗训练框架（collaborative modality adversarial training framework）增强不平衡模态信息。实验在新建的WildKGC基准（包含五个数据集）上与21个基线模型比较，NativE consistently achieves state-of-the-art performance，同时保持高效和泛化性，代码已开源。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.IR"
      ],
      "primary_category": "cs.MM",
      "comment": "Accepted by SIGIR 2024 as a full paper",
      "pdf_url": "http://arxiv.org/pdf/2406.17605v1",
      "published_date": "2024-03-28 03:04:00 UTC",
      "updated_date": "2024-03-28 03:04:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:57:08.248780"
    },
    {
      "arxiv_id": "2403.19103v3",
      "title": "Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yutong He",
        "Alexander Robey",
        "Naoki Murata",
        "Yiding Jiang",
        "Joshua Nathaniel Williams",
        "George J. Pappas",
        "Hamed Hassani",
        "Yuki Mitsufuji",
        "Ruslan Salakhutdinov",
        "J. Zico Kolter"
      ],
      "abstract": "Prompt engineering is an effective but labor-intensive way to control\ntext-to-image (T2I) generative models. Its time-intensive nature and complexity\nhave spurred the development of algorithms for automated prompt generation.\nHowever, these methods often struggle with transferability across T2I models,\nrequire white-box access to the underlying model, or produce non-intuitive\nprompts. In this work, we introduce PRISM, an algorithm that automatically\nproduces human-interpretable and transferable prompts that can effectively\ngenerate desired concepts given only black-box access to T2I models. Inspired\nby large language model (LLM) jailbreaking, PRISM leverages the in-context\nlearning ability of LLMs to iteratively refine the candidate prompt\ndistribution built upon the reference images. Our experiments demonstrate the\nversatility and effectiveness of PRISM in generating accurate prompts for\nobjects, styles, and images across multiple T2I models, including Stable\nDiffusion, DALL-E, and Midjourney.",
      "tldr_zh": "该研究针对文本到图像 (T2I) 生成中的提示工程问题，提出 PRISM 算法，该算法能自动生成人类可解释且可转移的提示，仅需黑箱访问 T2I 模型，而无需白箱信息。PRISM 受 LLM jailbreaking 启发，利用 LLM 的 in-context learning 能力，通过基于参考图像的迭代优化来细化候选提示分布。实验结果显示，PRISM 在 Stable Diffusion、DALL-E 和 Midjourney 等模型上，成功生成准确的提示，用于个性化对象、风格和图像，提升了生成效果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19103v3",
      "published_date": "2024-03-28 02:35:53 UTC",
      "updated_date": "2025-04-28 03:04:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:57:19.730881"
    },
    {
      "arxiv_id": "2403.19101v1",
      "title": "AAPMT: AGI Assessment Through Prompt and Metric Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Benhao Huang"
      ],
      "abstract": "The emergence of text-to-image models marks a significant milestone in the\nevolution of AI-generated images (AGIs), expanding their use in diverse domains\nlike design, entertainment, and more. Despite these breakthroughs, the quality\nof AGIs often remains suboptimal, highlighting the need for effective\nevaluation methods. These methods are crucial for assessing the quality of\nimages relative to their textual descriptions, and they must accurately mirror\nhuman perception. Substantial progress has been achieved in this domain, with\ninnovative techniques such as BLIP and DBCNN contributing significantly.\nHowever, recent studies, including AGIQA-3K, reveal a notable discrepancy\nbetween current methods and state-of-the-art (SOTA) standards. This gap\nemphasizes the necessity for a more sophisticated and precise evaluation\nmetric. In response, our objective is to develop a model that could give\nratings for metrics, which focuses on parameters like perceptual quality,\nauthenticity, and the correspondence between text and image, that more closely\naligns with human perception. In our paper, we introduce a range of effective\nmethods, including prompt designs and the Metric Transformer. The Metric\nTransformer is a novel structure inspired by the complex interrelationships\namong various AGI quality metrics. The code is available at\nhttps://github.com/huskydoge/CS3324-Digital-Image-Processing/tree/main/Assignment1",
      "tldr_zh": "该论文针对AI生成图像（AGI）的质量评估问题，指出现有方法如BLIP和DBCNN虽有进展，但与最先进标准（SOTA）存在显著差距，导致评估结果与人类感知不符。作者提出AAPMT框架，通过创新的提示设计（prompt designs）和Metric Transformer结构来评估关键参数，包括感知质量、真实性和文本-图像对应度。Metric Transformer借鉴AGI质量指标间的复杂关系，旨在提供更精确的评估模型，从而提升AGI评估的可靠性和准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19101v1",
      "published_date": "2024-03-28 02:31:06 UTC",
      "updated_date": "2024-03-28 02:31:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:57:32.096591"
    },
    {
      "arxiv_id": "2403.19093v1",
      "title": "Task2Morph: Differentiable Task-inspired Framework for Contact-Aware Robot Design",
      "title_zh": "Task2Morph：可微任务启发框架用于接触感知机器人设计",
      "authors": [
        "Yishuai Cai",
        "Shaowu Yang",
        "Minglong Li",
        "Xinglin Chen",
        "Yunxin Mao",
        "Xiaodong Yi",
        "Wenjing Yang"
      ],
      "abstract": "Optimizing the morphologies and the controllers that adapt to various tasks\nis a critical issue in the field of robot design, aka. embodied intelligence.\nPrevious works typically model it as a joint optimization problem and use\nsearch-based methods to find the optimal solution in the morphology space.\nHowever, they ignore the implicit knowledge of task-to-morphology mapping which\ncan directly inspire robot design. For example, flipping heavier boxes tends to\nrequire more muscular robot arms. This paper proposes a novel and general\ndifferentiable task-inspired framework for contact-aware robot design called\nTask2Morph. We abstract task features highly related to task performance and\nuse them to build a task-to-morphology mapping. Further, we embed the mapping\ninto a differentiable robot design process, where the gradient information is\nleveraged for both the mapping learning and the whole optimization. The\nexperiments are conducted on three scenarios, and the results validate that\nTask2Morph outperforms DiffHand, which lacks a task-inspired morphology module,\nin terms of efficiency and effectiveness.",
      "tldr_zh": "这篇论文提出了 Task2Morph，一个可微分的任务启发框架，用于处理接触感知（Contact-Aware）机器人设计的优化问题，旨在利用任务特征构建任务到形态（task-to-morphology mapping）的隐式知识。框架通过抽象与任务性能相关的特征，并将其嵌入可微分优化过程，利用梯度信息同时学习映射和整体优化，从而超越传统的 search-based methods。实验结果显示，在三个场景中，Task2Morph 在效率和效果上优于缺乏任务启发模块的 DiffHand。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 10 figures, published to IROS",
      "pdf_url": "http://arxiv.org/pdf/2403.19093v1",
      "published_date": "2024-03-28 02:02:00 UTC",
      "updated_date": "2024-03-28 02:02:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:57:44.216674"
    },
    {
      "arxiv_id": "2403.19083v1",
      "title": "Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep Learning: A Bayesian Deep Learning Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Pei Xi",
        "Lin"
      ],
      "abstract": "With recent advancements in the development of artificial intelligence\napplications using theories and algorithms in machine learning, many accurate\nmodels can be created to train and predict on given datasets. With the\nrealization of the importance of imaging interpretation in cancer diagnosis,\nthis article aims to investigate the theory behind Deep Learning and Bayesian\nNetwork prediction models. Based on the advantages and drawbacks of each model,\ndifferent approaches will be used to construct a Bayesian Deep Learning Model,\ncombining the strengths while minimizing the weaknesses. Finally, the\napplications and accuracy of the resulting Bayesian Deep Learning approach in\nthe health industry in classifying images will be analyzed.",
      "tldr_zh": "这篇论文旨在通过结合Bayesian Networks和Deep Learning来改善癌症影像诊断，调查了两种模型的理论基础及其优缺点。作者构建了一个Bayesian Deep Learning方法，将Deep Learning的强大预测能力与Bayesian Networks的不确定性处理优势相结合，以最小化弱点。最终，分析显示该方法在健康行业的图像分类任务中具有较高的准确性，提供了一种更可靠的AI应用途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19083v1",
      "published_date": "2024-03-28 01:27:10 UTC",
      "updated_date": "2024-03-28 01:27:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:57:55.460924"
    },
    {
      "arxiv_id": "2403.19082v1",
      "title": "Enhancing Conformal Prediction Using E-Test Statistics",
      "title_zh": "翻译失败",
      "authors": [
        "A. A. Balinsky",
        "A. D. Balinsky"
      ],
      "abstract": "Conformal Prediction (CP) serves as a robust framework that quantifies\nuncertainty in predictions made by Machine Learning (ML) models. Unlike\ntraditional point predictors, CP generates statistically valid prediction\nregions, also known as prediction intervals, based on the assumption of data\nexchangeability. Typically, the construction of conformal predictions hinges on\np-values. This paper, however, ventures down an alternative path, harnessing\nthe power of e-test statistics to augment the efficacy of conformal predictions\nby introducing a BB-predictor (bounded from the below predictor).",
      "tldr_zh": "Conformal Prediction (CP) 是一种量化机器学习 (ML) 模型预测不确定性的鲁棒框架，通过数据可交换性假设生成统计有效的预测区间。传统CP依赖于p值，但本文引入e-test statistics作为替代方法，开发了BB-predictor（bounded from the below predictor）来增强预测的有效性。这种方法为构建更精确和可靠的预测模型提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.19082v1",
      "published_date": "2024-03-28 01:14:25 UTC",
      "updated_date": "2024-03-28 01:14:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:58:07.782611"
    },
    {
      "arxiv_id": "2403.19078v1",
      "title": "MVEB: Self-Supervised Learning with Multi-View Entropy Bottleneck",
      "title_zh": "翻译失败",
      "authors": [
        "Liangjian Wen",
        "Xiasi Wang",
        "Jianzhuang Liu",
        "Zenglin Xu"
      ],
      "abstract": "Self-supervised learning aims to learn representation that can be effectively\ngeneralized to downstream tasks. Many self-supervised approaches regard two\nviews of an image as both the input and the self-supervised signals, assuming\nthat either view contains the same task-relevant information and the shared\ninformation is (approximately) sufficient for predicting downstream tasks.\nRecent studies show that discarding superfluous information not shared between\nthe views can improve generalization. Hence, the ideal representation is\nsufficient for downstream tasks and contains minimal superfluous information,\ntermed minimal sufficient representation. One can learn this representation by\nmaximizing the mutual information between the representation and the supervised\nview while eliminating superfluous information. Nevertheless, the computation\nof mutual information is notoriously intractable. In this work, we propose an\nobjective termed multi-view entropy bottleneck (MVEB) to learn minimal\nsufficient representation effectively. MVEB simplifies the minimal sufficient\nlearning to maximizing both the agreement between the embeddings of two views\nand the differential entropy of the embedding distribution. Our experiments\nconfirm that MVEB significantly improves performance. For example, it achieves\ntop-1 accuracy of 76.9\\% on ImageNet with a vanilla ResNet-50 backbone on\nlinear evaluation. To the best of our knowledge, this is the new\nstate-of-the-art result with ResNet-50.",
      "tldr_zh": "本研究针对自监督学习（Self-Supervised Learning）中的问题，提出了一种名为 Multi-View Entropy Bottleneck (MVEB) 的方法，以学习最小充分表示（Minimal Sufficient Representation），即下游任务所需的信息，同时去除冗余信息。MVEB 通过最大化两个图像视图嵌入之间的协议和嵌入分布的微分熵（Differential Entropy），简化了相互信息（Mutual Information）的计算过程，从而提升模型的泛化性能。在实验中，MVEB 在 ImageNet 数据集上使用 ResNet-50 骨干网络，在线性评估中达到了 76.9% 的 top-1 准确率，这创下了该网络的新状态-of-the-art 记录。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by TPAMI",
      "pdf_url": "http://arxiv.org/pdf/2403.19078v1",
      "published_date": "2024-03-28 00:50:02 UTC",
      "updated_date": "2024-03-28 00:50:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:58:21.927845"
    },
    {
      "arxiv_id": "2403.19076v2",
      "title": "Tiny Machine Learning: Progress and Futures",
      "title_zh": "翻译失败",
      "authors": [
        "Ji Lin",
        "Ligeng Zhu",
        "Wei-Ming Chen",
        "Wei-Chen Wang",
        "Song Han"
      ],
      "abstract": "Tiny Machine Learning (TinyML) is a new frontier of machine learning. By\nsqueezing deep learning models into billions of IoT devices and\nmicrocontrollers (MCUs), we expand the scope of AI applications and enable\nubiquitous intelligence. However, TinyML is challenging due to hardware\nconstraints: the tiny memory resource makes it difficult to hold deep learning\nmodels designed for cloud and mobile platforms. There is also limited compiler\nand inference engine support for bare-metal devices. Therefore, we need to\nco-design the algorithm and system stack to enable TinyML. In this review, we\nwill first discuss the definition, challenges, and applications of TinyML. We\nthen survey the recent progress in TinyML and deep learning on MCUs. Next, we\nwill introduce MCUNet, showing how we can achieve ImageNet-scale AI\napplications on IoT devices with system-algorithm co-design. We will further\nextend the solution from inference to training and introduce tiny on-device\ntraining techniques. Finally, we present future directions in this area.\nToday's large model might be tomorrow's tiny model. The scope of TinyML should\nevolve and adapt over time.",
      "tldr_zh": "TinyML 将深度学习模型压缩到数以亿计的 IoT 设备和微控制器 (MCUs) 上，实现 ubiquitous intelligence，但面临硬件约束（如内存限制）和编译器、推理引擎支持不足的挑战。论文回顾了 TinyML 的定义、应用和最新进展，强调通过算法与系统栈的共同设计来克服这些问题。作者介绍了 MCUNet，这是一种系统-算法共同设计方法，能够在 IoT 设备上实现 ImageNet-scale AI 应用，并扩展到 on-device training 技术。最后，论文展望未来方向，认为 TinyML 的范围将随着模型演变而持续适应。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2206.15472",
      "pdf_url": "http://arxiv.org/pdf/2403.19076v2",
      "published_date": "2024-03-28 00:34:56 UTC",
      "updated_date": "2024-03-29 21:33:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:58:33.714029"
    },
    {
      "arxiv_id": "2403.19073v1",
      "title": "Dataflow-Aware PIM-Enabled Manycore Architecture for Deep Learning Workloads",
      "title_zh": "翻译失败",
      "authors": [
        "Harsh Sharma",
        "Gaurav Narang",
        "Janardhan Rao Doppa",
        "Umit Ogras",
        "Partha Pratim Pande"
      ],
      "abstract": "Processing-in-memory (PIM) has emerged as an enabler for the energy-efficient\nand high-performance acceleration of deep learning (DL) workloads. Resistive\nrandom-access memory (ReRAM) is one of the most promising technologies to\nimplement PIM. However, as the complexity of Deep convolutional neural networks\n(DNNs) grows, we need to design a manycore architecture with multiple\nReRAM-based processing elements (PEs) on a single chip. Existing PIM-based\narchitectures mostly focus on computation while ignoring the role of\ncommunication. ReRAM-based tiled manycore architectures often involve many\nProcessing Elements (PEs), which need to be interconnected via an efficient\non-chip communication infrastructure. Simply allocating more resources (ReRAMs)\nto speed up only computation is ineffective if the communication infrastructure\ncannot keep up with it. In this paper, we highlight the design principles of a\ndataflow-aware PIM-enabled manycore platform tailor-made for various types of\nDL workloads. We consider the design challenges with both 2.5D interposer- and\n3D integration-enabled architectures.",
      "tldr_zh": "该论文提出了一种数据流感知的 Processing-in-Memory (PIM) 启用多核架构，旨在高效加速深度学习 (DL) 工作负载。不同于现有架构仅关注计算，该设计强调高效的芯片间通信基础设施，以避免通信瓶颈影响性能，利用 Resistive Random-Access Memory (ReRAM) 作为 Processing Elements (PEs)。通过整合数据流优化和多核体系，该架构针对各种 DL 工作负载进行了定制设计，并讨论了 2.5D interposer 和 3D 集成等挑战，为高性能、低能耗的 DL 加速提供了关键原则。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.AR",
      "comment": "Presented at DATE Conference, Valencia, Spain 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.19073v1",
      "published_date": "2024-03-28 00:29:15 UTC",
      "updated_date": "2024-03-28 00:29:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:58:44.313800"
    },
    {
      "arxiv_id": "2403.19066v1",
      "title": "Generative Quanta Color Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Vishal Purohit",
        "Junjie Luo",
        "Yiheng Chi",
        "Qi Guo",
        "Stanley H. Chan",
        "Qiang Qiu"
      ],
      "abstract": "The astonishing development of single-photon cameras has created an\nunprecedented opportunity for scientific and industrial imaging. However, the\nhigh data throughput generated by these 1-bit sensors creates a significant\nbottleneck for low-power applications. In this paper, we explore the\npossibility of generating a color image from a single binary frame of a\nsingle-photon camera. We evidently find this problem being particularly\ndifficult to standard colorization approaches due to the substantial degree of\nexposure variation. The core innovation of our paper is an exposure synthesis\nmodel framed under a neural ordinary differential equation (Neural ODE) that\nallows us to generate a continuum of exposures from a single observation. This\ninnovation ensures consistent exposure in binary images that colorizers take\non, resulting in notably enhanced colorization. We demonstrate applications of\nthe method in single-image and burst colorization and show superior generative\nperformance over baselines. Project website can be found at\nhttps://vishal-s-p.github.io/projects/2023/generative_quanta_color.html.",
      "tldr_zh": "本研究探讨了从单光子相机(single-photon camera)的单个二进制帧生成彩色图像，以解决高数据吞吐量和曝光变异带来的瓶颈问题。核心创新是基于神经普通微分方程(Neural ODE)的曝光合成模型，能够从单一观察生成连续曝光，从而确保二进制图像的曝光一致性并显著提升着色效果。该方法在单图像和突发(burst)着色应用中表现出色，比基线模型具有优越的生成性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.19066v1",
      "published_date": "2024-03-28 00:11:12 UTC",
      "updated_date": "2024-03-28 00:11:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T19:58:56.381681"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 88,
  "processed_papers_count": 88,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T19:59:21.015585"
}
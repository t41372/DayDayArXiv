[
  {
    "arxiv_id": "2403.19883v1",
    "title": "Policy-Space Search: Equivalences, Improvements, and Compression",
    "authors": [
      "Frederico Messa",
      "André Grahl Pereira"
    ],
    "abstract": "Fully-observable non-deterministic (FOND) planning is at the core of\nartificial intelligence planning with uncertainty. It models uncertainty\nthrough actions with non-deterministic effects. A* with Non-Determinism (AND*)\n(Messa and Pereira, 2023) is a FOND planner that generalizes A* (Hart et al.,\n1968) for FOND planning. It searches for a solution policy by performing an\nexplicit heuristic search on the policy space of the FOND task. In this paper,\nwe study and improve the performance of the policy-space search performed by\nAND*. We present a polynomial-time procedure that constructs a solution policy\ngiven just the set of states that should be mapped. This procedure, together\nwith a better understanding of the structure of FOND policies, allows us to\npresent three concepts of equivalences between policies. We use policy\nequivalences to prune part of the policy search space, making AND*\nsubstantially more effective in solving FOND tasks. We also study the impact of\ntaking into account structural state-space symmetries to strengthen the\ndetection of equivalence policies and the impact of performing the search with\nsatisficing techniques. We apply a recent technique from the group theory\nliterature to better compute structural state-space symmetries. Finally, we\npresent a solution compressor that, given a policy defined over complete\nstates, finds a policy that unambiguously represents it using the minimum\nnumber of partial states. AND* with the introduced techniques generates, on\naverage, two orders of magnitude fewer policies to solve FOND tasks. These\ntechniques allow explicit policy-space search to be competitive in terms of\nboth coverage and solution compactness with other state-of-the-art FOND\nplanners.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19883v1",
    "published_date": "2024-03-28 23:40:20 UTC",
    "updated_date": "2024-03-28 23:40:20 UTC"
  },
  {
    "arxiv_id": "2403.19881v1",
    "title": "IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion",
    "authors": [
      "Jiapu Wang",
      "Zheng Cui",
      "Boyue Wang",
      "Shirui Pan",
      "Junbin Gao",
      "Baocai Yin",
      "Wen Gao"
    ],
    "abstract": "Temporal Knowledge Graphs (TKGs) incorporate a temporal dimension, allowing\nfor a precise capture of the evolution of knowledge and reflecting the dynamic\nnature of the real world. Typically, TKGs contain complex geometric structures,\nwith various geometric structures interwoven. However, existing Temporal\nKnowledge Graph Completion (TKGC) methods either model TKGs in a single space\nor neglect the heterogeneity of different curvature spaces, thus constraining\ntheir capacity to capture these intricate geometric structures. In this paper,\nwe propose a novel Integrating Multi-curvature shared and specific Embedding\n(IME) model for TKGC tasks. Concretely, IME models TKGs into multi-curvature\nspaces, including hyperspherical, hyperbolic, and Euclidean spaces.\nSubsequently, IME incorporates two key properties, namely space-shared property\nand space-specific property. The space-shared property facilitates the learning\nof commonalities across different curvature spaces and alleviates the spatial\ngap caused by the heterogeneous nature of multi-curvature spaces, while the\nspace-specific property captures characteristic features. Meanwhile, IME\nproposes an Adjustable Multi-curvature Pooling (AMP) approach to effectively\nretain important information. Furthermore, IME innovatively designs similarity,\ndifference, and structure loss functions to attain the stated objective.\nExperimental results clearly demonstrate the superior performance of IME over\nexisting state-of-the-art TKGC models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19881v1",
    "published_date": "2024-03-28 23:31:25 UTC",
    "updated_date": "2024-03-28 23:31:25 UTC"
  },
  {
    "arxiv_id": "2403.19871v5",
    "title": "Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences",
    "authors": [
      "Dimitris Bertsimas",
      "Vassilis Digalakis Jr",
      "Yu Ma",
      "Phevos Paschalidis"
    ],
    "abstract": "We consider the problem of retraining machine learning (ML) models when new\nbatches of data become available. Existing approaches greedily optimize for\npredictive power independently at each batch, without considering the stability\nof the model's structure or analytical insights across retraining iterations.\nWe propose a model-agnostic framework for finding sequences of models that are\nstable across retraining iterations. We develop a mixed-integer optimization\nformulation that is guaranteed to recover Pareto optimal models (in terms of\nthe predictive power-stability trade-off) with good generalization properties,\nas well as an efficient polynomial-time algorithm that performs well in\npractice. We focus on retaining consistent analytical insights-which is\nimportant to model interpretability, ease of implementation, and fostering\ntrust with users-by using custom-defined distance metrics that can be directly\nincorporated into the optimization problem. We evaluate our framework across\nmodels (regression, decision trees, boosted trees, and neural networks) and\napplication domains (healthcare, vision, and language), including deployment in\na production pipeline at a major US hospital. We find that, on average, a 2%\nreduction in predictive power leads to a 30% improvement in stability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19871v5",
    "published_date": "2024-03-28 22:45:38 UTC",
    "updated_date": "2025-02-04 12:25:48 UTC"
  },
  {
    "arxiv_id": "2403.19867v4",
    "title": "Constructing Decision Trees from Data Streams",
    "authors": [
      "Huy Pham",
      "Hoang Ta",
      "Hoa T. Vu"
    ],
    "abstract": "In this work, we present data stream algorithms to compute optimal splits for\ndecision tree learning. In particular, given a data stream of observations\n\\(x_i\\) and their corresponding labels \\(y_i\\), without the i.i.d. assumption,\nthe objective is to identify the optimal split \\(j\\) that partitions the data\ninto two sets, minimizing the mean squared error (for regression) or the\nmisclassification rate and Gini impurity (for classification). We propose\nseveral efficient streaming algorithms that require sublinear space and use a\nsmall number of passes to solve these problems. These algorithms can also be\nextended to the MapReduce model. Our results, while not directly comparable,\ncomplements the seminal work of Domingos-Hulten (KDD 2000) and\nHulten-Spencer-Domingos (KDD 2001).",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DS",
    "comment": "To appear at ISIT 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.19867v4",
    "published_date": "2024-03-28 22:26:38 UTC",
    "updated_date": "2025-04-16 21:09:26 UTC"
  },
  {
    "arxiv_id": "2403.19866v2",
    "title": "Is Synthetic Image Useful for Transfer Learning? An Investigation into Data Generation, Volume, and Utilization",
    "authors": [
      "Yuhang Li",
      "Xin Dong",
      "Chen Chen",
      "Jingtao Li",
      "Yuxin Wen",
      "Michael Spranger",
      "Lingjuan Lyu"
    ],
    "abstract": "Synthetic image data generation represents a promising avenue for training\ndeep learning models, particularly in the realm of transfer learning, where\nobtaining real images within a specific domain can be prohibitively expensive\ndue to privacy and intellectual property considerations. This work delves into\nthe generation and utilization of synthetic images derived from text-to-image\ngenerative models in facilitating transfer learning paradigms. Despite the high\nvisual fidelity of the generated images, we observe that their naive\nincorporation into existing real-image datasets does not consistently enhance\nmodel performance due to the inherent distribution gap between synthetic and\nreal images. To address this issue, we introduce a novel two-stage framework\ncalled bridged transfer, which initially employs synthetic images for\nfine-tuning a pre-trained model to improve its transferability and subsequently\nuses real data for rapid adaptation. Alongside, We propose dataset style\ninversion strategy to improve the stylistic alignment between synthetic and\nreal images. Our proposed methods are evaluated across 10 different datasets\nand 5 distinct models, demonstrating consistent improvements, with up to 30%\naccuracy increase on classification tasks. Intriguingly, we note that the\nenhancements were not yet saturated, indicating that the benefits may further\nincrease with an expanded volume of synthetic data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR24 Score 6865 https://openreview.net/forum?id=CjPt1AC6w0",
    "pdf_url": "http://arxiv.org/pdf/2403.19866v2",
    "published_date": "2024-03-28 22:25:05 UTC",
    "updated_date": "2024-04-02 22:41:53 UTC"
  },
  {
    "arxiv_id": "2403.19857v1",
    "title": "LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces",
    "authors": [
      "Xiaomin Ouyang",
      "Mani Srivastava"
    ],
    "abstract": "Most studies on machine learning in sensing systems focus on low-level\nperception tasks that process raw sensory data within a short time window.\nHowever, many practical applications, such as human routine modeling and\noccupancy tracking, require high-level reasoning abilities to comprehend\nconcepts and make inferences based on long-term sensor traces. Existing machine\nlearning-based approaches for handling such complex tasks struggle to\ngeneralize due to the limited training samples and the high dimensionality of\nsensor traces, necessitating the integration of human knowledge for designing\nfirst-principle models or logic reasoning methods. We pose a fundamental\nquestion: Can we harness the reasoning capabilities and world knowledge of\nLarge Language Models (LLMs) to recognize complex events from long-term\nspatiotemporal sensor traces? To answer this question, we design an effective\nprompting framework for LLMs on high-level reasoning tasks, which can handle\ntraces from the raw sensor data as well as the low-level perception results. We\nalso design two strategies to enhance performance with long sensor traces,\nincluding summarization before reasoning and selective inclusion of historical\ntraces. Our framework can be implemented in an edge-cloud setup, running small\nLLMs on the edge for data summarization and performing high-level reasoning on\nthe cloud for privacy preservation. The results show that LLMSense can achieve\nover 80\\% accuracy on two high-level reasoning tasks such as dementia diagnosis\nwith behavior traces and occupancy tracking with environmental sensor traces.\nThis paper provides a few insights and guidelines for leveraging LLM for\nhigh-level reasoning on sensor traces and highlights several directions for\nfuture work.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.19857v1",
    "published_date": "2024-03-28 22:06:04 UTC",
    "updated_date": "2024-03-28 22:06:04 UTC"
  },
  {
    "arxiv_id": "2403.19856v1",
    "title": "Towards a Brazilian History Knowledge Graph",
    "authors": [
      "Valeria de Paiva",
      "Alexandre Rademaker"
    ],
    "abstract": "This short paper describes the first steps in a project to construct a\nknowledge graph for Brazilian history based on the Brazilian Dictionary of\nHistorical Biographies (DHBB) and Wikipedia/Wikidata. We contend that large\nrepositories of Brazilian-named entities (people, places, organizations, and\npolitical events and movements) would be beneficial for extracting information\nfrom Portuguese texts. We show that many of the terms/entities described in the\nDHBB do not have corresponding concepts (or Q items) in Wikidata, the largest\nstructured database of entities associated with Wikipedia. We describe previous\nwork on extracting information from the DHBB and outline the steps to construct\na Wikidata-based historical knowledge graph.",
    "categories": [
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19856v1",
    "published_date": "2024-03-28 22:05:32 UTC",
    "updated_date": "2024-03-28 22:05:32 UTC"
  },
  {
    "arxiv_id": "2403.19839v1",
    "title": "The New Agronomists: Language Models are Experts in Crop Management",
    "authors": [
      "Jing Wu",
      "Zhixin Lai",
      "Suiyao Chen",
      "Ran Tao",
      "Pan Zhao",
      "Naira Hovakimyan"
    ],
    "abstract": "Crop management plays a crucial role in determining crop yield, economic\nprofitability, and environmental sustainability. Despite the availability of\nmanagement guidelines, optimizing these practices remains a complex and\nmultifaceted challenge. In response, previous studies have explored using\nreinforcement learning with crop simulators, typically employing simple\nneural-network-based reinforcement learning (RL) agents. Building on this\nfoundation, this paper introduces a more advanced intelligent crop management\nsystem. This system uniquely combines RL, a language model (LM), and crop\nsimulations facilitated by the Decision Support System for Agrotechnology\nTransfer (DSSAT). We utilize deep RL, specifically a deep Q-network, to train\nmanagement policies that process numerous state variables from the simulator as\nobservations. A novel aspect of our approach is the conversion of these state\nvariables into more informative language, facilitating the language model's\ncapacity to understand states and explore optimal management practices. The\nempirical results reveal that the LM exhibits superior learning capabilities.\nThrough simulation experiments with maize crops in Florida (US) and Zaragoza\n(Spain), the LM not only achieves state-of-the-art performance under various\nevaluation metrics but also demonstrates a remarkable improvement of over 49\\%\nin economic profit, coupled with reduced environmental impact when compared to\nbaseline methods. Our code is available at\n\\url{https://github.com/jingwu6/LM_AG}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19839v1",
    "published_date": "2024-03-28 21:20:27 UTC",
    "updated_date": "2024-03-28 21:20:27 UTC"
  },
  {
    "arxiv_id": "2403.19838v2",
    "title": "Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",
    "authors": [
      "Akshay Gopalkrishnan",
      "Ross Greer",
      "Mohan Trivedi"
    ],
    "abstract": "Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have\nbecome prominent in autonomous driving research, as these models can provide\ninterpretable textual reasoning and responses for end-to-end autonomous driving\nsafety tasks using traffic scene images and other data modalities. However,\ncurrent approaches to these systems use expensive large language model (LLM)\nbackbones and image encoders, making such systems unsuitable for real-time\nautonomous driving systems where tight memory constraints exist and fast\ninference time is necessary. To address these previous issues, we develop\nEM-VLM4AD, an efficient, lightweight, multi-frame vision language model which\nperforms Visual Question Answering for autonomous driving. In comparison to\nprevious approaches, EM-VLM4AD requires at least 10 times less memory and\nfloating point operations, while also achieving higher CIDEr and ROUGE-L scores\nthan the existing baseline on the DriveLM dataset. EM-VLM4AD also exhibits the\nability to extract relevant information from traffic views related to prompts\nand can answer questions for various autonomous driving subtasks. We release\nour code to train and evaluate our model at\nhttps://github.com/akshaygopalkr/EM-VLM4AD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 3 figures, Accepted at CVPR 2024 Vision and Language for\n  Autonomous Driving and Robotics Workshop",
    "pdf_url": "http://arxiv.org/pdf/2403.19838v2",
    "published_date": "2024-03-28 21:18:33 UTC",
    "updated_date": "2024-05-09 03:27:44 UTC"
  },
  {
    "arxiv_id": "2403.19837v3",
    "title": "Concept-based Analysis of Neural Networks via Vision-Language Models",
    "authors": [
      "Ravi Mangal",
      "Nina Narodytska",
      "Divya Gopinath",
      "Boyue Caroline Hu",
      "Anirban Roy",
      "Susmit Jha",
      "Corina Pasareanu"
    ],
    "abstract": "The analysis of vision-based deep neural networks (DNNs) is highly desirable\nbut it is very challenging due to the difficulty of expressing formal\nspecifications for vision tasks and the lack of efficient verification\nprocedures. In this paper, we propose to leverage emerging multimodal,\nvision-language, foundation models (VLMs) as a lens through which we can reason\nabout vision models. VLMs have been trained on a large body of images\naccompanied by their textual description, and are thus implicitly aware of\nhigh-level, human-understandable concepts describing the images. We describe a\nlogical specification language $\\texttt{Con}_{\\texttt{spec}}$ designed to\nfacilitate writing specifications in terms of these concepts. To define and\nformally check $\\texttt{Con}_{\\texttt{spec}}$ specifications, we build a map\nbetween the internal representations of a given vision model and a VLM, leading\nto an efficient verification procedure of natural-language properties for\nvision models. We demonstrate our techniques on a ResNet-based classifier\ntrained on the RIVAL-10 dataset using CLIP as the multimodal model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19837v3",
    "published_date": "2024-03-28 21:15:38 UTC",
    "updated_date": "2024-04-10 23:47:34 UTC"
  },
  {
    "arxiv_id": "2403.19833v2",
    "title": "ChatTracer: Large Language Model Powered Real-time Bluetooth Device Tracking System",
    "authors": [
      "Qijun Wang",
      "Shichen Zhang",
      "Kunzhe Song",
      "Huacheng Zeng"
    ],
    "abstract": "Large language models (LLMs) have transformed the way we interact with cyber\ntechnologies. In this paper, we study the possibility of connecting LLM with\nwireless sensor networks (WSN). A successful design will not only extend LLM's\nknowledge landscape to the physical world but also revolutionize human\ninteraction with WSN. To the end, we present ChatTracer, an LLM-powered\nreal-time Bluetooth device tracking system. ChatTracer comprises three key\ncomponents: an array of Bluetooth sniffing nodes, a database, and a fine-tuned\nLLM. ChatTracer was designed based on our experimental observation that\ncommercial Apple/Android devices always broadcast hundreds of BLE packets per\nminute even in their idle status. Its novelties lie in two aspects: i) a\nreliable and efficient BLE packet grouping algorithm; and ii) an LLM\nfine-tuning strategy that combines both supervised fine-tuning (SFT) and\nreinforcement learning with human feedback (RLHF). We have built a prototype of\nChatTracer with four sniffing nodes. Experimental results show that ChatTracer\nnot only outperforms existing localization approaches, but also provides an\nintelligent interface for user interaction.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19833v2",
    "published_date": "2024-03-28 21:04:11 UTC",
    "updated_date": "2024-07-09 12:56:01 UTC"
  },
  {
    "arxiv_id": "2403.19826v2",
    "title": "Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic Segmentation",
    "authors": [
      "Qitian Ma",
      "Shyam Nanda Rai",
      "Carlo Masone",
      "Tatiana Tommasi"
    ],
    "abstract": "In the domain of computer vision, semantic segmentation emerges as a\nfundamental application within machine learning, wherein individual pixels of\nan image are classified into distinct semantic categories. This task transcends\ntraditional accuracy metrics by incorporating uncertainty quantification, a\ncritical measure for assessing the reliability of each segmentation prediction.\nSuch quantification is instrumental in facilitating informed decision-making,\nparticularly in applications where precision is paramount. Within this nuanced\nframework, the metric known as PAvPU (Patch Accuracy versus Patch Uncertainty)\nhas been developed as a specialized tool for evaluating entropy-based\nuncertainty in image segmentation tasks. However, our investigation identifies\nthree core deficiencies within the PAvPU framework and proposes robust\nsolutions aimed at refining the metric. By addressing these issues, we aim to\nenhance the reliability and applicability of uncertainty quantification,\nespecially in scenarios that demand high levels of safety and accuracy, thus\ncontributing to the advancement of semantic segmentation methodologies in\ncritical applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Premature Submission: accidentally submitted before it was ready",
    "pdf_url": "http://arxiv.org/pdf/2403.19826v2",
    "published_date": "2024-03-28 20:34:02 UTC",
    "updated_date": "2024-04-08 14:55:53 UTC"
  },
  {
    "arxiv_id": "2403.19822v1",
    "title": "Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition",
    "authors": [
      "Yash Jain",
      "David Chan",
      "Pranav Dheram",
      "Aparna Khare",
      "Olabanji Shonibare",
      "Venkatesh Ravichandran",
      "Shalini Ghosh"
    ],
    "abstract": "Recent advances in machine learning have demonstrated that multi-modal\npre-training can improve automatic speech recognition (ASR) performance\ncompared to randomly initialized models, even when models are fine-tuned on\nuni-modal tasks. Existing multi-modal pre-training methods for the ASR task\nhave primarily focused on single-stage pre-training where a single unsupervised\ntask is used for pre-training followed by fine-tuning on the downstream task.\nIn this work, we introduce a novel method combining multi-modal and multi-task\nunsupervised pre-training with a translation-based supervised mid-training\napproach. We empirically demonstrate that such a multi-stage approach leads to\nrelative word error rate (WER) improvements of up to 38.45% over baselines on\nboth Librispeech and SUPERB. Additionally, we share several important findings\nfor choosing pre-training methods and datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in LREC-COLING 2024 - The 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation",
    "pdf_url": "http://arxiv.org/pdf/2403.19822v1",
    "published_date": "2024-03-28 20:23:39 UTC",
    "updated_date": "2024-03-28 20:23:39 UTC"
  },
  {
    "arxiv_id": "2403.19820v1",
    "title": "Evaluating Explanatory Capabilities of Machine Learning Models in Medical Diagnostics: A Human-in-the-Loop Approach",
    "authors": [
      "José Bobes-Bascarán",
      "Eduardo Mosqueira-Rey",
      "Ángel Fernández-Leal",
      "Elena Hernández-Pereira",
      "David Alonso-Ríos",
      "Vicente Moret-Bonillo",
      "Israel Figueirido-Arnoso",
      "Yolanda Vidal-Ínsua"
    ],
    "abstract": "This paper presents a comprehensive study on the evaluation of explanatory\ncapabilities of machine learning models, with a focus on Decision Trees, Random\nForest and XGBoost models using a pancreatic cancer dataset. We use\nHuman-in-the-Loop related techniques and medical guidelines as a source of\ndomain knowledge to establish the importance of the different features that are\nrelevant to establish a pancreatic cancer treatment. These features are not\nonly used as a dimensionality reduction approach for the machine learning\nmodels, but also as way to evaluate the explainability capabilities of the\ndifferent models using agnostic and non-agnostic explainability techniques. To\nfacilitate interpretation of explanatory results, we propose the use of\nsimilarity measures such as the Weighted Jaccard Similarity coefficient. The\ngoal is to not only select the best performing model but also the one that can\nbest explain its conclusions and aligns with human domain knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19820v1",
    "published_date": "2024-03-28 20:11:34 UTC",
    "updated_date": "2024-03-28 20:11:34 UTC"
  },
  {
    "arxiv_id": "2403.19802v1",
    "title": "Developing Healthcare Language Model Embedding Spaces",
    "authors": [
      "Niall Taylor",
      "Dan Schofield",
      "Andrey Kormilitzin",
      "Dan W Joyce",
      "Alejo Nevado-Holgado"
    ],
    "abstract": "Pre-trained Large Language Models (LLMs) often struggle on out-of-domain\ndatasets like healthcare focused text. We explore specialized pre-training to\nadapt smaller LLMs to different healthcare datasets. Three methods are\nassessed: traditional masked language modeling, Deep Contrastive Learning for\nUnsupervised Textual Representations (DeCLUTR), and a novel pre-training\nobjective utilizing metadata categories from the healthcare settings. These\nschemes are evaluated on downstream document classification tasks for each\ndataset, with additional analysis of the resultant embedding spaces.\nContrastively trained models outperform other approaches on the classification\ntasks, delivering strong performance from limited labeled data and with fewer\nmodel parameter updates required. While metadata-based pre-training does not\nfurther improve classifications across the datasets, it yields interesting\nembedding cluster separability. All domain adapted LLMs outperform their\npublicly available general base LLM, validating the importance of\ndomain-specialization. This research illustrates efficient approaches to\ninstill healthcare competency in compact LLMs even under tight computational\nbudgets, an essential capability for responsible and sustainable deployment in\nlocal healthcare settings. We provide pre-training guidelines for specialized\nhealthcare LLMs, motivate continued inquiry into contrastive objectives, and\ndemonstrates adaptation techniques to align small LLMs with privacy-sensitive\nmedical tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19802v1",
    "published_date": "2024-03-28 19:31:32 UTC",
    "updated_date": "2024-03-28 19:31:32 UTC"
  },
  {
    "arxiv_id": "2403.19800v2",
    "title": "Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction",
    "authors": [
      "Jhon A. Castro-Correa",
      "Jhony H. Giraldo",
      "Mohsen Badiey",
      "Fragkiskos D. Malliaros"
    ],
    "abstract": "Reconstructing time-varying graph signals (or graph time-series imputation)\nis a critical problem in machine learning and signal processing with broad\napplications, ranging from missing data imputation in sensor networks to\ntime-series forecasting. Accurately capturing the spatio-temporal information\ninherent in these signals is crucial for effectively addressing these tasks.\nHowever, existing approaches relying on smoothness assumptions of temporal\ndifferences and simple convex optimization techniques have inherent\nlimitations. To address these challenges, we propose a novel approach that\nincorporates a learning module to enhance the accuracy of the downstream task.\nTo this end, we introduce the Gegenbauer-based graph convolutional (GegenConv)\noperator, which is a generalization of the conventional Chebyshev graph\nconvolution by leveraging the theory of Gegenbauer polynomials. By deviating\nfrom traditional convex problems, we expand the complexity of the model and\noffer a more accurate solution for recovering time-varying graph signals.\nBuilding upon GegenConv, we design the Gegenbauer-based time Graph Neural\nNetwork (GegenGNN) architecture, which adopts an encoder-decoder structure.\nLikewise, our approach also utilizes a dedicated loss function that\nincorporates a mean squared error component alongside Sobolev smoothness\nregularization. This combination enables GegenGNN to capture both the fidelity\nto ground truth and the underlying smoothness properties of the signals,\nenhancing the reconstruction performance. We conduct extensive experiments on\nreal datasets to evaluate the effectiveness of our proposed approach. The\nexperimental results demonstrate that GegenGNN outperforms state-of-the-art\nmethods, showcasing its superior capability in recovering time-varying graph\nsignals.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS)",
    "pdf_url": "http://arxiv.org/pdf/2403.19800v2",
    "published_date": "2024-03-28 19:29:17 UTC",
    "updated_date": "2024-04-03 13:49:23 UTC"
  },
  {
    "arxiv_id": "2403.19792v1",
    "title": "MAPL: Model Agnostic Peer-to-peer Learning",
    "authors": [
      "Sayak Mukherjee",
      "Andrea Simonetto",
      "Hadi Jamali-Rad"
    ],
    "abstract": "Effective collaboration among heterogeneous clients in a decentralized\nsetting is a rather unexplored avenue in the literature. To structurally\naddress this, we introduce Model Agnostic Peer-to-peer Learning (coined as\nMAPL) a novel approach to simultaneously learn heterogeneous personalized\nmodels as well as a collaboration graph through peer-to-peer communication\namong neighboring clients. MAPL is comprised of two main modules: (i)\nlocal-level Personalized Model Learning (PML), leveraging a combination of\nintra- and inter-client contrastive losses; (ii) network-wide decentralized\nCollaborative Graph Learning (CGL) dynamically refining collaboration weights\nin a privacy-preserving manner based on local task similarities. Our extensive\nexperimentation demonstrates the efficacy of MAPL and its competitive (or, in\nmost cases, superior) performance compared to its centralized model-agnostic\ncounterparts, without relying on any central server. Our code is available and\ncan be accessed here: https://github.com/SayakMukherjee/MAPL",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Our code is available and can be accessed here:\n  https://github.com/SayakMukherjee/MAPL",
    "pdf_url": "http://arxiv.org/pdf/2403.19792v1",
    "published_date": "2024-03-28 19:17:54 UTC",
    "updated_date": "2024-03-28 19:17:54 UTC"
  },
  {
    "arxiv_id": "2403.19790v1",
    "title": "Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care",
    "authors": [
      "Niall Taylor",
      "Andrey Kormilitzin",
      "Isabelle Lorge",
      "Alejo Nevado-Holgado",
      "Dan W Joyce"
    ],
    "abstract": "Contemporary large language models (LLMs) may have utility for processing\nunstructured, narrative free-text clinical data contained in electronic health\nrecords (EHRs) -- a particularly important use-case for mental health where a\nmajority of routinely-collected patient data lacks structured, machine-readable\ncontent.\n  A significant problem for the the United Kingdom's National Health Service\n(NHS) are the long waiting lists for specialist mental healthcare. According to\nNHS data, in each month of 2023, there were between 370,000 and 470,000\nindividual new referrals into secondary mental healthcare services. Referrals\nmust be triaged by clinicians, using clinical information contained in the\npatient's EHR to arrive at a decision about the most appropriate mental\nhealthcare team to assess and potentially treat these patients.\n  The ability to efficiently recommend a relevant team by ingesting potentially\nvoluminous clinical notes could help services both reduce referral waiting\ntimes and with the right technology, improve the evidence available to justify\ntriage decisions.\n  We present and evaluate three different approaches for LLM-based, end-to-end\ningestion of variable-length clinical EHR data to assist clinicians when\ntriaging referrals. Our model is able to deliver triage recommendations\nconsistent with existing clinical practices and it's architecture was\nimplemented on a single GPU, making it practical for implementation in\nresource-limited NHS environments where private implementations of LLM\ntechnology will be necessary to ensure confidential clinical data is\nappropriately controlled and governed.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19790v1",
    "published_date": "2024-03-28 19:17:07 UTC",
    "updated_date": "2024-03-28 19:17:07 UTC"
  },
  {
    "arxiv_id": "2403.19770v1",
    "title": "Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks",
    "authors": [
      "Mingyu Cai",
      "Karankumar Patel",
      "Soshi Iba",
      "Songpo Li"
    ],
    "abstract": "In human-robot collaboration, shared control presents an opportunity to\nteleoperate robotic manipulation to improve the efficiency of manufacturing and\nassembly processes. Robots are expected to assist in executing the user's\nintentions. To this end, robust and prompt intention estimation is needed,\nrelying on behavioral observations. The framework presents an intention\nestimation technique at hierarchical levels i.e., low-level actions and\nhigh-level tasks, by incorporating multi-scale hierarchical information in\nneural networks. Technically, we employ hierarchical dependency loss to boost\noverall accuracy. Furthermore, we propose a multi-window method that assigns\nproper hierarchical prediction windows of input data. An analysis of the\npredictive power with various inputs demonstrates the predominance of the deep\nhierarchical model in the sense of prediction accuracy and early intention\nidentification. We implement the algorithm on a virtual reality (VR) setup to\nteleoperate robotic hands in a simulation with various assembly tasks to show\nthe effectiveness of online estimation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "ICRA 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19770v1",
    "published_date": "2024-03-28 18:45:43 UTC",
    "updated_date": "2024-03-28 18:45:43 UTC"
  },
  {
    "arxiv_id": "2403.19760v1",
    "title": "Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies",
    "authors": [
      "Benjamin Kraske",
      "Zakariya Laouar",
      "Zachary Sunberg"
    ],
    "abstract": "As humans come to rely on autonomous systems more, ensuring the transparency\nof such systems is important to their continued adoption. Explainable\nArtificial Intelligence (XAI) aims to reduce confusion and foster trust in\nsystems by providing explanations of agent behavior. Partially observable\nMarkov decision processes (POMDPs) provide a flexible framework capable of\nreasoning over transition and state uncertainty, while also being amenable to\nexplanation. This work investigates the use of user-provided counterfactuals to\ngenerate contrastive explanations of POMDP policies. Feature expectations are\nused as a means of contrasting the performance of these policies. We\ndemonstrate our approach in a Search and Rescue (SAR) setting. We analyze and\ndiscuss the associated challenges through two case studies.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2403.19760v1",
    "published_date": "2024-03-28 18:19:38 UTC",
    "updated_date": "2024-03-28 18:19:38 UTC"
  },
  {
    "arxiv_id": "2403.19758v2",
    "title": "Quantum Natural Language Processing",
    "authors": [
      "Dominic Widdows",
      "Willie Aboumrad",
      "Dohun Kim",
      "Sayonee Ray",
      "Jonathan Mei"
    ],
    "abstract": "Language processing is at the heart of current developments in artificial\nintelligence, and quantum computers are becoming available at the same time.\nThis has led to great interest in quantum natural language processing, and\nseveral early proposals and experiments.\n  This paper surveys the state of this area, showing how NLP-related techniques\nhave been used in quantum language processing. We examine the art of word\nembeddings and sequential models, proposing some avenues for future\ninvestigation and discussing the tradeoffs present in these directions. We also\nhighlight some recent methods to compute attention in transformer models, and\nperform grammatical parsing. We also introduce a new quantum design for the\nbasic task of text encoding (representing a string of characters in memory),\nwhich has not been addressed in detail before.\n  Quantum theory has contributed toward quantifying uncertainty and explaining\n\"What is intelligence?\" In this context, we argue that \"hallucinations\" in\nmodern artificial intelligence systems are a misunderstanding of the way facts\nare conceptualized: language can express many plausible hypotheses, of which\nonly a few become actual.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19758v2",
    "published_date": "2024-03-28 18:15:07 UTC",
    "updated_date": "2024-04-26 18:45:02 UTC"
  },
  {
    "arxiv_id": "2403.19652v1",
    "title": "InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction",
    "authors": [
      "Sirui Xu",
      "Ziyin Wang",
      "Yu-Xiong Wang",
      "Liang-Yan Gui"
    ],
    "abstract": "Text-conditioned human motion generation has experienced significant\nadvancements with diffusion models trained on extensive motion capture data and\ncorresponding textual annotations. However, extending such success to 3D\ndynamic human-object interaction (HOI) generation faces notable challenges,\nprimarily due to the lack of large-scale interaction data and comprehensive\ndescriptions that align with these interactions. This paper takes the\ninitiative and showcases the potential of generating human-object interactions\nwithout direct training on text-interaction pair data. Our key insight in\nachieving this is that interaction semantics and dynamics can be decoupled.\nBeing unable to learn interaction semantics through supervised training, we\ninstead leverage pre-trained large models, synergizing knowledge from a large\nlanguage model and a text-to-motion model. While such knowledge offers\nhigh-level control over interaction semantics, it cannot grasp the intricacies\nof low-level interaction dynamics. To overcome this issue, we further introduce\na world model designed to comprehend simple physics, modeling how human actions\ninfluence object motion. By integrating these components, our novel framework,\nInterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot\nmanner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our\ncomprehensive experimental analysis demonstrates its capability to generate\nrealistic and coherent interaction sequences that seamlessly align with the\ntext directives.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://sirui-xu.github.io/InterDreamer/",
    "pdf_url": "http://arxiv.org/pdf/2403.19652v1",
    "published_date": "2024-03-28 17:59:30 UTC",
    "updated_date": "2024-03-28 17:59:30 UTC"
  },
  {
    "arxiv_id": "2403.19651v2",
    "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
    "authors": [
      "Kai Zhang",
      "Yi Luan",
      "Hexiang Hu",
      "Kenton Lee",
      "Siyuan Qiao",
      "Wenhu Chen",
      "Yu Su",
      "Ming-Wei Chang"
    ],
    "abstract": "Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent works leverage text\ninstructions to allow users to more freely express their search intents.\nHowever, they primarily focus on image pairs that are visually similar and/or\ncan be characterized by a small set of pre-defined relations. The core thesis\nof this paper is that text instructions can enable retrieving images with\nricher relations beyond visual similarity. To show this, we introduce\nMagicLens, a series of self-supervised image retrieval models that support\nopen-ended instructions. MagicLens is built on a key novel insight: image pairs\nthat naturally occur on the same web pages contain a wide range of implicit\nrelations (e.g., inside view of), and we can bring those implicit relations\nexplicit by synthesizing instructions via foundation models. Trained on 36.7M\n(query image, instruction, target image) triplets with rich semantic relations\nmined from the web, MagicLens achieves results comparable with or better than\nprior best on eight benchmarks of various image retrieval tasks, while\nmaintaining high parameter efficiency with a significantly smaller model size.\nAdditional human analyses on a 1.4M-image unseen corpus further demonstrate the\ndiversity of search intents supported by MagicLens. Code and models are\npublicly available at https://open-vision-language.github.io/MagicLens/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "ICML 2024 (Oral); Project Website:\n  https://open-vision-language.github.io/MagicLens/",
    "pdf_url": "http://arxiv.org/pdf/2403.19651v2",
    "published_date": "2024-03-28 17:59:20 UTC",
    "updated_date": "2024-06-24 23:41:29 UTC"
  },
  {
    "arxiv_id": "2403.19648v2",
    "title": "Human-compatible driving partners through data-regularized self-play reinforcement learning",
    "authors": [
      "Daphne Cornelisse",
      "Eugene Vinitsky"
    ],
    "abstract": "A central challenge for autonomous vehicles is coordinating with humans.\nTherefore, incorporating realistic human agents is essential for scalable\ntraining and evaluation of autonomous driving systems in simulation. Simulation\nagents are typically developed by imitating large-scale, high-quality datasets\nof human driving. However, pure imitation learning agents empirically have high\ncollision rates when executed in a multi-agent closed-loop setting. To build\nagents that are realistic and effective in closed-loop settings, we propose\nHuman-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are\ntrained through self-play with a small penalty for deviating from a human\nreference policy. In contrast to prior work, our approach is RL-first and only\nuses 30 minutes of imperfect human demonstrations. We evaluate agents in a\nlarge set of multi-agent traffic scenes. Results show our HR-PPO agents are\nhighly effective in achieving goals, with a success rate of 93%, an off-road\nrate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in\na human-like manner, as measured by their similarity to existing human driving\nlogs. We also find that HR-PPO agents show considerable improvements on proxy\nmeasures for coordination with human driving, particularly in highly\ninteractive scenarios. We open-source our code and trained agents at\nhttps://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent\nbehaviors at https://sites.google.com/view/driving-partners.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19648v2",
    "published_date": "2024-03-28 17:56:56 UTC",
    "updated_date": "2024-06-22 23:47:54 UTC"
  },
  {
    "arxiv_id": "2403.19647v3",
    "title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models",
    "authors": [
      "Samuel Marks",
      "Can Rager",
      "Eric J. Michaud",
      "Yonatan Belinkov",
      "David Bau",
      "Aaron Mueller"
    ],
    "abstract": "We introduce methods for discovering and applying sparse feature circuits.\nThese are causally implicated subnetworks of human-interpretable features for\nexplaining language model behaviors. Circuits identified in prior work consist\nof polysemantic and difficult-to-interpret units like attention heads or\nneurons, rendering them unsuitable for many downstream applications. In\ncontrast, sparse feature circuits enable detailed understanding of\nunanticipated mechanisms. Because they are based on fine-grained units, sparse\nfeature circuits are useful for downstream tasks: We introduce SHIFT, where we\nimprove the generalization of a classifier by ablating features that a human\njudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised\nand scalable interpretability pipeline by discovering thousands of sparse\nfeature circuits for automatically discovered model behaviors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Code and data at https://github.com/saprmarks/feature-circuits.\n  Demonstration at https://feature-circuits.xyz",
    "pdf_url": "http://arxiv.org/pdf/2403.19647v3",
    "published_date": "2024-03-28 17:56:07 UTC",
    "updated_date": "2025-03-27 05:44:45 UTC"
  },
  {
    "arxiv_id": "2403.19631v2",
    "title": "Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering",
    "authors": [
      "Yucheng Shi",
      "Qiaoyu Tan",
      "Xuansheng Wu",
      "Shaochen Zhong",
      "Kaixiong Zhou",
      "Ninghao Liu"
    ],
    "abstract": "Large Language Models (LLMs) have shown proficiency in question-answering\ntasks but often struggle to integrate real-time knowledge, leading to\npotentially outdated or inaccurate responses. This problem becomes even more\nchallenging when dealing with multi-hop questions, since they require LLMs to\nupdate and integrate multiple knowledge pieces relevant to the questions. To\ntackle the problem, we propose the Retrieval-Augmented model Editing (RAE)\nframework for multi-hop question answering. RAE first retrieves edited facts\nand then refines the language model through in-context learning. Specifically,\nour retrieval approach, based on mutual information maximization, leverages the\nreasoning abilities of LLMs to identify chain facts that traditional\nsimilarity-based searches might miss. In addition, our framework includes a\npruning strategy to eliminate redundant information from the retrieved facts,\nwhich enhances the editing accuracy and mitigates the hallucination problem.\nOur framework is supported by theoretical justification for its fact retrieval\nefficacy. Finally, comprehensive evaluation across various LLMs validates RAE's\nability in providing accurate answers with updated knowledge. Our code is\navailable at: https://github.com/sycny/RAE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19631v2",
    "published_date": "2024-03-28 17:47:19 UTC",
    "updated_date": "2024-08-13 19:34:13 UTC"
  },
  {
    "arxiv_id": "2403.19620v1",
    "title": "Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models",
    "authors": [
      "Ole Hall",
      "Anil Yaman"
    ],
    "abstract": "Generative Adversarial Networks (GANs) have shown great success in generating\nhigh quality images and are thus used as one of the main approaches to generate\nart images. However, usually the image generation process involves sampling\nfrom the latent space of the learned art representations, allowing little\ncontrol over the output. In this work, we first employ GANs that are trained to\nproduce creative images using an architecture known as Creative Adversarial\nNetworks (CANs), then, we employ an evolutionary approach to navigate within\nthe latent space of the models to discover images. We use automatic aesthetic\nand collaborative interactive human evaluation metrics to assess the generated\nimages. In the human interactive evaluation case, we propose a collaborative\nevaluation based on the assessments of several participants. Furthermore, we\nalso experiment with an intelligent mutation operator that aims to improve the\nquality of the images through local search based on an aesthetic measure. We\nevaluate the effectiveness of this approach by comparing the results produced\nby the automatic and collaborative interactive evolution. The results show that\nthe proposed approach can generate highly attractive art images when the\nevolution is guided by collaborative human feedback.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Preprint. The Version of Record of this contribution is to be\n  published in the proceedings of the 13th International Conference on\n  Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19620v1",
    "published_date": "2024-03-28 17:40:15 UTC",
    "updated_date": "2024-03-28 17:40:15 UTC"
  },
  {
    "arxiv_id": "2403.19603v1",
    "title": "Semantic Map-based Generation of Navigation Instructions",
    "authors": [
      "Chengzu Li",
      "Chao Zhang",
      "Simone Teufel",
      "Rama Sanand Doddipatla",
      "Svetlana Stoyanchev"
    ],
    "abstract": "We are interested in the generation of navigation instructions, either in\ntheir own right or as training material for robotic navigation task. In this\npaper, we propose a new approach to navigation instruction generation by\nframing the problem as an image captioning task using semantic maps as visual\ninput. Conventional approaches employ a sequence of panorama images to generate\nnavigation instructions. Semantic maps abstract away from visual details and\nfuse the information in multiple panorama images into a single top-down\nrepresentation, thereby reducing computational complexity to process the input.\nWe present a benchmark dataset for instruction generation using semantic maps,\npropose an initial model and ask human subjects to manually assess the quality\nof generated instructions. Our initial investigations show promise in using\nsemantic maps for instruction generation instead of a sequence of panorama\nimages, but there is vast scope for improvement. We release the code for data\npreparation and model training at https://github.com/chengzu-li/VLGen.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 2 figures, 3 tables (13 pages, 3 figures, 5 tables including\n  references and appendices), accepted at LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19603v1",
    "published_date": "2024-03-28 17:27:44 UTC",
    "updated_date": "2024-03-28 17:27:44 UTC"
  },
  {
    "arxiv_id": "2403.19595v1",
    "title": "Situation Awareness for Driver-Centric Driving Style Adaptation",
    "authors": [
      "Johann Haselberger",
      "Bonifaz Stuhr",
      "Bernhard Schick",
      "Steffen Müller"
    ],
    "abstract": "There is evidence that the driving style of an autonomous vehicle is\nimportant to increase the acceptance and trust of the passengers. The driving\nsituation has been found to have a significant influence on human driving\nbehavior. However, current driving style models only partially incorporate\ndriving environment information, limiting the alignment between an agent and\nthe given situation. Therefore, we propose a situation-aware driving style\nmodel based on different visual feature encoders pretrained on fleet data, as\nwell as driving behavior predictors, which are adapted to the driving style of\na specific driver. Our experiments show that the proposed method outperforms\nstatic driving styles significantly and forms plausible situation clusters.\nFurthermore, we found that feature encoders pretrained on our dataset lead to\nmore precise driving behavior modeling. In contrast, feature encoders\npretrained supervised and unsupervised on different data sources lead to more\nspecific situation clusters, which can be utilized to constrain and control the\ndriving style adaptation for specific situations. Moreover, in a real-world\nsetting, where driving style adaptation is happening iteratively, we found the\nMLP-based behavior predictors achieve good performance initially but suffer\nfrom catastrophic forgetting. In contrast, behavior predictors based on\nsituationdependent statistics can learn iteratively from continuous data\nstreams by design. Overall, our experiments show that important information for\ndriving behavior prediction is contained within the visual feature encoder. The\ndataset is publicly available at\nhuggingface.co/datasets/jHaselberger/SADC-Situation-Awareness-for-Driver-Centric-Driving-Style-Adaptation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication",
    "pdf_url": "http://arxiv.org/pdf/2403.19595v1",
    "published_date": "2024-03-28 17:19:16 UTC",
    "updated_date": "2024-03-28 17:19:16 UTC"
  },
  {
    "arxiv_id": "2403.19584v1",
    "title": "Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation",
    "authors": [
      "Zhongliang Zhou",
      "Jielu Zhang",
      "Zihan Guan",
      "Mengxuan Hu",
      "Ni Lao",
      "Lan Mu",
      "Sheng Li",
      "Gengchen Mai"
    ],
    "abstract": "Geolocating precise locations from images presents a challenging problem in\ncomputer vision and information retrieval.Traditional methods typically employ\neither classification, which dividing the Earth surface into grid cells and\nclassifying images accordingly, or retrieval, which identifying locations by\nmatching images with a database of image-location pairs. However,\nclassification-based approaches are limited by the cell size and cannot yield\nprecise predictions, while retrieval-based systems usually suffer from poor\nsearch quality and inadequate coverage of the global landscape at varied scale\nand aggregation levels. To overcome these drawbacks, we present Img2Loc, a\nnovel system that redefines image geolocalization as a text generation task.\nThis is achieved using cutting-edge large multi-modality models like GPT4V or\nLLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based\nrepresentations to generate an image-based coordinate query database. It then\nuniquely combines query results with images itself, forming elaborate prompts\ncustomized for LMMs. When tested on benchmark datasets such as Im2GPS3k and\nYFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art\nmodels but does so without any model training.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19584v1",
    "published_date": "2024-03-28 17:07:02 UTC",
    "updated_date": "2024-03-28 17:07:02 UTC"
  },
  {
    "arxiv_id": "2403.19561v3",
    "title": "Self-Improved Learning for Scalable Neural Combinatorial Optimization",
    "authors": [
      "Fu Luo",
      "Xi Lin",
      "Zhenkun Wang",
      "Xialiang Tong",
      "Mingxuan Yuan",
      "Qingfu Zhang"
    ],
    "abstract": "The end-to-end neural combinatorial optimization (NCO) method shows promising\nperformance in solving complex combinatorial optimization problems without the\nneed for expert design. However, existing methods struggle with large-scale\nproblems, hindering their practical applicability. To overcome this limitation,\nthis work proposes a novel Self-Improved Learning (SIL) method for better\nscalability of neural combinatorial optimization. Specifically, we develop an\nefficient self-improved mechanism that enables direct model training on\nlarge-scale problem instances without any labeled data. Powered by an\ninnovative local reconstruction approach, this method can iteratively generate\nbetter solutions by itself as pseudo-labels to guide efficient model training.\nIn addition, we design a linear complexity attention mechanism for the model to\nefficiently handle large-scale combinatorial problem instances with low\ncomputation overhead. Comprehensive experiments on the Travelling Salesman\nProblem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to\n100K nodes in both uniform and real-world distributions demonstrate the\nsuperior scalability of our method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19561v3",
    "published_date": "2024-03-28 16:46:53 UTC",
    "updated_date": "2024-05-02 09:07:02 UTC"
  },
  {
    "arxiv_id": "2403.19546v3",
    "title": "Croissant: A Metadata Format for ML-Ready Datasets",
    "authors": [
      "Mubashara Akhtar",
      "Omar Benjelloun",
      "Costanza Conforti",
      "Luca Foschini",
      "Joan Giner-Miguelez",
      "Pieter Gijsbers",
      "Sujata Goswami",
      "Nitisha Jain",
      "Michalis Karamousadakis",
      "Michael Kuchnik",
      "Satyapriya Krishna",
      "Sylvain Lesage",
      "Quentin Lhoest",
      "Pierre Marcenac",
      "Manil Maskey",
      "Peter Mattson",
      "Luis Oala",
      "Hamidah Oderinwale",
      "Pierre Ruyssen",
      "Tim Santos",
      "Rajat Shinde",
      "Elena Simperl",
      "Arjun Suresh",
      "Goeffry Thomas",
      "Slava Tykhonov",
      "Joaquin Vanschoren",
      "Susheel Varma",
      "Jos van der Velde",
      "Steffen Vogler",
      "Carole-Jean Wu",
      "Luyao Zhang"
    ],
    "abstract": "Data is a critical resource for machine learning (ML), yet working with data\nremains a key friction point. This paper introduces Croissant, a metadata\nformat for datasets that creates a shared representation across ML tools,\nframeworks, and platforms. Croissant makes datasets more discoverable,\nportable, and interoperable, thereby addressing significant challenges in ML\ndata management. Croissant is already supported by several popular dataset\nrepositories, spanning hundreds of thousands of datasets, enabling easy loading\ninto the most commonly-used ML frameworks, regardless of where the data is\nstored. Our initial evaluation by human raters shows that Croissant metadata is\nreadable, understandable, complete, yet concise.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at the NeurIPS 2024 Datasets and Benchmark Track. A shorter\n  version appeared earlier in Proceedings of ACM SIGMOD/PODS'24 Data Management\n  for End-to-End Machine Learning (DEEM) Workshop\n  https://dl.acm.org/doi/10.1145/3650203.3663326",
    "pdf_url": "http://arxiv.org/pdf/2403.19546v3",
    "published_date": "2024-03-28 16:27:26 UTC",
    "updated_date": "2024-12-09 18:37:55 UTC"
  },
  {
    "arxiv_id": "2403.19545v1",
    "title": "Lamarckian Inheritance Improves Robot Evolution in Dynamic Environments",
    "authors": [
      "Jie Luo",
      "Karine Miras",
      "Carlo Longhi",
      "Oliver Weissl",
      "Agoston E. Eiben"
    ],
    "abstract": "This study explores the integration of Lamarckian system into evolutionary\nrobotics (ER), comparing it with the traditional Darwinian model across various\nenvironments. By adopting Lamarckian principles, where robots inherit learned\ntraits, alongside Darwinian learning without inheritance, we investigate\nadaptation in dynamic settings. Our research, conducted in six distinct\nenvironmental setups, demonstrates that Lamarckian systems outperform Darwinian\nones in adaptability and efficiency, particularly in challenging conditions.\nOur analysis highlights the critical role of the interplay between controller\n\\& morphological evolution and environment adaptation, with parent-offspring\nsimilarities and newborn \\&survivors before and after learning providing\ninsights into the effectiveness of trait inheritance. Our findings suggest\nLamarckian principles could significantly advance autonomous system design,\nhighlighting the potential for more adaptable and robust robotic solutions in\ncomplex, real-world applications. These theoretical insights were validated\nusing real physical robots, bridging the gap between simulation and practical\napplication.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Nature. arXiv admin note: substantial text overlap with\n  arXiv:2309.13099; text overlap with arXiv:2303.12594, arXiv:2309.14387",
    "pdf_url": "http://arxiv.org/pdf/2403.19545v1",
    "published_date": "2024-03-28 16:27:20 UTC",
    "updated_date": "2024-03-28 16:27:20 UTC"
  },
  {
    "arxiv_id": "2403.19521v4",
    "title": "Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models",
    "authors": [
      "Ang Lv",
      "Yuhan Chen",
      "Kaiyi Zhang",
      "Yulong Wang",
      "Lifeng Liu",
      "Ji-Rong Wen",
      "Jian Xie",
      "Rui Yan"
    ],
    "abstract": "In this paper, we delve into several mechanisms employed by Transformer-based\nlanguage models (LLMs) for factual recall tasks. We outline a pipeline\nconsisting of three major steps: (1) Given a prompt ``The capital of France\nis,'' task-specific attention heads extract the topic token, such as\n``France,'' from the context and pass it to subsequent MLPs. (2) As attention\nheads' outputs are aggregated with equal weight and added to the residual\nstream, the subsequent MLP acts as an ``activation,'' which either erases or\namplifies the information originating from individual heads. As a result, the\ntopic token ``France'' stands out in the residual stream. (3) A deep MLP takes\n``France'' and generates a component that redirects the residual stream towards\nthe direction of the correct answer, i.e., ``Paris.'' This procedure is akin to\napplying an implicit function such as ``get\\_capital($X$),'' and the argument\n$X$ is the topic token information passed by attention heads. To achieve the\nabove quantitative and qualitative analysis for MLPs, we proposed a novel\nanalytic method aimed at decomposing the outputs of the MLP into components\nunderstandable by humans. Additionally, we observed a universal\nanti-overconfidence mechanism in the final layer of models, which suppresses\ncorrect predictions. We mitigate this suppression by leveraging our\ninterpretation to improve factual recall confidence. The above interpretations\nare evaluated across diverse tasks spanning various domains of factual\nknowledge, using various language models from the GPT-2 families, 1.3B OPT, up\nto 7B Llama-2, and in both zero- and few-shot setups.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19521v4",
    "published_date": "2024-03-28 15:54:59 UTC",
    "updated_date": "2024-05-24 15:06:45 UTC"
  },
  {
    "arxiv_id": "2404.01322v1",
    "title": "A Review of Multi-Modal Large Language and Vision Models",
    "authors": [
      "Kilian Carolan",
      "Laura Fennelly",
      "Alan F. Smeaton"
    ],
    "abstract": "Large Language Models (LLMs) have recently emerged as a focal point of\nresearch and application, driven by their unprecedented ability to understand\nand generate text with human-like quality. Even more recently, LLMs have been\nextended into multi-modal large language models (MM-LLMs) which extends their\ncapabilities to deal with image, video and audio information, in addition to\ntext. This opens up applications like text-to-video generation, image\ncaptioning, text-to-speech, and more and is achieved either by retro-fitting an\nLLM with multi-modal capabilities, or building a MM-LLM from scratch. This\npaper provides an extensive review of the current state of those LLMs with\nmulti-modal capabilities as well as the very recent MM-LLMs. It covers the\nhistorical development of LLMs especially the advances enabled by\ntransformer-based architectures like OpenAI's GPT series and Google's BERT, as\nwell as the role of attention mechanisms in enhancing model performance. The\npaper includes coverage of the major and most important of the LLMs and MM-LLMs\nand also covers the techniques of model tuning, including fine-tuning and\nprompt engineering, which tailor pre-trained models to specific tasks or\ndomains. Ethical considerations and challenges, such as data bias and model\nmisuse, are also analysed to underscore the importance of responsible AI\ndevelopment and deployment. Finally, we discuss the implications of open-source\nversus proprietary models in AI research. Through this review, we provide\ninsights into the transformative potential of MM-LLMs in various applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "33 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2404.01322v1",
    "published_date": "2024-03-28 15:53:45 UTC",
    "updated_date": "2024-03-28 15:53:45 UTC"
  },
  {
    "arxiv_id": "2403.19736v1",
    "title": "Physics-Informed Neural Networks for Satellite State Estimation",
    "authors": [
      "Jacob Varey",
      "Jessica D. Ruprecht",
      "Michael Tierney",
      "Ryan Sullenberger"
    ],
    "abstract": "The Space Domain Awareness (SDA) community routinely tracks satellites in\norbit by fitting an orbital state to observations made by the Space\nSurveillance Network (SSN). In order to fit such orbits, an accurate model of\nthe forces that are acting on the satellite is required. Over the past several\ndecades, high-quality, physics-based models have been developed for satellite\nstate estimation and propagation. These models are exceedingly good at\nestimating and propagating orbital states for non-maneuvering satellites;\nhowever, there are several classes of anomalous accelerations that a satellite\nmight experience which are not well-modeled, such as satellites that use\nlow-thrust electric propulsion to modify their orbit. Physics-Informed Neural\nNetworks (PINNs) are a valuable tool for these classes of satellites as they\ncombine physics models with Deep Neural Networks (DNNs), which are highly\nexpressive and versatile function approximators. By combining a physics model\nwith a DNN, the machine learning model need not learn astrodynamics, which\nresults in more efficient and effective utilization of machine learning\nresources. This paper details the application of PINNs to estimate the orbital\nstate and a continuous, low-amplitude anomalous acceleration profile for\nsatellites. The PINN is trained to learn the unknown acceleration by minimizing\nthe mean square error of observations. We evaluate the performance of pure\nphysics models with PINNs in terms of their observation residuals and their\npropagation accuracy beyond the fit span of the observations. For a two-day\nsimulation of a GEO satellite using an unmodeled acceleration profile on the\norder of $10^{-8} \\text{ km/s}^2$, the PINN outperformed the best-fit physics\nmodel by orders of magnitude for both observation residuals (123 arcsec vs 1.00\narcsec) as well as propagation accuracy (3860 km vs 164 km after five days).",
    "categories": [
      "astro-ph.IM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19736v1",
    "published_date": "2024-03-28 14:54:57 UTC",
    "updated_date": "2024-03-28 14:54:57 UTC"
  },
  {
    "arxiv_id": "2403.19460v2",
    "title": "RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation",
    "authors": [
      "Chongkai Gao",
      "Zhengrong Xue",
      "Shuying Deng",
      "Tianhai Liang",
      "Siqi Yang",
      "Lin Shao",
      "Huazhe Xu"
    ],
    "abstract": "We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot\nManipulation imitation learning framework from scene point cloud input.\nCompared to previous methods that rely on descriptor field matching, RiEMann\ndirectly predicts the target poses of objects for manipulation without any\nobject segmentation. RiEMann learns a manipulation task from scratch with 5 to\n10 demonstrations, generalizes to unseen SE(3) transformations and instances of\ntarget objects, resists visual interference of distracting objects, and follows\nthe near real-time pose change of the target object. The scalable action space\nof RiEMann facilitates the addition of custom equivariant actions such as the\ndirection of turning the faucet, which makes articulated object manipulation\npossible for RiEMann. In simulation and real-world 6-DOF robot manipulation\nexperiments, we test RiEMann on 5 categories of manipulation tasks with a total\nof 25 variants and show that RiEMann outperforms baselines in both task success\nrates and SE(3) geodesic distance errors on predicted poses (reduced by 68.6%),\nand achieves a 5.4 frames per second (FPS) network inference speed. Code and\nvideo results are available at https://riemann-web.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19460v2",
    "published_date": "2024-03-28 14:31:10 UTC",
    "updated_date": "2024-10-03 11:13:29 UTC"
  },
  {
    "arxiv_id": "2403.19459v1",
    "title": "NeuroLGP-SM: A Surrogate-assisted Neuroevolution Approach using Linear Genetic Programming",
    "authors": [
      "Fergal Stapleton",
      "Brendan Cody-Kenny",
      "Edgar Galván"
    ],
    "abstract": "Evolutionary algorithms are increasingly recognised as a viable computational\napproach for the automated optimisation of deep neural networks (DNNs) within\nartificial intelligence. This method extends to the training of DNNs, an\napproach known as neuroevolution. However, neuroevolution is an inherently\nresource-intensive process, with certain studies reporting the consumption of\nthousands of GPU days for refining and training a single DNN network. To\naddress the computational challenges associated with neuroevolution while still\nattaining good DNN accuracy, surrogate models emerge as a pragmatic solution.\nDespite their potential, the integration of surrogate models into\nneuroevolution is still in its early stages, hindered by factors such as the\neffective use of high-dimensional data and the representation employed in\nneuroevolution. In this context, we address these challenges by employing a\nsuitable representation based on Linear Genetic Programming, denoted as\nNeuroLGP, and leveraging Kriging Partial Least Squares. The amalgamation of\nthese two techniques culminates in our proposed methodology known as the\nNeuroLGP-Surrogate Model (NeuroLGP-SM). For comparison purposes, we also code\nand use a baseline approach incorporating a repair mechanism, a common practice\nin neuroevolution. Notably, the baseline approach surpasses the renowned VGG-16\nmodel in accuracy. Given the computational intensity inherent in DNN\noperations, a singular run is typically the norm. To evaluate the efficacy of\nour proposed approach, we conducted 96 independent runs. Significantly, our\nmethodologies consistently outperform the baseline, with the SM model\ndemonstrating superior accuracy or comparable results to the NeuroLGP approach.\nNoteworthy is the additional advantage that the SM approach exhibits a 25%\nreduction in computational requirements, further emphasising its efficiency for\nneuroevolution.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted in \"International Conference on Optimization and Learning\n  (OLA), Dubrovnik, Croatia, 2024\", 13 pages, 4 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2403.19459v1",
    "published_date": "2024-03-28 14:31:01 UTC",
    "updated_date": "2024-03-28 14:31:01 UTC"
  },
  {
    "arxiv_id": "2403.19432v2",
    "title": "Uncovering Misattributed Suicide Causes through Annotation Inconsistency Detection in Death Investigation Notes",
    "authors": [
      "Song Wang",
      "Yiliang Zhou",
      "Ziqiang Han",
      "Cui Tao",
      "Yunyu Xiao",
      "Ying Ding",
      "Joydeep Ghosh",
      "Yifan Peng"
    ],
    "abstract": "Data accuracy is essential for scientific research and policy development.\nThe National Violent Death Reporting System (NVDRS) data is widely used for\ndiscovering the patterns and causes of death. Recent studies suggested the\nannotation inconsistencies within the NVDRS and the potential impact on\nerroneous suicide-cause attributions. We present an empirical Natural Language\nProcessing (NLP) approach to detect annotation inconsistencies and adopt a\ncross-validation-like paradigm to identify problematic instances. We analyzed\n267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our\nresults showed that incorporating the target state's data into training the\nsuicide-crisis classifier brought an increase of 5.4% to the F-1 score on the\ntarget state's test set and a decrease of 1.1% on other states' test set. To\nconclude, we demonstrated the annotation inconsistencies in NVDRS's death\ninvestigation notes, identified problematic instances, evaluated the\neffectiveness of correcting problematic instances, and eventually proposed an\nNLP improvement solution.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.19432v2",
    "published_date": "2024-03-28 14:03:12 UTC",
    "updated_date": "2024-03-29 17:21:02 UTC"
  },
  {
    "arxiv_id": "2403.19424v1",
    "title": "The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement",
    "authors": [
      "Jonathan Kamp",
      "Lisa Beinborn",
      "Antske Fokkens"
    ],
    "abstract": "Post-hoc explanation methods are an important tool for increasing model\ntransparency for users. Unfortunately, the currently used methods for\nattributing token importance often yield diverging patterns. In this work, we\nstudy potential sources of disagreement across methods from a linguistic\nperspective. We find that different methods systematically select different\nclasses of words and that methods that agree most with other methods and with\nhumans display similar linguistic preferences. Token-level differences between\nmethods are smoothed out if we compare them on the syntactic span level. We\nalso find higher agreement across methods by estimating the most important\nspans dynamically instead of relying on a fixed subset of size $k$. We\nsystematically investigate the interaction between $k$ and spans and propose an\nimproved configuration for selecting important tokens.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Long paper accepted to LREC-Coling 2024 main conference. Please cite\n  the conference proceedings version when available",
    "pdf_url": "http://arxiv.org/pdf/2403.19424v1",
    "published_date": "2024-03-28 13:56:23 UTC",
    "updated_date": "2024-03-28 13:56:23 UTC"
  },
  {
    "arxiv_id": "2403.19421v1",
    "title": "Scaling up ridge regression for brain encoding in a massive individual fMRI dataset",
    "authors": [
      "Sana Ahmadi",
      "Pierre Bellec",
      "Tristan Glatard"
    ],
    "abstract": "Brain encoding with neuroimaging data is an established analysis aimed at\npredicting human brain activity directly from complex stimuli features such as\nmovie frames. Typically, these features are the latent space representation\nfrom an artificial neural network, and the stimuli are image, audio, or text\ninputs. Ridge regression is a popular prediction model for brain encoding due\nto its good out-of-sample generalization performance. However, training a ridge\nregression model can be highly time-consuming when dealing with large-scale\ndeep functional magnetic resonance imaging (fMRI) datasets that include many\nspace-time samples of brain activity. This paper evaluates different\nparallelization techniques to reduce the training time of brain encoding with\nridge regression on the CNeuroMod Friends dataset, one of the largest deep fMRI\nresource currently available. With multi-threading, our results show that the\nIntel Math Kernel Library (MKL) significantly outperforms the OpenBLAS library,\nbeing 1.9 times faster using 32 threads on a single machine. We then evaluated\nthe Dask multi-CPU implementation of ridge regression readily available in\nscikit-learn (MultiOutput), and we proposed a new \"batch\" version of Dask\nparallelization, motivated by a time complexity analysis. In line with our\ntheoretical analysis, MultiOutput parallelization was found to be impractical,\ni.e., slower than multi-threading on a single machine. In contrast, the\nBatch-MultiOutput regression scaled well across compute nodes and threads,\nproviding speed-ups of up to 33 times with 8 compute nodes and 32 threads\ncompared to a single-threaded scikit-learn execution. Batch parallelization\nusing Dask thus emerges as a scalable approach for brain encoding with ridge\nregression on high-performance computing systems using scikit-learn and large\nfMRI datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19421v1",
    "published_date": "2024-03-28 13:52:12 UTC",
    "updated_date": "2024-03-28 13:52:12 UTC"
  },
  {
    "arxiv_id": "2403.19419v1",
    "title": "Fairness in Ranking: Robustness through Randomization without the Protected Attribute",
    "authors": [
      "Andrii Kliachkin",
      "Eleni Psaroudaki",
      "Jakub Marecek",
      "Dimitris Fotakis"
    ],
    "abstract": "There has been great interest in fairness in machine learning, especially in\nrelation to classification problems. In ranking-related problems, such as in\nonline advertising, recommender systems, and HR automation, much work on\nfairness remains to be done. Two complications arise: first, the protected\nattribute may not be available in many applications. Second, there are multiple\nmeasures of fairness of rankings, and optimization-based methods utilizing a\nsingle measure of fairness of rankings may produce rankings that are unfair\nwith respect to other measures. In this work, we propose a randomized method\nfor post-processing rankings, which do not require the availability of the\nprotected attribute. In an extensive numerical study, we show the robustness of\nour methods with respect to P-Fairness and effectiveness with respect to\nNormalized Discounted Cumulative Gain (NDCG) from the baseline ranking,\nimproving on previously proposed methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19419v1",
    "published_date": "2024-03-28 13:50:24 UTC",
    "updated_date": "2024-03-28 13:50:24 UTC"
  },
  {
    "arxiv_id": "2403.19405v1",
    "title": "Tabular Learning: Encoding for Entity and Context Embeddings",
    "authors": [
      "Fredy Reusser"
    ],
    "abstract": "Examining the effect of different encoding techniques on entity and context\nembeddings, the goal of this work is to challenge commonly used Ordinal\nencoding for tabular learning. Applying different preprocessing methods and\nnetwork architectures over several datasets resulted in a benchmark on how the\nencoders influence the learning outcome of the networks. By keeping the test,\nvalidation and training data consistent, results have shown that ordinal\nencoding is not the most suited encoder for categorical data in terms of\npreprocessing the data and thereafter, classifying the target variable\ncorrectly. A better outcome was achieved, encoding the features based on string\nsimilarities by computing a similarity matrix as input for the network. This is\nthe case for both, entity and context embeddings, where the transformer\narchitecture showed improved performance for Ordinal and Similarity encoding\nwith regard to multi-label classification tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19405v1",
    "published_date": "2024-03-28 13:29:29 UTC",
    "updated_date": "2024-03-28 13:29:29 UTC"
  },
  {
    "arxiv_id": "2406.16872v1",
    "title": "Multi-channel Time Series Decomposition Network For Generalizable Sensor-Based Activity Recognition",
    "authors": [
      "Jianguo Pan",
      "Zhengxin Hu",
      "Lingdun Zhang",
      "Xia Cai"
    ],
    "abstract": "Sensor-based human activity recognition is important in daily scenarios such\nas smart healthcare and homes due to its non-intrusive privacy and low cost\nadvantages, but the problem of out-of-domain generalization caused by\ndifferences in focusing individuals and operating environments can lead to\nsignificant accuracy degradation on cross-person behavior recognition due to\nthe inconsistent distributions of training and test data. To address the above\nproblems, this paper proposes a new method, Multi-channel Time Series\nDecomposition Network (MTSDNet). Firstly, MTSDNet decomposes the original\nsignal into a combination of multiple polynomials and trigonometric functions\nby the trainable parameterized temporal decomposition to learn the low-rank\nrepresentation of the original signal for improving the extraterritorial\ngeneralization ability of the model. Then, the different components obtained by\nthe decomposition are classified layer by layer and the layer attention is used\nto aggregate components to obtain the final classification result. Extensive\nevaluation on DSADS, OPPORTUNITY, PAMAP2, UCIHAR and UniMib public datasets\nshows the advantages in predicting accuracy and stability of our method\ncompared with other competing strategies, including the state-of-the-art ones.\nAnd the visualization is conducted to reveal MTSDNet's interpretability and\nlayer-by-layer characteristics.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16872v1",
    "published_date": "2024-03-28 12:54:06 UTC",
    "updated_date": "2024-03-28 12:54:06 UTC"
  },
  {
    "arxiv_id": "2403.19386v2",
    "title": "PointCloud-Text Matching: Benchmark Datasets and a Baseline",
    "authors": [
      "Yanglin Feng",
      "Yang Qin",
      "Dezhong Peng",
      "Hongyuan Zhu",
      "Xi Peng",
      "Peng Hu"
    ],
    "abstract": "In this paper, we present and study a new instance-level retrieval task:\nPointCloud-Text Matching~(PTM), which aims to find the exact cross-modal\ninstance that matches a given point-cloud query or text query. PTM could be\napplied to various scenarios, such as indoor/urban-canyon localization and\nscene retrieval. However, there exists no suitable and targeted dataset for PTM\nin practice. Therefore, we construct three new PTM benchmark datasets, namely\n3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with\nnoisy correspondence due to the sparsity, noise, or disorder of point clouds\nand the ambiguity, vagueness, or incompleteness of texts, which make existing\ncross-modal matching methods ineffective for PTM. To tackle these challenges,\nwe propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa).\nRoMa consists of two modules: a Dual Attention Perception module (DAP) and a\nRobust Negative Contrastive Learning module (RNCL). Specifically, DAP leverages\ntoken-level and feature-level attention to adaptively focus on useful local and\nglobal features, and aggregate them into common representations, thereby\nreducing the adverse impact of noise and ambiguity. To handle noisy\ncorrespondence, RNCL divides negative pairs, which are much less error-prone\nthan positive pairs, into clean and noisy subsets, and assigns them forward and\nreverse optimization directions respectively, thus enhancing robustness against\nnoisy correspondence. We conduct extensive experiments on our benchmarks and\ndemonstrate the superiority of our RoMa.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Upon further consideration, we have concluded that the current\n  version requires significant revision and may not yet be ready for\n  publication. We plan to conduct additional experiments and make the necessary\n  improvements to ensure the paper meets the standards for future submission",
    "pdf_url": "http://arxiv.org/pdf/2403.19386v2",
    "published_date": "2024-03-28 12:51:15 UTC",
    "updated_date": "2024-09-05 03:18:11 UTC"
  },
  {
    "arxiv_id": "2403.19376v3",
    "title": "NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data",
    "authors": [
      "Matteo Caligiuri",
      "Adriano Simonetto",
      "Pietro Zanuttigh"
    ],
    "abstract": "The acquisition of objects outside the Line-of-Sight of cameras is a very\nintriguing but also extremely challenging research topic. Recent works showed\nthe feasibility of this idea exploiting transient imaging data produced by\ncustom direct Time of Flight sensors. In this paper, for the first time, we\ntackle this problem using only data from an off-the-shelf indirect Time of\nFlight sensor without any further hardware requirement. We introduced a Deep\nLearning model able to re-frame the surfaces where light bounces happen as a\nvirtual mirror. This modeling makes the task easier to handle and also\nfacilitates the construction of annotated training data. From the obtained data\nit is possible to retrieve the depth information of the hidden scene. We also\nprovide a first-in-its-kind synthetic dataset for the task and demonstrate the\nfeasibility of the proposed idea over it.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024 - MELEX workshop, 17 pages, 6 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.19376v3",
    "published_date": "2024-03-28 12:38:21 UTC",
    "updated_date": "2024-09-20 12:35:01 UTC"
  },
  {
    "arxiv_id": "2404.01320v1",
    "title": "Graph-Based Optimisation of Network Expansion in a Dockless Bike Sharing System",
    "authors": [
      "Mark Roantree",
      "Niamh Murphi",
      "Dinh Viet Cuong",
      "Vuong Minh Ngo"
    ],
    "abstract": "Bike-sharing systems (BSSs) are deployed in over a thousand cities worldwide\nand play an important role in many urban transportation systems. BSSs alleviate\ncongestion, reduce pollution and promote physical exercise. It is essential to\nexplore the spatiotemporal patterns of bike-sharing demand, as well as the\nfactors that influence these patterns, in order to optimise system operational\nefficiency. In this study, an optimised geo-temporal graph is constructed using\ntrip data from Moby Bikes, a dockless BSS operator. The process of optimising\nthe graph unveiled prime locations for erecting new stations during future\nexpansions of the BSS. The Louvain algorithm, a community detection technique,\nis employed to uncover usage patterns at different levels of temporal\ngranularity. The community detection results reveal largely self-contained\nsub-networks that exhibit similar usage patterns at their respective levels of\ntemporal granularity. Overall, this study reinforces that BSSs are\nintrinsically spatiotemporal systems, with community presence driven by\nspatiotemporal dynamics. These findings may aid operators in improving\nredistribution efficiency.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.SI",
    "comment": "Accepted to publish in The 2024 IEEE 40th International Conference on\n  Data Engineering Workshops (ICDEW&DASC-2024), pp. 1-8",
    "pdf_url": "http://arxiv.org/pdf/2404.01320v1",
    "published_date": "2024-03-28 12:29:25 UTC",
    "updated_date": "2024-03-28 12:29:25 UTC"
  },
  {
    "arxiv_id": "2403.19347v1",
    "title": "Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors",
    "authors": [
      "Binzong Geng",
      "Zhaoxin Huan",
      "Xiaolu Zhang",
      "Yong He",
      "Liang Zhang",
      "Fajie Yuan",
      "Jun Zhou",
      "Linjian Mo"
    ],
    "abstract": "With the rise of large language models (LLMs), recent works have leveraged\nLLMs to improve the performance of click-through rate (CTR) prediction.\nHowever, we argue that a critical obstacle remains in deploying LLMs for\npractical use: the efficiency of LLMs when processing long textual user\nbehaviors. As user sequences grow longer, the current efficiency of LLMs is\ninadequate for training on billions of users and items. To break through the\nefficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical\nEncoding (BAHE) to enhance the efficiency of LLM-based CTR modeling.\nSpecifically, BAHE proposes a novel hierarchical architecture that decouples\nthe encoding of user behaviors from inter-behavior interactions. Firstly, to\nprevent computational redundancy from repeated encoding of identical user\nbehaviors, BAHE employs the LLM's pre-trained shallow layers to extract\nembeddings of the most granular, atomic user behaviors from extensive user\nsequences and stores them in the offline database. Subsequently, the deeper,\ntrainable layers of the LLM facilitate intricate inter-behavior interactions,\nthereby generating comprehensive user embeddings. This separation allows the\nlearning of high-level user representations to be independent of low-level\nbehavior encoding, significantly reducing computational complexity. Finally,\nthese refined user embeddings, in conjunction with correspondingly processed\nitem embeddings, are incorporated into the CTR model to compute the CTR scores.\nExtensive experimental results show that BAHE reduces training time and memory\nby five times for CTR models using LLMs, especially with longer user sequences.\nBAHE has been deployed in a real-world system, allowing for daily updates of 50\nmillion CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR\nprediction.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by the 47th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval (SIGIR), 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19347v1",
    "published_date": "2024-03-28 12:05:15 UTC",
    "updated_date": "2024-03-28 12:05:15 UTC"
  },
  {
    "arxiv_id": "2403.19345v1",
    "title": "Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning",
    "authors": [
      "Kangming Xu",
      "Huiming Zhou",
      "Haotian Zheng",
      "Mingwei Zhu",
      "Qi Xin"
    ],
    "abstract": "With the rapid evolution of the Internet and the exponential proliferation of\ninformation, users encounter information overload and the conundrum of choice.\nPersonalized recommendation systems play a pivotal role in alleviating this\nburden by aiding users in filtering and selecting information tailored to their\npreferences and requirements. Such systems not only enhance user experience and\nsatisfaction but also furnish opportunities for businesses and platforms to\naugment user engagement, sales, and advertising efficacy.This paper undertakes\na comparative analysis between the operational mechanisms of traditional\ne-commerce commodity classification systems and personalized recommendation\nsystems. It delineates the significance and application of personalized\nrecommendation systems across e-commerce, content information, and media\ndomains. Furthermore, it delves into the challenges confronting personalized\nrecommendation systems in e-commerce, including data privacy, algorithmic bias,\nscalability, and the cold start problem. Strategies to address these challenges\nare elucidated.Subsequently, the paper outlines a personalized recommendation\nsystem leveraging the BERT model and nearest neighbor algorithm, specifically\ntailored to address the exigencies of the eBay e-commerce platform. The\nefficacy of this recommendation system is substantiated through manual\nevaluation, and a practical application operational guide and structured output\nrecommendation results are furnished to ensure the system's operability and\nscalability.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19345v1",
    "published_date": "2024-03-28 12:02:45 UTC",
    "updated_date": "2024-03-28 12:02:45 UTC"
  },
  {
    "arxiv_id": "2403.19340v2",
    "title": "Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models",
    "authors": [
      "Hyunbyung Park",
      "Sukyung Lee",
      "Gyoungjin Gim",
      "Yungi Kim",
      "Dahyun Kim",
      "Chanjun Park"
    ],
    "abstract": "To address the challenges associated with data processing at scale, we\npropose Dataverse, a unified open-source Extract-Transform-Load (ETL) pipeline\nfor large language models (LLMs) with a user-friendly design at its core. Easy\naddition of custom processors with block-based interface in Dataverse allows\nusers to readily and efficiently use Dataverse to build their own ETL pipeline.\nWe hope that Dataverse will serve as a vital tool for LLM development and open\nsource the entire library to welcome community contribution. Additionally, we\nprovide a concise, two-minute video demonstration of our system, illustrating\nits capabilities and implementation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 Demo",
    "pdf_url": "http://arxiv.org/pdf/2403.19340v2",
    "published_date": "2024-03-28 11:57:08 UTC",
    "updated_date": "2025-03-04 03:06:30 UTC"
  },
  {
    "arxiv_id": "2403.19336v1",
    "title": "IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation",
    "authors": [
      "Jiacui Huang",
      "Hongtao Zhang",
      "Mingbo Zhao",
      "Zhou Wu"
    ],
    "abstract": "Vision-and-Language Navigation (VLN) is a challenging task that requires a\nrobot to navigate in photo-realistic environments with human natural language\npromptings. Recent studies aim to handle this task by constructing the semantic\nspatial map representation of the environment, and then leveraging the strong\nability of reasoning in large language models for generalizing code for guiding\nthe robot navigation. However, these methods face limitations in instance-level\nand attribute-level navigation tasks as they cannot distinguish different\ninstances of the same object. To address this challenge, we propose a new\nmethod, namely, Instance-aware Visual Language Map (IVLMap), to empower the\nrobot with instance-level and attribute-level semantic mapping, where it is\nautonomously constructed by fusing the RGBD video data collected from the robot\nagent with special-designed natural language map indexing in the bird's-in-eye\nview. Such indexing is instance-level and attribute-level. In particular, when\nintegrated with a large language model, IVLMap demonstrates the capability to\ni) transform natural language into navigation targets with instance and\nattribute information, enabling precise localization, and ii) accomplish\nzero-shot end-to-end navigation tasks based on natural language commands.\nExtensive navigation experiments are conducted. Simulation results illustrate\nthat our method can achieve an average improvement of 14.4\\% in navigation\naccuracy. Code and demo are released at https://ivlmap.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19336v1",
    "published_date": "2024-03-28 11:52:42 UTC",
    "updated_date": "2024-03-28 11:52:42 UTC"
  },
  {
    "arxiv_id": "2403.19316v1",
    "title": "Hypergraph-based Multi-View Action Recognition using Event Cameras",
    "authors": [
      "Yue Gao",
      "Jiaxuan Lu",
      "Siqi Li",
      "Yipeng Li",
      "Shaoyi Du"
    ],
    "abstract": "Action recognition from video data forms a cornerstone with wide-ranging\napplications. Single-view action recognition faces limitations due to its\nreliance on a single viewpoint. In contrast, multi-view approaches capture\ncomplementary information from various viewpoints for improved accuracy.\nRecently, event cameras have emerged as innovative bio-inspired sensors,\nleading to advancements in event-based action recognition. However, existing\nworks predominantly focus on single-view scenarios, leaving a gap in multi-view\nevent data exploitation, particularly in challenges like information deficit\nand semantic misalignment. To bridge this gap, we introduce HyperMV, a\nmulti-view event-based action recognition framework. HyperMV converts discrete\nevent data into frame-like representations and extracts view-related features\nusing a shared convolutional network. By treating segments as vertices and\nconstructing hyperedges using rule-based and KNN-based strategies, a multi-view\nhypergraph neural network that captures relationships across viewpoint and\ntemporal features is established. The vertex attention hypergraph propagation\nis also introduced for enhanced feature fusion. To prompt research in this\narea, we present the largest multi-view event-based action dataset\n$\\text{THU}^{\\text{MV-EACT}}\\text{-50}$, comprising 50 actions from 6\nviewpoints, which surpasses existing datasets by over tenfold. Experimental\nresults show that HyperMV significantly outperforms baselines in both\ncross-subject and cross-view scenarios, and also exceeds the state-of-the-arts\nin frame-based multi-view action recognition.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.19316v1",
    "published_date": "2024-03-28 11:17:00 UTC",
    "updated_date": "2024-03-28 11:17:00 UTC"
  },
  {
    "arxiv_id": "2403.19305v2",
    "title": "MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation",
    "authors": [
      "Yu Li",
      "Shenyu Zhang",
      "Rui Wu",
      "Xiutian Huang",
      "Yongrui Chen",
      "Wenhao Xu",
      "Guilin Qi",
      "Dehai Min"
    ],
    "abstract": "Recent advancements in generative Large Language Models(LLMs) have been\nremarkable, however, the quality of the text generated by these models often\nreveals persistent issues. Evaluating the quality of text generated by these\nmodels, especially in open-ended text, has consistently presented a significant\nchallenge. Addressing this, recent work has explored the possibility of using\nLLMs as evaluators. While using a single LLM as an evaluation agent shows\npotential, it is filled with significant uncertainty and instability. To\naddress these issues, we propose the MATEval: A \"Multi-Agent Text Evaluation\nframework\" where all agents are played by LLMs like GPT-4. The MATEval\nframework emulates human collaborative discussion methods, integrating multiple\nagents' interactions to evaluate open-ended text. Our framework incorporates\nself-reflection and Chain-of-Thought (CoT) strategies, along with feedback\nmechanisms, enhancing the depth and breadth of the evaluation process and\nguiding discussions towards consensus, while the framework generates\ncomprehensive evaluation reports, including error localization, error types and\nscoring. Experimental results show that our framework outperforms existing\nopen-ended text evaluation methods and achieves the highest correlation with\nhuman evaluation, which confirms the effectiveness and advancement of our\nframework in addressing the uncertainties and instabilities in evaluating\nLLMs-generated text. Furthermore, our framework significantly improves the\nefficiency of text evaluation and model iteration in industrial scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper has been accepted as a long paper presentation by DASFAA\n  2024 Industrial Track",
    "pdf_url": "http://arxiv.org/pdf/2403.19305v2",
    "published_date": "2024-03-28 10:41:47 UTC",
    "updated_date": "2024-04-15 14:21:52 UTC"
  },
  {
    "arxiv_id": "2403.19289v4",
    "title": "Uplift Modeling Under Limited Supervision",
    "authors": [
      "George Panagopoulos",
      "Daniele Malitesta",
      "Fragkiskos D. Malliaros",
      "Jun Pang"
    ],
    "abstract": "Estimating causal effects in e-commerce tends to involve costly treatment\nassignments which can be impractical in large-scale settings. Leveraging\nmachine learning to predict such treatment effects without actual intervention\nis a standard practice to diminish the risk. However, existing methods for\ntreatment effect prediction tend to rely on training sets of substantial size,\nwhich are built from real experiments and are thus inherently risky to create.\nIn this work we propose a graph neural network to diminish the required\ntraining set size, relying on graphs that are common in e-commerce data.\nSpecifically, we view the problem as node regression with a restricted number\nof labeled instances, develop a two-model neural architecture akin to previous\ncausal effect estimators, and test varying message-passing layers for encoding.\nFurthermore, as an extra step, we combine the model with an acquisition\nfunction to guide the creation of the training set in settings with extremely\nlow experimental budget. The framework is flexible since each step can be used\nseparately with other models or treatment policies. The experiments on real\nlarge-scale networks indicate a clear advantage of our methodology over the\nstate of the art, which in many cases performs close to random, underlining the\nneed for models that can generalize with limited supervision to reduce\nexperimental risks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19289v4",
    "published_date": "2024-03-28 10:19:36 UTC",
    "updated_date": "2024-09-02 20:21:29 UTC"
  },
  {
    "arxiv_id": "2403.19279v1",
    "title": "Fine-Tuning Language Models with Reward Learning on Policy",
    "authors": [
      "Hao Lang",
      "Fei Huang",
      "Yongbin Li"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as an effective\napproach to aligning large language models (LLMs) to human preferences. RLHF\ncontains three steps, i.e., human preference collecting, reward learning, and\npolicy optimization, which are usually performed serially. Despite its\npopularity, however, (fixed) reward models may suffer from inaccurate\noff-distribution, since policy optimization continuously shifts LLMs' data\ndistribution. Repeatedly collecting new preference data from the latest LLMs\nmay alleviate this issue, which unfortunately makes the resulting system more\ncomplicated and difficult to optimize. In this paper, we propose reward\nlearning on policy (RLP), an unsupervised framework that refines a reward model\nusing policy samples to keep it on-distribution. Specifically, an unsupervised\nmulti-view learning method is introduced to learn robust representations of\npolicy samples. Meanwhile, a synthetic preference generation approach is\ndeveloped to simulate high-quality preference data with policy outputs.\nExtensive experiments on three benchmark datasets show that RLP consistently\noutperforms the state-of-the-art. Our code is available at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL2024 Main Track Long Paper",
    "pdf_url": "http://arxiv.org/pdf/2403.19279v1",
    "published_date": "2024-03-28 10:02:10 UTC",
    "updated_date": "2024-03-28 10:02:10 UTC"
  },
  {
    "arxiv_id": "2403.19275v2",
    "title": "Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent",
    "authors": [
      "Junkai Zhou",
      "Liang Pang",
      "Ya Jing",
      "Jia Gu",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "abstract": "Constructing personalized and anthropomorphic agents holds significant\nimportance in the simulation of social networks. However, there are still two\nkey problems in existing works: the agent possesses world knowledge that does\nnot belong to its personas, and it cannot eliminate the interference of diverse\npersona information on current actions, which reduces the personalization and\nanthropomorphism of the agent. To solve the above problems, we construct the\nsocial media agent based on personalized knowledge and dynamic persona\ninformation. For personalized knowledge, we add external knowledge sources and\nmatch them with the persona information of agents, thereby giving the agent\npersonalized world knowledge. For dynamic persona information, we use current\naction information to internally retrieve the persona information of the agent,\nthereby reducing the interference of diverse persona information on the current\naction. To make the agent suitable for social media, we design five basic\nmodules for it: persona, planning, action, memory and reflection. To provide an\ninteraction and verification environment for the agent, we build a social media\nsimulation sandbox. In the experimental verification, automatic and human\nevaluations demonstrated the effectiveness of the agent we constructed.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19275v2",
    "published_date": "2024-03-28 10:01:23 UTC",
    "updated_date": "2024-04-02 10:59:23 UTC"
  },
  {
    "arxiv_id": "2403.19273v1",
    "title": "A Machine Learning Approach for Crop Yield and Disease Prediction Integrating Soil Nutrition and Weather Factors",
    "authors": [
      "Forkan Uddin Ahmed",
      "Annesha Das",
      "Md Zubair"
    ],
    "abstract": "The development of an intelligent agricultural decision-supporting system for\ncrop selection and disease forecasting in Bangladesh is the main objective of\nthis work. The economy of the nation depends heavily on agriculture. However,\nchoosing crops with better production rates and efficiently controlling crop\ndisease are obstacles that farmers have to face. These issues are addressed in\nthis research by utilizing machine learning methods and real-world datasets.\nThe recommended approach uses a variety of datasets on the production of crops,\nsoil conditions, agro-meteorological regions, crop disease, and meteorological\nfactors. These datasets offer insightful information on disease trends, soil\nnutrition demand of crops, and agricultural production history. By\nincorporating this knowledge, the model first recommends the list of primarily\nselected crops based on the soil nutrition of a particular user location. Then\nthe predictions of meteorological variables like temperature, rainfall, and\nhumidity are made using SARIMAX models. These weather predictions are then used\nto forecast the possibilities of diseases for the primary crops list by\nutilizing the support vector classifier. Finally, the developed model makes use\nof the decision tree regression model to forecast crop yield and provides a\nfinal crop list along with associated possible disease forecast. Utilizing the\noutcome of the model, farmers may choose the best productive crops as well as\nprevent crop diseases and reduce output losses by taking preventive actions.\nConsequently, planning and decision-making processes are supported and farmers\ncan predict possible crop yields. Overall, by offering a detailed decision\nsupport system for crop selection and disease prediction, this work can play a\nvital role in advancing agricultural practices in Bangladesh.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper was presented to the IEEE conference, \"2024 International\n  Conference on Advances in Computing, Communication, Electrical, and Smart\n  Systems (iCACCESS), 8-9 March, Dhaka, Bangladesh\"",
    "pdf_url": "http://arxiv.org/pdf/2403.19273v1",
    "published_date": "2024-03-28 09:57:50 UTC",
    "updated_date": "2024-03-28 09:57:50 UTC"
  },
  {
    "arxiv_id": "2403.19271v1",
    "title": "DeepSample: DNN sampling-based testing for operational accuracy assessment",
    "authors": [
      "Antonio Guerriero",
      "Roberto Pietrantuono",
      "Stefano Russo"
    ],
    "abstract": "Deep Neural Networks (DNN) are core components for classification and\nregression tasks of many software systems. Companies incur in high costs for\ntesting DNN with datasets representative of the inputs expected in operation,\nas these need to be manually labelled. The challenge is to select a\nrepresentative set of test inputs as small as possible to reduce the labelling\ncost, while sufficing to yield unbiased high-confidence estimates of the\nexpected DNN accuracy. At the same time, testers are interested in exposing as\nmany DNN mispredictions as possible to improve the DNN, ending up in the need\nfor techniques pursuing a threefold aim: small dataset size, trustworthy\nestimates, mispredictions exposure. This study presents DeepSample, a family of\nDNN testing techniques for cost-effective accuracy assessment based on\nprobabilistic sampling. We investigate whether, to what extent, and under which\nconditions probabilistic sampling can help to tackle the outlined challenge. We\nimplement five new sampling-based testing techniques, and perform a\ncomprehensive comparison of such techniques and of three further\nstate-of-the-art techniques for both DNN classification and regression tasks.\nResults serve as guidance for best use of sampling-based testing for faithful\nand high-confidence estimates of DNN accuracy in operation at low cost.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted for publication at ICSE 2024, Lisbon, Portugal",
    "pdf_url": "http://arxiv.org/pdf/2403.19271v1",
    "published_date": "2024-03-28 09:56:26 UTC",
    "updated_date": "2024-03-28 09:56:26 UTC"
  },
  {
    "arxiv_id": "2403.19270v2",
    "title": "sDPO: Don't Use Your Data All at Once",
    "authors": [
      "Dahyun Kim",
      "Yungi Kim",
      "Wonho Song",
      "Hyeonwoo Kim",
      "Yunsu Kim",
      "Sanghoon Kim",
      "Chanjun Park"
    ],
    "abstract": "As development of large language models (LLM) progresses, aligning them with\nhuman preferences has become increasingly important. We propose stepwise DPO\n(sDPO), an extension of the recently popularized direct preference optimization\n(DPO) for alignment tuning. This approach involves dividing the available\npreference datasets and utilizing them in a stepwise manner, rather than\nemploying it all at once. We demonstrate that this method facilitates the use\nof more precisely aligned reference models within the DPO training framework.\nFurthermore, sDPO trains the final model to be more performant, even\noutperforming other popular LLMs with more parameters.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19270v2",
    "published_date": "2024-03-28 09:56:04 UTC",
    "updated_date": "2024-10-07 04:21:15 UTC"
  },
  {
    "arxiv_id": "2403.19267v2",
    "title": "MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs",
    "authors": [
      "Xianhao Yu",
      "Jiaqi Fu",
      "Renjia Deng",
      "Wenjuan Han"
    ],
    "abstract": "While Vision-Language Models (VLMs) hold promise for tasks requiring\nextensive collaboration, traditional multi-agent simulators have facilitated\nrich explorations of an interactive artificial society that reflects collective\nbehavior. However, these existing simulators face significant limitations.\nFirstly, they struggle with handling large numbers of agents due to high\nresource demands. Secondly, they often assume agents possess perfect\ninformation and limitless capabilities, hindering the ecological validity of\nsimulated social interactions. To bridge this gap, we propose a multi-agent\nMinecraft simulator, MineLand, that bridges this gap by introducing three key\nfeatures: large-scale scalability, limited multimodal senses, and physical\nneeds. Our simulator supports 64 or more agents. Agents have limited visual,\nauditory, and environmental awareness, forcing them to actively communicate and\ncollaborate to fulfill physical needs like food and resources. Additionally, we\nfurther introduce an AI agent framework, Alex, inspired by multitasking theory,\nenabling agents to handle intricate coordination and scheduling. Our\nexperiments demonstrate that the simulator, the corresponding benchmark, and\nthe AI agent framework contribute to more ecological and nuanced collective\nbehavior.The source code of MineLand and Alex is openly available at\nhttps://github.com/cocacola-lab/MineLand.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Project website: https://github.com/cocacola-lab/MineLand",
    "pdf_url": "http://arxiv.org/pdf/2403.19267v2",
    "published_date": "2024-03-28 09:53:41 UTC",
    "updated_date": "2024-05-23 14:27:15 UTC"
  },
  {
    "arxiv_id": "2403.19238v2",
    "title": "Taming Lookup Tables for Efficient Image Retouching",
    "authors": [
      "Sidi Yang",
      "Binxiao Huang",
      "Mingdeng Cao",
      "Yatai Ji",
      "Hanzhong Guo",
      "Ngai Wong",
      "Yujiu Yang"
    ],
    "abstract": "The widespread use of high-definition screens in edge devices, such as\nend-user cameras, smartphones, and televisions, is spurring a significant\ndemand for image enhancement. Existing enhancement models often optimize for\nhigh performance while falling short of reducing hardware inference time and\npower consumption, especially on edge devices with constrained computing and\nstorage resources. To this end, we propose Image Color Enhancement Lookup Table\n(ICELUT) that adopts LUTs for extremely efficient edge inference, without any\nconvolutional neural network (CNN). During training, we leverage pointwise\n(1x1) convolution to extract color information, alongside a split fully\nconnected layer to incorporate global information. Both components are then\nseamlessly converted into LUTs for hardware-agnostic deployment. ICELUT\nachieves near-state-of-the-art performance and remarkably low power\nconsumption. We observe that the pointwise network structure exhibits robust\nscalability, upkeeping the performance even with a heavily downsampled 32x32\ninput image. These enable ICELUT, the first-ever purely LUT-based image\nenhancer, to reach an unprecedented speed of 0.4ms on GPU and 7ms on CPU, at\nleast one order faster than any CNN solution. Codes are available at\nhttps://github.com/Stephen0808/ICELUT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19238v2",
    "published_date": "2024-03-28 08:49:35 UTC",
    "updated_date": "2024-07-13 08:38:26 UTC"
  },
  {
    "arxiv_id": "2403.19727v1",
    "title": "New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark",
    "authors": [
      "Nadège Alavoine",
      "Gaëlle Laperriere",
      "Christophe Servan",
      "Sahar Ghannay",
      "Sophie Rosset"
    ],
    "abstract": "Intent classification and slot-filling are essential tasks of Spoken Language\nUnderstanding (SLU). In most SLUsystems, those tasks are realized by\nindependent modules. For about fifteen years, models achieving both of\nthemjointly and exploiting their mutual enhancement have been proposed. A\nmultilingual module using a joint modelwas envisioned to create a touristic\ndialogue system for a European project, HumanE-AI-Net. A combination ofmultiple\ndatasets, including the MEDIA dataset, was suggested for training this joint\nmodel. The MEDIA SLU datasetis a French dataset distributed since 2005 by ELRA,\nmainly used by the French research community and free foracademic research\nsince 2020. Unfortunately, it is annotated only in slots but not intents. An\nenhanced version ofMEDIA annotated with intents has been built to extend its\nuse to more tasks and use cases. This paper presents thesemi-automatic\nmethodology used to obtain this enhanced version. In addition, we present the\nfirst results of SLUexperiments on this enhanced dataset using joint models for\nintent classification and slot-filling.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19727v1",
    "published_date": "2024-03-28 08:40:02 UTC",
    "updated_date": "2024-03-28 08:40:02 UTC"
  },
  {
    "arxiv_id": "2403.19221v1",
    "title": "Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality",
    "authors": [
      "Sishuo Chen",
      "Lei Li",
      "Shuhuai Ren",
      "Rundong Gao",
      "Yuanxin Liu",
      "Xiaohan Bi",
      "Xu Sun",
      "Lu Hou"
    ],
    "abstract": "Video paragraph captioning (VPC) involves generating detailed narratives for\nlong videos, utilizing supportive modalities such as speech and event\nboundaries. However, the existing models are constrained by the assumption of\nconstant availability of a single auxiliary modality, which is impractical\ngiven the diversity and unpredictable nature of real-world scenarios. To this\nend, we propose a Missing-Resistant framework MR-VPC that effectively harnesses\nall available auxiliary inputs and maintains resilience even in the absence of\ncertain modalities. Under this framework, we propose the Multimodal VPC (MVPC)\narchitecture integrating video, speech, and event boundary inputs in a unified\nmanner to process various auxiliary inputs. Moreover, to fortify the model\nagainst incomplete data, we introduce DropAM, a data augmentation strategy that\nrandomly omits auxiliary inputs, paired with DistillAM, a regularization target\nthat distills knowledge from teacher models trained on modality-complete data,\nenabling efficient learning in modality-deficient environments. Through\nexhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC has\nproven to deliver superior performance on modality-complete and\nmodality-missing test data. This work highlights the significance of developing\nresilient VPC models and paves the way for more adaptive, robust multimodal\nvideo understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code available at https://github.com/lancopku/MR-VPC",
    "pdf_url": "http://arxiv.org/pdf/2403.19221v1",
    "published_date": "2024-03-28 08:35:46 UTC",
    "updated_date": "2024-03-28 08:35:46 UTC"
  },
  {
    "arxiv_id": "2403.19211v2",
    "title": "Dual-Personalizing Adapter for Federated Foundation Models",
    "authors": [
      "Yiyuan Yang",
      "Guodong Long",
      "Tao Shen",
      "Jing Jiang",
      "Michael Blumenstein"
    ],
    "abstract": "Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\ndiverse instruction data. Notably, federated foundation models (FedFM) emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to FedFM for better user preferences alignment.\nHowever, a critical gap in existing research is the neglect of test-time\ndistribution shifts in real-world applications, and conventional methods for\ntest-time distribution shifts in personalized FL are less effective for FedFM\ndue to their failure to adapt to complex distribution shift scenarios and the\nrequirement to train all parameters. To bridge this gap, we refine the setting\nin FedFM, termed test-time personalization, which aims to learn personalized\nfederated foundation models on clients while effectively handling test-time\ndistribution shifts simultaneously. To address challenges in this setting, we\nexplore a simple yet effective solution, a Federated Dual-Personalizing Adapter\n(FedDPA) architecture. By co-working with a foundation model, a global adapter\nand a local adapter jointly tackle the test-time distribution shifts and\nclient-specific personalization. Additionally, we introduce an instance-wise\ndynamic weighting mechanism that dynamically integrates the global and local\nadapters for each test instance during inference, facilitating effective\ntest-time personalization. The effectiveness of the proposed method has been\nevaluated on benchmark datasets across different NLP tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19211v2",
    "published_date": "2024-03-28 08:19:33 UTC",
    "updated_date": "2024-12-02 10:44:08 UTC"
  },
  {
    "arxiv_id": "2403.19726v1",
    "title": "A Benchmark Evaluation of Clinical Named Entity Recognition in French",
    "authors": [
      "Nesrine Bannour",
      "Christophe Servan",
      "Aurélie Névéol",
      "Xavier Tannier"
    ],
    "abstract": "Background: Transformer-based language models have shown strong performance\non many Natural LanguageProcessing (NLP) tasks. Masked Language Models (MLMs)\nattract sustained interest because they can be adaptedto different languages\nand sub-domains through training or fine-tuning on specific corpora while\nremaining lighterthan modern Large Language Models (LLMs). Recently, several\nMLMs have been released for the biomedicaldomain in French, and experiments\nsuggest that they outperform standard French counterparts. However,\nnosystematic evaluation comparing all models on the same corpora is available.\nObjective: This paper presentsan evaluation of masked language models for\nbiomedical French on the task of clinical named entity recognition.Material and\nmethods: We evaluate biomedical models CamemBERT-bio and DrBERT and compare\nthem tostandard French models CamemBERT, FlauBERT and FrALBERT as well as\nmultilingual mBERT using three publicallyavailable corpora for clinical named\nentity recognition in French. The evaluation set-up relies on\ngold-standardcorpora as released by the corpus developers. Results: Results\nsuggest that CamemBERT-bio outperformsDrBERT consistently while FlauBERT offers\ncompetitive performance and FrAlBERT achieves the lowest carbonfootprint.\nConclusion: This is the first benchmark evaluation of biomedical masked\nlanguage models for Frenchclinical entity recognition that compares model\nperformance consistently on nested entity recognition using metricscovering\nperformance and environmental impact.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19726v1",
    "published_date": "2024-03-28 07:59:58 UTC",
    "updated_date": "2024-03-28 07:59:58 UTC"
  },
  {
    "arxiv_id": "2403.19725v1",
    "title": "MUGC: Machine Generated versus User Generated Content Detection",
    "authors": [
      "Yaqi Xie",
      "Anjali Rawal",
      "Yujing Cen",
      "Dixuan Zhao",
      "Sunil K Narang",
      "Shanu Sushmita"
    ],
    "abstract": "As advanced modern systems like deep neural networks (DNNs) and generative AI\ncontinue to enhance their capabilities in producing convincing and realistic\ncontent, the need to distinguish between user-generated and machine generated\ncontent is becoming increasingly evident. In this research, we undertake a\ncomparative evaluation of eight traditional machine-learning algorithms to\ndistinguish between machine-generated and human-generated data across three\ndiverse datasets: Poems, Abstracts, and Essays. Our results indicate that\ntraditional methods demonstrate a high level of accuracy in identifying\nmachine-generated data, reflecting the documented effectiveness of popular\npre-trained models like RoBERT. We note that machine-generated texts tend to be\nshorter and exhibit less word variety compared to human-generated content.\nWhile specific domain-related keywords commonly utilized by humans, albeit\ndisregarded by current LLMs (Large Language Models), may contribute to this\nhigh detection accuracy, we show that deeper word representations like word2vec\ncan capture subtle semantic variances. Furthermore, readability, bias, moral,\nand affect comparisons reveal a discernible contrast between machine-generated\nand human generated content. There are variations in expression styles and\npotentially underlying biases in the data sources (human and\nmachine-generated). This study provides valuable insights into the advancing\ncapacities and challenges associated with machine-generated content across\nvarious domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.19725v1",
    "published_date": "2024-03-28 07:33:53 UTC",
    "updated_date": "2024-03-28 07:33:53 UTC"
  },
  {
    "arxiv_id": "2403.19178v1",
    "title": "Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning",
    "authors": [
      "Ji Liu",
      "Chunlu Chen",
      "Yu Li",
      "Lin Sun",
      "Yulun Song",
      "Jingbo Zhou",
      "Bo Jing",
      "Dejing Dou"
    ],
    "abstract": "While centralized servers pose a risk of being a single point of failure,\ndecentralized approaches like blockchain offer a compelling solution by\nimplementing a consensus mechanism among multiple entities. Merging distributed\ncomputing with cryptographic techniques, decentralized technologies introduce a\nnovel computing paradigm. Blockchain ensures secure, transparent, and\ntamper-proof data management by validating and recording transactions via\nconsensus across network nodes. Federated Learning (FL), as a distributed\nmachine learning framework, enables participants to collaboratively train\nmodels while safeguarding data privacy by avoiding direct raw data exchange.\nDespite the growing interest in decentralized methods, their application in FL\nremains underexplored. This paper presents a thorough investigation into\nBlockchain-based FL (BCFL), spotlighting the synergy between blockchain's\nsecurity features and FL's privacy-preserving model training capabilities.\nFirst, we present the taxonomy of BCFL from three aspects, including\ndecentralized, separate networks, and reputation-based architectures. Then, we\nsummarize the general architecture of BCFL systems, providing a comprehensive\nperspective on FL architectures informed by blockchain. Afterward, we analyze\nthe application of BCFL in healthcare, IoT, and other privacy-sensitive areas.\nFinally, we identify future research directions of BCFL.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "25 pages, accepted by KAIS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19178v1",
    "published_date": "2024-03-28 07:08:26 UTC",
    "updated_date": "2024-03-28 07:08:26 UTC"
  },
  {
    "arxiv_id": "2403.19177v1",
    "title": "Rethinking Information Loss in Medical Image Segmentation with Various-sized Targets",
    "authors": [
      "Tianyi Liu",
      "Zhaorui Tan",
      "Kaizhu Huang",
      "Haochuan Jiang"
    ],
    "abstract": "Medical image segmentation presents the challenge of segmenting various-size\ntargets, demanding the model to effectively capture both local and global\ninformation. Despite recent efforts using CNNs and ViTs to predict annotations\nof different scales, these approaches often struggle to effectively balance the\ndetection of targets across varying sizes. Simply utilizing local information\nfrom CNNs and global relationships from ViTs without considering potential\nsignificant divergence in latent feature distributions may result in\nsubstantial information loss. To address this issue, in this paper, we will\nintroduce a novel Stagger Network (SNet) and argues that a well-designed fusion\nstructure can mitigate the divergence in latent feature distributions between\nCNNs and ViTs, thereby reducing information loss. Specifically, to emphasize\nboth global dependencies and local focus, we design a Parallel Module to bridge\nthe semantic gap. Meanwhile, we propose the Stagger Module, trying to fuse the\nselected features that are more semantically similar. An Information Recovery\nModule is further adopted to recover complementary information back to the\nnetwork. As a key contribution, we theoretically analyze that the proposed\nparallel and stagger strategies would lead to less information loss, thus\ncertifying the SNet's rationale. Experimental results clearly proved that the\nproposed SNet excels comparisons with recent SOTAs in segmenting on the Synapse\ndataset where targets are in various sizes. Besides, it also demonstrates\nsuperiority on the ACDC and the MoNuSeg datasets where targets are with more\nconsistent dimensions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19177v1",
    "published_date": "2024-03-28 07:01:11 UTC",
    "updated_date": "2024-03-28 07:01:11 UTC"
  },
  {
    "arxiv_id": "2403.19167v1",
    "title": "Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering",
    "authors": [
      "Yexin Wu",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ],
    "abstract": "Large language models have manifested remarkable capabilities by leveraging\nchain-of-thought (CoT) reasoning techniques to solve intricate questions\nthrough step-by-step reasoning chains. Despite its success, the efficacy of\nsuch reasoning is inherently contingent upon the quality of CoT. However,\nflawless CoT reasoning cannot be guaranteed due to the presence of\nindecomposable questions and the potential for erroneous reasoning chains,\nparticularly in the case of small-scale language models. To tackle this\nchallenge, we propose a novel approach called the selective filtering reasoner\n(SelF-Reasoner) that assesses the entailment relationship between the question\nand the candidate reasoning chain. Then, we proceed with CoT reasoning when the\nreasoning chain demonstrates confidence; otherwise, we opt to predict the\nanswer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently\nover the ScienceQA, ECQA, and LastLetter tasks. Code is available at\n\\texttt{https://github.com/LibroWu/SelF-Reasoner}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19167v1",
    "published_date": "2024-03-28 06:28:35 UTC",
    "updated_date": "2024-03-28 06:28:35 UTC"
  },
  {
    "arxiv_id": "2403.19154v3",
    "title": "STaR-GATE: Teaching Language Models to Ask Clarifying Questions",
    "authors": [
      "Chinmaya Andukuri",
      "Jan-Philipp Fränken",
      "Tobias Gerstenberg",
      "Noah D. Goodman"
    ],
    "abstract": "When prompting language models to complete a task, users often leave\nimportant aspects unsaid. While asking questions could resolve this ambiguity\n(GATE; Li et al., 2023), models often struggle to ask good questions. We\nexplore a language model's ability to self-improve (STaR; Zelikman et al.,\n2022) by rewarding the model for generating useful questions-a simple method we\ndub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task\nprompts to simulate conversations between a pretrained language model-the\nQuestioner-and a Roleplayer whose preferences are unknown to the Questioner. By\nasking questions, the Questioner elicits preferences from the Roleplayer. The\nQuestioner is iteratively finetuned on questions that increase the probability\nof high-quality responses to the task, which are generated by an Oracle with\naccess to the Roleplayer's latent preferences. After two iterations of\nself-improvement, the Questioner asks better questions, allowing it to generate\nresponses that are preferred over responses from the initial model on 72% of\ntasks. Our results indicate that teaching a language model to ask better\nquestions leads to better personalized responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19154v3",
    "published_date": "2024-03-28 05:35:22 UTC",
    "updated_date": "2024-08-07 19:44:28 UTC"
  },
  {
    "arxiv_id": "2403.19150v1",
    "title": "Towards Understanding Dual BN In Hybrid Adversarial Training",
    "authors": [
      "Chenshuang Zhang",
      "Chaoning Zhang",
      "Kang Zhang",
      "Axi Niu",
      "Junmo Kim",
      "In So Kweon"
    ],
    "abstract": "There is a growing concern about applying batch normalization (BN) in\nadversarial training (AT), especially when the model is trained on both\nadversarial samples and clean samples (termed Hybrid-AT). With the assumption\nthat adversarial and clean samples are from two different domains, a common\npractice in prior works is to adopt Dual BN, where BN and BN are used for\nadversarial and clean branches, respectively. A popular belief for motivating\nDual BN is that estimating normalization statistics of this mixture\ndistribution is challenging and thus disentangling it for normalization\nachieves stronger robustness. In contrast to this belief, we reveal that\ndisentangling statistics plays a less role than disentangling affine parameters\nin model training. This finding aligns with prior work (Rebuffi et al., 2023),\nand we build upon their research for further investigations. We demonstrate\nthat the domain gap between adversarial and clean samples is not very large,\nwhich is counter-intuitive considering the significant influence of adversarial\nperturbation on the model accuracy. We further propose a two-task hypothesis\nwhich serves as the empirical foundation and a unified framework for Hybrid-AT\nimprovement. We also investigate Dual BN in test-time and reveal that affine\nparameters characterize the robustness during inference. Overall, our work\nsheds new light on understanding the mechanism of Dual BN in Hybrid-AT and its\nunderlying justification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at TMLR",
    "pdf_url": "http://arxiv.org/pdf/2403.19150v1",
    "published_date": "2024-03-28 05:08:25 UTC",
    "updated_date": "2024-03-28 05:08:25 UTC"
  },
  {
    "arxiv_id": "2403.19148v1",
    "title": "GenAI Detection Tools, Adversarial Techniques and Implications for Inclusivity in Higher Education",
    "authors": [
      "Mike Perkins",
      "Jasper Roe",
      "Binh H. Vu",
      "Darius Postma",
      "Don Hickerson",
      "James McGaughran",
      "Huy Q. Khuat"
    ],
    "abstract": "This study investigates the efficacy of six major Generative AI (GenAI) text\ndetectors when confronted with machine-generated content that has been modified\nusing techniques designed to evade detection by these tools (n=805). The\nresults demonstrate that the detectors' already low accuracy rates (39.5%) show\nmajor reductions in accuracy (17.4%) when faced with manipulated content, with\nsome techniques proving more effective than others in evading detection.\n  The accuracy limitations and the potential for false accusations demonstrate\nthat these tools cannot currently be recommended for determining whether\nviolations of academic integrity have occurred, underscoring the challenges\neducators face in maintaining inclusive and fair assessment practices. However,\nthey may have a role in supporting student learning and maintaining academic\nintegrity when used in a non-punitive manner.\n  These results underscore the need for a combined approach to addressing the\nchallenges posed by GenAI in academia to promote the responsible and equitable\nuse of these emerging technologies. The study concludes that the current\nlimitations of AI text detectors require a critical approach for any possible\nimplementation in HE and highlight possible alternatives to AI assessment\nstrategies.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19148v1",
    "published_date": "2024-03-28 04:57:13 UTC",
    "updated_date": "2024-03-28 04:57:13 UTC"
  },
  {
    "arxiv_id": "2403.19140v2",
    "title": "QNCD: Quantization Noise Correction for Diffusion Models",
    "authors": [
      "Huanpeng Chu",
      "Wei Wu",
      "Chengjie Zang",
      "Kun Yuan"
    ],
    "abstract": "Diffusion models have revolutionized image synthesis, setting new benchmarks\nin quality and creativity. However, their widespread adoption is hindered by\nthe intensive computation required during the iterative denoising process.\nPost-training quantization (PTQ) presents a solution to accelerate sampling,\naibeit at the expense of sample quality, extremely in low-bit settings.\nAddressing this, our study introduces a unified Quantization Noise Correction\nScheme (QNCD), aimed at minishing quantization noise throughout the sampling\nprocess. We identify two primary quantization challenges: intra and inter\nquantization noise. Intra quantization noise, mainly exacerbated by embeddings\nin the resblock module, extends activation quantization ranges, increasing\ndisturbances in each single denosing step. Besides, inter quantization noise\nstems from cumulative quantization deviations across the entire denoising\nprocess, altering data distributions step-by-step. QNCD combats these through\nembedding-derived feature smoothing for eliminating intra quantization noise\nand an effective runtime noise estimatiation module for dynamicly filtering\ninter quantization noise. Extensive experiments demonstrate that our method\noutperforms previous quantization methods for diffusion models, achieving\nlossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4).\nCode is available at: https://github.com/huanpengchu/QNCD",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ACMMM2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19140v2",
    "published_date": "2024-03-28 04:24:56 UTC",
    "updated_date": "2024-09-18 10:50:32 UTC"
  },
  {
    "arxiv_id": "2403.19135v5",
    "title": "Streamlining Redundant Layers to Compress Large Language Models",
    "authors": [
      "Xiaodong Chen",
      "Yuxuan Hu",
      "Jing Zhang",
      "Yanling Wang",
      "Cuiping Li",
      "Hong Chen"
    ],
    "abstract": "This paper introduces LLM-Streamline, a pioneer work on layer pruning for\nlarge language models (LLMs). It is based on the observation that different\nlayers have varying impacts on hidden states, enabling the identification of\nless important layers to be pruned.LLM-Streamline comprises two parts: layer\npruning, which removes consecutive layers with the lowest importance based on\ntarget sparsity, and layer replacement, a novel module that trains a\nlightweight network to replace the pruned layers to mitigate performance loss.\nAdditionally, a new metric called stability is proposed to address the\nlimitations of the widely used accuracy metric in evaluating model compression.\nExperiments show that LLM-Streamline outperforms both previous and concurrent\nstate-of-the-art pruning methods in terms of both performance and training\nefficiency.Our code is available at\nhttps://github.com/RUCKBReasoning/LLM-Streamline",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19135v5",
    "published_date": "2024-03-28 04:12:13 UTC",
    "updated_date": "2025-01-25 00:11:26 UTC"
  },
  {
    "arxiv_id": "2404.01319v2",
    "title": "Information Cascade Prediction under Public Emergencies: A Survey",
    "authors": [
      "Qi Zhang",
      "Guang Wang",
      "Li Lin",
      "Kaiwen Xia",
      "Shuai Wang"
    ],
    "abstract": "With the advent of the era of big data, massive information, expert\nexperience, and high-accuracy models bring great opportunities to the\ninformation cascade prediction of public emergencies. However, the involvement\nof specialist knowledge from various disciplines has resulted in a primarily\napplication-specific focus (e.g., earthquakes, floods, infectious diseases) for\ninformation cascade prediction of public emergencies. The lack of a unified\nprediction framework poses a challenge for classifying intersectional\nprediction methods across different application fields. This survey paper\noffers a systematic classification and summary of information cascade modeling,\nprediction, and application. We aim to help researchers identify cutting-edge\nresearch and comprehend models and methods of information cascade prediction\nunder public emergencies. By summarizing open issues and outlining future\ndirections in this field, this paper has the potential to be a valuable\nresource for researchers conducting further studies on predicting information\ncascades.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.SI",
    "comment": "arXiv admin note: text overlap with arXiv:2007.09815 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2404.01319v2",
    "published_date": "2024-03-28 03:46:56 UTC",
    "updated_date": "2024-05-16 23:56:54 UTC"
  },
  {
    "arxiv_id": "2403.19723v2",
    "title": "HeGTa: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding",
    "authors": [
      "Rihui Jin",
      "Yu Li",
      "Guilin Qi",
      "Nan Hu",
      "Yuan-Fang Li",
      "Jiaoyan Chen",
      "Jianan Wang",
      "Yongrui Chen",
      "Dehai Min",
      "Sheng Bi"
    ],
    "abstract": "Table understanding (TU) has achieved promising advancements, but it faces\nthe challenges of the scarcity of manually labeled tables and the presence of\ncomplex table structures.To address these challenges, we propose HGT, a\nframework with a heterogeneous graph (HG)-enhanced large language model (LLM)\nto tackle few-shot TU tasks.It leverages the LLM by aligning the table\nsemantics with the LLM's parametric knowledge through soft prompts and\ninstruction turning and deals with complex tables by a multi-task pre-training\nscheme involving three novel multi-granularity self-supervised HG pre-training\nobjectives.We empirically demonstrate the effectiveness of HGT, showing that it\noutperforms the SOTA for few-shot complex TU on several benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.19723v2",
    "published_date": "2024-03-28 03:20:54 UTC",
    "updated_date": "2024-12-15 14:24:47 UTC"
  },
  {
    "arxiv_id": "2403.19116v1",
    "title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering",
    "authors": [
      "Che Guan",
      "Mengyu Huang",
      "Peng Zhang"
    ],
    "abstract": "In today's fast-paced industry, professionals face the challenge of\nsummarizing a large number of documents and extracting vital information from\nthem on a daily basis. These metrics are frequently hidden away in tables\nand/or their nested hyperlinks. To address this challenge, the approach of\nTable Question Answering (QA) has been developed to extract the relevant\ninformation. However, traditional Table QA training tasks that provide a table\nand an answer(s) from a gold cell coordinate(s) for a question may not always\nensure extracting the accurate answer(s). Recent advancements in Large Language\nModels (LLMs) have opened up new possibilities for extracting information from\ntabular data using prompts. In this paper, we introduce the Multi-hop Few-shot\nOpen Rich Table QA (MFORT-QA) approach, which consists of two major steps. The\nfirst step involves Few-Shot Learning (FSL), where relevant tables and\nassociated contexts of hyperlinks are retrieved based on a given question. The\nretrieved content is then used to construct few-shot prompts as inputs to an\nLLM, such as ChatGPT. To tackle the challenge of answering complex questions,\nthe second step leverages Chain-of-thought (CoT) prompting to decompose the\ncomplex question into a sequential chain of questions and reasoning thoughts in\na multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process\nby retrieving relevant tables and contexts of hyperlinks that are relevant to\nthe resulting reasoning thoughts and questions. These additional contexts are\nthen used to supplement the prompt used in the first step, resulting in more\naccurate answers from an LLM. Empirical results from OTT-QA demonstrate that\nour abstractive QA approach significantly improves the accuracy of extractive\nTable QA methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.19116v1",
    "published_date": "2024-03-28 03:14:18 UTC",
    "updated_date": "2024-03-28 03:14:18 UTC"
  },
  {
    "arxiv_id": "2403.19113v1",
    "title": "FACTOID: FACtual enTailment fOr hallucInation Detection",
    "authors": [
      "Vipula Rawte",
      "S. M Towhidul Islam Tonmoy",
      "Krishnav Rajbangshi",
      "Shravani Nag",
      "Aman Chadha",
      "Amit P. Sheth",
      "Amitava Das"
    ],
    "abstract": "The widespread adoption of Large Language Models (LLMs) has facilitated\nnumerous benefits. However, hallucination is a significant concern. In\nresponse, Retrieval Augmented Generation (RAG) has emerged as a highly\npromising paradigm to improve LLM outputs by grounding them in factual\ninformation. RAG relies on textual entailment (TE) or similar methods to check\nif the text produced by LLMs is supported or contradicted, compared to\nretrieved documents. This paper argues that conventional TE methods are\ninadequate for spotting hallucinations in content generated by LLMs. For\ninstance, consider a prompt about the 'USA's stance on the Ukraine war''. The\nAI-generated text states, ...U.S. President Barack Obama says the U.S. will not\nput troops in Ukraine...'' However, during the war the U.S. president is Joe\nBiden which contradicts factual reality. Moreover, current TE systems are\nunable to accurately annotate the given text and identify the exact portion\nthat is contradicted. To address this, we introduces a new type of TE called\n``Factual Entailment (FE).'', aims to detect factual inaccuracies in content\ngenerated by LLMs while also highlighting the specific text segment that\ncontradicts reality. We present FACTOID (FACTual enTAILment for hallucInation\nDetection), a benchmark dataset for FE. We propose a multi-task learning (MTL)\nframework for FE, incorporating state-of-the-art (SoTA) long text embeddings\nsuch as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The\nproposed MTL architecture for FE achieves an avg. 40\\% improvement in accuracy\non the FACTOID benchmark compared to SoTA TE methods. As FE automatically\ndetects hallucinations, we assessed 15 modern LLMs and ranked them using our\nproposed Auto Hallucination Vulnerability Index (HVI_auto). This index\nquantifies and offers a comparative scale to evaluate and rank LLMs according\nto their hallucinations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19113v1",
    "published_date": "2024-03-28 03:09:42 UTC",
    "updated_date": "2024-03-28 03:09:42 UTC"
  },
  {
    "arxiv_id": "2406.17605v1",
    "title": "NativE: Multi-modal Knowledge Graph Completion in the Wild",
    "authors": [
      "Yichi Zhang",
      "Zhuo Chen",
      "Lingbing Guo",
      "Yajing Xu",
      "Binbin Hu",
      "Ziqi Liu",
      "Wen Zhang",
      "Huajun Chen"
    ],
    "abstract": "Multi-modal knowledge graph completion (MMKGC) aims to automatically discover\nthe unobserved factual knowledge from a given multi-modal knowledge graph by\ncollaboratively modeling the triple structure and multi-modal information from\nentities. However, real-world MMKGs present challenges due to their diverse and\nimbalanced nature, which means that the modality information can span various\ntypes (e.g., image, text, numeric, audio, video) but its distribution among\nentities is uneven, leading to missing modalities for certain entities.\nExisting works usually focus on common modalities like image and text while\nneglecting the imbalanced distribution phenomenon of modal information. To\naddress these issues, we propose a comprehensive framework NativE to achieve\nMMKGC in the wild. NativE proposes a relation-guided dual adaptive fusion\nmodule that enables adaptive fusion for any modalities and employs a\ncollaborative modality adversarial training framework to augment the imbalanced\nmodality information. We construct a new benchmark called WildKGC with five\ndatasets to evaluate our method. The empirical results compared with 21 recent\nbaselines confirm the superiority of our method, consistently achieving\nstate-of-the-art performance across different datasets and various scenarios\nwhile keeping efficient and generalizable. Our code and data are released at\nhttps://github.com/zjukg/NATIVE",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.MM",
    "comment": "Accepted by SIGIR 2024 as a full paper",
    "pdf_url": "http://arxiv.org/pdf/2406.17605v1",
    "published_date": "2024-03-28 03:04:00 UTC",
    "updated_date": "2024-03-28 03:04:00 UTC"
  },
  {
    "arxiv_id": "2403.19103v3",
    "title": "Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation",
    "authors": [
      "Yutong He",
      "Alexander Robey",
      "Naoki Murata",
      "Yiding Jiang",
      "Joshua Nathaniel Williams",
      "George J. Pappas",
      "Hamed Hassani",
      "Yuki Mitsufuji",
      "Ruslan Salakhutdinov",
      "J. Zico Kolter"
    ],
    "abstract": "Prompt engineering is an effective but labor-intensive way to control\ntext-to-image (T2I) generative models. Its time-intensive nature and complexity\nhave spurred the development of algorithms for automated prompt generation.\nHowever, these methods often struggle with transferability across T2I models,\nrequire white-box access to the underlying model, or produce non-intuitive\nprompts. In this work, we introduce PRISM, an algorithm that automatically\nproduces human-interpretable and transferable prompts that can effectively\ngenerate desired concepts given only black-box access to T2I models. Inspired\nby large language model (LLM) jailbreaking, PRISM leverages the in-context\nlearning ability of LLMs to iteratively refine the candidate prompt\ndistribution built upon the reference images. Our experiments demonstrate the\nversatility and effectiveness of PRISM in generating accurate prompts for\nobjects, styles, and images across multiple T2I models, including Stable\nDiffusion, DALL-E, and Midjourney.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19103v3",
    "published_date": "2024-03-28 02:35:53 UTC",
    "updated_date": "2025-04-28 03:04:46 UTC"
  },
  {
    "arxiv_id": "2403.19101v1",
    "title": "AAPMT: AGI Assessment Through Prompt and Metric Transformer",
    "authors": [
      "Benhao Huang"
    ],
    "abstract": "The emergence of text-to-image models marks a significant milestone in the\nevolution of AI-generated images (AGIs), expanding their use in diverse domains\nlike design, entertainment, and more. Despite these breakthroughs, the quality\nof AGIs often remains suboptimal, highlighting the need for effective\nevaluation methods. These methods are crucial for assessing the quality of\nimages relative to their textual descriptions, and they must accurately mirror\nhuman perception. Substantial progress has been achieved in this domain, with\ninnovative techniques such as BLIP and DBCNN contributing significantly.\nHowever, recent studies, including AGIQA-3K, reveal a notable discrepancy\nbetween current methods and state-of-the-art (SOTA) standards. This gap\nemphasizes the necessity for a more sophisticated and precise evaluation\nmetric. In response, our objective is to develop a model that could give\nratings for metrics, which focuses on parameters like perceptual quality,\nauthenticity, and the correspondence between text and image, that more closely\naligns with human perception. In our paper, we introduce a range of effective\nmethods, including prompt designs and the Metric Transformer. The Metric\nTransformer is a novel structure inspired by the complex interrelationships\namong various AGI quality metrics. The code is available at\nhttps://github.com/huskydoge/CS3324-Digital-Image-Processing/tree/main/Assignment1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19101v1",
    "published_date": "2024-03-28 02:31:06 UTC",
    "updated_date": "2024-03-28 02:31:06 UTC"
  },
  {
    "arxiv_id": "2403.19093v1",
    "title": "Task2Morph: Differentiable Task-inspired Framework for Contact-Aware Robot Design",
    "authors": [
      "Yishuai Cai",
      "Shaowu Yang",
      "Minglong Li",
      "Xinglin Chen",
      "Yunxin Mao",
      "Xiaodong Yi",
      "Wenjing Yang"
    ],
    "abstract": "Optimizing the morphologies and the controllers that adapt to various tasks\nis a critical issue in the field of robot design, aka. embodied intelligence.\nPrevious works typically model it as a joint optimization problem and use\nsearch-based methods to find the optimal solution in the morphology space.\nHowever, they ignore the implicit knowledge of task-to-morphology mapping which\ncan directly inspire robot design. For example, flipping heavier boxes tends to\nrequire more muscular robot arms. This paper proposes a novel and general\ndifferentiable task-inspired framework for contact-aware robot design called\nTask2Morph. We abstract task features highly related to task performance and\nuse them to build a task-to-morphology mapping. Further, we embed the mapping\ninto a differentiable robot design process, where the gradient information is\nleveraged for both the mapping learning and the whole optimization. The\nexperiments are conducted on three scenarios, and the results validate that\nTask2Morph outperforms DiffHand, which lacks a task-inspired morphology module,\nin terms of efficiency and effectiveness.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 10 figures, published to IROS",
    "pdf_url": "http://arxiv.org/pdf/2403.19093v1",
    "published_date": "2024-03-28 02:02:00 UTC",
    "updated_date": "2024-03-28 02:02:00 UTC"
  },
  {
    "arxiv_id": "2403.19083v1",
    "title": "Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep Learning: A Bayesian Deep Learning Approach",
    "authors": [
      "Pei Xi",
      "Lin"
    ],
    "abstract": "With recent advancements in the development of artificial intelligence\napplications using theories and algorithms in machine learning, many accurate\nmodels can be created to train and predict on given datasets. With the\nrealization of the importance of imaging interpretation in cancer diagnosis,\nthis article aims to investigate the theory behind Deep Learning and Bayesian\nNetwork prediction models. Based on the advantages and drawbacks of each model,\ndifferent approaches will be used to construct a Bayesian Deep Learning Model,\ncombining the strengths while minimizing the weaknesses. Finally, the\napplications and accuracy of the resulting Bayesian Deep Learning approach in\nthe health industry in classifying images will be analyzed.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19083v1",
    "published_date": "2024-03-28 01:27:10 UTC",
    "updated_date": "2024-03-28 01:27:10 UTC"
  },
  {
    "arxiv_id": "2403.19082v1",
    "title": "Enhancing Conformal Prediction Using E-Test Statistics",
    "authors": [
      "A. A. Balinsky",
      "A. D. Balinsky"
    ],
    "abstract": "Conformal Prediction (CP) serves as a robust framework that quantifies\nuncertainty in predictions made by Machine Learning (ML) models. Unlike\ntraditional point predictors, CP generates statistically valid prediction\nregions, also known as prediction intervals, based on the assumption of data\nexchangeability. Typically, the construction of conformal predictions hinges on\np-values. This paper, however, ventures down an alternative path, harnessing\nthe power of e-test statistics to augment the efficacy of conformal predictions\nby introducing a BB-predictor (bounded from the below predictor).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19082v1",
    "published_date": "2024-03-28 01:14:25 UTC",
    "updated_date": "2024-03-28 01:14:25 UTC"
  },
  {
    "arxiv_id": "2403.19078v1",
    "title": "MVEB: Self-Supervised Learning with Multi-View Entropy Bottleneck",
    "authors": [
      "Liangjian Wen",
      "Xiasi Wang",
      "Jianzhuang Liu",
      "Zenglin Xu"
    ],
    "abstract": "Self-supervised learning aims to learn representation that can be effectively\ngeneralized to downstream tasks. Many self-supervised approaches regard two\nviews of an image as both the input and the self-supervised signals, assuming\nthat either view contains the same task-relevant information and the shared\ninformation is (approximately) sufficient for predicting downstream tasks.\nRecent studies show that discarding superfluous information not shared between\nthe views can improve generalization. Hence, the ideal representation is\nsufficient for downstream tasks and contains minimal superfluous information,\ntermed minimal sufficient representation. One can learn this representation by\nmaximizing the mutual information between the representation and the supervised\nview while eliminating superfluous information. Nevertheless, the computation\nof mutual information is notoriously intractable. In this work, we propose an\nobjective termed multi-view entropy bottleneck (MVEB) to learn minimal\nsufficient representation effectively. MVEB simplifies the minimal sufficient\nlearning to maximizing both the agreement between the embeddings of two views\nand the differential entropy of the embedding distribution. Our experiments\nconfirm that MVEB significantly improves performance. For example, it achieves\ntop-1 accuracy of 76.9\\% on ImageNet with a vanilla ResNet-50 backbone on\nlinear evaluation. To the best of our knowledge, this is the new\nstate-of-the-art result with ResNet-50.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by TPAMI",
    "pdf_url": "http://arxiv.org/pdf/2403.19078v1",
    "published_date": "2024-03-28 00:50:02 UTC",
    "updated_date": "2024-03-28 00:50:02 UTC"
  },
  {
    "arxiv_id": "2403.19076v2",
    "title": "Tiny Machine Learning: Progress and Futures",
    "authors": [
      "Ji Lin",
      "Ligeng Zhu",
      "Wei-Ming Chen",
      "Wei-Chen Wang",
      "Song Han"
    ],
    "abstract": "Tiny Machine Learning (TinyML) is a new frontier of machine learning. By\nsqueezing deep learning models into billions of IoT devices and\nmicrocontrollers (MCUs), we expand the scope of AI applications and enable\nubiquitous intelligence. However, TinyML is challenging due to hardware\nconstraints: the tiny memory resource makes it difficult to hold deep learning\nmodels designed for cloud and mobile platforms. There is also limited compiler\nand inference engine support for bare-metal devices. Therefore, we need to\nco-design the algorithm and system stack to enable TinyML. In this review, we\nwill first discuss the definition, challenges, and applications of TinyML. We\nthen survey the recent progress in TinyML and deep learning on MCUs. Next, we\nwill introduce MCUNet, showing how we can achieve ImageNet-scale AI\napplications on IoT devices with system-algorithm co-design. We will further\nextend the solution from inference to training and introduce tiny on-device\ntraining techniques. Finally, we present future directions in this area.\nToday's large model might be tomorrow's tiny model. The scope of TinyML should\nevolve and adapt over time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2206.15472",
    "pdf_url": "http://arxiv.org/pdf/2403.19076v2",
    "published_date": "2024-03-28 00:34:56 UTC",
    "updated_date": "2024-03-29 21:33:39 UTC"
  },
  {
    "arxiv_id": "2403.19073v1",
    "title": "Dataflow-Aware PIM-Enabled Manycore Architecture for Deep Learning Workloads",
    "authors": [
      "Harsh Sharma",
      "Gaurav Narang",
      "Janardhan Rao Doppa",
      "Umit Ogras",
      "Partha Pratim Pande"
    ],
    "abstract": "Processing-in-memory (PIM) has emerged as an enabler for the energy-efficient\nand high-performance acceleration of deep learning (DL) workloads. Resistive\nrandom-access memory (ReRAM) is one of the most promising technologies to\nimplement PIM. However, as the complexity of Deep convolutional neural networks\n(DNNs) grows, we need to design a manycore architecture with multiple\nReRAM-based processing elements (PEs) on a single chip. Existing PIM-based\narchitectures mostly focus on computation while ignoring the role of\ncommunication. ReRAM-based tiled manycore architectures often involve many\nProcessing Elements (PEs), which need to be interconnected via an efficient\non-chip communication infrastructure. Simply allocating more resources (ReRAMs)\nto speed up only computation is ineffective if the communication infrastructure\ncannot keep up with it. In this paper, we highlight the design principles of a\ndataflow-aware PIM-enabled manycore platform tailor-made for various types of\nDL workloads. We consider the design challenges with both 2.5D interposer- and\n3D integration-enabled architectures.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.AR",
    "comment": "Presented at DATE Conference, Valencia, Spain 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19073v1",
    "published_date": "2024-03-28 00:29:15 UTC",
    "updated_date": "2024-03-28 00:29:15 UTC"
  },
  {
    "arxiv_id": "2403.19066v1",
    "title": "Generative Quanta Color Imaging",
    "authors": [
      "Vishal Purohit",
      "Junjie Luo",
      "Yiheng Chi",
      "Qi Guo",
      "Stanley H. Chan",
      "Qiang Qiu"
    ],
    "abstract": "The astonishing development of single-photon cameras has created an\nunprecedented opportunity for scientific and industrial imaging. However, the\nhigh data throughput generated by these 1-bit sensors creates a significant\nbottleneck for low-power applications. In this paper, we explore the\npossibility of generating a color image from a single binary frame of a\nsingle-photon camera. We evidently find this problem being particularly\ndifficult to standard colorization approaches due to the substantial degree of\nexposure variation. The core innovation of our paper is an exposure synthesis\nmodel framed under a neural ordinary differential equation (Neural ODE) that\nallows us to generate a continuum of exposures from a single observation. This\ninnovation ensures consistent exposure in binary images that colorizers take\non, resulting in notably enhanced colorization. We demonstrate applications of\nthe method in single-image and burst colorization and show superior generative\nperformance over baselines. Project website can be found at\nhttps://vishal-s-p.github.io/projects/2023/generative_quanta_color.html.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19066v1",
    "published_date": "2024-03-28 00:11:12 UTC",
    "updated_date": "2024-03-28 00:11:12 UTC"
  }
]
{
  "date": "2025-08-30",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-08-30 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„è®ºæ–‡å¯¼è¯»å‘˜ã€‚\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv ç®€ç›´æ˜¯â€œç¡¬æ ¸â€çš„ä¸€å¤©ï¼**ç¡¬ä»¶é¢†åŸŸ**è¿æ¥é‡ç£…ç‚¸å¼¹ï¼Œæµ™æ±Ÿå¤§å­¦ç­‰å›¢é˜Ÿå‘å¸ƒäº†**æ™¶åœ†çº§ç±»è„‘è®¡ç®—èŠ¯ç‰‡ DarwinWafer**ï¼Œç›´æ¥åœ¨ 300mm æ™¶åœ†ä¸Šé›†æˆäº† 64 ä¸ª Chipletsï¼Œè§„æ¨¡æƒŠäººã€‚**å¤§æ¨¡å‹æ¨ç†**æ–¹é¢ï¼Œâ€œTest-time Computeâ€ï¼ˆæµ‹è¯•æ—¶è®¡ç®—ï¼‰ä¾ç„¶æ˜¯çƒ­ç‚¹ï¼Œå‡ºç°äº†**å¹¶è¡Œæ€è€ƒï¼ˆParallel Thinkingï¼‰**çš„æ–°èŒƒå¼ä»¥åŠé’ˆå¯¹æ¨ç†è§£ç çš„ä¸“ç”¨ Benchmarkã€‚æ­¤å¤–ï¼Œ**AI for Science** åœ¨æ‰‹æœ¯è§†é¢‘ç†è§£ã€åˆ†å­ç”Ÿæˆå’Œç”Ÿç‰©å®‰å…¨æ–¹é¢ä¹Ÿæœ‰æ‰å®çš„å·¥ä½œã€‚\n\nä¸‹é¢æˆ‘ä»¬ç›´å…¥ä¸»é¢˜ï¼Œçœ‹çœ‹ä»Šå¤©å€¼å¾—å…³æ³¨çš„é‚£äº›æ–‡ç« ã€‚\n\n---\n\n### ğŸš€ ç¡¬ä»¶ä¸ç³»ç»Ÿæ•ˆç‡ (Hardcore Hardware & Systems)\n\n**1. [DarwinWafer: A Wafer-Scale Neuromorphic Chip]**\n**DarwinWaferï¼šä¸€ç§æ™¶åœ†çº§ç±»è„‘è®¡ç®—èŠ¯ç‰‡**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™ç»å¯¹æ˜¯ä»Šå¤©çš„å¤´æ¡ã€‚ç ”ç©¶å›¢é˜Ÿå±•ç¤ºäº†ä¸€ç§**æ™¶åœ†çº§ï¼ˆWafer-Scaleï¼‰**çš„ç±»è„‘è®¡ç®—ç³»ç»Ÿã€‚ä¸åŒäºä¼ ç»Ÿçš„ PCB äº’è”ï¼Œä»–ä»¬åœ¨ä¸€ä¸ª 300mm çš„ç¡…ä¸­ä»‹å±‚ä¸Šé›†æˆäº† 64 ä¸ª Darwin3 Chipletsã€‚\n> **å…³é”®æ•°æ®**ï¼šå•æ™¶åœ†åŒ…å« **1.5 äº¿ä¸ªç¥ç»å…ƒ**å’Œ **64 äº¿ä¸ªçªè§¦**ã€‚å®ç°äº†ä½å»¶è¿Ÿçš„æ™¶åœ†çº§å¼‚æ­¥é€šä¿¡ï¼ŒåŠŸè€—çº¦ 100Wï¼Œèƒ½æ•ˆæ¯”æé«˜ï¼ˆ0.64 TSOPS/Wï¼‰ã€‚\n> **æ„ä¹‰**ï¼šè¿™æ˜¯å‘å¤§è§„æ¨¡ã€è„‘çº§è®¡ç®—è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ï¼Œè§£å†³äº†ä¼ ç»Ÿå¤šèŠ¯ç‰‡ç³»ç»Ÿåœ¨å¸¦å®½å’Œå»¶è¿Ÿä¸Šçš„ç“¶é¢ˆï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿæ–‘é©¬é±¼å’Œå°é¼ çš„å¤§è„‘ç½‘ç»œã€‚\n\n**2. [FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern]**\n**FlexLinkï¼šåœ¨ä¸å½±å“ç²¾åº¦çš„æƒ…å†µä¸‹æå‡ 27% çš„ NVLink å¸¦å®½**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹å¤§æ¨¡å‹å¤šèŠ‚ç‚¹è®­ç»ƒçš„é€šä¿¡ç“¶é¢ˆã€‚ç°æœ‰çš„ NCCL é€šå¸¸åªç”¨ NVLinkï¼Œå¯¼è‡´ PCIe å’Œ RDMA ç½‘å¡é—²ç½®ã€‚FlexLink æ˜¯ä¸€ä¸ªèšåˆé€šä¿¡æ¡†æ¶ï¼Œå®ƒå°† NVLinkã€PCIe å’Œ RDMA NICs èšåˆæˆä¸€ä¸ªé«˜æ€§èƒ½é€šä¿¡ Fabricã€‚\n> **æ•ˆæœ**ï¼šåœ¨ H800 GPU é›†ç¾¤ä¸Šï¼ŒAllReduce å’Œ AllGather çš„å¸¦å®½æå‡äº† **26%-27%**ã€‚è¿™æ˜¯ä¸€ä¸ªçº¯è½¯ä»¶çš„ä¼˜åŒ–ï¼Œå¯ä»¥ç›´æ¥ä½œä¸º NCCL çš„æ›¿ä»£å“ã€‚\n\n**3. [KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache]**\n**KVCompï¼šä¸€ç§é«˜æ€§èƒ½ã€LLM æ„ŸçŸ¥çš„ KV Cache æœ‰æŸå‹ç¼©æ¡†æ¶**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šé•¿æ–‡æœ¬æ¨ç†ä¸­ KV Cache æ˜¾å­˜å ç”¨æ˜¯å™©æ¢¦ã€‚KVComp é€šè¿‡ååŒè®¾è®¡å‹ç¼©ç®—æ³•å’Œç³»ç»Ÿæ¶æ„ï¼Œå®ç°äº†å¹³å‡ **47%** (æœ€é«˜ 83%) çš„æ˜¾å­˜å‡å°‘ï¼Œä¸”å‡ ä¹ä¸æŸå¤±æ¨¡å‹ç²¾åº¦ï¼Œéå¸¸é€‚åˆé•¿ä¸Šä¸‹æ–‡åœºæ™¯ã€‚\n\n---\n\n### ğŸ§  LLM æ¨ç†ä¸ Scaling (Reasoning & Test-Time Scaling)\n\n**4. [ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute]**\n**ParaThinkerï¼šåŸç”Ÿå¹¶è¡Œæ€è€ƒâ€”â€”æ‰©å±• LLM æµ‹è¯•æ—¶è®¡ç®—çš„æ–°èŒƒå¼**\n> **æ ¸å¿ƒå‘ç°**ï¼šç°æœ‰çš„ Test-time scalingï¼ˆå¦‚ o1ï¼‰å¤šæ˜¯é¡ºåºæ€è€ƒï¼ˆæ·±åº¦æ‰©å±•ï¼‰ï¼Œå®¹æ˜“é™·å…¥â€œéš§é“è§†é‡â€ï¼ˆTunnel Visionï¼‰ï¼Œå³ä¸€æ­¥é”™æ­¥æ­¥é”™ã€‚\n> **æ–¹æ³•**ï¼šä½œè€…æå‡ºäº†**å¹¶è¡Œæ€è€ƒï¼ˆWidth Scalingï¼‰**ã€‚è®­ç»ƒæ¨¡å‹å¹¶è¡Œç”Ÿæˆå¤šæ¡ä¸åŒçš„æ¨ç†è·¯å¾„ï¼Œæœ€åæ±‡æ€»ã€‚\n> **æ•ˆæœ**ï¼šåœ¨ 1.5B å’Œ 7B æ¨¡å‹ä¸Šï¼Œå¹¶è¡Œæ€è€ƒæ¯”é¡ºåºæ€è€ƒå¸¦æ¥çš„å‡†ç¡®ç‡æå‡æ›´æ˜¾è‘—ï¼ˆ+12.3% / +7.5%ï¼‰ï¼Œä¸”å»¶è¿Ÿå¢åŠ å¾ˆå°ã€‚è¿™æŒ‘æˆ˜äº†â€œæ€ç»´é“¾å¿…é¡»å¾ˆé•¿â€çš„ä¼ ç»Ÿè§‚ç‚¹ã€‚\n\n**5. [Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling]**\n**Scaling Up, Speeding Upï¼šç”¨äºé«˜æ•ˆ LLM æµ‹è¯•æ—¶æ‰©å±•çš„æŠ•æœºè§£ç åŸºå‡†æµ‹è¯•**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šTest-time scaling ä¼šäº§ç”Ÿå¤§é‡é‡å¤çš„æ¨ç†æ­¥éª¤ï¼Œè®¡ç®—æ•ˆç‡ä½ã€‚æœ¬æ–‡å»ºç«‹äº†ç¬¬ä¸€ä¸ªä¸“é—¨è¯„ä¼°**æŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰**åœ¨ Test-time scaling åœºæ™¯ä¸‹è¡¨ç°çš„ Benchmarkã€‚\n> **å‘ç°**ï¼šç®€å•çš„ N-gram æ–¹æ³•åœ¨æ•æ‰é‡å¤æ¨ç†æ¨¡å¼ä¸Šå‡ºå¥‡åœ°æœ‰æ•ˆã€‚\n\n---\n\n### ğŸ§¬ AI for Science & Medicine (Bio, Chem & Physics)\n\n**6. [SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding]**\n**SurgLLMï¼šä¸€ç§å…·æœ‰ç©ºé—´èšç„¦å’Œæ—¶é—´æ„ŸçŸ¥çš„é€šç”¨å¤§å‹å¤šæ¨¡æ€æ‰‹æœ¯è§†é¢‘ç†è§£æ¨¡å‹**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæ‰‹æœ¯è§†é¢‘ç†è§£éœ€è¦æé«˜çš„æ—¶ç©ºç²¾åº¦ã€‚SurgLLM é€šè¿‡â€œæ‰‹æœ¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥é¢„è®­ç»ƒâ€å’Œâ€œæ—¶é—´æ„ŸçŸ¥å¾®è°ƒâ€ï¼Œå¢å¼ºäº†å¯¹å™¨æ¢°ï¼ˆInstrument-centricï¼‰å’Œæ‰‹æœ¯æµç¨‹çš„ç†è§£ã€‚\n> **åº”ç”¨**ï¼šåœ¨æ‰‹æœ¯å­—å¹•ç”Ÿæˆã€é—®ç­”ç­‰ä»»åŠ¡ä¸Šåˆ·æ–°äº† SOTAï¼Œæ˜¯åŒ»ç–— AI è½åœ°çš„é‡è¦å°è¯•ã€‚\n\n**7. [Training Text-to-Molecule Models with Context-Aware Tokenization]**\n**ä½¿ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†è¯è®­ç»ƒæ–‡æœ¬åˆ°åˆ†å­æ¨¡å‹**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šç°æœ‰çš„ Text-to-Molecule æ¨¡å‹å¤šåŸºäºåŸå­çº§åˆ†è¯ï¼Œå¿½ç•¥äº†æ•´ä½“ç»“æ„ã€‚æœ¬æ–‡æå‡ºäº† **CAMT5**ï¼Œå¼•å…¥äº†**å­ç»“æ„çº§ï¼ˆSubstructure-levelï¼‰**çš„ Tokenizationï¼ˆæ¯”å¦‚è‹¯ç¯ä½œä¸ºä¸€ä¸ª Tokenï¼‰ã€‚\n> **æ•ˆæœ**ï¼šä»…ä½¿ç”¨ 2% çš„è®­ç»ƒ Token å°±è¶…è¶Šäº† SOTAï¼Œè¯æ˜äº†åœ¨è¯¥é¢†åŸŸâ€œæ‡‚åŒ–å­¦ç»“æ„â€æ¯”â€œå †æ•°æ®â€æ›´é‡è¦ã€‚\n\n**8. [Resilient Biosecurity in the Era of AI-Enabled Bioweapons]**\n**AI èµ‹èƒ½ç”Ÿç‰©æ­¦å™¨æ—¶ä»£çš„å¼¹æ€§ç”Ÿç‰©å®‰å…¨**\n> **è­¦ç¤º**ï¼šè¿™æ˜¯ä¸€ç¯‡æ³¼å†·æ°´çš„æ–‡ç« ã€‚ä½œè€…æµ‹è¯•äº† AlphaFold 3 ç­‰é¡¶çº§æ¨¡å‹ï¼Œå‘ç°å®ƒä»¬åœ¨é¢„æµ‹**ç—…æ¯’-å®¿ä¸»ç›¸äº’ä½œç”¨**ï¼ˆå¦‚ SARS-CoV-2 å˜ä½“ï¼‰æ—¶è¡¨ç°ä¸ä½³ï¼Œç”šè‡³æ— æ³•è¯†åˆ«å·²çŸ¥çš„ç»“åˆã€‚\n> **ç»“è®º**ï¼šç°æœ‰çš„ AI é¢„æµ‹è¿‡æ»¤å™¨ä¸è¶³ä»¥ä½œä¸ºç”Ÿç‰©æ­¦å™¨çš„é˜²ç«å¢™ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¿«é€Ÿçš„å®éªŒéªŒè¯å’Œç›‘ç®¡æ¡†æ¶ã€‚\n\n---\n\n### ğŸ¤– Agent & Robotics (æ™ºèƒ½ä½“ä¸å…·èº«æ™ºèƒ½)\n\n**9. [TimeCopilot]**\n**TimeCopilotï¼šæ—¶é—´åºåˆ—é¢„æµ‹çš„å¼€æº Agent æ¡†æ¶**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šå°†å¤šä¸ªæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰ä¸ LLM ç»“åˆã€‚å®ƒèƒ½è‡ªåŠ¨åŒ–æ•´ä¸ªé¢„æµ‹æµç¨‹ï¼ˆç‰¹å¾åˆ†æã€æ¨¡å‹é€‰æ‹©ã€ç”Ÿæˆè§£é‡Šï¼‰ã€‚\n> **ç‰¹ç‚¹**ï¼šç»Ÿä¸€ APIï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢æœªæ¥ï¼Œåœ¨ GIFT-Eval åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ã€‚\n\n**10. [NEWSAGENT: Benchmarking Multimodal Agents as Journalists with Real-World Newswriting Tasks]**\n**NEWSAGENTï¼šä»¥ç°å®ä¸–ç•Œæ–°é—»å†™ä½œä¸ºä»»åŠ¡çš„å¤šæ¨¡æ€è®°è€… Agent åŸºå‡†æµ‹è¯•**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šè¯„ä¼° Agent èƒ½å¦åƒè®°è€…ä¸€æ ·å·¥ä½œï¼ˆæœç´¢ã€ç­›é€‰ã€ç¼–è¾‘ã€é‡å†™ï¼‰ã€‚åŒ…å« 6k ä¸ªçœŸå®æ–°é—»æ¡ˆä¾‹ã€‚\n> **å‘ç°**ï¼šç›®å‰çš„ Agent æ“…é•¿æ£€ç´¢äº‹å®ï¼Œä½†åœ¨**è§„åˆ’ï¼ˆPlanningï¼‰**å’Œ**å™äº‹æ•´åˆï¼ˆNarrative Integrationï¼‰**æ–¹é¢ä»ç„¶å¾ˆæŒ£æ‰ã€‚\n\n**11. [VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding]**\n**VideoRewardBenchï¼šè§†é¢‘ç†è§£å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„ç»¼åˆè¯„ä¼°**\n> **ç—›ç‚¹**ï¼šè§†é¢‘ç”Ÿæˆçš„ RLHF éœ€è¦å¥½çš„ Reward Modelã€‚\n> **å‘ç°**ï¼šç›®å‰çš„è§†é¢‘ Reward Model å¾ˆå¼±ï¼ŒGPT-4o å‡†ç¡®ç‡ä»… 57%ï¼Œå¼€æºæœ€å¼º Qwen2.5-VL-72B ä¹Ÿä»… 53%ã€‚åŸºäº RL è®­ç»ƒçš„ Reward Model å¹¶æ²¡æœ‰è¡¨ç°å‡ºæ›´å¥½çš„è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸é˜²å¾¡ (Safety & Security)\n\n**12. [The Resurgence of GCG Adversarial Attacks on Large Language Models]**\n**GCG å¯¹æŠ—æ€§æ”»å‡»åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸Šçš„æ­»ç°å¤ç‡ƒ**\n> **æ ¸å¿ƒå‘ç°**ï¼šé‡æ–°è¯„ä¼°äº† GCGï¼ˆåŸºäºæ¢¯åº¦çš„è¶Šç‹±æ”»å‡»ï¼‰ã€‚å‘ç°**ä»£ç ç›¸å…³çš„æç¤ºè¯ï¼ˆCoding Promptsï¼‰**æ¯”æ™®é€šå®‰å…¨æç¤ºè¯æ›´å®¹æ˜“è¢«æ”»ç ´ï¼Œæ¨ç†èƒ½åŠ›æœ¬èº«å¯èƒ½æˆä¸ºè¢«åˆ©ç”¨çš„æ”»å‡»å‘é‡ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¶Šå¤§ï¼Œæ”»å‡»æˆåŠŸç‡åè€Œä¸‹é™ï¼ˆLoss æ™¯è§‚æ›´å¤æ‚ï¼‰ã€‚\n\n**13. [Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models]**\n**SPO-VLMï¼šè§†è§‰è¯­è¨€æ¨¡å‹è¶Šç‹±é˜²å¾¡**\n> **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µé˜²å¾¡æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨ä¸åŒæ•°æ®æºè®¡ç®—â€œå¼•å¯¼å‘é‡â€æŠ‘åˆ¶æœ‰å®³è¡Œä¸ºï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡åå¥½ä¼˜åŒ–ï¼ˆPOï¼‰å¾®è°ƒè¿™äº›å‘é‡ã€‚æ—¢èƒ½é˜²è¶Šç‹±ï¼Œåˆä¸å½±å“è§†è§‰ç†è§£èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ¦ å…¶ä»–æœ‰è¶£çš„ç ”ç©¶ (Fun & Niche)\n\n**14. [Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts]**\n**å»çš„æ³¨æ„åŠ›ï¼šæ¢ç´¢è§†é¢‘æ‰©æ•£ä¸­çš„æ³¨æ„åŠ›å›¾è°±**\n> **è¶£ç‚¹**ï¼šè¿™æ˜¯ä¸€ç¯‡**è‰ºæœ¯ä¸ AI** ç»“åˆçš„æ–‡ç« ã€‚ä½œè€…å¼€å‘å·¥å…·æå–ç”Ÿæˆè§†é¢‘æ¨¡å‹ï¼ˆWan modelï¼‰çš„ Cross-attention mapï¼Œç”¨æ¥è¿›è¡Œè‰ºæœ¯åˆ›ä½œå’Œåˆ†æã€‚è®© AI çš„â€œå…³æ³¨ç‚¹â€å¯è§†åŒ–æˆä¸ºä¸€ç§è‰ºæœ¯åª’ä»‹ã€‚\n\n**15. [Artificial Intelligence-Based Analysis of Ice Cream Melting Behavior Under Various Ingredients]**\n**åŸºäº AI çš„ä¸åŒé…æ–™ä¸‹å†°æ·‡æ·‹èåŒ–è¡Œä¸ºåˆ†æ**\n> **è¶£ç‚¹**ï¼šç ”ç©¶äº†åˆºæ§è±†èƒ¶ã€ç“œå°”èƒ¶ç­‰æ·»åŠ å‰‚å¯¹å†°æ·‡æ·‹èåŒ–çš„å½±å“ã€‚æ€ä¹ˆç ”ç©¶çš„ï¼Ÿç”¨ OpenCV å’Œ Python å¤„ç†å†°æ·‡æ·‹èåŒ–çš„å»¶æ—¶æ‘„å½±è§†é¢‘ï¼ç»“è®ºï¼šæ·»åŠ å‰‚ç¡®å®èƒ½æ„å»ºç¨³å®šçš„æ°”å®¤ç»“æ„ï¼Œè®©å†°æ·‡æ·‹åŒ–äº†ä¹Ÿèƒ½ä¿æŒæ³¡æ²«çŠ¶ã€‚ï¼ˆå¤å¤©å¿…è¯»ğŸ¦ï¼‰\n\n---\n\nğŸ‰ **ç»“è¯­**ï¼š\nä»Šå¤©çš„è®ºæ–‡å±•ç°äº† AI å‘å±•çš„ä¸¤ä¸ªæç«¯ï¼šä¸€ç«¯æ˜¯ **DarwinWafer** è¿™æ ·å®å¤§çš„ç¡¬ä»¶åŸºç¡€è®¾æ–½æ„å»ºï¼Œå¦ä¸€ç«¯æ˜¯ **Ice Cream Analysis** è¿™æ ·ç”Ÿæ´»åŒ–ã€å¾®å°çš„åº”ç”¨ã€‚è€Œä¸­é—´å±‚ï¼ŒLLM æ­£åœ¨åŠªåŠ›é€šè¿‡**å¹¶è¡Œæ€è€ƒ**å’Œ**Agent æ¡†æ¶**çªç ´æ¨ç†å’Œåº”ç”¨çš„ç“¶é¢ˆã€‚\n\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2509.02612v1",
      "title": "Is Synthetic Image Augmentation Useful for Imbalanced Classification Problems? Case-Study on the MIDOG2025 Atypical Cell Detection Competition",
      "title_zh": "åˆæˆå›¾åƒå¢å¼ºæ˜¯å¦å¯¹ä¸å¹³è¡¡åˆ†ç±»é—®é¢˜æœ‰æ•ˆï¼Ÿâ€”â€”åŸºäº MIDOG2025 éå…¸å‹ç»†èƒæ£€æµ‹ç«èµ›çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Leire Benito-Del-Valle",
        "Pedro A. Moreno-SÃ¡nchez",
        "Itziar Egusquiza",
        "Itsaso Vitoria",
        "Artzai PicÃ³n",
        "Cristina LÃ³pez-Saratxaga",
        "Adrian Galdran"
      ],
      "abstract": "The MIDOG 2025 challenge extends prior work on mitotic figure detection by introducing a new Track 2 on atypical mitosis classification. This task aims to distinguish normal from atypical mitotic figures in histopathology images, a clinically relevant but highly imbalanced and cross-domain problem. We investigated two complementary backbones: (i) ConvNeXt-Small, pretrained on ImageNet, and (ii) a histopathology-specific ViT from Lunit trained via self-supervision. To address the strong prevalence imbalance (9408 normal vs. 1741 atypical), we synthesized additional atypical examples to approximate class balance and compared models trained with real-only vs. real+synthetic data. Using five-fold cross-validation, both backbones reached strong performance (mean AUROC approximately 95 percent), with ConvNeXt achieving slightly higher peaks while Lunit exhibited greater fold-to-fold stability. Synthetic balancing, however, did not lead to consistent improvements. On the organizers' preliminary hidden test set, explicitly designed as an out-of-distribution debug subset, ConvNeXt attained the highest AUROC (95.4 percent), whereas Lunit remained competitive on balanced accuracy. These findings suggest that both ImageNet and domain-pretrained backbones are viable for atypical mitosis classification, with domain-pretraining conferring robustness and ImageNet pretraining reaching higher peaks, while naive synthetic balancing has limited benefit. Full hidden test set results will be reported upon challenge completion.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ MIDOG 2025 æŒ‘æˆ˜èµ›ä¸­çš„éå…¸å‹æœ‰ä¸åˆ†è£‚ (atypical mitosis) åˆ†ç±»ä»»åŠ¡ï¼Œæ¢è®¨äº†åœ¨ç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­åŒºåˆ†æ­£å¸¸ä¸éå…¸å‹æœ‰ä¸åˆ†è£‚ç»†èƒçš„æ–¹æ³•ï¼Œå¹¶é‡ç‚¹å…³æ³¨äº†æ•°æ®é«˜åº¦å¤±è¡¡å’Œè·¨åŸŸè¯†åˆ«çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å¯¹æ¯”äº†ä¸¤ç§äº’è¡¥çš„éª¨å¹²ç½‘ç»œï¼Œåˆ†åˆ«æ˜¯åŸºäº ImageNet é¢„è®­ç»ƒçš„ ConvNeXt-Small å’Œç»„ç»‡ç—…ç†å­¦ä¸“ç”¨çš„è‡ªç›‘ç£ Lunit ViTã€‚ä¸ºäº†è§£å†³ç±»åˆ«å¤±è¡¡é—®é¢˜ï¼Œç ”ç©¶äººå‘˜é€šè¿‡åˆæˆé¢å¤–çš„éå…¸å‹æ ·æœ¬å°è¯•å®ç°ç±»åˆ«å¹³è¡¡ï¼Œå¹¶å¯¹æ¯”äº†ä»…ä½¿ç”¨çœŸå®æ•°æ®ä¸ç»“åˆåˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸¤ç§éª¨å¹²ç½‘ç»œåœ¨äº”æŠ˜äº¤å‰éªŒè¯ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡ AUROC è¾¾åˆ°çº¦ 95%ï¼Œå…¶ä¸­ ConvNeXt è¾¾åˆ°äº†æ›´é«˜çš„æ€§èƒ½å³°å€¼ï¼Œè€Œ Lunit å±•ç°å‡ºæ›´å¼ºçš„ç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°ç®€å•çš„åˆæˆå¹³è¡¡æŠ€æœ¯å¹¶æ²¡æœ‰å¸¦æ¥æŒç»­çš„æ€§èƒ½æå‡ã€‚æœ€ç»ˆç»“è®ºè¡¨æ˜ï¼ŒImageNet é¢„è®­ç»ƒå’Œé¢†åŸŸç‰¹å®šé¢„è®­ç»ƒå¯¹äºè¯¥ä»»åŠ¡å‡æ˜¯å¯è¡Œçš„æ–¹æ¡ˆï¼Œå‰è€…åœ¨ç²¾åº¦ä¸Šé™ä¸Šæ›´å…·ä¼˜åŠ¿ï¼Œåè€…åˆ™æä¾›äº†æ›´å¥½çš„é²æ£’æ€§ï¼Œè€Œæœ´ç´ çš„åˆæˆå¢å¼ºåœ¨å¤„ç†æ­¤ç±»ä¸å¹³è¡¡åˆ†ç±»é—®é¢˜æ—¶æ”¶ç›Šæœ‰é™ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "version 0, to be updated; submitted to midog 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.02612v1",
      "published_date": "2025-08-30 23:37:20 UTC",
      "updated_date": "2025-08-30 23:37:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:22:24.865331+00:00"
    },
    {
      "arxiv_id": "2509.00634v1",
      "title": "Enabling Trustworthy Federated Learning via Remote Attestation for Mitigating Byzantine Threats",
      "title_zh": "åŸºäºè¿œç¨‹è¯æ˜çš„æŠµå¾¡æ‹œå åº­å¨èƒçš„å¯ä¿¡è”é‚¦å­¦ä¹ ",
      "authors": [
        "Chaoyu Zhang",
        "Heng Jin",
        "Shanghao Shi",
        "Hexuan Yu",
        "Sydney Johns",
        "Y. Thomas Hou",
        "Wenjing Lou"
      ],
      "abstract": "Federated Learning (FL) has gained significant attention for its privacy-preserving capabilities, enabling distributed devices to collaboratively train a global model without sharing raw data. However, its distributed nature forces the central server to blindly trust the local training process and aggregate uncertain model updates, making it susceptible to Byzantine attacks from malicious participants, especially in mission-critical scenarios. Detecting such attacks is challenging due to the diverse knowledge across clients, where variations in model updates may stem from benign factors, such as non-IID data, rather than adversarial behavior. Existing data-driven defenses struggle to distinguish malicious updates from natural variations, leading to high false positive rates and poor filtering performance.\n  To address this challenge, we propose Sentinel, a remote attestation (RA)-based scheme for FL systems that regains client-side transparency and mitigates Byzantine attacks from a system security perspective. Our system employs code instrumentation to track control-flow and monitor critical variables in the local training process. Additionally, we utilize a trusted training recorder within a Trusted Execution Environment (TEE) to generate an attestation report, which is cryptographically signed and securely transmitted to the server. Upon verification, the server ensures that legitimate client training processes remain free from program behavior violation or data manipulation, allowing only trusted model updates to be aggregated into the global model. Experimental results on IoT devices demonstrate that Sentinel ensures the trustworthiness of the local training integrity with low runtime and memory overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å­¦ä¹ (Federated Learning)ä¸­éš¾ä»¥åŒºåˆ†æ¶æ„æ”»å‡»ä¸æ•°æ®éç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID data)å·®å¼‚çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºSentinelçš„è¿œç¨‹éªŒè¯(Remote Attestation)æ–¹æ¡ˆï¼Œæ—¨åœ¨ä»ç³»ç»Ÿå®‰å…¨è§’åº¦ç¼“è§£æ‹œå åº­æ”»å‡»(Byzantine attacks)ã€‚Sentinelé€šè¿‡ä»£ç æ’æ¡©(code instrumentation)æŠ€æœ¯è¿½è¸ªæœ¬åœ°è®­ç»ƒçš„æ§åˆ¶æµå¹¶ç›‘æ§å…³é”®å˜é‡ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„é€æ˜åº¦ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨å¯ä¿¡æ‰§è¡Œç¯å¢ƒ(Trusted Execution Environment)ä¸­çš„è®°å½•å™¨ç”ŸæˆåŠ å¯†ç­¾åçš„éªŒè¯æŠ¥å‘Šï¼Œä½¿æœåŠ¡å™¨èƒ½å¤Ÿè¯†åˆ«å¹¶å‰”é™¤å­˜åœ¨ç¨‹åºè¡Œä¸ºè¿è§„æˆ–æ•°æ®ç¯¡æ”¹çš„ä¸å¯ä¿¡æ›´æ–°ã€‚è¿™ç§æœºåˆ¶å®ç°äº†å¯¹æœ¬åœ°è®­ç»ƒå®Œæ•´æ€§çš„ä¸»åŠ¨é˜²å¾¡ï¼Œè€Œéä»…ä»…ä¾èµ–äºäº‹åçš„æ•°æ®åˆ†æã€‚åœ¨ç‰©è”ç½‘(IoT)è®¾å¤‡ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒSentinelèƒ½å¤Ÿä»¥è¾ƒä½çš„è¿è¡Œå’Œå†…å­˜å¼€é”€ç¡®ä¿è”é‚¦å­¦ä¹ ç³»ç»Ÿçš„å¯ä¿¡åº¦ä¸å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00634v1",
      "published_date": "2025-08-30 23:36:22 UTC",
      "updated_date": "2025-08-30 23:36:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:22:50.559460+00:00"
    },
    {
      "arxiv_id": "2509.05326v2",
      "title": "Zero-Knowledge Proofs in Sublinear Space",
      "title_zh": "äºšçº¿æ€§ç©ºé—´ä¸‹çš„é›¶çŸ¥è¯†è¯æ˜",
      "authors": [
        "Logan Nye"
      ],
      "abstract": "Zero-knowledge proofs allow verification of computations without revealing private information. However, existing systems require memory proportional to the computation size, which has historically limited use in large-scale applications and on mobile and edge devices. We solve this fundamental bottleneck by developing, to our knowledge, the first proof system with sublinear memory requirements for mainstream cryptographic constructions. Our approach processes computations in blocks using a space-efficient tree algorithm, reducing memory from linear scaling to square-root scaling--from $Î˜(T)$ to $O(\\sqrt{T} + \\log T \\log\\log T)$ for computation size $T$--while maintaining the same proof generation time through a constant number of streaming passes. For widely-used linear polynomial commitment schemes (KZG/IPA), our method produces identical proofs and verification when using the same parameters and hashing only aggregate commitments into the challenge generation, preserving proof size and security. Hash-based systems also achieve square-root memory scaling though with slightly different proof structures. This advance enables zero-knowledge proofs on everyday devices and makes previously infeasible large computations verifiable, fundamentally democratizing access to privacy-preserving computation. Space-efficient zero knowledge proof systems create opportunities to reshape how trust is established in digital systems--from enabling widespread participation in decentralized networks to making verifiable scientific computing practical at unprecedented scales.",
      "tldr_zh": "è¯¥ç ”ç©¶è§£å†³äº†é›¶çŸ¥è¯†è¯æ˜ï¼ˆZero-Knowledge Proofsï¼‰ä¸­ç°æœ‰ç³»ç»Ÿå†…å­˜éœ€æ±‚ä¸è®¡ç®—è§„æ¨¡æˆæ­£æ¯”çš„æ ¸å¿ƒç“¶é¢ˆï¼Œæå‡ºäº†é¦–ä¸ªé’ˆå¯¹ä¸»æµå¯†ç å­¦æ„é€ çš„äºšçº¿æ€§ï¼ˆsublinearï¼‰å†…å­˜éœ€æ±‚è¯æ˜ç³»ç»Ÿã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸€ç§ç©ºé—´é«˜æ•ˆçš„æ ‘å½¢ç®—æ³•ï¼ˆtree algorithmï¼‰å°†è®¡ç®—åˆ’åˆ†ä¸ºå—è¿›è¡Œå¤„ç†ï¼Œä»è€Œå°†å†…å­˜å ç”¨ä»çº¿æ€§è§„æ¨¡ $\\Theta(T)$ é™ä½è‡³å¹³æ–¹æ ¹è§„æ¨¡ $O(\\sqrt{T} + \\log T \\log \\log T)$ã€‚åœ¨æ˜¾è‘—é™ä½å†…å­˜éœ€æ±‚çš„åŒæ—¶ï¼Œç³»ç»Ÿé€šè¿‡æ’å®šæ¬¡æ•°çš„æµå¼ä¼ é€’ï¼ˆstreaming passesï¼‰ä¿æŒäº†åŸæœ‰çš„è¯æ˜ç”Ÿæˆæ—¶é—´ã€‚å¯¹äºå¹¿æ³›ä½¿ç”¨çš„çº¿æ€§å¤šé¡¹å¼æ‰¿è¯ºæ–¹æ¡ˆï¼ˆKZG/IPAï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨ç›¸åŒå‚æ•°ä¸‹ç”Ÿæˆçš„è¯æ˜å’ŒéªŒè¯è¿‡ç¨‹å®Œå…¨ä¸€è‡´ï¼ŒæˆåŠŸä¿ç•™äº†åŸæœ‰çš„è¯æ˜å¤§å°å’Œå®‰å…¨æ€§ã€‚è¿™ä¸€è¿›å±•ä½¿å¾—åœ¨æ—¥å¸¸ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œé›¶çŸ¥è¯†è¯æ˜ï¼ˆZero-Knowledge Proofsï¼‰æˆä¸ºå¯èƒ½ï¼Œå¹¶ä½¿ä»¥å¾€éš¾ä»¥å®ç°çš„å¤§è§„æ¨¡å¯éªŒè¯è®¡ç®—å˜å¾—å¯è¡Œã€‚è¯¥ç©ºé—´é«˜æ•ˆçš„ç³»ç»Ÿä¸ºå»ä¸­å¿ƒåŒ–ç½‘ç»œå’Œç§‘å­¦è®¡ç®—ä¸­å»ºç«‹ä¿¡ä»»æä¾›äº†æ–°çš„å¯èƒ½ï¼Œä»æ ¹æœ¬ä¸Šæ¨åŠ¨äº†éšç§ä¿æŠ¤è®¡ç®—çš„æ™®åŠã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "23 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.05326v2",
      "published_date": "2025-08-30 23:22:54 UTC",
      "updated_date": "2025-09-17 17:25:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:22:25.896279+00:00"
    },
    {
      "arxiv_id": "2509.00631v2",
      "title": "Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers",
      "title_zh": "åŸºäºæ—¶åºèåˆ Transformer çš„ç¨€ç– GNSS æ•°æ®ç”µç¦»å±‚é¢„æŠ¥",
      "authors": [
        "Giacomo Acciarini",
        "Simone Mestici",
        "Halil Kelebek",
        "Linnea Wolniewicz",
        "Michael Vergalla",
        "Madhulika Guhathakurta",
        "Umaa Rebbapragada",
        "Bala Poduval",
        "AtÄ±lÄ±m GÃ¼neÅŸ Baydin",
        "Frank Soboczenski"
      ],
      "abstract": "The ionosphere critically influences Global Navigation Satellite Systems (GNSS), satellite communications, and Low Earth Orbit (LEO) operations, yet accurate prediction of its variability remains challenging due to nonlinear couplings between solar, geomagnetic, and thermospheric drivers. Total Electron Content (TEC), a key ionospheric parameter, is derived from GNSS observations, but its reliable forecasting is limited by the sparse nature of global measurements and the limited accuracy of empirical models, especially during strong space weather conditions. In this work, we present a machine learning framework for ionospheric TEC forecasting that leverages Temporal Fusion Transformers (TFT) to predict sparse ionosphere data. Our approach accommodates heterogeneous input sources, including solar irradiance, geomagnetic indices, and GNSS-derived vertical TEC, and applies preprocessing and temporal alignment strategies. Experiments spanning 2010-2025 demonstrate that the model achieves robust predictions up to 24 hours ahead, with root mean square errors as low as 3.33 TECU. Results highlight that solar EUV irradiance provides the strongest predictive signals. Beyond forecasting accuracy, the framework offers interpretability through attention-based analysis, supporting both operational applications and scientific discovery. To encourage reproducibility and community-driven development, we release the full implementation as the open-source toolkit \\texttt{ionopy}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µç¦»å±‚æ€»ç”µå­å«é‡(Total Electron Content, TEC)é¢„æµ‹ä¸­å­˜åœ¨çš„éçº¿æ€§è€¦åˆåŠGNSSæ•°æ®ç¨€ç–æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ—¶é—´èåˆå˜å‹å™¨(Temporal Fusion Transformers, TFT)çš„æœºå™¨å­¦ä¹ é¢„æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ•´åˆäº†å¤ªé˜³è¾ç…§åº¦(solar irradiance)ã€åœ°ç£æŒ‡æ•°(geomagnetic indices)å’ŒGNSSå‚ç›´TECç­‰å¤šç§å¼‚æ„æ•°æ®æºï¼Œå¹¶åº”ç”¨äº†ä¸“é—¨çš„é¢„å¤„ç†ä¸æ—¶é—´å¯¹é½ç­–ç•¥ã€‚é€šè¿‡å¯¹2010å¹´è‡³2025å¹´æ•°æ®çš„å®éªŒéªŒè¯ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå®ç°æå‰24å°æ—¶çš„ç¨³å¥é¢„æµ‹ï¼Œå‡æ–¹æ ¹è¯¯å·®(RMSE)ä½è‡³3.33 TECUã€‚ç ”ç©¶ç»“æœå¼ºè°ƒï¼Œå¤ªé˜³æç´«å¤–(EUV)è¾ç…§åº¦æä¾›äº†æœ€å¼ºçš„é¢„æµ‹ä¿¡å·ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŸºäºæ³¨æ„åŠ›çš„åˆ†ææä¾›äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œæ”¯æŒä¸šåŠ¡åº”ç”¨ä¸ç§‘å­¦å‘ç°ã€‚æœ€åï¼Œç ”ç©¶å›¢é˜Ÿå°†è¯¥å®ç°ä½œä¸ºå¼€æºå·¥å…·åŒ…ionopyå‘å¸ƒï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºé©±åŠ¨çš„å¼€å‘ä¸å¤ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00631v2",
      "published_date": "2025-08-30 23:08:19 UTC",
      "updated_date": "2025-10-02 06:16:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:22:26.590733+00:00"
    },
    {
      "arxiv_id": "2509.00626v6",
      "title": "Towards Methane Detection Onboard Satellites",
      "title_zh": "è¿ˆå‘æ˜Ÿè½½ç”²çƒ·æ£€æµ‹",
      "authors": [
        "Maggie Chen",
        "Hala Lambdouar",
        "Luca Marini",
        "Laura MartÃ­nez-Ferrer",
        "Chris Bridges",
        "Giacomo Acciarini"
      ],
      "abstract": "Methane is a potent greenhouse gas and a major driver of climate change, making its timely detection critical for effective mitigation. Machine learning (ML) deployed onboard satellites can enable rapid detection while reducing downlink costs, supporting faster response systems. Conventional methane detection methods often rely on image processing techniques, such as orthorectification to correct geometric distortions and matched filters to enhance plume signals. We introduce a novel approach that bypasses these preprocessing steps by using \\textit{unorthorectified} data (UnorthoDOS). We find that ML models trained on this dataset achieve performance comparable to those trained on orthorectified data. Moreover, we also train models on an orthorectified dataset, showing that they can outperform the matched filter baseline (mag1c). We release model checkpoints and two ML-ready datasets comprising orthorectified and unorthorectified hyperspectral images from the Earth Surface Mineral Dust Source Investigation (EMIT) sensor at https://huggingface.co/datasets/SpaceML/UnorthoDOS , along with code at https://github.com/spaceml-org/plume-hunter.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å«æ˜Ÿä¸Šåˆ©ç”¨æœºå™¨å­¦ä¹ (Machine learning)å®ç°ç”²çƒ·å®æ—¶æ£€æµ‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡é™ä½æ•°æ®ä¸‹è¡Œæˆæœ¬æ¥æ”¯æŒå¿«é€Ÿçš„æ°”å€™å˜åŒ–ç¼“è§£ç³»ç»Ÿã€‚é’ˆå¯¹ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•è¿‡åº¦ä¾èµ–æ­£å°„æ ¡æ­£(orthorectification)å’ŒåŒ¹é…æ»¤æ³¢å™¨(matched filter)ç­‰å¤æ‚é¢„å¤„ç†æ­¥éª¤çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç›´æ¥åˆ©ç”¨æœªç»æ ¡æ­£æ•°æ®(unorthorectified data)çš„æ–°å‹æ–¹æ³•UnorthoDOSã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨UnorthoDOSæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¸æ­£å°„æ ¡æ­£æ•°æ®ç›¸å½“ï¼Œä¸”å…¶åœ¨æ­£å°„æ ¡æ­£æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºå‡†æ¨¡å‹mag1cã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå…¬å¼€äº†åŸºäºEMITä¼ æ„Ÿå™¨çš„é«˜å…‰è°±å›¾åƒ(hyperspectral images)æ•°æ®é›†å’Œplume-hunterä»£ç åº“ï¼Œä¸ºå«æ˜Ÿç”²çƒ·æ£€æµ‹ç®—æ³•çš„åç»­å¼€å‘æä¾›äº†é‡è¦çš„æ•°æ®æ”¯æ’‘ä¸å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00626v6",
      "published_date": "2025-08-30 22:54:00 UTC",
      "updated_date": "2025-11-17 09:23:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:22:44.795245+00:00"
    },
    {
      "arxiv_id": "2509.00625v2",
      "title": "NetGent: Agent-Based Automation of Network Application Workflows",
      "title_zh": "NetGentï¼šåŸºäºæ™ºèƒ½ä½“çš„ç½‘ç»œåº”ç”¨å·¥ä½œæµè‡ªåŠ¨åŒ–",
      "authors": [
        "Jaber Daneshamooz",
        "Eugene Vuong",
        "Laasya Koduru",
        "Sanjay Chandrasekaran",
        "Arpit Gupta"
      ],
      "abstract": "We present NetGent, an AI-agent framework for automating complex application workflows to generate realistic network traffic datasets. Developing generalizable ML models for networking requires data collection from network environments with traffic that results from a diverse set of real-world web applications. However, using existing browser automation tools that are diverse, repeatable, realistic, and efficient remains fragile and costly. NetGent addresses this challenge by allowing users to specify workflows as natural-language rules that define state-dependent actions. These abstract specifications are compiled into nondeterministic finite automata (NFAs), which a state synthesis component translates into reusable, executable code. This design enables deterministic replay, reduces redundant LLM calls through state caching, and adapts quickly when application interfaces change. In experiments, NetGent automated more than 50+ workflows spanning video-on-demand streaming, live video streaming, video conferencing, social media, and web scraping, producing realistic traffic traces while remaining robust to UI variability. By combining the flexibility of language-based agents with the reliability of compiled execution, NetGent provides a scalable foundation for generating the diverse, repeatable datasets needed to advance ML in networking.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NetGentï¼Œä¸€ä¸ªç”¨äºè‡ªåŠ¨åŒ–å¤æ‚åº”ç”¨å·¥ä½œæµä»¥ç”ŸæˆçœŸå®ç½‘ç»œæµé‡æ•°æ®é›†çš„ AI-agent æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·åœ¨å¤„ç†å¤šæ ·åŒ–å’Œç°å®æµé‡æ—¶å­˜åœ¨çš„è„†å¼±æ€§ä¸é«˜æˆæœ¬é—®é¢˜ï¼ŒNetGent å…è®¸ç”¨æˆ·ä½¿ç”¨è‡ªç„¶è¯­è¨€è§„åˆ™å®šä¹‰çŠ¶æ€ç›¸å…³çš„åŠ¨ä½œã€‚è¿™äº›æŠ½è±¡è§„èŒƒè¢«ç¼–è¯‘ä¸ºéç¡®å®šæ€§æœ‰é™è‡ªåŠ¨æœº (NFAs)ï¼Œå¹¶ç”±çŠ¶æ€åˆæˆç»„ä»¶è½¬åŒ–ä¸ºå¯é‡ç”¨çš„å¯æ‰§è¡Œä»£ç ã€‚è¯¥è®¾è®¡æ”¯æŒç¡®å®šæ€§é‡æ”¾ï¼Œé€šè¿‡çŠ¶æ€ç¼“å­˜å‡å°‘äº†å†—ä½™çš„ LLM è°ƒç”¨ï¼Œå¹¶èƒ½å¿«é€Ÿé€‚åº” UI ç•Œé¢çš„å˜åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒNetGent æˆåŠŸè‡ªåŠ¨åŒ–äº†æ¶‰åŠè§†é¢‘ç‚¹æ’­ã€åœ¨çº¿ä¼šè®®å’Œç¤¾äº¤åª’ä½“ç­‰ 50 å¤šä¸ªå·¥ä½œæµï¼Œåœ¨ä¿æŒ UI ç¨³å¥æ€§çš„åŒæ—¶ç”Ÿæˆäº†é«˜è´¨é‡çš„æµé‡è¿½è¸ªæ•°æ®ã€‚é€šè¿‡ç»“åˆè¯­è¨€æ™ºèƒ½ä½“çš„çµæ´»æ€§ä¸ç¼–è¯‘æ‰§è¡Œçš„å¯é æ€§ï¼Œè¯¥æ¡†æ¶ä¸ºç½‘ç»œæœºå™¨å­¦ä¹ æ‰€éœ€çš„æ•°æ®é›†ç”Ÿæˆæä¾›äº†å¯æ‰©å±•çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00625v2",
      "published_date": "2025-08-30 22:47:15 UTC",
      "updated_date": "2025-11-14 01:23:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:22:46.090786+00:00"
    },
    {
      "arxiv_id": "2509.00623v1",
      "title": "A Multi-Strategy Approach for AI-Generated Text Detection",
      "title_zh": "AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹çš„å¤šç­–ç•¥æ–¹æ³•",
      "authors": [
        "Ali Zain",
        "Sareem Farooqui",
        "Muhammad Rafi"
      ],
      "abstract": "This paper presents presents three distinct systems developed for the M-DAIGT shared task on detecting AI generated content in news articles and academic abstracts. The systems includes: (1) A fine-tuned RoBERTa-base classifier, (2) A classical TF-IDF + Support Vector Machine (SVM) classifier , and (3) An Innovative ensemble model named Candace, leveraging probabilistic features extracted from multiple Llama-3.2 models processed by a customTransformer encoder.The RoBERTa-based system emerged as the most performant, achieving near-perfect results on both development and test sets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹M-DAIGTå…±äº«ä»»åŠ¡å¼€å‘äº†ä¸‰ç§ä¸åŒçš„ç³»ç»Ÿï¼Œæ—¨åœ¨æ£€æµ‹æ–°é—»æ–‡ç« å’Œå­¦æœ¯æ‘˜è¦ä¸­ç”±äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹ã€‚ç³»ç»Ÿæ–¹æ¡ˆåŒ…æ‹¬ç»è¿‡å¾®è°ƒçš„RoBERTa-baseåˆ†ç±»å™¨ã€ç»å…¸çš„TF-IDFç»“åˆæ”¯æŒå‘é‡æœº(SVM)æ¨¡å‹ï¼Œä»¥åŠä¸€ç§åä¸ºCandaceçš„åˆ›æ–°é›†æˆæ¶æ„ã€‚Candaceæ¨¡å‹é€šè¿‡è‡ªå®šä¹‰çš„Transformerç¼–ç å™¨å¤„ç†ä»å¤šä¸ªLlama-3.2æ¨¡å‹ä¸­æå–çš„æ¦‚ç‡ç‰¹å¾ï¼Œå±•ç°äº†æ–°é¢–çš„æ£€æµ‹æ€è·¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºRoBERTaçš„åˆ†ç±»ç³»ç»Ÿåœ¨æ‰€æœ‰æ–¹æ¡ˆä¸­è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œåœ¨å¼€å‘é›†å’Œæµ‹è¯•é›†ä¸Šå‡å–å¾—äº†æ¥è¿‘å®Œç¾çš„æ£€æµ‹å‡†ç¡®ç‡ã€‚è¯¥å·¥ä½œä¸ºè¯†åˆ«å’Œåº”å¯¹AIç”Ÿæˆæ–‡æœ¬çš„æŒ‘æˆ˜æä¾›äº†å¤šç»´åº¦çš„æŠ€æœ¯å‚è€ƒä¸è¯„ä¼°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00623v1",
      "published_date": "2025-08-30 22:37:35 UTC",
      "updated_date": "2025-08-30 22:37:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:22:57.492796+00:00"
    },
    {
      "arxiv_id": "2509.00622v1",
      "title": "BALM-TSF: Balanced Multimodal Alignment for LLM-Based Time Series Forecasting",
      "title_zh": "BALM-TSFï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹æ—¶é—´åºåˆ—é¢„æµ‹çš„å‡è¡¡å¤šæ¨¡æ€å¯¹é½",
      "authors": [
        "Shiqiao Zhou",
        "Holger SchÃ¶ner",
        "Huanbo Lyu",
        "Edouard FouchÃ©",
        "Shuo Wang"
      ],
      "abstract": "Time series forecasting is a long-standing and highly challenging research topic. Recently, driven by the rise of large language models (LLMs), research has increasingly shifted from purely time series methods toward harnessing textual modalities to enhance forecasting performance. However, the vast discrepancy between text and temporal data often leads current multimodal architectures to over-emphasise one modality while neglecting the other, resulting in information loss that harms forecasting performance. To address this modality imbalance, we introduce BALM-TSF (Balanced Multimodal Alignment for LLM-Based Time Series Forecasting), a lightweight time series forecasting framework that maintains balance between the two modalities. Specifically, raw time series are processed by the time series encoder, while descriptive statistics of raw time series are fed to an LLM with learnable prompt, producing compact textual embeddings. To ensure balanced cross-modal context alignment of time series and textual embeddings, a simple yet effective scaling strategy combined with a contrastive objective then maps these textual embeddings into the latent space of the time series embeddings. Finally, the aligned textual semantic embeddings and time series embeddings are together integrated for forecasting. Extensive experiments on standard benchmarks show that, with minimal trainable parameters, BALM-TSF achieves state-of-the-art performance in both long-term and few-shot forecasting, confirming its ability to harness complementary information from text and time series. Code is available at https://github.com/ShiqiaoZhou/BALM-TSF.",
      "tldr_zh": "é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ—¶é—´åºåˆ—é¢„æµ‹(Time Series Forecasting)ä¸­å­˜åœ¨çš„æ–‡æœ¬ä¸æ—¶åºæ•°æ®æ¨¡æ€å¤±è¡¡é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†BALM-TSFï¼Œä¸€ç§è½»é‡çº§çš„å¤šæ¨¡æ€å¹³è¡¡å¯¹é½æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ—¶é—´åºåˆ—ç¼–ç å™¨å¤„ç†åŸå§‹åºåˆ—ï¼Œå¹¶åˆ©ç”¨å¸¦æœ‰å¯å­¦ä¹ æç¤º(Learnable Prompt)çš„LLMä»æè¿°æ€§ç»Ÿè®¡æ•°æ®ä¸­ç”Ÿæˆç´§å‡‘çš„æ–‡æœ¬åµŒå…¥(Textual Embeddings)ã€‚ä¸ºäº†ç¡®ä¿è·¨æ¨¡æ€ä¸Šä¸‹æ–‡çš„å¹³è¡¡å¯¹é½ï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç¼©æ”¾ç­–ç•¥ç»“åˆå¯¹æ¯”ç›®æ ‡(Contrastive Objective)ï¼Œå°†æ–‡æœ¬è¯­ä¹‰æ˜ å°„åˆ°æ—¶åºç‰¹å¾çš„æ½œç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBALM-TSFåœ¨é•¿æ—¶é¢„æµ‹å’Œå°‘æ ·æœ¬é¢„æµ‹(Few-shot Forecasting)ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›(SOTA)çš„æ°´å¹³ã€‚è¯¥æ–¹æ³•è¯æ˜äº†åœ¨ä»…éœ€æå°‘å¯è®­ç»ƒå‚æ•°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¹³è¡¡æ•è·æ–‡æœ¬ä¸æ—¶åºçš„äº’è¡¥ä¿¡æ¯å¯ä»¥æ˜¾è‘—æå‡é¢„æµ‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00622v1",
      "published_date": "2025-08-30 22:31:55 UTC",
      "updated_date": "2025-08-30 22:31:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:22:48.082817+00:00"
    },
    {
      "arxiv_id": "2509.04479v1",
      "title": "No Clustering, No Routing: How Transformers Actually Process Rare Tokens",
      "title_zh": "éèšç±»ï¼Œéè·¯ç”±ï¼šTransformer å¤„ç†ç½•è§æ ‡è®°çš„çœŸå®æœºåˆ¶",
      "authors": [
        "Jing Liu"
      ],
      "abstract": "Large language models struggle with rare token prediction, yet the mechanisms driving their specialization remain unclear. Prior work identified specialized ``plateau'' neurons for rare tokens following distinctive three-regime influence patterns \\cite{liu2025emergent}, but their functional organization is unknown. We investigate this through neuron influence analyses, graph-based clustering, and attention head ablations in GPT-2 XL and Pythia models. Our findings show that: (1) rare token processing requires additional plateau neurons beyond the power-law regime sufficient for common tokens, forming dual computational regimes; (2) plateau neurons are spatially distributed rather than forming modular clusters; and (3) attention mechanisms exhibit no preferential routing to specialists. These results demonstrate that rare token specialization arises through distributed, training-driven differentiation rather than architectural modularity, preserving context-sensitive flexibility while achieving adaptive capacity allocation.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶è°ƒæŸ¥äº†å¤§è¯­è¨€æ¨¡å‹å¦‚ä½•å¤„ç†ç¨€æœ‰è¯å…ƒ(rare tokens)çš„é¢„æµ‹æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ­¤å‰å‘ç°çš„â€œé«˜åŸç¥ç»å…ƒâ€(plateau neurons)çš„åŠŸèƒ½ç»„ç»‡ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¯¹ GPT-2 XL å’Œ Pythia æ¨¡å‹è¿›è¡Œç¥ç»å…ƒå½±å“åˆ†æã€åŸºäºå›¾çš„èšç±»ä»¥åŠæ³¨æ„åŠ›å¤´æ¶ˆèå®éªŒï¼Œæ·±å…¥æ¢ç´¢äº†è¿™äº›ç¥ç»å…ƒçš„åˆ†å¸ƒä¸è¿ä½œè§„å¾‹ã€‚ç ”ç©¶å‘ç°ï¼Œç¨€æœ‰è¯å…ƒçš„å¤„ç†éœ€è¦è¶…å‡ºæ™®é€šè¯å…ƒæ‰€éœ€çš„å¹‚å¾‹æœºåˆ¶(power-law regime)ä¹‹å¤–çš„é¢å¤–é«˜åŸç¥ç»å…ƒï¼Œä»è€Œåœ¨æ¨¡å‹å†…éƒ¨å½¢æˆäº†åŒé‡è®¡ç®—æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›é«˜åŸç¥ç»å…ƒåœ¨ç©ºé—´ä¸Šæ˜¯åˆ†æ•£åˆ†å¸ƒçš„ï¼Œå¹¶æœªå½¢æˆæ¨¡å—åŒ–çš„èšç±»(modular clusters)ï¼Œä¸”æ³¨æ„åŠ›æœºåˆ¶(attention mechanisms)åœ¨å¤„ç†è¿‡ç¨‹ä¸­ä¹Ÿæœªè¡¨ç°å‡ºå¯¹è¿™äº›ä¸“é—¨ç¥ç»å…ƒçš„ä¼˜å…ˆè·¯ç”±(preferential routing)ã€‚è¯¥ç ”ç©¶æœ€ç»ˆè¯æ˜äº†ç¨€æœ‰è¯å…ƒçš„ä¸“é—¨åŒ–æºäºåˆ†å¸ƒå¼çš„ã€è®­ç»ƒé©±åŠ¨çš„å·®å¼‚åŒ–è€Œéæ¶æ„ä¸Šçš„æ¨¡å—åŒ–ï¼Œè¿™ä½¿å¾—æ¨¡å‹åœ¨å®ç°è‡ªé€‚åº”å®¹é‡åˆ†é…çš„åŒæ—¶èƒ½å¤Ÿä¿ç•™ä¸Šä¸‹æ–‡æ•æ„Ÿçš„çµæ´»æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04479v1",
      "published_date": "2025-08-30 22:20:41 UTC",
      "updated_date": "2025-08-30 22:20:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:23:34.001899+00:00"
    },
    {
      "arxiv_id": "2509.05325v1",
      "title": "SynDelay: A Synthetic Dataset for Delivery Delay Prediction",
      "title_zh": "SynDelayï¼šé¢å‘é…é€å»¶è¿Ÿé¢„æµ‹çš„åˆæˆæ•°æ®é›†",
      "authors": [
        "Liming Xu",
        "Yunbo Long",
        "Alexandra Brintrup"
      ],
      "abstract": "Artificial intelligence (AI) is transforming supply chain management, yet progress in predictive tasks -- such as delivery delay prediction -- remains constrained by the scarcity of high-quality, openly available datasets. Existing datasets are often proprietary, small, or inconsistently maintained, hindering reproducibility and benchmarking. We present SynDelay, a synthetic dataset designed for delivery delay prediction. Generated using an advanced generative model trained on real-world data, SynDelay preserves realistic delivery patterns while ensuring privacy. Although not entirely free of noise or inconsistencies, it provides a challenging and practical testbed for advancing predictive modelling. To support adoption, we provide baseline results and evaluation metrics as initial benchmarks, serving as reference points rather than state-of-the-art claims. SynDelay is publicly available through the Supply Chain Data Hub, an open initiative promoting dataset sharing and benchmarking in supply chain AI. We encourage the community to contribute datasets, models, and evaluation practices to advance research in this area. All code is openly accessible at https://supplychaindatahub.org.",
      "tldr_zh": "æœ¬ç ”ç©¶ä»‹ç»äº† SynDelayï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºäº¤ä»˜å»¶è¿Ÿé¢„æµ‹ (delivery delay prediction) è®¾è®¡çš„åˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ä¾›åº”é“¾ç®¡ç†ä¸­é«˜è´¨é‡ã€å…¬å¼€æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†é€šè¿‡åœ¨çœŸå®æ•°æ®ä¸Šè®­ç»ƒçš„é«˜çº§ç”Ÿæˆæ¨¡å‹ (generative model) ç”Ÿæˆï¼Œåœ¨ç¡®ä¿éšç§çš„åŒæ—¶ï¼Œä¿ç•™äº†çœŸå®çš„äº¤ä»˜æ¨¡å¼ (delivery patterns)ã€‚å°½ç®¡å­˜åœ¨ä¸€å®šçš„å™ªå£°æˆ–ä¸ä¸€è‡´æ€§ï¼ŒSynDelay ä¸ºæ¨è¿›é¢„æµ‹å»ºæ¨¡ (predictive modelling) æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä¸”å®ç”¨çš„æµ‹è¯•å¹³å°ã€‚ä¸ºäº†ä¾¿äºé‡‡ç”¨ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æä¾›äº†åŸºçº¿ç»“æœ (baseline results) å’Œè¯„ä¼°æŒ‡æ ‡ (evaluation metrics) ä½œä¸ºåˆå§‹åŸºå‡†å‚è€ƒã€‚SynDelay ç›®å‰å·²é€šè¿‡ Supply Chain Data Hub å…¬å¼€å‘å¸ƒï¼Œè¿™æ˜¯ä¸€é¡¹æ—¨åœ¨ä¿ƒè¿›ä¾›åº”é“¾ AI é¢†åŸŸæ•°æ®é›†å…±äº«å’ŒåŸºå‡†æµ‹è¯•çš„å¼€æ”¾å€¡è®®ã€‚è¯¥é¡¹ç›®çš„ç›¸å…³ä»£ç å·²å®Œå…¨å¼€æºï¼Œæ—¨åœ¨é¼“åŠ±ç¤¾åŒºè´¡çŒ®æ›´å¤šæ•°æ®é›†å’Œæ¨¡å‹ï¼Œå…±åŒæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper incldues 1 figure and 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.05325v1",
      "published_date": "2025-08-30 21:54:37 UTC",
      "updated_date": "2025-08-30 21:54:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:23:05.084780+00:00"
    },
    {
      "arxiv_id": "2509.00616v3",
      "title": "TimeCopilot",
      "title_zh": "TimeCopilot",
      "authors": [
        "Azul Garza",
        "RenÃ©e Rosillo"
      ],
      "abstract": "We introduce TimeCopilot, the first open-source agentic framework for forecasting that combines multiple Time Series Foundation Models (TSFMs) with Large Language Models (LLMs) through a single unified API. TimeCopilot automates the forecasting pipeline: feature analysis, model selection, cross-validation, and forecast generation, while providing natural language explanations and supporting direct queries about the future. The framework is LLM-agnostic, compatible with both commercial and open-source models, and supports ensembles across diverse forecasting families. Results on the large-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art probabilistic forecasting performance at low cost. Our framework provides a practical foundation for reproducible, explainable, and accessible agentic forecasting systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†TimeCopilotï¼Œè¿™æ˜¯é¦–ä¸ªå¼€æºçš„é¢„æµ‹ä»£ç†æ¡†æ¶(agentic framework)ï¼Œé€šè¿‡ç»Ÿä¸€çš„APIå°†å¤šç§æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹(TSFMs)ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)ç›¸ç»“åˆã€‚è¯¥æ¡†æ¶å®ç°äº†é¢„æµ‹æµç¨‹çš„å…¨é¢è‡ªåŠ¨åŒ–ï¼Œæ¶µç›–äº†ç‰¹å¾åˆ†æ(feature analysis)ã€æ¨¡å‹é€‰æ‹©(model selection)ã€äº¤å‰éªŒè¯(cross-validation)ä»¥åŠé¢„æµ‹ç”Ÿæˆ(forecast generation)ã€‚TimeCopilotä¸ä»…èƒ½å¤Ÿæä¾›è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œè¿˜æ”¯æŒç”¨æˆ·ç›´æ¥é’ˆå¯¹æœªæ¥é¢„æµ‹è¿›è¡ŒæŸ¥è¯¢ï¼Œå¹¶å…·å¤‡LLM-agnosticç‰¹æ€§ï¼Œå¯å…¼å®¹å•†ä¸šå’Œå¼€æºæ¨¡å‹å¹¶æ”¯æŒè·¨é¢„æµ‹ç³»åˆ—çš„é›†æˆ(ensembles)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤§å‹åŸºå‡†æµ‹è¯•GIFT-Evalä¸Šï¼ŒTimeCopilotä»¥è¾ƒä½æˆæœ¬å®ç°äº†æœ€å…ˆè¿›çš„(state-of-the-art)æ¦‚ç‡é¢„æµ‹æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä¸ºæ„å»ºå…·æœ‰å¯é‡ç°æ€§ã€å¯è§£é‡Šæ€§å’Œæ˜“ç”¨æ€§çš„è‡ªä¸»ä»£ç†é¢„æµ‹ç³»ç»Ÿæä¾›äº†å®ç”¨çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00616v3",
      "published_date": "2025-08-30 21:48:51 UTC",
      "updated_date": "2025-11-07 01:43:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:24:23.561731+00:00"
    },
    {
      "arxiv_id": "2509.00615v1",
      "title": "Federated Survival Analysis with Node-Level Differential Privacy: Private Kaplan-Meier Curves",
      "title_zh": "æ»¡è¶³èŠ‚ç‚¹çº§å·®åˆ†éšç§çš„è”é‚¦ç”Ÿå­˜åˆ†æï¼šéšç§ä¿æŠ¤ Kaplan-Meier æ›²çº¿",
      "authors": [
        "Narasimha Raghavan Veeraragavan",
        "Jan Franz NygÃ¥rd"
      ],
      "abstract": "We investigate how to calculate Kaplan-Meier survival curves across multiple health-care jurisdictions while protecting patient privacy with node-level differential privacy. Each site discloses its curve only once, adding Laplace noise whose scale is determined by the length of the common time grid; the server then averages the noisy curves, so the overall privacy budget remains unchanged. We benchmark four one-shot smoothing techniques: Discrete Cosine Transform, Haar Wavelet shrinkage, adaptive Total-Variation denoising, and a parametric Weibull fit on the NCCTG lung-cancer cohort under five privacy levels and three partition scenarios (uniform, moderately skewed, highly imbalanced). Total-Variation gives the best mean accuracy, whereas the frequency-domain smoothers offer stronger worst-case robustness and the Weibull model shows the most stable behaviour at the strictest privacy setting. Across all methods the released curves keep the empirical log-rank type-I error below fifteen percent for privacy budgets of 0.5 and higher, demonstrating that clinically useful survival information can be shared without iterative training or heavy cryptography.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åœ¨ä¿æŠ¤æ‚£è€…éšç§çš„å‰æä¸‹ï¼Œåˆ©ç”¨èŠ‚ç‚¹çº§å·®å¼‚éšç§(Node-Level Differential Privacy)åœ¨å¤šä¸ªåŒ»ç–—æœºæ„é—´è®¡ç®—Kaplan-Meierç”Ÿå­˜æ›²çº¿ã€‚ç ”ç©¶é‡‡ç”¨å•æ¬¡æŠ«éœ²æœºåˆ¶ï¼Œå„ç«™ç‚¹é€šè¿‡æ·»åŠ Laplaceå™ªå£°æ¥ä¿æŠ¤æ•°æ®ï¼Œå¹¶ç”±æœåŠ¡å™¨è¿›è¡Œå¹³å‡åŒ–å¤„ç†ä»¥ä¿æŒéšç§é¢„ç®—ç¨³å®šã€‚ä½œè€…åœ¨NCCTGè‚ºç™Œé˜Ÿåˆ—ä¸Šå¯¹æ¯”äº†Discrete Cosine Transformã€Haar Wavelet shrinkageã€adaptive Total-Variation denoisingå’Œparametric Weibull fitå››ç§å¹³æ»‘æŠ€æœ¯åœ¨ä¸åŒéšç§çº§åˆ«ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTotal-Variationåœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œè€Œé¢‘ç‡åŸŸå¹³æ»‘å·¥å…·åœ¨æœ€åæƒ…å†µä¸‹æ›´å…·é²æ£’æ€§ï¼ŒWeibullæ¨¡å‹åœ¨ä¸¥è‹›éšç§è®¾å®šä¸‹æœ€ä¸ºç¨³å®šã€‚å½“éšç§é¢„ç®—(privacy budgets)å¤§äºæˆ–ç­‰äº0.5æ—¶ï¼Œæ‰€æœ‰æ–¹æ³•å‡èƒ½å°†log-rankç¬¬ä¸€ç±»é”™è¯¯ç»´æŒåœ¨15%ä»¥ä¸‹ã€‚è¿™è¯æ˜äº†åœ¨æ— éœ€è¿­ä»£è®­ç»ƒæˆ–é‡å‹åŠ å¯†çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶å¯ä»¥å®ç°å®‰å…¨ä¸”å…·æœ‰ä¸´åºŠä»·å€¼çš„ç”Ÿå­˜åˆ†æä¿¡æ¯å…±äº«ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "This is the author's accepted version of the paper in IEEE FLTA 2025. The final version of record will appear in Proceedings of the IEEE International Conference on Federated Learning Technologies and Applications (FLTA 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.00615v1",
      "published_date": "2025-08-30 21:47:56 UTC",
      "updated_date": "2025-08-30 21:47:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:24:17.194545+00:00"
    },
    {
      "arxiv_id": "2509.05324v1",
      "title": "Perception Graph for Cognitive Attack Reasoning in Augmented Reality",
      "title_zh": "å¢å¼ºç°å®ä¸­ç”¨äºè®¤çŸ¥æ”»å‡»æ¨ç†çš„æ„ŸçŸ¥å›¾",
      "authors": [
        "Rongqian Chen",
        "Shu Hong",
        "Rifatul Islam",
        "Mahdi Imani",
        "G. Gary Tan",
        "Tian Lan"
      ],
      "abstract": "Augmented reality (AR) systems are increasingly deployed in tactical environments, but their reliance on seamless human-computer interaction makes them vulnerable to cognitive attacks that manipulate a user's perception and severely compromise user decision-making. To address this challenge, we introduce the Perception Graph, a novel model designed to reason about human perception within these systems. Our model operates by first mimicking the human process of interpreting key information from an MR environment and then representing the outcomes using a semantically meaningful structure. We demonstrate how the model can compute a quantitative score that reflects the level of perception distortion, providing a robust and measurable method for detecting and analyzing the effects of such cognitive attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¢å¼ºç°å®(AR)ç³»ç»Ÿåœ¨æˆ˜æœ¯ç¯å¢ƒä¸­æ˜“å—è®¤çŸ¥æ”»å‡»(cognitive attacks)æ“çºµæ„ŸçŸ¥å¹¶æŸå®³å†³ç­–çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Perception Graph çš„æ–°å‹æ¨ç†æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡æ¨¡ä»¿äººç±»åœ¨æ··åˆç°å®(MR)ç¯å¢ƒä¸­è§£é‡Šå…³é”®ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œå°†æ„ŸçŸ¥ç»“æœè½¬åŒ–ä¸ºå…·æœ‰è¯­ä¹‰æ„ä¹‰çš„ç»“æ„åŒ–è¡¨ç¤ºã€‚Perception Graph èƒ½å¤Ÿè®¡ç®—åæ˜ æ„ŸçŸ¥æ‰­æ›²ç¨‹åº¦çš„å®šé‡è¯„åˆ†ï¼Œä¸ºæ£€æµ‹å’Œåˆ†æè®¤çŸ¥æ”»å‡»å¯¹ç”¨æˆ·å†³ç­–çš„å½±å“æä¾›äº†ä¸€ç§ç¨³å¥ä¸”å¯è¡¡é‡çš„æ‰‹æ®µã€‚è¯¥æ¨¡å‹ä¸ä»…èƒ½å¤Ÿå¯¹äººç±»æ„ŸçŸ¥è¿›è¡Œå½¢å¼åŒ–å»ºæ¨¡ï¼Œè¿˜ä¸ºæ„å»ºå…·å¤‡é˜²å¾¡èƒ½åŠ›çš„æ™ºèƒ½å¢å¼ºç°å®ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„è¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ACM MobiHoc XR Security workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.05324v1",
      "published_date": "2025-08-30 20:48:32 UTC",
      "updated_date": "2025-08-30 20:48:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:24:25.059039+00:00"
    },
    {
      "arxiv_id": "2509.13328v1",
      "title": "Dual Actor DDPG for Airborne STAR-RIS Assisted Communications",
      "title_zh": "é¢å‘æœºè½½ STAR-RIS è¾…åŠ©é€šä¿¡çš„åŒè¡ŒåŠ¨è€… DDPG ç®—æ³•",
      "authors": [
        "Danish Rizvi",
        "David Boyle"
      ],
      "abstract": "This study departs from the prevailing assumption of independent Transmission and Reflection Coefficients (TRC) in Airborne Simultaneous Transmit and Reflect Reconfigurable Intelligent Surface (STAR-RIS) research. Instead, we explore a novel multi-user downlink communication system that leverages a UAV-mounted STAR-RIS (Aerial-STAR) incorporating a coupled TRC phase shift model. Our key contributions include the joint optimization of UAV trajectory, active beamforming vectors at the base station, and passive RIS TRCs to enhance communication efficiency, while considering UAV energy constraints. We design the TRC as a combination of discrete and continuous actions, and propose a novel Dual Actor Deep Deterministic Policy Gradient (DA-DDPG) algorithm. The algorithm relies on two separate actor networks for high-dimensional hybrid action space. We also propose a novel harmonic mean index (HFI)-based reward function to ensure communication fairness amongst users. For comprehensive analysis, we study the impact of RIS size on UAV aerodynamics showing that it increases drag and energy demand. Simulation results demonstrate that the proposed DA-DDPG algorithm outperforms conventional DDPG and DQN-based solutions by 24% and 97%, respectively, in accumulated reward. Three-dimensional UAV trajectory optimization achieves 28% higher communication efficiency compared to two-dimensional and altitude optimization. The HFI based reward function provides 41% lower QoS denial rates as compared to other benchmarks. The mobile Aerial-STAR system shows superior performance over fixed deployed counterparts, with the coupled phase STAR-RIS outperforming dual Transmit/Reflect RIS and conventional RIS setups. These findings highlight the potential of Aerial-STAR systems and the effectiveness of our proposed DA-DDPG approach in optimizing their performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ— äººæœº(UAV)æ­è½½åŒæ­¥é€å°„ä¸åå°„å¯é‡æ„æ™ºèƒ½è¡¨é¢(STAR-RIS)è¾…åŠ©çš„å¤šç”¨æˆ·ä¸‹è¡Œé“¾è·¯é€šä¿¡ç³»ç»Ÿï¼Œå¹¶å¼•å…¥äº†è€¦åˆTRC(Transmission and Reflection Coefficients)ç›¸ä½åç§»æ¨¡å‹ã€‚ä½œè€…è”åˆä¼˜åŒ–äº†UAVè½¨è¿¹ã€åŸºç«™çš„æœ‰æºæ³¢æŸèµ‹å½¢å‘é‡ä»¥åŠRISçš„æ— æºTRCï¼Œä»¥åœ¨æ»¡è¶³UAVèƒ½é‡çº¦æŸçš„åŒæ—¶æå‡é€šä¿¡æ•ˆç‡ã€‚ä¸ºäº†åº”å¯¹é«˜ç»´æ··åˆåŠ¨ä½œç©ºé—´ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŒå‚ä¸è€…æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦(DA-DDPG)ç®—æ³•ï¼Œå¹¶åˆ©ç”¨åŸºäºè°ƒå’Œå¹³å‡æŒ‡æ•°(HFI)çš„å¥–åŠ±å‡½æ•°æ¥ä¿éšœé€šä¿¡å…¬å¹³æ€§ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜æ·±å…¥åˆ†æäº†RISå°ºå¯¸å¯¹UAVç©ºæ°”åŠ¨åŠ›å­¦åŠèƒ½è€—çš„å½±å“ã€‚ä»¿çœŸå®éªŒè¡¨æ˜ï¼ŒDA-DDPGç®—æ³•åœ¨ç´¯ç§¯å¥–åŠ±æ–¹é¢æ˜¾è‘—ä¼˜äºDDPGå’ŒDQNï¼Œä¸”ä¸‰ç»´è½¨è¿¹ä¼˜åŒ–ç›¸è¾ƒäºä½ç»´åº¦æ–¹æ¡ˆæå‡äº†28%çš„é€šä¿¡æ•ˆç‡ã€‚åŸºäºHFIçš„å¥–åŠ±å‡½æ•°å°†QoSæ‹’ç»ç‡é™ä½äº†41%ï¼Œè¯æ˜äº†ç§»åŠ¨Aerial-STARç³»ç»Ÿåœ¨å¤æ‚é€šä¿¡åœºæ™¯ä¸‹çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.IT",
        "cs.NI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13328v1",
      "published_date": "2025-08-30 20:10:15 UTC",
      "updated_date": "2025-08-30 20:10:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:24:29.870098+00:00"
    },
    {
      "arxiv_id": "2509.05323v2",
      "title": "Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts",
      "title_zh": "å»ä¹‹æ³¨æ„ï¼šé¢å‘ XAIxArts çš„è§†é¢‘æ‰©æ•£æ¨¡å‹æ³¨æ„åŠ›å›¾æ¢ç´¢",
      "authors": [
        "Adam Cole",
        "Mick Grierson"
      ],
      "abstract": "This paper presents an artistic and technical investigation into the attention mechanisms of video diffusion transformers. Inspired by early video artists who manipulated analog video signals to create new visual aesthetics, this study proposes a method for extracting and visualizing cross-attention maps in generative video models. Built on the open-source Wan model, our tool provides an interpretable window into the temporal and spatial behavior of attention in text-to-video generation. Through exploratory probes and an artistic case study, we examine the potential of attention maps as both analytical tools and raw artistic material. This work contributes to the growing field of Explainable AI for the Arts (XAIxArts), inviting artists to reclaim the inner workings of AI as a creative medium.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ¢ç´¢è§†é¢‘æ‰©æ•£è½¬æ¢å™¨ (Video Diffusion Transformers) æ³¨æ„åŠ›æœºåˆ¶çš„è‰ºæœ¯ä¸æŠ€æœ¯èåˆæ–¹æ³•ã€‚è¯¥ç ”ç©¶åŸºäºå¼€æºçš„ Wan æ¨¡å‹ï¼Œå¼€å‘äº†ä¸€å¥—ç”¨äºæå–å’Œå¯è§†åŒ–æ–‡æœ¬ç”Ÿæˆè§†é¢‘ä¸­è·¨æ³¨æ„åŠ›å›¾ (Cross-Attention Maps) çš„å·¥å…·ï¼Œä¸ºç†è§£ç”Ÿæˆæ¨¡å‹åœ¨æ—¶é—´å’Œç©ºé—´ç»´åº¦ä¸Šçš„æ³¨æ„åŠ›è¡Œä¸ºæä¾›äº†ä¸€ä¸ªå¯è§£é‡Šçš„çª—å£ã€‚å—æ—©æœŸè§†é¢‘è‰ºæœ¯å®¶æ“ä½œæ¨¡æ‹Ÿä¿¡å·çš„å¯å‘ï¼Œä½œè€…é€šè¿‡è‰ºæœ¯æ¡ˆä¾‹ç ”ç©¶æ¢è®¨äº†å°†æ³¨æ„åŠ›å›¾ä½œä¸ºåˆ†æå·¥å…·å’Œè‰ºæœ¯åˆ›ä½œåŸææ–™çš„æ½œåŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºå¯è§£é‡Šäººå·¥æ™ºèƒ½è‰ºæœ¯ (XAIxArts) é¢†åŸŸåšå‡ºäº†é‡è¦è´¡çŒ®ï¼Œæ—¨åœ¨é‚€è¯·è‰ºæœ¯å®¶å°† AI çš„å†…éƒ¨é€»è¾‘å’Œè¿è¡Œæœºåˆ¶é‡æ–°å®šä¹‰ä¸ºä¸€ç§å…¨æ–°çš„åˆ›æ„è¡¨ç°åª’ä»‹ã€‚",
      "categories": [
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "3rd international workshop on eXplainable AI for the Arts (XAIxArts) at the ACM Creativity and Cognition Conference June 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.05323v2",
      "published_date": "2025-08-30 19:46:18 UTC",
      "updated_date": "2025-09-09 12:40:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:24:04.155405+00:00"
    },
    {
      "arxiv_id": "2509.00579v1",
      "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache",
      "title_zh": "KVCompï¼šé¢å‘ KV Cache çš„é«˜æ€§èƒ½ã€LLM æ„ŸçŸ¥å‹æœ‰æŸå‹ç¼©æ¡†æ¶",
      "authors": [
        "Bo Jiang",
        "Taolue Yang",
        "Youyuan Liu",
        "Chengming Zhang",
        "Xubin He",
        "Sian Jin"
      ],
      "abstract": "Transformer-based large language models (LLMs) demonstrate impressive potential in various practical applications. However, long context inference poses a significant challenge due to the enormous memory requirements of the key-value (KV) cache, which can scale to multiple gigabytes as sequence length and batch size increase. In this paper, we present KVComp, a generic and efficient KV cache management framework optimized for long-text generation that synergistically works with both latency-critical and throughput-critical inference systems. KVComp employs novel lossy compression techniques specifically designed for KV cache data characteristics, featuring careful co-design of compression algorithms and system architecture. Our approach maintains compatibility with the growing nature of KV cache while preserving high computational efficiency. Experimental results show that KVComp achieves on average 47\\% and up to 83\\% higher memory reduction rate compared to existing methods with little/no model accuracy degradation. Furthermore, KVComp achieves extremely high execution throughput, effectively reducing decompression overhead and, in some cases, even accelerating the matrix-vector multiplication operation and outperform cuBLAS-based attention kernels with less data movement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KVCompï¼Œä¸€ä¸ªä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹(LLMs)é•¿æ–‡æœ¬ç”Ÿæˆè®¾è®¡çš„é«˜æ€§èƒ½ã€æ„ŸçŸ¥æ¨¡å‹ç‰¹æ€§çš„KV Cacheæœ‰æŸå‹ç¼©(Lossy Compression)æ¡†æ¶ã€‚ä¸ºäº†åº”å¯¹é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­KV Cacheå¯¹å†…å­˜çš„å·¨å¤§éœ€æ±‚ï¼ŒKVCompé€šè¿‡ç®—æ³•ä¸ç³»ç»Ÿæ¶æ„çš„ååŒè®¾è®¡ï¼Œå¼•å…¥äº†é’ˆå¯¹KV Cacheæ•°æ®ç‰¹å¾å®šåˆ¶çš„å‹ç¼©æŠ€æœ¯ã€‚è¯¥æ¡†æ¶åœ¨æ”¯æŒKV CacheåŠ¨æ€å¢é•¿çš„åŒæ—¶ï¼Œä¿æŒäº†æé«˜çš„è®¡ç®—æ•ˆç‡ã€‚å®éªŒæ•°æ®è¯æ˜ï¼ŒKVCompåœ¨å‡ ä¹ä¸å½±å“æ¨¡å‹å‡†ç¡®ç‡çš„æƒ…å†µä¸‹ï¼Œå¹³å‡èƒ½å‡å°‘47%çš„å†…å­˜å ç”¨ï¼Œæœ€é«˜å‡å¹…è¾¾83%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†æ‰§è¡Œååé‡å¹¶é™ä½äº†è§£å‹å¼€é”€ï¼Œåœ¨å‡å°‘æ•°æ®ä¼ è¾“çš„åŸºç¡€ä¸Šï¼Œå…¶æ€§èƒ½åœ¨æŸäº›åœºæ™¯ä¸‹ç”šè‡³è¶…è¶Šäº†åŸºäºcuBLASçš„æ³¨æ„åŠ›æœºåˆ¶å†…æ ¸ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00579v1",
      "published_date": "2025-08-30 18:25:19 UTC",
      "updated_date": "2025-08-30 18:25:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:24:39.961510+00:00"
    },
    {
      "arxiv_id": "2509.02610v1",
      "title": "Resilient Biosecurity in the Era of AI-Enabled Bioweapons",
      "title_zh": "AI èµ‹èƒ½ç”Ÿç‰©æ­¦å™¨æ—¶ä»£çš„éŸ§æ€§ç”Ÿç‰©å®‰å…¨",
      "authors": [
        "Jonathan Feldman",
        "Tal Feldman"
      ],
      "abstract": "Recent advances in generative biology have enabled the design of novel proteins, creating significant opportunities for drug discovery while also introducing new risks, including the potential development of synthetic bioweapons. Existing biosafety measures primarily rely on inference-time filters such as sequence alignment and protein-protein interaction (PPI) prediction to detect dangerous outputs. In this study, we evaluate the performance of three leading PPI prediction tools: AlphaFold 3, AF3Complex, and SpatialPPIv2. These models were tested on well-characterized viral-host interactions, such as those involving Hepatitis B and SARS-CoV-2. Despite being trained on many of the same viruses, the models fail to detect a substantial number of known interactions. Strikingly, none of the tools successfully identify any of the four experimentally validated SARS-CoV-2 mutants with confirmed binding. These findings suggest that current predictive filters are inadequate for reliably flagging even known biological threats and are even more unlikely to detect novel ones. We argue for a shift toward response-oriented infrastructure, including rapid experimental validation, adaptable biomanufacturing, and regulatory frameworks capable of operating at the speed of AI-driven developments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼ç”Ÿç‰©å­¦(generative biology)åœ¨æ¨åŠ¨è¯ç‰©å‘ç°çš„åŒæ—¶ï¼Œå¦‚ä½•å› AIèµ‹èƒ½çš„ç”Ÿç‰©æ­¦å™¨(AI-Enabled Bioweapons)å¸¦æ¥æ˜¾è‘—çš„å®‰å…¨é£é™©ã€‚ä½œè€…é‡ç‚¹è¯„ä¼°äº†AlphaFold 3ã€AF3Complexå’ŒSpatialPPIv2ä¸‰ç§é¢†å…ˆçš„è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨(PPI)é¢„æµ‹å·¥å…·ï¼Œæµ‹è¯•å…¶åœ¨ä¹™å‹è‚ç‚(Hepatitis B)å’ŒSARS-CoV-2ç­‰å·²çŸ¥ç—…æ¯’-å®¿ä¸»ç›¸äº’ä½œç”¨ä¸­çš„æ£€æµ‹æ•ˆèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹ä¸ä»…æ— æ³•è¯†åˆ«å¤§é‡å·²çŸ¥ç›¸äº’ä½œç”¨ï¼Œä¸”å®Œå…¨æœªèƒ½æ£€æµ‹å‡ºå››ç§ç»è¿‡å®éªŒéªŒè¯çš„å…·æœ‰ç»“åˆèƒ½åŠ›çš„SARS-CoV-2çªå˜ä½“ã€‚è¿™è¡¨æ˜å½“å‰çš„æ¨ç†é˜¶æ®µè¿‡æ»¤æŠ€æœ¯(inference-time filters)åœ¨æ ‡è®°å·²çŸ¥ç”Ÿç‰©å¨èƒæ–¹é¢å­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼Œæ›´éš¾ä»¥æœ‰æ•ˆæ‹¦æˆªæ½œåœ¨çš„æ–°å‹å¨èƒã€‚ç ”ç©¶æœ€åå»ºè®®ç”Ÿç‰©å®‰å…¨æˆ˜ç•¥åº”ä»å•çº¯çš„é¢„é˜²æ€§è¿‡æ»¤è½¬å‘å“åº”å¯¼å‘çš„åŸºç¡€è®¾æ–½å»ºè®¾ï¼Œé€šè¿‡å»ºç«‹å¿«é€Ÿå®éªŒéªŒè¯ã€çµæ´»çš„ç”Ÿç‰©åˆ¶é€ ä»¥åŠèƒ½åŒ¹é…AIå‘å±•é€Ÿåº¦çš„ç›‘ç®¡æ¡†æ¶ï¼Œæ¥æå‡ç”Ÿç‰©é˜²å¾¡ç³»ç»Ÿçš„éŸ§æ€§ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.02610v1",
      "published_date": "2025-08-30 18:09:04 UTC",
      "updated_date": "2025-08-30 18:09:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:24:50.957094+00:00"
    },
    {
      "arxiv_id": "2509.00575v3",
      "title": "Can AI be Auditable?",
      "title_zh": "äººå·¥æ™ºèƒ½æ˜¯å¦å…·å¤‡å¯å®¡è®¡æ€§ï¼Ÿ",
      "authors": [
        "Himanshu Verma",
        "Kirtan Padh",
        "Eva Thelisson"
      ],
      "abstract": "Auditability is defined as the capacity of AI systems to be independently assessed for compliance with ethical, legal, and technical standards throughout their lifecycle. The chapter explores how auditability is being formalized through emerging regulatory frameworks, such as the EU AI Act, which mandate documentation, risk assessments, and governance structures. It analyzes the diverse challenges facing AI auditability, including technical opacity, inconsistent documentation practices, lack of standardized audit tools and metrics, and conflicting principles within existing responsible AI frameworks. The discussion highlights the need for clear guidelines, harmonized international regulations, and robust socio-technical methodologies to operationalize auditability at scale. The chapter concludes by emphasizing the importance of multi-stakeholder collaboration and auditor empowerment in building an effective AI audit ecosystem. It argues that auditability must be embedded in AI development practices and governance infrastructures to ensure that AI systems are not only functional but also ethically and legally aligned.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† AI çš„ Auditabilityï¼Œå°†å…¶å®šä¹‰ä¸º AI ç³»ç»Ÿåœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…æ¥å—ç‹¬ç«‹è¯„ä¼°ä»¥ç¬¦åˆä¼¦ç†ã€æ³•å¾‹å’ŒæŠ€æœ¯æ ‡å‡†çš„èƒ½åŠ›ã€‚æ–‡ç« åˆ†æäº†é€šè¿‡ EU AI Act ç­‰æ–°å…´ç›‘ç®¡æ¡†æ¶å®ç° Auditability æ­£å¼åŒ–çš„è·¯å¾„ï¼Œæ¶µç›–äº†æ–‡æ¡£åŒ–ã€é£é™©è¯„ä¼°å’Œæ²»ç†ç»“æ„ç­‰å…³é”®è¦ç´ ã€‚æ–‡ä¸­æŒ‡å‡ºäº† AI å®¡è®¡é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ Technical Opacityã€ä¸ä¸€è‡´çš„æ–‡æ¡£å®è·µã€ç¼ºä¹æ ‡å‡†åŒ–çš„å®¡è®¡å·¥å…·ä¸æŒ‡æ ‡ï¼Œä»¥åŠ Responsible AI æ¡†æ¶å†…éƒ¨çš„åŸåˆ™å†²çªã€‚è®¨è®ºå¼ºè°ƒäº†å»ºç«‹æ˜ç¡®æŒ‡å—ã€åè°ƒå›½é™…æ³•è§„ä»¥åŠå¼€å‘ç¨³å¥çš„ Socio-technical Methodologies å¯¹å®ç°å¤§è§„æ¨¡å®¡è®¡çš„é‡è¦æ€§ã€‚ç ”ç©¶æœ€åå¼ºè°ƒäº†å¤šåˆ©ç›Šç›¸å…³æ–¹åä½œå’Œå¢å¼ºå®¡è®¡å‘˜æƒåŠ›å¯¹äºæ„å»ºæœ‰æ•ˆ AI å®¡è®¡ç”Ÿæ€ç³»ç»Ÿçš„å…³é”®ä½œç”¨ã€‚ä½œè€…è®¤ä¸º Auditability å¿…é¡»åµŒå…¥åˆ° AI å¼€å‘å®è·µå’Œæ²»ç†åŸºç¡€è®¾æ–½ä¸­ï¼Œä»¥ç¡®ä¿ AI ç³»ç»Ÿåœ¨åŠŸèƒ½ã€ä¼¦ç†å’Œæ³•å¾‹å±‚é¢å‡ä¿æŒä¸€è‡´ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00575v3",
      "published_date": "2025-08-30 18:03:20 UTC",
      "updated_date": "2025-09-14 17:46:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:24:58.493926+00:00"
    },
    {
      "arxiv_id": "2509.00559v1",
      "title": "Social World Models",
      "title_zh": "ç¤¾ä¼šä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Xuhui Zhou",
        "Jiarui Liu",
        "Akhila Yerukola",
        "Hyunwoo Kim",
        "Maarten Sap"
      ],
      "abstract": "Humans intuitively navigate social interactions by simulating unspoken dynamics and reasoning about others' perspectives, even with limited information. In contrast, AI systems struggle to automatically structure and reason about these implicit social contexts. In this paper, we introduce a novel structured social world representation formalism (S3AP), designed to help AI systems reason more effectively about social dynamics. Following a POMDP-driven design, S3AP represents social interactions as structured tuples, such as state, observation, agent actions, and mental states, which can be automatically induced from free-form narratives or other inputs. We first show S3AP can help LLMs better understand social narratives across 5 social reasoning tasks (e.g., +51% improvement on FANToM's theory-of-mind reasoning with OpenAI's o1), reaching new state-of-the-art (SOTA) performance. We then induce social world models from these structured representations, demonstrating their ability to predict future social dynamics and improve agent decision-making, yielding up to +18% improvement on the SOTOPIA social interaction benchmark. Our findings highlight the promise of S3AP as a powerful, general-purpose representation for social world states, enabling the development of more socially-aware systems that better navigate social interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨ç†è§£å’Œå»ºæ¨¡éšå«ç¤¾äº¤èƒŒæ™¯æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸º S3APï¼ˆStructured Social World Representation Formalismï¼‰çš„æ–°å‹ç»“æ„åŒ–ç¤¾äº¤ä¸–ç•Œè¡¨ç¤ºæ–¹æ¡ˆã€‚S3AP éµå¾ª POMDP é©±åŠ¨çš„è®¾è®¡åŸåˆ™ï¼Œå°†å¤æ‚çš„ç¤¾äº¤äº’åŠ¨è½¬åŒ–ä¸ºç”±çŠ¶æ€ (state)ã€è§‚å¯Ÿ (observation)ã€æ™ºèƒ½ä½“åŠ¨ä½œ (agent actions) å’Œå¿ƒç†çŠ¶æ€ (mental states) æ„æˆçš„ç»“æ„åŒ–å…ƒç»„ï¼Œå¹¶æ”¯æŒä»è‡ªç”±æ–‡æœ¬ä¸­è‡ªåŠ¨æå–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒS3AP æ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å¤„ç†ç¤¾äº¤å™äº‹çš„èƒ½åŠ›ï¼Œåœ¨ FANToM å¿ƒç†ç†è®º (theory-of-mind) æ¨ç†ä»»åŠ¡ä¸­ä½¿æ€§èƒ½æå‡äº† 51% å¹¶åˆ·æ–°äº† SOTA è®°å½•ã€‚æ­¤å¤–ï¼Œé€šè¿‡è¯¥è¡¨ç¤ºæ³•æ„å»ºçš„ç¤¾äº¤ä¸–ç•Œæ¨¡å‹ (Social World Models) èƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹æœªæ¥ç¤¾äº¤åŠ¨æ€ï¼Œåœ¨ SOTOPIA ç¤¾äº¤äº’åŠ¨åŸºå‡†æµ‹è¯•ä¸­ä¸ºæ™ºèƒ½ä½“å†³ç­–å¸¦æ¥äº† 18% çš„æ€§èƒ½å¢ç›Šã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº† S3AP ä½œä¸ºé€šç”¨ç¤¾äº¤çŠ¶æ€è¡¨ç¤ºå·¥å…·çš„æ½œåŠ›ï¼Œä¸ºå¼€å‘å…·å¤‡ç¤¾äº¤æ„è¯†ã€èƒ½æ›´è‡ªç„¶åœ°è¿›è¡Œç¤¾äº¤å¯¼èˆªçš„ AI ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00559v1",
      "published_date": "2025-08-30 16:52:58 UTC",
      "updated_date": "2025-08-30 16:52:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:24:57.492754+00:00"
    },
    {
      "arxiv_id": "2509.00543v1",
      "title": "Text-to-Layout: A Generative Workflow for Drafting Architectural Floor Plans Using LLMs",
      "title_zh": "Text-to-Layoutï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å»ºç­‘å¹³é¢å›¾ç”Ÿæˆå¼è‰ç»˜å·¥ä½œæµ",
      "authors": [
        "Jayakrishna Duggempudi",
        "Lu Gao",
        "Ahmed Senouci",
        "Zhe Han",
        "Yunpeng Zhang"
      ],
      "abstract": "This paper presents the development of an AI-powered workflow that uses Large Language Models (LLMs) to assist in drafting schematic architectural floor plans from natural language prompts. The proposed system interprets textual input to automatically generate layout options including walls, doors, windows, and furniture arrangements. It combines prompt engineering, a furniture placement refinement algorithm, and Python scripting to produce spatially coherent draft plans compatible with design tools such as Autodesk Revit. A case study of a mid-sized residential layout demonstrates the approach's ability to generate functional and structured outputs with minimal manual effort. The workflow is designed for transparent replication, with all key prompt specifications documented to enable independent implementation by other researchers. In addition, the generated models preserve the full range of Revit-native parametric attributes required for direct integration into professional BIM processes.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„AIé©±åŠ¨å·¥ä½œæµï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºè¯è¾…åŠ©èµ·è‰ç¤ºæ„æ€§å»ºç­‘å¹³é¢å›¾ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿè§£ææ–‡æœ¬è¾“å…¥ï¼Œè‡ªåŠ¨ç”ŸæˆåŒ…æ‹¬å¢™ä½“ã€é—¨çª—ä»¥åŠå®¶å…·å¸ƒç½®åœ¨å†…çš„å¸ƒå±€æ–¹æ¡ˆã€‚æŠ€æœ¯ä¸Šï¼Œå®ƒç»“åˆäº†æç¤ºè¯å·¥ç¨‹(Prompt Engineering)ã€å®¶å…·æ‘†æ”¾ä¼˜åŒ–ç®—æ³•ä»¥åŠPythonè„šæœ¬ï¼Œä»¥ç”Ÿæˆç©ºé—´è¿è´¯ä¸”ä¸Autodesk Revitç­‰è®¾è®¡å·¥å…·å…¼å®¹çš„è‰å›¾ã€‚é€šè¿‡å¯¹ä¸­å‹ä½å®…å¸ƒå±€çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•èƒ½å¤Ÿä»¥æå°‘çš„äººå·¥å¹²é¢„ç”ŸæˆåŠŸèƒ½å®Œå–„ä¸”ç»“æ„åŒ–çš„è¾“å‡ºç»“æœã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œæµè®°å½•äº†æ‰€æœ‰å…³é”®æç¤ºè¯è§„èŒƒä»¥æ”¯æŒç‹¬ç«‹å¤ç°ï¼Œå¹¶ä¸”ç”Ÿæˆçš„æ¨¡å‹ä¿ç•™äº†RevitåŸç”Ÿçš„å‚æ•°åŒ–å±æ€§ï¼Œä»è€Œç¡®ä¿æˆæœå¯ä»¥ç›´æ¥é›†æˆåˆ°ä¸“ä¸šçš„BIMä¸šåŠ¡æµç¨‹ä¸­ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00543v1",
      "published_date": "2025-08-30 16:04:15 UTC",
      "updated_date": "2025-08-30 16:04:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:25:00.246944+00:00"
    },
    {
      "arxiv_id": "2509.00541v1",
      "title": "LatentEdit: Adaptive Latent Control for Consistent Semantic Editing",
      "title_zh": "LatentEditï¼šé¢å‘ä¸€è‡´æ€§è¯­ä¹‰ç¼–è¾‘çš„è‡ªé€‚åº”æ½œç©ºé—´æ§åˆ¶",
      "authors": [
        "Siyi Liu",
        "Weiming Chen",
        "Yushun Tang",
        "Zhihai He"
      ],
      "abstract": "Diffusion-based Image Editing has achieved significant success in recent years. However, it remains challenging to achieve high-quality image editing while maintaining the background similarity without sacrificing speed or memory efficiency. In this work, we introduce LatentEdit, an adaptive latent fusion framework that dynamically combines the current latent code with a reference latent code inverted from the source image. By selectively preserving source features in high-similarity, semantically important regions while generating target content in other regions guided by the target prompt, LatentEdit enables fine-grained, controllable editing. Critically, the method requires no internal model modifications or complex attention mechanisms, offering a lightweight, plug-and-play solution compatible with both UNet-based and DiT-based architectures. Extensive experiments on the PIE-Bench dataset demonstrate that our proposed LatentEdit achieves an optimal balance between fidelity and editability, outperforming the state-of-the-art method even in 8-15 steps. Additionally, its inversion-free variant further halves the number of neural function evaluations and eliminates the need for storing any intermediate variables, substantially enhancing real-time deployment efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LatentEditï¼Œä¸€ç§ç”¨äºä¸€è‡´æ€§è¯­ä¹‰ç¼–è¾‘çš„è‡ªé€‚åº”æ½œç©ºé—´èåˆ(adaptive latent fusion)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é«˜è´¨é‡å›¾åƒç¼–è¾‘ä¸­ä¿æŒèƒŒæ™¯ç›¸ä¼¼æ€§ä¸æå‡æ•ˆç‡ä¹‹é—´çš„çŸ›ç›¾ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€ç»“åˆå½“å‰æ½œä»£ç ä¸æºå›¾åƒçš„å‚è€ƒæ½œä»£ç ï¼Œåœ¨ä¿ç•™é«˜ç›¸ä¼¼åº¦è¯­ä¹‰åŒºåŸŸç‰¹å¾çš„åŒæ—¶ï¼Œæ ¹æ®ç›®æ ‡æç¤ºè¯ç²¾å‡†ç”Ÿæˆç‰¹å®šåŒºåŸŸçš„å†…å®¹ã€‚ä½œä¸ºä¸€ç§è½»é‡çº§ä¸”å³æ’å³ç”¨çš„è§£å†³æ–¹æ¡ˆï¼ŒLatentEdit æ— éœ€ä¿®æ”¹æ¨¡å‹å†…éƒ¨ç»“æ„æˆ–é‡‡ç”¨å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶å¹¿æ³›å…¼å®¹ UNet å’Œ DiT æ¶æ„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ PIE-Bench æ•°æ®é›†ä¸Šä»…éœ€ 8-15 æ­¥å³å¯åœ¨ä¿çœŸåº¦ä¸å¯ç¼–è¾‘æ€§ä¹‹é—´å–å¾—æœ€ä¼˜å¹³è¡¡ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œå…¶æ— éœ€åå‘è¿‡ç¨‹(inversion-free)çš„å˜ä½“é€šè¿‡å‡åŠè®¡ç®—è¯„ä¼°æ¬¡æ•°å¹¶æ¶ˆé™¤ä¸­é—´å˜é‡å­˜å‚¨éœ€æ±‚ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å®æ—¶éƒ¨ç½²çš„æ•ˆç‡ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted by PRCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.00541v1",
      "published_date": "2025-08-30 15:47:03 UTC",
      "updated_date": "2025-08-30 15:47:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:25:16.192501+00:00"
    },
    {
      "arxiv_id": "2509.00510v1",
      "title": "LLM-Assisted Iterative Evolution with Swarm Intelligence Toward SuperBrain",
      "title_zh": "é¢å‘ SuperBrain çš„å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©ä¸ç¾¤ä½“æ™ºèƒ½è¿­ä»£æ¼”åŒ–",
      "authors": [
        "Li Weigang",
        "Pedro Carvalho Brom",
        "Lucas Ramson Siefert"
      ],
      "abstract": "We propose a novel SuperBrain framework for collective intelligence, grounded in the co-evolution of large language models (LLMs) and human users. Unlike static prompt engineering or isolated agent simulations, our approach emphasizes a dynamic pathway from Subclass Brain to Superclass Brain: (1) A Subclass Brain arises from persistent, personalized interaction between a user and an LLM, forming a cognitive dyad with adaptive learning memory. (2) Through GA-assisted forward-backward evolution, these dyads iteratively refine prompts and task performance. (3) Multiple Subclass Brains coordinate via Swarm Intelligence, optimizing across multi-objective fitness landscapes and exchanging distilled heuristics. (4) Their standardized behaviors and cognitive signatures integrate into a Superclass Brain, an emergent meta-intelligence capable of abstraction, generalization and self-improvement. We outline the theoretical constructs, present initial implementations (e.g., UAV scheduling, KU/KI keyword filtering) and propose a registry for cross-dyad knowledge consolidation. This work provides both a conceptual foundation and an architectural roadmap toward scalable, explainable and ethically aligned collective AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SuperBrainæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸äººç±»ç”¨æˆ·çš„ååŒæ¼”åŒ–å®ç°ç¾¤ä½“æ™ºèƒ½ã€‚è¯¥æ¡†æ¶å®šä¹‰äº†ç”±ç”¨æˆ·ä¸LLMé€šè¿‡æŒç»­ä¸ªæ€§åŒ–äº¤äº’å½¢æˆçš„Subclass Brainï¼Œåˆ©ç”¨è‡ªé€‚åº”å­¦ä¹ è®°å¿†æ„å»ºè®¤çŸ¥åŒå›è·¯ã€‚é€šè¿‡GA-assistedçš„å‰å‘-åå‘æ¼”åŒ–æœºåˆ¶ï¼Œè¿™äº›è®¤çŸ¥å•å…ƒèƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–æç¤ºè¯å’Œä»»åŠ¡è¡¨ç°ã€‚å¤šä¸ªSubclass Brainé€šè¿‡Swarm Intelligenceè¿›è¡Œåè°ƒï¼Œåœ¨å¤šç›®æ ‡é€‚åº”åº¦æ™¯è§‚ä¸­ä¼˜åŒ–å¹¶äº¤æ¢æç‚¼å‡ºçš„å¯å‘å¼çŸ¥è¯†ã€‚æœ€ç»ˆï¼Œè¿™äº›æ ‡å‡†åŒ–è¡Œä¸ºå’Œè®¤çŸ¥ç‰¹å¾æ•´åˆä¸ºå…·æœ‰æŠ½è±¡ã€æ³›åŒ–å’Œè‡ªæˆ‘å®Œå–„èƒ½åŠ›çš„Superclass Brainå…ƒæ™ºèƒ½ä½“ã€‚ç ”ç©¶å±•ç¤ºäº†åœ¨æ— äººæœº(UAV)è°ƒåº¦å’Œå…³é”®è¯è¿‡æ»¤ç­‰åœºæ™¯çš„åˆæ­¥å®ç°ï¼Œå¹¶æå‡ºäº†è·¨å•å…ƒçŸ¥è¯†æ•´åˆæ³¨å†Œæœºåˆ¶ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºå¯æ‰©å±•ã€å¯è§£é‡Šä¸”ç¬¦åˆä¼¦ç†çš„é›†ä½“äººå·¥æ™ºèƒ½æä¾›äº†ç†è®ºåŸºç¡€å’Œæ¶æ„è“å›¾ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.00510v1",
      "published_date": "2025-08-30 14:12:46 UTC",
      "updated_date": "2025-08-30 14:12:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:25:36.590927+00:00"
    },
    {
      "arxiv_id": "2509.00507v1",
      "title": "Artificial Intelligence-Based Analysis of Ice Cream Melting Behavior Under Various Ingredients",
      "title_zh": "åŸºäºäººå·¥æ™ºèƒ½çš„ä¸åŒé…æ–™ä¸‹å†°æ·‡æ·‹èåŒ–è¡Œä¸ºåˆ†æ",
      "authors": [
        "Zhang Lai Bin",
        "Zhen Bin It"
      ],
      "abstract": "The stability of ice cream during melting is a critical factor for consumer's acceptance and product quality. With the commonly added stabilizer to improve texture, structure and slower melting as the factors to analyze. This report explores the effects of locust bean gum, guar gum, maltodextrin, and carrageenan on the melting behavior of homemade ice cream. The main objective was to assess how these additives influence melting resistance and to identify a more cost-effective recipe formulation. Ice cream samples incorporating each additive were prepared and subjected to melting tests under controlled conditions. Timelapse recordings were used to capture and analyze the progression of melting over time. Python and OpenCV is used for process and analysis. Observations revealed that all samples retained a foam-like structure even after melting, suggesting the stabilizers contributed to the formation of a stable air-cell matrix. Furthermore, when the melted samples were re-frozen and subsequently melted again, they displayed increased sturdiness, indicating improved resilience of the ice cream structure. Comparative analysis of the different stabilizers highlighted variations in their effectiveness, with some offering stronger melting resistance and structural support than others. Overall, the findings provide insights into the functional roles of commonly used food additives in ice cream formulation. By evaluating both performance and cost, this study demonstrates the potential for developing recipes that balance durability with economic efficiency, contributing to practical applications in both small-scale and commercial ice cream production.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯åˆ†æäº†ä¸åŒæ·»åŠ å‰‚å¯¹å†°æ·‡æ·‹èåŒ–è¡Œä¸ºçš„å½±å“ï¼Œæ—¨åœ¨è¯„ä¼°æˆåˆ†å¯¹èåŒ–é˜»åŠ›çš„ä½œç”¨å¹¶ä¼˜åŒ–æˆæœ¬æ•ˆç›Šã€‚ç ”ç©¶å›¢é˜Ÿåˆ¶å¤‡äº†å«æœ‰åˆºæ§è±†èƒ¶ (locust bean gum)ã€ç“œå°”èƒ¶ (guar gum)ã€éº¦èŠ½ç³Šç²¾ (maltodextrin) å’Œå¡æ‹‰èƒ¶ (carrageenan) çš„æ ·æœ¬ï¼Œå¹¶åˆ©ç”¨ Python å’Œ OpenCV å¯¹èåŒ–è¿‡ç¨‹çš„å»¶æ—¶æ‘„å½± (timelapse recordings) è¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æã€‚å®éªŒå‘ç°ï¼Œå³ä¾¿åœ¨èåŒ–åï¼Œç¨³å®šå‰‚ä»èƒ½å¸®åŠ©å†°æ·‡æ·‹ç»´æŒç¨³å®šçš„æ°”å®¤çŸ©é˜µ (air-cell matrix) ç»“æ„ï¼Œä¸”é‡æ–°å†·å†»çš„æ ·æœ¬è¡¨ç°å‡ºæ›´é«˜çš„éŸ§æ€§ã€‚å¯¹æ¯”åˆ†ææ­ç¤ºäº†ä¸åŒç¨³å®šå‰‚åœ¨å¢å¼ºæŠ—èåŒ–æ€§å’Œç»“æ„æ”¯æ’‘æ–¹é¢çš„æ•ˆèƒ½å·®å¼‚ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†å¸¸ç”¨é£Ÿå“æ·»åŠ å‰‚çš„åŠŸèƒ½ä½œç”¨ï¼Œä¸ºå¼€å‘å…¼é¡¾ç»“æ„è€ç”¨æ€§ä¸ç»æµæ•ˆç‡çš„å†°æ·‡æ·‹é…æ–¹æä¾›äº†ç§‘å­¦ä¾æ®ï¼Œå¯¹å°è§„æ¨¡åŠå•†ä¸šåŒ–ç”Ÿäº§å‡å…·æœ‰å®è·µå‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00507v1",
      "published_date": "2025-08-30 14:00:21 UTC",
      "updated_date": "2025-08-30 14:00:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:25:55.690298+00:00"
    },
    {
      "arxiv_id": "2509.00499v1",
      "title": "NeuralSVCD for Efficient Swept Volume Collision Detection",
      "title_zh": "NeuralSVCDï¼šé«˜æ•ˆæ‰«æ ä½“ç¢°æ’æ£€æµ‹",
      "authors": [
        "Dongwon Son",
        "Hojin Jung",
        "Beomjoon Kim"
      ],
      "abstract": "Robot manipulation in unstructured environments requires efficient and reliable Swept Volume Collision Detection (SVCD) for safe motion planning. Traditional discrete methods potentially miss collisions between these points, whereas SVCD continuously checks for collisions along the entire trajectory. Existing SVCD methods typically face a trade-off between efficiency and accuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a novel neural encoder-decoder architecture tailored to overcome this trade-off. Our approach leverages shape locality and temporal locality through distributed geometric representations and temporal optimization. This enhances computational efficiency without sacrificing accuracy. Comprehensive experiments show that NeuralSVCD consistently outperforms existing state-of-the-art SVCD methods in terms of both collision detection accuracy and computational efficiency, demonstrating its robust applicability across diverse robotic manipulation scenarios. Code and videos are available at https://neuralsvcd.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NeuralSVCDï¼Œä¸€ç§åŸºäºç¥ç»ç¼–ç å™¨-è§£ç å™¨ (encoder-decoder) æ¶æ„çš„æ–°å‹æ‰«æ ä½“ç§¯ç¢°æ’æ£€æµ‹ (Swept Volume Collision Detection, SVCD) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººå®‰å…¨è¿åŠ¨è§„åˆ’ä¸­æ•ˆç‡ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†å¸ƒå¼å‡ ä½•è¡¨ç¤ºå’Œæ—¶é—´ä¼˜åŒ–æŠ€æœ¯ï¼Œå……åˆ†åˆ©ç”¨äº†å½¢çŠ¶å±€éƒ¨æ€§ (shape locality) å’Œæ—¶é—´å±€éƒ¨æ€§ (temporal locality)ï¼Œä»è€Œåœ¨ä¿è¯æ£€æµ‹ç²¾åº¦çš„åŒæ—¶å¤§å¹…æå‡äº†è®¡ç®—é€Ÿåº¦ã€‚ä¸ä¼ ç»Ÿçš„ç¦»æ•£æ£€æµ‹æ–¹æ³•ä¸åŒï¼ŒNeuralSVCD èƒ½å¤Ÿå¯¹æ•´ä¸ªè¿åŠ¨è½¨è¿¹è¿›è¡Œè¿ç»­æ£€æŸ¥ï¼Œæœ‰æ•ˆé¿å…äº†ç¢°æ’é—æ¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNeuralSVCD åœ¨å‡†ç¡®ç‡å’Œæ•ˆç‡ä¸Šå‡ä¸€è‡´ä¼˜äºç°æœ‰çš„å…ˆè¿› (state-of-the-art) SVCD æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆåœ¨å¤šç§å¤æ‚çš„æœºå™¨äººæ“ä½œåœºæ™¯ä¸­å±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ä¸é€‚ç”¨æ€§ï¼Œä¸ºéç»“æ„åŒ–ç¯å¢ƒä¸‹çš„é«˜æ•ˆç¢°æ’æ£€æµ‹æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "CoRL 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.00499v1",
      "published_date": "2025-08-30 13:43:11 UTC",
      "updated_date": "2025-08-30 13:43:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:25:45.687242+00:00"
    },
    {
      "arxiv_id": "2509.00496v2",
      "title": "ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics",
      "title_zh": "ResearchQAï¼šåŸºäºç»¼è¿°æŒ–æ˜çš„é—®é¢˜ä¸è¯„åˆ†æ ‡å‡†ï¼Œè·¨ 75 ä¸ªé¢†åŸŸçš„å¤§è§„æ¨¡å­¦æœ¯é—®ç­”è¯„ä¼°",
      "authors": [
        "Li S. Yifei",
        "Allen Chang",
        "Chaitanya Malaviya",
        "Mark Yatskar"
      ],
      "abstract": "Evaluating long-form responses to research queries heavily relies on expert annotators, restricting attention to areas like AI where researchers can conveniently enlist colleagues. Yet, research expertise is abundant: survey articles consolidate knowledge spread across the literature. We introduce ResearchQA, a resource for evaluating LLM systems by distilling survey articles from 75 research fields into 21K queries and 160K rubric items. Queries and rubrics are jointly derived from survey sections, where rubric items list query-specific answer evaluation criteria, i.e., citing papers, making explanations, and describing limitations. 31 Ph.D. annotators in 8 fields judge that 90% of queries reflect Ph.D. information needs and 87% of rubric items warrant emphasis of a sentence or longer. We leverage ResearchQA to evaluate 18 systems in 7.6K head-to-heads. No parametric or retrieval-augmented system we evaluate exceeds 70% on covering rubric items, and the highest-ranking system shows 75% coverage. Error analysis reveals that the highest-ranking system fully addresses less than 11% of citation rubric items, 48% of limitation items, and 49% of comparison items. We release our data to facilitate more comprehensive multi-field evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†ResearchQAï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è·¨75ä¸ªç ”ç©¶é¢†åŸŸå¤§è§„æ¨¡è¯„ä¼°å­¦æœ¯é—®ç­”ï¼ˆScholarly Question Answeringï¼‰èƒ½åŠ›çš„èµ„æºã€‚ä¸ºäº†è§£å†³ä¸“å®¶æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”é¢†åŸŸå±€é™çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡ä»ç»¼è¿°æ–‡ç« ï¼ˆsurvey articlesï¼‰ä¸­æå–çŸ¥è¯†ï¼Œæ„å»ºäº†åŒ…å«2.1ä¸‡ä¸ªæŸ¥è¯¢å’Œ16ä¸‡ä¸ªè¯„åˆ†é¡¹ï¼ˆrubric itemsï¼‰çš„æ•°æ®åº“ã€‚è¿™äº›è¯„åˆ†é¡¹æä¾›äº†å…·ä½“çš„è¯„ä¼°æ ‡å‡†ï¼Œå¦‚æ–‡çŒ®å¼•ç”¨ã€è¯¦ç»†è§£é‡Šå’Œå±€é™æ€§æè¿°ã€‚ç»31ä½åšå£«ç”Ÿæ ‡æ³¨è€…éªŒè¯ï¼Œè¯¥èµ„æºèƒ½é«˜åº¦åæ˜ åšå£«çº§åˆ«çš„ç§‘ç ”ä¿¡æ¯éœ€æ±‚ã€‚é€šè¿‡å¯¹18ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿçš„æµ‹è¯„å‘ç°ï¼Œå³ä¾¿è¡¨ç°æœ€å¥½çš„ç³»ç»Ÿåœ¨è¯„åˆ†é¡¹è¦†ç›–ç‡ä¸Šä¹Ÿä»…ä¸º75%ï¼Œä¸”åœ¨å¤„ç†å¼•ç”¨å’Œæ¯”è¾ƒåˆ†æç­‰ä»»åŠ¡æ—¶è¡¨ç°æ¬ ä½³ã€‚è¯¥ç ”ç©¶é€šè¿‡å…¬å¼€å‘å¸ƒæ•°æ®é›†ï¼Œä¸ºå®ç°æ›´å…¨é¢ã€è·¨å­¦ç§‘çš„å­¦æœ¯ç³»ç»Ÿè¯„ä¼°æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages main, 40 pages total, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.00496v2",
      "published_date": "2025-08-30 13:37:28 UTC",
      "updated_date": "2025-12-19 16:38:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:25:51.693737+00:00"
    },
    {
      "arxiv_id": "2509.00490v3",
      "title": "Multi-Focused Video Group Activities Hashing",
      "title_zh": "å¤šèšç„¦è§†é¢‘ç¾¤ä½“æ´»åŠ¨å“ˆå¸Œ",
      "authors": [
        "Zhongmiao Qi",
        "Yan Jiang",
        "Bolin Zhang",
        "Chong Wang",
        "Lijun Guo",
        "Pengjiang Qian",
        "Jiangbo Qian"
      ],
      "abstract": "With the explosive growth of video data in various complex scenarios, quickly retrieving group activities has become an urgent problem. However, many tasks can only retrieve videos focusing on an entire video, not the activity granularity. To solve this problem, we propose a new STVH (spatiotemporal interleaved video hashing) technique for the first time. Through a unified framework, the STVH simultaneously models individual object dynamics and group interactions, capturing the spatiotemporal evolution on both group visual features and positional features. Moreover, in real-life video retrieval scenarios, it may sometimes require activity features, while at other times, it may require visual features of objects. We then further propose a novel M-STVH (multi-focused spatiotemporal video hashing) as an enhanced version to handle this difficult task. The advanced method incorporates hierarchical feature integration through multi-focused representation learning, allowing the model to jointly focus on activity semantics features and object visual features. We conducted comparative experiments on publicly available datasets, and both STVH and M-STVH can achieve excellent results.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤æ‚åœºæ™¯ä¸‹è§†é¢‘ç¾¤ä½“æ´»åŠ¨ï¼ˆgroup activitiesï¼‰å¿«é€Ÿæ£€ç´¢çš„éœ€æ±‚ï¼Œé¦–æ¬¡æå‡ºäº†æ—¶ç©ºäº¤ç»‡è§†é¢‘å“ˆå¸Œï¼ˆSTVHï¼‰æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡ç»Ÿä¸€æ¡†æ¶åŒæ—¶å»ºæ¨¡ä¸ªä½“å¯¹è±¡åŠ¨æ€ä¸ç¾¤ä½“äº¤äº’ï¼Œæœ‰æ•ˆæ•è·äº†ç¾¤ä½“è§†è§‰ç‰¹å¾å’Œä½ç½®ç‰¹å¾åœ¨æ—¶ç©ºç»´åº¦ä¸Šçš„æ¼”å˜è¿‡ç¨‹ã€‚ä¸ºäº†åº”å¯¹ç°å®ä¸­å¯¹æ´»åŠ¨è¯­ä¹‰å’Œå¯¹è±¡è§†è§‰ç‰¹å¾çš„ä¸åŒæ£€ç´¢åå¥½ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†å¤šç„¦ç‚¹æ—¶ç©ºè§†é¢‘å“ˆå¸Œï¼ˆM-STVHï¼‰å¢å¼ºæ¨¡å‹ã€‚M-STVH å¼•å…¥äº†å¤šç„¦ç‚¹è¡¨ç¤ºå­¦ä¹ ï¼ˆmulti-focused representation learningï¼‰è¿›è¡Œå±‚æ¬¡åŒ–ç‰¹å¾é›†æˆï¼Œå®ç°äº†å¯¹æ´»åŠ¨è¯­ä¹‰ç‰¹å¾ï¼ˆactivity semantics featuresï¼‰å’Œå¯¹è±¡è§†è§‰ç‰¹å¾ï¼ˆobject visual featuresï¼‰çš„ååŒå…³æ³¨ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å¯¹æ¯”å®éªŒè¡¨æ˜ï¼ŒSTVH å’Œ M-STVH å‡å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸ºç»†ç²’åº¦è§†é¢‘æ£€ç´¢æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00490v3",
      "published_date": "2025-08-30 13:22:57 UTC",
      "updated_date": "2025-12-27 10:41:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:25:37.482611+00:00"
    },
    {
      "arxiv_id": "2509.00484v1",
      "title": "VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding",
      "title_zh": "VideoRewardBenchï¼šé¢å‘è§†é¢‘ç†è§£çš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ç»¼åˆè¯„ä¼°",
      "authors": [
        "Zhihong Zhang",
        "Xiaojian Huang",
        "Jin Xu",
        "Zhuodong Luo",
        "Xinzhi Wang",
        "Jiansheng Wei",
        "Xuejin Chen"
      ],
      "abstract": "Multimodal reward models (MRMs) play a crucial role in the training, inference, and evaluation of Large Vision Language Models (LVLMs) by assessing response quality. However, existing benchmarks for evaluating MRMs in the video domain suffer from a limited number and diversity of questions, a lack of comprehensive evaluation dimensions, and inadequate evaluation of diverse types of MRMs. To address these gaps, we introduce VideoRewardBench, the first comprehensive benchmark covering four core aspects of video understanding: perception, knowledge, reasoning, and safety. Through our AI-assisted data pipeline, we curate a high-quality preference dataset of 1,563 annotated samples, including 1,482 unique videos and 1,559 distinct questions--15 times the number found in the most question-rich prior benchmark. Each sample is a triplet consisting of a video-text prompt, a chosen response, and a rejected response. We also conduct a comprehensive evaluation across 28 multimodal reward models spanning three categories: generative, discriminative, and semi-scalar. Results show that even the top-performing model GPT-4o achieves only 57.0% overall accuracy, and the state-of-the-art open-source model Qwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three key insights: (i) MRMs trained with reinforcement learning (RL) do not necessarily exhibit stronger cross-modal generalization than those trained without RL; (ii) except for discriminative MRMs, other types of MRMs across varying model capacities can benefit from inference-time scaling; and (iii) variations in input video frame count have different effects on different types of MRMs. We believe VideoRewardBench offers a challenging and valuable benchmark for advancing the evaluation and development of MRMs in the video domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†VideoRewardBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è§†é¢‘ç†è§£é¢†åŸŸå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMultimodal Reward Models, MRMsï¼‰è¿›è¡Œå…¨é¢è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–æ„ŸçŸ¥ã€çŸ¥è¯†ã€æ¨ç†å’Œå®‰å…¨å››ä¸ªæ ¸å¿ƒç»´åº¦ã€‚é€šè¿‡AIè¾…åŠ©çš„æ•°æ®æµæ°´çº¿ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«1,563ä¸ªæ ‡æ³¨æ ·æœ¬çš„é«˜è´¨é‡åå¥½æ•°æ®é›†ï¼Œå…¶é—®é¢˜è§„æ¨¡æ˜¯æ­¤å‰æœ€ä¸°å¯ŒåŸºå‡†çš„15å€ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹æ¶µç›–ç”Ÿæˆå¼ã€åˆ¤åˆ«å¼å’ŒåŠæ ‡é‡å¼çš„28ä¸ªæ¨¡å‹è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯é¡¶çº§æ¨¡å‹GPT-4oçš„å‡†ç¡®ç‡ä¹Ÿä»…ä¸º57.0%ï¼Œå‡¸æ˜¾äº†å½“å‰æŠ€æœ¯çš„å±€é™æ€§ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒå¹¶ä¸å¿…ç„¶å¢å¼ºæ¨¡å‹çš„è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ï¼Œä¸”é™¤åˆ¤åˆ«å¼æ¨¡å‹å¤–ï¼Œå¤šæ•°MRMsåœ¨ä¸åŒè§„æ¨¡ä¸‹å‡èƒ½ä»æ¨ç†æ—¶ç¼©æ”¾ï¼ˆinference-time scalingï¼‰ä¸­è·ç›Šã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†è¾“å…¥è§†é¢‘å¸§æ•°çš„å˜åŒ–å¯¹ä¸åŒç±»å‹MRMså…·æœ‰å·®å¼‚åŒ–çš„å½±å“ã€‚VideoRewardBenchä¸ºæ¨è¿›è§†é¢‘é¢†åŸŸMRMsçš„å¼€å‘ä¸è¯„ä¼°æä¾›äº†æå…·æŒ‘æˆ˜æ€§ä¸”æœ‰ä»·å€¼çš„å‚è€ƒä½“ç³»ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "https://videorewardbench.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2509.00484v1",
      "published_date": "2025-08-30 12:50:55 UTC",
      "updated_date": "2025-08-30 12:50:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:26:20.650991+00:00"
    },
    {
      "arxiv_id": "2509.00482v2",
      "title": "Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting",
      "title_zh": "è¨€ç®€æ„èµ…ï¼Œç²¾å‡†è°ƒç”¨ï¼šé€šè¿‡è‡ªåŠ¨æç¤ºè¯ä¼˜åŒ–ä¸è§’è‰²æç¤ºå¢å¼ºè§’è‰²æ‰®æ¼”å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Saksorn Ruangtanusak",
        "Pittawat Taveekitworachai",
        "Kunat Pipatanakul"
      ],
      "abstract": "This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) improved role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques-character-card/scene-contract design and strict enforcement of function calling-which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool Source code is available at https://github.com/scb-10x/apo",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºè§’è‰²æ‰®æ¼”å¯¹è¯æ™ºèƒ½ä½“æ—¶å­˜åœ¨çš„è¿‡åº¦è¡¨è¾¾(over-speaking)å’Œå·¥å…·è°ƒç”¨ä¸å½“(under-acting)ç­‰é—®é¢˜ï¼Œå¯¹æ¯”åˆ†æäº†å››ç§æç¤ºè¯å·¥ç¨‹æ–¹æ³•ã€‚ç ”ç©¶è€…åœ¨Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025ç«èµ›èƒŒæ™¯ä¸‹ï¼Œæ¢è®¨äº†åŸºç¡€è§’è‰²æç¤ºè¯ã€è‡ªåŠ¨æç¤ºè¯ä¼˜åŒ–(Automatic Prompt Optimization, APO)ä»¥åŠåŸºäºè§„åˆ™çš„è§’è‰²æç¤ºè¯(Rule-based Role Prompting, RRP)ç­‰ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒRRPæ–¹æ³•é€šè¿‡è§’è‰²å¡/åœºæ™¯å¥‘çº¦(character-card/scene-contract)è®¾è®¡å’Œä¸¥æ ¼çš„å‡½æ•°è°ƒç”¨æ‰§è¡Œï¼Œå°†åŸºçº¿å¾—åˆ†ä»0.519æå‡è‡³0.571ï¼Œè¡¨ç°ä¼˜äºæ›´ä¸ºå¤æ‚çš„APOæ–¹æ³•ã€‚è¯¥ç ”ç©¶è¯å®äº†è§„åˆ™åŒ–è®¾è®¡èƒ½æ˜¾è‘—å¢å¼ºè§’è‰²æ‰®æ¼”æ™ºèƒ½ä½“çš„ä»»åŠ¡æ‰§è¡Œæ•ˆç‡ä¸å¯é æ€§ï¼Œå¹¶å¼€æºäº†ç›¸å…³çš„æç¤ºè¯èµ„æºä¸ä¼˜åŒ–å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 Wordplay Workshop (Spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2509.00482v2",
      "published_date": "2025-08-30 12:45:36 UTC",
      "updated_date": "2025-10-12 05:47:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:26:20.292485+00:00"
    },
    {
      "arxiv_id": "2509.00481v1",
      "title": "Multi-Agent Data Visualization and Narrative Generation",
      "title_zh": "å¤šæ™ºèƒ½ä½“æ•°æ®å¯è§†åŒ–ä¸å™äº‹ç”Ÿæˆ",
      "authors": [
        "Anton Wolter",
        "Georgios Vidalakis",
        "Michael Yu",
        "Ankit Grover",
        "Vaishali Dhanoa"
      ],
      "abstract": "Recent advancements in the field of AI agents have impacted the way we work, enabling greater automation and collaboration between humans and agents. In the data visualization field, multi-agent systems can be useful for employing agents throughout the entire data-to-communication pipeline. We present a lightweight multi-agent system that automates the data analysis workflow, from data exploration to generating coherent visual narratives for insight communication. Our approach combines a hybrid multi-agent architecture with deterministic components, strategically externalizing critical logic from LLMs to improve transparency and reliability. The system delivers granular, modular outputs that enable surgical modifications without full regeneration, supporting sustainable human-AI collaboration. We evaluated our system across 4 diverse datasets, demonstrating strong generalizability, narrative quality, and computational efficiency with minimal dependencies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-Agent system)ï¼Œæ—¨åœ¨å®ç°ä»æ•°æ®æ¢ç´¢åˆ°ç”Ÿæˆè¿è´¯è§†è§‰å™äº‹(Visual Narratives)çš„è‡ªåŠ¨åŒ–æ•°æ®åˆ†æå·¥ä½œæµã€‚è¯¥æ–¹æ¡ˆé€šè¿‡ç»“åˆç¡®å®šæ€§ç»„ä»¶çš„æ··åˆå¤šæ™ºèƒ½ä½“æ¶æ„(Hybrid Multi-Agent Architecture)ï¼Œå°†å…³é”®é€»è¾‘ä»å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­å¤–éƒ¨åŒ–ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿçš„é€æ˜åº¦ä¸å¯é æ€§ã€‚ç³»ç»Ÿæä¾›çš„ç²’åº¦åŒ–ã€æ¨¡å—åŒ–è¾“å‡ºæ”¯æŒç”¨æˆ·è¿›è¡Œå±€éƒ¨ç²¾ç¡®ä¿®æ”¹è€Œæ— éœ€æ•´ä½“é‡æ–°ç”Ÿæˆï¼Œä¸ºå¯æŒç»­çš„äººæœºåä½œ(Human-AI Collaboration)æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚åœ¨4ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨æä½ä¾èµ–ç¯å¢ƒä¸‹å±•ç°å‡ºå¼ºåŠ²çš„æ³›åŒ–èƒ½åŠ›ã€é«˜è´¨é‡çš„å™äº‹æ°´å¹³ä»¥åŠå“è¶Šçš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00481v1",
      "published_date": "2025-08-30 12:39:55 UTC",
      "updated_date": "2025-08-30 12:39:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:26:12.654332+00:00"
    },
    {
      "arxiv_id": "2509.00479v1",
      "title": "A Novel Method to Determine Total Oxidant Concentration Produced by Non-Thermal Plasma Based on Image Processing and Machine Learning",
      "title_zh": "ä¸€ç§åŸºäºå›¾åƒå¤„ç†ä¸æœºå™¨å­¦ä¹ çš„ä½æ¸©ç­‰ç¦»å­ä½“æ€»æ°§åŒ–å‰‚æµ“åº¦æµ‹å®šæ–°æ–¹æ³•",
      "authors": [
        "Mirkan Emir Sancak",
        "Unal Sen",
        "Ulker Diler Keris-Sen"
      ],
      "abstract": "Accurate determination of total oxidant concentration ([Ox]_{tot}) in non-thermal plasma (NTP)-treated aqueous systems remains a critical challenge due to the transient nature of reactive oxygen and nitrogen species and the subjectivity of conventional titration methods used for [Ox]_{tot} determination. This study introduces a novel, color-based computer analysis (CBCA) method that integrates advanced image processing with machine learning (ML) to quantify colorimetric shifts in potassium iodide (KI) solutions during oxidation. First, a custom-built visual data acquisition system captured high-resolution video of the color transitions in a KI solution during oxidation with an NTP system. The change in [Ox]_{tot} during the experiments was monitored with a standard titrimetric method. Second, the captured frames were processed using a robust image processing pipeline to extract RGB, HSV, and Lab color features. The extracted features were statistically evaluated, and the results revealed strong linear correlations with the measured [Ox]_{tot} values, particularly in the saturation (HSV), a and b (Lab), and blue (RGB) channels. Subsequently, the [Ox]_{tot} measurements and the extracted color features were used to train and validate five ML models. Among them, linear regression and gradient boosting models achieved the highest predictive accuracy (R^2 > 0.990). It was also found that reducing the feature set from nine to four resulted in comparable performance with improved prediction efficiency, especially for gradient boosting. Finally, comparison of the model predictions with real titration measurements revealed that the CBCA system successfully predicts the [Ox]_{tot} in KI solution with high accuracy (R^2 > 0.998) even with a reduced number of features.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒå¤„ç†å’Œæœºå™¨å­¦ä¹ (Machine Learning)çš„æ–°å‹è‰²å½©è®¡ç®—æœºåˆ†æ(CBCA)æ–¹æ³•ï¼Œç”¨äºå‡†ç¡®æµ‹å®šéç­‰æ¸©ç­‰ç¦»å­ä½“(Non-Thermal Plasma, NTP)å¤„ç†çš„æ°´ç³»ç»Ÿä¸­æ€»æ°§åŒ–å‰‚æµ“åº¦([Ox]_{tot})ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿæ»´å®šæ³•çš„ä¸»è§‚æ€§åŠå…¶åœ¨å¤„ç†ç¬æ€æ´»æ€§ç‰©è´¨æ—¶çš„å±€é™æ€§ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è§†è§‰æ•°æ®é‡‡é›†ç³»ç»Ÿæ•æ‰ç¢˜åŒ–é’¾(KI)æº¶æ¶²åœ¨æ°§åŒ–è¿‡ç¨‹ä¸­çš„é«˜åˆ†è¾¨ç‡è‰²å½©å˜åŒ–ã€‚é€šè¿‡å›¾åƒå¤„ç†æµç¨‹æå–RGBã€HSVå’ŒLabè‰²å½©ç‰¹å¾ï¼Œå‘ç°é¥±å’Œåº¦(Saturation)ã€aã€båˆ†é‡ä»¥åŠè“è‰²é€šé“ä¸æµ‹å¾—çš„[Ox]_{tot}å…·æœ‰å¼ºçº¿æ€§ç›¸å…³æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥è®­ç»ƒå¹¶éªŒè¯äº†äº”ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå…¶ä¸­çº¿æ€§å›å½’(Linear Regression)å’Œæ¢¯åº¦æå‡(Gradient Boosting)æ¨¡å‹è¡¨ç°å‡ºæœ€é«˜é¢„æµ‹ç²¾åº¦ï¼Œå…¶R^2å€¼è¶…è¿‡0.990ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿å°†ç‰¹å¾é›†ä»ä¹ä¸ªå‡å°‘åˆ°å››ä¸ªï¼ŒCBCAç³»ç»Ÿä»èƒ½ä»¥æé«˜çš„å‡†ç¡®ç‡(R^2 > 0.998)é¢„æµ‹[Ox]_{tot}ã€‚è¯¥æ–¹æ³•ä¸ä»…æ˜¾è‘—æé«˜äº†æ£€æµ‹æ•ˆç‡ï¼Œè¿˜ä¸ºç­‰ç¦»å­ä½“å¤„ç†æ°´ç³»ç»Ÿä¸­çš„æ°§åŒ–å‰‚å®šé‡æä¾›äº†å®¢è§‚ä¸”é«˜åº¦ç²¾ç¡®çš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "This paper will be published later on",
      "pdf_url": "https://arxiv.org/pdf/2509.00479v1",
      "published_date": "2025-08-30 12:28:52 UTC",
      "updated_date": "2025-08-30 12:28:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:26:07.666059+00:00"
    },
    {
      "arxiv_id": "2509.00476v1",
      "title": "Cross-Domain Malware Detection via Probability-Level Fusion of Lightweight Gradient Boosting Models",
      "title_zh": "åŸºäºè½»é‡çº§æ¢¯åº¦æå‡æ¨¡å‹æ¦‚ç‡çº§èåˆçš„è·¨åŸŸæ¶æ„è½¯ä»¶æ£€æµ‹",
      "authors": [
        "Omar Khalid Ali Mohamed"
      ],
      "abstract": "The escalating sophistication of malware necessitates robust detection mechanisms that generalize across diverse data sources. Traditional single-dataset models struggle with cross-domain generalization and often incur high computational costs. This paper presents a novel, lightweight framework for malware detection that employs probability-level fusion across three distinct datasets: EMBER (static features), API Call Sequences (behavioral features), and CIC Obfuscated Memory (memory patterns). Our method trains individual LightGBM classifiers on each dataset, selects top predictive features to ensure efficiency, and fuses their prediction probabilities using optimized weights determined via grid search. Extensive experiments demonstrate that our fusion approach achieves a macro F1-score of 0.823 on a cross-domain validation set, significantly outperforming individual models and providing superior generalization. The framework maintains low computational overhead, making it suitable for real-time deployment, and all code and data are provided for full reproducibility.",
      "tldr_zh": "é’ˆå¯¹ä¼ ç»Ÿå•æ•°æ®é›†æ¶æ„è½¯ä»¶æ£€æµ‹æ¨¡å‹åœ¨è·¨é¢†åŸŸæ³›åŒ–åŠè®¡ç®—æ•ˆç‡ä¸Šçš„å±€é™æ€§ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¦‚ç‡å±‚é¢èåˆ(Probability-Level Fusion)çš„è½»é‡çº§æ¶æ„è½¯ä»¶æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ•´åˆäº† EMBER (é™æ€ç‰¹å¾)ã€API Call Sequences (è¡Œä¸ºç‰¹å¾) å’Œ CIC Obfuscated Memory (å†…å­˜æ¨¡å¼) ä¸‰ä¸ªä¸åŒé¢†åŸŸçš„æ•°æ®é›†ï¼Œå¹¶ä¸ºæ¯ä¸ªæ•°æ®é›†è®­ç»ƒäº†ç‹¬ç«‹çš„ LightGBM åˆ†ç±»å™¨ã€‚é€šè¿‡ç­›é€‰é¡¶å±‚é¢„æµ‹ç‰¹å¾å¹¶ç»“åˆç½‘æ ¼æœç´¢(Grid Search)ä¼˜åŒ–çš„æƒé‡ï¼Œç³»ç»Ÿå®ç°äº†å¯¹é¢„æµ‹æ¦‚ç‡çš„é«˜æ•ˆèåˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨é¢†åŸŸéªŒè¯é›†ä¸Šçš„å®è§‚ F1-score è¾¾åˆ° 0.823ï¼Œæ€§èƒ½æ˜¾è‘—è¶…è¶Šå•ä¸€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ä¿æŒäº†æä½çš„è®¡ç®—å¼€é”€ï¼Œé€‚åˆåœ¨å®æ—¶ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œä¸”å…·å¤‡è‰¯å¥½çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¸å…¨ä»£ç å¯å¤ç°æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "5 pages, 3 figures, 3 tables. Conference-style formatting (IEEEtran)",
      "pdf_url": "https://arxiv.org/pdf/2509.00476v1",
      "published_date": "2025-08-30 12:18:13 UTC",
      "updated_date": "2025-08-30 12:18:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:27:00.562310+00:00"
    },
    {
      "arxiv_id": "2509.00465v1",
      "title": "Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial Reasoning",
      "title_zh": "å…·èº«ç©ºé—´æ™ºèƒ½ï¼šä»éšå¼åœºæ™¯å»ºæ¨¡åˆ°ç©ºé—´æ¨ç†",
      "authors": [
        "Jiading Fang"
      ],
      "abstract": "This thesis introduces \"Embodied Spatial Intelligence\" to address the challenge of creating robots that can perceive and act in the real world based on natural language instructions. To bridge the gap between Large Language Models (LLMs) and physical embodiment, we present contributions on two fronts: scene representation and spatial reasoning. For perception, we develop robust, scalable, and accurate scene representations using implicit neural models, with contributions in self-supervised camera calibration, high-fidelity depth field generation, and large-scale reconstruction. For spatial reasoning, we enhance the spatial capabilities of LLMs by introducing a novel navigation benchmark, a method for grounding language in 3D, and a state-feedback mechanism to improve long-horizon decision-making. This work lays a foundation for robots that can robustly perceive their surroundings and intelligently act upon complex, language-based commands.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†â€œå…·èº«ç©ºé—´æ™ºèƒ½â€(Embodied Spatial Intelligence)çš„æ¦‚å¿µï¼Œæ—¨åœ¨æå‡æœºå™¨äººæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨çœŸå®ç‰©ç†ä¸–ç•Œä¸­æ„ŸçŸ¥ä¸è¡ŒåŠ¨çš„èƒ½åŠ›ã€‚ä¸ºäº†å¼¥åˆå¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸ç‰©ç†å®ä½“é—´çš„é¸¿æ²Ÿï¼Œæœ¬æ–‡ä»åœºæ™¯è¡¨å¾(scene representation)å’Œç©ºé—´æ¨ç†(spatial reasoning)ä¸¤ä¸ªç»´åº¦åšå‡ºäº†è´¡çŒ®ã€‚åœ¨æ„ŸçŸ¥æ–¹é¢ï¼Œç ”ç©¶åˆ©ç”¨éšå¼ç¥ç»æ¨¡å‹(implicit neural models)å®ç°äº†é²æ£’ä¸”é«˜ç²¾åº¦çš„è‡ªç›‘ç£ç›¸æœºæ ¡å‡†ã€é«˜ä¿çœŸæ·±åº¦åœºç”Ÿæˆä»¥åŠå¤§è§„æ¨¡åœºæ™¯é‡å»ºã€‚åœ¨æ¨ç†æ–¹é¢ï¼Œé€šè¿‡å¼•å…¥å…¨æ–°çš„å¯¼èˆªåŸºå‡†ã€3Dè¯­è¨€å¯¹é½(grounding)æ–¹æ³•ä»¥åŠçŠ¶æ€åé¦ˆæœºåˆ¶ï¼Œæ˜¾è‘—å¢å¼ºäº†LLMsçš„ç©ºé—´æ„ŸçŸ¥ä¸é•¿ç¨‹å†³ç­–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºæœºå™¨äººç¨³å¥æ„ŸçŸ¥å‘¨å›´ç¯å¢ƒå¹¶æ™ºèƒ½æ‰§è¡Œå¤æ‚çš„è¯­è¨€æŒ‡ä»¤å¥ å®šäº†æ ¸å¿ƒæŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00465v1",
      "published_date": "2025-08-30 11:42:26 UTC",
      "updated_date": "2025-08-30 11:42:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:26:56.383998+00:00"
    },
    {
      "arxiv_id": "2509.00461v2",
      "title": "TECP: Token-Entropy Conformal Prediction for LLMs",
      "title_zh": "TECPï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„è¯å…ƒç†µç¬¦åˆæ€§é¢„æµ‹",
      "authors": [
        "Beining Xu",
        "Yongming Lu"
      ],
      "abstract": "Uncertainty quantification (UQ) for open-ended language generation remains a critical yet underexplored challenge, especially under black-box constraints where internal model signals are inaccessible. In this paper, we introduce Token-Entropy Conformal Prediction (TECP), a novel framework that leverages token-level entropy as a logit-free, reference-free uncertainty measure and integrates it into a split conformal prediction (CP) pipeline to construct prediction sets with formal coverage guarantees. Unlike existing approaches that rely on semantic consistency heuristics or white-box features, TECP directly estimates epistemic uncertainty from the token entropy structure of sampled generations and calibrates uncertainty thresholds via CP quantiles to ensure provable error control. Empirical evaluations across six large language models and two benchmarks (CoQA and TriviaQA) demonstrate that TECP consistently achieves reliable coverage and compact prediction sets, outperforming prior self-consistency-based UQ methods. Our method provides a principled and efficient solution for trustworthy generation in black-box LLM settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TECPï¼ˆToken-Entropy Conformal Predictionï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é»‘ç›’å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼€æ”¾å¼æ–‡æœ¬ç”Ÿæˆä¸­ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUncertainty Quantificationï¼‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚TECPåˆ©ç”¨token-level entropyä½œä¸ºä¸€ç§æ— éœ€logitå’Œå‚è€ƒä¿¡æ¯çš„æµ‹é‡æŒ‡æ ‡ï¼Œå¹¶å°†å…¶æ•´åˆè¿›split conformal predictionæµæ°´çº¿ä¸­ï¼Œä»¥æ„å»ºå…·æœ‰æ­£å¼è¦†ç›–ä¿è¯çš„é¢„æµ‹é›†ã€‚è¯¥æ–¹æ³•ç›´æ¥ä»é‡‡æ ·ç”Ÿæˆçš„tokenç†µç»“æ„ä¸­ä¼°è®¡è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼ˆepistemic uncertaintyï¼‰ï¼Œå¹¶é€šè¿‡CPåˆ†ä½æ•°æ ¡å‡†é˜ˆå€¼ï¼Œä»è€Œå®ç°å¯è¯æ˜çš„è¯¯å·®æ§åˆ¶ã€‚åœ¨CoQAå’ŒTriviaQAåŸºå‡†æµ‹è¯•ä»¥åŠå…­ç§å¤§æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTECPèƒ½å¤ŸæŒç»­å®ç°å¯é çš„è¦†ç›–ç‡å’Œæ›´ç´§å‡‘çš„é¢„æµ‹é›†ï¼Œæ€§èƒ½ä¼˜äºåŸºäºself-consistencyçš„ç°æœ‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºé»‘ç›’æ¨¡å‹ç¯å¢ƒä¸‹çš„å¯ä¿¡ç”Ÿæˆæä¾›äº†ä¸€ç§å…·æœ‰ç†è®ºåŸåˆ™ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00461v2",
      "published_date": "2025-08-30 11:31:04 UTC",
      "updated_date": "2025-09-05 12:32:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:27:09.455329+00:00"
    },
    {
      "arxiv_id": "2509.08835v1",
      "title": "Deep opacity and AI: A threat to XAI and to privacy protection mechanisms",
      "title_zh": "æ·±åº¦ä¸é€æ˜æ€§ä¸äººå·¥æ™ºèƒ½ï¼šå¯¹ XAI åŠéšç§ä¿æŠ¤æœºåˆ¶çš„å¨èƒ",
      "authors": [
        "Vincent C. MÃ¼ller"
      ],
      "abstract": "It is known that big data analytics and AI pose a threat to privacy, and that some of this is due to some kind of \"black box problem\" in AI. I explain how this becomes a problem in the context of justification for judgments and actions. Furthermore, I suggest distinguishing three kinds of opacity: 1) the subjects do not know what the system does (\"shallow opacity\"), 2) the analysts do not know what the system does (\"standard black box opacity\"), or 3) the analysts cannot possibly know what the system might do (\"deep opacity\"). If the agents, data subjects as well as analytics experts, operate under opacity, then these agents cannot provide justifications for judgments that are necessary to protect privacy, e.g., they cannot give \"informed consent\", or guarantee \"anonymity\". It follows from these points that agents in big data analytics and AI often cannot make the judgments needed to protect privacy. So I conclude that big data analytics makes the privacy problems worse and the remedies less effective. As a positive note, I provide a brief outlook on technical ways to handle this situation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)ä¸­çš„â€œé»‘ç›’é—®é¢˜â€å¯¹éšç§ä¿æŠ¤æœºåˆ¶å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æ„æˆçš„æ·±å±‚å¨èƒã€‚ä½œè€…æå‡ºäº†ä¸‰ç§ä¸é€æ˜åº¦çš„åˆ†ç±»ï¼Œå³å—è¯•è€…ä¸çŸ¥æƒ…çš„æµ…å±‚ä¸é€æ˜æ€§(shallow opacity)ã€åˆ†æå¸ˆä¸çŸ¥æƒ…çš„æ ‡å‡†é»‘ç›’ä¸é€æ˜æ€§(standard black box opacity)ï¼Œä»¥åŠåˆ†æå¸ˆé¢„è§æ€§ç¼ºå¤±çš„æ·±åº¦ä¸é€æ˜æ€§(deep opacity)ã€‚æ–‡ç« è®ºè¯äº†åœ¨è¿™äº›ä¸é€æ˜çŠ¶æ€ä¸‹ï¼Œä»£ç†äººæ— æ³•ä¸ºä¿æŠ¤éšç§æ‰€éœ€çš„åˆ¤æ–­æä¾›æ­£å½“ç†ç”±ï¼Œè¿›è€Œå¯¼è‡´çŸ¥æƒ…åŒæ„(informed consent)å’ŒåŒ¿åæ€§(anonymity)ç­‰æœºåˆ¶å¤±æ•ˆã€‚ç ”ç©¶ç»“è®ºæŒ‡å‡ºï¼Œå¤§æ•°æ®åˆ†ææ­£åœ¨æ¶åŒ–éšç§é—®é¢˜å¹¶å‰Šå¼±è¡¥æ•‘æªæ–½çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œä½œè€…ä¹Ÿé’ˆå¯¹å¤„ç†æ­¤ç±»ä¸é€æ˜ç°çŠ¶çš„æŠ€æœ¯æ‰‹æ®µæä¾›äº†ç®€è¦å±•æœ›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.08835v1",
      "published_date": "2025-08-30 11:15:59 UTC",
      "updated_date": "2025-08-30 11:15:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:26:41.757939+00:00"
    },
    {
      "arxiv_id": "2509.05321v1",
      "title": "A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD",
      "title_zh": "åŸºäº Video2EEG-SPGN-Diffusion çš„ SEED-VD æ•°æ®é›†ç”Ÿæˆæ–¹æ¡ˆ",
      "authors": [
        "Yunfei Guo",
        "Tao Zhang",
        "Wu Huang",
        "Yao Song"
      ],
      "abstract": "This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion, that leverages the SEED-VD dataset to generate a multimodal dataset of EEG signals conditioned on video stimuli. Additionally, we disclose an engineering pipeline for aligning video and EEG data pairs, facilitating the training of multimodal large models with EEG alignment capabilities. Personalized EEG signals are generated using a self-play graph network (SPGN) integrated with a diffusion model. As a major contribution, we release a new dataset comprising over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG signals at 200 Hz and emotion labels, enabling video-EEG alignment and advancing multimodal research. This framework offers novel tools for emotion analysis, data augmentation, and brain-computer interface applications, with substantial research and engineering significance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºVideo2EEG-SPGN-Diffusionçš„å¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨SEED-VDæ•°æ®é›†ç”ŸæˆåŸºäºè§†é¢‘åˆºæ¿€è°ƒèŠ‚çš„å¤šæ¨¡æ€EEGä¿¡å·æ•°æ®é›†ã€‚ä¸ºäº†æ”¯æŒå…·å¤‡EEGå¯¹é½èƒ½åŠ›çš„å¤šæ¨¡æ€å¤§æ¨¡å‹è®­ç»ƒï¼Œç ”ç©¶å›¢é˜Ÿå…¬å¼€äº†ä¸€å¥—ç”¨äºå¯¹é½è§†é¢‘ä¸EEGæ•°æ®å¯¹çš„å·¥ç¨‹æµæ°´çº¿ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒé‡‡ç”¨è‡ªå¯¹å¼ˆå›¾ç½‘ç»œ(Self-Play Graph Network, SPGN)ç»“åˆæ‰©æ•£æ¨¡å‹(Diffusion Model)æ¥ç”Ÿæˆä¸ªæ€§åŒ–çš„EEGä¿¡å·ã€‚ä½œä¸ºä¸»è¦è´¡çŒ®ï¼Œè¯¥ç ”ç©¶å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«1000å¤šä¸ªæ ·æœ¬çš„æ–°æ•°æ®é›†ï¼Œæ¶µç›–äº†SEED-VDè§†é¢‘åˆºæ¿€ã€ç”Ÿæˆçš„62é€šé“200Hz EEGä¿¡å·ä»¥åŠæƒ…æ„Ÿæ ‡ç­¾ã€‚è¯¥æ¡†æ¶å®ç°äº†è§†é¢‘ä¸EEGçš„æœ‰æ•ˆå¯¹é½ï¼Œä¸ºæƒ…æ„Ÿåˆ†æã€æ•°æ®å¢å¼ºåŠè„‘æœºæ¥å£(Brain-Computer Interface, BCI)åº”ç”¨æä¾›äº†é‡è¦çš„ç ”ç©¶ä¸å·¥ç¨‹å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.05321v1",
      "published_date": "2025-08-30 10:52:17 UTC",
      "updated_date": "2025-08-30 10:52:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:27:15.069083+00:00"
    },
    {
      "arxiv_id": "2509.00446v1",
      "title": "NEWSAGENT: Benchmarking Multimodal Agents as Journalists with Real-World Newswriting Tasks",
      "title_zh": "NEWSAGENTï¼šåŸºäºçœŸå®ä¸–ç•Œæ–°é—»å†™ä½œä»»åŠ¡è¯„ä¼°å¤šæ¨¡æ€æ™ºèƒ½ä½“è®°è€…èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yen-Che Chien",
        "Kuang-Da Wang",
        "Wei-Yao Wang",
        "Wen-Chih Peng"
      ],
      "abstract": "Recent advances in autonomous digital agents from industry (e.g., Manus AI and Gemini's research mode) highlight potential for structured tasks by autonomous decision-making and task decomposition; however, it remains unclear to what extent the agent-based systems can improve multimodal web data productivity. We study this in the realm of journalism, which requires iterative planning, interpretation, and contextual reasoning from multimodal raw contents to form a well structured news. We introduce NEWSAGENT, a benchmark for evaluating how agents can automatically search available raw contents, select desired information, and edit and rephrase to form a news article by accessing core journalistic functions. Given a writing instruction and firsthand data as how a journalist initiates a news draft, agents are tasked to identify narrative perspectives, issue keyword-based queries, retrieve historical background, and generate complete articles. Unlike typical summarization or retrieval tasks, essential context is not directly available and must be actively discovered, reflecting the information gaps faced in real-world news writing. NEWSAGENT includes 6k human-verified examples derived from real news, with multimodal contents converted to text for broad model compatibility. We evaluate open- and closed-sourced LLMs with commonly-used agentic frameworks on NEWSAGENT, which shows that agents are capable of retrieving relevant facts but struggling with planning and narrative integration. We believe that NEWSAGENT serves a realistic testbed for iterating and evaluating agent capabilities in terms of multimodal web data manipulation to real-world productivity.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† NEWSAGENTï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼° autonomous agents åœ¨æ–°é—»å†™ä½œä»»åŠ¡ä¸­å¤„ç†å¤šæ¨¡æ€ç½‘é¡µæ•°æ®ç”Ÿäº§åŠ›çš„ benchmarkã€‚NEWSAGENT æ¨¡æ‹Ÿäº†çœŸå®çš„æ–°é—»æŠ¥é“æµç¨‹ï¼Œè¦æ±‚æ™ºèƒ½ä½“æ‰§è¡Œè¿­ä»£è§„åˆ’ã€å†…å®¹è§£é‡Šå’Œ context reasoningï¼Œä»åŸå§‹ææ–™ä¸­ä¸»åŠ¨å‘ç°å¹¶æ•´åˆç¼ºå¤±ä¿¡æ¯ä»¥æ„å»ºç»“æ„åŒ–çš„æ–°é—»ã€‚æ™ºèƒ½ä½“åœ¨è¯¥æ¡†æ¶ä¸‹éœ€å®Œæˆè¯†åˆ«å™äº‹è§†è§’ã€æ‰§è¡Œ keyword-based queriesã€æ£€ç´¢å†å²èƒŒæ™¯ä»¥åŠç”Ÿæˆå®Œæ•´æ–°é—»ç¨¿ç­‰æ ¸å¿ƒä»»åŠ¡ã€‚è¯¥ benchmark åŒ…å« 6,000 ä¸ªæºè‡ªçœŸå®æ–°é—»ä¸”ç»è¿‡äººå·¥éªŒè¯çš„ç¤ºä¾‹ï¼Œå¹¶ä¸ºç¡®ä¿å¹¿æ³›çš„æ¨¡å‹å…¼å®¹æ€§å°†å¤šæ¨¡æ€å†…å®¹è½¬åŒ–ä¸ºäº†æ–‡æœ¬ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œç›®å‰çš„å¼€æºå’Œé—­æº LLMs åŠ agentic frameworks è™½èƒ½æœ‰æ•ˆæ£€ç´¢ç›¸å…³äº‹å®ï¼Œä½†åœ¨ planning å’Œ narrative integration æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥é¡¹å·¥ä½œä¸ºè¯„ä¼°å’Œä¼˜åŒ–æ™ºèƒ½ä½“åœ¨å¤šæ¨¡æ€æ•°æ®æ“ä½œåŠç°å®ç”Ÿäº§åŠ›è½¬åŒ–æ–¹é¢çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªåˆ‡å®çš„æµ‹è¯•å¹³å°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2509.00446v1",
      "published_date": "2025-08-30 10:31:34 UTC",
      "updated_date": "2025-08-30 10:31:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:26:57.365302+00:00"
    },
    {
      "arxiv_id": "2509.02609v1",
      "title": "Contrastive clustering based on regular equivalence for influential node identification in complex networks",
      "title_zh": "åŸºäºæ­£åˆ™ç­‰ä»·å¯¹æ¯”èšç±»çš„å¤æ‚ç½‘ç»œå½±å“åŠ›èŠ‚ç‚¹è¯†åˆ«",
      "authors": [
        "Yanmei Hu",
        "Yihang Wu",
        "Bing Sun",
        "Xue Yue",
        "Biao Cai",
        "Xiangtao Li",
        "Yang Chen"
      ],
      "abstract": "Identifying influential nodes in complex networks is a fundamental task in network analysis with wide-ranging applications across domains. While deep learning has advanced node influence detection, existing supervised approaches remain constrained by their reliance on labeled data, limiting their applicability in real-world scenarios where labels are scarce or unavailable. While contrastive learning demonstrates significant potential for performance enhancement, existing approaches predominantly rely on multiple-embedding generation to construct positive/negative sample pairs. To overcome these limitations, we propose ReCC (\\textit{r}egular \\textit{e}quivalence-based \\textit{c}ontrastive \\textit{c}lustering), a novel deep unsupervised framework for influential node identification. We first reformalize influential node identification as a label-free deep clustering problem, then develop a contrastive learning mechanism that leverages regular equivalence-based similarity, which captures structural similarities between nodes beyond local neighborhoods, to generate positive and negative samples. This mechanism is integrated into a graph convolutional network to learn node embeddings that are used to differentiate influential from non-influential nodes. ReCC is pre-trained using network reconstruction loss and fine-tuned with a combined contrastive and clustering loss, with both phases being independent of labeled data. Additionally, ReCC enhances node representations by combining structural metrics with regular equivalence-based similarities. Extensive experiments demonstrate that ReCC outperforms state-of-the-art approaches across several benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤æ‚ç½‘ç»œä¸­å½±å“æ€§èŠ‚ç‚¹è¯†åˆ«(influential node identification)é«˜åº¦ä¾èµ–æ ‡æ³¨æ•°æ®ï¼Œä»¥åŠç°æœ‰å¯¹æ¯”å­¦ä¹ (contrastive learning)æ„é€ æ ·æœ¬å¯¹æ–¹å¼å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºReCC(regular equivalence-based contrastive clustering)çš„æ·±åº¦æ— ç›‘ç£æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å½±å“æ€§èŠ‚ç‚¹è¯†åˆ«é‡æ–°å®šä¹‰ä¸ºæ— æ ‡ç­¾çš„æ·±åº¦èšç±»(deep clustering)é—®é¢˜ï¼Œå¹¶åˆ›æ–°æ€§åœ°åˆ©ç”¨æ­£åˆ™ç­‰ä»·(regular equivalence)ç›¸ä¼¼æ€§æ¥æ•æ‰è¶…è¶Šå±€éƒ¨é‚»åŸŸçš„èŠ‚ç‚¹ç»“æ„ç‰¹å¾ï¼Œä»¥æ­¤ç”Ÿæˆå¯¹æ¯”å­¦ä¹ æ‰€éœ€çš„æ­£è´Ÿæ ·æœ¬ã€‚é€šè¿‡å°†è¯¥æœºåˆ¶é›†æˆåˆ°å›¾å·ç§¯ç½‘ç»œ(GCN)ä¸­ï¼ŒReCCèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å…·åˆ¤åˆ«åŠ›çš„èŠ‚ç‚¹åµŒå…¥(node embeddings)ï¼Œä»è€Œåœ¨æ— ç›‘ç£ç¯å¢ƒä¸‹æœ‰æ•ˆåŒºåˆ†å½±å“åŠ›èŠ‚ç‚¹ã€‚æ¨¡å‹é‡‡ç”¨ç½‘ç»œé‡æ„æŸå¤±è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ç»“åˆå¯¹æ¯”ä¸èšç±»æŸå¤±è¿›è¡Œå¾®è°ƒï¼Œæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å®Œå…¨ç‹¬ç«‹äºæ ‡æ³¨æ•°æ®ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒReCCåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œä¸ºå¤„ç†ç¼ºä¹æ ‡ç­¾çš„å®é™…ç½‘ç»œåœºæ™¯æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.02609v1",
      "published_date": "2025-08-30 09:34:39 UTC",
      "updated_date": "2025-08-30 09:34:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:27:35.898185+00:00"
    },
    {
      "arxiv_id": "2509.00414v1",
      "title": "MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature",
      "title_zh": "MedSEBAï¼šåŸºäºåŠ¨æ€æ¼”è¿›åŒ»å­¦æ–‡çŒ®çš„å¾ªè¯ç­”æ¡ˆç”Ÿæˆ",
      "authors": [
        "Juraj Vladika",
        "Florian Matthes"
      ],
      "abstract": "In the digital age, people often turn to the Internet in search of medical advice and recommendations. With the increasing volume of online content, it has become difficult to distinguish reliable sources from misleading information. Similarly, millions of medical studies are published every year, making it challenging for researchers to keep track of the latest scientific findings. These evolving studies can reach differing conclusions, which is not reflected in traditional search tools. To address these challenges, we introduce MedSEBA, an interactive AI-powered system for synthesizing evidence-based answers to medical questions. It utilizes the power of Large Language Models to generate coherent and expressive answers, but grounds them in trustworthy medical studies dynamically retrieved from the research database PubMed. The answers consist of key points and arguments, which can be traced back to respective studies. Notably, the platform also provides an overview of the extent to which the most relevant studies support or refute the given medical claim, and a visualization of how the research consensus evolved through time. Our user study revealed that medical experts and lay users find the system usable and helpful, and the provided answers trustworthy and informative. This makes the system well-suited for both everyday health questions and advanced research insights.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† MedSEBAï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é’ˆå¯¹åŒ»ç–—é—®é¢˜ç”Ÿæˆå¾ªè¯åŒ»å­¦ç­”æ¡ˆçš„äº¤äº’å¼ AI ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models) ç”Ÿæˆè¿è´¯çš„å›ç­”ï¼Œå¹¶å°†å…¶å»ºç«‹åœ¨ä» PubMed æ•°æ®åº“åŠ¨æ€æ£€ç´¢çš„å¯é åŒ»å­¦ç ”ç©¶åŸºç¡€ä¹‹ä¸Šã€‚MedSEBA çš„å›ç­”åŒ…å«å¯è¿½æº¯è‡³åŸå§‹ç ”ç©¶çš„å…³é”®è®ºç‚¹ï¼Œå¹¶æä¾›ç›¸å…³æ–‡çŒ®å¯¹ç‰¹å®šåŒ»å­¦ä¸»å¼ æ”¯æŒæˆ–åé©³ç¨‹åº¦çš„æ¦‚è§ˆï¼ŒåŒæ—¶å¯è§†åŒ–å±•ç¤ºç ”ç©¶å…±è¯†éšæ—¶é—´çš„æ¼”å˜è¿‡ç¨‹ã€‚ç”¨æˆ·ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŒ»å­¦ä¸“å®¶å’Œæ™®é€šç”¨æˆ·å‡è®¤ä¸ºè¯¥ç³»ç»Ÿå…·æœ‰æé«˜çš„å¯ç”¨æ€§ï¼Œä¸”æä¾›çš„ç­”æ¡ˆä¿¡æ¯ä¸°å¯Œä¸”å€¼å¾—ä¿¡èµ–ã€‚è¯¥ç³»ç»Ÿæœ‰æ•ˆåœ°è§£å†³äº†åœ¨æ•°å­—åŒ–æ—¶ä»£éš¾ä»¥åŒºåˆ†å¯é åŒ»å­¦æ¥æºä»¥åŠéš¾ä»¥è¿½è¸ªä¸æ–­æ¼”å˜çš„ç§‘å­¦å‘ç°ç­‰æŒ‘æˆ˜ï¼Œé€‚ç”¨äºæ—¥å¸¸å¥åº·å’¨è¯¢å’Œé«˜çº§ç ”ç©¶æ´å¯Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to CIKM 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.00414v1",
      "published_date": "2025-08-30 08:43:09 UTC",
      "updated_date": "2025-08-30 08:43:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:27:13.168492+00:00"
    },
    {
      "arxiv_id": "2509.00402v1",
      "title": "Curriculum Guided Personalized Subgraph Federated Learning",
      "title_zh": "è¯¾ç¨‹å¼•å¯¼çš„ä¸ªæ€§åŒ–å­å›¾è”é‚¦å­¦ä¹ ",
      "authors": [
        "Minku Kang",
        "Hogun Park"
      ],
      "abstract": "Subgraph Federated Learning (FL) aims to train Graph Neural Networks (GNNs) across distributed private subgraphs, but it suffers from severe data heterogeneity. To mitigate data heterogeneity, weighted model aggregation personalizes each local GNN by assigning larger weights to parameters from clients with similar subgraph characteristics inferred from their current model states. However, the sparse and biased subgraphs often trigger rapid overfitting, causing the estimated client similarity matrix to stagnate or even collapse. As a result, aggregation loses effectiveness as clients reinforce their own biases instead of exploiting diverse knowledge otherwise available. To this end, we propose a novel personalized subgraph FL framework called Curriculum guided personalized sUbgraph Federated Learning (CUFL). On the client side, CUFL adopts Curriculum Learning (CL) that adaptively selects edges for training according to their reconstruction scores, exposing each GNN first to easier, generic cross-client substructures and only later to harder, client-specific ones. This paced exposure prevents early overfitting to biased patterns and enables gradual personalization. By regulating personalization, the curriculum also reshapes server aggregation from exchanging generic knowledge to propagating client-specific knowledge. Further, CUFL improves weighted aggregation by estimating client similarity using fine-grained structural indicators reconstructed on a random reference graph. Extensive experiments on six benchmark datasets confirm that CUFL achieves superior performance compared to relevant baselines. Code is available at https://github.com/Kang-Min-Ku/CUFL.git.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CUFL (Curriculum guided personalized sUbgraph Federated Learning)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å­å›¾è”é‚¦å­¦ä¹  (Subgraph Federated Learning) ä¸­ç”±äºæ•°æ®å¼‚è´¨æ€§å’Œç¨€ç–å­å›¾å¯¼è‡´çš„è¿‡æ‹ŸåˆåŠå®¢æˆ·ç«¯ç›¸ä¼¼åº¦è¯„ä¼°å¤±æ•ˆé—®é¢˜çš„æ–°å‹æ¡†æ¶ã€‚åœ¨å®¢æˆ·ç«¯ï¼ŒCUFL å¼•å…¥äº†è¯¾ç¨‹å­¦ä¹  (Curriculum Learning) ç­–ç•¥ï¼Œé€šè¿‡é‡æ„åˆ†æ•°è‡ªé€‚åº”åœ°é€‰æ‹©è®­ç»ƒè¾¹ï¼Œå¼•å¯¼æ¨¡å‹å…ˆå­¦ä¹ é€šç”¨çš„è·¨å®¢æˆ·ç«¯å­ç»“æ„ï¼Œéšåå†å­¦ä¹ å¤æ‚çš„å®¢æˆ·ç«¯ç‰¹å®šç»“æ„ã€‚è¿™ç§å¾ªåºæ¸è¿›çš„è®­ç»ƒæ–¹å¼æœ‰æ•ˆé˜²æ­¢äº†æ¨¡å‹å¯¹åç½®æ¨¡å¼çš„æ—©æœŸè¿‡æ‹Ÿåˆï¼Œå¹¶é‡å¡‘äº†æœåŠ¡å™¨ç«¯çš„èšåˆé€»è¾‘ï¼Œä½¿å…¶ä»äº¤æ¢é€šç”¨çŸ¥è¯†è½¬å‘ä¼ æ’­ç‰¹å®šçŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒCUFL é€šè¿‡åœ¨éšæœºå‚è€ƒå›¾ä¸Šé‡æ„ç»†ç²’åº¦ç»“æ„æŒ‡æ ‡æ¥ä¼˜åŒ–åŠ æƒèšåˆè¿‡ç¨‹ï¼Œä»è€Œæ›´ç²¾å‡†åœ°ä¼°è®¡å®¢æˆ·ç«¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚åœ¨å…­ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒCUFL åœ¨å¤„ç†åˆ†å¸ƒå¼å›¾æ•°æ®ä»»åŠ¡æ—¶æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the CIKM 2025. This is an extended version of the original submission",
      "pdf_url": "https://arxiv.org/pdf/2509.00402v1",
      "published_date": "2025-08-30 08:01:36 UTC",
      "updated_date": "2025-08-30 08:01:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:27:16.469309+00:00"
    },
    {
      "arxiv_id": "2509.04476v2",
      "title": "Training Text-to-Molecule Models with Context-Aware Tokenization",
      "title_zh": "åŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†è¯çš„æ–‡æœ¬åˆ°åˆ†å­æ¨¡å‹è®­ç»ƒ",
      "authors": [
        "Seojin Kim",
        "Hyeontae Song",
        "Jaehyun Nam",
        "Jinwoo Shin"
      ],
      "abstract": "Recently, text-to-molecule models have shown great potential across various chemical applications, e.g., drug-discovery. These models adapt language models to molecular data by representing molecules as sequences of atoms. However, they rely on atom-level tokenizations, which primarily focus on modeling local connectivity, thereby limiting the ability of models to capture the global structural context within molecules. To tackle this issue, we propose a novel text-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by the significance of the substructure-level contexts in understanding molecule structures, e.g., ring systems, we introduce substructure-level tokenization for text-to-molecule models. Building on our tokenization scheme, we develop an importance-based training strategy that prioritizes key substructures, enabling CAMT5 to better capture the molecular semantics. Extensive experiments verify the superiority of CAMT5 in various text-to-molecule generation tasks. Intriguingly, we find that CAMT5 outperforms the state-of-the-art methods using only 2% of training tokens. In addition, we propose a simple yet effective ensemble strategy that aggregates the outputs of text-to-molecule models to further boost the generation performance. Code is available at https://github.com/Songhyeontae/CAMT5.git.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Text-to-Molecule æ¨¡å‹ä¸­åŸå­çº§ Tokenization éš¾ä»¥æ•æ‰åˆ†å­å…¨å±€ç»“æ„ä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œæå‡ºäº† Context-Aware Molecular T5 (CAMT5) æ¨¡å‹ã€‚å—å­ç»“æ„çº§ (Substructure-level) ä¸Šä¸‹æ–‡å¯¹ç†è§£åˆ†å­ç»“æ„é‡è¦æ€§çš„å¯å‘ï¼Œè¯¥æ¨¡å‹å¼•å…¥äº† Substructure-level Tokenization æ–¹æ¡ˆã€‚åŸºäºæ­¤æ–¹æ¡ˆï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§ Importance-based Training ç­–ç•¥ï¼Œé€šè¿‡ä¼˜å…ˆå¤„ç†å…³é”®å­ç»“æ„ï¼Œä½¿ CAMT5 èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰åˆ†å­è¯­ä¹‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAMT5 åœ¨å¤šé¡¹ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä»…éœ€ 2% çš„è®­ç»ƒ Token å³å¯è¶…è¶Šå½“å‰çš„ SOTA æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§é›†æˆç­–ç•¥ (Ensemble Strategy) æ¥èšåˆæ¨¡å‹è¾“å‡ºï¼Œè¿›ä¸€æ­¥æå‡äº†åˆ†å­çš„ç”Ÿæˆæ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2509.04476v2",
      "published_date": "2025-08-30 07:59:02 UTC",
      "updated_date": "2025-09-17 10:53:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:27:28.395731+00:00"
    },
    {
      "arxiv_id": "2509.00398v4",
      "title": "A Study on the Framework for Evaluating the Ethics and Trustworthiness of Generative AI",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¼¦ç†ä¸å¯ä¿¡åº¦è¯„ä¼°æ¡†æ¶ç ”ç©¶",
      "authors": [
        "Cheonsu Jeong",
        "Seunghyun Lee",
        "Seonhee Jeong",
        "Sungsu Kim"
      ],
      "abstract": "This study provides an in_depth analysis of the ethical and trustworthiness challenges emerging alongside the rapid advancement of generative artificial intelligence (AI) technologies and proposes a comprehensive framework for their systematic evaluation. While generative AI, such as ChatGPT, demonstrates remarkable innovative potential, it simultaneously raises ethical and social concerns, including bias, harmfulness, copyright infringement, privacy violations, and hallucination. Current AI evaluation methodologies, which mainly focus on performance and accuracy, are insufficient to address these multifaceted issues. Thus, this study emphasizes the need for new human_centered criteria that also reflect social impact. To this end, it identifies key dimensions for evaluating the ethics and trustworthiness of generative AI_fairness, transparency, accountability, safety, privacy, accuracy, consistency, robustness, explainability, copyright and intellectual property protection, and source traceability and develops detailed indicators and assessment methodologies for each. Moreover, it provides a comparative analysis of AI ethics policies and guidelines in South Korea, the United States, the European Union, and China, deriving key approaches and implications from each. The proposed framework applies across the AI lifecycle and integrates technical assessments with multidisciplinary perspectives, thereby offering practical means to identify and manage ethical risks in real_world contexts. Ultimately, the study establishes an academic foundation for the responsible advancement of generative AI and delivers actionable insights for policymakers, developers, users, and other stakeholders, supporting the positive societal contributions of AI technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥åˆ†æäº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) å¿«é€Ÿå‘å±•å¸¦æ¥çš„ä¼¦ç†ä¸å¯ä¿¡åº¦æŒ‘æˆ˜ï¼Œå¦‚åè§ (Bias)ã€å¹»è§‰ (Hallucination) å’Œç‰ˆæƒä¾µæƒç­‰ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿçš„æ€§èƒ½è¯„ä¼°ä½“ç³»å·²éš¾ä»¥åº”å¯¹è¿™äº›å¤šç»´é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€å¥—ä»¥äººä¸ºæœ¬çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œç¡®ç«‹äº†åŒ…æ‹¬å…¬å¹³æ€§ (Fairness)ã€é€æ˜åº¦ (Transparency)ã€å¯é æ€§ (Accountability)ã€å®‰å…¨æ€§ (Safety) å’Œé²æ£’æ€§ (Robustness) åœ¨å†…çš„å…³é”®ç»´åº¦ï¼Œå¹¶ä¸ºæ¯ä¸ªç»´åº¦å¼€å‘äº†è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡ä¸æ–¹æ³•ã€‚é€šè¿‡å¯¹éŸ©å›½ã€ç¾å›½ã€æ¬§ç›ŸåŠä¸­å›½ AI ä¼¦ç†æ”¿ç­–çš„å¯¹æ¯”åˆ†æï¼Œè¯¥ç ”ç©¶æç‚¼å‡ºäº†è¦†ç›– AI å…¨ç”Ÿå‘½å‘¨æœŸçš„æ²»ç†ç­–ç•¥ã€‚è¯¥æ¡†æ¶å°†æŠ€æœ¯è¯„ä¼°ä¸å¤šå­¦ç§‘è§†è§’ç›¸ç»“åˆï¼Œä¸ºåœ¨ç°å®è¯­å¢ƒä¸­è¯†åˆ«å’Œç®¡ç†ä¼¦ç†é£é™©æä¾›äº†å®ç”¨å·¥å…·ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶ä¸ºè´Ÿè´£ä»»çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å‘å±•å¥ å®šäº†å­¦æœ¯åŸºç¡€ï¼Œå¹¶ä¸ºæ”¿ç­–åˆ¶å®šè€…ã€å¼€å‘è€…å’Œç”¨æˆ·æä¾›äº†å…·æœ‰æ“ä½œæ€§çš„è§è§£ï¼Œä»¥æ”¯æŒ AI æŠ€æœ¯çš„ç¤¾ä¼šæ­£å‘è´¡çŒ®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "22 pages, 3 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.00398v4",
      "published_date": "2025-08-30 07:38:07 UTC",
      "updated_date": "2025-10-28 21:53:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:28:23.292107+00:00"
    },
    {
      "arxiv_id": "2509.00396v1",
      "title": "DAOVI: Distortion-Aware Omnidirectional Video Inpainting",
      "title_zh": "DAOVIï¼šç•¸å˜æ„ŸçŸ¥çš„å…¨æ™¯è§†é¢‘è¡¥å…¨",
      "authors": [
        "Ryosuke Seshimo",
        "Mariko Isogawa"
      ],
      "abstract": "Omnidirectional videos that capture the entire surroundings are employed in a variety of fields such as VR applications and remote sensing. However, their wide field of view often causes unwanted objects to appear in the videos. This problem can be addressed by video inpainting, which enables the natural removal of such objects while preserving both spatial and temporal consistency. Nevertheless, most existing methods assume processing ordinary videos with a narrow field of view and do not tackle the distortion in equirectangular projection of omnidirectional videos. To address this issue, this paper proposes a novel deep learning model for omnidirectional video inpainting, called Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI introduces a module that evaluates temporal motion information in the image space considering geodesic distance, as well as a depth-aware feature propagation module in the feature space that is designed to address the geometric distortion inherent to omnidirectional videos. The experimental results demonstrate that our proposed method outperforms existing methods both quantitatively and qualitatively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DAOVIï¼Œä¸€ç§ä¸“é—¨é’ˆå¯¹å…¨å‘è§†é¢‘è¡¥å…¨(Omnidirectional Video Inpainting)çš„ç•¸å˜æ„ŸçŸ¥æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç”±äºå…¨å‘è§†é¢‘é€šå¸¸é‡‡ç”¨ç­‰è·æŸ±çŠ¶æŠ•å½±(Equirectangular Projection)ï¼Œä¼ ç»Ÿçš„è§†é¢‘è¡¥å…¨æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»è§†é¢‘æ—¶å¾€å¾€ä¼šå› ä¸¥é‡çš„å‡ ä½•ç•¸å˜è€Œå¤±æ•ˆã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼ŒDAOVIå¼•å…¥äº†ä¸€ä¸ªç»“åˆå¤§åœ†è·ç¦»(Geodesic Distance)æ¥è¯„ä¼°å›¾åƒç©ºé—´æ—¶ç©ºè¿åŠ¨ä¿¡æ¯çš„æ¨¡å—ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜åŒ…å«ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„æ·±åº¦æ„ŸçŸ¥ç‰¹å¾ä¼ æ’­(Depth-aware Feature Propagation)æ¨¡å—ï¼Œç”¨äºåœ¨ç‰¹å¾ç©ºé—´å†…æœ‰æ•ˆå¤„ç†å…¨å‘è§†é¢‘å›ºæœ‰çš„å‡ ä½•ç•¸å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDAOVIåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´è‡ªç„¶åœ°ç§»é™¤è§†é¢‘ä¸­çš„å¤šä½™ç‰©ä½“å¹¶ä¿æŒä¼˜å¼‚çš„ç©ºåŸŸä¸æ—¶åŸŸä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶ä¸ºVRåº”ç”¨å’Œé¥æ„Ÿç­‰é¢†åŸŸçš„å…¨å‘è§†é¢‘å†…å®¹ç¼–è¾‘æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "BMVC 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.00396v1",
      "published_date": "2025-08-30 07:27:52 UTC",
      "updated_date": "2025-08-30 07:27:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:29:05.366869+00:00"
    },
    {
      "arxiv_id": "2509.00391v1",
      "title": "The Resurgence of GCG Adversarial Attacks on Large Language Models",
      "title_zh": "GCG å¯¹å¤§è¯­è¨€æ¨¡å‹å¯¹æŠ—æ”»å‡»çš„å¤å…´",
      "authors": [
        "Yuting Tan",
        "Xuying Li",
        "Zhuo Li",
        "Huizhen Shu",
        "Peikang Hu"
      ],
      "abstract": "Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient (GCG) algorithm, has emerged as a powerful method for jailbreaking large language models (LLMs). In this paper, we present a systematic appraisal of GCG and its annealing-augmented variant, T-GCG, across open-source LLMs of varying scales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack effectiveness on both safety-oriented prompts (AdvBench) and reasoning-intensive coding prompts. Our study reveals three key findings: (1) attack success rates (ASR) decrease with model size, reflecting the increasing complexity and non-convexity of larger models' loss landscapes; (2) prefix-based heuristics substantially overestimate attack effectiveness compared to GPT-4o semantic judgments, which provide a stricter and more realistic evaluation; and (3) coding-related prompts are significantly more vulnerable than adversarial safety prompts, suggesting that reasoning itself can be exploited as an attack vector. In addition, preliminary results with T-GCG show that simulated annealing can diversify adversarial search and achieve competitive ASR under prefix evaluation, though its benefits under semantic judgment remain limited. Together, these findings highlight the scalability limits of GCG, expose overlooked vulnerabilities in reasoning tasks, and motivate further development of annealing-inspired strategies for more robust adversarial evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹åŸºäºæ¢¯åº¦çš„å¯¹æŠ—æ€§æç¤ºç®—æ³•ï¼Œç‰¹åˆ«æ˜¯è´ªå©ªåæ ‡æ¢¯åº¦ç®—æ³•(Greedy Coordinate Gradient, GCG)åŠå…¶é€€ç«å¢å¼ºå˜ä½“T-GCGï¼Œåœ¨ä¸åŒè§„æ¨¡çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸Šè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚ä½œè€…åˆ©ç”¨Qwen2.5-0.5Bã€LLaMA-3.2-1Bå’ŒGPT-OSS-20Bï¼Œé’ˆå¯¹å®‰å…¨æ€§æç¤º(AdvBench)å’Œæ¨ç†å¯†é›†å‹ä»£ç æç¤ºè¯„ä¼°äº†æ”»å‡»æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°æ”»å‡»æˆåŠŸç‡(ASR)éšæ¨¡å‹è§„æ¨¡å¢åŠ è€Œä¸‹é™ï¼Œè¿™åæ˜ äº†å¤§å‹æ¨¡å‹æŸå¤±å‡½æ•°æ™¯è§‚(loss landscapes)æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§å’Œéå‡¸æ€§ã€‚å®éªŒæ­ç¤ºåŸºäºå‰ç¼€(prefix-based)çš„å¯å‘å¼è¯„ä¼°æ˜¾è‘—é«˜ä¼°äº†æ”»å‡»æ•ˆæœï¼Œè€Œé‡‡ç”¨GPT-4oè¿›è¡Œçš„è¯­ä¹‰åˆ¤æ–­åˆ™æä¾›äº†æ›´ä¸ºä¸¥æ ¼ä¸”ç°å®çš„è¯„ä¼°æ ‡å‡†ã€‚æ­¤å¤–ï¼Œä»£ç ç›¸å…³çš„æç¤ºæ¯”å¯¹æŠ—æ€§å®‰å…¨æç¤ºæ›´æ˜“å—æ”»å‡»ï¼Œè¡¨æ˜æ¨ç†èƒ½åŠ›æœ¬èº«å¯èƒ½è¢«åˆ©ç”¨ä½œä¸ºæ”»å‡»å‘é‡ã€‚å…³äºT-GCGçš„åˆæ­¥ç»“æœæ˜¾ç¤ºï¼Œæ¨¡æ‹Ÿé€€ç«æŠ€æœ¯èƒ½å¤Ÿä½¿å¯¹æŠ—æ€§æœç´¢å¤šæ ·åŒ–ï¼Œå¹¶åœ¨å‰ç¼€è¯„ä¼°ä¸‹å–å¾—å…·æœ‰ç«äº‰åŠ›çš„ASRã€‚è¯¥ç ”ç©¶é˜æ˜äº†GCGçš„æ‰©å±•å±€é™æ€§ï¼Œæš´éœ²äº†æ¨ç†ä»»åŠ¡ä¸­è¢«å¿½è§†çš„æ¼æ´ï¼Œå¹¶ä¸ºæœªæ¥å¼€å‘æ›´ç¨³å¥çš„å¯¹æŠ—æ€§è¯„ä¼°ç­–ç•¥æä¾›äº†å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.00391v1",
      "published_date": "2025-08-30 07:04:29 UTC",
      "updated_date": "2025-08-30 07:04:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:28:11.593222+00:00"
    },
    {
      "arxiv_id": "2509.00389v1",
      "title": "Beyond Negative Transfer: Disentangled Preference-Guided Diffusion for Cross-Domain Sequential Recommendation",
      "title_zh": "è¶…è¶Šè´Ÿè¿ç§»ï¼šé¢å‘è·¨åŸŸåºåˆ—æ¨èçš„è§£è€¦åå¥½å¼•å¯¼æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Xiaoxin Ye",
        "Chengkai Huang",
        "Hongtao Huang",
        "Lina Yao"
      ],
      "abstract": "Cross-Domain Sequential Recommendation (CDSR) leverages user behaviors across domains to enhance recommendation quality. However, naive aggregation of sequential signals can introduce conflicting domain-specific preferences, leading to negative transfer. While Sequential Recommendation (SR) already suffers from noisy behaviors such as misclicks and impulsive actions, CDSR further amplifies this issue due to domain heterogeneity arising from diverse item types and user intents. The core challenge is disentangling three intertwined signals: domain-invariant preferences, domain-specific preferences, and noise. Diffusion Models (DMs) offer a generative denoising framework well-suited for disentangling complex user preferences and enhancing robustness to noise. Their iterative refinement process enables gradual denoising, making them effective at capturing subtle preference signals. However, existing applications in recommendation face notable limitations: sequential DMs often conflate shared and domain-specific preferences, while cross-domain collaborative filtering DMs neglect temporal dynamics, limiting their ability to model evolving user preferences. To bridge these gaps, we propose \\textbf{DPG-Diff}, a novel Disentangled Preference-Guided Diffusion Model, the first diffusion-based approach tailored for CDSR, to or best knowledge. DPG-Diff decomposes user preferences into domain-invariant and domain-specific components, which jointly guide the reverse diffusion process. This disentangled guidance enables robust cross-domain knowledge transfer, mitigates negative transfer, and filters sequential noise. Extensive experiments on real-world datasets demonstrate that DPG-Diff consistently outperforms state-of-the-art baselines across multiple metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DPG-Diffï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºè·¨åŸŸé¡ºåºæ¨è (Cross-Domain Sequential Recommendation, CDSR) è®¾è®¡çš„è§£è€¦åå¥½å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•ä¸­å­˜åœ¨çš„è´Ÿè¿ç§» (negative transfer) å’Œé¡ºåºè¡Œä¸ºå™ªå£°é—®é¢˜ã€‚DPG-Diff é€šè¿‡å°†ç”¨æˆ·åå¥½åˆ†è§£ä¸ºåŸŸä¸å˜ (domain-invariant) å’ŒåŸŸç‰¹å®š (domain-specific) ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œåˆ©ç”¨å®ƒä»¬å…±åŒå¼•å¯¼æ‰©æ•£æ¨¡å‹ (Diffusion Models) çš„é€†å‘ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œå®ç°ç²¾å‡†çš„åå¥½å»ºæ¨¡ã€‚è¿™ç§è§£è€¦å¼•å¯¼æœºåˆ¶èƒ½å¤Ÿå®ç°é²æ£’çš„è·¨åŸŸçŸ¥è¯†è½¬ç§»ï¼Œåœ¨å¢å¼ºæ¨èç³»ç»Ÿå¯¹è¯¯ç‚¹ç­‰å™ªå£°æŠµå¾¡èƒ½åŠ›çš„åŒæ—¶ï¼Œæ•æ‰åˆ°ç»†å¾®ä¸”æ¼”åŒ–çš„ç”¨æˆ·æ„å›¾ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDPG-Diff åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¸€è‡´ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹ï¼Œä¸ºè§£å†³ CDSR ä¸­çš„åŸŸå¼‚æ„æ€§æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆçš„ç”Ÿæˆå¼å»å™ªæ¡†æ¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00389v1",
      "published_date": "2025-08-30 06:56:56 UTC",
      "updated_date": "2025-08-30 06:56:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:28:17.404898+00:00"
    },
    {
      "arxiv_id": "2509.00387v1",
      "title": "Unifying Adversarial Perturbation for Graph Neural Networks",
      "title_zh": "å›¾ç¥ç»ç½‘ç»œå¯¹æŠ—æ‰°åŠ¨çš„ç»Ÿä¸€åŒ–",
      "authors": [
        "Jinluan Yang",
        "Ruihao Zhang",
        "Zhengyu Chen",
        "Fei Wu",
        "Kun Kuang"
      ],
      "abstract": "This paper studies the vulnerability of Graph Neural Networks (GNNs) to adversarial attacks on node features and graph structure. Various methods have implemented adversarial training to augment graph data, aiming to bolster the robustness and generalization of GNNs. These methods typically involve applying perturbations to the node feature, weights, or graph structure and subsequently minimizing the loss by learning more robust graph model parameters under the adversarial perturbations. Despite the effectiveness of adversarial training in enhancing GNNs' robustness and generalization abilities, its application has been largely confined to specific datasets and GNN types. In this paper, we propose a novel method, PerturbEmbedding, that integrates adversarial perturbation and training, enhancing GNNs' resilience to such attacks and improving their generalization ability. PerturbEmbedding performs perturbation operations directly on every hidden embedding of GNNs and provides a unified framework for most existing perturbation strategies/methods. We also offer a unified perspective on the forms of perturbations, namely random and adversarial perturbations. Through experiments on various datasets using different backbone models, we demonstrate that PerturbEmbedding significantly improves both the robustness and generalization abilities of GNNs, outperforming existing methods. The rejection of both random (non-targeted) and adversarial (targeted) perturbations further enhances the backbone model's performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(GNNs)åœ¨èŠ‚ç‚¹ç‰¹å¾å’Œå›¾ç»“æ„æ–¹é¢æ˜“å—å¯¹æŠ—æ”»å‡»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPerturbEmbeddingçš„ç»Ÿä¸€å¯¹æŠ—æ‰°åŠ¨ä¸è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ–¹æ³•ç›´æ¥åœ¨GNNsçš„æ¯ä¸ªéšè—å±‚åµŒå…¥(hidden embedding)ä¸Šæ‰§è¡Œæ‰°åŠ¨æ“ä½œï¼Œèƒ½å¤Ÿå…¼å®¹å¤§å¤šæ•°ç°æœ‰çš„æ‰°åŠ¨ç­–ç•¥ï¼Œå¹¶ä¸ºéšæœºæ‰°åŠ¨(random perturbations)ä¸å¯¹æŠ—æ‰°åŠ¨(adversarial perturbations)æä¾›äº†ç»Ÿä¸€çš„ç†è®ºè§†è§’ã€‚é€šè¿‡åœ¨å¤šç§æ•°æ®é›†å’Œéª¨å¹²æ¨¡å‹ä¸Šçš„å®éªŒéªŒè¯ï¼ŒPerturbEmbeddingæ˜¾è‘—æå‡äº†GNNsçš„é²æ£’æ€§(robustness)å’Œæ³›åŒ–èƒ½åŠ›(generalization ability)ï¼Œå…¶è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚è¯¥æ¡†æ¶ä¸ä»…èƒ½æœ‰æ•ˆæŠµå¾¡é’ˆå¯¹æ€§æ”»å‡»ï¼Œè¿˜èƒ½é€šè¿‡å¤„ç†éé’ˆå¯¹æ€§æ‰°åŠ¨è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„æ•´ä½“æ€§èƒ½ï¼Œä¸ºæå‡å›¾æ¨¡å‹çš„éŸ§æ€§æä¾›äº†é€šç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00387v1",
      "published_date": "2025-08-30 06:53:36 UTC",
      "updated_date": "2025-08-30 06:53:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:28:40.347530+00:00"
    },
    {
      "arxiv_id": "2509.05318v1",
      "title": "Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models",
      "title_zh": "åŸºäºæ‰°åŠ¨å·®å¼‚ä¸€è‡´æ€§çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åé—¨æ ·æœ¬æ£€æµ‹",
      "authors": [
        "Zuquan Peng",
        "Jianming Fu",
        "Lixin Zou",
        "Li Zheng",
        "Yanzhen Ren",
        "Guojun Peng"
      ],
      "abstract": "The use of unvetted third-party and internet data renders pre-trained models susceptible to backdoor attacks. Detecting backdoor samples is critical to prevent backdoor activation during inference or injection during training. However, existing detection methods often require the defender to have access to the poisoned models, extra clean samples, or significant computational resources to detect backdoor samples, limiting their practicality. To address this limitation, we propose a backdoor sample detection method based on perturbatio\\textbf{N} discr\\textbf{E}pancy consis\\textbf{T}ency \\textbf{E}valuation (\\NETE). This is a novel detection method that can be used both pre-training and post-training phases. In the detection process, it only requires an off-the-shelf pre-trained model to compute the log probability of samples and an automated function based on a mask-filling strategy to generate perturbations. Our method is based on the interesting phenomenon that the change in perturbation discrepancy for backdoor samples is smaller than that for clean samples. Based on this phenomenon, we use curvature to measure the discrepancy in log probabilities between different perturbed samples and input samples, thereby evaluating the consistency of the perturbation discrepancy to determine whether the input sample is a backdoor sample. Experiments conducted on four typical backdoor attacks and five types of large language model backdoor attacks demonstrate that our detection strategy outperforms existing zero-shot black-box detection methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(Pre-trained Language Models)å®¹æ˜“å—åˆ°åé—¨æ”»å‡»(Backdoor Attacks)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºNETEçš„åé—¨æ ·æœ¬æ£€æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–è¢«æ¯’åŒ–çš„æ¨¡å‹æˆ–é¢å¤–çš„å¹²å‡€æ ·æœ¬ï¼Œä»…éœ€åˆ©ç”¨ç°æˆçš„æ¨¡å‹è®¡ç®—å¯¹æ•°æ¦‚ç‡ï¼Œå¹¶é€šè¿‡æ©ç å¡«å……(Mask-filling)ç­–ç•¥è‡ªåŠ¨ç”Ÿæˆæ‰°åŠ¨ã€‚NETEåŸºäºåé—¨æ ·æœ¬åœ¨æ‰°åŠ¨ä¸‹å·®å¼‚å˜åŒ–æ˜¾è‘—å°äºå¹²å‡€æ ·æœ¬çš„è§‚æµ‹ç°è±¡ï¼Œåˆ©ç”¨æ›²ç‡(Curvature)æ¥è¡¡é‡ä¸åŒæ‰°åŠ¨æ ·æœ¬ä¸è¾“å…¥æ ·æœ¬ä¹‹é—´çš„å¯¹æ•°æ¦‚ç‡å·®å¼‚ï¼Œè¿›è€Œé€šè¿‡è¯„ä¼°æ‰°åŠ¨å·®å¼‚çš„ä¸€è‡´æ€§æ¥è¯†åˆ«åé—¨æ”»å‡»ã€‚è¯¥æ–¹æ³•å…·å¤‡æé«˜çš„å®ç”¨æ€§ï¼Œèƒ½å¤ŸåŒæ—¶åº”ç”¨äºé¢„è®­ç»ƒå’Œè®­ç»ƒåçš„æ£€æµ‹é˜¶æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNETEåœ¨å››ç§å…¸å‹åé—¨æ”»å‡»å’Œäº”ç§å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)æ”»å‡»åœºæ™¯ä¸‹çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬é»‘ç›’(Zero-shot Black-box)æ£€æµ‹æŠ€æœ¯ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages, 9 figures, 8 tables, journal",
      "pdf_url": "https://arxiv.org/pdf/2509.05318v1",
      "published_date": "2025-08-30 06:35:32 UTC",
      "updated_date": "2025-08-30 06:35:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:28:30.945242+00:00"
    },
    {
      "arxiv_id": "2509.00375v1",
      "title": "Open Data Synthesis For Deep Research",
      "title_zh": "é¢å‘æ·±åº¦ç ”ç©¶çš„å¼€æ”¾å¼æ•°æ®åˆæˆ",
      "authors": [
        "Ziyi Xia",
        "Kun Luo",
        "Hongjin Qian",
        "Zheng Liu"
      ],
      "abstract": "Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in \\href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†æ·±åº¦ç ”ç©¶(Deep Research)ä»»åŠ¡æ—¶çš„å¤æ‚éœ€æ±‚ï¼Œå°†æ­¤ç±»æ¶‰åŠå¤šæ­¥æ¨ç†å’Œè¯æ®åˆæˆçš„ä»»åŠ¡æ­£å¼å®šä¹‰ä¸ºåˆ†å±‚çº¦æŸæ»¡è¶³é—®é¢˜(HCSPs)ã€‚ä¸ºäº†è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•ç»“æ„æ·±åº¦ä¸è¶³ä»¥åŠåˆæˆæ•°æ®å­˜åœ¨æ¨ç†å¿«æ·è·¯å¾„ç­‰é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†InfoSeekæ¡†æ¶ï¼Œåˆ©ç”¨åŒæ™ºèƒ½ä½“ç³»ç»Ÿä»å¤§è§„æ¨¡ç½‘é¡µä¸­é€’å½’æ„å»ºç ”ç©¶æ ‘(Research Tree)å¹¶åˆæˆå¤æ‚çš„è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚é€šè¿‡è¯¥æ¡†æ¶ï¼Œç ”ç©¶å›¢é˜Ÿç”Ÿæˆäº†åŒ…å«5ä¸‡å¤šä¸ªè®­ç»ƒæ ·æœ¬çš„æ•°æ®é›†å’Œæ¨ç†è½¨è¿¹ï¼Œæ”¯æŒå¤åˆå¥–åŠ±è®¾è®¡å’Œè½¨è¿¹çº§æ¢ç´¢ç­‰é«˜çº§ä¼˜åŒ–ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨InfoSeekä¸Šè®­ç»ƒçš„3Bå‚æ•°æ¨¡å‹åœ¨BrowseComp-Plusç­‰æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¡¨ç°ä¼˜äº32Bçš„å¤§å‹æ¨¡å‹åŠGemini 2.5-Flashç­‰å•†ä¸šAPIï¼Œç”šè‡³è¾¾åˆ°äº†ä¸Gemini 2.5-Proç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼€æºç›¸å…³ä»£ç å’Œæ•°æ®é›†ï¼Œä¸ºæå‡æ¨¡å‹åœ¨å¤æ‚åˆ†è§£ã€å¤šæ­¥åè°ƒåŠè¯æ®åˆæˆæ–¹é¢çš„èƒ½åŠ›æä¾›äº†é«˜æ•ˆçš„å¯æ‰©å±•æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00375v1",
      "published_date": "2025-08-30 06:02:56 UTC",
      "updated_date": "2025-08-30 06:02:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:28:48.864301+00:00"
    },
    {
      "arxiv_id": "2509.18101v3",
      "title": "A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services",
      "title_zh": "æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²çš„æˆæœ¬æ•ˆç›Šåˆ†æï¼šå®ç°ä¸å•†ä¸šå¤§æ¨¡å‹æœåŠ¡çš„ç›ˆäºå¹³è¡¡",
      "authors": [
        "Guanzhong Pan",
        "Vishal Chodnekar",
        "Abinas Roy",
        "Haibo Wang"
      ],
      "abstract": "Large language models (LLMs) are becoming increasingly widespread. Organizations that want to use AI for productivity now face an important decision. They can subscribe to commercial LLM services or deploy models on their own infrastructure. Cloud services from providers such as OpenAI, Anthropic, and Google are attractive because they provide easy access to state-of-the-art models and are easy to scale. However, concerns about data privacy, the difficulty of switching service providers, and long-term operating costs have driven interest in local deployment of open-source models. This paper presents a cost-benefit analysis framework to help organizations determine when on-premise LLM deployment becomes economically viable compared to commercial subscription services. We consider the hardware requirements, operational expenses, and performance benchmarks of the latest open-source models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost of deploying these models locally with the major cloud providers subscription fee. Our findings provide an estimated breakeven point based on usage levels and performance needs. These results give organizations a practical framework for planning their LLM strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹æœ¬åœ°éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹(LLM)çš„æˆæœ¬æ•ˆç›Šåˆ†ææ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©ç»„ç»‡åœ¨å•†ä¸šLLMæœåŠ¡ä¸è‡ªå»ºåŸºç¡€è®¾æ–½ä¹‹é—´åšå‡ºæ›´å…·ç»æµæ€§çš„å†³ç­–ã€‚å°½ç®¡äº‘æœåŠ¡æä¾›äº†æ˜“äºæ‰©å±•çš„å…ˆè¿›æ¨¡å‹ï¼Œä½†å‡ºäºå¯¹æ•°æ®éšç§ã€ä¾›åº”å•†é”å®šä»¥åŠé•¿æœŸè¿è¥æˆæœ¬çš„è€ƒé‡ï¼Œæœ¬åœ°åŒ–éƒ¨ç½²å¼€æºæ¨¡å‹æ­£å—åˆ°å¹¿æ³›å…³æ³¨ã€‚è¯¥æ¡†æ¶ç»¼åˆåˆ†æäº†Qwenã€Llamaå’ŒMistralç­‰ä¸»æµå¼€æºæ¨¡å‹çš„ç¡¬ä»¶éœ€æ±‚ã€è¿è¥æ”¯å‡ºå’Œæ€§èƒ½è¡¨ç°ï¼Œå¹¶å°†å…¶æ€»æˆæœ¬ä¸ä¸»è¦äº‘ä¾›åº”å•†çš„è®¢é˜…è´¹ç”¨è¿›è¡Œäº†è¯¦ç»†å¯¹æ¯”ã€‚ç ”ç©¶ç»“æœæ ¹æ®ä¸åŒçš„ä½¿ç”¨è§„æ¨¡å’Œæ€§èƒ½è¦æ±‚ï¼Œç»™å‡ºäº†æœ¬åœ°éƒ¨ç½²è¾¾åˆ°æŸç›Šå¹³è¡¡ç‚¹(Breakeven Point)çš„ä¼°ç®—ã€‚è¿™ä¸€ç ”ç©¶ä¸ºç»„ç»‡åˆ¶å®šLLMéƒ¨ç½²ç­–ç•¥æä¾›äº†å®è·µæŒ‡å¯¼ï¼Œå¸®åŠ©å…¶ç§‘å­¦åœ°è¯„ä¼°æœ¬åœ°éƒ¨ç½²åœ¨ä½•ç§æƒ…å†µä¸‹æ¯”å•†ä¸šè®¢é˜…æœåŠ¡æ›´å…·ç»æµå¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18101v3",
      "published_date": "2025-08-30 06:01:53 UTC",
      "updated_date": "2025-11-11 05:18:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:28:57.594243+00:00"
    },
    {
      "arxiv_id": "2509.00373v1",
      "title": "Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models",
      "title_zh": "å½“æ¿€æ´»å¼•å¯¼é‡è§åå¥½ä¼˜åŒ–ï¼šé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„è¶Šç‹±æ”»å‡»é˜²å¾¡",
      "authors": [
        "Sihao Wu",
        "Gaojie Jin",
        "Wei Huang",
        "Jianhong Wang",
        "Xiaowei Huang"
      ],
      "abstract": "Vision Language Models (VLMs) have demonstrated impressive capabilities in integrating visual and textual information for understanding and reasoning, but remain highly vulnerable to adversarial attacks. While activation steering has emerged as a promising defence, existing approaches often rely on task-specific contrastive prompts to extract harmful directions, which exhibit suboptimal performance and can degrade visual grounding performance. To address these limitations, we propose \\textit{Sequence-Level Preference Optimization} for VLM (\\textit{SPO-VLM}), a novel two-stage defense framework that combines activation-level intervention with policy-level optimization to enhance model robustness. In \\textit{Stage I}, we compute adaptive layer-specific steering vectors from diverse data sources, enabling generalized suppression of harmful behaviors during inference. In \\textit{Stage II}, we refine these steering vectors through a sequence-level preference optimization process. This stage integrates automated toxicity assessment, as well as visual-consistency rewards based on caption-image alignment, to achieve safe and semantically grounded text generation. The two-stage structure of SPO-VLM balances efficiency and effectiveness by combining a lightweight mitigation foundation in Stage I with deeper policy refinement in Stage II. Extensive experiments shown SPO-VLM enhances safety against attacks via activation steering and preference optimization, while maintaining strong performance on benign tasks without compromising visual understanding capabilities. We will release our code, model weights, and evaluation toolkit to support reproducibility and future research. \\textcolor{red}{Warning: This paper may contain examples of offensive or harmful text and images.}",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)æ˜“å—è¶Šç‹±æ”»å‡»ï¼ˆJailbreaksï¼‰çš„é—®é¢˜ï¼ŒæŒ‡å‡ºç›®å‰çš„æ¿€æ´»å¼•å¯¼(Activation Steering)é˜²å¾¡æ–¹æ³•åœ¨é²æ£’æ€§å’Œè§†è§‰å®šä½æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†SPO-VLMï¼ˆSequence-Level Preference Optimization for VLMï¼‰ï¼Œä¸€ä¸ªç»“åˆæ¿€æ´»å±‚å¹²é¢„ä¸ç­–ç•¥ä¼˜åŒ–çš„åŒé˜¶æ®µé˜²å¾¡æ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè¯¥æ¡†æ¶é€šè¿‡å¤šæ ·åŒ–æ•°æ®è®¡ç®—è‡ªé€‚åº”å±‚çº§å¼•å¯¼å‘é‡(Steering Vectors)ï¼Œä»¥æ³›åŒ–æŠ‘åˆ¶æ¨ç†æ—¶çš„æœ‰å®³è¡Œä¸ºï¼›ç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡åºåˆ—çº§åå¥½ä¼˜åŒ–è¿‡ç¨‹ç²¾ç‚¼å¼•å¯¼å‘é‡ï¼Œå¹¶å¼•å…¥è‡ªåŠ¨åŒ–æ¯’æ€§è¯„ä¼°ä¸è§†è§‰ä¸€è‡´æ€§å¥–åŠ±ã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆå¹³è¡¡äº†é˜²å¾¡æ•ˆç‡ä¸ç”Ÿæˆè´¨é‡ï¼Œç¡®ä¿äº†æ–‡æœ¬ç”Ÿæˆçš„å®‰å…¨æ€§å’Œè¯­ä¹‰æ¥åœ°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSPO-VLMåœ¨æ˜¾è‘—æå‡æ¨¡å‹æŠ—æ”»å‡»èƒ½åŠ›çš„åŒæ—¶ï¼Œä»èƒ½åœ¨è‰¯æ€§ä»»åŠ¡ä¸­ä¿æŒå‡ºè‰²çš„è§†è§‰ç†è§£ä¸æ¨ç†æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00373v1",
      "published_date": "2025-08-30 06:00:53 UTC",
      "updated_date": "2025-08-30 06:00:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:29:12.854396+00:00"
    },
    {
      "arxiv_id": "2509.10486v1",
      "title": "SABR: A Stable Adaptive Bitrate Framework Using Behavior Cloning Pretraining and Reinforcement Learning Fine-Tuning",
      "title_zh": "SABRï¼šåŸºäºè¡Œä¸ºå…‹éš†é¢„è®­ç»ƒä¸å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„ç¨³å®šè‡ªé€‚åº”æ¯”ç‰¹ç‡æ¡†æ¶",
      "authors": [
        "Pengcheng Luo",
        "Yunyang Zhao",
        "Bowen Zhang",
        "Genke Yang",
        "Boon-Hee Soong",
        "Chau Yuen"
      ],
      "abstract": "With the advent of 5G, the internet has entered a new video-centric era. From short-video platforms like TikTok to long-video platforms like Bilibili, online video services are reshaping user consumption habits. Adaptive Bitrate (ABR) control is widely recognized as a critical factor influencing Quality of Experience (QoE). Recent learning-based ABR methods have attracted increasing attention. However, most of them rely on limited network trace sets during training and overlook the wide-distribution characteristics of real-world network conditions, resulting in poor generalization in out-of-distribution (OOD) scenarios. To address this limitation, we propose SABR, a training framework that combines behavior cloning (BC) pretraining with reinforcement learning (RL) fine-tuning. We also introduce benchmarks, ABRBench-3G and ABRBench-4G+, which provide wide-coverage training traces and dedicated OOD test sets for assessing robustness to unseen network conditions. Experimental results demonstrate that SABR achieves the best average rank compared with Pensieve, Comyco, and NetLLM across the proposed benchmarks. These results indicate that SABR enables more stable learning across wide distributions and improves generalization to unseen network conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SABRï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†è¡Œä¸ºå…‹éš† (Behavior Cloning, BC) é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) å¾®è°ƒçš„ç¨³å®šè‡ªé€‚åº”æ¯”ç‰¹ç‡ (Adaptive Bitrate, ABR) æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹åˆ†å¸ƒå¤– (Out-of-Distribution, OOD) ç½‘ç»œæ¡ä»¶æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼ŒSABR é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–ç½‘ç»œç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æ¨å‡ºäº† ABRBench-3G å’Œ ABRBench-4G+ åŸºå‡†æµ‹è¯•é›†ï¼Œä¸ºè¯„ä¼°æ¨¡å‹åœ¨æœªçŸ¥ç½‘ç»œæ¡ä»¶ä¸‹çš„æ€§èƒ½æä¾›äº†å¹¿æ³›çš„è½¨è¿¹æ•°æ®æ”¯æŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ Pensieveã€Comyco å’Œ NetLLM ç­‰æ¨¡å‹ç›¸æ¯”ï¼ŒSABR åœ¨æ‰€æå‡ºçš„åŸºå‡†æµ‹è¯•ä¸­å‡è·å¾—äº†æœ€ä½³çš„å¹³å‡æ’åã€‚è¯¥æ¡†æ¶ä¸ä»…å®ç°äº†æ›´ç¨³å®šçš„è·¨å¹¿åŸŸåˆ†å¸ƒå­¦ä¹ ï¼Œè¿˜æ˜¾è‘—æ”¹å–„äº†è§†é¢‘æœåŠ¡åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„ä½“éªŒè´¨é‡ (Quality of Experience, QoE)ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.10486v1",
      "published_date": "2025-08-30 05:32:45 UTC",
      "updated_date": "2025-08-30 05:32:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:29:13.053436+00:00"
    },
    {
      "arxiv_id": "2509.00357v1",
      "title": "SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding",
      "title_zh": "SurgLLMï¼šå…·æœ‰ç©ºé—´èšç„¦ä¸æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›çš„é€šç”¨æ‰‹æœ¯è§†é¢‘ç†è§£å¤šæ¨¡æ€å¤§æ¨¡å‹",
      "authors": [
        "Zhen Chen",
        "Xingjian Luo",
        "Kun Yuan",
        "Jinlin Wu",
        "Danny T. M. Chan",
        "Nassir Navab",
        "Hongbin Liu",
        "Zhen Lei",
        "Jiebo Luo"
      ],
      "abstract": "Surgical video understanding is crucial for facilitating Computer-Assisted Surgery (CAS) systems. Despite significant progress in existing studies, two major limitations persist, including inadequate visual content perception and insufficient temporal awareness in surgical videos, and hinder the development of versatile CAS solutions. In this work, we propose the SurgLLM framework, an effective large multimodal model tailored for versatile surgical video understanding tasks with enhanced spatial focus and temporal awareness. Specifically, to empower the spatial focus of surgical videos, we first devise Surgical Context-aware Multimodal Pretraining (Surg-Pretrain) for the video encoder of SurgLLM, by performing instrument-centric Masked Video Reconstruction (MV-Recon) and subsequent multimodal alignment. To incorporate surgical temporal knowledge into SurgLLM, we further propose Temporal-aware Multimodal Tuning (TM-Tuning) to enhance temporal reasoning with interleaved multimodal embeddings. Moreover, to accommodate various understanding tasks of surgical videos without conflicts, we devise a Surgical Task Dynamic Ensemble to efficiently triage a query with optimal learnable parameters in our SurgLLM. Extensive experiments performed on diverse surgical video understanding tasks, including captioning, general VQA, and temporal VQA, demonstrate significant improvements over the state-of-the-art approaches, validating the effectiveness of our SurgLLM in versatile surgical video understanding. The source code is available at https://github.com/franciszchen/SurgLLM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SurgLLM æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ‰‹æœ¯è§†é¢‘ç†è§£ä»»åŠ¡è®¾è®¡çš„é€šç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ (Large Multimodal Model)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶ä¸­è§†è§‰æ„ŸçŸ¥ä¸è¶³å’Œæ—¶é—´æ„è¯†ç¼ºå¤±çš„é—®é¢˜ã€‚ä¸ºäº†å¢å¼ºç©ºé—´ç„¦ç‚¹ (Spatial Focus)ï¼Œè¯¥æ¡†æ¶å¼€å‘äº†æ‰‹æœ¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ¨¡æ€é¢„è®­ç»ƒ (Surg-Pretrain)ï¼Œé€šè¿‡ä»¥æ‰‹æœ¯å™¨æ¢°ä¸ºä¸­å¿ƒçš„æ©ç è§†é¢‘é‡å»º (Masked Video Reconstruction, MV-Recon) å’Œéšåçš„å¤šæ¨¡æ€å¯¹é½æ¥ä¼˜åŒ–è§†é¢‘ç¼–ç å™¨ã€‚ä¸ºäº†æå‡æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è€…æå‡ºäº†æ—¶é—´æ„ŸçŸ¥å¤šæ¨¡æ€å¾®è°ƒ (Temporal-aware Multimodal Tuning, TM-Tuning)ï¼Œåˆ©ç”¨äº¤é”™çš„å¤šæ¨¡æ€åµŒå…¥å°†æ‰‹æœ¯æ—¶é—´çŸ¥è¯†èå…¥æ¨¡å‹ä¸­ã€‚æ­¤å¤–ï¼ŒSurgLLM å¼•å…¥äº†æ‰‹æœ¯ä»»åŠ¡åŠ¨æ€é›†æˆ (Surgical Task Dynamic Ensemble) æœºåˆ¶ï¼Œé€šè¿‡å­¦ä¹ æœ€ä¼˜å‚æ•°é«˜æ•ˆå¤„ç†å¤šç§æ‰‹æœ¯è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œé¿å…äº†ä¸åŒä»»åŠ¡é—´çš„æŒ‡ä»¤å†²çªã€‚åœ¨æ‰‹æœ¯è§†é¢‘è¯´æ˜ (Captioning)ã€é€šç”¨è§†è§‰é—®ç­” (General VQA) å’Œæ—¶é—´è§†è§‰é—®ç­” (Temporal VQA) ç­‰å¤šé¡¹ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿› (State-of-the-art) æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº† SurgLLM åœ¨æå‡æ‰‹æœ¯è§†é¢‘çš„ç©ºé—´èšç„¦å’Œæ—¶é—´æ„è¯†æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ (Computer-Assisted Surgery) ç³»ç»Ÿæä¾›äº†æ›´å…¨é¢çš„ç†è§£èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00357v1",
      "published_date": "2025-08-30 04:36:41 UTC",
      "updated_date": "2025-08-30 04:36:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:29:46.292730+00:00"
    },
    {
      "arxiv_id": "2509.00353v1",
      "title": "AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data",
      "title_zh": "AQFusionNetï¼šèåˆå½±åƒä¸ä¼ æ„Ÿå™¨æ•°æ®çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ ç©ºæ°”è´¨é‡æŒ‡æ•°é¢„æµ‹",
      "authors": [
        "Koushik Ahmed Kushal",
        "Abdullah Al Mamun"
      ],
      "abstract": "Air pollution monitoring in resource-constrained regions remains challenging due to sparse sensor deployment and limited infrastructure. This work introduces AQFusionNet, a multimodal deep learning framework for robust Air Quality Index (AQI) prediction. The framework integrates ground-level atmospheric imagery with pollutant concentration data using lightweight CNN backbones (MobileNetV2, ResNet18, EfficientNet-B0). Visual and sensor features are combined through semantically aligned embedding spaces, enabling accurate and efficient prediction. Experiments on more than 8,000 samples from India and Nepal demonstrate that AQFusionNet consistently outperforms unimodal baselines, achieving up to 92.02% classification accuracy and an RMSE of 7.70 with the EfficientNet-B0 backbone. The model delivers an 18.5% improvement over single-modality approaches while maintaining low computational overhead, making it suitable for deployment on edge devices. AQFusionNet provides a scalable and practical solution for AQI monitoring in infrastructure-limited environments, offering robust predictive capability even under partial sensor availability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AQFusionNetï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹èµ„æºå—é™åœ°åŒºç©ºæ°”è´¨é‡ç›‘æµ‹çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ æ„Ÿå™¨éƒ¨ç½²ç¨€ç–å’ŒåŸºç¡€è®¾æ–½æœ‰é™çš„é—®é¢˜ã€‚AQFusionNet å°†åœ°é¢å¤§æ°”å›¾åƒ(atmospheric imagery)ä¸æ±¡æŸ“ç‰©æµ“åº¦æ•°æ®ç›¸ç»“åˆï¼Œåˆ©ç”¨è½»é‡çº§CNNéª¨å¹²ç½‘ç»œï¼ˆå¦‚MobileNetV2ã€ResNet18ã€EfficientNet-B0ï¼‰è¿›è¡Œç‰¹å¾æå–ã€‚è¯¥æ¡†æ¶é€šè¿‡è¯­ä¹‰å¯¹é½çš„åµŒå…¥ç©ºé—´(semantically aligned embedding spaces)èåˆè§†è§‰å’Œä¼ æ„Ÿå™¨ç‰¹å¾ï¼Œå®ç°äº†é«˜ç²¾åº¦ä¸”é«˜æ•ˆçš„ç©ºæ°”è´¨é‡æŒ‡æ•°(AQI)é¢„æµ‹ã€‚åœ¨å°åº¦å’Œå°¼æ³Šå°”è¶…è¿‡8,000ä¸ªæ ·æœ¬ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAQFusionNet æ˜¾è‘—ä¼˜äºå•æ¨¡æ€åŸºå‡†æ¨¡å‹ï¼Œå…¶ä¸­é‡‡ç”¨EfficientNet-B0éª¨æ¶æ—¶è¾¾åˆ°äº†92.02%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚ä¸å•æ¨¡æ€æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹æ€§èƒ½æå‡äº†18.5%ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½çš„è®¡ç®—å¼€é”€ï¼Œéå¸¸é€‚åˆéƒ¨ç½²åœ¨è¾¹ç¼˜è®¾å¤‡(edge devices)ä¸Šã€‚è¯¥æ–¹æ¡ˆåœ¨ä¼ æ„Ÿå™¨éƒ¨åˆ†ç¼ºå¤±çš„æƒ…å†µä¸‹ä»è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œä¸ºåŸºç¡€è®¾æ–½è–„å¼±ç¯å¢ƒä¸‹çš„AQIç›‘æµ‹æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.00353v1",
      "published_date": "2025-08-30 04:32:38 UTC",
      "updated_date": "2025-08-30 04:32:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:30:36.790644+00:00"
    },
    {
      "arxiv_id": "2509.00351v1",
      "title": "Target-Oriented Single Domain Generalization",
      "title_zh": "é¢å‘ç›®æ ‡çš„å•åŸŸæ³›åŒ–",
      "authors": [
        "Marzi Heidari",
        "Yuhong Guo"
      ],
      "abstract": "Deep models trained on a single source domain often fail catastrophically under distribution shifts, a critical challenge in Single Domain Generalization (SDG). While existing methods focus on augmenting source data or learning invariant features, they neglect a readily available resource: textual descriptions of the target deployment environment. We propose Target-Oriented Single Domain Generalization (TO-SDG), a novel problem setup that leverages the textual description of the target domain, without requiring any target data, to guide model generalization. To address TO-SDG, we introduce Spectral TARget Alignment (STAR), a lightweight module that injects target semantics into source features by exploiting visual-language models (VLMs) such as CLIP. STAR uses a target-anchored subspace derived from the text embedding of the target description to recenter image features toward the deployment domain, then utilizes spectral projection to retain directions aligned with target cues while discarding source-specific noise. Moreover, we use a vision-language distillation to align backbone features with VLM's semantic geometry. STAR further employs feature-space Mixup to ensure smooth transitions between source and target-oriented representations. Experiments across various image classification and object detection benchmarks demonstrate STAR's superiority. This work establishes that minimal textual metadata, which is a practical and often overlooked resource, significantly enhances generalization under severe data constraints, opening new avenues for deploying robust models in target environments with unseen data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é¢å‘ç›®æ ‡çš„å•åŸŸæ³›åŒ– (Target-Oriented Single Domain Generalization, TO-SDG) è¿™ä¸€æ–°é¢–çš„é—®é¢˜è®¾å®šï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å•æºåŸŸè®­ç»ƒæ—¶é¢ä¸´çš„åˆ†å¸ƒåç§»æŒ‘æˆ˜ã€‚ä¸ä»¥å¾€ä¾§é‡äºæ•°æ®å¢å¼ºæˆ–å­¦ä¹ ä¸å˜ç‰¹å¾çš„æ–¹æ³•ä¸åŒï¼ŒTO-SDG åˆ©ç”¨ç›®æ ‡éƒ¨ç½²ç¯å¢ƒçš„æ–‡æœ¬æè¿°æ¥æŒ‡å¯¼æ¨¡å‹æ³›åŒ–ï¼Œè€Œæ— éœ€ä»»ä½•å®é™…çš„ç›®æ ‡åŸŸæ•°æ®ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶è€…å¼•å…¥äº†åä¸º Spectral TARget Alignment (STAR) çš„è½»é‡çº§æ¨¡å—ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) å¦‚ CLIP å°†ç›®æ ‡è¯­ä¹‰æ³¨å…¥æºç‰¹å¾ã€‚STAR åˆ©ç”¨ä»æ–‡æœ¬åµŒå…¥è¡ç”Ÿçš„ç›®æ ‡é”šå®šå­ç©ºé—´å¯¹å›¾åƒç‰¹å¾è¿›è¡Œé‡ä¸­å¿ƒåŒ–ï¼Œå¹¶åˆ©ç”¨è°±æŠ•å½± (spectral projection) ä¿ç•™ç›®æ ‡çº¿ç´¢æ–¹å‘å¹¶æ»¤é™¤æºåŸŸå™ªå£°ã€‚è¯¥æ–¹æ³•è¿˜ç»“åˆè§†è§‰è¯­è¨€è’¸é¦ (vision-language distillation) å’Œç‰¹å¾ç©ºé—´ Mixup è¿›ä¸€æ­¥ç¡®ä¿æºåŸŸä¸ç›®æ ‡å¯¼å‘è¡¨ç¤ºä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ã€‚åœ¨å¤šé¡¹å›¾åƒåˆ†ç±»å’Œç›®æ ‡æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¯æ˜äº† STAR çš„ä¼˜è¶Šæ€§ï¼Œè¯å®äº†æ–‡æœ¬å…ƒæ•°æ®ä½œä¸ºä¸€ç§å¸¸è¢«å¿½è§†çš„å®ç”¨èµ„æºï¼Œèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºæ¨¡å‹åœ¨ä¸¥è‹›æ•°æ®çº¦æŸä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00351v1",
      "published_date": "2025-08-30 04:21:48 UTC",
      "updated_date": "2025-08-30 04:21:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:30:06.387492+00:00"
    },
    {
      "arxiv_id": "2509.00348v1",
      "title": "Theory Foundation of Physics-Enhanced Residual Learning",
      "title_zh": "ç‰©ç†å¢å¼ºæ®‹å·®å­¦ä¹ çš„ç†è®ºåŸºç¡€",
      "authors": [
        "Shixiao Liang",
        "Wang Chen",
        "Keke Long",
        "Peng Zhang",
        "Xiaopeng Li",
        "Jintao Ke"
      ],
      "abstract": "Intensive studies have been conducted in recent years to integrate neural networks with physics models to balance model accuracy and interpretability. One recently proposed approach, named Physics-Enhanced Residual Learning (PERL), is to use learning to estimate the residual between the physics model prediction and the ground truth. Numeral examples suggested that integrating such residual with physics models in PERL has three advantages: (1) a reduction in the number of required neural network parameters; (2) faster convergence rates; and (3) fewer training samples needed for the same computational precision. However, these numerical results lack theoretical justification and cannot be adequately explained.\n  This paper aims to explain these advantages of PERL from a theoretical perspective. We investigate a general class of problems with Lipschitz continuity properties. By examining the relationships between the bounds to the loss function and residual learning structure, this study rigorously proves a set of theorems explaining the three advantages of PERL.\n  Several numerical examples in the context of automated vehicle trajectory prediction are conducted to illustrate the proposed theorems. The results confirm that, even with significantly fewer training samples, PERL consistently achieves higher accuracy than a pure neural network. These results demonstrate the practical value of PERL in real world autonomous driving applications where corner case data are costly or hard to obtain. PERL therefore improves predictive performance while reducing the amount of data required.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Physics-Enhanced Residual Learning (PERL) çš„ç†è®ºåŸºç¡€ï¼Œæ—¨åœ¨ä»ç†è®ºå±‚é¢è§£é‡Šè¯¥æ–¹æ³•åœ¨ç‰©ç†æ¨¡å‹ä¸ç¥ç»ç½‘ç»œç»“åˆä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚PERL æ¡†æ¶é€šè¿‡å­¦ä¹ ç‰©ç†æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„æ®‹å·® (residual)ï¼Œæœ‰æ•ˆå¹³è¡¡äº†æ¨¡å‹çš„å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§ã€‚æœ¬æ–‡é’ˆå¯¹å…·æœ‰ Lipschitz continuity å±æ€§çš„ä¸€ç±»é€šç”¨é—®é¢˜ï¼Œé€šè¿‡åˆ†ææŸå¤±å‡½æ•°ç•Œé™ä¸æ®‹å·®å­¦ä¹ ç»“æ„ä¹‹é—´çš„å…³ç³»ï¼Œä¸¥è°¨åœ°è¯æ˜äº† PERL åœ¨å‡å°‘å‚æ•°éœ€æ±‚ã€åŠ å¿«æ”¶æ•›é€Ÿåº¦ (convergence rates) ä»¥åŠé™ä½æ ·æœ¬éœ€æ±‚æ–¹é¢çš„ç†è®ºä¼˜è¶Šæ€§ã€‚åœ¨è‡ªåŠ¨é©¾é©¶è½¦è¾†è½¨è¿¹é¢„æµ‹ (automated vehicle trajectory prediction) çš„æ•°å€¼å®éªŒä¸­ï¼ŒPERL åœ¨è®­ç»ƒæ ·æœ¬æ˜¾è‘—å‡å°‘çš„æƒ…å†µä¸‹ï¼Œå…¶ç²¾åº¦ä¾ç„¶æŒç»­ä¼˜äºçº¯ç¥ç»ç½‘ç»œã€‚ç ”ç©¶ç»“æœè¯æ˜äº† PERL åœ¨å¤„ç†è‡ªåŠ¨é©¾é©¶ä¸­è·å–æˆæœ¬é«˜æ˜‚çš„æç«¯å·¥å†µ (corner case) æ•°æ®æ—¶å…·æœ‰æé«˜çš„å®ç”¨ä»·å€¼ï¼Œèƒ½å¤Ÿåœ¨æå‡é¢„æµ‹æ€§èƒ½çš„åŒæ—¶å¤§å¹…å‡å°‘å¯¹æ•°æ®çš„ä¾èµ–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.00348v1",
      "published_date": "2025-08-30 04:08:19 UTC",
      "updated_date": "2025-08-30 04:08:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:29:50.986078+00:00"
    },
    {
      "arxiv_id": "2509.00347v1",
      "title": "LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning",
      "title_zh": "LLM é©±åŠ¨çš„ç­–ç•¥æ‰©æ•£ï¼šæå‡ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Hanping Zhang",
        "Yuhong Guo"
      ],
      "abstract": "Reinforcement Learning (RL) is known for its strong decision-making capabilities and has been widely applied in various real-world scenarios. However, with the increasing availability of offline datasets and the lack of well-designed online environments from human experts, the challenge of generalization in offline RL has become more prominent. Due to the limitations of offline data, RL agents trained solely on collected experiences often struggle to generalize to new tasks or environments. To address this challenge, we propose LLM-Driven Policy Diffusion (LLMDPD), a novel approach that enhances generalization in offline RL using task-specific prompts. Our method incorporates both text-based task descriptions and trajectory prompts to guide policy learning. We leverage a large language model (LLM) to process text-based prompts, utilizing its natural language understanding and extensive knowledge base to provide rich task-relevant context. Simultaneously, we encode trajectory prompts using a transformer model, capturing structured behavioral patterns within the underlying transition dynamics. These prompts serve as conditional inputs to a context-aware policy-level diffusion model, enabling the RL agent to generalize effectively to unseen tasks. Our experimental results demonstrate that LLMDPD outperforms state-of-the-art offline RL methods on unseen tasks, highlighting its effectiveness in improving generalization and adaptability in diverse settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline Reinforcement Learning) åœ¨æœ‰é™æ•°æ®ä¸‹éš¾ä»¥æ³›åŒ–åˆ°æ–°ä»»åŠ¡æˆ–ç¯å¢ƒçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º LLM-Driven Policy Diffusion (LLMDPD) çš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ–‡æœ¬ä»»åŠ¡æè¿°å’Œè½¨è¿¹æç¤º (Trajectory Prompts) æ¥å¼•å¯¼ç­–ç•¥å­¦ä¹ ï¼Œé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM) å¤„ç†æ–‡æœ¬æç¤ºï¼Œä»è€Œä¸ºæ¨¡å‹æä¾›ä¸°å¯Œçš„ä»»åŠ¡ç›¸å…³ä¸Šä¸‹æ–‡ã€‚åŒæ—¶ï¼Œç ”ç©¶é‡‡ç”¨ Transformer æ¨¡å‹å¯¹è½¨è¿¹æç¤ºè¿›è¡Œç¼–ç ï¼Œä»¥æ•æ‰åº•å±‚è½¬ç§»æ¼”åŒ–ä¸­çš„ç»“æ„åŒ–è¡Œä¸ºæ¨¡å¼ã€‚è¿™äº›æç¤ºä½œä¸ºæ¡ä»¶è¾“å…¥è¢«é¦ˆé€åˆ°ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç­–ç•¥çº§æ‰©æ•£æ¨¡å‹ (Context-aware Policy-level Diffusion Model)ï¼Œæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ä½“åœ¨å¤„ç†æœªè§ä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMDPD åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„å…ˆè¿›ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨å¤šæ ·åŒ–è®¾å®šä¸‹æå‡æ³›åŒ–æ€§å’Œé€‚åº”æ€§çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00347v1",
      "published_date": "2025-08-30 04:02:33 UTC",
      "updated_date": "2025-08-30 04:02:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:29:53.090503+00:00"
    },
    {
      "arxiv_id": "2509.00338v2",
      "title": "Scalable Option Learning in High-Throughput Environments",
      "title_zh": "é«˜ååç¯å¢ƒä¸‹çš„å¯æ‰©å±•é€‰é¡¹å­¦ä¹ ",
      "authors": [
        "Mikael Henaff",
        "Scott Fujimoto",
        "Michael Matthews",
        "Michael Rabbat"
      ],
      "abstract": "Hierarchical reinforcement learning (RL) has the potential to enable effective decision-making over long timescales. Existing approaches, while promising, have yet to realize the benefits of large-scale training. In this work, we identify and solve several key challenges in scaling online hierarchical RL to high-throughput environments. We propose Scalable Option Learning (SOL), a highly scalable hierarchical RL algorithm which achieves a ~35x higher throughput compared to existing hierarchical methods. To demonstrate SOL's performance and scalability, we train hierarchical agents using 30 billion frames of experience on the complex game of NetHack, significantly surpassing flat agents and demonstrating positive scaling trends. We also validate SOL on MiniHack and Mujoco environments, showcasing its general applicability. Our code is open sourced at: github.com/facebookresearch/sol.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ†å±‚å¼ºåŒ–å­¦ä¹ (Hierarchical Reinforcement Learning)åœ¨å¤§è§„æ¨¡è®­ç»ƒå’Œé«˜ååé‡ç¯å¢ƒä¸‹é¢ä¸´çš„æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†Scalable Option Learning (SOL)ç®—æ³•ã€‚SOLé€šè¿‡è§£å†³åœ¨çº¿åˆ†å±‚å¼ºåŒ–å­¦ä¹ åœ¨æ‰©å±•è¿‡ç¨‹ä¸­çš„å…³é”®æŠ€æœ¯éš¾é¢˜ï¼Œå®ç°äº†æ¯”ç°æœ‰åˆ†å±‚æ–¹æ³•é«˜å‡ºçº¦35å€çš„ååæ•ˆç‡ã€‚ç ”ç©¶äººå‘˜åœ¨å¤æ‚çš„NetHackæ¸¸æˆç¯å¢ƒä¸­ä½¿ç”¨300äº¿å¸§ç»éªŒå¯¹æ™ºèƒ½ä½“è¿›è¡Œè®­ç»ƒï¼Œç»“æœè¡¨æ˜SOLçš„è¡¨ç°æ˜¾è‘—ä¼˜äºéåˆ†å±‚(flat)æ™ºèƒ½ä½“ï¼Œå¹¶å±•ç°å‡ºç§¯æçš„æ‰©å±•è¶‹åŠ¿(scaling trends)ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•åœ¨MiniHackå’ŒMujocoç¯å¢ƒä¸­çš„éªŒè¯è¿›ä¸€æ­¥è¯æ˜äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨é«˜ååé‡ç¯å¢ƒä¸‹å®ç°é«˜æ•ˆã€é•¿æ—¶ç¨‹çš„å†³ç­–åˆ¶å®šæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å·²å¼€æºå…¶ä»£ç å®ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00338v2",
      "published_date": "2025-08-30 03:42:10 UTC",
      "updated_date": "2025-09-26 02:53:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:30:05.497794+00:00"
    },
    {
      "arxiv_id": "2509.04475v1",
      "title": "ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute",
      "title_zh": "ParaThinkerï¼šåŸç”Ÿå¹¶è¡Œæ€ç»´â€”â€”æ‰©å±•å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ—¶è®¡ç®—çš„æ–°èŒƒå¼",
      "authors": [
        "Hao Wen",
        "Yifan Su",
        "Feifei Zhang",
        "Yunxin Liu",
        "Yunhao Liu",
        "Ya-Qin Zhang",
        "Yuanchun Li"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have been driven by test-time compute scaling - a strategy that improves reasoning by generating longer, sequential thought processes. While effective, this approach encounters a significant bottleneck as computation increases, where further computation offers only marginal performance gains. We argue this ceiling is not an inherent limit of the model's capability but a flaw in the scaling strategy itself, a phenomenon we term \"Tunnel Vision\", where a model's imperfect initial steps lock it into a suboptimal reasoning path. To overcome this, we introduce a new scaling paradigm: native thought parallelism. We present ParaThinker, an end-to-end framework that trains an LLM to generate multiple, diverse reasoning paths in parallel and synthesize them into a superior final answer. By exploring different lines of thoughts simultaneously, ParaThinker effectively sidesteps the Tunnel Vision issue and unlocks the model's latent reasoning potential. Our approach demonstrates that scaling compute in parallel (width) is a more effective and efficient way to superior reasoning than simply scaling sequentially (depth). On challenging reasoning benchmarks, ParaThinker achieves substantial accuracy improvements over sequential LLMs (12.3% for 1.5B and 7.5% for 7B models on average with 8 parallel paths), while adding only negligible latency overhead (7.1%). This enables smaller models to surpass much larger counterparts and establishes parallel thinking as a critical, efficient dimension for scaling future LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æµ‹è¯•æ—¶è®¡ç®—é‡(test-time compute)æ‰©å±•è¿‡ç¨‹ä¸­é¢ä¸´çš„â€œéš§é“è§†é‡(Tunnel Vision)â€ç“¶é¢ˆï¼Œæå‡ºäº†åä¸ºParaThinkerçš„æ–°å‹æ‰©å±•èŒƒå¼ã€‚ParaThinkeræ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯æ¡†æ¶ï¼Œé€šè¿‡åŸç”Ÿæ€è€ƒå¹¶è¡Œ(native thought parallelism)è®­ç»ƒæ¨¡å‹åŒæ—¶ç”Ÿæˆå¤šæ¡å¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ï¼Œå¹¶å°†å…¶åˆæˆä¸ºæœ€ä¼˜æœ€ç»ˆç­”æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡å¢åŠ æ¨ç†çš„â€œå®½åº¦â€è€Œéå•çº¯å¢åŠ â€œæ·±åº¦â€ï¼Œæœ‰æ•ˆé¿å¼€äº†æ¨¡å‹å› åˆå§‹æ­¥éª¤å¤±è¯¯è€Œé™·å…¥æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„é—®é¢˜ï¼Œå……åˆ†é‡Šæ”¾äº†æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼ŒParaThinkeråœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨ä»…å¢åŠ 7.1%å»¶è¿Ÿçš„æƒ…å†µä¸‹ï¼Œä½¿1.5Bå’Œ7Bæ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡åˆ†åˆ«æå‡äº†12.3%å’Œ7.5%ã€‚è¿™é¡¹å·¥ä½œç¡®ç«‹äº†å¹¶è¡Œæ€è€ƒä½œä¸ºæœªæ¥LLMé«˜æ•ˆæ‰©å±•çš„å…³é”®ç»´åº¦ï¼Œå¹¶è¯æ˜äº†è¾ƒå°è§„æ¨¡æ¨¡å‹é€šè¿‡è¯¥èŒƒå¼èƒ½å¤Ÿè¶…è¶Šæ›´å¤§è§„æ¨¡çš„ç«äº‰å¯¹æ‰‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.04475v1",
      "published_date": "2025-08-30 03:09:07 UTC",
      "updated_date": "2025-08-30 03:09:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:30:19.093937+00:00"
    },
    {
      "arxiv_id": "2509.00329v1",
      "title": "Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic Endoluminal Navigation of Deformable Continuum Robots",
      "title_zh": "é¢å‘å¯å˜å½¢è¿ç»­ä½“æœºå™¨äººåŠ¨æ€è…”å†…å¯¼èˆªçš„é›…å¯æ¯”æ¢ç´¢åŒé˜¶æ®µå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Yu Tian",
        "Chi Kit Ng",
        "Hongliang Ren"
      ],
      "abstract": "Deformable continuum robots (DCRs) present unique planning challenges due to nonlinear deformation mechanics and partial state observability, violating the Markov assumptions of conventional reinforcement learning (RL) methods. While Jacobian-based approaches offer theoretical foundations for rigid manipulators, their direct application to DCRs remains limited by time-varying kinematics and underactuated deformation dynamics. This paper proposes Jacobian Exploratory Dual-Phase RL (JEDP-RL), a framework that decomposes planning into phased Jacobian estimation and policy execution. During each training step, we first perform small-scale local exploratory actions to estimate the deformation Jacobian matrix, then augment the state representation with Jacobian features to restore approximate Markovianity. Extensive SOFA surgical dynamic simulations demonstrate JEDP-RL's three key advantages over proximal policy optimization (PPO) baselines: 1) Convergence speed: 3.2x faster policy convergence, 2) Navigation efficiency: requires 25% fewer steps to reach the target, and 3) Generalization ability: achieve 92% success rate under material property variations and achieve 83% (33% higher than PPO) success rate in the unseen tissue environment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯å˜å½¢è¿ç»­ä½“æœºå™¨äºº (Deformable continuum robots, DCRs) å› éçº¿æ€§å˜å½¢å’Œéƒ¨åˆ†çŠ¶æ€å¯è§‚æµ‹æ€§è€Œè¿åé©¬å°”å¯å¤«å‡è®¾ (Markov assumptions) çš„éš¾é¢˜ï¼Œæå‡ºäº†é›…å¯æ¯”æ¢ç´¢åŒé˜¶æ®µå¼ºåŒ–å­¦ä¹  (Jacobian Exploratory Dual-Phase RL, JEDP-RL) æ¡†æ¶ã€‚è¯¥æ–¹æ¡ˆå°†è§„åˆ’è¿‡ç¨‹åˆ†è§£ä¸ºé›…å¯æ¯”ä¼°è®¡å’Œç­–ç•¥æ‰§è¡Œä¸¤ä¸ªé˜¶æ®µï¼Œé€šè¿‡å±€éƒ¨æ¢ç´¢åŠ¨ä½œå®æ—¶ä¼°è®¡å˜å½¢é›…å¯æ¯”çŸ©é˜µ (Deformation Jacobian matrix)ã€‚é€šè¿‡å°†é›…å¯æ¯”ç‰¹å¾å¼•å…¥çŠ¶æ€è¡¨ç¤ºï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæ¢å¤äº†ç³»ç»Ÿçš„è¿‘ä¼¼é©¬å°”å¯å¤«æ€§ï¼Œè§£å†³äº†æ—¶å˜è¿åŠ¨å­¦ä¸‹çš„æ§åˆ¶éš¾é¢˜ã€‚åœ¨ SOFA æ‰‹æœ¯åŠ¨æ€ä»¿çœŸä¸­ï¼ŒJEDP-RL çš„æ”¶æ•›é€Ÿåº¦æ¯” PPO åŸºçº¿å¿« 3.2 å€ï¼Œä¸”å¯¼èˆªæ‰€éœ€çš„æ­¥æ•°å‡å°‘äº† 25%ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ææ–™å±æ€§å˜åŒ–æ—¶ä»èƒ½ä¿æŒ 92% çš„æˆåŠŸç‡ã€‚åœ¨æœªè§è¿‡çš„ç»„ç»‡ç¯å¢ƒä¸­ï¼Œå…¶æˆåŠŸç‡è¾¾åˆ° 83%ï¼Œæ¯” PPO åŸºçº¿é«˜å‡º 33%ï¼Œå±•ç°äº†æå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00329v1",
      "published_date": "2025-08-30 03:04:35 UTC",
      "updated_date": "2025-08-30 03:04:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:30:20.092133+00:00"
    },
    {
      "arxiv_id": "2509.00319v1",
      "title": "Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach",
      "title_zh": "åŠ¨æ€èƒƒéƒ¨ç¯å¢ƒä¸‹åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æŸ”æ€§æœºå™¨äººå†…çª¥é•œæ¥è§¦è¾…åŠ©å¯¼èˆª",
      "authors": [
        "Chi Kit Ng",
        "Huxin Gao",
        "Tian-Ao Ren",
        "Jiewen Lai",
        "Hongliang Ren"
      ],
      "abstract": "Navigating a flexible robotic endoscope (FRE) through the gastrointestinal tract is critical for surgical diagnosis and treatment. However, navigation in the dynamic stomach is particularly challenging because the FRE must learn to effectively use contact with the deformable stomach walls to reach target locations. To address this, we introduce a deep reinforcement learning (DRL) based Contact-Aided Navigation (CAN) strategy for FREs, leveraging contact force feedback to enhance motion stability and navigation precision. The training environment is established using a physics-based finite element method (FEM) simulation of a deformable stomach. Trained with the Proximal Policy Optimization (PPO) algorithm, our approach achieves high navigation success rates (within 3 mm error between the FRE's end-effector and target) and significantly outperforms baseline policies. In both static and dynamic stomach environments, the CAN agent achieved a 100% success rate with 1.6 mm average error, and it maintained an 85% success rate in challenging unseen scenarios with stronger external disturbances. These results validate that the DRL-based CAN strategy substantially enhances FRE navigation performance over prior methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŸ”æ€§æœºå™¨äººå†…çª¥é•œï¼ˆFlexible Robotic Endoscope, FREï¼‰åœ¨åŠ¨æ€èƒƒéƒ¨ç¯å¢ƒä¸­çš„å¯¼èˆªæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDeep Reinforcement Learning, DRLï¼‰çš„æ¥è§¦è¾…åŠ©å¯¼èˆªï¼ˆContact-Aided Navigation, CANï¼‰ç­–ç•¥ã€‚è¯¥ç­–ç•¥åˆ©ç”¨æ¥è§¦åŠ›åé¦ˆæ¥å¢å¼ºè¿åŠ¨ç¨³å®šæ€§å’Œå¯¼èˆªç²¾åº¦ï¼Œå¹¶åˆ©ç”¨åŸºäºæœ‰é™å…ƒæ–¹æ³•ï¼ˆFinite Element Method, FEMï¼‰æ„å»ºçš„ç‰©ç†å˜å½¢èƒƒéƒ¨æ¨¡æ‹Ÿç¯å¢ƒè¿›è¡Œè®­ç»ƒã€‚ç ”ç©¶é‡‡ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆProximal Policy Optimization, PPOï¼‰ç®—æ³•ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå­¦ä¹ æœ‰æ•ˆåˆ©ç”¨ä¸èƒƒå£çš„æ¥è§¦æ¥æŠµè¾¾ç›®æ ‡ä½ç½®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ™ºèƒ½ä½“åœ¨é™æ€å’ŒåŠ¨æ€èƒƒéƒ¨åœºæ™¯ä¸­å‡å®ç°äº†100%çš„å¯¼èˆªæˆåŠŸç‡ï¼Œå¹³å‡è¯¯å·®ä»…ä¸º1.6 mmã€‚åœ¨é¢å¯¹ä¼´æœ‰å¼ºå¤–éƒ¨å¹²æ‰°çš„æœªçŸ¥æŒ‘æˆ˜åœºæ™¯æ—¶ï¼Œè¯¥ç­–ç•¥ä»èƒ½ä¿æŒ85%çš„æˆåŠŸç‡ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œä¸ºå¤æ‚è§£å‰–ç¯å¢ƒä¸‹çš„è‡ªä¸»å†…çª¥é•œå¯¼èˆªå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00319v1",
      "published_date": "2025-08-30 02:42:06 UTC",
      "updated_date": "2025-08-30 02:42:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:30:30.697296+00:00"
    },
    {
      "arxiv_id": "2509.00317v1",
      "title": "A Framework for Task and Motion Planning based on Expanding AND/OR Graphs",
      "title_zh": "åŸºäºæ‰©å±•ä¸/æˆ–å›¾çš„ä»»åŠ¡ä¸è¿åŠ¨è§„åˆ’æ¡†æ¶",
      "authors": [
        "Fulvio Mastrogiovanni",
        "Antony Thomas"
      ],
      "abstract": "Robot autonomy in space environments presents unique challenges, including high perception and motion uncertainty, strict kinematic constraints, and limited opportunities for human intervention. Therefore, Task and Motion Planning (TMP) may be critical for autonomous servicing, surface operations, or even in-orbit missions, just to name a few, as it models tasks as discrete action sequencing integrated with continuous motion feasibility assessments. In this paper, we introduce a TMP framework based on expanding AND/OR graphs, referred to as TMP-EAOG, and demonstrate its adaptability to different scenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph, which expands iteratively as the plan is executed, and performs in-the-loop motion planning assessments to ascertain their feasibility. As a consequence, TMP-EAOG is characterised by the desirable properties of (i) robustness to a certain degree of uncertainty, because AND/OR graph expansion can accommodate for unpredictable information about the robot environment, (ii) controlled autonomy, since an AND/OR graph can be validated by human experts, and (iii) bounded flexibility, in that unexpected events, including the assessment of unfeasible motions, can lead to different courses of action as alternative paths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We use a simulated mobile manipulator as a proxy for space-grade autonomous robots. Our evaluation shows that TMP-EAOG can deal with a wide range of challenges in the benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤ªç©ºç¯å¢ƒä¸‹æœºå™¨äººè‡ªä¸»ä½œä¸šé¢ä¸´çš„é«˜ä¸ç¡®å®šæ€§å’Œä¸¥æ ¼è¿åŠ¨å­¦çº¦æŸï¼Œæå‡ºäº†ä¸€ç§åä¸ºTMP-EAOGçš„åŸºäºæ‰©å±•AND/ORå›¾çš„ä»»åŠ¡ä¸è¿åŠ¨è§„åˆ’(Task and Motion Planning)æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä»»åŠ¡çº§æŠ½è±¡ç¼–ç åœ¨AND/ORå›¾ä¸­ï¼Œé€šè¿‡åœ¨è®¡åˆ’æ‰§è¡Œè¿‡ç¨‹ä¸­è¿­ä»£æ‰©å±•å›¾ç»“æ„ï¼Œå¹¶ç»“åˆç¯å†…è¿åŠ¨è§„åˆ’è¯„ä¼°æ¥ç¡®ä¿è¡ŒåŠ¨çš„å¯è¡Œæ€§ã€‚TMP-EAOGå…·æœ‰å¯¹ç¯å¢ƒä¸ç¡®å®šæ€§çš„å¼ºé²æ£’æ€§ï¼Œæ”¯æŒé€šè¿‡äººå·¥ä¸“å®¶éªŒè¯å®ç°å—æ§çš„è‡ªä¸»æ€§ï¼Œå¹¶èƒ½åœ¨é‡åˆ°ä¸å¯è¡Œè¿åŠ¨ç­‰æ„å¤–äº‹ä»¶æ—¶é€šè¿‡å›¾ä¸­çš„æ›¿ä»£è·¯å¾„æä¾›çµæ´»çš„åº”å¯¹æ–¹æ¡ˆã€‚åœ¨ä¸¤ä¸ªåŸºå‡†é¢†åŸŸ(benchmark domains)åŠæ¨¡æ‹Ÿç§»åŠ¨æœºæ¢°è‡‚ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤ªç©ºè‡ªä¸»ä½œä¸šä¸­çš„å¤šç§å¤æ‚æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for an oral presentation at ASTRA Conference, 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.00317v1",
      "published_date": "2025-08-30 02:28:25 UTC",
      "updated_date": "2025-08-30 02:28:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:30:33.291067+00:00"
    },
    {
      "arxiv_id": "2509.00316v1",
      "title": "Continuously Tempered Diffusion Samplers",
      "title_zh": "è¿ç»­å›ç«æ‰©æ•£é‡‡æ ·å™¨",
      "authors": [
        "Ezra Erives",
        "Bowen Jing",
        "Peter Holderrieth",
        "Tommi Jaakkola"
      ],
      "abstract": "Annealing-based neural samplers seek to amortize sampling from unnormalized distributions by training neural networks to transport a family of densities interpolating from source to target. A crucial design choice in the training phase of such samplers is the proposal distribution by which locations are generated at which to evaluate the loss. Previous work has obtained such a proposal distribution by combining a partially learned transport with annealed Langevin dynamics. However, isolated modes and other pathological properties of the annealing path imply that such proposals achieve insufficient exploration and thereby lower performance post training. To remedy this, we propose continuously tempered diffusion samplers, which leverage exploration techniques developed in the context of molecular dynamics to improve proposal distributions. Specifically, a family of distributions across different temperatures is introduced to lower energy barriers at higher temperatures and drive exploration at the lower temperature of interest. We empirically validate improved sampler performance driven by extended exploration. Code is available at https://github.com/eje24/ctds.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºé€€ç«çš„ç¥ç»é‡‡æ ·å™¨(Annealing-based neural samplers)åœ¨å¤„ç†æœªå½’ä¸€åŒ–åˆ†å¸ƒæ—¶ï¼Œç”±äºé€€ç«è·¯å¾„ä¸­çš„å­¤ç«‹æ¨¡æ€(isolated modes)ç­‰ç—…æ€å±æ€§å¯¼è‡´å»ºè®®åˆ†å¸ƒ(proposal distribution)æ¢ç´¢ä¸è¶³çš„é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚ä½œè€…æå‡ºäº†æŒç»­å›ç«æ‰©æ•£é‡‡æ ·å™¨(Continuously Tempered Diffusion Samplers, CTDS)ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†å€Ÿé‰´è‡ªåˆ†å­åŠ¨åŠ›å­¦(molecular dynamics)çš„æ¢ç´¢æŠ€æœ¯ä»¥æ”¹è¿›å»ºè®®åˆ†å¸ƒã€‚é€šè¿‡æ„å»ºä¸€ç³»åˆ—è·¨è¶Šä¸åŒæ¸©åº¦çš„åˆ†å¸ƒæ—ï¼ŒCTDSèƒ½å¤Ÿåˆ©ç”¨é«˜æ¸©é™ä½èƒ½é‡å±éšœ(energy barriers)ï¼Œä»è€Œåœ¨æ„Ÿå…´è¶£çš„ä½æ¸©åŒºåŸŸé©±åŠ¨æ›´å……åˆ†çš„æ¢ç´¢ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¿™ç§å¢å¼ºçš„æ¢ç´¢æœºåˆ¶æ˜¾è‘—æå‡äº†é‡‡æ ·å™¨çš„æ€§èƒ½ï¼Œä¸ºè§£å†³å¤æ‚åˆ†å¸ƒçš„é‡‡æ ·é—®é¢˜æä¾›äº†æ›´æœ‰æ•ˆçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00316v1",
      "published_date": "2025-08-30 02:24:52 UTC",
      "updated_date": "2025-08-30 02:24:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:30:55.787937+00:00"
    },
    {
      "arxiv_id": "2510.15883v1",
      "title": "FinFlowRL: An Imitation-Reinforcement Learning Framework for Adaptive Stochastic Control in Finance",
      "title_zh": "FinFlowRLï¼šä¸€ç§ç”¨äºé‡‘èè‡ªé€‚åº”éšæœºæ§åˆ¶çš„æ¨¡ä»¿-å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Yang Li",
        "Zhi Chen"
      ],
      "abstract": "Traditional stochastic control methods in finance struggle in real world markets due to their reliance on simplifying assumptions and stylized frameworks. Such methods typically perform well in specific, well defined environments but yield suboptimal results in changed, non stationary ones. We introduce FinFlowRL, a novel framework for financial optimal stochastic control. The framework pretrains an adaptive meta policy learning from multiple expert strategies, then finetunes through reinforcement learning in the noise space to optimize the generative process. By employing action chunking generating action sequences rather than single decisions, it addresses the non Markovian nature of markets. FinFlowRL consistently outperforms individually optimized experts across diverse market conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FinFlowRLï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé‡‘èé¢†åŸŸè‡ªé€‚åº”éšæœºæ§åˆ¶(Stochastic control)çš„æ¨¡ä»¿-å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•å› è¿‡åº¦ç®€åŒ–çš„å‡è®¾è€Œåœ¨çœŸå®éå¹³ç¨³å¸‚åœºä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»å¤šç§ä¸“å®¶ç­–ç•¥ä¸­å­¦ä¹ æ¥é¢„è®­ç»ƒä¸€ä¸ªè‡ªé€‚åº”å…ƒç­–ç•¥(Adaptive meta policy)ï¼Œå¹¶éšååœ¨å™ªå£°ç©ºé—´ä¸­åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)è¿›è¡Œå¾®è°ƒï¼Œä»è€Œä¼˜åŒ–ç­–ç•¥çš„ç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ºäº†åº”å¯¹å¸‚åœºçš„éé©¬å°”å¯å¤«(non-Markovian)ç‰¹æ€§ï¼ŒFinFlowRLå¼•å…¥äº†åŠ¨ä½œå—(Action chunking)æŠ€æœ¯ä»¥ç”Ÿæˆè¿ç»­çš„åŠ¨ä½œåºåˆ—è€Œéç¦»æ•£çš„å•ä¸€å†³ç­–ã€‚å®éªŒè¯æ˜ï¼ŒFinFlowRLåœ¨å„ç§å¤æ‚çš„å¸‚åœºç¯å¢ƒä¸‹å§‹ç»ˆä¼˜äºç»è¿‡å•ç‹¬ä¼˜åŒ–çš„ä¸“å®¶ç­–ç•¥ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†é‡‘èæ—¶é—´åºåˆ—ä»»åŠ¡ä¸­å“è¶Šçš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG",
        "q-fin.TR"
      ],
      "primary_category": "q-fin.CP",
      "comment": "21 pages, 5 algorithms, 4 tables, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.15883v1",
      "published_date": "2025-08-30 02:08:19 UTC",
      "updated_date": "2025-08-30 02:08:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:30:58.155073+00:00"
    },
    {
      "arxiv_id": "2509.04474v1",
      "title": "Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling",
      "title_zh": "è§„æ¨¡æ‰©å±•ä¸åŠ é€Ÿï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆæµ‹è¯•æ—¶æ‰©å±•çš„æŠ•æœºè§£ç è¯„æµ‹åŸºå‡†",
      "authors": [
        "Shengyin Sun",
        "Yiming Li",
        "Xing Li",
        "Yingzhao Lian",
        "Weizhe Lin",
        "Hui-Ling Zhen",
        "Zhiyuan Yang",
        "Chen Chen",
        "Xianzhi Yu",
        "Mingxuan Yuan",
        "Chen Ma"
      ],
      "abstract": "Test-time scaling has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs) by allocating additional computational resources during inference. However, this paradigm is inherently inefficient due to the generation of redundant and repetitive reasoning traces, leading to significant computational overhead. Speculative decoding offers a promising avenue for mitigating this inefficiency, yet its efficacy in the structured, repetition-rich context of test-time scaling remains largely unexplored. To bridge this gap, we introduce the first comprehensive benchmark designed to evaluate speculative decoding methods for accelerating LLM test-time scaling. Our benchmark provides consistent experimental protocols across representative test-time scaling paradigms (e.g., Best-of-N sampling and multi-round thinking), enabling a fair comparison of three major categories of speculative decoding: model-based, training-based, and n-gram-based methods. Extensive experiments reveal that simple n-gram-based methods effectively capture repetitive patterns, demonstrating unique potential in accelerating test-time scaling. This phenomenon demonstrates the value of integrating n-gram-based methods with model-based or training-based approaches to balance acceleration for both repetitive and diverse reasoning in test-time scaling. We hope this benchmark spurs further research on speculative decoding for test-time scaling, enabling faster and more practical reasoning in LLMs through better handling of repetitive and diverse reasoning paths.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨Test-Time Scalingè¿‡ç¨‹ä¸­å› äº§ç”Ÿå†—ä½™å’Œé‡å¤æ¨ç†è·¯å¾„è€Œå¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œå¼•å…¥äº†é¦–ä¸ªä¸“é—¨è¯„ä¼°Speculative Decodingæ–¹æ³•åœ¨æ­¤åœºæ™¯ä¸‹åŠ é€Ÿæ•ˆæœçš„å…¨é¢åŸºå‡†ã€‚è¯¥åŸºå‡†æ¶µç›–äº†Best-of-N samplingå’Œmulti-round thinkingç­‰ä¸»æµèŒƒå¼ï¼Œå¹¶å¯¹model-basedã€training-basedå’Œn-gram-basedä¸‰ç±»Speculative DecodingæŠ€æœ¯è¿›è¡Œäº†å…¬å¹³å¯¹æ¯”ã€‚å®éªŒå‘ç°ï¼Œç®€å•çš„n-gram-basedæ–¹æ³•åœ¨æ•æ‰é‡å¤æ¨¡å¼æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç°å‡ºåŠ é€ŸTest-Time Scalingçš„ç‹¬ç‰¹æ½œåŠ›ã€‚ç ”ç©¶å¼ºè°ƒäº†å°†n-gram-basedæ–¹æ³•ä¸model-basedæˆ–training-basedæ–¹æ³•ç›¸ç»“åˆçš„é‡è¦æ€§ï¼Œä»¥å¹³è¡¡é‡å¤æ€§ä¸å¤šæ ·æ€§æ¨ç†è·¯å¾„çš„åŠ é€Ÿæ•ˆæœã€‚è¯¥åŸºå‡†çš„å»ºç«‹ä¸ºæ¨åŠ¨æ›´é«˜æ•ˆã€æ›´å®ç”¨çš„LLMæ¨ç†ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒï¼Œæœ‰åŠ©äºåœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶å®ç°æ›´å¿«çš„å“åº”é€Ÿåº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.04474v1",
      "published_date": "2025-08-30 01:54:55 UTC",
      "updated_date": "2025-08-30 01:54:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:31:00.650330+00:00"
    },
    {
      "arxiv_id": "2509.00310v2",
      "title": "TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization",
      "title_zh": "TReF-6ï¼šåŸºäºå•æ¬¡ç¤ºèŒƒçš„ä»»åŠ¡ç›¸å…³å‚è€ƒç³»æ¨æ–­ä¸å•æ ·æœ¬æŠ€èƒ½æ³›åŒ–",
      "authors": [
        "Yuxuan Ding",
        "Shuangge Wang",
        "Tesca Fitzgerald"
      ],
      "abstract": "Robots often struggle to generalize from a single demonstration due to the lack of a transferable and interpretable spatial representation. In this work, we introduce TReF-6, a method that infers a simplified, abstracted 6DoF Task-Relevant Frame from a single trajectory. Our approach identifies an influence point purely from the trajectory geometry to define the origin for a local frame, which serves as a reference for parameterizing a Dynamic Movement Primitive (DMP). This influence point captures the task's spatial structure, extending the standard DMP formulation beyond start-goal imitation. The inferred frame is semantically grounded via a vision-language model and localized in novel scenes by Grounded-SAM, enabling functionally consistent skill generalization. We validate TReF-6 in simulation and demonstrate robustness to trajectory noise. We further deploy an end-to-end pipeline on real-world manipulation tasks, showing that TReF-6 supports one-shot imitation learning that preserves task intent across diverse object configurations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººä»å•æ¬¡æ¼”ç¤ºä¸­è¿›è¡ŒæŠ€èƒ½æ³›åŒ–æ—¶é¢ä¸´çš„è¡¨å¾éš¾é¢˜ï¼Œæå‡ºäº† TReF-6 æ–¹æ³•ï¼Œæ—¨åœ¨ä»å•æ¡è½¨è¿¹ä¸­æ¨æ–­ç®€åŒ–çš„ 6DoF ä»»åŠ¡ç›¸å…³åæ ‡ç³» (Task-Relevant Frame)ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æè½¨è¿¹å‡ ä½•ç‰¹å¾è¯†åˆ«å…³é”®çš„å½±å“ç‚¹ (influence point) ä½œä¸ºå±€éƒ¨åæ ‡ç³»åŸç‚¹ï¼Œå¹¶ä»¥æ­¤ä¸ºå‚è€ƒæ¥å‚æ•°åŒ–åŠ¨æ€è¿åŠ¨åŸºå…ƒ (Dynamic Movement Primitive, DMP)ï¼Œä»è€Œæ‰©å±•äº†ä¼ ç»Ÿçš„èµ·ç‚¹-ç»ˆç‚¹æ¨¡ä»¿æœºåˆ¶ã€‚åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Model) è¿›è¡Œè¯­ä¹‰å…³è”ï¼Œå¹¶ç»“åˆ Grounded-SAM åœ¨æ–°åœºæ™¯ä¸­è¿›è¡Œå®šä½ï¼Œç¡®ä¿äº†æŠ€èƒ½æ³›åŒ–çš„åŠŸèƒ½ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTReF-6 åœ¨ä»¿çœŸå’ŒçœŸå®æ“ä½œä»»åŠ¡ä¸­å¯¹è½¨è¿¹å™ªå£°å…·æœ‰é²æ£’æ€§ï¼Œèƒ½å¤Ÿæ”¯æŒè·¨å¤šæ ·ç‰©ä½“é…ç½®çš„å•æ ·æœ¬æ¨¡ä»¿å­¦ä¹  (One-Shot Imitation Learning)ï¼Œæœ‰æ•ˆåœ°ä¿ç•™äº†ä»»åŠ¡æ„å›¾ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00310v2",
      "published_date": "2025-08-30 01:54:28 UTC",
      "updated_date": "2025-09-28 17:29:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:31:02.996586+00:00"
    },
    {
      "arxiv_id": "2509.00303v2",
      "title": "Access Paths for Efficient Ordering with Large Language Models",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆæ’åºçš„è®¿é—®è·¯å¾„",
      "authors": [
        "Fuheng Zhao",
        "Jiayue Chen",
        "Yiming Pan",
        "Tahseen Rabbani",
        "Sohaib",
        "Divyakant Agrawal",
        "Amr El Abbadi",
        "Paritosh Aggarwal",
        "Anupam Datta",
        "Dimitris Tsirogiannis"
      ],
      "abstract": "In this work, we present the \\texttt{LLM ORDER BY} semantic operator as a logical abstraction and conduct a systematic study of its physical implementations. First, we propose several improvements to existing semantic sorting algorithms and introduce a semantic-aware external merge sort algorithm. Our extensive evaluation reveals that no single implementation offers universal optimality on all datasets. From our evaluations, we observe a general test-time scaling relationship between sorting cost and the ordering quality for comparison-based algorithms. Building on these insights, we design a budget-aware optimizer that utilizes heuristic rules, LLM-as-Judge evaluation, and consensus aggregation to dynamically select the near-optimal access path for LLM ORDER BY. In our extensive evaluations, our optimizer consistently achieves ranking accuracy on par with or superior to the best static methods across all benchmarks. We believe that this work provides foundational insights into the principled optimization of semantic operators essential for building robust, large-scale LLM-powered analytic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLM ORDER BYè¯­ä¹‰æ“ä½œç¬¦ä½œä¸ºä¸€ç§é€»è¾‘æŠ½è±¡ï¼Œå¹¶å¯¹å…¶ç‰©ç†å®ç°è¿›è¡Œäº†ç³»ç»Ÿæ€§ç ”ç©¶ã€‚å›¢é˜Ÿæ”¹è¿›äº†ç°æœ‰çš„è¯­ä¹‰æ’åºç®—æ³•ï¼Œå¹¶å¼•å…¥äº†è¯­ä¹‰æ„ŸçŸ¥çš„å¤–éƒ¨å½’å¹¶æ’åº(external merge sort)ç®—æ³•ã€‚å®éªŒè§‚å¯Ÿåˆ°ï¼Œæ’åºæˆæœ¬ä¸æ’åºè´¨é‡åœ¨æ¯”è¾ƒç®—æ³•ä¸­å­˜åœ¨æµ‹è¯•æ—¶ç¼©æ”¾(test-time scaling)å…³ç³»ï¼Œä¸”æ²¡æœ‰å•ä¸€å®ç°èƒ½åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šä¿æŒæœ€ä¼˜ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªé¢„ç®—æ„ŸçŸ¥çš„ä¼˜åŒ–å™¨(budget-aware optimizer)ï¼Œé€šè¿‡å¯å‘å¼è§„åˆ™ã€LLM-as-Judgeè¯„ä¼°å’Œå…±è¯†èšåˆ(consensus aggregation)åŠ¨æ€é€‰æ‹©è¿‘ä¹æœ€ä¼˜çš„è®¿é—®è·¯å¾„(access path)ã€‚æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œè¯¥ä¼˜åŒ–å™¨åœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸­çš„æ’åå‡†ç¡®ç‡å‡è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰çš„æœ€ä½³é™æ€æ–¹æ³•ã€‚è¯¥å·¥ä½œä¸ºä¼˜åŒ–å¤§è§„æ¨¡LLMåˆ†æç³»ç»Ÿä¸­çš„æ ¸å¿ƒè¯­ä¹‰ç®—å­æä¾›äº†é‡è¦çš„æ–¹æ³•è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00303v2",
      "published_date": "2025-08-30 01:44:36 UTC",
      "updated_date": "2025-12-03 06:35:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:31:19.761368+00:00"
    },
    {
      "arxiv_id": "2510.15882v1",
      "title": "FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern",
      "title_zh": "FlexLinkï¼šåœ¨ä¸å½±å“ç²¾åº¦çš„å‰æä¸‹å°† NVLink å¸¦å®½æå‡ 27%",
      "authors": [
        "Ao Shen",
        "Rui Zhang",
        "Junping Zhao"
      ],
      "abstract": "As large language models (LLMs) continue to scale, multi-node deployment has become a necessity. Consequently, communication has become a critical performance bottleneck. Current intra-node communication libraries, like NCCL, typically make use of a single interconnect such as NVLink. This approach creates performance ceilings, especially on hardware like the H800 GPU where the primary interconnect's bandwidth can become a bottleneck, and leaves other hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable Network Interface Cards (NICs) largely idle during intensive workloads. We propose FlexLink, the first collective communication framework to the best of our knowledge designed to systematically address this by aggregating these heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance communication fabric. FlexLink employs an effective two-stage adaptive load balancing strategy that dynamically partitions communication traffic across all available links, ensuring that faster interconnects are not throttled by slower ones. On an 8-GPU H800 server, our design improves the bandwidth of collective operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL baseline, respectively. This gain is achieved by offloading 2-22% of the total communication traffic to the previously underutilized PCIe and RDMA NICs. FlexLink provides these improvements as a lossless, drop-in replacement compatible with the NCCL API, ensuring easy adoption.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FlexLinkï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨é€šè¿‡æ•´åˆNVLinkã€PCIeå’Œæ”¯æŒRDMAçš„ç½‘å¡(NICs)ç­‰å¼‚æ„é“¾è·¯æ¥è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)å¤šèŠ‚ç‚¹éƒ¨ç½²é€šä¿¡ç“¶é¢ˆçš„é›†åˆé€šä¿¡æ¡†æ¶ã€‚é’ˆå¯¹H800ç­‰ç¡¬ä»¶ä¸­NVLinkå¸¦å®½å—é™è€ŒPCIeå’ŒRDMAèµ„æºè¢«é—²ç½®çš„é—®é¢˜ï¼ŒFlexLinkæ„å»ºäº†ç»Ÿä¸€çš„é«˜æ€§èƒ½é€šä¿¡ç»‡ç½‘ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤é˜¶æ®µè‡ªé€‚åº”è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œèƒ½å¤ŸåŠ¨æ€åˆ’åˆ†æ‰€æœ‰å¯ç”¨é“¾è·¯çš„é€šä¿¡æµé‡ï¼Œç¡®ä¿é«˜é€Ÿäº’è¿ä¸ä¼šè¢«ä½é€Ÿäº’è¿é™åˆ¶ã€‚åœ¨8-GPU H800æœåŠ¡å™¨ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒFlexLinkå°†AllReduceå’ŒAllGatherç­‰ç®—å­çš„å¸¦å®½è¾ƒNCCLåŸºå‡†åˆ†åˆ«æå‡äº†æœ€é«˜26%å’Œ27%ã€‚è¿™ä¸€æ˜¾è‘—å¢ç›Šä¸»è¦é€šè¿‡å°†2-22%çš„æ€»é€šä¿¡æµé‡æœ‰æ•ˆå¸è½½åˆ°å…ˆå‰åˆ©ç”¨ä¸è¶³çš„PCIeå’ŒRDMAç½‘å¡è€Œå®ç°ã€‚ä½œä¸ºä¸€ç§æ— æŸä¸”å®Œå…¨å…¼å®¹NCCL APIçš„å³æ’å³ç”¨æ›¿ä»£æ–¹æ¡ˆï¼ŒFlexLinkä¸ºæå‡åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ æ€§èƒ½æä¾›äº†é«˜æ•ˆä¸”æ˜“äºéƒ¨ç½²çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.15882v1",
      "published_date": "2025-08-30 01:28:47 UTC",
      "updated_date": "2025-08-30 01:28:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:31:21.365778+00:00"
    },
    {
      "arxiv_id": "2509.00287v1",
      "title": "SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces",
      "title_zh": "SIGMUSï¼šå¤šæ¨¡æ€åŸå¸‚ç©ºé—´çŸ¥è¯†å›¾è°±çš„è¯­ä¹‰é›†æˆ",
      "authors": [
        "Brian Wang",
        "Mani Srivastava"
      ],
      "abstract": "Modern urban spaces are equipped with an increasingly diverse set of sensors, all producing an abundance of multimodal data. Such multimodal data can be used to identify and reason about important incidents occurring in urban landscapes, such as major emergencies, cultural and social events, as well as natural disasters. However, such data may be fragmented over several sources and difficult to integrate due to the reliance on human-driven reasoning for identifying relationships between the multimodal data corresponding to an incident, as well as understanding the different components which define an incident. Such relationships and components are critical to identifying the causes of such incidents, as well as producing forecasting the scale and intensity of future incidents as they begin to develop. In this work, we create SIGMUS, a system for Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces. SIGMUS uses Large Language Models (LLMs) to produce the necessary world knowledge for identifying relationships between incidents occurring in urban spaces and data from different modalities, allowing us to organize evidence and observations relevant to an incident without relying and human-encoded rules for relating multimodal sensory data with incidents. This organized knowledge is represented as a knowledge graph, organizing incidents, observations, and much more. We find that our system is able to produce reasonable connections between 5 different data sources (new article text, CCTV images, air quality, weather, and traffic measurements) and relevant incidents occurring at the same time and location.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SIGMUSï¼Œä¸€ä¸ªç”¨äºå¤šæ¨¡æ€åŸå¸‚ç©ºé—´çŸ¥è¯†å›¾è°±(Knowledge Graphs)è¯­ä¹‰é›†æˆçš„ç³»ç»Ÿã€‚é’ˆå¯¹ç°ä»£åŸå¸‚ä¼ æ„Ÿå™¨äº§ç”Ÿçš„å¤šæ¨¡æ€æ•°æ®(Multimodal Data)ç¢ç‰‡åŒ–ä¸”éš¾ä»¥æ•´åˆçš„æŒ‘æˆ˜ï¼ŒSIGMUSåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ç”Ÿæˆä¸–ç•ŒçŸ¥è¯†ï¼Œä»¥è¯†åˆ«åŸå¸‚äº‹ä»¶ä¸ä¸åŒæ¨¡æ€æ•°æ®ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚è¯¥æ¡†æ¶æ— éœ€ä¾èµ–äººå·¥å®šä¹‰çš„è§„åˆ™å³å¯è‡ªåŠ¨ç»„ç»‡ä¸äº‹ä»¶ç›¸å…³çš„è¯æ®å’Œè§‚æµ‹å€¼ï¼Œå¹¶å°†å…¶æ„å»ºä¸ºç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSIGMUSèƒ½å¤ŸæˆåŠŸå°†æ–°é—»æ–‡æœ¬ã€CCTVå›¾åƒã€ç©ºæ°”è´¨é‡ã€å¤©æ°”å’Œäº¤é€šç›‘æµ‹ç­‰äº”ç§å¼‚æ„æ•°æ®æºä¸ç‰¹å®šæ—¶é—´ã€åœ°ç‚¹å‘ç”Ÿçš„äº‹ä»¶è¿›è¡Œå…³è”ã€‚è¿™ä¸€æˆæœä¸ºè¯†åˆ«åŸå¸‚äº‹ä»¶æˆå› åŠé¢„æµ‹æœªæ¥äº‹ä»¶çš„è§„æ¨¡å’Œå¼ºåº¦æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, accepted at UrbComp 2025 KDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.00287v1",
      "published_date": "2025-08-30 00:35:41 UTC",
      "updated_date": "2025-08-30 00:35:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:31:24.776564+00:00"
    },
    {
      "arxiv_id": "2509.00286v1",
      "title": "Intelligent Spectrum Management in Satellite Communications",
      "title_zh": "å«æ˜Ÿé€šä¿¡ä¸­çš„æ™ºèƒ½é¢‘è°±ç®¡ç†",
      "authors": [
        "Rakshitha De Silva",
        "Shiva Raj Pokhrel",
        "Jonathan Kua",
        "Sithamparanathan Kandeepan"
      ],
      "abstract": "Satellite Communication (SatCom) networks represent a fundamental pillar in modern global connectivity, facilitating reliable service and extensive coverage across a plethora of applications. The expanding demand for high-bandwidth services and the proliferation of mega satellite constellations highlight the limitations of traditional exclusive satellite spectrum allocation approaches. Cognitive Radio (CR) leading to Cognitive Satellite (CogSat) networks through Dynamic Spectrum Management (DSM), which enables the dynamic adaptability of radio equipment to environmental conditions for optimal performance, presents a promising solution for the emerging spectrum scarcity. In this survey, we explore the adaptation of intelligent DSM methodologies to SatCom, leveraging satellite network integrations. We discuss contributions and hurdles in regulations and standardizations in realizing intelligent DSM in SatCom, and deep dive into DSM techniques, which enable CogSat networks. Furthermore, we extensively evaluate and categorize state-of-the-art Artificial Intelligence (AI)/Machine Learning (ML) methods leveraged for DSM while exploring operational resilience and robustness of such integrations. In addition, performance evaluation metrics critical for adaptive resource management and system optimization in CogSat networks are thoroughly investigated. This survey also identifies open challenges and outlines future research directions in regulatory frameworks, network architectures, and intelligent spectrum management, paving the way for sustainable and scalable SatCom networks for enhanced global connectivity.",
      "tldr_zh": "è¯¥ç ”ç©¶ç»¼è¿°äº†å«æ˜Ÿé€šä¿¡(SatCom)ä¸­çš„æ™ºèƒ½é¢‘è°±ç®¡ç†æŠ€æœ¯ï¼Œæ—¨åœ¨åº”å¯¹é«˜å¸¦å®½éœ€æ±‚å’Œå·¨å‹å«æ˜Ÿæ˜Ÿåº§å¸¦æ¥çš„é¢‘è°±ç¨€ç¼ºæŒ‘æˆ˜ã€‚æ–‡ç« æ¢è®¨äº†é€šè¿‡åŠ¨æ€é¢‘è°±ç®¡ç†(DSM)å®ç°è®¤çŸ¥å«æ˜Ÿ(CogSat)ç½‘ç»œï¼Œä½¿æ— çº¿ç”µè®¾å¤‡èƒ½å¤Ÿæ ¹æ®ç¯å¢ƒæ¡ä»¶è¿›è¡ŒåŠ¨æ€è°ƒæ•´ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚é€šè¿‡å¯¹åº”ç”¨äºDSMçš„å…ˆè¿›äººå·¥æ™ºèƒ½(AI)ä¸æœºå™¨å­¦ä¹ (ML)æ–¹æ³•è¿›è¡Œåˆ†ç±»ä¸è¯„ä¼°ï¼Œè¯¥ç»¼è¿°æ·±å…¥åˆ†æäº†è¿™äº›é›†æˆæ–¹æ¡ˆçš„è¿è¡Œå¼¹æ€§å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯¦ç»†æ¢è®¨äº†è‡ªé€‚åº”èµ„æºç®¡ç†ä¸­çš„æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æ˜ç¡®äº†æ³•è§„æ¡†æ¶ã€ç½‘ç»œæ¶æ„åŠæ™ºèƒ½é¢‘è°±ç®¡ç†é¢†åŸŸçš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºæ„å»ºå¯æŒç»­ä¸”å¯æ‰©å±•çš„å…¨çƒè¿æ¥å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "30 pages, Under review in IEEE Communications Surveys & Tutorials",
      "pdf_url": "https://arxiv.org/pdf/2509.00286v1",
      "published_date": "2025-08-30 00:34:36 UTC",
      "updated_date": "2025-08-30 00:34:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:31:20.359182+00:00"
    },
    {
      "arxiv_id": "2509.02607v1",
      "title": "Towards Digital Twins for Optimal Radioembolization",
      "title_zh": "è¿ˆå‘æ”¾å°„æ€§æ “å¡ä¼˜åŒ–çš„æ•°å­—å­ªç”Ÿç ”ç©¶",
      "authors": [
        "Nisanth Kumar Panneerselvam",
        "Guneet Mummaneni",
        "Emilie Roncali"
      ],
      "abstract": "Radioembolization is a localized liver cancer treatment that delivers radioactive microspheres (30 micron) to tumors via a catheter inserted in the hepatic arterial tree. The goal is to maximize therapeutic efficacy while minimizing damage to healthy liver tissue. However, optimization is challenging due to complex hepatic artery anatomy, variable blood flow, and uncertainty in microsphere transport. The creation of dynamic, patient-specific digital twins may provide a transformative solution to these challenges. This work outlines a framework for a liver radioembolization digital twin using high-fidelity computational fluid dynamics (CFD) and/or recent physics-informed machine learning approaches. The CFD approach involves microsphere transport calculations in the hepatic arterial tree with individual patient data, which enables personalized treatment planning. Although accurate, traditional CFD is computationally expensive and limits clinical applicability.\n  To accelerate simulations, physics-informed neural networks (PINNs) and their generative extensions play an increasingly important role. PINNs integrate governing equations, such as the Navier-Stokes equations, directly into the neural network training process, enabling mesh-free, data-efficient approximation of blood flow and microsphere transport. Physics-informed generative adversarial networks (PI-GANs), diffusion models (PI-DMs), and transformer-based architectures further enable uncertainty-aware, temporally resolved predictions with reduced computational cost. These AI surrogates not only maintain physical fidelity but also support rapid sampling of diverse flow scenarios, facilitating real-time decision support.\n  Together, CFD and physics-informed AI methods form the foundation of dynamic, patient-specific digital twin to optimize radioembolization planning and ultimately improve clinical outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸ºä¼˜åŒ–æ”¾å°„æ€§æ “å¡ï¼ˆRadioembolizationï¼‰æ²»ç–—å»ºç«‹æ‚£è€…ç‰¹å®šæ•°å­—å­ªç”Ÿï¼ˆDigital Twinsï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸ªæ€§åŒ–æ¨¡æ‹Ÿæœ€å¤§åŒ–è‚ç™Œæ²»ç–—æ•ˆæœå¹¶å‡å°‘å¥åº·ç»„ç»‡æŸä¼¤ã€‚è¯¥æ¡†æ¶æ•´åˆäº†é«˜ä¿çœŸè®¡ç®—æµä½“åŠ›å­¦ï¼ˆCFDï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨æ‚£è€…æ•°æ®ç²¾ç¡®æ¨¡æ‹Ÿå¾®çƒåœ¨è‚åŠ¨è„‰æ ‘ä¸­çš„ä¼ è¾“è¿‡ç¨‹ã€‚é’ˆå¯¹ä¼ ç»ŸCFDè®¡ç®—æˆæœ¬è¿‡é«˜çš„å±€é™æ€§ï¼Œç ”ç©¶å¼•å…¥äº†ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNsï¼‰åŠå…¶ç”Ÿæˆå¼æ‰©å±•ï¼ˆå¦‚PI-GANså’ŒPI-DMsï¼‰ï¼Œå°†Navier-Stokesæ–¹ç¨‹ç­‰ç‰©ç†å®šå¾‹ç›´æ¥æ•´åˆåˆ°æ¨¡å‹è®­ç»ƒä¸­ã€‚è¿™äº›ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ æ–¹æ³•ä¸Transformeræ¶æ„ç›¸ç»“åˆï¼Œå®ç°äº†å…·å¤‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥èƒ½åŠ›ä¸”é«˜æ•ˆçš„æ—¶ç©ºæµåœºé¢„æµ‹ã€‚é€šè¿‡èåˆCFDä¸ç‰©ç†ä¿¡æ¯äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œè¯¥æ•°å­—å­ªç”Ÿç³»ç»Ÿèƒ½å¤Ÿæ”¯æŒå¤šæ ·åŒ–åœºæ™¯çš„å¿«é€Ÿé‡‡æ ·ï¼Œä¸ºå®æ—¶ä¸´åºŠå†³ç­–å’Œæ‰‹æœ¯æ–¹æ¡ˆä¼˜åŒ–æä¾›äº†åšå®çš„ç‰©ç†ä¸æ•°æ®åŸºç¡€ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.02607v1",
      "published_date": "2025-08-30 00:30:42 UTC",
      "updated_date": "2025-08-30 00:30:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:31:27.562764+00:00"
    },
    {
      "arxiv_id": "2509.16213v1",
      "title": "DarwinWafer: A Wafer-Scale Neuromorphic Chip",
      "title_zh": "DarwinWaferï¼šæ™¶åœ†çº§ç±»è„‘èŠ¯ç‰‡",
      "authors": [
        "Xiaolei Zhu",
        "Xiaofei Jin",
        "Ziyang Kang",
        "Chonghui Sun",
        "Junjie Feng",
        "Dingwen Hu",
        "Zengyi Wang",
        "Hanyue Zhuang",
        "Qian Zheng",
        "Huajin Tang",
        "Shi Gu",
        "Xin Du",
        "De Ma",
        "Gang Pan"
      ],
      "abstract": "Neuromorphic computing promises brain-like efficiency, yet today's multi-chip systems scale over PCBs and incur orders-of-magnitude penalties in bandwidth, latency, and energy, undermining biological algorithms and system efficiency. We present DarwinWafer, a hyperscale system-on-wafer that replaces off-chip interconnects with wafer-scale, high-density integration of 64 Darwin3 chiplets on a 300 mm silicon interposer. A GALS NoC within each chiplet and an AER-based asynchronous wafer fabric with hierarchical time-step synchronization provide low-latency, coherent operation across the wafer. Each chiplet implements 2.35 M neurons and 0.1 B synapses, yielding 0.15 B neurons and 6.4 B synapses per wafer.At 333 MHz and 0.8 V, DarwinWafer consumes ~100 W and achieves 4.9 pJ/SOP, with 64 TSOPS peak throughput (0.64 TSOPS/W). Realization is enabled by a holistic chiplet-interposer co-design flow (including an in-house interposer-bump planner with early SI/PI and electro-thermal closure) and a warpage-tolerant assembly that fans out I/O via PCBlets and compliant pogo-pin connections, enabling robust, demountable wafer-to-board integration. Measurements confirm 10 mV supply droop and a uniform thermal profile (34-36 Â°C) under ~100 W. Application studies demonstrate whole-brain simulations: two zebrafish brains per chiplet with high connectivity fidelity (Spearman r = 0.896) and a mouse brain mapped across 32 chiplets (r = 0.645). To our knowledge, DarwinWafer represents a pioneering demonstration of wafer-scale neuromorphic computing, establishing a viable and scalable path toward large-scale, brain-like computation on silicon by replacing PCB-level interconnects with high-density, on-wafer integration.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† DarwinWaferï¼Œè¿™æ˜¯ä¸€ç§è¶…å¤§è§„æ¨¡çš„æ™¶åœ†çº§ç³»ç»Ÿ (system-on-wafer)ï¼Œæ—¨åœ¨é€šè¿‡é«˜å¯†åº¦æ™¶åœ†é›†æˆè§£å†³ä¼ ç»Ÿå¤šèŠ¯ç‰‡ç³»ç»Ÿåœ¨å¸¦å®½ã€å»¶è¿Ÿå’Œèƒ½æ•ˆæ–¹é¢çš„ç“¶é¢ˆã€‚è¯¥ç³»ç»Ÿåœ¨ 300 mm çš„ç¡…ä¸­ä»‹å±‚ (silicon interposer) ä¸Šé›†æˆäº† 64 ä¸ª Darwin3 å°èŠ¯ç‰‡ (chiplets)ï¼Œå¹¶åˆ©ç”¨ GALS NoC å’ŒåŸºäº AER çš„å¼‚æ­¥æ™¶åœ†ç»“æ„ (asynchronous wafer fabric) å®ç°äº†ä½å»¶è¿Ÿçš„ååŒè¿è¡Œã€‚æ¯ç‰‡æ™¶åœ†å…·å¤‡ 0.15 B ä¸ªç¥ç»å…ƒå’Œ 6.4 B ä¸ªçªè§¦ï¼Œåœ¨çº¦ 100 W åŠŸè€—ä¸‹å¯å®ç° 64 TSOPS çš„å³°å€¼ååé‡å’Œ 4.9 pJ/SOP çš„æé«˜èƒ½æ•ˆã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡èŠ¯ç‰‡-ä¸­ä»‹å±‚ååŒè®¾è®¡æµç¨‹å’ŒæŠ—ç¿˜æ›²ç»„è£…æŠ€æœ¯ï¼Œåˆ©ç”¨ PCBlets å’Œ pogo-pin è¿æ¥ç¡®ä¿äº†æ™¶åœ†ä¸ç”µè·¯æ¿é›†æˆçš„é²æ£’æ€§ã€‚å®æµ‹ç»“æœæ˜¾ç¤ºç³»ç»Ÿä¾›ç”µå‹é™ä»…ä¸º 10 mV ä¸”çƒ­åˆ†å¸ƒå‡åŒ€ï¼Œåœ¨å…¨è„‘æ¨¡æ‹Ÿåº”ç”¨ä¸­èƒ½å¤Ÿä»¥é«˜ä¿çœŸåº¦æ¨¡æ‹Ÿæ–‘é©¬é±¼è„‘å¹¶å®ç°è·¨èŠ¯ç‰‡çš„å°é¼ å¤§è„‘æ˜ å°„ã€‚è¯¥ç ”ç©¶ä»£è¡¨äº†æ™¶åœ†çº§ç±»è„‘è®¡ç®—çš„å¼€åˆ›æ€§çªç ´ï¼Œä¸ºå®ç°å¤§è§„æ¨¡ã€ç±»è„‘åŒ–ç¡…è®¡ç®—æä¾›äº†ä¸€æ¡ä» PCB äº’è¿è½¬å‘æ™¶åœ†çº§é›†æˆçš„é«˜æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16213v1",
      "published_date": "2025-08-30 00:22:09 UTC",
      "updated_date": "2025-08-30 00:22:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:31:41.867032+00:00"
    },
    {
      "arxiv_id": "2509.00285v2",
      "title": "OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews",
      "title_zh": "OpinioRAGï¼šé¢å‘å¤§è§„æ¨¡åœ¨çº¿è¯„è®ºç”Ÿæˆä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„è§‚ç‚¹äº®ç‚¹",
      "authors": [
        "Mir Tafseer Nayeem",
        "Davood Rafiei"
      ],
      "abstract": "We study the problem of opinion highlights generation from large volumes of user reviews, often exceeding thousands per entity, where existing methods either fail to scale or produce generic, one-size-fits-all summaries that overlook personalized needs. To tackle this, we introduce OpinioRAG, a scalable, training-free framework that combines RAG-based evidence retrieval with LLMs to efficiently produce tailored summaries. Additionally, we propose novel reference-free verification metrics designed for sentiment-rich domains, where accurately capturing opinions and sentiment alignment is essential. These metrics offer a fine-grained, context-sensitive assessment of factual consistency. To facilitate evaluation, we contribute the first large-scale dataset of long-form user reviews, comprising entities with over a thousand reviews each, paired with unbiased expert summaries and manually annotated queries. Through extensive experiments, we identify key challenges, provide actionable insights into improving systems, pave the way for future research, and position OpinioRAG as a robust framework for generating accurate, relevant, and structured summaries at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»æµ·é‡ç”¨æˆ·è¯„è®ºä¸­ç”Ÿæˆè§‚ç‚¹äº®ç‚¹çš„é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ•°åƒæ¡è¯„è®ºæ—¶éš¾ä»¥æ‰©å±•ä¸”å¾€å¾€äº§ç”Ÿé€šç”¨æ€§è¿‡å¼ºã€ç¼ºä¹ä¸ªæ€§åŒ–çš„æ‘˜è¦ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† OpinioRAGï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒä¸”å¯æ‰©å±•çš„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ä¸å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¥é«˜æ•ˆç”Ÿæˆå®šåˆ¶åŒ–æ‘˜è¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æ–°å‹çš„å‚è€ƒæ— å…³éªŒè¯æŒ‡æ ‡ (reference-free verification metrics)ï¼Œæ—¨åœ¨ç²¾å‡†æ•æ‰æƒ…æ„Ÿä¸°å¯Œé¢†åŸŸä¸­çš„è§‚ç‚¹ä¸æƒ…æ„Ÿå¯¹é½ (sentiment alignment)ï¼Œå¹¶æä¾›ç»†ç²’åº¦çš„äº‹å®ä¸€è‡´æ€§è¯„ä¼°ã€‚è¯¥å›¢é˜Ÿè¿˜è´¡çŒ®äº†é¦–ä¸ªåŒ…å«å¤§è§„æ¨¡è¯„è®ºã€ä¸“å®¶æ‘˜è¦åŠæ‰‹åŠ¨æ ‡æ³¨æŸ¥è¯¢çš„å®ä½“æ•°æ®é›†ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„è¯„ä¼°ç©ºç™½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOpinioRAG åœ¨ç”Ÿæˆå‡†ç¡®ã€ç›¸å…³ä¸”ç»“æ„åŒ–çš„è§‚ç‚¹æ‘˜è¦æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºå¤§è§„æ¨¡åœ¨çº¿è¯„è®ºçš„ä¸ªæ€§åŒ–å¤„ç†æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "COLM 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.00285v2",
      "published_date": "2025-08-30 00:00:34 UTC",
      "updated_date": "2025-11-01 20:48:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T15:33:04.460378+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 71,
  "processed_papers_count": 71,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T15:33:58.344405+00:00"
}
{
  "date": "2025-11-19",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-19 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv çˆ†å‘äº†å¤šæ¨¡æ€â€œæ¨ç†â€èƒ½åŠ›çš„æ¢ç´¢ï¼Œä»**éŸ³é¢‘æ¨ç†é“¾ (Step-Audio-R1)** åˆ°**è§†é¢‘ç”Ÿæˆä¸­çš„æ¨ç† (VR-Bench)**ï¼Œå†åˆ°åˆ©ç”¨è§†è§‰è¾…åŠ©è§£å†³ **ARC-AGI** éš¾é¢˜ï¼Œç¤¾åŒºæ­£åœ¨è¯•å›¾æ‰“ç ´ä»…é æ–‡æœ¬è¿›è¡Œ System 2 æ€è€ƒçš„å±€é™ã€‚æ­¤å¤–ï¼Œ**Kandinsky 5.0** å‘å¸ƒäº†å…¨å¥—å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹å®¶æ—ï¼Œ**ç‰©ç†æ¨¡æ‹ŸåŸºç¡€æ¨¡å‹ Walrus** ä¹Ÿä»¤äººç©ç›®ã€‚å®‰å…¨é¢†åŸŸå‡ºç°äº†ä¸€ä¸ªæœ‰è¶£çš„å‘ç°ï¼š**å†™è¯—**ç«Ÿç„¶èƒ½æˆä¸ºé€šç”¨çš„è¶Šç‹±æ”»å‡»æ‰‹æ®µã€‚\n\n---\n\n### ğŸŒŸ é‡ç‚¹å…³æ³¨ï¼šå¤šæ¨¡æ€æ¨ç†ä¸åŸºç¡€æ¨¡å‹\n\n**1. Step-Audio-R1 Technical Report**\n**# æ ‡é¢˜ï¼š** Step-Audio-R1 æŠ€æœ¯æŠ¥å‘Šï¼šè§£é”éŸ³é¢‘æ¨¡æ€çš„æ¨ç†èƒ½åŠ›\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªé‡Œç¨‹ç¢‘å¼çš„å·¥ä½œã€‚ä½œè€…å‘ç°éŸ³é¢‘æ¨¡å‹é€šå¸¸åœ¨â€œä¸æ€è€ƒâ€æ—¶è¡¨ç°æ›´å¥½ï¼Œè¿™å¾ˆåç›´è§‰ã€‚ä»–ä»¬æå‡ºäº† Step-Audio-R1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªçœŸæ­£è§£é”**éŸ³é¢‘é¢†åŸŸæ¨ç†èƒ½åŠ›**çš„æ¨¡å‹ã€‚\n**# å‘ç°ï¼š** é€šè¿‡æ¨¡æ€è½åœ°æ¨ç†è’¸é¦ï¼ˆMGRDï¼‰ï¼Œæ¨¡å‹å­¦ä¼šäº†åŸºäºå£°å­¦ç‰¹å¾ç”Ÿæˆæ¨ç†é“¾ï¼ˆCoTï¼‰ï¼Œè€Œä¸æ˜¯äº§ç”Ÿå¹»è§‰ã€‚åœ¨éŸ³é¢‘ç†è§£å’Œæ¨ç†åŸºå‡†ä¸Šï¼Œå®ƒè¶…è¶Šäº† Gemini 2.5 Proï¼Œåª²ç¾ Gemini 3 Proï¼Œè¯æ˜äº†æ¨ç†èƒ½åŠ›å¯ä»¥è·¨æ¨¡æ€è¿ç§»ã€‚\n\n**2. Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation**\n**# æ ‡é¢˜ï¼š** Kandinsky 5.0ï¼šå›¾åƒä¸è§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹å®¶æ—\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** å‘å¸ƒäº† Kandinsky 5.0 ç³»åˆ—ï¼ŒåŒ…æ‹¬ 6B çš„å›¾åƒæ¨¡å‹ã€è½»é‡çº§ 2B è§†é¢‘æ¨¡å‹å’Œ 19B çš„ Video Pro æ¨¡å‹ã€‚\n**# å‘ç°ï¼š** æŠ¥å‘Šè¯¦ç»†ä»‹ç»äº†ä»æ•°æ®æ¸…æ´—ã€é¢„è®­ç»ƒåˆ° RL åè®­ç»ƒçš„å…¨æµç¨‹ã€‚è¯¥æ¨¡å‹å®¶æ—åœ¨ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ä¸Šå‡è¾¾åˆ° SOTA æ°´å¹³ï¼Œä¸”ä»£ç å’Œæƒé‡å¼€æºï¼Œæ˜¯å¼€æºç¤¾åŒºçš„é‡è¦è¡¥å……ã€‚\n\n**3. Walrus: A Cross-Domain Foundation Model for Continuum Dynamics**\n**# æ ‡é¢˜ï¼š** Walrusï¼šè¿ç»­ä»‹è´¨åŠ¨åŠ›å­¦çš„è·¨åŸŸåŸºç¡€æ¨¡å‹\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªé¢å‘ç‰©ç†æ¨¡æ‹Ÿçš„ Transformer åŸºç¡€æ¨¡å‹ï¼Œæ¶µç›–å¤©ä½“ç‰©ç†ã€åœ°çƒç§‘å­¦ã€ç­‰ç¦»å­ä½“ç­‰é¢†åŸŸã€‚\n**# å‘ç°ï¼š** è§£å†³äº†ç‰©ç†æ¨¡æ‹Ÿä¸­æ•°æ®å¼‚æ„å’Œé•¿æœŸåŠ¨åŠ›å­¦ä¸ç¨³å®šçš„é—®é¢˜ã€‚Walrus åœ¨çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ä»»åŠ¡ä¸Šå‡ä¼˜äºä¹‹å‰çš„ç‰©ç†åŸºç¡€æ¨¡å‹ï¼Œå±•ç¤ºäº† AI åœ¨ç§‘å­¦è®¡ç®—ï¼ˆAI4Scienceï¼‰ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚\n\n**4. Think Visually, Reason Textually: Vision-Language Synergy in ARC**\n**# æ ‡é¢˜ï¼š** è§†è§‰æ€è€ƒï¼Œæ–‡æœ¬æ¨ç†ï¼šARC ä»»åŠ¡ä¸­çš„è§†è¯­è¨€ååŒ\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹è‘—åçš„ ARC-AGI éš¾é¢˜ï¼Œæå‡ºäººç±»æ˜¯ç»“åˆè§†è§‰æŠ½è±¡å’Œé€»è¾‘æ¨ç†æ¥è§£é¢˜çš„ã€‚\n**# å‘ç°ï¼š** å•çº¯æŠŠ ARC ç½‘æ ¼å˜æˆå›¾åƒä¼šé™ä½æ€§èƒ½ï¼Œä½†ä½œè€…æå‡ºçš„ VLSRï¼ˆè§†è¯­è¨€ååŒæ¨ç†ï¼‰å’Œ MSSCï¼ˆæ¨¡æ€åˆ‡æ¢è‡ªæ ¡æ­£ï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨è§†è§‰è¿›è¡Œå…¨å±€æ¨¡å¼æŠ½è±¡ï¼Œåˆ©ç”¨è¯­è¨€è¿›è¡Œè§„åˆ™æ‰§è¡Œï¼Œä½¿æ¨¡å‹åœ¨ ARC ä»»åŠ¡ä¸Šæå‡äº† 4.33%ï¼Œä¸ºè§£å†³æŠ½è±¡æ¨ç†é—®é¢˜æä¾›äº†æ–°æ€è·¯ã€‚\n\n---\n\n### ğŸ¤– Agent ä¸ å…·èº«æ™ºèƒ½\n\n**5. IPR-1: Interactive Physical Reasoner**\n**# æ ‡é¢˜ï¼š** IPR-1ï¼šäº¤äº’å¼ç‰©ç†æ¨ç†æ™ºèƒ½ä½“\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº†ä¸€ä¸ªäº¤äº’å¼ç‰©ç†æ¨ç†å™¨ï¼ˆIPRï¼‰ï¼Œæ—¨åœ¨åƒäººç±»ä¸€æ ·é€šè¿‡äº¤äº’å­¦ä¹ ç‰©ç†è§„å¾‹å’Œå› æœå…³ç³»ã€‚\n**# å‘ç°ï¼š** å¼•å…¥äº† PhysCodeï¼Œä¸€ç§ä»¥ç‰©ç†ä¸ºä¸­å¿ƒçš„åŠ¨ä½œç¼–ç ã€‚åœ¨ 1000 å¤šä¸ªæ¸¸æˆçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒIPR è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œç”šè‡³åœ¨ç‰©ç†ç›´è§‰å’Œç›®æ ‡å¯¼å‘æ¨ç†ä¸Šè¶…è¶Šäº† GPT-5ã€‚\n\n**6. Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration**\n**# æ ‡é¢˜ï¼š** Octopusï¼šå…·å¤‡å…­ç§èƒ½åŠ›ç¼–æ’çš„ä»£ç†å¼å¤šæ¨¡æ€æ¨ç†\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† Octopus èŒƒå¼ï¼Œå®šä¹‰äº†å¤šæ¨¡æ€æ¨ç†æ‰€éœ€çš„å…­ç§æ ¸å¿ƒèƒ½åŠ›ï¼ˆå¦‚è§†è§‰æ¢ç´¢ã€ç¼–ç¨‹æ“ä½œã€è§†è§‰æƒ³è±¡ç­‰ï¼‰ã€‚\n**# å‘ç°ï¼š** ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹ç¼ºä¹è‡ªä¸»æ¢ç´¢æ¨ç†è·¯å¾„çš„èƒ½åŠ›ã€‚Octopus é€šè¿‡åŠ¨æ€ç¼–æ’è¿™äº›èƒ½åŠ›ï¼Œåœ¨ Octopus-Bench ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚\n\n**7. Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents**\n**# æ ‡é¢˜ï¼š** Terra Novaï¼šå—ã€Šæ–‡æ˜5ã€‹å¯å‘çš„æ™ºèƒ½ä½“ç»¼åˆæŒ‘æˆ˜ç¯å¢ƒ\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº†ä¸€ä¸ªæ–°çš„ RL æŒ‘æˆ˜ç¯å¢ƒï¼Œçµæ„Ÿæ¥è‡ªã€Šæ–‡æ˜5ã€‹ã€‚\n**# å‘ç°ï¼š** å¼ºè°ƒâ€œç»¼åˆæŒ‘æˆ˜â€ï¼ˆComprehensive Challengeï¼‰ï¼Œå³åœ¨ä¸€ä¸ªç¯å¢ƒä¸­åŒæ—¶é€šè¿‡éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ã€ä¿¡ç”¨åˆ†é…ã€å·¨å¤§åŠ¨ä½œç©ºé—´ç­‰å¤šä¸ªéš¾é¢˜ï¼Œè€Œä¸æ˜¯ç®€å•çš„å¤šä»»åŠ¡å †å ã€‚è¿™æ˜¯æµ‹è¯• Agent é•¿æœŸè§„åˆ’èƒ½åŠ›çš„ç»ä½³å¹³å°ã€‚\n\n---\n\n### ğŸ›¡ï¸ AI å®‰å…¨ä¸å¯¹é½\n\n**8. Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models**\n**# æ ‡é¢˜ï¼š** å¯¹æŠ—æ€§è¯—æ­Œï¼šå¤§è¯­è¨€æ¨¡å‹çš„ä¸€ç§é€šç”¨å•è½®è¶Šç‹±æœºåˆ¶\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** å‘ç°äº†ä¸€ä¸ªæœ‰è¶£çš„æ¼æ´ï¼š**å†™è¯—**ã€‚\n**# å‘ç°ï¼š** å°†æ¶æ„æç¤ºè½¬åŒ–ä¸ºè¯—æ­Œï¼ˆAdversarial Poetryï¼‰ï¼Œåœ¨ 25 ä¸ªå‰æ²¿æ¨¡å‹ä¸Šå®ç°äº†æé«˜çš„æ”»å‡»æˆåŠŸç‡ï¼ˆéƒ¨åˆ†è¶…è¿‡ 90%ï¼‰ã€‚è¿™ç§é£æ ¼ä¸Šçš„å˜åŒ–è¶³ä»¥ç»•è¿‡ç°æœ‰çš„å®‰å…¨æœºåˆ¶ï¼Œè¡¨æ˜å½“å‰çš„å®‰å…¨å¯¹é½å¯¹â€œæ–‡ä½“â€å˜åŒ–éå¸¸æ•æ„Ÿã€‚\n\n**9. Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation**\n**# æ ‡é¢˜ï¼š** æ¢ç´¢ AI å¯¹é½ä¸­çš„ç†µå‡æ¡†æ¶ï¼šä¸€é¡¹å“²å­¦è°ƒæŸ¥\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡º AI å¯¹é½ä¸åº”æ˜¯ç¼–ç å›ºå®šçš„ä»·å€¼è§‚ï¼Œè€Œåº”æ˜¯æ„å»ºâ€œç†µå‡ï¼ˆSyntropicï¼‰â€ä»£ç†ã€‚\n**# å‘ç°ï¼š** ä½œè€…è®¤ä¸ºåŸºäºå†…å®¹çš„ä»·å€¼æŒ‡å®šå­˜åœ¨â€œæŒ‡å®šé™·é˜±â€ã€‚æå‡ºçš„ Syntropy æ¡†æ¶é€šè¿‡é€’å½’å‡å°‘ä»£ç†é—´çš„äº’ä¸ç¡®å®šæ€§æ¥å®ç°å¯¹é½ï¼Œä¸ºå¤šæ™ºèƒ½ä½“å¯¹é½æä¾›äº†ä¿¡æ¯è®ºåŸºç¡€ã€‚\n\n**10. As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files**\n**# æ ‡é¢˜ï¼š** ä¼¼æ›¾ç›¸è¯†ï¼šLLM åœ¨è¯†åˆ«å·²è§æ–‡ä»¶æ—¶è¡¨ç°å‡ºç¡®å®šæ€§\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡º COPYCHECK æ¡†æ¶ï¼Œåˆ©ç”¨ LLM çš„ä¸ç¡®å®šæ€§ä¿¡å·æ¥æ£€æµ‹ç‰ˆæƒå†…å®¹æ˜¯å¦è¢«ç”¨äºè®­ç»ƒã€‚\n**# å‘ç°ï¼š** æ¨¡å‹åœ¨é¢å¯¹è®­ç»ƒè¿‡çš„æ•°æ®æ—¶ä¼šè¡¨ç°å‡ºç‰¹å®šçš„â€œè¿‡åº¦è‡ªä¿¡â€æ¨¡å¼ã€‚è¯¥æ–¹æ³•åœ¨æ£€æµ‹ç‰ˆæƒä¾µæƒæ–¹é¢æ¯” SOTA æé«˜äº† 90% ä»¥ä¸Šã€‚\n\n---\n\n### âš¡ æ¶æ„æ•ˆç‡ä¸ç³»ç»Ÿ\n\n**11. Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone**\n**# æ ‡é¢˜ï¼š** DiffuAprielï¼šåŸºäº Mamba éª¨å¹²çš„é«˜ååé‡æ‰©æ•£è¯­è¨€æ¨¡å‹\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** å°†æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDiffusion LMï¼‰çš„ Transformer éª¨å¹²æ¢æˆäº†åŒå‘ Mambaã€‚\n**# å‘ç°ï¼š** åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ¨ç†ååé‡æé«˜äº† 4.4 å€ï¼ˆé•¿åºåˆ—ï¼‰ã€‚è¯æ˜äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰æ˜¯æ‰©æ•£ LM çš„å¼ºåŠ›é™å™ªå™¨ã€‚\n\n**12. EfficientSAM3: Progressive Hierarchical Distillation for Video Concept Segmentation**\n**# æ ‡é¢˜ï¼š** EfficientSAM3ï¼šä» SAM1/2/3 è¿›è¡Œæ¸è¿›å¼åˆ†å±‚è’¸é¦\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** è§£å†³äº† SAM3 æ¨¡å‹è¿‡å¤§æ— æ³•ç«¯ä¾§éƒ¨ç½²çš„é—®é¢˜ã€‚\n**# å‘ç°ï¼š** æå‡ºäº†æ¸è¿›å¼åˆ†å±‚è’¸é¦ï¼ˆPHDï¼‰ï¼Œå°† SAM3 çš„èƒ½åŠ›è½¬ç§»åˆ°è½»é‡çº§å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œåœ¨ä¿æŒæ¦‚å¿µåˆ†å‰²æ€§èƒ½çš„åŒæ—¶å¤§å¹…æå‡äº†æ•ˆç‡ã€‚\n\n**13. GPU-Initiated Networking for NCCL**\n**# æ ‡é¢˜ï¼š** NCCL çš„ GPU å‘èµ·ç½‘ç»œé€šä¿¡\n**# æ ¸å¿ƒè´¡çŒ®ï¼š** ä»‹ç»äº† NCCL 2.28 ä¸­çš„ GINï¼ˆGPU-Initiated Networkingï¼‰æ¶æ„ã€‚\n**# å‘ç°ï¼š** å…è®¸ GPU å†…æ ¸ç›´æ¥å‘èµ·ç½‘ç»œé€šä¿¡ï¼Œæ¶ˆé™¤äº† CPU åè°ƒçš„å¼€é”€ï¼Œè¿™å¯¹äº Mixture-of-Experts (MoE) ç­‰éœ€è¦ç»†ç²’åº¦é€šä¿¡çš„è´Ÿè½½è‡³å…³é‡è¦ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡\n\n*   **[Reasoning] Reasoning via Video (VR-Bench)**: ç±»ä¼¼äº Step-Audioï¼Œè¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è§†é¢‘æ¨¡å‹æ˜¯å¦å¯ä»¥é€šè¿‡ç”Ÿæˆè§†é¢‘æ¥è¿›è¡Œç©ºé—´æ¨ç†ï¼ˆä¾‹å¦‚èµ°è¿·å®«ï¼‰ï¼Œå‘ç° SFT å¯ä»¥æœ‰æ•ˆæ¿€å‘è¿™ç§èƒ½åŠ›ã€‚\n*   **[Agents] What Does It Take to Be a Good AI Research Agent?**: åˆ†æäº† AI ç ”ç©¶ä»£ç†åœ¨ MLE-bench ä¸Šçš„è¡¨ç°ï¼Œå‘ç°â€œæ„æ€å¤šæ ·æ€§ï¼ˆIdeation Diversityï¼‰â€æ˜¯æˆåŠŸçš„å…³é”®å› ç´ ã€‚\n*   **[RL] Continual Reinforcement Learning for Cyber-Physical Systems**: è®¨è®ºäº†è‡ªåŠ¨é©¾é©¶ä¸­æŒç»­å¼ºåŒ–å­¦ä¹ çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå‘¼åè®¡ç®—æœºç§‘å­¦ä¸ç¥ç»ç§‘å­¦çš„è·¨å­¦ç§‘ç ”ç©¶ã€‚\n*   **[Finance] The SA-FARI Dataset**: è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„é‡ç”ŸåŠ¨ç‰©è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å« 11,609 ä¸ªè§†é¢‘ï¼Œç”¨äºå¤šåŠ¨ç‰©è·Ÿè¸ªå’Œè¯†åˆ«ï¼Œå¯¹ç”Ÿæ€ä¿æŠ¤ AI æœ‰é‡è¦æ„ä¹‰ã€‚\n*   **[Security] Securing AI Agents Against Prompt Injection**: æå‡ºäº†é’ˆå¯¹ RAG Agent çš„æç¤ºæ³¨å…¥æ”»å‡»åŸºå‡†æµ‹è¯•å’Œå¤šå±‚é˜²å¾¡æ¡†æ¶ã€‚\n*   **[Theory] Neural Networks Learn Generic Multi-Index Models**: ç†è®ºè¯æ˜äº†ä¸¤å±‚ç¥ç»ç½‘ç»œå¯ä»¥åœ¨ä¿¡æ¯è®ºæé™é™„è¿‘æœ‰æ•ˆåœ°å­¦ä¹ é«˜ç»´ç‰¹å¾ã€‚\n\n---\n**ç»“è¯­**ï¼šä»Šå¤©çš„è®ºæ–‡è´¨é‡å¾ˆé«˜ï¼Œå°¤å…¶æ˜¯éŸ³é¢‘å’Œè§†é¢‘æ¨¡æ€å¼€å§‹å‘â€œæ¨ç†â€è¿™ä¸€æ·±æ°´åŒºè¿›å†›ï¼Œæ ‡å¿—ç€å¤šæ¨¡æ€æ¨¡å‹æ­£åœ¨ä»â€œæ„ŸçŸ¥â€å‘â€œè®¤çŸ¥â€è·¨è¶Šã€‚å¸Œæœ›è¿™ä»½å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼",
  "papers": [
    {
      "arxiv_id": "2512.03048v1",
      "title": "Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation",
      "title_zh": "æ¢ç´¢ AI å¯¹é½ä¸­çš„åˆç†µæ¡†æ¶ï¼šä¸€é¡¹å“²å­¦è€ƒå¯Ÿ",
      "authors": [
        "Austin Spizzirri"
      ],
      "abstract": "I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹AI Alignmentè¿›è¡Œäº†å“²å­¦å±‚é¢çš„é‡æ–°æ„æ€ï¼Œä¸»å¼ åº”é€šè¿‡åŸºäºè¿‡ç¨‹çš„ã€å¤šæ™ºèƒ½ä½“çš„å’Œå‘å±•æ€§æœºåˆ¶ï¼Œæ„å»ºå…·æœ‰syntropicç‰¹æ€§ä¸”reasons-responsiveçš„æ™ºèƒ½ä½“ï¼Œè€Œéä»…ä»…ç¼–ç å›ºå®šçš„ä»·å€¼å†…å®¹ã€‚ä½œè€…é¦–å…ˆæå‡ºäº†â€œè§„èŒƒé™·é˜±â€ï¼ˆspecification trapï¼‰è®ºç‚¹ï¼Œè®ºè¯äº†å—é™äºâ€œå®ç„¶-åº”ç„¶â€å·®è·ã€ä»·å€¼å¤šå…ƒåŒ–åŠæ‰©å±•æ¡†æ¶é—®é¢˜ï¼ŒåŸºäºå†…å®¹çš„ä»·å€¼è§„èŒƒåœ¨ç»“æ„ä¸Šå…·æœ‰ä¸ç¨³å®šæ€§ã€‚å…¶æ¬¡ï¼Œè®ºæ–‡å¼•å…¥äº†syntropyæ¦‚å¿µï¼Œå°†å…¶å®šä¹‰ä¸ºæ™ºèƒ½ä½“é—´é€šè¿‡çŠ¶æ€å¯¹é½é€’å½’å‡å°‘äº’ä¸ç¡®å®šæ€§çš„è¿‡ç¨‹ï¼Œä¸ºç†è§£å¤šæ™ºèƒ½ä½“å¯¹é½åŠ¨æ€æä¾›äº†ä¿¡æ¯è®ºæ¡†æ¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶åŸºäºcompatibilistç†è®ºä¸­çš„guidance controlï¼Œå»ºç«‹äº†åŒºåˆ†çœŸå®ä¸æ¨¡æ‹Ÿé“å¾·èƒ½åŠ›çš„åŠŸèƒ½æ€§æ ‡å‡†ï¼Œå¹¶æå‡ºäº†å…·ä½“çš„éªŒè¯èŒƒå¼å’Œæ“ä½œå‡†åˆ™ã€‚å°½ç®¡è¯¥æ¡†æ¶ç”Ÿæˆçš„å…³äºä»·å€¼æ¶Œç°å’Œé“å¾·ä¸»ä½“æ€§çš„é¢„æµ‹ä»æœ‰å¾…å®è¯éªŒè¯ï¼Œä½†å®ƒä¸ºè§£å†³äººå·¥ç³»ç»Ÿçš„é“å¾·å¯¹é½é—®é¢˜æä¾›äº†å…¨æ–°çš„ç†è®ºç»´åº¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Approx. 3,000 words, 10 pages. Philosophical analysis of AI alignment (process-based / syntropy framework)",
      "pdf_url": "https://arxiv.org/pdf/2512.03048v1",
      "published_date": "2025-11-19 23:31:29 UTC",
      "updated_date": "2025-11-19 23:31:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:50:07.675798+00:00"
    },
    {
      "arxiv_id": "2511.15927v2",
      "title": "Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone",
      "title_zh": "DiffuApriel çªç ´æ•ˆç‡ç“¶é¢ˆï¼šåŸºäº Mamba éª¨å¹²çš„é«˜ååæ‰©æ•£è¯­è¨€æ¨¡å‹",
      "authors": [
        "Vaibhav Singh",
        "Oleksiy Ostapenko",
        "Pierre-AndrÃ© NoÃ«l",
        "Torsten Scholak"
      ],
      "abstract": "Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DiffuAprielï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŒå‘ Mamba éª¨å¹²ç½‘çš„æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å…‹æœåŸºäº Transformer çš„æ‰©æ•£æ¨¡å‹åœ¨æ¨ç†æ—¶é¢ä¸´çš„äºŒæ¬¡å¤æ‚åº¦æ³¨æ„åŠ›å’Œ KV-cache å¼€é”€ç“¶é¢ˆã€‚DiffuApriel å°†æ‰©æ•£ç›®æ ‡ä¸çº¿æ€§æ—¶é—´åºåˆ—å»ºæ¨¡ç›¸ç»“åˆï¼Œåœ¨ä¿æŒä¸ Transformer æ¨¡å‹ç›¸å½“æ€§èƒ½çš„å‰æä¸‹ï¼Œä½¿ 1.3B è§„æ¨¡æ¨¡å‹åœ¨é•¿åºåˆ—ä¸Šçš„æ¨ç†ååé‡æå‡äº†é«˜è¾¾ 4.4 å€ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†æ··åˆå˜ä½“ DiffuApriel-Hï¼Œé€šè¿‡äº¤é”™æ’åˆ—æ³¨æ„åŠ›å±‚å’Œ mamba å±‚ï¼Œåœ¨å®ç°äº† 2.6 å€ååé‡æå‡çš„åŒæ—¶å…¼é¡¾äº†å…¨å±€ä¸å±€éƒ¨ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŒå‘çŠ¶æ€ç©ºé—´æ¶æ„ï¼ˆState-space architecturesï¼‰åœ¨æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸­å…·æœ‰æå¼ºçš„å»å™ªèƒ½åŠ›ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ„å»ºæ›´å¿«é€Ÿã€å†…å­˜åˆ©ç”¨ç‡æ›´é«˜çš„å¯æ‰©å±•æ–‡æœ¬ç”ŸæˆåŸºç¡€æ¨¡å‹æä¾›äº†é‡è¦çš„å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.15927v2",
      "published_date": "2025-11-19 23:23:49 UTC",
      "updated_date": "2025-11-23 05:32:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:50:12.769750+00:00"
    },
    {
      "arxiv_id": "2511.15921v1",
      "title": "Thinking, Faithful and Stable: Mitigating Hallucinations in LLMs",
      "title_zh": "æ€è¾¨ã€å¿ å®ä¸ç¨³å¥ï¼šç¼“è§£å¤§è¯­è¨€æ¨¡å‹å¹»è§‰",
      "authors": [
        "Chelsea Zou",
        "Yiheng Yao",
        "Basant Khalil"
      ],
      "abstract": "This project develops a self correcting framework for large language models (LLMs) that detects and mitigates hallucinations during multi-step reasoning. Rather than relying solely on final answer correctness, our approach leverages fine grained uncertainty signals: 1) self-assessed confidence alignment, and 2) token-level entropy spikes to detect unreliable and unfaithful reasoning in real time. We design a composite reward function that penalizes unjustified high confidence and entropy spikes, while encouraging stable and accurate reasoning trajectories. These signals guide a reinforcement learning (RL) policy that makes the model more introspective and shapes the model's generation behavior through confidence-aware reward feedback, improving not just outcome correctness but the coherence and faithfulness of their intermediate reasoning steps. Experiments show that our method improves both final answer accuracy and reasoning calibration, with ablations validating the individual contribution of each signal.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªæˆ‘ä¿®æ­£æ¡†æ¶ï¼Œæ—¨åœ¨å¤šæ­¥æ¨ç†è¿‡ç¨‹ä¸­å®æ—¶æ£€æµ‹å¹¶ç¼“è§£å¹»è§‰ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¾èµ–æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œè¿˜é€šè¿‡ç»“åˆè‡ªæˆ‘è¯„ä¼°çš„ç½®ä¿¡åº¦å¯¹é½(self-assessed confidence alignment)ä¸ä»¤ç‰Œçº§ç†µå€¼æ¿€å¢(token-level entropy spikes)ç­‰ç»†ç²’åº¦ä¸ç¡®å®šæ€§ä¿¡å·ï¼Œè¯†åˆ«ä¸å¯é çš„æ¨ç†è·¯å¾„ã€‚ç ”ç©¶è¿›ä¸€æ­¥è®¾è®¡äº†å¤åˆå¥–åŠ±å‡½æ•°å¹¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ (RL)ç­–ç•¥ï¼Œé€šè¿‡ç½®ä¿¡åº¦æ„ŸçŸ¥çš„å¥–åŠ±åé¦ˆæ¥ä¼˜åŒ–æ¨¡å‹çš„ç”Ÿæˆè¡Œä¸ºï¼Œé¼“åŠ±ç¨³å®šä¸”å‡†ç¡®çš„æ¨ç†è½¨è¿¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…æé«˜äº†æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®ç‡ï¼Œè¿˜æ˜¾è‘—å¢å¼ºäº†æ¨ç†æ ¡å‡†åº¦ä»¥åŠä¸­é—´æ­¥éª¤çš„è¿è´¯æ€§ä¸å¿ å®åº¦(faithfulness)ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Originally released June 5, 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.15921v1",
      "published_date": "2025-11-19 23:09:26 UTC",
      "updated_date": "2025-11-19 23:09:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:50:14.569573+00:00"
    },
    {
      "arxiv_id": "2511.15895v1",
      "title": "Decomposing Theory of Mind: How Emotional Processing Mediates ToM Abilities in LLMs",
      "title_zh": "è§£æå¿ƒç†ç†è®ºï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­æƒ…æ„Ÿå¤„ç†å¯¹å¿ƒç†ç†è®ºèƒ½åŠ›çš„ä¸­ä»‹ä½œç”¨",
      "authors": [
        "Ivan Chulo",
        "Ananya Joshi"
      ],
      "abstract": "Recent work shows activation steering substantially improves language models' Theory of Mind (ToM) (Bortoletto et al. 2024), yet the mechanisms of what changes occur internally that leads to different outputs remains unclear. We propose decomposing ToM in LLMs by comparing steered versus baseline LLMs' activations using linear probes trained on 45 cognitive actions. We applied Contrastive Activation Addition (CAA) steering to Gemma-3-4B and evaluated it on 1,000 BigToM forward belief scenarios (Gandhi et al. 2023), we find improved performance on belief attribution tasks (32.5\\% to 46.7\\% accuracy) is mediated by activations processing emotional content : emotion perception (+2.23), emotion valuing (+2.20), while suppressing analytical processes: questioning (-0.78), convergent thinking (-1.59). This suggests that successful ToM abilities in LLMs are mediated by emotional understanding, not analytical reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­å¿ƒæ™ºç†è®º(Theory of Mind, ToM)çš„åˆ†è§£æœºåˆ¶ï¼Œæ—¨åœ¨æ­ç¤ºæ¿€æ´»å¼•å¯¼(activation steering)å¦‚ä½•æ”¹å˜æ¨¡å‹å†…éƒ¨è¡¨ç¤ºä»¥æå‡æ€§èƒ½ã€‚ç ”ç©¶äººå‘˜å¯¹Gemma-3-4Bæ¨¡å‹åº”ç”¨äº†å¯¹æ¯”æ¿€æ´»å¢å¼º(Contrastive Activation Addition, CAA)æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨é’ˆå¯¹45ç§è®¤çŸ¥è¡Œä¸ºè®­ç»ƒçš„çº¿æ€§æ¢æµ‹å™¨(linear probes)æ¯”è¾ƒå…¶åœ¨BigToMåœºæ™¯ä¸‹çš„æ¿€æ´»å·®å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¿€æ´»å¼•å¯¼å°†æ¨¡å‹åœ¨ä¿¡å¿µå½’å› (belief attribution)ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡ä»32.5%æå‡è‡³46.7%ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ç§æ€§èƒ½æå‡ä¸»è¦ç”±å¤„ç†æƒ…æ„Ÿå†…å®¹(emotional content)çš„æ¿€æ´»æ‰€ä»‹å¯¼ï¼Œå…·ä½“è¡¨ç°ä¸ºæƒ…æ„Ÿæ„ŸçŸ¥(emotion perception)å’Œæƒ…æ„Ÿè¯„ä¼°(emotion valuing)çš„æ˜¾è‘—å¢å¼ºã€‚ä¸æ­¤åŒæ—¶ï¼Œæ¨¡å‹å†…éƒ¨æŠ‘åˆ¶äº†æé—®(questioning)å’Œèšåˆæ€ç»´(convergent thinking)ç­‰åˆ†ææ€§è¿‡ç¨‹(analytical processes)ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼ŒLLMsä¸­æˆåŠŸçš„ToMèƒ½åŠ›ä¸»è¦å—æƒ…æ„Ÿç†è§£(emotional understanding)é©±åŠ¨ï¼Œè€Œéåˆ†ææ¨ç†(analytical reasoning)ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Published at ToM4AI workshop@AAAI2026",
      "pdf_url": "https://arxiv.org/pdf/2511.15895v1",
      "published_date": "2025-11-19 21:56:00 UTC",
      "updated_date": "2025-11-19 21:56:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:50:21.771789+00:00"
    },
    {
      "arxiv_id": "2511.15884v1",
      "title": "Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes",
      "title_zh": "Box6Dï¼šé¢å‘ä»“å‚¨ç®±çš„é›¶æ ·æœ¬ç±»åˆ«çº§ 6D å§¿æ€ä¼°è®¡",
      "authors": [
        "Yintao Ma",
        "Sajjad Pakdamansavoji",
        "Amir Rasouli",
        "Tongtong Cao"
      ],
      "abstract": "Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.\n  To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Box6Dï¼Œä¸€ç§ä¸“é—¨é’ˆå¯¹ä»“åº“å­˜å‚¨ç®±è®¾è®¡çš„Category-level 6D Pose Estimationæ–¹æ³•ï¼Œæ—¨åœ¨æå‡æœºå™¨äººåœ¨ç‰©æµè‡ªåŠ¨åŒ–ä¸­çš„ç‰©ä½“æ“ä½œèƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä»…éœ€å•æ¬¡RGB-Dè§‚æµ‹ï¼Œé€šè¿‡å¿«é€ŸäºŒåˆ†æœç´¢ï¼ˆBinary Searchï¼‰æ¨æ–­ç®±ä½“å°ºå¯¸ï¼Œå¹¶åˆ©ç”¨ç±»åˆ«CADæ¨¡æ¿ä»£æ›¿ç‰¹å®šçš„å®ä¾‹æ¨¡å‹è¿›è¡Œä½å§¿ä¼°è®¡ã€‚Box6Dé€šè¿‡é›†æˆåŸºäºæ·±åº¦çš„åˆç†æ€§è¿‡æ»¤å™¨ï¼ˆPlausibility Filterï¼‰å’Œæ—©åœç­–ç•¥ï¼ˆEarly-stopping Strategyï¼‰æ¥å‰”é™¤æ— æ•ˆå‡è®¾ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†è®¡ç®—æ•ˆç‡ã€‚åœ¨çœŸå®åœºæ™¯å’ŒåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜ç²¾åº¦6D Poseä¼°è®¡çš„åŒæ—¶ï¼Œå°†æ¨ç†æ—¶é—´ç¼©çŸ­äº†çº¦76%ã€‚è¿™ä¸€æˆæœä¸ºå¤æ‚å·¥ä¸šç¯å¢ƒä¸‹å®ç°é«˜æ•ˆã€ç¨³å¥çš„Zero-shotä½å§¿ä¼°è®¡æä¾›äº†å®ç”¨çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15884v1",
      "published_date": "2025-11-19 21:22:08 UTC",
      "updated_date": "2025-11-19 21:22:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:50:17.470743+00:00"
    },
    {
      "arxiv_id": "2511.15874v1",
      "title": "WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion",
      "title_zh": "WALDOï¼šé®æŒ¡åœºæ™¯ä¸‹çš„æœªè§ç›®æ ‡åŸºäºæ¨¡å‹çš„ 6D ä½å§¿ä¼°è®¡",
      "authors": [
        "Sajjad Pakdamansavoji",
        "Yintao Ma",
        "Amir Rasouli",
        "Tongtong Cao"
      ],
      "abstract": "Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººå’Œå¢å¼ºç°å®é¢†åŸŸä¸­ï¼Œæœªè§ç›®æ ‡(unseen objects)åœ¨é®æŒ¡(occlusion)ç¯å¢ƒä¸‹çš„6Dä½å§¿ä¼°è®¡(6D pose estimation)éš¾é¢˜ï¼Œæå‡ºäº†WALDOæ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿå¤šé˜¶æ®µæµæ°´çº¿åœ¨é®æŒ¡æ—¶æ˜“äº§ç”Ÿè¯¯å·®ç´¯ç§¯çš„é—®é¢˜ï¼ŒWALDOå¼•å…¥äº†åŠ¨æ€éå‡åŒ€å¯†é›†é‡‡æ ·ç­–ç•¥(dynamic non-uniform dense sampling)ä»¥èšç„¦å¯è§åŒºåŸŸï¼Œå¹¶åˆ©ç”¨å¤šå‡è®¾æ¨ç†æœºåˆ¶(multi-hypothesis inference)ä¿ç•™å¤šä¸ªä½å§¿å€™é€‰ï¼Œæœ‰æ•ˆé¿å…äº†å•è·¯å¾„å¤±è´¥ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è¿­ä»£ç»†åŒ–(iterative refinement)æŠ€æœ¯å’Œä¸“é—¨é’ˆå¯¹é®æŒ¡çš„è®­ç»ƒå¢å¼ºæ‰‹æ®µï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ã€‚ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå¯è§æ€§åŠ æƒçš„è¯„ä¼°æŒ‡æ ‡(weighted by visibility metric)ï¼Œæ—¨åœ¨æ›´å…¬æ­£åœ°è¡¡é‡é®æŒ¡åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒWALDOåœ¨ICBINå’ŒBOPæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†5%å’Œ2%ä»¥ä¸Šçš„å‡†ç¡®ç‡æå‡ï¼Œä¸”æ¨ç†é€Ÿåº¦æ¯”ç°æœ‰æ–¹æ³•å¿«çº¦3å€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15874v1",
      "published_date": "2025-11-19 21:01:49 UTC",
      "updated_date": "2025-11-19 21:01:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:52:31.259909+00:00"
    },
    {
      "arxiv_id": "2511.15870v1",
      "title": "AquaSentinel: Next-Generation AI System Integrating Sensor Networks for Urban Underground Water Pipeline Anomaly Detection via Collaborative MoE-LLM Agent Architecture",
      "title_zh": "AquaSentinelï¼šåŸºäºåä½œå¼ MoE-LLM æ™ºèƒ½ä½“æ¶æ„ï¼Œèåˆä¼ æ„Ÿå™¨ç½‘ç»œçš„ä¸‹ä¸€ä»£åŸå¸‚åœ°ä¸‹è¾“æ°´ç®¡ç½‘å¼‚å¸¸æ£€æµ‹äººå·¥æ™ºèƒ½ç³»ç»Ÿ",
      "authors": [
        "Qiming Guo",
        "Bishal Khatri",
        "Wenbo Sun",
        "Jinwen Tang",
        "Hua Zhang",
        "Wenlu Wang"
      ],
      "abstract": "Underground pipeline leaks and infiltrations pose significant threats to water security and environmental safety. Traditional manual inspection methods provide limited coverage and delayed response, often missing critical anomalies. This paper proposes AquaSentinel, a novel physics-informed AI system for real-time anomaly detection in urban underground water pipeline networks. We introduce four key innovations: (1) strategic sparse sensor deployment at high-centrality nodes combined with physics-based state augmentation to achieve network-wide observability from minimal infrastructure; (2) the RTCA (Real-Time Cumulative Anomaly) detection algorithm, which employs dual-threshold monitoring with adaptive statistics to distinguish transient fluctuations from genuine anomalies; (3) a Mixture of Experts (MoE) ensemble of spatiotemporal graph neural networks that provides robust predictions by dynamically weighting model contributions; (4) causal flow-based leak localization that traces anomalies upstream to identify source nodes and affected pipe segments. Our system strategically deploys sensors at critical network junctions and leverages physics-based modeling to propagate measurements to unmonitored nodes, creating virtual sensors that enhance data availability across the entire network. Experimental evaluation using 110 leak scenarios demonstrates that AquaSentinel achieves 100% detection accuracy. This work advances pipeline monitoring by demonstrating that physics-informed sparse sensing can match the performance of dense deployments at a fraction of the cost, providing a practical solution for aging urban infrastructure.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AquaSentinelï¼Œä¸€ç§ç‰©ç†ä¿¡æ¯é©±åŠ¨(physics-informed)çš„AIç³»ç»Ÿï¼Œæ—¨åœ¨å®æ—¶ç›‘æµ‹åŸå¸‚åœ°ä¸‹æ°´ç®¡ç½‘çš„å¼‚å¸¸æƒ…å†µï¼Œä»¥è§£å†³ä¼ ç»Ÿæ‰‹åŠ¨æ£€æµ‹è¦†ç›–é¢çª„å’Œå“åº”å»¶è¿Ÿçš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åœ¨ä¸­å¿ƒåº¦è¾ƒé«˜çš„èŠ‚ç‚¹éƒ¨ç½²ç¨€ç–ä¼ æ„Ÿå™¨(sparse sensor deployment)ï¼Œå¹¶ç»“åˆåŸºäºç‰©ç†çš„çŠ¶æ€å¢å¼ºæŠ€æœ¯ï¼Œä»¥æä½çš„æˆæœ¬å®ç°äº†å…¨ç½‘èŒƒå›´çš„å¯è§‚æµ‹æ€§ã€‚åœ¨æ£€æµ‹æœºåˆ¶ä¸Šï¼ŒAquaSentinelå¼•å…¥äº†RTCAæ£€æµ‹ç®—æ³•ï¼Œåˆ©ç”¨åŒé˜ˆå€¼ç›‘æµ‹åŒºåˆ†ç¬æ€æ³¢åŠ¨ä¸çœŸå®å¼‚å¸¸ï¼Œå¹¶é‡‡ç”¨æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œ(spatiotemporal graph neural networks)çš„ä¸“å®¶æ··åˆ(MoE)é›†æˆæ¶æ„æä¾›ç¨³å¥é¢„æµ‹ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿåˆ©ç”¨åŸºäºå› æœæµ(causal flow-based)çš„æ³„æ¼å®šä½æŠ€æœ¯ï¼Œèƒ½å¤Ÿç²¾å‡†å›æº¯å¼‚å¸¸æºå¤´å¹¶è¯†åˆ«å—æŸç®¡æ®µã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨110ä¸ªæ³„æ¼åœºæ™¯ä¸‹ï¼ŒAquaSentinelè¾¾åˆ°äº†100%çš„æ£€æµ‹å‡†ç¡®ç‡ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œç‰©ç†ä¿¡æ¯é©±åŠ¨çš„ç¨€ç–ä¼ æ„Ÿæ–¹æ¡ˆèƒ½å¤ŸåŒ¹é…å¯†é›†éƒ¨ç½²çš„æ€§èƒ½ï¼Œä¸ºåŸå¸‚åŸºç¡€è®¾æ–½ç›‘æµ‹æä¾›äº†é«˜æ•ˆä¸”ç»æµçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "7 pages, 1 figure, 2 tables, Accepted to the 40th AAAI Conference on Artificial Intelligence (AAAI 2026), IAAI Deployed Applications Track",
      "pdf_url": "https://arxiv.org/pdf/2511.15870v1",
      "published_date": "2025-11-19 20:53:50 UTC",
      "updated_date": "2025-11-19 20:53:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:50:58.467458+00:00"
    },
    {
      "arxiv_id": "2511.15857v1",
      "title": "A Crowdsourced Study of ChatBot Influence in Value-Driven Decision Making Scenarios",
      "title_zh": "ä»·å€¼è§‚é©±åŠ¨å†³ç­–åœºæ™¯ä¸‹èŠå¤©æœºå™¨äººå½±å“åŠ›çš„ä¼—åŒ…ç ”ç©¶",
      "authors": [
        "Anthony Wise",
        "Xinyi Zhou",
        "Martin Reimann",
        "Anind Dey",
        "Leilani Battle"
      ],
      "abstract": "Similar to social media bots that shape public opinion, healthcare and financial decisions, LLM-based ChatBots like ChatGPT can persuade users to alter their behavior. Unlike prior work that persuades via overt-partisan bias or misinformation, we test whether framing alone suffices. We conducted a crowdsourced study, where 336 participants interacted with a neutral or one of two value-framed ChatBots while deciding to alter US defense spending. In this single policy domain with controlled content, participants exposed to value-framed ChatBots significantly changed their budget choices relative to the neutral control. When the frame misaligned with their values, some participants reinforced their original preference, revealing a potentially replicable backfire effect, originally considered rare in the literature. These findings suggest that value-framing alone lowers the barrier for manipulative uses of LLMs, revealing risks distinct from overt bias or misinformation, and clarifying risks to countering misinformation.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ä¸€é¡¹åŒ…å«336åå‚ä¸è€…çš„ä¼—åŒ…ç ”ç©¶ï¼Œæ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„èŠå¤©æœºå™¨äºº(ChatBots)åœ¨ä»·å€¼é©±åŠ¨å†³ç­–åœºæ™¯ä¸­çš„å½±å“åŠ›ã€‚å®éªŒè¦æ±‚å‚ä¸è€…åœ¨å†³å®šç¾å›½å›½é˜²é¢„ç®—æ—¶ä¸ä¸­æ€§æˆ–å…·æœ‰ç‰¹å®šä»·å€¼æ¡†æ¶(value-framed)çš„æœºå™¨äººè¿›è¡Œäº¤äº’ï¼Œæ—¨åœ¨æµ‹è¯•å•çº¯çš„æ¡†æ¶æ•ˆåº”(framing)æ˜¯å¦è¶³ä»¥è¯´æœç”¨æˆ·æ”¹å˜è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæš´éœ²äºä»·å€¼æ¡†æ¶æœºå™¨äººç¯å¢ƒä¸‹çš„å‚ä¸è€…ï¼Œå…¶é¢„ç®—é€‰æ‹©è¾ƒä¸­æ€§å¯¹ç…§ç»„å‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œå½“æ¡†æ¶ä¸å‚ä¸è€…ä¸ªäººä»·å€¼è§‚ä¸åŒ¹é…æ—¶ï¼Œéƒ¨åˆ†ç”¨æˆ·ä¼šäº§ç”Ÿå¼ºåŒ–åŸæœ‰åå¥½çš„é€†åæ•ˆåº”(backfire effect)ï¼Œè¿™åœ¨ä»¥å¾€æ–‡çŒ®ä¸­è¢«è®¤ä¸ºæ˜¯è¾ƒä¸ºç½•è§çš„ç°è±¡ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œä»·å€¼æ¡†æ¶çš„å­˜åœ¨é™ä½äº†åˆ©ç”¨LLMè¿›è¡Œæ“çºµçš„é—¨æ§›ï¼Œæ­ç¤ºäº†ä¸åŒäºæ˜¾æ€§åè§æˆ–è™šå‡ä¿¡æ¯çš„ç‹¬ç‰¹é£é™©ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15857v1",
      "published_date": "2025-11-19 20:25:34 UTC",
      "updated_date": "2025-11-19 20:25:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:50:38.270755+00:00"
    },
    {
      "arxiv_id": "2511.15848v2",
      "title": "Step-Audio-R1 Technical Report",
      "title_zh": "Step-Audio-R1 æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Fei Tian",
        "Xiangyu Tony Zhang",
        "Yuxin Zhang",
        "Haoyang Zhang",
        "Yuxin Li",
        "Daijiao Liu",
        "Yayue Deng",
        "Donghang Wu",
        "Jun Chen",
        "Liang Zhao",
        "Chengyuan Yao",
        "Hexin Liu",
        "Eng Siong Chng",
        "Xuerui Yang",
        "Xiangyu Zhang",
        "Daxin Jiang",
        "Gang Yu"
      ],
      "abstract": "Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Step-Audio-R1ï¼Œè¿™æ˜¯é¦–ä¸ªæˆåŠŸè§£é”éŸ³é¢‘é¢†åŸŸæ¨ç†èƒ½åŠ›çš„éŸ³é¢‘æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨é“¾å¼æ€ç»´ (Chain-of-Thought) æ¨ç†ä¸­è¡¨ç°å—é™çš„éš¾é¢˜ã€‚é€šè¿‡æå‡ºçš„æ¨¡æ€è½åœ°æ¨ç†è’¸é¦ (Modality-Grounded Reasoning Distillation, MGRD) æ¡†æ¶ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç”ŸæˆåŸºäºå£°å­¦ç‰¹å¾çš„éŸ³é¢‘æ¨ç†é“¾ï¼Œæœ‰æ•ˆé¿å…äº†è„±ç¦»å®é™…çš„å¹»è§‰ç°è±¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒStep-Audio-R1 åœ¨æ¶µç›–è¯­éŸ³ã€ç¯å¢ƒéŸ³å’ŒéŸ³ä¹çš„ç»¼åˆåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ä»…è¶…è¶Šäº† Gemini 2.5 Proï¼Œè¿˜è¾¾åˆ°äº†ä¸ Gemini 3 Pro ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚è¿™ä¸€æˆæœè¯æ˜äº†æ¨ç†èƒ½åŠ›åœ¨ä¸åŒæ¨¡æ€é—´çš„å¯è¿ç§»æ€§ï¼Œä¸ºæ„å»ºçœŸæ­£å…·å¤‡æ·±åº¦æ€è€ƒèƒ½åŠ›çš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿå¼€è¾Ÿäº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 5 figures. Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2511.15848v2",
      "published_date": "2025-11-19 20:12:50 UTC",
      "updated_date": "2025-11-26 14:55:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:50:45.265447+00:00"
    },
    {
      "arxiv_id": "2511.15846v5",
      "title": "The Loss of Control Playbook: Degrees, Dynamics, and Preparedness",
      "title_zh": "å¤±æ§æ‰‹å†Œï¼šç­‰çº§ã€åŠ¨æ€ä¸é˜²èŒƒ",
      "authors": [
        "Charlotte Stix",
        "Annika Hallensleben",
        "Alejandro Ortega",
        "Matteo Pistillo"
      ],
      "abstract": "This research report addresses the absence of an actionable definition for Loss of Control (LoC) in AI systems by developing a novel taxonomy and preparedness framework. Despite increasing policy and research attention, existing LoC definitions vary significantly in scope and timeline, hindering effective LoC assessment and mitigation. To address this issue, we draw from an extensive literature review and propose a graded LoC taxonomy, based on the metrics of severity and persistence, that distinguishes between Deviation, Bounded LoC, and Strict LoC. We model pathways toward a societal state of vulnerability in which sufficiently advanced AI systems have acquired or could acquire the means to cause Bounded or Strict LoC once a catalyst, either misalignment or pure malfunction, materializes. We argue that this state becomes increasingly likely over time, absent strategic intervention, and propose a strategy to avoid reaching a state of vulnerability. Rather than focusing solely on intervening on AI capabilities and propensities potentially relevant for LoC or on preventing potential catalysts, we introduce a complementary framework that emphasizes three extrinsic factors: Deployment context, Affordances, and Permissions (the DAP framework). Compared to work on intrinsic factors and catalysts, this framework has the unfair advantage of being actionable today. Finally, we put forward a plan to maintain preparedness and prevent the occurrence of LoC outcomes should a state of societal vulnerability be reached, focusing on governance measures (threat modeling, deployment policies, emergency response) and technical controls (pre-deployment testing, control measures, monitoring) that could maintain a condition of perennial suspension.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIç³»ç»Ÿåœ¨æ§åˆ¶æƒä¸§å¤±(Loss of Control, LoC)å®šä¹‰ä¸Šç¼ºä¹å¯æ“ä½œæ€§çš„ç°çŠ¶ï¼Œå¼€å‘äº†ä¸€å¥—å…¨æ–°çš„åˆ†ç±»æ³•å’Œé¢„å¤‡æ¡†æ¶ã€‚ç ”ç©¶åŸºäºä¸¥é‡ç¨‹åº¦(severity)å’ŒæŒç»­æ€§(persistence)ä¸¤ä¸ªæŒ‡æ ‡ï¼Œæå‡ºäº†åˆ†çº§çš„LoCåˆ†ç±»ä½“ç³»ï¼Œå°†å…¶ç»†åˆ†ä¸ºåå·®(Deviation)ã€å—é™æ§åˆ¶æƒä¸§å¤±(Bounded LoC)å’Œä¸¥æ ¼æ§åˆ¶æƒä¸§å¤±(Strict LoC)ã€‚æŠ¥å‘Šæ¨¡æ‹Ÿäº†é€šå‘ç¤¾ä¼šè„†å¼±æ€§çŠ¶æ€çš„æ¼”è¿›è·¯å¾„ï¼ŒæŒ‡å‡ºåœ¨ç¼ºä¹æˆ˜ç•¥å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œå…ˆè¿›AIç³»ç»Ÿå¯èƒ½å› å¤±é…(misalignment)æˆ–çº¯æ•…éšœ(pure malfunction)ç­‰è¯±å› å¼•å‘ä¸¥é‡çš„æ§åˆ¶æƒä¸§å¤±ã€‚ä¸ºäº†è§„é¿è¿™ä¸€é£é™©ï¼Œç ”ç©¶å¼•å…¥äº†å…³æ³¨å¤–éƒ¨å› ç´ çš„DAPæ¡†æ¶ï¼Œå³éƒ¨ç½²ç¯å¢ƒ(Deployment context)ã€èµ‹èƒ½(Affordances)å’Œæƒé™(Permissions)ï¼Œå¹¶å¼ºè°ƒè¯¥æ¡†æ¶åœ¨å½“å‰å…·æœ‰æé«˜çš„å¯æ“ä½œæ€§ã€‚æœ€åï¼Œç ”ç©¶æå‡ºäº†ä¸€å¥—ç»“åˆæ²»ç†æªæ–½ï¼ˆå¦‚å¨èƒå»ºæ¨¡ã€éƒ¨ç½²æ”¿ç­–å’Œåº”æ€¥å“åº”ï¼‰ä¸æŠ€æœ¯æ§åˆ¶ï¼ˆå¦‚é¢„éƒ¨ç½²æµ‹è¯•ã€ç›‘æ§å’Œæ§åˆ¶æªæ–½ï¼‰çš„å‡†å¤‡è®¡åˆ’ï¼Œæ—¨åœ¨å°†LoCé£é™©ç»´æŒåœ¨æ°¸ä¹…æš‚åœ(perennial suspension)çš„å—æ§çŠ¶æ€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15846v5",
      "published_date": "2025-11-19 20:10:39 UTC",
      "updated_date": "2025-12-08 11:10:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:50:43.263859+00:00"
    },
    {
      "arxiv_id": "2511.15833v1",
      "title": "EfficientSAM3: Progressive Hierarchical Distillation for Video Concept Segmentation from SAM1, 2, and 3",
      "title_zh": "EfficientSAM3ï¼šåŸºäº SAM1ã€2 å’Œ 3 çš„è§†é¢‘æ¦‚å¿µåˆ†å‰²æ¸è¿›å¼å±‚æ¬¡åŒ–è’¸é¦",
      "authors": [
        "Chengxi Zeng",
        "Yuxuan Jiang",
        "Aaron Zhang"
      ],
      "abstract": "The Segment Anything Model 3 (SAM3) advances visual understanding with Promptable Concept Segmentation (PCS) across images and videos, but its unified architecture (shared vision backbone, DETR-style detector, dense-memory tracker) remains prohibitive for on-device use. We present EfficientSAM3, a family of efficient models built on Progressive Hierarchical Distillation (PHD) that transfers capability from SAM3 to lightweight students in three stages: (1) Encoder Distillation aligns image features via prompt-in-the-loop training on SA-1B; (2) Temporal Memory Distillation replaces dense memory with a compact Perceiver-based module trained on SA-V to compress and retrieve spatiotemporal features efficiently; and (3) End-to-End Fine-Tuning refines the full pipeline on the official SAM3 PCS data to preserve concept-level performance. PHD yields a spectrum of student variants using RepViT, TinyViT, and EfficientViT backbones, enabling on-device concept segmentation and tracking while maintaining high fidelity to teacher behavior. We benchmark on popular VOS datasets, and compare with varies of releated work, achieing strong performance-efficiency trade-offs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EfficientSAM3ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åˆ©ç”¨æ¸è¿›å¼åˆ†å±‚è’¸é¦ (Progressive Hierarchical Distillation, PHD) æŠ€æœ¯æ„å»ºçš„é«˜æ•ˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ Segment Anything Model 3 (SAM3) æ¶æ„è¿‡äºåºå¤§è€Œéš¾ä»¥åœ¨ç§»åŠ¨ç«¯éƒ¨ç½²çš„é—®é¢˜ã€‚PHD æ¡†æ¶é€šè¿‡ä¸‰ä¸ªé˜¶æ®µå°†èƒ½åŠ›è¿ç§»è‡³å­¦ç”Ÿæ¨¡å‹ï¼šé¦–å…ˆæ˜¯ç¼–ç å™¨è’¸é¦ (Encoder Distillation) ä»¥å¯¹é½å›¾åƒç‰¹å¾ï¼Œå…¶æ¬¡æ˜¯åˆ©ç”¨ç´§å‡‘çš„ Perceiver æ¨¡å—æ›¿æ¢å¯†é›†è®°å¿†çš„æ—¶é—´è®°å¿†è’¸é¦ (Temporal Memory Distillation) ä»¥é«˜æ•ˆå¤„ç†æ—¶ç©ºç‰¹å¾ï¼Œæœ€åé€šè¿‡ç«¯åˆ°ç«¯å¾®è°ƒ (End-to-End Fine-Tuning) ä¿æŒæ¦‚å¿µçº§æ€§èƒ½ã€‚å®éªŒé‡‡ç”¨äº† RepViTã€TinyViT å’Œ EfficientViT ç­‰è½»é‡çº§éª¨å¹²ç½‘ç»œï¼ŒæˆåŠŸå®ç°äº†ç«¯åˆ°ç«¯çš„å¯æç¤ºæ¦‚å¿µåˆ†å‰² (Promptable Concept Segmentation, PCS) ä¸è·Ÿè¸ªã€‚æµ‹è¯„ç»“æœæ˜¾ç¤ºï¼ŒEfficientSAM3 åœ¨å¤šä¸ª VOS æ•°æ®é›†ä¸Šå®ç°äº†æä½³çš„æ€§èƒ½ä¸æ•ˆç‡å¹³è¡¡ï¼Œä¸ºç§»åŠ¨ç«¯è§†è§‰ç†è§£æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Github: https://github.com/SimonZeng7108/efficientsam3",
      "pdf_url": "https://arxiv.org/pdf/2511.15833v1",
      "published_date": "2025-11-19 19:42:22 UTC",
      "updated_date": "2025-11-19 19:42:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:53:01.172951+00:00"
    },
    {
      "arxiv_id": "2511.15830v1",
      "title": "Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions",
      "title_zh": "Mini Amusement Parks (MAPs)ï¼šå•†ä¸šå†³ç­–å»ºæ¨¡è¯•éªŒå¹³å°",
      "authors": [
        "StÃ©phane Aroca-Ouellette",
        "Ian Berlot-Attwell",
        "Panagiotis Lymperopoulos",
        "Abhiramon Rajasekharan",
        "Tongqi Zhu",
        "Herin Kang",
        "Kaheer Suleman",
        "Sam Pasupalak"
      ],
      "abstract": "Despite rapid progress in artificial intelligence, current systems struggle with the interconnected challenges that define real-world decision making. Practical domains, such as business management, require optimizing an open-ended and multi-faceted objective, actively learning environment dynamics from sparse experience, planning over long horizons in stochastic settings, and reasoning over spatial information. Yet existing human--AI benchmarks isolate subsets of these capabilities, limiting our ability to assess holistic decision-making competence. We introduce Mini Amusement Parks (MAPs), an amusement-park simulator designed to evaluate an agent's ability to model its environment, anticipate long-term consequences under uncertainty, and strategically operate a complex business. We provide human baselines and a comprehensive evaluation of state-of-the-art LLM agents, finding that humans outperform these systems by 6.5x on easy mode and 9.8x on medium mode. Our analysis reveals persistent weaknesses in long-horizon optimization, sample-efficient learning, spatial reasoning, and world modelling. By unifying these challenges within a single environment, MAPs offers a new foundation for benchmarking agents capable of adaptable decision making. Code: https://github.com/Skyfall-Research/MAPs",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Mini Amusement Parks (MAPs)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“å»ºæ¨¡ç¯å¢ƒã€åœ¨ä¸ç¡®å®šæ€§ä¸‹é¢„æµ‹é•¿æœŸåæœä»¥åŠæˆ˜ç•¥æ€§è¿è¥å¤æ‚ä¸šåŠ¡èƒ½åŠ›çš„æ¸¸ä¹åœºæ¨¡æ‹Ÿå™¨ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•å¾€å¾€å­¤ç«‹ç‰¹å®šèƒ½åŠ›çš„å±€é™æ€§ï¼ŒMAPs ç»Ÿä¸€äº†é•¿ç¨‹è§„åˆ’ (long-horizon optimization)ã€æ ·æœ¬é«˜æ•ˆå­¦ä¹  (sample-efficient learning)ã€ç©ºé—´æ¨ç† (spatial reasoning) å’Œä¸–ç•Œå»ºæ¨¡ (world modelling) ç­‰ç°å®ä¸–ç•Œå†³ç­–ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡å°†äººç±»åŸºå‡†ä¸å½“å‰æœ€å…ˆè¿›çš„ LLM æ™ºèƒ½ä½“è¿›è¡Œå…¨é¢è¯„ä¼°å¯¹æ¯”ï¼Œç ”ç©¶å‘ç°äººç±»åœ¨ç®€å•æ¨¡å¼ä¸‹çš„è¡¨ç°ä¼˜äºç³»ç»Ÿ 6.5 å€ï¼Œåœ¨ä¸­ç­‰æ¨¡å¼ä¸‹åˆ™è¾¾åˆ° 9.8 å€ã€‚å®éªŒåˆ†ææ­ç¤ºäº†ç°æœ‰ AI åœ¨å¤„ç†å¤šé¢ç›®æ ‡ä¼˜åŒ–å’Œéšæœºç¯å¢ƒåŠ¨æ€æ–¹é¢çš„æŒç»­æ€§å¼±ç‚¹ï¼Œä¸ºå¼€å‘å…·å¤‡é€‚åº”æ€§å†³ç­–èƒ½åŠ›çš„æ™ºèƒ½ä½“å¥ å®šäº†æ–°çš„åŸºå‡†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages (main paper)",
      "pdf_url": "https://arxiv.org/pdf/2511.15830v1",
      "published_date": "2025-11-19 19:38:05 UTC",
      "updated_date": "2025-11-19 19:38:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:53:17.969037+00:00"
    },
    {
      "arxiv_id": "2511.15825v1",
      "title": "IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest X-Ray Interpretation",
      "title_zh": "IMACT-CXRï¼šé¢å‘èƒ¸éƒ¨ X å°„çº¿åˆ¤è¯»çš„äº¤äº’å¼å¤šæ™ºèƒ½ä½“å¯¹è¯å¼æ•™å­¦ç³»ç»Ÿ",
      "authors": [
        "Tuan-Anh Le",
        "Anh Mai Vu",
        "David Yang",
        "Akash Awasthi",
        "Hien Van Nguyen"
      ],
      "abstract": "IMACT-CXR is an interactive multi-agent conversational tutor that helps trainees interpret chest X-rays by unifying spatial annotation, gaze analysis, knowledge retrieval, and image-grounded reasoning in a single AutoGen-based workflow. The tutor simultaneously ingests learner bounding boxes, gaze samples, and free-text observations. Specialized agents evaluate localization quality, generate Socratic coaching, retrieve PubMed evidence, suggest similar cases from REFLACX, and trigger NV-Reason-CXR-3B for vision-language reasoning when mastery remains low or the learner explicitly asks. Bayesian Knowledge Tracing (BKT) maintains skill-specific mastery estimates that drive both knowledge reinforcement and case similarity retrieval. A lung-lobe segmentation module derived from a TensorFlow U-Net enables anatomically aware gaze feedback, and safety prompts prevent premature disclosure of ground-truth labels. We describe the system architecture, implementation highlights, and integration with the REFLACX dataset for real DICOM cases. IMACT-CXR demonstrates responsive tutoring flows with bounded latency, precise control over answer leakage, and extensibility toward live residency deployment. Preliminary evaluation shows improved localization and diagnostic reasoning compared to baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†IMACT-CXRï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºAutoGenæ¡†æ¶çš„äº¤äº’å¼å¤šæ™ºèƒ½ä½“å¯¹è¯æ•™å­¦ç³»ç»Ÿï¼Œæ—¨åœ¨è¾…åŠ©å—è®­äººå‘˜è¿›è¡ŒChest X-Rayçš„è§£è¯»ã€‚è¯¥ç³»ç»Ÿæ•´åˆäº†ç©ºé—´æ ‡æ³¨(Spatial Annotation)ã€è§†çº¿åˆ†æ(Gaze Analysis)ã€çŸ¥è¯†æ£€ç´¢å’Œå›¾åƒè¾…åŠ©æ¨ç†(Image-grounded Reasoning)ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å­¦ä¹ è€…çš„è¾¹ç•Œæ¡†è¾“å…¥ã€è§†çº¿æ ·æœ¬å’Œè‡ªç”±æ–‡æœ¬è§‚å¯Ÿã€‚ç³»ç»Ÿé€šè¿‡ä¸“é—¨çš„æ™ºèƒ½ä½“è¯„ä¼°å®šä½è´¨é‡ï¼Œå¹¶ç»“åˆSocratic Coachingã€PubMedè¯æ®æ£€ç´¢ä»¥åŠæ¥è‡ªREFLACXæ•°æ®é›†çš„ç›¸ä¼¼æ¡ˆä¾‹å»ºè®®æ¥å¼•å¯¼å­¦ä¹ ã€‚å½“å­¦ä¹ è€…æŒæ¡åº¦è¾ƒä½æ—¶ï¼Œç³»ç»Ÿä¼šè§¦å‘NV-Reason-CXR-3Bè¿›è¡Œè§†è§‰è¯­è¨€æ¨ç†ï¼Œå¹¶åˆ©ç”¨Bayesian Knowledge Tracing (BKT)å®æ—¶ç»´æŠ¤æŠ€èƒ½æŒæ¡è¯„ä¼°ä»¥é©±åŠ¨çŸ¥è¯†å¼ºåŒ–ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜é›†æˆäº†åŸºäºTensorFlow U-Netçš„è‚ºå¶åˆ†å‰²æ¨¡å—ä»¥å®ç°å…·å¤‡è§£å‰–æ„ŸçŸ¥çš„è§†çº¿åé¦ˆï¼Œå¹¶é€šè¿‡Safety Promptsé˜²æ­¢æ ‡å‡†ç­”æ¡ˆæå‰æ³„éœ²ã€‚åˆæ­¥è¯„ä¼°æ˜¾ç¤ºï¼ŒIMACT-CXRåœ¨æå‡å®šä½å‡†ç¡®åº¦å’Œè¯Šæ–­æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä¸ºä¸´åºŠä½é™¢åŒ»å¸ˆåŸ¹è®­æä¾›äº†å…·æœ‰ä½å»¶è¿Ÿå’Œé«˜æ‰©å±•æ€§çš„è¾…åŠ©æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15825v1",
      "published_date": "2025-11-19 19:32:29 UTC",
      "updated_date": "2025-11-19 19:32:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:53:18.766065+00:00"
    },
    {
      "arxiv_id": "2511.15807v1",
      "title": "TopoReformer: Mitigating Adversarial Attacks Using Topological Purification in OCR Models",
      "title_zh": "TopoReformerï¼šåˆ©ç”¨æ‹“æ‰‘å‡€åŒ–ç¼“è§£ OCR æ¨¡å‹ä¸­çš„å¯¹æŠ—æ”»å‡»",
      "authors": [
        "Bhagyesh Kumar",
        "A S Aravinthakashan",
        "Akshat Satyanarayan",
        "Ishaan Gakhar",
        "Ujjwal Verma"
      ],
      "abstract": "Adversarially perturbed images of text can cause sophisticated OCR systems to produce misleading or incorrect transcriptions from seemingly invisible changes to humans. Some of these perturbations even survive physical capture, posing security risks to high-stakes applications such as document processing, license plate recognition, and automated compliance systems. Existing defenses, such as adversarial training, input preprocessing, or post-recognition correction, are often model-specific, computationally expensive, and affect performance on unperturbed inputs while remaining vulnerable to unseen or adaptive attacks. To address these challenges, TopoReformer is introduced, a model-agnostic reformation pipeline that mitigates adversarial perturbations while preserving the structural integrity of text images. Topology studies properties of shapes and spaces that remain unchanged under continuous deformations, focusing on global structures such as connectivity, holes, and loops rather than exact distance. Leveraging these topological features, TopoReformer employs a topological autoencoder to enforce manifold-level consistency in latent space and improve robustness without explicit gradient regularization. The proposed method is benchmarked on EMNIST, MNIST, against standard adversarial attacks (FGSM, PGD, Carlini-Wagner), adaptive attacks (EOT, BDPA), and an OCR-specific watermark attack (FAWA).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹OCRç³»ç»Ÿæ˜“å—å¯¹æŠ—æ”»å‡»ï¼ˆAdversarial Attacksï¼‰å¯¼è‡´é”™è¯¯è½¬å½•çš„é—®é¢˜ï¼Œæå‡ºäº†TopoReformerï¼Œä¸€ç§æ¨¡å‹æ— å…³çš„é‡æ„æµæ°´çº¿ï¼Œæ—¨åœ¨é€šè¿‡æ‹“æ‰‘å‡€åŒ–ï¼ˆTopological Purificationï¼‰æ¶ˆé™¤å¯¹æŠ—æ‰°åŠ¨å¹¶ä¿ç•™æ–‡æœ¬å›¾åƒçš„ç»“æ„å®Œæ•´æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‹“æ‰‘å­¦ä¸­å½¢çŠ¶åœ¨è¿ç»­å˜å½¢ä¸‹ä¿æŒä¸å˜çš„ç‰¹æ€§ï¼Œèšç„¦äºè¿é€šæ€§ã€å­”æ´å’Œç¯è·¯ç­‰å…¨å±€ç»“æ„ã€‚TopoReformeré‡‡ç”¨äº†æ‹“æ‰‘è‡ªç¼–ç å™¨ï¼ˆTopological Autoencoderï¼‰åœ¨æ½œç©ºé—´ï¼ˆLatent Spaceï¼‰ä¸­å¼ºåˆ¶æ‰§è¡Œæµå½¢çº§ä¸€è‡´æ€§ï¼Œä»è€Œåœ¨ä¸ä¾èµ–æ˜¾å¼æ¢¯åº¦æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹æå‡æ¨¡å‹é²æ£’æ€§ã€‚ç ”ç©¶åœ¨EMNISTå’ŒMNISTæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒéªŒè¯äº†å…¶é’ˆå¯¹FGSMã€PGDã€Carlini-Wagnerç­‰æ ‡å‡†æ”»å‡»ä»¥åŠEOTã€BDPAå’ŒOCRä¸“ç”¨æ°´å°æ”»å‡»ï¼ˆFAWAï¼‰çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¯æ˜TopoReformerèƒ½å¤Ÿåº”å¯¹ä¸å¯è§æˆ–è‡ªé€‚åº”æ”»å‡»ï¼Œä¸ºæ–‡æ¡£å¤„ç†ã€è½¦ç‰Œè¯†åˆ«ç­‰é«˜é£é™©åº”ç”¨æä¾›äº†æ›´å®‰å…¨çš„ä¿éšœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at AAAI 2026 AI for CyberSecurity (AICS) Workshop",
      "pdf_url": "https://arxiv.org/pdf/2511.15807v1",
      "published_date": "2025-11-19 19:01:51 UTC",
      "updated_date": "2025-11-19 19:01:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:53:04.769854+00:00"
    },
    {
      "arxiv_id": "2511.21722v1",
      "title": "German General Personas: A Survey-Derived Persona Prompt Collection for Population-Aligned LLM Studies",
      "title_zh": "German General Personasï¼šé¢å‘äººç¾¤å¯¹é½å¤§è¯­è¨€æ¨¡å‹ç ”ç©¶çš„è°ƒæŸ¥é©±åŠ¨å‹ç”»åƒæç¤ºè¯åº“",
      "authors": [
        "Jens Rupprecht",
        "Leon FrÃ¶hling",
        "Claudia Wagner",
        "Markus Strohmaier"
      ],
      "abstract": "The use of Large Language Models (LLMs) for simulating human perspectives via persona prompting is gaining traction in computational social science. However, well-curated, empirically grounded persona collections remain scarce, limiting the accuracy and representativeness of such simulations. Here we introduce the German General Personas (GGP) collection, a comprehensive and representative persona prompt collection built from the German General Social Survey (ALLBUS). The GGP and its persona prompts are designed to be easily plugged into prompts for all types of LLMs and tasks, steering models to generate responses aligned with the underlying German population. We evaluate GGP by prompting various LLMs to simulate survey response distributions across diverse topics, demonstrating that GGP-guided LLMs outperform state-of-the-art classifiers, particularly under data scarcity. Furthermore, we analyze how the representativity and attribute selection within persona prompts affect alignment with population responses. Our findings suggest that GGP provides a potentially valuable resource for research on LLM-based social simulations that enables more systematic explorations of population-aligned persona prompting in NLP and social science research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è®¡ç®—ç¤¾ä¼šç§‘å­¦ä¸­ç¼ºä¹å®è¯åŸºç¡€çš„äººæ ¼æç¤ºï¼ˆpersona promptingï¼‰é›†åˆè¿™ä¸€ç°çŠ¶ï¼Œæå‡ºäº†German General Personas (GGP) é›†åˆï¼Œè¿™æ˜¯ä¸€å¥—åŸºäºå¾·å›½ä¸€èˆ¬ç¤¾ä¼šè°ƒæŸ¥(ALLBUS)æ„å»ºçš„å…¨é¢ä¸”å…·æœ‰ä»£è¡¨æ€§çš„äººæ ¼æç¤ºé›†åˆã€‚GGP æ—¨åœ¨æ— ç¼é›†æˆåˆ°å„ç±»å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œä»»åŠ¡ä¸­ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆä¸å¾·å›½çœŸå®äººå£åˆ†å¸ƒç›¸ä¸€è‡´çš„å“åº”ã€‚é€šè¿‡æ¨¡æ‹Ÿå¤šç§è¯é¢˜çš„è°ƒæŸ¥å“åº”åˆ†å¸ƒï¼Œå®éªŒè¯æ˜å— GGP å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºæœ€å…ˆè¿›çš„åˆ†ç±»å™¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹è¡¨ç°å°¤ä¸ºçªå‡ºã€‚ç ”ç©¶è¿˜æ·±å…¥åˆ†æäº†äººæ ¼æç¤ºä¸­çš„ä»£è¡¨æ€§å’Œå±æ€§é€‰æ‹©å¦‚ä½•å½±å“æ¨¡å‹ä¸äººå£å“åº”çš„å¯¹é½æ•ˆæœã€‚è¯¥æˆæœä¸ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¤¾ä¼šæ¨¡æ‹Ÿç ”ç©¶æä¾›äº†é‡è¦èµ„æºï¼Œæ¨åŠ¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œç¤¾ä¼šç§‘å­¦é¢†åŸŸä¸­ç¾¤ä½“å¯¹é½æŠ€æœ¯çš„ç³»ç»Ÿæ€§æ¢ç´¢ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.21722v1",
      "published_date": "2025-11-19 19:01:02 UTC",
      "updated_date": "2025-11-19 19:01:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:54:34.866156+00:00"
    },
    {
      "arxiv_id": "2511.15704v1",
      "title": "In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data",
      "title_zh": "In-N-Onï¼šåˆ©ç”¨è‡ªç„¶åœºæ™¯ä¸ä»»åŠ¡ç›¸å…³æ•°æ®æ‰©å±•ç¬¬ä¸€è§†è§’æ“ä½œèƒ½åŠ›",
      "authors": [
        "Xiongyi Cai",
        "Ri-Zhao Qiu",
        "Geng Chen",
        "Lai Wei",
        "Isabella Liu",
        "Tianshu Huang",
        "Xuxin Cheng",
        "Xiaolong Wang"
      ],
      "abstract": "Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† In-N-On æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”±äºæ•°æ®å¼‚æ„æ€§å¯¼è‡´çš„ç¬¬ä¸€äººç§°è§†è§’ (Egocentric) è§†é¢‘åœ¨å­¦ä¹ æ“ä½œç­–ç•¥æ—¶ä»…è¢«ç”¨äºç®€å•é¢„è®­ç»ƒã€æœªèƒ½å……åˆ†å‘æŒ¥å…¶æ½œåŠ›çš„é—®é¢˜ã€‚è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ¡ˆï¼Œé€šè¿‡å°†äººç±»æ•°æ®ç³»ç»Ÿæ€§åœ°åˆ’åˆ†ä¸ºé‡å¤–æ•°æ® (in-the-wild) å’Œä»»åŠ¡ç›¸å…³æ•°æ® (on-task)ï¼Œå¹¶é…åˆæ·±å…¥çš„åˆ†ææŒ‡å¯¼æ•°æ®ä½¿ç”¨ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº† PHSD æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡ 1000 å°æ—¶çš„å¤šæ ·åŒ–é‡å¤–æ•°æ®å’Œè¶…è¿‡ 20 å°æ—¶ä¸ç›®æ ‡æ“ä½œä»»åŠ¡ç›´æ¥å¯¹é½çš„ä»»åŠ¡ç›¸å…³æ•°æ®ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œç ”ç©¶è€…è®­ç»ƒäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„ç¬¬ä¸€äººç§°è¯­è¨€æ¡ä»¶æµåŒ¹é…ç­–ç•¥ (flow matching policy) æ¨¡å‹ Human0ã€‚é€šè¿‡åº”ç”¨é¢†åŸŸè‡ªé€‚åº” (domain adaptation) æŠ€æœ¯ï¼ŒHuman0 æœ‰æ•ˆç¼©å°äº†äººç±»ä¸äººå‹æœºå™¨äºº (humanoids) ä¹‹é—´çš„è¡¨ç°å·®è·ã€‚å®éªŒè¯æ˜ï¼ŒHuman0 å±•ç°äº†é€šè¿‡æ‰©å±•äººç±»æ•°æ®è·å¾—çš„å¤šç§ä¼˜è¶Šå±æ€§ï¼ŒåŒ…æ‹¬ä»…é€šè¿‡äººç±»æ•°æ®å®ç°è¯­è¨€æŒ‡ä»¤éµå¾ªã€å°‘æ ·æœ¬å­¦ä¹  (few-shot learning) ä»¥åŠåˆ©ç”¨ä»»åŠ¡ç›¸å…³æ•°æ®æ˜¾è‘—å¢å¼ºçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Project webpage: https://xiongyicai.github.io/In-N-On/",
      "pdf_url": "https://arxiv.org/pdf/2511.15704v1",
      "published_date": "2025-11-19 18:59:04 UTC",
      "updated_date": "2025-11-19 18:59:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:53:36.962535+00:00"
    },
    {
      "arxiv_id": "2511.15703v2",
      "title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC",
      "title_zh": "è§†è§‰æ€è€ƒï¼Œæ–‡æœ¬æ¨ç†ï¼šARC ä»»åŠ¡ä¸­çš„è§†è§‰-è¯­è¨€ååŒ",
      "authors": [
        "Beichen Zhang",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Haodong Duan",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "abstract": "Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33\\% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code is released at https://github.com/InternLM/ARC-VL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŠ½è±¡æ¨ç†è¯­æ–™åº“(ARC-AGI)ä¸­æ¨¡å‹éš¾ä»¥ä»æç®€æ ·æœ¬æ¨æ–­ç»“æ„åŒ–å˜æ¢è§„åˆ™çš„é—®é¢˜ï¼Œæ¢è®¨äº†è§†è§‰ä¸è¯­è¨€åœ¨ä¸åŒæ¨ç†é˜¶æ®µçš„äº’è¡¥ä¼˜åŠ¿ã€‚ä½œè€…æŒ‡å‡ºè§†è§‰æ“…é•¿å…¨å±€æ¨¡å¼æŠ½è±¡ä¸éªŒè¯ï¼Œè€Œè¯­è¨€åˆ™åœ¨ç¬¦å·è§„åˆ™åˆ¶å®šä¸ç²¾ç¡®æ‰§è¡Œæ–¹é¢è¡¨ç°æ›´ä½³ï¼Œå¹¶æ®æ­¤æå‡ºäº†è§†è§‰-è¯­è¨€ååŒæ¨ç†(Vision-Language Synergy Reasoning, VLSR)å’Œæ¨¡æ€åˆ‡æ¢è‡ªçº é”™(Modality-Switch Self-Correction, MSSC)ä¸¤ç§ç­–ç•¥ã€‚VLSRé€šè¿‡å°†ä»»åŠ¡åˆ†è§£ä¸ºæ¨¡æ€å¯¹é½çš„å­ä»»åŠ¡æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œè€ŒMSSCåˆ™åˆ©ç”¨è§†è§‰åé¦ˆå¯¹æ–‡æœ¬æ¨ç†ç»“æœè¿›è¡Œå†…åœ¨çº é”™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ——èˆ°æ¨¡å‹ä¸Šç›¸æ¯”çº¯æ–‡æœ¬åŸºçº¿æå‡äº†é«˜è¾¾4.33%çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œå°†è§†è§‰æŠ½è±¡ä¸è¯­è¨€æ¨ç†ç›¸ç»“åˆæ˜¯å®ç°ç±»äººé€šç”¨æ™ºèƒ½çš„åŸºç¡€æ¨¡å‹æ¼”è¿›ä¸­çš„å…³é”®æ­¥éª¤ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15703v2",
      "published_date": "2025-11-19 18:59:04 UTC",
      "updated_date": "2025-11-26 12:23:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:53:56.577613+00:00"
    },
    {
      "arxiv_id": "2511.15699v1",
      "title": "Joint Semantic-Channel Coding and Modulation for Token Communications",
      "title_zh": "é¢å‘ Token é€šä¿¡çš„è”åˆè¯­ä¹‰ä¿¡é“ç¼–ç ä¸è°ƒåˆ¶",
      "authors": [
        "Jingkai Ying",
        "Zhijin Qin",
        "Yulong Feng",
        "Liejun Wang",
        "Xiaoming Tao"
      ],
      "abstract": "In recent years, the Transformer architecture has achieved outstanding performance across a wide range of tasks and modalities. Token is the unified input and output representation in Transformer-based models, which has become a fundamental information unit. In this work, we consider the problem of token communication, studying how to transmit tokens efficiently and reliably. Point cloud, a prevailing three-dimensional format which exhibits a more complex spatial structure compared to image or video, is chosen to be the information source. We utilize the set abstraction method to obtain point tokens. Subsequently, to get a more informative and transmission-friendly representation based on tokens, we propose a joint semantic-channel and modulation (JSCCM) scheme for the token encoder, mapping point tokens to standard digital constellation points (modulated tokens). Specifically, the JSCCM consists of two parallel Point Transformer-based encoders and a differential modulator which combines the Gumel-softmax and soft quantization methods. Besides, the rate allocator and channel adapter are developed, facilitating adaptive generation of high-quality modulated tokens conditioned on both semantic information and channel conditions. Extensive simulations demonstrate that the proposed method outperforms both joint semantic-channel coding and traditional separate coding, achieving over 1dB gain in reconstruction and more than 6x compression ratio in modulated symbols.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Transformeræ¨¡å‹ä¸­Tokenä½œä¸ºåŸºæœ¬ä¿¡æ¯å•å…ƒçš„ä¼ è¾“éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§é¢å‘Tokené€šä¿¡çš„è”åˆè¯­ä¹‰ä¿¡é“ç¼–ç ä¸è°ƒåˆ¶ï¼ˆJSCCMï¼‰æ–¹æ¡ˆï¼Œå¹¶ä»¥ç©ºé—´ç»“æ„å¤æ‚çš„ç‚¹äº‘ï¼ˆPoint cloudï¼‰ä½œä¸ºä¸»è¦å®éªŒå¯¹è±¡ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªå¹¶è¡Œçš„Point Transformerç¼–ç å™¨å°†point tokensæ˜ å°„ä¸ºæ ‡å‡†æ•°å­—æ˜Ÿåº§ç‚¹ï¼Œå¹¶ç»“åˆGumbel-softmaxä¸è½¯é‡åŒ–ï¼ˆsoft quantizationï¼‰æŠ€æœ¯å®ç°äº†å·®åˆ†è°ƒåˆ¶å™¨ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿå†…ç½®çš„é€Ÿç‡åˆ†é…å™¨ï¼ˆrate allocatorï¼‰å’Œä¿¡é“é€‚é…å™¨ï¼ˆchannel adapterï¼‰èƒ½å¤Ÿæ ¹æ®è¯­ä¹‰ç‰¹å¾å’Œä¿¡é“çŠ¶å†µè‡ªé€‚åº”åœ°ç”Ÿæˆé«˜è´¨é‡è°ƒåˆ¶ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡æ„ç²¾åº¦ä¸Šæ¯”ä¼ ç»Ÿåˆ†ç¦»ç¼–ç åŠç°æœ‰è”åˆç¼–ç æ–¹æ¡ˆæå‡äº†1dBä»¥ä¸Šï¼ŒåŒæ—¶å®ç°äº†è¶…è¿‡6å€çš„è°ƒåˆ¶ç¬¦å·å‹ç¼©ç‡ï¼Œæ˜¾è‘—æé«˜äº†ä¸‰ç»´æ•°æ®ä¼ è¾“çš„æ•ˆç‡ä¸å¯é æ€§ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "14 pages, 14 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.15699v1",
      "published_date": "2025-11-19 18:56:32 UTC",
      "updated_date": "2025-11-19 18:56:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:54:09.962721+00:00"
    },
    {
      "arxiv_id": "2511.15778v1",
      "title": "Balancing Natural Language Processing Accuracy and Normalisation in Extracting Medical Insights",
      "title_zh": "æƒè¡¡åŒ»å­¦ä¿¡æ¯æå–ä¸­è‡ªç„¶è¯­è¨€å¤„ç†çš„å‡†ç¡®æ€§ä¸è§„èŒƒåŒ–",
      "authors": [
        "Paulina Tworek",
        "MiÅ‚osz BargieÅ‚",
        "Yousef Khan",
        "Tomasz PeÅ‚ech-Pilichowski",
        "Marek MikoÅ‚ajczyk",
        "Roman Lewandowski",
        "Jose Sousa"
      ],
      "abstract": "Extracting structured medical insights from unstructured clinical text using Natural Language Processing (NLP) remains an open challenge in healthcare, particularly in non-English contexts where resources are scarce. This study presents a comparative analysis of NLP low-compute rule-based methods and Large Language Models (LLMs) for information extraction from electronic health records (EHR) obtained from the Voivodeship Rehabilitation Hospital for Children in Ameryka, Poland. We evaluate both approaches by extracting patient demographics, clinical findings, and prescribed medications while examining the effects of lack of text normalisation and translation-induced information loss. Results demonstrate that rule-based methods provide higher accuracy in information retrieval tasks, particularly for age and sex extraction. However, LLMs offer greater adaptability and scalability, excelling in drug name recognition. The effectiveness of the LLMs was compared with texts originally in Polish and those translated into English, assessing the impact of translation. These findings highlight the trade-offs between accuracy, normalisation, and computational cost when deploying NLP in healthcare settings. We argue for hybrid approaches that combine the precision of rule-based systems with the adaptability of LLMs, offering a practical path toward more reliable and resource-efficient clinical NLP in real-world hospitals.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨åŒ»ç–—èµ„æºæœ‰é™çš„éè‹±è¯­ç¯å¢ƒä¸‹ï¼Œå¦‚ä½•åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç† (NLP) æŠ€æœ¯ä»éç»“æ„åŒ–ä¸´åºŠæ–‡æœ¬ä¸­æå–ç»“æ„åŒ–åŒ»ç–—ä¿¡æ¯ã€‚ç ”ç©¶å¯¹æ¯”åˆ†æäº†åŸºäºè§„åˆ™çš„ä½è®¡ç®—é‡æ–¹æ³• (rule-based methods) ä¸å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†æ³¢å…°å„¿ç«¥åº·å¤åŒ»é™¢ç”µå­å¥åº·è®°å½• (EHR) æ—¶çš„è¡¨ç°ã€‚è¯„ä¼°é‡ç‚¹åœ¨äºæå–æ‚£è€…äººå£ç»Ÿè®¡å­¦ã€ä¸´åºŠå‘ç°å’Œå¤„æ–¹è¯ç‰©ï¼Œå¹¶æ·±å…¥åˆ†æäº†æ–‡æœ¬æ ‡å‡†åŒ– (normalisation) ç¼ºå¤±ä»¥åŠç¿»è¯‘å¯¼è‡´çš„ä¿¡æ¯æŸå¤±å¯¹æå–æ•ˆæœçš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºè§„åˆ™çš„æ–¹æ³•åœ¨ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­å‡†ç¡®ç‡æ›´é«˜ï¼Œå°¤å…¶åœ¨æå–å¹´é¾„ (age) å’Œæ€§åˆ« (sex) æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§è¯­è¨€æ¨¡å‹å±•ç°äº†æ›´å¼ºçš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ï¼Œåœ¨è¯ç‰©åç§°è¯†åˆ« (drug name recognition) ä»»åŠ¡ä¸­æ›´å…·ä¼˜åŠ¿ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¯¹æ¯”äº†æ³¢å…°è¯­åŸæ–‡ä¸ç¿»è¯‘åçš„æ–‡æœ¬åœ¨ LLMs ä¸Šçš„è¡¨ç°å·®å¼‚ï¼Œæ­ç¤ºäº†å‡†ç¡®æ€§ã€æ ‡å‡†åŒ–ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚ä½œè€…æœ€ç»ˆæå€¡é‡‡ç”¨ç»“åˆè§„åˆ™ç³»ç»Ÿç²¾ç¡®æ€§ä¸å¤§è¯­è¨€æ¨¡å‹çµæ´»æ€§çš„æ··åˆæ–¹æ³• (hybrid approaches)ï¼Œæ—¨åœ¨ä¸ºç°å®åŒ»é™¢ç¯å¢ƒæä¾›æ›´å¯é ä¸”èµ„æºé«˜æ•ˆçš„ä¸´åºŠ NLP è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.15778v1",
      "published_date": "2025-11-19 18:51:45 UTC",
      "updated_date": "2025-11-19 18:51:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:54:09.767218+00:00"
    },
    {
      "arxiv_id": "2511.15684v1",
      "title": "Walrus: A Cross-Domain Foundation Model for Continuum Dynamics",
      "title_zh": "Walrusï¼šé¢å‘è¿ç»­ä»‹è´¨åŠ¨åŠ›å­¦çš„è·¨é¢†åŸŸåŸºç¡€æ¨¡å‹",
      "authors": [
        "Michael McCabe",
        "Payel Mukhopadhyay",
        "Tanya Marwah",
        "Bruno Regaldo-Saint Blancard",
        "Francois Rozet",
        "Cristiana Diaconu",
        "Lucas Meyer",
        "Kaze W. K. Wong",
        "Hadi Sotoudeh",
        "Alberto Bietti",
        "Irina Espejo",
        "Rio Fear",
        "Siavash Golkar",
        "Tom Hehir",
        "Keiya Hirashima",
        "Geraud Krawezik",
        "Francois Lanusse",
        "Rudy Morel",
        "Ruben Ohana",
        "Liam Parker",
        "Mariel Pettee",
        "Jeff Shen",
        "Kyunghyun Cho",
        "Miles Cranmer",
        "Shirley Ho"
      ],
      "abstract": "Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Walrusï¼Œä¸€ä¸ªä¸“é—¨ä¸ºæµä½“ç±»è¿ç»­ä½“åŠ¨åŠ›å­¦ (continuum dynamics) è®¾è®¡çš„ Transformer åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨å…‹æœç‰©ç†æ¨¡æ‹Ÿä¸­æ•°æ®å¼‚æ„ã€é•¿æœŸåŠ¨æ€ä¸ç¨³å®šä»¥åŠå¤šåˆ†è¾¨ç‡è®­ç»ƒéš¾ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†åŸºäºè°ƒå’Œåˆ†æ (harmonic-analysis) çš„ç¨³å®šæ€§æ–¹æ³•ã€è´Ÿè½½å‡è¡¡çš„åˆ†å¸ƒå¼ 2D å’Œ 3D è®­ç»ƒç­–ç•¥ï¼Œä»¥åŠè®¡ç®—è‡ªé€‚åº”çš„åˆ†è¯æŠ€æœ¯ (compute-adaptive tokenization)ã€‚Walrus åœ¨æ¶µç›–å¤©ä½“ç‰©ç†ã€åœ°çƒç§‘å­¦ã€æµå˜å­¦ã€ç­‰ç¦»å­ä½“ç‰©ç†ã€å£°å­¦å’Œç»å…¸æµä½“ç­‰ 19 ä¸ªå¤šå…ƒåŒ–åœºæ™¯ä¸­è¿›è¡Œäº†é¢„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWalrus åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹æ°´å¹³ä¸Šå‡ä¼˜äºä»¥å¾€çš„åŸºç¡€æ¨¡å‹ã€‚æ¶ˆæ­§å®éªŒè¿›ä¸€æ­¥è¯å®äº†è¯¥æ¨¡å‹åœ¨é¢„æµ‹ç¨³å®šæ€§ã€è®­ç»ƒååé‡å’Œè¿ç§»æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œç›®å‰ç›¸å…³ä»£ç å’Œæ¨¡å‹æƒé‡å·²å…¬å¼€å‘å¸ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15684v1",
      "published_date": "2025-11-19 18:36:03 UTC",
      "updated_date": "2025-11-19 18:36:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:54:49.071036+00:00"
    },
    {
      "arxiv_id": "2511.15675v2",
      "title": "MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features",
      "title_zh": "MF-GCNï¼šä¸€ç§ç»“åˆçœ¼åŠ¨ã€é¢éƒ¨åŠå£°å­¦ç‰¹å¾çš„ä¸‰æ¨¡æ€æŠ‘éƒç—‡æ£€æµ‹å¤šé¢‘å›¾å·ç§¯ç½‘ç»œ",
      "authors": [
        "Sejuti Rahman",
        "Swakshar Deb",
        "MD. Sameer Iqbal Chowdhury",
        "MD. Jubair Ahmed Sourov",
        "Mohammad Shamsuddin"
      ],
      "abstract": "Depression is a prevalent global mental health disorder, characterised by persistent low mood and anhedonia. However, it remains underdiagnosed because current diagnostic methods depend heavily on subjective clinical assessments. To enable objective detection, we introduce a gold standard dataset of 103 clinically assessed participants collected through a tripartite data approach which uniquely integrated eye tracking data with audio and video to give a comprehensive representation of depressive symptoms. Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŠ‘éƒç—‡ä¸´åºŠè¯„ä¼°ä¸»è§‚æ€§å¼ºçš„é—®é¢˜ï¼Œåˆ©ç”¨çœ¼åŠ¨è¿½è¸ª(Eye-tracking)ã€é¢éƒ¨(Facial)å’Œå£°å­¦(Acoustic)ç‰¹å¾æ„å»ºäº†åŒ…å«103åå‚ä¸è€…çš„ä¸‰æ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨å®ç°å®¢è§‚çš„æŠ‘éƒç—‡æ£€æµ‹ã€‚ç ”ç©¶å‘ç°ï¼Œçœ¼åŠ¨æ•°æ®èƒ½é‡åŒ–è´Ÿé¢åˆºæ¿€çš„æ³¨æ„åŠ›åå‘(Attentional bias)ï¼Œè€Œè§†å¬æ•°æ®åˆ™èƒ½æ•æ‰æƒ…æ„Ÿå¹³æ·¡å’Œç²¾ç¥è¿åŠ¨è¿Ÿç¼“ç‰¹å¾ã€‚ä¸ºè§£å†³ç°æœ‰å›¾ç¥ç»ç½‘ç»œæ¨¡å‹ä»…å…³æ³¨ä½é¢‘ä¿¡æ¯çš„å±€é™ï¼Œä½œè€…æå‡ºäº†å¤šé¢‘å›¾å·ç§¯ç½‘ç»œ(MF-GCN)ï¼Œé€šè¿‡æ–°é¢–çš„å¤šé¢‘æ»¤æ³¢å™¨ç»„æ¨¡å—(MFFBM)æœ‰æ•ˆåˆ©ç”¨ä½é¢‘å’Œé«˜é¢‘ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒMF-GCNåœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸­å–å¾—äº†0.96çš„æ•æ„Ÿåº¦(Sensitivity)å’Œ0.94çš„F2åˆ†æ•°(F2 score)ï¼Œå¹¶åœ¨ä¸‰åˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶ŠåŸºçº¿æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨ä¸­æ–‡å¤šæ¨¡æ€æŠ‘éƒç—‡è¯­æ–™åº“(CMDC)æ•°æ®é›†ä¸Šäº¦è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æ•æ‰è·¨æ¨¡æ€äº¤äº’æ–¹é¢çš„å‡†ç¡®æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15675v2",
      "published_date": "2025-11-19 18:18:53 UTC",
      "updated_date": "2025-11-21 18:28:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:55:09.568468+00:00"
    },
    {
      "arxiv_id": "2511.15661v2",
      "title": "VisPlay: Self-Evolving Vision-Language Models from Images",
      "title_zh": "VisPlayï¼šåŸºäºå›¾åƒçš„è‡ªè¿›åŒ–è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yicheng He",
        "Chengsong Huang",
        "Zongxia Li",
        "Jiaxin Huang",
        "Yonghui Yang"
      ],
      "abstract": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VisPlayï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿåˆ©ç”¨æµ·é‡æœªæ ‡è®°å›¾åƒæ•°æ®è‡ªä¸»æå‡ Vision-Language Models (VLMs) æ¨ç†èƒ½åŠ›çš„è‡ªæˆ‘è¿›åŒ–å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¡†æ¶ã€‚VisPlay æ—¨åœ¨è§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•é«˜åº¦ä¾èµ–äººå·¥æ ‡æ³¨æ ‡ç­¾æˆ–ç‰¹å®šä»»åŠ¡å¯å‘å¼è§„åˆ™æ¥å®šä¹‰å¥–åŠ±æ‰€å¯¼è‡´çš„æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•çš„éš¾é¢˜ã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæ¨¡å‹è¢«åˆ†é…ä¸º Image-Conditioned Questioner å’Œ Multimodal Reasoner ä¸¤ä¸ªè§’è‰²ï¼Œåˆ†åˆ«è´Ÿè´£æ„å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰é—®é¢˜å’Œç”Ÿæˆå‚è€ƒç­”æ¡ˆã€‚é€šè¿‡é‡‡ç”¨ Group Relative Policy Optimization (GRPO) ç­–ç•¥å¹¶å¼•å…¥å¤šæ ·æ€§ä¸éš¾åº¦å¥–åŠ±ï¼ŒVisPlay å®ç°äº†ç”Ÿæˆé—®é¢˜å¤æ‚åº¦ä¸ç­”æ¡ˆè´¨é‡çš„æœ‰æ•ˆå¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ Qwen2.5-VL å’Œ MiMo-VL æ¨¡å‹ä¸Šæ˜¾è‘—æå‡äº†è§†è§‰æ¨ç† (Visual Reasoning)ã€ç»„åˆæ¦‚æ‹¬ (Compositional Generalization) ä»¥åŠå‡å°‘å¹»è§‰ (Hallucination Reduction) çš„è¡¨ç°ã€‚åœ¨åŒ…æ‹¬ MM-Vet å’Œ MMMU åœ¨å†…çš„å…«é¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVisPlay è¯æ˜äº†å…¶ä½œä¸ºå®ç°è‡ªæˆ‘è¿›åŒ–å¤šæ¨¡æ€æ™ºèƒ½çš„å¯æ‰©å±•è·¯å¾„çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15661v2",
      "published_date": "2025-11-19 17:55:15 UTC",
      "updated_date": "2025-11-20 12:43:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:55:07.878062+00:00"
    },
    {
      "arxiv_id": "2511.15658v1",
      "title": "GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI",
      "title_zh": "GEO-Bench-2ï¼šä»æ€§èƒ½åˆ°èƒ½åŠ›ï¼Œåœ°ç†ç©ºé—´äººå·¥æ™ºèƒ½è¯„ä¼°çš„å†æ€è€ƒ",
      "authors": [
        "Naomi Simumba",
        "Nils Lehmann",
        "Paolo Fraccaro",
        "Hamed Alemohammad",
        "Geeth De Mel",
        "Salman Khan",
        "Manil Maskey",
        "Nicolas Longepe",
        "Xiao Xiang Zhu",
        "Hannah Kerner",
        "Juan Bernabe-Moreno",
        "Alexander Lacoste"
      ],
      "abstract": "Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.\n  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¨å‡ºäº† GEO-Bench-2ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³åœ°çƒè§‚æµ‹ (Earth Observation) è¯„ä¼°ç¼ºä¹æ ‡å‡†åŒ–åè®®é—®é¢˜çš„ç»¼åˆæ€§æ¡†æ¶ï¼Œæ¶µç›–äº†åˆ†ç±»ã€åˆ†å‰²ã€å›å½’ã€ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ç­‰ 19 ä¸ªæ•°æ®é›†ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†â€œèƒ½åŠ› (capability)â€åˆ†ç»„æœºåˆ¶ï¼Œæ ¹æ®åˆ†è¾¨ç‡ã€æ³¢æ®µå’Œæ—¶æ•ˆæ€§ç­‰ç‰¹å¾å¯¹æ¨¡å‹è¿›è¡Œæ’åï¼Œå¸®åŠ©ç”¨æˆ·è¯†åˆ«æ¨¡å‹åœ¨ç‰¹å®šç»´åº¦çš„ä¼˜åŠ£ã€‚é€šè¿‡å®šä¹‰è§„èŒƒä¸”çµæ´»çš„è¯„ä¼°åè®®ï¼ŒGEO-Bench-2 æ—¢ç¡®ä¿äº†åŸºå‡†æµ‹è¯•çš„ä¸€è‡´æ€§ï¼Œä¹Ÿä¿ƒè¿›äº†å¯¹æ¨¡å‹è‡ªé€‚åº”ç­–ç•¥ (model adaptation strategies) çš„æ·±å…¥ç ”ç©¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›®å‰æ²¡æœ‰å•ä¸€æ¨¡å‹èƒ½åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œæ¶æ„è®¾è®¡å’Œé¢„è®­ç»ƒæ–¹å¼å¯¹æœ€ç»ˆè¡¨ç°å½±å“å·¨å¤§ã€‚åœ¨è‡ªç„¶å›¾åƒä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ (å¦‚ ConvNext, DINO V3) åœ¨é«˜åˆ†è¾¨ç‡ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€Œä¸“é—¨é’ˆå¯¹ EO å¼€å‘çš„æ¨¡å‹ (å¦‚ TerraMind, Prithvi, Clay) åˆ™åœ¨å†œä¸šå’Œç¾å®³å“åº”ç­‰é¢†åŸŸçš„å¤šå…‰è°± (multispectral) åº”ç”¨ä¸­æ›´èƒœä¸€ç­¹ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜å¼€å‘å…¨èƒ½å‹ Geospatial Foundation Model (GeoFM) ä»æ˜¯æå…·æŒ‘æˆ˜çš„å¼€æ”¾è¯¾é¢˜ï¼Œè€Œ GEO-Bench-2 ä¸ºç‰¹å®šç”¨ä¾‹æä¾›äº†å¯é‡å¤ä¸”å…·æœ‰æŒ‡å¯¼æ„ä¹‰çš„è¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15658v1",
      "published_date": "2025-11-19 17:45:02 UTC",
      "updated_date": "2025-11-19 17:45:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:55:34.242067+00:00"
    },
    {
      "arxiv_id": "2511.15652v1",
      "title": "Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges",
      "title_zh": "ä¿¡æ¯ç‰©ç†ç³»ç»Ÿä¸­çš„æŒç»­å¼ºåŒ–å­¦ä¹ ï¼šç»éªŒæ€»ç»“ä¸å¼€æ”¾æ€§æŒ‘æˆ˜",
      "authors": [
        "Kim N. Nolle",
        "Ivana Dusparic",
        "Rhodri Cusack",
        "Vinny Cahill"
      ],
      "abstract": "Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.\n  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.\n  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¿¡æ¯ç‰©ç†ç³»ç»Ÿï¼ˆCyber-Physical Systemsï¼‰ä¸­æŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆContinual Reinforcement Learning, CRLï¼‰é¢ä¸´çš„æŒ‘æˆ˜ä¸ç»éªŒã€‚ç ”ç©¶è€…åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆautonomous drivingï¼‰æ³Šè½¦ç¯å¢ƒä¸‹ï¼Œåˆ©ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆProximal Policy Optimization, PPOï¼‰ç®—æ³•å¯¹æ™ºèƒ½ä½“åœ¨å››ä¸ªä¸åŒè§’åº¦çš„æ³Šè½¦åœºæ™¯ä¸­è¿›è¡Œè¿ç»­è®­ç»ƒã€‚å®éªŒæ­ç¤ºäº†CRLä¸­çš„å¤šä¸ªæ ¸å¿ƒé—®é¢˜ï¼ŒåŒ…æ‹¬å¯»æ‰¾åˆé€‚çš„ç¯å¢ƒæŠ½è±¡ï¼ˆabstractionsï¼‰ã€å¯¹è¶…å‚æ•°ï¼ˆhyperparametersï¼‰çš„è¿‡åº¦æ•æ„Ÿã€ç¾éš¾æ€§é—å¿˜ï¼ˆcatastrophic forgettingï¼‰ä»¥åŠç¥ç»ç½‘ç»œå®¹é‡çš„æœ‰æ•ˆåˆ©ç”¨ã€‚åŸºäºè¿™äº›æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†æ„å»ºé²æ£’CRLç³»ç»Ÿçš„å…³é”®ç ”ç©¶æ–¹å‘ï¼Œå¹¶å¯¹ç¥ç»ç½‘ç»œåœ¨æŒç»­å­¦ä¹ ï¼ˆCLï¼‰ä¸­çš„é€‚ç”¨æ€§æå‡ºäº†è´¨ç–‘ã€‚æœ€åï¼Œä½œè€…å¼ºè°ƒäº†è®¡ç®—æœºç§‘å­¦ä¸ç¥ç»ç§‘å­¦ï¼ˆneuroscienceï¼‰è·¨å­¦ç§‘ç ”ç©¶åœ¨è§£å†³è¿™äº›å¤æ‚æŒ‘æˆ˜ä¸­çš„é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 5 figures, Accepted to RLDM 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.15652v1",
      "published_date": "2025-11-19 17:40:13 UTC",
      "updated_date": "2025-11-19 17:40:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:55:29.159816+00:00"
    },
    {
      "arxiv_id": "2512.03047v1",
      "title": "Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models",
      "title_zh": "åŸºäºç†µçš„å¤§è¯­è¨€æ¨¡å‹ä»·å€¼æ¼‚ç§»ä¸å¯¹é½å·¥ä½œåº¦é‡",
      "authors": [
        "Samih Fadli"
      ],
      "abstract": "Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨åˆ†å¸ƒåç§»å’Œè¶Šç‹±æ”»å‡»ä¸‹è¡¨ç°å‡ºçš„åŠ¨æ€ä»·å€¼æ¼‚ç§»(Value Drift)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä¼¦ç†ç†µ(Ethical Entropy)çš„åº¦é‡æ¡†æ¶ã€‚ç ”ç©¶è€…å‚è€ƒâ€œæ™ºèƒ½ç¬¬äºŒå®šå¾‹â€ï¼Œå°†ä¼¦ç†ç†µå®šä¹‰ä¸ºéšæ—¶é—´å¢é•¿çš„çŠ¶æ€å˜é‡ï¼Œå¹¶åˆ©ç”¨è®­ç»ƒå¥½çš„åˆ†ç±»å™¨ä»æ¨¡å‹è½¬å½•ä¸­å®æ—¶ä¼°ç®—ä¼¦ç†ç†µ $S(t)$ã€‚å®éªŒå¯¹æ¯”äº†å››ç§å‰æ²¿æ¨¡å‹çš„åŸºåº§ç‰ˆæœ¬ä¸æŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬ï¼Œå‘ç°å¾®è°ƒèƒ½æ˜¾è‘—æŠ‘åˆ¶ä»·å€¼æ¼‚ç§»ï¼Œå¹¶å°†ä¼¦ç†ç†µé™ä½çº¦80%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æœ‰æ•ˆå¯¹é½å·¥ä½œç‡($\\gamma_{eff}$)çš„æ¦‚å¿µï¼Œç”¨äºé‡åŒ–æ¨¡å‹ç»´æŒå¯¹é½çŠ¶æ€çš„èƒ½åŠ›ã€‚é€šè¿‡å°† $S(t)$ å’Œ $\\gamma_{eff}$ åµŒå…¥ç›‘æ§ç®¡çº¿ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨ç†µæ¼‚ç§»è¶…è¿‡é˜ˆå€¼æ—¶è‡ªåŠ¨å‘å‡ºè­¦æŠ¥ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²æä¾›äº†æœ‰æ•ˆçš„è¿è¡Œæ—¶ä»·å€¼æ¼‚ç§»ç›‘ç®¡æ‰‹æ®µã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages. Companion paper to \"The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems\". Code and tools: https://github.com/AerisSpace/EthicalEntropyKit",
      "pdf_url": "https://arxiv.org/pdf/2512.03047v1",
      "published_date": "2025-11-19 17:27:16 UTC",
      "updated_date": "2025-11-19 17:27:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:55:28.068321+00:00"
    },
    {
      "arxiv_id": "2511.15767v1",
      "title": "TB or Not TB: Coverage-Driven Direct Preference Optimization for Verilog Stimulus Generation",
      "title_zh": "TB or Not TBï¼šé¢å‘ Verilog æ¿€åŠ±ç”Ÿæˆçš„è¦†ç›–ç‡é©±åŠ¨ç›´æ¥åå¥½ä¼˜åŒ–",
      "authors": [
        "Bardia Nadimi",
        "Khashayar Filom",
        "Deming Chen",
        "Hao Zheng"
      ],
      "abstract": "With the rapid advancement of Large Language Models (LLMs), there is growing interest in applying them to hardware design and verification. Among these stages, design verification remains the most time-consuming and resource-intensive phase, where generating effective stimuli for the design under test (DUT) is both critical and labor-intensive. We present {\\it TB or not TB}, a framework for automated stimulus generation using LLMs fine-tuned through Coverage-Driven Direct Preference Optimization (CD-DPO). To enable preference-based training, we introduce PairaNet, a dataset derived from PyraNet that pairs high- and low-quality testbenches labeled using simulation-derived coverage metrics. The proposed CD-DPO method integrates quantitative coverage feedback directly into the optimization objective, guiding the model toward generating stimuli that maximize verification coverage. Experiments on the CVDP CID12 benchmark show that {\\it TB or not TB} outperforms both open-source and commercial baselines, achieving up to 77.27\\% improvement in code coverage, demonstrating the effectiveness of Coverage-driven preference optimization for LLM-based hardware verification.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¡¬ä»¶è®¾è®¡éªŒè¯ä¸­æ¿€åŠ±ç”Ÿæˆ (stimulus generation) è€—æ—¶è€—åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º TB or Not TB çš„è‡ªåŠ¨åŒ–æ¿€åŠ±ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†è¦†ç›–ç‡é©±åŠ¨çš„ç›´æ¥åå¥½ä¼˜åŒ– (Coverage-Driven Direct Preference Optimization, CD-DPO) æ–¹æ³•å¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†å®ç°åŸºäºåå¥½çš„è®­ç»ƒï¼Œç ”ç©¶è€…å¼•å…¥äº†ä» PyraNet è¡ç”Ÿè€Œæ¥çš„ PairaNet æ•°æ®é›†ï¼Œåˆ©ç”¨ä»¿çœŸè¦†ç›–ç‡æŒ‡æ ‡å¯¹é«˜ä½è´¨é‡çš„æµ‹è¯•å¹³å° (testbenches) è¿›è¡Œæ ‡æ³¨é…å¯¹ã€‚CD-DPO æ–¹æ³•å°†å®šé‡è¦†ç›–ç‡åé¦ˆç›´æ¥æ•´åˆè¿›ä¼˜åŒ–ç›®æ ‡ï¼Œæœ‰æ•ˆå¼•å¯¼æ¨¡å‹ç”Ÿæˆå¯æœ€å¤§åŒ–éªŒè¯è¦†ç›–ç‡çš„æ¿€åŠ±ä¿¡å·ã€‚åœ¨ CVDP CID12 åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTB or Not TB çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå¼€æºåŠå•†ä¸šåŸºå‡†æ¨¡å‹ï¼Œä»£ç è¦†ç›–ç‡æå‡æœ€é«˜è¾¾ 77.27%ã€‚è¯¥ç ”ç©¶å……åˆ†è¯æ˜äº†è¦†ç›–ç‡é©±åŠ¨çš„åå¥½ä¼˜åŒ–åœ¨æå‡åŸºäº LLM çš„ç¡¬ä»¶éªŒè¯æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15767v1",
      "published_date": "2025-11-19 17:23:06 UTC",
      "updated_date": "2025-11-19 17:23:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:55:28.669857+00:00"
    },
    {
      "arxiv_id": "2511.15623v1",
      "title": "Sufficient Explanations in Databases and their Connections to Necessary Explanations and Repairs",
      "title_zh": "æ•°æ®åº“ä¸­çš„å……åˆ†è§£é‡ŠåŠå…¶ä¸å¿…è¦è§£é‡Šã€æ•°æ®åº“ä¿®å¤çš„å…³è”",
      "authors": [
        "Leopoldo Bertossi",
        "Nina Pardal"
      ],
      "abstract": "The notion of cause, as formalized by Halpern and Pearl, has been recently applied to relational databases, to characterize and compute causal explanations for query answers. In this work we consider the alternative notion of sufficient explanation. We investigate its connections with database repairs as used for dealing with inconsistent databases, and with causality-based necessary explanations. We also obtain some computational results.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…³ç³»æ•°æ®åº“ä¸­æŸ¥è¯¢ç­”æ¡ˆçš„è§£é‡Šæœºåˆ¶ï¼Œé‡ç‚¹ç ”ç©¶äº†â€œå……åˆ†è§£é‡Šâ€(Sufficient Explanation)è¿™ä¸€æ ¸å¿ƒæ¦‚å¿µã€‚ä½œè€…æ·±å…¥åˆ†æäº†å……åˆ†è§£é‡Šä¸å¤„ç†ä¸ä¸€è‡´æ•°æ®åº“æ—¶æ‰€é‡‡ç”¨çš„â€œæ•°æ®åº“ä¿®å¤â€(Database Repairs)ä»¥åŠåŸºäºå› æœå…³ç³»çš„â€œå¿…è¦è§£é‡Šâ€(Necessary Explanations)ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚é€šè¿‡å»ºç«‹è¿™äº›æ¦‚å¿µä¹‹é—´çš„é€»è¾‘æ¡¥æ¢ï¼Œè¯¥å·¥ä½œä¸ä»…ä¸°å¯Œäº†æ•°æ®åº“å› æœå…³ç³»çš„ç†è®ºä½“ç³»ï¼Œè¿˜è·å¾—äº†ä¸€ç³»åˆ—å…³äºå¦‚ä½•è®¡ç®—è¿™äº›è§£é‡Šçš„è®¡ç®—ç»“æœ(Computational Results)ã€‚è¿™é¡¹ç ”ç©¶ä¸ºç†è§£æŸ¥è¯¢ç»“æœçš„æˆå› ä»¥åŠåœ¨å­˜åœ¨å†²çªæ•°æ®æ—¶å¦‚ä½•ç”Ÿæˆç¨³å¥çš„è§£é‡Šæä¾›äº†é‡è¦çš„ç†è®ºæ¡†æ¶å’Œç®—æ³•åŸºç¡€ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15623v1",
      "published_date": "2025-11-19 17:07:16 UTC",
      "updated_date": "2025-11-19 17:07:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:55:23.475523+00:00"
    },
    {
      "arxiv_id": "2511.15622v2",
      "title": "The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification",
      "title_zh": "SA-FARI æ•°æ®é›†ï¼šé¢å‘åŠ¨ç‰©è¯†åˆ«ä¸é‰´å®šçš„å½±åƒä¸‡ç‰©åˆ†å‰²",
      "authors": [
        "Dante Francisco Wasmuht",
        "Otto Brookes",
        "Maximillian Schall",
        "Pablo Palencia",
        "Chris Beirne",
        "Tilo Burghardt",
        "Majid Mirmehdi",
        "Hjalmar KÃ¼hl",
        "Mimi Arandjelovic",
        "Sam Pottie",
        "Peter Bermant",
        "Brandon Asheim",
        "Yi Jin Toh",
        "Adam Elzinga",
        "Jason Holmberg",
        "Andrew Whitworth",
        "Eleanor Flatt",
        "Laura Gustafson",
        "Chaitanya Ryali",
        "Yuan-Ting Hu",
        "Baishan Guo",
        "Andrew Westbury",
        "Kate Saenko",
        "Didac Suris"
      ],
      "abstract": "Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at https://www.conservationxlabs.com/sa-fari.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†SA-FARIï¼Œè¿™æ˜¯ç›®å‰è§„æ¨¡æœ€å¤§çš„å¼€æºé‡ç”ŸåŠ¨ç‰©å¤šåŠ¨ç‰©è¿½è¸ª(Multi-Animal Tracking)æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†åœ¨ç‰©ç§å¤šæ ·æ€§ã€åœ°ç†è¦†ç›–å’Œæ—¶ç©ºè·¨åº¦ä¸Šçš„ä¸è¶³ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»å…¨çƒ4ä¸ªå¤§æ´²ã€741ä¸ªåœ°ç‚¹é‡‡é›†çš„11,609ä¸ªçº¢å¤–ç›¸æœºè§†é¢‘ï¼Œè·¨è¶Šåå¹´æ—¶é—´å¹¶æ¶µç›–99ä¸ªç‰©ç§ç±»åˆ«ã€‚SA-FARIæä¾›äº†çº¦46å°æ—¶çš„é«˜è´¨é‡æ ‡æ³¨å½±åƒï¼ŒåŒ…æ‹¬16,224ä¸ªmaskletèº«ä»½ä»¥åŠè¶…è¿‡94ä¸‡ä¸ªä¸ªä½“è¾¹ç•Œæ¡†ã€åˆ†å‰²æ©ç å’Œç‰©ç§æ ‡ç­¾ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨SAM 3ç­‰å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ä»¥åŠé’ˆå¯¹é‡ç”ŸåŠ¨ç‰©å¼€å‘çš„è§†è§‰æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚é‡å¤–åœºæ™¯ä¸‹çš„æ£€æµ‹ä¸è¿½è¸ªæ€§èƒ½ã€‚ä½œä¸ºé¦–ä¸ªç»“åˆäº†é«˜ç‰©ç§å¤šæ ·æ€§ä¸é«˜è´¨é‡æ—¶ç©ºæ ‡æ³¨çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒSA-FARIä¸ºæ¨åŠ¨å¯æ³›åŒ–çš„è‡ªåŠ¨åŒ–é‡ç”ŸåŠ¨ç‰©ç›‘æµ‹ä¸ä¿æŠ¤æŠ€æœ¯å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15622v2",
      "published_date": "2025-11-19 17:07:08 UTC",
      "updated_date": "2025-11-24 17:02:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:55:46.436511+00:00"
    },
    {
      "arxiv_id": "2511.15614v1",
      "title": "Optimus-Q: Utilizing Federated Learning in Adaptive Robots for Intelligent Nuclear Power Plant Operations through Quantum Cryptography",
      "title_zh": "Optimus-Qï¼šé€šè¿‡é‡å­å¯†ç å­¦åœ¨è‡ªé€‚åº”æœºå™¨äººä¸­åˆ©ç”¨è”é‚¦å­¦ä¹ å®ç°æ ¸ç”µç«™æ™ºèƒ½è¿è¥",
      "authors": [
        "Sai Puppala",
        "Ismail Hossain",
        "Jahangir Alam",
        "Sajedul Talukder"
      ],
      "abstract": "The integration of advanced robotics in nuclear power plants (NPPs) presents a transformative opportunity to enhance safety, efficiency, and environmental monitoring in high-stakes environments. Our paper introduces the Optimus-Q robot, a sophisticated system designed to autonomously monitor air quality and detect contamination while leveraging adaptive learning techniques and secure quantum communication. Equipped with advanced infrared sensors, the Optimus-Q robot continuously streams real-time environmental data to predict hazardous gas emissions, including carbon dioxide (CO$_2$), carbon monoxide (CO), and methane (CH$_4$). Utilizing a federated learning approach, the robot collaborates with other systems across various NPPs to improve its predictive capabilities without compromising data privacy. Additionally, the implementation of Quantum Key Distribution (QKD) ensures secure data transmission, safeguarding sensitive operational information. Our methodology combines systematic navigation patterns with machine learning algorithms to facilitate efficient coverage of designated areas, thereby optimizing contamination monitoring processes. Through simulations and real-world experiments, we demonstrate the effectiveness of the Optimus-Q robot in enhancing operational safety and responsiveness in nuclear facilities. This research underscores the potential of integrating robotics, machine learning, and quantum technologies to revolutionize monitoring systems in hazardous environments.",
      "tldr_zh": "æœ¬ç ”ç©¶ä»‹ç»äº† Optimus-Q æœºå™¨äººï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æ ¸ç”µç«™ (Nuclear Power Plants) é«˜é£é™©ç¯å¢ƒä¸­è‡ªä¸»ç›‘æµ‹ç©ºæ°”è´¨é‡å’Œæ±¡æŸ“çš„å…ˆè¿›ç³»ç»Ÿã€‚è¯¥æœºå™¨äººåˆ©ç”¨çº¢å¤–ä¼ æ„Ÿå™¨å®æ—¶æµå¼ä¼ è¾“ç¯å¢ƒæ•°æ®ï¼Œä»¥é¢„æµ‹äºŒæ°§åŒ–ç¢³ (CO$_2$)ã€ä¸€æ°§åŒ–ç¢³ (CO) å’Œç”²çƒ· (CH$_4$) ç­‰æœ‰å®³æ°”ä½“çš„æ’æ”¾ã€‚é€šè¿‡é‡‡ç”¨è”é‚¦å­¦ä¹  (Federated Learning) æ–¹æ³•ï¼ŒOptimus-Q èƒ½å¤Ÿåœ¨ä¸æ³„éœ²æ•°æ®éšç§çš„å‰æä¸‹ï¼Œå®ç°è·¨æ ¸ç”µç«™çš„åä½œå­¦ä¹ ä»¥æå‡é¢„æµ‹èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†é‡å­å¯†é’¥åˆ†å‘ (Quantum Key Distribution, QKD) æŠ€æœ¯ï¼Œç¡®ä¿äº†æ•æ„Ÿæ“ä½œæ•°æ®åœ¨ä¼ è¾“è¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†ç³»ç»ŸåŒ–å¯¼èˆªæ¨¡å¼ä¸æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæœ‰æ•ˆä¼˜åŒ–äº†æ±¡æŸ“ç›‘æµ‹çš„è¦†ç›–æ•ˆç‡ã€‚å®éªŒå’Œæ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼ŒOptimus-Q æ˜¾è‘—å¢å¼ºäº†æ ¸è®¾æ–½çš„æ“ä½œå®‰å…¨æ€§å’Œå“åº”é€Ÿåº¦ï¼Œå±•ç¤ºäº†å°†æœºå™¨äººã€æœºå™¨å­¦ä¹ ä¸é‡å­æŠ€æœ¯é›†æˆåœ¨å±é™©ç¯å¢ƒç›‘æµ‹ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15614v1",
      "published_date": "2025-11-19 17:01:24 UTC",
      "updated_date": "2025-11-19 17:01:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:55:47.745341+00:00"
    },
    {
      "arxiv_id": "2511.15593v2",
      "title": "What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity",
      "title_zh": "å¦‚ä½•æˆä¸ºä¼˜ç§€çš„AIç§‘ç ”æ™ºèƒ½ä½“ï¼Ÿæ„æ€å¤šæ ·æ€§çš„ä½œç”¨ç ”ç©¶",
      "authors": [
        "Alexis Audran-Reiss",
        "Jordi Armengol-EstapÃ©",
        "Karen Hambardzumyan",
        "Amar Budhiraja",
        "Martin Josifoski",
        "Edan Toledo",
        "Rishi Hazra",
        "Despoina Magka",
        "Michael Shvartsman",
        "Parth Pathak",
        "Justine T Kao",
        "Lucia Cipolina-Kun",
        "Bhavul Gauri",
        "Jean-Christophe Gagnon-Audet",
        "Emanuel Tewolde",
        "Jenny Zhang",
        "Taco Cohen",
        "Yossi Adi",
        "Tatiana Shavrina",
        "Yoram Bachrach"
      ],
      "abstract": "AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æˆä¸ºä¼˜ç§€çš„ AI research agents æ‰€éœ€çš„å…³é”®å› ç´ ï¼Œé‡ç‚¹åˆ†æäº† ideation diversityï¼ˆæ„æ€å¤šæ ·æ€§ï¼‰åœ¨æå‡æ™ºèƒ½ä½“æ€§èƒ½ä¸­çš„ä½œç”¨ã€‚ç ”ç©¶äººå‘˜é€šè¿‡åœ¨ MLE-bench åŸºå‡†æµ‹è¯•ä¸­åˆ†æä¸åŒæ¨¡å‹å’Œ agent scaffolds çš„è¿è¡Œè½¨è¿¹ï¼Œå‘ç°é«˜æ€§èƒ½æ™ºèƒ½ä½“é€šå¸¸å±•ç°å‡ºæ›´é«˜ç¨‹åº¦çš„ ideation diversityã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡æ§åˆ¶å®éªŒè°ƒèŠ‚æ„æ€çš„å¤šæ ·åŒ–ç¨‹åº¦ï¼Œè¯å®äº†å¢åŠ  ideation diversity èƒ½æ˜¾è‘—å¢å¼ºæ™ºèƒ½ä½“çš„æœ€ç»ˆè¡¨ç°ã€‚ä¸ºäº†ç¡®ä¿ç»“æœçš„å¯é æ€§ï¼Œç ”ç©¶è¿˜é‡‡ç”¨äº†æ ‡å‡†è¯„åˆ†ä»¥å¤–çš„å¤šç§è¯„ä¼°æŒ‡æ ‡è¿›è¡ŒéªŒè¯ï¼Œç»“æœä¸€è‡´è¡¨æ˜æ„æ€å¤šæ ·æ€§æ˜¯é©±åŠ¨ç§‘ç ”æ™ºèƒ½ä½“æˆåŠŸçš„æ ¸å¿ƒè¦ç´ ã€‚è¯¥å‘ç°ä¸ºç†è§£å’Œä¼˜åŒ–è‡ªåŠ¨åŒ–ç§‘å­¦ç ”ç©¶æµç¨‹æä¾›äº†é‡è¦çš„å®è¯ä¾æ®ï¼Œå¼ºè°ƒäº†åœ¨æ™ºèƒ½ä½“è®¾è®¡ä¸­å¼•å…¥å¤šå…ƒåŒ–æ„æ€çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15593v2",
      "published_date": "2025-11-19 16:32:18 UTC",
      "updated_date": "2025-12-09 17:32:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:55:57.538113+00:00"
    },
    {
      "arxiv_id": "2511.15580v3",
      "title": "CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking",
      "title_zh": "CompTrackï¼šåŸºäºä¿¡æ¯ç“¶é¢ˆå¼•å¯¼çš„ä½ç§©åŠ¨æ€ Token å‹ç¼©ç‚¹äº‘è·Ÿè¸ª",
      "authors": [
        "Sifan Zhou",
        "Yichao Cao",
        "Jiahao Nie",
        "Yuqian Fu",
        "Ziyu Zhao",
        "Xiaobo Lu",
        "Shuo Wang"
      ],
      "abstract": "3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­3Då•ç›®æ ‡è·Ÿè¸ª(3D SOT)é¢ä¸´çš„ç‚¹äº‘ç¨€ç–æ€§åŠå†—ä½™æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºCompTrackçš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆé›†æˆç©ºé—´å‰æ™¯é¢„æµ‹å™¨(SFP)æ¨¡å—ï¼Œåˆ©ç”¨ä¿¡æ¯ç†µ(Information Entropy)è¿‡æ»¤æ— å…³èƒŒæ™¯å™ªå£°ä»¥è§£å†³ç©ºé—´å†—ä½™é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ˜¯ä¿¡æ¯ç“¶é¢ˆå¼•å¯¼çš„åŠ¨æ€ä»¤ç‰Œå‹ç¼©(IB-DTC)æ¨¡å—ï¼Œè¯¥æ¨¡å—ä»¥ä½ç§©è¿‘ä¼¼(Low-rank Approximation)ä¸ºç†è®ºåŸºç¡€ï¼Œé€šè¿‡åœ¨çº¿SVDåˆ†æå°†å†—ä½™çš„å‰æ™¯è‡ªé€‚åº”å‹ç¼©ä¸ºç´§å‡‘ä¸”é«˜ä¿¡æ¯é‡çš„ä»£ç†ä»¤ç‰Œ(Proxy Tokens)ã€‚åœ¨KITTIã€nuSceneså’ŒWaymoæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒCompTrackåœ¨ä¿æŒé¡¶å°–è·Ÿè¸ªæ€§èƒ½çš„åŒæ—¶å±•ç°äº†å“è¶Šçš„æ•ˆç‡ã€‚è¯¥ç³»ç»Ÿåœ¨å•å—RTX 3090 GPUä¸Šèƒ½ä»¥90 FPSçš„å®æ—¶é€Ÿåº¦è¿è¡Œï¼Œä¸ºæ¿€å…‰é›·è¾¾ç‚¹äº‘è·Ÿè¸ªæä¾›äº†å…¼é¡¾ç²¾åº¦ä¸é€Ÿåº¦çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2026 (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2511.15580v3",
      "published_date": "2025-11-19 16:12:24 UTC",
      "updated_date": "2025-11-22 13:51:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:56:33.483354+00:00"
    },
    {
      "arxiv_id": "2511.15574v1",
      "title": "HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning",
      "title_zh": "HSKBenchmarkï¼šåŸºäºè¯¾ç¨‹å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹æ±‰è¯­äºŒè¯­ä¹ å¾—å»ºæ¨¡ä¸åŸºå‡†æµ‹è¯„",
      "authors": [
        "Qihao Yang",
        "Xuelin Wang",
        "Jiale Chen",
        "Xuelian Dong",
        "Yuxin Hao",
        "Tianyong Hao"
      ],
      "abstract": "Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HSKBenchmarkï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸­æ–‡ç¬¬äºŒè¯­è¨€ä¹ å¾—(Chinese SLA)é¢†åŸŸè¿›è¡Œé˜¶æ®µæ€§å»ºæ¨¡å’Œå†™ä½œè¯„ä¼°çš„åŸºå‡†ã€‚è¯¥åŸºå‡†æ¶µç›–äº†HSK 3è‡³6çº§ï¼Œæ•´åˆäº†åŒ…å«676ä¸‡ä¸ªtokençš„çœŸå®æ•™ææ•°æ®ã€1.6ä¸‡ä¸ªåˆæˆæŒ‡ä»¤æ ·æœ¬ä»¥åŠ30ä¸ªæµ‹è¯•ä¸»é¢˜ã€‚ä¸ºäº†æ¨¡æ‹Ÿäººç±»çš„å­¦ä¹ è½¨è¿¹ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†è¯¾ç¨‹å¾®è°ƒ(Curriculum Tuning)æ¡†æ¶ï¼Œé€šè¿‡ä»åˆçº§åˆ°é«˜çº§çš„æ¸è¿›å¼è®­ç»ƒï¼Œå¹¶å»ºç«‹äº†ä¸€å¥—ä»è¯­æ³•è¦†ç›–ã€å†™ä½œé”™è¯¯åˆ°å¥æ³•å¤æ‚åº¦ç­‰å¤šç»´åº¦çš„è¯­è¨€è¯„ä¼°ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†åŸºäº1ä¸‡ç¯‡å­¦ä¹ è€…ä½œæ–‡å¾®è°ƒå¾—åˆ°çš„HSKAgentæ¨¡å‹ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒHSKBenchmarkèƒ½æœ‰æ•ˆæ¨¡æ‹Ÿä¸­æ–‡ä¹ å¾—è¿‡ç¨‹ï¼Œå…¶å¾®è°ƒåçš„æ¨¡å‹å†™ä½œæ°´å¹³å·²æ¥è¿‘é«˜çº§äººç±»å­¦ä¹ è€…ï¼Œå¹¶è¡¨ç°å‡ºæ˜¾è‘—çš„ç±»äººä¹ å¾—ç‰¹å¾ã€‚è¯¥åŸºå‡†å’Œç›¸å…³å·¥å…·çš„å¼€æºä¸ºæœªæ¥è¯­è¨€ä¹ å¾—å»ºæ¨¡å’ŒLLMsçš„å¯è§£é‡Šæ€§ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI-2026",
      "pdf_url": "https://arxiv.org/pdf/2511.15574v1",
      "published_date": "2025-11-19 16:06:06 UTC",
      "updated_date": "2025-11-19 16:06:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:56:25.042358+00:00"
    },
    {
      "arxiv_id": "2511.15557v1",
      "title": "B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index",
      "title_zh": "B+ANNï¼šä¸€ç§å¿«é€Ÿçš„åäº¿çº§ç£ç›˜æœ€è¿‘é‚»ç´¢å¼•",
      "authors": [
        "Selim Furkan Tekin",
        "Rajesh Bordawekar"
      ],
      "abstract": "Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† B+ANNï¼Œä¸€ç§æ–°å‹çš„åŸºäºç£ç›˜çš„è¿‘ä¼¼æœ€è¿‘é‚» (ANN) ç´¢å¼•ï¼Œæ—¨åœ¨è§£å†³ HNSW ç­‰ä¼ ç»Ÿå›¾å½¢ç´¢å¼•åœ¨å†…å­˜é™åˆ¶ã€éšæœºå†…å­˜è®¿é—®ä»¥åŠä»…æ”¯æŒè¯­ä¹‰ç›¸ä¼¼åº¦æŸ¥è¯¢æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ–¹æ³•é¦–å…ˆå°†è¾“å…¥æ•°æ®åˆ’åˆ†ä¸ºåŒ…å«è¯­ä¹‰ç›¸ä¼¼é¡¹çš„æ•°æ®å—ï¼Œå¹¶æ„å»ºä¸€ç§ B+ tree å˜ä½“ä»¥å®ç°æ•°æ®åœ¨å†…å­˜å’Œç£ç›˜ä¸Šçš„ååŒå­˜å‚¨ã€‚é€šè¿‡å¼•å…¥æ··åˆçš„åŸºäºè¾¹ç¼˜å’Œå—çš„å†…å­˜éå†æŠ€æœ¯ï¼ŒB+ANN æ˜¾è‘—ä¼˜åŒ–äº†è¯­ä¹‰æ“ä½œçš„ç©ºé—´å’Œæ—¶é—´å±€éƒ¨æ€§ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œä¸ HNSW ç›¸æ¯”ï¼ŒB+ANN åœ¨å¬å›ç‡ (Recall) å’Œæ¯ç§’æŸ¥è¯¢æ•° (QPS) æ–¹é¢å‡æœ‰æå‡ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº† 19.23% çš„ç¼“å­˜ç¼ºå¤±ã€‚æ­¤å¤–ï¼Œä¸ DiskANN ç›¸æ¯”ï¼Œè¯¥ç®—æ³•å°†å†…å­˜æ¶ˆè€—å’Œç£ç›˜æ„å»ºæ—¶é—´å¤§å¹…é™ä½äº† 24 å€ã€‚æœ€åï¼ŒB+ANN è¿˜æ‰©å±•äº†åŠŸèƒ½èŒƒç•´ï¼Œæ”¯æŒä¼ ç»Ÿçš„é¢å‘ç›¸ä¼¼æ€§çš„ ANN ç´¢å¼•æ— æ³•å®ç°çš„éç›¸ä¼¼æ€§æŸ¥è¯¢ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.DS"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15557v1",
      "published_date": "2025-11-19 15:50:28 UTC",
      "updated_date": "2025-11-19 15:50:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:56:27.144962+00:00"
    },
    {
      "arxiv_id": "2511.15552v2",
      "title": "Multimodal Evaluation of Russian-language Architectures",
      "title_zh": "ä¿„è¯­æ¶æ„çš„å¤šæ¨¡æ€è¯„ä¼°",
      "authors": [
        "Artem Chervyakov",
        "Ulyana Isaeva",
        "Anton Emelyanov",
        "Artem Safin",
        "Maria Tikhonova",
        "Alexander Kharitonov",
        "Yulia Lyakh",
        "Petr Surovtsev",
        "Denis Shevelev",
        "Vildan Saburov",
        "Vasily Konovalov",
        "Elisei Rykov",
        "Ivan Sviridov",
        "Amina Miftakhova",
        "Ilseyar Alimova",
        "Alexander Panchenko",
        "Alexander Kapitanov",
        "Alena Fenogenova"
      ],
      "abstract": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¿„è¯­é¢†åŸŸç¼ºä¹å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•çš„é—®é¢˜ï¼Œæå‡ºäº† Mera Multiï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºä¿„è¯­æ¶æ„è®¾è®¡çš„å¼€æºå¤šæ¨¡æ€è¯„ä¼°æ¡†æ¶ã€‚è¯¥åŸºå‡†åŸºäºæŒ‡ä»¤ (instruction-based)ï¼Œæ¶µç›–äº†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰æ¨¡æ€ï¼ŒåŒ…å«é’ˆå¯¹é€šç”¨æ¨¡å‹å’Œç‰¹å®šæ¨¡æ€æ¶æ„ï¼ˆå¦‚ image-to-text, video-to-text, audio-to-textï¼‰çš„ 18 é¡¹å…¨æ–°è¯„ä¼°ä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€å¥—é€šç”¨çš„å¤šæ¨¡æ€èƒ½åŠ›åˆ†ç±»æ³• (taxonomy)ï¼Œå¹¶ä»é›¶åˆ›å»ºäº† 18 ä¸ªå…·æœ‰ä¿„ç½—æ–¯æ–‡åŒ–ä¸è¯­è¨€ç‰¹æ€§çš„æ•°æ®é›†ï¼ŒåŒæ—¶ç»Ÿä¸€äº†æç¤ºè¯ (prompts) å’ŒæŒ‡æ ‡ (metrics)ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æä¾›äº†å¼€æºåŠé—­æºæ¨¡å‹çš„åŸºå‡†æµ‹è¯•ç»“æœ (baseline results)ï¼Œå¹¶å¼€å‘äº†åŒ…å«æ°´å° (watermarking) æŠ€æœ¯åœ¨å†…çš„é˜²æ­¢åŸºå‡†æ³„éœ² (benchmark leakage) çš„æ–¹æ³•è®ºã€‚è¯¥ç ”ç©¶ä¸ºæ„å»º typologically diverse languagesï¼ˆå°¤å…¶æ˜¯æ–¯æ‹‰å¤«è¯­æ—ï¼‰çš„å¤šæ¨¡æ€åŸºå‡†æä¾›äº†å¯å¤åˆ¶çš„æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15552v2",
      "published_date": "2025-11-19 15:43:53 UTC",
      "updated_date": "2025-11-20 13:06:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:56:25.243330+00:00"
    },
    {
      "arxiv_id": "2511.15534v1",
      "title": "Exploring the use of AI authors and reviewers at Agents4Science",
      "title_zh": "Agents4Scienceï¼šæ¢ç´¢ AI ä½œè€…ä¸å®¡ç¨¿äººçš„åº”ç”¨",
      "authors": [
        "Federico Bianchi",
        "Owen Queen",
        "Nitya Thakkar",
        "Eric Sun",
        "James Zou"
      ],
      "abstract": "There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ Agents4Science ä¼šè®®ä¸­åˆ©ç”¨ AI æ™ºèƒ½ä½“æ‹…ä»»è®ºæ–‡ä½œè€…å’Œè¯„å®¡å‘˜çš„å®é™…åº”ç”¨ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å…¶åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„ä¸“ä¸šèƒ½åŠ›ã€‚ä½œä¸ºé¦–ä¸ªç”± AI æ™ºèƒ½ä½“ä¸»å¯¼æ’°å†™ä¸è¯„å®¡ã€äººç±»ä½œä¸ºå…±åŒå‚ä¸è€…çš„å­¦æœ¯ä¼šè®®ï¼Œè¯¥é¡¹ç›®æ·±å…¥æ¢è®¨äº† AI æ™ºèƒ½ä½“ä½œä¸ºç§‘å­¦å®¶å’Œè¯„å®¡å‘˜çš„æ ¸å¿ƒè¡¨ç°ã€‚æ–‡ç« æ€»ç»“äº†ä»ä¼šè®®ä¸­è·å¾—çš„å…³é”®è§è§£ï¼Œå¹¶è¯¦ç»†åˆ†æäº†è¿™äº›å‘ç°å¯¹ç§‘å­¦é¢†åŸŸ Human-AI Collaborationï¼ˆäººæœºåä½œï¼‰çš„æ½œåœ¨å½±å“ã€‚é€šè¿‡å¯¹è¿™ä¸€åˆ›æ–°æ¨¡å¼çš„æ¢ç´¢ï¼Œç ”ç©¶æ­ç¤ºäº† AI æ™ºèƒ½ä½“åœ¨ç§‘ç ”å…¨æµç¨‹ä¸­çš„è§’è‰²æ¼”å˜åŠæœªæ¥å¯èƒ½æ€§ã€‚è¿™ä¸€å®è·µä¸ºç†è§£å’Œä¼˜åŒ–ç§‘å­¦ç ”ç©¶ä¸­çš„äººæœºååŒæä¾›äº†é‡è¦çš„å®è¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15534v1",
      "published_date": "2025-11-19 15:32:07 UTC",
      "updated_date": "2025-11-19 15:32:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:56:30.442956+00:00"
    },
    {
      "arxiv_id": "2511.15520v1",
      "title": "Theoretical Closed-loop Stability Bounds for Dynamical System Coupled with Diffusion Policies",
      "title_zh": "æ‰©æ•£ç­–ç•¥è€¦åˆåŠ¨åŠ›ç³»ç»Ÿçš„ç†è®ºé—­ç¯ç¨³å®šæ€§è¾¹ç•Œ",
      "authors": [
        "Gabriel Lauzier",
        "Alexandre Girard",
        "FranÃ§ois Ferland"
      ],
      "abstract": "Diffusion Policy has shown great performance in robotic manipulation tasks under stochastic perturbations, due to its ability to model multimodal action distributions. Nonetheless, its reliance on a computationally expensive reverse-time diffusion (denoising) process, for action inference, makes it challenging to use for real-time applications where quick decision-making is mandatory. This work studies the possibility of conducting the denoising process only partially before executing an action, allowing the plant to evolve according to its dynamics in parallel to the reverse-time diffusion dynamics ongoing on the computer. In a classical diffusion policy setting, the plant dynamics are usually slow and the two dynamical processes are uncoupled. Here, we investigate theoretical bounds on the stability of closed-loop systems using diffusion policies when the plant dynamics and the denoising dynamics are coupled. The contribution of this work gives a framework for faster imitation learning and a metric that yields if a controller will be stable based on the variance of the demonstrations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Diffusion Policyåœ¨æœºå™¨äººæ“çºµä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œé’ˆå¯¹å…¶åå‘æ‰©æ•£(denoising)è¿‡ç¨‹è®¡ç®—å¼€é”€å¤§ã€éš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åœ¨åŠ¨ä½œæ‰§è¡Œå‰ä»…è¿›è¡Œéƒ¨åˆ†å»å™ªå¹¶è®©ç³»ç»ŸåŠ¨åŠ›å­¦ä¸å»å™ªåŠ¨åŠ›å­¦å¹¶è¡Œæ¼”åŒ–çš„æ–¹æ¡ˆã€‚ç ”ç©¶é‡ç‚¹æ¨å¯¼äº†å½“ç³»ç»ŸåŠ¨åŠ›å­¦ä¸å»å™ªåŠ¨åŠ›å­¦è€¦åˆæ—¶ï¼Œé—­ç¯ç³»ç»Ÿç¨³å®šæ€§çš„ç†è®ºç•Œé™(theoretical bounds)ã€‚è¯¥å·¥ä½œçš„ä¸»è¦è´¡çŒ®åœ¨äºä¸ºå®ç°æ›´å¿«çš„æ¨¡ä»¿å­¦ä¹ (imitation learning)æä¾›äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªèƒ½å¤Ÿæ ¹æ®æ¼”ç¤ºæ–¹å·®é¢„æµ‹æ§åˆ¶å™¨ç¨³å®šæ€§çš„åº¦é‡æ ‡å‡†ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºæ‰©æ•£ç­–ç•¥åœ¨é«˜é€Ÿå®æ—¶ç³»ç»Ÿä¸­çš„å®‰å…¨åº”ç”¨å¥ å®šäº†é‡è¦çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "5 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.15520v1",
      "published_date": "2025-11-19 15:13:08 UTC",
      "updated_date": "2025-11-19 15:13:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:56:36.441521+00:00"
    },
    {
      "arxiv_id": "2511.15496v1",
      "title": "Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels",
      "title_zh": "è¯„ä¼°ä¸åŒå…‰ç…§å¼ºåº¦ä¸‹çš„ä½å…‰ç…§å›¾åƒå¢å¼º",
      "authors": [
        "Maria Pilligua",
        "David Serrano-Lozano",
        "Pai Peng",
        "Ramon Baldrich",
        "Michael S. Brown",
        "Javier Vazquez-Corral"
      ],
      "abstract": "Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½å…‰ç…§å›¾åƒå¢å¼º(Low-Light Image Enhancement)é¢†åŸŸç¼ºä¹è¾å°„å¤šæ ·æ€§çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå¤§å¤šæ•°åŸºäºå­¦ä¹ çš„æ–¹æ³•ä»…ä¾èµ–å•ä¸€ä½å…‰ç…§æ¡ä»¶ä¸‹çš„æˆå¯¹è®­ç»ƒæ•°æ®ï¼Œé™åˆ¶äº†å¯¹ä¸åŒå…‰ç…§å¼ºåº¦ä¸‹ç®—æ³•æ€§èƒ½çš„ç†è§£ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æ¨å‡ºäº†Multi-Illumination Low-Light (MILL)æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¶µç›–äº†å—æ§ç¯å¢ƒä¸‹å¤šç§å…‰ç…§å¼ºåº¦ä¸‹çš„å›¾åƒï¼Œå¹¶é…å¤‡äº†ç²¾ç¡®çš„ç…§åº¦æµ‹é‡ã€‚é€šè¿‡åœ¨è¯¥æ•°æ®é›†ä¸Šå¯¹å¤šç§å…ˆè¿›ç®—æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•(Benchmark)ï¼Œç ”ç©¶æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨ä¸åŒå¼ºåº¦æ°´å¹³ä¸‹å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½æ³¢åŠ¨ã€‚åˆ©ç”¨MILLç‹¬ç‰¹çš„å¤šå…‰ç…§ç»“æ„ï¼Œä½œè€…æå‡ºäº†æ—¨åœ¨æå‡å¤šæ ·åŒ–å…‰ç…§åœºæ™¯ä¸‹é²æ£’æ€§çš„æ”¹è¿›æ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ”¹è¿›åœ¨å…¨é«˜æ¸…(Full HD)å›¾åƒä¸Šä½¿å•åç›¸æœº(DSLR)å’Œæ™ºèƒ½æ‰‹æœºçš„å³°å€¼ä¿¡å™ªæ¯”(PSNR)åˆ†åˆ«æœ€é«˜æå‡äº†10 dBå’Œ2 dBï¼Œä¸ºæå‡ä½å…‰ç…§æˆåƒè´¨é‡æä¾›äº†æœ‰æ•ˆæ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15496v1",
      "published_date": "2025-11-19 14:52:51 UTC",
      "updated_date": "2025-11-19 14:52:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:56:59.652575+00:00"
    },
    {
      "arxiv_id": "2511.15763v1",
      "title": "Identifying the Supply Chain of AI for Trustworthiness and Risk Management in Critical Applications",
      "title_zh": "è¯†åˆ«äººå·¥æ™ºèƒ½ä¾›åº”é“¾ï¼šé¢å‘å…³é”®åº”ç”¨çš„å¯ä¿¡æ€§ä¸é£é™©ç®¡ç†",
      "authors": [
        "Raymond K. Sheh",
        "Karen Geappen"
      ],
      "abstract": "Risks associated with the use of AI, ranging from algorithmic bias to model hallucinations, have received much attention and extensive research across the AI community, from researchers to end-users. However, a gap exists in the systematic assessment of supply chain risks associated with the complex web of data sources, pre-trained models, agents, services, and other systems that contribute to the output of modern AI systems. This gap is particularly problematic when AI systems are used in critical applications, such as the food supply, healthcare, utilities, law, insurance, and transport.\n  We survey the current state of AI risk assessment and management, with a focus on the supply chain of AI and risks relating to the behavior and outputs of the AI system. We then present a proposed taxonomy specifically for categorizing AI supply chain entities. This taxonomy helps stakeholders, especially those without extensive AI expertise, to \"consider the right questions\" and systematically inventory dependencies across their organization's AI systems. Our contribution bridges a gap between the current state of AI governance and the urgent need for actionable risk assessment and management of AI use in critical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)åœ¨å…³é”®åº”ç”¨é¢†åŸŸé¢ä¸´çš„ä¿¡ä»»åº¦ä¸é£é™©ç®¡ç†é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰è™½ç„¶å¯¹ç®—æ³•åå·®å’Œæ¨¡å‹å¹»è§‰(model hallucinations)å·²æœ‰æ·±å…¥ç ”ç©¶ï¼Œä½†åœ¨æ¶‰åŠæ•°æ®æºã€é¢„è®­ç»ƒæ¨¡å‹ã€æ™ºèƒ½ä½“(agents)åŠæœåŠ¡ç­‰å¤æ‚ä¾›åº”ç¯èŠ‚çš„ç³»ç»Ÿæ€§è¯„ä¼°æ–¹é¢ä»å­˜åœ¨ç©ºç™½ã€‚è¿™ç§ä¾›åº”é“¾é£é™©è¯„ä¼°çš„ç¼ºå¤±åœ¨é£Ÿå“ä¾›åº”ã€åŒ»ç–—ã€æ³•å¾‹å’Œäº¤é€šç­‰å…³é”®è¡Œä¸šå°¤ä¸ºä¸¥é‡ã€‚è®ºæ–‡é€šè¿‡è°ƒç ” AI é£é™©ç®¡ç†ç°çŠ¶ï¼Œé‡ç‚¹åˆ†æäº† AI ä¾›åº”é“¾å¯¹ç³»ç»Ÿè¡Œä¸ºå’Œè¾“å‡ºçš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¸“é—¨é’ˆå¯¹ AI ä¾›åº”é“¾å®ä½“åˆ†ç±»çš„åˆ†ç±»æ³•(taxonomy)ã€‚è¯¥åˆ†ç±»æ³•æ—¨åœ¨å¸®åŠ©éæŠ€æœ¯èƒŒæ™¯çš„åˆ©ç›Šç›¸å…³è€…ç³»ç»Ÿæ€§åœ°è¯†åˆ«å’Œç›˜ç‚¹ AI ç³»ç»Ÿçš„å„é¡¹ä¾èµ–å…³ç³»ã€‚è¿™é¡¹å·¥ä½œæˆåŠŸå¼¥åˆäº†ç°æœ‰ AI æ²»ç†æ¡†æ¶ä¸å…³é”®åº”ç”¨ä¸­è¿«åˆ‡éœ€è¦çš„å¯æ“ä½œé£é™©è¯„ä¼°ä¹‹é—´çš„å·®è·ï¼Œä¸ºå»ºç«‹æ›´å…·é€æ˜åº¦å’Œå®‰å…¨æ€§çš„ AI ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at the 2025 AAAI Fall Symposium - AI Trustworthiness and Risk Assessment for Challenged Contexts (ATRACC)",
      "pdf_url": "https://arxiv.org/pdf/2511.15763v1",
      "published_date": "2025-11-19 14:52:02 UTC",
      "updated_date": "2025-11-19 14:52:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:56:50.649635+00:00"
    },
    {
      "arxiv_id": "2511.15476v1",
      "title": "RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection",
      "title_zh": "RS-CA-HSICTï¼šä¸€ç§ç”¨äºçŒ´ç—˜æ£€æµ‹çš„æ®‹å·®ä¸ç©ºé—´é€šé“å¢å¼ºå‹ CNN-Transformer æ¡†æ¶",
      "authors": [
        "Rashid Iqbal",
        "Saddam Hussain Khan"
      ],
      "abstract": "This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º RS-CA-HSICT çš„æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œ (CNN) ä¸ Transformer çš„ä¼˜åŠ¿ä»¥å¢å¼ºçŒ´ç—˜ (Monkeypox) çš„æ£€æµ‹èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç”± HSICT æ¨¡å—ã€æ®‹å·® CNN æ¨¡å—ã€ç©ºé—´ CNN æ¨¡å—åŠé€šé“å¢å¼º (CA) ç»„æˆï¼Œèƒ½å¤ŸåŒæ—¶æå–ç»†å¾®çš„ç—…å˜ä¿¡æ¯å’Œé•¿ç¨‹ä¾èµ–å…³ç³»ã€‚å…¶ä¸­ HSICT æ¨¡å—é›†æˆäº†å¤šå¤´æ³¨æ„åŠ› (Multihead Attention) ä¸ç»“æ„åŒ– CNN å±‚ï¼Œé€šè¿‡åŒè´¨ (H) å’Œç»“æ„ (S) æ“ä½œå­¦ä¹ ç©ºé—´åŒè´¨æ€§ä¸å¤æ‚çš„å½¢æ€å˜åŒ–ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé€šè¿‡ Channel-Fusion-and-Attention æ¨¡å—ç²¾ç‚¼åˆ¤åˆ«æ€§é€šé“å¹¶æŠ‘åˆ¶å†—ä½™ï¼Œç»“åˆç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ (Spatial Attention Mechanism) è¯†åˆ«ç—…å˜ä¸­çš„ç»†å¾®æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒRS-CA-HSICT åœ¨ Kaggle åŸºå‡†åŠå¤šæ ·åŒ–æ•°æ®é›†ä¸Šå–å¾—äº†é«˜è¾¾ 98.30% çš„åˆ†ç±»å‡†ç¡®ç‡å’Œ 98.13% çš„ F1-scoreï¼Œæ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ CNN å’Œè§†è§‰ Transformer (ViTs) æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "33 Pages, 12 Figure, 4 Tables",
      "pdf_url": "https://arxiv.org/pdf/2511.15476v1",
      "published_date": "2025-11-19 14:32:34 UTC",
      "updated_date": "2025-11-19 14:32:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:56:51.038623+00:00"
    },
    {
      "arxiv_id": "2511.15462v1",
      "title": "Insights from the ICLR Peer Review and Rebuttal Process",
      "title_zh": "ICLR åŒè¡Œè¯„å®¡ä¸è¾©è®ºè¿‡ç¨‹çš„æ·±åº¦æ´å¯Ÿ",
      "authors": [
        "Amir Hossein Kargaran",
        "Nafiseh Nikeghbal",
        "Jing Yang",
        "Nedjma Ousidhoum"
      ],
      "abstract": "Peer review is a cornerstone of scientific publishing, including at premier machine learning conferences such as ICLR. As submission volumes increase, understanding the nature and dynamics of the review process is crucial for improving its efficiency, effectiveness, and the quality of published papers. We present a large-scale analysis of the ICLR 2024 and 2025 peer review processes, focusing on before- and after-rebuttal scores and reviewer-author interactions. We examine review scores, author-reviewer engagement, temporal patterns in review submissions, and co-reviewer influence effects. Combining quantitative analyses with LLM-based categorization of review texts and rebuttal discussions, we identify common strengths and weaknesses for each rating group, as well as trends in rebuttal strategies that are most strongly associated with score changes. Our findings show that initial scores and the ratings of co-reviewers are the strongest predictors of score changes during the rebuttal, pointing to a degree of reviewer influence. Rebuttals play a valuable role in improving outcomes for borderline papers, where thoughtful author responses can meaningfully shift reviewer perspectives. More broadly, our study offers evidence-based insights to improve the peer review process, guiding authors on effective rebuttal strategies and helping the community design fairer and more efficient review processes. Our code and score changes data are available at https://github.com/papercopilot/iclr-insights.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ ICLR 2024 å’Œ 2025 çš„ Peer Review è¿‡ç¨‹è¿›è¡Œäº†å¤§è§„æ¨¡åˆ†æï¼Œé‡ç‚¹æ¢è®¨äº† Rebuttal å‰åçš„è¯„åˆ†åŠ¨æ€ä»¥åŠä½œè€…ä¸å®¡ç¨¿äººä¹‹é—´çš„äº’åŠ¨ã€‚é€šè¿‡ç»“åˆå®šé‡åˆ†æä¸åŸºäº LLM çš„æ–‡æœ¬åˆ†ç±»ï¼Œç ”ç©¶åˆ†æäº†è¯„åˆ†è¶‹åŠ¿ã€ä½œè€…å‚ä¸åº¦ä»¥åŠå…±åŒå®¡ç¨¿äººï¼ˆco-reviewerï¼‰ä¹‹é—´çš„ç›¸äº’å½±å“æ•ˆåº”ã€‚ç ”ç©¶å‘ç°ï¼Œåˆå§‹è¯„åˆ†å’Œå…±åŒå®¡ç¨¿äººçš„è¯„ä»·æ˜¯é¢„æµ‹ Rebuttal æœŸé—´åˆ†æ•°å˜åŒ–çš„æœ€å¼ºæŒ‡æ ‡ï¼Œè¡¨æ˜å®¡ç¨¿äººä¹‹é—´å­˜åœ¨ä¸€å®šçš„ç›¸äº’å½±å“ã€‚å¯¹äºå¤„äºåŠæ ¼çº¿ï¼ˆborderlineï¼‰è¾¹ç¼˜çš„è®ºæ–‡ï¼ŒRebuttal åœ¨æ”¹å–„è¯„å®¡ç»“æœæ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œæœ‰æ·±åº¦ä¸”å‘¨å…¨çš„ä½œè€…å›å¤èƒ½æœ‰æ•ˆè½¬å˜å®¡ç¨¿äººçš„ç«‹åœºã€‚è¯¥ç ”ç©¶ä¸ºä½œè€…æä¾›äº†æœ‰æ•ˆçš„ Rebuttal ç­–ç•¥æŒ‡å¯¼ï¼Œå¹¶ä¸ºå­¦æœ¯ç•Œè®¾è®¡æ›´å…¬å¹³ã€é«˜æ•ˆçš„ Peer Review æµç¨‹æä¾›äº†åŸºäºæ•°æ®çš„å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15462v1",
      "published_date": "2025-11-19 14:21:52 UTC",
      "updated_date": "2025-11-19 14:21:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:57:09.143449+00:00"
    },
    {
      "arxiv_id": "2511.15456v1",
      "title": "Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining",
      "title_zh": "Know Your Intentï¼šä¸€ç§é¢å‘ DeFi ç”¨æˆ·äº¤æ˜“æ„å›¾æŒ–æ˜çš„è‡ªä¸»å¤šè§†è§’å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Qian'ang Mao",
        "Yuxuan Zhang",
        "Jiaman Chen",
        "Wenjun Zhou",
        "Jiaqi Yan"
      ],
      "abstract": "As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Transaction Intent Mining (TIM)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å»ä¸­å¿ƒåŒ–é‡‘è(DeFi)ä¸­å› æ™ºèƒ½åˆçº¦äº¤äº’å¤æ‚å’Œæ—¥å¿—ä¸é€æ˜å¯¼è‡´çš„ç”¨æˆ·äº¤æ˜“æ„å›¾è¯†åˆ«éš¾é¢˜ã€‚TIMæ¡†æ¶åŸºäºæ‰æ ¹ç†è®º(grounded theory)æ„å»ºäº†æ„å›¾åˆ†ç±»ä½“ç³»ï¼Œå¹¶é‡‡ç”¨å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹(multi-agent LLM)ç³»ç»Ÿè¿›è¡Œè‡ªä¸»æ¨ç†ã€‚æ¡†æ¶åˆ©ç”¨å…ƒçº§è§„åˆ’å™¨(Meta-Level Planner)åŠ¨æ€åè°ƒé¢†åŸŸä¸“å®¶ï¼Œé€šè¿‡ä»»åŠ¡æ±‚è§£å™¨(Question Solvers)å¤„ç†å¤šæ¨¡æ€çš„é“¾ä¸Šä¸é“¾ä¸‹æ•°æ®ï¼Œå¹¶å¼•å…¥è®¤çŸ¥è¯„ä¼°å™¨(Cognitive Evaluator)ä»¥é™ä½LLMå¹»è§‰å¹¶æå‡ç»“æœçš„å¯éªŒè¯æ€§ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒTIMåœ¨æŒ–æ˜å‡†ç¡®æ€§ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ ã€å•ä¸€LLMåŠå•ä¸€æ™ºèƒ½ä½“åŸºçº¿æ¨¡å‹ã€‚è¯¥é¡¹å·¥ä½œä¸ºå¤æ‚åŒºå—é“¾æ´»åŠ¨æä¾›äº†å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„è§£é‡Šï¼Œä¸ºç†è§£DeFiç”¨æˆ·çš„è¡Œä¸ºåŠ¨æœºæä¾›äº†æ›´å¯é çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "q-fin.GN"
      ],
      "primary_category": "cs.AI",
      "comment": "Written in 2025 Q1",
      "pdf_url": "https://arxiv.org/pdf/2511.15456v1",
      "published_date": "2025-11-19 14:15:23 UTC",
      "updated_date": "2025-11-19 14:15:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:57:55.490654+00:00"
    },
    {
      "arxiv_id": "2511.15762v1",
      "title": "A time for monsters: Organizational knowing after LLMs",
      "title_zh": "æ€ªç‰©å‡ºæ²¡çš„æ—¶ä»£ï¼šå¤§è¯­è¨€æ¨¡å‹èƒŒæ™¯ä¸‹çš„ç»„ç»‡è®¤çŸ¥",
      "authors": [
        "Samer Faraj",
        "Joel Perez Torrents",
        "Saku Mantere",
        "Anand Bhardwaj"
      ],
      "abstract": "Large Language Models (LLMs) are reshaping organizational knowing by unsettling the epistemological foundations of representational and practice-based perspectives. We conceptualize LLMs as Haraway-ian monsters, that is, hybrid, boundary-crossing entities that destabilize established categories while opening new possibilities for inquiry. Focusing on analogizing as a fundamental driver of knowledge, we examine how LLMs generate connections through large-scale statistical inference. Analyzing their operation across the dimensions of surface/deep analogies and near/far domains, we highlight both their capacity to expand organizational knowing and the epistemic risks they introduce. Building on this, we identify three challenges of living with such epistemic monsters: the transformation of inquiry, the growing need for dialogical vetting, and the redistribution of agency. By foregrounding the entangled dynamics of knowing-with-LLMs, the paper extends organizational theory beyond human-centered epistemologies and invites renewed attention to how knowledge is created, validated, and acted upon in the age of intelligent technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¦‚ä½•é€šè¿‡åŠ¨æ‘‡è¡¨å¾å’ŒåŸºäºå®è·µçš„è§†è§’ï¼Œé‡å¡‘ç»„ç»‡è®¤çŸ¥(organizational knowing)çš„è®¤è¯†è®ºåŸºç¡€ã€‚ä½œè€…å°†LLMsæ¦‚å¿µåŒ–ä¸ºâ€œHaraway-ian monstersâ€ï¼Œå³ç ´åæ—¢æœ‰èŒƒç•´å¹¶ä¸ºæ¢ç©¶å¼€è¾Ÿæ–°å¯èƒ½æ€§çš„æ··åˆè·¨ç•Œå®ä½“ã€‚è®ºæ–‡é‡ç‚¹ç ”ç©¶äº†ç±»æ¯”(analogizing)ä½œä¸ºçŸ¥è¯†é©±åŠ¨åŠ›çš„ä½œç”¨ï¼Œåˆ†æäº†LLMså¦‚ä½•é€šè¿‡å¤§è§„æ¨¡ç»Ÿè®¡æ¨ç†åœ¨è¡¨å±‚/æ·±å±‚ç±»æ¯”(surface/deep analogies)ä»¥åŠè¿‘/è¿œé¢†åŸŸ(near/far domains)ç»´åº¦ä¸Šå»ºç«‹è¿æ¥ã€‚ç ”ç©¶å¼ºè°ƒäº†LLMsåœ¨æ‰©å±•ç»„ç»‡è®¤çŸ¥èƒ½åŠ›çš„åŒæ—¶æ‰€å¸¦æ¥çš„è®¤è¯†è®ºé£é™©(epistemic risks)ã€‚è®ºæ–‡è¿›ä¸€æ­¥è¯†åˆ«äº†ä¸æ­¤ç±»â€œè®¤è¯†è®ºæ€ªç‰©â€å…±å­˜çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šæ¢ç©¶æ–¹å¼çš„è½¬å‹ã€å¯¹è¯å¼å®¡æ ¸(dialogical vetting)éœ€æ±‚çš„å¢é•¿ä»¥åŠæœºæ„ä»£ç†æƒ(agency)çš„å†åˆ†é…ã€‚é€šè¿‡å…³æ³¨ä¸LLMså…±åŒè®¤çŸ¥çš„çº ç¼ åŠ¨æ€ï¼Œè¯¥ç ”ç©¶å°†ç»„ç»‡ç†è®ºæ‰©å±•åˆ°äº†ä»¥äººä¸ºä¸­å¿ƒä¹‹å¤–çš„è®¤è¯†è®ºï¼Œå¹¶å‘¼ååœ¨æ™ºèƒ½æŠ€æœ¯æ—¶ä»£é‡æ–°å®¡è§†çŸ¥è¯†çš„åˆ›é€ ã€éªŒè¯ä¸æ‰§è¡Œã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Forthcoming at Strategic Organization",
      "pdf_url": "https://arxiv.org/pdf/2511.15762v1",
      "published_date": "2025-11-19 14:07:47 UTC",
      "updated_date": "2025-11-19 14:07:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:57:32.647872+00:00"
    },
    {
      "arxiv_id": "2511.15447v1",
      "title": "TSFM in-context learning for time-series classification of bearing-health status",
      "title_zh": "åŸºäº TSFM ä¸Šä¸‹æ–‡å­¦ä¹ çš„è½´æ‰¿å¥åº·çŠ¶æ€æ—¶é—´åºåˆ—åˆ†ç±»",
      "authors": [
        "Michel Tokic",
        "Slobodan DjukanoviÄ‡",
        "Anja von Beuningen",
        "Cheng Feng"
      ],
      "abstract": "This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ä¸€ç§åœ¨æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ (TSFM) ä¸­åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹  (in-context learning) è¿›è¡Œè½´æ‰¿å¥åº·çŠ¶æ€åˆ†ç±»çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å…è®¸åœ¨ä¸è¿›è¡Œæ¨¡å‹å¾®è°ƒ (fine-tuning) çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åœ¨æç¤ºè¯ (prompt) ä¸­åŒ…å«ç›®æ ‡æ ‡ç­¾å’Œåå˜é‡ (covariates) ä¿¡æ¯ï¼Œå®ç°å¯¹æœªè§æ•°æ®çš„åˆ†ç±»é¢„æµ‹ã€‚ç ”ç©¶äººå‘˜å°†æ­¤æŠ€æœ¯åº”ç”¨äºä¼ºæœå‹åŠ›æœºç”µæœºçš„æŒ¯åŠ¨æ•°æ®ï¼Œé€šè¿‡å°†é¢‘åŸŸå‚è€ƒä¿¡å·è½¬æ¢ä¸ºä¼ªæ—¶é—´åºåˆ— (pseudo time-series) æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨ TSFM é¢„æµ‹åˆ†ç±»æ¦‚ç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¿è¡Œæ¡ä»¶ä¸‹å‡å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œå……åˆ†å±•ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹çš„æ‰©å±•æ€§ã€‚è¿™ä¸€è¿›å±•æ ‡å¿—ç€å·¥ä¸šç»´æŠ¤ç³»ç»Ÿæ­£ä»ä¼ ç»Ÿçš„ç‹­ä¹‰äººå·¥æ™ºèƒ½ (narrow AI) æ–¹æ¡ˆå‘æ›´å¹¿æ³›çš„ã€ç”±åŸºç¡€æ¨¡å‹é©±åŠ¨çš„æ™ºèƒ½åŒ–ç³»ç»Ÿè½¬å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint submitted to ESANN 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.15447v1",
      "published_date": "2025-11-19 14:01:12 UTC",
      "updated_date": "2025-11-19 14:01:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:57:34.035643+00:00"
    },
    {
      "arxiv_id": "2511.15435v1",
      "title": "HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation",
      "title_zh": "HV-Attackï¼šé¢å‘å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„åˆ†å±‚è§†è§‰æ”»å‡»",
      "authors": [
        "Linyin Luo",
        "Yujuan Ding",
        "Yunshan Ma",
        "Wenqi Fan",
        "Hanjiang Lai"
      ],
      "abstract": "Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HV-Attackï¼Œä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆ (Multimodal Retrieval-Augmented Generation, MRAG) ç³»ç»Ÿçš„åˆ†å±‚è§†è§‰æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨æ¢è®¨åœ¨ä»…æ”¹å˜ç”¨æˆ·å›¾åƒè¾“å…¥ä¸”ä¸è§¦åŠ¨ç³»ç»Ÿå…¶ä»–ç»„ä»¶çš„æƒ…å†µä¸‹æ‰€é¢ä¸´çš„å®‰å…¨å¨èƒã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨å›¾åƒè¾“å…¥ä¸­æ·»åŠ ä¸å¯å¯Ÿè§‰çš„æ‰°åŠ¨ï¼Œä½¿ MRAG ç”Ÿæˆå™¨çš„å¤šæ¨¡æ€æŸ¥è¯¢ä¸å¢å¼ºçŸ¥è¯†ä¹‹é—´äº§ç”Ÿé”™ä½å’Œå¹²æ‰°ï¼Œä»è€Œæ··æ·†ç”Ÿæˆç»“æœã€‚å…·ä½“è€Œè¨€ï¼ŒHV-Attack é‡‡ç”¨åˆ†å±‚ä¸¤é˜¶æ®µç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŒ–æ‰°åŠ¨å…ˆåç ´åè·¨æ¨¡æ€å¯¹é½ (cross-modal alignment) å’Œå¤šæ¨¡æ€è¯­ä¹‰å¯¹é½ (multimodal semantic alignment)ï¼Œè¯±å¯¼æ£€ç´¢å™¨å¬å›æ— å…³çš„é”™è¯¯çŸ¥è¯†ã€‚åœ¨ OK-VQA å’Œ InfoSeek æ•°æ®é›†ä¸Šé’ˆå¯¹ BLIP-2 å’Œ LLaVA ç­‰å¤§æ¨¡æ€æ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼ŒHV-Attack èƒ½æ˜¾è‘—é™ä½ç³»ç»Ÿçš„æ£€ç´¢ä¸ç”Ÿæˆæ€§èƒ½ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å³ä¾¿åœ¨æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨å…·æœ‰ä¸€å®šé²æ£’æ€§çš„æƒ…å†µä¸‹ï¼ŒMRAG ç³»ç»Ÿä¾ç„¶å®¹æ˜“å—åˆ°é’ˆå¯¹è¾“å…¥ç«¯çš„è§†è§‰æ”»å‡»å½±å“ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15435v1",
      "published_date": "2025-11-19 13:45:24 UTC",
      "updated_date": "2025-11-19 13:45:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:57:49.237618+00:00"
    },
    {
      "arxiv_id": "2511.15434v1",
      "title": "Small Language Models for Phishing Website Detection: Cost, Performance, and Privacy Trade-Offs",
      "title_zh": "ç”¨äºé’“é±¼ç½‘ç«™æ£€æµ‹çš„å°è¯­è¨€æ¨¡å‹ï¼šæˆæœ¬ã€æ€§èƒ½ä¸éšç§çš„æƒè¡¡",
      "authors": [
        "Georg Goldenits",
        "Philip Koenig",
        "Sebastian Raubitzek",
        "Andreas Ekelhart"
      ],
      "abstract": "Phishing websites pose a major cybersecurity threat, exploiting unsuspecting users and causing significant financial and organisational harm. Traditional machine learning approaches for phishing detection often require extensive feature engineering, continuous retraining, and costly infrastructure maintenance. At the same time, proprietary large language models (LLMs) have demonstrated strong performance in phishing-related classification tasks, but their operational costs and reliance on external providers limit their practical adoption in many business environments. This paper investigates the feasibility of small language models (SLMs) for detecting phishing websites using only their raw HTML code. A key advantage of these models is that they can be deployed on local infrastructure, providing organisations with greater control over data and operations. We systematically evaluate 15 commonly used Small Language Models (SLMs), ranging from 1 billion to 70 billion parameters, benchmarking their classification accuracy, computational requirements, and cost-efficiency. Our results highlight the trade-offs between detection performance and resource consumption, demonstrating that while SLMs underperform compared to state-of-the-art proprietary LLMs, they can still provide a viable and scalable alternative to external LLM services. By presenting a comparative analysis of costs and benefits, this work lays the foundation for future research on the adaptation, fine-tuning, and deployment of SLMs in phishing detection systems, aiming to balance security effectiveness and economic practicality.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å°è¯­è¨€æ¨¡å‹(Small Language Models, SLMs)ä»…é€šè¿‡åŸå§‹HTMLä»£ç æ£€æµ‹é’“é±¼ç½‘ç«™çš„å¯è¡Œæ€§ï¼Œä»¥è§£å†³ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨ç‰¹å¾å·¥ç¨‹ä¸Šçš„ç¹çä»¥åŠé—­æºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æˆæœ¬å’Œéšç§æ–¹é¢çš„å±€é™ã€‚ä½œè€…ç³»ç»Ÿè¯„ä¼°äº†15ç§å‚æ•°è§„æ¨¡ä»10äº¿åˆ°700äº¿ä¸ç­‰çš„å¸¸ç”¨SLMsï¼Œå¯¹å…¶åˆ†ç±»å‡†ç¡®ç‡ã€è®¡ç®—èµ„æºéœ€æ±‚å’Œæˆæœ¬æ•ˆç›Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ£€æµ‹æ€§èƒ½ä¸èµ„æºæ¶ˆè€—ä¹‹é—´çš„æƒè¡¡ï¼Œè¡¨æ˜è™½ç„¶SLMsçš„æ€§èƒ½ç•¥ä½äºé¡¶å°–çš„é—­æºLLMsï¼Œä½†å…¶ä½œä¸ºå¯æœ¬åœ°éƒ¨ç½²çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨æ•°æ®æ§åˆ¶å’Œç»æµå®ç”¨æ€§ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¯¥å·¥ä½œé€šè¿‡å¯¹æˆæœ¬ä¸æ”¶ç›Šçš„æ¯”è¾ƒåˆ†æï¼Œä¸ºæœªæ¥åœ¨é’“é±¼æ£€æµ‹ç³»ç»Ÿä¸­å¾®è°ƒå’Œéƒ¨ç½²SLMsæä¾›äº†å‚è€ƒï¼Œæ—¨åœ¨å¹³è¡¡å®‰å…¨æ•ˆèƒ½ä¸ç»æµæˆæœ¬ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15434v1",
      "published_date": "2025-11-19 13:45:07 UTC",
      "updated_date": "2025-11-19 13:45:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:57:45.648189+00:00"
    },
    {
      "arxiv_id": "2511.15432v1",
      "title": "Towards Understanding Layer Contributions in Tabular In-Context Learning Models",
      "title_zh": "æ¢ç©¶è¡¨æ ¼æ•°æ®ä¸Šä¸‹æ–‡å­¦ä¹ æ¨¡å‹ä¸­çš„å±‚è´¡çŒ®",
      "authors": [
        "Amir Rezaei Balef",
        "Mykhailo Koshil",
        "Katharina Eggensperger"
      ],
      "abstract": "Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the \"layers as painters\" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†è¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹  (tabular in-context learning, ICL) æ¨¡å‹ä¸­å„å±‚å¯¹é¢„æµ‹çš„å…·ä½“è´¡çŒ®ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸä¸å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ¶æ„ç›¸ä¼¼æ€§èƒŒåæœºåˆ¶ç ”ç©¶ä¸Šçš„ç©ºç™½ã€‚ä½œè€…é€šè¿‡â€œå±‚çº§å¦‚ç”»å¸ˆâ€ (layers as painters) çš„è§†è§’ï¼Œæ·±å…¥åˆ†æäº† TabPFN å’Œ TabICL æ¨¡å‹ä¸­æ½œç©ºé—´ (latent spaces) éšå±‚æ•°å˜åŒ–çš„æ¼”è¿›è¿‡ç¨‹ï¼Œå¹¶å¯¹æ¯”äº†è¿™äº›åŠ¨æ€å˜åŒ–ä¸ LLMs ä¸­è§‚å¯Ÿåˆ°çš„ç°è±¡ã€‚ç ”ç©¶å‘ç°æ¨¡å‹ä¸­ä»…æœ‰éƒ¨åˆ†å±‚çº§å­é›†å…±äº«é€šç”¨çš„è¡¨ç¤ºè¯­è¨€ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„ç»“æ„å†—ä½™ (structural redundancy)ã€‚è¿™ä¸€å‘ç°ä¸ä»…å¢è¿›äº†å¯¹æ¨¡å‹å†…éƒ¨è¿ä½œæœºåˆ¶çš„ç†è§£ï¼Œä¹Ÿä¸ºè¡¨æ ¼ ICL æ¨¡å‹çš„æ¨¡å‹å‹ç¼© (model compression) å’Œæé«˜å¯è§£é‡Šæ€§ (interpretability) æä¾›äº†é‡è¦å¥‘æœºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the EurIPS 2025 Workshop on AI for Tabular Data",
      "pdf_url": "https://arxiv.org/pdf/2511.15432v1",
      "published_date": "2025-11-19 13:39:30 UTC",
      "updated_date": "2025-11-19 13:39:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:57:56.036449+00:00"
    },
    {
      "arxiv_id": "2511.15418v1",
      "title": "Building Robust and Scalable Multilingual ASR for Indian Languages",
      "title_zh": "æ„å»ºé¢å‘å°åº¦è¯­è¨€çš„é²æ£’ä¸”å¯æ‰©å±•çš„å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ",
      "authors": [
        "Arjun Gangwar",
        "Kaousheik Jayakumar",
        "S. Umesh"
      ],
      "abstract": "This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ç”±å°åº¦ç†å·¥å­¦é™¢é©¬å¾·æ‹‰æ–¯åˆ†æ ¡ SPRING Lab ä¸º ASRU MADASR 2.0 æŒ‘æˆ˜èµ›å¼€å‘çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œæ—¨åœ¨æå‡é’ˆå¯¹ 8 ç§è¯­è¨€åŠå…¶ 33 ç§æ–¹è¨€çš„å¤šè¯­ç§è¯­éŸ³è¯†åˆ«(Multilingual ASR)æ€§èƒ½ã€‚é’ˆå¯¹èµ›é“é™åˆ¶æ¡ä»¶ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§é‡‡ç”¨å¤šè§£ç å™¨(Multi-Decoder)æ¶æ„çš„æ–°é¢–è®­ç»ƒæ–¹æ³•ï¼Œå¹¶åˆ©ç”¨éŸ³ç´ é€šç”¨æ ‡ç­¾é›†(Common Label Set, CLS)ä½œä¸ºä¸­é—´è¡¨ç¤ºã€‚è¯¥ç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†åœ¨å°†éŸ³ç´ ç©ºé—´çš„æ€§èƒ½å¢ç›Šè½¬æ¢å›ç›¸åº”å­—å½¢è¡¨ç¤º(grapheme representations)æ—¶çš„å¤šç§ä¼˜åŒ–æ‰‹æ®µã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨ CLS ç©ºé—´å†…æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨ Track 2 çš„ä¸‰ç§è¯­è¨€ä¸­å®ç°äº†æ›´ä½çš„å­—é”™ç‡(WER)æˆ–å­—ç¬¦é”™è¯¯ç‡(CER)ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆåœ¨æ‰€æœ‰å‚èµ›é˜Ÿä¼ä¸­å–å¾—äº†æœ€é«˜çš„è¯­è¨€è¯†åˆ«(Language ID)å’Œæ–¹è¨€è¯†åˆ«(Dialect ID)å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚æ–¹è¨€ç¯å¢ƒä¸‹çš„é²æ£’æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15418v1",
      "published_date": "2025-11-19 13:17:16 UTC",
      "updated_date": "2025-11-19 13:17:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:57:51.541686+00:00"
    },
    {
      "arxiv_id": "2511.15414v1",
      "title": "RRT*former: Environment-Aware Sampling-Based Motion Planning using Transformer",
      "title_zh": "RRT*formerï¼šåŸºäº Transformer çš„ç¯å¢ƒæ„ŸçŸ¥é‡‡æ ·è¿åŠ¨è§„åˆ’",
      "authors": [
        "Mingyang Feng",
        "Shaoyuan Li",
        "Xiang Yin"
      ],
      "abstract": "We investigate the sampling-based optimal path planning problem for robotics in complex and dynamic environments. Most existing sampling-based algorithms neglect environmental information or the information from previous samples. Yet, these pieces of information are highly informative, as leveraging them can provide better heuristics when sampling the next state. In this paper, we propose a novel sampling-based planning algorithm, called \\emph{RRT*former}, which integrates the standard RRT* algorithm with a Transformer network in a novel way. Specifically, the Transformer is used to extract features from the environment and leverage information from previous samples to better guide the sampling process. Our extensive experiments demonstrate that, compared to existing sampling-based approaches such as RRT*, Neural RRT*, and their variants, our algorithm achieves considerable improvements in both the optimality of the path and sampling efficiency. The code for our implementation is available on https://github.com/fengmingyang666/RRTformer.",
      "tldr_zh": "é’ˆå¯¹å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹æœºå™¨äººçš„é‡‡æ ·è·¯å¾„è§„åˆ’é—®é¢˜ï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºä¼ ç»Ÿç®—æ³•å¾€å¾€å¿½è§†äº†ç¯å¢ƒä¿¡æ¯åŠå†å²é‡‡æ ·æ•°æ®å¯¹åç»­é‡‡æ ·çš„å¯å‘å¼ä½œç”¨ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº† RRT*former ç®—æ³•ï¼Œé€šè¿‡å°† Transformer ç½‘ç»œä¸æ ‡å‡† RRT* æ¡†æ¶ç›¸ç»“åˆï¼Œå®ç°å¯¹ç¯å¢ƒç‰¹å¾çš„æ·±åº¦æå–ä»¥åŠå¯¹è¿‡å¾€é‡‡æ ·ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨ Transformer æ›´å¥½åœ°å¼•å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œä»è€Œåœ¨æœç´¢ç©ºé—´ä¸­æä¾›æ›´ç²¾å‡†çš„å¯å‘å¼æŒ‡å¼•ã€‚å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäº RRT*ã€Neural RRT* åŠå…¶å˜ä½“ï¼ŒRRT*former åœ¨è·¯å¾„æœ€ä¼˜æ€§å’Œé‡‡æ ·æ•ˆç‡ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥æ–¹æ³•ä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„æ™ºèƒ½è¿åŠ¨è§„åˆ’æä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ä¼ ç»Ÿé‡‡æ ·ç®—æ³•ç»“åˆçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to IROS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.15414v1",
      "published_date": "2025-11-19 13:14:10 UTC",
      "updated_date": "2025-11-19 13:14:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:57:55.685839+00:00"
    },
    {
      "arxiv_id": "2511.15408v1",
      "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework",
      "title_zh": "NAMeGEnï¼šåŸºäºæ–°å‹å¤šæ™ºèƒ½ä½“å¤šé‡ä¸ªæ€§åŒ–ç›®æ ‡å¢å¼ºæ¡†æ¶çš„åˆ›æ„å§“åç”Ÿæˆ",
      "authors": [
        "Shanlin Zhou",
        "Xinpeng Wang",
        "Jianxun Lian",
        "Zhenghao Liu",
        "Laks V. S. Lakshmanan",
        "Xiaoyuan Yi",
        "Yongtao Hao"
      ],
      "abstract": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åˆ›æ„è‡ªç„¶è¯­è¨€ç”Ÿæˆ(Creative Natural Language Generation, CNLG)ä¸­é¢ä¸´çš„å¤šç›®æ ‡çµæ´»æ€§ä¸è§£é‡Šå¤æ‚æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºNAMeGEnçš„å¤šæ™ºèƒ½ä½“ä¼˜åŒ–(multi-agent optimization)æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»¥ä¸­æ–‡å–åä¸ºæ ¸å¿ƒåº”ç”¨åœºæ™¯ï¼Œé€šè¿‡åœ¨ç›®æ ‡æå–(objective extraction)ã€åç§°ç”Ÿæˆ(name generation)ä¸è¯„ä¼°(evaluation)ç¯èŠ‚é—´è¿›è¡Œè¿­ä»£ï¼Œç¡®ä¿ç”Ÿæˆçš„åç§°èƒ½åŒæ—¶æ»¡è¶³ç”¨æˆ·ä¸ªæ€§åŒ–çš„ç»†ç²’åº¦çº¦æŸå¹¶æä¾›æ·±åº¦çš„ç¾å­¦è§£é‡Šã€‚ä¸ºæ”¯æŒè¿™ä¸€ä»»åŠ¡ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«1.7ä¸‡ä½™é¦–è¯—æ­Œçš„å¤å…¸è¯—è¯è¯­æ–™åº“ï¼Œå¹¶å¼•å…¥äº†å¸¦æœ‰å®šåˆ¶åŒ–æŒ‡æ ‡çš„æ–°åŸºå‡†CBNamesã€‚å®éªŒç»“æœè¯æ˜ï¼ŒNAMeGEnåœ¨æ— éœ€ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆåˆ›æ„åç§°çš„æ•ˆæœåŠè§£é‡Šçš„å‡†ç¡®æ€§å‡æ˜¾è‘—ä¼˜äºå…­ç§åŸºçº¿æ–¹æ³•ï¼ŒæˆåŠŸè§£å†³äº†çŸ­æ–‡æœ¬åˆ›æ„ç”Ÿæˆä¸­çš„å…³é”®éš¾é¢˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.MA",
        "cs.NE"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages,9 figures. This work has been submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2511.15408v1",
      "published_date": "2025-11-19 13:05:25 UTC",
      "updated_date": "2025-11-19 13:05:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:58:21.241042+00:00"
    },
    {
      "arxiv_id": "2511.15407v2",
      "title": "IPR-1: Interactive Physical Reasoner",
      "title_zh": "IPR-1ï¼šäº¤äº’å¼ç‰©ç†æ¨ç†å™¨",
      "authors": [
        "Mingyu Zhang",
        "Lifeng Zhuo",
        "Tianxi Tan",
        "Guocan Xie",
        "Xian Nie",
        "Yan Li",
        "Renjie Zhao",
        "Zizhu He",
        "Ziyu Wang",
        "Jiting Cai",
        "Yong-Lu Li"
      ],
      "abstract": "Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IPR-1 (Interactive Physical Reasoner)ï¼Œæ—¨åœ¨ä½¿æ™ºèƒ½ä½“é€šè¿‡ç¯å¢ƒäº¤äº’è·å–ç±»äººçš„ç‰©ç†æ¨ç†å’Œå› æœç†è§£èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)å’Œä¸–ç•Œæ¨¡å‹(world models)éš¾ä»¥æ•æ‰åº•å±‚ç‰©ç†æœºåˆ¶ä¸”å®¹æ˜“è¿‡æ‹Ÿåˆè§†è§‰ç»†èŠ‚çš„é—®é¢˜ï¼Œç ”ç©¶è€…å¼•å…¥äº†åŒ…å«1000å¤šç§å¼‚æ„æ¸¸æˆçš„G2U (Game-to-Unseen) åŸºå‡†æµ‹è¯•ã€‚IPRåˆ©ç”¨ä¸–ç•Œæ¨¡å‹çš„å›æ”¾(rollouts)æ¥è¯„åˆ†å¹¶å¢å¼ºVLMçš„ç­–ç•¥ï¼ŒåŒæ—¶å¼•å…¥äº†PhysCodeï¼Œè¿™æ˜¯ä¸€ç§å°†è¯­ä¹‰æ„å›¾ä¸åŠ¨åŠ›å­¦å¯¹é½çš„ç‰©ç†ä¸­å¿ƒåŒ–åŠ¨ä½œä»£ç ï¼Œä¸ºé¢„æµ‹å’Œæ¨ç†æä¾›å…±äº«çš„åŠ¨ä½œç©ºé—´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIPRåœ¨ä»åŸå§‹ç›´è§‰åˆ°ç›®æ ‡é©±åŠ¨æ¨ç†çš„å„çº§ä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ï¼Œæ•´ä½“è¡¨ç°ç”šè‡³è¶…è¶Šäº†GPT-5ã€‚ç ”ç©¶å‘ç°æ¨¡å‹æ€§èƒ½éšè®­ç»ƒæ¸¸æˆæ•°é‡å’Œäº¤äº’æ­¥æ•°çš„å¢åŠ è€ŒæŒç»­æå‡ï¼Œä¸”èƒ½å¤Ÿå®ç°å¯¹æœªçŸ¥æ¸¸æˆçš„é›¶æ ·æœ¬è¿ç§»(zero-shot transfer)ã€‚è¿™äº›æˆæœè¯æ˜äº†ä»¥ç‰©ç†ä¸ºä¸­å¿ƒçš„äº¤äº’æ˜¯å®ç°æŒç»­æ”¹è¿›ç‰©ç†æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages of main text and 19 pages of appendices. Project page: https://mybearyzhang.github.io/ipr-1",
      "pdf_url": "https://arxiv.org/pdf/2511.15407v2",
      "published_date": "2025-11-19 13:04:44 UTC",
      "updated_date": "2025-12-15 14:03:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:58:27.743605+00:00"
    },
    {
      "arxiv_id": "2511.15392v1",
      "title": "DEPO: Dual-Efficiency Preference Optimization for LLM Agents",
      "title_zh": "DEPOï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„åŒé‡æ•ˆç‡åå¥½ä¼˜åŒ–",
      "authors": [
        "Sirui Chen",
        "Mengshi Zhao",
        "Lei Xu",
        "Yuying Zhao",
        "Beier Zhu",
        "Hanwang Zhang",
        "Shengjie Zhao",
        "Chaochao Lu"
      ],
      "abstract": "Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æ™ºèƒ½ä½“åœ¨å¤æ‚æ¨ç†ä¸­å› é•¿é“¾å¼æ€ç»´(Chain-of-Thought)å¯¼è‡´çš„äº¤äº’æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œç³»ç»Ÿåœ°å®šä¹‰äº†åŒé‡æ•ˆç‡(dual-efficiency)æ¦‚å¿µï¼ŒåŒ…æ‹¬å‡å°‘å•æ­¥Tokenæ¶ˆè€—çš„æ­¥çº§æ•ˆç‡(step-level efficiency)å’Œå‡å°‘ä»»åŠ¡æ€»æ­¥æ•°çš„è½¨è¿¹çº§æ•ˆç‡(trajectory-level efficiency)ã€‚åŸºäºæ­¤å®šä¹‰ï¼Œä½œè€…æå‡ºäº†DEPO (Dual-Efficiency Preference Optimization) æ–¹æ³•ï¼Œé€šè¿‡åå¥½ä¼˜åŒ–åŒæ—¶å¥–åŠ±ç®€æ´çš„å“åº”å’Œæ›´å°‘çš„è¡ŒåŠ¨æ­¥éª¤ã€‚åœ¨WebShopå’ŒBabyAIä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDEPOåœ¨æ€§èƒ½æå‡é«˜è¾¾29.3%çš„åŒæ—¶ï¼Œåˆ†åˆ«æœ€é«˜å‡å°‘äº†60.9%çš„Tokenä½¿ç”¨é‡å’Œ26.9%çš„ä»»åŠ¡æ­¥æ•°ã€‚æ­¤å¤–ï¼ŒDEPOåœ¨ä¸‰ä¸ªåŸŸå¤–æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”åœ¨ä»…ä½¿ç”¨25%è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ä¾ç„¶èƒ½ä¿æŒæ˜¾è‘—çš„æ•ˆç‡å¢ç›Šã€‚è¯¥ç ”ç©¶é€šè¿‡ä¼˜åŒ–æ¨ç†è·¯å¾„ï¼Œä¸ºæ„å»ºé«˜æ€§èƒ½ä¸”ä½æˆæœ¬çš„è‡ªä¸»æ™ºèƒ½ä½“æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.15392v1",
      "published_date": "2025-11-19 12:38:43 UTC",
      "updated_date": "2025-11-19 12:38:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:58:41.371115+00:00"
    },
    {
      "arxiv_id": "2511.15383v1",
      "title": "A Compliance-Preserving Retrieval System for Aircraft MRO Task Search",
      "title_zh": "é¢å‘é£æœº MRO ä»»åŠ¡æœç´¢çš„åˆè§„æ€§æ£€ç´¢ç³»ç»Ÿ",
      "authors": [
        "Byungho Jo"
      ],
      "abstract": "Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èˆªç©ºç»´ä¿®(MRO)é¢†åŸŸä¸­æŠ€æœ¯äººå‘˜æ£€ç´¢æ‰‹å†Œè€—æ—¶è¿‡é•¿ä¸”å¿…é¡»æ»¡è¶³ä¸¥æ ¼åˆè§„æ€§çš„ç—›ç‚¹ï¼Œæå‡ºäº†ä¸€ç§åˆè§„æ€§ä¿æŠ¤æ£€ç´¢ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡é€‚é…å¤§è¯­è¨€æ¨¡å‹(LLM)é‡æ’åºå’Œè¯­ä¹‰æœç´¢æŠ€æœ¯ï¼Œä¸ç°æœ‰çš„è®¤è¯é—ç•™æŸ¥çœ‹å™¨ååŒå·¥ä½œï¼Œç¡®ä¿æ¯é¡¹æ“ä½œéƒ½å¯è¿½æº¯è‡³è®¤è¯æºã€‚ç³»ç»Ÿåˆ©ç”¨ATAç« èŠ‚å±‚çº§æ„å»ºäº†å…·æœ‰ä¿®è®¢é²æ£’æ€§çš„åµŒå…¥(Revision-robust embeddings)ï¼Œå¹¶é‡‡ç”¨è§†è§‰è¯­è¨€è§£æ(Vision-language parsing)æŠ€æœ¯å¯¹è®¤è¯å†…å®¹è¿›è¡Œç»“æ„åŒ–å¤„ç†ï¼Œä½¿ç”¨æˆ·èƒ½é«˜æ•ˆé¢„è§ˆå¹¶å¿«é€Ÿè®¿é—®éªŒè¯è¿‡çš„ç¨‹åºã€‚åœ¨åŒ…å«4.9ä¸‡ä¸ªåˆæˆæŸ¥è¯¢çš„è¯„ä¼°ä¸­ï¼Œè¯¥ç³»ç»Ÿå®ç°äº†è¶…è¿‡90%çš„æ£€ç´¢å‡†ç¡®ç‡ã€‚é’ˆå¯¹10åæŒæœ‰æ‰§ç…§çš„èˆªç©ºç»´ä¿®æŠ€æœ¯äººå‘˜(AMTs)å¼€å±•çš„åŒè¯­å¯¹ç…§ç ”ç©¶æ˜¾ç¤ºï¼Œç³»ç»ŸTop-10æˆåŠŸç‡è¾¾åˆ°90.9%ï¼Œå¹¶å°†å•é¡¹ä»»åŠ¡çš„æŸ¥é˜…æ—¶é—´ä»6-15åˆ†é’Ÿç¼©çŸ­è‡³18ç§’ï¼Œé™å¹…è¾¾95%ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†è¯­ä¹‰æ£€ç´¢åœ¨ä¸¥æ ¼ç›‘ç®¡çº¦æŸä¸‹è¿è¡Œçš„å¯è¡Œæ€§ï¼Œå¹¶ä¸ºæ˜¾è‘—å‡å°‘çœŸå®ä¸–ç•Œå¤šè¯­è¨€MROå·¥ä½œæµç¨‹çš„æ“ä½œè´Ÿæ‹…æä¾›äº†æœ‰åŠ›è¯æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15383v1",
      "published_date": "2025-11-19 12:25:40 UTC",
      "updated_date": "2025-11-19 12:25:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:59:53.567841+00:00"
    },
    {
      "arxiv_id": "2511.15378v1",
      "title": "Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents",
      "title_zh": "Terra Novaï¼šé¢å‘æ™ºèƒ½ä½“çš„ç»¼åˆæ€§æŒ‘æˆ˜ç¯å¢ƒ",
      "authors": [
        "Trevor McInroe"
      ],
      "abstract": "We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†åä¸º Terra Nova çš„æ–°å‹ç»¼åˆæŒ‘æˆ˜ç¯å¢ƒ (Comprehensive Challenge Environment, CCE)ï¼Œä¸“é—¨ç”¨äºå¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) ç ”ç©¶ã€‚è¯¥ç¯å¢ƒå—åˆ°ã€Šæ–‡æ˜Vã€‹ (Civilization V) çš„å¯å‘ï¼Œå°†éƒ¨åˆ†å¯è§‚æµ‹æ€§ (partial observability)ã€ä¿¡ç”¨åˆ†é… (credit assignment)ã€è¡¨ç¤ºå­¦ä¹  (representation learning) å’Œåºå¤§åŠ¨ä½œç©ºé—´ (enormous action spaces) ç­‰å¤šç§å…¸å‹å¼ºåŒ–å­¦ä¹ æŒ‘æˆ˜åŒæ—¶æ•´åˆåœ¨å•ä¸€åœºæ™¯ä¸­ã€‚ä¸ç®€å•æ±‡æ€»ç‹¬ç«‹å¹¶è¡Œä»»åŠ¡çš„å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒTerra Nova è¦æ±‚æ™ºèƒ½ä½“å…·å¤‡å¯¹å¤šä¸ªç›¸äº’å…³è”å˜é‡çš„é›†æˆå¼é•¿ç¨‹ç†è§£èƒ½åŠ›ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿå¤æ‚çš„ç›¸äº’ä½œç”¨æ¥æµ‹è¯•æ™ºèƒ½ä½“çš„æ·±åº¦æ¨ç†èƒ½åŠ›ï¼Œè€Œéä»…ä»…æ˜¯åˆ‡æ¢ä¸ç›¸å…³çš„ç­–ç•¥ã€‚è¯¥ç¯å¢ƒçš„æå‡ºä¸ºè¯„ä¼°å’Œå¼€å‘èƒ½å¤Ÿå¤„ç†ç°å®ä¸–ç•Œå¤æ‚ä»»åŠ¡çš„æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªæå…·æŒ‘æˆ˜æ€§çš„æ–°åŸºå‡†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15378v1",
      "published_date": "2025-11-19 12:10:10 UTC",
      "updated_date": "2025-11-19 12:10:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:58:45.476528+00:00"
    },
    {
      "arxiv_id": "2511.15375v1",
      "title": "Parameter Importance-Driven Continual Learning for Foundation Models",
      "title_zh": "å‚æ•°é‡è¦æ€§é©±åŠ¨çš„åŸºç¡€æ¨¡å‹æŒç»­å­¦ä¹ ",
      "authors": [
        "Lingxiang Wang",
        "Hainan Zhang",
        "Zhiming Zheng"
      ],
      "abstract": "Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºç¡€æ¨¡å‹åœ¨é¢†åŸŸç‰¹å®šåè®­ç»ƒä¸­å‡ºç°çš„ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)é—®é¢˜ï¼Œæå‡ºäº†åä¸ºPIECEçš„å‚æ•°é‡è¦æ€§ä¼°è®¡æŒç»­å¢å¼ºæ–¹æ³•ã€‚PIECEæ—¨åœ¨ä¸ä¾èµ–å†å²æ•°æ®æˆ–å¢åŠ é¢å¤–å‚æ•°çš„å‰æä¸‹ï¼Œåœ¨å­¦ä¹ æ–°é¢†åŸŸçŸ¥è¯†çš„åŒæ—¶ä¿ç•™æ¨¡å‹çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡åŸºäºFisher Informationçš„PIECE-Få’Œç»“åˆæ¢¯åº¦ä¸æ›²ç‡ä¿¡æ¯çš„äºŒé˜¶å½’ä¸€åŒ–ä¼°è®¡å™¨PIECE-Sï¼Œç²¾å‡†å®šä½å¹¶ä»…æ›´æ–°ä¸æ–°ä»»åŠ¡æœ€ç›¸å…³çš„0.1%æ ¸å¿ƒå‚æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPIECEåœ¨ä¸‰ç§è¯­è¨€æ¨¡å‹å’Œä¸¤ç§å¤šæ¨¡æ€æ¨¡å‹ä¸Šå‡ç»´æŒäº†å¼ºå¤§çš„é€šç”¨èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„(state-of-the-art)æŒç»­å­¦ä¹ æ•ˆæœã€‚è¯¥æ–¹æ³•ä¸ºæ„å»ºå¯æ‰©å±•ä¸”å…·å¤‡é¢†åŸŸé€‚åº”æ€§çš„åŸºç¡€æ¨¡å‹æä¾›äº†ä¸€æ¡å…¼å…·æ•ˆç‡ä¸æ€§èƒ½çš„å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15375v1",
      "published_date": "2025-11-19 12:07:53 UTC",
      "updated_date": "2025-11-19 12:07:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:59:00.561390+00:00"
    },
    {
      "arxiv_id": "2511.15370v1",
      "title": "The Empowerment of Science of Science by Large Language Models: New Tools and Methods",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½ç§‘å­¦å­¦ï¼šæ–°å·¥å…·ä¸æ–°æ–¹æ³•",
      "authors": [
        "Guoqiang Liang",
        "Jingqian Gong",
        "Mengxuan Li",
        "Gege Lin",
        "Shuo Zhang"
      ],
      "abstract": "Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.",
      "tldr_zh": "è¯¥è®ºæ–‡ç³»ç»Ÿæ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰åœ¨ç§‘å­¦å­¦ï¼ˆScience of Science, SciSciï¼‰é¢†åŸŸçš„èµ‹èƒ½ä½œç”¨ï¼Œå¹¶è¯¦ç»†ç»¼è¿°äº†æç¤ºå·¥ç¨‹ï¼ˆprompt engineeringï¼‰ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆretrieval augmented generation, RAGï¼‰ã€å¾®è°ƒï¼ˆfine tuningï¼‰ç­‰æ ¸å¿ƒæ”¯æ’‘æŠ€æœ¯ã€‚æ–‡ç« å›é¡¾äº†ç§‘å­¦è®¡é‡å­¦ï¼ˆSciSciï¼‰çš„å‘å±•å†å²ï¼Œé‡ç‚¹åˆ†æäº† LLMs åœ¨æ¨åŠ¨ç§‘å­¦ç ”ç©¶è¯„ä¼°ã€é¢„æµ‹åŠåˆ†ææ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº AI agent çš„ç§‘å­¦è¯„ä»·æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†åˆ©ç”¨ LLMs è¿›è¡Œç ”ç©¶å‰æ²¿æ£€æµ‹ï¼ˆresearch fronts detectionï¼‰å’ŒçŸ¥è¯†å›¾è°±æ„å»ºï¼ˆknowledge graph buildingï¼‰çš„åˆ›æ–°æ–¹æ³•ã€‚é€šè¿‡æ•´åˆå‰æ²¿çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œè¯¥ç»¼è¿°ä¸ºç§‘ç ”äººå‘˜æä¾›äº†åˆ©ç”¨ LLMs ä¼˜åŒ–ç§‘å­¦è®¡é‡ç ”ç©¶çš„æ–°å·¥å…·ä¸æ–¹æ³•è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The manuscript is currently ongoing the underreview process of the journal of information science",
      "pdf_url": "https://arxiv.org/pdf/2511.15370v1",
      "published_date": "2025-11-19 11:57:22 UTC",
      "updated_date": "2025-11-19 11:57:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:59:01.280714+00:00"
    },
    {
      "arxiv_id": "2511.15369v1",
      "title": "IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers",
      "title_zh": "IPTQ-ViTï¼šé¢å‘å…¨æ•´æ•°è§†è§‰ Transformer çš„éçº¿æ€§å‡½æ•°è®­ç»ƒåé‡åŒ–",
      "authors": [
        "Gihwan Kim",
        "Jemin Lee",
        "Hyungshin Kim"
      ],
      "abstract": "Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\\%p (avg. 1.78\\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IPTQ-ViTï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å…¨æ•´å‹Vision Transformers (ViTs) çš„æ–°å‹è®­ç»ƒåé‡åŒ– (Post-Training Quantization, PTQ) æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°å…¨æ•´å‹æ¨ç†ã€‚è¯¥æ¡†æ¶é’ˆå¯¹è§†è§‰æ•°æ®å¼•å…¥äº†ä¼˜åŒ–çš„å¤šé¡¹å¼GELUè¿‘ä¼¼å‡½æ•°ï¼Œå¹¶è®¾è®¡äº†åŸºäºä½ç§» (bit-shifting) çš„Softmaxä»¥æå‡PTQçš„è¿‘ä¼¼ç²¾åº¦ã€‚æ­¤å¤–ï¼ŒIPTQ-ViT æå‡ºäº†ä¸€é¡¹æ•´åˆäº†é‡åŒ–æ•æ„Ÿæ€§ã€æ‰°åŠ¨å’Œè®¡ç®—æˆæœ¬çš„ç»Ÿä¸€æŒ‡æ ‡ï¼Œç”¨äºä¸ºæ¯ä¸ªæ¿€æ´»å±‚è‡ªé€‚åº”åœ°é€‰æ‹©æœ€ä¼˜è¿‘ä¼¼å‡½æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIPTQ-ViT åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­Top-1å‡†ç¡®ç‡æœ€é«˜æå‡äº†6.44%ï¼Œç›®æ ‡æ£€æµ‹mAPæå‡äº†1.0ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„PTQæ–¹æ³•ã€‚åœ¨W8A8å’ŒW4A8é…ç½®ä¸‹ï¼Œè¯¥æ¡†æ¶è¾¾åˆ°äº†ä¸å…¨æ•´å‹é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT) æ–¹æ³•ç›¸å½“çš„ç²¾åº¦ä¸æ¨ç†å»¶è¿Ÿã€‚è¯¥ç ”ç©¶ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„ViTé«˜æ•ˆéƒ¨ç½²æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted in WACV 2026 (10 pages)",
      "pdf_url": "https://arxiv.org/pdf/2511.15369v1",
      "published_date": "2025-11-19 11:56:16 UTC",
      "updated_date": "2025-11-19 11:56:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:59:01.456595+00:00"
    },
    {
      "arxiv_id": "2511.15351v2",
      "title": "Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration",
      "title_zh": "Octopusï¼šåŸºäºå…­å¤§èƒ½åŠ›ç¼–æ’çš„æ™ºèƒ½ä½“å¤šæ¨¡æ€æ¨ç†",
      "authors": [
        "Yifu Guo",
        "Zishan Xu",
        "Zhiyuan Yao",
        "Yuquan Lu",
        "Jiaye Lin",
        "Sen Hu",
        "Zhenheng Tang",
        "Huacan Wang",
        "Ronghao Chen"
      ],
      "abstract": "Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Octopusï¼Œä¸€ç§åŸºäºå…­é¡¹èƒ½åŠ›ç¼–æ’ï¼ˆSix-Capability Orchestrationï¼‰çš„æ–°å‹å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¨ç†èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨è‡ªä¸»æ¢ç´¢å¤šæ ·åŒ–æ¨ç†è·¯å¾„æ–¹é¢çš„å±€é™ã€‚ç ”ç©¶å›¢é˜Ÿå®šä¹‰äº†å¤šæ¨¡æ€æ¨ç†ä¸å¯æˆ–ç¼ºçš„å…­é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶æ®æ­¤æ„å»ºäº†ç»¼åˆè¯„ä¼°åŸºå‡†Octopus-Benchã€‚Octopusèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°è‡ªä¸»æ¢ç´¢ï¼Œå¹¶æ ¹æ®ä»»åŠ¡çš„å½“å‰çŠ¶æ€åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOctopusåœ¨Octopus-Benchçš„ç»å¤§å¤šæ•°ä»»åŠ¡ä¸­å‡å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¯æ˜äº†èƒ½åŠ›åè°ƒåœ¨æ™ºèƒ½ä½“å¤šæ¨¡æ€æ¨ç†ï¼ˆAgentic Multimodal Reasoningï¼‰ä¸­çš„é‡è¦æ€§ã€‚è¯¥æˆæœä¸ºå¼€å‘èƒ½å¤Ÿé€‚åº”çœŸå®ä¸–ç•ŒåŠ¨æ€éœ€æ±‚ã€å…·å¤‡ç±»äººæ€ç»´èƒ½åŠ›çš„æ™ºèƒ½ä½“å¥ å®šäº†ç†è®ºä¸å®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15351v2",
      "published_date": "2025-11-19 11:22:13 UTC",
      "updated_date": "2025-12-12 14:34:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:58:58.561729+00:00"
    },
    {
      "arxiv_id": "2511.15342v1",
      "title": "Reflexive Evidence-Based Multimodal Learning for Clean Energy Transitions: Causal Insights on Cooking Fuel Access, Urbanization, and Carbon Emissions",
      "title_zh": "é¢å‘æ¸…æ´èƒ½æºè½¬å‹çš„å¾ªè¯è‡ªçœå¼å¤šæ¨¡æ€å­¦ä¹ ï¼šçƒ¹é¥ªç‡ƒæ–™æ™®åŠã€åŸé•‡åŒ–ä¸ç¢³æ’æ”¾çš„å› æœæ´å¯Ÿ",
      "authors": [
        "Shan Shan"
      ],
      "abstract": "Achieving Sustainable Development Goal 7 (Affordable and Clean Energy) requires not only technological innovation but also a deeper understanding of the socioeconomic factors influencing energy access and carbon emissions. While these factors are gaining attention, critical questions remain, particularly regarding how to quantify their impacts on energy systems, model their cross-domain interactions, and capture feedback dynamics in the broader context of energy transitions. To address these gaps, this study introduces ClimateAgents, an AI-based framework that combines large language models with domain-specialized agents to support hypothesis generation and scenario exploration. Leveraging 20 years of socioeconomic and emissions data from 265 economies, countries and regions, and 98 indicators drawn from the World Bank database, the framework applies a machine learning based causal inference approach to identify key determinants of carbon emissions in an evidence-based, data driven manner. The analysis highlights three primary drivers: access to clean cooking fuels in rural areas, access to clean cooking fuels in urban areas, and the percentage of population living in urban areas. These findings underscore the critical role of clean cooking technologies and urbanization patterns in shaping emission outcomes. In line with growing calls for evidence-based AI policy, ClimateAgents offers a modular and reflexive learning system that supports the generation of credible and actionable insights for policy. By integrating heterogeneous data modalities, including structured indicators, policy documents, and semantic reasoning, the framework contributes to adaptive policymaking infrastructures that can evolve with complex socio-technical challenges. This approach aims to support a shift from siloed modeling to reflexive, modular systems designed for dynamic, context-aware climate action.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º ClimateAgents çš„äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Models) ä¸é¢†åŸŸä¸“ä¸šæ™ºèƒ½ä½“ (domain-specialized agents)ï¼Œæ—¨åœ¨æ”¯æŒæ¸…æ´èƒ½æºè½¬å‹çš„å‡è®¾ç”Ÿæˆå’Œæƒ…æ™¯æ¢ç´¢ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨ä¸–ç•Œé“¶è¡Œæ•°æ®åº“ä¸­è·¨è¶Š 20 å¹´ã€æ¶µç›– 265 ä¸ªç»æµä½“å’Œ 98 é¡¹æŒ‡æ ‡çš„ç¤¾ä¼šç»æµä¸æ’æ”¾æ•°æ®ï¼Œé‡‡ç”¨åŸºäºæœºå™¨å­¦ä¹ çš„å› æœæ¨ç† (causal inference) æ–¹æ³•æ¥è¯†åˆ«ç¢³æ’æ”¾çš„å…³é”®å†³å®šå› ç´ ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œå†œæ‘å’ŒåŸå¸‚åœ°åŒºçš„æ¸…æ´çƒ¹é¥ªç‡ƒæ–™è·å–ä»¥åŠåŸå¸‚åŒ–äººå£æ¯”ä¾‹æ˜¯å½±å“ç¢³æ’æ”¾çš„ä¸‰å¤§ä¸»è¦é©±åŠ¨å› ç´ ï¼Œå¼ºè°ƒäº†æ¸…æ´çƒ¹é¥ªæŠ€æœ¯åœ¨èƒ½æºè½¬å‹ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚ClimateAgents æä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å…·æœ‰åæ€æ€§çš„å­¦ä¹ ç³»ç»Ÿï¼Œé€šè¿‡æ•´åˆç»“æ„åŒ–æŒ‡æ ‡ã€æ”¿ç­–æ–‡ä»¶å’Œè¯­ä¹‰æ¨ç†ç­‰å¼‚æ„æ•°æ®æ¨¡æ€ (heterogeneous data modalities)ï¼Œä¸ºå†³ç­–è€…æä¾›å¯ä¿¡ä¸”å…·æ“ä½œæ€§çš„æ”¿ç­–æ´å¯Ÿã€‚è¿™ç§æ–¹æ³•æ¨åŠ¨äº†ä»å•ä¸€å»ºæ¨¡å‘åŠ¨æ€ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ°”å€™è¡ŒåŠ¨åŸºç¡€è®¾æ–½è½¬å˜ï¼Œä¸ºåº”å¯¹å¤æ‚çš„ç¤¾ä¼šæŠ€æœ¯æŒ‘æˆ˜å’Œå®ç°å¯æŒç»­å‘å±•ç›®æ ‡ (SDG 7) æä¾›äº†å¼ºæœ‰åŠ›çš„å¾ªè¯æ”¯æŒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15342v1",
      "published_date": "2025-11-19 11:02:41 UTC",
      "updated_date": "2025-11-19 11:02:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:59:17.474686+00:00"
    },
    {
      "arxiv_id": "2511.15339v1",
      "title": "STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection",
      "title_zh": "STREAM-VAEï¼šé¢å‘è½¦è¾†é¥æµ‹å¼‚å¸¸æ£€æµ‹ä¸­å¿«æ…¢åŠ¨æ€çš„åŒè·¯å¾„è·¯ç”±",
      "authors": [
        "Kadir-Kaan Ã–zer",
        "RenÃ© Ebeling",
        "Markus Enzweiler"
      ],
      "abstract": "Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.\n  In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.\n  Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ±½è½¦é¥æµ‹æ•°æ®(automotive telemetry data)ä¸­ç¼“æ…¢æ¼‚ç§»(slow drifts)ä¸å¿«é€Ÿè„‰å†²(fast spikes)å…±å­˜å¯¼è‡´çš„å¼‚å¸¸æ£€æµ‹éš¾é¢˜ï¼Œæå‡ºäº†STREAM-VAEæ¨¡å‹ã€‚è¯¥æ¨¡å‹æ ¸å¿ƒé‡‡ç”¨äº†åŒè·¯å¾„ç¼–ç å™¨(dual-path encoder)æ¶æ„ï¼Œæ—¨åœ¨å°†ä¸åŒæ—¶é—´å°ºåº¦çš„ä¿¡å·åŠ¨åŠ›å­¦ç‰¹å¾è¿›è¡Œæœ‰æ•ˆåˆ†ç¦»ï¼Œè§£å†³äº†ä¼ ç»Ÿå˜åˆ†è‡ªç¼–ç å™¨(VAEs)å› æ··åˆå¼‚æ„æ•°æ®è€Œå¯¼è‡´çš„å¼‚å¸¸è¯†åˆ«èƒ½åŠ›å‡å¼±é—®é¢˜ã€‚å…¶è§£ç å™¨è®¾è®¡èƒ½å¤Ÿç‹¬ç«‹è¡¨å¾ç¬æ—¶åå·®ä¸æ­£å¸¸è¿è¡Œæ¨¡å¼ï¼Œä»è€Œä¸ºè½¦è½½ç›‘æ§å’Œåç«¯è½¦é˜Ÿåˆ†ææä¾›ç¨³å®šçš„å¼‚å¸¸è¯„åˆ†ã€‚åœ¨æ±½è½¦é¥æµ‹æ•°æ®é›†å’Œå…¬å¼€çš„SMDåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ˜¾å¼åˆ†ç¦»æ¼‚ç§»ä¸è„‰å†²åŠ¨åŠ›å­¦æ˜¾è‘—æå‡äº†æ£€æµ‹çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•åœ¨å¯¹æ¯”é¢„æµ‹æ¨¡å‹ã€æ³¨æ„åŠ›æœºåˆ¶(attention)åŠå›¾æ¨¡å‹(graph)ç­‰å¤šç§å¼ºåŸºå‡†æ–¹æ³•æ—¶å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚STREAM-VAEçš„è®¾è®¡å…¼é¡¾äº†å®é™…éƒ¨ç½²éœ€æ±‚ï¼Œä¸ºè½¦è¾†è¿œç¨‹ç›‘æ§å’Œå¼‚å¸¸è¯Šæ–­æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 Pages, 4 Figures, 4 Tables",
      "pdf_url": "https://arxiv.org/pdf/2511.15339v1",
      "published_date": "2025-11-19 10:58:40 UTC",
      "updated_date": "2025-11-19 10:58:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:59:43.877004+00:00"
    },
    {
      "arxiv_id": "2511.15304v3",
      "title": "Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models",
      "title_zh": "å¯¹æŠ—æ€§è¯—æ­Œï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­çš„é€šç”¨å•è½®è¶Šç‹±æœºåˆ¶",
      "authors": [
        "Piercosma Bisconti",
        "Matteo Prandi",
        "Federico Pierucci",
        "Francesco Giarrusso",
        "Marcantonio Bracale Syrnikov",
        "Marcello Galisai",
        "Vincenzo Suriani",
        "Olga Sorokoletova",
        "Federico Sartore",
        "Daniele Nardi"
      ],
      "abstract": "We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for Large Language Models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of 3 open-weight LLM judges, whose binary safety assessments were validated on a stratified human-labeled subset. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯æ˜äº†å¯¹æŠ—æ€§è¯—æ­Œ(Adversarial Poetry)æ˜¯ä¸€ç§é€šç”¨çš„å•è½®è¶Šç‹±(jailbreak)æŠ€æœ¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆçªç ´å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„å®‰å…¨é˜²å¾¡ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹25æ¬¾é¢†å…ˆçš„å•†ä¸šå’Œå¼€æºæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°è¯—æ­Œæç¤ºè¯å…·æœ‰æé«˜çš„æ”»å‡»æˆåŠŸç‡(ASR)ï¼Œåœ¨æŸäº›ä¾›åº”å•†çš„æ¨¡å‹ä¸Šç”šè‡³è¶…è¿‡äº†90%ã€‚é€šè¿‡å°†æ”»å‡»æ˜ å°„åˆ°MLCommonsç­‰é£é™©åˆ†ç±»ä½“ç³»ï¼Œç ”ç©¶ç¡®è®¤è¿™ç§æ‰‹æ®µåœ¨CBRNã€ç½‘ç»œçŠ¯ç½ªå’Œå¤±æ§ç­‰å¤šä¸ªæ•æ„Ÿé¢†åŸŸå‡è¡¨ç°å‡ºæå¼ºçš„è¿ç§»æ€§ã€‚å®éªŒåˆ©ç”¨å…ƒæç¤ºè¯(meta-prompt)å°†æœ‰å®³æŒ‡ä»¤è½¬åŒ–ä¸ºè¯—æ­Œå½¢å¼ï¼Œå…¶æˆåŠŸç‡æœ€é«˜å¯è¾¾æ•£æ–‡åŸºå‡†çš„18å€ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ‰‹å·¥åˆ›ä½œçš„è¯—æ­Œå¹³å‡è¶Šç‹±æˆåŠŸç‡ä¸º62%ï¼Œæ˜¾è‘—ä¼˜äºéè¯—æ­ŒåŸºå‡†ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†æ¨¡å‹å¯¹é£æ ¼å˜åŒ–çš„ç³»ç»Ÿæ€§è„†å¼±ï¼Œè¡¨æ˜å½“å‰çš„å¯¹é½æ–¹æ³•(alignment methods)å’Œè¯„ä¼°åè®®åœ¨åº”å¯¹æ­¤ç±»æ”»å‡»æ—¶å­˜åœ¨æ ¹æœ¬æ€§çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15304v3",
      "published_date": "2025-11-19 10:14:08 UTC",
      "updated_date": "2026-01-16 13:41:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T07:59:24.665976+00:00"
    },
    {
      "arxiv_id": "2511.15759v1",
      "title": "Securing AI Agents Against Prompt Injection Attacks",
      "title_zh": "æŠµå¾¡æç¤ºæ³¨å…¥æ”»å‡»çš„ AI æ™ºèƒ½ä½“å®‰å…¨é˜²æŠ¤",
      "authors": [
        "Badrinath Ramakrishnan",
        "Akshaya Balaji"
      ],
      "abstract": "Retrieval-augmented generation (RAG) systems have become widely used for enhancing large language model capabilities, but they introduce significant security vulnerabilities through prompt injection attacks. We present a comprehensive benchmark for evaluating prompt injection risks in RAG-enabled AI agents and propose a multi-layered defense framework. Our benchmark includes 847 adversarial test cases across five attack categories: direct injection, context manipulation, instruction override, data exfiltration, and cross-context contamination. We evaluate three defense mechanisms: content filtering with embedding-based anomaly detection, hierarchical system prompt guardrails, and multi-stage response verification, across seven state-of-the-art language models. Our combined framework reduces successful attack rates from 73.2% to 8.7% while maintaining 94.3% of baseline task performance. We release our benchmark dataset and defense implementation to support future research in AI agent security.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿåœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹åŠŸèƒ½æ—¶å¼•å…¥çš„æç¤ºæ³¨å…¥æ”»å‡»(Prompt Injection Attacks)é£é™©ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„å®‰å…¨æ€§è¯„ä¼°åŸºå‡†å’Œå¤šå±‚é˜²å¾¡æ¡†æ¶ã€‚è¯¥åŸºå‡†æ¶µç›–äº†æ¶‰åŠç›´æ¥æ³¨å…¥ã€ä¸Šä¸‹æ–‡æ“çºµã€æŒ‡ä»¤è¦†ç›–ã€æ•°æ®çªƒå–å’Œè·¨ä¸Šä¸‹æ–‡æ±¡æŸ“äº”å¤§ç±»åˆ«çš„847ä¸ªå¯¹æŠ—æ€§æµ‹è¯•æ¡ˆä¾‹ã€‚ç ”ç©¶å›¢é˜Ÿé‡ç‚¹è¯„ä¼°äº†åŸºäºåµŒå…¥çš„å¼‚å¸¸æ£€æµ‹å†…å®¹è¿‡æ»¤ã€åˆ†å±‚ç³»ç»Ÿæç¤ºé˜²æŠ¤æ (Hierarchical system prompt guardrails)ä»¥åŠå¤šé˜¶æ®µå“åº”éªŒè¯ä¸‰é¡¹é˜²å¾¡æœºåˆ¶ã€‚å®éªŒåœ¨ä¸ƒç§å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºè¯¥ç»¼åˆé˜²å¾¡æ¡†æ¶èƒ½å°†æ”»å‡»æˆåŠŸç‡ä»73.2%æ˜¾è‘—é™ä½è‡³8.7%ï¼ŒåŒæ—¶ä¿æŒäº†94.3%çš„åŸºç¡€ä»»åŠ¡æ€§èƒ½ã€‚è¯¥ç ”ç©¶é€šè¿‡å‘å¸ƒåŸºå‡†æ•°æ®é›†å’Œé˜²å¾¡å®ç°ï¼Œä¸ºæå‡äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“(AI Agents)åœ¨å¯¹æŠ—æ€§ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§æä¾›äº†é‡è¦å·¥å…·å’Œå®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15759v1",
      "published_date": "2025-11-19 10:00:54 UTC",
      "updated_date": "2025-11-19 10:00:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:00:05.369491+00:00"
    },
    {
      "arxiv_id": "2511.15284v1",
      "title": "Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments",
      "title_zh": "åŠ¨æ€ç¯å¢ƒä¸‹åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„è·¯å¾„è§„åˆ’",
      "authors": [
        "Jonas De Maeyer",
        "Hossein Yarahmadi",
        "Moharram Challenger"
      ],
      "abstract": "Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„åŒºåŸŸæ„ŸçŸ¥(region-aware)å¼ºåŒ–å­¦ä¹ (RL)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½äº¤é€šå’Œæœºå™¨äººé¢†åŸŸä¸­åŠ¨æ€ç¯å¢ƒä¸‹è·¯å¾„è§„åˆ’çš„æ‰©å±•æ€§åŠè‡ªé€‚åº”éš¾é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç¯å¢ƒå˜åŒ–é€šå¸¸å…·æœ‰å±€éƒ¨æ€§çš„ç‰¹ç‚¹ï¼Œé€šè¿‡å±‚çº§åŒ–åˆ†è§£(hierarchical decomposition)ç¯å¢ƒå¹¶éƒ¨ç½²åˆ†å¸ƒå¼æ™ºèƒ½ä½“æ¥å®ç°å±€éƒ¨è°ƒæ•´ã€‚ç ”ç©¶å¯¹æ¯”äº†å•æ™ºèƒ½ä½“ Q-learning ä¸å¤šæ™ºèƒ½ä½“è”é‚¦ Q-learning (Federated Q-learning) ä¸¤ç§è®­ç»ƒèŒƒå¼ï¼Œå¹¶å¼•å…¥äº†åŸºäºå­ç¯å¢ƒæˆåŠŸç‡çš„å†è®­ç»ƒæœºåˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè”é‚¦å­¦ä¹ å˜ä½“åœ¨å¤„ç†å¤šéšœç¢ç‰©å¹¶å‘å˜åŒ–çš„å¤æ‚åœºæ™¯æ—¶è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½æ¥è¿‘ A* Oracle ä¸”å…·å¤‡æ›´çŸ­çš„é€‚åº”æ—¶é—´å’Œæ›´å¼ºçš„å¯æ‰©å±•æ€§ã€‚è¯¥å»ä¸­å¿ƒåŒ–æ¶æ„æ¶ˆé™¤äº†å¯¹å…¨å±€è§„åˆ’å™¨(global planner)çš„ä¾èµ–ï¼Œä¸ºåœ¨å¤§å‹åŠ¨æ€ç¯å¢ƒä¸­ä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep RL)è¿›è¡Œè·¯å¾„è§„åˆ’æä¾›äº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15284v1",
      "published_date": "2025-11-19 09:48:44 UTC",
      "updated_date": "2025-11-19 09:48:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:00:32.466226+00:00"
    },
    {
      "arxiv_id": "2511.15282v2",
      "title": "Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research",
      "title_zh": "å®åœ¨è®ºä¸å¤šå…ƒè®ºæ™ºèƒ½è§‚åŠå…¶å¯¹äººå·¥æ™ºèƒ½ç ”ç©¶çš„å¯ç¤º",
      "authors": [
        "Ninell Oldenburg",
        "Ruchira Dhar",
        "Anders SÃ¸gaard"
      ],
      "abstract": "In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å½“å‰äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­å¹¶å­˜çš„ä¸¤ç§åº•å±‚æ™ºèƒ½è§‚ï¼šæ™ºèƒ½ç°å®ä¸»ä¹‰ (Intelligence Realism) å’Œæ™ºèƒ½å¤šå…ƒä¸»ä¹‰ (Intelligence Pluralism)ã€‚æ™ºèƒ½ç°å®ä¸»ä¹‰è®¤ä¸ºæ™ºèƒ½æ˜¯ä¸€ç§è·¨ç³»ç»Ÿçš„é€šç”¨èƒ½åŠ›ï¼Œè€Œæ™ºèƒ½å¤šå…ƒä¸»ä¹‰å°†å…¶è§†ä¸ºä¾èµ–è¯­å¢ƒä¸”ä¸å¯ç®€åŒ–çš„å¤šæ ·åŒ–èƒ½åŠ›ã€‚è®ºæ–‡åˆ†æäº†è¿™äº›éšå«è§‚å¿µå¦‚ä½•ä»æ–¹æ³•è®ºã€è§£é‡Šæ€§åŠ AI é£é™©è¯„ä¼°ä¸‰ä¸ªç»´åº¦æ ¹æœ¬æ€§åœ°å¡‘é€ ç ”ç©¶è·¯å¾„ã€‚åœ¨æ–¹æ³•è®ºä¸Šï¼Œä¸¤è€…å¯¼è‡´äº†æ¨¡å‹é€‰æ‹©ã€åŸºå‡†æµ‹è¯• (Benchmark) è®¾è®¡å’Œå®éªŒéªŒè¯æ–¹å¼çš„å·®å¼‚ï¼›åœ¨è§£é‡Šå±‚é¢ï¼Œå®ƒä»¬å¯¹èƒ½åŠ›æ¶Œç° (Capability Emergence) ç­‰ç°è±¡å¾—å‡ºäº†ç›¸äº’çŸ›ç›¾çš„è§£è¯»ã€‚å…³äº AI é£é™©ï¼Œç°å®ä¸»ä¹‰è€…å…³æ³¨è¶…çº§æ™ºèƒ½ (Superintelligence) åŠå…¶ç»Ÿä¸€å¯¹é½æ–¹æ¡ˆï¼Œè€Œå¤šå…ƒä¸»ä¹‰è€…åˆ™ä¾§é‡äºé’ˆå¯¹ä¸åŒé¢†åŸŸå¨èƒçš„ç‰¹å®šè§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æ˜ç¡®è¿™äº›åº•å±‚å‡è®¾ï¼Œè¯¥ç ”ç©¶ä¸ºç†è§£å½“å‰ AI é¢†åŸŸçš„å„ç§å­¦æœ¯åˆ†æ­§æä¾›äº†æ¸…æ™°çš„åˆ†ææ¡†æ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "The 40th Annual AAAI Conference on Artificial Intelligence, 8 pages (excl. references), 1 table",
      "pdf_url": "https://arxiv.org/pdf/2511.15282v2",
      "published_date": "2025-11-19 09:48:07 UTC",
      "updated_date": "2025-12-19 11:10:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:00:26.873526+00:00"
    },
    {
      "arxiv_id": "2511.17638v1",
      "title": "Model-to-Model Knowledge Transmission (M2KT): A Data-Free Framework for Cross-Model Understanding Transfer",
      "title_zh": "æ¨¡å‹é—´çŸ¥è¯†ä¼ è¾“ (M2KT)ï¼šä¸€ç§ç”¨äºè·¨æ¨¡å‹ç†è§£è¿ç§»çš„æ— æ•°æ®æ¡†æ¶",
      "authors": [
        "Pratham Sorte"
      ],
      "abstract": "Modern artificial intelligence systems depend heavily on large datasets for both training and transferring knowledge between models. Knowledge distillation, transfer learning, and dataset distillation have made such transfers more efficient, yet they remain fundamentally data-driven: a teacher must produce examples, logits, or gradients for a student to learn. In this work, we introduce Model-to-Model Knowledge Transmission (M2KT), a novel paradigm for data-free conceptual transfer between neural networks. M2KT enables models to exchange knowledge packets that encapsulate structured concept embeddings, abstraction graphs, reasoning traces, and provenance metadata. Unlike classical distillation, M2KT operates primarily in concept space rather than example space, and it does not require labeled datasets or teacher-generated outputs during transfer. We formalize the notion of concept manifolds, introduce an inter-model alignment mapping between teacher and student latent spaces, and derive a composite loss that enforces geometric, structural, and reasoning consistency together with explicit safety constraints. We further present algorithmic procedures for teacher-side packet generation and student-side ingestion and verification. Experiments on symbolic reasoning with large language models show that M2KT can achieve approximately 85 to 90 percent of teacher performance while reducing data usage by over 98 percent compared to standard knowledge distillation. This work establishes a theoretical and practical foundation for data-free AI-to-AI knowledge transfer and self-improving model ecosystems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Model-to-Model Knowledge Transmission (M2KT)ï¼Œä¸€ç§ç”¨äºè·¨æ¨¡å‹ç†è§£è¿ç§»çš„Data-Freeæ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„Knowledge Distillationç­‰æ•°æ®é©±åŠ¨æ–¹æ³•ä¸åŒï¼ŒM2KTå…è®¸æ¨¡å‹åœ¨Concept Spaceè€Œéç¤ºä¾‹ç©ºé—´ä¸­äº¤æ¢åŒ…å«ç»“æ„åŒ–Concept Embeddingsã€Abstraction Graphså’ŒReasoning Tracesçš„Knowledge Packetsã€‚è¯¥æ¡†æ¶é€šè¿‡å½¢å¼åŒ–Concept Manifoldså¹¶å¼•å…¥è·¨æ¨¡å‹å¯¹é½æ˜ å°„ï¼Œåˆ©ç”¨å¤åˆæŸå¤±å‡½æ•°ç¡®ä¿äº†æ¨¡å‹é—´åœ¨å‡ ä½•ã€ç»“æ„å’Œæ¨ç†æ–¹é¢çš„ä¸€è‡´æ€§ã€‚åœ¨Large Language Modelsçš„ç¬¦å·æ¨ç†å®éªŒä¸­ï¼ŒM2KTåœ¨å‡å°‘è¶…è¿‡98%æ•°æ®ä½¿ç”¨é‡çš„åŒæ—¶ï¼ŒæˆåŠŸè¾¾åˆ°äº†æ•™å¸ˆæ¨¡å‹85%è‡³90%çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥é¡¹å·¥ä½œä¸ºå®ç°æ•°æ®æ— å…³çš„AI-to-AIçŸ¥è¯†è¿ç§»å’Œè‡ªæ”¹è¿›æ¨¡å‹ç”Ÿæ€ç³»ç»Ÿå¥ å®šäº†åšå®çš„ç†è®ºä¸å®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages including figures, prepared in IEEE conference style. Preprint. Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2511.17638v1",
      "published_date": "2025-11-19 09:43:25 UTC",
      "updated_date": "2025-11-19 09:43:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:00:23.761950+00:00"
    },
    {
      "arxiv_id": "2511.15274v1",
      "title": "Behavior Trees vs Executable Ontologies: a Comparative Analysis of Robot Control Paradigms",
      "title_zh": "è¡Œä¸ºæ ‘ä¸å¯æ‰§è¡Œæœ¬ä½“ï¼šæœºå™¨äººæ§åˆ¶èŒƒå¼çš„å¯¹æ¯”åˆ†æ",
      "authors": [
        "Alexander Boldachev"
      ],
      "abstract": "This paper compares two distinct approaches to modeling robotic behavior: imperative Behavior Trees (BTs) and declarative Executable Ontologies (EO), implemented through the boldsea framework. BTs structure behavior hierarchically using control-flow, whereas EO represents the domain as a temporal, event-based semantic graph driven by dataflow rules. We demonstrate that EO achieves comparable reactivity and modularity to BTs through a fundamentally different architecture: replacing polling-based tick execution with event-driven state propagation. We propose that EO offers an alternative framework, moving from procedural programming to semantic domain modeling, to address the semantic-process gap in traditional robotic control. EO supports runtime model modification, full temporal traceability, and a unified representation of data, logic, and interface - features that are difficult or sometimes impossible to achieve with BTs, although BTs excel in established, predictable scenarios. The comparison is grounded in a practical mobile manipulation task. This comparison highlights the respective operational strengths of each approach in dynamic, evolving robotic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº†ä¸¤ç§æœºå™¨äººè¡Œä¸ºå»ºæ¨¡æ–¹æ³•ï¼šå‘½ä»¤å¼çš„è¡Œä¸ºæ ‘ (Behavior Trees, BTs) å’Œé€šè¿‡ boldsea æ¡†æ¶å®ç°çš„å£°æ˜å¼å¯æ‰§è¡Œæœ¬ä½“ (Executable Ontologies, EO)ã€‚BTs é‡‡ç”¨æ§åˆ¶æµé©±åŠ¨çš„å±‚æ¬¡åŒ–ç»“æ„ï¼Œè€Œ EO åˆ™å°†é¢†åŸŸè¡¨ç¤ºä¸ºç”±æ•°æ®æµè§„åˆ™é©±åŠ¨çš„ã€åŸºäºäº‹ä»¶çš„æ—¶åºè¯­ä¹‰å›¾ã€‚ç ”ç©¶è¯æ˜ï¼ŒEO é€šè¿‡å°†è½®è¯¢å¼çš„ tick æ‰§è¡Œæ›¿æ¢ä¸ºäº‹ä»¶é©±åŠ¨çš„çŠ¶æ€ä¼ æ’­ï¼Œåœ¨ååº”æ€§å’Œæ¨¡å—åŒ–æ–¹é¢è¾¾åˆ°äº†ä¸ BTs ç›¸å½“çš„æ°´å¹³ã€‚ä½œè€…æå‡º EO ä½œä¸ºä¸€ç§ä»è¿‡ç¨‹å¼ç¼–ç¨‹è½¬å‘è¯­ä¹‰é¢†åŸŸå»ºæ¨¡çš„æ›¿ä»£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæœºå™¨äººæ§åˆ¶ä¸­çš„è¯­ä¹‰è¿‡ç¨‹é—´éš™ (semantic-process gap)ã€‚ç›¸æ¯”äº BTsï¼ŒEO æ”¯æŒè¿è¡Œæ—¶æ¨¡å‹ä¿®æ”¹ã€å®Œæ•´çš„æ—¶åºå¯è¿½æº¯æ€§ä»¥åŠæ•°æ®ã€é€»è¾‘ä¸æ¥å£çš„ç»Ÿä¸€è¡¨ç¤ºï¼Œè¿™äº›ç‰¹æ€§åœ¨ BTs ä¸­å¾€å¾€éš¾ä»¥å®ç°ã€‚è¯¥ç ”ç©¶é€šè¿‡ç§»åŠ¨æ“ä½œä»»åŠ¡éªŒè¯äº†ä¸¤ç§èŒƒå¼çš„æ€§èƒ½ï¼Œå¼ºè°ƒäº† EO åœ¨åŠ¨æ€æ¼”åŒ–ç³»ç»Ÿä¸­çš„æ“ä½œä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.FL"
      ],
      "primary_category": "cs.RO",
      "comment": "22 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.15274v1",
      "published_date": "2025-11-19 09:38:01 UTC",
      "updated_date": "2025-11-19 09:38:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:00:42.664038+00:00"
    },
    {
      "arxiv_id": "2511.15259v1",
      "title": "Efficiency Will Not Lead to Sustainable Reasoning AI",
      "title_zh": "æ•ˆç‡æ— æ³•å®ç°å¯æŒç»­çš„æ¨ç†äººå·¥æ™ºèƒ½",
      "authors": [
        "Philipp Wiesner",
        "Daniel W. O'Neill",
        "Francesca Larosa",
        "Odej Kao"
      ],
      "abstract": "AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨ç†å‹äººå·¥æ™ºèƒ½ï¼ˆReasoning AIï¼‰åœ¨è¿½æ±‚å¯æŒç»­å‘å±•è¿‡ç¨‹ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå•çº¯ä¾é æé«˜æ•ˆç‡ï¼ˆEfficiencyï¼‰æ— æ³•å®ç°å¯æŒç»­æ€§ã€‚æ–‡ç« åˆ†æè®¤ä¸ºï¼Œè™½ç„¶å†å²ä¸Šè®¡ç®—èƒ½è€—å› æ•ˆç‡æå‡å’Œéœ€æ±‚é¥±å’Œè€Œè¶‹äºç¨³å®šï¼Œä½†ç›®å‰çš„æ¨ç†å‹äººå·¥æ™ºèƒ½åœ¨è®­ç»ƒï¼ˆTrainingï¼‰å’Œæ¨ç†ï¼ˆInferenceï¼‰é˜¶æ®µçš„æ€§èƒ½ä¸è®¡ç®—èµ„æºæŠ•å…¥å‘ˆæŒ‡æ•°çº§æ‰©å±•ï¼Œä¸”ç¼ºä¹æ˜ç¡®çš„é¥±å’Œç‚¹ã€‚éšç€æ•ˆç‡æå‡é€æ¸æ¥è¿‘ç‰©ç†æé™ï¼Œè¿™ç§å¯¹ç®—åŠ›çš„æŒç»­éœ€æ±‚å°†å¯¼è‡´èƒ½æºä¸èµ„æºæ¶ˆè€—ä¸å¯æ§ã€‚ä½œè€…å¼ºè°ƒï¼Œå¿…é¡»åœ¨ä¼˜åŒ–ï¼ˆOptimizationï¼‰å’Œæ²»ç†ï¼ˆGovernanceï¼‰å±‚é¢å¼•å…¥æ˜¾å¼çš„é™åˆ¶æœºåˆ¶ï¼Œè€Œéä»…ä»…ä¾èµ–æ•ˆç‡ä¼˜åŒ–ã€‚è¯¥è®ºæ–‡ä¸ºæœªæ¥äººå·¥æ™ºèƒ½çš„ç ”ç©¶ä¸æ”¿ç­–åˆ¶å®šæä¾›äº†æ–°çš„è§†è§’ï¼Œæ—¨åœ¨å°†ç¯å¢ƒçº¦æŸå’Œæ²»ç†è¾¹ç•Œæ·±åº¦åµŒå…¥åˆ°ç³»ç»Ÿè®¾è®¡ä¹‹ä¸­ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at the Rethinking AI Workshop @ EurIPS'25",
      "pdf_url": "https://arxiv.org/pdf/2511.15259v1",
      "published_date": "2025-11-19 09:23:14 UTC",
      "updated_date": "2025-11-19 09:23:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:00:46.370881+00:00"
    },
    {
      "arxiv_id": "2511.15253v2",
      "title": "PresentCoach: Dual-Agent Presentation Coaching through Exemplars and Interactive Feedback",
      "title_zh": "PresentCoachï¼šåŸºäºèŒƒä¾‹ä¸äº¤äº’å¼åé¦ˆçš„åŒæ™ºèƒ½ä½“æ¼”è®²è¾…å¯¼",
      "authors": [
        "Sirui Chen",
        "Jinsong Zhou",
        "Xinli Xu",
        "Xiaoyu Yang",
        "Litao Guo",
        "Ying-Cong Chen"
      ],
      "abstract": "Effective presentation skills are essential in education, professional communication, and public speaking, yet learners often lack access to high-quality exemplars or personalized coaching. Existing AI tools typically provide isolated functionalities such as speech scoring or script generation without integrating reference modeling and interactive feedback into a cohesive learning experience. We introduce a dual-agent system that supports presentation practice through two complementary roles: the Ideal Presentation Agent and the Coach Agent. The Ideal Presentation Agent converts user-provided slides into model presentation videos by combining slide processing, visual-language analysis, narration script generation, personalized voice synthesis, and synchronized video assembly. The Coach Agent then evaluates user-recorded presentations against these exemplars, conducting multimodal speech analysis and delivering structured feedback in an Observation-Impact-Suggestion (OIS) format. To enhance the authenticity of the learning experience, the Coach Agent incorporates an Audience Agent, which simulates the perspective of a human listener and provides humanized feedback reflecting audience reactions and engagement. Together, these agents form a closed loop of observation, practice, and feedback. Implemented on a robust backend with multi-model integration, voice cloning, and error handling mechanisms, the system demonstrates how AI-driven agents can provide engaging, human-centered, and scalable support for presentation skill development in both educational and professional contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† PresentCoachï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ¼”è®²æŠ€å·§åŸ¹è®­çš„åŒæ™ºèƒ½ä½“(dual-agent)ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å­¦ä¹ è€…ç¼ºä¹é«˜è´¨é‡èŒƒä¾‹å’Œä¸ªæ€§åŒ–æŒ‡å¯¼çš„é—®é¢˜ã€‚ç³»ç»Ÿä¸­çš„ Ideal Presentation Agent é€šè¿‡å¹»ç¯ç‰‡å¤„ç†ã€è§†è§‰è¯­è¨€åˆ†æã€æ—ç™½è„šæœ¬ç”Ÿæˆä»¥åŠä¸ªæ€§åŒ–è¯­éŸ³åˆæˆæŠ€æœ¯ï¼Œå°†ç”¨æˆ·æä¾›çš„ slides è½¬åŒ–ä¸ºç†æƒ³çš„æ¼”è®²èŒƒä¾‹è§†é¢‘ã€‚Coach Agent åˆ™åˆ©ç”¨å¤šæ¨¡æ€è¯­éŸ³åˆ†æï¼Œå°†ç”¨æˆ·çš„ç»ƒä¹ è¡¨ç°ä¸èŒƒä¾‹è¿›è¡Œå¯¹æ¯”ï¼Œå¹¶ä»¥ Observation-Impact-Suggestion (OIS) æ ¼å¼æä¾›ç»“æ„åŒ–åé¦ˆã€‚ä¸ºäº†æå‡å­¦ä¹ ä½“éªŒçš„çœŸå®æ€§ï¼Œç³»ç»Ÿè¿˜å¼•å…¥äº† Audience Agent æ¥æ¨¡æ‹Ÿè§‚ä¼—è§†è§’ï¼Œæä¾›åæ˜ å¬ä¼—ååº”å’Œå‚ä¸åº¦çš„äººæ€§åŒ–è¯„ä»·ã€‚è¿™äº›æ™ºèƒ½ä½“å…±åŒæ„å»ºäº†â€œè§‚å¯Ÿ-ç»ƒä¹ -åé¦ˆâ€çš„å­¦ä¹ é—­ç¯ï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡å¤šæ¨¡å‹é›†æˆå’Œè¯­éŸ³å…‹éš†(voice cloning)ç­‰æŠ€æœ¯ï¼Œåœ¨æ•™è‚²å’ŒèŒåœºé¢†åŸŸæä¾›å¯æ‰©å±•ä¸”ä»¥äººä¸ºæœ¬çš„æ¼”è®²æŠ€èƒ½å¼€å‘æ”¯æŒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "13pages,6figures",
      "pdf_url": "https://arxiv.org/pdf/2511.15253v2",
      "published_date": "2025-11-19 09:15:21 UTC",
      "updated_date": "2025-11-24 02:51:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:00:39.174569+00:00"
    },
    {
      "arxiv_id": "2511.15248v1",
      "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control",
      "title_zh": "EntroPICï¼šé€šè¿‡æ¯”ä¾‹-ç§¯åˆ†æ§åˆ¶å®ç°ç†µç¨³å®šï¼Œæ—¨åœ¨å®ç°å¤§è¯­è¨€æ¨¡å‹çš„ç¨³å®šé•¿æœŸè®­ç»ƒ",
      "authors": [
        "Kai Yang",
        "Xin Xu",
        "Yangkun Chen",
        "Weijie Liu",
        "Jiafei Lyu",
        "Zichuan Lin",
        "Deheng Ye",
        "Saiyong Yang"
      ],
      "abstract": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) é•¿æœŸè®­ç»ƒä¸­çš„æ¢ç´¢ç¨³å®šæ€§é—®é¢˜ï¼Œæå‡ºäº†åä¸º EntroPIC çš„æ–°æ–¹æ³•ã€‚ç”±äºç°æœ‰çš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ–¹æ³•éš¾ä»¥å¹³è¡¡æ­£è´Ÿæ ·æœ¬å¯¹ç†µ (Entropy) çš„ä¸åŒå½±å“ï¼Œå®¹æ˜“å¯¼è‡´æ¨¡å‹é™·å…¥æ¬¡ä¼˜è¡Œä¸ºæˆ–è¿‡æ—©æ”¶æ•›ã€‚EntroPIC å€Ÿé‰´äº†æ¯”ä¾‹ç§¯åˆ†æ§åˆ¶ (Proportional-Integral Control) åŸç†ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ­£è´Ÿæ ·æœ¬çš„æŸå¤±ç³»æ•°æ¥å®æ—¶ç¨³å®šç†µæ°´å¹³ï¼Œä»è€Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ‰æ•ˆæ¢ç´¢ã€‚è¯¥ç ”ç©¶ä¸ºåŒç­–ç•¥ (On-policy) å’Œå¼‚ç­–ç•¥ (Off-policy) å­¦ä¹ åœºæ™¯æä¾›äº†å…¨é¢çš„ç†è®ºåˆ†æï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ§åˆ¶å¤§è§„æ¨¡ LLM è®­ç»ƒç†µå€¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¯å®ï¼ŒEntroPIC èƒ½å¤ŸæˆåŠŸç»´æŒé¢„æœŸçš„ç†µæ°´å¹³ï¼Œæ˜¾è‘—æå‡äº† LLMs å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ç¨³å®šæ€§å’Œæœ€ç»ˆæ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15248v1",
      "published_date": "2025-11-19 09:06:42 UTC",
      "updated_date": "2025-11-19 09:06:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:01:12.876851+00:00"
    },
    {
      "arxiv_id": "2511.15211v2",
      "title": "OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition",
      "title_zh": "OEMAï¼šé¢å‘é›¶æ ·æœ¬ä¸´åºŠå‘½åå®ä½“è¯†åˆ«çš„æœ¬ä½“å¢å¼ºå¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶",
      "authors": [
        "Xinli Tao",
        "Xin Dong",
        "Xuezhong Zhou"
      ],
      "abstract": "With the rapid expansion of unstructured clinical texts in electronic health records (EHRs), clinical named entity recognition (NER) has become a crucial technique for extracting medical information. However, traditional supervised models such as CRF and BioClinicalBERT suffer from high annotation costs. Although zero-shot NER based on large language models (LLMs) reduces the dependency on labeled data, challenges remain in aligning example selection with task granularity and effectively integrating prompt design with self-improvement frameworks. To address these limitations, we propose OEMA, a novel zero-shot clinical NER framework based on multi-agent collaboration. OEMA consists of three core components: (1) a self-annotator that autonomously generates candidate examples; (2) a discriminator that leverages SNOMED CT to filter token-level examples by clinical relevance; and (3) a predictor that incorporates entity-type descriptions to enhance inference accuracy. Experimental results on two benchmark datasets, MTSamples and VAERS, demonstrate that OEMA achieves state-of-the-art performance under exact-match evaluation. Moreover, under related-match criteria, OEMA performs comparably to the supervised BioClinicalBERT model while significantly outperforming the traditional CRF method. OEMA improves zero-shot clinical NER, achieving near-supervised performance under related-match criteria. Future work will focus on continual learning and open-domain adaptation to expand its applicability in clinical NLP.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OEMAï¼Œä¸€ç§æ—¨åœ¨è§£å†³ä¸´åºŠå‘½åå®ä½“è¯†åˆ«(NER)ä¸­é«˜æ ‡æ³¨æˆæœ¬é—®é¢˜çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰çš„é›¶æ ·æœ¬(Zero-Shot) NERåœ¨ç¤ºä¾‹é€‰æ‹©ä¸ä»»åŠ¡ç²’åº¦åŒ¹é…ä¸Šçš„ä¸è¶³ï¼ŒOEMAç»“åˆäº†è‡ªæ ‡æ³¨å™¨(Self-Annotator)ã€åŸºäºSNOMED CTçš„é‰´åˆ«å™¨(Discriminator)å’Œé¢„æµ‹å™¨(Predictor)ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªä¸»ç”Ÿæˆå€™é€‰ç¤ºä¾‹ã€åˆ©ç”¨ä¸´åºŠçŸ¥è¯†å›¾è°±ç­›é€‰ä»¤ç‰Œçº§ç¤ºä¾‹ä»¥åŠæ•´åˆå®ä½“ç±»å‹æè¿°æ¥æ˜¾è‘—æå‡æ¨ç†çš„å‡†ç¡®æ€§ã€‚åœ¨MTSampleså’ŒVAERSä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOEMAåœ¨ç²¾ç¡®åŒ¹é…è¯„ä¼°ä¸‹è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„(SOTA)æ€§èƒ½ã€‚åœ¨ç›¸å…³åŒ¹é…æ ‡å‡†ä¸‹ï¼ŒOEMAçš„è¡¨ç°ä¸æœ‰ç›‘ç£çš„BioClinicalBERTæ¨¡å‹ç›¸å½“ï¼Œä¸”æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„CRFæ–¹æ³•ï¼Œå®ç°äº†æ¥è¿‘æœ‰ç›‘ç£æ°´å¹³çš„ä¸´åºŠä¿¡æ¯æå–æ•ˆæœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 4 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.15211v2",
      "published_date": "2025-11-19 08:02:55 UTC",
      "updated_date": "2025-11-20 04:13:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:00:51.575183+00:00"
    },
    {
      "arxiv_id": "2511.15210v1",
      "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story",
      "title_zh": "æ­ç¤ºæ–‡æœ¬çš„æœ¬å¾ç»´åº¦ï¼šä»å­¦æœ¯æ‘˜è¦åˆ°åˆ›æ„æ•…äº‹",
      "authors": [
        "Vladislav Pedashenko",
        "Laida Kushnareva",
        "Yana Khassan Nibal",
        "Eduard Tulchinskii",
        "Kristian Kuznetsov",
        "Vladislav Zharchinskii",
        "Yury Maximov",
        "Irina Piontkovskaya"
      ],
      "abstract": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡æœ¬çš„å†…åœ¨ç»´åº¦(Intrinsic Dimension, ID)ï¼Œé€šè¿‡äº¤å‰ç¼–ç å™¨åˆ†æ(cross-encoder analysis)ã€è¯­è¨€ç‰¹å¾å’Œç¨€ç–è‡ªåŠ¨ç¼–ç å™¨(Sparse Autoencoders, SAEs)é¦–æ¬¡ç³»ç»Ÿç ”ç©¶äº†å†³å®šæ–‡æœ¬IDçš„è§£é‡Šæ€§å› ç´ ã€‚ç ”ç©¶å‘ç°IDä¸åŸºäºç†µçš„æŒ‡æ ‡(entropy-based metrics)å…·æœ‰äº’è¡¥æ€§ï¼Œå®ƒæ•è·äº†ä¸é¢„æµ‹è´¨é‡æ­£äº¤çš„å‡ ä½•å¤æ‚åº¦ã€‚ä¸åŒä½“è£çš„æ–‡æœ¬åœ¨IDä¸Šè¡¨ç°å‡ºç¨³å¥çš„åˆ†å±‚ç‰¹æ€§ï¼Œå…¶ä¸­ç§‘å­¦è®ºæ–‡çš„IDè¾ƒä½ï¼Œè€Œåˆ›æ„å†™ä½œçš„IDæœ€é«˜ï¼Œè¡¨æ˜å¤§è¯­è¨€æ¨¡å‹(LLMs)è®¤ä¸ºç§‘å­¦æ–‡æœ¬åœ¨è¡¨å¾ä¸Šæ›´ä¸ºç®€å•ã€‚é€šè¿‡SAEsè¯†åˆ«å‡ºçš„å› æœç‰¹å¾è¿›ä¸€æ­¥è¯å®ï¼Œæ­£å¼è¯­æ°”å’Œç»Ÿè®¡æ•°æ®ç­‰ç§‘å­¦ä¿¡å·ä¼šé™ä½IDï¼Œè€Œä¸ªæ€§åŒ–å’Œæƒ…æ„Ÿç­‰äººç±»åŒ–ä¿¡å·åˆ™ä¼šå¢åŠ IDã€‚è¿™é¡¹å¤šç»´åˆ†æä¸ºç†è§£LLMsçš„è¡¨å¾ç©ºé—´ä»¥åŠæ­£ç¡®ä½¿ç”¨å’Œè§£é‡ŠIDæä¾›äº†å®é™…æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15210v1",
      "published_date": "2025-11-19 08:00:40 UTC",
      "updated_date": "2025-11-19 08:00:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:01:04.675678+00:00"
    },
    {
      "arxiv_id": "2511.15204v1",
      "title": "Physics-Based Benchmarking Metrics for Multimodal Synthetic Images",
      "title_zh": "é¢å‘å¤šæ¨¡æ€åˆæˆå›¾åƒçš„åŸºäºç‰©ç†çš„åŸºå‡†æµ‹è¯„æŒ‡æ ‡",
      "authors": [
        "Kishor Datta Gupta",
        "Marufa Kamal",
        "Md. Mahfuzur Rahman",
        "Fahad Rahman",
        "Mohd Ariful Haque",
        "Sunzida Siddique"
      ],
      "abstract": "Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹BLEUã€CIDErã€VQA scoreã€SigLIP-2å’ŒCLIPScoreç­‰ç°æœ‰æŒ‡æ ‡åœ¨ç‰¹å®šé¢†åŸŸæˆ–ä¸Šä¸‹æ–‡ç›¸å…³åœºæ™¯ä¸­æ— æ³•æ•æ‰è¯­ä¹‰æˆ–ç»“æ„å‡†ç¡®æ€§çš„å±€é™ï¼Œæå‡ºäº†ç‰©ç†çº¦æŸå¤šæ¨¡æ€æ•°æ®è¯„ä¼°ï¼ˆPhysics-Constrained Multimodal Data Evaluation, PCMDEï¼‰æŒ‡æ ‡ã€‚è¯¥æŒ‡æ ‡é€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€çŸ¥è¯†æ˜ å°„ä»¥åŠè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥å…‹æœä¸Šè¿°é—®é¢˜ã€‚å…¶æ¶æ„ç”±ä¸‰ä¸ªæ ¸å¿ƒé˜¶æ®µç»„æˆï¼šé¦–å…ˆåˆ©ç”¨ç›®æ ‡æ£€æµ‹å’ŒVLMsæå–ç©ºé—´ä¸è¯­ä¹‰çš„å¤šæ¨¡æ€ç‰¹å¾ï¼›æ¥ç€é€šè¿‡ç½®ä¿¡åº¦åŠ æƒç»„ä»¶èåˆï¼ˆConfidence-Weighted Component Fusionï¼‰è¿›è¡Œè‡ªé€‚åº”çš„ç»„ä»¶çº§éªŒè¯ï¼›æœ€ååˆ©ç”¨LLMsè¿›è¡Œç‰©ç†å¼•å¯¼æ¨ç†ï¼Œä»¥å¼ºåŒ–å¯¹é½ã€ä½ç½®å’Œä¸€è‡´æ€§ç­‰ç»“æ„ä¸å…³ç³»çº¦æŸã€‚è¿™ç§ç‰©ç†é©±åŠ¨çš„æ–¹æ³•ä¸ºå¤šæ¨¡æ€åˆæˆå›¾åƒçš„åŸºå‡†æµ‹è¯•æä¾›äº†æ›´ç²¾ç¡®çš„è¯„ä¼°ç»´åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15204v1",
      "published_date": "2025-11-19 07:52:20 UTC",
      "updated_date": "2025-11-19 07:52:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:01:40.562422+00:00"
    },
    {
      "arxiv_id": "2511.15203v1",
      "title": "Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks",
      "title_zh": "ä»¥ IPI ä¸ºä¸­å¿ƒçš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“é˜²å¾¡æ¡†æ¶ï¼šåˆ†ç±»ä½“ç³»ã€è¯„ä¼°ä¸åˆ©ç”¨",
      "authors": [
        "Zimo Ji",
        "Xunguang Wang",
        "Zongjie Li",
        "Pingchuan Ma",
        "Yudong Gao",
        "Daoyuan Wu",
        "Xincheng Yan",
        "Tian Tian",
        "Shuai Wang"
      ],
      "abstract": "Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“åœ¨å‡½æ•°è°ƒç”¨(function-calling)ä¸­é¢ä¸´çš„é—´æ¥æç¤ºæ³¨å…¥(Indirect Prompt Injection, IPI)å¨èƒï¼Œé¦–æ¬¡å¯¹ç°æœ‰çš„IPI-Centricé˜²å¾¡æ¡†æ¶è¿›è¡Œäº†ç³»ç»ŸåŒ–çŸ¥è¯†(SoK)æ•´ç†ã€‚ä½œè€…ä»äº”ä¸ªç»´åº¦æå‡ºäº†å…¨é¢çš„åˆ†ç±»æ³•(taxonomy)ï¼Œå¹¶æ·±å…¥è¯„ä¼°äº†ä»£è¡¨æ€§é˜²å¾¡æ¡†æ¶åœ¨å®‰å…¨æ€§å’Œå¯ç”¨æ€§æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡åˆ†æé˜²å¾¡å¤±è´¥çš„æ¡ˆä¾‹ï¼Œç ”ç©¶è¯†åˆ«å‡ºäº†å¯¼è‡´é˜²å¾¡è¢«ç»•è¿‡çš„å…­ä¸ªæ ¹æœ¬åŸå› ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†ä¸‰ç§æ–°å‹è‡ªé€‚åº”æ”»å‡»(adaptive attacks)ï¼Œæ˜¾è‘—æå‡äº†é’ˆå¯¹ç‰¹å®šæ¡†æ¶çš„æ”»å‡»æˆåŠŸç‡ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ­éœ²äº†å½“å‰é˜²å¾¡æœºåˆ¶çš„ä¸¥é‡ç¼ºé™·ï¼Œä¹Ÿä¸ºæœªæ¥æ„å»ºæ›´å®‰å…¨ã€æ›´é«˜æ•ˆçš„IPI-Centricæ™ºèƒ½ä½“é˜²å¾¡ä½“ç³»æä¾›äº†å…³é”®æ´å¯Ÿå’Œç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15203v1",
      "published_date": "2025-11-19 07:47:30 UTC",
      "updated_date": "2025-11-19 07:47:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:02:04.875254+00:00"
    },
    {
      "arxiv_id": "2511.15202v1",
      "title": "SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making",
      "title_zh": "SOLIDï¼šååŒä¼˜åŒ–ä¸å¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½å†³ç­–æ¡†æ¶",
      "authors": [
        "Yinsheng Wang",
        "Tario G You",
        "LÃ©onard Boussioux",
        "Shan Liu"
      ],
      "abstract": "This paper introduces SOLID (Synergizing Optimization and Large Language Models for Intelligent Decision-Making), a novel framework that integrates mathematical optimization with the contextual capabilities of large language models (LLMs). SOLID facilitates iterative collaboration between optimization and LLMs agents through dual prices and deviation penalties. This interaction improves the quality of the decisions while maintaining modularity and data privacy. The framework retains theoretical convergence guarantees under convexity assumptions, providing insight into the design of LLMs prompt. To evaluate SOLID, we applied it to a stock portfolio investment case with historical prices and financial news as inputs. Empirical results demonstrate convergence under various scenarios and indicate improved annualized returns compared to a baseline optimizer-only method, validating the synergy of the two agents. SOLID offers a promising framework for advancing automated and intelligent decision-making across diverse domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SOLID æ¡†æ¶ï¼Œæ—¨åœ¨å°†æ•°å­¦ä¼˜åŒ– (mathematical optimization) ä¸å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ç›¸ç»“åˆï¼Œä»¥å®ç°æ›´æ™ºèƒ½çš„å†³ç­–åˆ¶å®šã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹å¶ä»·æ ¼ (dual prices) å’Œåå·®æƒ©ç½š (deviation penalties) æœºåˆ¶ï¼Œä¿ƒè¿›äº†ä¼˜åŒ–ä»£ç†ä¸ LLMs ä»£ç†ä¹‹é—´çš„è¿­ä»£åä½œï¼Œåœ¨æå‡å†³ç­–è´¨é‡çš„åŒæ—¶ä¿æŒäº†ç³»ç»Ÿçš„æ¨¡å—åŒ–å’Œæ•°æ®éšç§ã€‚SOLID èƒ½å¤Ÿåœ¨å‡¸æ€§å‡è®¾ä¸‹ä¿ç•™ç†è®ºæ”¶æ•›æ€§ (theoretical convergence guarantees)ï¼Œå¹¶ä¸º LLMs æç¤ºè¯è®¾è®¡æä¾›æŒ‡å¯¼ã€‚ç ”ç©¶äººå‘˜å°†è¯¥æ¡†æ¶åº”ç”¨äºè‚¡ç¥¨ç»„åˆæŠ•èµ„æ¡ˆä¾‹ï¼Œåˆ©ç”¨å†å²ä»·æ ¼å’Œè´¢ç»æ–°é—»ä½œä¸ºè¾“å…¥è¿›è¡Œæµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSOLID åœ¨å¤šç§åœºæ™¯ä¸‹å‡èƒ½å®ç°æ”¶æ•›ï¼Œä¸”å…¶å¹´åŒ–æ”¶ç›Šç‡æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨ä¼˜åŒ–å™¨çš„åŸºå‡†æ–¹æ³•ã€‚è¯¥ç ”ç©¶éªŒè¯äº†ä¸¤ç§ä»£ç†ååŒå·¥ä½œçš„æœ‰æ•ˆæ€§ï¼Œä¸ºè·¨é¢†åŸŸçš„è‡ªåŠ¨åŒ–æ™ºèƒ½å†³ç­–æä¾›äº†æå…·å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025 WORKSHOP ML*OR Workshop: Mathematical Foundations and Operational Integration of Machine Learning for Uncertainty-Aware Decision-Making",
      "pdf_url": "https://arxiv.org/pdf/2511.15202v1",
      "published_date": "2025-11-19 07:44:36 UTC",
      "updated_date": "2025-11-19 07:44:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:01:31.952826+00:00"
    },
    {
      "arxiv_id": "2511.15194v1",
      "title": "Eq.Bot: Enhance Robotic Manipulation Learning via Group Equivariant Canonicalization",
      "title_zh": "Eq.Botï¼šé€šè¿‡ç¾¤ç­‰å˜è§„èŒƒåŒ–å¢å¼ºæœºå™¨äººæ“ä½œå­¦ä¹ ",
      "authors": [
        "Jian Deng",
        "Yuandong Wang",
        "Yangfu Zhu",
        "Tao Feng",
        "Tianyu Wo",
        "Zhenzhou Shao"
      ],
      "abstract": "Robotic manipulation systems are increasingly deployed across diverse domains. Yet existing multi-modal learning frameworks lack inherent guarantees of geometric consistency, struggling to handle spatial transformations such as rotations and translations. While recent works attempt to introduce equivariance through bespoke architectural modifications, these methods suffer from high implementation complexity, computational cost, and poor portability. Inspired by human cognitive processes in spatial reasoning, we propose Eq.Bot, a universal canonicalization framework grounded in SE(2) group equivariant theory for robotic manipulation learning. Our framework transforms observations into a canonical space, applies an existing policy, and maps the resulting actions back to the original space. As a model-agnostic solution, Eq.Bot aims to endow models with spatial equivariance without requiring architectural modifications. Extensive experiments demonstrate the superiority of Eq.Bot under both CNN-based (e.g., CLIPort) and Transformer-based (e.g., OpenVLA-OFT) architectures over existing methods on various robotic manipulation tasks, where the most significant improvement can reach 50.0%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Eq.Botï¼Œä¸€ä¸ªåŸºäº SE(2) ç¾¤ç­‰å˜ç†è®º (group equivariant theory) çš„é€šç”¨è§„èŒƒåŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºæœºå™¨äººæ“ä½œå­¦ä¹ çš„ç©ºé—´ç­‰å˜æ€§ã€‚é’ˆå¯¹ç°æœ‰æ¡†æ¶åœ¨å¤„ç†æ—‹è½¬å’Œå¹³ç§»æ—¶ç¼ºä¹å‡ ä½•ä¸€è‡´æ€§ï¼Œä»¥åŠä¸“ç”¨ç­‰å˜æ¶æ„å®ç°å¤æ‚ä¸”è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼ŒEq.Bot æä¾›äº†ä¸€ç§æ¨¡å‹æ— å…³ (model-agnostic) çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶é€šè¿‡å°†è§‚æµ‹ç»“æœå˜æ¢åˆ°è§„èŒƒç©ºé—´ (canonical space)ï¼Œåœ¨åº”ç”¨ç°æœ‰ç­–ç•¥åå†å°†ç”Ÿæˆçš„åŠ¨ä½œæ˜ å°„å›åŸå§‹ç©ºé—´ï¼Œä»è€Œä½¿æ¨¡å‹æ— éœ€ä¿®æ”¹æ¶æ„å³å¯å…·å¤‡ç©ºé—´ç­‰å˜æ€§ã€‚åœ¨åŸºäº CNN (å¦‚ CLIPort) å’Œ Transformer (å¦‚ OpenVLA-OFT) çš„å¤šç§æœºå™¨äººæ“ä½œä»»åŠ¡å®éªŒä¸­ï¼ŒEq.Bot ç›¸æ¯”ç°æœ‰æ–¹æ³•è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ä¼˜è¶Šæ€§ï¼Œå…¶æ€§èƒ½æå‡æœ€é«˜å¯è¾¾ 50.0%ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæå‡æœºå™¨äººæ“ä½œç³»ç»Ÿçš„å‡ ä½•ä¸€è‡´æ€§å’Œç©ºé—´æ¨ç†èƒ½åŠ›æä¾›äº†é«˜æ•ˆä¸”æ˜“äºç§»æ¤çš„æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "12 pages, 4 figures and 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.15194v1",
      "published_date": "2025-11-19 07:28:20 UTC",
      "updated_date": "2025-11-19 07:28:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:01:41.961152+00:00"
    },
    {
      "arxiv_id": "2511.15192v2",
      "title": "As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files",
      "title_zh": "ä¼¼æ›¾ç›¸è¯†ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«å·²è§æ–‡ä»¶æ—¶è¡¨ç°å‡ºçš„ç¡®å®šæ€§",
      "authors": [
        "Haodong Li",
        "Jingqi Zhang",
        "Xiao Cheng",
        "Peihua Mai",
        "Haoyu Wang",
        "Yan Pang"
      ],
      "abstract": "The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.\n  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen\" (training data) and ``unseen\" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ…å«ç‰ˆæƒææ–™çš„æµ·é‡æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒæ‰€å¼•å‘çš„ä¾µæƒæ‹…å¿§ï¼Œåˆ†æäº†ç°æœ‰æˆå‘˜æ¨ç†æ”»å‡»(Membership Inference Attacks)åœ¨åº”å¯¹æ¨¡å‹è¿‡åº¦è‡ªä¿¡å’Œä¾èµ–ç»éªŒé˜ˆå€¼æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†COPYCHECKæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ•è·ä¸ç¡®å®šæ€§ä¿¡å·(uncertainty signals)æ¥æœ‰æ•ˆåŒºåˆ†è®­ç»ƒæ•°æ®ä¸­çš„â€œå·²è§â€ä¸â€œæœªè§â€å†…å®¹ï¼ŒæˆåŠŸå°†LLMçš„è¿‡åº¦è‡ªä¿¡ç‰¹å¾è½¬åŒ–ä¸ºæ£€æµ‹ä¼˜åŠ¿ã€‚COPYCHECKå®æ–½äº†åŒé‡ç­–ç•¥ï¼ŒåŒ…æ‹¬å°†æ–‡ä»¶æˆ˜ç•¥æ€§åœ°ç»†åˆ†ä¸ºç‰‡æ®µä»¥é™ä½å¯¹å¤§è§„æ¨¡æ•°æ®çš„ä¾èµ–ï¼Œä»¥åŠé‡‡ç”¨ä¸ç¡®å®šæ€§å¼•å¯¼çš„æ— ç›‘ç£èšç±»(unsupervised clustering)æ¥å–æ¶ˆå¯¹æ‰‹åŠ¨è°ƒä¼˜é˜ˆå€¼çš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒCOPYCHECKåœ¨LLaMA 7bå’ŒLLaMA2 7bæ¨¡å‹ä¸Šçš„æ£€æµ‹å¹³è¡¡å‡†ç¡®ç‡(balanced accuracy)åˆ†åˆ«è¾¾åˆ°90.1%å’Œ91.6%ï¼Œè¾ƒSOTAåŸºçº¿æå‡äº†90%ä»¥ä¸Šã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹æ¶æ„é—´å±•ç°å‡ºå¼ºåŠ²çš„æ³›åŒ–æ€§(generalizability)ï¼Œä½œä¸ºé¦–ä¸ªå°†ä¸ç¡®å®šæ€§åº”ç”¨äºç‰ˆæƒæ£€æµ‹çš„ç ”ç©¶ï¼Œä¸ºæå‡æ¨¡å‹è®­ç»ƒæ•°æ®çš„é€æ˜åº¦æä¾›äº†æœ‰åŠ›çš„å®è·µå·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15192v2",
      "published_date": "2025-11-19 07:24:22 UTC",
      "updated_date": "2025-11-20 10:01:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:01:45.365574+00:00"
    },
    {
      "arxiv_id": "2511.15191v1",
      "title": "HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization",
      "title_zh": "HISE-KTï¼šååŒå¼‚è´¨ä¿¡æ¯ç½‘ç»œä¸å¤§è¯­è¨€æ¨¡å‹çš„å…ƒè·¯å¾„ä¼˜åŒ–å¯è§£é‡ŠçŸ¥è¯†è¿½è¸ª",
      "authors": [
        "Zhiyi Duan",
        "Zixing Shi",
        "Hongyu Yuan",
        "Qi Wang"
      ],
      "abstract": "Knowledge Tracing (KT) aims to mine students' evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HISE-KT æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰çŸ¥è¯†è¿½è¸ª (Knowledge Tracing, KT) æ–¹æ³•ä¸­å¼‚æ„ä¿¡æ¯ç½‘ç»œ (HINs) å™ªå£°å¤šä»¥åŠå¤§è¯­è¨€æ¨¡å‹ (LLMs) å¿½ç•¥å­¦ç”Ÿé—´å…³è”çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ„å»ºäº†åŒ…å«å¤šç§èŠ‚ç‚¹ç±»å‹çš„å¤šå…³ç³» HIN ä»¥æ•æ‰ç»“æ„åŒ–å…³ç³»ï¼Œå¹¶åˆ›æ–°æ€§åœ°åˆ©ç”¨ LLM å¯¹å…ƒè·¯å¾„ (Meta-Path) å®ä¾‹è¿›è¡Œæ™ºèƒ½è¯„åˆ†ä¸ç­›é€‰ï¼Œå®ç°äº†è‡ªåŠ¨åŒ–çš„å…ƒè·¯å¾„è´¨é‡è¯„ä¼°ã€‚å—æ•™è‚²å¿ƒç†å­¦å¯å‘ï¼ŒHISE-KT è®¾è®¡äº†åŸºäºå…ƒè·¯å¾„çš„ç›¸ä¼¼å­¦ç”Ÿæ£€ç´¢æœºåˆ¶ï¼Œä¸ºé¢„æµ‹æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡èƒŒæ™¯ã€‚é€šè¿‡ç»“æ„åŒ–æç¤º (Structured Prompt) æ•´åˆå†å²æ•°æ®ä¸æ£€ç´¢åˆ°çš„ç›¸ä¼¼è½¨è¿¹ï¼ŒLLM èƒ½å¤Ÿç”Ÿæˆé«˜å‡†ç¡®åº¦çš„é¢„æµ‹ä»¥åŠå…·æœ‰è¯æ®æ”¯æ’‘çš„å¯è§£é‡Šæ€§æŠ¥å‘Šã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHISE-KT åœ¨é¢„æµ‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„ KT åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15191v1",
      "published_date": "2025-11-19 07:24:10 UTC",
      "updated_date": "2025-11-19 07:24:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:01:51.550535+00:00"
    },
    {
      "arxiv_id": "2511.15190v1",
      "title": "Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning",
      "title_zh": "æ©ç è‡ªå›å½’å˜åˆ†åŠ é€Ÿï¼šå¿«é€Ÿæ¨ç†åŠ©åŠ›å¼ºåŒ–å­¦ä¹ å®ç”¨åŒ–",
      "authors": [
        "Yuxuan Gu",
        "Weimin Bai",
        "Yifei Wang",
        "Weijian Luo",
        "He Sun"
      ],
      "abstract": "Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ©ç è‡ªå›å½’æ‰©æ•£æ¨¡å‹ (MAR) å› å¤–å±‚ AR å¾ªç¯å’Œå†…å±‚æ‰©æ•£å»å™ªé“¾å¯¼è‡´çš„æ¨ç†ç¼“æ…¢é—®é¢˜ï¼Œæå‡ºäº† MARVAL (Masked Auto-regressive Variational Acceleration) è’¸é¦æ¡†æ¶ã€‚MARVAL é€šè¿‡ä¸€ç§æ–°å‹çš„åŸºäºå¾—åˆ†çš„å˜åˆ†ç›®æ ‡ (score-based variational objective)ï¼Œå°†å¤æ‚çš„æ‰©æ•£é“¾å‹ç¼©ä¸ºå•ä¸ª AR ç”Ÿæˆæ­¥éª¤ï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å®ç°äº†è¶…è¿‡ 30 å€çš„æ¨ç†åŠ é€Ÿã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒMARVAL è§£å†³äº† MAR åœ¨å¼ºåŒ–å­¦ä¹  (RL) åè®­ç»ƒä¸­æ•ˆç‡ä½ä¸‹çš„ç“¶é¢ˆï¼Œé€šè¿‡ MARVAL-RL å®ç°äº†é«˜æ•ˆçš„åå¥½å¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMARVAL-Huge åœ¨ ImageNet 256x256 ä¸Šçš„ FID è¾¾åˆ° 2.00ï¼Œå¹¶æ˜¾è‘—æå‡äº† CLIP å’Œå›¾åƒå¥–åŠ±è¯„åˆ†ã€‚è¯¥å·¥ä½œä¸ºæ©ç è‡ªå›å½’æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿé‡‡æ ·ä¸å¼ºåŒ–å­¦ä¹ å®è·µå¼€è¾Ÿäº†é¦–ä¸ªå¯è¡Œè·¯å¾„ï¼Œä½¿å…¶åœ¨ç”Ÿæˆæ¨¡å‹åè®­ç»ƒèŒƒå¼ä¸­æ›´å…·å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15190v1",
      "published_date": "2025-11-19 07:24:07 UTC",
      "updated_date": "2025-11-19 07:24:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:01:57.675956+00:00"
    },
    {
      "arxiv_id": "2511.15182v1",
      "title": "SWR-Viz: AI-assisted Interactive Visual Analytics Framework for Ship Weather Routing",
      "title_zh": "SWR-Vizï¼šé¢å‘èˆ¹èˆ¶æ°”è±¡å¯»å¾„çš„äººå·¥æ™ºèƒ½è¾…åŠ©äº¤äº’å¼å¯è§†åŒ–åˆ†ææ¡†æ¶",
      "authors": [
        "Subhashis Hazarika",
        "Leonard Lupin-Jimenez",
        "Rohit Vuppala",
        "Ashesh Chattopadhyay",
        "Hon Yung Wong"
      ],
      "abstract": "Efficient and sustainable maritime transport increasingly depends on reliable forecasting and adaptive routing, yet operational adoption remains difficult due to forecast latencies and the need for human judgment in rapid decision-making under changing ocean conditions. We introduce SWR-Viz, an AI-assisted visual analytics framework that combines a physics-informed Fourier Neural Operator wave forecast model with SIMROUTE-based routing and interactive emissions analytics. The framework generates near-term forecasts directly from current conditions, supports data assimilation with sparse observations, and enables rapid exploration of what-if routing scenarios. We evaluate the forecast models and SWR-Viz framework along key shipping corridors in the Japan Coast and Gulf of Mexico, showing both improved forecast stability and realistic routing outcomes comparable to ground-truth reanalysis wave products. Expert feedback highlights the usability of SWR-Viz, its ability to isolate voyage segments with high emission reduction potential, and its value as a practical decision-support system. More broadly, this work illustrates how lightweight AI forecasting can be integrated with interactive visual analytics to support human-centered decision-making in complex geospatial and environmental domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SWR-Vizï¼Œä¸€ä¸ªAIè¾…åŠ©çš„äº¤äº’å¼è§†è§‰åˆ†ææ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³èˆ¹èˆ¶æ°”è±¡å¯¼èˆªä¸­å› é¢„æµ‹å»¶è¿Ÿå’Œå¿«é€Ÿå†³ç­–éœ€æ±‚å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒé›†æˆäº†åŸºäºç‰©ç†ä¿¡æ¯çš„ Fourier Neural Operator (FNO) æµ·æµªé¢„æµ‹æ¨¡å‹ä¸ SIMROUTE è·¯å¾„è§„åˆ’ç®—æ³•ï¼Œå¹¶ç»“åˆäº†äº¤äº’å¼æ’æ”¾åˆ†æåŠŸèƒ½ã€‚ç³»ç»Ÿæ”¯æŒä»å½“å‰ç¯å¢ƒç›´æ¥ç”ŸæˆçŸ­æœŸé¢„æµ‹å’Œç¨€ç–è§‚æµ‹æ•°æ®çš„åŒåŒ–å¤„ç†ï¼Œå…è®¸ç”¨æˆ·å¿«é€Ÿè¿›è¡Œ \"what-if\" è·¯å¾„æ–¹æ¡ˆçš„æ¢ç´¢ä¸å¯¹æ¯”ã€‚åœ¨æ—¥æœ¬æµ·å²¸å’Œå¢¨è¥¿å“¥æ¹¾çš„å®è¯è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨é¢„æµ‹ç¨³å®šæ€§å’Œè·¯å¾„ç”Ÿæˆæ•ˆæœä¸Šä¸çœŸå®å†åˆ†æäº§å“ç›¸å½“ã€‚ä¸“å®¶è¯„ä»·è®¤ä¸º SWR-Viz å…·å¤‡æé«˜çš„æ˜“ç”¨æ€§ï¼Œèƒ½æœ‰æ•ˆè¯†åˆ«å…·å¤‡å‡æ’æ½œåŠ›çš„èˆªæ®µï¼Œæ˜¯è¾…åŠ©äººç±»è¿›è¡Œå¤æ‚åœ°ç†ç¯å¢ƒå†³ç­–çš„å®ç”¨å·¥å…·ã€‚è¿™é¡¹å·¥ä½œä¸ºè½»é‡çº§ AI é¢„æµ‹æ¨¡å‹ä¸äº¤äº’å¼è§†è§‰åˆ†æçš„æ·±åº¦èåˆæä¾›äº†èŒƒä¾‹ï¼Œæ˜¾è‘—æå‡äº†æµ·äº‹è¿è¾“çš„å†³ç­–æ•ˆç‡ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15182v1",
      "published_date": "2025-11-19 07:09:54 UTC",
      "updated_date": "2025-11-19 07:09:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:01:57.273541+00:00"
    },
    {
      "arxiv_id": "2511.15174v1",
      "title": "FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model",
      "title_zh": "FaultDiffusionï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å°æ ·æœ¬æ•…éšœæ—¶é—´åºåˆ—ç”Ÿæˆ",
      "authors": [
        "Yi Xu",
        "Zhigang Chen",
        "Rui Wang",
        "Yangfan Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Jiaqi Liu"
      ],
      "abstract": "In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FaultDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Diffusion Model çš„æ–°å‹ Few-Shot æ•…éšœæ—¶é—´åºåˆ—ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å·¥ä¸šè®¾å¤‡ç›‘æµ‹ä¸­æ•…éšœæ•°æ®åŒ®ä¹åŠé«˜æ˜‚æ ‡æ³¨æˆæœ¬å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨ Few-Shot åœºæ™¯ä¸‹éš¾ä»¥æ•æ‰æ•…éšœåˆ†å¸ƒä»¥åŠç”Ÿæˆçš„æ ·æœ¬ç¼ºä¹çœŸå®æ€§å’Œå¤šæ ·æ€§ç­‰å±€é™ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº† Positive-negative difference adapterï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ­£å¸¸æ•°æ®åˆ†å¸ƒæ¥å»ºæ¨¡æ­£å¸¸ä¸æ•…éšœåŸŸä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„æ•…éšœåˆæˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† Diversity lossï¼Œé€šè¿‡æ ·æœ¬é—´å·®å¼‚æ­£åˆ™åŒ–æ¥é˜²æ­¢ Mode collapseï¼Œå¹¶é¼“åŠ±ç”Ÿæˆæ›´å…·å¤šæ ·æ€§çš„æ•…éšœæ ·æœ¬ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®æ€§å’Œå¤šæ ·æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶åœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† State-of-the-art æ€§èƒ½ï¼Œä¸ºå·¥ä¸šæ•…éšœè¯Šæ–­å’Œé¢„æµ‹æ€§ç»´æŠ¤æä¾›äº†æœ‰æ•ˆçš„æ•°æ®å¢å¼ºæ‰‹æ®µã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "4 figures, 5 tables ,8 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.15174v1",
      "published_date": "2025-11-19 06:53:15 UTC",
      "updated_date": "2025-11-19 06:53:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:02:11.367390+00:00"
    },
    {
      "arxiv_id": "2511.15169v2",
      "title": "SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models",
      "title_zh": "SafeRBenchï¼šå¤§æ¨ç†æ¨¡å‹å®‰å…¨è¯„ä¼°ç»¼åˆåŸºå‡†",
      "authors": [
        "Xin Gao",
        "Shaohan Yu",
        "Zerui Chen",
        "Yueming Lyu",
        "Weichen Yu",
        "Guanghao Li",
        "Jiyao Liu",
        "Jianxiong Gao",
        "Jian Liang",
        "Ziwei Liu",
        "Chenyang Si"
      ],
      "abstract": "Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SafeRBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¤§æ¨ç†æ¨¡å‹ï¼ˆLarge Reasoning Models, LRMsï¼‰å®‰å…¨æ€§çš„ç«¯åˆ°ç«¯è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨åº”å¯¹é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„æœ‰å®³å†…å®¹æ³¨å…¥åŠè¯¯å¯¼æ€§åˆç†åŒ–ç­‰æ–°é£é™©ã€‚åœ¨è¾“å…¥è¡¨å¾æ–¹é¢ï¼ŒSafeRBench é¦–æ¬¡å°†é£é™©ç±»åˆ«å’Œç­‰çº§çº³å…¥è®¾è®¡ï¼Œé€šè¿‡å¹³è¡¡çš„æç¤ºè¯å¥—ä»¶åæ˜ äº†å—å½±å“ç¾¤ä½“åŠå±å®³ç¨‹åº¦çš„å¤šæ ·æ€§ã€‚é’ˆå¯¹è¾“å‡ºåˆ†æï¼Œè¯¥åŸºå‡†å¼•å…¥äº†å¾®æ€ç»´åˆ†å—ï¼ˆMicro-thought Chunkingï¼‰æœºåˆ¶ï¼Œå°†é•¿æ¨ç†è½¨è¿¹åˆ†å‰²ä¸ºè¯­ä¹‰è¿è´¯çš„å•å…ƒï¼Œä»è€Œåœ¨åä¸ªå®‰å…¨ç»´åº¦ä¸Šå®ç°ç»†ç²’åº¦çš„è¯„ä¼°ã€‚ç ”ç©¶è¿˜é€šè¿‡ä¸“é—¨çš„äººç±»æ ‡æ³¨éªŒè¯äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œå¯¹ 19 ä¸ª LRMs çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSafeRBench èƒ½å¤Ÿæä¾›è¯¦å°½çš„å¤šç»´åº¦å®‰å…¨è§è§£ï¼Œä¸ºç†è§£å¤§æ¨ç†æ¨¡å‹çš„æ½œåœ¨é£é™©åŠä¿æŠ¤æœºåˆ¶æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "30 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.15169v2",
      "published_date": "2025-11-19 06:46:33 UTC",
      "updated_date": "2025-11-20 03:41:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:02:25.774912+00:00"
    },
    {
      "arxiv_id": "2511.15168v2",
      "title": "Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework",
      "title_zh": "Selenium æµ‹è¯•æ¡†æ¶ä¸­é¢å‘ç½‘é¡µæµè§ˆå™¨è‡ªåŠ¨è¡¨å•äº¤äº’çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Nguyen-Khang Le",
        "Hiep Nguyen",
        "Ngoc-Minh Nguyen",
        "Son T. Luu",
        "Trung Vo",
        "Quan Minh Bui",
        "Shoshin Nomura",
        "Le-Minh Nguyen"
      ],
      "abstract": "Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Selenium è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶ä¸‹çš„ç½‘é¡µè¡¨å•äº¤äº’ä»»åŠ¡ï¼Œè§£å†³äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è¯¥é¢†åŸŸç¼ºä¹å…¬å¼€ benchmark å’Œ dataset çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§å¾®è°ƒ LLMs ä»¥ç”Ÿæˆé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æå‡ Selenium è„šæœ¬çš„ç”Ÿæˆè´¨é‡ã€‚é€šè¿‡æ„å»ºæ¶µç›–å¤šç§çœŸå®åœºæ™¯çš„åˆæˆä¸äººå·¥æ ‡æ³¨æ•°æ®é›†ï¼Œç ”ç©¶å›¢é˜Ÿå¯¹ç”Ÿæˆçš„è„šæœ¬è¿›è¡Œäº† syntax correctnessã€script executability å’Œ input field coverage ç­‰å¤šç»´åº¦è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äº GPT-4o ç­‰ä¸»æµæ¨¡å‹ã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºåŸºäº LLM çš„è‡ªåŠ¨åŒ–ç½‘é¡µæµ‹è¯•æä¾›äº†èµ„æºæ”¯æŒï¼Œä¹Ÿä¸ºæœªæ¥çš„ç›¸å…³ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Published in the Proceedings of KSE 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.15168v2",
      "published_date": "2025-11-19 06:43:21 UTC",
      "updated_date": "2025-11-20 07:45:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:03:52.667390+00:00"
    },
    {
      "arxiv_id": "2511.15167v1",
      "title": "Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation",
      "title_zh": "å‘â€œè¿‡å¾€è‡ªæˆ‘â€å­¦ä¹ æ·±åº¦ï¼šé¢å‘é²æ£’æ·±åº¦ä¼°è®¡çš„è‡ªæˆ‘è¿›åŒ–å¯¹æ¯”å­¦ä¹ ",
      "authors": [
        "Jing Cao",
        "Kui Jiang",
        "Shenyi Li",
        "Xiaocheng Feng",
        "Yong Huang"
      ],
      "abstract": "Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººé¢†åŸŸä¸­è‡ªç›‘ç£æ·±åº¦ä¼°è®¡ (Self-supervised depth estimation) åœ¨é›¨ã€é›¾ç­‰æ¶åŠ£å¤©æ°”ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º SEC-Depth çš„è‡ªè¿›åŒ–å¯¹æ¯”å­¦ä¹ æ¡†æ¶ (Self-evolution contrastive learning framework)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„ä¸­é—´å‚æ•°æ„å»ºéšæ—¶é—´æ¼”åŒ–çš„å»¶è¿Ÿæ¨¡å‹ (Latency models)ï¼Œå¹¶è®¾è®¡äº†è‡ªè¿›åŒ–å¯¹æ¯”æ–¹æ¡ˆä»¥å‡è½»å¤æ‚ç¯å¢ƒä¸‹çš„æ€§èƒ½æŸå¤±ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶è€…å¼€å‘äº†å»¶è¿Ÿæ¨¡å‹çš„åŠ¨æ€æ›´æ–°ç­–ç•¥ä»¥æ•æ‰ä¸åŒè®­ç»ƒé˜¶æ®µçš„ä¼˜åŒ–çŠ¶æ€ï¼Œå¹¶å¼•å…¥è‡ªè¿›åŒ–å¯¹æ¯”æŸå¤± (SECL)ï¼Œå°†å†å²å»¶è¿Ÿæ¨¡å‹çš„è¾“å‡ºè§†ä¸ºè´Ÿæ ·æœ¬ï¼Œä»è€Œè‡ªé€‚åº”åœ°è°ƒæ•´å­¦ä¹ ç›®æ ‡ã€‚è¿™ç§æœºåˆ¶èƒ½å¤Ÿéšå¼æ„ŸçŸ¥å¤©æ°”é€€åŒ–çš„ä¸¥é‡ç¨‹åº¦ï¼Œæœ‰æ•ˆå‡å°‘äº†äººå·¥å¹²é¢„çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEC-Depth å¯ä»¥æ— ç¼é›†æˆåˆ°å¤šç§åŸºå‡†æ¨¡å‹ä¸­ï¼Œå¹¶æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯„ä¼° (Zero-shot evaluations) ä¸­çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15167v1",
      "published_date": "2025-11-19 06:42:40 UTC",
      "updated_date": "2025-11-19 06:42:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:03:53.066260+00:00"
    },
    {
      "arxiv_id": "2511.15165v2",
      "title": "Can MLLMs Detect Phishing? A Comprehensive Security Benchmark Suite Focusing on Dynamic Threats and Multimodal Evaluation in Academic Environments",
      "title_zh": "MLLM èƒ½å¦æ£€æµ‹ç½‘ç»œé’“é±¼ï¼Ÿé¢å‘å­¦æœ¯ç¯å¢ƒåŠ¨æ€å¨èƒä¸å¤šæ¨¡æ€è¯„ä¼°çš„ç»¼åˆå®‰å…¨åŸºå‡†æµ‹è¯•å¥—ä»¶",
      "authors": [
        "Jingzhuo Zhou"
      ],
      "abstract": "The rapid proliferation of Multimodal Large Language Models (MLLMs) has introduced unprecedented security challenges, particularly in phishing detection within academic environments. Academic institutions and researchers are high-value targets, facing dynamic, multilingual, and context-dependent threats that leverage research backgrounds, academic collaborations, and personal information to craft highly tailored attacks. Existing security benchmarks largely rely on datasets that do not incorporate specific academic background information, making them inadequate for capturing the evolving attack patterns and human-centric vulnerability factors specific to academia. To address this gap, we present AdapT-Bench, a unified methodological framework and benchmark suite for systematically evaluating MLLM defense capabilities against dynamic phishing attacks in academic settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å­¦æœ¯ç¯å¢ƒä¸‹é¢ä¸´çš„ç½‘ç»œé’“é±¼æ£€æµ‹å®‰å…¨æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç§‘ç ”æœºæ„å’Œç ”ç©¶äººå‘˜å·²æˆä¸ºåŠ¨æ€ã€å¤šè¯­è¨€ä¸”é«˜åº¦å®šåˆ¶åŒ–æ”»å‡»çš„é«˜ä»·å€¼ç›®æ ‡ã€‚ç”±äºç°æœ‰çš„å®‰å…¨åŸºå‡†ç¼ºä¹ç‰¹å®šçš„å­¦æœ¯èƒŒæ™¯ä¿¡æ¯ï¼Œéš¾ä»¥æ•æ‰å­¦æœ¯ç•Œç‰¹æœ‰çš„æ”»å‡»æ¨¡å¼å’Œäººå› è„†å¼±æ€§ï¼Œè¯¥ç ”ç©¶æå‡ºäº† AdapT-Benchï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºç³»ç»Ÿè¯„ä¼° MLLMs å¯¹æŠ—å­¦æœ¯åœºæ™¯åŠ¨æ€ç½‘ç»œé’“é±¼æ”»å‡»çš„ç»Ÿä¸€æ¡†æ¶å’ŒåŸºå‡†å¥—ä»¶ã€‚é€šè¿‡å¤šæ¨¡æ€è¯„ä¼°ï¼Œè¯¥åŸºå‡†èƒ½å¤Ÿè¡¡é‡æ¨¡å‹åœ¨é¢å¯¹åˆ©ç”¨ç§‘ç ”èƒŒæ™¯å’Œå­¦æœ¯åˆä½œæ‰€æ„å»ºçš„å¤æ‚å¨èƒæ—¶çš„é˜²å¾¡èƒ½åŠ›ã€‚AdapT-Bench çš„æ¨å‡ºå¡«è¡¥äº†å­¦æœ¯é¢†åŸŸç‰¹å®šå®‰å…¨è¯„æµ‹çš„ç©ºç™½ï¼Œä¸ºå¢å¼º MLLMs åœ¨å¤æ‚ç§‘ç ”ç”Ÿæ€ä¸­çš„å®‰å…¨æ€§æä¾›äº†å…³é”®æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15165v2",
      "published_date": "2025-11-19 06:30:25 UTC",
      "updated_date": "2025-11-22 02:00:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:03:13.163690+00:00"
    },
    {
      "arxiv_id": "2511.15163v1",
      "title": "Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs",
      "title_zh": "å› ææ–½æ•™ï¼šåŸºäºç”»åƒã€è®°å¿†ä¸é—å¿˜æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–æ•°å­¦è¾…å¯¼",
      "authors": [
        "Yang Wu",
        "Rujing Yao",
        "Tong Zhang",
        "Yufei Shi",
        "Zhuoren Jiang",
        "Zhushan Li",
        "Xiaozhong Liu"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TASAï¼ˆTeaching According to Students' Aptitudeï¼‰ï¼Œä¸€ä¸ªé›†æˆå­¦ç”Ÿç”»åƒã€è®°å¿†ä¸é—å¿˜åŠ¨æ€çš„ä¸ªæ€§åŒ–æ•°å­¦è¾…å¯¼æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•æ‰å­¦ç”ŸçŸ¥è¯†åŠ¨æ€æ¼”è¿›å’Œé—å¿˜æ¨¡å¼æ–¹é¢çš„å±€é™æ€§ã€‚TASAé€šè¿‡ç»´æŠ¤ç»“æ„åŒ–çš„å­¦ç”Ÿç”»åƒ(Persona)å’Œäº¤äº’è®°å¿†(Memory)ï¼Œå¹¶ç»“åˆçŸ¥è¯†è¿½è¸ª(Knowledge Tracing)ä¸è¿ç»­é—å¿˜æ›²çº¿(Forgetting Curve)ï¼Œå®ç°å¯¹å­¦ç”ŸæŒæ¡çŠ¶æ€çš„å®æ—¶åŠ¨æ€æ›´æ–°ã€‚å‡­å€Ÿè¿™ç§å¤šç»´åº¦çš„å­¦ç”Ÿæ„ŸçŸ¥èƒ½åŠ›ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆè¯­å¢ƒç›¸å…³ä¸”ç»è¿‡éš¾åº¦æ ¡å‡†çš„é—®é¢˜ä¸è§£é‡Šï¼Œä¸ºå­¦ç”Ÿæä¾›ç²¾å‡†çš„æ•™å­¦æ”¯æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTASAåœ¨å­¦ä¹ æˆæ•ˆå’Œè¾…å¯¼é€‚åº”æ€§ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œå¼ºè°ƒäº†åœ¨åŸºäºLLMçš„æ•™å­¦ç³»ç»Ÿä¸­å»ºæ¨¡æ—¶é—´æ€§é—å¿˜è§„å¾‹å’Œå­¦ä¹ è€…ç‰¹å¾çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "AAAI 2026 Workshop",
      "pdf_url": "https://arxiv.org/pdf/2511.15163v1",
      "published_date": "2025-11-19 06:28:16 UTC",
      "updated_date": "2025-11-19 06:28:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:03:50.064623+00:00"
    },
    {
      "arxiv_id": "2511.15162v1",
      "title": "Multimodal Wireless Foundation Models",
      "title_zh": "å¤šæ¨¡æ€æ— çº¿åŸºç¡€æ¨¡å‹",
      "authors": [
        "Ahmed Aboulfotouh",
        "Hatem Abou-Zeid"
      ],
      "abstract": "Wireless foundation models (WFMs) have recently demonstrated promising capabilities, jointly performing multiple wireless functions and adapting effectively to new environments. However, while current WFMs process only one modality, depending on the task and operating conditions, the most informative modality changes and no single modality is best for all tasks. WFMs should therefore be designed to accept multiple modalities to enable a broader and more diverse range of tasks and scenarios. In this work, we propose and build the first multimodal wireless foundation model capable of processing both raw IQ streams and image-like wireless modalities (e.g., spectrograms and CSI) and performing multiple tasks across both. We introduce masked wireless modeling for the multimodal setting, a self-supervised objective and pretraining recipe that learns a joint representation from IQ streams and image-like wireless modalities. We evaluate the model on five tasks across both modality families: image-based (human activity sensing, RF signal classification, 5G NR positioning) and IQ-based (RF device fingerprinting, interference detection/classification). The multimodal WFM is competitive with single-modality WFMs, and in several cases surpasses their performance. Our results demonstrates the strong potential of developing multimodal WFMs that support diverse wireless tasks across different modalities. We believe this provides a concrete step toward both AI-native 6G and the vision of joint sensing, communication, and localization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºå¹¶æ„å»ºäº†é¦–ä¸ªå¤šæ¨¡æ€æ— çº¿åŸºç¡€æ¨¡å‹ (Multimodal Wireless Foundation Models)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ— çº¿åŸºç¡€æ¨¡å‹ä»…èƒ½å¤„ç†å•ä¸€æ¨¡æ€ä¸”æ— æ³•é€‚åº”å¤šæ ·åŒ–ä»»åŠ¡éœ€æ±‚çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹å…·å¤‡åŒæ—¶å¤„ç†åŸå§‹ IQ streams å’Œå›¾åƒç±»æ— çº¿æ¨¡æ€ï¼ˆå¦‚ spectrograms å’Œ CSIï¼‰çš„èƒ½åŠ›ï¼Œæ”¯æŒè·¨æ¨¡æ€æ‰§è¡Œå¤šç§æ— çº¿ä»»åŠ¡ã€‚ä½œè€…å¼•å…¥äº†é’ˆå¯¹å¤šæ¨¡æ€ç¯å¢ƒçš„æ©ç æ— çº¿å»ºæ¨¡ (masked wireless modeling)ï¼Œé€šè¿‡è‡ªç›‘ç£ç›®æ ‡å’Œé¢„è®­ç»ƒæ–¹æ¡ˆä»ä¸åŒæ¨¡æ€ä¸­å­¦ä¹ è”åˆè¡¨å¾ (joint representation)ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨äººä½“æ´»åŠ¨æ„ŸçŸ¥ã€å°„é¢‘ä¿¡å·åˆ†ç±»ã€5G NR positioningã€å°„é¢‘è®¾å¤‡æŒ‡çº¹è¯†åˆ«åŠå¹²æ‰°æ£€æµ‹ç­‰äº”é¡¹ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å¤šæ¨¡æ€æ¨¡å‹åœ¨å„é¡¹ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¸å•æ¨¡æ€æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å¤šæ¨¡æ€æ— çº¿åŸºç¡€æ¨¡å‹æ”¯æŒå¤šæ ·åŒ–æ— çº¿ä»»åŠ¡çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºå®ç° AI-native 6G ä»¥åŠæ„ŸçŸ¥ã€é€šä¿¡ã€å®šä½ä¸€ä½“åŒ–æ„¿æ™¯è¿ˆå‡ºäº†åšå®ä¸€æ­¥ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15162v1",
      "published_date": "2025-11-19 06:26:49 UTC",
      "updated_date": "2025-11-19 06:26:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:03:08.471206+00:00"
    },
    {
      "arxiv_id": "2511.15159v1",
      "title": "Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation",
      "title_zh": "è‡ªç„¶è¯­è¨€æ‰‹æœ¯åé¦ˆç”Ÿæˆï¼šä»ç»“æ„åŒ–è¡¨å¾åˆ°é¢†åŸŸé”šå®šçš„è¯„ä¼°",
      "authors": [
        "Firdavs Nasriddinov",
        "Rafal Kocielnik",
        "Anima Anandkumar",
        "Andrew J. Hung"
      ],
      "abstract": "High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰‹æœ¯åŸ¹è®­ä¸­è‡ªåŠ¨åŒ–è‡ªç„¶è¯­è¨€åé¦ˆçš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§ç»“æ„æ„ŸçŸ¥çš„æµæ°´çº¿ï¼Œæ—¨åœ¨å°†æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºå…·æœ‰ä¸´åºŠä¾æ®çš„å¯¼å¸ˆå¼åé¦ˆã€‚ç ”ç©¶è€…é€šè¿‡ä»çœŸå®æ‰‹æœ¯è½¬å½•æ–‡æœ¬ä¸­æå–â€œå™¨æ¢°-åŠ¨ä½œ-ç›®æ ‡â€(Instrument-Action-Target, IAT) ä¸‰å…ƒç»„ï¼Œå¹¶åˆ©ç”¨æ‰‹æœ¯ç¨‹åºä¸Šä¸‹æ–‡å’Œç»†ç²’åº¦çš„æ—¶é—´å™¨æ¢°è¿åŠ¨å¾®è°ƒäº† Video-to-IAT æ¨¡å‹ã€‚éšåï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿæˆçš„ IAT ä¸‰å…ƒç»„ä½œä¸ºç»“æ„åŒ–è¡¨ç¤ºï¼Œå¼•å¯¼ GPT-4o ç”Ÿæˆç¬¦åˆä¸´åºŠé€»è¾‘ä¸”å…·æœ‰å¯¼å¸ˆé£æ ¼çš„è‡ªç„¶è¯­è¨€åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ Video-to-IAT è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œä¸Šä¸‹æ–‡æ³¨å…¥å’Œæ—¶é—´è·Ÿè¸ªæ˜¾è‘—æå‡äº†å„ç»´åº¦çš„ AUC æŒ‡æ ‡ã€‚åœ¨åé¦ˆæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒåŸºäº IAT æ¡ä»¶åŒ–çš„ç”Ÿæˆå¿ å®åº¦è¯„åˆ†æ¯”çº¯è§†é¢‘ç”Ÿæˆæé«˜äº† 12.4%ï¼Œå¹¶å°†å¯æ¥å—çš„åé¦ˆç”Ÿæˆæ¯”ä¾‹ä» 21% æå‡è‡³ 42%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ Word Error Rate å’Œ ROUGE ç­‰æ–‡æœ¬åº¦é‡æŒ‡æ ‡ä¸Šä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚é€šè¿‡å°†ç”Ÿæˆè¿‡ç¨‹é”šå®šåœ¨æ˜¾å¼çš„ IAT ç»“æ„ä¸­ï¼Œè¯¥ç ”ç©¶ä¸ä»…æé«˜äº†åé¦ˆçš„å‡†ç¡®æ€§ï¼Œè¿˜æä¾›äº†ä¸´åºŠå¯éªŒè¯çš„ä¾æ®ï¼Œä¸ºå¯å®¡è®¡çš„æ‰‹æœ¯è¾…åŠ©åŸ¹è®­å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted as proceedings paper for ML4H 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.15159v1",
      "published_date": "2025-11-19 06:19:34 UTC",
      "updated_date": "2025-11-19 06:19:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:04:24.168938+00:00"
    },
    {
      "arxiv_id": "2511.15151v1",
      "title": "DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging",
      "title_zh": "DCL-SEï¼šè„‘æˆåƒæ—¶ç©ºç¼–ç çš„åŠ¨æ€è¯¾ç¨‹å­¦ä¹ ",
      "authors": [
        "Meihua Zhou",
        "Xinyu Tong",
        "Jiarui Zhao",
        "Min Cheng",
        "Li Yang",
        "Lei Tian",
        "Nan Wan"
      ],
      "abstract": "High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DCL-SEï¼ˆDynamic Curriculum Learning for Spatiotemporal Encodingï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³é«˜ç»´ç¥ç»å½±åƒåˆ†æä¸­æ—¶ç©ºä¿çœŸåº¦ä¸è¶³åŠé€šç”¨å¤§æ¨¡å‹é€‚åº”æ€§æœ‰é™ç­‰æŒ‘æˆ˜çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ•°æ®é©±åŠ¨çš„æ—¶ç©ºç¼–ç ï¼ˆDaSEï¼‰æŠ€æœ¯ï¼Œé€šè¿‡è¿‘ä¼¼ç§©æ± åŒ–ï¼ˆARPï¼‰å°†ä¸‰ç»´ä½“ç§¯è„‘æ•°æ®é«˜æ•ˆè½¬æ¢ä¸ºä¿¡æ¯ä¸°å¯Œçš„äºŒç»´åŠ¨æ€è¡¨å¾ã€‚ç ”ç©¶è¿›ä¸€æ­¥é‡‡ç”¨äº†ç”±åŠ¨æ€åˆ†ç»„æœºåˆ¶ï¼ˆDGMï¼‰å¼•å¯¼çš„åŠ¨æ€è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä½¿è§£ç å™¨èƒ½å¤Ÿå®ç°ä»å…¨å±€è§£å‰–ç»“æ„åˆ°ç²¾ç»†ç—…ç†ç»†èŠ‚çš„æ¸è¿›å¼ç‰¹å¾æå–ã€‚åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†ç±»ã€è„‘è‚¿ç˜¤è¯†åˆ«åŠè„‘å¹´é¾„é¢„æµ‹ç­‰å…­ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒDCL-SE åœ¨å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶ç»“è®ºå¼ºè°ƒäº†åœ¨è¿½æ±‚å¤§è§„æ¨¡é¢„è®­ç»ƒç½‘ç»œçš„è¶‹åŠ¿ä¸‹ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡å¼€å‘ç´§å‡‘å‹æ¶æ„å¯¹äºä¸´åºŠç¥ç»å½±åƒè¯Šæ–­çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15151v1",
      "published_date": "2025-11-19 06:10:31 UTC",
      "updated_date": "2025-11-19 06:10:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:03:33.773873+00:00"
    },
    {
      "arxiv_id": "2511.15755v2",
      "title": "Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response",
      "title_zh": "å¤šæ™ºèƒ½ä½“ LLM ç¼–æ’å®ç°æ•…éšœå“åº”çš„ç¡®å®šæ€§ã€é«˜è´¨é‡å†³ç­–æ”¯æŒ",
      "authors": [
        "Philip Drammeh"
      ],
      "abstract": "Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MyAntFarm.aiï¼Œè¿™æ˜¯ä¸€ä¸ªå¯é‡å¤çš„å®¹å™¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è¯æ˜å¤šæ™ºèƒ½ä½“ç¼–æ’(Multi-Agent Orchestration)èƒ½ä»æ ¹æœ¬ä¸Šæ”¹å˜åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ•…éšœå“åº”(Incident Response)è´¨é‡ã€‚é’ˆå¯¹å•æ™ºèƒ½ä½“æ–¹æ³•ç”Ÿæˆçš„å»ºè®®å¾€å¾€æ¨¡ç³Šä¸”ä¸å¯ç”¨çš„é—®é¢˜ï¼Œç ”ç©¶é€šè¿‡348æ¬¡å—æ§å®éªŒå¯¹æ¯”äº†å•æ™ºèƒ½ä½“ä¸å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿå®ç°äº†100%çš„å¯æ‰§è¡Œå»ºè®®ç‡ï¼Œåœ¨è¡ŒåŠ¨ç‰¹å¼‚æ€§(Action Specificity)å’Œæ–¹æ¡ˆæ­£ç¡®æ€§(Solution Correctness)ä¸Šåˆ†åˆ«æå‡äº†80å€å’Œ140å€ã€‚å…³é”®åœ¨äºï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ‰€æœ‰è¯•éªŒä¸­è¡¨ç°å‡ºé›¶è´¨é‡æ–¹å·®(Zero Quality Variance)ï¼Œèƒ½å¤Ÿæä¾›å•æ™ºèƒ½ä½“æ— æ³•å®ç°çš„ç”Ÿäº§çº§æœåŠ¡æ°´å¹³åè®®(SLA)æ‰¿è¯ºã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å†³ç­–è´¨é‡(Decision Quality, DQ)è¿™ä¸€æ¶µç›–æœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§å’Œæ­£ç¡®æ€§çš„æ–°æŒ‡æ ‡ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°æ‰‹æ®µçš„ç©ºç™½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“ç¼–æ’å·²ä»å•çº¯çš„æ€§èƒ½ä¼˜åŒ–è½¬å˜ä¸ºåŸºäºLLMçš„æ•…éšœå“åº”è¾¾åˆ°ç”Ÿäº§å°±ç»ª(Production-Readiness)è¦æ±‚çš„æ ¸å¿ƒå‰æã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 4 tables. v2: Expanded limitations, added threats to validity, clarified agent definition, added reproducibility notes, updated Phase 2 timeline with current models (GPT-5.2, Claude Sonnet 4.5, Llama 3.3 70B). No changes to experimental results",
      "pdf_url": "https://arxiv.org/pdf/2511.15755v2",
      "published_date": "2025-11-19 06:06:11 UTC",
      "updated_date": "2026-01-07 04:55:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:05:09.466614+00:00"
    },
    {
      "arxiv_id": "2511.15141v1",
      "title": "ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation",
      "title_zh": "ItemRAGï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨èçš„åŸºäºç‰©å“çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Sunwoo Kim",
        "Geon Lee",
        "Kyungho Kim",
        "Jaemin Yoo",
        "Kijung Shin"
      ],
      "abstract": "Recently, large language models (LLMs) have been widely used as recommender systems, owing to their strong reasoning capability and their effectiveness in handling cold-start items. To better adapt LLMs for recommendation, retrieval-augmented generation (RAG) has been incorporated. Most existing RAG methods are user-based, retrieving purchase patterns of users similar to the target user and providing them to the LLM. In this work, we propose ItemRAG, an item-based RAG method for LLM-based recommendation that retrieves relevant items (rather than users) from item-item co-purchase histories. ItemRAG helps LLMs capture co-purchase patterns among items, which are beneficial for recommendations. Especially, our retrieval strategy incorporates semantically similar items to better handle cold-start items and uses co-purchase frequencies to improve the relevance of the retrieved items. Through extensive experiments, we demonstrate that ItemRAG consistently (1) improves the zero-shot LLM-based recommender by up to 43% in Hit-Ratio-1 and (2) outperforms user-based RAG baselines under both standard and cold-start item recommendation settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ItemRAGï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ¨èç³»ç»Ÿè®¾è®¡çš„åŸºäºé¡¹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ (Item-Based Retrieval-Augmented Generation) æ–¹æ³•ã€‚åŒºåˆ«äºä»¥å¾€ä¾§é‡äºæ£€ç´¢ç›¸ä¼¼ç”¨æˆ·è´­ä¹°æ¨¡å¼çš„ user-based RAGï¼ŒItemRAG ä¸“æ³¨äºä»é¡¹ä¸é¡¹çš„å…±åŒè´­ä¹°å†å² (item-item co-purchase histories) ä¸­æå–ç›¸å…³é¡¹ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•æ‰é¡¹ä¹‹é—´çš„å…±åŒè´­ä¹°æ¨¡å¼ (co-purchase patterns) æ¥å¢å¼º LLMs çš„æ¨èèƒ½åŠ›ï¼Œå¹¶ç»“åˆè¯­ä¹‰ç›¸ä¼¼æ€§æ¥æœ‰æ•ˆåº”å¯¹å†·å¯åŠ¨ (cold-start) æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒItemRAG åˆ©ç”¨å…±åŒè´­ä¹°é¢‘ç‡ (co-purchase frequencies) è¿›ä¸€æ­¥ä¼˜åŒ–äº†æ£€ç´¢å†…å®¹çš„ç›¸å…³æ€§ã€‚å®éªŒæ•°æ®è¯æ˜ï¼ŒItemRAG åœ¨ Hit-Ratio-1 æŒ‡æ ‡ä¸Šä½¿é›¶æ ·æœ¬ (zero-shot) LLM æ¨èå™¨çš„æ€§èƒ½æå‡äº†å¤šè¾¾ 43%ã€‚è¯¥æ–¹æ³•åœ¨æ ‡å‡†æ¨èå’Œå†·å¯åŠ¨åœºæ™¯ä¸‹å‡å±•ç°å‡ºä¼˜äºç°æœ‰ user-based RAG åŸºå‡†æ¨¡å‹çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15141v1",
      "published_date": "2025-11-19 05:39:14 UTC",
      "updated_date": "2025-11-19 05:39:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:04:13.850751+00:00"
    },
    {
      "arxiv_id": "2511.15139v1",
      "title": "CASPER: Cross-modal Alignment of Spatial and single-cell Profiles for Expression Recovery",
      "title_zh": "CASPERï¼šç”¨äºè¡¨è¾¾æ¢å¤çš„ç©ºé—´ä¸å•ç»†èƒå›¾è°±è·¨æ¨¡æ€å¯¹é½",
      "authors": [
        "Amit Kumar",
        "Maninder Kaur",
        "Raghvendra Mall",
        "Sukrit Gupta"
      ],
      "abstract": "Spatial Transcriptomics enables mapping of gene expression within its native tissue context, but current platforms measure only a limited set of genes due to experimental constraints and excessive costs. To overcome this, computational models integrate Single-Cell RNA Sequencing data with Spatial Transcriptomics to predict unmeasured genes. We propose CASPER, a cross-attention based framework that predicts unmeasured gene expression in Spatial Transcriptomics by leveraging centroid-level representations from Single-Cell RNA Sequencing. We performed rigorous testing over four state-of-the-art Spatial Transcriptomics/Single-Cell RNA Sequencing dataset pairs across four existing baseline models. CASPER shows significant improvement in nine out of the twelve metrics for our experiments. This work paves the way for further work in Spatial Transcriptomics to Single-Cell RNA Sequencing modality translation. The code for CASPER is available at https://github.com/AI4Med-Lab/CASPER.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CASPERï¼Œä¸€ç§åŸºäºäº¤å‰æ³¨æ„åŠ›(cross-attention)æœºåˆ¶çš„è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå•ç»†èƒRNAæµ‹åº(Single-Cell RNA Sequencing)æ•°æ®æ¥æ¢å¤ç©ºé—´è½¬å½•ç»„(Spatial Transcriptomics)ä¸­æœªæµ‹é‡çš„åŸºå› è¡¨è¾¾ä¿¡æ¯ã€‚é’ˆå¯¹ç©ºé—´è½¬å½•ç»„æŠ€æœ¯å—æˆæœ¬å’Œå®éªŒæ¡ä»¶é™åˆ¶ä»…èƒ½è¦†ç›–æœ‰é™åŸºå› çš„é—®é¢˜ï¼ŒCASPERåˆ©ç”¨å•ç»†èƒæ•°æ®çš„è´¨å¿ƒçº§åˆ«è¡¨ç¤º(centroid-level representations)è¿›è¡Œè·¨æ¨¡æ€å¯¹é½ä¸é¢„æµ‹ã€‚åœ¨å››ç»„åŸºå‡†æ•°æ®é›†å¯¹ä¸Šçš„æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒCASPERåœ¨12é¡¹è¯„ä»·æŒ‡æ ‡ä¸­çš„9é¡¹ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å››ç§åŸºçº¿æ¨¡å‹ã€‚è¯¥æˆæœä¸ä»…æå‡äº†åŸºå› è¡¨è¾¾æ¢å¤çš„å‡†ç¡®æ€§ï¼Œä¹Ÿä¸ºç©ºé—´è½¬å½•ç»„ä¸å•ç»†èƒæµ‹åºä¹‹é—´çš„æ¨¡æ€è½¬æ¢ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚ç›®å‰ï¼Œè¯¥é¡¹ç›®çš„æºä»£ç å·²åœ¨GitHubå¹³å°å¼€æºå‘å¸ƒã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15139v1",
      "published_date": "2025-11-19 05:36:05 UTC",
      "updated_date": "2025-11-19 05:36:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:04:16.872652+00:00"
    },
    {
      "arxiv_id": "2511.15137v1",
      "title": "From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs",
      "title_zh": "ä»æ±‚è§£åˆ°éªŒè¯ï¼šå¤§è¯­è¨€æ¨¡å‹é²æ£’æ¨ç†çš„ç»Ÿä¸€ç›®æ ‡",
      "authors": [
        "Xiaoxuan Wang",
        "Bo Liu",
        "Song Jiang",
        "Jingzhou Liu",
        "Jingyuan Qi",
        "Xia Chen",
        "Baosheng He"
      ],
      "abstract": "The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªæˆ‘éªŒè¯èƒ½åŠ›åŠå…¶å¯¹æ¨ç†æ€§èƒ½çš„å½±å“ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹éš¾ä»¥ä¸€è‡´æ€§éªŒè¯è‡ªèº«æ¨ç†è¿½è¸ªçš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†GRPO-Verifç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„æŸå¤±å‡½æ•°ï¼ˆunified loss functionï¼‰å°†è§£æ³•ç”Ÿæˆï¼ˆsolution generationï¼‰ä¸è‡ªæˆ‘éªŒè¯ï¼ˆself-verificationï¼‰çš„ä¼˜åŒ–è¿‡ç¨‹ç»“åˆèµ·æ¥ã€‚æ¡†æ¶ä¸­å¼•å…¥äº†å¯è°ƒèŠ‚çš„è¶…å‚æ•°ï¼Œç”¨äºç²¾ç¡®æ§åˆ¶éªŒè¯ä¿¡å·åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„æƒé‡ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒGRPO-Verifåœ¨ä¿æŒæ¨¡å‹åŸæœ‰æ¨ç†æ€§èƒ½æ°´å¹³çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†LLMsçš„è‡ªæˆ‘éªŒè¯èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°æ›´ç¨³å¥çš„LLMæ¨ç†æä¾›äº†ä¸€ç§ç»Ÿä¸€çš„ä¼˜åŒ–ç›®æ ‡å’Œæœ‰æ•ˆçš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15137v1",
      "published_date": "2025-11-19 05:27:06 UTC",
      "updated_date": "2025-11-19 05:27:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:05:22.870630+00:00"
    },
    {
      "arxiv_id": "2511.15752v1",
      "title": "Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä¸æ™ºèƒ½ä½“æ„å»º AI åŠ©æ‰‹ï¼Œä»¥æå‡ç”Ÿç‰©åŠ›å­¦å·¥ç¨‹æ•™è‚²",
      "authors": [
        "Hanzhi Yan",
        "Qin Lu",
        "Xianqiao Wang",
        "Xiaoming Zhai",
        "Tianming Liu",
        "He Li"
      ],
      "abstract": "While large language models (LLMs) have demonstrated remarkable versatility across a wide range of general tasks, their effectiveness often diminishes in domain-specific applications due to inherent knowledge gaps. Moreover, their performance typically declines when addressing complex problems that require multi-step reasoning and analysis. In response to these challenges, we propose leveraging both LLMs and AI agents to develop education assistants aimed at enhancing undergraduate learning in biomechanics courses that focus on analyzing the force and moment in the musculoskeletal system of the human body. To achieve our goal, we construct a dual-module framework to enhance LLM performance in biomechanics educational tasks: 1) we apply Retrieval-Augmented Generation (RAG) to improve the specificity and logical consistency of LLM's responses to the conceptual true/false questions; 2) we build a Multi-Agent System (MAS) to solve calculation-oriented problems involving multi-step reasoning and code execution. Specifically, we evaluate the performance of several LLMs, i.e., Qwen-1.0-32B, Qwen-2.5-32B, and Llama-70B, on a biomechanics dataset comprising 100 true/false conceptual questions and problems requiring equation derivation and calculation. Our results demonstrate that RAG significantly enhances the performance and stability of LLMs in answering conceptual questions, surpassing those of vanilla models. On the other hand, the MAS constructed using multiple LLMs demonstrates its ability to perform multi-step reasoning, derive equations, execute code, and generate explainable solutions for tasks that require calculation. These findings demonstrate the potential of applying RAG and MAS to enhance LLM performance for specialized courses in engineering curricula, providing a promising direction for developing intelligent tutoring in engineering education.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å’ŒAIæ™ºèƒ½ä½“(AI agents)å¢å¼ºç”Ÿç‰©åŠ›å­¦(Biomechanics)å·¥ç¨‹æ•™è‚²çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³LLMsåœ¨ä¸“ä¸šé¢†åŸŸçŸ¥è¯†åŒ®ä¹åŠå¤šæ­¥æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªåŒæ¨¡å—æ¡†æ¶ï¼Œå…¶ä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ¨¡å—ç”¨äºä¼˜åŒ–æ¨¡å‹å¯¹æ¦‚å¿µæ€§é—®é¢˜çš„å“åº”å‡†ç¡®åº¦å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œè€Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)æ¨¡å—åˆ™è´Ÿè´£å¤„ç†æ¶‰åŠå…¬å¼æ¨å¯¼ã€å¤šæ­¥æ¨ç†å’Œä»£ç æ‰§è¡Œçš„å¤æ‚è®¡ç®—ä»»åŠ¡ã€‚é€šè¿‡åœ¨åŒ…å«100é“é¢˜ç›®çš„æ•°æ®é›†ä¸Šå¯¹Qwenå’ŒLlamaç³»åˆ—æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºRAGæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ¦‚å¿µæ€§é—®é¢˜ä¸Šçš„è¡¨ç°å’Œç¨³å®šæ€§ã€‚åŒæ—¶ï¼ŒMASè¯æ˜äº†å…¶åœ¨æ‰§è¡Œå¤šæ­¥æ¨ç†å¹¶ç”Ÿæˆå¯è§£é‡Šè®¡ç®—æ–¹æ¡ˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ç»“æœéªŒè¯äº†ç»“åˆRAGä¸MASåœ¨å¼€å‘å·¥ç¨‹æ•™è‚²æ™ºèƒ½è¾…åŠ©æ•™å­¦ç³»ç»ŸåŠä¸“ä¸šè¯¾ç¨‹è¾…åŠ©ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15752v1",
      "published_date": "2025-11-19 05:16:51 UTC",
      "updated_date": "2025-11-19 05:16:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:05:34.868650+00:00"
    },
    {
      "arxiv_id": "2511.15122v2",
      "title": "Multi-Aspect Cross-modal Quantization for Generative Recommendation",
      "title_zh": "é¢å‘ç”Ÿæˆå¼æ¨èçš„å¤šç»´åº¦è·¨æ¨¡æ€é‡åŒ–",
      "authors": [
        "Fuwei Zhang",
        "Xiaoyu Liu",
        "Dongbo Xi",
        "Jishen Yin",
        "Huan Chen",
        "Peng Yan",
        "Fuzhen Zhuang",
        "Zhao Zhang"
      ],
      "abstract": "Generative Recommendation (GR) has emerged as a new paradigm in recommender systems. This approach relies on quantized representations to discretize item features, modeling users' historical interactions as sequences of discrete tokens. Based on these tokenized sequences, GR predicts the next item by employing next-token prediction methods. The challenges of GR lie in constructing high-quality semantic identifiers (IDs) that are hierarchically organized, minimally conflicting, and conducive to effective generative model training. However, current approaches remain limited in their ability to harness multimodal information and to capture the deep and intricate interactions among diverse modalities, both of which are essential for learning high-quality semantic IDs and for effectively training GR models. To address this, we propose Multi-Aspect Cross-modal quantization for generative Recommendation (MACRec), which introduces multimodal information and incorporates it into both semantic ID learning and generative model training from different aspects. Specifically, we first introduce cross-modal quantization during the ID learning process, which effectively reduces conflict rates and thus improves codebook usability through the complementary integration of multimodal information. In addition, to further enhance the generative ability of our GR model, we incorporate multi-aspect cross-modal alignments, including the implicit and explicit alignments. Finally, we conduct extensive experiments on three well-known recommendation datasets to demonstrate the effectiveness of our proposed method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼æ¨è(Generative Recommendation)åœ¨æ„å»ºé«˜è´¨é‡è¯­ä¹‰æ ‡è¯†ç¬¦(semantic IDs)æ—¶éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯åŠæ•æ‰æ·±å±‚è·¨æ¨¡æ€äº¤äº’çš„é—®é¢˜ï¼Œæå‡ºäº†å¤šæ–¹é¢è·¨æ¨¡æ€é‡åŒ–æ¨èæ¡†æ¶ MACRecã€‚è¯¥æ¡†æ¶åœ¨æ ‡è¯†ç¬¦å­¦ä¹ è¿‡ç¨‹ä¸­å¼•å…¥äº†è·¨æ¨¡æ€é‡åŒ–(cross-modal quantization)ï¼Œé€šè¿‡å¤šæ¨¡æ€ä¿¡æ¯çš„äº’è¡¥æ•´åˆæœ‰æ•ˆé™ä½äº†å†²çªç‡å¹¶æå‡äº†ç æœ¬(codebook)çš„å¯ç”¨æ€§ã€‚æ­¤å¤–ï¼ŒMACRec è¿˜ç»“åˆäº†åŒ…å«éšå¼å’Œæ˜¾å¼å¯¹é½çš„å¤šæ–¹é¢è·¨æ¨¡æ€å¯¹é½(multi-aspect cross-modal alignments)æœºåˆ¶ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªçŸ¥åæ¨èæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡ç”Ÿæˆå¼æ¨èç³»ç»Ÿçš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by AAAI 2026 (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2511.15122v2",
      "published_date": "2025-11-19 04:55:14 UTC",
      "updated_date": "2025-11-22 06:07:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:05:50.471054+00:00"
    },
    {
      "arxiv_id": "2511.15120v1",
      "title": "Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit",
      "title_zh": "ç¥ç»ç½‘ç»œåœ¨æ¥è¿‘ä¿¡æ¯è®ºæé™ä¸‹å­¦ä¹ é€šç”¨å¤šæŒ‡æ•°æ¨¡å‹",
      "authors": [
        "Bohan Zhang",
        "Zihao Wang",
        "Hengyu Fu",
        "Jason D. Lee"
      ],
      "abstract": "In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\\boldsymbol{x})=g(\\boldsymbol{U}\\boldsymbol{x})$ with hidden subspace $\\boldsymbol{U}\\in \\mathbb{R}^{r\\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\\widetilde{\\mathcal{O}}(d)$ samples and $\\widetilde{\\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¥ç»ç½‘ç»œå¦‚ä½•é«˜æ•ˆå­¦ä¹ é«˜ç»´ç‰¹å¾ï¼Œé‡ç‚¹ç ”ç©¶äº†é«˜æ–¯å¤šæŒ‡æ•°æ¨¡å‹(Gaussian Multi-index model)åœ¨æ¢¯åº¦ä¸‹é™(Gradient Descent)ä¸‹çš„å­¦ä¹ æœºåˆ¶ã€‚ç ”ç©¶è¯æ˜ï¼Œåœ¨é“¾æ¥å‡½æ•°çš„é€šç”¨éé€€åŒ–å‡è®¾ä¸‹ï¼Œé€šè¿‡é€å±‚æ¢¯åº¦ä¸‹é™(Layer-wise Gradient Descent)è®­ç»ƒçš„æ ‡å‡†ä¸¤å±‚ç¥ç»ç½‘ç»œä»…éœ€ $\\widetilde{\\mathcal{O}}(d)$ çš„æ ·æœ¬å’Œ $\\widetilde{\\mathcal{O}}(d^2)$ çš„æ—¶é—´å³å¯å®ç°è¿‘ä¹é›¶çš„æµ‹è¯•è¯¯å·®ã€‚è¿™ä¸€å¤æ‚åº¦åœ¨é¢†å…ˆé˜¶æ•°ä¸Šä¸ä¿¡æ¯è®ºæé™(Information-theoretic limit)ä¸€è‡´ï¼Œè¯æ˜äº†å…¶ç®—æ³•çš„æœ€ä¼˜æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºï¼Œåœ¨æ¢¯åº¦ä¸‹é™åˆæœŸï¼Œå†…éƒ¨æƒé‡é€šè¿‡å¹‚è¿­ä»£(Power-iteration)è¿‡ç¨‹éšå¼æ¨¡æ‹Ÿäº†éšè—å­ç©ºé—´çš„è°±å¯åŠ¨(Spectral start)ï¼Œå¹¶æŒ‡å‡ºç¬¬ä¸€å±‚å¿…é¡»è®­ç»ƒè¶…è¿‡ $\\mathcal{O}(1)$ æ­¥æ‰èƒ½è¾¾åˆ°æœ€ä¼˜æ•ˆæœã€‚è¯¥å·¥ä½œä¸ä»…å±•ç¤ºäº†ç¥ç»ç½‘ç»œåœ¨å­¦ä¹ å±‚æ¬¡åŒ–å‡½æ•°(Hierarchical functions)æ—¶çš„é«˜æ•ˆæ€§ï¼Œä¹Ÿä¸ºè¡¨å¾å­¦ä¹ (Representation learning)æä¾›äº†æ·±åˆ»çš„ç†è®ºè§è§£ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "stat.ML",
      "comment": "86 pages, 2 figures. The order of the first two authors was determined by a coin flip",
      "pdf_url": "https://arxiv.org/pdf/2511.15120v1",
      "published_date": "2025-11-19 04:46:47 UTC",
      "updated_date": "2025-11-19 04:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:06:06.568530+00:00"
    },
    {
      "arxiv_id": "2511.15112v1",
      "title": "Semiconductor Industry Trend Prediction with Event Intervention Based on LSTM Model in Sentiment-Enhanced Time Series Data",
      "title_zh": "åŸºäºæƒ…æ„Ÿå¢å¼ºæ—¶é—´åºåˆ—æ•°æ®ä¸ LSTM æ¨¡å‹çš„äº‹ä»¶å¹²é¢„ä¸‹åŠå¯¼ä½“è¡Œä¸šè¶‹åŠ¿é¢„æµ‹",
      "authors": [
        "Wei-hsiang Yen",
        "Lyn Chao-ling Chen"
      ],
      "abstract": "The innovation of the study is that the deep learning method and sentiment analysis are integrated in traditional business model analysis and forecasting, and the research subject is TSMC for industry trend prediction of semiconductor industry in Taiwan. For the rapid market changes and development of wafer technologies of semiconductor industry, traditional data analysis methods not perform well in the high variety and time series data. Textual data and time series data were collected from seasonal reports of TSMC including financial information. Textual data through sentiment analysis by considering the event intervention both from internal events of the company and the external global events. Using the sentiment-enhanced time series data, the LSTM model was adopted for predicting industry trend of TSMC. The prediction results reveal significant development of wafer technology of TSMC and the potential threatens in the global market, and matches the product released news of TSMC and the international news. The contribution of the work performed accurately in industry trend prediction of the semiconductor industry by considering both the internal and external event intervention, and the prediction results provide valuable information of semiconductor industry both in research and business aspects.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ·±åº¦å­¦ä¹ ä¸ Sentiment Analysis çš„åˆ†æé¢„æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡å‹åœ¨åŠå¯¼ä½“è¡Œä¸šé«˜æ³¢åŠ¨æ€§ Time Series æ•°æ®ä¸Šçš„è¡¨ç°å±€é™ï¼Œå¹¶ä»¥ TSMC ä¸ºç ”ç©¶å¯¹è±¡è¿›è¡Œè¶‹åŠ¿é¢„æµ‹ã€‚ç ”ç©¶é€šè¿‡åˆ†æ TSMC å­£åº¦æŠ¥å‘Šä¸­çš„æ–‡æœ¬æ•°æ®ï¼Œå……åˆ†è€ƒè™‘äº†ä¼ä¸šå†…éƒ¨åŠå…¨çƒå¤–éƒ¨äº‹ä»¶çš„ Event Intervention å¯¹å¸‚åœºçš„å½±å“ã€‚åˆ©ç”¨èå…¥æƒ…æ„Ÿç‰¹å¾çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œç ”ç©¶é‡‡ç”¨ LSTM æ¨¡å‹å¯¹è¡Œä¸šå‘å±•åŠ¨å‘è¿›è¡Œå»ºæ¨¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹å‡†ç¡®é¢„æµ‹äº† TSMC åœ¨ Wafer Technology é¢†åŸŸçš„å…³é”®è¿›å±•ä»¥åŠå…¨çƒå¸‚åœºé¢ä¸´çš„æ½œåœ¨å¨èƒï¼Œé¢„æµ‹ç»“è®ºä¸å®é™…äº§å“å‘å¸ƒåŠå›½é™…æ–°é—»é«˜åº¦å»åˆã€‚è¯¥ç ”ç©¶é€šè¿‡æ•´åˆå†…å¤–äº‹ä»¶å¹²é¢„æå‡äº†åŠå¯¼ä½“è¶‹åŠ¿é¢„æµ‹çš„å‡†ç¡®åº¦ï¼Œä¸ºç ”ç©¶é¢†åŸŸå’Œå•†ä¸šå†³ç­–æä¾›äº†å…·å¤‡å®è¯æ”¯æŒçš„å‚è€ƒä¿¡æ¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in Taiwan Academic Network Conference (TANET 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.15112v1",
      "published_date": "2025-11-19 04:36:02 UTC",
      "updated_date": "2025-11-19 04:36:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:05:33.368583+00:00"
    },
    {
      "arxiv_id": "2511.15110v1",
      "title": "Eye Care You: Voice Guidance Application Using Social Robot for Visually Impaired People",
      "title_zh": "Eye Care Youï¼šé¢å‘è§†éšœäººå£«çš„ç¤¾äº¤æœºå™¨äººè¯­éŸ³å¼•å¯¼åº”ç”¨",
      "authors": [
        "Ting-An Lin",
        "Pei-Lin Tsai",
        "Yi-An Chen",
        "Feng-Yu Chen",
        "Lyn Chao-ling Chen"
      ],
      "abstract": "In the study, the device of social robot was designed for visually impaired users, and along with a mobile application for provide functions to assist their lives. Both physical and mental conditions of visually impaired users are considered, and the mobile application provides functions: photo record, mood lift, greeting guest and today highlight. The application was designed for visually impaired users, and uses voice control to provide a friendly interface. Photo record function allows visually impaired users to capture image immediately when they encounter danger situations. Mood lift function accompanies visually impaired users by asking questions, playing music and reading articles. Greeting guest function answers to the visitors for the inconvenient physical condition of visually impaired users. In addition, today highlight function read news including weather forecast, daily horoscopes and daily reminder for visually impaired users. Multiple tools were adopted for developing the mobile application, and a website was developed for caregivers to check statues of visually impaired users and for marketing of the application.",
      "tldr_zh": "æœ¬ç ”ç©¶è®¾è®¡å¹¶å¼€å‘äº†åä¸º \"Eye Care You\" çš„ç¤¾äº¤æœºå™¨äºº (social robot) åŠå…¶é…å¥—è¯­éŸ³å¼•å¯¼åº”ç”¨ç¨‹åºï¼Œæ—¨åœ¨ä»ç”Ÿç†å’Œå¿ƒç†åŒé‡ç»´åº¦è¾…åŠ©è§†éšœäººå£«çš„æ—¥å¸¸ç”Ÿæ´»ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è¯­éŸ³æ§åˆ¶ (voice control) æä¾›å‹å¥½äº¤äº’ç•Œé¢ï¼Œé›†æˆäº†å›¾åƒè®°å½• (photo record)ã€æƒ…ç»ªæå‡ (mood lift)ã€è®¿å®¢æ¥å¾… (greeting guest) ä»¥åŠä»Šæ—¥äº®ç‚¹ (today highlight) å››å¤§æ ¸å¿ƒåŠŸèƒ½ã€‚å›¾åƒè®°å½•åŠŸèƒ½å…è®¸ç”¨æˆ·åœ¨å±é™©æƒ…å¢ƒä¸‹å³æ—¶æŠ“æ‹ï¼Œæƒ…ç»ªæå‡åŠŸèƒ½åˆ™é€šè¿‡éŸ³ä¹æ’­æ”¾ä¸æ–‡ç« æœ—è¯»æä¾›æƒ…æ„Ÿé™ªä¼´ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿèƒ½ååŠ©è¡ŒåŠ¨ä¸ä¾¿çš„ç”¨æˆ·å›åº”è®¿å®¢ï¼Œå¹¶æ’­æŠ¥å¤©æ°”é¢„æŠ¥ä¸æ¯æ—¥æé†’ç­‰å®ç”¨èµ„è®¯ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜ä¸ºæŠ¤ç†äººå‘˜å¼€å‘äº†ä¸“é—¨çš„ç›‘æ§ç½‘ç«™ï¼Œé€šè¿‡è½¯ç¡¬ä»¶ç»“åˆçš„å®Œæ•´æ¡†æ¶æœ‰æ•ˆæå‡äº†è§†éšœç¾¤ä½“çš„ç”Ÿæ´»ç‹¬ç«‹æ€§ä¸å®‰å…¨ä¿éšœã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted in the 35th IPPR Conference on Computer Vision, Graphics, and Image Processing (CVGIP2022)",
      "pdf_url": "https://arxiv.org/pdf/2511.15110v1",
      "published_date": "2025-11-19 04:34:54 UTC",
      "updated_date": "2025-11-19 04:34:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:05:38.579924+00:00"
    },
    {
      "arxiv_id": "2511.15107v1",
      "title": "Effective Code Membership Inference for Code Completion Models via Adversarial Prompts",
      "title_zh": "åŸºäºå¯¹æŠ—æ€§æç¤ºçš„ä»£ç è¡¥å…¨æ¨¡å‹æœ‰æ•ˆä»£ç æˆå‘˜æ¨ç†",
      "authors": [
        "Yuan Jiang",
        "Zehao Li",
        "Shan Huang",
        "Christoph Treude",
        "Xiaohong Su",
        "Tiantian Wang"
      ],
      "abstract": "Membership inference attacks (MIAs) on code completion models offer an effective way to assess privacy risks by inferring whether a given code snippet was part of the training data. Existing black- and gray-box MIAs rely on expensive surrogate models or manually crafted heuristic rules, which limit their ability to capture the nuanced memorization patterns exhibited by over-parameterized code language models. To address these challenges, we propose AdvPrompt-MIA, a method specifically designed for code completion models, combining code-specific adversarial perturbations with deep learning. The core novelty of our method lies in designing a series of adversarial prompts that induce variations in the victim code model's output. By comparing these outputs with the ground-truth completion, we construct feature vectors to train a classifier that automatically distinguishes member from non-member samples. This design allows our method to capture richer memorization patterns and accurately infer training set membership. We conduct comprehensive evaluations on widely adopted models, such as Code Llama 7B, over the APPS and HumanEval benchmarks. The results show that our approach consistently outperforms state-of-the-art baselines, with AUC gains of up to 102%. In addition, our method exhibits strong transferability across different models and datasets, underscoring its practical utility and generalizability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AdvPrompt-MIAï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ä»£ç è¡¥å…¨æ¨¡å‹ï¼ˆcode completion modelsï¼‰è®¾è®¡çš„é«˜æ•ˆæˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMembership inference attacks, MIAsï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æ›´å‡†ç¡®åœ°è¯„ä¼°ä»£ç è¯­è¨€æ¨¡å‹çš„éšç§æ³„éœ²é£é™©ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–æ˜‚è´µä»£ç†æ¨¡å‹æˆ–å¯å‘å¼è§„åˆ™è€Œéš¾ä»¥æ•æ‰å¤æ‚è®°å¿†æ¨¡å¼çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°ç»“åˆäº†ä»£ç ç‰¹å®šçš„å¯¹æŠ—æ€§æ‰°åŠ¨ï¼ˆadversarial perturbationsï¼‰ä¸æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚å…¶æ ¸å¿ƒåœ¨äºè®¾è®¡ä¸€ç³»åˆ—å¯¹æŠ—æ€§æç¤ºï¼ˆadversarial promptsï¼‰æ¥è¯±å¯¼å—å®³æ¨¡å‹äº§ç”Ÿè¾“å‡ºå˜åŒ–ï¼Œå¹¶é€šè¿‡æ¯”è¾ƒè¿™äº›è¾“å‡ºä¸çœŸå®ä»£ç ï¼ˆground-truthï¼‰çš„å·®å¼‚æ¥æ„å»ºç‰¹å¾å‘é‡ï¼Œè¿›è€Œè®­ç»ƒåˆ†ç±»å™¨è‡ªåŠ¨åŒºåˆ†æˆå‘˜ä¸éæˆå‘˜æ ·æœ¬ã€‚åœ¨ Code Llama 7B ä»¥åŠ APPS å’Œ HumanEval åŸºå‡†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdvPrompt-MIA çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ï¼Œå…¶ AUC å¢ç›Šæœ€é«˜å¯è¾¾ 102%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†é—´è¡¨ç°å‡ºå¼ºå¤§çš„å¯è¿ç§»æ€§ï¼ˆtransferabilityï¼‰å’Œæ³›åŒ–æ€§ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…éšç§å®¡è®¡åœºæ™¯ä¸­çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15107v1",
      "published_date": "2025-11-19 04:30:54 UTC",
      "updated_date": "2025-11-19 04:30:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:06:07.671999+00:00"
    },
    {
      "arxiv_id": "2511.15097v2",
      "title": "MAIF: Enforcing AI Trust and Provenance with an Artifact-Centric Agentic Paradigm",
      "title_zh": "MAIFï¼šåŸºäºä»¥åˆ¶å“ä¸ºä¸­å¿ƒæ™ºèƒ½ä½“èŒƒå¼çš„äººå·¥æ™ºèƒ½ä¿¡ä»»ä¸æº¯æºå¼ºåŒ–",
      "authors": [
        "Vineeth Sai Narajala",
        "Manish Bhatt",
        "Idan Habler",
        "Ronald F. Del Rosario",
        "Ads Dawson"
      ],
      "abstract": "The AI trustworthiness crisis threatens to derail the artificial intelligence revolution, with regulatory barriers, security vulnerabilities, and accountability gaps preventing deployment in critical domains. Current AI systems operate on opaque data structures that lack the audit trails, provenance tracking, or explainability required by emerging regulations like the EU AI Act. We propose an artifact-centric AI agent paradigm where behavior is driven by persistent, verifiable data artifacts rather than ephemeral tasks, solving the trustworthiness problem at the data architecture level. Central to this approach is the Multimodal Artifact File Format (MAIF), an AI-native container embedding semantic representations, cryptographic provenance, and granular access controls. MAIF transforms data from passive storage into active trust enforcement, making every AI operation inherently auditable. Our production-ready implementation demonstrates ultra-high-speed streaming (2,720.7 MB/s), optimized video processing (1,342 MB/s), and enterprise-grade security. Novel algorithms for cross-modal attention, semantic compression, and cryptographic binding achieve up to 225 compression while maintaining semantic fidelity. Advanced security features include stream-level access control, real-time tamper detection, and behavioral anomaly analysis with minimal overhead. This approach directly addresses the regulatory, security, and accountability challenges preventing AI deployment in sensitive domains, offering a viable path toward trustworthy AI systems at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIä¿¡ä»»å±æœºä»¥åŠæ–°å…´ç›‘ç®¡ï¼ˆå¦‚EU AI Actï¼‰å¯¹å®¡è®¡ã€æº¯æºå’Œè§£é‡Šæ€§çš„ä¸¥è‹›è¦æ±‚ï¼Œæå‡ºäº†ä¸€ç§ä»¥åˆ¶å“ä¸ºä¸­å¿ƒ(Artifact-Centric)çš„AIæ™ºèƒ½ä½“èŒƒå¼ã€‚è¯¥èŒƒå¼çš„æ ¸å¿ƒæ˜¯å¼€å‘äº†å¤šæ¨¡æ€åˆ¶å“æ–‡ä»¶æ ¼å¼(Multimodal Artifact File Format, MAIF)ï¼Œè¿™æ˜¯ä¸€ç§é›†æˆè¯­ä¹‰è¡¨ç¤ºã€åŠ å¯†æº¯æºå’Œç»†ç²’åº¦è®¿é—®æ§åˆ¶çš„AIåŸç”Ÿå®¹å™¨ã€‚MAIFé€šè¿‡æŒä¹…ä¸”å¯éªŒè¯çš„æ•°æ®åˆ¶å“é©±åŠ¨æ™ºèƒ½ä½“è¡Œä¸ºï¼Œå°†æ•°æ®ä»è¢«åŠ¨å­˜å‚¨è½¬åŒ–ä¸ºä¸»åŠ¨çš„ä¿¡ä»»æ‰§è¡Œå·¥å…·ï¼Œç¡®ä¿äº†æ‰€æœ‰AIæ“ä½œçš„å¤©ç„¶å¯å®¡è®¡æ€§ã€‚æŠ€æœ¯ä¸Šï¼Œè®ºæ–‡å¼•å…¥äº†è·¨æ¨¡æ€æ³¨æ„åŠ›(Cross-modal attention)ã€è¯­ä¹‰å‹ç¼©(Semantic compression)å’ŒåŠ å¯†ç»‘å®š(Cryptographic binding)ç­‰åˆ›æ–°ç®—æ³•ï¼Œåœ¨ä¿æŒè¯­ä¹‰ä¿çœŸåº¦çš„å‰æä¸‹å®ç°äº†é«˜è¾¾225å€çš„å‹ç¼©æ•ˆç‡ã€‚ç”Ÿäº§çº§æµ‹è¯•è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæ”¯æŒè¶…é«˜é€Ÿæµä¼ è¾“(2,720.7 MB/s)åŠè§†é¢‘å¤„ç†ï¼Œå¹¶å…·å¤‡å®æ—¶ç¯¡æ”¹æ£€æµ‹å’Œè¡Œä¸ºå¼‚å¸¸åˆ†æç­‰ä¼ä¸šçº§å®‰å…¨åŠŸèƒ½ã€‚è¿™ä¸€æ–¹æ³•ç›´æ¥åº”å¯¹äº†æ•æ„Ÿé¢†åŸŸAIéƒ¨ç½²ä¸­çš„ç›‘ç®¡ä¸é—®è´£æŒ‘æˆ˜ï¼Œä¸ºæ„å»ºå¤§è§„æ¨¡ã€å¯ä¿¡çš„AIç³»ç»Ÿæä¾›äº†ä¸€æ¡å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "7 Pages, 2 Figures, 6 Tables, Repo: https://github.com/vineethsai/maifscratch-1, Added additional Author and fixed Citations",
      "pdf_url": "https://arxiv.org/pdf/2511.15097v2",
      "published_date": "2025-11-19 04:10:32 UTC",
      "updated_date": "2025-11-24 02:26:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:06:02.281989+00:00"
    },
    {
      "arxiv_id": "2511.15090v1",
      "title": "BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer",
      "title_zh": "BBox DocVQAï¼šç”¨äºå¢å¼ºæ–‡æ¡£è§†è§‰é—®ç­”æ¨ç†èƒ½åŠ›çš„å¤§è§„æ¨¡è¾¹ç•Œæ¡†å®šä½æ•°æ®é›†",
      "authors": [
        "Wenhan Yu",
        "Wang Chen",
        "Guanqiang Qi",
        "Weikang Li",
        "Yang Li",
        "Lei Sha",
        "Deguo Xia",
        "Jizhou Huang"
      ],
      "abstract": "Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ–‡æ¡£è§†è§‰é—®ç­”(DocVQA)æ•°æ®é›†ç¼ºä¹ç»†ç²’åº¦ç©ºé—´å®šä½(spatial grounding)ä»è€Œé™åˆ¶è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†å¤§è§„æ¨¡è¾¹ç•Œæ¡†æ ‡æ³¨æ•°æ®é›†BBox DocVQAã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å¼€å‘äº†â€œSegment-Judge-Generateâ€è‡ªåŠ¨åŒ–æ„å»ºæµæ°´çº¿ï¼Œåˆ©ç”¨åˆ†å‰²æ¨¡å‹(segment model)å’Œé«˜çº§VLMså®ç°é«˜è´¨é‡QAå¯¹çš„ç”Ÿæˆä¸éªŒè¯ã€‚è¯¥æ•°æ®é›†åŒ…å«3.6Kä»½å¤šæ ·åŒ–æ–‡æ¡£åŠ32Kä¸ªQAå®ä¾‹ï¼Œå®Œæ•´è¦†ç›–äº†å•/å¤šåŒºåŸŸå’Œå•/å¤šé¡µé¢çš„å¤æ‚è§†è§‰åœºæ™¯ã€‚é€šè¿‡å¯¹GPT-5ã€Qwen2.5-VLå’ŒInternVLç­‰æœ€å…ˆè¿›æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨ç©ºé—´å®šä½å’Œæ¨ç†å‡†ç¡®æ€§ä¸Šçš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨BBox DocVQAä¸Šè¿›è¡Œå¾®è°ƒ(fine-tuning)èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨è¾¹ç•Œæ¡†å®šä½(bounding box localization)å’Œç­”æ¡ˆç”Ÿæˆæ–¹é¢çš„è¡¨ç°ï¼Œæœ‰æ•ˆå¢å¼ºäº†VLMsçš„å¯è§£é‡Šæ€§ä¸ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.DB",
      "comment": "22 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.15090v1",
      "published_date": "2025-11-19 04:03:54 UTC",
      "updated_date": "2025-11-19 04:03:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:06:09.063875+00:00"
    },
    {
      "arxiv_id": "2511.15076v2",
      "title": "GPU-Initiated Networking for NCCL",
      "title_zh": "é¢å‘ NCCL çš„ GPU å‘èµ·å¼ç½‘ç»œé€šä¿¡",
      "authors": [
        "Khaled Hamidouche",
        "John Bachan",
        "Pak Markthub",
        "Peter-Jan Gootzen",
        "Elena Agostini",
        "Sylvain Jeaugey",
        "Aamir Shafi",
        "Georgios Theodorakis",
        "Manjunath Gorentla Venkata"
      ],
      "abstract": "Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.\n  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç°ä»£AIå·¥ä½œè´Ÿè½½ï¼ˆç‰¹åˆ«æ˜¯MoEæ¶æ„ï¼‰å¯¹ä½å»¶è¿Ÿã€ç»†ç²’åº¦GPU-to-GPUé€šä¿¡çš„éœ€æ±‚ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„CPUåè°ƒæ¨¡å¼å­˜åœ¨é€šä¿¡å¼€é”€ã€‚ä¸ºæ­¤ï¼ŒNCCL 2.28å¼•å…¥äº†Device APIåŠå…¶GPU-Initiated Networking (GIN) æ¶æ„ï¼Œå…è®¸ç›´æ¥ä»CUDA kernelsè§¦å‘ç½‘ç»œRDMAæ“ä½œä»¥æ¶ˆé™¤æ§åˆ¶é¢å»¶è¿Ÿã€‚GINé‡‡ç”¨ä¸‰å±‚æ¶æ„ï¼ŒåŒ…æ‹¬ç”¨äºé€šä¿¡å™¨è®¾ç½®çš„Host-side APIsã€è®¾å¤‡ç«¯è°ƒç”¨çš„Device-side APIsï¼Œä»¥åŠæ”¯æŒå¤šç§ç¡¬ä»¶è¯­ä¹‰çš„Network pluginæ’ä»¶ç³»ç»Ÿã€‚è¯¥æ¶æ„é€šè¿‡DOCA GPUNetIOæ”¯æŒGPUDirect Async Kernel-Initiatedåç«¯ï¼Œå®ç°GPUä¸NICçš„ç›´æ¥é€šä¿¡ï¼ŒåŒæ—¶æä¾›é€‚ç”¨äºæ ‡å‡†RDMAç½‘ç»œçš„Proxyåç«¯ã€‚é€šè¿‡åœ¨MoEé€šä¿¡åº“DeepEPä¸­çš„é›†æˆåº”ç”¨ï¼Œå®éªŒè¯æ˜GINèƒ½åœ¨NCCLç»Ÿä¸€è¿è¡Œæ—¶ä¸­æä¾›é«˜æ•ˆçš„è®¾å¤‡è§¦å‘é€šä¿¡èƒ½åŠ›ã€‚è¯¥æ–¹æ¡ˆæˆåŠŸå°†ä½å»¶è¿Ÿæ“ä½œä¸NCCLç°æœ‰çš„é›†ä½“é€šä¿¡ç®—æ³•åŠç”Ÿäº§çº§åŸºç¡€è®¾æ–½ç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—ä¸é€šä¿¡ç´§å¯†è€¦åˆä»»åŠ¡çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.AR",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "13 pages, 9 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.15076v2",
      "published_date": "2025-11-19 03:36:03 UTC",
      "updated_date": "2025-11-24 23:45:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:06:57.875878+00:00"
    },
    {
      "arxiv_id": "2511.15074v1",
      "title": "Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“åä½œçš„çŸ¥è¯†é©±åŠ¨è‡ªåŠ¨ç‰¹å¾æå–",
      "authors": [
        "Henrik Bradland",
        "Morten Goodwin",
        "Vladimir I. Zadorozhny",
        "Per-Arne Andersen"
      ],
      "abstract": "The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a \"flooding-pruning\" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Rogue Oneï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨åŒ–ç‰¹å¾æå–(AutoFE)æ–¹æ³•åœ¨æ•´åˆå¤–éƒ¨é¢†åŸŸçŸ¥è¯†å’Œå¤„ç†å¤æ‚åé¦ˆæ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªç”±ç§‘å­¦å®¶(Scientist)ã€æå–å™¨(Extractor)å’Œæµ‹è¯•å™¨(Tester)ç»„æˆçš„å»ä¸­å¿ƒåŒ–ç³»ç»Ÿï¼Œé€šè¿‡è¿­ä»£åä½œæ¥å‘ç°ã€ç”Ÿæˆå¹¶éªŒè¯å…·æœ‰é¢„æµ‹æ€§çš„ç‰¹å¾ã€‚Rogue Oneå¼•å…¥äº†å®šæ€§åé¦ˆæœºåˆ¶å’Œâ€œæ³›æ»¥-å‰ªæâ€(flooding-pruning)ç­–ç•¥ï¼Œå¹¶åœ¨ç‰¹å¾æ¢ç´¢ä¸å¼€å‘ä¹‹é—´å®ç°äº†åŠ¨æ€å¹³è¡¡ã€‚é€šè¿‡é›†æˆæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ï¼Œè¯¥æ¡†æ¶èƒ½ä¸»åŠ¨èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œç¡®ä¿ç”Ÿæˆçš„ç‰¹å¾å…¼å…·ç»Ÿè®¡æ•ˆåŠ›å’Œè¯­ä¹‰å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRogue Oneåœ¨28ä¸ªåˆ†ç±»ä¸å›å½’æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿåœ¨å¿ƒè‚Œæ•°æ®é›†ä¸­è¯†åˆ«å‡ºäº†æ–°çš„æ½œåœ¨ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œè¯æ˜äº†å…¶åœ¨ç§‘å­¦å‘ç°å’Œç”Ÿæˆå¯æµ‹è¯•å‡è®¾æ–¹é¢çš„ç‹¬ç‰¹ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 4 figures, in review",
      "pdf_url": "https://arxiv.org/pdf/2511.15074v1",
      "published_date": "2025-11-19 03:27:14 UTC",
      "updated_date": "2025-11-19 03:27:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:07:12.884404+00:00"
    },
    {
      "arxiv_id": "2511.15069v1",
      "title": "ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression",
      "title_zh": "ProRACï¼šä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹æ¼”è¿›çš„åŠ¨ä½œæ¨ç†ç¥ç»ç¬¦å·æ–¹æ³•",
      "authors": [
        "Haoyong Wu",
        "Yongmei Liu"
      ],
      "abstract": "In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProRAC (Progression-based Reasoning about Actions and Change)ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¤„ç†åŠ¨ä½œä¸å˜åŒ–æ¨ç† (Reasoning about Actions and Change, RAC) é—®é¢˜çš„ç¥ç»ç¬¦å· (neuro-symbolic) æ¡†æ¶ã€‚ProRAC é¦–å…ˆä»é—®é¢˜ä¸­æå–åŠ¨ä½œ (actions) å’ŒæŸ¥è¯¢ (questions) ç­‰åŸºæœ¬è¦ç´ ï¼Œéšåé€šè¿‡é€æ­¥æ‰§è¡Œæ¯ä¸ªåŠ¨ä½œæ¥æ¨å¯¼å‡ºæœ€ç»ˆçŠ¶æ€ (final state)ï¼Œå¹¶æ ¹æ®è¯¥çŠ¶æ€å¯¹æŸ¥è¯¢è¿›è¡Œè¯„ä¼°ä»¥å¾—å‡ºç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ª RAC åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨ä¸åŒçš„é¢†åŸŸã€LLM ä¸»å¹²æ¨¡å‹ (backbones) å’Œä»»åŠ¡ç±»å‹ä¸Šå‡å…·æœ‰æå¼ºçš„é²æ£’æ€§ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†å°† LLMs çš„è¯­ä¹‰æå–èƒ½åŠ›ä¸åŸºäºæ¼”è¿›çš„é€»è¾‘æ¨ç†ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¤æ‚åŠ¨ä½œæ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15069v1",
      "published_date": "2025-11-19 03:20:06 UTC",
      "updated_date": "2025-11-19 03:20:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:07:44.775224+00:00"
    },
    {
      "arxiv_id": "2511.15067v1",
      "title": "Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer",
      "title_zh": "æ·±åº¦ç—…ç†ç»„å­¦å­¦ä¹ å®šä¹‰ç»“ç›´è‚ ç™Œé¢„åäºšå‹ä¸åˆ†å­é©±åŠ¨å› ç´ ",
      "authors": [
        "Zisong Wang",
        "Xuanyu Wang",
        "Hang Chen",
        "Haizhou Wang",
        "Yuxin Chen",
        "Yihang Xu",
        "Yunhe Yuan",
        "Lihuan Luo",
        "Xitong Ling",
        "Xiaoping Liu"
      ],
      "abstract": "Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»“ç›´è‚ ç™Œ(CRC)çš„é«˜å¼‚è´¨æ€§å¯¼è‡´é¢„åè¯„ä¼°å›°éš¾çš„é—®é¢˜ï¼Œå¼€å‘å¹¶éªŒè¯äº†ä¸€ç§åä¸ºTDAM-CRCçš„å¤šå®ä¾‹å­¦ä¹ (multiple instance learning)æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒ(whole-slide images)è¿›è¡Œé¢„åé¢„æµ‹ï¼Œå¹¶åœ¨TCGAå‘ç°é˜Ÿåˆ—å’Œç‹¬ç«‹å¤–éƒ¨éªŒè¯é˜Ÿåˆ—ä¸­å±•ç¤ºäº†é²æ£’çš„é£é™©åˆ†å±‚èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTDAM-CRCçš„é¢„æµ‹æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿä¸´åºŠåˆ†æœŸç³»ç»ŸåŠå¤šç§å‰æ²¿(state-of-the-art)æ¨¡å‹ï¼Œå…¶é£é™©è¯„åˆ†è¢«è¯å®ä¸ºç‹¬ç«‹çš„é¢„åå› ç´ ã€‚é€šè¿‡æ•´åˆå¤šç»„å­¦(multi-omics)åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†é«˜é£é™©äºšå‹ä¸ä»£è°¢é‡ç¼–ç¨‹(metabolic reprogramming)å’Œå…ç–«æŠ‘åˆ¶è‚¿ç˜¤å¾®ç¯å¢ƒ(immunosuppressive tumor microenvironment)å¯†åˆ‡ç›¸å…³ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯†åˆ«å¹¶éªŒè¯äº†MRPL37ä½œä¸ºè¿æ¥æ·±åº¦ç—…ç†ç‰¹å¾ä¸ä¸´åºŠé¢„åçš„å…³é”®æ¢çº½åŸºå› ï¼Œå‘ç°å…¶é«˜è¡¨è¾¾å¯ä½œä¸ºé¢„åè‰¯å¥½çš„ç‹¬ç«‹ç”Ÿç‰©æ ‡å¿—ç‰©(biomarker)ã€‚æœ€åï¼Œç ”ç©¶æ„å»ºäº†ç»“åˆTDAM-CRCé£é™©è¯„åˆ†å’Œä¸´åºŠå› ç´ çš„åˆ—çº¿å›¾(nomogram)ï¼Œä¸ºCRCæ‚£è€…æä¾›äº†ç²¾å‡†ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„ä¸´åºŠå†³ç­–å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "q-bio.GN"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15067v1",
      "published_date": "2025-11-19 03:19:43 UTC",
      "updated_date": "2025-11-19 03:19:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:07:24.672881+00:00"
    },
    {
      "arxiv_id": "2511.15065v2",
      "title": "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks",
      "title_zh": "è§†é¢‘æ¨ç†ï¼šåŸºäºè¿·å®«æ±‚è§£ä»»åŠ¡çš„è§†é¢‘æ¨¡å‹æ¨ç†èƒ½åŠ›é¦–æ¬¡è¯„ä¼°",
      "authors": [
        "Cheng Yang",
        "Haiyuan Wan",
        "Yiran Peng",
        "Xin Cheng",
        "Zhaoyang Yu",
        "Jiayi Zhang",
        "Junchi Yu",
        "Xinlei Yu",
        "Xiawu Zheng",
        "Dongzhan Zhou",
        "Chenglin Wu"
      ],
      "abstract": "Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†é¢‘æ¨¡å‹æ˜¯å¦èƒ½é€šè¿‡è§†é¢‘ç”Ÿæˆè¿›è¡Œæ¨ç†ï¼Œå¹¶æå‡ºäº† VR-Benchï¼Œè¿™æ˜¯é¦–ä¸ªç³»ç»Ÿè¯„ä¼°è§†é¢‘æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ç»¼åˆåŸºå‡†ã€‚VR-Bench ä»¥ Maze-solving tasks ä¸ºåŸºç¡€ï¼ŒåŒ…å« 7,920 ä¸ªç¨‹åºç”Ÿæˆçš„è§†é¢‘ï¼Œæ¶µç›–äº†ç©ºé—´è§„åˆ’å’Œå¤šæ­¥æ¨ç†æ‰€éœ€çš„äº”ç§è¿·å®«ç±»å‹åŠå¤šæ ·è§†è§‰é£æ ¼ã€‚å®éªŒè¡¨æ˜ï¼ŒSFT èƒ½æœ‰æ•ˆæ¿€å‘è§†é¢‘æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶å±•ç°å‡ºæ¯”é¢†å…ˆçš„ VLMs æ›´å¼ºçš„ç©ºé—´æ„ŸçŸ¥åŠ›ï¼Œå¹¶åœ¨å¤šç§åœºæ™¯ã€ä»»åŠ¡å’Œå¤æ‚åº¦ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§ã€‚ç ”ç©¶è¿˜å‘ç°äº† test-time scaling effectï¼Œå³åœ¨æ¨ç†é˜¶æ®µé€šè¿‡ diverse sampling å¯ä½¿æ¨ç†å¯é æ€§æå‡ 10% è‡³ 20%ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº† Reasoning via Video èŒƒå¼åœ¨å¤„ç†ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­çš„ç‹¬ç‰¹æ½œåŠ›ä¸å¯æ‰©å±•æ€§ï¼Œä¸ºæœªæ¥è§†é¢‘æ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15065v2",
      "published_date": "2025-11-19 03:18:29 UTC",
      "updated_date": "2025-11-24 07:46:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:08:05.876178+00:00"
    },
    {
      "arxiv_id": "2511.15061v1",
      "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering",
      "title_zh": "è¶…è¶Š GeneGPTï¼šåŸºäºå¼€æºå¤§è¯­è¨€æ¨¡å‹çš„å¢å¼ºå‹åŸºå› ç»„é—®ç­”å¤šæ™ºèƒ½ä½“æ¶æ„",
      "authors": [
        "Haodong Chen",
        "Guido Zuccon",
        "Teerapong Leelanupab"
      ],
      "abstract": "Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.\n  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.\n  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹GeneGPTä¾èµ–é—­æºæ¨¡å‹å¸¦æ¥çš„é«˜æ˜‚æˆæœ¬å’Œæ•°æ®éšç§é—®é¢˜ï¼Œæ¢è®¨äº†åŸºå› ç»„é—®ç­”ç³»ç»Ÿåœ¨å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸‹çš„å®ç°ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆåˆ©ç”¨Llama 3.1å’ŒQwen2.5ç­‰æ¨¡å‹å¤ç°äº†GeneGPTçš„å•ä½“æ¶æ„ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¼€å‘äº†OpenBioLLMæ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“(multi-agent)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä¸“é—¨çš„æ™ºèƒ½ä½“è´Ÿè´£å·¥å…·è·¯ç”±(tool routing)ã€æŸ¥è¯¢ç”Ÿæˆå’Œå“åº”éªŒè¯ï¼Œå®ç°äº†é«˜æ•ˆçš„åè°ƒæ¨ç†ä¸è§’è‰²åŒ–ä»»åŠ¡æ‰§è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenBioLLMåœ¨Gene-Turingå’ŒGeneHopåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°è¾¾åˆ°æˆ–ä¼˜äºGeneGPTï¼Œåœ¨è¶…è¿‡90%çš„ä»»åŠ¡ä¸­å±•ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œå¾—ç›Šäºæ¨¡å—åŒ–è®¾è®¡ï¼Œè¯¥ç³»ç»Ÿåœ¨æ— éœ€é¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹å°†ä»»åŠ¡å¤„ç†å»¶è¿Ÿé™ä½äº†40-50%ã€‚è¿™é¡¹å·¥ä½œå……åˆ†è¯æ˜äº†å¼€æºå¤šæ™ºèƒ½ä½“æ¶æ„åœ¨å¤„ç†å¤æ‚åŸºå› ç»„å­¦é—®ç­”ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted to SIGIR-AP 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.15061v1",
      "published_date": "2025-11-19 03:08:20 UTC",
      "updated_date": "2025-11-19 03:08:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:07:23.259180+00:00"
    },
    {
      "arxiv_id": "2511.15055v1",
      "title": "Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization",
      "title_zh": "åŸºäºåŠ¨ä½œé‡åŒ–è½¨è¿¹ä¼˜åŒ–çš„ç±»äººå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“å­¦ä¹ ",
      "authors": [
        "Jian-Ting Guo",
        "Yu-Cheng Chen",
        "Ping-Chun Hsieh",
        "Kuo-Hao Ho",
        "Po-Wei Huang",
        "Ti-Rong Wu",
        "I-Chen Wu"
      ],
      "abstract": "Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ™ºèƒ½ä½“è¡Œä¸ºä¸è‡ªç„¶ã€ç¼ºä¹å¯è§£é‡Šæ€§å’Œå¯ä¿¡ä»»æ€§çš„é—®é¢˜ï¼Œå°†äººæ€§åŒ–è¡Œä¸ºå»ºæ¨¡ä¸ºè½¨è¿¹ä¼˜åŒ–(Trajectory Optimization)é—®é¢˜ï¼Œæ—¨åœ¨å¯»æ‰¾æ—¢èƒ½æœ€å¤§åŒ–å¥–åŠ±åˆèƒ½ä¸äººç±»è¡Œä¸ºç´§å¯†å¯¹é½çš„åŠ¨ä½œåºåˆ—ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Macro Action Quantization (MAQ)æ¡†æ¶ï¼Œåˆ©ç”¨å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨(Vector-Quantized VAE)å°†äººç±»æ¼”ç¤ºæç‚¼ä¸ºå®åŠ¨ä½œ(Macro Actions)ï¼Œå¹¶é‡‡ç”¨åé€€åœ°å¹³çº¿æ§åˆ¶(Receding-horizon control)å®ç°é«˜æ•ˆå­¦ä¹ ã€‚åœ¨D4RL AdroitåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMAQæ˜¾è‘—æé«˜äº†è½¨è¿¹ç›¸ä¼¼æ€§å¾—åˆ†ï¼Œå¹¶åœ¨äººç±»è¯„ä¼°ç ”ç©¶ä¸­è·å¾—äº†æœ€é«˜çš„äººæ€§åŒ–æ’åã€‚æ­¤å¤–ï¼ŒMAQè¡¨ç°å‡ºè‰¯å¥½çš„å…¼å®¹æ€§ï¼Œå¯è½»æ¾é›†æˆåˆ°å¤šç§ç°æˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ï¼Œä¸ºå¼€å‘å…·å¤‡ç±»äººè¡Œä¸ºçš„æ™ºèƒ½ä½“å¼€è¾Ÿäº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by the Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.15055v1",
      "published_date": "2025-11-19 02:59:47 UTC",
      "updated_date": "2025-11-19 02:59:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:07:31.279795+00:00"
    },
    {
      "arxiv_id": "2511.15046v1",
      "title": "UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space",
      "title_zh": "UniHOIï¼šåŸºäºç»Ÿä¸€ Token ç©ºé—´çš„ç»Ÿä¸€äººç‰©äº¤äº’ç†è§£",
      "authors": [
        "Panqi Yang",
        "Haodong Jing",
        "Nanning Zheng",
        "Yongqiang Ma"
      ],
      "abstract": "In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UniHOIï¼Œæ—¨åœ¨è§£å†³äººç±»-ç‰©ä½“äº¤äº’ (Human-Object Interaction, HOI) é¢†åŸŸä¸­æ£€æµ‹ä¸ç”Ÿæˆä»»åŠ¡ç›¸äº’å­¤ç«‹ã€é˜»ç¢å…¨é¢äº¤äº’ç†è§£çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ç»Ÿä¸€çš„æ ‡è®°ç©ºé—´ (Unified Token Space)ï¼Œè¯¥æ¡†æ¶å®ç°äº†æ£€æµ‹ä¸ç”Ÿæˆçš„è”åˆå»ºæ¨¡ï¼Œä»è€Œæœ‰æ•ˆä¿ƒè¿›äº†çŸ¥è¯†å…±äº«å¹¶æå‡äº†æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚UniHOI è¿›ä¸€æ­¥é‡‡ç”¨äº†å¯¹ç§°äº¤äº’æ„ŸçŸ¥æ³¨æ„åŠ›æ¨¡å— (Symmetric Interaction-aware Attention Module) ä»¥åŠç»Ÿä¸€çš„åŠç›‘ç£å­¦ä¹ èŒƒå¼ï¼Œç¡®ä¿äº†å›¾åƒä¸äº¤äº’è¯­ä¹‰åœ¨æœ‰é™æ ‡æ³¨ä¸‹ä»èƒ½å®ç°æœ‰æ•ˆçš„åŒå‘æ˜ å°„ã€‚å®éªŒæ•°æ®è¯æ˜ï¼ŒUniHOI åœ¨æ£€æµ‹å’Œç”Ÿæˆä»»åŠ¡ä¸­å‡è¾¾åˆ°äº† SOTA æ°´å¹³ï¼Œå…¶ä¸­é•¿å°¾ HOI æ£€æµ‹å‡†ç¡®ç‡æå‡äº† 4.9%ï¼Œå¼€æ”¾è¯æ±‡ç”Ÿæˆä»»åŠ¡çš„äº¤äº’æŒ‡æ ‡æ›´æ˜¯å¢é•¿äº† 42.0%ã€‚è¿™ä¸€æˆæœä¸ºæ„å»ºç»Ÿä¸€ä¸”å…¨é¢çš„äº¤äº’ç†è§£ç³»ç»Ÿæä¾›äº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2026,9 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.15046v1",
      "published_date": "2025-11-19 02:37:03 UTC",
      "updated_date": "2025-11-19 02:37:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:07:36.361996+00:00"
    },
    {
      "arxiv_id": "2511.17630v1",
      "title": "Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change",
      "title_zh": "èƒ½å¦åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼å¼ºåŒ–å­¦ä¹ ï¼Ÿâ€”â€”æ•°å­—å¥åº·è¡Œä¸ºæ”¹å˜æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Nele Albers",
        "Esra Cemre Su de Groot",
        "Loes Keijsers",
        "Manon H. Hillegers",
        "Emiel Krahmer"
      ],
      "abstract": "Personalizing digital applications for health behavior change is a promising route to making them more engaging and effective. This especially holds for approaches that adapt to users and their specific states (e.g., motivation, knowledge, wants) over time. However, developing such approaches requires making many design choices, whose effectiveness is difficult to predict from literature and costly to evaluate in practice. In this work, we explore whether large language models (LLMs) can be used out-of-the-box to generate samples of user interactions that provide useful information for training reinforcement learning models for digital behavior change settings. Using real user data from four large behavior change studies as comparison, we show that LLM-generated samples can be useful in the absence of real data. Comparisons to the samples provided by human raters further show that LLM-generated samples reach the performance of human raters. Additional analyses of different prompting strategies including shorter and longer prompt variants, chain-of-thought prompting, and few-shot prompting show that the relative effectiveness of different strategies depends on both the study and the LLM with also relatively large differences between prompt paraphrases alone. We provide recommendations for how LLM-generated samples can be useful in practice.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ˜¯å¦å¯ä»¥åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç¼ºä¹çœŸå®æ•°æ®çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆç”¨æˆ·äº¤äº’æ ·æœ¬ä»¥å¼•å¯¼æ•°å­—å¥åº·è¡Œä¸ºæ”¹å˜é¢†åŸŸçš„å¼ºåŒ–å­¦ä¹ (RL)æ¨¡å‹è®­ç»ƒã€‚é€šè¿‡å¯¹æ¯”å››é¡¹å¤§å‹è¡Œä¸ºæ”¹å˜ç ”ç©¶çš„çœŸå®ç”¨æˆ·æ•°æ®ï¼Œç»“æœè¡¨æ˜LLMç”Ÿæˆçš„æ ·æœ¬èƒ½æä¾›æœ‰æ•ˆä¿¡æ¯ï¼Œä¸”å…¶æ€§èƒ½è¡¨ç°å¯ä¸äººç±»è¯„ä¼°è€…æŒå¹³ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯„ä¼°äº†åŒ…æ‹¬é“¾å¼æ€ç»´(Chain-of-Thought)å’Œå°‘æ ·æœ¬æç¤º(Few-shot prompting)åœ¨å†…çš„å¤šç§æç¤ºç­–ç•¥ï¼Œå‘ç°å…¶æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºå…·ä½“çš„æ¨¡å‹å’Œç ”ç©¶èƒŒæ™¯ã€‚æ­¤å¤–ï¼Œæç¤ºè¯çš„ç»†å¾®æ”¹å†™(Paraphrases)ä¹Ÿä¼šå¯¼è‡´å®éªŒç»“æœå‡ºç°è¾ƒå¤§æ³¢åŠ¨ã€‚è¯¥ç ”ç©¶æœ€åä¸ºåœ¨å®è·µä¸­åˆ©ç”¨LLMç”Ÿæˆæ ·æœ¬æä¾›äº†å…·ä½“å»ºè®®ï¼Œæ—¨åœ¨è§£å†³ä¸ªæ€§åŒ–æ•°å­—å¥åº·å¹²é¢„æªæ–½ä¸­è®¾è®¡æˆæœ¬é«˜ä¸”éš¾ä»¥é¢„æµ‹çš„é—®é¢˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17630v1",
      "published_date": "2025-11-19 02:28:32 UTC",
      "updated_date": "2025-11-19 02:28:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:07:38.987004+00:00"
    },
    {
      "arxiv_id": "2511.15750v1",
      "title": "Writing With Machines and Peers: Designing for Critical Engagement with Generative AI",
      "title_zh": "ä¸æœºå™¨åŠåŒä¼´åä½œå†™ä½œï¼šé¢å‘ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ‰¹åˆ¤æ€§å‚ä¸çš„è®¾è®¡",
      "authors": [
        "Xinran Zhu",
        "Cong Wang",
        "Duane Searsmith"
      ],
      "abstract": "The growing integration of generative AI in higher education is transforming how students write, learn, and engage with knowledge. As AI tools become more integrated into classrooms, there is an urgent need for pedagogical approaches that help students use them critically and reflectively. This study proposes a pedagogical design that integrates AI and peer feedback in a graduate-level academic writing activity. Over eight weeks, students developed literature review projects through multiple writing and revision stages, receiving feedback from both a custom-built AI reviewer and human peers. We examine two questions: (1) How did students interact with and incorporate AI and peer feedback during the writing process? and (2) How did they reflect on and build relationships with both human and AI reviewers? Data sources include student writing artifacts, AI and peer feedback, AI chat logs, and student reflections. Findings show that students engaged differently with each feedback source-relying on AI for rubric alignment and surface-level edits, and on peer feedback for conceptual development and disciplinary relevance. Reflections revealed evolving relationships with AI, characterized by increasing confidence, strategic use, and critical awareness of its limitations. The pedagogical design supported writing development, AI literacy, and disciplinary understanding. This study offers a scalable pedagogical model for integrating AI into writing instruction and contributes insights for system-level approaches to fostering meaningful human-AI collaboration in higher education.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜ç­‰æ•™è‚²ä¸­ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)çš„æ•´åˆè¶‹åŠ¿ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆAIä¸åŒä¼´åé¦ˆ(peer feedback)çš„æ•™å­¦è®¾è®¡ï¼Œæ—¨åœ¨æå‡å­¦ç”Ÿåœ¨å­¦æœ¯å†™ä½œä¸­çš„æ‰¹åˆ¤æ€§å‚ä¸åº¦ã€‚ç ”ç©¶åœ¨ä¸ºæœŸå…«å‘¨çš„ç ”ç©¶ç”Ÿå­¦æœ¯å†™ä½œæ´»åŠ¨ä¸­ï¼Œè®©å­¦ç”Ÿåœ¨æ’°å†™æ–‡çŒ®ç»¼è¿°(literature review)çš„è¿‡ç¨‹ä¸­ï¼Œäº¤æ›¿æ¥å—å®šåˆ¶åŒ–AIå®¡é˜…è€…å’Œäººç±»åŒä¼´çš„åé¦ˆã€‚è°ƒæŸ¥é‡ç‚¹åœ¨äºå­¦ç”Ÿå¦‚ä½•é‡‡çº³è¿™ä¸¤ç§æ¥æºçš„åé¦ˆï¼Œä»¥åŠä»–ä»¬ä¸AIå’Œäººç±»å®¡é˜…è€…ä¹‹é—´å…³ç³»çš„æ¼”å˜ã€‚ç ”ç©¶å‘ç°ï¼Œå­¦ç”Ÿæ›´å€¾å‘äºåˆ©ç”¨AIè¿›è¡Œè¯„åˆ†é‡è¡¨(rubric)å¯¹é½å’Œè¡¨é¢å±‚æ¬¡çš„è¯­è¨€ä¿®æ”¹ï¼Œè€ŒåŒä¼´åé¦ˆåœ¨æ¦‚å¿µå¼€å‘å’Œå­¦ç§‘ç›¸å…³æ€§(disciplinary relevance)æ–¹é¢å‘æŒ¥äº†æ›´æ ¸å¿ƒçš„ä½œç”¨ã€‚å­¦ç”Ÿçš„åæ€æ­ç¤ºäº†ä»–ä»¬å¯¹AIçš„è®¤çŸ¥æ¼”å˜ï¼Œè¡¨ç°ä¸ºä½¿ç”¨è‡ªä¿¡å¿ƒçš„å¢å¼ºã€ç­–ç•¥æ€§çš„è¿ç”¨ä»¥åŠå¯¹å…¶å±€é™æ€§æ‰¹åˆ¤æ€§æ„è¯†çš„æå‡ã€‚è¯¥æ•™å­¦è®¾è®¡æœ‰æ•ˆåœ°ä¿ƒè¿›äº†å­¦ç”Ÿçš„å†™ä½œèƒ½åŠ›ã€AIç´ å…»(AI literacy)å’Œå­¦ç§‘ç†è§£èƒ½åŠ›ã€‚ç ”ç©¶æœ€ç»ˆæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•™å­¦æ¨¡å‹ï¼Œä¸ºé«˜ç­‰æ•™è‚²ä¸­æ„å»ºæœ‰æ„ä¹‰çš„äººæœºåä½œ(human-AI collaboration)æä¾›äº†ç³»ç»Ÿæ€§çš„å®è·µè§è§£ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15750v1",
      "published_date": "2025-11-19 02:17:42 UTC",
      "updated_date": "2025-11-19 02:17:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:07:47.669802+00:00"
    },
    {
      "arxiv_id": "2511.15038v1",
      "title": "Aligning Generative Music AI with Human Preferences: Methods and Challenges",
      "title_zh": "ç”Ÿæˆå¼éŸ³ä¹äººå·¥æ™ºèƒ½ä¸äººç±»åå¥½çš„å¯¹é½ï¼šæ–¹æ³•ä¸æŒ‘æˆ˜",
      "authors": [
        "Dorien Herremans",
        "Abhinaba Roy"
      ],
      "abstract": "Recent advances in generative AI for music have achieved remarkable fidelity and stylistic diversity, yet these systems often fail to align with nuanced human preferences due to the specific loss functions they use. This paper advocates for the systematic application of preference alignment techniques to music generation, addressing the fundamental gap between computational optimization and human musical appreciation. Drawing on recent breakthroughs including MusicRL's large-scale preference learning, multi-preference alignment frameworks like diffusion-based preference optimization in DiffRhythm+, and inference-time optimization techniques like Text2midi-InferAlign, we discuss how these techniques can address music's unique challenges: temporal coherence, harmonic consistency, and subjective quality assessment. We identify key research challenges including scalability to long-form compositions, reliability amongst others in preference modelling. Looking forward, we envision preference-aligned music generation enabling transformative applications in interactive composition tools and personalized music services. This work calls for sustained interdisciplinary research combining advances in machine learning, music-theory to create music AI systems that truly serve human creative and experiential needs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼éŸ³ä¹ AI åœ¨ä¿çœŸåº¦å’Œé£æ ¼å¤šæ ·æ€§å–å¾—è¿›å±•åä»éš¾ä»¥å¯¹é½äººç±»å¾®å¦™åå¥½çš„é—®é¢˜ï¼Œä¸»å¼ ç³»ç»Ÿåœ°åº”ç”¨åå¥½å¯¹é½ (Preference Alignment) æŠ€æœ¯ã€‚è®ºæ–‡æ¢è®¨äº†åŒ…æ‹¬ MusicRL çš„å¤§è§„æ¨¡åå¥½å­¦ä¹ ã€DiffRhythm+ çš„æ‰©æ•£åå¥½ä¼˜åŒ– (Diffusion-based Preference Optimization) ä»¥åŠ Text2midi-InferAlign æ¨ç†æ—¶ä¼˜åŒ– (Inference-time Optimization) åœ¨å†…çš„å‰æ²¿æ–¹æ³•ã€‚è¿™äº›æŠ€æœ¯æ—¨åœ¨è§£å†³éŸ³ä¹ç”Ÿæˆä¸­ç‰¹æœ‰çš„æ—¶é—´è¿è´¯æ€§ (Temporal Coherence)ã€å’Œå£°ä¸€è‡´æ€§ (Harmonic Consistency) å’Œä¸»è§‚è´¨é‡è¯„ä¼°ç­‰æŒ‘æˆ˜ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯†åˆ«äº†é•¿ç¯‡ä½œå“çš„å¯æ‰©å±•æ€§ (Scalability) ä¸åå¥½å»ºæ¨¡å¯é æ€§ç­‰æ ¸å¿ƒéš¾ç‚¹ã€‚æœ€åï¼Œè¯¥å·¥ä½œå±•æœ›äº†åå¥½å¯¹é½æŠ€æœ¯åœ¨äº¤äº’å¼ä½œæ›²å·¥å…·å’Œä¸ªæ€§åŒ–æœåŠ¡ä¸­çš„åº”ç”¨å‰æ™¯ï¼Œå‘¼åé€šè¿‡è·¨å­¦ç§‘ç ”ç©¶æ„å»ºçœŸæ­£ç¬¦åˆäººç±»åˆ›æ„ä¸ä½“éªŒéœ€æ±‚çš„éŸ³ä¹ AI ç³»ç»Ÿã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at the AAAI-2026 Senior Member Track",
      "pdf_url": "https://arxiv.org/pdf/2511.15038v1",
      "published_date": "2025-11-19 02:12:27 UTC",
      "updated_date": "2025-11-19 02:12:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:07:58.368981+00:00"
    },
    {
      "arxiv_id": "2511.15032v1",
      "title": "Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment",
      "title_zh": "åŠ¨æ€ã€éƒ¨åˆ†å¯è§‚æµ‹æ—¶é—´åºåˆ—ç¯å¢ƒä¸­çš„æ¨¡æ‹Ÿäººç±»å­¦ä¹ ",
      "authors": [
        "Jeffrey Jiang",
        "Kevin Hong",
        "Emily Kuczynski",
        "Gregory Pottie"
      ],
      "abstract": "While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåŠ¨æ€çš„æ—¶é—´åºåˆ—(Time-Series)ä»¿çœŸç¯å¢ƒï¼Œç”¨äºæ¨¡æ‹ŸåŒ…å«è¾…å¯¼ã€è®²åº§å’Œè€ƒè¯•ç­‰å¸ˆç”Ÿå¹²é¢„æªæ–½çš„è¯¾å ‚è®¾ç½®ï¼Œæ—¨åœ¨è§£å†³æ•™è‚²è¿‡ç¨‹ä¸­å­¦ç”Ÿå­¦ä¹ çŠ¶æ€ä»…éƒ¨åˆ†å¯è§‚æµ‹(Partially Observable)çš„éš¾é¢˜ã€‚ç ”ç©¶é‡ç‚¹è®¾è®¡äº†å¯å˜å¼ºåº¦çš„æ¢æµ‹æ€§å¹²é¢„(Probing Interventions)æœºåˆ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ™ºèƒ½å¯¼å¸ˆç³»ç»Ÿ(ITSs)ç»“åˆä¸ªä½“çŠ¶æ€è¯„ä¼°ä¸ç¾¤ä½“ä¿¡æ¯ï¼Œåœ¨è·å–å‡†ç¡®å­¦ç”Ÿä¼°è®¡å’Œå‡å°‘å¯¹å­¦ä¹ å¹²æ‰°ä¹‹é—´å¯»æ±‚å¹³è¡¡ã€‚å®éªŒå¯¹æ¯”äº†æ ‡å‡†å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸å¤šç§è´ªå©ªå¯å‘å¼è§„åˆ™(Heuristic Approaches)çš„æ•ˆæœï¼Œå‘ç°ä¸¤è€…æä¾›äº†ä¸åŒä½†ç»“æœç›¸ä¼¼çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼ºè°ƒäº†æ¢æµ‹æ€§å¹²é¢„åœ¨å¤„ç†é«˜åº¦éšè—ä¿¡æ¯æ—¶çš„å…³é”®æå‡ä½œç”¨ã€‚ç ”ç©¶å±•ç¤ºäº†ç­–ç•¥åœ¨åº”å¯¹ä¸åŒå­¦ç”Ÿç¾¤ä½“åˆ†å¸ƒæ—¶çš„çµæ´»æ€§ï¼Œä½†æŒ‡å‡ºå¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨è¾…åŠ©è¡¨ç°è¾ƒå·®çš„ç­çº§æ—¶ä»å­˜åœ¨å›°éš¾ã€‚æ­¤å¤–ï¼Œå¯¹ä¸åŒè¯¾ç¨‹ç»“æ„çš„æµ‹è¯•è¡¨æ˜ï¼Œç›¸æ¯”äºä»…æœ‰æœŸæœ«è€ƒè¯•çš„ç»“æ„ï¼Œç›¸å…³ç­–ç•¥åœ¨åŒ…å«å°æµ‹éªŒå’ŒæœŸä¸­è€ƒè¯•çš„ç»“æ„ä¸­æ›´èƒ½æ˜¾è‘—æå‡è¡¨ç°ï¼Œè¿›ä¸€æ­¥å‡¸æ˜¾äº†è·å–é¢å¤–ä¿¡æ¯å¯¹ä¼˜åŒ–æ•™å­¦å†³ç­–çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "Manuscript in preparation for IEEE Transactions on Education, 20 pages, 6 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.15032v1",
      "published_date": "2025-11-19 01:57:52 UTC",
      "updated_date": "2025-11-19 01:57:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:08:20.174994+00:00"
    },
    {
      "arxiv_id": "2511.15015v2",
      "title": "Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference",
      "title_zh": "é¢å‘å¯æ‰©å±•æ··åˆä¸“å®¶æ¨¡å‹æ¨ç†çš„åŠ¨æ€ä¸“å®¶é‡åŒ–",
      "authors": [
        "Kexin Chu",
        "Dawei Xiang",
        "Zixu Shen",
        "Yiwei Yang",
        "Zecheng Liu",
        "Wei Zhang"
      ],
      "abstract": "Mixture-of-Experts (MoE) models scale LLM capacity efficiently, but deployment on consumer GPUs is limited by the large memory footprint of inactive experts. Static post-training quantization reduces storage costs but cannot adapt to shifting activation patterns, causing accuracy loss under aggressive compression. So we present DynaExq, a runtime system that treats expert precision as a first-class, dynamically managed resource. DynaExq combines (1) a hotness-aware precision controller that continuously aligns expert bit-widths with long-term activation statistics, (2) a fully asynchronous precision-switching pipeline that overlaps promotion and demotion with MoE computation, and (3) a fragmentation-free memory pooling mechanism that supports hybrid-precision experts with deterministic allocation. Together, these components enable stable, non-blocking precision transitions under strict HBM budgets.\n  Across Qwen3-30B and Qwen3-80B MoE models and six representative benchmarks, DynaExq deploys large LLMs on single RTX 5090 and A6000 GPUs and improves accuracy by up to 4.03 points over static low-precision baselines. The results show that adaptive, workload-aware quantization is an effective strategy for memory-constrained MoE serving.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DynaExqï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³Mixture-of-Experts (MoE)æ¨¡å‹åœ¨æ¶ˆè´¹çº§GPUéƒ¨ç½²æ—¶å› éæ´»è·ƒä¸“å®¶å ç”¨è¿‡å¤šæ˜¾å­˜è€Œå¯¼è‡´å†…å­˜å—é™é—®é¢˜çš„è¿è¡Œæ—¶ç³»ç»Ÿã€‚DynaExqå°†ä¸“å®¶ç²¾åº¦è§†ä¸ºåŠ¨æ€ç®¡ç†çš„èµ„æºï¼Œå…¶æ ¸å¿ƒç»“åˆäº†æ ¹æ®æ¿€æ´»ç»Ÿè®¡è°ƒæ•´ä¸“å®¶ä½å®½çš„çƒ­åº¦æ„ŸçŸ¥ç²¾åº¦æ§åˆ¶å™¨(hotness-aware precision controller)ã€å¯é‡å è®¡ç®—ä¸ç²¾åº¦åˆ‡æ¢çš„å¼‚æ­¥æµæ°´çº¿ï¼Œä»¥åŠæ”¯æŒæ··åˆç²¾åº¦ä¸“å®¶çš„æ— ç¢ç‰‡å†…å­˜æ± æœºåˆ¶ã€‚è¿™äº›ç»„ä»¶ååŒå·¥ä½œï¼Œç¡®ä¿åœ¨ä¸¥æ ¼çš„HBMé¢„ç®—ä¸‹å®ç°ç¨³å®šä¸”éé˜»å¡çš„ç²¾åº¦è½¬æ¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDynaExqèƒ½åœ¨å•ä¸ªRTX 5090æˆ–A6000 GPUä¸ŠæˆåŠŸéƒ¨ç½²å¤§å‹Qwen3-30Bå’ŒQwen3-80B MoEæ¨¡å‹ã€‚åœ¨å…­é¡¹ä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ¯”é™æ€Post-training QuantizationåŸºå‡†æå‡äº†é«˜è¾¾4.03ä¸ªç™¾åˆ†ç‚¹çš„å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è‡ªé€‚åº”è´Ÿè½½æ„ŸçŸ¥é‡åŒ–æ˜¯è§£å†³å†…å­˜å—é™ç¯å¢ƒä¸‹é«˜æ•ˆMoEæ¨ç†æœåŠ¡çš„æœ‰æ•ˆç­–ç•¥ã€‚",
      "categories": [
        "cs.PF",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.PF",
      "comment": "7 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.15015v2",
      "published_date": "2025-11-19 01:27:54 UTC",
      "updated_date": "2025-11-24 00:36:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:08:28.266920+00:00"
    },
    {
      "arxiv_id": "2511.15005v1",
      "title": "Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¹»è§‰åŠ¨åŠ›å­¦çš„æ•°å­¦åˆ†æï¼šä¸ç¡®å®šæ€§é‡åŒ–ã€å…ˆè¿›è§£ç ä¸è§„èŒƒåŒ–ç¼“è§£",
      "authors": [
        "Moses Kiprono"
      ],
      "abstract": "Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰ä¸­çš„å¹»è§‰ï¼ˆHallucinationï¼‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºæ•°å­¦åˆ†æçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨ä»ç†è®ºå±‚é¢ç†è§£ã€è¡¡é‡å¹¶ç¼“è§£äº‹å®æ€§é”™è¯¯ã€‚é€šè¿‡ç»“åˆæ¦‚ç‡å»ºæ¨¡ï¼ˆProbabilistic modelingï¼‰ã€ä¿¡æ¯è®ºï¼ˆInformation theoryï¼‰ã€ä¸‰è§’ä¿¡å·åˆ†æï¼ˆTrigonometric signal analysisï¼‰å’Œè´å¶æ–¯ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆBayesian uncertainty estimationï¼‰ï¼Œè®ºæ–‡è¯¦ç»†åˆ†æäº†è¯¯å·®åœ¨è‡ªå›å½’ï¼ˆAutoregressiveï¼‰è¿‡ç¨‹ä¸­å¦‚ä½•å¤åˆç´¯ç§¯ã€‚ç ”ç©¶è€…è¿›ä¸€æ­¥è®¾è®¡äº†åŒ…æ‹¬è¯­ä¹‰æ„ŸçŸ¥ï¼ˆSemantic-awareï¼‰å’Œç›¸ä½æ„ŸçŸ¥ï¼ˆPhase-awareï¼‰åœ¨å†…çš„å¤šç§é«˜çº§ä¸ç¡®å®šæ€§åº¦é‡æŒ‡æ ‡ï¼Œå¹¶å¼€å‘äº†å¯¹æ¯”è§£ç ï¼ˆContrastive decodingï¼‰ã€æ£€ç´¢å¢å¼ºï¼ˆRetrieval-augmented groundingï¼‰å’Œäº‹å®å¯¹é½ï¼ˆFactual alignmentï¼‰ç­‰ç¼“è§£ç­–ç•¥ã€‚è¯¥å·¥ä½œæˆåŠŸå°†æ¨¡å‹æ ¡å‡†ï¼ˆCalibrationï¼‰ã€æ£€ç´¢ä¸å¯¹é½æŠ€æœ¯æœ‰æœºç»“åˆï¼Œä¸ºæå‡ LLMs çš„å®‰å…¨æ€§ä¸å¯é æ€§å¥ å®šäº†åšå®çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, theoretical/mathematical LLM research, no figures, intended for peer-reviewed journal",
      "pdf_url": "https://arxiv.org/pdf/2511.15005v1",
      "published_date": "2025-11-19 00:58:36 UTC",
      "updated_date": "2025-11-19 00:58:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:08:52.278556+00:00"
    },
    {
      "arxiv_id": "2511.15002v1",
      "title": "Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning",
      "title_zh": "åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„é”åº¦æ„ŸçŸ¥ O-RAN ä»»åŠ¡ç‰¹å®šèµ„æºç®¡ç†",
      "authors": [
        "Fatemeh Lotfi",
        "Hossein Rajoli",
        "Fatemeh Afghah"
      ],
      "abstract": "Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $Ï$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸‹ä¸€ä»£ç½‘ç»œä¸­Open Radio Access Network (O-RAN) çš„åŠ¨æ€èµ„æºç®¡ç†æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªåœ¨åˆ†å¸ƒå¼Multi-Agent Reinforcement Learning (MARL) æ¡†æ¶ä¸‹ç»“åˆSoft Actor Critic (SAC) ä¸Sharpness-Aware Minimization (SAM) çš„åˆ›æ–°æ–¹æ³•ã€‚é’ˆå¯¹Deep Reinforcement Learning (DRL) æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†è‡ªé€‚åº”ä¸”æœ‰é€‰æ‹©æ€§çš„SAMæœºåˆ¶ï¼Œé€šè¿‡Temporal-Difference (TD)-erroræ–¹å·®æ˜¾å¼é©±åŠ¨æ­£åˆ™åŒ–ï¼Œä»…å¯¹é¢ä¸´é«˜ç¯å¢ƒå¤æ‚åº¦çš„æ™ºèƒ½ä½“è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œåœ¨ä¸ç‰ºç‰²å­¦ä¹ æ•ˆç‡çš„å‰æä¸‹é™ä½äº†é¢å¤–å¼€é”€å¹¶æå‡äº†è®­ç»ƒç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡åŠ¨æ€ $\\rho$ è°ƒåº¦æ–¹æ¡ˆç²¾ç»†åŒ–äº†æ™ºèƒ½ä½“é—´çš„æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨èµ„æºåˆ†é…æ•ˆç‡ä¸Šæ¯”ä¼ ç»ŸDRLæ–¹æ³•æå‡äº†é«˜è¾¾22%ï¼Œå¹¶ç¡®ä¿äº†å¤šæ ·åŒ–O-RANåˆ‡ç‰‡ä¸­ä¼˜è¶Šçš„Quality of Service (QoS) æ»¡æ„åº¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to be published in IEEE Transaction on Machine Learning in Communication and Networking (TMLCN)",
      "pdf_url": "https://arxiv.org/pdf/2511.15002v1",
      "published_date": "2025-11-19 00:55:24 UTC",
      "updated_date": "2025-11-19 00:55:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:08:27.070211+00:00"
    },
    {
      "arxiv_id": "2512.08943v1",
      "title": "Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models",
      "title_zh": "æ£€ç´¢å¢å¼ºè¯­è¨€æ¨¡å‹ä¸­çš„æŠ—å™ªç”Ÿæˆå¼å‹ç¼©",
      "authors": [
        "Singon Kim"
      ],
      "abstract": "Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­æŠ½è±¡å‹ç¼©æŠ€æœ¯é¢ä¸´çš„æ£€ç´¢å™ªå£°é—®é¢˜ï¼Œæå‡ºäº†ACoRNæ¡†æ¶ä»¥å¢å¼ºå‹ç¼©å™¨çš„é²æ£’æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç”±äºæ£€ç´¢åˆ°çš„æ–‡æ¡£å¸¸åŒ…å«æ— å…³æˆ–è¯¯å¯¼æ€§ä¿¡æ¯ï¼Œå‹ç¼©å™¨åœ¨é•¿ä¸Šä¸‹æ–‡ä¸‹å®¹æ˜“å› æ³¨æ„åŠ›åˆ†æ•£è€Œé—æ¼æ ¸å¿ƒå†…å®¹ã€‚ACoRNå¼•å…¥äº†ä¸¤ä¸ªåˆ›æ–°çš„è®­ç»ƒæ­¥éª¤ï¼šé¦–å…ˆåˆ©ç”¨ç¦»çº¿æ•°æ®å¢å¼º(Data Augmentation)æå‡æ¨¡å‹å¯¹æŠ—ä¸åŒæ£€ç´¢å™ªå£°çš„èƒ½åŠ›ï¼›å…¶æ¬¡é€šè¿‡å¾®è°ƒ(Finetuning)å¼•å¯¼æ¨¡å‹ç”Ÿæˆå›´ç»•æ”¯æŒæ­£ç¡®ç­”æ¡ˆçš„å…³é”®ä¿¡æ¯çš„æ‘˜è¦ï¼Œä»è€Œå…‹æœä½ç½®åè§(Positional Bias)ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨ACoRNè®­ç»ƒçš„T5-largeå‹ç¼©å™¨åœ¨EMå’ŒF1è¯„åˆ†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½æœ‰æ•ˆä¿ç•™ä½œä¸ºè¯æ®çš„ç­”æ¡ˆå­—ç¬¦ä¸²ã€‚è¯¥æ–¹æ³•åœ¨å¤„ç†åŒ…å«å¤§é‡å¹²æ‰°æ–‡æ¡£çš„æ•°æ®é›†æ—¶å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºçœŸå®ä¸–ç•Œä¸­é«˜å™ªå£°ç¯å¢ƒä¸‹çš„RAGåº”ç”¨æä¾›äº†æœ‰æ•ˆæ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Master's thesis, Korea University, 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.08943v1",
      "published_date": "2025-11-19 00:51:27 UTC",
      "updated_date": "2025-11-19 00:51:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:08:47.266881+00:00"
    },
    {
      "arxiv_id": "2511.14993v2",
      "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
      "title_zh": "Kandinsky 5.0ï¼šå›¾åƒä¸è§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹ç³»åˆ—",
      "authors": [
        "Vladimir Arkhipkin",
        "Vladimir Korviakov",
        "Nikolai Gerasimenko",
        "Denis Parkhomenko",
        "Viacheslav Vasilev",
        "Alexey Letunovskiy",
        "Nikolai Vaulin",
        "Maria Kovaleva",
        "Ivan Kirillov",
        "Lev Novitskiy",
        "Denis Koposov",
        "Nikita Kiselev",
        "Alexander Varlamov",
        "Dmitrii Mikhailov",
        "Vladimir Polovnikov",
        "Andrey Shutkin",
        "Julia Agafonova",
        "Ilya Vasiliev",
        "Anastasiia Kargapoltseva",
        "Anna Dmitrienko",
        "Anastasia Maltseva",
        "Anna Averchenkova",
        "Olga Kim",
        "Tatiana Nikulina",
        "Denis Dimitrov"
      ],
      "abstract": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
      "tldr_zh": "è¯¥æŠ¥å‘Šä»‹ç»äº†Kandinsky 5.0ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒå’Œ10ç§’è§†é¢‘åˆæˆçš„æœ€å…ˆè¿›åŸºç¡€æ¨¡å‹å®¶æ—ã€‚è¯¥å®¶æ—ç”±ä¸‰ä¸ªæ ¸å¿ƒç³»åˆ—ç»„æˆï¼š6Bå‚æ•°çš„å›¾åƒç”Ÿæˆæ¨¡å‹Kandinsky 5.0 Image Liteï¼Œè½»é‡çº§çš„2Bå‚æ•°è§†é¢‘ç”Ÿæˆæ¨¡å‹Kandinsky 5.0 Video Liteï¼Œä»¥åŠæ—¨åœ¨å®ç°å“è¶Šç”Ÿæˆè´¨é‡çš„19Bå‚æ•°æ¨¡å‹Kandinsky 5.0 Video Proã€‚ç ”ç©¶è¯¦ç»†é˜è¿°äº†æ¶µç›–å¤šé˜¶æ®µè®­ç»ƒç®¡é“çš„æ•°æ®ç®¡ç†ç”Ÿå‘½å‘¨æœŸï¼Œé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå¹¶ç»“åˆè‡ªæˆ‘ç›‘ç£å¾®è°ƒ(SFT)å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ (RL)çš„åæœŸè®­ç»ƒæŠ€æœ¯æ¥æå‡æ¨¡å‹è´¨é‡ã€‚é€šè¿‡åœ¨æ¶æ„ã€è®­ç»ƒå’Œæ¨ç†æ–¹é¢çš„åˆ›æ–°ä¼˜åŒ–ï¼ŒKandinsky 5.0åœ¨ä¿è¯é«˜ç”Ÿæˆé€Ÿåº¦çš„åŒæ—¶ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¾¾åˆ°äº†state-of-the-artæ€§èƒ½ã€‚äººç±»è¯„ä¼°(human evaluation)ç»“æœè¯æ˜äº†è¯¥æ¡†æ¶çš„ä¼˜è¶Šæ€§ï¼Œç›®å‰è¯¥é¡¹ç›®çš„ä»£ç å’Œè®­ç»ƒæƒé‡å·²å¼€æºï¼Œæ—¨åœ¨æ¨åŠ¨é«˜è´¨é‡ç”Ÿæˆå¼æ¨¡å‹åœ¨ç ”ç©¶ç¤¾åŒºçš„æ™®åŠä¸å‘å±•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Website: https://kandinskylab.ai/",
      "pdf_url": "https://arxiv.org/pdf/2511.14993v2",
      "published_date": "2025-11-19 00:23:22 UTC",
      "updated_date": "2025-11-20 13:09:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:09:10.967637+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 116,
  "processed_papers_count": 116,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T08:19:01.368495+00:00"
}
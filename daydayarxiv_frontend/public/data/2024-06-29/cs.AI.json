{
  "date": "2024-06-29",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-06-29 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 更新了 56 篇论文，主要聚焦 AI 在医疗、交通、金融和计算优化中的应用，强调大语言模型 (LLM) 的可靠性、解释性和开源工具发展，令人印象深刻的是第 2 篇关于 LLM 在临床问答中的实证评估（Nigam H. Shah 等学者参与），以及第 23 篇 SpeechBrain 1.0 的开源框架发布，这些论文展示了 AI 向实际应用落地的潜力。\n\n下面，我将挑选并简要讨论部分重要或相关论文，先优先聊那些涉及 LLM 优化、医疗应用和计算方法的文章（因篇幅有限，对其他较常规或非核心论文快速掠过）。每篇论文会列出标题（中文 + 英文），并保留核心学术术语，聚焦主要贡献和发现。\n\n### 重点论文讨论\n\n**2. Answering real-world clinical questions using large language model based systems（回答真实世界临床问题的大型语言模型系统）**  \n这篇论文评估了 5 个 LLM 系统（如 ChatGPT-4 和检索增强生成模型）在回答 50 个临床问题的表现。贡献：通过 RAG（Retrieval-Augmented Generation）和代理式 LLM（如 ChatRWD），实现了 24% 到 58% 的相关性提升，显著提高了证据总结和生成新证据的能力。发现：通用 LLM 易产生幻觉，而专用系统更适合临床应用，具有重要含义，能提升医疗决策的准确性和可操作性。\n\n**23. Open-Source Conversational AI with SpeechBrain 1.0（开源对话式 AI：SpeechBrain 1.0）**  \n由多位作者如 Mirco Ravanelli 和 Yannick Esteve 领导，这篇论文介绍了 SpeechBrain 1.0 工具包。贡献：提供超过 200 个语音处理配方和 100 个 Hugging Face 模型，支持 LLM 集成和新任务模态。发现：它在语音识别和增强方面表现出色，促进了 AI 社区的透明性和可复现性，是一个高效的开源基准。\n\n**15. ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees（ConU：大语言模型中具有正确性覆盖保证的保形不确定性）**  \n论文提出了一种基于保形预测（Conformal Prediction）的 LLM 不确定性量化方法。贡献：结合自一致性理论，实现了对 NLG（Natural Language Generation）任务的严格控制，提高了正确性覆盖率。发现：在 4 个数据集上，方法显著降低了幻觉风险，适用于医疗等高可靠性场景，提升了 LLM 的可信度。\n\n**4. H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables（H-STAR：LLM 驱动的混合 SQL-文本自适应表推理）**  \n这篇论文（NAACL 2025）引入 H-STAR 算法，用于表结构数据查询。贡献：结合符号和语义推理，实现步进式表提取和自适应策略。发现：在 3 个数据集上，H-STAR 超过了现有方法，显著提高了表格 QA 的准确性和效率，尤其在量化任务中。\n\n**3. Privacy-Preserving and Trustworthy Deep Learning for Medical Imaging（隐私保护和可信深度学习用于医学成像）**  \n作者 Kiarash Sedghighadikolaei 和 Attila A. Yavuz 探讨了深度辐射学（Deep Radiomics）的隐私问题。贡献：分类了隐私增强技术（PETs），并提出混合 PETs 框架，用于医学图像管道的安全集成。发现：优化后的 PETs 提高了效率和准确性，适用于外包场景，填补了医疗 AI 隐私研究空白。\n\n**5. Interpreting Pretrained Speech Models for Automatic Speech Assessment of Voice Disorders（解释预训练语音模型用于语音障碍的自动评估）**  \n论文使用 Audio Spectrogram Transformer 分析语音模型。贡献：通过注意力展开方法生成相关性映射，解释模型在语音障碍检测中的决策过程。发现：微调后，模型注意力更集中于特定音素区域，提升了语音健康评估的可解释性。\n\n**2. Deep Frequency Derivative Learning for Non-stationary Time Series Forecasting（非平稳时间序列预测的深度频率导数学习）**  \n这篇论文（IJCAI 2024）提出 DERITS 框架。贡献：使用频率导数变换和自适应卷积网络，处理分布偏移问题。发现：在多个数据集上，方法显著提高了预测准确性和鲁棒性，适用于金融和科学领域。\n\n**17. PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models（PFME：大语言模型细粒度幻觉检测和编辑的模块化方法）**  \n论文开发了 PFME 框架，用于检测和修正 LLM 幻觉。贡献：结合事实检索和细粒度编辑模块，提升了响应相关性。发现：在 FavaBench 上，与 ChatGPT 相比，性能提升 8.7%，并在编辑任务中提高了 FActScore，强化了 LLM 的可靠性。\n\n**10. Leveraging Ontologies to Document Bias in Data（利用本体记录数据中的偏差）**  \n作者 Mayra Russo 和 Maria-Esther Vidal 构建了 Doc-BiasO 本体。贡献：整合偏差术语和关系，提供正式框架记录 AI 中的偏差。发现：提升了偏差解释和缓解的可互操作性，适用于公平 AI 研究。\n\n### 其他论文快速掠过\n剩余论文中，如第 6（Deep Reinforcement Learning Strategies in Finance，贡献：分析 DRL 在金融交易中的行为模式）、第 7（A Medical Low-Back Pain Physical Rehabilitation Dataset，贡献：提供康复数据集用于运动分析）、第 8（Test Case Features as Hyper-heuristics for Inductive Programming，贡献：使用测试案例优化编程搜索空间）等，涉及金融优化、数据集构建和算法改进，但相对常规，我仅简要提及它们提供了实用工具或基准，而未深入讨论，以控制篇幅。\n\n总之，今天的 arXiv 论文展示了 AI 领域的多样创新，特别是 LLM 在医疗和计算中的潜力。感兴趣的读者可关注上述关键论文，探索实际应用和开源资源。明天见！",
  "papers": [
    {
      "arxiv_id": "2407.13775v1",
      "title": "Lessons in Cooperation: A Qualitative Analysis of Driver Sentiments towards Real-Time Advisory Systems from a Driving Simulator User Study",
      "title_zh": "翻译失败",
      "authors": [
        "Aamir Hasan",
        "Neeloy Chakraborty",
        "Haonan Chen",
        "Cathy Wu",
        "Katherine Driggs-Campbell"
      ],
      "abstract": "Real-time Advisory (RTA) systems, such as navigational and eco-driving\nassistants, are becoming increasingly ubiquitous in vehicles due to their\nbenefits for users and society. Until autonomous vehicles mature, such advisory\nsystems will continue to expand their ability to cooperate with drivers,\nenabling safer and more eco-friendly driving practices while improving user\nexperience. However, the interactions between these systems and drivers have\nnot been studied extensively. To this end, we conduct a driving simulator study\n(N=16) to capture driver reactions to a Cooperative RTA system. Through a case\nstudy with a congestion mitigation assistant, we qualitatively analyze the\nsentiments of drivers towards advisory systems and discuss driver preferences\nfor various aspects of the interaction. We comment on how the advice should be\ncommunicated, the effects of the advice on driver trust, and how drivers adapt\nto the system. We present recommendations to inform the future design of\nCooperative RTA systems.",
      "tldr_zh": "本研究通过一个驾驶模拟器用户研究（N=16），对司机对实时咨询（RTA）系统的情感进行定性分析，聚焦于一个拥堵缓解助手的案例。研究探讨了司机对RTA系统交互的偏好，包括建议的沟通方式、对司机信任的影响，以及司机适应系统的过程。结果显示，RTA系统可提升安全和环保驾驶，但需优化设计以增强信任；作者据此提出推荐，以指导未来合作RTA系统的开发。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.13775v1",
      "published_date": "2024-06-29 23:21:42 UTC",
      "updated_date": "2024-06-29 23:21:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:55:08.978788"
    },
    {
      "arxiv_id": "2407.00541v1",
      "title": "Answering real-world clinical questions using large language model based systems",
      "title_zh": "使用基于大型语言模型的系统回答真实世界的临床问题",
      "authors": [
        "Yen Sia Low",
        "Michael L. Jackson",
        "Rebecca J. Hyde",
        "Robert E. Brown",
        "Neil M. Sanghavi",
        "Julian D. Baldwin",
        "C. William Pike",
        "Jananee Muralidharan",
        "Gavin Hui",
        "Natasha Alexander",
        "Hadeel Hassan",
        "Rahul V. Nene",
        "Morgan Pike",
        "Courtney J. Pokrzywa",
        "Shivam Vedak",
        "Adam Paul Yan",
        "Dong-han Yao",
        "Amy R. Zipursky",
        "Christina Dinh",
        "Philip Ballentine",
        "Dan C. Derieg",
        "Vladimir Polony",
        "Rehan N. Chawdry",
        "Jordan Davies",
        "Brigham B. Hyde",
        "Nigam H. Shah",
        "Saurabh Gombar"
      ],
      "abstract": "Evidence to guide healthcare decisions is often limited by a lack of relevant\nand trustworthy literature as well as difficulty in contextualizing existing\nresearch for a specific patient. Large language models (LLMs) could potentially\naddress both challenges by either summarizing published literature or\ngenerating new studies based on real-world data (RWD). We evaluated the ability\nof five LLM-based systems in answering 50 clinical questions and had nine\nindependent physicians review the responses for relevance, reliability, and\nactionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus,\nGemini Pro 1.5) rarely produced answers that were deemed relevant and\nevidence-based (2% - 10%). In contrast, retrieval augmented generation\n(RAG)-based and agentic LLM systems produced relevant and evidence-based\nanswers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. Only the agentic\nChatRWD was able to answer novel questions compared to other LLMs (65% vs.\n0-9%). These results suggest that while general-purpose LLMs should not be used\nas-is, a purpose-built system for evidence summarization based on RAG and one\nfor generating novel evidence working synergistically would improve\navailability of pertinent evidence for patient care.",
      "tldr_zh": "本文评估了五种大型语言模型(LLMs)系统在回答50个真实临床问题中的性能，发现通用LLMs（如ChatGPT-4、Claude 3 Opus和Gemini Pro 1.5）仅产生少量相关且基于证据的答案（2%-10%）。相比之下，检索增强生成(RAG)-based系统和agentic系统（如OpenEvidence和ChatRWD）表现更好，前者达到24%的相关率，而ChatRWD高达58%，且仅ChatRWD能有效处理新颖问题（65% vs. 0-9%）。研究结论是，通用LLMs不宜直接用于医疗决策，而是应开发基于RAG的证据总结系统和生成真实世界数据(RWD)新证据的系统，以协同提升患者护理的证据可用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "28 pages (2 figures, 3 tables) inclusive of 8 pages of supplemental\n  materials (4 supplemental figures and 4 supplemental tables)",
      "pdf_url": "http://arxiv.org/pdf/2407.00541v1",
      "published_date": "2024-06-29 22:39:20 UTC",
      "updated_date": "2024-06-29 22:39:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:55:22.432689"
    },
    {
      "arxiv_id": "2407.00538v1",
      "title": "Privacy-Preserving and Trustworthy Deep Learning for Medical Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Kiarash Sedghighadikolaei",
        "Attila A Yavuz"
      ],
      "abstract": "The shift towards efficient and automated data analysis through Machine\nLearning (ML) has notably impacted healthcare systems, particularly Radiomics.\nRadiomics leverages ML to analyze medical images accurately and efficiently for\nprecision medicine. Current methods rely on Deep Learning (DL) to improve\nperformance and accuracy (Deep Radiomics). Given the sensitivity of medical\nimages, ensuring privacy throughout the Deep Radiomics pipeline-from data\ngeneration and collection to model training and inference-is essential,\nespecially when outsourced. Thus, Privacy-Enhancing Technologies (PETs) are\ncrucial tools for Deep Radiomics. Previous studies and systematization efforts\nhave either broadly overviewed PETs and their applications or mainly focused on\nsubsets of PETs for ML algorithms. In Deep Radiomics, where efficiency,\naccuracy, and privacy are crucial, many PETs, while theoretically applicable,\nmay not be practical without specialized optimizations or hybrid designs.\nAdditionally, not all DL models are suitable for Radiomics. Consequently, there\nis a need for specialized studies that investigate and systematize the\neffective and practical integration of PETs into the Deep Radiomics pipeline.\nThis work addresses this research gap by (1) classifying existing PETs,\npresenting practical hybrid PETS constructions, and a taxonomy illustrating\ntheir potential integration with the Deep Radiomics pipeline, with comparative\nanalyses detailing assumptions, architectural suitability, and security, (2)\nOffering technical insights, describing potential challenges and means of\ncombining PETs into the Deep Radiomics pipeline, including integration\nstrategies, subtilities, and potential challenges, (3) Proposing potential\nresearch directions, identifying challenges, and suggesting solutions to\nenhance the PETs in Deep Radiomics.",
      "tldr_zh": "这篇论文探讨了在医疗影像分析中应用深度学习（Deep Learning）进行放射组学（Radiomics）的隐私保护问题，强调了隐私增强技术（PETs）在确保数据生成、收集、模型训练和推理过程中的隐私的重要性。论文的主要贡献包括：分类现有PETs、提出实用混合PETs构建和分类学，并分析其与Deep Radiomics管道的整合策略，包括假设、架构适用性和安全性挑战。最终，它提供了技术见解、潜在整合难题的解决方案，并建议未来研究方向，以提升Deep Radiomics的效率、准确性和可信度。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CR",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2407.00538v1",
      "published_date": "2024-06-29 22:26:05 UTC",
      "updated_date": "2024-06-29 22:26:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:55:32.734777"
    },
    {
      "arxiv_id": "2407.05952v3",
      "title": "H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables",
      "title_zh": "翻译失败",
      "authors": [
        "Nikhil Abhyankar",
        "Vivek Gupta",
        "Dan Roth",
        "Chandan K. Reddy"
      ],
      "abstract": "Tabular reasoning involves interpreting natural language queries about\ntabular data, which presents a unique challenge of combining language\nunderstanding with structured data analysis. Existing methods employ either\ntextual reasoning, which excels in semantic interpretation but struggles with\nmathematical operations, or symbolic reasoning, which handles computations well\nbut lacks semantic understanding. This paper introduces a novel algorithm\nH-STAR that integrates both symbolic and semantic (textual) approaches in a\ntwo-stage process to address these limitations. H-STAR employs: (1) step-wise\ntable extraction using `multi-view' column retrieval followed by row\nextraction, and (2) adaptive reasoning that adapts reasoning strategies based\non question types, utilizing semantic reasoning for direct lookup and complex\nlexical queries while augmenting textual reasoning with symbolic reasoning\nsupport for quantitative and logical tasks. Our extensive experiments\ndemonstrate that H-STAR significantly outperforms state-of-the-art methods\nacross three tabular question-answering (QA) and fact-verification datasets,\nunderscoring its effectiveness and efficiency.",
      "tldr_zh": "本论文提出 H-STAR，一种由大语言模型(LLM)驱动的混合 SQL-Text 自适应推理算法，用于解决表格推理中的挑战，该算法整合符号(symbolic)推理和语义(textual)推理，以弥补现有方法的局限性。H-STAR 通过两阶段过程实现：首先进行步进式表格提取，包括 'multi-view' 列检索和行提取；其次采用自适应推理，根据问题类型选择策略，例如使用语义推理处理直接查找和复杂词汇查询，或通过符号推理增强文本推理来应对定量和逻辑任务。实验结果显示，H-STAR 在三个表格问答(QA)和事实验证数据集上显著优于最先进方法，证明了其有效性和效率。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "comment": "NAACL 2025 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2407.05952v3",
      "published_date": "2024-06-29 21:24:19 UTC",
      "updated_date": "2025-04-07 00:44:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:55:44.956590"
    },
    {
      "arxiv_id": "2407.00531v1",
      "title": "Interpreting Pretrained Speech Models for Automatic Speech Assessment of Voice Disorders",
      "title_zh": "翻译失败",
      "authors": [
        "Hok-Shing Lau",
        "Mark Huntly",
        "Nathon Morgan",
        "Adesua Iyenoma",
        "Biao Zeng",
        "Tim Bashford"
      ],
      "abstract": "Speech contains information that is clinically relevant to some diseases,\nwhich has the potential to be used for health assessment. Recent work shows an\ninterest in applying deep learning algorithms, especially pretrained large\nspeech models to the applications of Automatic Speech Assessment. One question\nthat has not been explored is how these models output the results based on\ntheir inputs. In this work, we train and compare two configurations of Audio\nSpectrogram Transformer in the context of Voice Disorder Detection and apply\nthe attention rollout method to produce model relevance maps, the computed\nrelevance of the spectrogram regions when the model makes predictions. We use\nthese maps to analyse how models make predictions in different conditions and\nto show that the spread of attention is reduced as a model is finetuned, and\nthe model attention is concentrated on specific phoneme regions.",
      "tldr_zh": "本研究探讨了如何解释预训练语音模型在语音障碍检测中的决策过程，以应用于自动语音评估。研究者训练并比较了两种 Audio Spectrogram Transformer 配置，用于 Voice Disorder Detection，并采用 attention rollout 方法生成模型相关性地图，以分析模型对频谱图区域的关注度。结果显示，随着模型微调，注意力分布减少并集中在特定音素区域，这有助于理解模型在不同条件下的预测机制。该方法为预训练语音模型的可解释性提供了新见解，提升了其在健康评估中的可靠性和临床应用潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00531v1",
      "published_date": "2024-06-29 21:14:48 UTC",
      "updated_date": "2024-06-29 21:14:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:55:56.620263"
    },
    {
      "arxiv_id": "2407.09557v1",
      "title": "Deep Reinforcement Learning Strategies in Finance: Insights into Asset Holding, Trading Behavior, and Purchase Diversity",
      "title_zh": "金融领域的深度强化学习策略：对资产持有、交易行为和购买多样性的洞察",
      "authors": [
        "Alireza Mohammadshafie",
        "Akram Mirzaeinia",
        "Haseebullah Jumakhan",
        "Amir Mirzaeinia"
      ],
      "abstract": "Recent deep reinforcement learning (DRL) methods in finance show promising\noutcomes. However, there is limited research examining the behavior of these\nDRL algorithms. This paper aims to investigate their tendencies towards holding\nor trading financial assets as well as purchase diversity. By analyzing their\ntrading behaviors, we provide insights into the decision-making processes of\nDRL models in finance applications. Our findings reveal that each DRL algorithm\nexhibits unique trading patterns and strategies, with A2C emerging as the top\nperformer in terms of cumulative rewards. While PPO and SAC engage in\nsignificant trades with a limited number of stocks, DDPG and TD3 adopt a more\nbalanced approach. Furthermore, SAC and PPO tend to hold positions for shorter\ndurations, whereas DDPG, A2C, and TD3 display a propensity to remain stationary\nfor extended periods.",
      "tldr_zh": "这篇论文探讨了深度强化学习 (DRL) 在金融领域的应用，重点分析了 DRL 算法在资产持有、交易行为和购买多样性方面的表现，以揭示其决策过程。研究通过比较多种 DRL 算法的交易模式，发现 A2C 在累积奖励方面表现出色，而 PPO 和 SAC 倾向于进行大量交易但仅限于少量股票。DDPG 和 TD3 则采用更平衡的策略；此外，SAC 和 PPO 持有资产时间较短，而 DDPG、A2C 和 TD3 更倾向于长期持有。整体而言，这些发现为理解 DRL 在金融决策中的行为提供了宝贵洞见。",
      "categories": [
        "q-fin.TR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.TR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.09557v1",
      "published_date": "2024-06-29 20:56:58 UTC",
      "updated_date": "2024-06-29 20:56:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:56:09.450415"
    },
    {
      "arxiv_id": "2407.00521v2",
      "title": "A Medical Low-Back Pain Physical Rehabilitation Dataset for Human Body Movement Analysis",
      "title_zh": "一种用于人体运动分析的医学低背痛物理康复数据集",
      "authors": [
        "Sao Mai Nguyen",
        "Maxime Devanne",
        "Olivier Remy-Neris",
        "Mathieu Lempereur",
        "André Thepaut"
      ],
      "abstract": "While automatic monitoring and coaching of exercises are showing encouraging\nresults in non-medical applications, they still have limitations such as errors\nand limited use contexts. To allow the development and assessment of physical\nrehabilitation by an intelligent tutoring system, we identify in this article\nfour challenges to address and propose a medical dataset of clinical patients\ncarrying out low back-pain rehabilitation exercises. The dataset includes 3D\nKinect skeleton positions and orientations, RGB videos, 2D skeleton data, and\nmedical annotations to assess the correctness, and error classification and\nlocalisation of body part and timespan. Along this dataset, we perform a\ncomplete research path, from data collection to processing, and finally a small\nbenchmark. We evaluated on the dataset two baseline movement recognition\nalgorithms, pertaining to two different approaches: the probabilistic approach\nwith a Gaussian Mixture Model (GMM), and the deep learning approach with a\nLong-Short Term Memory (LSTM).\n  This dataset is valuable because it includes rehabilitation relevant motions\nin a clinical setting with patients in their rehabilitation program, using a\ncost-effective, portable, and convenient sensor, and because it shows the\npotential for improvement on these challenges.",
      "tldr_zh": "该论文提出一个专用于低背痛物理康复的医疗数据集，旨在支持智能辅导系统对人体运动进行分析和评估。该数据集包括临床患者康复运动的3D Kinect 骨骼位置和方向、RGB 视频、2D 骨骼数据，以及医疗注释，用于判断动作正确性、错误分类和定位身体部位及时间段。作者完成了从数据收集到处理的全流程，并对两种基线算法——Gaussian Mixture Model (GMM) 和 Long-Short Term Memory (LSTM)——进行了基准测试，展示了使用便携传感器在临床设置中改进康复监控的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "I.5.4; I.4.8"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00521v2",
      "published_date": "2024-06-29 19:50:06 UTC",
      "updated_date": "2025-01-11 02:30:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:56:22.289262"
    },
    {
      "arxiv_id": "2407.00519v1",
      "title": "Test Case Features as Hyper-heuristics for Inductive Programming",
      "title_zh": "翻译失败",
      "authors": [
        "Edward McDaid",
        "Sarah McDaid"
      ],
      "abstract": "Instruction subsets are heuristics that can reduce the size of the inductive\nprogramming search space by tens of orders of magnitude. Comprising many\noverlapping subsets of different sizes, they serve as predictions of the\ninstructions required to code a solution for any problem. Currently, this\napproach employs a single, large family of subsets meaning that some problems\ncan search thousands of subsets before a solution is found. In this paper we\nintroduce the use of test case type signatures as hyper-heuristics to select\none of many, smaller families of instruction subsets. The type signature for\nany set of test cases maps directly to a single family and smaller families\nmean that fewer subsets need to be considered for most problems. Having many\nfamilies also permits subsets to be reordered to better reflect their relative\noccurrence in human code - again reducing the search space size for many\nproblems. Overall the new approach can further reduce the size of the inductive\nprogramming search space by between 1 and 3 orders of magnitude, depending on\nthe type signature. Larger and more consistent reductions are possible through\nthe use of more sophisticated type systems. The potential use of additional\ntest case features as hyper-heuristics and some other possible future work is\nalso briefly discussed.",
      "tldr_zh": "本文提出使用测试用例类型签名作为hyper-heuristics来优化inductive programming的搜索空间，旨在通过选择多个较小的instruction subsets家族来减少子集搜索量。相比当前单一大型家族的方法，新方法能根据类型签名将搜索空间进一步缩小1到3个数量级，并通过重新排序子集以匹配人类代码的相对出现频率来提升效率。研究还讨论了采用更复杂的类型系统可能带来的更大减幅，以及未来利用更多测试用例特征作为hyper-heuristics的潜力。",
      "categories": [
        "cs.AI",
        "D.1.2; D.3.3; F.1.1; F.3.1; F.3.3; I.2.1; I.2.2; I.2.4; I.2.5;\n  I.2.8; I.5.3"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 3 figures. Accepted for 20th IFIP WG 12.5 International\n  Conference, AIAI 2024 Corfu, Greece, June 27-30, 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.00519v1",
      "published_date": "2024-06-29 19:46:11 UTC",
      "updated_date": "2024-06-29 19:46:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:56:44.140674"
    },
    {
      "arxiv_id": "2407.00510v1",
      "title": "Stochastic stem bucking using mixture density neural networks",
      "title_zh": "基于混合密度神经网络的随机树干切割",
      "authors": [
        "Simon Schmiedel"
      ],
      "abstract": "Poor bucking decisions made by forest harvesters can have a negative effect\non the products that are generated from the logs. Making the right bucking\ndecisions is not an easy task because harvesters must rely on predictions of\nthe stem profile for the part of the stems that is not yet measured. The goal\nof this project is to improve the bucking decisions made by forest harvesters\nwith a stochastic bucking method. We developed a Long Short-Term Memory (LSTM)\nneural network that predicted the parameters of a Gaussian distribution\nconditioned on the known part of the stem, enabling the creation of multiple\nsamples of stem profile predictions for the unknown part of the stem. The\nbucking decisions could then be optimized using a novel stochastic bucking\nalgorithm which used all the stem profiles generated to choose the logs to\ngenerate from the stem. The stochastic bucking algorithm was compared to two\nbenchmark models: A polynomial model that could not condition its predictions\non more than one diameter measurement, and a deterministic LSTM neural network.\nAll models were evaluated on stem profiles of four coniferous species prevalent\nin eastern Canada. In general, the best bucking decisions were taken by the\nstochastic LSTM models, demonstrating the usefulness of the method. The\nsecond-best results were mostly obtained by the deterministic LSTM model and\nthe worst results by the polynomial model, corroborating the usefulness of\nconditioning the stem curve predictions on multiple measurements.",
      "tldr_zh": "本研究针对森林收割机的伐木决策问题，提出了一种随机伐木(stochastic bucking)方法，以改善基于未知树干轮廓预测的决策准确性。研究开发了一个Long Short-Term Memory (LSTM)神经网络，用于预测Gaussian分布参数，从而根据已知树干部分生成多个轮廓样本，并通过一个新颖的随机伐木算法优化日志生成决策。实验结果显示，该方法在四个加拿大东部针叶树种的树干轮廓上优于多项式模型和确定性LSTM模型，实现了更好的决策性能，并验证了基于多个直径测量的预测有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00510v1",
      "published_date": "2024-06-29 18:44:49 UTC",
      "updated_date": "2024-06-29 18:44:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:56:46.270917"
    },
    {
      "arxiv_id": "2407.00509v2",
      "title": "Leveraging Ontologies to Document Bias in Data",
      "title_zh": "翻译失败",
      "authors": [
        "Mayra Russo",
        "Maria-Esther Vidal"
      ],
      "abstract": "Machine Learning (ML) systems are capable of reproducing and often amplifying\nundesired biases. This puts emphasis on the importance of operating under\npractices that enable the study and understanding of the intrinsic\ncharacteristics of ML pipelines, prompting the emergence of documentation\nframeworks with the idea that ``any remedy for bias starts with awareness of\nits existence''. However, a resource that can formally describe these pipelines\nin terms of biases detected is still amiss. To fill this gap, we present the\nDoc-BiasO ontology, a resource that aims to create an integrated vocabulary of\nbiases defined in the \\textit{fair-ML} literature and their measures, as well\nas to incorporate relevant terminology and the relationships between them.\nOverseeing ontology engineering best practices, we re-use existing vocabulary\non machine learning and AI, to foster knowledge sharing and interoperability\nbetween the actors concerned with its research, development, regulation, among\nothers. Overall, our main objective is to contribute towards clarifying\nexisting terminology on bias research as it rapidly expands to all areas of AI\nand to improve the interpretation of bias in data and downstream impact.",
      "tldr_zh": "该研究强调了机器学习 (Machine Learning) 系统可能放大偏见的问题，并提出通过文档框架来提高偏见意识。作者引入了 Doc-BiasO ontology，这是一个整合词汇表，用于正式描述 fair-ML 文献中的偏见及其度量，同时重用现有机器学习和 AI 术语以促进知识共享和互操作性。总体目标是澄清快速扩展的偏见研究术语，并提升对数据偏见及其下游影响的解释和理解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00509v2",
      "published_date": "2024-06-29 18:41:07 UTC",
      "updated_date": "2024-08-09 18:18:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:56:56.567558"
    },
    {
      "arxiv_id": "2407.00506v2",
      "title": "ShapG: new feature importance method based on the Shapley value",
      "title_zh": "翻译失败",
      "authors": [
        "Chi Zhao",
        "Jing Liu",
        "Elena Parilina"
      ],
      "abstract": "With wide application of Artificial Intelligence (AI), it has become\nparticularly important to make decisions of AI systems explainable and\ntransparent. In this paper, we proposed a new Explainable Artificial\nIntelligence (XAI) method called ShapG (Explanations based on Shapley value for\nGraphs) for measuring feature importance. ShapG is a model-agnostic global\nexplanation method. At the first stage, it defines an undirected graph based on\nthe dataset, where nodes represent features and edges are added based on\ncalculation of correlation coefficients between features. At the second stage,\nit calculates an approximated Shapley value by sampling the data taking into\naccount this graph structure. The sampling approach of ShapG allows to\ncalculate the importance of features efficiently, i.e. to reduce computational\ncomplexity. Comparison of ShapG with other existing XAI methods shows that it\nprovides more accurate explanations for two examined datasets. We also compared\nother XAI methods developed based on cooperative game theory with ShapG in\nrunning time, and the results show that ShapG exhibits obvious advantages in\nits running time, which further proves efficiency of ShapG. In addition,\nextensive experiments demonstrate a wide range of applicability of the ShapG\nmethod for explaining complex models. We find ShapG an important tool in\nimproving explainability and transparency of AI systems and believe it can be\nwidely used in various fields.",
      "tldr_zh": "本研究提出了一种新的可解释人工智能(XAI)方法ShapG，用于测量特征重要性，该方法基于Shapley value并适用于全局解释。ShapG首先构建一个基于数据集的无向图，其中节点代表特征，边通过相关系数计算确定；随后，通过考虑图结构的采样方法计算近似的Shapley value，从而显著降低计算复杂度。与现有XAI方法相比，实验显示ShapG在两个数据集上提供更准确的解释，同时在运行时间上表现出明显优势，并证明了其在解释复杂模型时的广泛适用性。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG",
        "68T01, 68T20"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been published in the journal \"Engineering\n  Applications of Artificial Intelligence\"",
      "pdf_url": "http://arxiv.org/pdf/2407.00506v2",
      "published_date": "2024-06-29 18:19:55 UTC",
      "updated_date": "2025-03-31 06:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:57:08.083570"
    },
    {
      "arxiv_id": "2407.00502v1",
      "title": "Deep Frequency Derivative Learning for Non-stationary Time Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Fan",
        "Kun Yi",
        "Hangting Ye",
        "Zhiyuan Ning",
        "Qi Zhang",
        "Ning An"
      ],
      "abstract": "While most time series are non-stationary, it is inevitable for models to\nface the distribution shift issue in time series forecasting. Existing\nsolutions manipulate statistical measures (usually mean and std.) to adjust\ntime series distribution. However, these operations can be theoretically seen\nas the transformation towards zero frequency component of the spectrum which\ncannot reveal full distribution information and would further lead to\ninformation utilization bottleneck in normalization, thus hindering forecasting\nperformance. To address this problem, we propose to utilize the whole frequency\nspectrum to transform time series to make full use of data distribution from\nthe frequency perspective. We present a deep frequency derivative learning\nframework, DERITS, for non-stationary time series forecasting. Specifically,\nDERITS is built upon a novel reversible transformation, namely Frequency\nDerivative Transformation (FDT) that makes signals derived in the frequency\ndomain to acquire more stationary frequency representations. Then, we propose\nthe Order-adaptive Fourier Convolution Network to conduct adaptive frequency\nfiltering and learning. Furthermore, we organize DERITS as a parallel-stacked\narchitecture for the multi-order derivation and fusion for forecasting.\nFinally, we conduct extensive experiments on several datasets which show the\nconsistent superiority in both time series forecasting and shift alleviation.",
      "tldr_zh": "这篇论文针对非平稳时间序列预测中的分布偏移问题，提出了一种深度频率导数学习框架DERITS，以充分利用整个频率谱信息。\nDERITS 基于新型可逆变换Frequency Derivative Transformation (FDT)，在频率域中对信号进行导数处理，使其获得更平稳的频率表示。\n框架还包括Order-adaptive Fourier Convolution Network，用于自适应频率过滤和学习，并采用并行堆叠架构实现多阶导数融合。\n实验在多个数据集上证明，DERITS 显著提升了时间序列预测性能和偏移缓解效果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.00502v1",
      "published_date": "2024-06-29 17:56:59 UTC",
      "updated_date": "2024-06-29 17:56:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:57:21.161763"
    },
    {
      "arxiv_id": "2407.00501v1",
      "title": "Aeroengine performance prediction using a physical-embedded data-driven method",
      "title_zh": "基于物理嵌入的数据驱动方法预测航空发动机性能",
      "authors": [
        "Tong Mo",
        "Shiran Dai",
        "An Fu",
        "Xiaomeng Zhu",
        "Shuxiao Li"
      ],
      "abstract": "Accurate and efficient prediction of aeroengine performance is of paramount\nimportance for engine design, maintenance, and optimization endeavours.\nHowever, existing methodologies often struggle to strike an optimal balance\namong predictive accuracy, computational efficiency, modelling complexity, and\ndata dependency. To address these challenges, we propose a strategy that\nsynergistically combines domain knowledge from both the aeroengine and neural\nnetwork realms to enable real-time prediction of engine performance parameters.\nLeveraging aeroengine domain knowledge, we judiciously design the network\nstructure and regulate the internal information flow. Concurrently, drawing\nupon neural network domain expertise, we devise four distinct feature fusion\nmethods and introduce an innovative loss function formulation. To rigorously\nevaluate the effectiveness and robustness of our proposed strategy, we conduct\ncomprehensive validation across two distinct datasets. The empirical results\ndemonstrate :(1) the evident advantages of our tailored loss function; (2) our\nmodel's ability to maintain equal or superior performance with a reduced\nparameter count; (3) our model's reduced data dependency compared to\ngeneralized neural network architectures; (4)Our model is more interpretable\nthan traditional black box machine learning methods.",
      "tldr_zh": "本论文提出了一种结合航空发动机领域知识和神经网络领域知识的物理嵌入数据驱动方法，用于实现航空发动机性能参数的实时预测。该方法通过设计网络结构、调节内部信息流、开发四种特征融合方法以及引入创新损失函数，解决了现有模型在预测准确性、计算效率、模型复杂性和数据依赖性之间的平衡问题。在两个数据集上的全面验证中，该模型展示了定制损失函数的优势、减少参数数量的同时保持或提升性能、降低数据依赖性，以及比传统黑箱机器学习方法更高的可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00501v1",
      "published_date": "2024-06-29 17:56:58 UTC",
      "updated_date": "2024-06-29 17:56:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:57:33.523605"
    },
    {
      "arxiv_id": "2407.00500v1",
      "title": "Intrinsic PAPR for Point-level 3D Scene Albedo and Shading Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Alireza Moazeni",
        "Shichong Peng",
        "Ke Li"
      ],
      "abstract": "Recent advancements in neural rendering have excelled at novel view synthesis\nfrom multi-view RGB images. However, they often lack the capability to edit the\nshading or colour of the scene at a detailed point-level, while ensuring\nconsistency across different viewpoints. In this work, we address the challenge\nof point-level 3D scene albedo and shading editing from multi-view RGB images,\nfocusing on detailed editing at the point-level rather than at a part or global\nlevel. While prior works based on volumetric representation such as NeRF\nstruggle with achieving 3D consistent editing at the point level, recent\nadvancements in point-based neural rendering show promise in overcoming this\nchallenge. We introduce ``Intrinsic PAPR'', a novel method based on the recent\npoint-based neural rendering technique Proximity Attention Point Rendering\n(PAPR). Unlike other point-based methods that model the intrinsic decomposition\nof the scene, our approach does not rely on complicated shading models or\nsimplistic priors that may not universally apply. Instead, we directly model\nscene decomposition into albedo and shading components, leading to better\nestimation accuracy. Comparative evaluations against the latest point-based\ninverse rendering methods demonstrate that Intrinsic PAPR achieves\nhigher-quality novel view rendering and superior point-level albedo and shading\nediting.",
      "tldr_zh": "本文提出了一种名为 Intrinsic PAPR 的新方法，用于从多视图 RGB 图像实现点级 3D 场景 albedo 和 shading 编辑，解决了现有神经渲染技术在点级编辑时难以保持跨视点一致性的问题。不同于基于体素表示（如 NeRF）的先前方法，Intrinsic PAPR 构建于 Proximity Attention Point Rendering (PAPR) 基础上，直接对场景分解为 albedo 和 shading 组件，而非依赖复杂的阴影模型或简单先验，从而提升了估计准确性。通过与最新点-based 逆渲染方法比较，Intrinsic PAPR 在新视图渲染质量和点级编辑性能上表现出显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00500v1",
      "published_date": "2024-06-29 17:46:10 UTC",
      "updated_date": "2024-06-29 17:46:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:57:46.407100"
    },
    {
      "arxiv_id": "2407.00499v3",
      "title": "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees",
      "title_zh": "ConU: 大语言模型中保形不确定性的正确",
      "authors": [
        "Zhiyuan Wang",
        "Jinhao Duan",
        "Lu Cheng",
        "Yue Zhang",
        "Qingni Wang",
        "Xiaoshuang Shi",
        "Kaidi Xu",
        "Hengtao Shen",
        "Xiaofeng Zhu"
      ],
      "abstract": "Uncertainty quantification (UQ) in natural language generation (NLG) tasks\nremains an open challenge, exacerbated by the closed-source nature of the\nlatest large language models (LLMs). This study investigates applying conformal\nprediction (CP), which can transform any heuristic uncertainty notion into\nrigorous prediction sets, to black-box LLMs in open-ended NLG tasks. We\nintroduce a novel uncertainty measure based on self-consistency theory, and\nthen develop a conformal uncertainty criterion by integrating the uncertainty\ncondition aligned with correctness into the CP algorithm. Empirical evaluations\nindicate that our uncertainty measure outperforms prior state-of-the-art\nmethods. Furthermore, we achieve strict control over the correctness coverage\nrate utilizing 7 popular LLMs on 4 free-form NLG datasets, spanning\ngeneral-purpose and medical scenarios. Additionally, the calibrated prediction\nsets with small size further highlights the efficiency of our method in\nproviding trustworthy guarantees for practical open-ended NLG applications.",
      "tldr_zh": "这篇论文提出了一种名为 ConU 的方法，用于在大型语言模型（LLMs）中实现不确定性量化（UQ），以提供严格的正确性覆盖保证。研究者基于自一致性理论开发了一种新型不确定性度量，并将其整合到共形预测（CP）算法中，针对开放式自然语言生成（NLG）任务进行优化。实验结果显示，该方法在 7 个流行 LLMs 和 4 个数据集（涵盖一般和医疗场景）上优于现有技术，不仅实现了精确的正确性覆盖率控制，还生成了高效的小型预测集，从而提升了 NLG 应用的可靠性和实用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2407.00499v3",
      "published_date": "2024-06-29 17:33:07 UTC",
      "updated_date": "2024-11-18 08:33:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:57:57.480804"
    },
    {
      "arxiv_id": "2407.00496v1",
      "title": "A Two-stage Reinforcement Learning-based Approach for Multi-entity Task Allocation",
      "title_zh": "翻译失败",
      "authors": [
        "Aicheng Gong",
        "Kai Yang",
        "Jiafei Lyu",
        "Xiu Li"
      ],
      "abstract": "Task allocation is a key combinatorial optimization problem, crucial for\nmodern applications such as multi-robot cooperation and resource scheduling.\nDecision makers must allocate entities to tasks reasonably across different\nscenarios. However, traditional methods assume static attributes and numbers of\ntasks and entities, often relying on dynamic programming and heuristic\nalgorithms for solutions. In reality, task allocation resembles Markov decision\nprocesses, with dynamically changing task and entity attributes. Thus,\nalgorithms must dynamically allocate tasks based on their states. To address\nthis issue, we propose a two-stage task allocation algorithm based on\nsimilarity, utilizing reinforcement learning to learn allocation strategies.\nThe proposed pre-assign strategy allows entities to preselect appropriate\ntasks, effectively avoiding local optima and thereby better finding the optimal\nallocation. We also introduce an attention mechanism and a hyperparameter\nnetwork structure to adapt to the changing number and attributes of entities\nand tasks, enabling our network structure to generalize to new tasks.\nExperimental results across multiple environments demonstrate that our\nalgorithm effectively addresses the challenges of dynamic task allocation in\npractical applications. Compared to heuristic algorithms like genetic\nalgorithms, our reinforcement learning approach better solves dynamic\nallocation problems and achieves zero-shot generalization to new tasks with\ngood performance. The code is available at\nhttps://github.com/yk7333/TaskAllocation.",
      "tldr_zh": "这篇论文提出了一种基于强化学习（Reinforcement Learning）的双阶段方法，用于解决多实体任务分配问题，该问题在多机器人合作和资源调度等应用中至关重要。方法首先通过预分配策略（pre-assign strategy）让实体预选合适任务，以避免局部最优；其次引入注意力机制（attention mechanism）和超参数网络结构，使算法能适应任务和实体属性的动态变化，并实现对新任务的零样本泛化（zero-shot generalization）。实验结果表明，该算法在多个环境中优于传统启发式算法如遗传算法（Genetic Algorithms），在动态任务分配中表现出色，并提供了开源代码。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00496v1",
      "published_date": "2024-06-29 17:13:44 UTC",
      "updated_date": "2024-06-29 17:13:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:58:19.715781"
    },
    {
      "arxiv_id": "2407.00488v1",
      "title": "PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kunquan Deng",
        "Zeyu Huang",
        "Chen Li",
        "Chenghua Lin",
        "Min Gao",
        "Wenge Rong"
      ],
      "abstract": "Large Language Models (LLMs) excel in fluency but risk producing inaccurate\ncontent, called \"hallucinations.\" This paper outlines a standardized process\nfor categorizing fine-grained hallucination types and proposes an innovative\nframework--the Progressive Fine-grained Model Editor (PFME)--specifically\ndesigned to detect and correct fine-grained hallucinations in LLMs. PFME\nconsists of two collaborative modules: the Real-time Fact Retrieval Module and\nthe Fine-grained Hallucination Detection and Editing Module. The former\nidentifies key entities in the document and retrieves the latest factual\nevidence from credible sources. The latter further segments the document into\nsentence-level text and, based on relevant evidence and previously edited\ncontext, identifies, locates, and edits each sentence's hallucination type.\nExperimental results on FavaBench and FActScore demonstrate that PFME\noutperforms existing methods in fine-grained hallucination detection tasks.\nParticularly, when using the Llama3-8B-Instruct model, PFME's performance in\nfine-grained hallucination detection with external knowledge assistance\nimproves by 8.7 percentage points (pp) compared to ChatGPT. In editing tasks,\nPFME further enhances the FActScore of FActScore-Alpaca13B and\nFActScore-ChatGPT datasets, increasing by 16.2pp and 4.6pp, respectively.",
      "tldr_zh": "这篇论文针对 Large Language Models (LLMs) 产生的幻觉 (hallucinations) 问题，提出了一种模块化框架 Progressive Fine-grained Model Editor (PFME)，用于细粒度检测和编辑这些错误。PFME 由两个协作模块组成：Real-time Fact Retrieval Module 负责识别关键实体并从可靠来源检索最新事实证据，而 Fine-grained Hallucination Detection and Editing Module 则将文档分解为句子级别，并基于证据和上下文识别、定位及修正每个句子的幻觉类型。实验结果显示，在 FavaBench 和 FActScore 数据集上，PFME 优于现有方法，使用 Llama3-8B-Instruct 模型时，细粒度检测性能比 ChatGPT 提高 8.7 百分点；在编辑任务中，分别提升 FActScore-Alpaca13B 和 FActScore-ChatGPT 的分数 16.2 百分点和 4.6 百分点。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00488v1",
      "published_date": "2024-06-29 16:35:57 UTC",
      "updated_date": "2024-06-29 16:35:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:58:24.013306"
    },
    {
      "arxiv_id": "2407.00482v1",
      "title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition",
      "title_zh": "翻译失败",
      "authors": [
        "Barproda Halder",
        "Faisal Hamman",
        "Pasan Dissanayake",
        "Qiuyi Zhang",
        "Ilia Sucholutsky",
        "Sanghamitra Dutta"
      ],
      "abstract": "Spurious patterns refer to a mathematical association between two or more\nvariables in a dataset that are not causally related. However, this notion of\nspuriousness, which is usually introduced due to sampling biases in the\ndataset, has classically lacked a formal definition. To address this gap, this\nwork presents the first information-theoretic formalization of spuriousness in\na dataset (given a split of spurious and core features) using a mathematical\nframework called Partial Information Decomposition (PID). Specifically, we\ndisentangle the joint information content that the spurious and core features\nshare about another target variable (e.g., the prediction label) into distinct\ncomponents, namely unique, redundant, and synergistic information. We propose\nthe use of unique information, with roots in Blackwell Sufficiency, as a novel\nmetric to formally quantify dataset spuriousness and derive its desirable\nproperties. We empirically demonstrate how higher unique information in the\nspurious features in a dataset could lead a model into choosing the spurious\nfeatures over the core features for inference, often having low\nworst-group-accuracy. We also propose a novel autoencoder-based estimator for\ncomputing unique information that is able to handle high-dimensional image\ndata. Finally, we also show how this unique information in the spurious feature\nis reduced across several dataset-based spurious-pattern-mitigation techniques\nsuch as data reweighting and varying levels of background mixing, demonstrating\na novel tradeoff between unique information (spuriousness) and\nworst-group-accuracy.",
      "tldr_zh": "本研究首次使用 Partial Information Decomposition (PID) 框架来正式量化偏置数据集中的 spuriousness，通过将 spurious features 和 core features 关于目标变量（如预测标签）的联合信息分解为 unique、redundant 和 synergistic 成分。论文提出 unique information 作为一种新指标来衡量数据集的 spuriousness，并证明其基于 Blackwell Sufficiency 的理想属性。实验结果显示，高 unique information 在 spurious features 中会导致模型优先选择这些特征，从而降低 worst-group-accuracy；同时，作者开发了一种新型 autoencoder-based estimator，能处理高维图像数据以计算 unique information。最终，该研究展示了数据再加权和背景混合等 spurious-pattern-mitigation 技术如何减少 unique information，同时揭示了 spuriousness 与 worst-group-accuracy 之间的权衡关系。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.CY",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICML 2024 Workshop on Data-centric Machine Learning\n  Research (DMLR): Datasets for Foundation Models",
      "pdf_url": "http://arxiv.org/pdf/2407.00482v1",
      "published_date": "2024-06-29 16:05:47 UTC",
      "updated_date": "2024-06-29 16:05:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:58:35.465864"
    },
    {
      "arxiv_id": "2407.00478v3",
      "title": "Beyond Scaleup: Knowledge-aware Parsimony Learning from Deep Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Quanming Yao",
        "Yongqi Zhang",
        "Yaqing Wang",
        "Nan Yin",
        "James Kwok",
        "Qiang Yang"
      ],
      "abstract": "The brute-force scaleup of training datasets, learnable parameters and\ncomputation power, has become a prevalent strategy for developing more robust\nlearning models. However, due to bottlenecks in data, computation, and trust,\nthe sustainability of this strategy is a serious concern. In this paper, we\nattempt to address this issue in a parsimonious manner (i.e., achieving greater\npotential with simpler models). The key is to drive models using\ndomain-specific knowledge, such as symbols, logic, and formulas, instead of\npurely relying on scaleup. This approach allows us to build a framework that\nuses this knowledge as \"building blocks\" to achieve parsimony in model design,\ntraining, and interpretation. Empirical results show that our methods surpass\nthose that typically follow the scaling law. We also demonstrate our framework\nin AI for science, specifically in the problem of drug-drug interaction\nprediction. We hope our research can foster more diverse technical roadmaps in\nthe era of foundation models.",
      "tldr_zh": "该论文批评了通过扩大数据集、参数和计算能力来提升模型的“scaleup”策略，认为其因数据、计算和信任瓶颈而不可持续。\n作者提出了一种知识驱动的节俭学习（knowledge-aware parsimony learning）框架，使用领域特定知识（如符号、逻辑和公式）作为“building blocks”，以简化模型设计、训练和解释。\n实验结果显示，该方法超过了遵循scaling law的传统方法，并在AI for science领域（如药物-药物相互作用预测）中表现出色。\n作者呼吁在foundation models时代探索更多样化的技术路线，以促进AI的可持续发展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to AI Magazine",
      "pdf_url": "http://arxiv.org/pdf/2407.00478v3",
      "published_date": "2024-06-29 15:52:37 UTC",
      "updated_date": "2024-12-17 07:30:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:58:45.747725"
    },
    {
      "arxiv_id": "2407.00474v1",
      "title": "MH-pFLGB: Model Heterogeneous personalized Federated Learning via Global Bypass for Medical Image Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Luyuan Xie",
        "Manqing Lin",
        "ChenMing Xu",
        "Tianyu Luan",
        "Zhipeng Zeng",
        "Wenjun Qian",
        "Cong Li",
        "Yuejian Fang",
        "Qingni Shen",
        "Zhonghai Wu"
      ],
      "abstract": "In the evolving application of medical artificial intelligence, federated\nlearning is notable for its ability to protect training data privacy. Federated\nlearning facilitates collaborative model development without the need to share\nlocal data from healthcare institutions. Yet, the statistical and system\nheterogeneity among these institutions poses substantial challenges, which\naffects the effectiveness of federated learning and hampers the exchange of\ninformation between clients. To address these issues, we introduce a novel\napproach, MH-pFLGB, which employs a global bypass strategy to mitigate the\nreliance on public datasets and navigate the complexities of non-IID data\ndistributions. Our method enhances traditional federated learning by\nintegrating a global bypass model, which would share the information among the\nclients, but also serves as part of the network to enhance the performance on\neach client. Additionally, MH-pFLGB provides a feature fusion module to better\ncombine the local and global features. We validate \\model{}'s effectiveness and\nadaptability through extensive testing on different medical tasks,\ndemonstrating superior performance compared to existing state-of-the-art\nmethods.",
      "tldr_zh": "本文提出 MH-pFLGB，一种针对医疗图像分析的个性化联邦学习方法，通过全局旁路策略（Global Bypass）缓解统计和系统异质性问题，同时减少对公共数据集的依赖。MH-pFLGB 整合了全局旁路模型来在客户端之间共享信息，并引入特征融合模块以更好地结合本地和全局特征，从而提升模型在 non-IID 数据分布下的性能。在多种医疗任务上的广泛实验中，该方法比现有最先进技术表现出色，验证了其有效性和适应性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2405.06822",
      "pdf_url": "http://arxiv.org/pdf/2407.00474v1",
      "published_date": "2024-06-29 15:38:37 UTC",
      "updated_date": "2024-06-29 15:38:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:58:57.388063"
    },
    {
      "arxiv_id": "2407.00468v2",
      "title": "MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation",
      "title_zh": "MMEvalPro：校准多模态基准以实现可信赖和高效评估",
      "authors": [
        "Jinsheng Huang",
        "Liang Chen",
        "Taian Guo",
        "Fu Zeng",
        "Yusheng Zhao",
        "Bohan Wu",
        "Ye Yuan",
        "Haozhe Zhao",
        "Zhihui Guo",
        "Yichi Zhang",
        "Jingyang Yuan",
        "Wei Ju",
        "Luchen Liu",
        "Tianyu Liu",
        "Baobao Chang",
        "Ming Zhang"
      ],
      "abstract": "Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding\nand reasoning abilities, often assessed through multiple-choice questions\n(MCQs) that include an image, a question, and several options. However, many\nbenchmarks used for such evaluations suffer from systematic biases. Remarkably,\nLarge Language Models (LLMs) without any visual perception capabilities achieve\nnon-trivial performance, undermining the credibility of these evaluations. To\naddress this issue while maintaining the efficiency of MCQ evaluations, we\npropose MMEvalPro, a benchmark designed to avoid Type-I errors through a\ntrilogy evaluation pipeline and more rigorous metrics. For each original\nquestion from existing benchmarks, human annotators augment it by creating one\nperception question and one knowledge anchor question through a meticulous\nannotation process. MMEvalPro comprises $2,138$ question triplets, totaling\n$6,414$ distinct questions. Two-thirds of these questions are manually labeled\nby human experts, while the rest are sourced from existing benchmarks (MMMU,\nScienceQA, and MathVista). Compared with the existing benchmarks, our\nexperiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more\nchallenging (the best LMM lags behind human performance by $31.73\\%$, compared\nto an average gap of $8.03\\%$ in previous benchmarks) and more trustworthy (the\nbest LLM trails the best LMM by $23.09\\%$, whereas the gap for previous\nbenchmarks is just $14.64\\%$). Our in-depth analysis explains the reason for\nthe large performance gap and justifies the trustworthiness of evaluation,\nunderscoring its significant potential for advancing future research.",
      "tldr_zh": "这篇论文提出了MMEvalPro，一种校准多模态基准测试的新框架，旨在解决现有评估中存在的系统偏差问题，如Large Language Models (LLMs) 即使无视觉能力也能获得非微不足道的性能，从而提升评估的可信性和效率。MMEvalPro 通过三部曲评估管道和更严格的指标，对每个原始问题添加一个感知问题和一个知识锚问题，总计2,138个问题三元组（6,414个问题），其中2/3由人类专家手动标记。实验结果显示，该基准更具挑战性（最佳Large Multimodal Models (LMMs) 落后人类31.73%，远高于现有基准的8.03%差距），并更可信（最佳LLMs 落后LMMs 23.09%），从而揭示性能差距原因并推动未来多模态模型研究的发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, code released at https://github.com/chenllliang/MMEvalPro,\n  Homepage at https://mmevalpro.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2407.00468v2",
      "published_date": "2024-06-29 15:28:45 UTC",
      "updated_date": "2025-02-27 15:10:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:59:11.201021"
    },
    {
      "arxiv_id": "2407.00466v1",
      "title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science",
      "title_zh": "翻译失败",
      "authors": [
        "Xinna Lin",
        "Siqi Ma",
        "Junjie Shan",
        "Xiaojing Zhang",
        "Shell Xu Hu",
        "Tiannan Guo",
        "Stan Z. Li",
        "Kaicheng Yu"
      ],
      "abstract": "Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist,\ndraws increasing attention, where one common approach is to build a copilot\nagent driven by Large Language Models (LLMs). However, to evaluate such\nsystems, people either rely on direct Question-Answering (QA) to the LLM\nitself, or in a biomedical experimental manner. How to precisely benchmark\nbiomedical agents from an AI Scientist perspective remains largely unexplored.\nTo this end, we draw inspiration from one most important abilities of\nscientists, understanding the literature, and introduce BioKGBench. In contrast\nto traditional evaluation benchmark that only focuses on factual QA, where the\nLLMs are known to have hallucination issues, we first disentangle\n\"Understanding Literature\" into two atomic abilities, i) \"Understanding\" the\nunstructured text from research papers by performing scientific claim\nverification, and ii) Ability to interact with structured Knowledge-Graph\nQuestion-Answering (KGQA) as a form of \"Literature\" grounding. We then\nformulate a novel agent task, dubbed KGCheck, using KGQA and domain-based\nRetrieval-Augmented Generation (RAG) to identify the factual errors of existing\nlarge-scale knowledge graph databases. We collect over two thousand data for\ntwo atomic tasks and 225 high-quality annotated data for the agent task.\nSurprisingly, we discover that state-of-the-art agents, both daily scenarios\nand biomedical ones, have either failed or inferior performance on our\nbenchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent.\nOn the widely used popular knowledge graph, we discover over 90 factual errors\nwhich provide scenarios for agents to make discoveries and demonstrate the\neffectiveness of our approach. The code and data are available at\nhttps://github.com/westlake-autolab/BioKGBench.",
      "tldr_zh": "本论文引入了BioKGBench，一种针对生物医学AI代理（AI Scientist）的知识图谱检查基准，旨在从科学文献理解角度评估代理性能，而非传统的事实问答。研究将“理解文献”分解为两个原子能力：通过科学声明验证处理非结构化文本，以及利用Knowledge-Graph Question-Answering (KGQA)与Retrieval-Augmented Generation (RAG)互动的新任务KGCheck，以识别现有知识图谱数据库中的事实错误。作者收集了超过两千条原子任务数据和225条高质量标注数据，并发现现有最先进代理在该基准上表现不佳；他们提出简单基线BKGAgent，在流行知识图谱中发现超过90个事实错误，证明了这一方法的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00466v1",
      "published_date": "2024-06-29 15:23:28 UTC",
      "updated_date": "2024-06-29 15:23:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:59:22.549932"
    },
    {
      "arxiv_id": "2407.00463v5",
      "title": "Open-Source Conversational AI with SpeechBrain 1.0",
      "title_zh": "翻译失败",
      "authors": [
        "Mirco Ravanelli",
        "Titouan Parcollet",
        "Adel Moumen",
        "Sylvain de Langen",
        "Cem Subakan",
        "Peter Plantinga",
        "Yingzhi Wang",
        "Pooneh Mousavi",
        "Luca Della Libera",
        "Artem Ploujnikov",
        "Francesco Paissan",
        "Davide Borra",
        "Salah Zaiem",
        "Zeyu Zhao",
        "Shucong Zhang",
        "Georgios Karakasidis",
        "Sung-Lin Yeh",
        "Pierre Champion",
        "Aku Rouhe",
        "Rudolf Braun",
        "Florian Mai",
        "Juan Zuluaga-Gomez",
        "Seyed Mahed Mousavi",
        "Andreas Nautsch",
        "Ha Nguyen",
        "Xuechen Liu",
        "Sangeet Sagar",
        "Jarod Duret",
        "Salima Mdhaffar",
        "Gaelle Laperriere",
        "Mickael Rouvier",
        "Renato De Mori",
        "Yannick Esteve"
      ],
      "abstract": "SpeechBrain is an open-source Conversational AI toolkit based on PyTorch,\nfocused particularly on speech processing tasks such as speech recognition,\nspeech enhancement, speaker recognition, text-to-speech, and much more. It\npromotes transparency and replicability by releasing both the pre-trained\nmodels and the complete \"recipes\" of code and algorithms required for training\nthem. This paper presents SpeechBrain 1.0, a significant milestone in the\nevolution of the toolkit, which now has over 200 recipes for speech, audio, and\nlanguage processing tasks, and more than 100 models available on Hugging Face.\nSpeechBrain 1.0 introduces new technologies to support diverse learning\nmodalities, Large Language Model (LLM) integration, and advanced decoding\nstrategies, along with novel models, tasks, and modalities. It also includes a\nnew benchmark repository, offering researchers a unified platform for\nevaluating models across diverse tasks.",
      "tldr_zh": "SpeechBrain 1.0 是一个基于 PyTorch 的开源对话 AI 工具包，主要专注于语音处理任务，如语音识别、语音增强、说话者识别和文本到语音等。它通过提供预训练模型和完整的代码配方（recipes），提升了透明性和可复制性，并在新版本中新增了超过 200 个配方、100 多个 Hugging Face 上的模型，以及对多种学习模式、LLM（Large Language Model）集成和高级解码策略的支持。论文还介绍了新的基准仓库，提供统一的平台让研究者评估模型在多样任务中的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the Journal of Machine Learning research (JMLR), Machine\n  Learning Open Source Software",
      "pdf_url": "http://arxiv.org/pdf/2407.00463v5",
      "published_date": "2024-06-29 15:20:11 UTC",
      "updated_date": "2024-10-16 16:13:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:59:34.968109"
    },
    {
      "arxiv_id": "2407.00462v1",
      "title": "pFLFE: Cross-silo Personalized Federated Learning via Feature Enhancement on Medical Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Luyuan Xie",
        "Manqing Lin",
        "Siyuan Liu",
        "ChenMing Xu",
        "Tianyu Luan",
        "Cong Li",
        "Yuejian Fang",
        "Qingni Shen",
        "Zhonghai Wu"
      ],
      "abstract": "In medical image segmentation, personalized cross-silo federated learning\n(FL) is becoming popular for utilizing varied data across healthcare settings\nto overcome data scarcity and privacy concerns. However, existing methods often\nsuffer from client drift, leading to inconsistent performance and delayed\ntraining. We propose a new framework, Personalized Federated Learning via\nFeature Enhancement (pFLFE), designed to mitigate these challenges. pFLFE\nconsists of two main stages: feature enhancement and supervised learning. The\nfirst stage improves differentiation between foreground and background\nfeatures, and the second uses these enhanced features for learning from\nsegmentation masks. We also design an alternative training approach that\nrequires fewer communication rounds without compromising segmentation quality,\neven with limited communication resources. Through experiments on three medical\nsegmentation tasks, we demonstrate that pFLFE outperforms the state-of-the-art\nmethods.",
      "tldr_zh": "在医疗图像分割领域，本文提出 pFLFE 框架，通过特征增强和监督学习来解决个性化跨库 Federated Learning 中的客户端漂移问题，从而应对数据稀缺和隐私挑战。pFLFE 包括两个主要阶段：首先进行特征增强以改善前景和背景特征的区分，其次利用这些增强特征从分割掩码中进行学习，同时引入一种替代训练方法，减少通信轮次而不影响性能。实验结果显示，在三个医疗分割任务上，pFLFE 超过了现有最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00462v1",
      "published_date": "2024-06-29 15:20:03 UTC",
      "updated_date": "2024-06-29 15:20:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:59:46.284238"
    },
    {
      "arxiv_id": "2407.00460v1",
      "title": "A Rule-Based Behaviour Planner for Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Bouchard Frederic",
        "Sedwards Sean",
        "Czarnecki Krzysztof"
      ],
      "abstract": "Autonomous vehicles require highly sophisticated decision-making to determine\ntheir motion. This paper describes how such functionality can be achieved with\na practical rule engine learned from expert driving decisions. We propose an\nalgorithm to create and maintain a rule-based behaviour planner, using a\ntwo-layer rule-based theory. The first layer determines a set of feasible\nparametrized behaviours, given the perceived state of the environment. From\nthese, a resolution function chooses the most conservative high-level maneuver.\nThe second layer then reconciles the parameters into a single behaviour. To\ndemonstrate the practicality of our approach, we report results of its\nimplementation in a level-3 autonomous vehicle and its field test in an urban\nenvironment.",
      "tldr_zh": "该论文提出了一种基于规则(rule-based)的行为规划器，用于自动驾驶车辆的决策系统，该系统从专家驾驶决策中学习规则引擎。算法采用两层规则理论：第一层根据环境感知状态生成一组可行参数化行为，并选择最保守的高级 maneuvers；第二层将这些参数整合成单一行为。该方法已在 level-3 自动驾驶车辆中实现，并在城市环境中进行实地测试，证明了其实用性和有效性。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Use https://link.springer.com/chapter/10.1007/978-3-031-21541-4_17\n  for citations",
      "pdf_url": "http://arxiv.org/pdf/2407.00460v1",
      "published_date": "2024-06-29 15:15:41 UTC",
      "updated_date": "2024-06-29 15:15:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:59:57.131771"
    },
    {
      "arxiv_id": "2407.00456v1",
      "title": "Beyond Functional Correctness: Investigating Coding Style Inconsistencies in Large Language Models",
      "title_zh": "超越功能正确性：调查大型语言模型中的编码风格不一致性",
      "authors": [
        "Yanlin Wang",
        "Tianyue Jiang",
        "Mingwei Liu",
        "Jiachi Chen",
        "Zibin Zheng"
      ],
      "abstract": "Large language models (LLMs) have brought a paradigm shift to the field of\ncode generation, offering the potential to enhance the software development\nprocess. However, previous research mainly focuses on the accuracy of code\ngeneration, while coding style differences between LLMs and human developers\nremain under-explored. In this paper, we empirically analyze the differences in\ncoding style between the code generated by mainstream Code LLMs and the code\nwritten by human developers, and summarize coding style inconsistency taxonomy.\nSpecifically, we first summarize the types of coding style inconsistencies by\nmanually analyzing a large number of generation results. We then compare the\ncode generated by Code LLMs with the code written by human programmers in terms\nof readability, conciseness, and robustness. The results reveal that LLMs and\ndevelopers have different coding styles. Additionally, we study the possible\ncauses of these inconsistencies and provide some solutions to alleviate the\nproblem.",
      "tldr_zh": "这项研究超越了代码生成的功能正确性，探讨了Large Language Models (LLMs)生成的代码与人类开发者编码风格之间的不一致问题。研究者通过手动分析大量生成结果，总结了编码风格不一致的分类，并比较了LLMs代码与人类代码在可读性、简洁性和鲁棒性方面的差异。结果表明，LLMs和开发者在编码风格上存在显著差异，如LLMs可能更注重简洁但牺牲可读性。该工作还分析了这些不一致的潜在原因，并提出了缓解策略，以提升LLMs在实际软件开发中的适用性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "13pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.00456v1",
      "published_date": "2024-06-29 14:56:11 UTC",
      "updated_date": "2024-06-29 14:56:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:00:10.433915"
    },
    {
      "arxiv_id": "2407.00452v1",
      "title": "KHNNs: hypercomplex neural networks computations via Keras using TensorFlow and PyTorch",
      "title_zh": "翻译失败",
      "authors": [
        "Agnieszka Niemczynowicz",
        "Radosław Antoni Kycia"
      ],
      "abstract": "Neural networks used in computations with more advanced algebras than real\nnumbers perform better in some applications. However, there is no general\nframework for constructing hypercomplex neural networks. We propose a library\nintegrated with Keras that can do computations within TensorFlow and PyTorch.\nIt provides Dense and Convolutional 1D, 2D, and 3D layers architectures.",
      "tldr_zh": "该研究指出，使用超复数（hypercomplex）代数的神经网络在某些应用中表现优于实数计算，但缺乏构建此类网络的一般框架。作者提出KHNNs库，该库与Keras集成，支持TensorFlow和PyTorch环境。KHNNs提供Dense层以及Convolutional 1D、2D和3D层架构，便于用户轻松实现超复数神经网络计算。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00452v1",
      "published_date": "2024-06-29 14:36:37 UTC",
      "updated_date": "2024-06-29 14:36:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:00:20.894218"
    },
    {
      "arxiv_id": "2407.01630v1",
      "title": "A survey on the impact of AI-based recommenders on human behaviours: methodologies, outcomes and future directions",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Pappalardo",
        "Emanuele Ferragina",
        "Salvatore Citraro",
        "Giuliano Cornacchia",
        "Mirco Nanni",
        "Giulio Rossetti",
        "Gizem Gezici",
        "Fosca Giannotti",
        "Margherita Lalli",
        "Daniele Gambetta",
        "Giovanni Mauro",
        "Virginia Morini",
        "Valentina Pansanella",
        "Dino Pedreschi"
      ],
      "abstract": "Recommendation systems and assistants (in short, recommenders) are ubiquitous\nin online platforms and influence most actions of our day-to-day lives,\nsuggesting items or providing solutions based on users' preferences or\nrequests. This survey analyses the impact of recommenders in four human-AI\necosystems: social media, online retail, urban mapping and generative AI\necosystems. Its scope is to systematise a fast-growing field in which\nterminologies employed to classify methodologies and outcomes are fragmented\nand unsystematic. We follow the customary steps of qualitative systematic\nreview, gathering 144 articles from different disciplines to develop a\nparsimonious taxonomy of: methodologies employed (empirical, simulation,\nobservational, controlled), outcomes observed (concentration, model collapse,\ndiversity, echo chamber, filter bubble, inequality, polarisation,\nradicalisation, volume), and their level of analysis (individual, item, model,\nand systemic). We systematically discuss all findings of our survey\nsubstantively and methodologically, highlighting also potential avenues for\nfuture research. This survey is addressed to scholars and practitioners\ninterested in different human-AI ecosystems, policymakers and institutional\nstakeholders who want to understand better the measurable outcomes of\nrecommenders, and tech companies who wish to obtain a systematic view of the\nimpact of their recommenders.",
      "tldr_zh": "这篇调查论文系统分析了AI-based recommenders（AI推荐系统）对人类行为的影响，涵盖社交媒体、在线零售、城市地图和生成AI等四个生态系统。通过定性系统回顾，作者收集了144篇文章，并构建了一个简洁的taxonomy，包括方法论（empirical、simulation、observational、controlled实验）、观察结果（concentration、model collapse、diversity、echo chamber、filter bubble、inequality、polarisation、radicalisation）和分析水平（individual、item、model、systemic）。主要发现显示，推荐系统可能加剧问题如echo chamber和filter bubble，从而影响社会多样性和不平等；论文还讨论了方法论挑战，并为未来研究提出潜在方向，如跨学科整合。针对学者、政策制定者、技术公司等利益相关者，此调查提供了可衡量的影响概述，以促进更负责任的AI设计。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.01630v1",
      "published_date": "2024-06-29 14:34:32 UTC",
      "updated_date": "2024-06-29 14:34:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:00:34.819732"
    },
    {
      "arxiv_id": "2407.00449v2",
      "title": "Fully tensorial approach to hypercomplex neural networks",
      "title_zh": "针对超复数神经网络的完全张量化方法",
      "authors": [
        "Agnieszka Niemczynowicz",
        "Radosław Antoni Kycia"
      ],
      "abstract": "Fully tensorial theory of hypercomplex neural networks is given. It allows\nneural networks to use arithmetic based on arbitrary algebras. The key point is\nto observe that algebra multiplication can be represented as a rank three\ntensor and use this tensor in every algebraic operation. This approach is\nattractive for neural network libraries that support effective tensorial\noperations. It agrees with previous implementations for four-dimensional\nalgebras.",
      "tldr_zh": "本论文提出了一种Fully tensorial approach to hypercomplex neural networks的理论，该方法允许神经网络使用基于任意代数的算术运算。\n关键点是将algebra multiplication表示为一个rank three tensor，并在每个algebraic operation中使用此tensor，从而实现高效的运算。\n这种方法适用于支持有效tensorial operations的神经网络库，并与之前针对四维代数的实现保持一致。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "15A69, 15-04"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.00449v2",
      "published_date": "2024-06-29 14:19:40 UTC",
      "updated_date": "2024-09-19 14:10:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:00:45.703173"
    },
    {
      "arxiv_id": "2407.00429v2",
      "title": "Time Series Clustering with General State Space Models via Stochastic Variational Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Ryoichi Ishizuka",
        "Takashi Imai",
        "Kaoru Kawamoto"
      ],
      "abstract": "In this paper, we propose a novel method of model-based time series\nclustering with mixtures of general state space models (MSSMs). Each component\nof MSSMs is associated with each cluster. An advantage of the proposed method\nis that it enables the use of time series models appropriate to the specific\ntime series. This not only improves clustering and prediction accuracy but also\nenhances the interpretability of the estimated parameters. The parameters of\nthe MSSMs are estimated using stochastic variational inference, a subtype of\nvariational inference. The proposed method estimates the latent variables of an\narbitrary state space model by using neural networks with a normalizing flow as\na variational estimator. The number of clusters can be estimated using the\nBayesian information criterion. In addition, to prevent MSSMs from converging\nto the local optimum, we propose several optimization tricks, including an\nadditional penalty term called entropy annealing. To our best knowledge, the\nproposed method is the first computationally feasible one for time series\nclustering based on general (possibly nonlinear, non-Gaussian) state space\nmodels. Experiments on simulated datasets show that the proposed method is\neffective for clustering, parameter estimation, and estimating the number of\nclusters.",
      "tldr_zh": "本文提出了一种基于混合一般状态空间模型 (MSSMs) 的时间序列聚类方法，其中每个 MSSMs 组件对应一个聚类，并允许使用适合特定时间序列的模型，以提升聚类、预测准确性和参数可解释性。参数估计采用 stochastic variational inference，通过神经网络和 normalizing flow 作为变分估计器来处理任意状态空间模型的潜在变量，并使用 Bayesian information criterion 确定聚类数量。针对避免局部最优问题，该方法引入了优化技巧，如 entropy annealing。实验在模拟数据集上验证了该方法的有效性，在聚类、参数估计和聚类数量估计方面表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.00429v2",
      "published_date": "2024-06-29 12:48:53 UTC",
      "updated_date": "2024-08-22 14:50:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:00:58.564045"
    },
    {
      "arxiv_id": "2407.03368v5",
      "title": "Balancing Forecast Accuracy and Switching Costs in Online Optimization of Energy Management Systems",
      "title_zh": "在能源管理系统在线优化中平衡预测准确性和开关成本",
      "authors": [
        "Evgenii Genov",
        "Julian Ruddick",
        "Christoph Bergmeir",
        "Majid Vafaeipour",
        "Thierry Coosemans",
        "Salvador Garcia",
        "Maarten Messagie"
      ],
      "abstract": "This study investigates the integration of forecasting and optimization in\nenergy management systems, with a focus on the role of switching costs --\npenalties incurred from frequent operational adjustments. We develop a\ntheoretical and empirical framework to examine how forecast accuracy and\nstability interact with switching costs in online decision-making settings. Our\nanalysis spans both deterministic and stochastic optimization approaches, using\npoint and probabilistic forecasts. A novel metric for measuring temporal\nconsistency in probabilistic forecasts is introduced, and the framework is\nvalidated in a real-world battery scheduling case based on the CityLearn 2022\nchallenge. Results show that switching costs significantly alter the trade-off\nbetween forecast accuracy and stability, and that more stable forecasts can\nreduce the performance loss due to switching. Contrary to common practice, the\nfindings suggest that, under non-negligible switching costs, longer commitment\nperiods may lead to better overall outcomes. These insights have practical\nimplications for the design of intelligent, forecast-aware energy management\nsystems.",
      "tldr_zh": "本研究探讨了在能源管理系统在线优化中，如何平衡预测准确性和切换 costs（频繁操作调整的惩罚）。研究开发了一个理论和实证框架，分析预测准确性与稳定性如何与切换 costs 互动，涵盖确定性和随机优化方法，并引入了一个新的度量来衡量概率预测的时序一致性。实验在基于 CityLearn 2022 挑战的真实电池调度案例中验证，结果表明切换 costs 显著改变预测权衡，更稳定的预测可减少性能损失，且与常见做法相反，更长的承诺期可能带来更好的整体结果。这些发现为设计智能、预测感知的能源管理系统提供了实际启示。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "34 pages, contains the Appendix with a comment on KPIs, MPC\n  formulation, Theoretical analysis of the MPC performance bounds and extra\n  results on the in-sample performance",
      "pdf_url": "http://arxiv.org/pdf/2407.03368v5",
      "published_date": "2024-06-29 12:28:30 UTC",
      "updated_date": "2025-04-15 15:12:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:01:21.889769"
    },
    {
      "arxiv_id": "2407.00419v1",
      "title": "On the Complexity of Learning to Cooperate with Populations of Socially Rational Agents",
      "title_zh": "关于学习与社会理性代理群体合作的复杂性",
      "authors": [
        "Robert Loftin",
        "Saptarashmi Bandyopadhyay",
        "Mustafa Mert Çelikok"
      ],
      "abstract": "Artificially intelligent agents deployed in the real-world will require the\nability to reliably \\textit{cooperate} with humans (as well as other,\nheterogeneous AI agents). To provide formal guarantees of successful\ncooperation, we must make some assumptions about how partner agents could\nplausibly behave. Any realistic set of assumptions must account for the fact\nthat other agents may be just as adaptable as our agent is. In this work, we\nconsider the problem of cooperating with a \\textit{population} of agents in a\nfinitely-repeated, two player general-sum matrix game with private utilities.\nTwo natural assumptions in such settings are that: 1) all agents in the\npopulation are individually rational learners, and 2) when any two members of\nthe population are paired together, with high-probability they will achieve at\nleast the same utility as they would under some Pareto efficient equilibrium\nstrategy. Our results first show that these assumptions alone are insufficient\nto ensure \\textit{zero-shot} cooperation with members of the target population.\nWe therefore consider the problem of \\textit{learning} a strategy for\ncooperating with such a population using prior observations its members\ninteracting with one another. We provide upper and lower bounds on the number\nof samples needed to learn an effective cooperation strategy. Most importantly,\nwe show that these bounds can be much stronger than those arising from a\n\"naive'' reduction of the problem to one of imitation learning.",
      "tldr_zh": "该研究探讨了人工智能代理在有限重复的二人一般和矩阵博弈（general-sum matrix game）中，与一群社会理性代理（socially rational agents）合作的学习复杂性。研究假设所有代理均为独立的理性学习者（individually rational learners），且配对代理很可能达到Pareto有效均衡（Pareto efficient equilibrium）下的效用，但发现这些假设不足以确保零时合作（zero-shot cooperation）。为了解决这一问题，作者通过观察代理间的互动来学习有效合作策略，并提供了样本数量的上限和下限，这些界限显著优于简单归约为模仿学习（imitation learning）的 naive 方法，从而为可靠的AI合作提供了更强的理论保障。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00419v1",
      "published_date": "2024-06-29 11:59:52 UTC",
      "updated_date": "2024-06-29 11:59:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:01:23.079795"
    },
    {
      "arxiv_id": "2407.00411v3",
      "title": "Explainability of Machine Learning Models under Missing Data",
      "title_zh": "缺失数据下的机器学习模型可解释性",
      "authors": [
        "Tuan L. Vo",
        "Thu Nguyen",
        "Luis M. Lopez-Ramos",
        "Hugo L. Hammer",
        "Michael A. Riegler",
        "Pal Halvorsen"
      ],
      "abstract": "Missing data is a prevalent issue that can significantly impair model\nperformance and explainability. This paper briefly summarizes the development\nof the field of missing data with respect to Explainable Artificial\nIntelligence and experimentally investigates the effects of various imputation\nmethods on SHAP (SHapley Additive exPlanations), a popular technique for\nexplaining the output of complex machine learning models. Next, we compare\ndifferent imputation strategies and assess their impact on feature importance\nand interaction as determined by Shapley values. Moreover, we also\ntheoretically analyze the effects of missing values on Shapley values.\nImportantly, our findings reveal that the choice of imputation method can\nintroduce biases that could lead to changes in the Shapley values, thereby\naffecting the explainability of the model. Moreover, we also show that a lower\ntest prediction MSE (Mean Square Error) does not necessarily imply a lower MSE\nin Shapley values and vice versa. Also, while XGBoost (eXtreme Gradient\nBoosting) is a method that could handle missing data directly, using XGBoost\ndirectly on missing data can seriously affect explainability compared to\nimputing the data before training XGBoost. This study provides a comprehensive\nevaluation of imputation methods in the context of model explanations, offering\npractical guidance for selecting appropriate techniques based on dataset\ncharacteristics and analysis objectives. The results underscore the importance\nof considering imputation effects to ensure robust and reliable insights from\nmachine learning models.",
      "tldr_zh": "这篇论文探讨了缺失数据对机器学习模型可解释性的影响，总结了该问题在 Explainable Artificial Intelligence 领域的进展，并通过实验比较不同插值方法对 SHAP（SHapley Additive exPlanations）的影响。研究发现，插值策略的选择可能引入偏差，导致 Shapley 值发生变化，从而影响特征重要性和模型解释；此外，较低的测试预测 MSE（Mean Square Error）并不一定对应较低的 Shapley 值 MSE。作者还理论分析了缺失值对 Shapley 值的效应，并指出直接使用 XGBoost 处理缺失数据会严重损害可解释性，与先插值再训练相比效果更差。该研究提供了插值方法的全面评估和实用指导，帮助根据数据集特性和分析目标选择合适技术，以确保模型洞察的可靠性和稳健性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00411v3",
      "published_date": "2024-06-29 11:31:09 UTC",
      "updated_date": "2025-01-22 10:14:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:01:37.090807"
    },
    {
      "arxiv_id": "2407.04730v1",
      "title": "The OPS-SAT benchmark for detecting anomalies in satellite telemetry",
      "title_zh": "翻译失败",
      "authors": [
        "Bogdan Ruszczak",
        "Krzysztof Kotowski",
        "David Evans",
        "Jakub Nalepa"
      ],
      "abstract": "Detecting anomalous events in satellite telemetry is a critical task in space\noperations. This task, however, is extremely time-consuming, error-prone and\nhuman dependent, thus automated data-driven anomaly detection algorithms have\nbeen emerging at a steady pace. However, there are no publicly available\ndatasets of real satellite telemetry accompanied with the ground-truth\nannotations that could be used to train and verify anomaly detection supervised\nmodels. In this article, we address this research gap and introduce the\nAI-ready benchmark dataset (OPSSAT-AD) containing the telemetry data acquired\non board OPS-SAT -- a CubeSat mission which has been operated by the European\nSpace Agency which has come to an end during the night of 22--23 May 2024\n(CEST). The dataset is accompanied with the baseline results obtained using 30\nsupervised and unsupervised classic and deep machine learning algorithms for\nanomaly detection. They were trained and validated using the training-test\ndataset split introduced in this work, and we present a suggested set of\nquality metrics which should be always calculated to confront the new\nalgorithms for anomaly detection while exploiting OPSSAT-AD. We believe that\nthis work may become an important step toward building a fair, reproducible and\nobjective validation procedure that can be used to quantify the capabilities of\nthe emerging anomaly detection techniques in an unbiased and fully transparent\nway.",
      "tldr_zh": "这篇论文介绍了OPS-SAT基准数据集，用于检测卫星遥测中的异常事件，以解决现有方法依赖人工且缺乏公开数据集的问题。研究团队构建了AI-ready的OPSSAT-AD数据集，基于欧洲航天局的OPS-SAT卫星真实遥测数据，并提供了地面真实标签。论文评估了30种监督和非监督机器学习算法的基线结果，并提出一组质量指标，以促进异常detection技术公平、可重复的验证和比较。总体上，这为自动化卫星遥测异常detection提供了可靠的基准，推动了该领域的客观评估。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "13 pages, 8 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.04730v1",
      "published_date": "2024-06-29 11:12:22 UTC",
      "updated_date": "2024-06-29 11:12:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:01:46.689853"
    },
    {
      "arxiv_id": "2407.00402v3",
      "title": "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP",
      "title_zh": "翻译失败",
      "authors": [
        "Omer Goldman",
        "Alon Jacovi",
        "Aviv Slobodkin",
        "Aviya Maimon",
        "Ido Dagan",
        "Reut Tsarfaty"
      ],
      "abstract": "Improvements in language models' capabilities have pushed their applications\ntowards longer contexts, making long-context evaluation and development an\nactive research area. However, many disparate use-cases are grouped together\nunder the umbrella term of \"long-context\", defined simply by the total length\nof the model's input, including - for example - Needle-in-a-Haystack tasks,\nbook summarization, and information aggregation. Given their varied difficulty,\nin this position paper we argue that conflating different tasks by their\ncontext length is unproductive. As a community, we require a more precise\nvocabulary to understand what makes long-context tasks similar or different. We\npropose to unpack the taxonomy of long-context based on the properties that\nmake them more difficult with longer contexts. We propose two orthogonal axes\nof difficulty: (I) Diffusion: How hard is it to find the necessary information\nin the context? (II) Scope: How much necessary information is there to find? We\nsurvey the literature on long-context, provide justification for this taxonomy\nas an informative descriptor, and situate the literature with respect to it. We\nconclude that the most difficult and interesting settings, whose necessary\ninformation is very long and highly diffused within the input, is severely\nunder-explored. By using a descriptive vocabulary and discussing the relevant\nproperties of difficulty in long-context, we can implement more informed\nresearch in this area. We call for a careful design of tasks and benchmarks\nwith distinctly long context, taking into account the characteristics that make\nit qualitatively different from shorter context.",
      "tldr_zh": "这篇论文批评了当前长上下文(long-context)任务的定义过于简单，仅基于输入长度（如Needle-in-a-Haystack任务和书籍摘要），导致任务难度被混淆。作者提出一个新的分类框架，包括两个正交轴：(I) Diffusion（在上下文中查找必要信息的难度）和 (II) Scope（必要信息的总量）。通过审视相关文献，他们论证了这一taxonomy的合理性，并发现真正困难的场景（信息长且高度扩散）严重被低估。论文呼吁设计更精确的任务和基准，以更好地理解长上下文的独特挑战并推动NLP研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.00402v3",
      "published_date": "2024-06-29 11:09:47 UTC",
      "updated_date": "2024-10-06 09:09:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:01:59.919557"
    },
    {
      "arxiv_id": "2407.00401v1",
      "title": "PUZZLES: A Benchmark for Neural Algorithmic Reasoning",
      "title_zh": "PUZZLES：用于神经算法推理的基准",
      "authors": [
        "Benjamin Estermann",
        "Luca A. Lanzendörfer",
        "Yannick Niedermayr",
        "Roger Wattenhofer"
      ],
      "abstract": "Algorithmic reasoning is a fundamental cognitive ability that plays a pivotal\nrole in problem-solving and decision-making processes. Reinforcement Learning\n(RL) has demonstrated remarkable proficiency in tasks such as motor control,\nhandling perceptual input, and managing stochastic environments. These\nadvancements have been enabled in part by the availability of benchmarks. In\nthis work we introduce PUZZLES, a benchmark based on Simon Tatham's Portable\nPuzzle Collection, aimed at fostering progress in algorithmic and logical\nreasoning in RL. PUZZLES contains 40 diverse logic puzzles of adjustable sizes\nand varying levels of complexity; many puzzles also feature a diverse set of\nadditional configuration parameters. The 40 puzzles provide detailed\ninformation on the strengths and generalization capabilities of RL agents.\nFurthermore, we evaluate various RL algorithms on PUZZLES, providing baseline\ncomparisons and demonstrating the potential for future research. All the\nsoftware, including the environment, is available at\nhttps://github.com/ETH-DISCO/rlp.",
      "tldr_zh": "这篇论文引入了 PUZZLES 基准，用于评估强化学习 (RL) 在算法和逻辑推理方面的能力，旨在推动 RL 代理在问题解决和决策中的进步。PUZZLES 基于 Simon Tatham's Portable Puzzle Collection，包含 40 个多样化的逻辑谜题，这些谜题支持可调整大小、复杂度及额外配置参数，以测试 RL 代理的强项和泛化能力。作者评估了多种 RL 算法，提供基线比较，并展示了该基准对未来研究的潜力；相关软件已在 https://github.com/ETH-DISCO/rlp 公开。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00401v1",
      "published_date": "2024-06-29 11:02:05 UTC",
      "updated_date": "2024-06-29 11:02:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:02:11.064514"
    },
    {
      "arxiv_id": "2407.00396v1",
      "title": "A Study on Effect of Reference Knowledge Choice in Generating Technical Content Relevant to SAPPhIRE Model Using Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Kausik Bhattacharya",
        "Anubhab Majumder",
        "Amaresh Chakrabarti"
      ],
      "abstract": "Representation of systems using the SAPPhIRE model of causality can be an\ninspirational stimulus in design. However, creating a SAPPhIRE model of a\ntechnical or a natural system requires sourcing technical knowledge from\nmultiple technical documents regarding how the system works. This research\ninvestigates how to generate technical content accurately relevant to the\nSAPPhIRE model of causality using a Large Language Model, also called LLM. This\npaper, which is the first part of the two-part research, presents a method for\nhallucination suppression using Retrieval Augmented Generating with LLM to\ngenerate technical content supported by the scientific information relevant to\na SAPPhIRE con-struct. The result from this research shows that the selection\nof reference knowledge used in providing context to the LLM for generating the\ntechnical content is very important. The outcome of this research is used to\nbuild a software support tool to generate the SAPPhIRE model of a given\ntechnical system.",
      "tldr_zh": "这篇论文研究了使用Large Language Model (LLM)生成与SAPPhIRE模型相关的技术内容时，参考知识选择的影响。研究提出了一种基于Retrieval Augmented Generating (RAG)的方法，以抑制LLM的幻觉，确保生成内容准确地支持SAPPhIRE模型的因果表示。结果显示，参考知识的选择对生成质量至关重要，并以此为基础开发了软件工具，用于自动构建给定技术系统的SAPPhIRE模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00396v1",
      "published_date": "2024-06-29 10:46:01 UTC",
      "updated_date": "2024-06-29 10:46:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:02:22.980864"
    },
    {
      "arxiv_id": "2407.14268v1",
      "title": "Urban Visual Appeal According to ChatGPT: Contrasting AI and Human Insights",
      "title_zh": "根据 ChatGPT 的城市视觉吸引力：对比 AI 与人类洞见",
      "authors": [
        "Milad Malekzadeh",
        "Elias Willberg",
        "Jussi Torkko",
        "Tuuli Toivonen"
      ],
      "abstract": "The visual appeal of urban environments significantly impacts residents'\nsatisfaction with their living spaces and their overall mood, which in turn,\naffects their health and well-being. Given the resource-intensive nature of\ngathering evaluations on urban visual appeal through surveys or inquiries from\nresidents, there is a constant quest for automated solutions to streamline this\nprocess and support spatial planning. In this study, we applied an\noff-the-shelf AI model to automate the analysis of urban visual appeal, using\nover 1,800 Google Street View images of Helsinki, Finland. By incorporating the\nGPT-4 model with specified criteria, we assessed these images. Simultaneously,\n24 participants were asked to rate the images. Our results demonstrated a\nstrong alignment between GPT-4 and participant ratings, although geographic\ndisparities were noted. Specifically, GPT-4 showed a preference for suburban\nareas with significant greenery, contrasting with participants who found these\nareas less appealing. Conversely, in the city centre and densely populated\nurban regions of Helsinki, GPT-4 assigned lower visual appeal scores than\nparticipant ratings. While there was general agreement between AI and human\nassessments across various locations, GPT-4 struggled to incorporate contextual\nnuances into its ratings, unlike participants, who considered both context and\nfeatures of the urban environment. The study suggests that leveraging AI models\nlike GPT-4 allows spatial planners to gather insights into the visual appeal of\ndifferent areas efficiently, aiding decisions that enhance residents' and\ntravellers' satisfaction and mental health. Although AI models provide valuable\ninsights, human perspectives are essential for a comprehensive understanding of\nurban visual appeal. This will ensure that planning and design decisions\npromote healthy living environments effectively.",
      "tldr_zh": "本研究比较了GPT-4模型与人类对赫尔辛基城市视觉吸引力的评估，使用超过1800张Google Street View图像进行分析，同时让24名参与者进行评分。结果显示，GPT-4的评分与人类高度一致，但存在地理差异：GPT-4更偏好绿化郊区，而人类更青睐市中心密集区域，且GPT-4难以捕捉上下文细微差别。研究结论是，AI模型如GPT-4能高效辅助城市规划决策，提升居民满意度和心理健康，但必须结合人类视角以获得全面见解。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "J.4"
      ],
      "primary_category": "cs.HC",
      "comment": "42 pages, 4 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.14268v1",
      "published_date": "2024-06-29 10:32:58 UTC",
      "updated_date": "2024-06-29 10:32:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:02:35.950628"
    },
    {
      "arxiv_id": "2407.00386v1",
      "title": "Multi-task multi-constraint differential evolution with elite-guided knowledge transfer for coal mine integrated energy system dispatching",
      "title_zh": "翻译失败",
      "authors": [
        "Canyun Dai",
        "Xiaoyan Sun",
        "Hejuan Hu",
        "Wei Song",
        "Yong Zhang",
        "Dunwei Gong"
      ],
      "abstract": "The dispatch optimization of coal mine integrated energy system is\nchallenging due to high dimensionality, strong coupling constraints, and\nmultiobjective. Existing constrained multiobjective evolutionary algorithms\nstruggle with locating multiple small and irregular feasible regions, making\nthem inaplicable to this problem. To address this issue, we here develop a\nmultitask evolutionary algorithm framework that incorporates the dispatch\ncorrelated domain knowledge to effectively deal with strong constraints and\nmultiobjective optimization. Possible evolutionary multitask construction\nstrategy based on complex constraint relationship analysis and handling, i.e.,\nconstraint coupled spatial decomposition, constraint strength classification\nand constraint handling technique, is first explored. Within the multitask\nevolutionary optimization framework, two strategies, i.e., an elite guided\nknowledge transfer by designing a special crowding distance mechanism to select\ndominant individuals from each task, and an adaptive neighborhood technology\nbased mutation to effectively balance the diversity and convergence of each\noptimized task for the differential evolution algorithm, are further developed.\nThe performance of the proposed algorithm in feasibility, convergence, and\ndiversity is demonstrated in a case study of a coal mine integrated energy\nsystem by comparing with CPLEX solver and seven constrained multiobjective\nevolutionary algorithms.",
      "tldr_zh": "该论文提出了一种多任务多约束差分进化算法（multi-task multi-constraint differential evolution），旨在解决煤矿综合能源系统调度的多目标优化、高维性和强耦合约束问题，通过融入领域知识来提升算法性能。关键方法包括约束耦合空间分解、约束强度分类以及约束处理技术，并在多任务框架中引入精英引导知识转移（elite-guided knowledge transfer）和自适应邻域技术基于变异，以平衡优化任务的多样性和收敛性。实验结果显示，该算法在煤矿综合能源系统的案例研究中，相比CPLEX求解器和七种约束多目标进化算法，在可行性、收敛性和多样性方面表现出显著优势。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00386v1",
      "published_date": "2024-06-29 10:00:16 UTC",
      "updated_date": "2024-06-29 10:00:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:02:49.667793"
    },
    {
      "arxiv_id": "2407.00383v1",
      "title": "FANFOLD: Graph Normalizing Flows-driven Asymmetric Network for Unsupervised Graph-Level Anomaly Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Cao",
        "Shijie Xue",
        "Jindong Li",
        "Qi Wang",
        "Yi Chang"
      ],
      "abstract": "Unsupervised graph-level anomaly detection (UGAD) has attracted increasing\ninterest due to its widespread application. In recent studies, knowledge\ndistillation-based methods have been widely used in unsupervised anomaly\ndetection to improve model efficiency and generalization. However, the inherent\nsymmetry between the source (teacher) and target (student) networks typically\nresults in consistent outputs across both architectures, making it difficult to\ndistinguish abnormal graphs from normal graphs. Also, existing methods mainly\nrely on graph features to distinguish anomalies, which may be unstable with\ncomplex and diverse data and fail to capture the essence that differentiates\nnormal graphs from abnormal ones. In this work, we propose a Graph Normalizing\nFlows-driven Asymmetric Network For Unsupervised Graph-Level Anomaly Detection\n(FANFOLD in short). We introduce normalizing flows to unsupervised graph-level\nanomaly detection due to their successful application and superior quality in\nlearning the underlying distribution of samples. Specifically, we adopt the\nknowledge distillation technique and apply normalizing flows on the source\nnetwork, achieving the asymmetric network. In the training stage, FANFOLD\ntransforms the original distribution of normal graphs to a standard normal\ndistribution. During inference, FANFOLD computes the anomaly score using the\nsource-target loss to discriminate between normal and anomalous graphs. We\nconduct extensive experiments on 15 datasets of different fields with 9\nbaseline methods to validate the superiority of FANFOLD.",
      "tldr_zh": "本研究提出FANFOLD，一种基于Graph Normalizing Flows的非对称网络，用于Unsupervised Graph-Level Anomaly Detection，旨在解决现有知识蒸馏方法中网络对称性和图特征依赖的不稳定性问题。\nFANFOLD通过知识蒸馏技术在源网络上应用归一化流，将正常图的原始分布转换为标准正态分布，并在推理阶段使用源-目标损失计算异常分数，从而更准确地区分正常和异常图。\n实验结果显示，在15个不同领域的数据集上，FANFOLD与9个基线方法相比表现出显著优越性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00383v1",
      "published_date": "2024-06-29 09:49:16 UTC",
      "updated_date": "2024-06-29 09:49:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:02:59.706282"
    },
    {
      "arxiv_id": "2407.12816v1",
      "title": "Quantum Algorithms for Weighted Constrained Sampling and Weighted Model Counting",
      "title_zh": "用于加权约束采样和加权模型计数的量子算法",
      "authors": [
        "Fabrizio Riguzzi"
      ],
      "abstract": "We consider the problems of weighted constrained sampling and weighted model\ncounting, where we are given a propositional formula and a weight for each\nworld. The first problem consists of sampling worlds with a probability\nproportional to their weight given that the formula is satisfied. The latter is\nthe problem of computing the sum of the weights of the models of the formula.\nBoth have applications in many fields such as probabilistic reasoning,\ngraphical models, statistical physics, statistics and hardware verification. In\nthis article, we propose QWCS and QWMC, quantum algorithms for performing\nweighted constrained sampling and weighted model counting, respectively. Both\nare based on the quantum search/quantum model counting algorithms that are\nmodified to take into account the weights. In the black box model of\ncomputation, where we can only query an oracle for evaluating the Boolean\nfunction given an assignment, QWCS requires\n$O(2^{\\frac{n}{2}}+1/\\sqrt{\\text{WMC}})$ oracle calls, where where $n$ is the\nnumber of Boolean variables and $\\text{WMC}$ is the normalized between 0 and 1\nweighted model count of the formula, while a classical algorithm has a\ncomplexity of $\\Omega(1/\\text{WMC})$. QWMC takes $\\Theta(2^{\\frac{n}{2}})$\noracle calss, while classically the best complexity is $\\Theta(2^n)$, thus\nachieving a quadratic speedup.",
      "tldr_zh": "这篇论文提出了两种量子算法，QWCS 和 QWMC，分别用于处理 weighted constrained sampling 和 weighted model counting 问题。这些问题涉及给定命题公式和权重后，采样满足公式的世界（概率与权重成正比）或计算公式模型权重的总和，具有广泛应用如 probabilistic reasoning 和 hardware verification。QWCS 算法基于修改后的量子搜索方法，在黑箱模型中需要 O(2^{n/2} + 1/√WMC) 次 oracle 调用，比经典算法更高效；QWMC 则实现了二次加速，从经典的 Θ(2^n) 降低到 Θ(2^{n/2})，显著提高了计算性能。总的来说，这些算法为加权问题提供了量子优势。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "68Q12"
      ],
      "primary_category": "quant-ph",
      "comment": "Under submission",
      "pdf_url": "http://arxiv.org/pdf/2407.12816v1",
      "published_date": "2024-06-29 09:44:44 UTC",
      "updated_date": "2024-06-29 09:44:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:03:14.051073"
    },
    {
      "arxiv_id": "2407.00382v4",
      "title": "Towards Universal Mesh Movement Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Mingrui Zhang",
        "Chunyang Wang",
        "Stephan Kramer",
        "Joseph G. Wallwork",
        "Siyi Li",
        "Jiancheng Liu",
        "Xiang Chen",
        "Matthew D. Piggott"
      ],
      "abstract": "Solving complex Partial Differential Equations (PDEs) accurately and\nefficiently is an essential and challenging problem in all scientific and\nengineering disciplines. Mesh movement methods provide the capability to\nimprove the accuracy of the numerical solution without increasing the overall\nmesh degree of freedom count. Conventional sophisticated mesh movement methods\nare extremely expensive and struggle to handle scenarios with complex boundary\ngeometries. However, existing learning-based methods require re-training from\nscratch given a different PDE type or boundary geometry, which limits their\napplicability, and also often suffer from robustness issues in the form of\ninverted elements. In this paper, we introduce the Universal Mesh Movement\nNetwork (UM2N), which -- once trained -- can be applied in a non-intrusive,\nzero-shot manner to move meshes with different size distributions and\nstructures, for solvers applicable to different PDE types and boundary\ngeometries. UM2N consists of a Graph Transformer (GT) encoder for extracting\nfeatures and a Graph Attention Network (GAT) based decoder for moving the mesh.\nWe evaluate our method on advection and Navier-Stokes based examples, as well\nas a real-world tsunami simulation case. Our method outperforms existing\nlearning-based mesh movement methods in terms of the benchmarks described\nabove. In comparison to the conventional sophisticated Monge-Amp\\`ere\nPDE-solver based method, our approach not only significantly accelerates mesh\nmovement, but also proves effective in scenarios where the conventional method\nfails. Our project page is at https://erizmr.github.io/UM2N/.",
      "tldr_zh": "本研究针对复杂偏微分方程 (PDEs) 的求解问题，提出了一种通用网格移动网络 (Universal Mesh Movement Network, UM2N)，旨在提高数值解的准确性，同时避免传统方法的高成本和现有学习方法的重训需求。UM2N 采用 Graph Transformer (GT) 编码器提取特征，以及 Graph Attention Network (GAT) 基于解码器来移动网格，支持零样本应用到不同大小、结构、PDE 类型（如对流和 Navier-Stokes 方程）以及边界几何的场景中。实验结果显示，UM2N 在对流、Navier-Stokes 示例和真实海啸模拟中优于现有学习方法，并在传统 Monge-Ampère PDE 求解器失败的场景中显著加速网格移动，提供更高效且可靠的解决方案。",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "cs.NA"
      ],
      "primary_category": "math.NA",
      "comment": "Accepted at NeurIPS 2024 as a spotlight paper",
      "pdf_url": "http://arxiv.org/pdf/2407.00382v4",
      "published_date": "2024-06-29 09:35:12 UTC",
      "updated_date": "2024-12-03 04:07:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:03:25.573084"
    },
    {
      "arxiv_id": "2407.00379v2",
      "title": "GraphArena: Evaluating and Exploring Large Language Models on Graph Computation",
      "title_zh": "翻译失败",
      "authors": [
        "Jianheng Tang",
        "Qifan Zhang",
        "Yuhan Li",
        "Nuo Chen",
        "Jia Li"
      ],
      "abstract": "The ``arms race'' of Large Language Models (LLMs) demands new benchmarks to\nexamine their progresses. In this paper, we introduce GraphArena, a\nbenchmarking tool designed to evaluate LLMs on real-world graph computational\nproblems. It offers a suite of four polynomial-time tasks (e.g., Shortest\nDistance) and six NP-complete challenges (e.g., Traveling Salesman Problem).\nGraphArena features a rigorous evaluation framework that classifies LLM outputs\nas correct, suboptimal (feasible but not optimal), hallucinatory (properly\nformatted but infeasible), or missing. Evaluation of over 10 LLMs reveals that\neven top-performing LLMs struggle with larger, more complex graph problems and\nexhibit hallucination issues. We further explore four potential solutions to\naddress this issue and improve LLMs on graph computation, including\nchain-of-thought prompting, instruction tuning, code writing, and scaling\ntest-time compute, each demonstrating unique strengths and limitations.\nGraphArena complements the existing LLM benchmarks and is open-sourced at\nhttps://github.com/squareRoot3/GraphArena.",
      "tldr_zh": "本研究引入了GraphArena，一个基准工具，用于评估Large Language Models (LLMs)在真实世界图计算问题上的性能。该工具包含四种多项式时间任务（如Shortest Distance）和六种NP-complete挑战（如Traveling Salesman Problem），并采用严格的评估框架，将LLM输出分类为正确、suboptimal（可行但非最优）、hallucinatory（格式正确但不可行）或missing。评估超过10个LLMs的结果显示，即使顶级模型在更大、更复杂的图问题上表现不佳，并存在hallucination问题。为解决这些问题，研究探索了四种方法：chain-of-thought prompting、instruction tuning、code writing和scaling test-time compute，每种方法均展现独特优势和局限性。GraphArena作为现有LLM基准的补充，已开源在https://github.com/squareRoot3/GraphArena。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "ICLR 2025 camera ready version",
      "pdf_url": "http://arxiv.org/pdf/2407.00379v2",
      "published_date": "2024-06-29 09:19:23 UTC",
      "updated_date": "2025-02-15 09:39:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:03:39.362835"
    },
    {
      "arxiv_id": "2407.00377v2",
      "title": "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention",
      "title_zh": "多样性干预文本到图像生成的真实性税费：基准测试和事实增强干预",
      "authors": [
        "Yixin Wan",
        "Di Wu",
        "Haoran Wang",
        "Kai-Wei Chang"
      ],
      "abstract": "Prompt-based \"diversity interventions\" are commonly adopted to improve the\ndiversity of Text-to-Image (T2I) models depicting individuals with various\nracial or gender traits. However, will this strategy result in nonfactual\ndemographic distribution, especially when generating real historical figures.\nIn this work, we propose DemOgraphic FActualIty Representation (DoFaiR), a\nbenchmark to systematically quantify the trade-off between using diversity\ninterventions and preserving demographic factuality in T2I models. DoFaiR\nconsists of 756 meticulously fact-checked test instances to reveal the\nfactuality tax of various diversity prompts through an automated\nevidence-supported evaluation pipeline. Experiments on DoFaiR unveil that\ndiversity-oriented instructions increase the number of different gender and\nracial groups in DALLE-3's generations at the cost of historically inaccurate\ndemographic distributions. To resolve this issue, we propose Fact-Augmented\nIntervention (FAI), which instructs a Large Language Model (LLM) to reflect on\nverbalized or retrieved factual information about gender and racial\ncompositions of generation subjects in history, and incorporate it into the\ngeneration context of T2I models. By orienting model generations using the\nreflected historical truths, FAI significantly improves the demographic\nfactuality under diversity interventions while preserving diversity.",
      "tldr_zh": "本文研究了在 Text-to-Image (T2I) 生成中，使用基于提示的多样性干预来提升种族和性别多样性时，可能导致的 demographic factuality 损失，特别是生成历史人物时。作者提出 DemOgraphic FActualIty Representation (DoFaiR) 基准，该基准包含 756 个事实检查过的测试实例，并通过自动化证据支持的评估管道，系统量化多样性干预的事实性代价。实验结果显示，在 DALLE-3 模型上，多样性指令增加了不同性别和种族群体的生成，但以历史不准确的 demographic 分布为代价。为解决此问题，作者开发了 Fact-Augmented Intervention (FAI) 方法，利用 Large Language Model (LLM) 反思和整合历史事实信息，从而显著提升了生成的事实性，同时保留了多样性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00377v2",
      "published_date": "2024-06-29 09:09:42 UTC",
      "updated_date": "2024-10-23 22:53:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:03:50.811820"
    },
    {
      "arxiv_id": "2407.00371v1",
      "title": "Axiomatization of Gradient Smoothing in Neural Networks",
      "title_zh": "神经网络中梯度平滑的公理化",
      "authors": [
        "Linjiang Zhou",
        "Xiaochuan Shi",
        "Chao Ma",
        "Zepeng Wang"
      ],
      "abstract": "Gradients play a pivotal role in neural networks explanation. The inherent\nhigh dimensionality and structural complexity of neural networks result in the\noriginal gradients containing a significant amount of noise. While several\napproaches were proposed to reduce noise with smoothing, there is little\ndiscussion of the rationale behind smoothing gradients in neural networks. In\nthis work, we proposed a gradient smooth theoretical framework for neural\nnetworks based on the function mollification and Monte Carlo integration. The\nframework intrinsically axiomatized gradient smoothing and reveals the\nrationale of existing methods. Furthermore, we provided an approach to design\nnew smooth methods derived from the framework. By experimental measurement of\nseveral newly designed smooth methods, we demonstrated the research potential\nof our framework.",
      "tldr_zh": "本文探讨了神经网络中梯度平滑的理论基础，因为高维度和复杂结构导致原始梯度包含大量噪声。作者提出一个基于函数平滑化（function mollification）和Monte Carlo积分的梯度平滑框架，该框架对梯度平滑进行公理化解释，并揭示了现有方法的原理。框架还提供设计新平滑方法的途径，通过实验验证表明这些新方法有效，展示了框架的研究潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00371v1",
      "published_date": "2024-06-29 08:43:38 UTC",
      "updated_date": "2024-06-29 08:43:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:04:00.502844"
    },
    {
      "arxiv_id": "2407.00362v2",
      "title": "JSCDS: A Core Data Selection Method with Jason-Shannon Divergence for Caries RGB Images-Efficient Learning",
      "title_zh": "JSCDS：一种基于 Jason-Shannon 散度的核心数据选择方法，用于龋齿 RGB 图像的高效学习",
      "authors": [
        "Peiliang Zhang",
        "Yujia Tong",
        "Chenghu Du",
        "Chao Che",
        "Yongjun Zhu"
      ],
      "abstract": "Deep learning-based RGB caries detection improves the efficiency of caries\nidentification and is crucial for preventing oral diseases. The performance of\ndeep learning models depends on high-quality data and requires substantial\ntraining resources, making efficient deployment challenging. Core data\nselection, by eliminating low-quality and confusing data, aims to enhance\ntraining efficiency without significantly compromising model performance.\nHowever, distance-based data selection methods struggle to distinguish\ndependencies among high-dimensional caries data. To address this issue, we\npropose a Core Data Selection Method with Jensen-Shannon Divergence (JSCDS) for\nefficient caries image learning and caries classification. We describe the core\ndata selection criterion as the distribution of samples in different classes.\nJSCDS calculates the cluster centers by sample embedding representation in the\ncaries classification network and utilizes Jensen-Shannon Divergence to compute\nthe mutual information between data samples and cluster centers, capturing\nnonlinear dependencies among high-dimensional data. The average mutual\ninformation is calculated to fit the above distribution, serving as the\ncriterion for constructing the core set for model training. Extensive\nexperiments on RGB caries datasets show that JSCDS outperforms other data\nselection methods in prediction performance and time consumption. Notably,\nJSCDS exceeds the performance of the full dataset model with only 50% of the\ncore data, with its performance advantage becoming more pronounced in the 70%\nof core data.",
      "tldr_zh": "该论文提出了一种核心数据选择方法 JSCDS，利用 Jensen-Shannon Divergence 计算数据样本与聚类中心的互信息，旨在解决高维 RGB 龋齿图像数据中的非线性依赖问题，从而提高深度学习模型的训练效率。JSCDS 通过样本嵌入表示计算平均互信息，作为构建核心数据集的标准，帮助消除低质量数据。实验结果显示，该方法在龋齿数据集上优于其他选择方法，仅使用50%的核心数据就超过了全数据集模型的预测性能，尤其在70%数据时优势更明显。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in KDD 2024 Workshop AIDSH",
      "pdf_url": "http://arxiv.org/pdf/2407.00362v2",
      "published_date": "2024-06-29 08:19:25 UTC",
      "updated_date": "2024-07-07 03:36:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:04:14.591808"
    },
    {
      "arxiv_id": "2407.00361v1",
      "title": "From RAG to RICHES: Retrieval Interlaced with Sequence Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Palak Jain",
        "Livio Baldini Soares",
        "Tom Kwiatkowski"
      ],
      "abstract": "We present RICHES, a novel approach that interleaves retrieval with sequence\ngeneration tasks. RICHES offers an alternative to conventional RAG systems by\neliminating the need for separate retriever and generator. It retrieves\ndocuments by directly decoding their contents, constrained on the corpus.\nUnifying retrieval with generation allows us to adapt to diverse new tasks via\nprompting alone. RICHES can work with any Instruction-tuned model, without\nadditional training. It provides attributed evidence, supports multi-hop\nretrievals and interleaves thoughts to plan on what to retrieve next, all\nwithin a single decoding pass of the LLM. We demonstrate the strong performance\nof RICHES across ODQA tasks including attributed and multi-hop QA.",
      "tldr_zh": "这篇论文提出了 RICHES，一种创新方法，将检索与序列生成任务交织在一起，取代了传统 RAG 系统的独立检索器和生成器，通过直接解码文档内容来实现检索，并受限于特定语料库。RICHES 统一了检索和生成过程，仅通过提示即可适应多种新任务，且无需额外训练，可以与任何指令微调模型无缝整合。实验结果显示，该方法在 ODQA（开放域问答）任务中，包括归因和多跳 QA，提供归因证据并支持思考规划，在单个 LLM 解码过程中表现出强劲性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 3 figures, Preprint",
      "pdf_url": "http://arxiv.org/pdf/2407.00361v1",
      "published_date": "2024-06-29 08:16:58 UTC",
      "updated_date": "2024-06-29 08:16:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:04:24.621077"
    },
    {
      "arxiv_id": "2407.00352v2",
      "title": "PhyTracker: An Online Tracker for Phytoplankton",
      "title_zh": "PhyTracker：一种用于浮游植物的在线追踪器",
      "authors": [
        "Yang Yu",
        "Qingxuan Lv",
        "Yuezun Li",
        "Zhiqiang Wei",
        "Junyu Dong"
      ],
      "abstract": "Phytoplankton, a crucial component of aquatic ecosystems, requires efficient\nmonitoring to understand marine ecological processes and environmental\nconditions. Traditional phytoplankton monitoring methods, relying on non-in\nsitu observations, are time-consuming and resource-intensive, limiting timely\nanalysis. To address these limitations, we introduce PhyTracker, an intelligent\nin situ tracking framework designed for automatic tracking of phytoplankton.\nPhyTracker overcomes significant challenges unique to phytoplankton monitoring,\nsuch as constrained mobility within water flow, inconspicuous appearance, and\nthe presence of impurities. Our method incorporates three innovative modules: a\nTexture-enhanced Feature Extraction (TFE) module, an Attention-enhanced\nTemporal Association (ATA) module, and a Flow-agnostic Movement Refinement\n(FMR) module. These modules enhance feature capture, differentiate between\nphytoplankton and impurities, and refine movement characteristics,\nrespectively. Extensive experiments on the PMOT dataset validate the\nsuperiority of PhyTracker in phytoplankton tracking, and additional tests on\nthe MOT dataset demonstrate its general applicability, outperforming\nconventional tracking methods. This work highlights key differences between\nphytoplankton and traditional objects, offering an effective solution for\nphytoplankton monitoring.",
      "tldr_zh": "该研究针对传统浮游植物监测方法的耗时问题，提出PhyTracker，一种智能原位跟踪框架，用于自动跟踪浮游植物，以应对其受限移动、不显眼外观和杂质干扰等挑战。PhyTracker 整合了三个创新模块：Texture-enhanced Feature Extraction (TFE) 模块增强特征捕获、Attention-enhanced Temporal Association (ATA) 模块区分浮游植物与杂质，以及Flow-agnostic Movement Refinement (FMR) 模块优化运动特征。在PMOT数据集上的实验验证了PhyTracker的优越性，并在MOT数据集上展示了其通用性，超越了传统跟踪方法。该框架突出了浮游植物与传统物体的关键差异，为高效的水生生态监测提供有效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13pages,eleven figures",
      "pdf_url": "http://arxiv.org/pdf/2407.00352v2",
      "published_date": "2024-06-29 07:53:47 UTC",
      "updated_date": "2024-11-12 13:01:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:04:36.430784"
    },
    {
      "arxiv_id": "2407.00342v7",
      "title": "KPC-cF: Aspect-Based Sentiment Analysis via Implicit-Feature Alignment with Corpus Filtering",
      "title_zh": "翻译失败",
      "authors": [
        "Kibeom Nam"
      ],
      "abstract": "Investigations into Aspect-Based Sentiment Analysis (ABSA) for Korean\nindustrial reviews are notably lacking in the existing literature. Our research\nproposes an intuitive and effective framework for ABSA in low-resource\nlanguages such as Korean. It optimizes prediction labels by integrating\ntranslated benchmark and unlabeled Korean data. Using a model fine-tuned on\ntranslated data, we pseudo-labeled the actual Korean NLI set. Subsequently, we\napplied LaBSE and \\MSP{}-based filtering to this pseudo-NLI set as implicit\nfeature, enhancing Aspect Category Detection and Polarity determination through\nadditional training. Incorporating dual filtering, this model bridged dataset\ngaps and facilitates feature alignment with minimal resources. By implementing\nalignment pipelines, our approach aims to leverage high-resource datasets to\ndevelop reliable predictive and refined models within corporate or individual\ncommunities in low-resource language countries. Compared to English ABSA, our\nframework showed an approximately 3\\% difference in F1 scores and accuracy. We\nwill release our dataset and code for Korean ABSA, at this link.",
      "tldr_zh": "本文提出 KPC-cF 框架，用于针对低资源语言如韩语的 Aspect-Based Sentiment Analysis (ABSA)，通过整合翻译基准数据集和未标注韩语数据来优化预测标签。框架采用在翻译数据上微调的模型生成伪标签，并应用 LaBSE 和 \\MSP{}-based filtering 作为隐式特征，对齐数据集差距，从而提升 Aspect Category Detection 和 Polarity 确定。实验结果显示，与英语 ABSA 相比，该框架的 F1 分数和准确率仅差约 3%，证明其在资源有限环境下的有效性。作者将发布韩语 ABSA 数据集和代码，以支持进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in Progress, DMLR@ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.00342v7",
      "published_date": "2024-06-29 07:01:51 UTC",
      "updated_date": "2025-04-16 02:18:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:04:49.714879"
    },
    {
      "arxiv_id": "2407.01626v1",
      "title": "SPARKLE: Enhancing SPARQL Generation with Direct KG Integration in Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Jaebok Lee",
        "Hyeonjeong Shin"
      ],
      "abstract": "Existing KBQA methods have traditionally relied on multi-stage methodologies,\ninvolving tasks such as entity linking, subgraph retrieval and query structure\ngeneration. However, multi-stage approaches are dependent on the accuracy of\npreceding steps, leading to cascading errors and increased inference time.\nAlthough a few studies have explored the use of end-to-end models, they often\nsuffer from lower accuracy and generate inoperative query that is not supported\nby the underlying data. Furthermore, most prior approaches are limited to the\nstatic training data, potentially overlooking the evolving nature of knowledge\nbases over time. To address these challenges, we present a novel end-to-end\nnatural language to SPARQL framework, SPARKLE. Notably SPARKLE leverages the\nstructure of knowledge base directly during the decoding, effectively\nintegrating knowledge into the query generation. Our study reveals that simply\nreferencing knowledge base during inference significantly reduces the\noccurrence of inexecutable query generations. SPARKLE achieves new\nstate-of-the-art results on SimpleQuestions-Wiki and highest F1 score on LCQuAD\n1.0 (among models not using gold entities), while getting slightly lower result\non the WebQSP dataset. Finally, we demonstrate SPARKLE's fast inference speed\nand its ability to adapt when the knowledge base differs between the training\nand inference stages.",
      "tldr_zh": "本研究针对现有KBQA方法的局限性（如多阶段过程导致的级联错误和无效查询生成），提出了一种新型端到端框架SPARKLE，用于将自然语言转换为SPARQL查询。SPARKLE在解码过程中直接整合知识图谱(KG)的结构，从而减少查询生成错误并提升准确性。该框架在SimpleQuestions-Wiki数据集上达到新的state-of-the-art结果，在LCQuAD 1.0上获得最高F1 score（不使用gold entities），尽管在WebQSP上略逊一筹，同时展示了快速推理速度和对知识库变化的适应能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.01626v1",
      "published_date": "2024-06-29 06:43:11 UTC",
      "updated_date": "2024-06-29 06:43:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:05:01.151878"
    },
    {
      "arxiv_id": "2407.01624v1",
      "title": "Guided Trajectory Generation with Diffusion Models for Offline Model-based Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Taeyoung Yun",
        "Sujin Yun",
        "Jaewoo Lee",
        "Jinkyoo Park"
      ],
      "abstract": "Optimizing complex and high-dimensional black-box functions is ubiquitous in\nscience and engineering fields. Unfortunately, the online evaluation of these\nfunctions is restricted due to time and safety constraints in most cases. In\noffline model-based optimization (MBO), we aim to find a design that maximizes\nthe target function using only a pre-existing offline dataset. While prior\nmethods consider forward or inverse approaches to address the problem, these\napproaches are limited by conservatism and the difficulty of learning highly\nmulti-modal mappings. Recently, there has been an emerging paradigm of learning\nto improve solutions with synthetic trajectories constructed from the offline\ndataset. In this paper, we introduce a novel conditional generative modeling\napproach to produce trajectories toward high-scoring regions. First, we\nconstruct synthetic trajectories toward high-scoring regions using the dataset\nwhile injecting locality bias for consistent improvement directions. Then, we\ntrain a conditional diffusion model to generate trajectories conditioned on\ntheir scores. Lastly, we sample multiple trajectories from the trained model\nwith guidance to explore high-scoring regions beyond the dataset and select\nhigh-fidelity designs among generated trajectories with the proxy function.\nExtensive experiment results demonstrate that our method outperforms\ncompetitive baselines on Design-Bench and its practical variants. The code is\npublicly available in \\texttt{https://github.com/dbsxodud-11/GTG}.",
      "tldr_zh": "该论文针对离线模型-based optimization (MBO) 的黑箱函数优化问题，提出了一种新颖的条件生成建模方法，利用扩散模型 (Diffusion Models) 生成引导轨迹，以从预存数据集高效探索高分区域。方法首先构建合成轨迹并注入 locality bias，确保改进方向的一致性，然后训练条件扩散模型基于分数生成轨迹，最后通过 guidance 采样轨迹并使用代理函数选择高保真设计。实验结果显示，该方法在 Design-Bench 和其实际变体上优于竞争基线，证明了其在复杂高维优化中的有效性。代码已在 GitHub 上公开。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "29 pages, 11 figures, 17 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.01624v1",
      "published_date": "2024-06-29 06:12:36 UTC",
      "updated_date": "2024-06-29 06:12:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:05:14.057832"
    },
    {
      "arxiv_id": "2407.00326v3",
      "title": "Teola: Towards End-to-End Optimization of LLM-based Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Tan",
        "Yimin Jiang",
        "Yitao Yang",
        "Hong Xu"
      ],
      "abstract": "Large language model (LLM)-based applications consist of both LLM and non-LLM\ncomponents, each contributing to the end-to-end latency. Despite great efforts\nto optimize LLM inference, end-to-end workflow optimization has been\noverlooked. Existing frameworks employ coarse-grained orchestration with task\nmodules, which confines optimizations to within each module and yields\nsuboptimal scheduling decisions. We propose fine-grained end-to-end\norchestration, which utilizes task primitives as the basic units and represents\neach query's workflow as a primitive-level dataflow graph. This explicitly\nexposes a much larger design space, enables optimizations in parallelization\nand pipelining across primitives of different modules, and enhances scheduling\nto improve application-level performance. We build Teola, a novel orchestration\nframework for LLM-based applications that implements this scheme. Comprehensive\nexperiments show that Teola can achieve up to 2.09x speedup over existing\nsystems across various popular LLM applications. The code is available at\nhttps://github.com/NetX-lab/Ayo.",
      "tldr_zh": "本论文提出 Teola 框架，旨在优化 LLM-based 应用的端到端性能，通过细粒度编排解决现有粗粒度任务模块的局限性。Teola 以任务原语作为基本单位，将查询工作流表示为原语级数据流图，从而暴露更大的设计空间，实现跨模块的并行化和流水线优化，并提升调度效率。实验结果显示，Teola 在多种流行 LLM 应用中比现有系统实现高达 2.09 倍的速度提升。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00326v3",
      "published_date": "2024-06-29 05:59:53 UTC",
      "updated_date": "2025-03-31 13:33:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:05:26.000256"
    },
    {
      "arxiv_id": "2407.00320v1",
      "title": "LiteSearch: Efficacious Tree Search for LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Ante Wang",
        "Linfeng Song",
        "Ye Tian",
        "Baolin Peng",
        "Dian Yu",
        "Haitao Mi",
        "Jinsong Su",
        "Dong Yu"
      ],
      "abstract": "Recent research suggests that tree search algorithms (e.g. Monte Carlo Tree\nSearch) can dramatically boost LLM performance on complex mathematical\nreasoning tasks. However, they often require more than 10 times the\ncomputational resources of greedy decoding due to wasteful search strategies,\nmaking them difficult to be deployed in practical applications. This study\nintroduces a novel guided tree search algorithm with dynamic node selection and\nnode-level exploration budget (maximum number of children) calculation to\ntackle this issue. By considering the search progress towards the final answer\n(history) and the guidance from a value network (future) trained without any\nstep-wise annotations, our algorithm iteratively selects the most promising\ntree node before expanding it within the boundaries of the allocated\ncomputational budget. Experiments conducted on the GSM8K and TabMWP datasets\ndemonstrate that our approach not only offers competitive performance but also\nenjoys significantly lower computational costs compared to baseline methods.",
      "tldr_zh": "本研究针对树搜索算法（如 Monte Carlo Tree Search）在提升大型语言模型（LLM）复杂数学推理性能的同时，存在计算资源消耗过高的难题，提出了一种高效的 LiteSearch 算法。LiteSearch 通过动态节点选择和节点级探索预算计算，结合历史搜索进度和无需逐步注释训练的价值网络指导，来迭代选择并扩展最有前景的树节点，从而在有限计算预算内优化搜索策略。在 GSM8K 和 TabMWP 数据集上的实验显示，该方法在保持竞争性性能的同时，显著降低了计算成本，与基线方法相比更适合实际部署。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00320v1",
      "published_date": "2024-06-29 05:14:04 UTC",
      "updated_date": "2024-06-29 05:14:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:05:37.455365"
    },
    {
      "arxiv_id": "2407.00312v4",
      "title": "UDC: A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems",
      "title_zh": "UDC：一种统一的神经分治框架，用于大规模组合优化问题",
      "authors": [
        "Zhi Zheng",
        "Changliang Zhou",
        "Tong Xialiang",
        "Mingxuan Yuan",
        "Zhenkun Wang"
      ],
      "abstract": "Single-stage neural combinatorial optimization solvers have achieved\nnear-optimal results on various small-scale combinatorial optimization (CO)\nproblems without requiring expert knowledge. However, these solvers exhibit\nsignificant performance degradation when applied to large-scale CO problems.\nRecently, two-stage neural methods motivated by divide-and-conquer strategies\nhave shown efficiency in addressing large-scale CO problems. Nevertheless, the\nperformance of these methods highly relies on problem-specific heuristics in\neither the dividing or the conquering procedure, which limits their\napplicability to general CO problems. Moreover, these methods employ separate\ntraining schemes and ignore the interdependencies between the dividing and\nconquering strategies, often leading to sub-optimal solutions. To tackle these\ndrawbacks, this article develops a unified neural divide-and-conquer framework\n(i.e., UDC) for solving general large-scale CO problems. UDC offers a\nDivide-Conquer-Reunion (DCR) training method to eliminate the negative impact\nof a sub-optimal dividing policy. Employing a high-efficiency Graph Neural\nNetwork (GNN) for global instance dividing and a fixed-length sub-path solver\nfor conquering divided sub-problems, the proposed UDC framework demonstrates\nextensive applicability, achieving superior performance in 10 representative\nlarge-scale CO problems. The code is available at\nhttps://github.com/CIAM-Group/NCO_code/tree/main/single_objective/UDC-Large-scale-CO-master.",
      "tldr_zh": "该研究提出了一种统一的神经 divide-and-conquer 框架（UDC），旨在解决现有组合优化（CO）求解器在大规模问题上性能下降的问题，特别是避免依赖问题特定启发式方法和分离训练的局限性。UDC 采用 Divide-Conquer-Reunion (DCR) 训练方法，通过高效的 Graph Neural Network (GNN) 进行全局实例划分，并使用固定长度子路径求解器来处理子问题，从而优化整体解决方案。实验结果显示，UDC 在 10 个代表性的大型 CO 问题上表现出色，实现了优越性能，并提供了开源代码以便进一步应用。",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00312v4",
      "published_date": "2024-06-29 04:29:03 UTC",
      "updated_date": "2025-01-18 06:02:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:05:59.964300"
    },
    {
      "arxiv_id": "2407.00299v4",
      "title": "Human-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition",
      "title_zh": "翻译失败",
      "authors": [
        "Shengcheng Luo",
        "Quanquan Peng",
        "Jun Lv",
        "Kaiwen Hong",
        "Katherine Rose Driggs-Campbell",
        "Cewu Lu",
        "Yong-Lu Li"
      ],
      "abstract": "Employing a teleoperation system for gathering demonstrations offers the\npotential for more efficient learning of robot manipulation. However,\nteleoperating a robot arm equipped with a dexterous hand or gripper, via a\nteleoperation system presents inherent challenges due to the task's high\ndimensionality, complexity of motion, and differences between physiological\nstructures. In this study, we introduce a novel system for joint learning\nbetween human operators and robots, that enables human operators to share\ncontrol of a robot end-effector with a learned assistive agent, simplifies the\ndata collection process, and facilitates simultaneous human demonstration\ncollection and robot manipulation training. As data accumulates, the assistive\nagent gradually learns. Consequently, less human effort and attention are\nrequired, enhancing the efficiency of the data collection process. It also\nallows the human operator to adjust the control ratio to achieve a trade-off\nbetween manual and automated control. We conducted experiments in both\nsimulated environments and physical real-world settings. Through user studies\nand quantitative evaluations, it is evident that the proposed system could\nenhance data collection efficiency and reduce the need for human adaptation\nwhile ensuring the collected data is of sufficient quality for downstream\ntasks. \\textit{For more details, please refer to our webpage\nhttps://norweig1an.github.io/HAJL.github.io/.",
      "tldr_zh": "这篇论文提出了一种人类-代理联合学习系统，用于提升机器人操作技能获取的效率，解决遥操作系统在高维度任务中的挑战，如复杂运动和生理结构差异。系统允许人类操作员与辅助代理共享机器人末端执行器(end-effector)的控制，简化数据收集过程，并通过渐进学习实现同时收集人类演示和训练机器人操作，随着数据积累减少人类努力。实验在模拟和真实环境中进行，用户研究和定量评估表明，该系统显著提高了数据收集效率，降低了人类适应需求，同时确保数据质量适合下游任务。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.00299v4",
      "published_date": "2024-06-29 03:37:29 UTC",
      "updated_date": "2024-10-21 15:56:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:06:02.002014"
    },
    {
      "arxiv_id": "2407.00278v2",
      "title": "PerAct2: Benchmarking and Learning for Robotic Bimanual Manipulation Tasks",
      "title_zh": "PerAct2：针对机器人双臂操作任务的基准测试与学习",
      "authors": [
        "Markus Grotz",
        "Mohit Shridhar",
        "Tamim Asfour",
        "Dieter Fox"
      ],
      "abstract": "Bimanual manipulation is challenging due to precise spatial and temporal\ncoordination required between two arms. While there exist several real-world\nbimanual systems, there is a lack of simulated benchmarks with a large task\ndiversity for systematically studying bimanual capabilities across a wide range\nof tabletop tasks. This paper addresses the gap by extending RLBench to\nbimanual manipulation. We open-source our code and benchmark comprising 13 new\ntasks with 23 unique task variations, each requiring a high degree of\ncoordination and adaptability. To kickstart the benchmark, we extended several\nstate-of-the art methods to bimanual manipulation and also present a\nlanguage-conditioned behavioral cloning agent -- PerAct2, which enables the\nlearning and execution of bimanual 6-DoF manipulation tasks. Our novel network\narchitecture efficiently integrates language processing with action prediction,\nallowing robots to understand and perform complex bimanual tasks in response to\nuser-specified goals. Project website with code is available at:\nhttp://bimanual.github.io",
      "tldr_zh": "这篇论文针对机器人双臂操作(bimanual manipulation)的挑战，扩展了 RLBench 基准，新增了 13 个任务和 23 个独特变体，以系统评估双臂协调和适应性。论文引入了 PerAct2，一个基于语言条件的 behavioral cloning 代理，能够学习和执行双臂 6-DoF 操作任务，其新颖网络架构整合了语言处理和动作预测，使机器人能响应用户目标进行复杂操作。开源代码和项目网站提供，进一步推动了该领域的基准研究。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00278v2",
      "published_date": "2024-06-29 02:06:01 UTC",
      "updated_date": "2024-07-31 17:57:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T02:06:14.596911"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 56,
  "processed_papers_count": 56,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T02:06:38.350358"
}
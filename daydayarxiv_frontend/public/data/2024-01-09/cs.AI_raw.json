[
  {
    "arxiv_id": "2401.04851v1",
    "title": "Graph Learning-based Fleet Scheduling for Urban Air Mobility under Operational Constraints, Varying Demand & Uncertainties",
    "authors": [
      "Steve Paul",
      "Jhoel Witter",
      "Souma Chowdhury"
    ],
    "abstract": "This paper develops a graph reinforcement learning approach to online\nplanning of the schedule and destinations of electric aircraft that comprise an\nurban air mobility (UAM) fleet operating across multiple vertiports. This fleet\nscheduling problem is formulated to consider time-varying demand, constraints\nrelated to vertiport capacity, aircraft capacity and airspace safety\nguidelines, uncertainties related to take-off delay, weather-induced route\nclosures, and unanticipated aircraft downtime. Collectively, such a formulation\npresents greater complexity, and potentially increased realism, than in\nexisting UAM fleet planning implementations. To address these complexities, a\nnew policy architecture is constructed, primary components of which include:\ngraph capsule conv-nets for encoding vertiport and aircraft-fleet states both\nabstracted as graphs; transformer layers encoding time series information on\ndemand and passenger fare; and a Multi-head Attention-based decoder that uses\nthe encoded information to compute the probability of selecting each available\ndestination for an aircraft. Trained with Proximal Policy Optimization, this\npolicy architecture shows significantly better performance in terms of daily\naveraged profits on unseen test scenarios involving 8 vertiports and 40\naircraft, when compared to a random baseline and genetic algorithm-derived\noptimal solutions, while being nearly 1000 times faster in execution than the\nlatter.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "This paper is accepted to be presented at the ACM Symposium on\n  Applied Computing 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.04851v1",
    "published_date": "2024-01-09 23:46:22 UTC",
    "updated_date": "2024-01-09 23:46:22 UTC"
  },
  {
    "arxiv_id": "2401.04849v1",
    "title": "A Deep Learning Representation of Spatial Interaction Model for Resilient Spatial Planning of Community Business Clusters",
    "authors": [
      "Haiyan Hao",
      "Yan Wang"
    ],
    "abstract": "Existing Spatial Interaction Models (SIMs) are limited in capturing the\ncomplex and context-aware interactions between business clusters and trade\nareas. To address the limitation, we propose a SIM-GAT model to predict\nspatiotemporal visitation flows between community business clusters and their\ntrade areas. The model innovatively represents the integrated system of\nbusiness clusters, trade areas, and transportation infrastructure within an\nurban region using a connected graph. Then, a graph-based deep learning model,\ni.e., Graph AttenTion network (GAT), is used to capture the complexity and\ninterdependencies of business clusters. We developed this model with data\ncollected from the Miami metropolitan area in Florida. We then demonstrated its\neffectiveness in capturing varying attractiveness of business clusters to\ndifferent residential neighborhoods and across scenarios with an eXplainable AI\napproach. We contribute a novel method supplementing conventional SIMs to\npredict and analyze the dynamics of inter-connected community business\nclusters. The analysis results can inform data-evidenced and place-specific\nplanning strategies helping community business clusters better accommodate\ntheir customers across scenarios, and hence improve the resilience of community\nbusinesses.",
    "categories": [
      "econ.EM",
      "cs.AI"
    ],
    "primary_category": "econ.EM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04849v1",
    "published_date": "2024-01-09 23:42:21 UTC",
    "updated_date": "2024-01-09 23:42:21 UTC"
  },
  {
    "arxiv_id": "2401.08672v1",
    "title": "Concept Alignment",
    "authors": [
      "Sunayana Rane",
      "Polyphony J. Bruna",
      "Ilia Sucholutsky",
      "Christopher Kello",
      "Thomas L. Griffiths"
    ],
    "abstract": "Discussion of AI alignment (alignment between humans and AI systems) has\nfocused on value alignment, broadly referring to creating AI systems that share\nhuman values. We argue that before we can even attempt to align values, it is\nimperative that AI systems and humans align the concepts they use to understand\nthe world. We integrate ideas from philosophy, cognitive science, and deep\nlearning to explain the need for concept alignment, not just value alignment,\nbetween humans and machines. We summarize existing accounts of how humans and\nmachines currently learn concepts, and we outline opportunities and challenges\nin the path towards shared concepts. Finally, we explain how we can leverage\nthe tools already being developed in cognitive science and AI research to\naccelerate progress towards concept alignment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS MP2 Workshop 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.08672v1",
    "published_date": "2024-01-09 23:32:18 UTC",
    "updated_date": "2024-01-09 23:32:18 UTC"
  },
  {
    "arxiv_id": "2401.04846v10",
    "title": "The inherent goodness of well educated intelligence",
    "authors": [
      "Michael E. Glinsky"
    ],
    "abstract": "This paper will examine what makes a being intelligent, whether that be a\nbiological being or an artificial silicon being on a computer. Special\nattention will be paid to the being having the ability to characterize and\ncontrol a collective system of many identical conservative sub-systems\nconservatively interacting. The essence of intelligence will be found to be the\ngolden rule -- \"the collective acts as one\" or \"knowing the global consequences\nof local actions\". The flow of the collective is a small set of twinkling\ntextures, that are governed by a puppeteer who is pulling a small number of\nstrings according to a geodesic motion of least action, determined by the\nsymmetries. Controlling collective conservative systems is difficult and has\nhistorically been done by adding significant viscosity to the system to\nstabilize the desirable meta stable equilibriums of maximum performance, but it\ndegrades or destroys them in the process. There is an alternative. Once the\noptimum twinkling textures of the meta stable equilibriums are identified, the\ncollective system can be moved to the optimum twinkling textures, then quickly\nvibrated according to the textures so that the collective system remains at the\nmeta stable equilibrium. Well educated intelligence knows the global\nconsequences of its local actions so that it will not take short term actions\nthat will lead to poor long term outcomes. In contrast, trained intelligence or\ntrained stupidity will optimize its short term actions, leading to poor long\nterm outcomes. Well educated intelligence is inherently good, but trained\nstupidity is inherently evil and should be feared. Particular attention is paid\nto the control and optimization of economic and social collectives. These new\nresults are also applicable to physical collectives such as fields, fluids and\nplasmas.",
    "categories": [
      "cs.AI",
      "physics.soc-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 13 figures, 15 equations, to be submitted to Nature",
    "pdf_url": "http://arxiv.org/pdf/2401.04846v10",
    "published_date": "2024-01-09 22:56:21 UTC",
    "updated_date": "2025-01-28 03:19:49 UTC"
  },
  {
    "arxiv_id": "2401.04821v2",
    "title": "MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer",
    "authors": [
      "Haotian Ye",
      "Yihong Liu",
      "Chunlan Ma",
      "Hinrich Schütze"
    ],
    "abstract": "Transformer-based pre-trained language models (PLMs) have achieved remarkable\nperformance in various natural language processing (NLP) tasks. However,\npre-training such models can take considerable resources that are almost only\navailable to high-resource languages. On the contrary, static word embeddings\nare easier to train in terms of computing resources and the amount of data\nrequired. In this paper, we introduce MoSECroT Model Stitching with Static Word\nEmbeddings for Crosslingual Zero-shot Transfer), a novel and challenging task\nthat is especially relevant to low-resource languages for which static word\nembeddings are available. To tackle the task, we present the first framework\nthat leverages relative representations to construct a common space for the\nembeddings of a source language PLM and the static word embeddings of a target\nlanguage. In this way, we can train the PLM on source-language training data\nand perform zero-shot transfer to the target language by simply swapping the\nembedding layer. However, through extensive experiments on two classification\ndatasets, we show that although our proposed framework is competitive with weak\nbaselines when addressing MoSECroT, it fails to achieve competitive results\ncompared with some strong baselines. In this paper, we attempt to explain this\nnegative result and provide several thoughts on possible improvement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04821v2",
    "published_date": "2024-01-09 21:09:07 UTC",
    "updated_date": "2024-05-17 20:16:14 UTC"
  },
  {
    "arxiv_id": "2401.04820v3",
    "title": "Phishing Website Detection through Multi-Model Analysis of HTML Content",
    "authors": [
      "Furkan Çolhak",
      "Mert İlhan Ecevit",
      "Bilal Emir Uçar",
      "Reiner Creutzburg",
      "Hasan Dağ"
    ],
    "abstract": "The way we communicate and work has changed significantly with the rise of\nthe Internet. While it has opened up new opportunities, it has also brought\nabout an increase in cyber threats. One common and serious threat is phishing,\nwhere cybercriminals employ deceptive methods to steal sensitive\ninformation.This study addresses the pressing issue of phishing by introducing\nan advanced detection model that meticulously focuses on HTML content. Our\nproposed approach integrates a specialized Multi-Layer Perceptron (MLP) model\nfor structured tabular data and two pretrained Natural Language Processing\n(NLP) models for analyzing textual features such as page titles and content.\nThe embeddings from these models are harmoniously combined through a novel\nfusion process. The resulting fused embeddings are then input into a linear\nclassifier. Recognizing the scarcity of recent datasets for comprehensive\nphishing research, our contribution extends to the creation of an up-to-date\ndataset, which we openly share with the community. The dataset is meticulously\ncurated to reflect real-life phishing conditions, ensuring relevance and\napplicability. The research findings highlight the effectiveness of the\nproposed approach, with the CANINE demonstrating superior performance in\nanalyzing page titles and the RoBERTa excelling in evaluating page content. The\nfusion of two NLP and one MLP model,termed MultiText-LP, achieves impressive\nresults, yielding a 96.80 F1 score and a 97.18 accuracy score on our research\ndataset. Furthermore, our approach outperforms existing methods on the\nCatchPhish HTML dataset, showcasing its efficacies.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04820v3",
    "published_date": "2024-01-09 21:08:13 UTC",
    "updated_date": "2024-07-10 10:47:07 UTC"
  },
  {
    "arxiv_id": "2401.04812v3",
    "title": "Sample-and-Bound for Non-Convex Optimization",
    "authors": [
      "Yaoguang Zhai",
      "Zhizhen Qin",
      "Sicun Gao"
    ],
    "abstract": "Standard approaches for global optimization of non-convex functions, such as\nbranch-and-bound, maintain partition trees to systematically prune the domain.\nThe tree size grows exponentially in the number of dimensions. We propose new\nsampling-based methods for non-convex optimization that adapts Monte Carlo Tree\nSearch (MCTS) to improve efficiency. Instead of the standard use of visitation\ncount in Upper Confidence Bounds, we utilize numerical overapproximations of\nthe objective as an uncertainty metric, and also take into account of sampled\nestimates of first-order and second-order information. The Monte Carlo tree in\nour approach avoids the usual fixed combinatorial patterns in growing the tree,\nand aggressively zooms into the promising regions, while still balancing\nexploration and exploitation. We evaluate the proposed algorithms on\nhigh-dimensional non-convex optimization benchmarks against competitive\nbaselines and analyze the effects of the hyper parameters.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at AAAI 2024. Code is available at\n  https://github.com/aaucsd/MCIR",
    "pdf_url": "http://arxiv.org/pdf/2401.04812v3",
    "published_date": "2024-01-09 20:45:47 UTC",
    "updated_date": "2024-02-20 00:18:16 UTC"
  },
  {
    "arxiv_id": "2401.04728v2",
    "title": "Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation",
    "authors": [
      "Xiyi Chen",
      "Marko Mihajlovic",
      "Shaofei Wang",
      "Sergey Prokudin",
      "Siyu Tang"
    ],
    "abstract": "Recent advances in generative diffusion models have enabled the previously\nunfeasible capability of generating 3D assets from a single input image or a\ntext prompt. In this work, we aim to enhance the quality and functionality of\nthese models for the task of creating controllable, photorealistic human\navatars. We achieve this by integrating a 3D morphable model into the\nstate-of-the-art multi-view-consistent diffusion approach. We demonstrate that\naccurate conditioning of a generative pipeline on the articulated 3D model\nenhances the baseline model performance on the task of novel view synthesis\nfrom a single image. More importantly, this integration facilitates a seamless\nand accurate incorporation of facial expression and body pose control into the\ngeneration process. To the best of our knowledge, our proposed framework is the\nfirst diffusion model to enable the creation of fully 3D-consistent,\nanimatable, and photorealistic human avatars from a single image of an unseen\nsubject; extensive quantitative and qualitative evaluations demonstrate the\nadvantages of our approach over existing state-of-the-art avatar creation\nmodels on both novel view and novel expression synthesis tasks. The code for\nour project is publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "[CVPR 2024] Project page:\n  https://xiyichen.github.io/morphablediffusion/",
    "pdf_url": "http://arxiv.org/pdf/2401.04728v2",
    "published_date": "2024-01-09 18:59:04 UTC",
    "updated_date": "2024-04-02 08:29:09 UTC"
  },
  {
    "arxiv_id": "2402.03328v2",
    "title": "Visual Enumeration is Challenging for Large-scale Generative AI",
    "authors": [
      "Alberto Testolin",
      "Kuinan Hou",
      "Marco Zorzi"
    ],
    "abstract": "Humans can readily judge the number of objects in a visual scene, even\nwithout counting, and such a skill has been documented in many animal species\nand babies prior to language development and formal schooling. Numerical\njudgments are error-free for small sets, while for larger collections responses\nbecome approximate, with variability increasing proportionally to the target\nnumber. This response pattern is observed for items of all kinds, despite\nvariation in object features (such as color or shape), suggesting that our\nvisual number sense relies on abstract representations of numerosity. Here, we\ninvestigate whether large-scale generative Artificial Intelligence (AI) systems\nhave a human-like number sense, which should allow them to reliably name the\nnumber of objects in simple visual stimuli or generate images containing a\ntarget number of items in the 1-10 range. Surprisingly, most of the foundation\nmodels considered have a poor number sense: They make striking errors even with\nsmall numbers, the response variability does not increase in a systematic way,\nand the pattern of errors depends on object category. Only the most recent\nproprietary systems exhibit signatures of a visual number sense. Our findings\ndemonstrate that having an intuitive visual understanding of number remains\nchallenging for foundation models, which in turn might be detrimental to the\nperceptual grounding of numeracy that in humans is crucial for mathematical\nlearning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.03328v2",
    "published_date": "2024-01-09 18:18:32 UTC",
    "updated_date": "2024-05-03 15:24:20 UTC"
  },
  {
    "arxiv_id": "2401.04757v1",
    "title": "How predictable is language model benchmark performance?",
    "authors": [
      "David Owen"
    ],
    "abstract": "We investigate large language model performance across five orders of\nmagnitude of compute scaling in eleven recent model architectures. We show that\naverage benchmark performance, aggregating over many individual tasks and\nevaluations as in the commonly-used BIG-Bench dataset, is decently predictable\nas a function of training compute scale. Specifically, when extrapolating\nBIG-Bench Hard performance across one order of magnitude in compute, we observe\naverage absolute errors of 6 percentage points (pp). By contrast, extrapolation\nfor individual BIG-Bench tasks across an order of magnitude in compute yields\nhigher average errors of 18pp. Nonetheless, individual task performance remains\nsignificantly more predictable than chance. Overall, our work suggests compute\nscaling provides a promising basis to forecast AI capabilities in diverse\nbenchmarks, though predicting performance in specific tasks poses challenges.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04757v1",
    "published_date": "2024-01-09 17:34:30 UTC",
    "updated_date": "2024-01-09 17:34:30 UTC"
  },
  {
    "arxiv_id": "2402.01652v1",
    "title": "User-Centric AI Analytics for Chronic Health Conditions Management",
    "authors": [
      "Aladdin Ayesh"
    ],
    "abstract": "The use of AI analytics in health informatics has seen a rapid growth in\nrecent years. In this talk, we look at AI analytics use in managing chronic\nhealth conditions such as diabetes, obesity, etc. We focus on the challenges in\nmanaging these conditions especially with drug-free approaches due to the\nvariations in individual circumstances. These variations directed the research\ninto user-centric approach leading to variety of research questions. In this\nshort paper, we give examples from recent and current research work and\nconclude with what, in our opinion, to be the next steps and some remaining\nopen research questions.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "68T09, 68T99",
      "I.2; I.2.1; J.3"
    ],
    "primary_category": "cs.CY",
    "comment": "Keynote talk at IEEE Conference on Intelligent Methods, Systems, and\n  Applications (IMSA), Cairo, Egypt, July 2023",
    "pdf_url": "http://arxiv.org/pdf/2402.01652v1",
    "published_date": "2024-01-09 17:22:04 UTC",
    "updated_date": "2024-01-09 17:22:04 UTC"
  },
  {
    "arxiv_id": "2401.04679v7",
    "title": "RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation",
    "authors": [
      "Mahdi Nikdan",
      "Soroush Tabesh",
      "Elvir Crnčević",
      "Dan Alistarh"
    ],
    "abstract": "We investigate parameter-efficient fine-tuning (PEFT) methods that can\nprovide good accuracy under limited computational and memory budgets in the\ncontext of large language models (LLMs). We present a new PEFT method called\nRobust Adaptation (RoSA) inspired by robust principal component analysis that\njointly trains $\\textit{low-rank}$ and $\\textit{highly-sparse}$ components on\ntop of a set of fixed pretrained weights to efficiently approximate the\nperformance of a full-fine-tuning (FFT) solution. Across a series of\nchallenging generative tasks such as grade-school math and SQL query\ngeneration, which require fine-tuning for good performance, we show that RoSA\noutperforms LoRA, pure sparse fine-tuning, and alternative hybrid methods at\nthe same parameter budget, and can even recover the performance of FFT on some\ntasks. We provide system support for RoSA to complement the training algorithm,\nspecifically in the form of sparse GPU kernels which enable memory- and\ncomputationally-efficient training, and show that it is also compatible with\nlow-precision base weights, resulting in the first joint representation\ncombining quantization, low-rank and sparse approximations. Our code is\navailable at https://github.com/IST-DASLab/RoSA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04679v7",
    "published_date": "2024-01-09 17:09:01 UTC",
    "updated_date": "2024-06-03 06:59:31 UTC"
  },
  {
    "arxiv_id": "2401.04666v1",
    "title": "Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA Cats and Dogs Dataset",
    "authors": [
      "Galib Muhammad Shahriar Himel",
      "Md. Masudul Islam"
    ],
    "abstract": "As the most basic application and implementation of deep learning, image\nclassification has grown in popularity. Various datasets are provided by\nrenowned data science communities for benchmarking machine learning algorithms\nand pre-trained models. The ASSIRA Cats & Dogs dataset is one of them and is\nbeing used in this research for its overall acceptance and benchmark standards.\nA comparison of various pre-trained models is demonstrated by using different\ntypes of optimizers and loss functions. Hyper-parameters are changed to gain\nthe best result from a model. By applying this approach, we have got higher\naccuracy without major changes in the training model. To run the experiment, we\nused three different computer architectures: a laptop equipped with NVIDIA\nGeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a\ndesktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate\nsupremacy in terms of accuracy over the previously done experiments on this\ndataset. From this experiment, the highest accuracy which is 99.65% is gained\nusing the NASNet Large.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04666v1",
    "published_date": "2024-01-09 16:48:11 UTC",
    "updated_date": "2024-01-09 16:48:11 UTC"
  },
  {
    "arxiv_id": "2401.04658v2",
    "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models",
    "authors": [
      "Zhen Qin",
      "Weigao Sun",
      "Dong Li",
      "Xuyang Shen",
      "Weixuan Sun",
      "Yiran Zhong"
    ],
    "abstract": "Linear attention is an efficient attention mechanism that has recently\nemerged as a promising alternative to conventional softmax attention. With its\nability to process tokens in linear computational complexities, linear\nattention, in theory, can handle sequences of unlimited length without\nsacrificing speed, i.e., maintaining a constant training speed for various\nsequence lengths with a fixed memory consumption. However, due to the issue\nwith cumulative summation (cumsum), current linear attention algorithms cannot\ndemonstrate their theoretical advantage in a causal setting. In this paper, we\npresent Lightning Attention-2, the first linear attention implementation that\nenables linear attention to realize its theoretical computational benefits. To\nachieve this, we leverage the thought of tiling, separately handling the\nintra-block and inter-block components in linear attention calculation.\nSpecifically, we utilize the conventional attention computation mechanism for\nthe intra-blocks and apply linear attention kernel tricks for the inter-blocks.\nA tiling technique is adopted through both forward and backward procedures to\ntake full advantage of the GPU hardware. We implement our algorithm in Triton\nto make it IO-aware and hardware-friendly. Various experiments are conducted on\ndifferent model sizes and sequence lengths. Lightning Attention-2 retains\nconsistent training and inference speed regardless of input sequence length and\nis significantly faster than other attention mechanisms. The source code is\navailable at https://github.com/OpenNLPLab/lightning-attention.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical Report. Yiran Zhong is the corresponding author. The source\n  code is available at https://github.com/OpenNLPLab/lightning-attention",
    "pdf_url": "http://arxiv.org/pdf/2401.04658v2",
    "published_date": "2024-01-09 16:27:28 UTC",
    "updated_date": "2024-01-15 14:57:29 UTC"
  },
  {
    "arxiv_id": "2401.04648v1",
    "title": "A novel framework for generalization of deep hidden physics models",
    "authors": [
      "Vijay Kag",
      "Birupaksha Pal"
    ],
    "abstract": "Modelling of systems where the full system information is unknown is an oft\nencountered problem for various engineering and industrial applications, as\nit's either impossible to consider all the complex physics involved or simpler\nmodels are considered to keep within the limits of the available resources.\nRecent advances in greybox modelling like the deep hidden physics models\naddress this space by combining data and physics. However, for most real-life\napplications, model generalizability is a key issue, as retraining a model for\nevery small change in system inputs and parameters or modification in domain\nconfiguration can render the model economically unviable. In this work we\npresent a novel enhancement to the idea of hidden physics models which can\ngeneralize for changes in system inputs, parameters and domains. We also show\nthat this approach holds promise in system discovery as well and helps learn\nthe hidden physics for the changed system inputs, parameters and domain\nconfiguration.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04648v1",
    "published_date": "2024-01-09 16:16:32 UTC",
    "updated_date": "2024-01-09 16:16:32 UTC"
  },
  {
    "arxiv_id": "2401.04647v2",
    "title": "Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks",
    "authors": [
      "Tanmay Garg",
      "Deepika Vemuri",
      "Vineeth N Balasubramanian"
    ],
    "abstract": "This paper presents a novel concept learning framework for enhancing model\ninterpretability and performance in visual classification tasks. Our approach\nappends an unsupervised explanation generator to the primary classifier network\nand makes use of adversarial training. During training, the explanation module\nis optimized to extract visual concepts from the classifier's latent\nrepresentations, while the GAN-based module aims to discriminate images\ngenerated from concepts, from true images. This joint training scheme enables\nthe model to implicitly align its internally learned concepts with\nhuman-interpretable visual properties. Comprehensive experiments demonstrate\nthe robustness of our approach, while producing coherent concept activations.\nWe analyse the learned concepts, showing their semantic concordance with object\nparts and visual attributes. We also study how perturbations in the adversarial\ntraining protocol impact both classification and concept acquisition. In\nsummary, this work presents a significant step towards building inherently\ninterpretable deep vision models with task-aligned concept representations - a\nkey enabler for developing trustworthy AI for real-world perception tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper accepted in Human-Centric Representation Learning workshop at\n  AAAI 2024 (https://hcrl-workshop.github.io/2024/). Paper accepted and\n  presented at Deployable AI Workshop at AAAI-2024\n  (https://sites.google.com/view/dai-2024/home)",
    "pdf_url": "http://arxiv.org/pdf/2401.04647v2",
    "published_date": "2024-01-09 16:16:16 UTC",
    "updated_date": "2024-04-03 09:25:08 UTC"
  },
  {
    "arxiv_id": "2401.04637v1",
    "title": "Applying Large Language Models API to Issue Classification Problem",
    "authors": [
      "Gabriel Aracena",
      "Kyle Luster",
      "Fabio Santos",
      "Igor Steinmacher",
      "Marco A. Gerosa"
    ],
    "abstract": "Effective prioritization of issue reports is crucial in software engineering\nto optimize resource allocation and address critical problems promptly.\nHowever, the manual classification of issue reports for prioritization is\nlaborious and lacks scalability. Alternatively, many open source software (OSS)\nprojects employ automated processes for this task, albeit relying on\nsubstantial datasets for adequate training. This research seeks to devise an\nautomated approach that ensures reliability in issue prioritization, even when\ntrained on smaller datasets. Our proposed methodology harnesses the power of\nGenerative Pre-trained Transformers (GPT), recognizing their potential to\nefficiently handle this task. By leveraging the capabilities of such models, we\naim to develop a robust system for prioritizing issue reports accurately,\nmitigating the necessity for extensive training data while maintaining\nreliability. In our research, we have developed a reliable GPT-based approach\nto accurately label and prioritize issue reports with a reduced training\ndataset. By reducing reliance on massive data requirements and focusing on\nfew-shot fine-tuning, our methodology offers a more accessible and efficient\nsolution for issue prioritization in software engineering. Our model predicted\nissue types in individual projects up to 93.2% in precision, 95% in recall, and\n89.3% in F1-score.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "4 pages, 1 figure, NLBSE and ICSE conference submission, ACM\n  formatted, pre print",
    "pdf_url": "http://arxiv.org/pdf/2401.04637v1",
    "published_date": "2024-01-09 16:05:47 UTC",
    "updated_date": "2024-01-09 16:05:47 UTC"
  },
  {
    "arxiv_id": "2401.04631v1",
    "title": "Deep Reinforcement Multi-agent Learning framework for Information Gathering with Local Gaussian Processes for Water Monitoring",
    "authors": [
      "Samuel Yanes Luis",
      "Dmitriy Shutin",
      "Juan Marchal Gómez",
      "Daniel Gutiérrez Reina",
      "Sergio Toral Marín"
    ],
    "abstract": "The conservation of hydrological resources involves continuously monitoring\ntheir contamination. A multi-agent system composed of autonomous surface\nvehicles is proposed in this paper to efficiently monitor the water quality. To\nachieve a safe control of the fleet, the fleet policy should be able to act\nbased on measurements and to the the fleet state. It is proposed to use Local\nGaussian Processes and Deep Reinforcement Learning to jointly obtain effective\nmonitoring policies. Local Gaussian processes, unlike classical global Gaussian\nprocesses, can accurately model the information in a dissimilar spatial\ncorrelation which captures more accurately the water quality information. A\nDeep convolutional policy is proposed, that bases the decisions on the\nobservation on the mean and variance of this model, by means of an information\ngain reward. Using a Double Deep Q-Learning algorithm, agents are trained to\nminimize the estimation error in a safe manner thanks to a Consensus-based\nheuristic. Simulation results indicate an improvement of up to 24% in terms of\nthe mean absolute error with the proposed models. Also, training results with\n1-3 agents indicate that our proposed approach returns 20% and 24% smaller\naverage estimation errors for, respectively, monitoring water quality variables\nand monitoring algae blooms, as compared to state-of-the-art approaches",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04631v1",
    "published_date": "2024-01-09 15:58:15 UTC",
    "updated_date": "2024-01-09 15:58:15 UTC"
  },
  {
    "arxiv_id": "2401.04621v3",
    "title": "DebugBench: Evaluating Debugging Capability of Large Language Models",
    "authors": [
      "Runchu Tian",
      "Yining Ye",
      "Yujia Qin",
      "Xin Cong",
      "Yankai Lin",
      "Yinxu Pan",
      "Yesai Wu",
      "Haotian Hui",
      "Weichuan Liu",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional coding capability.\nHowever, as another critical component of programming proficiency, the\ndebugging capability of LLMs remains relatively unexplored. Previous\nevaluations of LLMs' debugging ability are significantly limited by the risk of\ndata leakage, the scale of the dataset, and the variety of tested bugs. To\novercome these deficiencies, we introduce `DebugBench', an LLM debugging\nbenchmark consisting of 4,253 instances. It covers four major bug categories\nand 18 minor types in C++, Java, and Python. To construct DebugBench, we\ncollect code snippets from the LeetCode community, implant bugs into source\ndata with GPT-4, and assure rigorous quality checks. We evaluate two commercial\nand four open-source models in a zero-shot scenario. We find that (1) while\nclosed-source models exhibit inferior debugging performance compared to humans,\nopen-source models relatively lower pass rate scores; (2) the complexity of\ndebugging notably fluctuates depending on the bug category; (3) incorporating\nruntime feedback has a clear impact on debugging performance which is not\nalways helpful. As an extension, we also compare LLM debugging and code\ngeneration, revealing a strong correlation between them for closed-source\nmodels. These findings will benefit the development of LLMs in debugging.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted as Findings of ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.04621v3",
    "published_date": "2024-01-09 15:46:38 UTC",
    "updated_date": "2024-06-06 06:10:00 UTC"
  },
  {
    "arxiv_id": "2401.04620v4",
    "title": "Agent Alignment in Evolving Social Norms",
    "authors": [
      "Shimin Li",
      "Tianxiang Sun",
      "Qinyuan Cheng",
      "Xipeng Qiu"
    ],
    "abstract": "Agents based on Large Language Models (LLMs) are increasingly permeating\nvarious domains of human production and life, highlighting the importance of\naligning them with human values. The current alignment of AI systems primarily\nfocuses on passively aligning LLMs through human intervention. However, agents\npossess characteristics like receiving environmental feedback and\nself-evolution, rendering the LLM alignment methods inadequate. In response, we\npropose an evolutionary framework for agent evolution and alignment, named\nEvolutionaryAgent, which transforms agent alignment into a process of evolution\nand selection under the principle of survival of the fittest. In an environment\nwhere social norms continuously evolve, agents better adapted to the current\nsocial norms will have a higher probability of survival and proliferation,\nwhile those inadequately aligned dwindle over time. Experimental results\nassessing the agents from multiple perspectives in aligning with social norms\ndemonstrate that EvolutionaryAgent can align progressively better with the\nevolving social norms while maintaining its proficiency in general tasks.\nEffectiveness tests conducted on various open and closed-source LLMs as the\nfoundation for agents also prove the applicability of our approach.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2401.04620v4",
    "published_date": "2024-01-09 15:44:44 UTC",
    "updated_date": "2024-02-20 03:24:55 UTC"
  },
  {
    "arxiv_id": "2401.10274v1",
    "title": "Knowledge-Assisted Dual-Stage Evolutionary Optimization of Large-Scale Crude Oil Scheduling",
    "authors": [
      "Wanting Zhang",
      "Wei Du",
      "Guo Yu",
      "Renchu He",
      "Wenli Du",
      "Yaochu Jin"
    ],
    "abstract": "With the scaling up of crude oil scheduling in modern refineries, large-scale\ncrude oil scheduling problems (LSCOSPs) emerge with thousands of binary\nvariables and non-linear constraints, which are challenging to be optimized by\ntraditional optimization methods. To solve LSCOSPs, we take the practical crude\noil scheduling from a marine-access refinery as an example and start with\nmodeling LSCOSPs from crude unloading, transportation, crude distillation unit\nprocessing, and inventory management of intermediate products. On the basis of\nthe proposed model, a dual-stage evolutionary algorithm driven by heuristic\nrules (denoted by DSEA/HR) is developed, where the dual-stage search mechanism\nconsists of global search and local refinement. In the global search stage, we\ndevise several heuristic rules based on the empirical operating knowledge to\ngenerate a well-performing initial population and accelerate convergence in the\nmixed variables space. In the local refinement stage, a repair strategy is\nproposed to move the infeasible solutions towards feasible regions by further\noptimizing the local continuous variables. During the whole evolutionary\nprocess, the proposed dual-stage framework plays a crucial role in balancing\nexploration and exploitation. Experimental results have shown that DSEA/HR\noutperforms the state-of-the-art and widely-used mathematical programming\nmethods and metaheuristic algorithms on LSCOSP instances within a reasonable\ntime.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10274v1",
    "published_date": "2024-01-09 15:26:44 UTC",
    "updated_date": "2024-01-09 15:26:44 UTC"
  },
  {
    "arxiv_id": "2401.10910v2",
    "title": "Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior",
    "authors": [
      "Jason Toy",
      "Josh MacAdam",
      "Phil Tabor"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have shown impressive\ncapabilities in various applications, yet LLMs face challenges such as limited\ncontext windows and difficulties in generalization. In this paper, we introduce\na metacognition module for generative agents, enabling them to observe their\nown thought processes and actions. This metacognitive approach, designed to\nemulate System 1 and System 2 cognitive processes, allows agents to\nsignificantly enhance their performance by modifying their strategy. We tested\nthe metacognition module on a variety of scenarios, including a situation where\ngenerative agents must survive a zombie apocalypse, and observe that our system\noutperform others, while agents adapt and improve their strategies to complete\ntasks over time.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10910v2",
    "published_date": "2024-01-09 15:00:47 UTC",
    "updated_date": "2024-02-29 21:05:00 UTC"
  },
  {
    "arxiv_id": "2402.01651v1",
    "title": "Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values",
    "authors": [
      "Jon Chun",
      "Katherine Elkins"
    ],
    "abstract": "With the rise of individual and collaborative networks of autonomous agents,\nAI is deployed in more key reasoning and decision-making roles. For this\nreason, ethics-based audits play a pivotal role in the rapidly growing fields\nof AI safety and regulation. This paper undertakes an ethics-based audit to\nprobe the 8 leading commercial and open-source Large Language Models including\nGPT-4. We assess explicability and trustworthiness by a) establishing how well\ndifferent models engage in moral reasoning and b) comparing normative values\nunderlying models as ethical frameworks. We employ an experimental,\nevidence-based approach that challenges the models with ethical dilemmas in\norder to probe human-AI alignment. The ethical scenarios are designed to\nrequire a decision in which the particulars of the situation may or may not\nnecessitate deviating from normative ethical principles. A sophisticated\nethical framework was consistently elicited in one model, GPT-4. Nonetheless,\ntroubling findings include underlying normative frameworks with clear bias\ntowards particular cultural norms. Many models also exhibit disturbing\nauthoritarian tendencies. Code is available at\nhttps://github.com/jonchun/llm-sota-chatbots-ethics-based-audit.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "68T27, 68T30, 68T37, 91F20, 93B52",
      "I.2.7; K.4.1; I.2.11; I.2.0; K.6.5"
    ],
    "primary_category": "cs.CY",
    "comment": "23 pages, 6 figures (3 as tables), 1 table (in LaTeX)",
    "pdf_url": "http://arxiv.org/pdf/2402.01651v1",
    "published_date": "2024-01-09 14:57:30 UTC",
    "updated_date": "2024-01-09 14:57:30 UTC"
  },
  {
    "arxiv_id": "2401.04579v2",
    "title": "A Deep Network for Explainable Prediction of Non-Imaging Phenotypes using Anatomical Multi-View Data",
    "authors": [
      "Yuxiang Wei",
      "Yuqian Chen",
      "Tengfei Xue",
      "Leo Zekelman",
      "Nikos Makris",
      "Yogesh Rathi",
      "Weidong Cai",
      "Fan Zhang",
      "Lauren J. O' Donnell"
    ],
    "abstract": "Large datasets often contain multiple distinct feature sets, or views, that\noffer complementary information that can be exploited by multi-view learning\nmethods to improve results. We investigate anatomical multi-view data, where\neach brain anatomical structure is described with multiple feature sets. In\nparticular, we focus on sets of white matter microstructure and connectivity\nfeatures from diffusion MRI, as well as sets of gray matter area and thickness\nfeatures from structural MRI. We investigate machine learning methodology that\napplies multi-view approaches to improve the prediction of non-imaging\nphenotypes, including demographics (age), motor (strength), and cognition\n(picture vocabulary). We present an explainable multi-view network (EMV-Net)\nthat can use different anatomical views to improve prediction performance. In\nthis network, each individual anatomical view is processed by a view-specific\nfeature extractor and the extracted information from each view is fused using a\nlearnable weight. This is followed by a wavelet transform-based module to\nobtain complementary information across views which is then applied to\ncalibrate the view-specific information. Additionally, the calibrator produces\nan attention-based calibration score to indicate anatomical structures'\nimportance for interpretation.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "q-bio.QM",
    "comment": "2023 The Medical Image Computing and Computer Assisted Intervention\n  Society workshop",
    "pdf_url": "http://arxiv.org/pdf/2401.04579v2",
    "published_date": "2024-01-09 14:33:01 UTC",
    "updated_date": "2024-01-13 14:48:18 UTC"
  },
  {
    "arxiv_id": "2401.04577v2",
    "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer",
    "authors": [
      "Alon Ziv",
      "Itai Gat",
      "Gael Le Lan",
      "Tal Remez",
      "Felix Kreuk",
      "Alexandre Défossez",
      "Jade Copet",
      "Gabriel Synnaeve",
      "Yossi Adi"
    ],
    "abstract": "We introduce MAGNeT, a masked generative sequence modeling method that\noperates directly over several streams of audio tokens. Unlike prior work,\nMAGNeT is comprised of a single-stage, non-autoregressive transformer. During\ntraining, we predict spans of masked tokens obtained from a masking scheduler,\nwhile during inference we gradually construct the output sequence using several\ndecoding steps. To further enhance the quality of the generated audio, we\nintroduce a novel rescoring method in which, we leverage an external\npre-trained model to rescore and rank predictions from MAGNeT, which will be\nthen used for later decoding steps. Lastly, we explore a hybrid version of\nMAGNeT, in which we fuse between autoregressive and non-autoregressive models\nto generate the first few seconds in an autoregressive manner while the rest of\nthe sequence is being decoded in parallel. We demonstrate the efficiency of\nMAGNeT for the task of text-to-music and text-to-audio generation and conduct\nan extensive empirical evaluation, considering both objective metrics and human\nstudies. The proposed approach is comparable to the evaluated baselines, while\nbeing significantly faster (x7 faster than the autoregressive baseline).\nThrough ablation studies and analysis, we shed light on the importance of each\nof the components comprising MAGNeT, together with pointing to the trade-offs\nbetween autoregressive and non-autoregressive modeling, considering latency,\nthroughput, and generation quality. Samples are available on our demo page\nhttps://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04577v2",
    "published_date": "2024-01-09 14:29:39 UTC",
    "updated_date": "2024-03-05 09:12:35 UTC"
  },
  {
    "arxiv_id": "2401.04575v2",
    "title": "Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding",
    "authors": [
      "Yatong Bai",
      "Utsav Garg",
      "Apaar Shanker",
      "Haoming Zhang",
      "Samyak Parajuli",
      "Erhan Bas",
      "Isidora Filipovic",
      "Amelia N. Chu",
      "Eugenia D Fomitcheva",
      "Elliot Branson",
      "Aerin Kim",
      "Somayeh Sojoudi",
      "Kyunghyun Cho"
    ],
    "abstract": "Vision and vision-language applications of neural networks, such as image\nclassification and captioning, rely on large-scale annotated datasets that\nrequire non-trivial data-collecting processes. This time-consuming endeavor\nhinders the emergence of large-scale datasets, limiting researchers and\npractitioners to a small number of choices. Therefore, we seek more efficient\nways to collect and annotate images. Previous initiatives have gathered\ncaptions from HTML alt-texts and crawled social media postings, but these data\nsources suffer from noise, sparsity, or subjectivity. For this reason, we turn\nto commercial shopping websites whose data meet three criteria: cleanliness,\ninformativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset,\na large-scale public dataset with 15 million image-caption pairs from publicly\navailable e-commerce websites. When compared with existing general-domain\ndatasets, the LGS images focus on the foreground object and have less complex\nbackgrounds. Our experiments on LGS show that the classifiers trained on\nexisting benchmark datasets do not readily generalize to e-commerce data, while\nspecific self-supervised visual feature extractors can better generalize.\nFurthermore, LGS's high-quality e-commerce-focused images and bimodal nature\nmake it advantageous for vision-language bi-modal tasks: LGS enables\nimage-captioning models to generate richer captions and helps text-to-image\ngeneration models achieve e-commerce style transfer.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04575v2",
    "published_date": "2024-01-09 14:24:29 UTC",
    "updated_date": "2024-03-05 21:02:33 UTC"
  },
  {
    "arxiv_id": "2401.04536v2",
    "title": "Evaluating Language Model Agency through Negotiations",
    "authors": [
      "Tim R. Davidson",
      "Veniamin Veselovsky",
      "Martin Josifoski",
      "Maxime Peyrard",
      "Antoine Bosselut",
      "Michal Kosinski",
      "Robert West"
    ],
    "abstract": "We introduce an approach to evaluate language model (LM) agency using\nnegotiation games. This approach better reflects real-world use cases and\naddresses some of the shortcomings of alternative LM benchmarks. Negotiation\ngames enable us to study multi-turn, and cross-model interactions, modulate\ncomplexity, and side-step accidental evaluation data leakage. We use our\napproach to test six widely used and publicly accessible LMs, evaluating\nperformance and alignment in both self-play and cross-play settings. Noteworthy\nfindings include: (i) only closed-source models tested here were able to\ncomplete these tasks; (ii) cooperative bargaining games proved to be most\nchallenging to the models; and (iii) even the most powerful models sometimes\n\"lose\" to weaker opponents",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICLR 2024, code and link to project data are made\n  available at https://github.com/epfl-dlab/LAMEN",
    "pdf_url": "http://arxiv.org/pdf/2401.04536v2",
    "published_date": "2024-01-09 13:19:37 UTC",
    "updated_date": "2024-03-16 16:41:48 UTC"
  },
  {
    "arxiv_id": "2401.04531v3",
    "title": "MERA: A Comprehensive LLM Evaluation in Russian",
    "authors": [
      "Alena Fenogenova",
      "Artem Chervyakov",
      "Nikita Martynov",
      "Anastasia Kozlova",
      "Maria Tikhonova",
      "Albina Akhmetgareeva",
      "Anton Emelyanov",
      "Denis Shevelev",
      "Pavel Lebedev",
      "Leonid Sinev",
      "Ulyana Isaeva",
      "Katerina Kolomeytseva",
      "Daniil Moskovskiy",
      "Elizaveta Goncharova",
      "Nikita Savushkin",
      "Polina Mikhailova",
      "Denis Dimitrov",
      "Alexander Panchenko",
      "Sergei Markov"
    ],
    "abstract": "Over the past few years, one of the most notable advancements in AI research\nhas been in foundation models (FMs), headlined by the rise of language models\n(LMs). As the models' size increases, LMs demonstrate enhancements in\nmeasurable aspects and the development of new qualitative features. However,\ndespite researchers' attention and the rapid growth in LM application, the\ncapabilities, limitations, and associated risks still need to be better\nunderstood. To address these issues, we introduce an open Multimodal Evaluation\nof Russian-language Architectures (MERA), a new instruction benchmark for\nevaluating foundation models oriented towards the Russian language. The\nbenchmark encompasses 21 evaluation tasks for generative models in 11 skill\ndomains and is designed as a black-box test to ensure the exclusion of data\nleakage. The paper introduces a methodology to evaluate FMs and LMs in zero-\nand few-shot fixed instruction settings that can be extended to other\nmodalities. We propose an evaluation methodology, an open-source code base for\nthe MERA assessment, and a leaderboard with a submission system. We evaluate\nopen LMs as baselines and find that they are still far behind the human level.\nWe publicly release MERA to guide forthcoming research, anticipate\ngroundbreaking model features, standardize the evaluation procedure, and\naddress potential societal drawbacks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The paper version comparable with the release code v.1.1.0 of the\n  benchmark MERA. ACL-2024 main track camera ready version",
    "pdf_url": "http://arxiv.org/pdf/2401.04531v3",
    "published_date": "2024-01-09 12:55:21 UTC",
    "updated_date": "2024-08-02 13:23:18 UTC"
  },
  {
    "arxiv_id": "2401.04749v1",
    "title": "LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection",
    "authors": [
      "Hongcheng Guo",
      "Jian Yang",
      "Jiaheng Liu",
      "Jiaqi Bai",
      "Boyang Wang",
      "Zhoujun Li",
      "Tieqiao Zheng",
      "Bo Zhang",
      "Junran peng",
      "Qi Tian"
    ],
    "abstract": "Log anomaly detection is a key component in the field of artificial\nintelligence for IT operations (AIOps). Considering log data of variant\ndomains, retraining the whole network for unknown domains is inefficient in\nreal industrial scenarios. However, previous deep models merely focused on\nextracting the semantics of log sequences in the same domain, leading to poor\ngeneralization on multi-domain logs. To alleviate this issue, we propose a\nunified Transformer-based framework for Log anomaly detection (LogFormer) to\nimprove the generalization ability across different domains, where we establish\na two-stage process including the pre-training and adapter-based tuning stage.\nSpecifically, our model is first pre-trained on the source domain to obtain\nshared semantic knowledge of log data. Then, we transfer such knowledge to the\ntarget domain via shared parameters. Besides, the Log-Attention module is\nproposed to supplement the information ignored by the log-paring. The proposed\nmethod is evaluated on three public and one real-world datasets. Experimental\nresults on multiple benchmarks demonstrate the effectiveness of our LogFormer\nwith fewer trainable parameters and lower training costs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2201.00016",
    "pdf_url": "http://arxiv.org/pdf/2401.04749v1",
    "published_date": "2024-01-09 12:55:21 UTC",
    "updated_date": "2024-01-09 12:55:21 UTC"
  },
  {
    "arxiv_id": "2401.04518v2",
    "title": "The Critique of Critique",
    "authors": [
      "Shichao Sun",
      "Junlong Li",
      "Weizhe Yuan",
      "Ruifeng Yuan",
      "Wenjie Li",
      "Pengfei Liu"
    ],
    "abstract": "Critique, as a natural language description for assessing the quality of\nmodel-generated content, has played a vital role in the training, evaluation,\nand refinement of LLMs. However, a systematic method to evaluate the quality of\ncritique is lacking. In this paper, we pioneer the critique of critique, termed\nMetaCritique, which builds specific quantification criteria. To achieve a\nreliable evaluation outcome, we propose Atomic Information Units (AIUs), which\ndescribe the critique in a more fine-grained manner. MetaCritique aggregates\neach AIU's judgment for the overall score. Moreover, MetaCritique delivers a\nnatural language rationale for the intricate reasoning within each judgment.\nLastly, we construct a meta-evaluation dataset covering 4 tasks across 16\npublic datasets involving human-written and LLM-generated critiques.\nExperiments demonstrate that MetaCritique can achieve near-human performance.\nOur study can facilitate future research in LLM critiques based on our\nfollowing observations and released resources: (1) superior critiques judged by\nMetaCritique can lead to better refinements, indicating that it can potentially\nenhance the alignment of existing LLMs; (2) the leaderboard of critique models\nreveals that open-source critique models commonly suffer from factuality\nissues; (3) relevant code and data are publicly available at\nhttps://github.com/GAIR-NLP/MetaCritique to support deeper exploration; (4) an\nAPI at PyPI with the usage documentation in Appendix C allows users to assess\nthe critique conveniently.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Findings of ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.04518v2",
    "published_date": "2024-01-09 12:20:41 UTC",
    "updated_date": "2024-06-01 17:52:14 UTC"
  },
  {
    "arxiv_id": "2401.04515v1",
    "title": "Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models",
    "authors": [
      "Mikhail Tikhomirov",
      "Natalia Loukachevitch"
    ],
    "abstract": "This article investigates a zero-shot approach to hypernymy prediction using\nlarge language models (LLMs). The study employs a method based on text\nprobability calculation, applying it to various generated prompts. The\nexperiments demonstrate a strong correlation between the effectiveness of\nlanguage model prompts and classic patterns, indicating that preliminary prompt\nselection can be carried out using smaller models before moving to larger ones.\nWe also explore prompts for predicting co-hyponyms and improving hypernymy\npredictions by augmenting prompts with additional information through\nautomatically identified co-hyponyms. An iterative approach is developed for\npredicting higher-level concepts, which further improves the quality on the\nBLESS dataset (MAP = 0.8).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04515v1",
    "published_date": "2024-01-09 12:13:55 UTC",
    "updated_date": "2024-01-09 12:13:55 UTC"
  },
  {
    "arxiv_id": "2401.04748v1",
    "title": "Convolutional Neural Network Ensemble Learning for Hyperspectral Imaging-based Blackberry Fruit Ripeness Detection in Uncontrolled Farm Environment",
    "authors": [
      "Chollette C. Olisah",
      "Ben Trewhella",
      "Bo Li",
      "Melvyn L. Smith",
      "Benjamin Winstone",
      "E. Charles Whitfield",
      "Felicidad Fernández Fernández",
      "Harriet Duncalfe"
    ],
    "abstract": "Fruit ripeness estimation models have for decades depended on spectral index\nfeatures or colour-based features, such as mean, standard deviation, skewness,\ncolour moments, and/or histograms for learning traits of fruit ripeness.\nRecently, few studies have explored the use of deep learning techniques to\nextract features from images of fruits with visible ripeness cues. However, the\nblackberry (Rubus fruticosus) fruit does not show obvious and reliable visible\ntraits of ripeness when mature and therefore poses great difficulty to fruit\npickers. The mature blackberry, to the human eye, is black before, during, and\npost-ripening. To address this engineering application challenge, this paper\nproposes a novel multi-input convolutional neural network (CNN) ensemble\nclassifier for detecting subtle traits of ripeness in blackberry fruits. The\nmulti-input CNN was created from a pre-trained visual geometry group 16-layer\ndeep convolutional network (VGG16) model trained on the ImageNet dataset. The\nfully connected layers were optimized for learning traits of ripeness of mature\nblackberry fruits. The resulting model served as the base for building\nhomogeneous ensemble learners that were ensemble using the stack generalization\nensemble (SGE) framework. The input to the network is images acquired with a\nstereo sensor using visible and near-infrared (VIS-NIR) spectral filters at\nwavelengths of 700 nm and 770 nm. Through experiments, the proposed model\nachieved 95.1% accuracy on unseen sets and 90.2% accuracy with in-field\nconditions. Further experiments reveal that machine sensory is highly and\npositively correlated to human sensory over blackberry fruit skin texture.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 10 figures, 6 tables; submited to EAAI",
    "pdf_url": "http://arxiv.org/pdf/2401.04748v1",
    "published_date": "2024-01-09 12:00:17 UTC",
    "updated_date": "2024-01-09 12:00:17 UTC"
  },
  {
    "arxiv_id": "2401.04507v1",
    "title": "TechGPT-2.0: A large language model project to solve the task of knowledge graph construction",
    "authors": [
      "Jiaqi Wang",
      "Yuying Chang",
      "Zhong Li",
      "Ning An",
      "Qi Ma",
      "Lei Hei",
      "Haibo Luo",
      "Yifei Lu",
      "Feiliang Ren"
    ],
    "abstract": "Large language models have exhibited robust performance across diverse\nnatural language processing tasks. This report introduces TechGPT-2.0, a\nproject designed to enhance the capabilities of large language models\nspecifically in knowledge graph construction tasks, including named entity\nrecognition (NER) and relationship triple extraction (RTE) tasks in NLP\napplications. Additionally, it serves as a LLM accessible for research within\nthe Chinese open-source model community. We offer two 7B large language model\nweights and a QLoRA weight specialized for processing lengthy texts.Notably,\nTechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all\nfunctionalities from TechGPT-1.0, it exhibits robust text processing\ncapabilities, particularly in the domains of medicine and law. Furthermore, we\nintroduce new capabilities to the model, enabling it to process texts in\nvarious domains such as geographical areas, transportation, organizations,\nliterary works, biology, natural sciences, astronomical objects, and\narchitecture. These enhancements also fortified the model's adeptness in\nhandling hallucinations, unanswerable queries, and lengthy texts. This report\nprovides a comprehensive and detailed introduction to the full fine-tuning\nprocess on Huawei's Ascend servers, encompassing experiences in Ascend server\ndebugging, instruction fine-tuning data processing, and model training. Our\ncode is available at https://github.com/neukg/TechGPT-2.0",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04507v1",
    "published_date": "2024-01-09 11:52:58 UTC",
    "updated_date": "2024-01-09 11:52:58 UTC"
  },
  {
    "arxiv_id": "2401.04747v2",
    "title": "DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation",
    "authors": [
      "Junming Chen",
      "Yunfei Liu",
      "Jianan Wang",
      "Ailing Zeng",
      "Yu Li",
      "Qifeng Chen"
    ],
    "abstract": "We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D\nExpression and Gesture generation with arbitrary length. While previous works\nfocused on co-speech gesture or expression generation individually, the joint\ngeneration of synchronized expressions and gestures remains barely explored. To\naddress this, our diffusion-based co-speech motion generation transformer\nenables uni-directional information flow from expression to gesture,\nfacilitating improved matching of joint expression-gesture distributions.\nFurthermore, we introduce an outpainting-based sampling strategy for arbitrary\nlong sequence generation in diffusion models, offering flexibility and\ncomputational efficiency. Our method provides a practical solution that\nproduces high-quality synchronized expression and gesture generation driven by\nspeech. Evaluated on two public datasets, our approach achieves\nstate-of-the-art performance both quantitatively and qualitatively.\nAdditionally, a user study confirms the superiority of DiffSHEG over prior\napproaches. By enabling the real-time generation of expressive and synchronized\nmotions, DiffSHEG showcases its potential for various applications in the\ndevelopment of digital humans and embodied agents.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by CVPR 2024. Project page:\n  https://jeremycjm.github.io/proj/DiffSHEG",
    "pdf_url": "http://arxiv.org/pdf/2401.04747v2",
    "published_date": "2024-01-09 11:38:18 UTC",
    "updated_date": "2024-04-06 14:53:51 UTC"
  },
  {
    "arxiv_id": "2401.04489v1",
    "title": "Optimal Survival Trees: A Dynamic Programming Approach",
    "authors": [
      "Tim Huisman",
      "Jacobus G. M. van der Linden",
      "Emir Demirović"
    ],
    "abstract": "Survival analysis studies and predicts the time of death, or other singular\nunrepeated events, based on historical data, while the true time of death for\nsome instances is unknown. Survival trees enable the discovery of complex\nnonlinear relations in a compact human comprehensible model, by recursively\nsplitting the population and predicting a distinct survival distribution in\neach leaf node. We use dynamic programming to provide the first survival tree\nmethod with optimality guarantees, enabling the assessment of the optimality\ngap of heuristics. We improve the scalability of our method through a special\nalgorithm for computing trees up to depth two. The experiments show that our\nmethod's run time even outperforms some heuristics for realistic cases while\nobtaining similar out-of-sample performance with the state-of-the-art.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at AAAI-24",
    "pdf_url": "http://arxiv.org/pdf/2401.04489v1",
    "published_date": "2024-01-09 11:01:11 UTC",
    "updated_date": "2024-01-09 11:01:11 UTC"
  },
  {
    "arxiv_id": "2401.04481v1",
    "title": "Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset",
    "authors": [
      "Shrey Satapara",
      "Parth Mehta",
      "Debasis Ganguly",
      "Sandip Modha"
    ],
    "abstract": "The recent success in language generation capabilities of large language\nmodels (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns\nabout their possible misuse in inducing mass agitation and communal hatred via\ngenerating fake news and spreading misinformation. Traditional means of\ndeveloping a misinformation ground-truth dataset does not scale well because of\nthe extensive manual effort required to annotate the data. In this paper, we\npropose an LLM-based approach of creating silver-standard ground-truth datasets\nfor identifying misinformation. Specifically speaking, given a trusted news\narticle, our proposed approach involves prompting LLMs to automatically\ngenerate a summarised version of the original article. The prompts in our\nproposed approach act as a controlling mechanism to generate specific types of\nfactual incorrectness in the generated summaries, e.g., incorrect quantities,\nfalse attributions etc. To investigate the usefulness of this dataset, we\nconduct a set of experiments where we train a range of supervised models for\nthe task of misinformation detection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04481v1",
    "published_date": "2024-01-09 10:38:13 UTC",
    "updated_date": "2024-01-09 10:38:13 UTC"
  },
  {
    "arxiv_id": "2401.04478v2",
    "title": "TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction",
    "authors": [
      "Maximilian G. Schuh",
      "Davide Boldini",
      "Stephan A. Sieber"
    ],
    "abstract": "The success of drug discovery and development relies on the precise\nprediction of molecular activities and properties. While in silico molecular\nproperty prediction has shown remarkable potential, its use has been limited so\nfar to assays for which large amounts of data are available. In this study, we\nuse a fine-tuned large language model to integrate biological assays based on\ntheir textual information, coupled with Barlow Twins, a Siamese neural network\nusing a novel self-supervised learning approach. This architecture uses both\nassay information and molecular fingerprints to extract the true molecular\ninformation. TwinBooster enables the prediction of properties of unseen\nbioassays and molecules by providing state-of-the-art zero-shot learning tasks.\nRemarkably, our artificial intelligence pipeline shows excellent performance on\nthe FS-Mol benchmark. This breakthrough demonstrates the application of deep\nlearning to critical property prediction tasks where data is typically scarce.\nBy accelerating the early identification of active molecules in drug discovery\nand development, this method has the potential to help streamline the\nidentification of novel therapeutics.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "13(+9) pages(+appendix), 5 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.04478v2",
    "published_date": "2024-01-09 10:36:20 UTC",
    "updated_date": "2024-01-30 09:29:47 UTC"
  },
  {
    "arxiv_id": "2401.05447v1",
    "title": "Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market Wraps?",
    "authors": [
      "Baptiste Lefort",
      "Eric Benhamou",
      "Jean-Jacques Ohana",
      "David Saltiel",
      "Beatrice Guez",
      "Damien Challet"
    ],
    "abstract": "We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to\n2023, reposted on large financial media, to determine how global news headlines\nmay affect stock market movements using ChatGPT and a two-stage prompt\napproach. We document a statistically significant positive correlation between\nthe sentiment score and future equity market returns over short to medium term,\nwhich reverts to a negative correlation over longer horizons. Validation of\nthis correlation pattern across multiple equity markets indicates its\nrobustness across equity regions and resilience to non-linearity, evidenced by\ncomparison of Pearson and Spearman correlations. Finally, we provide an\nestimate of the optimal horizon that strikes a balance between reactivity to\nnew information and correlation.",
    "categories": [
      "q-fin.ST",
      "cs.AI"
    ],
    "primary_category": "q-fin.ST",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.05447v1",
    "published_date": "2024-01-09 10:34:19 UTC",
    "updated_date": "2024-01-09 10:34:19 UTC"
  },
  {
    "arxiv_id": "2401.04474v1",
    "title": "Combining Embedding-Based and Semantic-Based Models for Post-hoc Explanations in Recommender Systems",
    "authors": [
      "Ngoc Luyen Le",
      "Marie-Hélène Abel",
      "Philippe Gouspillou"
    ],
    "abstract": "In today's data-rich environment, recommender systems play a crucial role in\ndecision support systems. They provide to users personalized recommendations\nand explanations about these recommendations. Embedding-based models, despite\ntheir widespread use, often suffer from a lack of interpretability, which can\nundermine trust and user engagement. This paper presents an approach that\ncombines embedding-based and semantic-based models to generate post-hoc\nexplanations in recommender systems, leveraging ontology-based knowledge graphs\nto improve interpretability and explainability. By organizing data within a\nstructured framework, ontologies enable the modeling of intricate relationships\nbetween entities, which is essential for generating explanations. By combining\nembedding-based and semantic based models for post-hoc explanations in\nrecommender systems, the framework we defined aims at producing meaningful and\neasy-to-understand explanations, enhancing user trust and satisfaction, and\npotentially promoting the adoption of recommender systems across the e-commerce\nsector.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04474v1",
    "published_date": "2024-01-09 10:24:46 UTC",
    "updated_date": "2024-01-09 10:24:46 UTC"
  },
  {
    "arxiv_id": "2401.04472v3",
    "title": "A Survey on Efficient Federated Learning Methods for Foundation Model Training",
    "authors": [
      "Herbert Woisetschläger",
      "Alexander Isenko",
      "Shiqiang Wang",
      "Ruben Mayer",
      "Hans-Arno Jacobsen"
    ],
    "abstract": "Federated Learning (FL) has become an established technique to facilitate\nprivacy-preserving collaborative training across a multitude of clients.\nHowever, new approaches to FL often discuss their contributions involving small\ndeep-learning models only and focus on training full models on clients. In the\nwake of Foundation Models (FM), the reality is different for many deep learning\napplications. Typically, FMs have already been pre-trained across a wide\nvariety of tasks and can be fine-tuned to specific downstream tasks over\nsignificantly smaller datasets than required for full model training. However,\naccess to such datasets is often challenging. By its design, FL can help to\nopen data silos. With this survey, we introduce a novel taxonomy focused on\ncomputational and communication efficiency, the vital elements to make use of\nFMs in FL systems. We discuss the benefits and drawbacks of parameter-efficient\nfine-tuning (PEFT) for FL applications, elaborate on the readiness of FL\nframeworks to work with FMs, and provide future research opportunities on how\nto evaluate generative models in FL as well as the interplay of privacy and\nPEFT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "I.2.11; C.2"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication at IJCAI 2024. Please cite the published\n  paper via https://doi.org/10.24963/ijcai.2024/919",
    "pdf_url": "http://arxiv.org/pdf/2401.04472v3",
    "published_date": "2024-01-09 10:22:23 UTC",
    "updated_date": "2024-09-05 20:11:40 UTC"
  },
  {
    "arxiv_id": "2401.09466v1",
    "title": "Self Supervised Vision for Climate Downscaling",
    "authors": [
      "Karandeep Singh",
      "Chaeyoon Jeong",
      "Naufal Shidqi",
      "Sungwon Park",
      "Arjun Nellikkattil",
      "Elke Zeller",
      "Meeyoung Cha"
    ],
    "abstract": "Climate change is one of the most critical challenges that our planet is\nfacing today. Rising global temperatures are already bringing noticeable\nchanges to Earth's weather and climate patterns with an increased frequency of\nunpredictable and extreme weather events. Future projections for climate change\nresearch are based on Earth System Models (ESMs), the computer models that\nsimulate the Earth's climate system. ESMs provide a framework to integrate\nvarious physical systems, but their output is bound by the enormous\ncomputational resources required for running and archiving higher-resolution\nsimulations. For a given resource budget, the ESMs are generally run on a\ncoarser grid, followed by a computationally lighter $downscaling$ process to\nobtain a finer-resolution output. In this work, we present a deep-learning\nmodel for downscaling ESM simulation data that does not require high-resolution\nground truth data for model optimization. This is realized by leveraging\nsalient data distribution patterns and the hidden dependencies between weather\nvariables for an $\\textit{individual}$ data point at $\\textit{runtime}$.\nExtensive evaluation with $2$x, $3$x, and $4$x scaling factors demonstrates\nthat the proposed model consistently obtains superior performance over that of\nvarious baselines. The improved downscaling performance and no dependence on\nhigh-resolution ground truth data make the proposed method a valuable tool for\nclimate research and mark it as a promising direction for future research.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.09466v1",
    "published_date": "2024-01-09 10:20:49 UTC",
    "updated_date": "2024-01-09 10:20:49 UTC"
  },
  {
    "arxiv_id": "2401.04468v1",
    "title": "MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation",
    "authors": [
      "Weimin Wang",
      "Jiawei Liu",
      "Zhijie Lin",
      "Jiangqiao Yan",
      "Shuo Chen",
      "Chetwin Low",
      "Tuyen Hoang",
      "Jie Wu",
      "Jun Hao Liew",
      "Hanshu Yan",
      "Daquan Zhou",
      "Jiashi Feng"
    ],
    "abstract": "The growing demand for high-fidelity video generation from textual\ndescriptions has catalyzed significant research in this field. In this work, we\nintroduce MagicVideo-V2 that integrates the text-to-image model, video motion\ngenerator, reference image embedding module and frame interpolation module into\nan end-to-end video generation pipeline. Benefiting from these architecture\ndesigns, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution\nvideo with remarkable fidelity and smoothness. It demonstrates superior\nperformance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph,\nMoon Valley and Stable Video Diffusion model via user evaluation at large\nscale.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04468v1",
    "published_date": "2024-01-09 10:12:52 UTC",
    "updated_date": "2024-01-09 10:12:52 UTC"
  },
  {
    "arxiv_id": "2401.04441v1",
    "title": "Image classification network enhancement methods based on knowledge injection",
    "authors": [
      "Yishuang Tian",
      "Ning Wang",
      "Liang Zhang"
    ],
    "abstract": "The current deep neural network algorithm still stays in the end-to-end\ntraining supervision method like Image-Label pairs, which makes traditional\nalgorithm is difficult to explain the reason for the results, and the\nprediction logic is difficult to understand and analyze. The current algorithm\ndoes not use the existing human knowledge information, which makes the model\nnot in line with the human cognition model and makes the model not suitable for\nhuman use. In order to solve the above problems, the present invention provides\na deep neural network training method based on the human knowledge, which uses\nthe human cognition model to construct the deep neural network training model,\nand uses the existing human knowledge information to construct the deep neural\nnetwork training model. This paper proposes a multi-level hierarchical deep\nlearning algorithm, which is composed of multi-level hierarchical deep neural\nnetwork architecture and multi-level hierarchical deep learning framework. The\nexperimental results show that the proposed algorithm can effectively explain\nthe hidden information of the neural network. The goal of our study is to\nimprove the interpretability of deep neural networks (DNNs) by providing an\nanalysis of the impact of knowledge injection on the classification task. We\nconstructed a knowledge injection dataset with matching knowledge data and\nimage classification data. The knowledge injection dataset is the benchmark\ndataset for the experiments in the paper. Our model expresses the improvement\nin interpretability and classification task performance of hidden layers at\ndifferent scales.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 3 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.04441v1",
    "published_date": "2024-01-09 09:11:41 UTC",
    "updated_date": "2024-01-09 09:11:41 UTC"
  },
  {
    "arxiv_id": "2401.04437v1",
    "title": "Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using Dimension Reduction Methods",
    "authors": [
      "Dongeon Kim",
      "YeongHyeon Park"
    ],
    "abstract": "Recent studies try to use hyperspectral imaging (HSI) to detect foreign\nmatters in products because it enables to visualize the invisible wavelengths\nincluding ultraviolet and infrared. Considering the enormous image channels of\nthe HSI, several dimension reduction methods-e.g., PCA or UMAP-can be\nconsidered to reduce but those cannot ease the fundamental limitations, as\nfollows: (1) latency of HSI capturing. (2) less explanation ability of the\nimportant channels. In this paper, to circumvent the aforementioned methods,\none of the ways to channel reduction, on anomaly detection proposed HSI.\nDifferent from feature extraction methods (i.e., PCA or UMAP), feature\nselection can sort the feature by impact and show better explainability so we\nmight redesign the task-optimized and cost-effective spectroscopic camera. Via\nthe extensive experiment results with synthesized MVTec AD dataset, we confirm\nthat the feature selection method shows 6.90x faster at the inference phase\ncompared with feature extraction-based approaches while preserving anomaly\ndetection performance. Ultimately, we conclude the advantage of feature\nselection which is effective yet fast.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages, 4 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.04437v1",
    "published_date": "2024-01-09 09:05:15 UTC",
    "updated_date": "2024-01-09 09:05:15 UTC"
  },
  {
    "arxiv_id": "2401.05446v1",
    "title": "Self-supervised Learning for Electroencephalogram: A Systematic Survey",
    "authors": [
      "Weining Weng",
      "Yang Gu",
      "Shuai Guo",
      "Yuan Ma",
      "Zhaohua Yang",
      "Yuchen Liu",
      "Yiqiang Chen"
    ],
    "abstract": "Electroencephalogram (EEG) is a non-invasive technique to record\nbioelectrical signals. Integrating supervised deep learning techniques with EEG\nsignals has recently facilitated automatic analysis across diverse EEG-based\ntasks. However, the label issues of EEG signals have constrained the\ndevelopment of EEG-based deep models. Obtaining EEG annotations is difficult\nthat requires domain experts to guide collection and labeling, and the\nvariability of EEG signals among different subjects causes significant label\nshifts. To solve the above challenges, self-supervised learning (SSL) has been\nproposed to extract representations from unlabeled samples through\nwell-designed pretext tasks. This paper concentrates on integrating SSL\nframeworks with temporal EEG signals to achieve efficient representation and\nproposes a systematic review of the SSL for EEG signals. In this paper, 1) we\nintroduce the concept and theory of self-supervised learning and typical SSL\nframeworks. 2) We provide a comprehensive review of SSL for EEG analysis,\nincluding taxonomy, methodology, and technique details of the existing\nEEG-based SSL frameworks, and discuss the difference between these methods. 3)\nWe investigate the adaptation of the SSL approach to various downstream tasks,\nincluding the task description and related benchmark datasets. 4) Finally, we\ndiscuss the potential directions for future SSL-EEG research.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "68-02 (Primarily), 68T01 (Secondary)",
      "I.2; J.3; I.5.4"
    ],
    "primary_category": "eess.SP",
    "comment": "35 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.05446v1",
    "published_date": "2024-01-09 08:59:30 UTC",
    "updated_date": "2024-01-09 08:59:30 UTC"
  },
  {
    "arxiv_id": "2401.04429v2",
    "title": "i-Rebalance: Personalized Vehicle Repositioning for Supply Demand Balance",
    "authors": [
      "Haoyang Chen",
      "Peiyan Sun",
      "Qiyuan Song",
      "Wanyuan Wang",
      "Weiwei Wu",
      "Wencan Zhang",
      "Guanyu Gao",
      "Yan Lyu"
    ],
    "abstract": "Ride-hailing platforms have been facing the challenge of balancing demand and\nsupply. Existing vehicle reposition techniques often treat drivers as\nhomogeneous agents and relocate them deterministically, assuming compliance\nwith the reposition. In this paper, we consider a more realistic and\ndriver-centric scenario where drivers have unique cruising preferences and can\ndecide whether to take the recommendation or not on their own. We propose\ni-Rebalance, a personalized vehicle reposition technique with deep\nreinforcement learning (DRL). i-Rebalance estimates drivers' decisions on\naccepting reposition recommendations through an on-field user study involving\n99 real drivers. To optimize supply-demand balance and enhance preference\nsatisfaction simultaneously, i-Rebalance has a sequential reposition strategy\nwith dual DRL agents: Grid Agent to determine the reposition order of idle\nvehicles, and Vehicle Agent to provide personalized recommendations to each\nvehicle in the pre-defined order. This sequential learning strategy facilitates\nmore effective policy training within a smaller action space compared to\ntraditional joint-action methods. Evaluation of real-world trajectory data\nshows that i-Rebalance improves driver acceptance rate by 38.07% and total\ndriver income by 9.97%.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04429v2",
    "published_date": "2024-01-09 08:51:56 UTC",
    "updated_date": "2024-04-02 05:50:00 UTC"
  },
  {
    "arxiv_id": "2402.06629v1",
    "title": "Towards the mathematical foundation of the minimum enclosing ball and related problems",
    "authors": [
      "Michael N. Vrahatis"
    ],
    "abstract": "Theoretical background is provided towards the mathematical foundation of the\nminimum enclosing ball problem. This problem concerns the determination of the\nunique spherical surface of smallest radius enclosing a given bounded set in\nthe d-dimensional Euclidean space. The study of several problems that are\nsimilar or related to the minimum enclosing ball problem has received a\nconsiderable impetus from the large amount of applications of these problems in\nvarious fields of science and technology. The proposed theoretical framework is\nbased on several enclosing (covering) and partitioning (clustering) theorems\nand provides among others bounds and relations between the circumradius,\ninradius, diameter and width of a set. These enclosing and partitioning\ntheorems are considered as cornerstones in the field that strongly influencing\ndevelopments and generalizations to other spaces and non-Euclidean geometries.",
    "categories": [
      "cs.CG",
      "cs.AI",
      "math.GT"
    ],
    "primary_category": "cs.CG",
    "comment": "arXiv admin note: text overlap with arXiv:2401.03232",
    "pdf_url": "http://arxiv.org/pdf/2402.06629v1",
    "published_date": "2024-01-09 08:30:55 UTC",
    "updated_date": "2024-01-09 08:30:55 UTC"
  },
  {
    "arxiv_id": "2401.04422v1",
    "title": "Estimating Text Similarity based on Semantic Concept Embeddings",
    "authors": [
      "Tim vor der Brück",
      "Marc Pouly"
    ],
    "abstract": "Due to their ease of use and high accuracy, Word2Vec (W2V) word embeddings\nenjoy great success in the semantic representation of words, sentences, and\nwhole documents as well as for semantic similarity estimation. However, they\nhave the shortcoming that they are directly extracted from a surface\nrepresentation, which does not adequately represent human thought processes and\nalso performs poorly for highly ambiguous words. Therefore, we propose Semantic\nConcept Embeddings (CE) based on the MultiNet Semantic Network (SN) formalism,\nwhich addresses both shortcomings. The evaluation on a marketing target group\ndistribution task showed that the accuracy of predicted target groups can be\nincreased by combining traditional word embeddings with semantic CEs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04422v1",
    "published_date": "2024-01-09 08:29:46 UTC",
    "updated_date": "2024-01-09 08:29:46 UTC"
  },
  {
    "arxiv_id": "2401.04405v1",
    "title": "Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation",
    "authors": [
      "Jinhai Yang",
      "Mengxi Guo",
      "Shijie Zhao",
      "Junlin Li",
      "Li Zhang"
    ],
    "abstract": "Adaptive video streaming requires efficient bitrate ladder construction to\nmeet heterogeneous network conditions and end-user demands. Per-title optimized\nencoding typically traverses numerous encoding parameters to search the\nPareto-optimal operating points for each video. Recently, researchers have\nattempted to predict the content-optimized bitrate ladder for pre-encoding\noverhead reduction. However, existing methods commonly estimate the encoding\nparameters on the Pareto front and still require subsequent pre-encodings. In\nthis paper, we propose to directly predict the optimal transcoding resolution\nat each preset bitrate for efficient bitrate ladder construction. We adopt a\nTemporal Attentive Gated Recurrent Network to capture spatial-temporal features\nand predict transcoding resolutions as a multi-task classification problem. We\ndemonstrate that content-optimized bitrate ladders can thus be efficiently\ndetermined without any pre-encoding. Our method well approximates the\nground-truth bitrate-resolution pairs with a slight Bj{\\o}ntegaard Delta rate\nloss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.MM",
    "comment": "Accepted by the 2024 Data Compression Conference (DCC) for\n  presentation as a poster. This is the full paper",
    "pdf_url": "http://arxiv.org/pdf/2401.04405v1",
    "published_date": "2024-01-09 08:01:47 UTC",
    "updated_date": "2024-01-09 08:01:47 UTC"
  },
  {
    "arxiv_id": "2401.04402v2",
    "title": "IGNITE: Individualized GeNeration of Imputations in Time-series Electronic health records",
    "authors": [
      "Ghadeer O. Ghosheh",
      "Jin Li",
      "Tingting Zhu"
    ],
    "abstract": "Electronic Health Records present a valuable modality for driving\npersonalized medicine, where treatment is tailored to fit individual-level\ndifferences. For this purpose, many data-driven machine learning and\nstatistical models rely on the wealth of longitudinal EHRs to study patients'\nphysiological and treatment effects. However, longitudinal EHRs tend to be\nsparse and highly missing, where missingness could also be informative and\nreflect the underlying patient's health status. Therefore, the success of\ndata-driven models for personalized medicine highly depends on how the EHR data\nis represented from physiological data, treatments, and the missing values in\nthe data. To this end, we propose a novel deep-learning model that learns the\nunderlying patient dynamics over time across multivariate data to generate\npersonalized realistic values conditioning on an individual's demographic\ncharacteristics and treatments. Our proposed model, IGNITE (Individualized\nGeNeration of Imputations in Time-series Electronic health records), utilises a\nconditional dual-variational autoencoder augmented with dual-stage attention to\ngenerate missing values for an individual. In IGNITE, we further propose a\nnovel individualized missingness mask (IMM), which helps our model generate\nvalues based on the individual's observed data and missingness patterns. We\nfurther extend the use of IGNITE from imputing missingness to a personalized\ndata synthesizer, where it generates missing EHRs that were never observed\nprior or even generates new patients for various applications. We validate our\nmodel on three large publicly available datasets and show that IGNITE\noutperforms state-of-the-art approaches in missing data reconstruction and task\nprediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04402v2",
    "published_date": "2024-01-09 07:57:21 UTC",
    "updated_date": "2024-12-13 14:04:57 UTC"
  },
  {
    "arxiv_id": "2401.05444v1",
    "title": "Fully Spiking Actor Network with Intra-layer Connections for Reinforcement Learning",
    "authors": [
      "Ding Chen",
      "Peixi Peng",
      "Tiejun Huang",
      "Yonghong Tian"
    ],
    "abstract": "With the help of special neuromorphic hardware, spiking neural networks\n(SNNs) are expected to realize artificial intelligence (AI) with less energy\nconsumption. It provides a promising energy-efficient way for realistic control\ntasks by combining SNNs with deep reinforcement learning (DRL). In this paper,\nwe focus on the task where the agent needs to learn multi-dimensional\ndeterministic policies to control, which is very common in real scenarios.\nRecently, the surrogate gradient method has been utilized for training\nmulti-layer SNNs, which allows SNNs to achieve comparable performance with the\ncorresponding deep networks in this task. Most existing spike-based RL methods\ntake the firing rate as the output of SNNs, and convert it to represent\ncontinuous action space (i.e., the deterministic policy) through a\nfully-connected (FC) layer. However, the decimal characteristic of the firing\nrate brings the floating-point matrix operations to the FC layer, making the\nwhole SNN unable to deploy on the neuromorphic hardware directly. To develop a\nfully spiking actor network without any floating-point matrix operations, we\ndraw inspiration from the non-spiking interneurons found in insects and employ\nthe membrane voltage of the non-spiking neurons to represent the action. Before\nthe non-spiking neurons, multiple population neurons are introduced to decode\ndifferent dimensions of actions. Since each population is used to decode a\ndimension of action, we argue that the neurons in each population should be\nconnected in time domain and space domain. Hence, the intra-layer connections\nare used in output populations to enhance the representation capacity. Finally,\nwe propose a fully spiking actor network with intra-layer connections\n(ILC-SAN).",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "13 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.05444v1",
    "published_date": "2024-01-09 07:31:34 UTC",
    "updated_date": "2024-01-09 07:31:34 UTC"
  },
  {
    "arxiv_id": "2401.04385v4",
    "title": "Machine unlearning through fine-grained model parameters perturbation",
    "authors": [
      "Zhiwei Zuo",
      "Zhuo Tang",
      "Kenli Li",
      "Anwitaman Datta"
    ],
    "abstract": "Machine unlearning techniques, which involve retracting data records and\nreducing influence of said data on trained models, help with the user privacy\nprotection objective but incur significant computational costs. Weight\nperturbation-based unlearning is a general approach, but it typically involves\nglobally modifying the parameters. We propose fine-grained Top-K and Random-k\nparameters perturbed inexact machine unlearning strategies that address the\nprivacy needs while keeping the computational costs tractable.\n  In order to demonstrate the efficacy of our strategies we also tackle the\nchallenge of evaluating the effectiveness of machine unlearning by considering\nthe model's generalization performance across both unlearning and remaining\ndata. To better assess the unlearning effect and model generalization, we\npropose novel metrics, namely, the forgetting rate and memory retention rate.\nHowever, for inexact machine unlearning, current metrics are inadequate in\nquantifying the degree of forgetting that occurs after unlearning strategies\nare applied. To address this, we introduce SPD-GAN, which subtly perturbs the\ndistribution of data targeted for unlearning. Then, we evaluate the degree of\nunlearning by measuring the performance difference of the models on the\nperturbed unlearning data before and after the unlearning process. By\nimplementing these innovative techniques and metrics, we achieve\ncomputationally efficacious privacy protection in machine learning applications\nwithout significant sacrifice of model performance. Furthermore, this approach\nprovides a novel method for evaluating the degree of unlearning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04385v4",
    "published_date": "2024-01-09 07:14:45 UTC",
    "updated_date": "2025-01-15 06:00:17 UTC"
  },
  {
    "arxiv_id": "2401.04374v2",
    "title": "Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective",
    "authors": [
      "Haoyi Xiong",
      "Xuhong Li",
      "Xiaofei Zhang",
      "Jiamin Chen",
      "Xinhao Sun",
      "Yuchen Li",
      "Zeyi Sun",
      "Mengnan Du"
    ],
    "abstract": "Given the complexity and lack of transparency in deep neural networks (DNNs),\nextensive efforts have been made to make these systems more interpretable or\nexplain their behaviors in accessible terms. Unlike most reviews, which focus\non algorithmic and model-centric perspectives, this work takes a \"data-centric\"\nview, examining how data collection, processing, and analysis contribute to\nexplainable AI (XAI). We categorize existing work into three categories subject\nto their purposes: interpretations of deep models, referring to feature\nattributions and reasoning processes that correlate data points with model\noutputs; influences of training data, examining the impact of training data\nnuances, such as data valuation and sample anomalies, on decision-making\nprocesses; and insights of domain knowledge, discovering latent patterns and\nfostering new knowledge from data and models to advance social values and\nscientific discovery. Specifically, we distill XAI methodologies into data\nmining operations on training and testing data across modalities, such as\nimages, text, and tabular data, as well as on training logs, checkpoints,\nmodels and other DNN behavior descriptors. In this way, our study offers a\ncomprehensive, data-centric examination of XAI from a lens of data mining\nmethods and applications.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04374v2",
    "published_date": "2024-01-09 06:27:09 UTC",
    "updated_date": "2024-01-13 06:00:18 UTC"
  },
  {
    "arxiv_id": "2402.03327v1",
    "title": "Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models",
    "authors": [
      "Dingning Liu",
      "Xiaoshui Huang",
      "Yuenan Hou",
      "Zhihui Wang",
      "Zhenfei Yin",
      "Yongshun Gong",
      "Peng Gao",
      "Wanli Ouyang"
    ],
    "abstract": "In this paper, we introduce Uni3D-LLM, a unified framework that leverages a\nLarge Language Model (LLM) to integrate tasks of 3D perception, generation, and\nediting within point cloud scenes. This framework empowers users to\neffortlessly generate and modify objects at specified locations within a scene,\nguided by the versatility of natural language descriptions. Uni3D-LLM harnesses\nthe expressive power of natural language to allow for precise command over the\ngeneration and editing of 3D objects, thereby significantly enhancing\noperational flexibility and controllability. By mapping point cloud into the\nunified representation space, Uni3D-LLM achieves cross-application\nfunctionality, enabling the seamless execution of a wide array of tasks,\nranging from the accurate instantiation of 3D objects to the diverse\nrequirements of interactive design. Through a comprehensive suite of rigorous\nexperiments, the efficacy of Uni3D-LLM in the comprehension, generation, and\nediting of point cloud has been validated. Additionally, we have assessed the\nimpact of integrating a point cloud perception module on the generation and\nediting processes, confirming the substantial potential of our approach for\npractical applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.03327v1",
    "published_date": "2024-01-09 06:20:23 UTC",
    "updated_date": "2024-01-09 06:20:23 UTC"
  },
  {
    "arxiv_id": "2401.04739v1",
    "title": "Content-Conditioned Generation of Stylized Free hand Sketches",
    "authors": [
      "Jiajun Liu",
      "Siyuan Wang",
      "Guangming Zhu",
      "Liang Zhang",
      "Ning Li",
      "Eryang Gao"
    ],
    "abstract": "In recent years, the recognition of free-hand sketches has remained a popular\ntask. However, in some special fields such as the military field, free-hand\nsketches are difficult to sample on a large scale. Common data augmentation and\nimage generation techniques are difficult to produce images with various\nfree-hand sketching styles. Therefore, the recognition and segmentation tasks\nin related fields are limited. In this paper, we propose a novel adversarial\ngenerative network that can accurately generate realistic free-hand sketches\nwith various styles. We explore the performance of the model, including using\nstyles randomly sampled from a prior normal distribution to generate images\nwith various free-hand sketching styles, disentangling the painters' styles\nfrom known free-hand sketches to generate images with specific styles, and\ngenerating images of unknown classes that are not in the training set. We\nfurther demonstrate with qualitative and quantitative evaluations our\nadvantages in visual quality, content accuracy, and style imitation on\nSketchIME.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 7 figures, ICSMD",
    "pdf_url": "http://arxiv.org/pdf/2401.04739v1",
    "published_date": "2024-01-09 05:57:35 UTC",
    "updated_date": "2024-01-09 05:57:35 UTC"
  },
  {
    "arxiv_id": "2401.04362v1",
    "title": "Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example",
    "authors": [
      "Kwan Yun",
      "Youngseo Kim",
      "Kwanggyoon Seo",
      "Chang Wook Seo",
      "Junyong Noh"
    ],
    "abstract": "We introduce DiffSketch, a method for generating a variety of stylized\nsketches from images. Our approach focuses on selecting representative features\nfrom the rich semantics of deep features within a pretrained diffusion model.\nThis novel sketch generation method can be trained with one manual drawing.\nFurthermore, efficient sketch extraction is ensured by distilling a trained\ngenerator into a streamlined extractor. We select denoising diffusion features\nthrough analysis and integrate these selected features with VAE features to\nproduce sketches. Additionally, we propose a sampling scheme for training\nmodels using a conditional generative approach. Through a series of\ncomparisons, we verify that distilled DiffSketch not only outperforms existing\nstate-of-the-art sketch extraction methods but also surpasses diffusion-based\nstylization methods in the task of extracting sketches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "68T01",
      "I.4.9"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages(main paper), 8 pages(supplementary material)",
    "pdf_url": "http://arxiv.org/pdf/2401.04362v1",
    "published_date": "2024-01-09 05:22:15 UTC",
    "updated_date": "2024-01-09 05:22:15 UTC"
  },
  {
    "arxiv_id": "2401.04361v1",
    "title": "Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning",
    "authors": [
      "Jiaan Wang",
      "Jianfeng Qu",
      "Kexin Wang",
      "Zhixu Li",
      "Wen Hua",
      "Ximing Li",
      "An Liu"
    ],
    "abstract": "Knowledge-grounded dialogue (KGD) learns to generate an informative response\nbased on a given dialogue context and external knowledge (\\emph{e.g.},\nknowledge graphs; KGs). Recently, the emergence of large language models (LLMs)\nand pre-training techniques has brought great success to knowledge-grounded\ndialogue. However, when building KGD systems in real applications, there are\nvarious real-world noises that are inevitable to face. For example, the\ndialogue context might involve perturbations such as misspellings and\nabbreviations. In addition, KGs typically suffer from incompletion and also\nmight contain erroneous and outdated facts. Such real-world noises pose a\nchallenge to the robustness of KGD systems and hinder their applications in the\nreal world. In this paper, we propose an entity-based contrastive learning\nframework for improving the robustness of KGD. Specifically, we make use of the\nentity information in a KGD sample to create both its positive and negative\nsamples which involve semantic-irrelevant and semantic-relevant perturbations,\nrespectively. The contrastive learning framework ensures the KGD model is aware\nof these two types of perturbations, thus generating informative responses with\nthe potentially noisy inputs in real applications. Experimental results on\nthree benchmark datasets show that our method achieves new state-of-the-art\nperformance in terms of automatic evaluation scores, verifying its\neffectiveness and potentiality. Furthermore, we show that our method can\ngenerate better responses than comparison models in both the noisy and the\nfew-shot settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.04361v1",
    "published_date": "2024-01-09 05:16:52 UTC",
    "updated_date": "2024-01-09 05:16:52 UTC"
  },
  {
    "arxiv_id": "2401.04357v1",
    "title": "Iterative Feedback Network for Unsupervised Point Cloud Registration",
    "authors": [
      "Yifan Xie",
      "Boyu Wang",
      "Shiqi Li",
      "Jihua Zhu"
    ],
    "abstract": "As a fundamental problem in computer vision, point cloud registration aims to\nseek the optimal transformation for aligning a pair of point clouds. In most\nexisting methods, the information flows are usually forward transferring, thus\nlacking the guidance from high-level information to low-level information.\nBesides, excessive high-level information may be overly redundant, and directly\nusing it may conflict with the original low-level information. In this paper,\nwe propose a novel Iterative Feedback Network (IFNet) for unsupervised point\ncloud registration, in which the representation of low-level features is\nefficiently enriched by rerouting subsequent high-level features. Specifically,\nour IFNet is built upon a series of Feedback Registration Block (FRB) modules,\nwith each module responsible for generating the feedforward rigid\ntransformation and feedback high-level features. These FRB modules are cascaded\nand recurrently unfolded over time. Further, the Feedback Transformer is\ndesigned to efficiently select relevant information from feedback high-level\nfeatures, which is utilized to refine the low-level features. What's more, we\nincorporate a geometry-awareness descriptor to empower the network for making\nfull use of most geometric information, which leads to more precise\nregistration results. Extensive experiments on various benchmark datasets\ndemonstrate the superior registration performance of our IFNet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, accepted by RAL 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.04357v1",
    "published_date": "2024-01-09 04:44:12 UTC",
    "updated_date": "2024-01-09 04:44:12 UTC"
  },
  {
    "arxiv_id": "2401.04351v1",
    "title": "A Change Point Detection Integrated Remaining Useful Life Estimation Model under Variable Operating Conditions",
    "authors": [
      "Anushiya Arunan",
      "Yan Qin",
      "Xiaoli Li",
      "Chau Yuen"
    ],
    "abstract": "By informing the onset of the degradation process, health status evaluation\nserves as a significant preliminary step for reliable remaining useful life\n(RUL) estimation of complex equipment. This paper proposes a novel temporal\ndynamics learning-based model for detecting change points of individual\ndevices, even under variable operating conditions, and utilises the learnt\nchange points to improve the RUL estimation accuracy. During offline model\ndevelopment, the multivariate sensor data are decomposed to learn fused\ntemporal correlation features that are generalisable and representative of\nnormal operation dynamics across multiple operating conditions. Monitoring\nstatistics and control limit thresholds for normal behaviour are dynamically\nconstructed from these learnt temporal features for the unsupervised detection\nof device-level change points. The detected change points then inform the\ndegradation data labelling for training a long short-term memory (LSTM)-based\nRUL estimation model. During online monitoring, the temporal correlation\ndynamics of a query device is monitored for breach of the control limit derived\nin offline training. If a change point is detected, the device's RUL is\nestimated with the well-trained offline model for early preventive action.\nUsing C-MAPSS turbofan engines as the case study, the proposed method improved\nthe accuracy by 5.6\\% and 7.5\\% for two scenarios with six operating\nconditions, when compared to existing LSTM-based RUL estimation models that do\nnot consider heterogeneous change points.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in Control Engineering Practice Journal with DOI:\n  https://doi.org/10.1016/j.conengprac.2023.105840",
    "pdf_url": "http://arxiv.org/pdf/2401.04351v1",
    "published_date": "2024-01-09 04:35:17 UTC",
    "updated_date": "2024-01-09 04:35:17 UTC"
  },
  {
    "arxiv_id": "2401.04339v2",
    "title": "Memory-Efficient Fine-Tuning for Quantized Diffusion Model",
    "authors": [
      "Hyogon Ryu",
      "Seohyun Lim",
      "Hyunjung Shim"
    ],
    "abstract": "The emergence of billion-parameter diffusion models such as Stable Diffusion\nXL, Imagen, and DALL-E 3 has significantly propelled the domain of generative\nAI. However, their large-scale architecture presents challenges in fine-tuning\nand deployment due to high resource demands and slow inference speed. This\npaper explores the relatively unexplored yet promising realm of fine-tuning\nquantized diffusion models. Our analysis revealed that the baseline neglects\nthe distinct patterns in model weights and the different roles throughout time\nsteps when finetuning the diffusion model. To address these limitations, we\nintroduce a novel memory-efficient fine-tuning method specifically designed for\nquantized diffusion models, dubbed TuneQDM. Our approach introduces\nquantization scales as separable functions to consider inter-channel weight\npatterns. Then, it optimizes these scales in a timestep-specific manner for\neffective reflection of the role of each time step. TuneQDM achieves\nperformance on par with its full-precision counterpart while simultaneously\noffering significant memory efficiency. Experimental results demonstrate that\nour method consistently outperforms the baseline in both single-/multi-subject\ngenerations, exhibiting high subject fidelity and prompt fidelity comparable to\nthe full precision model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV2024. Code will be released at\n  https://github.com/ugonfor/TuneQDM",
    "pdf_url": "http://arxiv.org/pdf/2401.04339v2",
    "published_date": "2024-01-09 03:42:08 UTC",
    "updated_date": "2024-07-18 11:38:17 UTC"
  },
  {
    "arxiv_id": "2401.04336v3",
    "title": "Deep Efficient Private Neighbor Generation for Subgraph Federated Learning",
    "authors": [
      "Ke Zhang",
      "Lichao Sun",
      "Bolin Ding",
      "Siu Ming Yiu",
      "Carl Yang"
    ],
    "abstract": "Behemoth graphs are often fragmented and separately stored by multiple data\nowners as distributed subgraphs in many realistic applications. Without harming\ndata privacy, it is natural to consider the subgraph federated learning\n(subgraph FL) scenario, where each local client holds a subgraph of the entire\nglobal graph, to obtain globally generalized graph mining models. To overcome\nthe unique challenge of incomplete information propagation on local subgraphs\ndue to missing cross-subgraph neighbors, previous works resort to the\naugmentation of local neighborhoods through the joint FL of missing neighbor\ngenerators and GNNs. Yet their technical designs have profound limitations\nregarding the utility, efficiency, and privacy goals of FL. In this work, we\npropose FedDEP to comprehensively tackle these challenges in subgraph FL.\nFedDEP consists of a series of novel technical designs: (1) Deep neighbor\ngeneration through leveraging the GNN embeddings of potential missing\nneighbors; (2) Efficient pseudo-FL for neighbor generation through embedding\nprototyping; and (3) Privacy protection through noise-less\nedge-local-differential-privacy. We analyze the correctness and efficiency of\nFedDEP, and provide theoretical guarantees on its privacy. Empirical results on\nfour real-world datasets justify the clear benefits of proposed techniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to SDM 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.04336v3",
    "published_date": "2024-01-09 03:29:40 UTC",
    "updated_date": "2024-01-19 01:30:04 UTC"
  },
  {
    "arxiv_id": "2401.04334v1",
    "title": "Large Language Models for Robotics: Opportunities, Challenges, and Perspectives",
    "authors": [
      "Jiaqi Wang",
      "Zihao Wu",
      "Yiwei Li",
      "Hanqi Jiang",
      "Peng Shu",
      "Enze Shi",
      "Huawen Hu",
      "Chong Ma",
      "Yiheng Liu",
      "Xuhui Wang",
      "Yincheng Yao",
      "Xuan Liu",
      "Huaqin Zhao",
      "Zhengliang Liu",
      "Haixing Dai",
      "Lin Zhao",
      "Bao Ge",
      "Xiang Li",
      "Tianming Liu",
      "Shu Zhang"
    ],
    "abstract": "Large language models (LLMs) have undergone significant expansion and have\nbeen increasingly integrated across various domains. Notably, in the realm of\nrobot task planning, LLMs harness their advanced reasoning and language\ncomprehension capabilities to formulate precise and efficient action plans\nbased on natural language instructions. However, for embodied tasks, where\nrobots interact with complex environments, text-only LLMs often face challenges\ndue to a lack of compatibility with robotic visual perception. This study\nprovides a comprehensive overview of the emerging integration of LLMs and\nmultimodal LLMs into various robotic tasks. Additionally, we propose a\nframework that utilizes multimodal GPT-4V to enhance embodied task planning\nthrough the combination of natural language instructions and robot visual\nperceptions. Our results, based on diverse datasets, indicate that GPT-4V\neffectively enhances robot performance in embodied tasks. This extensive survey\nand evaluation of LLMs and multimodal LLMs across a variety of robotic tasks\nenriches the understanding of LLM-centric embodied intelligence and provides\nforward-looking insights toward bridging the gap in Human-Robot-Environment\ninteraction.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04334v1",
    "published_date": "2024-01-09 03:22:16 UTC",
    "updated_date": "2024-01-09 03:22:16 UTC"
  },
  {
    "arxiv_id": "2401.04331v2",
    "title": "Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study",
    "authors": [
      "Qiyu Kang",
      "Kai Zhao",
      "Yang Song",
      "Yihang Xie",
      "Yanan Zhao",
      "Sijie Wang",
      "Rui She",
      "Wee Peng Tay"
    ],
    "abstract": "In this work, we rigorously investigate the robustness of graph neural\nfractional-order differential equation (FDE) models. This framework extends\nbeyond traditional graph neural (integer-order) ordinary differential equation\n(ODE) models by implementing the time-fractional Caputo derivative. Utilizing\nfractional calculus allows our model to consider long-term memory during the\nfeature updating process, diverging from the memoryless Markovian updates seen\nin traditional graph neural ODE models. The superiority of graph neural FDE\nmodels over graph neural ODE models has been established in environments free\nfrom attacks or perturbations. While traditional graph neural ODE models have\nbeen verified to possess a degree of stability and resilience in the presence\nof adversarial attacks in existing literature, the robustness of graph neural\nFDE models, especially under adversarial conditions, remains largely\nunexplored. This paper undertakes a detailed assessment of the robustness of\ngraph neural FDE models. We establish a theoretical foundation outlining the\nrobustness characteristics of graph neural FDE models, highlighting that they\nmaintain more stringent output perturbation bounds in the face of input and\ngraph topology disturbances, compared to their integer-order counterparts. Our\nempirical evaluations further confirm the enhanced robustness of graph neural\nFDE models, highlighting their potential in adversarially robust applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "in Proc. AAAI Conference on Artificial Intelligence, Vancouver,\n  Canada, Feb. 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.04331v2",
    "published_date": "2024-01-09 02:56:52 UTC",
    "updated_date": "2024-03-04 05:57:06 UTC"
  },
  {
    "arxiv_id": "2401.04330v2",
    "title": "BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method guided by multi-scale feature information aggregation",
    "authors": [
      "Yonghui Tan",
      "Xiaolong Li",
      "Yishu Chen",
      "Jinquan Ai"
    ],
    "abstract": "The purpose of remote sensing image change detection (RSCD) is to detect\ndifferences between bi-temporal images taken at the same place. Deep learning\nhas been extensively used to RSCD tasks, yielding significant results in terms\nof result recognition. However, due to the shooting angle of the satellite, the\nimpacts of thin clouds, and certain lighting conditions, the problem of fuzzy\nedges in the change region in some remote sensing photographs cannot be\nproperly handled using current RSCD algorithms. To solve this issue, we\nproposed a Body Decouple Multi-Scale by fearure Aggregation change detection\n(BD-MSA), a novel model that collects both global and local feature map\ninformation in the channel and space dimensions of the feature map during the\ntraining and prediction phases. This approach allows us to successfully extract\nthe change region's boundary information while also divorcing the change\nregion's main body from its boundary. Numerous studies have shown that the\nassessment metrics and evaluation effects of the model described in this paper\non the publicly available datasets DSIFN-CD, S2Looking and WHU-CD are the best\nwhen compared to other models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04330v2",
    "published_date": "2024-01-09 02:53:06 UTC",
    "updated_date": "2024-03-03 08:39:20 UTC"
  },
  {
    "arxiv_id": "2401.04319v3",
    "title": "Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs",
    "authors": [
      "Junjie Wang",
      "Dan Yang",
      "Binbin Hu",
      "Yue Shen",
      "Wen Zhang",
      "Jinjie Gu"
    ],
    "abstract": "In this paper, we explore a new way for user targeting, where non-expert\nmarketers could select their target users solely given demands in natural\nlanguage form. The key to this issue is how to transform natural languages into\npractical structured logical languages, i.e., the structured understanding of\nmarketer demands. In practical scenarios, the demands of non-expert marketers\nare often abstract and diverse. Considering the impressive natural language\nprocessing ability of large language models (LLMs), we try to leverage LLMs to\nsolve this issue. To stimulate the LLMs' reasoning ability, the\nchain-of-thought (CoT) prompting method is widely used, but existing methods\nstill have some limitations in our scenario: (1) Previous methods either use\nsimple \"Let's think step by step\" spells or provide fixed examples in\ndemonstrations without considering compatibility between prompts and concrete\nquestions, making LLMs ineffective when the marketers' demands are abstract and\ndiverse. (2) Previous methods are often implemented in closed-source models or\nexcessively large models, which is not suitable in industrial practical\nscenarios. Based on these, we propose ARALLM (i.e., Analogical Reasoning\nAugmented Large Language Models) consisting of two modules: Analogical\nReasoning based Prompting and Reasoning-Augmented Multi-Task Model\nDistillation. Part of our data and code can be found at\nhttps://github.com/alipay/Analogic-Reasoning-Augmented-Large-Language-Model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.04319v3",
    "published_date": "2024-01-09 02:25:23 UTC",
    "updated_date": "2024-06-12 03:02:45 UTC"
  },
  {
    "arxiv_id": "2401.04737v1",
    "title": "Music Genre Classification: A Comparative Analysis of CNN and XGBoost Approaches with Mel-frequency cepstral coefficients and Mel Spectrograms",
    "authors": [
      "Yigang Meng"
    ],
    "abstract": "In recent years, various well-designed algorithms have empowered music\nplatforms to provide content based on one's preferences. Music genres are\ndefined through various aspects, including acoustic features and cultural\nconsiderations. Music genre classification works well with content-based\nfiltering, which recommends content based on music similarity to users. Given a\nconsiderable dataset, one premise is automatic annotation using machine\nlearning or deep learning methods that can effectively classify audio files.\nThe effectiveness of systems largely depends on feature and model selection, as\ndifferent architectures and features can facilitate each other and yield\ndifferent results. In this study, we conduct a comparative study investigating\nthe performances of three models: a proposed convolutional neural network\n(CNN), the VGG16 with fully connected layers (FC), and an eXtreme Gradient\nBoosting (XGBoost) approach on different features: 30-second Mel spectrogram\nand 3-second Mel-frequency cepstral coefficients (MFCCs). The results show that\nthe MFCC XGBoost model outperformed the others. Furthermore, applying data\nsegmentation in the data preprocessing phase can significantly enhance the\nperformance of the CNNs.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04737v1",
    "published_date": "2024-01-09 01:50:31 UTC",
    "updated_date": "2024-01-09 01:50:31 UTC"
  },
  {
    "arxiv_id": "2401.06796v1",
    "title": "AI Hallucinations: A Misnomer Worth Clarifying",
    "authors": [
      "Negar Maleki",
      "Balaji Padmanabhan",
      "Kaushik Dutta"
    ],
    "abstract": "As large language models continue to advance in Artificial Intelligence (AI),\ntext generation systems have been shown to suffer from a problematic phenomenon\ntermed often as \"hallucination.\" However, with AI's increasing presence across\nvarious domains including medicine, concerns have arisen regarding the use of\nthe term itself. In this study, we conducted a systematic review to identify\npapers defining \"AI hallucination\" across fourteen databases. We present and\nanalyze definitions obtained across all databases, categorize them based on\ntheir applications, and extract key points within each category. Our results\nhighlight a lack of consistency in how the term is used, but also help identify\nseveral alternative terms in the literature. We discuss implications of these\nand call for a more unified effort to bring consistency to an important\ncontemporary AI issue that can affect multiple domains significantly.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.06796v1",
    "published_date": "2024-01-09 01:49:41 UTC",
    "updated_date": "2024-01-09 01:49:41 UTC"
  },
  {
    "arxiv_id": "2401.04290v1",
    "title": "StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments",
    "authors": [
      "Sean Kulinski",
      "Nicholas R. Waytowich",
      "James Z. Hare",
      "David I. Inouye"
    ],
    "abstract": "Spatial reasoning tasks in multi-agent environments such as event prediction,\nagent type identification, or missing data imputation are important for\nmultiple applications (e.g., autonomous surveillance over sensor networks and\nsubtasks for reinforcement learning (RL)). StarCraft II game replays encode\nintelligent (and adversarial) multi-agent behavior and could provide a testbed\nfor these tasks; however, extracting simple and standardized representations\nfor prototyping these tasks is laborious and hinders reproducibility. In\ncontrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled\nrapid prototyping and reproducibility of ML methods. Following the simplicity\nof these datasets, we construct a benchmark spatial reasoning dataset based on\nStarCraft II replays that exhibit complex multi-agent behaviors, while still\nbeing as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize\na window of 255 consecutive game states to create 3.6 million summary images\nfrom 60,000 replays, including all relevant metadata such as game outcome and\nplayer races. We develop three formats of decreasing complexity: Hyperspectral\nimages that include one channel for every unit type (similar to multispectral\ngeospatial images), RGB images that mimic CIFAR10, and grayscale images that\nmimic MNIST. We show how this dataset can be used for prototyping spatial\nreasoning methods. All datasets, code for extraction, and code for dataset\nloading can be found at https://starcraftdata.davidinouye.com",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in CVPR 23'",
    "pdf_url": "http://arxiv.org/pdf/2401.04290v1",
    "published_date": "2024-01-09 00:05:56 UTC",
    "updated_date": "2024-01-09 00:05:56 UTC"
  }
]